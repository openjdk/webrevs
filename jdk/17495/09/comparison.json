{"files":[{"patch":"@@ -2208,4 +2208,3 @@\n-    st->print_cr(\"\\tldrw rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    if (CompressedKlassPointers::shift() != 0) {\n-      st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    }\n+    st->print_cr(\"\\tldrw rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldrw r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpw rscratch1, r10\");\n@@ -2213,1 +2212,3 @@\n-   st->print_cr(\"\\tldr rscratch1, j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tldr r10, [rscratch2 + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmp rscratch1, r10\");\n@@ -2215,1 +2216,0 @@\n-  st->print_cr(\"\\tcmp r0, rscratch1\\t # Inline cache check\");\n@@ -2224,8 +2224,1 @@\n-\n-  __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n-  Label skip;\n-  \/\/ TODO\n-  \/\/ can we avoid this skip and still use a reloc?\n-  __ br(Assembler::EQ, skip);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  __ bind(skip);\n+  __ ic_check(InteriorEntryAlignment);\n@@ -3718,1 +3711,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, call);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, call);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":8,"deletions":15,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -56,1 +56,0 @@\n-const Register IC_Klass    = rscratch2;   \/\/ where the IC klass is cached\n@@ -296,21 +295,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  int start_offset = __ offset();\n-  __ inline_cache_check(receiver, ic_klass);\n-\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  Label dont;\n-  __ br(Assembler::EQ, dont);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ We align the verified entry point unless the method body\n-  \/\/ (including its inline cache check) will fit in a single 64-byte\n-  \/\/ icache line.\n-  if (! method()->is_accessor() || __ offset() - start_offset > 4 * 4) {\n-    \/\/ force alignment after the cache check.\n-    __ align(CodeEntryAlignment);\n-  }\n-\n-  __ bind(dont);\n-  return start_offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -2045,1 +2024,1 @@\n-  assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()\n+  assert(__ offset() - start + CompiledDirectCall::to_trampoline_stub_size()\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":2,"deletions":23,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -74,2 +74,2 @@\n-    \/\/ call stub: CompiledStaticCall::to_interp_stub_size() +\n-    \/\/            CompiledStaticCall::to_trampoline_stub_size()\n+    \/\/ call stub: CompiledDirectCall::to_interp_stub_size() +\n+    \/\/            CompiledDirectCall::to_trampoline_stub_size()\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -311,11 +311,0 @@\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-\n-  cmp_klass(receiver, iCache, rscratch1);\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +38,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n@@ -74,1 +73,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -78,1 +77,1 @@\n-int CompiledStaticCall::to_trampoline_stub_size() {\n+int CompiledDirectCall::to_trampoline_stub_size() {\n@@ -86,1 +85,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -90,1 +89,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -94,7 +93,0 @@\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n-                  p2i(instruction_address()),\n-                  callee->name_and_sig_as_C_string());\n-  }\n-\n@@ -118,1 +110,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -135,1 +127,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/aarch64\/compiledIC_aarch64.cpp","additions":7,"deletions":15,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1,82 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_aarch64.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  return (MacroAssembler::far_branches() ? 6 : 4) * NativeInstruction::instruction_size;\n-}\n-\n-#define __ masm->\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point) {\n-  ResourceMark rm;\n-  CodeBuffer      code(code_begin, ic_stub_code_size());\n-  MacroAssembler* masm            = new MacroAssembler(&code);\n-  \/\/ note: even though the code contains an embedded value, we do not need reloc info\n-  \/\/ because\n-  \/\/ (1) the value is old (i.e., doesn't matter for scavenges)\n-  \/\/ (2) these ICStubs are removed *before* a GC happens, so the roots disappear\n-  \/\/ assert(cached_value == nullptr || cached_oop->is_perm(), \"must be perm oop\");\n-\n-  address start = __ pc();\n-  Label l;\n-  __ ldr(rscratch2, l);\n-  int jump_code_size = __ far_jump(ExternalAddress(entry_point));\n-  \/\/ IC stub code size is not expected to vary depending on target address.\n-  \/\/ We use NOPs to make the [ldr + far_jump + nops + int64] stub size equal to ic_stub_code_size.\n-  for (int size = NativeInstruction::instruction_size + jump_code_size + 8;\n-           size < ic_stub_code_size(); size += NativeInstruction::instruction_size) {\n-    __ nop();\n-  }\n-  __ bind(l);\n-  assert((uintptr_t)__ pc() % wordSize == 0, \"\");\n-  __ emit_int64((int64_t)cached_value);\n-  \/\/ Only need to invalidate the 1st two instructions - not the whole ic stub\n-  ICache::invalidate_range(code_begin, InlineCacheBuffer::ic_stub_code_size());\n-  assert(__ pc() - start == ic_stub_code_size(), \"must be\");\n-}\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ creation also verifies the object\n-  NativeJump* jump = nativeJump_at(code_begin + 4);\n-  return jump->jump_destination();\n-}\n-\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  \/\/ The word containing the cached value is at the end of this IC buffer\n-  uintptr_t *p = (uintptr_t *)(code_begin + ic_stub_code_size() - wordSize);\n-  void* o = (void*)*p;\n-  return o;\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/icBuffer_aarch64.cpp","additions":0,"deletions":82,"binary":false,"changes":82,"status":"deleted"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -968,1 +969,1 @@\n-  \/\/ CompiledDirectStaticCall::set_to_interpreted knows the\n+  \/\/ CompiledDirectCall::set_to_interpreted knows the\n@@ -998,1 +999,1 @@\n-  movptr(rscratch2, (uintptr_t)Universe::non_oop_word());\n+  movptr(rscratch2, (intptr_t)Universe::non_oop_word());\n@@ -1002,0 +1003,41 @@\n+int MacroAssembler::ic_check_size() {\n+  if (target_needs_far_branch(CAST_FROM_FN_PTR(address, SharedRuntime::get_ic_miss_stub()))) {\n+    return NativeInstruction::instruction_size * 7;\n+  } else {\n+    return NativeInstruction::instruction_size * 5;\n+  }\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = j_rarg0;\n+  Register data = rscratch2;\n+  Register tmp1 = rscratch1;\n+  Register tmp2 = r10;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+    cmp(tmp1, tmp2);\n+  }\n+\n+  Label dont;\n+  br(Assembler::EQ, dont);\n+  far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  bind(dont);\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -1103,1 +1145,8 @@\n-  while (offset() % modulus != 0) nop();\n+  align(modulus, offset());\n+}\n+\n+\/\/ Ensure that the code at target bytes offset from the current offset() is aligned\n+\/\/ according to modulus.\n+void MacroAssembler::align(int modulus, int target) {\n+  int delta = target - offset();\n+  while ((offset() + delta) % modulus != 0) nop();\n@@ -1200,1 +1249,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":53,"deletions":4,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -723,0 +723,1 @@\n+  void align(int modulus, int target);\n@@ -1250,0 +1251,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -42,1 +41,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -743,3 +741,1 @@\n-  Label ok;\n-\n-  Register holder = rscratch2;\n+  Register data = rscratch2;\n@@ -760,8 +756,0 @@\n-    __ load_klass(rscratch1, receiver);\n-    __ ldr(tmp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ cmp(rscratch1, tmp);\n-    __ ldr(rmethod, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ br(Assembler::EQ, ok);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-    __ bind(ok);\n@@ -771,0 +759,3 @@\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ ldr(rmethod, Address(data, CompiledICData::speculated_method_offset()));\n+\n@@ -1121,1 +1112,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1186,1 +1177,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1541,2 +1532,0 @@\n-\n-  const Register ic_reg = rscratch2;\n@@ -1545,1 +1534,0 @@\n-  Label hit;\n@@ -1548,1 +1536,1 @@\n-  assert_different_registers(ic_reg, receiver, rscratch1);\n+  assert_different_registers(receiver, rscratch1);\n@@ -1550,4 +1538,1 @@\n-  __ cmp_klass(receiver, ic_reg, rscratch1);\n-  __ br(Assembler::EQ, hit);\n-\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  __ ic_check(8 \/* end_alignment *\/);\n@@ -1556,4 +1541,0 @@\n-  __ align(8);\n-\n-  __ bind(hit);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":8,"deletions":27,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -171,1 +171,1 @@\n-  \/\/  rscratch2: CompiledICHolder\n+  \/\/  rscratch2: CompiledICData\n@@ -177,1 +177,1 @@\n-  const Register holder_klass_reg   = r16; \/\/ declaring interface klass (DECC)\n+  const Register holder_klass_reg   = r16; \/\/ declaring interface klass (DEFC)\n@@ -181,1 +181,1 @@\n-  const Register icholder_reg       = rscratch2;\n+  const Register icdata_reg         = rscratch2;\n@@ -185,2 +185,2 @@\n-  __ ldr(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n-  __ ldr(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+  __ ldr(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n+  __ ldr(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -872,6 +872,1 @@\n-  if (UseCompressedClassPointers) {\n-    st->print_cr(\"\\tLDR_w \" R_RTEMP \",[R_R0 + oopDesc::klass_offset_in_bytes]\\t! Inline cache check\");\n-    st->print_cr(\"\\tdecode_klass \" R_RTEMP);\n-  } else {\n-    st->print_cr(\"\\tLDR   \" R_RTEMP \",[R_R0 + oopDesc::klass_offset_in_bytes]\\t! Inline cache check\");\n-  }\n+  st->print_cr(\"\\tLDR   \" R_RTEMP \",[R_R0 + oopDesc::klass_offset_in_bytes]\\t! Inline cache check\");\n@@ -885,7 +880,1 @@\n-  Register iCache  = reg_to_register_object(Matcher::inline_cache_reg_encode());\n-  assert(iCache == Ricklass, \"should be\");\n-  Register receiver = R0;\n-\n-  __ load_klass(Rtemp, receiver);\n-  __ cmp(Rtemp, iCache);\n-  __ jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, noreg, ne);\n+  __ ic_check(InteriorEntryAlignment);\n@@ -1244,1 +1233,1 @@\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+      address stub = CompiledDirectCall::emit_to_interp_stub(cbuf);\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":3,"deletions":14,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -164,4 +164,1 @@\n-  Register receiver = LIR_Assembler::receiverOpr()->as_register();\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, Ricklass);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -1953,1 +1950,1 @@\n-  \/\/ (See CompiledStaticCall::set_to_interpreted())\n+  \/\/ (See CompiledDirectCall::set_to_interpreted())\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -46,10 +46,0 @@\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  Label verified;\n-  load_klass(Rtemp, receiver);\n-  cmp(Rtemp, iCache);\n-  b(verified, eq); \/\/ jump over alignment no-ops\n-  jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);\n-  align(CodeEntryAlignment);\n-  bind(verified);\n-}\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/arm\/c1_Runtime1_arm.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -40,1 +39,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n@@ -62,1 +61,1 @@\n-  \/\/ CompiledStaticCall::set_to_interpreted()\n+  \/\/ CompiledDirectCall::set_to_interpreted()\n@@ -90,1 +89,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -95,1 +94,1 @@\n-int CompiledStaticCall::to_trampoline_stub_size() {\n+int CompiledDirectCall::to_trampoline_stub_size() {\n@@ -101,1 +100,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -105,1 +104,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -109,7 +108,0 @@\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n-                  p2i(instruction_address()),\n-                  callee->name_and_sig_as_C_string());\n-  }\n-\n@@ -131,1 +123,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -147,1 +139,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/arm\/compiledIC_arm.cpp","additions":8,"deletions":16,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -1,65 +0,0 @@\n-\/*\n- * Copyright (c) 2008, 2016, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_arm.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-#define __ masm->\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  return (4 * Assembler::InstructionSize);\n-}\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point) {\n-  ResourceMark rm;\n-  CodeBuffer code(code_begin, ic_stub_code_size());\n-  MacroAssembler* masm = new MacroAssembler(&code);\n-\n-  InlinedAddress oop_literal((address) cached_value);\n-  __ ldr_literal(Ricklass, oop_literal);\n-  \/\/ FIXME: OK to remove reloc here?\n-  __ patchable_jump(entry_point, relocInfo::runtime_call_type, Rtemp);\n-  __ bind_literal(oop_literal);\n-  __ flush();\n-}\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  address jump_address;\n-  jump_address = code_begin + NativeInstruction::instruction_size;\n-  NativeJump* jump = nativeJump_at(jump_address);\n-  return jump->jump_destination();\n-}\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);\n-  return (void*)move->data();\n-}\n-\n-#undef __\n","filename":"src\/hotspot\/cpu\/arm\/icBuffer_arm.cpp","additions":0,"deletions":65,"binary":false,"changes":65,"status":"deleted"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -300,0 +301,4 @@\n+void MacroAssembler::align(int modulus, int target) {\n+  int delta = target - offset();\n+  while ((offset() + delta) % modulus != 0) nop();\n+}\n@@ -302,3 +307,1 @@\n-  while (offset() % modulus != 0) {\n-    nop();\n-  }\n+  align(modulus, offset());\n@@ -1863,0 +1866,28 @@\n+\n+int MacroAssembler::ic_check_size() {\n+  return NativeInstruction::instruction_size * 7;\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = j_rarg0;\n+  Register tmp1 = R4;\n+  Register tmp2 = R5;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  ldr(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+  ldr(tmp2, Address(Ricklass, CompiledICData::speculated_klass_offset()));\n+  cmp(tmp1, tmp2);\n+\n+  Label dont;\n+  b(dont, eq);\n+  jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type);\n+  bind(dont);\n+  return uep_offset;\n+}\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -224,0 +224,1 @@\n+  void align(int modulus, int target);\n@@ -1080,0 +1081,3 @@\n+\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/cpu\/arm\/nativeInst_arm_32.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -388,1 +388,1 @@\n-    \/\/ NOTE: CompiledStaticCall::set_to_interpreted() calls this but\n+    \/\/ NOTE: CompiledDirectCall::set_to_interpreted() calls this but\n","filename":"src\/hotspot\/cpu\/arm\/nativeInst_arm_32.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -28,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -35,1 +35,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -629,1 +628,0 @@\n-  const Register receiver_klass = R4;\n@@ -631,4 +629,2 @@\n-  __ load_klass(receiver_klass, receiver);\n-  __ ldr(holder_klass, Address(Ricklass, CompiledICHolder::holder_klass_offset()));\n-  __ ldr(Rmethod, Address(Ricklass, CompiledICHolder::holder_metadata_offset()));\n-  __ cmp(receiver_klass, holder_klass);\n+  __ ic_check(1 \/* end_alignment *\/);\n+  __ ldr(Rmethod, Address(Ricklass, CompiledICData::speculated_method_offset()));\n@@ -822,1 +818,0 @@\n-  \/\/ Inline cache check, same as in C1_MacroAssembler::inline_cache_check()\n@@ -824,7 +819,3 @@\n-  __ load_klass(Rtemp, receiver);\n-  __ cmp(Rtemp, Ricklass);\n-  Label verified;\n-\n-  __ b(verified, eq); \/\/ jump over alignment no-ops too\n-  __ jump(SharedRuntime::get_ic_miss_stub(), relocInfo::runtime_call_type, Rtemp);\n-  __ align(CodeEntryAlignment);\n+  __ verify_oop(receiver);\n+  \/\/ Inline cache check\n+  __ ic_check(CodeEntryAlignment \/* end_alignment *\/);\n@@ -833,1 +824,0 @@\n-  __ bind(verified);\n@@ -836,1 +826,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":6,"deletions":17,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -163,1 +163,1 @@\n-  __ ldr(Rintf, Address(Ricklass, CompiledICHolder::holder_klass_offset()));\n+  __ ldr(Rintf, Address(Ricklass, CompiledICData::itable_refc_klass_offset()));\n@@ -174,1 +174,1 @@\n-  __ ldr(Rintf, Address(Ricklass, CompiledICHolder::holder_metadata_offset()));\n+  __ ldr(Rintf, Address(Ricklass, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/arm\/vtableStubs_arm.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -454,1 +454,1 @@\n-  \/\/ Guard against illegal branch targets, e.g. -1 (see CompiledStaticCall and ad-file).\n+  \/\/ Guard against illegal branch targets, e.g. -1 (see CompiledDirectCall and ad-file).\n@@ -468,1 +468,1 @@\n-  \/\/ Guard against illegal branch targets, e.g. -1 (see CompiledStaticCall and ad-file).\n+  \/\/ Guard against illegal branch targets, e.g. -1 (see CompiledDirectCall and ad-file).\n","filename":"src\/hotspot\/cpu\/ppc\/assembler_ppc.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -80,3 +80,1 @@\n-  int offset = __ offset();\n-  __ inline_cache_check(R3_ARG1, R19_inline_cache_reg);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,23 +43,0 @@\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  const Register temp_reg = R12_scratch2;\n-  Label Lmiss;\n-\n-  verify_oop(receiver, FILE_AND_LINE);\n-  load_klass_check_null(temp_reg, receiver, &Lmiss);\n-\n-  if (TrapBasedICMissChecks && TrapBasedNullChecks) {\n-    trap_ic_miss_check(temp_reg, iCache);\n-  } else {\n-    Label Lok;\n-    cmpd(CCR0, temp_reg, iCache);\n-    beq(CCR0, Lok);\n-    bind(Lmiss);\n-    \/\/load_const_optimized(temp_reg, SharedRuntime::get_ic_miss_stub(), R0);\n-    calculate_address_from_global_toc(temp_reg, SharedRuntime::get_ic_miss_stub(), true, true, false);\n-    mtctr(temp_reg);\n-    bctr();\n-    align(32, 12);\n-    bind(Lok);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":0,"deletions":23,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/c1_Runtime1_ppc.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -40,1 +39,1 @@\n-\/\/ A PPC CompiledDirectStaticCall looks like this:\n+\/\/ A PPC CompiledDirectCall looks like this:\n@@ -82,1 +81,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark\/* = nullptr*\/) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark\/* = nullptr*\/) {\n@@ -94,1 +93,1 @@\n-  address stub = __ start_a_stub(CompiledStaticCall::to_interp_stub_size());\n+  address stub = __ start_a_stub(CompiledDirectCall::to_interp_stub_size());\n@@ -138,1 +137,1 @@\n-  assert((__ offset() - stub_start_offset) <= CompiledStaticCall::to_interp_stub_size(),\n+  assert((__ offset() - stub_start_offset) <= CompiledDirectCall::to_interp_stub_size(),\n@@ -156,1 +155,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -162,1 +161,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -166,1 +165,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -170,7 +169,0 @@\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n-                  p2i(instruction_address()),\n-                  callee->name_and_sig_as_C_string());\n-  }\n-\n@@ -191,1 +183,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -207,1 +199,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/ppc\/compiledIC_ppc.cpp","additions":9,"deletions":17,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1,69 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2015, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2013 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_ppc.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-#define __ masm.\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  return MacroAssembler::load_const_size + MacroAssembler::b64_patchable_size;\n-}\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point) {\n-  ResourceMark rm;\n-  CodeBuffer code(code_begin, ic_stub_code_size());\n-  MacroAssembler masm(&code);\n-  \/\/ Note: even though the code contains an embedded metadata, we do not need reloc info\n-  \/\/ because\n-  \/\/ (1) the metadata is old (i.e., doesn't matter for scavenges)\n-  \/\/ (2) these ICStubs are removed *before* a GC happens, so the roots disappear.\n-\n-  \/\/ Load the oop ...\n-  __ load_const(R19_method, (address) cached_value, R0);\n-  \/\/ ... and jump to entry point.\n-  __ b64_patchable((address) entry_point, relocInfo::none);\n-\n-  __ flush();\n-}\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ creation also verifies the object\n-  NativeJump*        jump = nativeJump_at(move->next_instruction_address());\n-  return jump->jump_destination();\n-}\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ creation also verifies the object\n-  void* o = (void*)move->data();\n-  return o;\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/icBuffer_ppc.cpp","additions":0,"deletions":69,"binary":false,"changes":69,"status":"deleted"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -1198,0 +1199,75 @@\n+int MacroAssembler::ic_check_size() {\n+  bool implicit_null_checks_available = ImplicitNullChecks && os::zero_page_read_protected(),\n+       use_fast_receiver_null_check   = implicit_null_checks_available || TrapBasedNullChecks,\n+       use_trap_based_null_check      = !implicit_null_checks_available && TrapBasedNullChecks;\n+\n+  int num_ins;\n+  if (use_fast_receiver_null_check && TrapBasedICMissChecks) {\n+    num_ins = 3;\n+    if (use_trap_based_null_check) num_ins += 1;\n+  } else {\n+    num_ins = 7;\n+    if (!implicit_null_checks_available) num_ins += 2;\n+  }\n+  return num_ins * BytesPerInstWord;\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  bool implicit_null_checks_available = ImplicitNullChecks && os::zero_page_read_protected(),\n+       use_fast_receiver_null_check   = implicit_null_checks_available || TrapBasedNullChecks,\n+       use_trap_based_null_check      = !implicit_null_checks_available && TrapBasedNullChecks;\n+\n+  Register receiver = R3_ARG1;\n+  Register data = R19_inline_cache_reg;\n+  Register tmp1 = R11_scratch1;\n+  Register tmp2 = R12_scratch2;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, end_alignment, end_alignment - ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (use_fast_receiver_null_check && TrapBasedICMissChecks) {\n+    \/\/ Fast version which uses SIGTRAP\n+\n+    if (use_trap_based_null_check) {\n+      trap_null_check(receiver);\n+    }\n+    if (UseCompressedClassPointers) {\n+      lwz(tmp1, oopDesc::klass_offset_in_bytes(), receiver);\n+    } else {\n+      ld(tmp1, oopDesc::klass_offset_in_bytes(), receiver);\n+    }\n+    ld(tmp2, in_bytes(CompiledICData::speculated_klass_offset()), data);\n+    trap_ic_miss_check(tmp1, tmp2);\n+\n+  } else {\n+    \/\/ Slower version which doesn't use SIGTRAP\n+\n+    \/\/ Load stub address using toc (fixed instruction size, unlike load_const_optimized)\n+    calculate_address_from_global_toc(tmp1, SharedRuntime::get_ic_miss_stub(),\n+                                      true, true, false); \/\/ 2 instructions\n+    mtctr(tmp1);\n+\n+    if (!implicit_null_checks_available) {\n+      cmpdi(CCR0, receiver, 0);\n+      beqctr(CCR0);\n+    }\n+    if (UseCompressedClassPointers) {\n+      lwz(tmp1, oopDesc::klass_offset_in_bytes(), receiver);\n+    } else {\n+      ld(tmp1, oopDesc::klass_offset_in_bytes(), receiver);\n+    }\n+    ld(tmp2, in_bytes(CompiledICData::speculated_klass_offset()), data);\n+    cmpd(CCR0, tmp1, tmp2);\n+    bnectr(CCR0);\n+  }\n+\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -370,0 +370,3 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1981,36 +1981,1 @@\n-  \/\/ Inline_cache contains a klass.\n-  Register ic_klass       = as_Register(Matcher::inline_cache_reg_encode());\n-  Register receiver_klass = R12_scratch2;  \/\/ tmp\n-\n-  assert_different_registers(ic_klass, receiver_klass, R11_scratch1, R3_ARG1);\n-  assert(R11_scratch1 == R11, \"need prologue scratch register\");\n-\n-  \/\/ Check for NULL argument if we don't have implicit null checks.\n-  if (!ImplicitNullChecks || !os::zero_page_read_protected()) {\n-    if (TrapBasedNullChecks) {\n-      __ trap_null_check(R3_ARG1);\n-    } else {\n-      Label valid;\n-      __ cmpdi(CCR0, R3_ARG1, 0);\n-      __ bne_predict_taken(CCR0, valid);\n-      \/\/ We have a null argument, branch to ic_miss_stub.\n-      __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),\n-                           relocInfo::runtime_call_type);\n-      __ bind(valid);\n-    }\n-  }\n-  \/\/ Assume argument is not NULL, load klass from receiver.\n-  __ load_klass(receiver_klass, R3_ARG1);\n-\n-  if (TrapBasedICMissChecks) {\n-    __ trap_ic_miss_check(receiver_klass, ic_klass);\n-  } else {\n-    Label valid;\n-    __ cmpd(CCR0, receiver_klass, ic_klass);\n-    __ beq_predict_taken(CCR0, valid);\n-    \/\/ We have an unexpected klass, branch to ic_miss_stub.\n-    __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),\n-                         relocInfo::runtime_call_type);\n-    __ bind(valid);\n-  }\n-\n+  __ ic_check(CodeEntryAlignment);\n@@ -3455,1 +3420,1 @@\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+      address stub = CompiledDirectCall::emit_to_interp_stub(cbuf);\n@@ -3510,1 +3475,1 @@\n-      loadConLNodesTuple_create(ra_, n_toc, new immLOper((jlong)Universe::non_oop_word()),\n+      loadConLNodesTuple_create(ra_, n_toc, new immLOper((jlong) Universe::non_oop_word()),\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":3,"deletions":38,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -38,1 +37,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -1177,2 +1175,2 @@\n-  \/\/ inline_cache contains a compiledICHolder\n-  const Register ic             = R19_method;\n+  \/\/ inline_cache contains a CompiledICData\n+  const Register ic             = R19_inline_cache_reg;\n@@ -1189,33 +1187,2 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()),\n-         \"klass offset should reach into any page\");\n-  \/\/ Check for null argument if we don't have implicit null checks.\n-  if (!ImplicitNullChecks || !os::zero_page_read_protected()) {\n-    if (TrapBasedNullChecks) {\n-      __ trap_null_check(R3_ARG1);\n-    } else {\n-      Label valid;\n-      __ cmpdi(CCR0, R3_ARG1, 0);\n-      __ bne_predict_taken(CCR0, valid);\n-      \/\/ We have a null argument, branch to ic_miss_stub.\n-      __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),\n-                       relocInfo::runtime_call_type);\n-      __ BIND(valid);\n-    }\n-  }\n-  \/\/ Assume argument is not null, load klass from receiver.\n-  __ load_klass(receiver_klass, R3_ARG1);\n-\n-  __ ld(ic_klass, CompiledICHolder::holder_klass_offset(), ic);\n-\n-  if (TrapBasedICMissChecks) {\n-    __ trap_ic_miss_check(receiver_klass, ic_klass);\n-  } else {\n-    Label valid;\n-    __ cmpd(CCR0, receiver_klass, ic_klass);\n-    __ beq_predict_taken(CCR0, valid);\n-    \/\/ We have an unexpected klass, branch to ic_miss_stub.\n-    __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),\n-                     relocInfo::runtime_call_type);\n-    __ BIND(valid);\n-  }\n-\n+  __ ic_check(4 \/* end_alignment *\/);\n+  __ ld(R19_method, CompiledICData::speculated_method_offset(), ic);\n@@ -1224,4 +1191,0 @@\n-  \/\/ Extract method from inline cache, verified entry point needs it.\n-  __ ld(R19_method, CompiledICHolder::holder_metadata_offset(), ic);\n-  assert(R19_method == ic, \"the inline cache register is dead here\");\n-\n@@ -1801,1 +1764,1 @@\n-    stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, c2i_call_pc);\n+    stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, c2i_call_pc);\n@@ -1894,1 +1857,1 @@\n-  stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, call_pc);\n+  stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, call_pc);\n@@ -2190,1 +2153,0 @@\n-  Label    ic_miss;\n@@ -2214,10 +2176,1 @@\n-  Register ic = R19_inline_cache_reg;\n-  Register receiver_klass = r_temp_1;\n-\n-  __ cmpdi(CCR0, R3_ARG1, 0);\n-  __ beq(CCR0, ic_miss);\n-  __ verify_oop(R3_ARG1, FILE_AND_LINE);\n-  __ load_klass(receiver_klass, R3_ARG1);\n-\n-  __ cmpd(CCR0, receiver_klass, ic);\n-  __ bne(CCR0, ic_miss);\n+    __ ic_check(4 \/* end_alignment *\/);\n@@ -2226,1 +2179,0 @@\n-\n@@ -2706,10 +2658,0 @@\n-  \/\/ Handler for a cache miss (out-of-line).\n-  \/\/ --------------------------------------------------------------------------\n-\n-  if (!method_is_static) {\n-  __ bind(ic_miss);\n-\n-  __ b64_patchable((address)SharedRuntime::get_ic_miss_stub(),\n-                       relocInfo::runtime_call_type);\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":7,"deletions":65,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -184,1 +184,1 @@\n-  __ ld(interface, CompiledICHolder::holder_klass_offset(), R19_method);\n+  __ ld(interface, CompiledICData::itable_refc_klass_offset(), R19_method);\n@@ -190,1 +190,1 @@\n-  __ ld(interface, CompiledICHolder::holder_metadata_offset(), R19_method);\n+  __ ld(interface, CompiledICData::itable_defc_klass_offset(), R19_method);\n","filename":"src\/hotspot\/cpu\/ppc\/vtableStubs_ppc_64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-const Register IC_Klass    = t1;    \/\/ where the IC klass is cached\n@@ -268,20 +267,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  int start_offset = __ offset();\n-  Label dont;\n-  __ inline_cache_check(receiver, ic_klass, dont);\n-\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ We align the verified entry point unless the method body\n-  \/\/ (including its inline cache check) will fit in a single 64-byte\n-  \/\/ icache line.\n-  if (!method()->is_accessor() || __ offset() - start_offset > 4 * 4) {\n-    \/\/ force alignment after the cache check.\n-    __ align(CodeEntryAlignment);\n-  }\n-\n-  __ bind(dont);\n-  return start_offset;\n+  return __ ic_check(CodeEntryAlignment);\n@@ -1401,1 +1381,1 @@\n-  assert(__ offset() - start + CompiledStaticCall::to_trampoline_stub_size()\n+  assert(__ offset() - start + CompiledDirectCall::to_trampoline_stub_size()\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":2,"deletions":22,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-    \/\/ CompiledStaticCall::to_interp_stub_size() (14) + CompiledStaticCall::to_trampoline_stub_size() (1 + 3 + address)\n+    \/\/ CompiledDirectCall::to_interp_stub_size() (14) + CompiledDirectCall::to_trampoline_stub_size() (1 + 3 + address)\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -317,9 +317,0 @@\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache, Label &L) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-  assert_different_registers(receiver, iCache, t0, t2);\n-  cmp_klass(receiver, iCache, t0, t2 \/* call-clobbered t2 as a tmp *\/, L);\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Runtime1_riscv.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -40,1 +39,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n@@ -72,1 +71,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -76,1 +75,1 @@\n-int CompiledStaticCall::to_trampoline_stub_size() {\n+int CompiledDirectCall::to_trampoline_stub_size() {\n@@ -84,1 +83,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -88,1 +87,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -92,7 +91,0 @@\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n-                  p2i(instruction_address()),\n-                  callee->name_and_sig_as_C_string());\n-  }\n-\n@@ -115,1 +107,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -132,1 +124,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/riscv\/compiledIC_riscv.cpp","additions":7,"deletions":15,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1,78 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_riscv.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  \/\/ 6: auipc + ld + auipc + jalr + address(2 * instruction_size)\n-  return 6 * NativeInstruction::instruction_size;\n-}\n-\n-#define __ masm->\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point) {\n-  assert_cond(code_begin != nullptr && entry_point != nullptr);\n-  ResourceMark rm;\n-  CodeBuffer      code(code_begin, ic_stub_code_size());\n-  MacroAssembler* masm            = new MacroAssembler(&code);\n-  \/\/ Note: even though the code contains an embedded value, we do not need reloc info\n-  \/\/ because\n-  \/\/ (1) the value is old (i.e., doesn't matter for scavenges)\n-  \/\/ (2) these ICStubs are removed *before* a GC happens, so the roots disappear\n-\n-  address start = __ pc();\n-  Label l;\n-  __ ld(t1, l);\n-  __ far_jump(ExternalAddress(entry_point));\n-  __ align(wordSize);\n-  __ bind(l);\n-  __ emit_int64((intptr_t)cached_value);\n-  \/\/ Only need to invalidate the 1st two instructions - not the whole ic stub\n-  ICache::invalidate_range(code_begin, InlineCacheBuffer::ic_stub_code_size());\n-  assert(__ pc() - start == ic_stub_code_size(), \"must be\");\n-}\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ creation also verifies the object\n-  NativeJump* jump = nativeJump_at(move->next_instruction_address());\n-  return jump->jump_destination();\n-}\n-\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  \/\/ The word containing the cached value is at the end of this IC buffer\n-  uintptr_t *p = (uintptr_t *)(code_begin + ic_stub_code_size() - wordSize);\n-  void* o = (void*)*p;\n-  return o;\n-}\n","filename":"src\/hotspot\/cpu\/riscv\/icBuffer_riscv.cpp","additions":0,"deletions":78,"binary":false,"changes":78,"status":"deleted"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -637,2 +638,2 @@\n-  IncompressibleRegion ir(this);  \/\/ Fixed length: see CompiledStaticCall::to_interp_stub_size().\n-  \/\/ CompiledDirectStaticCall::set_to_interpreted knows the\n+  IncompressibleRegion ir(this);  \/\/ Fixed length: see CompiledDirectCall::to_interp_stub_size().\n+  \/\/ CompiledDirectCall::set_to_interpreted knows the\n@@ -2545,1 +2546,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n@@ -3545,0 +3546,42 @@\n+int MacroAssembler::ic_check_size() {\n+  \/\/ No compressed\n+  return (NativeInstruction::instruction_size * (2 \/* 2 loads *\/ + 1 \/* branch *\/)) +\n+          far_branch_size();\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  IncompressibleRegion ir(this);\n+  Register receiver = j_rarg0;\n+  Register data = t1;\n+\n+  Register tmp1 = t0; \/\/ t0 always scratch\n+  \/\/ t2 is saved on call, thus should have been saved before this check.\n+  \/\/ Hence we can clobber it.\n+  Register tmp2 = t2;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, ic_check_size());\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    lwu(tmp1, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    lwu(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else {\n+    ld(tmp1,  Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    ld(tmp2, Address(data, CompiledICData::speculated_klass_offset()));\n+  }\n+\n+  Label ic_hit;\n+  beq(tmp1, tmp2, ic_hit);\n+  \/\/ Note, far_jump is not fixed size.\n+  \/\/ Is this ever generates a movptr alignment\/size will be off.\n+  far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  bind(ic_hit);\n+\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point.\");\n+  return uep_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":46,"deletions":3,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -1196,0 +1196,1 @@\n+\n@@ -1197,0 +1198,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment = NativeInstruction::instruction_size);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1811,4 +1811,2 @@\n-    st->print_cr(\"\\tlwu t0, [j_rarg0, oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n-    if (CompressedKlassPointers::shift() != 0) {\n-      st->print_cr(\"\\tdecode_klass_not_null t0, t0\");\n-    }\n+    st->print_cr(\"\\tlwu t0, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tlwu t2, [t1      + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n@@ -1816,1 +1814,2 @@\n-    st->print_cr(\"\\tld t0, [j_rarg0, oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tld t0, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tld t2, [t1      + CompiledICData::speculated_klass_offset()]\\t# compressed klass\");\n@@ -1818,1 +1817,1 @@\n-  st->print_cr(\"\\tbeq t0, t1, ic_hit\");\n+  st->print_cr(\"\\tbeq t0, t2, ic_hit\");\n@@ -1828,0 +1827,1 @@\n+  __ ic_check(CodeEntryAlignment);\n@@ -1829,8 +1829,3 @@\n-  Label skip;\n-  __ cmp_klass(j_rarg0, t1, t0, t2 \/* call-clobbered t2 as a tmp *\/, skip);\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  __ bind(skip);\n-\n-  \/\/ These NOPs are critical so that verified entry point is properly\n-  \/\/ 4 bytes aligned for patching by NativeJump::patch_verified_entry()\n-  __ align(NativeInstruction::instruction_size);\n+  \/\/ Verified entry point must be properly 4 bytes aligned for patching by NativeJump::patch_verified_entry().\n+  \/\/ ic_check() aligns to CodeEntryAlignment >= InteriorEntryAlignment(min 16) > NativeInstruction::instruction_size(4).\n+  assert(((__ offset()) % CodeEntryAlignment) == 0, \"Misaligned verified entry point\");\n@@ -2404,1 +2399,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, call);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, call);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":10,"deletions":15,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -41,1 +40,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -625,3 +623,0 @@\n-  Label ok;\n-\n-  const Register holder = t1;\n@@ -629,0 +624,1 @@\n+  const Register data = t1;\n@@ -642,5 +638,0 @@\n-    __ load_klass(t0, receiver, tmp);\n-    __ ld(tmp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ ld(xmethod, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ beq(t0, tmp, ok);\n-    __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n@@ -648,4 +639,3 @@\n-    __ bind(ok);\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted; if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n+    __ ic_check();\n+    __ ld(xmethod, Address(data, CompiledICData::speculated_method_offset()));\n+\n@@ -988,1 +978,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1054,1 +1044,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, tr_call);\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, tr_call);\n@@ -1427,3 +1417,0 @@\n-  Label hit;\n-  Label exception_pending;\n-\n@@ -1431,4 +1418,1 @@\n-  assert_different_registers(ic_reg, receiver, t0, t2);\n-  __ cmp_klass(receiver, ic_reg, t0, t2 \/* call-clobbered t2 as a tmp *\/, hit);\n-\n-  __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  assert_different_registers(receiver, t0, t1);\n@@ -1436,4 +1420,1 @@\n-  \/\/ Verified entry point must be aligned\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check();\n@@ -1874,0 +1855,1 @@\n+  Label exception_pending;\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":9,"deletions":27,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -174,1 +174,1 @@\n-  \/\/  t1: CompiledICHolder\n+  \/\/  t1: CompiledICData\n@@ -180,1 +180,1 @@\n-  const Register holder_klass_reg   = x19; \/\/ declaring interface klass (DECC)\n+  const Register holder_klass_reg   = x19; \/\/ declaring interface klass (DEFC)\n@@ -184,1 +184,1 @@\n-  const Register icholder_reg       = t1;\n+  const Register icdata_reg         = t1;\n@@ -188,2 +188,2 @@\n-  __ ld(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n-  __ ld(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+  __ ld(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n+  __ ld(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/riscv\/vtableStubs_riscv.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -110,1 +110,1 @@\n-    \/\/ CompiledStaticCall and ad-file. Do not assert (it's a test\n+    \/\/ CompiledDirectCall and ad-file. Do not assert (it's a test\n","filename":"src\/hotspot\/cpu\/s390\/assembler_s390.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,4 +79,1 @@\n-  Register receiver = receiverOpr()->as_register();\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, Z_inline_cache);\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-    _call_stub_size = 512, \/\/ See Compile::MAX_stubs_size and CompiledStaticCall::emit_to_interp_stub.\n+    _call_stub_size = 512, \/\/ See Compile::MAX_stubs_size and CompiledDirectCall::emit_to_interp_stub.\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,25 +43,0 @@\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  Label ic_miss, ic_hit;\n-  verify_oop(receiver, FILE_AND_LINE);\n-  int klass_offset = oopDesc::klass_offset_in_bytes();\n-\n-  if (!ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(klass_offset)) {\n-    if (VM_Version::has_CompareBranch()) {\n-      z_cgij(receiver, 0, Assembler::bcondEqual, ic_miss);\n-    } else {\n-      z_ltgr(receiver, receiver);\n-      z_bre(ic_miss);\n-    }\n-  }\n-\n-  compare_klass_ptr(iCache, klass_offset, receiver, false);\n-  z_bre(ic_hit);\n-\n-  \/\/ If icache check fails, then jump to runtime routine.\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  load_const_optimized(Z_R1_scratch, AddressLiteral(SharedRuntime::get_ic_miss_stub()));\n-  z_br(Z_R1_scratch);\n-  align(CodeEntryAlignment);\n-  bind(ic_hit);\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/c1_MacroAssembler_s390.cpp","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/c1_Runtime1_s390.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -43,1 +42,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark\/* = nullptr*\/) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark\/* = nullptr*\/) {\n@@ -57,1 +56,1 @@\n-  address stub = __ start_a_stub(CompiledStaticCall::to_interp_stub_size());\n+  address stub = __ start_a_stub(CompiledDirectCall::to_interp_stub_size());\n@@ -84,1 +83,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -90,1 +89,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -94,1 +93,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -98,7 +97,0 @@\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n-                  p2i(instruction_address()),\n-                  callee->name_and_sig_as_C_string());\n-  }\n-\n@@ -118,1 +110,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -134,1 +126,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/s390\/compiledIC_s390.cpp","additions":7,"deletions":15,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1,65 +0,0 @@\n-\/*\n- * Copyright (c) 2016, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_s390.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-#define __ masm.\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  return MacroAssembler::load_const_size() + Assembler::z_brul_size();\n-}\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_oop, address entry_point) {\n-  ResourceMark rm;\n-  CodeBuffer code(code_begin, ic_stub_code_size());\n-  MacroAssembler masm(&code);\n-  \/\/ Note: even though the code contains an embedded oop, we do not need reloc info\n-  \/\/ because\n-  \/\/ (1) the oop is old (i.e., doesn't matter for scavenges)\n-  \/\/ (2) these ICStubs are removed *before* a GC happens, so the roots disappear.\n-\n-  \/\/ Load the oop,\n-  __ load_const(Z_method, (address) cached_oop); \/\/ inline cache reg = Z_method\n-  \/\/ and do a tail-call (pc-relative).\n-  __ z_brul((address) entry_point);\n-  __ flush();\n-}\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ Creation also verifies the object.\n-  return MacroAssembler::get_target_addr_pcrel(move->next_instruction_address());\n-}\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ Creation also verifies the object.\n-  return (void*)move->data();\n-}\n","filename":"src\/hotspot\/cpu\/s390\/icBuffer_s390.cpp","additions":0,"deletions":65,"binary":false,"changes":65,"status":"deleted"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -1100,1 +1101,7 @@\n-  while (offset() % modulus != 0) z_nop();\n+  align(modulus, offset());\n+}\n+\n+void MacroAssembler::align(int modulus, int target) {\n+  assert(((modulus % 2 == 0) && (target % 2 == 0)), \"needs to be even\");\n+  int delta = target - offset();\n+  while ((offset() + delta) % modulus != 0) z_nop();\n@@ -2153,0 +2160,39 @@\n+int MacroAssembler::ic_check_size() {\n+  return 30 + (ImplicitNullChecks ? 0 : 6);\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register R2_receiver = Z_ARG1;\n+  Register R0_scratch  = Z_R0_scratch;\n+  Register R1_scratch  = Z_R1_scratch;\n+  Register R9_data     = Z_inline_cache;\n+  Label success, failure;\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+  if (!ImplicitNullChecks) {\n+    z_cgij(R2_receiver, 0, Assembler::bcondEqual, failure);\n+  }\n+\n+  if (UseCompressedClassPointers) {\n+    z_llgf(R1_scratch, Address(R2_receiver, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    z_lg(R1_scratch, Address(R2_receiver, oopDesc::klass_offset_in_bytes()));\n+  }\n+  z_cg(R1_scratch, Address(R9_data, in_bytes(CompiledICData::speculated_klass_offset())));\n+  z_bre(success);\n+\n+  bind(failure);\n+  load_const(R1_scratch, AddressLiteral(SharedRuntime::get_ic_miss_stub()));\n+  z_br(R1_scratch);\n+  bind(success);\n+\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point, offset() = %d, end_alignment = %d\", offset(), end_alignment);\n+  return uep_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":47,"deletions":1,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -260,0 +260,1 @@\n+  void align(int modulus, int target);\n@@ -569,0 +570,3 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n+\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1344,0 +1344,1 @@\n+  \/\/ This is Unverified Entry Point\n@@ -1345,44 +1346,1 @@\n-  const int ic_miss_offset = 2;\n-\n-  \/\/ Inline_cache contains a klass.\n-  Register ic_klass = as_Register(Matcher::inline_cache_reg_encode());\n-  \/\/ ARG1 is the receiver oop.\n-  Register R2_receiver = Z_ARG1;\n-  int      klass_offset = oopDesc::klass_offset_in_bytes();\n-  AddressLiteral icmiss(SharedRuntime::get_ic_miss_stub());\n-  Register R1_ic_miss_stub_addr = Z_R1_scratch;\n-\n-  \/\/ Null check of receiver.\n-  \/\/ This is the null check of the receiver that actually should be\n-  \/\/ done in the caller. It's here because in case of implicit null\n-  \/\/ checks we get it for free.\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()),\n-         \"second word in oop should not require explicit null check.\");\n-  if (!ImplicitNullChecks) {\n-    Label valid;\n-    if (VM_Version::has_CompareBranch()) {\n-      __ z_cgij(R2_receiver, 0, Assembler::bcondNotEqual, valid);\n-    } else {\n-      __ z_ltgr(R2_receiver, R2_receiver);\n-      __ z_bre(valid);\n-    }\n-    \/\/ The ic_miss_stub will handle the null pointer exception.\n-    __ load_const_optimized(R1_ic_miss_stub_addr, icmiss);\n-    __ z_br(R1_ic_miss_stub_addr);\n-    __ bind(valid);\n-  }\n-\n-  \/\/ Check whether this method is the proper implementation for the class of\n-  \/\/ the receiver (ic miss check).\n-  {\n-    Label valid;\n-    \/\/ Compare cached class against klass from receiver.\n-    \/\/ This also does an implicit null check!\n-    __ compare_klass_ptr(ic_klass, klass_offset, R2_receiver, false);\n-    __ z_bre(valid);\n-    \/\/ The inline cache points to the wrong method. Call the\n-    \/\/ ic_miss_stub to find the proper method.\n-    __ load_const_optimized(R1_ic_miss_stub_addr, icmiss);\n-    __ z_br(R1_ic_miss_stub_addr);\n-    __ bind(valid);\n-  }\n+  __ ic_check(CodeEntryAlignment);\n@@ -2149,1 +2107,1 @@\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+      address stub = CompiledDirectCall::emit_to_interp_stub(cbuf);\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":3,"deletions":45,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -31,0 +30,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -38,1 +38,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -1503,1 +1502,0 @@\n-  Label     ic_miss;\n@@ -1508,1 +1506,0 @@\n-  wrapper_UEPStart = __ offset();\n@@ -1511,3 +1508,3 @@\n-  if (!method_is_static) __ nmethod_UEP(ic_miss);\n-  \/\/ Fill with nops (alignment of verified entry point).\n-  __ align(CodeEntryAlignment);\n+  if (!method_is_static) {\n+    wrapper_UEPStart = __ ic_check(CodeEntryAlignment \/* end_alignment *\/);\n+  }\n@@ -2029,4 +2026,0 @@\n-  \/\/---------------------------------------------------------------------\n-  \/\/ Handler for a cache miss (out-of-line)\n-  \/\/---------------------------------------------------------------------\n-  __ call_ic_miss_handler(ic_miss, 0x77, 0, Z_R1_scratch);\n@@ -2034,2 +2027,0 @@\n-\n-\n@@ -2321,3 +2312,0 @@\n-    const int klass_offset           = oopDesc::klass_offset_in_bytes();\n-    const int holder_klass_offset    = in_bytes(CompiledICHolder::holder_klass_offset());\n-    const int holder_metadata_offset = in_bytes(CompiledICHolder::holder_metadata_offset());\n@@ -2332,17 +2320,2 @@\n-    \/\/ Check the pointers.\n-    if (!ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(klass_offset)) {\n-      __ z_ltgr(Z_ARG1, Z_ARG1);\n-      __ z_bre(ic_miss);\n-    }\n-    __ verify_oop(Z_ARG1, FILE_AND_LINE);\n-\n-    \/\/ Check ic: object class <-> cached class\n-    \/\/ Compress cached class for comparison. That's more efficient.\n-    if (UseCompressedClassPointers) {\n-      __ z_lg(Z_R11, holder_klass_offset, Z_method);             \/\/ Z_R11 is overwritten a few instructions down anyway.\n-      __ compare_klass_ptr(Z_R11, klass_offset, Z_ARG1, false); \/\/ Cached class can't be zero.\n-    } else {\n-      __ z_clc(klass_offset, sizeof(void *)-1, Z_ARG1, holder_klass_offset, Z_method);\n-    }\n-    __ z_brne(ic_miss);  \/\/ Cache miss: call runtime to handle this.\n-\n+    __ ic_check(2);\n+    __ z_lg(Z_method, Address(Z_inline_cache, CompiledICData::speculated_method_offset()));\n@@ -2352,1 +2325,0 @@\n-    __ z_lg(Z_method, holder_metadata_offset, Z_method);\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":6,"deletions":34,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2016, 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2023 SAP SE. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -200,1 +200,1 @@\n-  __ z_lg(interface, Address(Z_method, CompiledICHolder::holder_klass_offset()));\n+  __ z_lg(interface, Address(Z_method, CompiledICData::itable_refc_klass_offset()));\n@@ -205,1 +205,1 @@\n-  __ z_lg(interface, Address(Z_method, CompiledICHolder::holder_metadata_offset()));\n+  __ z_lg(interface, Address(Z_method, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/s390\/vtableStubs_s390.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -75,1 +75,0 @@\n-const Register IC_Klass    = rax;   \/\/ where the IC klass is cached\n@@ -339,17 +338,1 @@\n-  Register receiver = FrameMap::receiver_opr->as_register();\n-  Register ic_klass = IC_Klass;\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  const bool do_post_padding = VerifyOops || UseCompressedClassPointers;\n-  if (!do_post_padding) {\n-    \/\/ insert some nops so that the verified entry point is aligned on CodeEntryAlignment\n-    __ align(CodeEntryAlignment, __ offset() + ic_cmp_size);\n-  }\n-  int offset = __ offset();\n-  __ inline_cache_check(receiver, IC_Klass);\n-  assert(__ offset() % CodeEntryAlignment == 0 || do_post_padding, \"alignment must be correct\");\n-  if (do_post_padding) {\n-    \/\/ force alignment after the cache check.\n-    \/\/ It's been verified to be aligned if !VerifyOops\n-    __ align(CodeEntryAlignment);\n-  }\n-  return offset;\n+  return __ ic_check(CodeEntryAlignment);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -298,24 +299,0 @@\n-\n-\n-void C1_MacroAssembler::inline_cache_check(Register receiver, Register iCache) {\n-  verify_oop(receiver);\n-  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n-  \/\/ check against inline cache\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n-  int start_offset = offset();\n-\n-  if (UseCompressedClassPointers) {\n-    load_klass(rscratch1, receiver, rscratch2);\n-    cmpptr(rscratch1, iCache);\n-  } else {\n-    cmpptr(iCache, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  }\n-  \/\/ if icache check fails, then jump to runtime routine\n-  \/\/ Note: RECEIVER must still contain the receiver!\n-  jump_cc(Assembler::notEqual,\n-          RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  const int ic_cmp_size = LP64_ONLY(10) NOT_LP64(9);\n-  assert(UseCompressedClassPointers || offset() - start_offset == ic_cmp_size, \"check alignment in emit_method_entry\");\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":1,"deletions":24,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +38,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n@@ -69,1 +68,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -74,1 +73,1 @@\n-int CompiledStaticCall::to_trampoline_stub_size() {\n+int CompiledDirectCall::to_trampoline_stub_size() {\n@@ -80,1 +79,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -84,1 +83,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -88,7 +87,0 @@\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"CompiledDirectStaticCall@\" INTPTR_FORMAT \": set_to_interpreted %s\",\n-                  p2i(instruction_address()),\n-                  callee->name_and_sig_as_C_string());\n-  }\n-\n@@ -108,1 +100,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -125,1 +117,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/x86\/compiledIC_x86.cpp","additions":7,"deletions":15,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1,95 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_x86.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  \/\/ Worst case, if destination is not a near call:\n-  \/\/ lea rax, lit1\n-  \/\/ lea scratch, lit2\n-  \/\/ jmp scratch\n-\n-  \/\/ Best case\n-  \/\/ lea rax, lit1\n-  \/\/ jmp lit2\n-\n-  int best = NativeMovConstReg::instruction_size + NativeJump::instruction_size;\n-  int worst = 2 * NativeMovConstReg::instruction_size + 3;\n-  return MAX2(best, worst);\n-}\n-\n-\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point) {\n-  ResourceMark rm;\n-  CodeBuffer      code(code_begin, ic_stub_code_size());\n-  MacroAssembler* masm            = new MacroAssembler(&code);\n-  \/\/ note: even though the code contains an embedded value, we do not need reloc info\n-  \/\/ because\n-  \/\/ (1) the value is old (i.e., doesn't matter for scavenges)\n-  \/\/ (2) these ICStubs are removed *before* a GC happens, so the roots disappear\n-  \/\/ assert(cached_value == nullptr || cached_oop->is_perm(), \"must be perm oop\");\n-  masm->lea(rax, AddressLiteral((address) cached_value, relocInfo::metadata_type));\n-  masm->jump(ExternalAddress(entry_point));\n-}\n-\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);   \/\/ creation also verifies the object\n-  address jmp = move->next_instruction_address();\n-  NativeInstruction* ni = nativeInstruction_at(jmp);\n-  if (ni->is_jump()) {\n-    NativeJump*        jump = nativeJump_at(jmp);\n-    return jump->jump_destination();\n-  } else {\n-    assert(ni->is_far_jump(), \"unexpected instruction\");\n-    NativeFarJump*     jump = nativeFarJump_at(jmp);\n-    return jump->jump_destination();\n-  }\n-}\n-\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  \/\/ creation also verifies the object\n-  NativeMovConstReg* move = nativeMovConstReg_at(code_begin);\n-  \/\/ Verifies the jump\n-  address jmp = move->next_instruction_address();\n-  NativeInstruction* ni = nativeInstruction_at(jmp);\n-  if (ni->is_jump()) {\n-    NativeJump*        jump = nativeJump_at(jmp);\n-  } else {\n-    assert(ni->is_far_jump(), \"unexpected instruction\");\n-    NativeFarJump*     jump = nativeFarJump_at(jmp);\n-  }\n-  void* o = (void*)move->data();\n-  return o;\n-}\n","filename":"src\/hotspot\/cpu\/x86\/icBuffer_x86.cpp","additions":0,"deletions":95,"binary":false,"changes":95,"status":"deleted"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -1344,1 +1345,1 @@\n-  mov64(rax, (intptr_t)Universe::non_oop_word());\n+  mov64(rax, (int64_t)Universe::non_oop_word());\n@@ -1351,0 +1352,32 @@\n+int MacroAssembler::ic_check_size() {\n+  return LP64_ONLY(14) NOT_LP64(12);\n+}\n+\n+int MacroAssembler::ic_check(int end_alignment) {\n+  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register data = rax;\n+  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+\n+  \/\/ The UEP of a code blob ensures that the VEP is padded. However, the padding of the UEP is placed\n+  \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+  \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately. That's why we align\n+  \/\/ before the inline cache check here, and not after\n+  align(end_alignment, offset() + ic_check_size());\n+\n+  int uep_offset = offset();\n+\n+  if (UseCompressedClassPointers) {\n+    movl(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpl(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  } else {\n+    movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(temp, Address(data, CompiledICData::speculated_klass_offset()));\n+  }\n+\n+  \/\/ if inline cache check fails, then jump to runtime routine\n+  jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+  assert((offset() % end_alignment) == 0, \"Misaligned verified entry point\");\n+\n+  return uep_offset;\n+}\n+\n@@ -4355,1 +4388,1 @@\n-\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICData\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":35,"deletions":2,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -899,0 +899,2 @@\n+  static int ic_check_size();\n+  int ic_check(int end_alignment);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +39,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -947,1 +946,1 @@\n-  Register holder = rax;\n+  Register data = rax;\n@@ -952,6 +951,2 @@\n-\n-    Label missed;\n-    __ movptr(temp, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::notEqual, missed);\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n@@ -963,3 +958,0 @@\n-\n-    __ bind(missed);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n@@ -1452,1 +1444,0 @@\n-  const Register ic_reg = rax;\n@@ -1454,1 +1445,0 @@\n-  Label hit;\n@@ -1458,5 +1448,0 @@\n-  __ cmpptr(ic_reg, Address(receiver, oopDesc::klass_offset_in_bytes()));\n-  __ jcc(Assembler::equal, hit);\n-\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n@@ -1464,5 +1449,1 @@\n-  \/\/ and the first 5 bytes must be in the same cache line\n-  \/\/ if we align at 8 then we will be sure 5 bytes are in the same line\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check(8 \/* end_alignment *\/);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":5,"deletions":24,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -45,1 +44,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -1003,1 +1001,0 @@\n-  Label ok;\n@@ -1005,1 +1002,1 @@\n-  Register holder = rax;\n+  Register data = rax;\n@@ -1010,7 +1007,2 @@\n-    __ load_klass(temp, receiver, rscratch1);\n-    __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));\n-    __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));\n-    __ jcc(Assembler::equal, ok);\n-    __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-    __ bind(ok);\n+    __ ic_check(1 \/* end_alignment *\/);\n+    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n@@ -1453,1 +1445,1 @@\n-    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+    address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, __ pc());\n@@ -1490,1 +1482,1 @@\n-  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+  address stub = CompiledDirectCall::emit_to_interp_stub(*cbuf, __ pc());\n@@ -1885,2 +1877,0 @@\n-\n-  const Register ic_reg = rax;\n@@ -1889,1 +1879,0 @@\n-  Label hit;\n@@ -1892,1 +1881,1 @@\n-  assert_different_registers(ic_reg, receiver, rscratch1, rscratch2);\n+  assert_different_registers(receiver, rscratch1, rscratch2);\n@@ -1894,10 +1883,1 @@\n-  __ load_klass(rscratch1, receiver, rscratch2);\n-  __ cmpq(ic_reg, rscratch1);\n-  __ jcc(Assembler::equal, hit);\n-\n-  __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/\/ Verified entry point must be aligned\n-  __ align(8);\n-\n-  __ bind(hit);\n+  __ ic_check(8 \/* end_alignment *\/);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":7,"deletions":27,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -179,1 +179,1 @@\n-  \/\/  rax: CompiledICHolder\n+  \/\/  rax: CompiledICData\n@@ -185,1 +185,1 @@\n-  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DECC)\n+  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DEFC)\n@@ -189,1 +189,1 @@\n-  const Register icholder_reg       = rax;\n+  const Register icdata_reg         = rax;\n@@ -192,2 +192,2 @@\n-  __ movptr(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n-  __ movptr(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+  __ movptr(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n+  __ movptr(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_32.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -171,1 +171,1 @@\n-  \/\/  rax: CompiledICHolder\n+  \/\/  rax: CompiledICData\n@@ -177,1 +177,1 @@\n-  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DECC)\n+  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DEFC)\n@@ -182,1 +182,1 @@\n-  const Register icholder_reg       = rax;\n+  const Register icdata_reg         = rax;\n@@ -184,2 +184,2 @@\n-  __ movptr(resolved_klass_reg, Address(icholder_reg, CompiledICHolder::holder_klass_offset()));\n-  __ movptr(holder_klass_reg,   Address(icholder_reg, CompiledICHolder::holder_metadata_offset()));\n+  __ movptr(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n+  __ movptr(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1386,14 +1386,1 @@\n-#ifdef ASSERT\n-  uint insts_size = cbuf.insts_size();\n-#endif\n-  masm.cmpptr(rax, Address(rcx, oopDesc::klass_offset_in_bytes()));\n-  masm.jump_cc(Assembler::notEqual,\n-               RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 2;\n-  if( !OptoBreakpoint ) \/\/ Leave space for int3\n-     nops_cnt += 1;\n-  masm.nop(nops_cnt);\n-\n-  assert(cbuf.insts_size() - insts_size == size(ra_), \"checking code size of inline cache node\");\n+  masm.ic_check(CodeEntryAlignment);\n@@ -1403,1 +1390,2 @@\n-  return OptoBreakpoint ? 11 : 12;\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n@@ -1845,1 +1833,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":4,"deletions":16,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1475,2 +1475,1 @@\n-    st->print_cr(\"\\tdecode_klass_not_null rscratch1, rscratch1\");\n-    st->print_cr(\"\\tcmpq    rax, rscratch1\\t # Inline cache check\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1478,2 +1477,2 @@\n-    st->print_cr(\"\\tcmpq    rax, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t\"\n-                 \"# Inline cache check\");\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1482,1 +1481,0 @@\n-  st->print_cr(\"\\tnop\\t# nops to align entry point\");\n@@ -1489,20 +1487,1 @@\n-  uint insts_size = cbuf.insts_size();\n-  if (UseCompressedClassPointers) {\n-    masm.load_klass(rscratch1, j_rarg0, rscratch2);\n-    masm.cmpptr(rax, rscratch1);\n-  } else {\n-    masm.cmpptr(rax, Address(j_rarg0, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n-  masm.jump_cc(Assembler::notEqual, RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n-\n-  \/* WARNING these NOPs are critical so that verified entry point is properly\n-     4 bytes aligned for patching by NativeJump::patch_verified_entry() *\/\n-  int nops_cnt = 4 - ((cbuf.insts_size() - insts_size) & 0x3);\n-  if (OptoBreakpoint) {\n-    \/\/ Leave space for int3\n-    nops_cnt -= 1;\n-  }\n-  nops_cnt &= 0x3; \/\/ Do not add nops if code is aligned.\n-  if (nops_cnt > 0)\n-    masm.nop(nops_cnt);\n+  masm.ic_check(InteriorEntryAlignment);\n@@ -1843,1 +1822,1 @@\n-        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        address stub = CompiledDirectCall::emit_to_interp_stub(cbuf, mark);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":5,"deletions":26,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -46,1 +45,1 @@\n-address CompiledStaticCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n+address CompiledDirectCall::emit_to_interp_stub(CodeBuffer &cbuf, address mark) {\n@@ -51,1 +50,1 @@\n-int CompiledStaticCall::to_interp_stub_size() {\n+int CompiledDirectCall::to_interp_stub_size() {\n@@ -57,1 +56,1 @@\n-int CompiledStaticCall::reloc_to_interp_stub() {\n+int CompiledDirectCall::reloc_to_interp_stub() {\n@@ -62,1 +61,1 @@\n-void CompiledDirectStaticCall::set_to_interpreted(const methodHandle& callee, address entry) {\n+void CompiledDirectCall::set_to_interpreted(const methodHandle& callee, address entry) {\n@@ -66,1 +65,1 @@\n-void CompiledDirectStaticCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n+void CompiledDirectCall::set_stub_to_clean(static_stub_Relocation* static_stub) {\n@@ -74,1 +73,1 @@\n-void CompiledDirectStaticCall::verify() {\n+void CompiledDirectCall::verify() {\n","filename":"src\/hotspot\/cpu\/zero\/compiledIC_zero.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1,56 +0,0 @@\n-\/*\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright 2007 Red Hat, Inc.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"nativeInst_zero.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-\n-int InlineCacheBuffer::ic_stub_code_size() {\n-  \/\/ NB set this once the functions below are implemented\n-  return 4;\n-}\n-\n-void InlineCacheBuffer::assemble_ic_buffer_code(address code_begin,\n-                                                void* cached_oop,\n-                                                address entry_point) {\n-  \/\/ NB ic_stub_code_size() must return the size of the code we generate\n-  ShouldNotCallThis();\n-}\n-\n-address InlineCacheBuffer::ic_buffer_entry_point(address code_begin) {\n-  \/\/ NB ic_stub_code_size() must return the size of the code we generate\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n-\n-void* InlineCacheBuffer::ic_buffer_cached_value(address code_begin) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n","filename":"src\/hotspot\/cpu\/zero\/icBuffer_zero.cpp","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -32,1 +31,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/os_aix_ppc.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/os_linux_ppc.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/os_linux_s390.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/windows_aarch64\/os_windows_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/os_windows_x86.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -219,1 +219,0 @@\n-  AD.addInclude(AD._CPP_file, \"oops\/compiledICHolder.hpp\");\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    address stub = __ start_a_stub(CompiledStaticCall::to_interp_stub_size());\n+    address stub = __ start_a_stub(CompiledDirectCall::to_interp_stub_size());\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -609,1 +609,1 @@\n-    case lir_std_entry:\n+    case lir_std_entry: {\n@@ -612,1 +612,0 @@\n-      _masm->align(CodeEntryAlignment);\n@@ -614,1 +613,2 @@\n-        check_icache();\n+        int offset = check_icache();\n+        offsets()->set_value(CodeOffsets::Entry, offset);\n@@ -616,0 +616,1 @@\n+      _masm->align(CodeEntryAlignment);\n@@ -624,0 +625,1 @@\n+    }\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-  void inline_cache_check(Register receiver, Register iCache);\n","filename":"src\/hotspot\/share\/c1\/c1_MacroAssembler.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -652,5 +651,0 @@\n-    \/\/ the InlineCacheBuffer is using stubs generated into a buffer blob\n-    if (InlineCacheBuffer::contains(addr)) {\n-      st->print_cr(INTPTR_FORMAT \" is pointing into InlineCacheBuffer\", p2i(addr));\n-      return;\n-    }\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -916,17 +915,0 @@\n-void CodeCache::verify_icholder_relocations() {\n-#ifdef ASSERT\n-  \/\/ make sure that we aren't leaking icholders\n-  int count = 0;\n-  FOR_ALL_HEAPS(heap) {\n-    FOR_ALL_BLOBS(cb, *heap) {\n-      CompiledMethod *nm = cb->as_compiled_method_or_null();\n-      if (nm != nullptr) {\n-        count += nm->verify_icholder_relocations();\n-      }\n-    }\n-  }\n-  assert(count + InlineCacheBuffer::pending_icholder_count() + CompiledICHolder::live_not_claimed_count() ==\n-         CompiledICHolder::live_count(), \"must agree\");\n-#endif\n-}\n-\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -297,1 +297,0 @@\n-  static void verify_icholder_relocations();\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -32,4 +31,0 @@\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"interpreter\/linkResolver.hpp\"\n-#include \"memory\/metadataFactory.hpp\"\n-#include \"memory\/oopFactory.hpp\"\n@@ -38,0 +33,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -40,2 +36,1 @@\n-#include \"oops\/oop.inline.hpp\"\n-#include \"oops\/symbol.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -44,2 +39,1 @@\n-#include \"runtime\/icache.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -47,1 +41,0 @@\n-#include \"runtime\/stubRoutines.hpp\"\n@@ -49,1 +42,0 @@\n-#include \"utilities\/events.hpp\"\n@@ -78,3 +70,6 @@\n-\/\/-----------------------------------------------------------------------------\n-\/\/ Low-level access to an inline cache. Private, since they might not be\n-\/\/ MT-safe to use.\n+CompiledICData::CompiledICData()\n+  : _speculated_method(),\n+    _speculated_klass(),\n+    _itable_defc_klass(),\n+    _itable_refc_klass(),\n+    _is_initialized() {}\n@@ -82,10 +77,5 @@\n-void* CompiledIC::cached_value() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert (!is_optimized(), \"an optimized virtual call does not have a cached metadata\");\n-\n-  if (!is_in_transition_state()) {\n-    void* data = get_data();\n-    \/\/ If we let the metadata value here be initialized to zero...\n-    assert(data != nullptr || Universe::non_oop_word() == nullptr,\n-           \"no raw nulls in CompiledIC metadatas, because of patching races\");\n-    return (data == (void*)Universe::non_oop_word()) ? nullptr : data;\n+\/\/ Inline cache callsite info is initialized once the first time it is resolved\n+void CompiledICData::initialize(CallInfo* call_info, Klass* receiver_klass) {\n+  _speculated_method = call_info->selected_method();\n+  if (UseCompressedClassPointers) {\n+    _speculated_klass = (uintptr_t)CompressedKlassPointers::encode_not_null(receiver_klass);\n@@ -93,1 +83,1 @@\n-    return InlineCacheBuffer::cached_value_for((CompiledIC *)this);\n+    _speculated_klass = (uintptr_t)receiver_klass;\n@@ -95,0 +85,5 @@\n+  if (call_info->call_kind() == CallInfo::itable_call) {\n+    _itable_defc_klass = call_info->resolved_method()->method_holder();\n+    _itable_refc_klass = call_info->resolved_klass();\n+  }\n+  _is_initialized = true;\n@@ -97,0 +92,3 @@\n+bool CompiledICData::is_speculated_klass_unloaded() const {\n+  return is_initialized() && _speculated_klass == 0;\n+}\n@@ -98,18 +96,3 @@\n-void CompiledIC::internal_set_ic_destination(address entry_point, bool is_icstub, void* cache, bool is_icholder) {\n-  assert(entry_point != nullptr, \"must set legal entry point\");\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert (!is_optimized() || cache == nullptr, \"an optimized virtual call does not have a cached metadata\");\n-  assert (cache == nullptr || cache != (Metadata*)badOopVal, \"invalid metadata\");\n-\n-  assert(!is_icholder || is_icholder_entry(entry_point), \"must be\");\n-\n-  \/\/ Don't use ic_destination for this test since that forwards\n-  \/\/ through ICBuffer instead of returning the actual current state of\n-  \/\/ the CompiledIC.\n-  if (is_icholder_entry(_call->destination())) {\n-    \/\/ When patching for the ICStub case the cached value isn't\n-    \/\/ overwritten until the ICStub copied into the CompiledIC during\n-    \/\/ the next safepoint.  Make sure that the CompiledICHolder* is\n-    \/\/ marked for release at this point since it won't be identifiable\n-    \/\/ once the entry point is overwritten.\n-    InlineCacheBuffer::queue_for_release((CompiledICHolder*)get_data());\n+void CompiledICData::clean_metadata() {\n+  if (!is_initialized() || is_speculated_klass_unloaded()) {\n+    return;\n@@ -118,11 +101,8 @@\n-  if (TraceCompiledIC) {\n-    tty->print(\"  \");\n-    print_compiled_ic();\n-    tty->print(\" changing destination to \" INTPTR_FORMAT, p2i(entry_point));\n-    if (!is_optimized()) {\n-      tty->print(\" changing cached %s to \" INTPTR_FORMAT, is_icholder ? \"icholder\" : \"metadata\", p2i((address)cache));\n-    }\n-    if (is_icstub) {\n-      tty->print(\" (icstub)\");\n-    }\n-    tty->cr();\n+  \/\/ GC cleaning doesn't need to change the state of the inline cache,\n+  \/\/ only nuke stale speculated metadata if it gets unloaded. If the\n+  \/\/ inline cache is monomorphic, the unverified entries will miss, and\n+  \/\/ subsequent miss handlers will upgrade the callsite to megamorphic,\n+  \/\/ which makes sense as it obviously is megamorphic then.\n+  if (!speculated_klass()->is_loader_alive()) {\n+    Atomic::store(&_speculated_klass, (uintptr_t)0);\n+    Atomic::store(&_speculated_method, (Method*)nullptr);\n@@ -131,7 +111,3 @@\n-#ifdef ASSERT\n-  {\n-    CodeBlob* cb = CodeCache::find_blob(_call->instruction_address());\n-    assert(cb != nullptr && cb->is_compiled(), \"must be compiled\");\n-  }\n-#endif\n-  _call->set_destination_mt_safe(entry_point);\n+  assert(_speculated_method == nullptr || _speculated_method->method_holder()->is_loader_alive(),\n+         \"Speculated method is not unloaded despite class being unloaded\");\n+}\n@@ -139,5 +115,2 @@\n-  if (is_optimized() || is_icstub) {\n-    \/\/ Optimized call sites don't have a cache value and ICStub call\n-    \/\/ sites only change the entry point.  Changing the value in that\n-    \/\/ case could lead to MT safety issues.\n-    assert(cache == nullptr, \"must be null\");\n+void CompiledICData::metadata_do(MetadataClosure* cl) {\n+  if (!is_initialized()) {\n@@ -147,8 +120,10 @@\n-  if (cache == nullptr)  cache = Universe::non_oop_word();\n-\n-  set_data((intptr_t)cache);\n-}\n-\n-\n-void CompiledIC::set_ic_destination(ICStub* stub) {\n-  internal_set_ic_destination(stub->code_begin(), true, nullptr, false);\n+  if (!is_speculated_klass_unloaded()) {\n+    cl->do_metadata(_speculated_method);\n+    cl->do_metadata(speculated_klass());\n+  }\n+  if (_itable_refc_klass != nullptr) {\n+    cl->do_metadata(_itable_refc_klass);\n+  }\n+  if (_itable_defc_klass != nullptr) {\n+    cl->do_metadata(_itable_defc_klass);\n+  }\n@@ -157,0 +132,4 @@\n+Klass* CompiledICData::speculated_klass() const {\n+  if (is_speculated_klass_unloaded()) {\n+    return nullptr;\n+  }\n@@ -158,5 +137,2 @@\n-\n-address CompiledIC::ic_destination() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  if (!is_in_transition_state()) {\n-    return _call->destination();\n+  if (UseCompressedClassPointers) {\n+    return CompressedKlassPointers::decode_not_null((narrowKlass)_speculated_klass);\n@@ -164,1 +140,1 @@\n-    return InlineCacheBuffer::ic_destination_for((CompiledIC *)this);\n+    return (Klass*)_speculated_klass;\n@@ -168,0 +144,2 @@\n+\/\/-----------------------------------------------------------------------------\n+\/\/ High-level access to an inline cache. Guaranteed to be MT-safe.\n@@ -169,3 +147,2 @@\n-bool CompiledIC::is_in_transition_state() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  return InlineCacheBuffer::contains(_call->destination());;\n+CompiledICData* CompiledIC::data() const {\n+  return _data;\n@@ -174,0 +151,8 @@\n+CompiledICData* data_from_reloc_iter(RelocIterator* iter) {\n+  assert(iter->type() == relocInfo::virtual_call_type, \"wrong reloc. info\");\n+\n+  virtual_call_Relocation* r = iter->virtual_call_reloc();\n+  NativeMovConstReg* value = nativeMovConstReg_at(r->cached_value());\n+\n+  return (CompiledICData*)value->data();\n+}\n@@ -175,1 +160,7 @@\n-bool CompiledIC::is_icholder_call() const {\n+CompiledIC::CompiledIC(RelocIterator* iter)\n+  : _method(iter->code()),\n+    _data(data_from_reloc_iter(iter)),\n+    _call(nativeCall_at(iter->addr()))\n+{\n+  assert(_method != nullptr, \"must pass compiled method\");\n+  assert(_method->contains(iter->addr()), \"must be in compiled method\");\n@@ -177,1 +168,0 @@\n-  return !_is_optimized && is_icholder_entry(ic_destination());\n@@ -180,5 +170,3 @@\n-\/\/ Returns native address of 'call' instruction in inline-cache. Used by\n-\/\/ the InlineCacheBuffer when it needs to find the stub.\n-address CompiledIC::stub_address() const {\n-  assert(is_in_transition_state(), \"should only be called when we are in a transition state\");\n-  return _call->destination();\n+CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr) {\n+  address call_site = nativeCall_before(return_addr)->instruction_address();\n+  return CompiledIC_at(nm, call_site);\n@@ -187,6 +175,4 @@\n-\/\/ Clears the IC stub if the compiled IC is in transition state\n-void CompiledIC::clear_ic_stub() {\n-  if (is_in_transition_state()) {\n-    ICStub* stub = ICStub::from_destination_address(stub_address());\n-    stub->clear();\n-  }\n+CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site) {\n+  RelocIterator iter(nm, call_site, call_site + 1);\n+  iter.next();\n+  return CompiledIC_at(&iter);\n@@ -195,2 +181,5 @@\n-\/\/-----------------------------------------------------------------------------\n-\/\/ High-level access to an inline cache. Guaranteed to be MT-safe.\n+CompiledIC* CompiledIC_at(Relocation* call_reloc) {\n+  address call_site = call_reloc->addr();\n+  CompiledMethod* cm = CodeCache::find_blob(call_reloc->addr())->as_compiled_method();\n+  return CompiledIC_at(cm, call_site);\n+}\n@@ -198,2 +187,5 @@\n-void CompiledIC::initialize_from_iter(RelocIterator* iter) {\n-  assert(iter->addr() == _call->instruction_address(), \"must find ic_call\");\n+CompiledIC* CompiledIC_at(RelocIterator* reloc_iter) {\n+  CompiledIC* c_ic = new CompiledIC(reloc_iter);\n+  c_ic->verify();\n+  return c_ic;\n+}\n@@ -201,8 +193,3 @@\n-  if (iter->type() == relocInfo::virtual_call_type) {\n-    virtual_call_Relocation* r = iter->virtual_call_reloc();\n-    _is_optimized = false;\n-    _value = _call->get_load_instruction(r);\n-  } else {\n-    assert(iter->type() == relocInfo::opt_virtual_call_type, \"must be a virtual call\");\n-    _is_optimized = true;\n-    _value = nullptr;\n+void CompiledIC::ensure_initialized(CallInfo* call_info, Klass* receiver_klass) {\n+  if (!_data->is_initialized()) {\n+    _data->initialize(call_info, receiver_klass);\n@@ -212,17 +199,3 @@\n-CompiledIC::CompiledIC(CompiledMethod* cm, NativeCall* call)\n-  : _method(cm)\n-{\n-  _call = _method->call_wrapper_at((address) call);\n-  address ic_call = _call->instruction_address();\n-\n-  assert(ic_call != nullptr, \"ic_call address must be set\");\n-  assert(cm != nullptr, \"must pass compiled method\");\n-  assert(cm->contains(ic_call), \"must be in compiled method\");\n-\n-  \/\/ Search for the ic_call at the given address.\n-  RelocIterator iter(cm, ic_call, ic_call+1);\n-  bool ret = iter.next();\n-  assert(ret == true, \"relocInfo must exist at this address\");\n-  assert(iter.addr() == ic_call, \"must find ic_call\");\n-\n-  initialize_from_iter(&iter);\n+void CompiledIC::set_to_clean() {\n+  log_debug(inlinecache)(\"IC@\" INTPTR_FORMAT \": set to clean\", p2i(_call->instruction_address()));\n+  _call->set_destination_mt_safe(SharedRuntime::get_resolve_virtual_call_stub());\n@@ -231,5 +204,12 @@\n-CompiledIC::CompiledIC(RelocIterator* iter)\n-  : _method(iter->code())\n-{\n-  _call = _method->call_wrapper_at(iter->addr());\n-  address ic_call = _call->instruction_address();\n+void CompiledIC::set_to_monomorphic() {\n+  assert(data()->is_initialized(), \"must be initialized\");\n+  Method* method = data()->speculated_method();\n+  CompiledMethod* code = method->code();\n+  address entry;\n+  bool to_compiled = code != nullptr && code->is_in_use() && !code->is_unloading();\n+\n+  if (to_compiled) {\n+    entry = code->entry_point();\n+  } else {\n+    entry = method->get_c2i_unverified_entry();\n+  }\n@@ -237,4 +217,4 @@\n-  CompiledMethod* nm = iter->code();\n-  assert(ic_call != nullptr, \"ic_call address must be set\");\n-  assert(nm != nullptr, \"must pass compiled method\");\n-  assert(nm->contains(ic_call), \"must be in compiled method\");\n+  log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to %s: %s\",\n+                         p2i(_call->instruction_address()),\n+                         to_compiled ? \"compiled\" : \"interpreter\",\n+                         method->print_value_string());\n@@ -242,1 +222,1 @@\n-  initialize_from_iter(iter);\n+  _call->set_destination_mt_safe(entry);\n@@ -245,10 +225,2 @@\n-\/\/ This function may fail for two reasons: either due to running out of vtable\n-\/\/ stubs, or due to running out of IC stubs in an attempted transition to a\n-\/\/ transitional state. The needs_ic_stub_refill value will be set if the failure\n-\/\/ was due to running out of IC stubs, in which case the caller will refill IC\n-\/\/ stubs and retry.\n-bool CompiledIC::set_to_megamorphic(CallInfo* call_info, Bytecodes::Code bytecode,\n-                                    bool& needs_ic_stub_refill, TRAPS) {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert(!is_optimized(), \"cannot set an optimized virtual call to megamorphic\");\n-  assert(is_call_to_compiled() || is_call_to_interpreted(), \"going directly to megamorphic?\");\n+void CompiledIC::set_to_megamorphic(CallInfo* call_info) {\n+  assert(data()->is_initialized(), \"must be initialized\");\n@@ -257,2 +229,6 @@\n-  if (call_info->call_kind() == CallInfo::itable_call) {\n-    assert(bytecode == Bytecodes::_invokeinterface, \"\");\n+  if (call_info->call_kind() == CallInfo::direct_call) {\n+    \/\/ C1 sometimes compiles a callsite before the target method is loaded, resulting in\n+    \/\/ dynamically bound callsites that should really be statically bound. However, the\n+    \/\/ target method might not have a vtable or itable. We just wait for better code to arrive\n+    return;\n+  } else if (call_info->call_kind() == CallInfo::itable_call) {\n@@ -262,1 +238,1 @@\n-      return false;\n+      return;\n@@ -270,11 +246,0 @@\n-    CompiledICHolder* holder = new CompiledICHolder(call_info->resolved_method()->method_holder(),\n-                                                    call_info->resolved_klass(), false);\n-    holder->claim();\n-    if (!InlineCacheBuffer::create_transition_stub(this, holder, entry)) {\n-      delete holder;\n-      needs_ic_stub_refill = true;\n-      return false;\n-    }\n-    \/\/ LSan appears unable to follow malloc-based memory consistently when embedded as an immediate\n-    \/\/ in generated machine code. So we have to ignore it.\n-    LSAN_IGNORE_OBJECT(holder);\n@@ -282,1 +247,1 @@\n-    assert(call_info->call_kind() == CallInfo::vtable_call, \"either itable or vtable\");\n+    assert(call_info->call_kind() == CallInfo::vtable_call, \"what else?\");\n@@ -288,1 +253,1 @@\n-      return false;\n+      return;\n@@ -290,11 +255,0 @@\n-    if (!InlineCacheBuffer::create_transition_stub(this, nullptr, entry)) {\n-      needs_ic_stub_refill = true;\n-      return false;\n-    }\n-  }\n-\n-  {\n-    ResourceMark rm;\n-    assert(call_info->selected_method() != nullptr, \"Unexpected null selected method\");\n-    log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": to megamorphic %s entry: \" INTPTR_FORMAT,\n-                   p2i(instruction_address()), call_info->selected_method()->print_value_string(), p2i(entry));\n@@ -303,10 +257,2 @@\n-  \/\/ We can't check this anymore. With lazy deopt we could have already\n-  \/\/ cleaned this IC entry before we even return. This is possible if\n-  \/\/ we ran out of space in the inline cache buffer trying to do the\n-  \/\/ set_next and we safepointed to free up space. This is a benign\n-  \/\/ race because the IC entry was complete when we safepointed so\n-  \/\/ cleaning it immediately is harmless.\n-  \/\/ assert(is_megamorphic(), \"sanity check\");\n-  return true;\n-}\n-\n+  log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": to megamorphic %s entry: \" INTPTR_FORMAT,\n+                         p2i(_call->instruction_address()), call_info->selected_method()->print_value_string(), p2i(entry));\n@@ -314,7 +260,2 @@\n-\/\/ true if destination is megamorphic stub\n-bool CompiledIC::is_megamorphic() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  assert(!is_optimized(), \"an optimized call cannot be megamorphic\");\n-\n-  \/\/ Cannot rely on cached_value. It is either an interface or a method.\n-  return VtableStubs::entry_point(ic_destination()) != nullptr;\n+  _call->set_destination_mt_safe(entry);\n+  assert(is_megamorphic(), \"sanity check\");\n@@ -323,20 +264,3 @@\n-bool CompiledIC::is_call_to_compiled() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-\n-  CodeBlob* cb = CodeCache::find_blob(ic_destination());\n-  bool is_monomorphic = (cb != nullptr && cb->is_compiled());\n-  \/\/ Check that the cached_value is a klass for non-optimized monomorphic calls\n-  \/\/ This assertion is invalid for compiler1: a call that does not look optimized (no static stub) can be used\n-  \/\/ for calling directly to vep without using the inline cache (i.e., cached_value == nullptr).\n-  \/\/ For JVMCI this occurs because CHA is only used to improve inlining so call sites which could be optimized\n-  \/\/ virtuals because there are no currently loaded subclasses of a type are left as virtual call sites.\n-#ifdef ASSERT\n-  CodeBlob* caller = CodeCache::find_blob(instruction_address());\n-  bool is_c1_or_jvmci_method = caller->is_compiled_by_c1() || caller->is_compiled_by_jvmci();\n-  assert( is_c1_or_jvmci_method ||\n-         !is_monomorphic ||\n-         is_optimized() ||\n-         (cached_metadata() != nullptr && cached_metadata()->is_klass()), \"sanity check\");\n-#endif \/\/ ASSERT\n-  return is_monomorphic;\n-}\n+void CompiledIC::update(CallInfo* call_info, Klass* receiver_klass) {\n+  \/\/ If this is the first time we fix the inline cache, we ensure it's initialized\n+  ensure_initialized(call_info, receiver_klass);\n@@ -344,0 +268,4 @@\n+  if (is_megamorphic()) {\n+    \/\/ Terminal state for the inline cache\n+    return;\n+  }\n@@ -345,9 +273,4 @@\n-bool CompiledIC::is_call_to_interpreted() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  \/\/ Call to interpreter if destination is either calling to a stub (if it\n-  \/\/ is optimized), or calling to an I2C blob\n-  bool is_call_to_interpreted = false;\n-  if (!is_optimized()) {\n-    CodeBlob* cb = CodeCache::find_blob(ic_destination());\n-    is_call_to_interpreted = (cb != nullptr && cb->is_adapter_blob());\n-    assert(!is_call_to_interpreted || (is_icholder_call() && cached_icholder() != nullptr), \"sanity check\");\n+  if (is_speculated_klass(receiver_klass)) {\n+    \/\/ If the speculated class matches the receiver klass, we can speculate that will\n+    \/\/ continue to be the case with a monomorphic inline cache\n+    set_to_monomorphic();\n@@ -355,8 +278,3 @@\n-    \/\/ Check if we are calling into our own codeblob (i.e., to a stub)\n-    address dest = ic_destination();\n-#ifdef ASSERT\n-    {\n-      _call->verify_resolve_call(dest);\n-    }\n-#endif \/* ASSERT *\/\n-    is_call_to_interpreted = _call->is_call_to_interpreted(dest);\n+    \/\/ If the dynamic type speculation fails, we try to transform to a megamorphic state\n+    \/\/ for the inline cache using stubs to dispatch in tables\n+    set_to_megamorphic(call_info);\n@@ -364,1 +282,0 @@\n-  return is_call_to_interpreted;\n@@ -367,11 +284,3 @@\n-bool CompiledIC::set_to_clean(bool in_use) {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  if (TraceInlineCacheClearing) {\n-    tty->print_cr(\"IC@\" INTPTR_FORMAT \": set to clean\", p2i(instruction_address()));\n-    print();\n-  }\n-  log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": set to clean\", p2i(instruction_address()));\n-\n-  address entry = _call->get_resolve_call_stub(is_optimized());\n-\n-  bool safe_transition = _call->is_safe_for_patching() || !in_use || is_optimized() || SafepointSynchronize::is_at_safepoint();\n+bool CompiledIC::is_clean() const {\n+  return destination() == SharedRuntime::get_resolve_virtual_call_stub();\n+}\n@@ -379,22 +288,2 @@\n-  if (safe_transition) {\n-    \/\/ Kill any leftover stub we might have too\n-    clear_ic_stub();\n-    if (is_optimized()) {\n-      set_ic_destination(entry);\n-    } else {\n-      set_ic_destination_and_value(entry, (void*)nullptr);\n-    }\n-  } else {\n-    \/\/ Unsafe transition - create stub.\n-    if (!InlineCacheBuffer::create_transition_stub(this, nullptr, entry)) {\n-      return false;\n-    }\n-  }\n-  \/\/ We can't check this anymore. With lazy deopt we could have already\n-  \/\/ cleaned this IC entry before we even return. This is possible if\n-  \/\/ we ran out of space in the inline cache buffer trying to do the\n-  \/\/ set_next and we safepointed to free up space. This is a benign\n-  \/\/ race because the IC entry was complete when we safepointed so\n-  \/\/ cleaning it immediately is harmless.\n-  \/\/ assert(is_clean(), \"sanity check\");\n-  return true;\n+bool CompiledIC::is_monomorphic() const {\n+  return !is_clean() && !is_megamorphic();\n@@ -403,7 +292,2 @@\n-bool CompiledIC::is_clean() const {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  bool is_clean = false;\n-  address dest = ic_destination();\n-  is_clean = dest == _call->get_resolve_call_stub(is_optimized());\n-  assert(!is_clean || is_optimized() || cached_value() == nullptr, \"sanity check\");\n-  return is_clean;\n+bool CompiledIC::is_megamorphic() const {\n+  return VtableStubs::entry_point(destination()) != nullptr;;\n@@ -412,72 +296,3 @@\n-bool CompiledIC::set_to_monomorphic(CompiledICInfo& info) {\n-  assert(CompiledICLocker::is_safe(_method), \"mt unsafe call\");\n-  \/\/ Updating a cache to the wrong entry can cause bugs that are very hard\n-  \/\/ to track down - if cache entry gets invalid - we just clean it. In\n-  \/\/ this way it is always the same code path that is responsible for\n-  \/\/ updating and resolving an inline cache\n-  \/\/\n-  \/\/ The above is no longer true. SharedRuntime::fixup_callers_callsite will change optimized\n-  \/\/ callsites. In addition ic_miss code will update a site to monomorphic if it determines\n-  \/\/ that an monomorphic call to the interpreter can now be monomorphic to compiled code.\n-  \/\/\n-  \/\/ In both of these cases the only thing being modified is the jump\/call target and these\n-  \/\/ transitions are mt_safe\n-\n-  Thread *thread = Thread::current();\n-  if (info.to_interpreter()) {\n-    \/\/ Call to interpreter\n-    if (info.is_optimized() && is_optimized()) {\n-      assert(is_clean(), \"unsafe IC path\");\n-      \/\/ the call analysis (callee structure) specifies that the call is optimized\n-      \/\/ (either because of CHA or the static target is final)\n-      \/\/ At code generation time, this call has been emitted as static call\n-      \/\/ Call via stub\n-      assert(info.cached_metadata() != nullptr && info.cached_metadata()->is_method(), \"sanity check\");\n-      methodHandle method (thread, (Method*)info.cached_metadata());\n-      _call->set_to_interpreted(method, info);\n-\n-      {\n-        ResourceMark rm(thread);\n-        log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to interpreter: %s\",\n-           p2i(instruction_address()),\n-           method->print_value_string());\n-      }\n-    } else {\n-      \/\/ Call via method-klass-holder\n-      CompiledICHolder* holder = info.claim_cached_icholder();\n-      if (!InlineCacheBuffer::create_transition_stub(this, holder, info.entry())) {\n-        delete holder;\n-        return false;\n-      }\n-      \/\/ LSan appears unable to follow malloc-based memory consistently when embedded as an\n-      \/\/ immediate in generated machine code. So we have to ignore it.\n-      LSAN_IGNORE_OBJECT(holder);\n-      {\n-         ResourceMark rm(thread);\n-         log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to interpreter via icholder \", p2i(instruction_address()));\n-      }\n-    }\n-  } else {\n-    \/\/ Call to compiled code\n-    bool static_bound = info.is_optimized() || (info.cached_metadata() == nullptr);\n-#ifdef ASSERT\n-    CodeBlob* cb = CodeCache::find_blob(info.entry());\n-    assert (cb != nullptr && cb->is_compiled(), \"must be compiled!\");\n-#endif \/* ASSERT *\/\n-\n-    \/\/ This is MT safe if we come from a clean-cache and go through a\n-    \/\/ non-verified entry point\n-    bool safe = SafepointSynchronize::is_at_safepoint() ||\n-                (!is_in_transition_state() && (info.is_optimized() || static_bound || is_clean()));\n-\n-    if (!safe) {\n-      if (!InlineCacheBuffer::create_transition_stub(this, info.cached_metadata(), info.entry())) {\n-        return false;\n-      }\n-    } else {\n-      if (is_optimized()) {\n-        set_ic_destination(info.entry());\n-      } else {\n-        set_ic_destination_and_value(info.entry(), info.cached_metadata());\n-      }\n-    }\n+bool CompiledIC::is_speculated_klass(Klass* receiver_klass) {\n+  return data()->speculated_klass() == receiver_klass;\n+}\n@@ -485,75 +300,3 @@\n-    {\n-      ResourceMark rm(thread);\n-      assert(info.cached_metadata() == nullptr || info.cached_metadata()->is_klass(), \"must be\");\n-      log_trace(inlinecache)(\"IC@\" INTPTR_FORMAT \": monomorphic to compiled (rcvr klass = %s) %s\",\n-        p2i(instruction_address()),\n-        (info.cached_metadata() != nullptr) ? ((Klass*)info.cached_metadata())->print_value_string() : \"nullptr\",\n-        (safe) ? \"\" : \" via stub\");\n-    }\n-  }\n-  \/\/ We can't check this anymore. With lazy deopt we could have already\n-  \/\/ cleaned this IC entry before we even return. This is possible if\n-  \/\/ we ran out of space in the inline cache buffer trying to do the\n-  \/\/ set_next and we safepointed to free up space. This is a benign\n-  \/\/ race because the IC entry was complete when we safepointed so\n-  \/\/ cleaning it immediately is harmless.\n-  \/\/ assert(is_call_to_compiled() || is_call_to_interpreted(), \"sanity check\");\n-  return true;\n-}\n-\n-\n-\/\/ is_optimized: Compiler has generated an optimized call (i.e. fixed, no inline cache)\n-\/\/ static_bound: The call can be static bound. If it isn't also optimized, the property\n-\/\/ wasn't provable at time of compilation. An optimized call will have any necessary\n-\/\/ null check, while a static_bound won't. A static_bound (but not optimized) must\n-\/\/ therefore use the unverified entry point.\n-void CompiledIC::compute_monomorphic_entry(const methodHandle& method,\n-                                           Klass* receiver_klass,\n-                                           bool is_optimized,\n-                                           bool static_bound,\n-                                           bool caller_is_nmethod,\n-                                           CompiledICInfo& info,\n-                                           TRAPS) {\n-  CompiledMethod* method_code = method->code();\n-\n-  address entry = nullptr;\n-  if (method_code != nullptr && method_code->is_in_use() && !method_code->is_unloading()) {\n-    assert(method_code->is_compiled(), \"must be compiled\");\n-    \/\/ Call to compiled code\n-    \/\/\n-    \/\/ Note: the following problem exists with Compiler1:\n-    \/\/   - at compile time we may or may not know if the destination is final\n-    \/\/   - if we know that the destination is final (is_optimized), we will emit\n-    \/\/     an optimized virtual call (no inline cache), and need a Method* to make\n-    \/\/     a call to the interpreter\n-    \/\/   - if we don't know if the destination is final, we emit a standard\n-    \/\/     virtual call, and use CompiledICHolder to call interpreted code\n-    \/\/     (no static call stub has been generated)\n-    \/\/   - In the case that we here notice the call is static bound we\n-    \/\/     convert the call into what looks to be an optimized virtual call,\n-    \/\/     but we must use the unverified entry point (since there will be no\n-    \/\/     null check on a call when the target isn't loaded).\n-    \/\/     This causes problems when verifying the IC because\n-    \/\/     it looks vanilla but is optimized. Code in is_call_to_interpreted\n-    \/\/     is aware of this and weakens its asserts.\n-    if (is_optimized) {\n-      entry      = method_code->verified_entry_point();\n-    } else {\n-      entry      = method_code->entry_point();\n-    }\n-  }\n-  if (entry != nullptr) {\n-    \/\/ Call to near compiled code.\n-    info.set_compiled_entry(entry, is_optimized ? nullptr : receiver_klass, is_optimized);\n-  } else {\n-    if (is_optimized) {\n-      \/\/ Use stub entry\n-      info.set_interpreter_entry(method()->get_c2i_entry(), method());\n-    } else {\n-      \/\/ Use icholder entry\n-      assert(method_code == nullptr || method_code->is_compiled(), \"must be compiled\");\n-      CompiledICHolder* holder = new CompiledICHolder(method(), receiver_klass);\n-      info.set_icholder_entry(method()->get_c2i_unverified_entry(), holder);\n-    }\n-  }\n-  assert(info.is_optimized() == is_optimized, \"must agree\");\n+\/\/ GC support\n+void CompiledIC::clean_metadata() {\n+  data()->clean_metadata();\n@@ -562,0 +305,3 @@\n+void CompiledIC::metadata_do(MetadataClosure* cl) {\n+  data()->metadata_do(cl);\n+}\n@@ -563,11 +309,5 @@\n-bool CompiledIC::is_icholder_entry(address entry) {\n-  CodeBlob* cb = CodeCache::find_blob(entry);\n-  if (cb == nullptr) {\n-    return false;\n-  }\n-  if (cb->is_adapter_blob()) {\n-    return true;\n-  } else if (cb->is_vtable_blob()) {\n-    return VtableStubs::is_icholder_entry(entry);\n-  }\n-  return false;\n+#ifndef PRODUCT\n+void CompiledIC::print() {\n+  tty->print(\"Inline cache at \" INTPTR_FORMAT \", calling \" INTPTR_FORMAT \" cached_value \" INTPTR_FORMAT,\n+             p2i(instruction_address()), p2i(destination()), p2i(data()));\n+  tty->cr();\n@@ -576,4 +316,2 @@\n-bool CompiledIC::is_icholder_call_site(virtual_call_Relocation* call_site, const CompiledMethod* cm) {\n-  \/\/ This call site might have become stale so inspect it carefully.\n-  address dest = cm->call_wrapper_at(call_site->addr())->destination();\n-  return is_icholder_entry(dest);\n+void CompiledIC::verify() {\n+  _call->verify();\n@@ -581,0 +319,1 @@\n+#endif\n@@ -584,1 +323,1 @@\n-bool CompiledStaticCall::set_to_clean(bool in_use) {\n+void CompiledDirectCall::set_to_clean() {\n@@ -588,1 +327,14 @@\n-  set_destination_mt_safe(resolve_call_stub());\n+  RelocIterator iter((nmethod*)nullptr, instruction_address(), instruction_address() + 1);\n+  while (iter.next()) {\n+    switch(iter.type()) {\n+    case relocInfo::static_call_type:\n+      _call->set_destination_mt_safe(SharedRuntime::get_resolve_static_call_stub());\n+      break;\n+    case relocInfo::opt_virtual_call_type:\n+      _call->set_destination_mt_safe(SharedRuntime::get_resolve_opt_virtual_call_stub());\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+  }\n+  assert(is_clean(), \"should be clean after cleaning\");\n@@ -590,4 +342,1 @@\n-  \/\/ Do not reset stub here:  It is too expensive to call find_stub.\n-  \/\/ Instead, rely on caller (nmethod::clear_inline_caches) to clear\n-  \/\/ both the call and its stub.\n-  return true;\n+  log_debug(inlinecache)(\"DC@\" INTPTR_FORMAT \": set to clean\", p2i(_call->instruction_address()));\n@@ -596,3 +345,3 @@\n-bool CompiledStaticCall::is_clean() const {\n-  return destination() == resolve_call_stub();\n-}\n+void CompiledDirectCall::set(const methodHandle& callee_method) {\n+  CompiledMethod* code = callee_method->code();\n+  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n@@ -600,3 +349,2 @@\n-bool CompiledStaticCall::is_call_to_compiled() const {\n-  return CodeCache::contains(destination());\n-}\n+  bool to_interp_cont_enter = caller->method()->is_continuation_enter_intrinsic() &&\n+                              ContinuationEntry::is_interpreted_call(instruction_address());\n@@ -604,6 +352,1 @@\n-bool CompiledDirectStaticCall::is_call_to_interpreted() const {\n-  \/\/ It is a call to interpreted, if it calls to a stub. Hence, the destination\n-  \/\/ must be in the stub part of the nmethod that contains the call\n-  CompiledMethod* cm = CodeCache::find_compiled(instruction_address());\n-  return cm->stub_contains(destination());\n-}\n+  bool to_compiled = !to_interp_cont_enter && code != nullptr && code->is_in_use() && !code->is_unloading();\n@@ -611,7 +354,9 @@\n-void CompiledStaticCall::set_to_compiled(address entry) {\n-  {\n-    ResourceMark rm;\n-    log_trace(inlinecache)(\"%s@\" INTPTR_FORMAT \": set_to_compiled \" INTPTR_FORMAT,\n-        name(),\n-        p2i(instruction_address()),\n-        p2i(entry));\n+  if (to_compiled) {\n+    _call->set_destination_mt_safe(code->verified_entry_point());\n+    assert(is_call_to_compiled(), \"should be compiled after set to compiled\");\n+  } else {\n+    \/\/ Patch call site to C2I adapter if code is deoptimized or unloaded.\n+    \/\/ We also need to patch the static call stub to set the rmethod register\n+    \/\/ to the callee_method so the c2i adapter knows how to build the frame\n+    set_to_interpreted(callee_method, callee_method->get_c2i_entry());\n+    assert(is_call_to_interpreted(), \"should be interpreted after set to interpreted\");\n@@ -619,3 +364,6 @@\n-  \/\/ Call to compiled code\n-  assert(CodeCache::contains(entry), \"wrong entry point\");\n-  set_destination_mt_safe(entry);\n+\n+  log_trace(inlinecache)(\"DC@\" INTPTR_FORMAT \": set to %s: %s: \" INTPTR_FORMAT,\n+                         p2i(_call->instruction_address()),\n+                         to_compiled ? \"compiled\" : \"interpreter\",\n+                         callee_method->print_value_string(),\n+                         p2i(_call->destination()));\n@@ -624,14 +372,3 @@\n-void CompiledStaticCall::set(const StaticCallInfo& info) {\n-  assert(CompiledICLocker::is_safe(instruction_address()), \"mt unsafe call\");\n-  \/\/ Updating a cache to the wrong entry can cause bugs that are very hard\n-  \/\/ to track down - if cache entry gets invalid - we just clean it. In\n-  \/\/ this way it is always the same code path that is responsible for\n-  \/\/ updating and resolving an inline cache\n-  assert(is_clean(), \"do not update a call entry - use clean\");\n-\n-  if (info._to_interpreter) {\n-    \/\/ Call to interpreted code\n-    set_to_interpreted(info.callee(), info.entry());\n-  } else {\n-    set_to_compiled(info.entry());\n-  }\n+bool CompiledDirectCall::is_clean() const {\n+  return destination() == SharedRuntime::get_resolve_static_call_stub() ||\n+         destination() == SharedRuntime::get_resolve_opt_virtual_call_stub();\n@@ -640,15 +377,5 @@\n-\/\/ Compute settings for a CompiledStaticCall. Since we might have to set\n-\/\/ the stub when calling to the interpreter, we need to return arguments.\n-void CompiledStaticCall::compute_entry(const methodHandle& m, bool caller_is_nmethod, StaticCallInfo& info) {\n-  CompiledMethod* m_code = m->code();\n-  info._callee = m;\n-  if (m_code != nullptr && m_code->is_in_use() && !m_code->is_unloading()) {\n-    info._to_interpreter = false;\n-    info._entry  = m_code->verified_entry_point();\n-  } else {\n-    \/\/ Callee is interpreted code.  In any case entering the interpreter\n-    \/\/ puts a converter-frame on the stack to save arguments.\n-    assert(!m->is_method_handle_intrinsic(), \"Compiled code should never call interpreter MH intrinsics\");\n-    info._to_interpreter = true;\n-    info._entry      = m()->get_c2i_entry();\n-  }\n+bool CompiledDirectCall::is_call_to_interpreted() const {\n+  \/\/ It is a call to interpreted, if it calls to a stub. Hence, the destination\n+  \/\/ must be in the stub part of the nmethod that contains the call\n+  CompiledMethod* cm = CodeCache::find_compiled(instruction_address());\n+  return cm->stub_contains(destination());\n@@ -657,5 +384,4 @@\n-void CompiledStaticCall::compute_entry_for_continuation_entry(const methodHandle& m, StaticCallInfo& info) {\n-  if (ContinuationEntry::is_interpreted_call(instruction_address())) {\n-    info._to_interpreter = true;\n-    info._entry = m()->get_c2i_entry();\n-  }\n+bool CompiledDirectCall::is_call_to_compiled() const {\n+  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n+  CodeBlob* dest_cb = CodeCache::find_blob(destination());\n+  return !caller->stub_contains(destination()) && dest_cb->is_compiled();\n@@ -664,1 +390,1 @@\n-address CompiledDirectStaticCall::find_stub_for(address instruction) {\n+address CompiledDirectCall::find_stub_for(address instruction) {\n@@ -676,2 +402,0 @@\n-        case relocInfo::poll_type:\n-        case relocInfo::poll_return_type: \/\/ A safepoint can't overlap a call.\n@@ -686,2 +410,2 @@\n-address CompiledDirectStaticCall::find_stub() {\n-  return CompiledDirectStaticCall::find_stub_for(instruction_address());\n+address CompiledDirectCall::find_stub() {\n+  return find_stub_for(instruction_address());\n@@ -690,6 +414,0 @@\n-address CompiledDirectStaticCall::resolve_call_stub() const {\n-  return SharedRuntime::get_resolve_static_call_stub();\n-}\n-\n-\/\/-----------------------------------------------------------------------------\n-\/\/ Non-product mode code\n@@ -697,19 +415,2 @@\n-\n-void CompiledIC::verify() {\n-  _call->verify();\n-  assert(is_clean() || is_call_to_compiled() || is_call_to_interpreted()\n-          || is_optimized() || is_megamorphic(), \"sanity check\");\n-}\n-\n-void CompiledIC::print() {\n-  print_compiled_ic();\n-  tty->cr();\n-}\n-\n-void CompiledIC::print_compiled_ic() {\n-  tty->print(\"Inline cache at \" INTPTR_FORMAT \", calling %s \" INTPTR_FORMAT \" cached_value \" INTPTR_FORMAT,\n-             p2i(instruction_address()), is_call_to_interpreted() ? \"interpreted \" : \"\", p2i(ic_destination()), p2i(is_optimized() ? nullptr : cached_value()));\n-}\n-\n-void CompiledDirectStaticCall::print() {\n-  tty->print(\"static call at \" INTPTR_FORMAT \" -> \", p2i(instruction_address()));\n+void CompiledDirectCall::print() {\n+  tty->print(\"direct call at \" INTPTR_FORMAT \" to \" INTPTR_FORMAT \" -> \", p2i(instruction_address()), p2i(destination()));\n@@ -726,3 +427,4 @@\n-void CompiledDirectStaticCall::verify_mt_safe(const methodHandle& callee, address entry,\n-                                              NativeMovConstReg* method_holder,\n-                                              NativeJump*        jump) {\n+void CompiledDirectCall::verify_mt_safe(const methodHandle& callee, address entry,\n+                                        NativeMovConstReg* method_holder,\n+                                        NativeJump* jump) {\n+  _call->verify();\n@@ -746,1 +448,1 @@\n-#endif \/\/ !PRODUCT\n+#endif\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":227,"deletions":525,"binary":false,"changes":752,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -36,25 +35,4 @@\n-\/\/ In order to make patching of the inline cache MT-safe, we only allow the following\n-\/\/ transitions (when not at a safepoint):\n-\/\/\n-\/\/\n-\/\/         [1] --<--  Clean -->---  [1]\n-\/\/            \/       (null)      \\\n-\/\/           \/                     \\      \/-<-\\\n-\/\/          \/          [2]          \\    \/     \\\n-\/\/      Interpreted  ---------> Monomorphic     | [3]\n-\/\/  (CompiledICHolder*)            (Klass*)     |\n-\/\/          \\                        \/   \\     \/\n-\/\/       [4] \\                      \/ [4] \\->-\/\n-\/\/            \\->-  Megamorphic -<-\/\n-\/\/              (CompiledICHolder*)\n-\/\/\n-\/\/ The text in parentheses () refers to the value of the inline cache receiver (mov instruction)\n-\/\/\n-\/\/ The numbers in square brackets refer to the kind of transition:\n-\/\/ [1]: Initial fixup. Receiver it found from debug information\n-\/\/ [2]: Compilation of a method\n-\/\/ [3]: Recompilation of a method (note: only entry is changed. The Klass* must stay the same)\n-\/\/ [4]: Inline cache miss. We go directly to megamorphic call.\n-\/\/\n-\/\/ The class automatically inserts transition stubs (using the InlineCacheBuffer) when an MT-unsafe\n-\/\/ transition is made to a stub.\n+\/\/ It's safe to transition from any state to any state. Typically an inline cache starts\n+\/\/ in the clean state, meaning it will resolve the call when called. Then it typically\n+\/\/ transitions to monomorphic, assuming the first dynamic receiver will be the only one\n+\/\/ observed. If that speculation fails, we transition to megamorphic.\n@@ -65,1 +43,0 @@\n-class ICStub;\n@@ -80,8 +57,17 @@\n-class CompiledICInfo : public StackObj {\n- private:\n-  address _entry;              \/\/ entry point for call\n-  void*   _cached_value;         \/\/ Value of cached_value (either in stub or inline cache)\n-  bool    _is_icholder;          \/\/ Is the cached value a CompiledICHolder*\n-  bool    _is_optimized;       \/\/ it is an optimized virtual call (i.e., can be statically bound)\n-  bool    _to_interpreter;     \/\/ Call it to interpreter\n-  bool    _release_icholder;\n+\/\/ A CompiledICData is a helper object for the inline cache implementation.\n+\/\/ It comprises:\n+\/\/ (1) The first receiver klass and its selected method\n+\/\/ (2) Itable call metadata\n+\n+class CompiledICData : public CHeapObj<mtCode> {\n+  friend class VMStructs;\n+  friend class JVMCIVMStructs;\n+\n+  Method*   volatile _speculated_method;\n+  uintptr_t volatile _speculated_klass;\n+  Klass*             _itable_defc_klass;\n+  Klass*             _itable_refc_klass;\n+  bool               _is_initialized;\n+\n+  bool is_speculated_klass_unloaded() const;\n+\n@@ -89,21 +75,2 @@\n-  address entry() const        { return _entry; }\n-  Metadata*    cached_metadata() const         { assert(!_is_icholder, \"\"); return (Metadata*)_cached_value; }\n-  CompiledICHolder*    claim_cached_icholder() {\n-    assert(_is_icholder, \"\");\n-    assert(_cached_value != nullptr, \"must be non-null\");\n-    _release_icholder = false;\n-    CompiledICHolder* icholder = (CompiledICHolder*)_cached_value;\n-    icholder->claim();\n-    return icholder;\n-  }\n-  bool    is_optimized() const { return _is_optimized; }\n-  bool  to_interpreter() const { return _to_interpreter; }\n-\n-  void set_compiled_entry(address entry, Klass* klass, bool is_optimized) {\n-    _entry      = entry;\n-    _cached_value = (void*)klass;\n-    _to_interpreter = false;\n-    _is_icholder = false;\n-    _is_optimized = is_optimized;\n-    _release_icholder = false;\n-  }\n+  \/\/ Constructor\n+  CompiledICData();\n@@ -111,8 +78,5 @@\n-  void set_interpreter_entry(address entry, Method* method) {\n-    _entry      = entry;\n-    _cached_value = (void*)method;\n-    _to_interpreter = true;\n-    _is_icholder = false;\n-    _is_optimized = true;\n-    _release_icholder = false;\n-  }\n+  \/\/ accessors\n+  Klass*    speculated_klass()  const;\n+  Method*   speculated_method() const { return _speculated_method; }\n+  Klass*    itable_defc_klass() const { return _itable_defc_klass; }\n+  Klass*    itable_refc_klass() const { return _itable_refc_klass; }\n@@ -120,8 +84,2 @@\n-  void set_icholder_entry(address entry, CompiledICHolder* icholder) {\n-    _entry      = entry;\n-    _cached_value = (void*)icholder;\n-    _to_interpreter = true;\n-    _is_icholder = true;\n-    _is_optimized = false;\n-    _release_icholder = true;\n-  }\n+  static ByteSize speculated_method_offset() { return byte_offset_of(CompiledICData, _speculated_method); }\n+  static ByteSize speculated_klass_offset()  { return byte_offset_of(CompiledICData, _speculated_klass); }\n@@ -129,14 +87,2 @@\n-  CompiledICInfo(): _entry(nullptr), _cached_value(nullptr), _is_icholder(false),\n-                    _is_optimized(false), _to_interpreter(false), _release_icholder(false) {\n-  }\n-  ~CompiledICInfo() {\n-    \/\/ In rare cases the info is computed but not used, so release any\n-    \/\/ CompiledICHolder* that was created\n-    if (_release_icholder) {\n-      assert(_is_icholder, \"must be\");\n-      CompiledICHolder* icholder = (CompiledICHolder*)_cached_value;\n-      icholder->claim();\n-      delete icholder;\n-    }\n-  }\n-};\n+  static ByteSize itable_defc_klass_offset() { return byte_offset_of(CompiledICData, _itable_defc_klass); }\n+  static ByteSize itable_refc_klass_offset() { return byte_offset_of(CompiledICData, _itable_refc_klass); }\n@@ -144,19 +90,7 @@\n-class NativeCallWrapper: public ResourceObj {\n-public:\n-  virtual address destination() const = 0;\n-  virtual address instruction_address() const = 0;\n-  virtual address next_instruction_address() const = 0;\n-  virtual address return_address() const = 0;\n-  virtual address get_resolve_call_stub(bool is_optimized) const = 0;\n-  virtual void set_destination_mt_safe(address dest) = 0;\n-  virtual void set_to_interpreted(const methodHandle& method, CompiledICInfo& info) = 0;\n-  virtual void verify() const = 0;\n-  virtual void verify_resolve_call(address dest) const = 0;\n-\n-  virtual bool is_call_to_interpreted(address dest) const = 0;\n-  virtual bool is_safe_for_patching() const = 0;\n-\n-  virtual NativeInstruction* get_load_instruction(virtual_call_Relocation* r) const = 0;\n-\n-  virtual void *get_data(NativeInstruction* instruction) const = 0;\n-  virtual void set_data(NativeInstruction* instruction, intptr_t data) = 0;\n+  void initialize(CallInfo* call_info, Klass* receiver_klass);\n+\n+  bool is_initialized()       const { return _is_initialized; }\n+\n+  \/\/ GC Support\n+  void clean_metadata();\n+  void metadata_do(MetadataClosure* cl);\n@@ -166,7 +100,1 @@\n-  friend class InlineCacheBuffer;\n-  friend class ICStub;\n-\n- private:\n-  NativeCallWrapper* _call;\n-  NativeInstruction* _value;    \/\/ patchable value cell for this IC\n-  bool          _is_optimized;  \/\/ an optimized virtual call (i.e., no compiled IC)\n+private:\n@@ -174,0 +102,2 @@\n+  CompiledICData* _data;\n+  NativeCall* _call;\n@@ -175,1 +105,0 @@\n-  CompiledIC(CompiledMethod* cm, NativeCall* ic_call);\n@@ -178,1 +107,3 @@\n-  void initialize_from_iter(RelocIterator* iter);\n+  \/\/ CompiledICData wrappers\n+  void ensure_initialized(CallInfo* call_info, Klass* receiver_klass);\n+  bool is_speculated_klass(Klass* receiver_klass);\n@@ -180,1 +111,3 @@\n-  static bool is_icholder_entry(address entry);\n+  \/\/ Inline cache states\n+  void set_to_monomorphic();\n+  void set_to_megamorphic(CallInfo* call_info);\n@@ -182,26 +115,1 @@\n-  \/\/ low-level inline-cache manipulation. Cannot be accessed directly, since it might not be MT-safe\n-  \/\/ to change an inline-cache. These changes the underlying inline-cache directly. They *newer* make\n-  \/\/ changes to a transition stub.\n-  void internal_set_ic_destination(address entry_point, bool is_icstub, void* cache, bool is_icholder);\n-  void set_ic_destination(ICStub* stub);\n-  void set_ic_destination(address entry_point) {\n-    assert(_is_optimized, \"use set_ic_destination_and_value instead\");\n-    internal_set_ic_destination(entry_point, false, nullptr, false);\n-  }\n-  \/\/ This only for use by ICStubs where the type of the value isn't known\n-  void set_ic_destination_and_value(address entry_point, void* value) {\n-    internal_set_ic_destination(entry_point, false, value, is_icholder_entry(entry_point));\n-  }\n-  void set_ic_destination_and_value(address entry_point, Metadata* value) {\n-    internal_set_ic_destination(entry_point, false, value, false);\n-  }\n-  void set_ic_destination_and_value(address entry_point, CompiledICHolder* value) {\n-    internal_set_ic_destination(entry_point, false, value, true);\n-  }\n-\n-  \/\/ Reads the location of the transition stub. This will fail with an assertion, if no transition stub is\n-  \/\/ associated with the inline cache.\n-  address stub_address() const;\n-  bool is_in_transition_state() const;  \/\/ Use InlineCacheBuffer\n-\n- public:\n+public:\n@@ -214,25 +122,1 @@\n-  static bool is_icholder_call_site(virtual_call_Relocation* call_site, const CompiledMethod* cm);\n-\n-  \/\/ Return the cached_metadata\/destination associated with this inline cache. If the cache currently points\n-  \/\/ to a transition stub, it will read the values from the transition stub.\n-  void* cached_value() const;\n-  CompiledICHolder* cached_icholder() const {\n-    assert(is_icholder_call(), \"must be\");\n-    return (CompiledICHolder*) cached_value();\n-  }\n-  Metadata* cached_metadata() const {\n-    assert(!is_icholder_call(), \"must be\");\n-    return (Metadata*) cached_value();\n-  }\n-\n-  void* get_data() const {\n-    return _call->get_data(_value);\n-  }\n-\n-  void set_data(intptr_t data) {\n-    _call->set_data(_value, data);\n-  }\n-\n-  address ic_destination() const;\n-\n-  bool is_optimized() const   { return _is_optimized; }\n+  CompiledICData* data() const;\n@@ -241,1 +125,2 @@\n-  bool is_clean() const;\n+  bool is_clean()       const;\n+  bool is_monomorphic() const;\n@@ -243,4 +128,0 @@\n-  bool is_call_to_compiled() const;\n-  bool is_call_to_interpreted() const;\n-\n-  bool is_icholder_call() const;\n@@ -248,1 +129,1 @@\n-  address end_of_call() const { return  _call->return_address(); }\n+  address end_of_call() const { return _call->return_address(); }\n@@ -250,1 +131,1 @@\n-  \/\/ MT-safe patching of inline caches. Note: Only safe to call is_xxx when holding the CompiledIC_ock\n+  \/\/ MT-safe patching of inline caches. Note: Only safe to call is_xxx when holding the CompiledICLocker\n@@ -252,17 +133,6 @@\n-  \/\/\n-  \/\/ Note: We do not provide any direct access to the stub code, to prevent parts of the code\n-  \/\/ to manipulate the inline cache in MT-unsafe ways.\n-  \/\/\n-  \/\/ They all takes a TRAP argument, since they can cause a GC if the inline-cache buffer is full.\n-  \/\/\n-  bool set_to_clean(bool in_use = true);\n-  bool set_to_monomorphic(CompiledICInfo& info);\n-  void clear_ic_stub();\n-\n-  \/\/ Returns true if successful and false otherwise. The call can fail if memory\n-  \/\/ allocation in the code cache fails, or ic stub refill is required.\n-  bool set_to_megamorphic(CallInfo* call_info, Bytecodes::Code bytecode, bool& needs_ic_stub_refill, TRAPS);\n-\n-  static void compute_monomorphic_entry(const methodHandle& method, Klass* receiver_klass,\n-                                        bool is_optimized, bool static_bound, bool caller_is_nmethod,\n-                                        CompiledICInfo& info, TRAPS);\n+  void set_to_clean();\n+  void update(CallInfo* call_info, Klass* receiver_klass);\n+\n+  \/\/ GC support\n+  void clean_metadata();\n+  void metadata_do(MetadataClosure* cl);\n@@ -272,0 +142,1 @@\n+  address destination() const         { return _call->destination(); }\n@@ -275,1 +146,0 @@\n-  void print_compiled_ic() PRODUCT_RETURN;\n@@ -279,27 +149,4 @@\n-inline CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr) {\n-  CompiledIC* c_ic = new CompiledIC(nm, nativeCall_before(return_addr));\n-  c_ic->verify();\n-  return c_ic;\n-}\n-\n-inline CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site) {\n-  CompiledIC* c_ic = new CompiledIC(nm, nativeCall_at(call_site));\n-  c_ic->verify();\n-  return c_ic;\n-}\n-\n-inline CompiledIC* CompiledIC_at(Relocation* call_site) {\n-  assert(call_site->type() == relocInfo::virtual_call_type ||\n-         call_site->type() == relocInfo::opt_virtual_call_type, \"wrong reloc. info\");\n-  CompiledIC* c_ic = new CompiledIC(call_site->code(), nativeCall_at(call_site->addr()));\n-  c_ic->verify();\n-  return c_ic;\n-}\n-\n-inline CompiledIC* CompiledIC_at(RelocIterator* reloc_iter) {\n-  assert(reloc_iter->type() == relocInfo::virtual_call_type ||\n-      reloc_iter->type() == relocInfo::opt_virtual_call_type, \"wrong reloc. info\");\n-  CompiledIC* c_ic = new CompiledIC(reloc_iter);\n-  c_ic->verify();\n-  return c_ic;\n-}\n+CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr);\n+CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site);\n+CompiledIC* CompiledIC_at(Relocation* call_site);\n+CompiledIC* CompiledIC_at(RelocIterator* reloc_iter);\n@@ -308,3 +155,1 @@\n-\/\/ The CompiledStaticCall represents a call to a static method in the compiled\n-\/\/\n-\/\/ Transition diagram of a static call site is somewhat simpler than for an inlined cache:\n+\/\/ The CompiledDirectCall represents a call to a method in the compiled code\n@@ -324,57 +169,1 @@\n-class StaticCallInfo {\n- private:\n-  address      _entry;          \/\/ Entrypoint\n-  methodHandle _callee;         \/\/ Callee (used when calling interpreter)\n-  bool         _to_interpreter; \/\/ call to interpreted method (otherwise compiled)\n-\n-  friend class CompiledStaticCall;\n-  friend class CompiledDirectStaticCall;\n-  friend class CompiledPltStaticCall;\n- public:\n-  address      entry() const    { return _entry;  }\n-  methodHandle callee() const   { return _callee; }\n-};\n-\n-class CompiledStaticCall : public ResourceObj {\n- public:\n-  \/\/ Code\n-\n-  \/\/ Returns null if CodeBuffer::expand fails\n-  static address emit_to_interp_stub(CodeBuffer &cbuf, address mark = nullptr);\n-  static int to_interp_stub_size();\n-  static int to_trampoline_stub_size();\n-  static int reloc_to_interp_stub();\n-\n-  \/\/ Compute entry point given a method\n-  static void compute_entry(const methodHandle& m, bool caller_is_nmethod, StaticCallInfo& info);\n-  void compute_entry_for_continuation_entry(const methodHandle& m, StaticCallInfo& info);\n-\n-public:\n-  \/\/ Clean static call (will force resolving on next use)\n-  virtual address destination() const = 0;\n-\n-  \/\/ Clean static call (will force resolving on next use)\n-  bool set_to_clean(bool in_use = true);\n-\n-  \/\/ Set state. The entry must be the same, as computed by compute_entry.\n-  \/\/ Computation and setting is split up, since the actions are separate during\n-  \/\/ a OptoRuntime::resolve_xxx.\n-  void set(const StaticCallInfo& info);\n-\n-  \/\/ State\n-  bool is_clean() const;\n-  bool is_call_to_compiled() const;\n-  virtual bool is_call_to_interpreted() const = 0;\n-\n-  virtual address instruction_address() const = 0;\n-  virtual address end_of_call() const = 0;\n-protected:\n-  virtual address resolve_call_stub() const = 0;\n-  virtual void set_destination_mt_safe(address dest) = 0;\n-  virtual void set_to_interpreted(const methodHandle& callee, address entry) = 0;\n-  virtual const char* name() const = 0;\n-\n-  void set_to_compiled(address entry);\n-};\n-\n-class CompiledDirectStaticCall : public CompiledStaticCall {\n+class CompiledDirectCall : public ResourceObj {\n@@ -395,1 +184,1 @@\n-  CompiledDirectStaticCall(NativeCall* call) : _call(call) {}\n+  CompiledDirectCall(NativeCall* call) : _call(call) {}\n@@ -398,2 +187,8 @@\n-  static inline CompiledDirectStaticCall* before(address return_addr) {\n-    CompiledDirectStaticCall* st = new CompiledDirectStaticCall(nativeCall_before(return_addr));\n+  \/\/ Returns null if CodeBuffer::expand fails\n+  static address emit_to_interp_stub(CodeBuffer &cbuf, address mark = nullptr);\n+  static int to_interp_stub_size();\n+  static int to_trampoline_stub_size();\n+  static int reloc_to_interp_stub();\n+\n+  static inline CompiledDirectCall* before(address return_addr) {\n+    CompiledDirectCall* st = new CompiledDirectCall(nativeCall_before(return_addr));\n@@ -404,2 +199,2 @@\n-  static inline CompiledDirectStaticCall* at(address native_call) {\n-    CompiledDirectStaticCall* st = new CompiledDirectStaticCall(nativeCall_at(native_call));\n+  static inline CompiledDirectCall* at(address native_call) {\n+    CompiledDirectCall* st = new CompiledDirectCall(nativeCall_at(native_call));\n@@ -410,1 +205,1 @@\n-  static inline CompiledDirectStaticCall* at(Relocation* call_site) {\n+  static inline CompiledDirectCall* at(Relocation* call_site) {\n@@ -418,0 +213,5 @@\n+  \/\/ Clean static call (will force resolving on next use)\n+  void set_to_clean();\n+\n+  void set(const methodHandle& callee_method);\n+\n@@ -419,1 +219,3 @@\n-  virtual bool is_call_to_interpreted() const;\n+  bool is_clean() const;\n+  bool is_call_to_interpreted() const;\n+  bool is_call_to_compiled() const;\n@@ -429,4 +231,0 @@\n-\n- protected:\n-  virtual address resolve_call_stub() const;\n-  virtual const char* name() const { return \"CompiledDirectStaticCall\"; }\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":87,"deletions":289,"binary":false,"changes":376,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -39,1 +38,0 @@\n-#include \"oops\/compiledICHolder.inline.hpp\"\n@@ -338,22 +336,0 @@\n-int CompiledMethod::verify_icholder_relocations() {\n-  ResourceMark rm;\n-  int count = 0;\n-\n-  RelocIterator iter(this);\n-  while(iter.next()) {\n-    if (iter.type() == relocInfo::virtual_call_type) {\n-      if (CompiledIC::is_icholder_call_site(iter.virtual_call_reloc(), this)) {\n-        CompiledIC *ic = CompiledIC_at(&iter);\n-        if (TraceCompiledIC) {\n-          tty->print(\"noticed icholder \" INTPTR_FORMAT \" \", p2i(ic->cached_icholder()));\n-          ic->print();\n-        }\n-        assert(ic->cached_icholder() != nullptr, \"must be non-nullptr\");\n-        count++;\n-      }\n-    }\n-  }\n-\n-  return count;\n-}\n-\n@@ -434,14 +410,0 @@\n-\/\/ Clear IC callsites, releasing ICStubs of all compiled ICs\n-\/\/ as well as any associated CompiledICHolders.\n-void CompiledMethod::clear_ic_callsites() {\n-  assert(CompiledICLocker::is_safe(this), \"mt unsafe call\");\n-  ResourceMark rm;\n-  RelocIterator iter(this);\n-  while(iter.next()) {\n-    if (iter.type() == relocInfo::virtual_call_type) {\n-      CompiledIC* ic = CompiledIC_at(&iter);\n-      ic->set_to_clean(false);\n-    }\n-  }\n-}\n-\n@@ -469,36 +431,2 @@\n-bool CompiledMethod::clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n-  if (ic->is_clean()) {\n-    return true;\n-  }\n-  if (ic->is_icholder_call()) {\n-    \/\/ The only exception is compiledICHolder metadata which may\n-    \/\/ yet be marked below. (We check this further below).\n-    CompiledICHolder* cichk_metdata = ic->cached_icholder();\n-\n-    if (cichk_metdata->is_loader_alive()) {\n-      return true;\n-    }\n-  } else {\n-    Metadata* ic_metdata = ic->cached_metadata();\n-    if (ic_metdata != nullptr) {\n-      if (ic_metdata->is_klass()) {\n-        if (((Klass*)ic_metdata)->is_loader_alive()) {\n-          return true;\n-        }\n-      } else if (ic_metdata->is_method()) {\n-        Method* method = (Method*)ic_metdata;\n-        assert(!method->is_old(), \"old method should have been cleaned\");\n-        if (method->method_holder()->is_loader_alive()) {\n-          return true;\n-        }\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-    } else {\n-      \/\/ This inline cache is a megamorphic vtable call. Those ICs never hold\n-      \/\/ any Metadata and should therefore never be cleaned by this function.\n-      return true;\n-    }\n-  }\n-\n-  return ic->set_to_clean();\n+static void clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n+  ic->clean_metadata();\n@@ -508,2 +436,2 @@\n-template <class CompiledICorStaticCall>\n-static bool clean_if_nmethod_is_unloaded(CompiledICorStaticCall *ic, address addr, CompiledMethod* from,\n+template <typename CallsiteT>\n+static void clean_if_nmethod_is_unloaded(CallsiteT* callsite, CompiledMethod* from,\n@@ -511,10 +439,7 @@\n-  CodeBlob *cb = CodeCache::find_blob(addr);\n-  CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n-  if (nm != nullptr) {\n-    \/\/ Clean inline caches pointing to bad nmethods\n-    if (clean_all || !nm->is_in_use() || nm->is_unloading() || (nm->method()->code() != nm)) {\n-      if (!ic->set_to_clean(!from->is_unloading())) {\n-        return false;\n-      }\n-      assert(ic->is_clean(), \"nmethod \" PTR_FORMAT \"not clean %s\", p2i(from), from->method()->name_and_sig_as_C_string());\n-    }\n+  CodeBlob* cb = CodeCache::find_blob(callsite->destination());\n+  if (!cb->is_compiled()) {\n+    return;\n+  }\n+  CompiledMethod* cm = cb->as_compiled_method();\n+  if (clean_all || !cm->is_in_use() || cm->is_unloading() || cm->method()->code() != cm) {\n+    callsite->set_to_clean();\n@@ -522,11 +447,0 @@\n-  return true;\n-}\n-\n-static bool clean_if_nmethod_is_unloaded(CompiledIC *ic, CompiledMethod* from,\n-                                         bool clean_all) {\n-  return clean_if_nmethod_is_unloaded(ic, ic->ic_destination(), from, clean_all);\n-}\n-\n-static bool clean_if_nmethod_is_unloaded(CompiledStaticCall *csc, CompiledMethod* from,\n-                                         bool clean_all) {\n-  return clean_if_nmethod_is_unloaded(csc, csc->destination(), from, clean_all);\n@@ -542,1 +456,1 @@\n-bool CompiledMethod::unload_nmethod_caches(bool unloading_occurred) {\n+void CompiledMethod::unload_nmethod_caches(bool unloading_occurred) {\n@@ -550,3 +464,1 @@\n-  if (!cleanup_inline_caches_impl(unloading_occurred, false)) {\n-    return false;\n-  }\n+  cleanup_inline_caches_impl(unloading_occurred, false);\n@@ -559,1 +471,0 @@\n-  return true;\n@@ -581,2 +492,1 @@\n-  guarantee(cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/),\n-            \"Inline cache cleaning in a safepoint can't fail\");\n+  cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/);\n@@ -590,1 +500,1 @@\n-bool CompiledMethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n+void CompiledMethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n@@ -605,3 +515,1 @@\n-        if (!clean_ic_if_metadata_is_dead(CompiledIC_at(&iter))) {\n-          return false;\n-        }\n+        clean_ic_if_metadata_is_dead(CompiledIC_at(&iter));\n@@ -610,3 +518,1 @@\n-      if (!clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all)) {\n-        return false;\n-      }\n+      clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all);\n@@ -616,5 +522,0 @@\n-      if (!clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all)) {\n-        return false;\n-      }\n-      break;\n-\n@@ -622,3 +523,1 @@\n-      if (!clean_if_nmethod_is_unloaded(compiledStaticCall_at(iter.reloc()), this, clean_all)) {\n-        return false;\n-      }\n+      clean_if_nmethod_is_unloaded(CompiledDirectCall::at(iter.reloc()), this, clean_all);\n@@ -675,2 +574,0 @@\n-\n-  return true;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":18,"deletions":121,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-class CompiledStaticCall;\n+class CompiledDirectCall;\n@@ -367,1 +367,1 @@\n-  bool cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n+  void cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n@@ -376,1 +376,0 @@\n-  void clear_ic_callsites();\n@@ -381,2 +380,0 @@\n-  \/\/ Verify and count cached icholder relocations.\n-  int  verify_icholder_relocations();\n@@ -392,2 +389,0 @@\n-  virtual NativeCallWrapper* call_wrapper_at(address call) const = 0;\n-  virtual NativeCallWrapper* call_wrapper_before(address return_pc) const = 0;\n@@ -396,4 +391,0 @@\n-  virtual CompiledStaticCall* compiledStaticCall_at(Relocation* call_site) const = 0;\n-  virtual CompiledStaticCall* compiledStaticCall_at(address addr) const = 0;\n-  virtual CompiledStaticCall* compiledStaticCall_before(address addr) const = 0;\n-\n@@ -409,3 +400,0 @@\n- private:\n-  bool static clean_ic_if_metadata_is_dead(CompiledIC *ic);\n-\n@@ -418,1 +406,1 @@\n-  bool unload_nmethod_caches(bool class_unloading_occurred);\n+  void unload_nmethod_caches(bool class_unloading_occurred);\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1,293 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"code\/compiledIC.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"code\/scopeDesc.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"interpreter\/linkResolver.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"runtime\/vmOperations.hpp\"\n-\n-DEF_STUB_INTERFACE(ICStub);\n-\n-StubQueue* InlineCacheBuffer::_buffer    = nullptr;\n-\n-CompiledICHolder* volatile InlineCacheBuffer::_pending_released = nullptr;\n-volatile int InlineCacheBuffer::_pending_count = 0;\n-\n-#ifdef ASSERT\n-ICRefillVerifier::ICRefillVerifier()\n-  : _refill_requested(false),\n-    _refill_remembered(false)\n-{\n-  Thread* thread = Thread::current();\n-  assert(thread->missed_ic_stub_refill_verifier() == nullptr, \"nesting not supported\");\n-  thread->set_missed_ic_stub_refill_verifier(this);\n-}\n-\n-ICRefillVerifier::~ICRefillVerifier() {\n-  assert(!_refill_requested || _refill_remembered,\n-         \"Forgot to refill IC stubs after failed IC transition\");\n-  Thread::current()->set_missed_ic_stub_refill_verifier(nullptr);\n-}\n-\n-ICRefillVerifierMark::ICRefillVerifierMark(ICRefillVerifier* verifier) {\n-  Thread* thread = Thread::current();\n-  assert(thread->missed_ic_stub_refill_verifier() == nullptr, \"nesting not supported\");\n-  thread->set_missed_ic_stub_refill_verifier(verifier);\n-}\n-\n-ICRefillVerifierMark::~ICRefillVerifierMark() {\n-  Thread::current()->set_missed_ic_stub_refill_verifier(nullptr);\n-}\n-\n-static ICRefillVerifier* current_ic_refill_verifier() {\n-  Thread* current = Thread::current();\n-  ICRefillVerifier* verifier = current->missed_ic_stub_refill_verifier();\n-  assert(verifier != nullptr, \"need a verifier for safety\");\n-  return verifier;\n-}\n-#endif\n-\n-void ICStub::finalize() {\n-  if (!is_empty()) {\n-    ResourceMark rm;\n-    CompiledIC *ic = CompiledIC_at(CodeCache::find_compiled(ic_site()), ic_site());\n-    assert(CodeCache::find_compiled(ic->instruction_address()) != nullptr, \"inline cache in non-compiled?\");\n-\n-    assert(this == ICStub::from_destination_address(ic->stub_address()), \"wrong owner of ic buffer\");\n-    ic->set_ic_destination_and_value(destination(), cached_value());\n-  }\n-}\n-\n-\n-address ICStub::destination() const {\n-  return InlineCacheBuffer::ic_buffer_entry_point(code_begin());\n-}\n-\n-void* ICStub::cached_value() const {\n-  return InlineCacheBuffer::ic_buffer_cached_value(code_begin());\n-}\n-\n-\n-void ICStub::set_stub(CompiledIC *ic, void* cached_val, address dest_addr) {\n-  \/\/ We cannot store a pointer to the 'ic' object, since it is resource allocated. Instead we\n-  \/\/ store the location of the inline cache. Then we have enough information recreate the CompiledIC\n-  \/\/ object when we need to remove the stub.\n-  _ic_site = ic->instruction_address();\n-\n-  \/\/ Assemble new stub\n-  InlineCacheBuffer::assemble_ic_buffer_code(code_begin(), cached_val, dest_addr);\n-  assert(destination() == dest_addr,   \"can recover destination\");\n-  assert(cached_value() == cached_val, \"can recover destination\");\n-}\n-\n-\n-void ICStub::clear() {\n-  if (CompiledIC::is_icholder_entry(destination())) {\n-    InlineCacheBuffer::queue_for_release((CompiledICHolder*)cached_value());\n-  }\n-  _ic_site = nullptr;\n-}\n-\n-\n-#ifndef PRODUCT\n-\/\/ anybody calling to this stub will trap\n-\n-void ICStub::verify() {\n-}\n-\n-void ICStub::print() {\n-  tty->print_cr(\"ICStub: site: \" INTPTR_FORMAT, p2i(_ic_site));\n-}\n-#endif\n-\n-\/\/-----------------------------------------------------------------------------------------------\n-\/\/ Implementation of InlineCacheBuffer\n-\n-\n-void InlineCacheBuffer::initialize() {\n-  if (_buffer != nullptr) return; \/\/ already initialized\n-  _buffer = new StubQueue(new ICStubInterface, checked_cast<int>(InlineCacheBufferSize), InlineCacheBuffer_lock, \"InlineCacheBuffer\");\n-  assert (_buffer != nullptr, \"cannot allocate InlineCacheBuffer\");\n-}\n-\n-\n-void InlineCacheBuffer::refill_ic_stubs() {\n-#ifdef ASSERT\n-  ICRefillVerifier* verifier = current_ic_refill_verifier();\n-  verifier->request_remembered();\n-#endif\n-  \/\/ we ran out of inline cache buffer space; must enter safepoint.\n-  \/\/ We do this by forcing a safepoint\n-  VM_ICBufferFull ibf;\n-  VMThread::execute(&ibf);\n-}\n-\n-bool InlineCacheBuffer::needs_update_inline_caches() {\n-  \/\/ Stub removal\n-  if (buffer()->number_of_stubs() > 0) {\n-    return true;\n-  }\n-\n-  \/\/ Release pending CompiledICHolder\n-  if (pending_icholder_count() > 0) {\n-    return true;\n-  }\n-\n-  return false;\n-}\n-\n-void InlineCacheBuffer::update_inline_caches() {\n-  if (buffer()->number_of_stubs() > 0) {\n-    if (TraceICBuffer) {\n-      tty->print_cr(\"[updating inline caches with %d stubs]\", buffer()->number_of_stubs());\n-    }\n-    buffer()->remove_all();\n-  }\n-  release_pending_icholders();\n-}\n-\n-\n-bool InlineCacheBuffer::contains(address instruction_address) {\n-  return buffer()->contains(instruction_address);\n-}\n-\n-\n-bool InlineCacheBuffer::is_empty() {\n-  return buffer()->number_of_stubs() == 0;\n-}\n-\n-\n-void InlineCacheBuffer_init() {\n-  InlineCacheBuffer::initialize();\n-}\n-\n-bool InlineCacheBuffer::create_transition_stub(CompiledIC *ic, void* cached_value, address entry) {\n-  assert(!SafepointSynchronize::is_at_safepoint(), \"should not be called during a safepoint\");\n-  assert(CompiledICLocker::is_safe(ic->instruction_address()), \"mt unsafe call\");\n-  if (TraceICBuffer) {\n-    tty->print_cr(\"  create transition stub for \" INTPTR_FORMAT \" destination \" INTPTR_FORMAT \" cached value \" INTPTR_FORMAT,\n-                  p2i(ic->instruction_address()), p2i(entry), p2i(cached_value));\n-  }\n-\n-  \/\/ allocate and initialize new \"out-of-line\" inline-cache\n-  ICStub* ic_stub = (ICStub*) buffer()->request_committed(ic_stub_code_size());\n-  if (ic_stub == nullptr) {\n-#ifdef ASSERT\n-    ICRefillVerifier* verifier = current_ic_refill_verifier();\n-    verifier->request_refill();\n-#endif\n-    return false;\n-  }\n-\n-#ifdef ASSERT\n-  {\n-    ICStub* rev_stub = ICStub::from_destination_address(ic_stub->code_begin());\n-    assert(ic_stub == rev_stub,\n-           \"ICStub mapping is reversible: stub=\" PTR_FORMAT \", code=\" PTR_FORMAT \", rev_stub=\" PTR_FORMAT,\n-           p2i(ic_stub), p2i(ic_stub->code_begin()), p2i(rev_stub));\n-  }\n-#endif\n-\n-  \/\/ If an transition stub is already associate with the inline cache, then we remove the association.\n-  if (ic->is_in_transition_state()) {\n-    ICStub* old_stub = ICStub::from_destination_address(ic->stub_address());\n-    old_stub->clear();\n-  }\n-\n-  ic_stub->set_stub(ic, cached_value, entry);\n-\n-  \/\/ Update inline cache in nmethod to point to new \"out-of-line\" allocated inline cache\n-  ic->set_ic_destination(ic_stub);\n-  return true;\n-}\n-\n-\n-address InlineCacheBuffer::ic_destination_for(CompiledIC *ic) {\n-  ICStub* stub = ICStub::from_destination_address(ic->stub_address());\n-  return stub->destination();\n-}\n-\n-\n-void* InlineCacheBuffer::cached_value_for(CompiledIC *ic) {\n-  ICStub* stub = ICStub::from_destination_address(ic->stub_address());\n-  return stub->cached_value();\n-}\n-\n-\n-\/\/ Free CompiledICHolder*s that are no longer in use\n-void InlineCacheBuffer::release_pending_icholders() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"should only be called during a safepoint\");\n-  CompiledICHolder* holder = Atomic::load(&_pending_released);\n-  _pending_released = nullptr;\n-  int count = 0;\n-  while (holder != nullptr) {\n-    CompiledICHolder* next = holder->next();\n-    delete holder;\n-    holder = next;\n-    count++;\n-  }\n-  assert(pending_icholder_count() == count, \"wrong count\");\n-  Atomic::store(&_pending_count, 0);\n-}\n-\n-\/\/ Enqueue this icholder for release during the next safepoint.  It's\n-\/\/ not safe to free them until then since they might be visible to\n-\/\/ another thread.\n-void InlineCacheBuffer::queue_for_release(CompiledICHolder* icholder) {\n-  assert(icholder->next() == nullptr, \"multiple enqueue?\");\n-\n-  CompiledICHolder* old = Atomic::load(&_pending_released);\n-  for (;;) {\n-    icholder->set_next(old);\n-    \/\/ The only reader runs at a safepoint serially so there is no need for a more strict atomic.\n-    CompiledICHolder* cur = Atomic::cmpxchg(&_pending_released, old, icholder, memory_order_relaxed);\n-    if (cur == old) {\n-      break;\n-    }\n-    old = cur;\n-  }\n-  Atomic::inc(&_pending_count, memory_order_relaxed);\n-\n-  if (TraceICBuffer) {\n-    tty->print_cr(\"enqueueing icholder \" INTPTR_FORMAT \" to be freed\", p2i(icholder));\n-  }\n-}\n-\n-int InlineCacheBuffer::pending_icholder_count() {\n-  return Atomic::load(&_pending_count);\n-}\n","filename":"src\/hotspot\/share\/code\/icBuffer.cpp","additions":0,"deletions":293,"binary":false,"changes":293,"status":"deleted"},{"patch":"@@ -1,193 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_CODE_ICBUFFER_HPP\n-#define SHARE_CODE_ICBUFFER_HPP\n-\n-#include \"asm\/codeBuffer.hpp\"\n-#include \"code\/stubs.hpp\"\n-#include \"interpreter\/bytecodes.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/safepointVerifiers.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class CompiledIC;\n-class CompiledICHolder;\n-\n-\/\/\n-\/\/ For CompiledIC's:\n-\/\/\n-\/\/ In cases where we do not have MT-safe state transformation,\n-\/\/ we go to a transition state, using ICStubs. At a safepoint,\n-\/\/ the inline caches are transferred from the transitional code:\n-\/\/\n-\/\/    instruction_address --> 01 set xxx_oop, Ginline_cache_klass\n-\/\/                            23 jump_to Gtemp, yyyy\n-\/\/                            4  nop\n-\n-class ICStub: public Stub {\n- private:\n-  int                 _size;       \/\/ total size of the stub incl. code\n-  address             _ic_site;    \/\/ points at call instruction of owning ic-buffer\n-  \/* stub code follows here *\/\n- protected:\n-  friend class ICStubInterface;\n-  \/\/ This will be called only by ICStubInterface\n-  void    initialize(int size)                   { _size = size; _ic_site = nullptr; }\n-  void    finalize(); \/\/ called when a method is removed\n-\n-  \/\/ General info\n-  int     size() const                           { return _size; }\n-\n-  \/\/ To be cautious, we want to make sure that each ICStub is in a separate instruction\n-  \/\/ cache line. This would allow for piggybacking on instruction cache coherency on\n-  \/\/ some architectures to order the updates to ICStub and setting the destination to\n-  \/\/ the ICStub. Note that cache line size might be larger than CodeEntryAlignment\n-  \/\/ that is normal alignment for CodeBlobs.\n-  static int alignment()                         { return DEFAULT_CACHE_LINE_SIZE; }\n-\n-  \/\/ Aligning the code section is normally done for performance reasons, which is not\n-  \/\/ required for ICStubs, as these stubs are transitional. Setting code alignment\n-  \/\/ to CodeEntryAlignment would waste a lot of memory in ICBuffer. Aligning to\n-  \/\/ word size should be enough. This also offsets the costs of aligning the entire\n-  \/\/ ICStub to cache line (see above), as smaller code alignment would allow ICStub\n-  \/\/ to fit a _single_ cache line.\n-  static int code_alignment()                    { return HeapWordSize; }\n-\n- public:\n-  \/\/ Creation\n-  void set_stub(CompiledIC *ic, void* cached_value, address dest_addr);\n-\n-  \/\/ Code info\n-  address code_begin() const                     { return align_up((address)this + sizeof(ICStub), code_alignment()); }\n-  address code_end() const                       { return (address)this + size(); }\n-\n-  \/\/ Call site info\n-  address ic_site() const                        { return _ic_site; }\n-  void    clear();\n-  bool    is_empty() const                       { return _ic_site == nullptr; }\n-\n-  \/\/ stub info\n-  address destination() const;  \/\/ destination of jump instruction\n-  void* cached_value() const;   \/\/ cached_value for stub\n-\n-  \/\/ Debugging\n-  void    verify()            PRODUCT_RETURN;\n-  void    print()             PRODUCT_RETURN;\n-\n-  \/\/ Creation\n-  static inline ICStub* from_destination_address(address destination_address) {\n-    ICStub* stub = (ICStub*) align_down(destination_address - sizeof(ICStub), alignment());\n-#ifdef ASSERT\n-    stub->verify();\n-#endif\n-    return stub;\n-  }\n-};\n-\n-#ifdef ASSERT\n-\/\/ The ICRefillVerifier class is a stack allocated RAII object used to\n-\/\/ detect if a failed IC transition that required IC stub refilling has\n-\/\/ been accidentally missed. It is up to the caller to in that case\n-\/\/ refill IC stubs.\n-class ICRefillVerifier: StackObj {\n-  bool _refill_requested;\n-  bool _refill_remembered;\n-\n- public:\n-  ICRefillVerifier();\n-  ~ICRefillVerifier();\n-\n-  void request_refill() { _refill_requested = true; }\n-  void request_remembered() { _refill_remembered = true; }\n-};\n-\n-\/\/ The ICRefillVerifierMark is used to set the thread's current\n-\/\/ ICRefillVerifier to a provided one. This is useful in particular\n-\/\/ when transitioning IC stubs in parallel and refilling from the\n-\/\/ master thread invoking the IC stub transitioning code.\n-class ICRefillVerifierMark: StackObj {\n- public:\n-  ICRefillVerifierMark(ICRefillVerifier* verifier);\n-  ~ICRefillVerifierMark();\n-};\n-#else\n-class ICRefillVerifier: StackObj {\n- public:\n-  ICRefillVerifier() {}\n-};\n-class ICRefillVerifierMark: StackObj {\n- public:\n-  ICRefillVerifierMark(ICRefillVerifier* verifier) {}\n-};\n-#endif\n-\n-class InlineCacheBuffer: public AllStatic {\n- private:\n-  \/\/ friends\n-  friend class ICStub;\n-\n-  static int ic_stub_code_size();\n-\n-  static StubQueue* _buffer;\n-\n-  static CompiledICHolder* volatile _pending_released;\n-  static volatile int _pending_count;\n-\n-  static StubQueue* buffer()                         { return _buffer;         }\n-\n-  \/\/ Machine-dependent implementation of ICBuffer\n-  static void    assemble_ic_buffer_code(address code_begin, void* cached_value, address entry_point);\n-  static address ic_buffer_entry_point  (address code_begin);\n-  static void*   ic_buffer_cached_value (address code_begin);\n-\n- public:\n-\n-    \/\/ Initialization; must be called before first usage\n-  static void initialize();\n-\n-  \/\/ Access\n-  static bool contains(address instruction_address);\n-\n-    \/\/ removes the ICStubs after backpatching\n-  static bool needs_update_inline_caches();\n-  static void update_inline_caches();\n-  static void refill_ic_stubs();\n-\n-  \/\/ for debugging\n-  static bool is_empty();\n-\n-  static void release_pending_icholders();\n-  static void queue_for_release(CompiledICHolder* icholder);\n-  static int pending_icholder_count();\n-\n-  \/\/ New interface\n-  static bool    create_transition_stub(CompiledIC *ic, void* cached_value, address entry);\n-  static address ic_destination_for(CompiledIC *ic);\n-  static void*   cached_value_for(CompiledIC *ic);\n-};\n-\n-#endif \/\/ SHARE_CODE_ICBUFFER_HPP\n","filename":"src\/hotspot\/share\/code\/icBuffer.hpp","additions":0,"deletions":193,"binary":false,"changes":193,"status":"deleted"},{"patch":"@@ -643,0 +643,1 @@\n+  _compiled_ic_data(nullptr),\n@@ -700,0 +701,2 @@\n+    finalize_relocations();\n+\n@@ -704,2 +707,0 @@\n-\n-    finalize_relocations();\n@@ -787,0 +788,1 @@\n+  _compiled_ic_data(nullptr),\n@@ -890,0 +892,2 @@\n+    finalize_relocations();\n+\n@@ -895,2 +899,0 @@\n-    finalize_relocations();\n-\n@@ -1148,0 +1150,2 @@\n+  GrowableArray<NativeMovConstReg*> virtual_call_data;\n+\n@@ -1152,1 +1156,5 @@\n-    if (iter.type() == relocInfo::post_call_nop_type) {\n+    if (iter.type() == relocInfo::virtual_call_type) {\n+      virtual_call_Relocation* r = iter.virtual_call_reloc();\n+      NativeMovConstReg* value = nativeMovConstReg_at(r->cached_value());\n+      virtual_call_data.append(value);\n+    } else if (iter.type() == relocInfo::post_call_nop_type) {\n@@ -1158,0 +1166,11 @@\n+\n+  if (virtual_call_data.length() > 0) {\n+    \/\/ We allocate a block of CompiledICData per nmethod so the GC can purge this faster.\n+    _compiled_ic_data = new CompiledICData[virtual_call_data.length()];\n+    CompiledICData* next_data = _compiled_ic_data;\n+\n+    for (NativeMovConstReg* value : virtual_call_data) {\n+      value->set_data((intptr_t)next_data);\n+      next_data++;\n+    }\n+  }\n@@ -1183,2 +1202,1 @@\n-      case relocInfo::virtual_call_type:\n-      case relocInfo::opt_virtual_call_type: {\n+      case relocInfo::virtual_call_type: {\n@@ -1194,2 +1212,3 @@\n-      case relocInfo::static_call_type: {\n-        CompiledStaticCall *csc = compiledStaticCall_at(iter.reloc());\n+      case relocInfo::static_call_type:\n+      case relocInfo::opt_virtual_call_type: {\n+        CompiledDirectCall *csc = CompiledDirectCall::at(iter.reloc());\n@@ -1222,2 +1241,1 @@\n-      case relocInfo::virtual_call_type:\n-      case relocInfo::opt_virtual_call_type: {\n+      case relocInfo::virtual_call_type: {\n@@ -1225,1 +1243,1 @@\n-        CodeBlob *cb = CodeCache::find_blob(ic->ic_destination());\n+        CodeBlob *cb = CodeCache::find_blob(ic->destination());\n@@ -1228,1 +1246,1 @@\n-        if( nm != nullptr ) {\n+        if (nm != nullptr) {\n@@ -1230,1 +1248,1 @@\n-          if (!nm->is_in_use() || (nm->method()->code() != nm)) {\n+          if (!nm->is_in_use() || nm->is_unloading()) {\n@@ -1236,3 +1254,4 @@\n-      case relocInfo::static_call_type: {\n-        CompiledStaticCall *csc = compiledStaticCall_at(iter.reloc());\n-        CodeBlob *cb = CodeCache::find_blob(csc->destination());\n+      case relocInfo::static_call_type:\n+      case relocInfo::opt_virtual_call_type: {\n+        CompiledDirectCall *cdc = CompiledDirectCall::at(iter.reloc());\n+        CodeBlob *cb = CodeCache::find_blob(cdc->destination());\n@@ -1241,1 +1260,1 @@\n-        if( nm != nullptr ) {\n+        if (nm != nullptr) {\n@@ -1243,2 +1262,2 @@\n-          if (!nm->is_in_use() || (nm->method()->code() != nm)) {\n-            assert(csc->is_clean(), \"IC should be clean\");\n+          if (!nm->is_in_use() || nm->is_unloading() || nm->method()->code() != nm) {\n+            assert(cdc->is_clean(), \"IC should be clean\");\n@@ -1408,3 +1427,1 @@\n-    \/\/ Already unlinked. It can be invoked twice because concurrent code cache\n-    \/\/ unloading might need to restart when inline cache cleaning fails due to\n-    \/\/ running out of ICStubs, which can only be refilled at safepoints\n+    \/\/ Already unlinked.\n@@ -1421,1 +1438,0 @@\n-  clear_ic_callsites();\n@@ -1466,0 +1482,2 @@\n+  delete[] _compiled_ic_data;\n+\n@@ -1469,1 +1487,0 @@\n-\n@@ -1607,10 +1624,1 @@\n-        if (ic->is_icholder_call()) {\n-          CompiledICHolder* cichk = ic->cached_icholder();\n-          f->do_metadata(cichk->holder_metadata());\n-          f->do_metadata(cichk->holder_klass());\n-        } else {\n-          Metadata* ic_oop = ic->cached_metadata();\n-          if (ic_oop != nullptr) {\n-            f->do_metadata(ic_oop);\n-          }\n-        }\n+        ic->metadata_do(f);\n@@ -1753,2 +1761,1 @@\n-    guarantee(unload_nmethod_caches(unloading_occurred),\n-              \"Should not need transition stubs\");\n+    unload_nmethod_caches(unloading_occurred);\n@@ -2287,1 +2294,1 @@\n-void nmethod::verify_interrupt_point(address call_site) {\n+void nmethod::verify_interrupt_point(address call_site, bool is_inline_cache) {\n@@ -2292,1 +2299,5 @@\n-      CompiledIC_at(this, call_site);\n+      if (is_inline_cache) {\n+        CompiledIC_at(this, call_site);\n+      } else {\n+        CompiledDirectCall::at(call_site);\n+      }\n@@ -2295,1 +2306,5 @@\n-      CompiledIC_at(this, call_site);\n+      if (is_inline_cache) {\n+        CompiledIC_at(this, call_site);\n+      } else {\n+        CompiledDirectCall::at(call_site);\n+      }\n@@ -2319,1 +2334,1 @@\n-        verify_interrupt_point(iter.addr());\n+        verify_interrupt_point(iter.addr(), true \/* is_inline_cache *\/);\n@@ -2323,1 +2338,1 @@\n-        verify_interrupt_point(iter.addr());\n+        verify_interrupt_point(iter.addr(), false \/* is_inline_cache *\/);\n@@ -2327,1 +2342,1 @@\n-        \/\/verify_interrupt_point(iter.addr());\n+        verify_interrupt_point(iter.addr(), false \/* is_inline_cache *\/);\n@@ -3242,69 +3257,0 @@\n-class DirectNativeCallWrapper: public NativeCallWrapper {\n-private:\n-  NativeCall* _call;\n-\n-public:\n-  DirectNativeCallWrapper(NativeCall* call) : _call(call) {}\n-\n-  virtual address destination() const { return _call->destination(); }\n-  virtual address instruction_address() const { return _call->instruction_address(); }\n-  virtual address next_instruction_address() const { return _call->next_instruction_address(); }\n-  virtual address return_address() const { return _call->return_address(); }\n-\n-  virtual address get_resolve_call_stub(bool is_optimized) const {\n-    if (is_optimized) {\n-      return SharedRuntime::get_resolve_opt_virtual_call_stub();\n-    }\n-    return SharedRuntime::get_resolve_virtual_call_stub();\n-  }\n-\n-  virtual void set_destination_mt_safe(address dest) {\n-    _call->set_destination_mt_safe(dest);\n-  }\n-\n-  virtual void set_to_interpreted(const methodHandle& method, CompiledICInfo& info) {\n-    CompiledDirectStaticCall* csc = CompiledDirectStaticCall::at(instruction_address());\n-    {\n-      csc->set_to_interpreted(method, info.entry());\n-    }\n-  }\n-\n-  virtual void verify() const {\n-    \/\/ make sure code pattern is actually a call imm32 instruction\n-    _call->verify();\n-    _call->verify_alignment();\n-  }\n-\n-  virtual void verify_resolve_call(address dest) const {\n-    CodeBlob* db = CodeCache::find_blob(dest);\n-    assert(db != nullptr && !db->is_adapter_blob(), \"must use stub!\");\n-  }\n-\n-  virtual bool is_call_to_interpreted(address dest) const {\n-    CodeBlob* cb = CodeCache::find_blob(_call->instruction_address());\n-    return cb->contains(dest);\n-  }\n-\n-  virtual bool is_safe_for_patching() const { return false; }\n-\n-  virtual NativeInstruction* get_load_instruction(virtual_call_Relocation* r) const {\n-    return nativeMovConstReg_at(r->cached_value());\n-  }\n-\n-  virtual void *get_data(NativeInstruction* instruction) const {\n-    return (void*)((NativeMovConstReg*) instruction)->data();\n-  }\n-\n-  virtual void set_data(NativeInstruction* instruction, intptr_t data) {\n-    ((NativeMovConstReg*) instruction)->set_data(data);\n-  }\n-};\n-\n-NativeCallWrapper* nmethod::call_wrapper_at(address call) const {\n-  return new DirectNativeCallWrapper((NativeCall*) call);\n-}\n-\n-NativeCallWrapper* nmethod::call_wrapper_before(address return_pc) const {\n-  return new DirectNativeCallWrapper(nativeCall_before(return_pc));\n-}\n-\n@@ -3319,12 +3265,0 @@\n-CompiledStaticCall* nmethod::compiledStaticCall_at(Relocation* call_site) const {\n-  return CompiledDirectStaticCall::at(call_site);\n-}\n-\n-CompiledStaticCall* nmethod::compiledStaticCall_at(address call_site) const {\n-  return CompiledDirectStaticCall::at(call_site);\n-}\n-\n-CompiledStaticCall* nmethod::compiledStaticCall_before(address return_addr) const {\n-  return CompiledDirectStaticCall::before(return_addr);\n-}\n-\n@@ -3344,2 +3278,1 @@\n-    case relocInfo::virtual_call_type:\n-    case relocInfo::opt_virtual_call_type: {\n+    case relocInfo::virtual_call_type: {\n@@ -3351,2 +3284,3 @@\n-      st->print_cr(\"Static call at \" INTPTR_FORMAT, p2i(iter.reloc()->addr()));\n-      CompiledDirectStaticCall::at(iter.reloc())->print();\n+    case relocInfo::opt_virtual_call_type:\n+      st->print_cr(\"Direct call at \" INTPTR_FORMAT, p2i(iter.reloc()->addr()));\n+      CompiledDirectCall::at(iter.reloc())->print();\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":62,"deletions":128,"binary":false,"changes":190,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+class CompiledICData;\n@@ -199,0 +200,1 @@\n+  CompiledICData* _compiled_ic_data;\n@@ -607,1 +609,1 @@\n-  void verify_interrupt_point(address interrupt_point);\n+  void verify_interrupt_point(address interrupt_point, bool is_inline_cache);\n@@ -704,2 +706,0 @@\n-  NativeCallWrapper* call_wrapper_at(address call) const;\n-  NativeCallWrapper* call_wrapper_before(address return_pc) const;\n@@ -708,4 +708,0 @@\n-  virtual CompiledStaticCall* compiledStaticCall_at(Relocation* call_site) const;\n-  virtual CompiledStaticCall* compiledStaticCall_at(address addr) const;\n-  virtual CompiledStaticCall* compiledStaticCall_before(address addr) const;\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -644,3 +644,1 @@\n-bool virtual_call_Relocation::clear_inline_cache() {\n-  \/\/ No stubs for ICs\n-  \/\/ Clean IC\n+void virtual_call_Relocation::clear_inline_cache() {\n@@ -649,1 +647,1 @@\n-  return icache->set_to_clean();\n+  icache->set_to_clean();\n@@ -672,9 +670,1 @@\n-template<typename CompiledICorStaticCall>\n-static bool set_to_clean_no_ic_refill(CompiledICorStaticCall* ic) {\n-  guarantee(ic->set_to_clean(), \"Should not need transition stubs\");\n-  return true;\n-}\n-\n-bool opt_virtual_call_Relocation::clear_inline_cache() {\n-  \/\/ No stubs for ICs\n-  \/\/ Clean IC\n+void opt_virtual_call_Relocation::clear_inline_cache() {\n@@ -682,2 +672,2 @@\n-  CompiledIC* icache = CompiledIC_at(this);\n-  return set_to_clean_no_ic_refill(icache);\n+  CompiledDirectCall* callsite = CompiledDirectCall::at(this);\n+  callsite->set_to_clean();\n@@ -720,4 +710,4 @@\n-bool static_call_Relocation::clear_inline_cache() {\n-  \/\/ Safe call site info\n-  CompiledStaticCall* handler = this->code()->compiledStaticCall_at(this);\n-  return set_to_clean_no_ic_refill(handler);\n+void static_call_Relocation::clear_inline_cache() {\n+  ResourceMark rm;\n+  CompiledDirectCall* callsite = CompiledDirectCall::at(this);\n+  callsite->set_to_clean();\n@@ -762,1 +752,1 @@\n-bool static_stub_Relocation::clear_inline_cache() {\n+void static_stub_Relocation::clear_inline_cache() {\n@@ -765,2 +755,1 @@\n-  CompiledDirectStaticCall::set_stub_to_clean(this);\n-  return true;\n+  CompiledDirectCall::set_stub_to_clean(this);\n","filename":"src\/hotspot\/share\/code\/relocInfo.cpp","additions":11,"deletions":22,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -865,1 +865,1 @@\n-  virtual bool clear_inline_cache()              { return true; }\n+  virtual void clear_inline_cache() {}\n@@ -1144,1 +1144,1 @@\n-  bool clear_inline_cache() override;\n+  void clear_inline_cache() override;\n@@ -1173,1 +1173,1 @@\n-  bool clear_inline_cache() override;\n+  void clear_inline_cache() override;\n@@ -1205,1 +1205,1 @@\n-  bool clear_inline_cache() override;\n+  void clear_inline_cache() override;\n@@ -1230,1 +1230,1 @@\n-  bool clear_inline_cache() override;\n+  void clear_inline_cache() override;\n","filename":"src\/hotspot\/share\/code\/relocInfo.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -286,7 +286,0 @@\n-bool VtableStubs::is_icholder_entry(address pc) {\n-  assert(contains(pc), \"must contain all vtable blobs\");\n-  VtableStub* stub = (VtableStub*)(pc - VtableStub::entry_offset());\n-  \/\/ itable stubs use CompiledICHolder.\n-  return stub->is_itable_stub();\n-}\n-\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -110,1 +110,0 @@\n-  static bool        is_icholder_entry(address pc);                  \/\/ is the blob containing pc (which must be a vtable blob) an icholder?\n","filename":"src\/hotspot\/share\/code\/vtableStubs.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -2205,2 +2204,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -1066,2 +1065,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-  CodeCache::verify_icholder_relocations();\n","filename":"src\/hotspot\/share\/gc\/shared\/parallelCleaning.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -131,1 +130,0 @@\n-  volatile bool             _failed;\n@@ -135,4 +133,0 @@\n-  void set_failed() {\n-    Atomic::store(&_failed, true);\n-  }\n-\n@@ -142,1 +136,0 @@\n-      _failed(false),\n@@ -148,3 +141,0 @@\n-    if (failed()) {\n-      return;\n-    }\n@@ -173,7 +163,1 @@\n-    if (!nm->unload_nmethod_caches(_unloading_occurred)) {\n-      set_failed();\n-    }\n-  }\n-\n-  bool failed() const {\n-    return Atomic::load(&_failed);\n+    nm->unload_nmethod_caches(_unloading_occurred);\n@@ -186,1 +170,0 @@\n-  ICRefillVerifier*                   _verifier;\n@@ -190,1 +173,1 @@\n-  ShenandoahUnlinkTask(bool unloading_occurred, ICRefillVerifier* verifier) :\n+  ShenandoahUnlinkTask(bool unloading_occurred) :\n@@ -193,1 +176,0 @@\n-    _verifier(verifier),\n@@ -205,1 +187,0 @@\n-    ICRefillVerifierMark mark(_verifier);\n@@ -208,4 +189,0 @@\n-\n-  bool success() const {\n-    return !_cl.failed();\n-  }\n@@ -217,17 +194,2 @@\n-  for (;;) {\n-    ICRefillVerifier verifier;\n-\n-    {\n-      ShenandoahUnlinkTask task(unloading_occurred, &verifier);\n-      workers->run_task(&task);\n-      if (task.success()) {\n-        return;\n-      }\n-    }\n-\n-    \/\/ Cleaning failed because we ran out of transitional IC stubs,\n-    \/\/ so we have to refill and try again. Refilling requires taking\n-    \/\/ a safepoint, so we temporarily leave the suspendible thread set.\n-    SuspendibleThreadSetLeaver sts;\n-    InlineCacheBuffer::refill_ic_stubs();\n-  }\n+  ShenandoahUnlinkTask task(unloading_occurred);\n+  workers->run_task(&task);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCodeRoots.cpp","additions":4,"deletions":42,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -109,1 +109,1 @@\n-    if (SafepointSynchronize::is_at_safepoint()) {\n+    if (SafepointSynchronize::is_at_safepoint() || method->is_unloading()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -308,3 +307,1 @@\n-    if (!nm->unload_nmethod_caches(_unloading_occurred)) {\n-      set_failed();\n-    }\n+    nm->unload_nmethod_caches(_unloading_occurred);\n@@ -321,1 +318,0 @@\n-  ICRefillVerifier*     _verifier;\n@@ -324,1 +320,1 @@\n-  XNMethodUnlinkTask(bool unloading_occurred, ICRefillVerifier* verifier) :\n+  XNMethodUnlinkTask(bool unloading_occurred) :\n@@ -326,2 +322,1 @@\n-      _cl(unloading_occurred),\n-      _verifier(verifier) {\n+      _cl(unloading_occurred) {\n@@ -336,1 +331,0 @@\n-    ICRefillVerifierMark mark(_verifier);\n@@ -339,4 +333,0 @@\n-\n-  bool success() const {\n-    return !_cl.failed();\n-  }\n@@ -346,17 +336,2 @@\n-  for (;;) {\n-    ICRefillVerifier verifier;\n-\n-    {\n-      XNMethodUnlinkTask task(unloading_occurred, &verifier);\n-      workers->run(&task);\n-      if (task.success()) {\n-        return;\n-      }\n-    }\n-\n-    \/\/ Cleaning failed because we ran out of transitional IC stubs,\n-    \/\/ so we have to refill and try again. Refilling requires taking\n-    \/\/ a safepoint, so we temporarily leave the suspendible thread set.\n-    SuspendibleThreadSetLeaver sts;\n-    InlineCacheBuffer::refill_ic_stubs();\n-  }\n+  XNMethodUnlinkTask task(unloading_occurred);\n+  workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethod.cpp","additions":5,"deletions":30,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTable.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-    if (SafepointSynchronize::is_at_safepoint()) {\n+    if (SafepointSynchronize::is_at_safepoint() || method->is_unloading()) {\n","filename":"src\/hotspot\/share\/gc\/x\/xUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -337,6 +336,1 @@\n-  bool          _unloading_occurred;\n-  volatile bool _failed;\n-\n-  void set_failed() {\n-    Atomic::store(&_failed, true);\n-  }\n+  bool _unloading_occurred;\n@@ -346,2 +340,1 @@\n-    : _unloading_occurred(unloading_occurred),\n-      _failed(false) {}\n+    : _unloading_occurred(unloading_occurred) {}\n@@ -350,4 +343,0 @@\n-    if (failed()) {\n-      return;\n-    }\n-\n@@ -389,7 +378,1 @@\n-    if (!nm->unload_nmethod_caches(_unloading_occurred)) {\n-      set_failed();\n-    }\n-  }\n-\n-  bool failed() const {\n-    return Atomic::load(&_failed);\n+    nm->unload_nmethod_caches(_unloading_occurred);\n@@ -402,1 +385,0 @@\n-  ICRefillVerifier*     _verifier;\n@@ -405,1 +387,1 @@\n-  ZNMethodUnlinkTask(bool unloading_occurred, ICRefillVerifier* verifier)\n+  ZNMethodUnlinkTask(bool unloading_occurred)\n@@ -407,2 +389,1 @@\n-      _cl(unloading_occurred),\n-      _verifier(verifier) {\n+      _cl(unloading_occurred) {\n@@ -417,1 +398,0 @@\n-    ICRefillVerifierMark mark(_verifier);\n@@ -420,4 +400,0 @@\n-\n-  bool success() const {\n-    return !_cl.failed();\n-  }\n@@ -427,17 +403,2 @@\n-  for (;;) {\n-    ICRefillVerifier verifier;\n-\n-    {\n-      ZNMethodUnlinkTask task(unloading_occurred, &verifier);\n-      workers->run(&task);\n-      if (task.success()) {\n-        return;\n-      }\n-    }\n-\n-    \/\/ Cleaning failed because we ran out of transitional IC stubs,\n-    \/\/ so we have to refill and try again. Refilling requires taking\n-    \/\/ a safepoint, so we temporarily leave the suspendible thread set.\n-    SuspendibleThreadSetLeaver sts_leaver;\n-    InlineCacheBuffer::refill_ic_stubs();\n-  }\n+  ZNMethodUnlinkTask task(unloading_occurred);\n+  workers->run(&task);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":7,"deletions":46,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodTable.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-    if (SafepointSynchronize::is_at_safepoint()) {\n+    if (SafepointSynchronize::is_at_safepoint() || method->is_unloading()) {\n","filename":"src\/hotspot\/share\/gc\/z\/zUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -899,2 +899,2 @@\n-  int size = static_call_stubs * CompiledStaticCall::to_interp_stub_size();\n-  size += trampoline_stubs * CompiledStaticCall::to_trampoline_stub_size();\n+  int size = static_call_stubs * CompiledDirectCall::to_interp_stub_size();\n+  size += trampoline_stubs * CompiledDirectCall::to_trampoline_stub_size();\n@@ -1246,1 +1246,1 @@\n-      if (CompiledStaticCall::emit_to_interp_stub(buffer, _instructions->start() + pc_offset) == nullptr) {\n+      if (CompiledDirectCall::emit_to_interp_stub(buffer, _instructions->start() + pc_offset) == nullptr) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -147,0 +148,5 @@\n+  volatile_nonstatic_field(CompiledICData,     _speculated_method,                     Method*)                                      \\\n+  volatile_nonstatic_field(CompiledICData,     _speculated_klass,                      uintptr_t)                                    \\\n+  nonstatic_field(CompiledICData,              _itable_defc_klass,                     Klass*)                                       \\\n+  nonstatic_field(CompiledICData,              _itable_refc_klass,                     Klass*)                                       \\\n+                                                                                                                                     \\\n@@ -433,0 +439,1 @@\n+  declare_toplevel_type(CompiledICData)                                   \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1,75 +0,0 @@\n-\/*\n- * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"oops\/compiledICHolder.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-\n-#ifdef ASSERT\n-volatile int CompiledICHolder::_live_count;\n-volatile int CompiledICHolder::_live_not_claimed_count;\n-#endif\n-\n-CompiledICHolder::CompiledICHolder(Metadata* metadata, Klass* klass, bool is_method)\n-  : _holder_metadata(metadata), _holder_klass(klass), _next(nullptr), _is_metadata_method(is_method) {\n-#ifdef ASSERT\n-  Atomic::inc(&_live_count);\n-  Atomic::inc(&_live_not_claimed_count);\n-#endif \/\/ ASSERT\n-}\n-\n-#ifdef ASSERT\n-CompiledICHolder::~CompiledICHolder() {\n-  assert(_live_count > 0, \"underflow\");\n-  Atomic::dec(&_live_count);\n-}\n-#endif \/\/ ASSERT\n-\n-\/\/ Printing\n-\n-void CompiledICHolder::print_on(outputStream* st) const {\n-  st->print(\"%s\", internal_name());\n-  st->print(\" - metadata: \"); holder_metadata()->print_value_on(st); st->cr();\n-  st->print(\" - klass:    \"); holder_klass()->print_value_on(st); st->cr();\n-}\n-\n-void CompiledICHolder::print_value_on(outputStream* st) const {\n-  st->print(\"%s\", internal_name());\n-}\n-\n-\n-\/\/ Verification\n-\n-void CompiledICHolder::verify_on(outputStream* st) {\n-  guarantee(holder_metadata()->is_method() || holder_metadata()->is_klass(), \"should be method or klass\");\n-  guarantee(holder_klass()->is_klass(),   \"should be klass\");\n-}\n-\n-#ifdef ASSERT\n-\n-void CompiledICHolder::claim() {\n-  Atomic::dec(&_live_not_claimed_count);\n-}\n-\n-#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/oops\/compiledICHolder.cpp","additions":0,"deletions":75,"binary":false,"changes":75,"status":"deleted"},{"patch":"@@ -1,92 +0,0 @@\n-\/*\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_OOPS_COMPILEDICHOLDER_HPP\n-#define SHARE_OOPS_COMPILEDICHOLDER_HPP\n-\n-#include \"oops\/oop.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#include \"oops\/klass.hpp\"\n-#include \"oops\/method.hpp\"\n-\n-\/\/ A CompiledICHolder* is a helper object for the inline cache implementation.\n-\/\/ It holds:\n-\/\/   (1) (method+klass pair) when converting from compiled to an interpreted call\n-\/\/   (2) (klass+klass pair) when calling itable stub from megamorphic compiled call\n-\/\/\n-\/\/ These are always allocated in the C heap and are freed during a\n-\/\/ safepoint by the ICBuffer logic.  It's unsafe to free them earlier\n-\/\/ since they might be in use.\n-\/\/\n-\n-\n-class CompiledICHolder : public CHeapObj<mtCompiler> {\n-  friend class VMStructs;\n- private:\n-#ifdef ASSERT\n-  static volatile int _live_count; \/\/ allocated\n-  static volatile int _live_not_claimed_count; \/\/ allocated but not yet in use so not\n-                                               \/\/ reachable by iterating over nmethods\n-#endif\n-\n-  Metadata* _holder_metadata;\n-  Klass*    _holder_klass;    \/\/ to avoid name conflict with oopDesc::_klass\n-  CompiledICHolder* _next;\n-  bool _is_metadata_method;\n-\n- public:\n-  \/\/ Constructor\n-  CompiledICHolder(Metadata* metadata, Klass* klass, bool is_method = true);\n-  ~CompiledICHolder() NOT_DEBUG_RETURN;\n-\n-#ifdef ASSERT\n-  static int live_count() { return _live_count; }\n-  static int live_not_claimed_count() { return _live_not_claimed_count; }\n-#endif\n-\n-  \/\/ accessors\n-  Klass*    holder_klass()  const     { return _holder_klass; }\n-  Metadata* holder_metadata() const   { return _holder_metadata; }\n-\n-  static ByteSize holder_metadata_offset() { return byte_offset_of(CompiledICHolder, _holder_metadata); }\n-  static ByteSize holder_klass_offset()    { return byte_offset_of(CompiledICHolder, _holder_klass); }\n-\n-  CompiledICHolder* next()     { return _next; }\n-  void set_next(CompiledICHolder* n) { _next = n; }\n-\n-  inline bool is_loader_alive();\n-\n-  \/\/ Verify\n-  void verify_on(outputStream* st);\n-\n-  \/\/ Printing\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n-\n-  const char* internal_name() const { return \"{compiledICHolder}\"; }\n-\n-  void claim() NOT_DEBUG_RETURN;\n-};\n-\n-#endif \/\/ SHARE_OOPS_COMPILEDICHOLDER_HPP\n","filename":"src\/hotspot\/share\/oops\/compiledICHolder.hpp","additions":0,"deletions":92,"binary":false,"changes":92,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_OOPS_COMPILEDICHOLDER_INLINE_HPP\n-#define SHARE_OOPS_COMPILEDICHOLDER_INLINE_HPP\n-\n-#include \"oops\/compiledICHolder.hpp\"\n-\n-#include \"oops\/klass.inline.hpp\"\n-\n-inline bool CompiledICHolder::is_loader_alive() {\n-  Klass* k = _is_metadata_method ? ((Method*)_holder_metadata)->method_holder() : (Klass*)_holder_metadata;\n-  if (!k->is_loader_alive()) {\n-    return false;\n-  }\n-  if (!_holder_klass->is_loader_alive()) {\n-    return false;\n-  }\n-  return true;\n-}\n-\n-#endif \/\/ SHARE_OOPS_COMPILEDICHOLDER_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/compiledICHolder.inline.hpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -182,3 +182,0 @@\n-\/\/      class CHeapObj\n-class   CompiledICHolder;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -510,2 +510,2 @@\n-            stub_size  += CompiledStaticCall::to_interp_stub_size();\n-            reloc_size += CompiledStaticCall::reloc_to_interp_stub();\n+            stub_size  += CompiledDirectCall::to_interp_stub_size();\n+            reloc_size += CompiledDirectCall::reloc_to_interp_stub();\n@@ -3415,0 +3415,6 @@\n+      if (!target->is_static()) {\n+        \/\/ The UEP of an nmethod ensures that the VEP is padded. However, the padding of the UEP is placed\n+        \/\/ before the inline cache check, so we don't have to execute any nop instructions when dispatching\n+        \/\/ through the UEP, yet we can ensure that the VEP is aligned appropriately.\n+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size - MacroAssembler::ic_check_size());\n+      }\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -883,3 +883,0 @@\n-  develop(bool, TraceInlineCacheClearing, false,                            \\\n-          \"Trace clearing of inline caches in nmethods\")                    \\\n-                                                                            \\\n@@ -904,6 +901,0 @@\n-  develop(bool, TraceICBuffer, false,                                       \\\n-          \"Trace usage of IC buffer\")                                       \\\n-                                                                            \\\n-  develop(bool, TraceCompiledIC, false,                                     \\\n-          \"Trace changes of compiled IC\")                                   \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -86,1 +85,0 @@\n-void InlineCacheBuffer_init();\n@@ -160,1 +158,0 @@\n-  InlineCacheBuffer_init();\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -516,1 +515,0 @@\n-  if (!InlineCacheBuffer::is_empty()) return true;\n@@ -561,4 +559,0 @@\n-    if (InlineCacheBuffer::needs_update_inline_caches()) {\n-      workers++;\n-    }\n-\n@@ -602,5 +596,0 @@\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES)) {\n-      Tracer t(\"updating inline caches\");\n-      InlineCacheBuffer::update_inline_caches();\n-    }\n-\n@@ -636,2 +625,0 @@\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -74,1 +74,0 @@\n-    SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES,\n","filename":"src\/hotspot\/share\/runtime\/safepoint.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"code\/icBuffer.hpp\"\n@@ -50,1 +49,0 @@\n-#include \"oops\/compiledICHolder.inline.hpp\"\n@@ -1287,118 +1285,0 @@\n-  methodHandle callee_method;\n-  callee_method = resolve_sub_helper(is_virtual, is_optimized, THREAD);\n-  if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n-    int retry_count = 0;\n-    while (!HAS_PENDING_EXCEPTION && callee_method->is_old() &&\n-           callee_method->method_holder() != vmClasses::Object_klass()) {\n-      \/\/ If has a pending exception then there is no need to re-try to\n-      \/\/ resolve this method.\n-      \/\/ If the method has been redefined, we need to try again.\n-      \/\/ Hack: we have no way to update the vtables of arrays, so don't\n-      \/\/ require that java.lang.Object has been updated.\n-\n-      \/\/ It is very unlikely that method is redefined more than 100 times\n-      \/\/ in the middle of resolve. If it is looping here more than 100 times\n-      \/\/ means then there could be a bug here.\n-      guarantee((retry_count++ < 100),\n-                \"Could not resolve to latest version of redefined method\");\n-      \/\/ method is redefined in the middle of resolve so re-try.\n-      callee_method = resolve_sub_helper(is_virtual, is_optimized, THREAD);\n-    }\n-  }\n-  return callee_method;\n-}\n-\n-\/\/ This fails if resolution required refilling of IC stubs\n-bool SharedRuntime::resolve_sub_helper_internal(methodHandle callee_method, const frame& caller_frame,\n-                                                CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n-                                                Handle receiver, CallInfo& call_info, Bytecodes::Code invoke_code, TRAPS) {\n-  StaticCallInfo static_call_info;\n-  CompiledICInfo virtual_call_info;\n-\n-  \/\/ Make sure the callee nmethod does not get deoptimized and removed before\n-  \/\/ we are done patching the code.\n-  CompiledMethod* callee = callee_method->code();\n-\n-  if (callee != nullptr) {\n-    assert(callee->is_compiled(), \"must be nmethod for patching\");\n-  }\n-\n-  if (callee != nullptr && !callee->is_in_use()) {\n-    \/\/ Patch call site to C2I adapter if callee nmethod is deoptimized or unloaded.\n-    callee = nullptr;\n-  }\n-#ifdef ASSERT\n-  address dest_entry_point = callee == nullptr ? 0 : callee->entry_point(); \/\/ used below\n-#endif\n-\n-  bool is_nmethod = caller_nm->is_nmethod();\n-\n-  if (is_virtual) {\n-    assert(receiver.not_null() || invoke_code == Bytecodes::_invokehandle, \"sanity check\");\n-    bool static_bound = call_info.resolved_method()->can_be_statically_bound();\n-    Klass* klass = invoke_code == Bytecodes::_invokehandle ? nullptr : receiver->klass();\n-    CompiledIC::compute_monomorphic_entry(callee_method, klass,\n-                     is_optimized, static_bound, is_nmethod, virtual_call_info,\n-                     CHECK_false);\n-  } else {\n-    \/\/ static call\n-    CompiledStaticCall::compute_entry(callee_method, is_nmethod, static_call_info);\n-  }\n-\n-  JFR_ONLY(bool patched_caller = false;)\n-  \/\/ grab lock, check for deoptimization and potentially patch caller\n-  {\n-    CompiledICLocker ml(caller_nm);\n-\n-    \/\/ Lock blocks for safepoint during which both nmethods can change state.\n-\n-    \/\/ Now that we are ready to patch if the Method* was redefined then\n-    \/\/ don't update call site and let the caller retry.\n-    \/\/ Don't update call site if callee nmethod was unloaded or deoptimized.\n-    \/\/ Don't update call site if callee nmethod was replaced by an other nmethod\n-    \/\/ which may happen when multiply alive nmethod (tiered compilation)\n-    \/\/ will be supported.\n-    if (!callee_method->is_old() &&\n-        (callee == nullptr || (callee->is_in_use() && callee_method->code() == callee))) {\n-      NoSafepointVerifier nsv;\n-#ifdef ASSERT\n-      \/\/ We must not try to patch to jump to an already unloaded method.\n-      if (dest_entry_point != 0) {\n-        CodeBlob* cb = CodeCache::find_blob(dest_entry_point);\n-        assert((cb != nullptr) && cb->is_compiled() && (((CompiledMethod*)cb) == callee),\n-               \"should not call unloaded nmethod\");\n-      }\n-#endif\n-      if (is_virtual) {\n-        CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n-        if (inline_cache->is_clean()) {\n-          if (!inline_cache->set_to_monomorphic(virtual_call_info)) {\n-            return false;\n-          }\n-          JFR_ONLY(patched_caller = true;)\n-        }\n-      } else {\n-        if (VM_Version::supports_fast_class_init_checks() &&\n-            invoke_code == Bytecodes::_invokestatic &&\n-            callee_method->needs_clinit_barrier() &&\n-            callee != nullptr && callee->is_compiled_by_jvmci()) {\n-          return true; \/\/ skip patching for JVMCI\n-        }\n-        CompiledStaticCall* ssc = caller_nm->compiledStaticCall_before(caller_frame.pc());\n-        if (is_nmethod && caller_nm->method()->is_continuation_enter_intrinsic()) {\n-          ssc->compute_entry_for_continuation_entry(callee_method, static_call_info);\n-        }\n-        if (ssc->is_clean()) {\n-          ssc->set(static_call_info);\n-          JFR_ONLY(patched_caller = true;)\n-        }\n-      }\n-    }\n-  } \/\/ unlock CompiledICLocker\n-  JFR_ONLY(if (patched_caller) Jfr::on_backpatching(callee_method(), THREAD);)\n-  return true;\n-}\n-\n-\/\/ Resolves a call.  The compilers generate code for calls that go here\n-\/\/ and are patched with the real destination of the call.\n-methodHandle SharedRuntime::resolve_sub_helper(bool is_virtual, bool is_optimized, TRAPS) {\n@@ -1415,1 +1295,1 @@\n-  CompiledMethod* caller_nm = caller_cb->as_compiled_method_or_null();\n+  CompiledMethod* caller_nm = caller_cb->as_compiled_method();\n@@ -1423,0 +1303,3 @@\n+\n+  NoSafepointVerifier nsv;\n+\n@@ -1467,0 +1350,1 @@\n+\n@@ -1471,24 +1355,16 @@\n-  \/\/ TODO detune for now but we might need it again\n-\/\/  assert(!callee_method->is_compiled_lambda_form() ||\n-\/\/         caller_nm->is_method_handle_return(caller_frame.pc()), \"must be MH call site\");\n-\n-  \/\/ Compute entry points. This might require generation of C2I converter\n-  \/\/ frames, so we cannot be holding any locks here. Furthermore, the\n-  \/\/ computation of the entry points is independent of patching the call.  We\n-  \/\/ always return the entry-point, but we only patch the stub if the call has\n-  \/\/ not been deoptimized.  Return values: For a virtual call this is an\n-  \/\/ (cached_oop, destination address) pair. For a static call\/optimized\n-  \/\/ virtual this is just a destination address.\n-\n-  \/\/ Patching IC caches may fail if we run out if transition stubs.\n-  \/\/ We refill the ic stubs then and try again.\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    bool successful = resolve_sub_helper_internal(callee_method, caller_frame, caller_nm,\n-                                                  is_virtual, is_optimized, receiver,\n-                                                  call_info, invoke_code, CHECK_(methodHandle()));\n-    if (successful) {\n-      return callee_method;\n-    } else {\n-      InlineCacheBuffer::refill_ic_stubs();\n-    }\n+\n+  \/\/ Compute entry points. The computation of the entry points is independent of\n+  \/\/ patching the call.\n+\n+  \/\/ Make sure the callee nmethod does not get deoptimized and removed before\n+  \/\/ we are done patching the code.\n+\n+\n+  CompiledICLocker ml(caller_nm);\n+  if (is_virtual && !is_optimized) {\n+    CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n+    inline_cache->update(&call_info, receiver->klass());\n+  } else {\n+    \/\/ Callsite is a direct call - set it to the destination method\n+    CompiledDirectCall* callsite = CompiledDirectCall::before(caller_frame.pc());\n+    callsite->set(callee_method);\n@@ -1497,0 +1373,1 @@\n+  return callee_method;\n@@ -1499,1 +1376,0 @@\n-\n@@ -1677,80 +1553,0 @@\n-\/\/ The handle_ic_miss_helper_internal function returns false if it failed due\n-\/\/ to either running out of vtable stubs or ic stubs due to IC transitions\n-\/\/ to transitional states. The needs_ic_stub_refill value will be set if\n-\/\/ the failure was due to running out of IC stubs, in which case handle_ic_miss_helper\n-\/\/ refills the IC stubs and tries again.\n-bool SharedRuntime::handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm,\n-                                                   const frame& caller_frame, methodHandle callee_method,\n-                                                   Bytecodes::Code bc, CallInfo& call_info,\n-                                                   bool& needs_ic_stub_refill, TRAPS) {\n-  CompiledICLocker ml(caller_nm);\n-  CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n-  bool should_be_mono = false;\n-  if (inline_cache->is_optimized()) {\n-    if (TraceCallFixup) {\n-      ResourceMark rm(THREAD);\n-      tty->print(\"OPTIMIZED IC miss (%s) call to\", Bytecodes::name(bc));\n-      callee_method->print_short_name(tty);\n-      tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-    }\n-    should_be_mono = true;\n-  } else if (inline_cache->is_icholder_call()) {\n-    CompiledICHolder* ic_oop = inline_cache->cached_icholder();\n-    if (ic_oop != nullptr) {\n-      if (!ic_oop->is_loader_alive()) {\n-        \/\/ Deferred IC cleaning due to concurrent class unloading\n-        if (!inline_cache->set_to_clean()) {\n-          needs_ic_stub_refill = true;\n-          return false;\n-        }\n-      } else if (receiver()->klass() == ic_oop->holder_klass()) {\n-        \/\/ This isn't a real miss. We must have seen that compiled code\n-        \/\/ is now available and we want the call site converted to a\n-        \/\/ monomorphic compiled call site.\n-        \/\/ We can't assert for callee_method->code() != nullptr because it\n-        \/\/ could have been deoptimized in the meantime\n-        if (TraceCallFixup) {\n-          ResourceMark rm(THREAD);\n-          tty->print(\"FALSE IC miss (%s) converting to compiled call to\", Bytecodes::name(bc));\n-          callee_method->print_short_name(tty);\n-          tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-        }\n-        should_be_mono = true;\n-      }\n-    }\n-  }\n-\n-  if (should_be_mono) {\n-    \/\/ We have a path that was monomorphic but was going interpreted\n-    \/\/ and now we have (or had) a compiled entry. We correct the IC\n-    \/\/ by using a new icBuffer.\n-    CompiledICInfo info;\n-    Klass* receiver_klass = receiver()->klass();\n-    inline_cache->compute_monomorphic_entry(callee_method,\n-                                            receiver_klass,\n-                                            inline_cache->is_optimized(),\n-                                            false, caller_nm->is_nmethod(),\n-                                            info, CHECK_false);\n-    if (!inline_cache->set_to_monomorphic(info)) {\n-      needs_ic_stub_refill = true;\n-      return false;\n-    }\n-  } else if (!inline_cache->is_megamorphic() && !inline_cache->is_clean()) {\n-    \/\/ Potential change to megamorphic\n-\n-    bool successful = inline_cache->set_to_megamorphic(&call_info, bc, needs_ic_stub_refill, CHECK_false);\n-    if (needs_ic_stub_refill) {\n-      return false;\n-    }\n-    if (!successful) {\n-      if (!inline_cache->set_to_clean()) {\n-        needs_ic_stub_refill = true;\n-        return false;\n-      }\n-    }\n-  } else {\n-    \/\/ Either clean or megamorphic\n-  }\n-  return true;\n-}\n-\n@@ -1766,26 +1562,0 @@\n-  \/\/ Compiler1 can produce virtual call sites that can actually be statically bound\n-  \/\/ If we fell thru to below we would think that the site was going megamorphic\n-  \/\/ when in fact the site can never miss. Worse because we'd think it was megamorphic\n-  \/\/ we'd try and do a vtable dispatch however methods that can be statically bound\n-  \/\/ don't have vtable entries (vtable_index < 0) and we'd blow up. So we force a\n-  \/\/ reresolution of the  call site (as if we did a handle_wrong_method and not an\n-  \/\/ plain ic_miss) and the site will be converted to an optimized virtual call site\n-  \/\/ never to miss again. I don't believe C2 will produce code like this but if it\n-  \/\/ did this would still be the correct thing to do for it too, hence no ifdef.\n-  \/\/\n-  if (call_info.resolved_method()->can_be_statically_bound()) {\n-    methodHandle callee_method = SharedRuntime::reresolve_call_site(CHECK_(methodHandle()));\n-    if (TraceCallFixup) {\n-      RegisterMap reg_map(current,\n-                          RegisterMap::UpdateMap::skip,\n-                          RegisterMap::ProcessFrames::include,\n-                          RegisterMap::WalkContinuation::skip);\n-      frame caller_frame = current->last_frame().sender(&reg_map);\n-      ResourceMark rm(current);\n-      tty->print(\"converting IC miss to reresolve (%s) call to\", Bytecodes::name(bc));\n-      callee_method->print_short_name(tty);\n-      tty->print_cr(\" from pc: \" INTPTR_FORMAT, p2i(caller_frame.pc()));\n-      tty->print_cr(\" code: \" INTPTR_FORMAT, p2i(callee_method->code()));\n-    }\n-    return callee_method;\n-  }\n@@ -1826,3 +1596,0 @@\n-  \/\/ Transitioning IC caches may require transition stubs. If we run out\n-  \/\/ of transition stubs, we have to drop locks and perform a safepoint\n-  \/\/ that refills them.\n@@ -1837,14 +1604,0 @@\n-  for (;;) {\n-    ICRefillVerifier ic_refill_verifier;\n-    bool needs_ic_stub_refill = false;\n-    bool successful = handle_ic_miss_helper_internal(receiver, caller_nm, caller_frame, callee_method,\n-                                                     bc, call_info, needs_ic_stub_refill, CHECK_(methodHandle()));\n-    if (successful || !needs_ic_stub_refill) {\n-      return callee_method;\n-    } else {\n-      InlineCacheBuffer::refill_ic_stubs();\n-    }\n-  }\n-}\n-\n-static bool clear_ic_at_addr(CompiledMethod* caller_nm, address call_addr, bool is_static_call) {\n@@ -1852,13 +1605,4 @@\n-  if (is_static_call) {\n-    CompiledStaticCall* ssc = caller_nm->compiledStaticCall_at(call_addr);\n-    if (!ssc->is_clean()) {\n-      return ssc->set_to_clean();\n-    }\n-  } else {\n-    \/\/ compiled, dispatched call (which used to call an interpreted method)\n-    CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n-    if (!inline_cache->is_clean()) {\n-      return inline_cache->set_to_clean();\n-    }\n-  }\n-  return true;\n+  CompiledIC* inline_cache = CompiledIC_before(caller_nm, caller_frame.pc());\n+  inline_cache->update(&call_info, receiver()->klass());\n+\n+  return callee_method;\n@@ -1893,2 +1637,0 @@\n-    \/\/ Check for static or virtual call\n-    bool is_static_call = false;\n@@ -1915,8 +1657,2 @@\n-    address call_addr = nullptr;\n-    {\n-      \/\/ Get call instruction under lock because another thread may be\n-      \/\/ busy patching it.\n-      CompiledICLocker ml(caller_nm);\n-      \/\/ Location of call instruction\n-      call_addr = caller_nm->call_instruction_address(pc);\n-    }\n+    CompiledICLocker ml(caller_nm);\n+    address call_addr = caller_nm->call_instruction_address(pc);\n@@ -1924,2 +1660,0 @@\n-    \/\/ Check relocations for the matching call to 1) avoid false positives,\n-    \/\/ and 2) determine the type.\n@@ -1932,1 +1666,0 @@\n-        bool is_static_call = false;\n@@ -1935,19 +1668,10 @@\n-            is_static_call = true;\n-\n-          case relocInfo::virtual_call_type:\n-          case relocInfo::opt_virtual_call_type:\n-            \/\/ Cleaning the inline cache will force a new resolve. This is more robust\n-            \/\/ than directly setting it to the new destination, since resolving of calls\n-            \/\/ is always done through the same code path. (experience shows that it\n-            \/\/ leads to very hard to track down bugs, if an inline cache gets updated\n-            \/\/ to a wrong method). It should not be performance critical, since the\n-            \/\/ resolve is only done once.\n-            guarantee(iter.addr() == call_addr, \"must find call\");\n-            for (;;) {\n-              ICRefillVerifier ic_refill_verifier;\n-              if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {\n-                InlineCacheBuffer::refill_ic_stubs();\n-              } else {\n-                break;\n-              }\n-            }\n+          case relocInfo::opt_virtual_call_type: {\n+            CompiledDirectCall* cdc = CompiledDirectCall::at(call_addr);\n+            cdc->set_to_clean();\n+            break;\n+          }\n+\n+          case relocInfo::virtual_call_type: {\n+            \/\/ compiled, dispatched call (which used to call an interpreted method)\n+            CompiledIC* inline_cache = CompiledIC_at(caller_nm, call_addr);\n+            inline_cache->set_to_clean();\n@@ -1955,0 +1679,1 @@\n+          }\n@@ -2018,31 +1743,0 @@\n-bool SharedRuntime::should_fixup_call_destination(address destination, address entry_point, address caller_pc, Method* moop, CodeBlob* cb) {\n-  if (destination != entry_point) {\n-    CodeBlob* callee = CodeCache::find_blob(destination);\n-    \/\/ callee == cb seems weird. It means calling interpreter thru stub.\n-    if (callee != nullptr && (callee == cb || callee->is_adapter_blob())) {\n-      \/\/ static call or optimized virtual\n-      if (TraceCallFixup) {\n-        tty->print(\"fixup callsite           at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-        moop->print_short_name(tty);\n-        tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-      }\n-      return true;\n-    } else {\n-      if (TraceCallFixup) {\n-        tty->print(\"failed to fixup callsite at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-        moop->print_short_name(tty);\n-        tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-      }\n-      \/\/ assert is too strong could also be resolve destinations.\n-      \/\/ assert(InlineCacheBuffer::contains(destination) || VtableStubs::contains(destination), \"must be\");\n-    }\n-  } else {\n-    if (TraceCallFixup) {\n-      tty->print(\"already patched callsite at \" INTPTR_FORMAT \" to compiled code for\", p2i(caller_pc));\n-      moop->print_short_name(tty);\n-      tty->print_cr(\" to \" INTPTR_FORMAT, p2i(entry_point));\n-    }\n-  }\n-  return false;\n-}\n-\n@@ -2056,2 +1750,0 @@\n-  Method* moop(method);\n-\n@@ -2073,1 +1765,1 @@\n-  CompiledMethod* callee = moop->code();\n+  CompiledMethod* callee = method->code();\n@@ -2082,1 +1774,1 @@\n-  if (cb == nullptr || !cb->is_compiled() || callee->is_unloading()) {\n+  if (cb == nullptr || !cb->is_compiled() || !callee->is_in_use() || callee->is_unloading()) {\n@@ -2087,2 +1779,1 @@\n-  CompiledMethod* nm = cb->as_compiled_method_or_null();\n-  assert(nm, \"must be\");\n+  CompiledMethod* caller = cb->as_compiled_method();\n@@ -2093,47 +1784,15 @@\n-  \/\/ There is a benign race here. We could be attempting to patch to a compiled\n-  \/\/ entry point at the same time the callee is being deoptimized. If that is\n-  \/\/ the case then entry_point may in fact point to a c2i and we'd patch the\n-  \/\/ call site with the same old data. clear_code will set code() to null\n-  \/\/ at the end of it. If we happen to see that null then we can skip trying\n-  \/\/ to patch. If we hit the window where the callee has a c2i in the\n-  \/\/ from_compiled_entry and the null isn't present yet then we lose the race\n-  \/\/ and patch the code with the same old data. Asi es la vida.\n-\n-  if (moop->code() == nullptr) return;\n-\n-  if (nm->is_in_use()) {\n-    \/\/ Expect to find a native call there (unless it was no-inline cache vtable dispatch)\n-    CompiledICLocker ic_locker(nm);\n-    if (NativeCall::is_call_before(return_pc)) {\n-      ResourceMark mark;\n-      NativeCallWrapper* call = nm->call_wrapper_before(return_pc);\n-      \/\/\n-      \/\/ bug 6281185. We might get here after resolving a call site to a vanilla\n-      \/\/ virtual call. Because the resolvee uses the verified entry it may then\n-      \/\/ see compiled code and attempt to patch the site by calling us. This would\n-      \/\/ then incorrectly convert the call site to optimized and its downhill from\n-      \/\/ there. If you're lucky you'll get the assert in the bugid, if not you've\n-      \/\/ just made a call site that could be megamorphic into a monomorphic site\n-      \/\/ for the rest of its life! Just another racing bug in the life of\n-      \/\/ fixup_callers_callsite ...\n-      \/\/\n-      RelocIterator iter(nm, call->instruction_address(), call->next_instruction_address());\n-      iter.next();\n-      assert(iter.has_current(), \"must have a reloc at java call site\");\n-      relocInfo::relocType typ = iter.reloc()->type();\n-      if (typ != relocInfo::static_call_type &&\n-           typ != relocInfo::opt_virtual_call_type &&\n-           typ != relocInfo::static_stub_type) {\n-        return;\n-      }\n-      if (nm->method()->is_continuation_enter_intrinsic()) {\n-        if (ContinuationEntry::is_interpreted_call(call->instruction_address())) {\n-          return;\n-        }\n-      }\n-      address destination = call->destination();\n-      address entry_point = callee->verified_entry_point();\n-      if (should_fixup_call_destination(destination, entry_point, caller_pc, moop, cb)) {\n-        call->set_destination_mt_safe(entry_point);\n-      }\n-    }\n+  if (!caller->is_in_use() || !NativeCall::is_call_before(return_pc)) {\n+    return;\n+  }\n+\n+  \/\/ Expect to find a native call there (unless it was no-inline cache vtable dispatch)\n+  CompiledICLocker ic_locker(caller);\n+  ResourceMark rm;\n+\n+  \/\/ If we got here through a static call or opt_virtual call, then we know where the\n+  \/\/ call address would be; let's peek at it\n+  address callsite_addr = (address)nativeCall_before(return_pc);\n+  RelocIterator iter(caller, callsite_addr, callsite_addr + 1);\n+  if (!iter.next()) {\n+    \/\/ No reloc entry found; not a static or optimized virtual call\n+    return;\n@@ -2141,0 +1800,9 @@\n+\n+  relocInfo::relocType type = iter.reloc()->type();\n+  if (type != relocInfo::static_call_type &&\n+      type != relocInfo::opt_virtual_call_type) {\n+    return;\n+  }\n+\n+  CompiledDirectCall* callsite = CompiledDirectCall::before(return_pc);\n+  callsite->set_to_clean();\n@@ -3406,2 +3074,2 @@\n-      }\n-    }\n+          }\n+        }\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":68,"deletions":400,"binary":false,"changes":468,"status":"modified"},{"patch":"@@ -49,3 +49,0 @@\n-  static bool resolve_sub_helper_internal(methodHandle callee_method, const frame& caller_frame,\n-                                          CompiledMethod* caller_nm, bool is_virtual, bool is_optimized,\n-                                          Handle receiver, CallInfo& call_info, Bytecodes::Code invoke_code, TRAPS);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -66,1 +67,0 @@\n-#include \"oops\/compiledICHolder.hpp\"\n@@ -214,2 +214,0 @@\n-  nonstatic_field(CompiledICHolder,            _holder_metadata,                              Metadata*)                             \\\n-  nonstatic_field(CompiledICHolder,            _holder_klass,                                 Klass*)                                \\\n@@ -1165,1 +1163,0 @@\n-  declare_toplevel_type(CompiledICHolder)                                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/icBuffer.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/debug.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1,70 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.oops;\n-\n-import java.io.*;\n-import java.util.*;\n-import sun.jvm.hotspot.debugger.*;\n-import sun.jvm.hotspot.runtime.*;\n-import sun.jvm.hotspot.types.*;\n-import sun.jvm.hotspot.utilities.Observable;\n-import sun.jvm.hotspot.utilities.Observer;\n-\n-public class CompiledICHolder extends VMObject {\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static synchronized void initialize(TypeDataBase db) throws WrongTypeException {\n-    Type type      = db.lookupType(\"CompiledICHolder\");\n-    holderMetadata = new MetadataField(type.getAddressField(\"_holder_metadata\"), 0);\n-    holderKlass    = new MetadataField(type.getAddressField(\"_holder_klass\"), 0);\n-    headerSize     = type.getSize();\n-  }\n-\n-  public CompiledICHolder(Address addr) {\n-      super(addr);\n-  }\n-\n-  public boolean isCompiledICHolder()  { return true; }\n-\n-  private static long headerSize;\n-\n-  \/\/ Fields\n-  private static MetadataField holderMetadata;\n-  private static MetadataField holderKlass;\n-\n-  \/\/ Accessors for declared fields\n-  public Metadata getHolderMetadata() { return holderMetadata.getValue(this); }\n-  public Klass    getHolderKlass()    { return (Klass)    holderKlass.getValue(this); }\n-\n-  public void printValueOn(PrintStream tty) {\n-    tty.print(\"CompiledICHolder\");\n-  }\n-  }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/CompiledICHolder.java","additions":0,"deletions":70,"binary":false,"changes":70,"status":"deleted"},{"patch":"@@ -43,1 +43,0 @@\n-        output.shouldContain(\"updating inline caches\");\n","filename":"test\/hotspot\/jtreg\/runtime\/logging\/SafepointCleanupTest.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+        \/\/ EventNames.SafepointCleanupTask,\n@@ -56,1 +57,0 @@\n-        EventNames.SafepointCleanupTask,\n","filename":"test\/jdk\/jdk\/jfr\/event\/runtime\/TestSafepointEvents.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}