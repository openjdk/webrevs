{"files":[{"patch":"@@ -1472,0 +1472,139 @@\n+  \/\/ Helper for generate_unsafe_setmemory\n+  \/\/\n+  \/\/ Non-atomically fill an array of memory using 1 byte chunk and return.\n+  \/\/ We don't care about atomicity because the address and size are not aligned, So we are\n+  \/\/ free to fill the memory with best possible ways.\n+  static void do_setmemory_atomic_loop_mvc(Register dest, Register size, Register byteVal,\n+                                           MacroAssembler *_masm) {\n+    NearLabel L_loop, L_tail, L_mvc;\n+\n+    __ z_aghi(size, -1); \/\/ -1 because first byte is preset by stc\n+    __ z_bcr(Assembler::bcondLow, Z_R14);   \/\/ result  < 0 means size == 0 => return\n+    __ z_stc(byteVal, Address(dest));       \/\/ initialize first byte\n+    __ z_bcr(Assembler::bcondEqual, Z_R14); \/\/ result == 0 means size == 1 => return\n+\n+    \/\/ handle complete 256 byte blocks\n+    __ bind(L_loop);\n+    __ z_aghi(size, -256);            \/\/ decrement remaining #bytes\n+    __ z_brl(L_tail);                 \/\/ skip loop if no full 256 byte block left\n+\n+    __ z_mvc(1, 255, dest, 0, dest);  \/\/ propagate byte from dest[0+i*256] to dest[1+i*256]\n+    __ z_bcr(Assembler::bcondEqual, Z_R14); \/\/ remaining size == 0 => return (mvc does not touch CC)\n+\n+    __ z_aghi(dest, 256);             \/\/ increment target address\n+    __ z_bru(L_loop);\n+\n+    \/\/ handle remaining bytes. We know 0 < size < 256\n+    __ bind(L_tail);\n+    __ z_aghi(size, +256-1);         \/\/ prepare size value for mvc via exrl\n+    __ z_exrl(size, L_mvc);\n+    __ z_br(Z_R14);\n+\n+    __ bind(L_mvc);\n+    __ z_mvc(1, 0, dest, 0, dest);   \/\/ mvc template, needs to be generated, not executed\n+  }\n+\n+  static void do_setmemory_atomic_loop(int elem_size, Register dest, Register size, Register byteVal,\n+                                       MacroAssembler *_masm) {\n+\n+    NearLabel L_Loop, L_Tail; \/\/ 2x unrolled loop\n+    Register tmp = Z_R1; \/\/ R1 is free at this point\n+\n+    if (elem_size > 1) {\n+      __ rotate_then_insert(byteVal, byteVal, 64 - 2 * 8 , 63 - 8,  8, false);\n+    }\n+\n+    if (elem_size > 2) {\n+      __ rotate_then_insert(byteVal, byteVal, 64 - 2 * 16, 63 - 16, 16, false);\n+    }\n+\n+    if (elem_size > 4) {\n+      __ rotate_then_insert(byteVal, byteVal, 64 - 2 * 32, 63 - 32, 32, false);\n+    }\n+\n+    __ z_risbg(tmp, size, 32, 63, 64 - exact_log2(2 * elem_size), \/* zero_rest *\/ true); \/\/ just do the right shift and set cc\n+    __ z_bre(L_Tail);\n+\n+    __ align(32); \/\/ loop alignment\n+    __ bind(L_Loop);\n+    __ store_sized_value(byteVal, Address(dest, 0), elem_size);\n+    __ store_sized_value(byteVal, Address(dest, elem_size), elem_size);\n+    __ z_aghi(dest, 2 * elem_size);\n+    __ z_brct(tmp, L_Loop);\n+\n+    __ bind(L_Tail);\n+    __ z_nilf(size, elem_size);\n+    __ z_bcr(Assembler::bcondEqual, Z_R14);\n+    __ store_sized_value(byteVal, Address(dest, 0), elem_size);\n+    __ z_br(Z_R14);\n+  }\n+\n+  \/\/\n+  \/\/  Generate 'unsafe' set memory stub\n+  \/\/  Though just as safe as the other stubs, it takes an unscaled\n+  \/\/  size_t (# bytes) argument instead of an element count.\n+  \/\/\n+  \/\/  Input:\n+  \/\/    Z_ARG1   - destination array address\n+  \/\/    Z_ARG2   - byte count (size_t)\n+  \/\/    Z_ARG3   - byte value\n+  \/\/\n+  address generate_unsafe_setmemory(address unsafe_byte_fill) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, StubGenStubId::unsafe_setmemory_id);\n+    unsigned int start_off = __ offset();\n+\n+    \/\/ bump this on entry, not on exit:\n+    \/\/ inc_counter_np(SharedRuntime::_unsafe_set_memory_ctr);\n+\n+    {\n+      const Register dest = Z_ARG1;\n+      const Register size = Z_ARG2;\n+      const Register byteVal = Z_ARG3;\n+      const Register rScratch1 = Z_R1_scratch;\n+      NearLabel L_fill8Bytes, L_fill4Bytes, L_fillBytes;\n+      \/\/ fill_to_memory_atomic(unsigned char*, unsigned long, unsigned char)\n+\n+      \/\/ Check for pointer & size alignment\n+      __ z_ogrk(rScratch1, dest, size);\n+\n+      __ z_nill(rScratch1, 7);\n+      __ z_braz(L_fill8Bytes); \/\/ branch if 0\n+\n+      __ z_nill(rScratch1, 3);\n+      __ z_braz(L_fill4Bytes); \/\/ branch if 0\n+\n+      __ z_nill(rScratch1, 1);\n+      __ z_brnaz(L_fillBytes); \/\/ branch if not 0\n+\n+      \/\/ Mark remaining code as such which performs Unsafe accesses.\n+      UnsafeMemoryAccessMark umam(this, true, false);\n+\n+      \/\/ At this point, we know the lower bit of size is zero and a\n+      \/\/ multiple of 2\n+      do_setmemory_atomic_loop(2, dest, size, byteVal, _masm);\n+\n+      __ bind(L_fill8Bytes);\n+      \/\/ At this point, we know the lower 3 bits of size are zero and a\n+      \/\/ multiple of 8\n+      do_setmemory_atomic_loop(8, dest, size, byteVal, _masm);\n+\n+      __ bind(L_fill4Bytes);\n+      \/\/ At this point, we know the lower 2 bits of size are zero and a\n+      \/\/ multiple of 4\n+      do_setmemory_atomic_loop(4, dest, size, byteVal, _masm);\n+\n+      __ bind(L_fillBytes);\n+      do_setmemory_atomic_loop_mvc(dest, size, byteVal, _masm);\n+    }\n+    return __ addr_at(start_off);\n+  }\n+\n+  \/\/ This is common errorexit stub for UnsafeMemoryAccess.\n+  address generate_unsafecopy_common_error_exit() {\n+    unsigned int start_off = __ offset();\n+    __ z_lghi(Z_RET, 0); \/\/ return 0\n+    __ z_br(Z_R14);\n+    return __ addr_at(start_off);\n+  }\n+\n@@ -1476,0 +1615,4 @@\n+\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeMemoryAccess::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -1503,0 +1646,4 @@\n+\n+#ifdef COMPILER2\n+    StubRoutines::_unsafe_setmemory = generate_unsafe_setmemory(StubRoutines::_jbyte_fill);\n+#endif \/\/ COMPILER2\n@@ -3187,0 +3334,4 @@\n+    if (UnsafeMemoryAccess::_table == nullptr) {\n+      UnsafeMemoryAccess::create_table(4); \/\/ 4 for setMemory\n+    }\n+\n","filename":"src\/hotspot\/cpu\/s390\/stubGenerator_s390.cpp","additions":151,"deletions":0,"binary":false,"changes":151,"status":"modified"}]}