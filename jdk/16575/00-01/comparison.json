{"files":[{"patch":"@@ -582,4 +582,6 @@\n-    __ movq(temp2, temp1);\n-    __ shlq(temp2, shift);\n-    __ cmpq(temp2, large_threshold);\n-    __ jcc(Assembler::greaterEqual, L_copy_large);\n+    if (MaxVectorSize == 64) {\n+      __ movq(temp2, temp1);\n+      __ shlq(temp2, shift);\n+      __ cmpq(temp2, large_threshold);\n+      __ jcc(Assembler::greaterEqual, L_copy_large);\n+    }\n@@ -727,3 +729,6 @@\n-  __ BIND(L_copy_large);\n-  arraycopy_avx3_large(to, from, temp1, temp2, temp3, temp4, count, xmm1, xmm2, xmm3, xmm4, shift);\n-  __ jmp(L_finish);  return start;\n+  if (MaxVectorSize == 64) {\n+    __ BIND(L_copy_large);\n+    arraycopy_avx3_large(to, from, temp1, temp2, temp3, temp4, count, xmm1, xmm2, xmm3, xmm4, shift);\n+    __ jmp(L_finish);\n+  }  \n+  return start;\n@@ -748,1 +753,25 @@\n-  __ BIND(L_entry_large);\n+  if (MaxVectorSize == 64) {\n+    __ BIND(L_entry_large);\n+\n+    __ BIND(L_pre_main_post_large);\n+    \/\/ Partial copy to make dst address 64 byte aligned.\n+    __ movq(temp2, to);\n+    __ andq(temp2, 63);\n+    __ jcc(Assembler::equal, L_main_pre_loop_large);\n+\n+    __ negptr(temp2);\n+    __ addq(temp2, 64);\n+    if (shift) {\n+      __ shrq(temp2, shift);\n+    }\n+    __ movq(temp3, temp2);\n+    copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0);\n+    __ movq(temp4, temp2);\n+    __ movq(temp1, count);\n+    __ subq(temp1, temp2);\n+\n+    __ cmpq(temp1, loop_size[shift]);\n+    __ jcc(Assembler::less, L_tail_large);\n+\n+    __ BIND(L_main_pre_loop_large);\n+    __ subq(temp1, loop_size[shift]);  \/\/ whay is this here\n@@ -750,5 +779,9 @@\n-  __ BIND(L_pre_main_post_large);\n-  \/\/ Partial copy to make dst address 64 byte aligned.\n-  __ movq(temp2, to);\n-  __ andq(temp2, 63);\n-  __ jcc(Assembler::equal, L_main_pre_loop_large);\n+    \/\/ Main loop with aligned copy block size of 256 bytes at 64 byte copy granularity.\n+    __ align32();\n+    __ BIND(L_main_loop_large);\n+    copy256_avx3(to, from, temp4, xmm1, xmm2, xmm3, xmm4, false, shift, 0);\n+    __ addptr(temp4, loop_size[shift]);\n+    __ subq(temp1, loop_size[shift]);\n+    __ jcc(Assembler::greater, L_main_loop_large);\n+    \/\/ fence needed because copy256_avx 3 uses non-temporal stores\n+    __ sfence();\n@@ -756,4 +789,10 @@\n-  __ negptr(temp2);\n-  __ addq(temp2, 64);\n-  if (shift) {\n-    __ shrq(temp2, shift);\n+    __ addq(temp1, loop_size[shift]);\n+    \/\/ Zero length check.\n+    __ jcc(Assembler::lessEqual, L_exit_large);\n+    __ BIND(L_tail_large);\n+    \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit_large);\n+    arraycopy_avx3_special_cases_256(xmm1, k2, from, to, temp1, shift,\n+                                 temp4, temp3, L_entry_large, L_exit_large);\n+    __ BIND(L_exit_large);\n@@ -761,32 +800,0 @@\n-  __ movq(temp3, temp2);\n-  copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0);\n-  __ movq(temp4, temp2);\n-  __ movq(temp1, count);\n-  __ subq(temp1, temp2);\n-\n-  __ cmpq(temp1, loop_size[shift]);\n-  __ jcc(Assembler::less, L_tail_large);\n-\n-  __ BIND(L_main_pre_loop_large);\n-  __ subq(temp1, loop_size[shift]);  \/\/ whay is this here\n-\n-  \/\/ Main loop with aligned copy block size of 256 bytes at 64 byte copy granularity.\n-  __ align32();\n-  __ BIND(L_main_loop_large);\n-  copy256_avx3(to, from, temp4, xmm1, xmm2, xmm3, xmm4, false, shift, 0);\n-  __ addptr(temp4, loop_size[shift]);\n-  __ subq(temp1, loop_size[shift]);\n-  __ jcc(Assembler::greater, L_main_loop_large);\n-  \/\/ fence needed because copy256_avx 3 uses non-temporal stores\n-  __ sfence();\n-\n-  __ addq(temp1, loop_size[shift]);\n-  \/\/ Zero length check.\n-  __ jcc(Assembler::lessEqual, L_exit_large);\n-  __ BIND(L_tail_large);\n-  \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-  __ cmpq(temp1, 0);\n-  __ jcc(Assembler::lessEqual, L_exit_large);\n-  arraycopy_avx3_special_cases_256(xmm1, k2, from, to, temp1, shift,\n-                               temp4, temp3, L_entry_large, L_exit_large);\n-  __ BIND(L_exit_large);\n@@ -1051,36 +1058,38 @@\n-  \/\/ Case A) Special case for length less than or equal to 64 bytes.\n-  __ BIND(L_entry_64);\n-  __ cmpq(count, size_mat[shift][0]);\n-  __ jccb(Assembler::greater, L_entry_128);\n-  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, true);\n-  __ jmp(L_exit);\n-\n-  \/\/ Case B) Special case for length less than or equal to 128 bytes.\n-  __ BIND(L_entry_128);\n-  __ cmpq(count, size_mat[shift][1]);\n-  __ jccb(Assembler::greater, L_entry_192);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, true);\n-  __ subq(count, 64 >> shift);\n-  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64, true);\n-  __ jmp(L_exit);\n-\n-  \/\/ Case C) Special case for length less than or equal to 192 bytes.\n-  __ BIND(L_entry_192);\n-  __ cmpq(count, size_mat[shift][2]);\n-  __ jcc(Assembler::greater, L_entry_256);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, true);\n-  copy64_avx(to, from, index, xmm, false, shift, 64, true);\n-  __ subq(count, 128 >> shift);\n-  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128, true);\n-  __ jmp(L_exit);\n-\n-  \/\/ Case D) Special case for length less than or equal to 256 bytes.\n-  __ BIND(L_entry_256);\n-  __ cmpq(count, size_mat[shift][3]);\n-  __ jcc(Assembler::greater, L_entry);\n-  copy64_avx(to, from, index, xmm, false, shift, 0, true);\n-  copy64_avx(to, from, index, xmm, false, shift, 64, true);\n-  copy64_avx(to, from, index, xmm, false, shift, 128, true);\n-  __ subq(count, 192 >> shift);\n-  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 192, true);\n-  __ jmp(L_exit);\n+  if (MaxVectorSize == 64) {\n+    \/\/ Case A) Special case for length less than or equal to 64 bytes.\n+    __ BIND(L_entry_64);\n+    __ cmpq(count, size_mat[shift][0]);\n+    __ jccb(Assembler::greater, L_entry_128);\n+    copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, true);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case B) Special case for length less than or equal to 128 bytes.\n+    __ BIND(L_entry_128);\n+    __ cmpq(count, size_mat[shift][1]);\n+    __ jccb(Assembler::greater, L_entry_192);\n+    copy64_avx(to, from, index, xmm, false, shift, 0, true);\n+    __ subq(count, 64 >> shift);\n+    copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64, true);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case C) Special case for length less than or equal to 192 bytes.\n+    __ BIND(L_entry_192);\n+    __ cmpq(count, size_mat[shift][2]);\n+    __ jcc(Assembler::greater, L_entry_256);\n+    copy64_avx(to, from, index, xmm, false, shift, 0, true);\n+    copy64_avx(to, from, index, xmm, false, shift, 64, true);\n+    __ subq(count, 128 >> shift);\n+    copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128, true);\n+    __ jmp(L_exit);\n+\n+    \/\/ Case D) Special case for length less than or equal to 256 bytes.\n+    __ BIND(L_entry_256);\n+    __ cmpq(count, size_mat[shift][3]);\n+    __ jcc(Assembler::greater, L_entry);\n+    copy64_avx(to, from, index, xmm, false, shift, 0, true);\n+    copy64_avx(to, from, index, xmm, false, shift, 64, true);\n+    copy64_avx(to, from, index, xmm, false, shift, 128, true);\n+    __ subq(count, 192 >> shift);\n+    copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 192, true);\n+    __ jmp(L_exit);\n+  }\n@@ -1167,0 +1176,1 @@\n+  if (MaxVectorSize == 64) {\n@@ -1187,0 +1197,1 @@\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_arraycopy.cpp","additions":96,"deletions":85,"binary":false,"changes":181,"status":"modified"}]}