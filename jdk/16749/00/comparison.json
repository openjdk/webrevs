{"files":[{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -225,42 +226,0 @@\n-\/\/ Sets cache, index.\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, Register index, int bcp_offset, size_t index_size) {\n-  assert(bcp_offset > 0, \"bcp is still pointing to start of bytecode\");\n-  assert_different_registers(cache, index);\n-\n-  get_index_at_bcp(index, bcp_offset, cache, index_size);\n-\n-  \/\/ load constant pool cache pointer\n-  ldr(cache, Address(FP, frame::interpreter_frame_cache_offset * wordSize));\n-\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  assert(sizeof(ConstantPoolCacheEntry) == 4*wordSize, \"adjust code below\");\n-  logical_shift_left(index, index, 2);\n-}\n-\n-\/\/ Sets cache, index, bytecode.\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache, Register index, Register bytecode, int byte_no, int bcp_offset, size_t index_size) {\n-  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n-  \/\/ caution index and bytecode can be the same\n-  add(bytecode, cache, AsmOperand(index, lsl, LogBytesPerWord));\n-  ldrb(bytecode, Address(bytecode, (1 + byte_no) + in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset())));\n-  TemplateTable::volatile_barrier(MacroAssembler::LoadLoad, noreg, true);\n-}\n-\n-\/\/ Sets cache. Blows reg_tmp.\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache, Register reg_tmp, int bcp_offset, size_t index_size) {\n-  assert(bcp_offset > 0, \"bcp is still pointing to start of bytecode\");\n-  assert_different_registers(cache, reg_tmp);\n-\n-  get_index_at_bcp(reg_tmp, bcp_offset, cache, index_size);\n-\n-  \/\/ load constant pool cache pointer\n-  ldr(cache, Address(FP, frame::interpreter_frame_cache_offset * wordSize));\n-\n-  \/\/ skip past the header\n-  add(cache, cache, in_bytes(ConstantPoolCache::base_offset()));\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  \/\/ and from word offset to byte offset\n-  assert(sizeof(ConstantPoolCacheEntry) == 4*wordSize, \"adjust code below\");\n-  add(cache, cache, AsmOperand(reg_tmp, lsl, 2 + LogBytesPerWord));\n-}\n-\n@@ -346,0 +305,14 @@\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  get_index_at_bcp(index, bcp_offset, cache \/* as tmp *\/, sizeof(u2));\n+  mov(cache, sizeof(ResolvedMethodEntry));\n+  mul(index, index, cache); \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+\n+  \/\/ load constant pool cache pointer\n+  ldr(cache, Address(FP, frame::interpreter_frame_cache_offset * wordSize));\n+  \/\/ Get address of method entries array\n+  ldr(cache, Address(cache, ConstantPoolCache::method_entries_offset()));\n+  add(cache, cache, Array<ResolvedMethodEntry>::base_offset_in_bytes());\n+  add(cache, cache, index);\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":15,"deletions":42,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -92,5 +92,0 @@\n-  \/\/ Sets cache, index.\n-  void get_cache_and_index_at_bcp(Register cache, Register index, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register index, Register bytecode, int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n-  \/\/ Sets cache. Blows reg_tmp.\n-  void get_cache_entry_pointer_at_bcp(Register cache, Register reg_tmp, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -106,0 +101,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -376,4 +377,3 @@\n-    __ get_cache_and_index_at_bcp(Rcache, Rindex, 1, index_size);\n-\n-    __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-    __ ldrb(Rtemp, Address(Rtemp, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(Rcache, Rindex);\n+    __ ldrh(Rcache, Address(Rcache, in_bytes(ResolvedIndyEntry::num_parameters_offset())));\n@@ -381,1 +381,1 @@\n-    __ add(Rstack_top, Rstack_top, AsmOperand(Rtemp, lsl, Interpreter::logStackElementSize));\n+    __ add(Rstack_top, Rstack_top, AsmOperand(Rcache, lsl, Interpreter::logStackElementSize));\n","filename":"src\/hotspot\/cpu\/arm\/templateInterpreterGenerator_arm.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -547,2 +548,2 @@\n-  __ logical_shift_left( off, off, 32 - ConstantPoolCacheEntry::field_index_bits);\n-  __ logical_shift_right(off, off, 32 - ConstantPoolCacheEntry::field_index_bits);\n+  __ logical_shift_left( off, off, 32 - ConstantPoolCache::field_index_bits);\n+  __ logical_shift_right(off, off, 32 - ConstantPoolCache::field_index_bits);\n@@ -552,3 +553,1 @@\n-  __ logical_shift_right(flags, flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n+  __ logical_shift_right(flags, flags, ConstantPoolCache::tos_state_shift);\n@@ -2572,4 +2571,3 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register Rcache,\n-                                            Register Rindex,\n-                                            size_t index_size) {\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register Rcache,\n+                                                       Register Rindex) {\n@@ -2577,0 +2575,1 @@\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n@@ -2580,2 +2579,10 @@\n-  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, Rindex, Rtemp, byte_no, 1, index_size);\n+  __ load_method_entry(Rcache, Rindex);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ add(Rtemp, Rcache, in_bytes(ResolvedMethodEntry::bytecode1_offset()));\n+    case f2_byte:\n+      __ add(Rtemp, Rcache, in_bytes(ResolvedMethodEntry::bytecode2_offset()));\n+  }\n+  \/\/ Load-acquire the bytecode to match store-release in InterpreterRuntime\n+  __ ldrb(Rtemp, Rtemp);\n+  __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::LoadLoad | MacroAssembler::LoadStore), noreg, true);\n@@ -2590,1 +2597,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, Rindex, 1, index_size);\n+  __ load_method_entry(Rcache, Rindex);\n@@ -2658,32 +2665,0 @@\n-\/\/ The Rcache and Rindex registers must be set before call\n-void TemplateTable::load_field_cp_cache_entry(Register Rcache,\n-                                              Register Rindex,\n-                                              Register Roffset,\n-                                              Register Rflags,\n-                                              Register Robj,\n-                                              bool is_static = false) {\n-\n-  assert_different_registers(Rcache, Rindex, Rtemp);\n-  assert_different_registers(Roffset, Rflags, Robj, Rtemp);\n-\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-\n-  __ add(Rtemp, Rcache, AsmOperand(Rindex, lsl, LogBytesPerWord));\n-\n-  \/\/ Field offset\n-  __ ldr(Roffset, Address(Rtemp,\n-           cp_base_offset + ConstantPoolCacheEntry::f2_offset()));\n-\n-  \/\/ Flags\n-  __ ldr_u32(Rflags, Address(Rtemp,\n-           cp_base_offset + ConstantPoolCacheEntry::flags_offset()));\n-\n-  if (is_static) {\n-    __ ldr(Robj, Address(Rtemp,\n-             cp_base_offset + ConstantPoolCacheEntry::f1_offset()));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ ldr(Robj, Address(Robj, mirror_offset));\n-    __ resolve_oop_handle(Robj);\n-  }\n-}\n-\n@@ -2752,7 +2727,42 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal\/*unused*\/,\n-                                               bool is_invokedynamic \/*unused*\/) {\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register Rcache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  Register index = flags;\n+  assert_different_registers(Rcache, method, flags);\n+  resolve_cache_and_index_for_method(f1_byte, Rcache, index);\n+  __ ldrb(flags, Address(Rcache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ ldr(method, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register Rcache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  Register index = ref_index;\n+  assert_different_registers(method, flags);\n+  assert_different_registers(Rcache, method, index);\n+\n+\n+  resolve_cache_and_index_for_method(f1_byte, Rcache, index);\n+  __ ldrb(flags, Address(Rcache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ tbz(flags, ResolvedMethodEntry::has_appendix_shift, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ ldrh(ref_index, Address(Rcache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ ldr(method, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register Rcache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n@@ -2760,5 +2770,2 @@\n-  const Register cache = R2_tmp;\n-  const Register index = R3_tmp;\n-  const Register temp_reg = Rtemp;\n-  assert_different_registers(cache, index, temp_reg);\n-  assert_different_registers(method, itable_index, temp_reg);\n+  const Register index = method_or_table_index;\n+  assert_different_registers(method_or_table_index, Rcache, flags);\n@@ -2767,13 +2774,14 @@\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()\n-      )\n-    );\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n+  resolve_cache_and_index_for_method(f1_byte, Rcache, index);\n+  __ ldrb(flags, Address(Rcache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_forced_virtual_shift, NotVirtual);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n@@ -2781,4 +2789,3 @@\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-    __ add(temp_reg, cache, AsmOperand(index, lsl, LogBytesPerWord));\n-    __ ldr(method, Address(temp_reg, method_offset));\n+  __ bind(NotVFinal);\n+  __ ldrh(method_or_table_index, Address(Rcache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ b(Done);\n@@ -2786,4 +2793,4 @@\n-  if (itable_index != noreg) {\n-    __ ldr(itable_index, Address(temp_reg, index_offset));\n-  }\n-  __ ldr_u32(flags, Address(temp_reg, flags_offset));\n+  __ bind(NotVirtual);\n+  __ ldr(method_or_table_index, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ ldr(klass, Address(Rcache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n@@ -2792,0 +2799,21 @@\n+void TemplateTable::load_resolved_method_entry_virtual(Register Rcache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method_or_table_index, Rcache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, Rcache, index);\n+  __ ldrb(flags, Address(Rcache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ ldrh(method_or_table_index, Address(Rcache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ bind(Done);\n+}\n@@ -3623,22 +3651,2 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method,  \/\/ linked method (or i-klass)\n-                                   Register index,   \/\/ itable index, MethodType, etc.\n-                                   Register recv,    \/\/ if caller wants to see it\n-                                   Register flags    \/\/ if caller wants to test it\n-                                   ) {\n-  \/\/ determine flags\n-  const Bytecodes::Code code = bytecode();\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(recv  == noreg || recv  == R2, \"\");\n-  assert(flags == noreg || flags == R3, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = R2;\n-  if (flags == noreg)  flags = R3;\n-  const Register temp = Rtemp;\n+void TemplateTable::prepare_invoke(Register Rcache, Register recv) {\n+\n@@ -3646,1 +3654,3 @@\n-  assert_different_registers(method, index, flags, recv, LR, ret_type, temp);\n+\n+  const Bytecodes::Code code = bytecode();\n+  const bool load_receiver = (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic);\n@@ -3651,12 +3661,2 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push extra argument\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ tbz(flags, ConstantPoolCacheEntry::has_appendix_shift, L_no_push);\n-    __ mov(temp, index);\n-    __ load_resolved_reference_at_index(index, temp);\n-    __ verify_oop(index);\n-    __ push_ptr(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Load TOS state for later\n+  __ ldrb(ret_type, Address(Rcache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3666,2 +3666,2 @@\n-    __ andr(temp, flags, (uintx)ConstantPoolCacheEntry::parameter_size_mask);  \/\/ get parameter size\n-    Address recv_addr = __ receiver_argument_address(Rstack_top, temp, recv);\n+    __ ldrh(recv, Address(Rcache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n+    Address recv_addr = __ receiver_argument_address(Rstack_top, Rtemp, recv);\n@@ -3672,4 +3672,0 @@\n-  \/\/ compute return type\n-  __ logical_shift_right(ret_type, flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n@@ -3678,2 +3674,2 @@\n-    __ mov_slow(temp, table);\n-    __ ldr(LR, Address::indexed_ptr(temp, ret_type));\n+    __ mov_slow(Rtemp, table);\n+    __ ldr(LR, Address::indexed_ptr(Rtemp, ret_type));\n@@ -3695,1 +3691,1 @@\n-  __ tbz(flags, ConstantPoolCacheEntry::is_vfinal_shift, notFinal);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, notFinal);\n@@ -3732,1 +3728,4 @@\n-  prepare_invoke(byte_no, Rmethod, noreg, Rrecv, Rflags);\n+  load_resolved_method_entry_virtual(Rrecv,   \/\/ ResolvedMethodEntry*\n+                                     Rmethod, \/\/ Method* or itable index\n+                                     Rflags); \/\/ Flags\n+  prepare_invoke(Rrecv, Rrecv);\n@@ -3747,1 +3746,4 @@\n-  prepare_invoke(byte_no, Rmethod, noreg, Rrecv);\n+  load_resolved_method_entry_special_or_static(R2_tmp,  \/\/ ResolvedMethodEntry*\n+                                               Rmethod, \/\/ Method*\n+                                               R3_tmp); \/\/ Flags\n+  prepare_invoke(Rrecv, Rrecv);\n@@ -3759,1 +3761,4 @@\n-  prepare_invoke(byte_no, Rmethod);\n+  load_resolved_method_entry_special_or_static(R2_tmp,  \/\/ ResolvedMethodEntry*\n+                                               Rmethod, \/\/ Method*\n+                                               R3_tmp); \/\/ Flags\n+  prepare_invoke(R2_tmp, R2_tmp);\n@@ -3784,1 +3789,5 @@\n-  prepare_invoke(byte_no, Rinterf, Rmethod, Rrecv, Rflags);\n+  load_resolved_method_entry_interface(R2_tmp,  \/\/ ResolvedMethodEntry*\n+                                       R1_tmp,  \/\/ Klass*\n+                                       Rmethod, \/\/ Method* or itable\/vtable index\n+                                       R3_tmp); \/\/ Flags\n+  prepare_invoke(Rrecv, Rrecv);\n@@ -3792,1 +3801,1 @@\n-  __ tbz(Rflags, ConstantPoolCacheEntry::is_forced_virtual_shift, notObjectMethod);\n+  __ tbz(Rflags, ResolvedMethodEntry::is_forced_virtual_shift, notObjectMethod);\n@@ -3803,1 +3812,1 @@\n-  __ tbz(Rflags, ConstantPoolCacheEntry::is_vfinal_shift, notVFinal);\n+  __ tbz(Rflags, ResolvedMethodEntry::is_vfinal_shift, notVFinal);\n@@ -3877,1 +3886,0 @@\n-  const Register R5_method = R5_tmp;  \/\/ can't reuse Rmethod!\n@@ -3879,1 +3887,5 @@\n-  prepare_invoke(byte_no, R5_method, Rmtype, Rrecv);\n+  load_resolved_method_entry_handle(R2_tmp,  \/\/ ResolvedMethodEntry*\n+                                    Rmethod, \/\/ Method*\n+                                    Rmtype,  \/\/ Resolved Reference\n+                                    R3_tmp); \/\/ Flags\n+  prepare_invoke(Rrecv, Rrecv);\n@@ -3889,1 +3901,0 @@\n-  __ mov(Rmethod, R5_method);\n","filename":"src\/hotspot\/cpu\/arm\/templateTable_arm.cpp","additions":141,"deletions":130,"binary":false,"changes":271,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,6 +28,1 @@\n-  static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ if caller wants to see it\n-                             Register flags = noreg   \/\/ if caller wants to test it\n-                             );\n+  static void prepare_invoke(Register cache, Register recv);\n","filename":"src\/hotspot\/cpu\/arm\/templateTable_arm.hpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"}]}