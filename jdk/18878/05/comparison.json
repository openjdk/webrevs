{"files":[{"patch":"@@ -70,3 +70,0 @@\n-  \/\/ Load object header.\n-  z_lg(Rmark, Address(Roop, hdr_offset));\n-\n@@ -88,0 +85,4 @@\n+\n+    \/\/ Load object header.\n+    z_lg(Rmark, Address(Roop, hdr_offset));\n+\n@@ -144,6 +145,1 @@\n-    const Register tmp = Z_R1_scratch;\n-    z_lg(Rmark, Address(Roop, hdr_offset));\n-    z_lgr(tmp, Rmark);\n-    z_nill(tmp, markWord::monitor_value);\n-    branch_optimized(Assembler::bcondNotZero, slow_case);\n-    lightweight_unlock(Roop, Rmark, tmp, slow_case);\n+    lightweight_unlock(Roop, Rmark, Z_R1_scratch, slow_case);\n","filename":"src\/hotspot\/cpu\/s390\/c1_MacroAssembler_s390.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2017, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2024 SAP SE. All rights reserved.\n@@ -36,0 +36,9 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register box, Register temp1, Register temp2) {\n+  compiler_fast_lock_lightweight_object(obj, temp1, temp2);\n+}\n+\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register box, Register temp1, Register temp2) {\n+  compiler_fast_unlock_lightweight_object(obj, temp1, temp2);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c2_MacroAssembler_s390.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2017, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2024 SAP SE. All rights reserved.\n@@ -32,0 +32,4 @@\n+  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in s390.ad file.\n+  void fast_lock_lightweight(Register obj, Register box, Register temp1, Register temp2);\n+  void fast_unlock_lightweight(Register obj, Register box, Register temp1, Register temp2);\n+\n","filename":"src\/hotspot\/cpu\/s390\/c2_MacroAssembler_s390.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1008,3 +1008,0 @@\n-  \/\/ Load markWord from object into header.\n-  z_lg(header, hdr_offset, object);\n-\n@@ -1018,1 +1015,1 @@\n-    lightweight_lock(object, \/* mark word *\/ header, tmp, slow_case);\n+    lightweight_lock(object, header, tmp, slow_case);\n@@ -1021,0 +1018,3 @@\n+    \/\/ Load markWord from object into header.\n+    z_lg(header, hdr_offset, object);\n+\n@@ -1156,18 +1156,0 @@\n-    \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n-    \/\/ must handle it.\n-\n-    Register tmp = current_header;\n-\n-    \/\/ First check for lock-stack underflow.\n-    z_lgf(tmp, Address(Z_thread, JavaThread::lock_stack_top_offset()));\n-    compareU32_and_branch(tmp, (unsigned)LockStack::start_offset(), Assembler::bcondNotHigh, slow_case);\n-\n-    \/\/ Then check if the top of the lock-stack matches the unlocked object.\n-    z_aghi(tmp, -oopSize);\n-    z_lg(tmp, Address(Z_thread, tmp));\n-    compare64_and_branch(tmp, object, Assembler::bcondNotEqual, slow_case);\n-\n-    z_lg(header, Address(object, hdr_offset));\n-    z_lgr(tmp, header);\n-    z_nill(tmp, markWord::monitor_value);\n-    z_brne(slow_case);\n@@ -1175,1 +1157,1 @@\n-    lightweight_unlock(object, header, tmp, slow_case);\n+    lightweight_unlock(object, header, current_header, slow_case);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":5,"deletions":23,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -3193,0 +3193,1 @@\n+\/\/ \"The box\" is the space on the stack where we copy the object mark.\n@@ -3194,0 +3195,4 @@\n+\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"uses fast_lock_lightweight\");\n+  assert_different_registers(oop, box, temp1, temp2);\n+\n@@ -3195,2 +3200,3 @@\n-  Register currentHeader = temp1;\n-  Register temp = temp2;\n+  Register currentHeader   = temp1;\n+  Register temp            = temp2;\n+\n@@ -3201,2 +3207,0 @@\n-  assert_different_registers(temp1, temp2, oop, box);\n-\n@@ -3210,2 +3214,4 @@\n-    testbit(Address(temp, Klass::access_flags_offset()), exact_log2(JVM_ACC_IS_VALUE_BASED_CLASS));\n-    z_btrue(done);\n+    z_l(temp, Address(temp, Klass::access_flags_offset()));\n+    assert((JVM_ACC_IS_VALUE_BASED_CLASS & 0xFFFF) == 0, \"or change following instruction\");\n+    z_nilh(temp, JVM_ACC_IS_VALUE_BASED_CLASS >> 16);\n+    z_brne(done);\n@@ -3225,1 +3231,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -3254,4 +3261,0 @@\n-    z_bru(done);\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_lock(oop, displacedHeader, temp, done);\n@@ -3273,4 +3276,3 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ Store a non-null value into the box.\n-    z_stg(box, BasicLock::displaced_header_offset_in_bytes(), box);\n-  }\n+\n+  \/\/ Store a non-null value into the box.\n+  z_stg(box, BasicLock::displaced_header_offset_in_bytes(), box);\n@@ -3298,0 +3300,4 @@\n+\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"uses fast_unlock_lightweight\");\n+  assert_different_registers(oop, box, temp1, temp2);\n+\n@@ -3299,2 +3305,2 @@\n-  Register currentHeader = temp2;\n-  Register temp = temp1;\n+  Register currentHeader   = temp2;\n+  Register temp            = temp1;\n@@ -3304,2 +3310,0 @@\n-  assert_different_registers(temp1, temp2, oop, box);\n-\n@@ -3329,1 +3333,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -3336,5 +3341,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-\n-    lightweight_unlock(oop, currentHeader, displacedHeader, done);\n-    z_bru(done);\n@@ -5708,3 +5708,0 @@\n-\/\/ Branches to slow upon failure to lock the object.\n-\/\/ Falls through upon success.\n-\/\/\n@@ -5712,1 +5709,1 @@\n-\/\/  - hdr: the header, already loaded from obj, contents destroyed.\n+\/\/  - temp1, temp2: temporary registers, contents destroyed.\n@@ -5714,1 +5711,1 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register temp, Label& slow_case) {\n+void MacroAssembler::lightweight_lock(Register obj, Register temp1, Register temp2, Label& slow) {\n@@ -5717,1 +5714,12 @@\n-  assert_different_registers(obj, hdr, temp);\n+  assert_different_registers(obj, temp1, temp2);\n+\n+  Label push;\n+  const Register top           = temp1;\n+  const Register mark          = temp2;\n+  const int mark_offset        = oopDesc::mark_offset_in_bytes();\n+  const ByteSize ls_top_offset = JavaThread::lock_stack_top_offset();\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  z_lg(mark, Address(obj, mark_offset));\n+\n@@ -5720,1 +5728,1 @@\n-  z_lgf(temp, Address(Z_thread, JavaThread::lock_stack_top_offset()));\n+  z_lgf(top, Address(Z_thread, ls_top_offset));\n@@ -5722,1 +5730,1 @@\n-  compareU32_and_branch(temp, (unsigned)LockStack::end_offset()-1, bcondHigh, slow_case);\n+  compareU32_and_branch(top, (unsigned)LockStack::end_offset(), bcondNotLow, slow);\n@@ -5724,3 +5732,2 @@\n-  \/\/ attempting a lightweight_lock\n-  \/\/ Load (object->mark() | 1) into hdr\n-  z_oill(hdr, markWord::unlocked_value);\n+  \/\/ The underflow check is elided. The recursive check will always fail\n+  \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n@@ -5728,1 +5735,4 @@\n-  z_lgr(temp, hdr);\n+  \/\/ Check for recursion:\n+  z_aghi(top, -oopSize);\n+  z_cg(obj, Address(Z_thread, top));\n+  z_bre(push);\n@@ -5730,2 +5740,3 @@\n-  \/\/ Clear lock-bits from hdr (locked state)\n-  z_xilf(temp, markWord::unlocked_value);\n+  \/\/ Check header for monitor (0b10).\n+  z_tmll(mark, markWord::monitor_value);\n+  branch_optimized(bcondNotAllZero, slow);\n@@ -5733,2 +5744,9 @@\n-  z_csg(hdr, temp, oopDesc::mark_offset_in_bytes(), obj);\n-  branch_optimized(Assembler::bcondNotEqual, slow_case);\n+  { \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+    const Register locked_obj = top;\n+    z_oill(mark, markWord::unlocked_value);\n+    z_lgr(locked_obj, mark);\n+    \/\/ Clear lock-bits from locked_obj (locked state)\n+    z_xilf(locked_obj, markWord::unlocked_value);\n+    z_csg(mark, locked_obj, mark_offset, obj);\n+    branch_optimized(Assembler::bcondNotEqual, slow);\n+  }\n@@ -5736,5 +5754,1 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  z_lgf(temp, Address(Z_thread, JavaThread::lock_stack_top_offset()));\n-  z_stg(obj, Address(Z_thread, temp));\n-  z_ahi(temp, oopSize);\n-  z_st(temp, Address(Z_thread, JavaThread::lock_stack_top_offset()));\n+  bind(push);\n@@ -5742,2 +5756,4 @@\n-  \/\/ as locking was successful, set CC to EQ\n-  z_cr(temp, temp);\n+  \/\/ After successful lock, push object on lock-stack\n+  z_lgf(top, Address(Z_thread, ls_top_offset));\n+  z_stg(obj, Address(Z_thread, top));\n+  z_alsi(in_bytes(ls_top_offset), Z_thread, oopSize);\n@@ -5747,3 +5763,0 @@\n-\/\/ Branches to slow upon failure.\n-\/\/ Falls through upon success.\n-\/\/\n@@ -5751,1 +5764,1 @@\n-\/\/ - hdr: the (pre-loaded) header of the object, will be destroyed\n+\/\/ - temp1, temp2: temporary registers, will be destroyed\n@@ -5753,1 +5766,1 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow) {\n+void MacroAssembler::lightweight_unlock(Register obj, Register temp1, Register temp2, Label& slow) {\n@@ -5756,1 +5769,7 @@\n-  assert_different_registers(obj, hdr, tmp);\n+  assert_different_registers(obj, temp1, temp2);\n+\n+  Label unlocked, push_and_slow;\n+  const Register mark          = temp1;\n+  const Register top           = temp2;\n+  const int mark_offset        = oopDesc::mark_offset_in_bytes();\n+  const ByteSize ls_top_offset = JavaThread::lock_stack_top_offset();\n@@ -5759,9 +5778,0 @@\n-  {\n-    \/\/ Check that hdr is lightweight-locked.\n-    Label hdr_ok;\n-    z_lgr(tmp, hdr);\n-    z_nill(tmp, markWord::lock_mask_in_place);\n-    z_bre(hdr_ok);\n-    stop(\"Header is not lightweight-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -5774,3 +5784,3 @@\n-    Label stack_ok;\n-    z_lgf(tmp, Address(Z_thread, JavaThread::lock_stack_top_offset()));\n-    compareU32_and_branch(tmp, (unsigned)LockStack::start_offset(), Assembler::bcondHigh, stack_ok);\n+    NearLabel stack_ok;\n+    z_lgf(top, Address(Z_thread, ls_top_offset));\n+    compareU32_and_branch(top, (unsigned)LockStack::start_offset(), bcondNotLow, stack_ok);\n@@ -5780,8 +5790,155 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    z_aghi(tmp, -oopSize);\n-    z_lg(tmp, Address(Z_thread, tmp));\n-    compare64_and_branch(tmp, obj, Assembler::bcondEqual, tos_ok);\n-    stop(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n+#endif \/\/ ASSERT\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  z_lgf(top, Address(Z_thread, ls_top_offset));\n+  z_aghi(top, -oopSize);\n+  z_cg(obj, Address(Z_thread, top));\n+  branch_optimized(bcondNotEqual, slow);\n+\n+  \/\/ pop object from lock-stack\n+#ifdef ASSERT\n+  const Register temp_top = temp1; \/\/ mark is not yet loaded, but be careful\n+  z_agrk(temp_top, top, Z_thread);\n+  z_xc(0, oopSize-1, temp_top, 0, temp_top);  \/\/ wipe out lock-stack entry\n+#endif \/\/ ASSERT\n+  z_alsi(in_bytes(ls_top_offset), Z_thread, -oopSize);  \/\/ pop object\n+\n+  \/\/ The underflow check is elided. The recursive check will always fail\n+  \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n+\n+  \/\/ Check if recursive. (this is a check for the 2nd object on the stack)\n+  z_aghi(top, -oopSize);\n+  z_cg(obj, Address(Z_thread, top));\n+  branch_optimized(bcondEqual, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  z_lg(mark, Address(obj, mark_offset));\n+  z_tmll(mark, markWord::monitor_value);\n+  z_brnaz(push_and_slow);\n+\n+#ifdef ASSERT\n+  \/\/ Check header not unlocked (0b01).\n+  NearLabel not_unlocked;\n+  z_tmll(mark, markWord::unlocked_value);\n+  z_braz(not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n+#endif \/\/ ASSERT\n+\n+  { \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    Register unlocked_obj = top;\n+    z_lgr(unlocked_obj, mark);\n+    z_oill(unlocked_obj, markWord::unlocked_value);\n+    z_csg(mark, unlocked_obj, mark_offset, obj);\n+    branch_optimized(Assembler::bcondEqual, unlocked);\n+  }\n+\n+  bind(push_and_slow);\n+\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  z_lgf(top, Address(Z_thread, ls_top_offset));\n+  DEBUG_ONLY(z_stg(obj, Address(Z_thread, top));)\n+  z_alsi(in_bytes(ls_top_offset), Z_thread, oopSize);\n+  \/\/ set CC to NE\n+  z_ltgr(obj, obj); \/\/ object shouldn't be null at this point\n+  branch_optimized(bcondAlways, slow);\n+\n+  bind(unlocked);\n+}\n+\n+void MacroAssembler::compiler_fast_lock_lightweight_object(Register obj, Register tmp1, Register tmp2) {\n+  assert_different_registers(obj, tmp1, tmp2);\n+\n+  \/\/ Handle inflated monitor.\n+  NearLabel inflated;\n+  \/\/ Finish fast lock successfully. MUST reach to with flag == NE\n+  NearLabel locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == EQ\n+  NearLabel slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(tmp1, obj);\n+    z_l(tmp1, Address(tmp1, Klass::access_flags_offset()));\n+    assert((JVM_ACC_IS_VALUE_BASED_CLASS & 0xFFFF) == 0, \"or change following instruction\");\n+    z_nilh(tmp1, JVM_ACC_IS_VALUE_BASED_CLASS >> 16);\n+    z_brne(slow_path);\n+  }\n+\n+  const Register mark          = tmp1;\n+  const int mark_offset        = oopDesc::mark_offset_in_bytes();\n+  const ByteSize ls_top_offset = JavaThread::lock_stack_top_offset();\n+\n+  BLOCK_COMMENT(\"compiler_fast_lightweight_locking {\");\n+  { \/\/ lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST reach to with flag == EQ\n+    NearLabel push;\n+\n+    const Register top = tmp2;\n+\n+    \/\/ Check if lock-stack is full.\n+    z_lgf(top, Address(Z_thread, ls_top_offset));\n+    compareU32_and_branch(top, (unsigned) LockStack::end_offset() - 1, bcondHigh, slow_path);\n+\n+    \/\/ The underflow check is elided. The recursive check will always fail\n+    \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n+\n+    \/\/ Check if recursive.\n+    z_aghi(top, -oopSize);\n+    z_cg(obj, Address(Z_thread, top));\n+    z_bre(push);\n+\n+    \/\/ Check for monitor (0b10)\n+    z_lg(mark, Address(obj, mark_offset));\n+    z_tmll(mark, markWord::monitor_value);\n+    z_brnaz(inflated);\n+\n+    \/\/ not inflated\n+\n+    { \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+      assert(mark_offset == 0, \"required to avoid a lea\");\n+      const Register locked_obj = top;\n+      z_oill(mark, markWord::unlocked_value);\n+      z_lgr(locked_obj, mark);\n+      \/\/ Clear lock-bits from locked_obj (locked state)\n+      z_xilf(locked_obj, markWord::unlocked_value);\n+      z_csg(mark, locked_obj, mark_offset, obj);\n+      branch_optimized(Assembler::bcondNotEqual, slow_path);\n+    }\n+\n+    bind(push);\n+\n+    \/\/ After successful lock, push object on lock-stack.\n+    z_lgf(top, Address(Z_thread, ls_top_offset));\n+    z_stg(obj, Address(Z_thread, top));\n+    z_alsi(in_bytes(ls_top_offset), Z_thread, oopSize);\n+\n+    z_cgr(obj, obj); \/\/ set the CC to EQ, as it could be changed by alsi\n+    z_bru(locked);\n+  }\n+  BLOCK_COMMENT(\"} compiler_fast_lightweight_locking\");\n+\n+  BLOCK_COMMENT(\"handle_inflated_monitor_lightweight_locking {\");\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register tagged_monitor = mark;\n+    const Register zero           = tmp2;\n+\n+    \/\/ Try to CAS m->owner from null to current thread.\n+    \/\/ If m->owner is null, then csg succeeds and sets m->owner=THREAD and CR=EQ.\n+    \/\/ Otherwise, register zero is filled with the current owner.\n+    z_lghi(zero, 0);\n+    z_csg(zero, Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), tagged_monitor);\n+    z_bre(locked);\n+\n+    \/\/ Check if recursive.\n+    z_cgr(Z_thread, zero); \/\/ zero contains the owner from z_csg instruction\n+    z_brne(slow_path);\n+\n+    \/\/ Recursive\n+    z_agsi(Address(tagged_monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 1ll);\n+    z_cgr(zero, zero);\n+    \/\/ z_bru(locked);\n+    \/\/ Uncomment above line in the future, for now jump address is right next to us.\n@@ -5789,0 +5946,18 @@\n+  BLOCK_COMMENT(\"} handle_inflated_monitor_lightweight_locking\");\n+\n+  bind(locked);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with flag == EQ.\n+  NearLabel flag_correct;\n+  z_bre(flag_correct);\n+  stop(\"CC is not set to EQ, it should be - lock\");\n+#endif \/\/ ASSERT\n+\n+  bind(slow_path);\n+\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with flag == NE.\n+  z_brne(flag_correct);\n+  stop(\"CC is not set to NE, it should be - lock\");\n+  bind(flag_correct);\n@@ -5791,4 +5966,5 @@\n-  z_lgr(tmp, hdr);\n-  z_oill(tmp, markWord::unlocked_value);\n-  z_csg(hdr, tmp, oopDesc::mark_offset_in_bytes(), obj);\n-  branch_optimized(Assembler::bcondNotEqual, slow);\n+  \/\/ C2 uses the value of flag (NE vs EQ) to determine the continuation.\n+}\n+\n+void MacroAssembler::compiler_fast_unlock_lightweight_object(Register obj, Register tmp1, Register tmp2) {\n+  assert_different_registers(obj, tmp1, tmp2);\n@@ -5796,1 +5972,23 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n+  \/\/ Handle inflated monitor.\n+  NearLabel inflated, inflated_load_monitor;\n+  \/\/ Finish fast unlock successfully. MUST reach to with flag == EQ.\n+  NearLabel unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE.\n+  NearLabel slow_path;\n+\n+  const Register mark          = tmp1;\n+  const Register top           = tmp2;\n+  const int mark_offset        = oopDesc::mark_offset_in_bytes();\n+  const ByteSize ls_top_offset = JavaThread::lock_stack_top_offset();\n+\n+  BLOCK_COMMENT(\"compiler_fast_lightweight_unlock {\");\n+  { \/\/ Lightweight Unlock\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    z_lgf(top, Address(Z_thread, ls_top_offset));\n+\n+    z_aghi(top, -oopSize);\n+    z_cg(obj, Address(Z_thread, top));\n+    branch_optimized(bcondNotEqual, inflated_load_monitor);\n+\n+    \/\/ Pop lock-stack.\n@@ -5798,4 +5996,3 @@\n-  z_lgf(tmp, Address(Z_thread, JavaThread::lock_stack_top_offset()));\n-  z_aghi(tmp, -oopSize);\n-  z_agr(tmp, Z_thread);\n-  z_xc(0, oopSize-1, tmp, 0, tmp);  \/\/ wipe out lock-stack entry\n+    const Register temp_top = tmp1; \/\/ let's not kill top here, we can use for recursive check\n+    z_agrk(temp_top, top, Z_thread);\n+    z_xc(0, oopSize-1, temp_top, 0, temp_top);  \/\/ wipe out lock-stack entry\n@@ -5803,2 +6000,128 @@\n-  z_alsi(in_bytes(JavaThread::lock_stack_top_offset()), Z_thread, -oopSize);  \/\/ pop object\n-  z_cr(tmp, tmp); \/\/ set CC to EQ\n+    z_alsi(in_bytes(ls_top_offset), Z_thread, -oopSize);  \/\/ pop object\n+\n+    \/\/ The underflow check is elided. The recursive check will always fail\n+    \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n+\n+    \/\/ Check if recursive.\n+    z_aghi(top, -oopSize);\n+    z_cg(obj, Address(Z_thread, top));\n+    z_bre(unlocked);\n+\n+    \/\/ Not recursive\n+\n+    \/\/ Check for monitor (0b10).\n+    z_lg(mark, Address(obj, mark_offset));\n+    z_tmll(mark, markWord::monitor_value);\n+    z_brnaz(inflated);\n+\n+#ifdef ASSERT\n+    \/\/ Check header not unlocked (0b01).\n+    NearLabel not_unlocked;\n+    z_tmll(mark, markWord::unlocked_value);\n+    z_braz(not_unlocked);\n+    stop(\"lightweight_unlock already unlocked\");\n+    bind(not_unlocked);\n+#endif \/\/ ASSERT\n+\n+    { \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+      Register unlocked_obj = top;\n+      z_lgr(unlocked_obj, mark);\n+      z_oill(unlocked_obj, markWord::unlocked_value);\n+      z_csg(mark, unlocked_obj, mark_offset, obj);\n+      branch_optimized(Assembler::bcondEqual, unlocked);\n+    }\n+\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    z_lgf(top, Address(Z_thread, ls_top_offset));\n+    DEBUG_ONLY(z_stg(obj, Address(Z_thread, top));)\n+    z_alsi(in_bytes(ls_top_offset), Z_thread, oopSize);\n+    \/\/ set CC to NE\n+    z_ltgr(obj, obj); \/\/ object is not null here\n+    z_bru(slow_path);\n+  }\n+  BLOCK_COMMENT(\"} compiler_fast_lightweight_unlock\");\n+\n+  { \/\/ Handle inflated monitor.\n+\n+    bind(inflated_load_monitor);\n+\n+    z_lg(mark, Address(obj, mark_offset));\n+\n+#ifdef ASSERT\n+    z_tmll(mark, markWord::monitor_value);\n+    z_brnaz(inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif \/\/ ASSERT\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    NearLabel check_done, loop;\n+    z_lgf(top, Address(Z_thread, ls_top_offset));\n+    bind(loop);\n+    z_aghi(top, -oopSize);\n+    compareU32_and_branch(top, in_bytes(JavaThread::lock_stack_base_offset()),\n+                          bcondLow, check_done);\n+    z_cg(obj, Address(Z_thread, top));\n+    z_brne(loop);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif \/\/ ASSERT\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register monitor = mark;\n+\n+    NearLabel not_recursive;\n+    const Register recursions = tmp2;\n+\n+    \/\/ Check if recursive.\n+    load_and_test_long(recursions, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n+    z_bre(not_recursive); \/\/ if 0 then jump, it's not recursive locking\n+\n+    \/\/ Recursive unlock\n+    z_agsi(Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), -1ll);\n+    z_cgr(monitor, monitor); \/\/ set the CC to EQUAL\n+    z_bru(unlocked);\n+\n+    bind(not_recursive);\n+\n+    NearLabel not_ok;\n+    \/\/ Check if the entry lists are empty.\n+    load_and_test_long(tmp2, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n+    z_brne(not_ok);\n+    load_and_test_long(tmp2, Address(monitor, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+    z_brne(not_ok);\n+\n+    z_release();\n+    z_stg(tmp2 \/*=0*\/, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor);\n+\n+    z_bru(unlocked); \/\/ CC = EQ here\n+\n+    bind(not_ok);\n+\n+    \/\/ The owner may be anonymous, and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    z_stg(Z_thread, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner), monitor);\n+    z_bru(slow_path); \/\/ CC = NE here\n+  }\n+\n+  bind(unlocked);\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with flag == EQ.\n+  NearLabel flag_correct;\n+  z_bre(flag_correct);\n+  stop(\"CC is not set to EQ, it should be - unlock\");\n+#endif \/\/ ASSERT\n+\n+  bind(slow_path);\n+\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with flag == NE.\n+  z_brne(flag_correct);\n+  stop(\"CC is not set to NE, it should be - unlock\");\n+  bind(flag_correct);\n+#endif \/\/ ASSERT\n+\n+  \/\/ C2 uses the value of flag (NE vs EQ) to determine the continuation.\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":408,"deletions":85,"binary":false,"changes":493,"status":"modified"},{"patch":"@@ -730,2 +730,4 @@\n-  void lightweight_lock(Register obj, Register hdr, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow);\n+  void lightweight_lock(Register obj, Register tmp1, Register tmp2, Label& slow);\n+  void lightweight_unlock(Register obj, Register tmp1, Register tmp2, Label& slow);\n+  void compiler_fast_lock_lightweight_object(Register obj, Register tmp1, Register tmp2);\n+  void compiler_fast_unlock_lightweight_object(Register obj, Register tmp1, Register tmp2);\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -9582,0 +9582,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -9592,0 +9593,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -9601,0 +9603,32 @@\n+instruct cmpFastLockLightweight(flagsReg pcc, iRegP_N2P oop, iRegP_N2P box, iRegP tmp1, iRegP tmp2) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set pcc (FastLock oop box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+  ins_cost(100);\n+  \/\/ TODO: s390 port size(VARIABLE_SIZE);\n+  format %{ \"FASTLOCK  $oop, $box; KILL Z_ARG4, Z_ARG5\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($oop$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n+    \/\/ If unlocking was successful, cc should indicate 'EQ'.\n+    \/\/ The compiler generates a branch to the runtime call to\n+    \/\/ _complete_monitor_unlocking_Java for the case where cc is 'NE'.\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n+instruct cmpFastUnlockLightweight(flagsReg pcc, iRegP_N2P oop, iRegP_N2P box, iRegP tmp1, iRegP tmp2) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set pcc (FastUnlock oop box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+  ins_cost(100);\n+  \/\/ TODO: s390 port size(FIXED_SIZE);\n+  format %{ \"FASTUNLOCK  $oop, $box; KILL Z_ARG4, Z_ARG5\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($oop$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n+    \/\/ If unlocking was successful, cc should indicate 'EQ'.\n+    \/\/ The compiler generates a branch to the runtime call to\n+    \/\/ _complete_monitor_unlocking_Java for the case where cc is 'NE'.\n+  %}\n+  ins_pipe(pipe_class_dummy);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -1714,2 +1714,7 @@\n-    \/\/ Fast_lock kills r_temp_1, r_temp_2.\n-    __ compiler_fast_lock_object(r_oop, r_box, r_tmp1, r_tmp2);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast_lock kills r_temp_1, r_temp_2.\n+      __ compiler_fast_lock_lightweight_object(r_oop, r_tmp1, r_tmp2);\n+    } else {\n+      \/\/ Fast_lock kills r_temp_1, r_temp_2.\n+      __ compiler_fast_lock_object(r_oop, r_box, r_tmp1, r_tmp2);\n+    }\n@@ -1913,2 +1918,7 @@\n-    \/\/ Fast_unlock kills r_tmp1, r_tmp2.\n-    __ compiler_fast_unlock_object(r_oop, r_box, r_tmp1, r_tmp2);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast_unlock kills r_tmp1, r_tmp2.\n+      __ compiler_fast_unlock_lightweight_object(r_oop, r_tmp1, r_tmp2);\n+    } else {\n+      \/\/ Fast_unlock kills r_tmp1, r_tmp2.\n+      __ compiler_fast_unlock_object(r_oop, r_box, r_tmp1, r_tmp2);\n+    }\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -416,0 +416,2 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n+\n","filename":"src\/hotspot\/cpu\/s390\/vm_version_s390.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}