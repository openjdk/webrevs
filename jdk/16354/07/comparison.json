{"files":[{"patch":"@@ -172,1 +172,0 @@\n-      case Op_LoadVectorGather:\n@@ -174,1 +173,0 @@\n-      case Op_LoadVectorGatherMasked:\n@@ -183,0 +181,6 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        if (UseSVE == 0 || is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -76,0 +76,5 @@\n+      case Op_LoadVectorGatherMasked:\n+        if (is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2848,0 +2848,7 @@\n+void Assembler::kxnorwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n@@ -3289,0 +3296,13 @@\n+\/\/ Move Unaligned EVEX enabled Vector (programmable : 8,16,32,64)\n+void Assembler::evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x7F, (0xC0 | encode));\n+}\n+\n@@ -13566,0 +13586,5 @@\n+void Assembler::bt(Register dst, Register src) {\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n+  emit_int24(0x0F, (unsigned char)0xA3, (encode | 0xC0));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1527,0 +1527,2 @@\n+  void kxnorwl(KRegister dst, KRegister src1, KRegister src2);\n+\n@@ -1736,0 +1738,1 @@\n+  void bt(Register dst, Register src);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1572,0 +1572,267 @@\n+void C2_MacroAssembler::vpackI2X(BasicType elem_bt, XMMRegister dst,\n+                                 XMMRegister ones, XMMRegister xtmp,\n+                                 int vlen_enc) {\n+  assert(VM_Version::supports_avx512vl(), \"\");\n+  if (elem_bt == T_SHORT) {\n+    evpmovdw(dst, dst, vlen_enc);\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    evpmovdb(dst, dst, vlen_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather_subword_avx3(BasicType elem_bt, XMMRegister dst, Register base, XMMRegister offset,\n+                                             Register idx_base, int idx_off, XMMRegister idx_vec, XMMRegister ones,\n+                                             XMMRegister xtmp, KRegister gmask, int vlen_enc) {\n+  if (elem_bt == T_SHORT) {\n+    evmovdquq(idx_vec, Address(idx_base, idx_off, Address::times_4), vlen_enc);\n+    if (offset != xnoreg) {\n+      vpaddd(idx_vec, idx_vec, offset, vlen_enc);\n+    }\n+    \/\/ Normalize the indices to multiple of 2.\n+    vpslld(xtmp, ones, 1, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    \/\/ Load double words from normalized indices.\n+    evpgatherdd(dst, gmask, Address(base, xtmp, Address::times_2), vlen_enc);\n+    \/\/ Compute bit level offset of actual short value with in each double word\n+    \/\/ lane.\n+    vpsrld(xtmp, ones, 31, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    vpslld(xtmp, xtmp, 4, vlen_enc);\n+    \/\/ Move the short value at respective bit offset to lower 16 bits of each\n+    \/\/ double word lane.\n+    vpsrlvd(dst, dst, xtmp, vlen_enc);\n+    \/\/ Pack double word vector into short vector.\n+    vpackI2X(T_SHORT, dst, ones, xtmp, vlen_enc);\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    evmovdquq(idx_vec, Address(idx_base, idx_off, Address::times_4), vlen_enc);\n+    if (offset != xnoreg) {\n+      vpaddd(idx_vec, idx_vec, offset, vlen_enc);\n+    }\n+    \/\/ Normalize the indices to multiple of 4.\n+    vpslld(xtmp, ones, 2, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    \/\/ Load double words from normalized indices.\n+    evpgatherdd(dst, gmask, Address(base, xtmp, Address::times_1), vlen_enc);\n+    \/\/ Compute bit level offset of actual byte value with in each double word\n+    \/\/ lane.\n+    vpsrld(xtmp, ones, 30, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    vpslld(xtmp, xtmp, 3, vlen_enc);\n+    \/\/ Move the byte value at respective bit offset to lower 8 bits of each\n+    \/\/ double word lane.\n+    vpsrlvd(dst, dst, xtmp, vlen_enc);\n+    \/\/ Pack double word vector into byte vector.\n+    vpackI2X(T_BYTE, dst, ones, xtmp, vlen_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather_subword_masked_avx3(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                                    Register offset, XMMRegister offset_vec, XMMRegister idx_vec,\n+                                                    XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, KRegister mask,\n+                                                    KRegister gmask, int vlen_enc, int vlen) {\n+  int shuf_mask[] = {0xFC, 0xF3, 0xCF, 0x3F};\n+  int lane_count_subwords = vlen;\n+  int lane_count_ints = MIN2(Matcher::max_vector_size(T_INT), vlen);\n+  Assembler::AvxVectorLen int_vec_enc =\n+      vector_length_encoding(lane_count_ints * type2aelembytes(T_INT));\n+\n+  if (offset != noreg) {\n+    evpbroadcastd(offset_vec, offset, int_vec_enc);\n+  }\n+  vpxor(dst, dst, dst, int_vec_enc);\n+  vallones(xtmp1, int_vec_enc);\n+\n+  if (elem_bt == T_BYTE) {\n+    \/\/ Loop to gather 8(64bit), 16(128bit), 32(256bit) or 64(512bit) bytes from\n+    \/\/ memory into vector using integral gather instructions. Number of loop\n+    \/\/ iterations depends on the maximum integral vector size supported by\n+    \/\/ target capped by the gather count i.e. in order to gather 8 bytes over\n+    \/\/ AVX-512 targets we need to use 256bit integer gather even though target\n+    \/\/ supports 512 bit integral gather operation.\n+    for (int i = 0, j = 0; i < lane_count_subwords; i += lane_count_ints, j++) {\n+      vpxor(xtmp2, xtmp2, xtmp2, int_vec_enc);\n+      kxnorwl(gmask, gmask, gmask);\n+      vgather_subword_avx3(elem_bt, xtmp2, base, offset_vec, idx_base, i,\n+                           idx_vec, xtmp1, xtmp3, gmask, int_vec_enc);\n+      if (vlen_enc == Assembler::AVX_512bit) {\n+        \/\/ Case to handle 64 byte gather operation.\n+        assert(int_vec_enc == Assembler::AVX_512bit, \"\");\n+        \/\/ Appropriately permute 128 bit lane holding 16 bytes accumulated using\n+        \/\/ 512 bit integral gather operation.\n+        if (j > 0) {\n+          vinserti32x4(dst, dst, xtmp2, j);\n+        } else {\n+          evmovdquq(dst, xtmp2, vlen_enc);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_256bit) {\n+        \/\/ Case to handle 32 byte gather operation.\n+        if (j > 0) {\n+          if (int_vec_enc == Assembler::AVX_512bit) {\n+            vinserti32x4(dst, dst, xtmp2, j);\n+          } else {\n+            assert(int_vec_enc == Assembler::AVX_256bit, \"\");\n+            \/\/ Permute 8 bytes loaded using 256 bit integral gather.\n+            vpermq(xtmp2, xtmp2, shuf_mask[j], vlen_enc);\n+            vpor(dst, dst, xtmp2, vlen_enc);\n+          }\n+        } else {\n+          vpor(dst, dst, xtmp2, vlen_enc);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_128bit) {\n+        \/\/ Case to handle 16 byte gather operation.\n+        if (j > 0) {\n+          \/\/ We enter here only if maximum integer vector size is less than 512\n+          \/\/ bits.\n+          if (int_vec_enc == Assembler::AVX_256bit) {\n+            vpermq(xtmp2, xtmp2, shuf_mask[j], int_vec_enc);\n+          } else {\n+            assert(int_vec_enc == Assembler::AVX_128bit, \"\");\n+            vpshufd(xtmp2, xtmp2, shuf_mask[j], vlen_enc);\n+          }\n+        }\n+        vpor(dst, dst, xtmp2, vlen_enc);\n+      }\n+    }\n+  } else {\n+    assert(elem_bt == T_SHORT, \"\");\n+    \/\/ Loop to gather 4(64bit), 8(128bit), 16(256bit) or 32(512bit) short values\n+    \/\/ from memory into vector using integral gather instruction.\n+    for (int i = 0, j = 0; i < lane_count_subwords; i += lane_count_ints, j++) {\n+      vpxor(xtmp2, xtmp2, xtmp2, int_vec_enc);\n+      kxnorwl(gmask, gmask, gmask);\n+      vgather_subword_avx3(elem_bt, xtmp2, base, offset_vec, idx_base, i,\n+                           idx_vec, xtmp1, xtmp3, gmask, int_vec_enc);\n+      if (vlen_enc == Assembler::AVX_512bit) {\n+        \/\/ Case to handle 32 byte gather operation.\n+        assert(int_vec_enc == Assembler::AVX_512bit, \"\");\n+        \/\/ Appropriately permute 256 bit lane holding 16 shorts accumulated\n+        \/\/ using 512 bit integral gather operation.\n+        if (j > 0) {\n+          vinserti64x4(dst, dst, xtmp2, j);\n+        } else {\n+          evmovdquq(dst, xtmp2, vlen_enc);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_256bit) {\n+        \/\/ Case to handle 16 byte gather operation.\n+        if (int_vec_enc == Assembler::AVX_512bit) {\n+          \/\/ All 16 short values are loaded in one short by 512 bit integral\n+          \/\/ gather.\n+          vmovdqu(dst, xtmp2);\n+        } else {\n+          assert(int_vec_enc == Assembler::AVX_256bit, \"\");\n+          \/\/ Permute 8 short values loaded using 256 bit integral gather.\n+          vinserti32x4(dst, dst, xtmp2, j);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_128bit) {\n+        if (j > 0) {\n+          \/\/ We enter here only if maximum integer vector size is less than 256\n+          \/\/ bits.\n+          assert(int_vec_enc == Assembler::AVX_128bit, \"\");\n+          vpslldq(xtmp2, xtmp2, 8, vlen_enc);\n+        }\n+        vpor(dst, dst, xtmp2, vlen_enc);\n+      }\n+    }\n+  }\n+\n+  if (mask != knoreg) {\n+    if (elem_bt == T_BYTE) {\n+      evmovdqub(dst, mask, dst, false, vlen_enc);\n+    } else {\n+      assert(elem_bt == T_SHORT, \"\");\n+      evmovdquw(dst, mask, dst, false, vlen_enc);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather8b(BasicType elem_bt, XMMRegister dst,\n+                                  Register base, Register idx_base,\n+                                  Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      movl(rtmp, Address(idx_base, i * 4));\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      movl(rtmp, Address(idx_base, i * 4));\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst,\n+                                         Register base, Register idx_base,\n+                                         Register offset, Register rtmp,\n+                                         int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      movl(rtmp, Address(idx_base, i * 4));\n+      addl(rtmp, offset);\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      movl(rtmp, Address(idx_base, i * 4));\n+      addl(rtmp, offset);\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+\/*\n+ * Gather loop first packs 4 short \/ 8 byte values from gather indices\n+ * into quadword lane and then permutes quadword lane into appropriate\n+ * location in destination vector. Following pseudo code describes the\n+ * algorithm in detail:-\n+ *\n+ * DST_VEC = ZERO_VEC\n+ * PERM_INDEX = {0, 1, 2, 3, 4, 5, 6, 7, 8..}\n+ * TWO_VEC = {2, 2, 2, 2, 2, 2, 2, 2..}\n+ * FOREACH_ITER:\n+ *     TMP_VEC_64 = PICK_SUB_WORDS_FROM_GATHER_INDICES\n+ *     TEMP_PERM_VEC = PERMUTE TMP_VEC_64 PERM_INDEX\n+ *     DST_VEC = DST_VEC OR TEMP_PERM_VEC\n+ *     PERM_INDEX = PERM_INDEX - TWO_VEC\n+ *\n+ * With each iteration permute index 0,1 holding assembled quadword\n+ * gets right shifted by two lane position.\n+ *\n+ *\/\n+void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,\n+                                        Register base, Register idx_base,\n+                                        Register offset, XMMRegister xtmp1,\n+                                        XMMRegister xtmp2, XMMRegister xtmp3,\n+                                        Register rtmp, Register midx,\n+                                        Register length, int vector_len,\n+                                        int vlen_enc) {\n+  assert(is_subword_type(elem_ty), \"\");\n+  Label GATHER8_LOOP;\n+  movl(length, vector_len);\n+  vpxor(xtmp1, xtmp1, xtmp1, vlen_enc);\n+  vpxor(dst, dst, dst, vlen_enc);\n+  vallones(xtmp2, vlen_enc);\n+  vpsubd(xtmp2, xtmp1, xtmp2, vlen_enc);\n+  vpslld(xtmp2, xtmp2, 1, vlen_enc);\n+  load_iota_indices(xtmp1, vector_len * type2aelembytes(elem_ty), T_INT);\n+  bind(GATHER8_LOOP);\n+  if (offset == noreg) {\n+    vgather8b(elem_ty, xtmp3, base, idx_base, rtmp, vlen_enc);\n+  } else {\n+    vgather8b_offset(elem_ty, xtmp3, base, idx_base, offset, rtmp, vlen_enc);\n+  }\n+  vpermd(xtmp3, xtmp1, xtmp3,\n+         vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n+  vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n+  vpor(dst, dst, xtmp3, vlen_enc);\n+  addptr(idx_base, 32 >> (type2aelembytes(elem_ty) - 1));\n+  subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n+  jcc(Assembler::notEqual, GATHER8_LOOP);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":267,"deletions":0,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -495,0 +495,19 @@\n+  void vpackI2X(BasicType elem_bt, XMMRegister dst, XMMRegister mask, XMMRegister xtmp, int vlen_enc);\n+\n+  void vgather_subword_avx3(BasicType elem_bt, XMMRegister dst, Register base, XMMRegister offset,\n+                            Register idx_base, int idx_off, XMMRegister idx_vec, XMMRegister ones,\n+                            XMMRegister xtmp, KRegister gmask, int vlen_enc);\n+\n+  void vgather_subword_masked_avx3(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                   Register offset, XMMRegister offset_vec, XMMRegister idx_vec, XMMRegister xtmp1,\n+                                   XMMRegister xtmp2, XMMRegister xtmp3, KRegister mask, KRegister ktmp, int vlen_enc, int vlen);\n+\n+  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n+                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, Register midx,\n+                       Register length, int vector_len, int vlen_enc);\n+\n+  void vgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base, Register rtmp, int vlen_enc);\n+\n+  void vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                              Register offset, Register rtmp, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -217,0 +217,3 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        return is_subword_type(ety) ? 70 : 0;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1572,0 +1572,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -1916,0 +1917,7 @@\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) && (size_in_bits > 256 && !VM_Version::supports_avx512bw())) {\n+        return false;\n+      }\n+      break;\n@@ -1925,1 +1933,1 @@\n-      if (size_in_bits == 64 ) {\n+      if (!is_subword_type(bt) && size_in_bits == 64 ) {\n@@ -4048,1 +4056,1 @@\n-\/\/ Gather INT, LONG, FLOAT, DOUBLE\n+\/\/ Gather BYTE, SHORT, INT, LONG, FLOAT, DOUBLE\n@@ -4051,1 +4059,2 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && !is_subword_type(Matcher::vector_element_basic_type(n)) &&\n+            Matcher::vector_length_in_bytes(n) <= 32);\n@@ -4056,2 +4065,0 @@\n-    assert(UseAVX >= 2, \"sanity\");\n-\n@@ -4060,2 +4067,0 @@\n-\n-    assert(Matcher::vector_length_in_bytes(this) >= 16, \"sanity\");\n@@ -4063,6 +4068,1 @@\n-\n-    if (vlen_enc == Assembler::AVX_128bit) {\n-      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n-    } else {\n-      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n-    }\n+    __ vpcmpeqd($mask$$XMMRegister, $mask$$XMMRegister, $mask$$XMMRegister, vlen_enc);\n@@ -4075,0 +4075,1 @@\n+\n@@ -4076,1 +4077,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4081,2 +4083,0 @@\n-    assert(UseAVX > 2, \"sanity\");\n-\n@@ -4085,4 +4085,1 @@\n-\n-    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n-\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n+    __ kxnorwl($ktmp$$KRegister, $ktmp$$KRegister, $ktmp$$KRegister);\n@@ -4096,0 +4093,2 @@\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4113,0 +4112,136 @@\n+\n+instruct vgather_subword_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && VM_Version::supports_avx512vl());\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2 and  $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    uint vlen = Matcher::vector_length(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, xnoreg, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, knoreg, $ktmp$$KRegister, vlen_enc, vlen);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subword_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec offset_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && VM_Version::supports_avx512vl());\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP offset_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx, $offset\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    uint vlen = Matcher::vector_length(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $offset_vec$$XMMRegister, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, knoreg, $ktmp$$KRegister, vlen_enc, vlen);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subword_mask_avx3(vec dst, memory mem, rRegP idx, kReg mask, immI_0 offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && n->in(MemNode::ValueIn)->in(2)->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx, $offset\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    uint vlen = Matcher::vector_length(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, xnoreg, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $mask$$KRegister, $ktmp$$KRegister, vlen_enc, vlen);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subword_mask_off_avx3(vec dst, memory mem, rRegP idx, kReg mask, rRegI offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec offset_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && n->in(MemNode::ValueIn)->in(2)->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP offset_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    uint vlen = Matcher::vector_length(this);\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $offset_vec$$XMMRegister, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $mask$$KRegister, $ktmp$$KRegister, vlen_enc, vlen);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordLE8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordGT8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordLE8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP tmp, TEMP rtmp, KILL cr);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vgather_subwordGT8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":155,"deletions":20,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -1013,0 +1013,2 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2481,0 +2481,7 @@\n+    case Op_LoadVectorGather:\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn+1);\n+      }\n+      break;\n@@ -2482,0 +2489,8 @@\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair2 = new BinaryNode(n->in(MemNode::ValueIn + 1), n->in(MemNode::ValueIn + 2));\n+        Node* pair1 = new BinaryNode(n->in(MemNode::ValueIn), pair2);\n+        n->set_req(MemNode::ValueIn, pair1);\n+        n->del_req(MemNode::ValueIn+2);\n+        n->del_req(MemNode::ValueIn+1);\n+        break;\n+      } \/\/ fall-through\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1503,2 +1503,2 @@\n-    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n-                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+    VectorMaskUseType mask = (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred);\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt, mask)) {\n@@ -1525,1 +1525,2 @@\n-  if (!arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n+  \/\/ For sub-word gathers expander receive index array.\n+  if (!is_subword_type(elem_bt) && !arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n@@ -1567,0 +1568,1 @@\n+  Node* index_vect = nullptr;\n@@ -1568,5 +1570,7 @@\n-  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n-  if (index_vect == nullptr) {\n-    set_map(old_map);\n-    set_sp(old_sp);\n-    return false;\n+  if (!is_subword_type(elem_bt)) {\n+    index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n+    if (index_vect == nullptr) {\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n@@ -1611,0 +1615,3 @@\n+    Node* index    = argument(11);\n+    Node* indexMap = argument(12);\n+    Node* indexM   = argument(13);\n@@ -1612,1 +1619,6 @@\n-      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+      if (is_subword_type(elem_bt)) {\n+        Node* index_arr_base = array_element_address(indexMap, indexM, T_INT);\n+        vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_arr_base, mask, index));\n+      } else {\n+        vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+      }\n@@ -1614,1 +1626,6 @@\n-      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+      if (is_subword_type(elem_bt)) {\n+        Node* index_arr_base = array_element_address(indexMap, indexM, T_INT);\n+        vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_arr_base, index));\n+      } else {\n+        vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+      }\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":27,"deletions":10,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -880,1 +880,1 @@\n-  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices)\n+  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* offset = nullptr)\n@@ -883,1 +883,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n@@ -885,1 +884,7 @@\n-    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that last input is in MemNode::ValueIn\");\n+    DEBUG_ONLY(bool is_subword = is_subword_type(vt->element_basic_type()));\n+    assert(is_subword || indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(is_subword || !offset, \"\");\n+    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that index input is in MemNode::ValueIn\");\n+    if (offset) {\n+      add_req(offset);\n+    }\n@@ -889,1 +894,6 @@\n-  virtual uint match_edge(uint idx) const { return idx == MemNode::Address || idx == MemNode::ValueIn; }\n+  virtual uint match_edge(uint idx) const {\n+     return idx == MemNode::Address ||\n+            idx == MemNode::ValueIn ||\n+            ((is_subword_type(vect_type()->element_basic_type())) &&\n+              idx == MemNode::ValueIn + 1);\n+  }\n@@ -984,1 +994,1 @@\n-  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask, Node* offset = nullptr)\n@@ -987,2 +997,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n-    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -992,0 +1000,3 @@\n+    if (is_subword_type(vt->element_basic_type())) {\n+      add_req(offset);\n+    }\n@@ -997,1 +1008,3 @@\n-                                                   idx == MemNode::ValueIn + 1; }\n+                                                   idx == MemNode::ValueIn + 1 ||\n+                                                   (is_subword_type(vect_type()->is_vect()->element_basic_type()) &&\n+                                                   idx == MemNode::ValueIn + 2); }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":21,"deletions":8,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -896,0 +896,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, indexMap, mapOffset, (Byte128Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -928,0 +928,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, indexMap, mapOffset, (Byte256Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -992,0 +992,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, indexMap, mapOffset, (Byte512Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -880,0 +880,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, indexMap, mapOffset, (Byte64Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -866,0 +866,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, indexMap, mapOffset, (ByteMaxMask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3052,1 +3052,29 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, byte.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3097,2 +3125,7 @@\n-        ByteSpecies vsp = (ByteSpecies) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        if (m.allTrue()) {\n+            return fromArray(species, a, offset, indexMap, mapOffset);\n+        }\n+        else {\n+            ByteSpecies vsp = (ByteSpecies) species;\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n+        }\n@@ -3763,0 +3796,43 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, byte.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":79,"deletions":3,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -880,0 +880,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, indexMap, mapOffset, (Short128Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -896,0 +896,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, indexMap, mapOffset, (Short256Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -928,0 +928,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, indexMap, mapOffset, (Short512Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -872,0 +872,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, indexMap, mapOffset, (Short64Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -866,0 +866,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, indexMap, mapOffset, (ShortMaxMask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3053,1 +3053,29 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, short.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3098,2 +3126,7 @@\n-        ShortSpecies vsp = (ShortSpecies) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        if (m.allTrue()) {\n+            return fromArray(species, a, offset, indexMap, mapOffset);\n+        }\n+        else {\n+            ShortSpecies vsp = (ShortSpecies) species;\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n+        }\n@@ -3749,0 +3782,43 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, short.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":79,"deletions":3,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -3625,1 +3625,29 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3717,11 +3745,0 @@\n-#if[byteOrShort]\n-    @ForceInline\n-    public static\n-    $abstractvectortype$ fromArray(VectorSpecies<$Boxtype$> species,\n-                                   $type$[] a, int offset,\n-                                   int[] indexMap, int mapOffset,\n-                                   VectorMask<$Boxtype$> m) {\n-        $Type$Species vsp = ($Type$Species) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n-    }\n-#else[byteOrShort]\n@@ -3742,1 +3759,0 @@\n-#end[byteOrShort]\n@@ -4796,1 +4812,0 @@\n-#if[!byteOrShort]\n@@ -4802,0 +4817,40 @@\n+#if[byteOrShort]\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#else[byteOrShort]\n@@ -4855,1 +4910,1 @@\n-#end[!byteOrShort]\n+#end[byteOrShort]\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":70,"deletions":15,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -1154,1 +1154,0 @@\n-#if[!byteOrShort]\n@@ -1161,1 +1160,0 @@\n-#end[!byteOrShort]\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,357 @@\n+\/*\n+ *  Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+\n+package org.openjdk.bench.jdk.incubator.vector;\n+\n+import jdk.incubator.vector.*;\n+import java.util.Random;\n+import java.util.stream.IntStream;\n+import java.util.concurrent.TimeUnit;\n+import org.openjdk.jmh.annotations.*;\n+\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+@State(Scope.Thread)\n+@Fork(jvmArgsPrepend = {\"--add-modules=jdk.incubator.vector\"})\n+public class GatherOperationsBenchmark {\n+    @Param({\"64\", \"256\", \"1024\", \"4096\"})\n+    int SIZE;\n+    byte  [] barr;\n+    byte  [] bres;\n+    short [] sarr;\n+    short [] sres;\n+    int   [] index;\n+\n+    static final VectorSpecies<Short> S64 = ShortVector.SPECIES_64;\n+    static final VectorSpecies<Short> S128 = ShortVector.SPECIES_128;\n+    static final VectorSpecies<Short> S256 = ShortVector.SPECIES_256;\n+    static final VectorSpecies<Short> S512 = ShortVector.SPECIES_512;\n+    static final VectorSpecies<Byte> B64 = ByteVector.SPECIES_64;\n+    static final VectorSpecies<Byte> B128 = ByteVector.SPECIES_128;\n+    static final VectorSpecies<Byte> B256 = ByteVector.SPECIES_256;\n+    static final VectorSpecies<Byte> B512 = ByteVector.SPECIES_512;\n+\n+    @Setup(Level.Trial)\n+    public void BmSetup() {\n+        Random r = new Random(1245);\n+        index = new int[SIZE];\n+        barr = new byte[SIZE];\n+        bres = new byte[SIZE];\n+        sarr = new short[SIZE];\n+        sres = new short[SIZE];\n+        for (int i = 0; i < SIZE; i++) {\n+           barr[i] = (byte)i;\n+           sarr[i] = (short)i;\n+           index[i] = r.nextInt(SIZE-1);\n+        }\n+    }\n+\n+\n+\n+    @Benchmark\n+    public void microByteGather64() {\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather64_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather64_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather64_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather128() {\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather128_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather128_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather128_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather256() {\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather256_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather256_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather256_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather512() {\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather512_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather512_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather512_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather64() {\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather64_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather64_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather64_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather128() {\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather128_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather128_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather128_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather256() {\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather256_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather256_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather256_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather512() {\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather512_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather512_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather512_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/GatherOperationsBenchmark.java","additions":357,"deletions":0,"binary":false,"changes":357,"status":"added"}]}