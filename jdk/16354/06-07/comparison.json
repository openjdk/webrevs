{"files":[{"patch":"@@ -3296,0 +3296,13 @@\n+\/\/ Move Unaligned EVEX enabled Vector (programmable : 8,16,32,64)\n+void Assembler::evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_avx512vlbw(), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x7F, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1572,4 +1572,4 @@\n-#ifdef _LP64\n-void C2_MacroAssembler::vgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                                          Register mask, Register midx, Register rtmp, int vlen_enc) {\n-  vpxor(dst, dst, dst, vlen_enc);\n+void C2_MacroAssembler::vpackI2X(BasicType elem_bt, XMMRegister dst,\n+                                 XMMRegister ones, XMMRegister xtmp,\n+                                 int vlen_enc) {\n+  assert(VM_Version::supports_avx512vl(), \"\");\n@@ -1577,10 +1577,1 @@\n-    Label case0, case1, case2, case3;\n-    Label* larr[] = { &case0, &case1, &case2, &case3 };\n-    for (int i = 0; i < 4; i++) {\n-      bt(mask, midx);\n-      jccb(Assembler::carryClear, *larr[i]);\n-      movl(rtmp, Address(idx_base, i*4));\n-      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n-      bind(*larr[i]);\n-      incq(midx);\n-    }\n+    evpmovdw(dst, dst, vlen_enc);\n@@ -1589,10 +1580,1 @@\n-    Label case0, case1, case2, case3, case4, case5, case6, case7;\n-    Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };\n-    for (int i = 0; i < 8; i++) {\n-      bt(mask, midx);\n-      jccb(Assembler::carryClear, *larr[i]);\n-      movl(rtmp, Address(idx_base, i*4));\n-      pinsrb(dst, Address(base, rtmp), i);\n-      bind(*larr[i]);\n-      incq(midx);\n-    }\n+    evpmovdb(dst, dst, vlen_enc);\n@@ -1602,3 +1584,3 @@\n-void C2_MacroAssembler::vgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                                                 Register offset, Register mask, Register midx, Register rtmp, int vlen_enc) {\n-  vpxor(dst, dst, dst, vlen_enc);\n+void C2_MacroAssembler::vgather_subword_avx3(BasicType elem_bt, XMMRegister dst, Register base, XMMRegister offset,\n+                                             Register idx_base, int idx_off, XMMRegister idx_vec, XMMRegister ones,\n+                                             XMMRegister xtmp, KRegister gmask, int vlen_enc) {\n@@ -1606,10 +1588,3 @@\n-    Label case0, case1, case2, case3;\n-    Label* larr[] = { &case0, &case1, &case2, &case3 };\n-    for (int i = 0; i < 4; i++) {\n-      bt(mask, midx);\n-      jccb(Assembler::carryClear, *larr[i]);\n-      movl(rtmp, Address(idx_base, i*4));\n-      addl(rtmp, offset);\n-      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n-      bind(*larr[i]);\n-      incq(midx);\n+    evmovdquq(idx_vec, Address(idx_base, idx_off, Address::times_4), vlen_enc);\n+    if (offset != xnoreg) {\n+      vpaddd(idx_vec, idx_vec, offset, vlen_enc);\n@@ -1617,0 +1592,15 @@\n+    \/\/ Normalize the indices to multiple of 2.\n+    vpslld(xtmp, ones, 1, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    \/\/ Load double words from normalized indices.\n+    evpgatherdd(dst, gmask, Address(base, xtmp, Address::times_2), vlen_enc);\n+    \/\/ Compute bit level offset of actual short value with in each double word\n+    \/\/ lane.\n+    vpsrld(xtmp, ones, 31, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    vpslld(xtmp, xtmp, 4, vlen_enc);\n+    \/\/ Move the short value at respective bit offset to lower 16 bits of each\n+    \/\/ double word lane.\n+    vpsrlvd(dst, dst, xtmp, vlen_enc);\n+    \/\/ Pack double word vector into short vector.\n+    vpackI2X(T_SHORT, dst, ones, xtmp, vlen_enc);\n@@ -1619,10 +1609,3 @@\n-    Label case0, case1, case2, case3, case4, case5, case6, case7;\n-    Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };\n-    for (int i = 0; i < 8; i++) {\n-      bt(mask, midx);\n-      jccb(Assembler::carryClear, *larr[i]);\n-      movl(rtmp, Address(idx_base, i*4));\n-      addl(rtmp, offset);\n-      pinsrb(dst, Address(base, rtmp), i);\n-      bind(*larr[i]);\n-      incq(midx);\n+    evmovdquq(idx_vec, Address(idx_base, idx_off, Address::times_4), vlen_enc);\n+    if (offset != xnoreg) {\n+      vpaddd(idx_vec, idx_vec, offset, vlen_enc);\n@@ -1630,0 +1613,15 @@\n+    \/\/ Normalize the indices to multiple of 4.\n+    vpslld(xtmp, ones, 2, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    \/\/ Load double words from normalized indices.\n+    evpgatherdd(dst, gmask, Address(base, xtmp, Address::times_1), vlen_enc);\n+    \/\/ Compute bit level offset of actual byte value with in each double word\n+    \/\/ lane.\n+    vpsrld(xtmp, ones, 30, vlen_enc);\n+    vpand(xtmp, idx_vec, xtmp, vlen_enc);\n+    vpslld(xtmp, xtmp, 3, vlen_enc);\n+    \/\/ Move the byte value at respective bit offset to lower 8 bits of each\n+    \/\/ double word lane.\n+    vpsrlvd(dst, dst, xtmp, vlen_enc);\n+    \/\/ Pack double word vector into byte vector.\n+    vpackI2X(T_BYTE, dst, ones, xtmp, vlen_enc);\n@@ -1632,1 +1630,0 @@\n-#endif \/\/ _LP64\n@@ -1634,1 +1631,122 @@\n-void C2_MacroAssembler::vgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base, Register rtmp, int vlen_enc) {\n+void C2_MacroAssembler::vgather_subword_masked_avx3(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                                    Register offset, XMMRegister offset_vec, XMMRegister idx_vec,\n+                                                    XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, KRegister mask,\n+                                                    KRegister gmask, int vlen_enc, int vlen) {\n+  int shuf_mask[] = {0xFC, 0xF3, 0xCF, 0x3F};\n+  int lane_count_subwords = vlen;\n+  int lane_count_ints = MIN2(Matcher::max_vector_size(T_INT), vlen);\n+  Assembler::AvxVectorLen int_vec_enc =\n+      vector_length_encoding(lane_count_ints * type2aelembytes(T_INT));\n+\n+  if (offset != noreg) {\n+    evpbroadcastd(offset_vec, offset, int_vec_enc);\n+  }\n+  vpxor(dst, dst, dst, int_vec_enc);\n+  vallones(xtmp1, int_vec_enc);\n+\n+  if (elem_bt == T_BYTE) {\n+    \/\/ Loop to gather 8(64bit), 16(128bit), 32(256bit) or 64(512bit) bytes from\n+    \/\/ memory into vector using integral gather instructions. Number of loop\n+    \/\/ iterations depends on the maximum integral vector size supported by\n+    \/\/ target capped by the gather count i.e. in order to gather 8 bytes over\n+    \/\/ AVX-512 targets we need to use 256bit integer gather even though target\n+    \/\/ supports 512 bit integral gather operation.\n+    for (int i = 0, j = 0; i < lane_count_subwords; i += lane_count_ints, j++) {\n+      vpxor(xtmp2, xtmp2, xtmp2, int_vec_enc);\n+      kxnorwl(gmask, gmask, gmask);\n+      vgather_subword_avx3(elem_bt, xtmp2, base, offset_vec, idx_base, i,\n+                           idx_vec, xtmp1, xtmp3, gmask, int_vec_enc);\n+      if (vlen_enc == Assembler::AVX_512bit) {\n+        \/\/ Case to handle 64 byte gather operation.\n+        assert(int_vec_enc == Assembler::AVX_512bit, \"\");\n+        \/\/ Appropriately permute 128 bit lane holding 16 bytes accumulated using\n+        \/\/ 512 bit integral gather operation.\n+        if (j > 0) {\n+          vinserti32x4(dst, dst, xtmp2, j);\n+        } else {\n+          evmovdquq(dst, xtmp2, vlen_enc);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_256bit) {\n+        \/\/ Case to handle 32 byte gather operation.\n+        if (j > 0) {\n+          if (int_vec_enc == Assembler::AVX_512bit) {\n+            vinserti32x4(dst, dst, xtmp2, j);\n+          } else {\n+            assert(int_vec_enc == Assembler::AVX_256bit, \"\");\n+            \/\/ Permute 8 bytes loaded using 256 bit integral gather.\n+            vpermq(xtmp2, xtmp2, shuf_mask[j], vlen_enc);\n+            vpor(dst, dst, xtmp2, vlen_enc);\n+          }\n+        } else {\n+          vpor(dst, dst, xtmp2, vlen_enc);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_128bit) {\n+        \/\/ Case to handle 16 byte gather operation.\n+        if (j > 0) {\n+          \/\/ We enter here only if maximum integer vector size is less than 512\n+          \/\/ bits.\n+          if (int_vec_enc == Assembler::AVX_256bit) {\n+            vpermq(xtmp2, xtmp2, shuf_mask[j], int_vec_enc);\n+          } else {\n+            assert(int_vec_enc == Assembler::AVX_128bit, \"\");\n+            vpshufd(xtmp2, xtmp2, shuf_mask[j], vlen_enc);\n+          }\n+        }\n+        vpor(dst, dst, xtmp2, vlen_enc);\n+      }\n+    }\n+  } else {\n+    assert(elem_bt == T_SHORT, \"\");\n+    \/\/ Loop to gather 4(64bit), 8(128bit), 16(256bit) or 32(512bit) short values\n+    \/\/ from memory into vector using integral gather instruction.\n+    for (int i = 0, j = 0; i < lane_count_subwords; i += lane_count_ints, j++) {\n+      vpxor(xtmp2, xtmp2, xtmp2, int_vec_enc);\n+      kxnorwl(gmask, gmask, gmask);\n+      vgather_subword_avx3(elem_bt, xtmp2, base, offset_vec, idx_base, i,\n+                           idx_vec, xtmp1, xtmp3, gmask, int_vec_enc);\n+      if (vlen_enc == Assembler::AVX_512bit) {\n+        \/\/ Case to handle 32 byte gather operation.\n+        assert(int_vec_enc == Assembler::AVX_512bit, \"\");\n+        \/\/ Appropriately permute 256 bit lane holding 16 shorts accumulated\n+        \/\/ using 512 bit integral gather operation.\n+        if (j > 0) {\n+          vinserti64x4(dst, dst, xtmp2, j);\n+        } else {\n+          evmovdquq(dst, xtmp2, vlen_enc);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_256bit) {\n+        \/\/ Case to handle 16 byte gather operation.\n+        if (int_vec_enc == Assembler::AVX_512bit) {\n+          \/\/ All 16 short values are loaded in one short by 512 bit integral\n+          \/\/ gather.\n+          vmovdqu(dst, xtmp2);\n+        } else {\n+          assert(int_vec_enc == Assembler::AVX_256bit, \"\");\n+          \/\/ Permute 8 short values loaded using 256 bit integral gather.\n+          vinserti32x4(dst, dst, xtmp2, j);\n+        }\n+      } else if (vlen_enc == Assembler::AVX_128bit) {\n+        if (j > 0) {\n+          \/\/ We enter here only if maximum integer vector size is less than 256\n+          \/\/ bits.\n+          assert(int_vec_enc == Assembler::AVX_128bit, \"\");\n+          vpslldq(xtmp2, xtmp2, 8, vlen_enc);\n+        }\n+        vpor(dst, dst, xtmp2, vlen_enc);\n+      }\n+    }\n+  }\n+\n+  if (mask != knoreg) {\n+    if (elem_bt == T_BYTE) {\n+      evmovdqub(dst, mask, dst, false, vlen_enc);\n+    } else {\n+      assert(elem_bt == T_SHORT, \"\");\n+      evmovdquw(dst, mask, dst, false, vlen_enc);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather8b(BasicType elem_bt, XMMRegister dst,\n+                                  Register base, Register idx_base,\n+                                  Register rtmp, int vlen_enc) {\n@@ -1638,1 +1756,1 @@\n-      movl(rtmp, Address(idx_base, i*4));\n+      movl(rtmp, Address(idx_base, i * 4));\n@@ -1644,1 +1762,1 @@\n-      movl(rtmp, Address(idx_base, i*4));\n+      movl(rtmp, Address(idx_base, i * 4));\n@@ -1650,2 +1768,4 @@\n-void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                                          Register offset, Register rtmp, int vlen_enc) {\n+void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst,\n+                                         Register base, Register idx_base,\n+                                         Register offset, Register rtmp,\n+                                         int vlen_enc) {\n@@ -1655,1 +1775,1 @@\n-      movl(rtmp, Address(idx_base, i*4));\n+      movl(rtmp, Address(idx_base, i * 4));\n@@ -1662,1 +1782,1 @@\n-      movl(rtmp, Address(idx_base, i*4));\n+      movl(rtmp, Address(idx_base, i * 4));\n@@ -1688,3 +1808,7 @@\n-void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base,\n-                                        Register offset, Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n-                                        Register rtmp, Register midx, Register length, int vector_len, int vlen_enc) {\n+void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,\n+                                        Register base, Register idx_base,\n+                                        Register offset, XMMRegister xtmp1,\n+                                        XMMRegister xtmp2, XMMRegister xtmp3,\n+                                        Register rtmp, Register midx,\n+                                        Register length, int vector_len,\n+                                        int vlen_enc) {\n@@ -1697,1 +1821,1 @@\n-  vpsubd(xtmp2, xtmp1, xtmp2 ,vlen_enc);\n+  vpsubd(xtmp2, xtmp1, xtmp2, vlen_enc);\n@@ -1701,19 +1825,12 @@\n-    if (offset == noreg) {\n-      if (mask == noreg) {\n-        vgather8b(elem_ty, xtmp3, base, idx_base, rtmp, vlen_enc);\n-      } else {\n-        LP64_ONLY(vgather8b_masked(elem_ty, xtmp3, base, idx_base, mask, midx, rtmp, vlen_enc));\n-      }\n-    } else {\n-      if (mask == noreg) {\n-        vgather8b_offset(elem_ty, xtmp3, base, idx_base, offset, rtmp, vlen_enc);\n-      } else {\n-        LP64_ONLY(vgather8b_masked_offset(elem_ty, xtmp3, base, idx_base, offset, mask, midx, rtmp, vlen_enc));\n-      }\n-    }\n-    vpermd(xtmp3, xtmp1, xtmp3, vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n-    vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n-    vpor(dst, dst, xtmp3, vlen_enc);\n-    addptr(idx_base,  32 >> (type2aelembytes(elem_ty) - 1));\n-    subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n-    jcc(Assembler::notEqual, GATHER8_LOOP);\n+  if (offset == noreg) {\n+    vgather8b(elem_ty, xtmp3, base, idx_base, rtmp, vlen_enc);\n+  } else {\n+    vgather8b_offset(elem_ty, xtmp3, base, idx_base, offset, rtmp, vlen_enc);\n+  }\n+  vpermd(xtmp3, xtmp1, xtmp3,\n+         vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n+  vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n+  vpor(dst, dst, xtmp3, vlen_enc);\n+  addptr(idx_base, 32 >> (type2aelembytes(elem_ty) - 1));\n+  subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n+  jcc(Assembler::notEqual, GATHER8_LOOP);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":195,"deletions":78,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -495,3 +495,1 @@\n-  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n-                       Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                       Register midx, Register length, int vector_len, int vlen_enc);\n+  void vpackI2X(BasicType elem_bt, XMMRegister dst, XMMRegister mask, XMMRegister xtmp, int vlen_enc);\n@@ -499,3 +497,3 @@\n-#ifdef _LP64\n-  void vgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                         Register mask, Register midx, Register rtmp, int vlen_enc);\n+  void vgather_subword_avx3(BasicType elem_bt, XMMRegister dst, Register base, XMMRegister offset,\n+                            Register idx_base, int idx_off, XMMRegister idx_vec, XMMRegister ones,\n+                            XMMRegister xtmp, KRegister gmask, int vlen_enc);\n@@ -503,3 +501,7 @@\n-  void vgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                                     Register offset, Register mask, Register midx, Register rtmp, int vlen_enc);\n-#endif\n+  void vgather_subword_masked_avx3(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                   Register offset, XMMRegister offset_vec, XMMRegister idx_vec, XMMRegister xtmp1,\n+                                   XMMRegister xtmp2, XMMRegister xtmp3, KRegister mask, KRegister ktmp, int vlen_enc, int vlen);\n+\n+  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n+                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, Register midx,\n+                       Register length, int vector_len, int vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":11,"deletions":9,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -219,1 +219,1 @@\n-        return is_subword_type(ety) ? 50 : 0;\n+        return is_subword_type(ety) ? 70 : 0;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1920,1 +1920,1 @@\n-      if (is_subword_type(bt) && (!is_LP64 || (size_in_bits > 256 && !VM_Version::supports_avx512bw()))) {\n+      if (is_subword_type(bt) && (size_in_bits > 256 && !VM_Version::supports_avx512bw())) {\n@@ -4113,2 +4113,2 @@\n-instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+instruct vgather_subword_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && VM_Version::supports_avx512vl());\n@@ -4116,2 +4116,2 @@\n-  effect(TEMP tmp, TEMP rtmp);\n-  format %{ \"vector_gatherLE8 $dst, $mem, $idx\\t! using $tmp and $rtmp as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2 and  $xtmp3 as TEMP\" %}\n@@ -4119,0 +4119,1 @@\n+    uint vlen = Matcher::vector_length(this);\n@@ -4122,1 +4123,3 @@\n-    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, xnoreg, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, knoreg, $ktmp$$KRegister, vlen_enc, vlen);\n@@ -4127,3 +4130,2 @@\n-instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n-                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+instruct vgather_subword_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec offset_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && VM_Version::supports_avx512vl());\n@@ -4131,2 +4133,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n-  format %{ \"vector_gatherGT8 $dst, $mem, $idx\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP offset_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx, $offset\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP\" %}\n@@ -4134,0 +4136,1 @@\n+    uint vlen = Matcher::vector_length(this);\n@@ -4135,1 +4138,0 @@\n-    int vector_len = Matcher::vector_length(this);\n@@ -4137,1 +4139,0 @@\n-    __ lea($tmp$$Register, $mem$$Address);\n@@ -4139,2 +4140,3 @@\n-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,\n-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $offset_vec$$XMMRegister, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, knoreg, $ktmp$$KRegister, vlen_enc, vlen);\n@@ -4145,6 +4147,5 @@\n-instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{\n-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n-  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n-  ins_cost(200);\n-  effect(TEMP tmp, TEMP rtmp, KILL cr);\n-  format %{ \"vector_gatherLE8 $dst, $mem, $idx, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n+instruct vgather_subword_mask_avx3(vec dst, memory mem, rRegP idx, kReg mask, immI_0 offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && n->in(MemNode::ValueIn)->in(2)->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx, $offset\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP\" %}\n@@ -4152,0 +4153,1 @@\n+    uint vlen = Matcher::vector_length(this);\n@@ -4155,1 +4157,3 @@\n-    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, xnoreg, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $mask$$KRegister, $ktmp$$KRegister, vlen_enc, vlen);\n@@ -4160,8 +4164,5 @@\n-\n-instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n-                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n-  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n-  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n-  ins_cost(200);\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n-  format %{ \"vector_gatherGT8 $dst, $mem, $idx, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+instruct vgather_subword_mask_off_avx3(vec dst, memory mem, rRegP idx, kReg mask, rRegI offset, rRegP tmp, kReg ktmp, rRegP idx_base_temp, vec idx_vec, vec offset_vec, vec xtmp1, vec xtmp2, vec xtmp3, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && n->in(MemNode::ValueIn)->in(2)->in(1)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP ktmp, TEMP idx_base_temp, TEMP idx_vec, TEMP offset_vec, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, KILL cr);\n+  format %{ \"vector_gather_subword $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $ktmp, $idx_base_temp, $idx_vec, $xtmp1, $xtmp2, $xtmp3 as TEMP\" %}\n@@ -4169,0 +4170,1 @@\n+    uint vlen = Matcher::vector_length(this);\n@@ -4170,1 +4172,0 @@\n-    int vector_len = Matcher::vector_length(this);\n@@ -4172,1 +4173,0 @@\n-    __ lea($tmp$$Register, $mem$$Address);\n@@ -4174,2 +4174,3 @@\n-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,\n-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather_subword_masked_avx3(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $offset_vec$$XMMRegister, $idx_vec$$XMMRegister,\n+                                   $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $mask$$KRegister, $ktmp$$KRegister, vlen_enc, vlen);\n@@ -4180,7 +4181,5 @@\n-\n-#ifdef _LP64\n-instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n-  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n-  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+instruct vgather_subwordLE8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx\\t! using $tmp and $rtmp as TEMP\" %}\n@@ -4190,1 +4189,0 @@\n-    __ xorq($midx$$Register, $midx$$Register);\n@@ -4192,2 +4190,1 @@\n-    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n-    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);\n@@ -4198,7 +4195,6 @@\n-instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n-                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n-  ins_cost(200);\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length, KILL cr);\n-  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+instruct vgather_subwordGT8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n@@ -4209,1 +4205,0 @@\n-    __ xorq($midx$$Register, $midx$$Register);\n@@ -4212,3 +4207,2 @@\n-    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n@@ -4219,5 +4213,6 @@\n-instruct vgather_masked_subwordLE8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n-  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n-  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx, $offset, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+instruct vgather_subwordLE8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP tmp, TEMP rtmp, KILL cr);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n@@ -4227,1 +4222,0 @@\n-    __ xorq($midx$$Register, $midx$$Register);\n@@ -4229,3 +4223,1 @@\n-    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n-    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register,\n-                                $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n@@ -4236,4 +4228,5 @@\n-instruct vgather_masked_subwordGT8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n-                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length, rFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n-  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+\n+instruct vgather_subwordGT8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n@@ -4241,2 +4234,2 @@\n-  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length, KILL cr);\n-  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n@@ -4247,1 +4240,0 @@\n-    __ xorq($midx$$Register, $midx$$Register);\n@@ -4250,3 +4242,2 @@\n-    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n-    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n-                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n@@ -4256,1 +4247,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":65,"deletions":75,"binary":false,"changes":140,"status":"modified"}]}