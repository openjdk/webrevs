{"files":[{"patch":"@@ -171,1 +171,0 @@\n-      case Op_LoadVectorGather:\n@@ -173,1 +172,0 @@\n-      case Op_LoadVectorGatherMasked:\n@@ -182,0 +180,6 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        if (UseSVE == 0 || is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -76,0 +76,5 @@\n+      case Op_LoadVectorGatherMasked:\n+        if (is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2848,0 +2848,7 @@\n+void Assembler::kxnorwl(KRegister dst, KRegister src1, KRegister src2) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x46, (0xC0 | encode));\n+}\n+\n@@ -13556,0 +13563,5 @@\n+void Assembler::bt(Register dst, Register src) {\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n+  emit_int24(0x0F, 0xA3, (encode | 0xC0));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1527,0 +1527,2 @@\n+  void kxnorwl(KRegister dst, KRegister src1, KRegister src2);\n+\n@@ -1734,0 +1736,1 @@\n+  void bt(Register dst, Register src);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1526,0 +1526,131 @@\n+#ifdef _LP64\n+void C2_MacroAssembler::vpgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                          Register mask, Register midx, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    Label case0, case1, case2, case3;\n+    Label* larr[] = { &case0, &case1, &case2, &case3 };\n+    for (int i = 0; i < 4; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    Label case0, case1, case2, case3, case4, case5, case6, case7;\n+    Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };\n+    for (int i = 0; i < 8; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrb(dst, Address(base, rtmp), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vpgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                                 Register offset, Register mask, Register midx, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    Label case0, case1, case2, case3;\n+    Label* larr[] = { &case0, &case1, &case2, &case3 };\n+    for (int i = 0; i < 4; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    Label case0, case1, case2, case3, case4, case5, case6, case7;\n+    Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };\n+    for (int i = 0; i < 8; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrb(dst, Address(base, rtmp), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  }\n+}\n+#endif \/\/ _LP64\n+\n+void C2_MacroAssembler::vpgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vpgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                          Register offset, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base,\n+                                        Register offset, Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                        Register rtmp, Register midx, Register length, int vector_len, int vlen_enc) {\n+  assert(is_subword_type(elem_ty), \"\");\n+  Label GATHER8_LOOP;\n+  movl(length, vector_len);\n+  vpxor(xtmp1, xtmp1, xtmp1, vlen_enc);\n+  vpxor(dst, dst, dst, vlen_enc);\n+  vallones(xtmp2, vlen_enc);\n+  vpsubd(xtmp2, xtmp1, xtmp2 ,vlen_enc);\n+  vpslld(xtmp2, xtmp2, 1, vlen_enc);\n+  load_iota_indices(xtmp1, vector_len * type2aelembytes(elem_ty), T_INT);\n+  bind(GATHER8_LOOP);\n+    if (offset == noreg) {\n+      if (mask == noreg) {\n+        vpgather8b(elem_ty, xtmp3, base, idx_base, rtmp, vlen_enc);\n+      } else {\n+        LP64_ONLY(vpgather8b_masked(elem_ty, xtmp3, base, idx_base, mask, midx, rtmp, vlen_enc));\n+      }\n+    } else {\n+      if (mask == noreg) {\n+        vpgather8b_offset(elem_ty, xtmp3, base, idx_base, offset, rtmp, vlen_enc);\n+      } else {\n+        LP64_ONLY(vpgather8b_masked_offset(elem_ty, xtmp3, base, idx_base, offset, mask, midx, rtmp, vlen_enc));\n+      }\n+    }\n+    vpermd(xtmp3, xtmp1, xtmp3, vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n+    vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n+    vpor(dst, dst, xtmp3, vlen_enc);\n+    addptr(idx_base,  32 >> (type2aelembytes(elem_ty) - 1));\n+    subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n+    jcc(Assembler::notEqual, GATHER8_LOOP);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":131,"deletions":0,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -495,0 +495,17 @@\n+  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n+                       Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n+                       Register midx, Register length, int vector_len, int vlen_enc);\n+\n+#ifdef _LP64\n+  void vpgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                         Register mask, Register midx, Register rtmp, int vlen_enc);\n+\n+  void vpgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                     Register offset, Register mask, Register midx, Register rtmp, int vlen_enc);\n+#endif\n+\n+  void vpgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base, Register rtmp, int vlen_enc);\n+\n+  void vpgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                              Register offset, Register rtmp, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -217,0 +217,3 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        return is_subword_type(ety) ? 50 : 0;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1575,0 +1575,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -1919,0 +1920,10 @@\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         (!is_LP64                                                ||\n+         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+         (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n@@ -1928,1 +1939,1 @@\n-      if (size_in_bits == 64 ) {\n+      if (!is_subword_type(bt) && size_in_bits == 64 ) {\n@@ -4051,1 +4062,1 @@\n-\/\/ Gather INT, LONG, FLOAT, DOUBLE\n+\/\/ Gather BYTE, SHORT, INT, LONG, FLOAT, DOUBLE\n@@ -4054,1 +4065,2 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && !is_subword_type(Matcher::vector_element_basic_type(n)) &&\n+            Matcher::vector_length_in_bytes(n) <= 32);\n@@ -4059,2 +4071,0 @@\n-    assert(UseAVX >= 2, \"sanity\");\n-\n@@ -4063,2 +4073,0 @@\n-\n-    assert(Matcher::vector_length_in_bytes(this) >= 16, \"sanity\");\n@@ -4066,6 +4074,1 @@\n-\n-    if (vlen_enc == Assembler::AVX_128bit) {\n-      __ movdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n-    } else {\n-      __ vmovdqu($mask$$XMMRegister, ExternalAddress(vector_all_bits_set()), noreg);\n-    }\n+    __ vpcmpeqd($mask$$XMMRegister, $mask$$XMMRegister, $mask$$XMMRegister, vlen_enc);\n@@ -4078,0 +4081,1 @@\n+\n@@ -4079,1 +4083,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4084,2 +4089,0 @@\n-    assert(UseAVX > 2, \"sanity\");\n-\n@@ -4088,4 +4091,1 @@\n-\n-    assert(!is_subword_type(elem_bt), \"sanity\"); \/\/ T_INT, T_LONG, T_FLOAT, T_DOUBLE\n-\n-    __ kmovwl($ktmp$$KRegister, ExternalAddress(vector_all_bits_set()), noreg);\n+    __ kxnorwl($ktmp$$KRegister, $ktmp$$KRegister, $ktmp$$KRegister);\n@@ -4099,0 +4099,2 @@\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4116,0 +4118,238 @@\n+\n+instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+#ifdef _LP64\n+instruct vgather_masked_subwordLE8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, vec mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vpgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, vec mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx, $offset, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vpgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vpgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegI midx, rRegI length) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegI midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx, $offset, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vpgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegI midx, rRegI length) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":260,"deletions":20,"binary":false,"changes":280,"status":"modified"},{"patch":"@@ -1008,0 +1008,2 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2480,0 +2480,7 @@\n+    case Op_LoadVectorGather:\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn+1);\n+      }\n+      break;\n@@ -2481,0 +2488,8 @@\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair2 = new BinaryNode(n->in(MemNode::ValueIn + 1), n->in(MemNode::ValueIn + 2));\n+        Node* pair1 = new BinaryNode(n->in(MemNode::ValueIn), pair2);\n+        n->set_req(MemNode::ValueIn, pair1);\n+        n->del_req(MemNode::ValueIn+2);\n+        n->del_req(MemNode::ValueIn+1);\n+        break;\n+      }\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1485,2 +1485,2 @@\n-    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n-                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+    VectorMaskUseType mask = (is_scatter || !is_subword_type(elem_bt)) ? (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred) : VecMaskUseLoad;\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt, mask)) {\n@@ -1507,1 +1507,1 @@\n-  if (!arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n+  if (!is_subword_type(elem_bt) && !arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n@@ -1549,0 +1549,1 @@\n+  Node* index_vect = nullptr;\n@@ -1550,5 +1551,7 @@\n-  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n-  if (index_vect == nullptr) {\n-    set_map(old_map);\n-    set_sp(old_sp);\n-    return false;\n+  if (!is_subword_type(elem_bt)) {\n+    index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n+    if (index_vect == nullptr) {\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n@@ -1594,1 +1597,6 @@\n-      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+      if (is_subword_type(elem_bt)) {\n+        Node* index_arr_base = array_element_address(argument(12), argument(13), T_INT);\n+        vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_arr_base, mask, argument(11)));\n+      } else {\n+        vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+      }\n@@ -1596,1 +1604,6 @@\n-      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+      if (is_subword_type(elem_bt)) {\n+        Node* index_arr_base = array_element_address(argument(12), argument(13), T_INT);\n+        vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_arr_base, argument(11)));\n+      } else {\n+        vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+      }\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":23,"deletions":10,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -882,1 +882,1 @@\n-  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices)\n+  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* offset = nullptr)\n@@ -885,1 +885,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n@@ -887,1 +886,5 @@\n-    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that last input is in MemNode::ValueIn\");\n+    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that index input is in MemNode::ValueIn\");\n+    if (offset) {\n+      assert(is_subword_type(vect_type()->element_basic_type()), \"\");\n+      add_req(offset);\n+    }\n@@ -891,1 +894,6 @@\n-  virtual uint match_edge(uint idx) const { return idx == MemNode::Address || idx == MemNode::ValueIn; }\n+  virtual uint match_edge(uint idx) const {\n+     return idx == MemNode::Address ||\n+            idx == MemNode::ValueIn ||\n+            ((is_subword_type(vect_type()->element_basic_type())) &&\n+              idx == MemNode::ValueIn + 1);\n+  }\n@@ -986,1 +994,1 @@\n-  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask, Node* offset = nullptr)\n@@ -989,2 +997,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n-    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -994,0 +1000,3 @@\n+    if (is_subword_type(vect_type()->is_vect()->element_basic_type())) {\n+      add_req(offset);\n+    }\n@@ -999,1 +1008,3 @@\n-                                                   idx == MemNode::ValueIn + 1; }\n+                                                   idx == MemNode::ValueIn + 1 ||\n+                                                   (is_subword_type(vect_type()->is_vect()->element_basic_type()) &&\n+                                                   idx == MemNode::ValueIn + 2); }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -896,0 +896,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, indexMap, mapOffset, (Byte128Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -928,0 +928,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, indexMap, mapOffset, (Byte256Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -992,0 +992,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, indexMap, mapOffset, (Byte512Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -880,0 +880,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, indexMap, mapOffset, (Byte64Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -866,0 +866,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, indexMap, mapOffset, (ByteMaxMask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3052,1 +3052,30 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+\n+        int loopIncr = 0;\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, byte.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3097,2 +3126,7 @@\n-        ByteSpecies vsp = (ByteSpecies) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        if (m.allTrue()) {\n+            return fromArray(species, a, offset, indexMap, mapOffset);\n+        }\n+        else {\n+            ByteSpecies vsp = (ByteSpecies) species;\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n+        }\n@@ -3767,0 +3801,44 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+\n+        int loopIncr = 0;\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, byte.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":81,"deletions":3,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -2844,0 +2844,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2868,0 +2868,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3024,0 +3024,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2885,0 +2885,1 @@\n+\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -880,0 +880,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, indexMap, mapOffset, (Short128Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -896,0 +896,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, indexMap, mapOffset, (Short256Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -928,0 +928,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, indexMap, mapOffset, (Short512Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -872,0 +872,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, indexMap, mapOffset, (Short64Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -866,0 +866,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, indexMap, mapOffset, (ShortMaxMask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3053,1 +3053,30 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+\n+        int loopIncr = 0;\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, short.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3098,2 +3127,7 @@\n-        ShortSpecies vsp = (ShortSpecies) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        if (m.allTrue()) {\n+            return fromArray(species, a, offset, indexMap, mapOffset);\n+        }\n+        else {\n+            ShortSpecies vsp = (ShortSpecies) species;\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n+        }\n@@ -3753,0 +3787,44 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+\n+        int loopIncr = 0;\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, short.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":81,"deletions":3,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -3625,1 +3625,30 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+        int loopIncr = 0;\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3628,0 +3657,1 @@\n+\n@@ -3717,11 +3747,0 @@\n-#if[byteOrShort]\n-    @ForceInline\n-    public static\n-    $abstractvectortype$ fromArray(VectorSpecies<$Boxtype$> species,\n-                                   $type$[] a, int offset,\n-                                   int[] indexMap, int mapOffset,\n-                                   VectorMask<$Boxtype$> m) {\n-        $Type$Species vsp = ($Type$Species) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n-    }\n-#else[byteOrShort]\n@@ -3742,1 +3761,0 @@\n-#end[byteOrShort]\n@@ -4800,1 +4818,0 @@\n-#if[!byteOrShort]\n@@ -4806,0 +4823,41 @@\n+#if[byteOrShort]\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+        int loopIncr = 0;\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#else[byteOrShort]\n@@ -4859,1 +4917,1 @@\n-#end[!byteOrShort]\n+#end[byteOrShort]\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":73,"deletions":15,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -1154,1 +1154,0 @@\n-#if[!byteOrShort]\n@@ -1161,1 +1160,0 @@\n-#end[!byteOrShort]\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,357 @@\n+\/*\n+ *  Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+\n+package org.openjdk.bench.jdk.incubator.vector;\n+\n+import jdk.incubator.vector.*;\n+import java.util.Random;\n+import java.util.stream.IntStream;\n+import java.util.concurrent.TimeUnit;\n+import org.openjdk.jmh.annotations.*;\n+\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+@State(Scope.Thread)\n+@Fork(jvmArgsPrepend = {\"--add-modules=jdk.incubator.vector\"})\n+public class GatherOperationsBenchmark {\n+    @Param({\"64\", \"256\", \"1024\", \"4096\"})\n+    int SIZE;\n+    byte  [] barr;\n+    byte  [] bres;\n+    short [] sarr;\n+    short [] sres;\n+    int   [] index;\n+\n+    static final VectorSpecies<Short> S64 = ShortVector.SPECIES_64;\n+    static final VectorSpecies<Short> S128 = ShortVector.SPECIES_128;\n+    static final VectorSpecies<Short> S256 = ShortVector.SPECIES_256;\n+    static final VectorSpecies<Short> S512 = ShortVector.SPECIES_512;\n+    static final VectorSpecies<Byte> B64 = ByteVector.SPECIES_64;\n+    static final VectorSpecies<Byte> B128 = ByteVector.SPECIES_128;\n+    static final VectorSpecies<Byte> B256 = ByteVector.SPECIES_256;\n+    static final VectorSpecies<Byte> B512 = ByteVector.SPECIES_512;\n+\n+    @Setup(Level.Trial)\n+    public void BmSetup() {\n+        Random r = new Random(1245);\n+        index = new int[SIZE];\n+        barr = new byte[SIZE];\n+        bres = new byte[SIZE];\n+        sarr = new short[SIZE];\n+        sres = new short[SIZE];\n+        for (int i = 0; i < SIZE; i++) {\n+           barr[i] = (byte)i;\n+           sarr[i] = (short)i;\n+           index[i] = r.nextInt(SIZE-1);\n+        }\n+    }\n+\n+\n+\n+    @Benchmark\n+    public void microByteGather64() {\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather64_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather64_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather64_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather128() {\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather128_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather128_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather128_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather256() {\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather256_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather256_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather256_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather512() {\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather512_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather512_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather512_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather64() {\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather64_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather64_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather64_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather128() {\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather128_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather128_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather128_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather256() {\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather256_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather256_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather256_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather512() {\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather512_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather512_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather512_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/GatherOperationsBenchmark.java","additions":357,"deletions":0,"binary":false,"changes":357,"status":"added"}]}