{"files":[{"patch":"@@ -172,1 +172,0 @@\n-      case Op_LoadVectorGather:\n@@ -174,1 +173,0 @@\n-      case Op_LoadVectorGatherMasked:\n@@ -183,0 +181,6 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        if (UseSVE == 0 || is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -136,0 +136,5 @@\n+  \/\/ Does target support predicated operation emulation.\n+  static bool supports_vector_predicate_op_emulation(int vopc, int vlen, BasicType bt) {\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/matcher_aarch64.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -129,0 +129,5 @@\n+  \/\/ Does target support predicated operation emulation.\n+  static bool supports_vector_predicate_op_emulation(int vopc, int vlen, BasicType bt) {\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/arm\/matcher_arm.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -136,0 +136,5 @@\n+  \/\/ Does target support predicated operation emulation.\n+  static bool supports_vector_predicate_op_emulation(int vopc, int vlen, BasicType bt) {\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/ppc\/matcher_ppc.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -133,0 +133,5 @@\n+  }\n+\n+  \/\/ Does target support predicated operation emulation.\n+  static bool supports_vector_predicate_op_emulation(int vopc, int vlen, BasicType bt) {\n+    return false;\n","filename":"src\/hotspot\/cpu\/riscv\/matcher_riscv.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -76,0 +76,5 @@\n+      case Op_LoadVectorGatherMasked:\n+        if (is_subword_type(bt)) {\n+          return false;\n+        }\n+        break;\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_v.ad","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -127,0 +127,5 @@\n+  \/\/ Does target support predicated operation emulation.\n+  static bool supports_vector_predicate_op_emulation(int vopc, int vlen, BasicType bt) {\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/s390\/matcher_s390.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -13574,0 +13574,5 @@\n+void Assembler::bt(Register dst, Register src) {\n+  int encode = prefixq_and_encode(src->encoding(), dst->encoding());\n+  emit_int24(0x0F, (unsigned char)0xA3, (encode | 0xC0));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1738,0 +1738,1 @@\n+  void bt(Register dst, Register src);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1574,0 +1574,155 @@\n+#ifdef _LP64\n+void C2_MacroAssembler::vgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                          Register mask, Register midx, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    Label case0, case1, case2, case3;\n+    Label* larr[] = { &case0, &case1, &case2, &case3 };\n+    for (int i = 0; i < 4; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    Label case0, case1, case2, case3, case4, case5, case6, case7;\n+    Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };\n+    for (int i = 0; i < 8; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrb(dst, Address(base, rtmp), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                                 Register offset, Register mask, Register midx, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    Label case0, case1, case2, case3;\n+    Label* larr[] = { &case0, &case1, &case2, &case3 };\n+    for (int i = 0; i < 4; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    Label case0, case1, case2, case3, case4, case5, case6, case7;\n+    Label* larr[] = { &case0, &case1, &case2, &case3, &case4, &case5, &case6, &case7 };\n+    for (int i = 0; i < 8; i++) {\n+      bt(mask, midx);\n+      jccb(Assembler::carryClear, *larr[i]);\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrb(dst, Address(base, rtmp), i);\n+      bind(*larr[i]);\n+      incq(midx);\n+    }\n+  }\n+}\n+#endif \/\/ _LP64\n+\n+void C2_MacroAssembler::vgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+void C2_MacroAssembler::vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                                          Register offset, Register rtmp, int vlen_enc) {\n+  vpxor(dst, dst, dst, vlen_enc);\n+  if (elem_bt == T_SHORT) {\n+    for (int i = 0; i < 4; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrw(dst, Address(base, rtmp, Address::times_2), i);\n+    }\n+  } else {\n+    assert(elem_bt == T_BYTE, \"\");\n+    for (int i = 0; i < 8; i++) {\n+      movl(rtmp, Address(idx_base, i*4));\n+      addl(rtmp, offset);\n+      pinsrb(dst, Address(base, rtmp), i);\n+    }\n+  }\n+}\n+\n+\/*\n+ * Gather loop first packs 4 short \/ 8 byte values from gather indices\n+ * into quadword lane and then permutes quadword lane into appropriate\n+ * location in destination vector. Following pseudo code describes the\n+ * algorithm in detail:-\n+ *\n+ * DST_VEC = ZERO_VEC\n+ * PERM_INDEX = {0, 1, 2, 3, 4, 5, 6, 7, 8..}\n+ * TWO_VEC = {2, 2, 2, 2, 2, 2, 2, 2..}\n+ * FOREACH_ITER:\n+ *     TMP_VEC_64 = PICK_SUB_WORDS_FROM_GATHER_INDICES\n+ *     TEMP_PERM_VEC = PERMUTE TMP_VEC_64 PERM_INDEX\n+ *     DST_VEC = DST_VEC OR TEMP_PERM_VEC\n+ *     PERM_INDEX = PERM_INDEX - TWO_VEC\n+ *\n+ * With each iteration permute index 0,1 holding assembled quadword\n+ * gets right shifted by two lane position.\n+ *\n+ *\/\n+\n+void C2_MacroAssembler::vgather_subword(BasicType elem_ty, XMMRegister dst,\n+                                        Register base, Register idx_base,\n+                                        Register offset, Register mask,\n+                                        XMMRegister xtmp1, XMMRegister xtmp2,\n+                                        XMMRegister xtmp3, Register rtmp,\n+                                        Register midx, Register length,\n+                                        int vector_len, int vlen_enc) {\n+  assert(is_subword_type(elem_ty), \"\");\n+  Label GATHER8_LOOP;\n+  movl(length, vector_len);\n+  vpxor(xtmp1, xtmp1, xtmp1, vlen_enc);\n+  vpxor(dst, dst, dst, vlen_enc);\n+  vallones(xtmp2, vlen_enc);\n+  vpsubd(xtmp2, xtmp1, xtmp2 ,vlen_enc);\n+  vpslld(xtmp2, xtmp2, 1, vlen_enc);\n+  load_iota_indices(xtmp1, vector_len * type2aelembytes(elem_ty), T_INT);\n+  bind(GATHER8_LOOP);\n+    if (offset == noreg) {\n+      if (mask == noreg) {\n+        vgather8b(elem_ty, xtmp3, base, idx_base, rtmp, vlen_enc);\n+      } else {\n+        LP64_ONLY(vgather8b_masked(elem_ty, xtmp3, base, idx_base, mask, midx, rtmp, vlen_enc));\n+      }\n+    } else {\n+      if (mask == noreg) {\n+        vgather8b_offset(elem_ty, xtmp3, base, idx_base, offset, rtmp, vlen_enc);\n+      } else {\n+        LP64_ONLY(vgather8b_masked_offset(elem_ty, xtmp3, base, idx_base, offset, mask, midx, rtmp, vlen_enc));\n+      }\n+    }\n+    vpermd(xtmp3, xtmp1, xtmp3, vlen_enc == Assembler::AVX_512bit ? vlen_enc : Assembler::AVX_256bit);\n+    vpsubd(xtmp1, xtmp1, xtmp2, vlen_enc);\n+    vpor(dst, dst, xtmp3, vlen_enc);\n+    addptr(idx_base,  32 >> (type2aelembytes(elem_ty) - 1));\n+    subl(length, 8 >> (type2aelembytes(elem_ty) - 1));\n+    jcc(Assembler::notEqual, GATHER8_LOOP);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":155,"deletions":0,"binary":false,"changes":155,"status":"modified"},{"patch":"@@ -499,0 +499,18 @@\n+\n+  void vgather_subword(BasicType elem_ty, XMMRegister dst,  Register base, Register idx_base, Register offset,\n+                       Register mask, XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n+                       Register midx, Register length, int vector_len, int vlen_enc);\n+\n+#ifdef _LP64\n+  void vgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                        Register mask, Register midx, Register rtmp, int vlen_enc);\n+\n+  void vgather8b_masked_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                               Register offset, Register mask, Register midx, Register rtmp, int vlen_enc);\n+#endif\n+\n+  void vgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base, Register rtmp, int vlen_enc);\n+\n+  void vgather8b_offset(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                              Register offset, Register rtmp, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -157,0 +157,10 @@\n+  \/\/ Does target support predicated operation emulation.\n+  static bool supports_vector_predicate_op_emulation(int vopc, int vlen, BasicType bt) {\n+    switch(vopc) {\n+      case Op_LoadVectorGatherMasked:\n+        return is_subword_type(bt) && VM_Version::supports_avx2();\n+      default:\n+        return false;\n+    }\n+  }\n+\n@@ -217,0 +227,3 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n+        return is_subword_type(ety) ? 50 : 0;\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1574,0 +1574,1 @@\n+    case Op_LoadVectorGatherMasked:\n@@ -1912,0 +1913,10 @@\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         (!is_LP64                                                ||\n+         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+         (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n@@ -1921,1 +1932,1 @@\n-      if (size_in_bits == 64 ) {\n+      if (!is_subword_type(bt) && size_in_bits == 64 ) {\n@@ -4043,1 +4054,1 @@\n-\/\/ Gather INT, LONG, FLOAT, DOUBLE\n+\/\/ Gather BYTE, SHORT, INT, LONG, FLOAT, DOUBLE\n@@ -4046,1 +4057,2 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  predicate(!VM_Version::supports_avx512vl() && !is_subword_type(Matcher::vector_element_basic_type(n)) &&\n+            Matcher::vector_length_in_bytes(n) <= 32);\n@@ -4063,1 +4075,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4078,1 +4091,2 @@\n-  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64) &&\n+            !is_subword_type(Matcher::vector_element_basic_type(n)));\n@@ -4096,0 +4110,238 @@\n+\n+instruct vgather_subwordLE8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegI rtmp) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP tmp, TEMP rtmp);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordGT8B(vec dst, memory mem, rRegP idx, immI_0 offset, rRegP tmp, rRegP idx_base_temp,\n+                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_subwordLE8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP tmp, TEMP rtmp, KILL cr);\n+  format %{ \"vector_gatherLE8 $dst, $mem, $idx, $offset\\t! using $tmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vgather8b_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vgather_subwordGT8B_off(vec dst, memory mem, rRegP idx, rRegI offset, rRegP tmp, rRegP idx_base_temp,\n+                                 vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegI length, rFlagsReg cr) %{\n+  predicate(is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGather mem (Binary idx offset)));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8 $dst, $mem, $idx, $offset\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, noreg, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, noreg, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+#ifdef _LP64\n+instruct vgather_masked_subwordLE8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx3(vec dst, memory mem, rRegP idx, immI_0 offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2, KILL cr);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx, $offset, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx3(vec dst, memory mem, rRegP idx, rRegI offset, kReg mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512bw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length, KILL cr);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorq($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ kmovql($rtmp2$$Register, $mask$$KRegister);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, vec mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2);\n+  format %{ \"vector_masked_gatherLE8 $dst, $mem, $idx, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_avx2(vec dst, memory mem, rRegP idx, immI_0 offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                         vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length);\n+  format %{ \"vector_gatherGT8_masked $dst, $mem, $idx, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, noreg, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordLE8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, vec mask, rRegL midx, rRegP tmp, rRegI rtmp, rRegL rtmp2) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) <= 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  effect(TEMP midx, TEMP tmp, TEMP rtmp, TEMP rtmp2);\n+  format %{ \"vector_masked_gatherLE8_off $dst, $mem, $idx, $offset, $mask\\t! using $midx, $tmp, $rtmp and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vgather8b_masked_offset(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx$$Register, $offset$$Register,\n+                                $rtmp2$$Register, $midx$$Register, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vgather_masked_subwordGT8B_off_avx2(vec dst, memory mem, rRegP idx, rRegI offset, vec mask, rRegP tmp, rRegP idx_base_temp,\n+                                             vec xtmp1, vec xtmp2, vec xtmp3, rRegI rtmp, rRegL rtmp2, rRegL midx, rRegI length) %{\n+  predicate(!VM_Version::supports_avx512vlbw() && is_subword_type(Matcher::vector_element_basic_type(n)) && Matcher::vector_length_in_bytes(n) > 8);\n+  match(Set dst (LoadVectorGatherMasked mem (Binary idx (Binary mask offset))));\n+  ins_cost(200);\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP idx_base_temp, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, TEMP rtmp2, TEMP midx, TEMP length);\n+  format %{ \"vector_gatherGT8_masked_off $dst, $mem, $idx, $offset, $mask\\t! using $tmp, $idx_base_temp, $xtmp1, $xtmp2, $xtmp3, $rtmp, $rtmp2, $midx and $length as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    int vector_len = Matcher::vector_length(this);\n+    BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ lea($tmp$$Register, $mem$$Address);\n+    __ movptr($idx_base_temp$$Register, $idx$$Register);\n+    __ vpmovmskb($rtmp2$$Register, $mask$$XMMRegister, vlen_enc);\n+    if (elem_bt == T_SHORT) {\n+      __ mov64($midx$$Register, 0x5555555555555555ULL);\n+      __ pextq($rtmp2$$Register, $rtmp2$$Register, $midx$$Register);\n+    }\n+    __ xorl($midx$$Register, $midx$$Register);\n+    __ vgather_subword(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base_temp$$Register, $offset$$Register, $rtmp2$$Register, $xtmp1$$XMMRegister,\n+                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, $midx$$Register, $length$$Register, vector_len, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":257,"deletions":5,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -1018,0 +1018,2 @@\n+      case Op_LoadVectorGather:\n+      case Op_LoadVectorGatherMasked:\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2481,0 +2481,7 @@\n+    case Op_LoadVectorGather:\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair = new BinaryNode(n->in(MemNode::ValueIn), n->in(MemNode::ValueIn+1));\n+        n->set_req(MemNode::ValueIn, pair);\n+        n->del_req(MemNode::ValueIn+1);\n+      }\n+      break;\n@@ -2482,0 +2489,8 @@\n+      if (is_subword_type(n->bottom_type()->is_vect()->element_basic_type())) {\n+        Node* pair2 = new BinaryNode(n->in(MemNode::ValueIn + 1), n->in(MemNode::ValueIn + 2));\n+        Node* pair1 = new BinaryNode(n->in(MemNode::ValueIn), pair2);\n+        n->set_req(MemNode::ValueIn, pair1);\n+        n->del_req(MemNode::ValueIn+2);\n+        n->del_req(MemNode::ValueIn+1);\n+        break;\n+      } \/\/ fall-through\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -305,0 +305,1 @@\n+    is_supported |= Matcher::supports_vector_predicate_op_emulation(sopc, num_elem, type);\n@@ -1503,2 +1504,2 @@\n-    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt,\n-                              (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred))) {\n+    VectorMaskUseType mask = (VectorMaskUseType) (VecMaskUseLoad | VecMaskUsePred);\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt, mask)) {\n@@ -1525,1 +1526,2 @@\n-  if (!arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n+  \/\/ For sub-word gathers expander receive index array.\n+  if (!is_subword_type(elem_bt) && !arch_supports_vector(Op_LoadVector, num_elem, T_INT, VecMaskNotUsed)) {\n@@ -1567,0 +1569,1 @@\n+  Node* index_vect = nullptr;\n@@ -1568,5 +1571,7 @@\n-  Node* index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n-  if (index_vect == nullptr) {\n-    set_map(old_map);\n-    set_sp(old_sp);\n-    return false;\n+  if (!is_subword_type(elem_bt)) {\n+    index_vect = unbox_vector(argument(8), vbox_idx_type, T_INT, num_elem);\n+    if (index_vect == nullptr) {\n+      set_map(old_map);\n+      set_sp(old_sp);\n+      return false;\n+    }\n@@ -1611,0 +1616,3 @@\n+    Node* index    = argument(11);\n+    Node* indexMap = argument(12);\n+    Node* indexM   = argument(13);\n@@ -1612,1 +1620,6 @@\n-      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+      if (is_subword_type(elem_bt)) {\n+        Node* index_arr_base = array_element_address(indexMap, indexM, T_INT);\n+        vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_arr_base, mask, index));\n+      } else {\n+        vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, index_vect, mask));\n+      }\n@@ -1614,1 +1627,6 @@\n-      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+      if (is_subword_type(elem_bt)) {\n+        Node* index_arr_base = array_element_address(indexMap, indexM, T_INT);\n+        vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_arr_base, index));\n+      } else {\n+        vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, index_vect));\n+      }\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":28,"deletions":10,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -893,1 +893,1 @@\n-  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices)\n+  LoadVectorGatherNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* offset = nullptr)\n@@ -896,1 +896,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n@@ -898,1 +897,7 @@\n-    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that last input is in MemNode::ValueIn\");\n+    DEBUG_ONLY(bool is_subword = is_subword_type(vt->element_basic_type()));\n+    assert(is_subword || indices->bottom_type()->is_vect(), \"indices must be in vector\");\n+    assert(is_subword || !offset, \"\");\n+    assert(req() == MemNode::ValueIn + 1, \"match_edge expects that index input is in MemNode::ValueIn\");\n+    if (offset) {\n+      add_req(offset);\n+    }\n@@ -902,1 +907,6 @@\n-  virtual uint match_edge(uint idx) const { return idx == MemNode::Address || idx == MemNode::ValueIn; }\n+  virtual uint match_edge(uint idx) const {\n+     return idx == MemNode::Address ||\n+            idx == MemNode::ValueIn ||\n+            ((is_subword_type(vect_type()->element_basic_type())) &&\n+              idx == MemNode::ValueIn + 1);\n+  }\n@@ -1006,1 +1016,1 @@\n-  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask)\n+  LoadVectorGatherMaskedNode(Node* c, Node* mem, Node* adr, const TypePtr* at, const TypeVect* vt, Node* indices, Node* mask, Node* offset = nullptr)\n@@ -1009,2 +1019,0 @@\n-    assert(indices->bottom_type()->is_vect(), \"indices must be in vector\");\n-    assert(mask->bottom_type()->isa_vectmask(), \"sanity\");\n@@ -1014,0 +1022,3 @@\n+    if (is_subword_type(vt->element_basic_type())) {\n+      add_req(offset);\n+    }\n@@ -1019,1 +1030,3 @@\n-                                                   idx == MemNode::ValueIn + 1; }\n+                                                   idx == MemNode::ValueIn + 1 ||\n+                                                   (is_subword_type(vect_type()->is_vect()->element_basic_type()) &&\n+                                                   idx == MemNode::ValueIn + 2); }\n","filename":"src\/hotspot\/share\/opto\/vectornode.hpp","additions":21,"deletions":8,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -896,0 +896,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte128Mask.class, a, offset, indexMap, mapOffset, (Byte128Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte128Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -928,0 +928,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte256Mask.class, a, offset, indexMap, mapOffset, (Byte256Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte256Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -992,0 +992,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte512Mask.class, a, offset, indexMap, mapOffset, (Byte512Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte512Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -880,0 +880,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(Byte64Mask.class, a, offset, indexMap, mapOffset, (Byte64Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Byte64Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -866,0 +866,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ByteVector fromArray0(byte[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Byte> m) {\n+        return super.fromArray0Template(ByteMaxMask.class, a, offset, indexMap, mapOffset, (ByteMaxMask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteMaxVector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3052,1 +3052,29 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, byte.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3097,2 +3125,7 @@\n-        ByteSpecies vsp = (ByteSpecies) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        if (m.allTrue()) {\n+            return fromArray(species, a, offset, indexMap, mapOffset);\n+        }\n+        else {\n+            ByteSpecies vsp = (ByteSpecies) species;\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n+        }\n@@ -3763,0 +3796,43 @@\n+    \/*package-private*\/\n+    abstract\n+    ByteVector fromArray0(byte[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Byte> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Byte>>\n+    ByteVector fromArray0Template(Class<M> maskClass, byte[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        ByteSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, byte.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":79,"deletions":3,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -880,0 +880,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short128Mask.class, a, offset, indexMap, mapOffset, (Short128Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short128Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -896,0 +896,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short256Mask.class, a, offset, indexMap, mapOffset, (Short256Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short256Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -928,0 +928,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short512Mask.class, a, offset, indexMap, mapOffset, (Short512Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short512Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -872,0 +872,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(Short64Mask.class, a, offset, indexMap, mapOffset, (Short64Mask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/Short64Vector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -866,0 +866,6 @@\n+    @ForceInline\n+    @Override\n+    final\n+    ShortVector fromArray0(short[] a, int offset, int[] indexMap, int mapOffset, VectorMask<Short> m) {\n+        return super.fromArray0Template(ShortMaxMask.class, a, offset, indexMap, mapOffset, (ShortMaxMask) m);\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortMaxVector.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3053,1 +3053,29 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, short.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3098,2 +3126,7 @@\n-        ShortSpecies vsp = (ShortSpecies) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        if (m.allTrue()) {\n+            return fromArray(species, a, offset, indexMap, mapOffset);\n+        }\n+        else {\n+            ShortSpecies vsp = (ShortSpecies) species;\n+            return vsp.dummyVector().fromArray0(a, offset, indexMap, mapOffset, m);\n+        }\n@@ -3749,0 +3782,43 @@\n+    \/*package-private*\/\n+    abstract\n+    ShortVector fromArray0(short[] a, int offset,\n+                                    int[] indexMap, int mapOffset,\n+                                    VectorMask<Short> m);\n+    @ForceInline\n+    final\n+    <M extends VectorMask<Short>>\n+    ShortVector fromArray0Template(Class<M> maskClass, short[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        ShortSpecies vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, short.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":79,"deletions":3,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -3625,1 +3625,29 @@\n-        return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3717,11 +3745,0 @@\n-#if[byteOrShort]\n-    @ForceInline\n-    public static\n-    $abstractvectortype$ fromArray(VectorSpecies<$Boxtype$> species,\n-                                   $type$[] a, int offset,\n-                                   int[] indexMap, int mapOffset,\n-                                   VectorMask<$Boxtype$> m) {\n-        $Type$Species vsp = ($Type$Species) species;\n-        return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n-    }\n-#else[byteOrShort]\n@@ -3742,1 +3759,0 @@\n-#end[byteOrShort]\n@@ -4796,1 +4812,0 @@\n-#if[!byteOrShort]\n@@ -4802,0 +4817,40 @@\n+#if[byteOrShort]\n+    @ForceInline\n+    final\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ fromArray0Template(Class<M> maskClass, $type$[] a, int offset,\n+                                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = vspecies();\n+        IntVector.IntSpecies isp = IntVector.species(vsp.indexShape());\n+        Objects.requireNonNull(a);\n+        Objects.requireNonNull(indexMap);\n+        m.check(vsp);\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+\n+        VectorSpecies<Integer> lsp = null;\n+\n+        \/\/ Constant folding should sweep out following conditonal logic.\n+        if (isp.length() > IntVector.SPECIES_PREFERRED.length()) {\n+            lsp = IntVector.SPECIES_PREFERRED;\n+        } else {\n+            lsp = isp;\n+        }\n+\n+        \/\/ Check indices are within array bounds.\n+        \/\/ FIXME: Check index under mask controlling.\n+        for (int i = 0; i < vsp.length(); i += lsp.length()) {\n+            IntVector vix = IntVector\n+                .fromArray(lsp, indexMap, mapOffset + i)\n+                .add(offset);\n+            vix = VectorIntrinsics.checkIndex(vix, a.length);\n+        }\n+\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(),\n+            a, ARRAY_BASE, null, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm) ->\n+            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+    }\n+#else[byteOrShort]\n@@ -4855,1 +4910,1 @@\n-#end[!byteOrShort]\n+#end[byteOrShort]\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":70,"deletions":15,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -1154,1 +1154,0 @@\n-#if[!byteOrShort]\n@@ -1161,1 +1160,0 @@\n-#end[!byteOrShort]\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-VectorBits.java.template","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,357 @@\n+\/*\n+ *  Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\n+ *\/\n+\n+package org.openjdk.bench.jdk.incubator.vector;\n+\n+import jdk.incubator.vector.*;\n+import java.util.Random;\n+import java.util.stream.IntStream;\n+import java.util.concurrent.TimeUnit;\n+import org.openjdk.jmh.annotations.*;\n+\n+@OutputTimeUnit(TimeUnit.MILLISECONDS)\n+@State(Scope.Thread)\n+@Fork(jvmArgsPrepend = {\"--add-modules=jdk.incubator.vector\"})\n+public class GatherOperationsBenchmark {\n+    @Param({\"64\", \"256\", \"1024\", \"4096\"})\n+    int SIZE;\n+    byte  [] barr;\n+    byte  [] bres;\n+    short [] sarr;\n+    short [] sres;\n+    int   [] index;\n+\n+    static final VectorSpecies<Short> S64 = ShortVector.SPECIES_64;\n+    static final VectorSpecies<Short> S128 = ShortVector.SPECIES_128;\n+    static final VectorSpecies<Short> S256 = ShortVector.SPECIES_256;\n+    static final VectorSpecies<Short> S512 = ShortVector.SPECIES_512;\n+    static final VectorSpecies<Byte> B64 = ByteVector.SPECIES_64;\n+    static final VectorSpecies<Byte> B128 = ByteVector.SPECIES_128;\n+    static final VectorSpecies<Byte> B256 = ByteVector.SPECIES_256;\n+    static final VectorSpecies<Byte> B512 = ByteVector.SPECIES_512;\n+\n+    @Setup(Level.Trial)\n+    public void BmSetup() {\n+        Random r = new Random(1245);\n+        index = new int[SIZE];\n+        barr = new byte[SIZE];\n+        bres = new byte[SIZE];\n+        sarr = new short[SIZE];\n+        sres = new short[SIZE];\n+        for (int i = 0; i < SIZE; i++) {\n+           barr[i] = (byte)i;\n+           sarr[i] = (short)i;\n+           index[i] = r.nextInt(SIZE-1);\n+        }\n+    }\n+\n+\n+\n+    @Benchmark\n+    public void microByteGather64() {\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather64_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather64_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather64_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B64.length()) {\n+            ByteVector.fromArray(B64, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather128() {\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather128_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather128_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather128_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B128.length()) {\n+            ByteVector.fromArray(B128, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather256() {\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather256_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather256_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather256_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B256.length()) {\n+            ByteVector.fromArray(B256, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather512() {\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 0, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microByteGather512_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 1, index, i)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather512_MASK() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 0, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microByteGather512_MASK_NZ_OFF() {\n+        VectorMask<Byte> VMASK = VectorMask.fromLong(B512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += B512.length()) {\n+            ByteVector.fromArray(B512, barr, 1, index, i, VMASK)\n+                            .intoArray(bres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather64() {\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather64_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather64_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather64_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S64, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S64.length()) {\n+            ShortVector.fromArray(S64, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather128() {\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather128_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather128_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather128_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S128, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S128.length()) {\n+            ShortVector.fromArray(S128, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather256() {\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather256_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather256_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather256_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S256, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S256.length()) {\n+            ShortVector.fromArray(S256, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather512() {\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 0, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+\n+    @Benchmark\n+    public void microShortGather512_NZ_OFF() {\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 1, index, i)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather512_MASK() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 0, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void microShortGather512_MASK_NZ_OFF() {\n+        VectorMask<Short> VMASK = VectorMask.fromLong(S512, 0x5555555555555555L);\n+        for (int i = 0; i < SIZE; i += S512.length()) {\n+            ShortVector.fromArray(S512, sarr, 1, index, i, VMASK)\n+                            .intoArray(sres, i);\n+        }\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/GatherOperationsBenchmark.java","additions":357,"deletions":0,"binary":false,"changes":357,"status":"added"}]}