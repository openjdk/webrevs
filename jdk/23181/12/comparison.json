{"files":[{"patch":"@@ -130,0 +130,11 @@\n+    \/\/ Do not auto-vectorize these FP operations, neither NEON or SVE\/SVE2 support them directly:\n+    \/\/   1. The non_strict_order SVE implementation for 256-bit wide vectors does recursive folding\n+    \/\/      and doesn't conform to the JLS, Section Evaluation Order.\n+    \/\/   2. A strictly ordered SVE implementation for 256-bit wide vectors isn't currently\n+    \/\/      profitable performance-wise.\n+    \/\/   3. The strictly ordered NEON implementation for 64-bit and 128-bit wide vectors isn't\n+    \/\/      profitable performance-wise.\n+    if (opcode == Op_MulReductionVD || opcode == Op_MulReductionVF) {\n+      return false;\n+    }\n+\n@@ -142,1 +153,0 @@\n-          opcode == Op_MulReductionVD || opcode == Op_MulReductionVF ||\n@@ -208,3 +218,3 @@\n-        \/\/ No vector multiply reduction instructions, but we do\n-        \/\/ emit scalar instructions for 64\/128-bit vectors.\n-        if (length_in_bytes != 8 && length_in_bytes != 16) {\n+        \/\/ No vector multiply reduction instructions, but we do emit ASIMD instructions for\n+        \/\/ 64\/128-bit vectors. For 256-bit vectors it's a combination of SVE and ASIMD instructions.\n+        if (length_in_bytes < 8 || length_in_bytes > 32) {\n@@ -3480,2 +3490,2 @@\n-instruct reduce_mulI(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n-                     vReg tmp1, vReg tmp2) %{\n+instruct reduce_mulI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp1, vReg tmp2) %{\n@@ -3486,1 +3496,17 @@\n-  format %{ \"reduce_mulI $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  format %{ \"reduce_mulI_le128b $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_le128b($dst$$Register, bt, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulI_256b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                          vReg tmp1, vReg tmp2, vReg tmp3) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"reduce_mulI_256b $dst, $isrc, $vsrc\\t# vector (256 bits). KILL $tmp1, $tmp2, $tmp3\" %}\n@@ -3488,0 +3514,1 @@\n+    assert(UseSVE > 0, \"must be sve\");\n@@ -3490,1 +3517,1 @@\n-    __ neon_reduce_mul_integral($dst$$Register, bt, $isrc$$Register,\n+    __ reduce_mul_integral_256b($dst$$Register, bt, $isrc$$Register,\n@@ -3492,1 +3519,1 @@\n-                                $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+                                $tmp1$$FloatRegister, $tmp2$$FloatRegister, $tmp3$$FloatRegister);\n@@ -3497,1 +3524,1 @@\n-instruct reduce_mulL(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+instruct reduce_mulL_128b(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n@@ -3501,1 +3528,1 @@\n-  format %{ \"reduce_mulL $dst, $isrc, $vsrc\\t# 2L\" %}\n+  format %{ \"reduce_mulL_128b $dst, $isrc, $vsrc\\t# 2L\" %}\n@@ -3503,2 +3530,2 @@\n-    __ neon_reduce_mul_integral($dst$$Register, T_LONG, $isrc$$Register,\n-                                $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+    __ reduce_mul_integral_le128b($dst$$Register, T_LONG, $isrc$$Register, $vsrc$$FloatRegister, 16,\n+                                  fnoreg, fnoreg);\n@@ -3509,1 +3536,16 @@\n-instruct reduce_mulF(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+instruct reduce_mulL_256b(iRegLNoSp dst, iRegL isrc, vReg vsrc, vReg tmp1) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32);\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1);\n+  format %{ \"reduce_mulL_256b $dst, $isrc, $vsrc\\t# 4L. KILL $tmp1\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_256b($dst$$Register, T_LONG, $isrc$$Register,\n+                                $vsrc$$FloatRegister, length_in_bytes,\n+                                $tmp1$$FloatRegister, fnoreg, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulF_le128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n@@ -3513,1 +3555,15 @@\n-  format %{ \"reduce_mulF $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  format %{ \"reduce_mulF_le128b $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulF_256b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp1, vReg tmp2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32 &&\n+            !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2);\n+  format %{ \"reduce_non_strict_order_mulF_256b $dst, $fsrc, $vsrc\\t# 8F. KILL $tmp1, $tmp2\" %}\n@@ -3515,0 +3571,1 @@\n+    assert(UseSVE > 0, \"must be sve\");\n@@ -3516,2 +3573,3 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+    __ reduce_non_strict_order_mul_fp_256b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                                           $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                           $tmp2$$FloatRegister);\n@@ -3522,1 +3580,1 @@\n-instruct reduce_mulD(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n+instruct reduce_mulD_128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n@@ -3526,1 +3584,1 @@\n-  format %{ \"reduce_mulD $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n+  format %{ \"reduce_mulD_128b $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n@@ -3528,2 +3586,18 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulD_256b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp1, vReg tmp2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32 &&\n+            !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVD dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2);\n+  format %{ \"reduce_non_strict_order_mulD_256b $dst, $dsrc, $vsrc\\t# 4D. KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_non_strict_order_mul_fp_256b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                                           $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                           $tmp2$$FloatRegister);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":95,"deletions":21,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -120,0 +120,11 @@\n+    \/\/ Do not auto-vectorize these FP operations, neither NEON or SVE\/SVE2 support them directly:\n+    \/\/   1. The non_strict_order SVE implementation for 256-bit wide vectors does recursive folding\n+    \/\/      and doesn't conform to the JLS, Section Evaluation Order.\n+    \/\/   2. A strictly ordered SVE implementation for 256-bit wide vectors isn't currently\n+    \/\/      profitable performance-wise.\n+    \/\/   3. The strictly ordered NEON implementation for 64-bit and 128-bit wide vectors isn't\n+    \/\/      profitable performance-wise.\n+    if (opcode == Op_MulReductionVD || opcode == Op_MulReductionVF) {\n+      return false;\n+    }\n+\n@@ -132,1 +143,0 @@\n-          opcode == Op_MulReductionVD || opcode == Op_MulReductionVF ||\n@@ -198,3 +208,3 @@\n-        \/\/ No vector multiply reduction instructions, but we do\n-        \/\/ emit scalar instructions for 64\/128-bit vectors.\n-        if (length_in_bytes != 8 && length_in_bytes != 16) {\n+        \/\/ No vector multiply reduction instructions, but we do emit ASIMD instructions for\n+        \/\/ 64\/128-bit vectors. For 256-bit vectors it's a combination of SVE and ASIMD instructions.\n+        if (length_in_bytes < 8 || length_in_bytes > 32) {\n@@ -2107,2 +2117,2 @@\n-instruct reduce_mulI(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n-                     vReg tmp1, vReg tmp2) %{\n+instruct reduce_mulI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp1, vReg tmp2) %{\n@@ -2113,1 +2123,17 @@\n-  format %{ \"reduce_mulI $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  format %{ \"reduce_mulI_le128b $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_le128b($dst$$Register, bt, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulI_256b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                          vReg tmp1, vReg tmp2, vReg tmp3) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+  format %{ \"reduce_mulI_256b $dst, $isrc, $vsrc\\t# vector (256 bits). KILL $tmp1, $tmp2, $tmp3\" %}\n@@ -2115,0 +2141,1 @@\n+    assert(UseSVE > 0, \"must be sve\");\n@@ -2117,1 +2144,1 @@\n-    __ neon_reduce_mul_integral($dst$$Register, bt, $isrc$$Register,\n+    __ reduce_mul_integral_256b($dst$$Register, bt, $isrc$$Register,\n@@ -2119,1 +2146,1 @@\n-                                $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+                                $tmp1$$FloatRegister, $tmp2$$FloatRegister, $tmp3$$FloatRegister);\n@@ -2124,1 +2151,1 @@\n-instruct reduce_mulL(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+instruct reduce_mulL_128b(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n@@ -2128,1 +2155,1 @@\n-  format %{ \"reduce_mulL $dst, $isrc, $vsrc\\t# 2L\" %}\n+  format %{ \"reduce_mulL_128b $dst, $isrc, $vsrc\\t# 2L\" %}\n@@ -2130,2 +2157,2 @@\n-    __ neon_reduce_mul_integral($dst$$Register, T_LONG, $isrc$$Register,\n-                                $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+    __ reduce_mul_integral_le128b($dst$$Register, T_LONG, $isrc$$Register, $vsrc$$FloatRegister, 16,\n+                                  fnoreg, fnoreg);\n@@ -2136,1 +2163,16 @@\n-instruct reduce_mulF(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+instruct reduce_mulL_256b(iRegLNoSp dst, iRegL isrc, vReg vsrc, vReg tmp1) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32);\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1);\n+  format %{ \"reduce_mulL_256b $dst, $isrc, $vsrc\\t# 4L. KILL $tmp1\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_256b($dst$$Register, T_LONG, $isrc$$Register,\n+                                $vsrc$$FloatRegister, length_in_bytes,\n+                                $tmp1$$FloatRegister, fnoreg, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulF_le128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n@@ -2140,1 +2182,15 @@\n-  format %{ \"reduce_mulF $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  format %{ \"reduce_mulF_le128b $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulF_256b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp1, vReg tmp2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32 &&\n+            !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2);\n+  format %{ \"reduce_non_strict_order_mulF_256b $dst, $fsrc, $vsrc\\t# 8F. KILL $tmp1, $tmp2\" %}\n@@ -2142,0 +2198,1 @@\n+    assert(UseSVE > 0, \"must be sve\");\n@@ -2143,2 +2200,3 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+    __ reduce_non_strict_order_mul_fp_256b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                                           $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                           $tmp2$$FloatRegister);\n@@ -2149,1 +2207,1 @@\n-instruct reduce_mulD(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n+instruct reduce_mulD_128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n@@ -2153,1 +2211,1 @@\n-  format %{ \"reduce_mulD $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n+  format %{ \"reduce_mulD_128b $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n@@ -2155,2 +2213,18 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulD_256b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp1, vReg tmp2) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) == 32 &&\n+            !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVD dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2);\n+  format %{ \"reduce_non_strict_order_mulD_256b $dst, $dsrc, $vsrc\\t# 4D. KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_non_strict_order_mul_fp_256b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                                           $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                           $tmp2$$FloatRegister);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4","additions":95,"deletions":21,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -1793,4 +1793,4 @@\n-void C2_MacroAssembler::neon_reduce_mul_integral(Register dst, BasicType bt,\n-                                                 Register isrc, FloatRegister vsrc,\n-                                                 unsigned vector_length_in_bytes,\n-                                                 FloatRegister vtmp1, FloatRegister vtmp2) {\n+void C2_MacroAssembler::reduce_mul_integral_le128b(Register dst, BasicType bt, Register isrc,\n+                                                   FloatRegister vsrc,\n+                                                   unsigned vector_length_in_bytes,\n+                                                   FloatRegister vtmp1, FloatRegister vtmp2) {\n@@ -1798,0 +1798,3 @@\n+  if (bt != T_LONG) {\n+    assert_different_registers(vsrc, vtmp1, vtmp2);\n+  }\n@@ -1800,1 +1803,1 @@\n-  BLOCK_COMMENT(\"neon_reduce_mul_integral {\");\n+  BLOCK_COMMENT(\"reduce_mul_integral_le128b {\");\n@@ -1868,1 +1871,29 @@\n-  BLOCK_COMMENT(\"} neon_reduce_mul_integral\");\n+  BLOCK_COMMENT(\"} reduce_mul_integral_le128b\");\n+}\n+\n+\/\/ Vector reduction multiply for integral type with SVE instructions. Multiplies halves of the\n+\/\/ source vector to get to a 128b vector that fits into a SIMD&FP register. After that point ASIMD\n+\/\/ instructions are used. Note: temporary registers vtmp2 and vtmp3 are not used in some cases.\n+\/\/ Clobbers: rscratch1\n+void C2_MacroAssembler::reduce_mul_integral_256b(Register dst, BasicType bt, Register isrc,\n+                                                 FloatRegister vsrc,\n+                                                 unsigned vector_length_in_bytes,\n+                                                 FloatRegister vtmp1, FloatRegister vtmp2,\n+                                                 FloatRegister vtmp3) {\n+  assert(vector_length_in_bytes == 32, \"unsupported vector length\");\n+  assert(bt == T_BYTE || bt == T_SHORT || bt == T_INT || bt == T_LONG, \"unsupported type\");\n+  if (bt == T_LONG) {\n+    assert_different_registers(vsrc, vtmp1);\n+  } else {\n+    assert_different_registers(vsrc, vtmp1, vtmp2, vtmp3);\n+  }\n+\n+  BLOCK_COMMENT(\"reduce_mul_integral_256b {\");\n+\n+  \/\/ Handle the first iteration separately to preserve the original values in vsrc\n+  sve_movprfx(vtmp1, vsrc);                                \/\/ copy\n+  sve_ext(vtmp1, vtmp1, vector_length_in_bytes \/ 2);       \/\/ swap halves\n+  sve_mul(vtmp1, elemType_to_regVariant(bt), ptrue, vsrc); \/\/ multiply halves\n+\n+  reduce_mul_integral_le128b(dst, bt, isrc, vtmp1, FloatRegister::neon_vl, vtmp2, vtmp3);\n+  BLOCK_COMMENT(\"} reduce_mul_integral_256b\");\n@@ -1872,4 +1903,4 @@\n-void C2_MacroAssembler::neon_reduce_mul_fp(FloatRegister dst, BasicType bt,\n-                                           FloatRegister fsrc, FloatRegister vsrc,\n-                                           unsigned vector_length_in_bytes,\n-                                           FloatRegister vtmp) {\n+\/\/ Strictly-ordered, used for both strictly-ordered and unordered operations.\n+void C2_MacroAssembler::reduce_mul_fp_le128b(FloatRegister dst, BasicType bt, FloatRegister fsrc,\n+                                             FloatRegister vsrc, unsigned vector_length_in_bytes,\n+                                             FloatRegister vtmp) {\n@@ -1879,1 +1910,1 @@\n-  BLOCK_COMMENT(\"neon_reduce_mul_fp {\");\n+  BLOCK_COMMENT(\"reduce_mul_fp_le128b {\");\n@@ -1902,1 +1933,20 @@\n-  BLOCK_COMMENT(\"} neon_reduce_mul_fp\");\n+  BLOCK_COMMENT(\"} reduce_mul_fp_le128b\");\n+}\n+\n+\/\/ Unordered vector reduction multiply for floating-point type with SVE instructions. Multiplies\n+\/\/ halves of the source vector to get to a 128b vector that fits into a SIMD&FP register. After that\n+\/\/ point ASIMD instructions are used.\n+void C2_MacroAssembler::reduce_non_strict_order_mul_fp_256b(FloatRegister dst, BasicType bt,\n+                                                            FloatRegister fsrc, FloatRegister vsrc,\n+                                                            unsigned vector_length_in_bytes,\n+                                                            FloatRegister vtmp1,\n+                                                            FloatRegister vtmp2) {\n+  assert(vector_length_in_bytes == 32, \"unsupported vector length\");\n+\n+  \/\/ Handle the first iteration separately to preserve the original values in vsrc\n+  sve_movprfx(vtmp1, vsrc);                                 \/\/ copy\n+  sve_ext(vtmp1, vtmp1, vector_length_in_bytes \/ 2);        \/\/ swap halves\n+  sve_fmul(vtmp1, elemType_to_regVariant(bt), ptrue, vsrc); \/\/ multiply halves\n+\n+  reduce_mul_fp_le128b(dst, bt, fsrc, vtmp1, FloatRegister::neon_vl, vtmp2);\n+  BLOCK_COMMENT(\"} reduce_non_strict_order_mul_fp_gt128b\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":62,"deletions":12,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -134,4 +134,10 @@\n-  void neon_reduce_mul_integral(Register dst, BasicType bt,\n-                                Register isrc, FloatRegister vsrc,\n-                                unsigned vector_length_in_bytes,\n-                                FloatRegister vtmp1, FloatRegister vtmp2);\n+  void reduce_mul_integral_le128b(Register dst, BasicType bt, Register isrc, FloatRegister vsrc,\n+                                  unsigned vector_length_in_bytes, FloatRegister vtmp1,\n+                                  FloatRegister vtmp2);\n+\n+  void reduce_mul_integral_256b(Register dst, BasicType bt, Register isrc, FloatRegister vsrc,\n+                                unsigned vector_length_in_bytes, FloatRegister vtmp1,\n+                                FloatRegister vtmp2, FloatRegister vtmp3);\n+\n+  void reduce_mul_fp_le128b(FloatRegister dst, BasicType bt, FloatRegister fsrc, FloatRegister vsrc,\n+                            unsigned vector_length_in_bytes, FloatRegister vtmp);\n@@ -139,3 +145,3 @@\n-  void neon_reduce_mul_fp(FloatRegister dst, BasicType bt,\n-                          FloatRegister fsrc, FloatRegister vsrc,\n-                          unsigned vector_length_in_bytes, FloatRegister vtmp);\n+  void reduce_non_strict_order_mul_fp_256b(FloatRegister dst, BasicType bt, FloatRegister fsrc,\n+                                           FloatRegister vsrc, unsigned vector_length_in_bytes,\n+                                           FloatRegister vtmp1, FloatRegister vtmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Arm Limited. All rights reserved.\n+ * Copyright (c) 2025, Arm Limited. All rights reserved.\n@@ -84,1 +84,1 @@\n-        applyIfCPUFeatureAnd = {\"asimd\", \"true\", \"sve\", \"false\"})\n+        applyIfCPUFeature = {\"asimd\", \"true\"})\n@@ -87,1 +87,1 @@\n-        applyIfCPUFeatureOr = {\"sve\", \"true\", \"sse2\", \"true\"},\n+        applyIfCPUFeature = {\"sse2\", \"true\"},\n@@ -99,1 +99,1 @@\n-        applyIfCPUFeatureAnd = {\"asimd\", \"true\", \"sve\", \"false\"})\n+        applyIfCPUFeature = {\"asimd\", \"true\"})\n@@ -102,1 +102,1 @@\n-        applyIfCPUFeatureOr = {\"sve\", \"true\", \"sse2\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestVectorFPReduction.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"}]}