{"files":[{"patch":"@@ -130,3 +130,7 @@\n-    \/\/ The non_strict_order implementations of Op_MulReductionVD\/F are not suitable for\n-    \/\/ auto-vectorization as the result would not conform to the JLS, Section Evaluation Order.\n-    \/\/ The strictly ordered implementations aren't profitable in terms of performance.\n+    \/\/ Do not auto-vectorize these FP operations, neither NEON or SVE\/SVE2 support them directly:\n+    \/\/   1. The non_strict_order SVE implementation for 256-bit wide and above vectors does\n+    \/\/      recursive folding and doesn't conform to the JLS, Section Evaluation Order.\n+    \/\/   2. A strictly ordered SVE implementation for 256-bit wide and above vectors isn't currently\n+    \/\/      profitable performance-wise.\n+    \/\/   3. The strictly ordered NEON implementation for 64-bit and 128-bit wide vectors isn't\n+    \/\/      profitable performance-wise.\n@@ -3533,16 +3537,0 @@\n-\n-instruct reduce_mulF_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n-  match(Set dst (MulReductionVF fsrc vsrc));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"reduce_mulF_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n-  ins_encode %{\n-    assert(UseSVE > 0, \"must be sve\");\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n-    assert(length_in_bytes == MaxVectorSize, \"invalid vector length\");\n-    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n-                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -3577,15 +3565,0 @@\n-instruct reduce_mulD_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n-  match(Set dst (MulReductionVD fsrc vsrc));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"reduce_mulD_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n-  ins_encode %{\n-    assert(UseSVE > 0, \"must be sve\");\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n-    assert(length_in_bytes == MaxVectorSize, \"invalid vector length\");\n-    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_DOUBLE, $fsrc$$FloatRegister,\n-                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":7,"deletions":34,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -120,3 +120,7 @@\n-    \/\/ The non_strict_order implementations of Op_MulReductionVD\/F are not suitable for\n-    \/\/ auto-vectorization as the result would not conform to the JLS, Section Evaluation Order.\n-    \/\/ The strictly ordered implementations aren't profitable in terms of performance.\n+    \/\/ Do not auto-vectorize these FP operations, neither NEON or SVE\/SVE2 support them directly:\n+    \/\/   1. The non_strict_order SVE implementation for 256-bit wide and above vectors does\n+    \/\/      recursive folding and doesn't conform to the JLS, Section Evaluation Order.\n+    \/\/   2. A strictly ordered SVE implementation for 256-bit wide and above vectors isn't currently\n+    \/\/      profitable performance-wise.\n+    \/\/   3. The strictly ordered NEON implementation for 64-bit and 128-bit wide vectors isn't\n+    \/\/      profitable performance-wise.\n@@ -2160,16 +2164,0 @@\n-\n-instruct reduce_mulF_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n-  match(Set dst (MulReductionVF fsrc vsrc));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"reduce_mulF_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n-  ins_encode %{\n-    assert(UseSVE > 0, \"must be sve\");\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n-    assert(length_in_bytes == MaxVectorSize, \"invalid vector length\");\n-    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n-                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -2204,15 +2192,0 @@\n-instruct reduce_mulD_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n-  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n-  match(Set dst (MulReductionVD fsrc vsrc));\n-  effect(TEMP_DEF dst, TEMP tmp);\n-  format %{ \"reduce_mulD_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n-  ins_encode %{\n-    assert(UseSVE > 0, \"must be sve\");\n-    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n-    assert(length_in_bytes == MaxVectorSize, \"invalid vector length\");\n-    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_DOUBLE, $fsrc$$FloatRegister,\n-                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4","additions":7,"deletions":34,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -2144,40 +2144,0 @@\n-\/\/ Strictly-ordered vector reduction multiply for floating-point type with SVE instructions.\n-void C2_MacroAssembler::reduce_mul_fp_gt128b(FloatRegister dst, BasicType bt, FloatRegister fsrc,\n-                                             FloatRegister vsrc, unsigned vector_length_in_bytes,\n-                                             FloatRegister vtmp) {\n-  assert(vector_length_in_bytes > FloatRegister::neon_vl, \"ASIMD impl should be used instead\");\n-  assert(is_power_of_2(vector_length_in_bytes), \"unsupported vector length\");\n-\n-  BLOCK_COMMENT(\"reduce_mul_fp_gt128b {\");\n-    \/\/ Scalar multiply the bottom element of the vector\n-    switch (bt) {\n-    case T_FLOAT:\n-      fmuls(dst, fsrc, vsrc);\n-      break;\n-    case T_DOUBLE:\n-      fmuld(dst, fsrc, vsrc);\n-      break;\n-    default:\n-      assert(false, \"unsupported\");\n-      ShouldNotReachHere();\n-    }\n-\n-    for (unsigned i = 0; i < vector_length_in_bytes \/ type2aelembytes(bt); i++) {\n-      \/\/ Shuffle the elements one position to the right\n-      sve_ext(vtmp, i ? vtmp : vsrc, type2aelembytes(bt));\n-      \/\/ Scalar multiply the bottom element of the vector\n-      switch (bt) {\n-      case T_FLOAT:\n-        fmuls(dst, dst, vtmp);\n-        break;\n-      case T_DOUBLE:\n-        fmuld(dst, dst, vtmp);\n-        break;\n-      default:\n-        assert(false, \"unsupported\");\n-        ShouldNotReachHere();\n-      }\n-    }\n-  BLOCK_COMMENT(\"} reduce_mul_fp_gt128b\");\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -139,3 +139,0 @@\n-  void reduce_mul_fp_gt128b(FloatRegister dst, BasicType bt, FloatRegister fsrc, FloatRegister vsrc,\n-                            unsigned vector_length_in_bytes, FloatRegister vtmp);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"}]}