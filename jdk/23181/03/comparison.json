{"files":[{"patch":"@@ -130,0 +130,7 @@\n+    \/\/ The non_strict_order implementations of Op_MulReductionVD\/F are not suitable for\n+    \/\/ auto-vectorization as the result would not conform to the JLS, Section Evaluation Order.\n+    \/\/ The strictly ordered implementations aren't profitable in terms of performance.\n+    if (opcode == Op_MulReductionVD || opcode == Op_MulReductionVF) {\n+      return false;\n+    }\n+\n@@ -142,1 +149,0 @@\n-          opcode == Op_MulReductionVD || opcode == Op_MulReductionVF ||\n@@ -196,3 +202,3 @@\n-        \/\/ No vector multiply reduction instructions, but we do\n-        \/\/ emit scalar instructions for 64\/128-bit vectors.\n-        if (length_in_bytes != 8 && length_in_bytes != 16) {\n+        \/\/ No vector multiply reduction instructions, but we do emit ASIMD instructions for\n+        \/\/ 64\/128-bit vectors. For wider vectors it's a combination of SVE and ASIMD instructions.\n+        if (length_in_bytes < 8) {\n@@ -3451,2 +3457,2 @@\n-instruct reduce_mulI(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n-                     vReg tmp1, vReg tmp2) %{\n+instruct reduce_mulI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp1, vReg tmp2) %{\n@@ -3457,1 +3463,17 @@\n-  format %{ \"reduce_mulI $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  format %{ \"reduce_mulI_le128b $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_le128b($dst$$Register, bt, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulI_gt128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp1, vReg tmp2, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  format %{ \"reduce_mulI_gt128b $dst, $isrc, $vsrc\\t# vector (> 128 bits). KILL $tmp1, $tmp2, $pgtmp\" %}\n@@ -3459,0 +3481,1 @@\n+    assert(UseSVE > 0, \"must be sve\");\n@@ -3461,3 +3484,4 @@\n-    __ neon_reduce_mul_integral($dst$$Register, bt, $isrc$$Register,\n-                                $vsrc$$FloatRegister, length_in_bytes,\n-                                $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+    __ reduce_mul_integral_gt128b($dst$$Register, bt, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                                  $pgtmp$$PRegister);\n@@ -3468,1 +3492,1 @@\n-instruct reduce_mulL(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+instruct reduce_mulL_128b(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n@@ -3472,1 +3496,14 @@\n-  format %{ \"reduce_mulL $dst, $isrc, $vsrc\\t# 2L\" %}\n+  format %{ \"reduce_mulL_128b $dst, $isrc, $vsrc\\t# 2L\" %}\n+  ins_encode %{\n+    __ reduce_mul_integral_le128b($dst$$Register, T_LONG, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulL_gt128b(iRegLNoSp dst, iRegL isrc, vReg vsrc, vReg tmp,\n+                            pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16);\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_mulL_gt128b $dst, $isrc, $vsrc\\t# vector (> 128 bits). KILL $tmp, $pgtmp\" %}\n@@ -3474,2 +3511,6 @@\n-    __ neon_reduce_mul_integral($dst$$Register, T_LONG, $isrc$$Register,\n-                                $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_gt128b($dst$$Register, T_LONG, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp$$FloatRegister, fnoreg,\n+                                  $pgtmp$$PRegister);\n@@ -3480,1 +3521,1 @@\n-instruct reduce_mulF(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+instruct reduce_mulF_le128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n@@ -3484,1 +3525,1 @@\n-  format %{ \"reduce_mulF $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  format %{ \"reduce_mulF_le128b $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n@@ -3487,2 +3528,2 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n@@ -3493,1 +3534,32 @@\n-instruct reduce_mulD(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n+\n+instruct reduce_mulF_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_mulF_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulF_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp1, vReg tmp2,\n+                                             pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  format %{ \"reduce_non_strict_order_mulF_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp1, $tmp2, $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_non_strict_order_mul_fp_gt128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                                             $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                             $tmp2$$FloatRegister, $pgtmp$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulD_128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n@@ -3497,1 +3569,1 @@\n-  format %{ \"reduce_mulD $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n+  format %{ \"reduce_mulD_128b $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n@@ -3499,2 +3571,32 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulD_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVD fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_mulD_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_DOUBLE, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulD_gt128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp1, vReg tmp2,\n+                                             pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVD dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  format %{ \"reduce_mulD_gt128b $dst, $dsrc, $vsrc\\t# (> 16 bits). KILL $tmp1, $tmp2, $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_non_strict_order_mul_fp_gt128b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                                             $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                             $tmp2$$FloatRegister, $pgtmp$$PRegister);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":124,"deletions":22,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -120,0 +120,7 @@\n+    \/\/ The non_strict_order implementations of Op_MulReductionVD\/F are not suitable for\n+    \/\/ auto-vectorization as the result would not conform to the JLS, Section Evaluation Order.\n+    \/\/ The strictly ordered implementations aren't profitable in terms of performance.\n+    if (opcode == Op_MulReductionVD || opcode == Op_MulReductionVF) {\n+      return false;\n+    }\n+\n@@ -132,1 +139,0 @@\n-          opcode == Op_MulReductionVD || opcode == Op_MulReductionVF ||\n@@ -186,3 +192,3 @@\n-        \/\/ No vector multiply reduction instructions, but we do\n-        \/\/ emit scalar instructions for 64\/128-bit vectors.\n-        if (length_in_bytes != 8 && length_in_bytes != 16) {\n+        \/\/ No vector multiply reduction instructions, but we do emit ASIMD instructions for\n+        \/\/ 64\/128-bit vectors. For wider vectors it's a combination of SVE and ASIMD instructions.\n+        if (length_in_bytes < 8) {\n@@ -2078,2 +2084,2 @@\n-instruct reduce_mulI(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n-                     vReg tmp1, vReg tmp2) %{\n+instruct reduce_mulI_le128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp1, vReg tmp2) %{\n@@ -2084,1 +2090,17 @@\n-  format %{ \"reduce_mulI $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  format %{ \"reduce_mulI_le128b $dst, $isrc, $vsrc\\t# vector (64\/128 bits). KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    BasicType bt = Matcher::vector_element_basic_type(this, $vsrc);\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_le128b($dst$$Register, bt, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulI_gt128b(iRegINoSp dst, iRegIorL2I isrc, vReg vsrc,\n+                            vReg tmp1, vReg tmp2, pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16);\n+  match(Set dst (MulReductionVI isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  format %{ \"reduce_mulI_gt128b $dst, $isrc, $vsrc\\t# vector (> 128 bits). KILL $tmp1, $tmp2, $pgtmp\" %}\n@@ -2086,0 +2108,1 @@\n+    assert(UseSVE > 0, \"must be sve\");\n@@ -2088,3 +2111,4 @@\n-    __ neon_reduce_mul_integral($dst$$Register, bt, $isrc$$Register,\n-                                $vsrc$$FloatRegister, length_in_bytes,\n-                                $tmp1$$FloatRegister, $tmp2$$FloatRegister);\n+    __ reduce_mul_integral_gt128b($dst$$Register, bt, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                                  $pgtmp$$PRegister);\n@@ -2095,1 +2119,1 @@\n-instruct reduce_mulL(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n+instruct reduce_mulL_128b(iRegLNoSp dst, iRegL isrc, vReg vsrc) %{\n@@ -2099,1 +2123,14 @@\n-  format %{ \"reduce_mulL $dst, $isrc, $vsrc\\t# 2L\" %}\n+  format %{ \"reduce_mulL_128b $dst, $isrc, $vsrc\\t# 2L\" %}\n+  ins_encode %{\n+    __ reduce_mul_integral_le128b($dst$$Register, T_LONG, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulL_gt128b(iRegLNoSp dst, iRegL isrc, vReg vsrc, vReg tmp,\n+                            pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16);\n+  match(Set dst (MulReductionVL isrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp, TEMP pgtmp);\n+  format %{ \"reduce_mulL_gt128b $dst, $isrc, $vsrc\\t# vector (> 128 bits). KILL $tmp, $pgtmp\" %}\n@@ -2101,2 +2138,6 @@\n-    __ neon_reduce_mul_integral($dst$$Register, T_LONG, $isrc$$Register,\n-                                $vsrc$$FloatRegister, 16, fnoreg, fnoreg);\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_integral_gt128b($dst$$Register, T_LONG, $isrc$$Register,\n+                                  $vsrc$$FloatRegister, length_in_bytes,\n+                                  $tmp$$FloatRegister, fnoreg,\n+                                  $pgtmp$$PRegister);\n@@ -2107,1 +2148,1 @@\n-instruct reduce_mulF(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+instruct reduce_mulF_le128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n@@ -2111,1 +2152,1 @@\n-  format %{ \"reduce_mulF $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n+  format %{ \"reduce_mulF_le128b $dst, $fsrc, $vsrc\\t# 2F\/4F. KILL $tmp\" %}\n@@ -2114,2 +2155,2 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n@@ -2120,1 +2161,32 @@\n-instruct reduce_mulD(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n+\n+instruct reduce_mulF_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_mulF_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulF_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp1, vReg tmp2,\n+                                             pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVF fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  format %{ \"reduce_non_strict_order_mulF_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp1, $tmp2, $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_non_strict_order_mul_fp_gt128b($dst$$FloatRegister, T_FLOAT, $fsrc$$FloatRegister,\n+                                             $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                             $tmp2$$FloatRegister, $pgtmp$$PRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulD_128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp) %{\n@@ -2124,1 +2196,1 @@\n-  format %{ \"reduce_mulD $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n+  format %{ \"reduce_mulD_128b $dst, $dsrc, $vsrc\\t# 2D. KILL $tmp\" %}\n@@ -2126,2 +2198,32 @@\n-    __ neon_reduce_mul_fp($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n-                          $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+    __ reduce_mul_fp_le128b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, 16, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_mulD_gt128b(vRegF dst, vRegF fsrc, vReg vsrc, vReg tmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVD fsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp);\n+  format %{ \"reduce_mulD_gt128b $dst, $fsrc, $vsrc\\t# (> 128 bits). KILL $tmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_mul_fp_gt128b($dst$$FloatRegister, T_DOUBLE, $fsrc$$FloatRegister,\n+                            $vsrc$$FloatRegister, length_in_bytes, $tmp$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct reduce_non_strict_order_mulD_gt128b(vRegD dst, vRegD dsrc, vReg vsrc, vReg tmp1, vReg tmp2,\n+                                             pRegGov pgtmp) %{\n+  predicate(Matcher::vector_length_in_bytes(n->in(2)) > 16 && !n->as_Reduction()->requires_strict_order());\n+  match(Set dst (MulReductionVD dsrc vsrc));\n+  effect(TEMP_DEF dst, TEMP tmp1, TEMP tmp2, TEMP pgtmp);\n+  format %{ \"reduce_mulD_gt128b $dst, $dsrc, $vsrc\\t# (> 16 bits). KILL $tmp1, $tmp2, $pgtmp\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this, $vsrc);\n+    __ reduce_non_strict_order_mul_fp_gt128b($dst$$FloatRegister, T_DOUBLE, $dsrc$$FloatRegister,\n+                                             $vsrc$$FloatRegister, length_in_bytes, $tmp1$$FloatRegister,\n+                                             $tmp2$$FloatRegister, $pgtmp$$PRegister);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4","additions":124,"deletions":22,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -4067,0 +4067,7 @@\n+  \/\/ SVE move prefix (unpredicated)\n+  void sve_movprfx(FloatRegister Zd, FloatRegister Zn) {\n+    starti;\n+    f(0b00000100, 31, 24), f(0b00, 23, 22), f(0b1, 21), f(0b00000, 20, 16);\n+    f(0b101111, 15, 10), rf(Zn, 5), rf(Zd, 0);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1995,0 +1995,1 @@\n+\/\/ Note: vsrc and vtmp2 may match.\n@@ -1996,4 +1997,4 @@\n-void C2_MacroAssembler::neon_reduce_mul_integral(Register dst, BasicType bt,\n-                                                 Register isrc, FloatRegister vsrc,\n-                                                 unsigned vector_length_in_bytes,\n-                                                 FloatRegister vtmp1, FloatRegister vtmp2) {\n+void C2_MacroAssembler::reduce_mul_integral_le128b(Register dst, BasicType bt, Register isrc,\n+                                                   FloatRegister vsrc,\n+                                                   unsigned vector_length_in_bytes,\n+                                                   FloatRegister vtmp1, FloatRegister vtmp2) {\n@@ -2001,0 +2002,2 @@\n+  assert_different_registers(vtmp1, vsrc);\n+  assert_different_registers(vtmp1, vtmp2);\n@@ -2003,1 +2006,1 @@\n-  BLOCK_COMMENT(\"neon_reduce_mul_integral {\");\n+  BLOCK_COMMENT(\"reduce_mul_integral_le128b {\");\n@@ -2034,3 +2037,3 @@\n-          ins(vtmp2, D, vsrc, 0, 1);\n-          mulv(vtmp2, T4H, vtmp2, vsrc);\n-          ins(vtmp1, S, vtmp2, 0, 1);\n+          ins(vtmp1, D, vsrc, 0, 1);\n+          mulv(vtmp1, T4H, vtmp1, vsrc);\n+          ins(vtmp2, S, vtmp1, 0, 1);\n@@ -2071,1 +2074,37 @@\n-  BLOCK_COMMENT(\"} neon_reduce_mul_integral\");\n+  BLOCK_COMMENT(\"} reduce_mul_integral_le128b\");\n+}\n+\n+\/\/ Vector reduction multiply for integral type with SVE instructions. Multiplies halves of the\n+\/\/ source vector to get to a 128b vector that fits into a SIMD&FP register. After that point ASIMD\n+\/\/ instructions are used. Note: temporary registers vtmp1 and vtmp2 are not used in some cases.\n+\/\/ Clobbers: rscratch1\n+void C2_MacroAssembler::reduce_mul_integral_gt128b(Register dst, BasicType bt, Register isrc,\n+                                                   FloatRegister vsrc,\n+                                                   unsigned vector_length_in_bytes,\n+                                                   FloatRegister vtmp1, FloatRegister vtmp2,\n+                                                   PRegister pgtmp) {\n+  assert(vector_length_in_bytes > FloatRegister::neon_vl, \"ASIMD impl should be used instead\");\n+  assert(vector_length_in_bytes <= FloatRegister::sve_vl_max, \"unsupported vector length\");\n+  assert(is_power_of_2(vector_length_in_bytes), \"unsupported vector length\");\n+\n+  BLOCK_COMMENT(\"reduce_mul_integral_gt128b {\");\n+  unsigned vector_length = vector_length_in_bytes \/ type2aelembytes(bt);\n+\n+  \/\/ Handle the first iteration separately to preserve the original values in vsrc\n+  sve_gen_mask_imm(pgtmp, bt, vector_length \/ 2);          \/\/ gen mask\n+  sve_movprfx(vtmp1, vsrc);                                \/\/ copy\n+  sve_ext(vtmp1, vtmp1, vector_length_in_bytes \/ 2);       \/\/ swap halves\n+  sve_mul(vtmp1, elemType_to_regVariant(bt), pgtmp, vsrc); \/\/ multiply halves\n+  vector_length_in_bytes = vector_length_in_bytes \/ 2;\n+  vector_length = vector_length \/ 2;\n+\n+  while (vector_length_in_bytes > FloatRegister::neon_vl) {\n+    sve_movprfx(vtmp2, vtmp1);                                \/\/ copy\n+    sve_ext(vtmp2, vtmp2, vector_length_in_bytes \/ 2);        \/\/ swap halves\n+    sve_mul(vtmp1, elemType_to_regVariant(bt), pgtmp, vtmp2); \/\/ multiply halves\n+    vector_length_in_bytes = vector_length_in_bytes \/ 2;\n+    vector_length = vector_length \/ 2;\n+  }\n+\n+  reduce_mul_integral_le128b(dst, bt, isrc, vtmp1, FloatRegister::neon_vl, vtmp2, vtmp1);\n+  BLOCK_COMMENT(\"} reduce_mul_integral_gt128b\");\n@@ -2075,4 +2114,4 @@\n-void C2_MacroAssembler::neon_reduce_mul_fp(FloatRegister dst, BasicType bt,\n-                                           FloatRegister fsrc, FloatRegister vsrc,\n-                                           unsigned vector_length_in_bytes,\n-                                           FloatRegister vtmp) {\n+\/\/ Strictly-ordered, used for both strictly-ordered and unordered operations.\n+void C2_MacroAssembler::reduce_mul_fp_le128b(FloatRegister dst, BasicType bt, FloatRegister fsrc,\n+                                             FloatRegister vsrc, unsigned vector_length_in_bytes,\n+                                             FloatRegister vtmp) {\n@@ -2082,1 +2121,1 @@\n-  BLOCK_COMMENT(\"neon_reduce_mul_fp {\");\n+  BLOCK_COMMENT(\"reduce_mul_fp_le128b {\");\n@@ -2105,1 +2144,73 @@\n-  BLOCK_COMMENT(\"} neon_reduce_mul_fp\");\n+  BLOCK_COMMENT(\"} reduce_mul_fp_le128b\");\n+}\n+\n+\/\/ Strictly-ordered vector reduction multiply for floating-point type with SVE instructions.\n+void C2_MacroAssembler::reduce_mul_fp_gt128b(FloatRegister dst, BasicType bt, FloatRegister fsrc,\n+                                             FloatRegister vsrc, unsigned vector_length_in_bytes,\n+                                             FloatRegister vtmp) {\n+  assert(vector_length_in_bytes > FloatRegister::neon_vl, \"ASIMD impl should be used instead\");\n+  assert(vector_length_in_bytes <= FloatRegister::sve_vl_max, \"unsupported vector length\");\n+  assert(is_power_of_2(vector_length_in_bytes), \"unsupported vector length\");\n+\n+  BLOCK_COMMENT(\"reduce_mul_fp_gt128b {\");\n+    \/\/ Scalar multiply the bottom element of the vector\n+    switch (bt) {\n+    case T_FLOAT:\n+      fmuls(dst, fsrc, vsrc);\n+      break;\n+    case T_DOUBLE:\n+      fmuld(dst, fsrc, vsrc);\n+      break;\n+    default:\n+      assert(false, \"unsupported\");\n+      ShouldNotReachHere();\n+    }\n+\n+    for (unsigned i = 0; i < vector_length_in_bytes \/ type2aelembytes(bt); i++) {\n+      \/\/ Shuffle the elements one position to the right\n+      sve_ext(vtmp, i ? vtmp : vsrc, type2aelembytes(bt));\n+      \/\/ Scalar multiply the bottom element of the vector\n+      switch (bt) {\n+      case T_FLOAT:\n+        fmuls(dst, dst, vtmp);\n+        break;\n+      case T_DOUBLE:\n+        fmuld(dst, dst, vtmp);\n+        break;\n+      default:\n+        assert(false, \"unsupported\");\n+        ShouldNotReachHere();\n+      }\n+    }\n+  BLOCK_COMMENT(\"} reduce_mul_fp_gt128b\");\n+}\n+\n+\/\/ Unordered vector reduction multiply for floating-point type with SVE instructions. Multiplies\n+\/\/ halves of the source vector to get to a 128b vector that fits into a SIMD&FP register. After that\n+\/\/ point ASIMD instructions are used.\n+void C2_MacroAssembler::reduce_non_strict_order_mul_fp_gt128b(\n+    FloatRegister dst, BasicType bt, FloatRegister fsrc, FloatRegister vsrc,\n+    unsigned vector_length_in_bytes, FloatRegister vtmp1, FloatRegister vtmp2, PRegister pgtmp) {\n+  assert(vector_length_in_bytes > FloatRegister::neon_vl, \"ASIMD impl should be used instead\");\n+  assert(vector_length_in_bytes <= FloatRegister::sve_vl_max, \"unsupported vector length\");\n+  assert(is_power_of_2(vector_length_in_bytes), \"unsupported vector length\");\n+\n+  \/\/ Handle the first iteration separately to preserve the original values in vsrc\n+  unsigned vector_length = vector_length_in_bytes \/ type2aelembytes(bt);\n+  sve_gen_mask_imm(pgtmp, bt, vector_length \/ 2);           \/\/ gen mask\n+  sve_movprfx(vtmp1, vsrc);                                 \/\/ copy\n+  sve_ext(vtmp1, vtmp1, vector_length_in_bytes \/ 2);        \/\/ swap halves\n+  sve_fmul(vtmp1, elemType_to_regVariant(bt), pgtmp, vsrc); \/\/ multiply halves\n+  vector_length_in_bytes = vector_length_in_bytes \/ 2;\n+\n+  BLOCK_COMMENT(\"reduce_non_strict_order_mul_fp_gt128b {\");\n+  while (vector_length_in_bytes > FloatRegister::neon_vl) {\n+    unsigned vector_length = vector_length_in_bytes \/ type2aelembytes(bt);\n+    sve_movprfx(vtmp2, vtmp1);                                 \/\/ copy\n+    sve_ext(vtmp2, vtmp2, vector_length_in_bytes \/ 2);         \/\/ swap halves\n+    sve_fmul(vtmp1, elemType_to_regVariant(bt), pgtmp, vtmp2); \/\/ multiply halves\n+    vector_length_in_bytes = vector_length_in_bytes \/ 2;\n+  }\n+\n+  reduce_mul_fp_le128b(dst, bt, fsrc, vtmp1, FloatRegister::neon_vl, vtmp2);\n+  BLOCK_COMMENT(\"} reduce_non_strict_order_mul_fp_gt128b\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":126,"deletions":15,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -128,4 +128,13 @@\n-  void neon_reduce_mul_integral(Register dst, BasicType bt,\n-                                Register isrc, FloatRegister vsrc,\n-                                unsigned vector_length_in_bytes,\n-                                FloatRegister vtmp1, FloatRegister vtmp2);\n+  void reduce_mul_integral_le128b(Register dst, BasicType bt, Register isrc, FloatRegister vsrc,\n+                                  unsigned vector_length_in_bytes, FloatRegister vtmp1,\n+                                  FloatRegister vtmp2);\n+\n+  void reduce_mul_integral_gt128b(Register dst, BasicType bt, Register isrc, FloatRegister vsrc,\n+                                  unsigned vector_length_in_bytes, FloatRegister vtmp1,\n+                                  FloatRegister vtmp2, PRegister pgtmp1);\n+\n+  void reduce_mul_fp_le128b(FloatRegister dst, BasicType bt, FloatRegister fsrc, FloatRegister vsrc,\n+                            unsigned vector_length_in_bytes, FloatRegister vtmp);\n+\n+  void reduce_mul_fp_gt128b(FloatRegister dst, BasicType bt, FloatRegister fsrc, FloatRegister vsrc,\n+                            unsigned vector_length_in_bytes, FloatRegister vtmp);\n@@ -133,3 +142,4 @@\n-  void neon_reduce_mul_fp(FloatRegister dst, BasicType bt,\n-                          FloatRegister fsrc, FloatRegister vsrc,\n-                          unsigned vector_length_in_bytes, FloatRegister vtmp);\n+  void reduce_non_strict_order_mul_fp_gt128b(FloatRegister dst, BasicType bt, FloatRegister fsrc,\n+                                             FloatRegister vsrc, unsigned vector_length_in_bytes,\n+                                             FloatRegister vtmp1, FloatRegister vtmp2,\n+                                             PRegister pgtmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":17,"deletions":7,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -4714,1 +4714,2 @@\n-    Node* post_loop_reduction = ReductionNode::make(sopc, nullptr, init, last_accumulator, bt);\n+    Node* post_loop_reduction = ReductionNode::make(sopc, nullptr, init, last_accumulator, bt,\n+                                                    \/* requires_strict_order *\/ false);\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2101,0 +2101,4 @@\n+                        [\"mov\",      \"__ sve_mov(p3, p1, p7);\",                            \"and\\tp3.b, p1\/z, p7.b, p1.b\"],\n+                        [\"movs\",     \"__ sve_movs(p4, p12, p5);\",                          \"ands\\tp4.b, p12\/z, p5.b, p12.b\"],\n+                        [\"not\",      \"__ sve_not(p3, p1, p7);\",                            \"eor\\tp3.b, p1\/z, p7.b, p1.b\"],\n+                        [\"nots\",     \"__ sve_nots(p4, p12, p5);\",                          \"eors\\tp4.b, p12\/z, p5.b, p12.b\"],\n@@ -2132,0 +2136,1 @@\n+                        [\"movprfx\",  \"__ sve_movprfx(z17, z15);\",                          \"movprfx\\tz17, z15\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1114,0 +1114,4 @@\n+    __ sve_mov(p3, p1, p7);                            \/\/       and     p3.b, p1\/z, p7.b, p1.b\n+    __ sve_movs(p4, p12, p5);                          \/\/       ands    p4.b, p12\/z, p5.b, p12.b\n+    __ sve_not(p3, p1, p7);                            \/\/       eor     p3.b, p1\/z, p7.b, p1.b\n+    __ sve_nots(p4, p12, p5);                          \/\/       eors    p4.b, p12\/z, p5.b, p12.b\n@@ -1145,0 +1149,1 @@\n+    __ sve_movprfx(z17, z15);                          \/\/       movprfx z17, z15\n@@ -1441,7 +1446,7 @@\n-    0x14000000,     0x17ffffd7,     0x140004b0,     0x94000000,\n-    0x97ffffd4,     0x940004ad,     0x3400000a,     0x34fffa2a,\n-    0x3400954a,     0x35000008,     0x35fff9c8,     0x350094e8,\n-    0xb400000b,     0xb4fff96b,     0xb400948b,     0xb500001d,\n-    0xb5fff91d,     0xb500943d,     0x10000013,     0x10fff8b3,\n-    0x100093d3,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36309356,     0x3758000c,     0x375ff7cc,     0x375892ec,\n+    0x14000000,     0x17ffffd7,     0x140004b5,     0x94000000,\n+    0x97ffffd4,     0x940004b2,     0x3400000a,     0x34fffa2a,\n+    0x340095ea,     0x35000008,     0x35fff9c8,     0x35009588,\n+    0xb400000b,     0xb4fff96b,     0xb400952b,     0xb500001d,\n+    0xb5fff91d,     0xb50094dd,     0x10000013,     0x10fff8b3,\n+    0x10009473,     0x90000013,     0x36300016,     0x3637f836,\n+    0x363093f6,     0x3758000c,     0x375ff7cc,     0x3758938c,\n@@ -1452,13 +1457,13 @@\n-    0x540090c0,     0x54000001,     0x54fff541,     0x54009061,\n-    0x54000002,     0x54fff4e2,     0x54009002,     0x54000002,\n-    0x54fff482,     0x54008fa2,     0x54000003,     0x54fff423,\n-    0x54008f43,     0x54000003,     0x54fff3c3,     0x54008ee3,\n-    0x54000004,     0x54fff364,     0x54008e84,     0x54000005,\n-    0x54fff305,     0x54008e25,     0x54000006,     0x54fff2a6,\n-    0x54008dc6,     0x54000007,     0x54fff247,     0x54008d67,\n-    0x54000008,     0x54fff1e8,     0x54008d08,     0x54000009,\n-    0x54fff189,     0x54008ca9,     0x5400000a,     0x54fff12a,\n-    0x54008c4a,     0x5400000b,     0x54fff0cb,     0x54008beb,\n-    0x5400000c,     0x54fff06c,     0x54008b8c,     0x5400000d,\n-    0x54fff00d,     0x54008b2d,     0x5400000e,     0x54ffefae,\n-    0x54008ace,     0x5400000f,     0x54ffef4f,     0x54008a6f,\n+    0x54009160,     0x54000001,     0x54fff541,     0x54009101,\n+    0x54000002,     0x54fff4e2,     0x540090a2,     0x54000002,\n+    0x54fff482,     0x54009042,     0x54000003,     0x54fff423,\n+    0x54008fe3,     0x54000003,     0x54fff3c3,     0x54008f83,\n+    0x54000004,     0x54fff364,     0x54008f24,     0x54000005,\n+    0x54fff305,     0x54008ec5,     0x54000006,     0x54fff2a6,\n+    0x54008e66,     0x54000007,     0x54fff247,     0x54008e07,\n+    0x54000008,     0x54fff1e8,     0x54008da8,     0x54000009,\n+    0x54fff189,     0x54008d49,     0x5400000a,     0x54fff12a,\n+    0x54008cea,     0x5400000b,     0x54fff0cb,     0x54008c8b,\n+    0x5400000c,     0x54fff06c,     0x54008c2c,     0x5400000d,\n+    0x54fff00d,     0x54008bcd,     0x5400000e,     0x54ffefae,\n+    0x54008b6e,     0x5400000f,     0x54ffef4f,     0x54008b0f,\n@@ -1674,1 +1679,2 @@\n-    0x250b5d3a,     0x2550dc20,     0x2518e3e1,     0x2518e021,\n+    0x250b5d3a,     0x250144e3,     0x254c70a4,     0x250146e3,\n+    0x254c72a4,     0x2550dc20,     0x2518e3e1,     0x2518e021,\n@@ -1682,60 +1688,60 @@\n-    0x05271e11,     0x6545e891,     0x6585e891,     0x65c5e891,\n-    0x6545c891,     0x6585c891,     0x65c5c891,     0x45b0c210,\n-    0x45f1c231,     0x1e601000,     0x1e603000,     0x1e621000,\n-    0x1e623000,     0x1e641000,     0x1e643000,     0x1e661000,\n-    0x1e663000,     0x1e681000,     0x1e683000,     0x1e6a1000,\n-    0x1e6a3000,     0x1e6c1000,     0x1e6c3000,     0x1e6e1000,\n-    0x1e6e3000,     0x1e701000,     0x1e703000,     0x1e721000,\n-    0x1e723000,     0x1e741000,     0x1e743000,     0x1e761000,\n-    0x1e763000,     0x1e781000,     0x1e783000,     0x1e7a1000,\n-    0x1e7a3000,     0x1e7c1000,     0x1e7c3000,     0x1e7e1000,\n-    0x1e7e3000,     0xf8268267,     0xf82d023c,     0xf8301046,\n-    0xf83d2083,     0xf8263290,     0xf82d528c,     0xf8284299,\n-    0xf8337160,     0xf8386286,     0xf8bf820e,     0xf8a600e0,\n-    0xf8af1353,     0xf8a922ea,     0xf8b53396,     0xf8a251e3,\n-    0xf8b340f4,     0xf8a470fd,     0xf8a06209,     0xf8f48097,\n-    0xf8f002ea,     0xf8eb10d9,     0xf8ff21b0,     0xf8f7302c,\n-    0xf8ee52a9,     0xf8f041fa,     0xf8e471e4,     0xf8e863c6,\n-    0xf864823d,     0xf87d013a,     0xf86f1162,     0xf87d20e3,\n-    0xf86132bb,     0xf870510e,     0xf8704336,     0xf86572b4,\n-    0xf8706217,     0xb83e8294,     0xb8200264,     0xb8381284,\n-    0xb8242358,     0xb8333102,     0xb828530e,     0xb83042df,\n-    0xb824703f,     0xb82a6194,     0xb8a080e9,     0xb8b80090,\n-    0xb8bb1146,     0xb8bb21b8,     0xb8b032df,     0xb8b653f4,\n-    0xb8bd41c9,     0xb8b47287,     0xb8bc6169,     0xb8ee828c,\n-    0xb8e10138,     0xb8f3126d,     0xb8f020b0,     0xb8e03183,\n-    0xb8e851ef,     0xb8f041e4,     0xb8fe7005,     0xb8ea6376,\n-    0xb8638120,     0xb873015d,     0xb8781284,     0xb86723b8,\n-    0xb86e3175,     0xb87b51ed,     0xb87f41d1,     0xb863721e,\n-    0xb87660f4,     0xce216874,     0xce104533,     0xce648c15,\n-    0xce8e3302,     0xce6e82ab,     0xce6c87d1,     0xcec08063,\n-    0xce638937,     0x25e0c358,     0x25a1c7d3,     0x0580785a,\n-    0x05426328,     0x05009892,     0x25a0cc29,     0x2561cec8,\n-    0x058044b3,     0x05401c99,     0x05006b49,     0x25e0d6f7,\n-    0x2561c528,     0x0583c8bc,     0x0542522f,     0x05001ec0,\n-    0x25e0de65,     0x25a1c113,     0x05803cad,     0x0540f3c0,\n-    0x0500ab15,     0x2560c28c,     0x2561d7c0,     0x05801ed7,\n-    0x0542633b,     0x05003696,     0x2560d4b4,     0x25e1c918,\n-    0x058021ff,     0x05400e15,     0x0500f3de,     0x0473025a,\n-    0x04bd05ab,     0x658e0025,     0x658a08e2,     0x659a0493,\n-    0x043e1062,     0x04f418b4,     0x046d15bd,     0x04611fce,\n-    0x04d6a07c,     0x04001929,     0x041a09da,     0x04d098f4,\n-    0x04db10d4,     0x0459a3ad,     0x041aa029,     0x041919fb,\n-    0x04d39e24,     0x04118302,     0x04101dba,     0x04d7ae16,\n-    0x04dea571,     0x04180210,     0x05e786fc,     0x05e4915c,\n-    0x04881cf1,     0x044a0f04,     0x04090969,     0x048b16c4,\n-    0x044101e4,     0x04dcbf44,     0x65809745,     0x658d833f,\n-    0x65c68468,     0x65c79b07,     0x65829e38,     0x049dafca,\n-    0x6582bba8,     0x65c0b7ff,     0x65c1b4e0,     0x658dbadd,\n-    0x65819a9d,     0x65ed9246,     0x65b30815,     0x65e6263c,\n-    0x65eebb94,     0x65bad14e,     0x65efe178,     0x65fc5697,\n-    0x65e07f14,     0x040c55a6,     0x04977f4d,     0x043d3046,\n-    0x04b733a0,     0x046830a4,     0x04ed322d,     0x05686948,\n-    0x05bd6c13,     0x65c88ef0,     0x450db3d7,     0x4540b6d9,\n-    0x043e3979,     0x445896ce,     0x445a9005,     0x44d98069,\n-    0x445b87ae,     0x04da348e,     0x04982edb,     0x0499397f,\n-    0x0408338c,     0x04ca309c,     0x65c721e6,     0x65c63641,\n-    0x65982882,     0x04812b8b,     0x0e251083,     0x4e3712d5,\n-    0x0e61101f,     0x4e6d118b,     0x0eba1338,     0x4eb712d5,\n-    0x2e31120f,     0x6e2e11ac,     0x2e6810e6,     0x6e6f11cd,\n-    0x2eaa1128,     0x6eb1120f,\n+    0x0420bdf1,     0x05271e11,     0x6545e891,     0x6585e891,\n+    0x65c5e891,     0x6545c891,     0x6585c891,     0x65c5c891,\n+    0x45b0c210,     0x45f1c231,     0x1e601000,     0x1e603000,\n+    0x1e621000,     0x1e623000,     0x1e641000,     0x1e643000,\n+    0x1e661000,     0x1e663000,     0x1e681000,     0x1e683000,\n+    0x1e6a1000,     0x1e6a3000,     0x1e6c1000,     0x1e6c3000,\n+    0x1e6e1000,     0x1e6e3000,     0x1e701000,     0x1e703000,\n+    0x1e721000,     0x1e723000,     0x1e741000,     0x1e743000,\n+    0x1e761000,     0x1e763000,     0x1e781000,     0x1e783000,\n+    0x1e7a1000,     0x1e7a3000,     0x1e7c1000,     0x1e7c3000,\n+    0x1e7e1000,     0x1e7e3000,     0xf8268267,     0xf82d023c,\n+    0xf8301046,     0xf83d2083,     0xf8263290,     0xf82d528c,\n+    0xf8284299,     0xf8337160,     0xf8386286,     0xf8bf820e,\n+    0xf8a600e0,     0xf8af1353,     0xf8a922ea,     0xf8b53396,\n+    0xf8a251e3,     0xf8b340f4,     0xf8a470fd,     0xf8a06209,\n+    0xf8f48097,     0xf8f002ea,     0xf8eb10d9,     0xf8ff21b0,\n+    0xf8f7302c,     0xf8ee52a9,     0xf8f041fa,     0xf8e471e4,\n+    0xf8e863c6,     0xf864823d,     0xf87d013a,     0xf86f1162,\n+    0xf87d20e3,     0xf86132bb,     0xf870510e,     0xf8704336,\n+    0xf86572b4,     0xf8706217,     0xb83e8294,     0xb8200264,\n+    0xb8381284,     0xb8242358,     0xb8333102,     0xb828530e,\n+    0xb83042df,     0xb824703f,     0xb82a6194,     0xb8a080e9,\n+    0xb8b80090,     0xb8bb1146,     0xb8bb21b8,     0xb8b032df,\n+    0xb8b653f4,     0xb8bd41c9,     0xb8b47287,     0xb8bc6169,\n+    0xb8ee828c,     0xb8e10138,     0xb8f3126d,     0xb8f020b0,\n+    0xb8e03183,     0xb8e851ef,     0xb8f041e4,     0xb8fe7005,\n+    0xb8ea6376,     0xb8638120,     0xb873015d,     0xb8781284,\n+    0xb86723b8,     0xb86e3175,     0xb87b51ed,     0xb87f41d1,\n+    0xb863721e,     0xb87660f4,     0xce216874,     0xce104533,\n+    0xce648c15,     0xce8e3302,     0xce6e82ab,     0xce6c87d1,\n+    0xcec08063,     0xce638937,     0x25e0c358,     0x25a1c7d3,\n+    0x0580785a,     0x05426328,     0x05009892,     0x25a0cc29,\n+    0x2561cec8,     0x058044b3,     0x05401c99,     0x05006b49,\n+    0x25e0d6f7,     0x2561c528,     0x0583c8bc,     0x0542522f,\n+    0x05001ec0,     0x25e0de65,     0x25a1c113,     0x05803cad,\n+    0x0540f3c0,     0x0500ab15,     0x2560c28c,     0x2561d7c0,\n+    0x05801ed7,     0x0542633b,     0x05003696,     0x2560d4b4,\n+    0x25e1c918,     0x058021ff,     0x05400e15,     0x0500f3de,\n+    0x0473025a,     0x04bd05ab,     0x658e0025,     0x658a08e2,\n+    0x659a0493,     0x043e1062,     0x04f418b4,     0x046d15bd,\n+    0x04611fce,     0x04d6a07c,     0x04001929,     0x041a09da,\n+    0x04d098f4,     0x04db10d4,     0x0459a3ad,     0x041aa029,\n+    0x041919fb,     0x04d39e24,     0x04118302,     0x04101dba,\n+    0x04d7ae16,     0x04dea571,     0x04180210,     0x05e786fc,\n+    0x05e4915c,     0x04881cf1,     0x044a0f04,     0x04090969,\n+    0x048b16c4,     0x044101e4,     0x04dcbf44,     0x65809745,\n+    0x658d833f,     0x65c68468,     0x65c79b07,     0x65829e38,\n+    0x049dafca,     0x6582bba8,     0x65c0b7ff,     0x65c1b4e0,\n+    0x658dbadd,     0x65819a9d,     0x65ed9246,     0x65b30815,\n+    0x65e6263c,     0x65eebb94,     0x65bad14e,     0x65efe178,\n+    0x65fc5697,     0x65e07f14,     0x040c55a6,     0x04977f4d,\n+    0x043d3046,     0x04b733a0,     0x046830a4,     0x04ed322d,\n+    0x05686948,     0x05bd6c13,     0x65c88ef0,     0x450db3d7,\n+    0x4540b6d9,     0x043e3979,     0x445896ce,     0x445a9005,\n+    0x44d98069,     0x445b87ae,     0x04da348e,     0x04982edb,\n+    0x0499397f,     0x0408338c,     0x04ca309c,     0x65c721e6,\n+    0x65c63641,     0x65982882,     0x04812b8b,     0x0e251083,\n+    0x4e3712d5,     0x0e61101f,     0x4e6d118b,     0x0eba1338,\n+    0x4eb712d5,     0x2e31120f,     0x6e2e11ac,     0x2e6810e6,\n+    0x6e6f11cd,     0x2eaa1128,     0x6eb1120f,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":87,"deletions":81,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Arm Limited. All rights reserved.\n+ * Copyright (c) 2025, Arm Limited. All rights reserved.\n@@ -84,1 +84,1 @@\n-        applyIfCPUFeatureAnd = {\"asimd\", \"true\", \"sve\", \"false\"})\n+        applyIfCPUFeature = {\"asimd\", \"true\"})\n@@ -87,1 +87,1 @@\n-        applyIfCPUFeatureOr = {\"sve\", \"true\", \"sse2\", \"true\"},\n+        applyIfCPUFeature = {\"sse2\", \"true\"},\n@@ -99,1 +99,1 @@\n-        applyIfCPUFeatureAnd = {\"asimd\", \"true\", \"sve\", \"false\"})\n+        applyIfCPUFeature = {\"asimd\", \"true\"})\n@@ -102,1 +102,1 @@\n-        applyIfCPUFeatureOr = {\"sve\", \"true\", \"sse2\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\"},\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestVectorFPReduction.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"}]}