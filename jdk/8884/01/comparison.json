{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,0 +69,5 @@\n+\n+  size_t back_skip_granularity_bytes = (size_t)1 << G1LogBackScanSkipGranularity;\n+  if (back_skip_granularity_bytes > G1HeapRegionSize) {\n+    FLAG_SET_ERGO(G1LogBackScanSkipGranularity, HeapRegion::LogOfHRGrainBytes);\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -171,1 +171,1 @@\n-  T* address_mapped_to(HeapWord* address) {\n+  T* address_mapped_to(HeapWord* address) const {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BiasedArray.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-void G1BlockOffsetTablePart::update() {\n+void G1BlockOffsetTablePart::update(HeapWord* const pb) {\n@@ -89,1 +89,1 @@\n-    next_addr  = prev_addr + block_size(prev_addr);\n+    next_addr  = prev_addr + block_size(prev_addr, pb);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -125,0 +125,1 @@\n+  inline size_t block_size(const HeapWord* p, HeapWord* pb) const;\n@@ -133,1 +134,2 @@\n-                                                    const void* addr) const;\n+                                                    const void* addr,\n+                                                    HeapWord* pb) const;\n@@ -155,1 +157,1 @@\n-  void update();\n+  void update(HeapWord* pb);\n@@ -164,1 +166,1 @@\n-  inline HeapWord* block_start(const void* addr);\n+  inline HeapWord* block_start(const void* addr, HeapWord* pb);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-inline HeapWord* G1BlockOffsetTablePart::block_start(const void* addr) {\n+inline HeapWord* G1BlockOffsetTablePart::block_start(const void* addr, HeapWord* const pb) {\n@@ -38,2 +38,2 @@\n-  HeapWord* n = q + block_size(q);\n-  return forward_to_block_containing_addr(q, n, addr);\n+  HeapWord* n = q + block_size(q, pb);\n+  return forward_to_block_containing_addr(q, n, addr, pb);\n@@ -102,0 +102,4 @@\n+inline size_t G1BlockOffsetTablePart::block_size(const HeapWord* p, HeapWord* const pb) const {\n+  return _hr->block_size(p, pb);\n+}\n+\n@@ -129,1 +133,2 @@\n-                                                                          const void* addr) const {\n+                                                                          const void* addr,\n+                                                                          HeapWord* const pb) const {\n@@ -141,1 +146,1 @@\n-    n += block_size(q);\n+    n += block_size(q, pb);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.inline.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-    _cm->mark_in_next_bitmap(_worker_id, o);\n+    _cm->mark_in_bitmap(_worker_id, o);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CodeBlobClosure.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -439,1 +439,1 @@\n-                                                              : GCCause::_g1_inc_collection_pause;\n+                                                               : GCCause::_g1_inc_collection_pause;\n@@ -1080,1 +1080,1 @@\n-  \/\/ This call implicitly verifies that the next bitmap is clear after Full GC.\n+  \/\/ This call implicitly verifies that the marking bitmap is cleared after Full GC.\n@@ -1631,1 +1631,1 @@\n-  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.\n+  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmap.\n@@ -1648,4 +1648,2 @@\n-  G1RegionToSpaceMapper* prev_bitmap_storage =\n-    create_aux_memory_mapper(\"Prev Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n-  G1RegionToSpaceMapper* next_bitmap_storage =\n-    create_aux_memory_mapper(\"Next Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n+  G1RegionToSpaceMapper* bitmap_storage =\n+    create_aux_memory_mapper(\"Mark Bitmap\", bitmap_size, G1CMBitMap::heap_map_factor());\n@@ -1653,1 +1651,1 @@\n-  _hrm.initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n+  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n@@ -1700,1 +1698,1 @@\n-  _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);\n+  _cm = new G1ConcurrentMark(this, bitmap_storage);\n@@ -2351,1 +2349,1 @@\n-  return hr->block_start(addr);\n+  return hr->block_start(addr, hr->parsable_bottom_acquire());\n@@ -2356,1 +2354,1 @@\n-  return hr->block_is_obj(addr);\n+  return hr->block_is_obj(addr, hr->parsable_bottom_acquire());\n@@ -2411,1 +2409,1 @@\n-    case VerifyOption::G1UsePrevMarking: return is_obj_dead(obj, hr);\n+    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj, hr);\n@@ -2421,1 +2419,1 @@\n-    case VerifyOption::G1UsePrevMarking: return is_obj_dead(obj);\n+    case VerifyOption::G1UseConcMarking: return is_obj_dead(obj);\n@@ -2471,1 +2469,2 @@\n-               \"TAMS=top-at-mark-start (previous, next)\");\n+               \"TAMS=top-at-mark-start, \"\n+               \"PB=parsable bottom\");\n@@ -2913,1 +2912,1 @@\n-      _cm->mark_in_next_bitmap(0 \/* worker_id *\/, pll_head);\n+      _cm->mark_in_bitmap(0 \/* worker_id *\/, pll_head);\n@@ -2956,1 +2955,1 @@\n-void G1CollectedHeap::clear_prev_bitmap_for_region(HeapRegion* hr) {\n+void G1CollectedHeap::clear_bitmap_for_region(HeapRegion* hr) {\n@@ -2958,1 +2957,1 @@\n-  concurrent_mark()->clear_range_in_prev_bitmap(mr);\n+  concurrent_mark()->clear_range_in_bitmap(mr);\n@@ -2966,4 +2965,0 @@\n-  if (G1VerifyBitmaps) {\n-    clear_prev_bitmap_for_region(hr);\n-  }\n-\n@@ -3301,1 +3296,1 @@\n-    _cm->root_regions()->add(alloc_region->next_top_at_mark_start(), alloc_region->top());\n+    _cm->root_regions()->add(alloc_region->top_at_mark_start(), alloc_region->top());\n@@ -3322,3 +3317,3 @@\n-  \/\/ that we'll update the prev marking info so that they are\n-  \/\/ all under PTAMS and explicitly marked.\n-  _cm->par_mark_in_prev_bitmap(obj);\n+  \/\/ that we'll update the marking info so that they are\n+  \/\/ all below TAMS and explicitly marked.\n+  _cm->raw_mark_in_bitmap(obj);\n@@ -3328,1 +3323,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":20,"deletions":26,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -626,1 +626,1 @@\n-  void clear_prev_bitmap_for_region(HeapRegion* hr);\n+  void clear_bitmap_for_region(HeapRegion* hr);\n@@ -1246,3 +1246,1 @@\n-\n-  \/\/ Added if it is NULL it isn't dead.\n-\n+  \/\/ If obj is NULL it is not dead.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/g1\/g1ConcurrentMark.inline.hpp\"\n@@ -162,1 +163,1 @@\n-  return _cm->next_mark_bitmap()->is_marked(obj);\n+  return _cm->mark_bitmap()->is_marked(obj);\n@@ -223,1 +224,1 @@\n-  return hr->is_obj_dead(obj, _cm->prev_mark_bitmap());\n+  return hr->is_obj_dead(obj, hr->parsable_bottom());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -378,1 +378,1 @@\n-    _st->print_cr(\"  \" HR_FORMAT \", P: \" PTR_FORMAT \"N: \" PTR_FORMAT \", age: %4d\",\n+    _st->print_cr(\"  \" HR_FORMAT \", TAMS: \" PTR_FORMAT \" PB: \" PTR_FORMAT \", age: %4d\",\n@@ -380,2 +380,2 @@\n-                  p2i(r->prev_top_at_mark_start()),\n-                  p2i(r->next_top_at_mark_start()),\n+                  p2i(r->top_at_mark_start()),\n+                  p2i(r->parsable_bottom()),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectionSet.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,2 +45,2 @@\n-  \/\/ previous marking cycle (e.g., clearing the next marking\n-  \/\/ bitmap). If that is the case we cannot start a new cycle and\n+  \/\/ previous marking cycle (e.g., clearing the marking bitmap).\n+  \/\/ If that is the case we cannot start a new cycle and\n@@ -67,3 +67,2 @@\n-  \/\/ The next bitmap is currently being cleared or about to be cleared. TAMS and bitmap\n-  \/\/ may be out of sync.\n-  bool _clearing_next_bitmap;\n+  \/\/ The marking bitmap is currently being cleared or about to be cleared.\n+  bool _clearing_bitmap;\n@@ -83,1 +82,1 @@\n-    _clearing_next_bitmap(false),\n+    _clearing_bitmap(false),\n@@ -97,1 +96,1 @@\n-  void set_clearing_next_bitmap(bool v) { _clearing_next_bitmap = v; }\n+  void set_clearing_bitmap(bool v) { _clearing_bitmap = v; }\n@@ -111,1 +110,1 @@\n-  bool clearing_next_bitmap() const { return _clearing_next_bitmap; }\n+  bool clearing_bitmap() const { return _clearing_bitmap; }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectorState.hpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/g1\/g1ConcurrentRebuildAndScrub.hpp\"\n@@ -364,2 +365,1 @@\n-                                   G1RegionToSpaceMapper* prev_bitmap_storage,\n-                                   G1RegionToSpaceMapper* next_bitmap_storage) :\n+                                   G1RegionToSpaceMapper* bitmap_storage) :\n@@ -369,4 +369,1 @@\n-  _mark_bitmap_1(),\n-  _mark_bitmap_2(),\n-  _prev_mark_bitmap(&_mark_bitmap_1),\n-  _next_mark_bitmap(&_mark_bitmap_2),\n+  _mark_bitmap(),\n@@ -421,2 +418,1 @@\n-  _mark_bitmap_1.initialize(g1h->reserved(), prev_bitmap_storage);\n-  _mark_bitmap_2.initialize(g1h->reserved(), next_bitmap_storage);\n+  _mark_bitmap.initialize(g1h->reserved(), bitmap_storage);\n@@ -469,1 +465,1 @@\n-    _tasks[i]->reset(_next_mark_bitmap);\n+    _tasks[i]->reset(mark_bitmap());\n@@ -502,6 +498,0 @@\n-static void clear_mark_if_set(G1CMBitMap* bitmap, HeapWord* addr) {\n-  if (bitmap->is_marked(addr)) {\n-    bitmap->clear(addr);\n-  }\n-}\n-\n@@ -511,3 +501,2 @@\n-  \/\/ Need to clear all mark bits of the humongous object.\n-  clear_mark_if_set(_prev_mark_bitmap, r->bottom());\n-  clear_mark_if_set(_next_mark_bitmap, r->bottom());\n+  \/\/ Need to clear mark bit of the humongous object. Doing this unconditionally is fine.\n+  mark_bitmap()->clear(r->bottom());\n@@ -589,1 +578,1 @@\n-  \/\/ Heap region closure used for clearing the _next_mark_bitmap.\n+  \/\/ Heap region closure used for clearing the _mark_bitmap.\n@@ -613,5 +602,3 @@\n-      \/\/ During a Concurrent Undo Mark cycle, the _next_mark_bitmap is  cleared\n-      \/\/ without swapping with the _prev_mark_bitmap. Therefore, the per region\n-      \/\/ next_top_at_mark_start and live_words data are current wrt\n-      \/\/ _next_mark_bitmap. We use this information to only clear ranges of the\n-      \/\/ bitmap that require clearing.\n+      \/\/ During a Concurrent Undo Mark cycle, the per region top_at_mark_start and\n+      \/\/ live_words data are current wrt to the _mark_bitmap. We use this information\n+      \/\/ to only clear ranges of the bitmap that require clearing.\n@@ -624,1 +611,1 @@\n-        assert(_bitmap->get_next_marked_addr(r->next_top_at_mark_start(), r->end()) == r->end(), \"Should not have marked bits above ntams\");\n+        assert(_bitmap->get_next_marked_addr(r->top_at_mark_start(), r->end()) == r->end(), \"Should not have marked bits above tams\");\n@@ -633,1 +620,1 @@\n-      _bitmap(cm->next_mark_bitmap()),\n+      _bitmap(cm->mark_bitmap()),\n@@ -694,1 +681,1 @@\n-void G1ConcurrentMark::clear_next_bitmap(WorkerThreads* workers, bool may_yield) {\n+void G1ConcurrentMark::clear_bitmap(WorkerThreads* workers, bool may_yield) {\n@@ -720,1 +707,1 @@\n-  clear_next_bitmap(_concurrent_workers, true);\n+  clear_bitmap(_concurrent_workers, true);\n@@ -727,1 +714,1 @@\n-void G1ConcurrentMark::clear_next_bitmap(WorkerThreads* workers) {\n+void G1ConcurrentMark::clear_bitmap(WorkerThreads* workers) {\n@@ -734,1 +721,1 @@\n-  clear_next_bitmap(workers, false);\n+  clear_bitmap(workers, false);\n@@ -957,1 +944,1 @@\n-  assert(hr->is_old() || hr->next_top_at_mark_start() == hr->bottom(),\n+  assert(hr->is_old() || hr->top_at_mark_start() == hr->bottom(),\n@@ -959,2 +946,2 @@\n-  assert(hr->next_top_at_mark_start() == region->start(),\n-         \"MemRegion start should be equal to nTAMS\");\n+  assert(hr->top_at_mark_start() == region->start(),\n+         \"MemRegion start should be equal to TAMS\");\n@@ -1028,1 +1015,1 @@\n-  _g1h->collector_state()->set_clearing_next_bitmap(false);\n+  _g1h->collector_state()->set_clearing_bitmap(false);\n@@ -1161,1 +1148,0 @@\n-      hr->add_to_marked_bytes(marked_bytes);\n@@ -1163,1 +1149,1 @@\n-      hr->note_end_of_marking();\n+      hr->note_end_of_marking(marked_bytes);\n@@ -1197,1 +1183,1 @@\n-class G1UpdateRemSetTrackingAfterRebuild : public HeapRegionClosure {\n+class G1UpdateRegionsAfterRebuild : public HeapRegionClosure {\n@@ -1199,0 +1185,1 @@\n+\n@@ -1200,1 +1187,3 @@\n-  G1UpdateRemSetTrackingAfterRebuild(G1CollectedHeap* g1h) : _g1h(g1h) { }\n+  G1UpdateRegionsAfterRebuild(G1CollectedHeap* g1h) :\n+    _g1h(g1h) {\n+  }\n@@ -1203,0 +1192,2 @@\n+    \/\/ Update the remset tracking state from updating to complete\n+    \/\/ if remembered sets have been rebuilt.\n@@ -1222,1 +1213,1 @@\n-  verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UsePrevMarking, \"Remark before\");\n+  verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UseConcMarking, \"Remark before\");\n@@ -1247,4 +1238,1 @@\n-    \/\/ Install newly created mark bitmap as \"prev\".\n-    swap_mark_bitmaps();\n-\n-    _g1h->collector_state()->set_clearing_next_bitmap(true);\n+    _g1h->collector_state()->set_clearing_bitmap(true);\n@@ -1283,1 +1271,1 @@\n-    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UsePrevMarking, \"Remark after\");\n+    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UseConcMarking, \"Remark after\");\n@@ -1292,1 +1280,1 @@\n-    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UsePrevMarking, \"Remark overflow\");\n+    verify_during_pause(G1HeapVerifier::G1VerifyRemark, VerifyOption::G1UseConcMarking, \"Remark overflow\");\n@@ -1438,1 +1426,1 @@\n-  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption::G1UsePrevMarking, \"Cleanup before\");\n+  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption::G1UseConcMarking, \"Cleanup before\");\n@@ -1441,0 +1429,2 @@\n+    \/\/ Update the remset tracking information as well as marking all regions\n+    \/\/ as fully parsable.\n@@ -1442,1 +1432,1 @@\n-    G1UpdateRemSetTrackingAfterRebuild cl(_g1h);\n+    G1UpdateRegionsAfterRebuild cl(_g1h);\n@@ -1445,1 +1435,1 @@\n-    log_debug(gc, phases)(\"No Remembered Sets to update after rebuild\");\n+    log_debug(gc, phases)(\"No Remembered Set Tracking Update After Rebuild\");\n@@ -1448,1 +1438,1 @@\n-  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption::G1UsePrevMarking, \"Cleanup after\");\n+  verify_during_pause(G1HeapVerifier::G1VerifyCleanup, VerifyOption::G1UseConcMarking, \"Cleanup after\");\n@@ -1713,2 +1703,0 @@\n-\/\/ When sampling object counts, we already swapped the mark bitmaps, so we need to use\n-\/\/ the prev bitmap determining liveness.\n@@ -1728,1 +1716,1 @@\n-  \/\/ using either the next or prev bitmap.\n+  \/\/ using either the bitmap or after the cycle using the scrubbing information.\n@@ -1738,7 +1726,0 @@\n-\n-void G1ConcurrentMark::swap_mark_bitmaps() {\n-  G1CMBitMap* temp = _prev_mark_bitmap;\n-  _prev_mark_bitmap = _next_mark_bitmap;\n-  _next_mark_bitmap = temp;\n-}\n-\n@@ -1876,2 +1857,3 @@\n-void G1ConcurrentMark::clear_range_in_prev_bitmap(MemRegion mr) {\n-  _prev_mark_bitmap->clear_range(mr);\n+void G1ConcurrentMark::clear_range_in_bitmap(MemRegion mr) {\n+  assert_at_safepoint();\n+  _mark_bitmap.clear_range(mr);\n@@ -1899,2 +1881,2 @@\n-      HeapWord*   bottom        = curr_region->bottom();\n-      HeapWord*   limit         = curr_region->next_top_at_mark_start();\n+      HeapWord* bottom = curr_region->bottom();\n+      HeapWord* limit = curr_region->top_at_mark_start();\n@@ -1997,3 +1979,1 @@\n-void G1ConcurrentMark::rebuild_rem_set_concurrently() {\n-  \/\/ If Remark did not select any regions for RemSet rebuild,\n-  \/\/ skip the rebuild remembered set phase\n+void G1ConcurrentMark::rebuild_and_scrub() {\n@@ -2001,2 +1981,1 @@\n-    log_debug(gc, marking)(\"Skipping Remembered Set Rebuild. No regions selected for rebuild\");\n-    return;\n+    log_debug(gc, marking)(\"Skipping Remembered Set Rebuild. No regions selected for rebuild, will only scrub\");\n@@ -2004,1 +1983,2 @@\n-  _g1h->rem_set()->rebuild_rem_set(this, _concurrent_workers, _worker_id_offset);\n+\n+  G1ConcurrentRebuildAndScrub::rebuild_and_scrub(this, _concurrent_workers);\n@@ -2036,1 +2016,1 @@\n-    clear_next_bitmap(_g1h->workers());\n+    clear_bitmap(_g1h->workers());\n@@ -2038,3 +2018,0 @@\n-  \/\/ Note we cannot clear the previous marking bitmap here\n-  \/\/ since VerifyDuringGC verifies the objects marked during\n-  \/\/ a full GC against the previous bitmap.\n@@ -2104,4 +2081,2 @@\n-  st->print_cr(\"Marking Bits (Prev, Next): (CMBitMap*) \" PTR_FORMAT \", (CMBitMap*) \" PTR_FORMAT,\n-               p2i(_prev_mark_bitmap), p2i(_next_mark_bitmap));\n-  _prev_mark_bitmap->print_on_error(st, \" Prev Bits: \");\n-  _next_mark_bitmap->print_on_error(st, \" Next Bits: \");\n+  st->print_cr(\"Marking Bits: (CMBitMap*) \" PTR_FORMAT, p2i(mark_bitmap()));\n+  _mark_bitmap.print_on_error(st, \" Bits: \");\n@@ -2131,3 +2106,3 @@\n-  HeapRegion* hr            = _curr_region;\n-  HeapWord* bottom          = hr->bottom();\n-  HeapWord* limit           = hr->next_top_at_mark_start();\n+  HeapRegion* hr = _curr_region;\n+  HeapWord* bottom = hr->bottom();\n+  HeapWord* limit = hr->top_at_mark_start();\n@@ -2147,2 +2122,2 @@\n-    \/\/ evacuation pause empties the region underneath our feet (NTAMS\n-    \/\/ at bottom). We then do some allocation in the region (NTAMS\n+    \/\/ evacuation pause empties the region underneath our feet (TAMS\n+    \/\/ at bottom). We then do some allocation in the region (TAMS\n@@ -2150,1 +2125,1 @@\n-    \/\/ alloc region (NTAMS will move to top() and the objects\n+    \/\/ alloc region (TAMS will move to top() and the objects\n@@ -2184,3 +2159,3 @@\n-void G1CMTask::reset(G1CMBitMap* next_mark_bitmap) {\n-  guarantee(next_mark_bitmap != NULL, \"invariant\");\n-  _next_mark_bitmap              = next_mark_bitmap;\n+void G1CMTask::reset(G1CMBitMap* mark_bitmap) {\n+  guarantee(mark_bitmap != NULL, \"invariant\");\n+  _mark_bitmap              = mark_bitmap;\n@@ -2670,1 +2645,1 @@\n-        if (_next_mark_bitmap->is_marked(mr.start())) {\n+        if (_mark_bitmap->is_marked(mr.start())) {\n@@ -2678,1 +2653,1 @@\n-      } else if (_next_mark_bitmap->iterate(&bitmap_closure, mr)) {\n+      } else if (_mark_bitmap->iterate(&bitmap_closure, mr)) {\n@@ -2896,1 +2871,1 @@\n-  _next_mark_bitmap(NULL),\n+  _mark_bitmap(NULL),\n@@ -2962,3 +2937,5 @@\n-  _total_used_bytes(0), _total_capacity_bytes(0),\n-  _total_prev_live_bytes(0), _total_next_live_bytes(0),\n-  _total_remset_bytes(0), _total_code_roots_bytes(0)\n+  _total_used_bytes(0),\n+  _total_capacity_bytes(0),\n+  _total_live_bytes(0),\n+  _total_remset_bytes(0),\n+  _total_code_roots_bytes(0)\n@@ -2987,1 +2964,0 @@\n-                          G1PPRL_BYTE_H_FORMAT\n@@ -2993,1 +2969,1 @@\n-                          \"used\", \"prev-live\", \"next-live\", \"gc-eff\",\n+                          \"used\", \"live\", \"gc-eff\",\n@@ -3000,1 +2976,0 @@\n-                          G1PPRL_BYTE_H_FORMAT\n@@ -3006,1 +2981,1 @@\n-                          \"(bytes)\", \"(bytes)\", \"(bytes)\", \"(bytes\/ms)\",\n+                          \"(bytes)\", \"(bytes)\", \"(bytes\/ms)\",\n@@ -3020,2 +2995,1 @@\n-  size_t prev_live_bytes = r->live_bytes();\n-  size_t next_live_bytes = r->next_live_bytes();\n+  size_t live_bytes      = r->live_bytes();\n@@ -3030,2 +3004,1 @@\n-  _total_prev_live_bytes += prev_live_bytes;\n-  _total_next_live_bytes += next_live_bytes;\n+  _total_live_bytes      += live_bytes;\n@@ -3047,1 +3020,0 @@\n-                        G1PPRL_BYTE_FORMAT\n@@ -3053,1 +3025,1 @@\n-                        used_bytes, prev_live_bytes, next_live_bytes, gc_efficiency.buffer(),\n+                        used_bytes, live_bytes, gc_efficiency.buffer(),\n@@ -3072,2 +3044,1 @@\n-                         G1PPRL_SUM_MB_PERC_FORMAT(\"prev-live\")\n-                         G1PPRL_SUM_MB_PERC_FORMAT(\"next-live\")\n+                         G1PPRL_SUM_MB_PERC_FORMAT(\"live\")\n@@ -3079,4 +3050,2 @@\n-                         bytes_to_mb(_total_prev_live_bytes),\n-                         percent_of(_total_prev_live_bytes, _total_capacity_bytes),\n-                         bytes_to_mb(_total_next_live_bytes),\n-                         percent_of(_total_next_live_bytes, _total_capacity_bytes),\n+                         bytes_to_mb(_total_live_bytes),\n+                         percent_of(_total_live_bytes, _total_capacity_bytes),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":77,"deletions":108,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -219,1 +219,1 @@\n-\/\/ Typically they contain the areas from nTAMS to top of the regions.\n+\/\/ Typically they contain the areas from TAMS to top of the regions.\n@@ -295,4 +295,1 @@\n-  G1CMBitMap              _mark_bitmap_1;\n-  G1CMBitMap              _mark_bitmap_2;\n-  G1CMBitMap*             _prev_mark_bitmap; \/\/ Completed mark bitmap\n-  G1CMBitMap*             _next_mark_bitmap; \/\/ Under-construction mark bitmap\n+  G1CMBitMap              _mark_bitmap;\n@@ -446,1 +443,1 @@\n-  void clear_next_bitmap(WorkerThreads* workers, bool may_yield);\n+  void clear_bitmap(WorkerThreads* workers, bool may_yield);\n@@ -458,1 +455,3 @@\n-  void add_to_liveness(uint worker_id, oop const obj, size_t size);\n+  \/\/ To be called when an object is marked the first time, e.g. after a successful\n+  \/\/ mark_in_bitmap call. Updates various statistics data.\n+  void new_obj_marked(uint worker_id, oop const obj, size_t size);\n@@ -460,1 +459,1 @@\n-  \/\/ live words between bottom and nTAMS.\n+  \/\/ live words between bottom and TAMS.\n@@ -519,2 +518,1 @@\n-                   G1RegionToSpaceMapper* prev_bitmap_storage,\n-                   G1RegionToSpaceMapper* next_bitmap_storage);\n+                   G1RegionToSpaceMapper* bitmap_storage);\n@@ -525,2 +523,1 @@\n-  const G1CMBitMap* const prev_mark_bitmap() const { return _prev_mark_bitmap; }\n-  G1CMBitMap* next_mark_bitmap() const { return _next_mark_bitmap; }\n+  G1CMBitMap* mark_bitmap() const { return (G1CMBitMap*)&_mark_bitmap; }\n@@ -543,1 +540,1 @@\n-  void clear_next_bitmap(WorkerThreads* workers);\n+  void clear_bitmap(WorkerThreads* workers);\n@@ -566,2 +563,0 @@\n-  void swap_mark_bitmaps();\n-\n@@ -569,3 +564,0 @@\n-  \/\/ Mark in the previous bitmap. Caution: the prev bitmap is usually read-only, so use\n-  \/\/ this carefully.\n-  inline void par_mark_in_prev_bitmap(oop p);\n@@ -573,4 +565,3 @@\n-  \/\/ Clears marks for all objects in the given range, for the prev or\n-  \/\/ next bitmaps.  Caution: the previous bitmap is usually\n-  \/\/ read-only, so use this carefully!\n-  void clear_range_in_prev_bitmap(MemRegion mr);\n+  \/\/ Mark in the marking bitmap. Used during evacuation failure to\n+  \/\/ remember what objects need handling. Not for use during marking.\n+  inline void raw_mark_in_bitmap(oop p);\n@@ -578,1 +569,4 @@\n-  inline bool is_marked_in_prev_bitmap(oop p) const;\n+  \/\/ Clears marks for all objects in the given range in the marking\n+  \/\/ bitmap. This should only be used clean the bitmap during a\n+  \/\/ safepoint.\n+  void clear_range_in_bitmap(MemRegion mr);\n@@ -595,3 +589,3 @@\n-  \/\/ Mark the given object on the next bitmap if it is below nTAMS.\n-  inline bool mark_in_next_bitmap(uint worker_id, HeapRegion* const hr, oop const obj);\n-  inline bool mark_in_next_bitmap(uint worker_id, oop const obj);\n+  \/\/ Mark the given object on the marking bitmap if it is below TAMS.\n+  inline bool mark_in_bitmap(uint worker_id, HeapRegion* const hr, oop const obj);\n+  inline bool mark_in_bitmap(uint worker_id, oop const obj);\n@@ -599,1 +593,1 @@\n-  inline bool is_marked_in_next_bitmap(oop p) const;\n+  inline bool is_marked_in_bitmap(oop p) const;\n@@ -604,2 +598,3 @@\n-  \/\/ Rebuilds the remembered sets for chosen regions in parallel and concurrently to the application.\n-  void rebuild_rem_set_concurrently();\n+  \/\/ Rebuilds the remembered sets for chosen regions in parallel and concurrently\n+  \/\/ to the application. Also scrubs dead objects to ensure region is parsable.\n+  void rebuild_and_scrub();\n@@ -630,1 +625,1 @@\n-  G1CMBitMap*                 _next_mark_bitmap;\n+  G1CMBitMap*                 _mark_bitmap;\n@@ -736,1 +731,1 @@\n-  void reset(G1CMBitMap* next_mark_bitmap);\n+  void reset(G1CMBitMap* mark_bitmap);\n@@ -780,1 +775,1 @@\n-  \/\/ the local queue if below the finger. obj is required to be below its region's NTAMS.\n+  \/\/ the local queue if below the finger. obj is required to be below its region's TAMS.\n@@ -785,1 +780,1 @@\n-  \/\/ e.g. obj is below its containing region's NTAMS.\n+  \/\/ e.g. obj is below its containing region's TAMS.\n@@ -787,1 +782,1 @@\n-  \/\/ Returns true if the reference caused a mark to be set in the next bitmap.\n+  \/\/ Returns true if the reference caused a mark to be set in the marking bitmap.\n@@ -844,1 +839,1 @@\n-  size_t _total_prev_live_bytes;\n+  size_t _total_live_bytes;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.hpp","additions":31,"deletions":36,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,1 +53,1 @@\n-  if (hr->obj_allocated_since_next_marking(obj)) {\n+  if (hr->obj_allocated_since_marking_start(obj)) {\n@@ -78,1 +78,1 @@\n-inline bool G1ConcurrentMark::mark_in_next_bitmap(uint const worker_id, oop const obj) {\n+inline bool G1ConcurrentMark::mark_in_bitmap(uint const worker_id, oop const obj) {\n@@ -80,1 +80,1 @@\n-  return mark_in_next_bitmap(worker_id, hr, obj);\n+  return mark_in_bitmap(worker_id, hr, obj);\n@@ -83,1 +83,1 @@\n-inline bool G1ConcurrentMark::mark_in_next_bitmap(uint const worker_id, HeapRegion* const hr, oop const obj) {\n+inline bool G1ConcurrentMark::mark_in_bitmap(uint const worker_id, HeapRegion* const hr, oop const obj) {\n@@ -87,1 +87,1 @@\n-  if (hr->obj_allocated_since_next_marking(obj)) {\n+  if (hr->obj_allocated_since_marking_start(obj)) {\n@@ -91,1 +91,1 @@\n-  \/\/ Some callers may have stale objects to mark above nTAMS after humongous reclaim.\n+  \/\/ Some callers may have stale objects to mark above TAMS after humongous reclaim.\n@@ -93,1 +93,1 @@\n-  assert(!hr->is_continues_humongous(), \"Should not try to mark object \" PTR_FORMAT \" in Humongous continues region %u above nTAMS \" PTR_FORMAT, p2i(obj), hr->hrm_index(), p2i(hr->next_top_at_mark_start()));\n+  assert(!hr->is_continues_humongous(), \"Should not try to mark object \" PTR_FORMAT \" in Humongous continues region %u above TAMS \" PTR_FORMAT, p2i(obj), hr->hrm_index(), p2i(hr->top_at_mark_start()));\n@@ -95,1 +95,1 @@\n-  bool success = _next_mark_bitmap->par_mark(obj);\n+  bool success = _mark_bitmap.par_mark(obj);\n@@ -97,1 +97,1 @@\n-    add_to_liveness(worker_id, obj, obj->size());\n+    new_obj_marked(worker_id, obj, obj->size());\n@@ -132,1 +132,1 @@\n-  assert(task_entry.is_array_slice() || _next_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())), \"invariant\");\n+  assert(task_entry.is_array_slice() || _mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())), \"invariant\");\n@@ -180,1 +180,1 @@\n-  assert(task_entry.is_array_slice() || _next_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())),\n+  assert(task_entry.is_array_slice() || _mark_bitmap->is_marked(cast_from_oop<HeapWord*>(task_entry.obj())),\n@@ -226,1 +226,1 @@\n-inline void G1ConcurrentMark::add_to_liveness(uint worker_id, oop const obj, size_t size) {\n+inline void G1ConcurrentMark::new_obj_marked(uint worker_id, oop const obj, size_t size) {\n@@ -228,0 +228,1 @@\n+  _mark_bitmap.update_back_skip_table(obj);\n@@ -237,1 +238,1 @@\n-  if (!_cm->mark_in_next_bitmap(_worker_id, obj)) {\n+  if (!_cm->mark_in_bitmap(_worker_id, obj)) {\n@@ -289,2 +290,2 @@\n-inline void G1ConcurrentMark::par_mark_in_prev_bitmap(oop p) {\n-  _prev_mark_bitmap->par_mark(p);\n+inline void G1ConcurrentMark::raw_mark_in_bitmap(oop p) {\n+  _mark_bitmap.par_mark(p);\n@@ -293,1 +294,1 @@\n-bool G1ConcurrentMark::is_marked_in_prev_bitmap(oop p) const {\n+bool G1ConcurrentMark::is_marked_in_bitmap(oop p) const {\n@@ -295,6 +296,1 @@\n-  return _prev_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(p));\n-}\n-\n-bool G1ConcurrentMark::is_marked_in_next_bitmap(oop p) const {\n-  assert(p != NULL && oopDesc::is_oop(p), \"expected an oop\");\n-  return _next_mark_bitmap->is_marked(cast_from_oop<HeapWord*>(p));\n+  return _mark_bitmap.is_marked(cast_from_oop<HeapWord*>(p));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.inline.hpp","additions":19,"deletions":23,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,2 @@\n+\n+#include \"gc\/g1\/g1_globals.hpp\"\n@@ -31,0 +33,29 @@\n+G1CMBackScanSkipTable::G1CMBackScanSkipTable(size_t log_mapping_granularity) :\n+  _mapping_granularity((size_t)1 << log_mapping_granularity) {\n+  assert(_mapping_granularity <= HeapRegion::GrainBytes, \"must be\");\n+}\n+\n+void G1CMBackScanSkipTable::initialize(MemRegion mr) {\n+  G1BiasedMappedArray<bool>::initialize(mr, mapping_granularity());\n+}\n+\n+void G1CMBackScanSkipTable::clear_range(MemRegion mr) {\n+  assert(is_aligned(mr.start(), mapping_granularity()), \"must be\");\n+  assert(is_aligned(mr.end(), mapping_granularity()), \"must be\");\n+\n+  bool* start = address_mapped_to(mr.start());\n+  bool* const end = address_mapped_to(mr.end());\n+\n+  while (start < end) {\n+    *start++ = default_value();\n+  }\n+}\n+\n+G1CMBitMap::G1CMBitMap() :\n+  _bitmap(),\n+  _back_scan_skip_table(G1LogBackScanSkipGranularity),\n+  _listener() {\n+\n+  _listener.set_bitmap(this);\n+}\n+\n@@ -32,1 +63,2 @@\n-  MarkBitMap::initialize(heap, storage->reserved());\n+  _bitmap.initialize(heap, storage->reserved());\n+  _back_scan_skip_table.initialize(heap);\n@@ -36,0 +68,2 @@\n+void G1CMBitMap::print_on_error(outputStream* out, const char* prefix) const { _bitmap.print_on_error(out, prefix); }\n+\n@@ -44,15 +78,0 @@\n-\n-void G1CMBitMap::clear_region(HeapRegion* region) {\n- if (!region->is_empty()) {\n-   MemRegion mr(region->bottom(), region->top());\n-   clear_range(mr);\n- }\n-}\n-\n-#ifdef ASSERT\n-void G1CMBitMap::check_mark(HeapWord* addr) {\n-  assert(G1CollectedHeap::heap()->is_in(addr),\n-         \"Trying to access bitmap \" PTR_FORMAT \" for address \" PTR_FORMAT \" not in the heap.\",\n-         p2i(this), p2i(addr));\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.cpp","additions":36,"deletions":17,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1BiasedArray.hpp\"\n@@ -42,1 +43,1 @@\n-class G1CMBitMapClosure {\n+class G1CMBitMapClosure : public MarkBitMapClosure {\n@@ -46,1 +47,1 @@\n-  G1CMBitMapClosure(G1CMTask *task, G1ConcurrentMark* cm) : _cm(cm), _task(task) { }\n+  G1CMBitMapClosure(G1CMTask *task, G1ConcurrentMark* cm) : MarkBitMapClosure(), _cm(cm), _task(task) { }\n@@ -61,5 +62,8 @@\n-\/\/ A generic mark bitmap for concurrent marking.  This is essentially a wrapper\n-\/\/ around the BitMap class that is based on HeapWords, with one bit per (1 << _shifter) HeapWords.\n-class G1CMBitMap : public MarkBitMap {\n-\n-  G1CMBitMapMappingChangedListener _listener;\n+\/\/ This table is used to quickly scan back large areas within regions: whenever\n+\/\/ concurrent marking marks an object on the mark bitmap, this class also remembers\n+\/\/ its approximate location in units of subdivisions of a heap region.\n+\/\/\n+\/\/ When scanning the bitmap backwards for object starts, this table is consulted\n+\/\/ to skip large unmarked areas.\n+class G1CMBackScanSkipTable : private G1BiasedMappedArray<bool> {\n+  const size_t _mapping_granularity;\n@@ -68,0 +72,8 @@\n+  virtual bool default_value() const { return false; }\n+\n+public:\n+  G1CMBackScanSkipTable(size_t log_mapping_granularity);\n+\n+  void initialize(MemRegion mr);\n+\n+  size_t mapping_granularity() const { return _mapping_granularity; }\n@@ -69,1 +81,14 @@\n-  virtual void check_mark(HeapWord* addr) NOT_DEBUG_RETURN;\n+  void mark(HeapWord* const addr);\n+  void clear_range(MemRegion mr);\n+\n+  \/\/ Scans backward from address start-1 to bottom, returning the start of the corresponding\n+  \/\/ area if marked. Otherwise, if no mark has been found, returns nullptr.\n+  HeapWord* find_marked_area(HeapWord* const bottom, HeapWord* const start) const;\n+};\n+\n+\/\/ A generic mark bitmap for concurrent marking.  This is essentially a wrapper\n+\/\/ around the BitMap class that is based on HeapWords, with one bit per (1 << _shifter) HeapWords.\n+class G1CMBitMap {\n+  MarkBitMap _bitmap;\n+  G1CMBackScanSkipTable _back_scan_skip_table;\n+  G1CMBitMapMappingChangedListener _listener;\n@@ -72,0 +97,1 @@\n+  G1CMBitMap();\n@@ -73,1 +99,2 @@\n-  G1CMBitMap() : MarkBitMap(), _listener() { _listener.set_bitmap(this); }\n+  static size_t compute_size(size_t heap_size) { return MarkBitMap::compute_size(heap_size); }\n+  static size_t heap_map_factor() { return MarkBitMap::heap_map_factor(); }\n@@ -75,1 +102,0 @@\n-  \/\/ Initializes the underlying BitMap to cover the given area.\n@@ -78,2 +104,28 @@\n-  \/\/ Apply the closure to the addresses that correspond to marked bits in the bitmap.\n-  inline bool iterate(G1CMBitMapClosure* cl, MemRegion mr);\n+  bool is_marked(oop obj) const;\n+  bool is_marked(HeapWord* addr) const;\n+\n+  inline bool iterate(MarkBitMapClosure* cl, MemRegion mr);\n+  \/\/ Return the address corresponding to the next marked bit at or after\n+  \/\/ \"addr\", and before \"limit\", if \"limit\" is non-NULL.  If there is no\n+  \/\/ such bit, returns \"limit\" if that is non-NULL, or else \"endWord()\".\n+  inline HeapWord* get_next_marked_addr(const HeapWord* addr,\n+                                        HeapWord* limit) const;\n+\n+  \/\/ Return the address corresponding to the previous marked bit at or before\n+  \/\/ \"addr\", and after or at \"limit\". If there is no such bit, returns nullptr.\n+  \/\/ Uses the back scan skip table to speed up the process in case of large areas\n+  \/\/ without marked objects. The back scan skip table is expected to be properly\n+  \/\/ initialized.\n+  inline HeapWord* get_prev_marked_addr(HeapWord* limit,\n+                                        const HeapWord* addr) const;\n+\n+  \/\/ Write marks.\n+  inline void clear(HeapWord* addr);\n+  inline void clear(oop obj);\n+  inline bool par_mark(HeapWord* addr);\n+  inline bool par_mark(oop obj);\n+\n+  inline void update_back_skip_table(oop obj);\n+\n+  \/\/ Clear bitmap.\n+  void clear_range(MemRegion mr);\n@@ -81,1 +133,1 @@\n-  void clear_region(HeapRegion* hr);\n+  void print_on_error(outputStream* out, const char* prefix) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.hpp","additions":66,"deletions":14,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/g1\/g1ConcurrentRefine.hpp\"\n@@ -35,5 +36,37 @@\n-inline bool G1CMBitMap::iterate(G1CMBitMapClosure* cl, MemRegion mr) {\n-  assert(!mr.is_empty(), \"Does not support empty memregion to iterate over\");\n-  assert(_covered.contains(mr),\n-         \"Given MemRegion from \" PTR_FORMAT \" to \" PTR_FORMAT \" not contained in heap area\",\n-         p2i(mr.start()), p2i(mr.end()));\n+inline void G1CMBackScanSkipTable::mark(HeapWord* const addr) {\n+  if (!get_by_address(addr)) {\n+    set_by_address(addr, true);\n+  }\n+}\n+\n+inline HeapWord* G1CMBackScanSkipTable::find_marked_area(HeapWord* const bottom, HeapWord* const start) const {\n+  assert(bottom != nullptr, \"must be\");\n+  assert(is_aligned(bottom, mapping_granularity()), \"must be\");\n+  assert(is_aligned(start, mapping_granularity()), \"must be\");\n+\n+  bool* cur = address_mapped_to(start - 1);\n+  bool* bot = address_mapped_to(bottom);\n+  assert(cur >= bot, \"must be\");\n+\n+  while (cur >= bot) {\n+    if (*cur) {\n+      idx_t index = pointer_delta(cur, bot, sizeof(bool));\n+      HeapWord* result = bottom + index * mapping_granularity() \/ HeapWordSize;\n+      return result;\n+    }\n+    cur--;\n+  }\n+  return nullptr;\n+}\n+\n+inline bool G1CMBitMap::is_marked(oop obj) const { return _bitmap.is_marked(obj); }\n+inline bool G1CMBitMap::is_marked(HeapWord* addr) const { return _bitmap.is_marked(addr); }\n+\n+inline bool G1CMBitMap::iterate(MarkBitMapClosure* cl, MemRegion mr) {\n+  return _bitmap.iterate(cl, mr);\n+}\n+\n+inline HeapWord* G1CMBitMap::get_next_marked_addr(const HeapWord* addr,\n+                                                  HeapWord* const limit) const {\n+  return _bitmap.get_next_marked_addr(addr, limit);\n+}\n@@ -41,2 +74,4 @@\n-  BitMap::idx_t const end_offset = addr_to_offset(mr.end());\n-  BitMap::idx_t offset = _bm.get_next_one_offset(addr_to_offset(mr.start()), end_offset);\n+inline HeapWord* G1CMBitMap::get_prev_marked_addr(HeapWord* const limit,\n+                                                  const HeapWord* addr) const {\n+  const size_t BackSkipGranularity = _back_scan_skip_table.mapping_granularity();\n+  assert((uintptr_t)addr >= BackSkipGranularity, \"must be\");\n@@ -44,4 +79,12 @@\n-  while (offset < end_offset) {\n-    HeapWord* const addr = offset_to_addr(offset);\n-    if (!cl->do_addr(addr)) {\n-      return false;\n+  \/\/ Scan at least half of the backskip size immediately before trying to use the\n+  \/\/ back scan skip table. It is extremely likely that there is a marked object\n+  \/\/ in \"close\" vicinity.\n+  HeapWord* scan_until_directly = MAX2(limit,\n+                                       (HeapWord*)align_down((uintptr_t)addr - BackSkipGranularity \/ 2, BackSkipGranularity));\n+  HeapWord* result = _bitmap.get_prev_marked_addr(scan_until_directly, addr);\n+  if (result == nullptr) {\n+    \/\/ No previous marked object found when scanning until scan_until_directly.\n+    \/\/ If scan_until_directly is limit, then we are done - we could not find any\n+    \/\/ mark in this region.\n+    if (scan_until_directly == limit) {\n+      return nullptr;\n@@ -49,2 +92,13 @@\n-    size_t const obj_size = cast_to_oop(addr)->size();\n-    offset = _bm.get_next_one_offset(offset + (obj_size >> _shifter), end_offset);\n+    \/\/ Then use the back skip table to jump over large areas in the bitmap.\n+    HeapWord* bottom_with_mark = _back_scan_skip_table.find_marked_area(limit, scan_until_directly);\n+    assert(bottom_with_mark < scan_until_directly, \"must scan backward\");\n+    \/\/ If no mark found with the backskip table either, we are done.\n+    if (bottom_with_mark == nullptr) {\n+      return nullptr;\n+    }\n+    \/\/ In the current backskip area there must be a mark. Find the first on the bitmap.\n+    result = _bitmap.get_prev_marked_addr(bottom_with_mark, bottom_with_mark + BackSkipGranularity \/ HeapWordSize);\n+    assert(result != nullptr, \"must be\");\n+    assert(result >= bottom_with_mark, \"result \" PTR_FORMAT \" beyond lower range \" PTR_FORMAT,\n+           p2i(result), p2i(bottom_with_mark));\n+    assert(_bitmap.get_prev_marked_addr(result, addr) == result, \"scanning directly must yield same result\");\n@@ -52,1 +106,16 @@\n-  return true;\n+\n+  return result;\n+}\n+\n+inline void G1CMBitMap::clear(HeapWord* addr) { _bitmap.clear(addr); }\n+inline void G1CMBitMap::clear(oop obj) { _bitmap.clear(obj); }\n+inline bool G1CMBitMap::par_mark(HeapWord* addr) { return _bitmap.par_mark(addr); }\n+inline bool G1CMBitMap::par_mark(oop obj) { return _bitmap.par_mark(obj); }\n+\n+inline void G1CMBitMap::update_back_skip_table(oop obj) {\n+  _back_scan_skip_table.mark(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline void G1CMBitMap::clear_range(MemRegion mr) {\n+  _bitmap.clear_range(mr);\n+  _back_scan_skip_table.clear_range(mr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.inline.hpp","additions":84,"deletions":15,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -236,3 +236,3 @@\n-bool G1ConcurrentMarkThread::phase_rebuild_remembered_sets() {\n-  G1ConcPhaseTimer p(_cm, \"Concurrent Rebuild Remembered Sets\");\n-  _cm->rebuild_rem_set_concurrently();\n+bool G1ConcurrentMarkThread::phase_rebuild_and_scrub() {\n+  G1ConcPhaseTimer p(_cm, \"Concurrent Rebuild Remembered Sets and Scrub Regions\");\n+  _cm->rebuild_and_scrub();\n@@ -293,2 +293,2 @@\n-  \/\/ Phase 4: Rebuild remembered sets.\n-  if (phase_rebuild_remembered_sets()) return;\n+  \/\/ Phase 4: Rebuild remembered sets and scrub dead objects.\n+  if (phase_rebuild_and_scrub()) return;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkThread.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -70,1 +70,1 @@\n-  bool phase_rebuild_remembered_sets();\n+  bool phase_rebuild_and_scrub();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkThread.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,350 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/g1\/g1ConcurrentRebuildAndScrub.hpp\"\n+\n+#include \"gc\/g1\/g1ConcurrentMark.inline.hpp\"\n+#include \"gc\/g1\/g1ConcurrentMarkBitMap.inline.hpp\"\n+#include \"gc\/g1\/g1_globals.hpp\"\n+#include \"gc\/g1\/heapRegion.inline.hpp\"\n+#include \"gc\/g1\/heapRegionManager.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Worker task that scans the objects in the old generation to rebuild the remembered\n+\/\/ set and at the same time scrubs dead objects by replacing them with filler objects\n+\/\/ to make them completely parseable.\n+\/\/\n+\/\/ The remark pause recorded two pointers within the regions:\n+\/\/\n+\/\/ parsable_bottom: this is the TAMS of the recent marking for that region. Objects\n+\/\/                  below that may or may not be dead (as per mark bitmap).\n+\/\/                  This task needs to remove the dead objects, replacing them\n+\/\/                  with filler objects so that they can be walked through later.\n+\/\/\n+\/\/ top_at_rebuild_start (tars): at rebuild phase start we record the current top: up to\n+\/\/                              this address (live) objects need to be scanned for references\n+\/\/                              that might need to be added to the remembered sets.\n+\/\/\n+\/\/ Note that bottom <= parsable_bottom <= tars; if there is no tars (i.e. NULL),\n+\/\/ obviously there can not be a parsable_bottom.\n+\/\/\n+\/\/ We need to scrub and scan objects to rebuild remembered sets until parsable_bottom;\n+\/\/ we need to scan objects to rebuild remembered sets until tars.\n+class G1RebuildRSAndScrubTask : public WorkerTask {\n+  G1ConcurrentMark* _cm;\n+  HeapRegionClaimer _hr_claimer;\n+\n+  class G1RebuildRSAndScrubRegionClosure : public HeapRegionClosure {\n+    G1ConcurrentMark* _cm;\n+    const G1CMBitMap* _bitmap;\n+\n+    G1RebuildRemSetClosure _rebuild_closure;\n+\n+    size_t _marked_words;\n+    size_t _processed_words;\n+    const size_t ProcessingYieldLimitInWords = G1RebuildRemSetChunkSize \/ HeapWordSize;\n+\n+    void reset_marked_words() {\n+      _marked_words = 0;\n+    }\n+\n+    void reset_processed_words() {\n+      _processed_words = 0;\n+    }\n+\n+    void assert_marked_words(HeapRegion* hr) {\n+      assert((_marked_words * HeapWordSize) == hr->marked_bytes(),\n+             \"Mismatch between marking and re-calculation for region %u, %zu != %zu\",\n+             hr->hrm_index(), (_marked_words * HeapWordSize), hr->marked_bytes());\n+    }\n+\n+    void add_processed_words(size_t processed) {\n+      _processed_words += processed;\n+      _marked_words += processed;\n+    }\n+\n+    \/\/ Yield if enough has been processed; returns if the concurrent marking cycle\n+    \/\/ has been aborted for any reason.\n+    bool yield_if_necessary() {\n+      if (_processed_words >= ProcessingYieldLimitInWords) {\n+        reset_processed_words();\n+        _cm->do_yield_check();\n+      }\n+      return _cm->has_aborted();\n+    }\n+\n+    \/\/ Checks the top at rebuild start value for the given region. If the value\n+    \/\/ is NULL the region has either:\n+    \/\/  - been allocated after rebuild start, or\n+    \/\/  - been eagerly reclaimed by a young collection (only humongous)\n+    \/\/ In these cases we do not need to scan through the given region.\n+    bool should_rebuild_or_scrub(HeapRegion* hr) {\n+      return _cm->top_at_rebuild_start(hr->hrm_index()) != NULL;\n+    }\n+\n+    \/\/ Helper used by both humongous objects and when chunking an object larger than the\n+    \/\/ G1RebuildRemSetChunkSize. The heap region is needed to ensure a humongous object\n+    \/\/ is not eagerly reclaimed during yielding.\n+    \/\/ Returns whether marking has been aborted.\n+    bool scan_large_object(HeapRegion* hr, const oop obj, MemRegion scan_range) {\n+      HeapWord* start = scan_range.start();\n+      HeapWord* limit = scan_range.end();\n+      do {\n+        MemRegion mr(start, MIN2(start + ProcessingYieldLimitInWords, limit));\n+        obj->oop_iterate(&_rebuild_closure, mr);\n+\n+        \/\/ Update processed words and yield, for humongous objects we will yield\n+        \/\/ after each chunk.\n+        add_processed_words(mr.word_size());\n+        bool mark_aborted = yield_if_necessary();\n+        if (mark_aborted) {\n+          return true;\n+        } else if (!should_rebuild_or_scrub(hr)) {\n+          \/\/ We need to check should_rebuild_or_scrub() again (for humongous objects)\n+          \/\/ because the region might have been eagerly reclaimed during the yield.\n+          log_trace(gc, marking)(\"Rebuild aborted for eagerly reclaimed humongous region: %u\", hr->hrm_index());\n+          return false;\n+        }\n+\n+        \/\/ Step to next chunk of the humongous object\n+        start = mr.end();\n+      } while (start < limit);\n+      return false;\n+    }\n+\n+    \/\/ Scan for references into regions that need remembered set update for the given\n+    \/\/ live object. Returns the offset to the next object.\n+    size_t scan_object(HeapRegion* hr, HeapWord* current) {\n+      oop obj = cast_to_oop(current);\n+      size_t obj_size = obj->size();\n+\n+      if (obj_size > ProcessingYieldLimitInWords) {\n+        \/\/ Large object, needs to be chunked to avoid stalling safepoints.\n+        MemRegion mr(current, obj_size);\n+        scan_large_object(hr, obj, mr);\n+        \/\/ No need to add to _processed_words, this is all handled by the above call;\n+        \/\/ we also ignore the marking abort result of scan_large_object - we will check\n+        \/\/ again right afterwards.\n+      } else {\n+        \/\/ Object smaller than yield limit, process it fully.\n+        obj->oop_iterate(&_rebuild_closure);\n+        \/\/ Update how much we have processed. Yield check in main loop\n+        \/\/ will handle this case.\n+        add_processed_words(obj_size);\n+      }\n+\n+      return obj_size;\n+    }\n+\n+    \/\/ Scrub a range of dead objects starting at scrub_start. Will never scrub past limit.\n+    HeapWord* scrub_to_next_live(HeapRegion* hr, HeapWord* scrub_start, HeapWord* limit) {\n+      assert(!_bitmap->is_marked(scrub_start), \"Should not scrub live object\");\n+\n+      HeapWord* scrub_end = _bitmap->get_next_marked_addr(scrub_start, limit);\n+      if (scrub_start != scrub_end) {\n+        \/\/ Only scrub if range is non-empty.\n+        hr->fill_range_with_dead_objects(scrub_start, scrub_end);\n+        assert(hr->obj_is_scrubbed(cast_to_oop(scrub_start)), \"Scrubbing failed\");\n+      }\n+\n+      \/\/ At this point make sure we are either at the scrubbing limit or that the next\n+      \/\/ object is live. Need to compare to the limit first to not accidentally query the\n+      \/\/ bitmap outside the committed heap.\n+      assert(scrub_end == limit || _bitmap->is_marked(scrub_end),\n+             \"We should either step to the next live object or the limit\");\n+\n+      \/\/ Return the next object to handle.\n+      return scrub_end;\n+    }\n+\n+    \/\/ Scan the given region from bottom to parsable_bottom. Returns whether marking has\n+    \/\/ been aborted.\n+    bool scan_and_scrub_to_pb(HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n+\n+      while (start < limit) {\n+        if (_bitmap->is_marked(start)) {\n+          assert(!cast_to_oop(start)->is_gc_marked(), \"No live objects in G1 should be GC marked\");\n+          \/\/  Live object, need to scan to rebuild remembered sets for this object.\n+          start += scan_object(hr, start);\n+        } else {\n+          \/\/ Found dead object (which klass has potentially been unloaded). Scrub to next\n+          \/\/ marked object and continue.\n+          start = scrub_to_next_live(hr, start, limit);\n+        }\n+\n+        bool mark_aborted = yield_if_necessary();\n+        if (mark_aborted) {\n+          return true;\n+        }\n+      }\n+\n+      hr->note_end_of_scrubbing();\n+      return false;\n+    }\n+\n+    \/\/ Scan the given region from parsable_bottom to tars. Returns whether marking has\n+    \/\/ been aborted.\n+    bool scan_from_pb_to_tars(HeapRegion* hr, HeapWord* start, HeapWord* const limit) {\n+\n+      while (start < limit) {\n+        start += scan_object(hr, start);\n+        \/\/ Avoid stalling safepoints and stop iteration if mark cycle has been aborted.\n+        bool mark_aborted = yield_if_necessary();\n+        if (mark_aborted) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Scan and scrub the given region to tars. Returns whether marking has\n+    \/\/ been aborted.\n+    bool scan_and_scrub_region(HeapRegion* hr, HeapWord* const pb) {\n+      assert(should_rebuild_or_scrub(hr), \"must be\");\n+\n+      reset_marked_words();\n+      log_debug(gc, marking)(\"Scrub and rebuild region: \" HR_FORMAT \" pb: \" PTR_FORMAT,\n+                             HR_FORMAT_PARAMS(hr), p2i(pb));\n+\n+      if (scan_and_scrub_to_pb(hr, hr->bottom(), pb)) {\n+        log_trace(gc, marking)(\"Scan and scrub aborted for region: %u\", hr->hrm_index());\n+        return true;\n+      }\n+\n+      \/\/ Assert that the size of marked objects from the marking matches\n+      \/\/ the size of the objects which we scanned to rebuild remembered sets.\n+      assert_marked_words(hr);\n+\n+      \/\/ Rebuild from TAMS (= parsable_bottom) to TARS.\n+      if (scan_from_pb_to_tars(hr, pb, _cm->top_at_rebuild_start(hr->hrm_index()))) {\n+        log_trace(gc, marking)(\"Rebuild aborted for region: %u (%s)\", hr->hrm_index(), hr->get_short_type_str());\n+        return true;\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Scan a humongous region for remembered set updates. Scans in chunks to avoid\n+    \/\/ stalling safepoints. Returns whether the concurrent marking phase has been aborted.\n+    bool scan_humongous_region(HeapRegion* hr, HeapWord* const pb) {\n+      assert(should_rebuild_or_scrub(hr), \"must be\");\n+\n+      \/\/ At this point we should only have live humongous objects, that\n+      \/\/ means it must either be:\n+      \/\/ - marked\n+      \/\/ - or seen as fully parsable, i.e. allocated after the marking started\n+      oop humongous = cast_to_oop(hr->humongous_start_region()->bottom());\n+      assert(_bitmap->is_marked(humongous) || pb == hr->bottom(),\n+             \"Humongous object not live\");\n+\n+      reset_marked_words();\n+      log_debug(gc, marking)(\"Rebuild for humongous region: \" HR_FORMAT \" pb: \" PTR_FORMAT \" TARS: \" PTR_FORMAT,\n+                             HR_FORMAT_PARAMS(hr), p2i(pb), p2i(_cm->top_at_rebuild_start(hr->hrm_index())));\n+\n+      \/\/ Scan the humongous object in chunks from bottom to top to rebuild remembered sets.\n+      HeapWord* humongous_end = hr->humongous_start_region()->bottom() + humongous->size();\n+      MemRegion mr(hr->bottom(), MIN2(hr->top(), humongous_end));\n+\n+      bool mark_aborted = scan_large_object(hr, humongous, mr);\n+      if (mark_aborted) {\n+        log_trace(gc, marking)(\"Rebuild aborted for region: %u (%s)\", hr->hrm_index(), hr->get_short_type_str());\n+        return true;\n+      } else if (_bitmap->is_marked(humongous) && should_rebuild_or_scrub(hr)) {\n+        \/\/ Only verify that the marked size matches the rebuilt size if this object was marked\n+        \/\/ and the object should still be handled. The should handle state can change during\n+        \/\/ rebuild for humongous objects that are eagerly reclaimed so we need to check this.\n+        \/\/ If the object has not been marked the size from marking will be 0.\n+        assert_marked_words(hr);\n+      }\n+      return false;\n+    }\n+\n+  public:\n+    G1RebuildRSAndScrubRegionClosure(G1ConcurrentMark* cm, uint worker_id) :\n+      _cm(cm),\n+      _bitmap(_cm->mark_bitmap()),\n+      _rebuild_closure(G1CollectedHeap::heap(), worker_id),\n+      _marked_words(0),\n+      _processed_words(0) { }\n+\n+    bool do_heap_region(HeapRegion* hr) {\n+      \/\/ Avoid stalling safepoints and stop iteration if mark cycle has been aborted.\n+      _cm->do_yield_check();\n+      if (_cm->has_aborted()) {\n+        return true;\n+      }\n+\n+      HeapWord* const pb = hr->parsable_bottom_acquire();\n+\n+      if (!should_rebuild_or_scrub(hr)) {\n+        \/\/ Region has been allocated during this phase, no need to either scrub or\n+        \/\/ scan to rebuild remembered sets.\n+        log_trace(gc, marking)(\"Scrub and rebuild region skipped for \" HR_FORMAT \" pb: \" PTR_FORMAT,\n+                               HR_FORMAT_PARAMS(hr), p2i(pb));\n+        assert(hr->bottom() == pb, \"Region must be fully parsable\");\n+        return false;\n+      }\n+\n+      bool mark_aborted;\n+      if (hr->needs_scrubbing()) {\n+        \/\/ Old and open archive regions need to have their dead objects scrubbed\n+        \/\/ to make sure the region is parsable using object sizes.\n+        mark_aborted = scan_and_scrub_region(hr, pb);\n+      } else if (hr->is_humongous()) {\n+        \/\/ No need to scrub humongous, but we should scan it to rebuild remsets.\n+        mark_aborted = scan_humongous_region(hr, pb);\n+      }\n+\n+      return mark_aborted;\n+    }\n+  };\n+\n+public:\n+  G1RebuildRSAndScrubTask(G1ConcurrentMark* cm, bool should_rebuild, uint num_workers) :\n+    WorkerTask(\"Scrub dead objects\"),\n+    _cm(cm),\n+    _hr_claimer(num_workers) { }\n+\n+  void work(uint worker_id) {\n+    SuspendibleThreadSetJoiner sts_join;\n+\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+    G1RebuildRSAndScrubRegionClosure cl(_cm, worker_id);\n+    g1h->heap_region_par_iterate_from_worker_offset(&cl, &_hr_claimer, worker_id);\n+  }\n+};\n+\n+void G1ConcurrentRebuildAndScrub::rebuild_and_scrub(G1ConcurrentMark* cm, WorkerThreads* workers) {\n+  uint num_workers = workers->active_workers();\n+\n+  G1RebuildRSAndScrubTask task(cm, true, num_workers);\n+  workers->run_task(&task, num_workers);\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRebuildAndScrub.cpp","additions":350,"deletions":0,"binary":false,"changes":350,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1CONCURRENTREBUILDANDSCRUB_HPP\n+#define SHARE_GC_G1_G1CONCURRENTREBUILDANDSCRUB_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class G1ConcurrentMark;\n+class WorkerThreads;\n+\n+\/\/ Rebuild and scrubbing helper class.\n+class G1ConcurrentRebuildAndScrub : AllStatic {\n+public:\n+\n+  static void rebuild_and_scrub(G1ConcurrentMark* cm, WorkerThreads* workers);\n+};\n+\n+\n+#endif \/* SHARE_GC_G1_G1CONCURRENTREBUILDANDSCRUB_HPP *\/\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRebuildAndScrub.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -71,0 +71,1 @@\n+    size_t obj_size = obj->size();\n@@ -79,1 +80,1 @@\n-    assert(_cm->is_marked_in_prev_bitmap(obj), \"should be correctly marked\");\n+    assert(_cm->is_marked_in_bitmap(obj), \"should be correctly marked\");\n@@ -81,11 +82,3 @@\n-      \/\/ For the next marking info we'll only mark the\n-      \/\/ self-forwarded objects explicitly if we are during\n-      \/\/ concurrent start (since, normally, we only mark objects pointed\n-      \/\/ to by roots if we succeed in copying them). By marking all\n-      \/\/ self-forwarded objects we ensure that we mark any that are\n-      \/\/ still pointed to be roots. During concurrent marking, and\n-      \/\/ after concurrent start, we don't need to mark any objects\n-      \/\/ explicitly and all objects in the CSet are considered\n-      \/\/ (implicitly) live. So, we won't mark them explicitly and\n-      \/\/ we'll leave them over NTAMS.\n-      _cm->mark_in_next_bitmap(_worker_id, obj);\n+      \/\/ If the evacuation failure occurs during concurrent start we should do\n+      \/\/ any additional necessary per-object actions.\n+      _cm->new_obj_marked(_worker_id, obj, obj_size);\n@@ -93,1 +86,0 @@\n-    size_t obj_size = obj->size();\n@@ -106,2 +98,1 @@\n-  \/\/ accordingly. Since we clear and use the prev bitmap for marking objects that\n-  \/\/ failed evacuation, there is no work to be done there.\n+  \/\/ accordingly.\n@@ -113,24 +104,1 @@\n-    size_t gap_size = pointer_delta(end, start);\n-    MemRegion mr(start, gap_size);\n-    if (gap_size >= CollectedHeap::min_fill_size()) {\n-      CollectedHeap::fill_with_objects(start, gap_size);\n-\n-      HeapWord* end_first_obj = start + cast_to_oop(start)->size();\n-      _hr->update_bot_for_block(start, end_first_obj);\n-      \/\/ Fill_with_objects() may have created multiple (i.e. two)\n-      \/\/ objects, as the max_fill_size() is half a region.\n-      \/\/ After updating the BOT for the first object, also update the\n-      \/\/ BOT for the second object to make the BOT complete.\n-      if (end_first_obj != end) {\n-        _hr->update_bot_for_block(end_first_obj, end);\n-#ifdef ASSERT\n-        size_t size_second_obj = cast_to_oop(end_first_obj)->size();\n-        HeapWord* end_of_second_obj = end_first_obj + size_second_obj;\n-        assert(end == end_of_second_obj,\n-               \"More than two objects were used to fill the area from \" PTR_FORMAT \" to \" PTR_FORMAT \", \"\n-               \"second objects size \" SIZE_FORMAT \" ends at \" PTR_FORMAT,\n-               p2i(start), p2i(end), size_second_obj, p2i(end_of_second_obj));\n-#endif\n-      }\n-    }\n-    assert(!_cm->is_marked_in_prev_bitmap(cast_to_oop(start)), \"should not be marked in prev bitmap\");\n+    _hr->fill_range_with_dead_objects(start, end);\n@@ -167,1 +135,1 @@\n-    \/\/ All objects that failed evacuation has been marked in the prev bitmap.\n+    \/\/ All objects that failed evacuation has been marked in the bitmap.\n@@ -169,1 +137,1 @@\n-    G1CMBitMap* bitmap = const_cast<G1CMBitMap*>(_g1h->concurrent_mark()->prev_mark_bitmap());\n+    G1CMBitMap* bitmap = _g1h->concurrent_mark()->mark_bitmap();\n@@ -173,0 +141,12 @@\n+    \/\/ Now clear all the marks to be ready for a new marking cyle.\n+    if (!during_concurrent_start) {\n+      assert(hr->top_at_mark_start() == hr->bottom(), \"TAMS must be bottom to make all objects look live\");\n+      _g1h->clear_bitmap_for_region(hr);\n+    } else {\n+      assert(hr->top_at_mark_start() == hr->top(), \"TAMS must be top for bitmap to have any value\");\n+      \/\/ Keep the bits.\n+    }\n+    \/\/ We never evacuate Old (non-humongous, non-archive) regions during scrubbing\n+    \/\/ (only afterwards); other regions (young, humongous, archive) never need\n+    \/\/ scrubbing, so the following must hold.\n+    assert(hr->parsable_bottom() == hr->bottom(), \"PB must be bottom to make the whole area parsable\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailure.cpp","additions":21,"deletions":41,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  return _heap->concurrent_mark()->next_mark_bitmap();\n+  return _heap->concurrent_mark()->mark_bitmap();\n@@ -123,1 +123,1 @@\n-    _is_alive(this, heap->concurrent_mark()->next_mark_bitmap()),\n+    _is_alive(this, heap->concurrent_mark()->mark_bitmap()),\n@@ -172,0 +172,2 @@\n+  _heap->verify_before_full_collection(scope()->is_explicit_gc());\n+\n@@ -175,1 +177,0 @@\n-  _heap->verify_before_full_collection(scope()->is_explicit_gc());\n@@ -217,1 +218,0 @@\n-  _heap->concurrent_mark()->swap_mark_bitmaps();\n@@ -219,1 +219,1 @@\n-  _heap->concurrent_mark()->clear_next_bitmap(_heap->workers());\n+  _heap->concurrent_mark()->clear_bitmap(_heap->workers());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_prev_bitmap(oop obj) {\n+void G1FullGCCompactTask::G1CompactRegionClosure::clear_in_bitmap(oop obj) {\n@@ -80,1 +80,1 @@\n-    clear_in_prev_bitmap(obj);\n+    clear_in_bitmap(obj);\n@@ -97,1 +97,1 @@\n-  clear_in_prev_bitmap(obj);\n+  clear_in_bitmap(obj);\n@@ -108,1 +108,1 @@\n-    \/\/ for bitmap verification and to be able to use the prev_bitmap\n+    \/\/ for bitmap verification and to be able to use the bitmap\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,1 +53,1 @@\n-    void clear_in_prev_bitmap(oop object);\n+    void clear_in_bitmap(oop object);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -141,0 +141,5 @@\n+\n+    if (_collector->is_skip_compacting(region_idx) &&\n+        hr->needs_scrubbing_during_full_gc()) {\n+      scrub_skip_compacting_region(hr);\n+    }\n@@ -164,0 +169,25 @@\n+\n+void G1FullGCPrepareTask::G1ResetMetadataClosure::scrub_skip_compacting_region(HeapRegion* hr) {\n+  assert(hr->needs_scrubbing_during_full_gc(), \"must be\");\n+\n+  HeapWord* limit = hr->top();\n+  HeapWord* current_obj = hr->bottom();\n+  G1CMBitMap* bitmap = _collector->mark_bitmap();\n+\n+  while (current_obj < limit) {\n+    if (bitmap->is_marked(current_obj)) {\n+      oop current = cast_to_oop(current_obj);\n+      current_obj += current->size();\n+      continue;\n+    }\n+    \/\/ Found dead object, which is potentially unloaded, scrub to next\n+    \/\/ marked object.\n+    HeapWord* scrub_start = current_obj;\n+    HeapWord* scrub_end = bitmap->get_next_marked_addr(scrub_start, limit);\n+    if (scrub_start != scrub_end) {\n+      hr->fill_range_with_dead_objects(scrub_start, scrub_end);\n+    }\n+\n+    current_obj = scrub_end;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+    void scrub_skip_compacting_region(HeapRegion* hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -227,1 +227,1 @@\n-      if (!_hr->obj_allocated_since_prev_marking(o)) {\n+      if (_hr->obj_in_scrubbing_area(o, _hr->parsable_bottom())) {\n@@ -403,2 +403,2 @@\n-          log_error(gc, verify)(\"[\" PTR_FORMAT \",\" PTR_FORMAT \"] max_live_bytes \" SIZE_FORMAT \" < calculated \" SIZE_FORMAT,\n-                                  p2i(r->bottom()), p2i(r->end()), r->max_live_bytes(), not_dead_yet_cl.live_bytes());\n+          log_error(gc, verify)(HR_FORMAT \" max_live_bytes %zu < calculated %zu\",\n+                                HR_FORMAT_PARAMS(r), r->max_live_bytes(), not_dead_yet_cl.live_bytes());\n@@ -591,1 +591,1 @@\n-  verify(type, VerifyOption::G1UsePrevMarking, \"Before GC\");\n+  verify(type, VerifyOption::G1UseConcMarking, \"Before GC\");\n@@ -595,1 +595,1 @@\n-  verify(type, VerifyOption::G1UsePrevMarking, \"After GC\");\n+  verify(type, VerifyOption::G1UseConcMarking, \"After GC\");\n@@ -662,2 +662,2 @@\n-bool G1HeapVerifier::verify_no_bits_over_tams(const char* bitmap_name, const G1CMBitMap* const bitmap,\n-                                               HeapWord* tams, HeapWord* end) {\n+bool G1HeapVerifier::verify_no_bits_over_tams(const G1CMBitMap* const bitmap,\n+                                              HeapWord* tams, HeapWord* end) {\n@@ -668,2 +668,2 @@\n-    log_error(gc, verify)(\"## wrong marked address on %s bitmap: \" PTR_FORMAT, bitmap_name, p2i(result));\n-    log_error(gc, verify)(\"## %s tams: \" PTR_FORMAT \" end: \" PTR_FORMAT, bitmap_name, p2i(tams), p2i(end));\n+    log_error(gc, verify)(\"## wrong marked address on bitmap: \" PTR_FORMAT, p2i(result));\n+    log_error(gc, verify)(\"## tams: \" PTR_FORMAT \" end: \" PTR_FORMAT, p2i(tams), p2i(end));\n@@ -676,2 +676,1 @@\n-  const G1CMBitMap* const prev_bitmap = _g1h->concurrent_mark()->prev_mark_bitmap();\n-  const G1CMBitMap* const next_bitmap = _g1h->concurrent_mark()->next_mark_bitmap();\n+  const G1CMBitMap* const bitmap = _g1h->concurrent_mark()->mark_bitmap();\n@@ -679,2 +678,1 @@\n-  HeapWord* ptams  = hr->prev_top_at_mark_start();\n-  HeapWord* ntams  = hr->next_top_at_mark_start();\n+  HeapWord* tams   = hr->top_at_mark_start();\n@@ -683,6 +681,4 @@\n-  bool res_p = verify_no_bits_over_tams(\"prev\", prev_bitmap, ptams, end);\n-\n-  bool res_n = true;\n-  \/\/ We cannot verify the next bitmap while we are about to clear it.\n-  if (!_g1h->collector_state()->clearing_next_bitmap()) {\n-    res_n = verify_no_bits_over_tams(\"next\", next_bitmap, ntams, end);\n+  bool result = true;\n+  \/\/ We cannot verify the marking bitmap while we are clearing it.\n+  if (!_g1h->collector_state()->clearing_bitmap()) {\n+    result = verify_no_bits_over_tams(bitmap, tams, end);\n@@ -690,1 +686,1 @@\n-  if (!res_p || !res_n) {\n+  if (!result) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.cpp","additions":17,"deletions":21,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,3 +76,2 @@\n-  \/\/ false. Otherwise, just return true. bitmap_name should be \"prev\"\n-  \/\/ or \"next\".\n-  bool verify_no_bits_over_tams(const char* bitmap_name, const G1CMBitMap* const bitmap,\n+  \/\/ false. Otherwise, just return true.\n+  bool verify_no_bits_over_tams(const G1CMBitMap* const bitmap,\n@@ -81,1 +80,1 @@\n-  \/\/ Verify that the prev \/ next bitmap range [tams,end) for the given\n+  \/\/ Verify that the marking bitmap range [tams,end) for the given\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -111,1 +111,1 @@\n-  _cm->mark_in_next_bitmap(_worker_id, obj);\n+  _cm->mark_in_bitmap(_worker_id, obj);\n@@ -213,1 +213,1 @@\n-  _cm->mark_in_next_bitmap(_worker_id, obj);\n+  _cm->mark_in_bitmap(_worker_id, obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -627,2 +627,2 @@\n-    \/\/ are relabeled as such. We mark the failing objects in the prev bitmap and\n-    \/\/ later use it to handle all failed objects.\n+    \/\/ are relabeled as such. We mark the failing objects in the marking bitmap\n+    \/\/ and later use it to handle all failed objects.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -451,1 +451,1 @@\n-  collector_state()->set_clearing_next_bitmap(false);\n+  collector_state()->set_clearing_bitmap(false);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,2 +38,2 @@\n-\/\/ to ntams. This is an exact measure.\n-\/\/ The code corrects later for the live data between ntams and top.\n+\/\/ to tams. This is an exact measure.\n+\/\/ The code corrects later for the live data between tams and top.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RegionMarkStatsCache.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1263,1 +1263,2 @@\n-  \/\/ Closure to clear the prev bitmap for any old region in the collection set.\n+  \/\/ Closure to make sure that the marking bitmap is clear for any old region in\n+  \/\/ the collection set.\n@@ -1267,0 +1268,1 @@\n+\n@@ -1269,1 +1271,1 @@\n-             \"Bitmap should have no mark for young regions\");\n+             \"Bitmap should have no mark for region %u\", hr->hrm_index());\n@@ -1271,0 +1273,1 @@\n+\n@@ -1276,3 +1279,11 @@\n-      \/\/ Young regions should always have cleared bitmaps, so only clear old.\n-      if (hr->is_old()) {\n-        _g1h->clear_prev_bitmap_for_region(hr);\n+\n+      \/\/ Evacuation failure uses the bitmap to record evacuation failed objects,\n+      \/\/ so the bitmap for the regions in the collection set must be cleared if not already.\n+      \/\/\n+      \/\/ A clear bitmap is obvious for young regions as we never mark through them;\n+      \/\/ old regions are only in the collection set after the concurrent cycle completed,\n+      \/\/ so their bitmaps must also be clear except when the pause occurs during the\n+      \/\/ concurrent bitmap clear. At that point the region's bitmap may contain mark\n+      \/\/ while being in the collection set at the same time.\n+      if (_g1h->collector_state()->clearing_bitmap() && hr->is_old()) {\n+        _g1h->clear_bitmap_for_region(hr);\n@@ -1280,2 +1291,1 @@\n-        assert(hr->is_young(), \"Should only be young and old regions in collection set\");\n-        assert_bitmap_clear(hr, _g1h->concurrent_mark()->prev_mark_bitmap());\n+        assert_bitmap_clear(hr, _g1h->concurrent_mark()->mark_bitmap());\n@@ -1793,263 +1803,0 @@\n-\n-class G1RebuildRemSetTask: public WorkerTask {\n-  \/\/ Aggregate the counting data that was constructed concurrently\n-  \/\/ with marking.\n-  class G1RebuildRemSetHeapRegionClosure : public HeapRegionClosure {\n-    G1ConcurrentMark* _cm;\n-    G1RebuildRemSetClosure _update_cl;\n-\n-    \/\/ Applies _update_cl to the references of the given object, limiting objArrays\n-    \/\/ to the given MemRegion. Returns the amount of words actually scanned.\n-    size_t scan_for_references(oop const obj, MemRegion mr) {\n-      size_t const obj_size = obj->size();\n-      \/\/ All non-objArrays and objArrays completely within the mr\n-      \/\/ can be scanned without passing the mr.\n-      if (!obj->is_objArray() || mr.contains(MemRegion(cast_from_oop<HeapWord*>(obj), obj_size))) {\n-        obj->oop_iterate(&_update_cl);\n-        return obj_size;\n-      }\n-      \/\/ This path is for objArrays crossing the given MemRegion. Only scan the\n-      \/\/ area within the MemRegion.\n-      obj->oop_iterate(&_update_cl, mr);\n-      return mr.intersection(MemRegion(cast_from_oop<HeapWord*>(obj), obj_size)).word_size();\n-    }\n-\n-    \/\/ A humongous object is live (with respect to the scanning) either\n-    \/\/ a) it is marked on the bitmap as such\n-    \/\/ b) its TARS is larger than TAMS, i.e. has been allocated during marking.\n-    bool is_humongous_live(oop const humongous_obj, const G1CMBitMap* const bitmap, HeapWord* tams, HeapWord* tars) const {\n-      return bitmap->is_marked(humongous_obj) || (tars > tams);\n-    }\n-\n-    \/\/ Iterator over the live objects within the given MemRegion.\n-    class LiveObjIterator : public StackObj {\n-      const G1CMBitMap* const _bitmap;\n-      const HeapWord* _tams;\n-      const MemRegion _mr;\n-      HeapWord* _current;\n-\n-      bool is_below_tams() const {\n-        return _current < _tams;\n-      }\n-\n-      bool is_live(HeapWord* obj) const {\n-        return !is_below_tams() || _bitmap->is_marked(obj);\n-      }\n-\n-      HeapWord* bitmap_limit() const {\n-        return MIN2(const_cast<HeapWord*>(_tams), _mr.end());\n-      }\n-\n-      void move_if_below_tams() {\n-        if (is_below_tams() && has_next()) {\n-          _current = _bitmap->get_next_marked_addr(_current, bitmap_limit());\n-        }\n-      }\n-    public:\n-      LiveObjIterator(const G1CMBitMap* const bitmap, const HeapWord* tams, const MemRegion mr, HeapWord* first_oop_into_mr) :\n-          _bitmap(bitmap),\n-          _tams(tams),\n-          _mr(mr),\n-          _current(first_oop_into_mr) {\n-\n-        assert(_current <= _mr.start(),\n-               \"First oop \" PTR_FORMAT \" should extend into mr [\" PTR_FORMAT \", \" PTR_FORMAT \")\",\n-               p2i(first_oop_into_mr), p2i(mr.start()), p2i(mr.end()));\n-\n-        \/\/ Step to the next live object within the MemRegion if needed.\n-        if (is_live(_current)) {\n-          \/\/ Non-objArrays were scanned by the previous part of that region.\n-          if (_current < mr.start() && !cast_to_oop(_current)->is_objArray()) {\n-            _current += cast_to_oop(_current)->size();\n-            \/\/ We might have positioned _current on a non-live object. Reposition to the next\n-            \/\/ live one if needed.\n-            move_if_below_tams();\n-          }\n-        } else {\n-          \/\/ The object at _current can only be dead if below TAMS, so we can use the bitmap.\n-          \/\/ immediately.\n-          _current = _bitmap->get_next_marked_addr(_current, bitmap_limit());\n-          assert(_current == _mr.end() || is_live(_current),\n-                 \"Current \" PTR_FORMAT \" should be live (%s) or beyond the end of the MemRegion (\" PTR_FORMAT \")\",\n-                 p2i(_current), BOOL_TO_STR(is_live(_current)), p2i(_mr.end()));\n-        }\n-      }\n-\n-      void move_to_next() {\n-        _current += next()->size();\n-        move_if_below_tams();\n-      }\n-\n-      oop next() const {\n-        oop result = cast_to_oop(_current);\n-        assert(is_live(_current),\n-               \"Object \" PTR_FORMAT \" must be live TAMS \" PTR_FORMAT \" below %d mr \" PTR_FORMAT \" \" PTR_FORMAT \" outside %d\",\n-               p2i(_current), p2i(_tams), _tams > _current, p2i(_mr.start()), p2i(_mr.end()), _mr.contains(result));\n-        return result;\n-      }\n-\n-      bool has_next() const {\n-        return _current < _mr.end();\n-      }\n-    };\n-\n-    \/\/ Rebuild remembered sets in the part of the region specified by mr and hr.\n-    \/\/ Objects between the bottom of the region and the TAMS are checked for liveness\n-    \/\/ using the given bitmap. Objects between TAMS and TARS are assumed to be live.\n-    \/\/ Returns the number of live words between bottom and TAMS.\n-    size_t rebuild_rem_set_in_region(const G1CMBitMap* const bitmap,\n-                                     HeapWord* const top_at_mark_start,\n-                                     HeapWord* const top_at_rebuild_start,\n-                                     HeapRegion* hr,\n-                                     MemRegion mr) {\n-      size_t marked_words = 0;\n-\n-      if (hr->is_humongous()) {\n-        oop const humongous_obj = cast_to_oop(hr->humongous_start_region()->bottom());\n-        if (is_humongous_live(humongous_obj, bitmap, top_at_mark_start, top_at_rebuild_start)) {\n-          \/\/ We need to scan both [bottom, TAMS) and [TAMS, top_at_rebuild_start);\n-          \/\/ however in case of humongous objects it is sufficient to scan the encompassing\n-          \/\/ area (top_at_rebuild_start is always larger or equal to TAMS) as one of the\n-          \/\/ two areas will be zero sized. I.e. TAMS is either\n-          \/\/ the same as bottom or top(_at_rebuild_start). There is no way TAMS has a different\n-          \/\/ value: this would mean that TAMS points somewhere into the object.\n-          assert(hr->top() == top_at_mark_start || hr->top() == top_at_rebuild_start,\n-                 \"More than one object in the humongous region?\");\n-          humongous_obj->oop_iterate(&_update_cl, mr);\n-          return top_at_mark_start != hr->bottom() ? mr.intersection(MemRegion(cast_from_oop<HeapWord*>(humongous_obj), humongous_obj->size())).byte_size() : 0;\n-        } else {\n-          return 0;\n-        }\n-      }\n-\n-      for (LiveObjIterator it(bitmap, top_at_mark_start, mr, hr->block_start(mr.start())); it.has_next(); it.move_to_next()) {\n-        oop obj = it.next();\n-        size_t scanned_size = scan_for_references(obj, mr);\n-        if (cast_from_oop<HeapWord*>(obj) < top_at_mark_start) {\n-          marked_words += scanned_size;\n-        }\n-      }\n-\n-      return marked_words * HeapWordSize;\n-    }\n-public:\n-  G1RebuildRemSetHeapRegionClosure(G1CollectedHeap* g1h,\n-                                   G1ConcurrentMark* cm,\n-                                   uint worker_id) :\n-    HeapRegionClosure(),\n-    _cm(cm),\n-    _update_cl(g1h, worker_id) { }\n-\n-    bool do_heap_region(HeapRegion* hr) {\n-      if (_cm->has_aborted()) {\n-        return true;\n-      }\n-\n-      uint const region_idx = hr->hrm_index();\n-      DEBUG_ONLY(HeapWord* const top_at_rebuild_start_check = _cm->top_at_rebuild_start(region_idx);)\n-      assert(top_at_rebuild_start_check == NULL ||\n-             top_at_rebuild_start_check > hr->bottom(),\n-             \"A TARS (\" PTR_FORMAT \") == bottom() (\" PTR_FORMAT \") indicates the old region %u is empty (%s)\",\n-             p2i(top_at_rebuild_start_check), p2i(hr->bottom()),  region_idx, hr->get_type_str());\n-\n-      size_t total_marked_bytes = 0;\n-      size_t const chunk_size_in_words = G1RebuildRemSetChunkSize \/ HeapWordSize;\n-\n-      HeapWord* const top_at_mark_start = hr->prev_top_at_mark_start();\n-\n-      HeapWord* cur = hr->bottom();\n-      while (true) {\n-        \/\/ After every iteration (yield point) we need to check whether the region's\n-        \/\/ TARS changed due to e.g. eager reclaim.\n-        HeapWord* const top_at_rebuild_start = _cm->top_at_rebuild_start(region_idx);\n-        if (top_at_rebuild_start == NULL) {\n-          return false;\n-        }\n-\n-        MemRegion next_chunk = MemRegion(hr->bottom(), top_at_rebuild_start).intersection(MemRegion(cur, chunk_size_in_words));\n-        if (next_chunk.is_empty()) {\n-          break;\n-        }\n-\n-        const Ticks start = Ticks::now();\n-        size_t marked_bytes = rebuild_rem_set_in_region(_cm->prev_mark_bitmap(),\n-                                                        top_at_mark_start,\n-                                                        top_at_rebuild_start,\n-                                                        hr,\n-                                                        next_chunk);\n-        Tickspan time = Ticks::now() - start;\n-\n-        log_trace(gc, remset, tracking)(\"Rebuilt region %u \"\n-                                        \"live \" SIZE_FORMAT \" \"\n-                                        \"time %.3fms \"\n-                                        \"marked bytes \" SIZE_FORMAT \" \"\n-                                        \"bot \" PTR_FORMAT \" \"\n-                                        \"TAMS \" PTR_FORMAT \" \"\n-                                        \"TARS \" PTR_FORMAT,\n-                                        region_idx,\n-                                        _cm->live_bytes(region_idx),\n-                                        time.seconds() * 1000.0,\n-                                        marked_bytes,\n-                                        p2i(hr->bottom()),\n-                                        p2i(top_at_mark_start),\n-                                        p2i(top_at_rebuild_start));\n-\n-        if (marked_bytes > 0) {\n-          total_marked_bytes += marked_bytes;\n-        }\n-        cur += chunk_size_in_words;\n-\n-        _cm->do_yield_check();\n-        if (_cm->has_aborted()) {\n-          return true;\n-        }\n-      }\n-      \/\/ In the final iteration of the loop the region might have been eagerly reclaimed.\n-      \/\/ Simply filter out those regions. We can not just use region type because there\n-      \/\/ might have already been new allocations into these regions.\n-      DEBUG_ONLY(HeapWord* const top_at_rebuild_start = _cm->top_at_rebuild_start(region_idx);)\n-      assert(top_at_rebuild_start == NULL ||\n-             total_marked_bytes == hr->marked_bytes(),\n-             \"Marked bytes \" SIZE_FORMAT \" for region %u (%s) in [bottom, TAMS) do not match calculated marked bytes \" SIZE_FORMAT \" \"\n-             \"(\" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \")\",\n-             total_marked_bytes, hr->hrm_index(), hr->get_type_str(), hr->marked_bytes(),\n-             p2i(hr->bottom()), p2i(top_at_mark_start), p2i(top_at_rebuild_start));\n-       \/\/ Abort state may have changed after the yield check.\n-      return _cm->has_aborted();\n-    }\n-  };\n-\n-  HeapRegionClaimer _hr_claimer;\n-  G1ConcurrentMark* _cm;\n-\n-  uint _worker_id_offset;\n-public:\n-  G1RebuildRemSetTask(G1ConcurrentMark* cm,\n-                      uint n_workers,\n-                      uint worker_id_offset) :\n-      WorkerTask(\"G1 Rebuild Remembered Set\"),\n-      _hr_claimer(n_workers),\n-      _cm(cm),\n-      _worker_id_offset(worker_id_offset) {\n-  }\n-\n-  void work(uint worker_id) {\n-    SuspendibleThreadSetJoiner sts_join;\n-\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-\n-    G1RebuildRemSetHeapRegionClosure cl(g1h, _cm, _worker_id_offset + worker_id);\n-    g1h->heap_region_par_iterate_from_worker_offset(&cl, &_hr_claimer, worker_id);\n-  }\n-};\n-\n-void G1RemSet::rebuild_rem_set(G1ConcurrentMark* cm,\n-                               WorkerThreads* workers,\n-                               uint worker_id_offset) {\n-  uint num_workers = workers->active_workers();\n-\n-  G1RebuildRemSetTask cl(cm,\n-                         num_workers,\n-                         worker_id_offset);\n-  workers->run_task(&cl, num_workers);\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":17,"deletions":270,"binary":false,"changes":287,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -150,4 +150,0 @@\n-\n-  \/\/ Rebuilds the remembered set by scanning from bottom to TARS for all regions\n-  \/\/ using the given workers.\n-  void rebuild_rem_set(G1ConcurrentMark* cm, WorkerThreads* workers, uint worker_id_offset);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,2 +65,2 @@\n-                                  \"(ntams: \" PTR_FORMAT \") \"\n-                                  \"total_live_bytes \" SIZE_FORMAT \" \"\n+                                  \"(tams: \" PTR_FORMAT \") \"\n+                                  \"total_live_bytes %zu \"\n@@ -68,3 +68,2 @@\n-                                  \"(live_bytes \" SIZE_FORMAT \" \"\n-                                  \"next_marked \" SIZE_FORMAT \" \"\n-                                  \"marked \" SIZE_FORMAT \" \"\n+                                  \"(live_bytes %zu \"\n+                                  \"marked %zu \"\n@@ -73,1 +72,1 @@\n-                                  p2i(r->next_top_at_mark_start()),\n+                                  p2i(r->top_at_mark_start()),\n@@ -77,1 +76,0 @@\n-                                  r->next_marked_bytes(),\n@@ -119,2 +117,2 @@\n-  size_t between_ntams_and_top = (r->top() - r->next_top_at_mark_start()) * HeapWordSize;\n-  size_t total_live_bytes = live_bytes + between_ntams_and_top;\n+  size_t between_tams_and_top = (r->top() - r->top_at_mark_start()) * HeapWordSize;\n+  size_t total_live_bytes = live_bytes + between_tams_and_top;\n@@ -166,5 +164,4 @@\n-                                    \"(ntams \" PTR_FORMAT \" \"\n-                                    \"liveness \" SIZE_FORMAT \" \"\n-                                    \"next_marked_bytes \" SIZE_FORMAT \" \"\n-                                    \"remset occ \" SIZE_FORMAT \" \"\n-                                    \"size \" SIZE_FORMAT \")\",\n+                                    \"(tams \" PTR_FORMAT \" \"\n+                                    \"liveness %zu \"\n+                                    \"remset occ %zu \"\n+                                    \"size %zu)\",\n@@ -172,1 +169,1 @@\n-                                    p2i(r->next_top_at_mark_start()),\n+                                    p2i(r->top_at_mark_start()),\n@@ -174,1 +171,0 @@\n-                                    r->next_marked_bytes(),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSetTrackingPolicy.cpp","additions":13,"deletions":17,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,1 +56,1 @@\n-\/\/ An entry that is below the NTAMS pointer for the containing heap\n+\/\/ An entry that is below the TAMS pointer for the containing heap\n@@ -59,1 +59,1 @@\n-\/\/ An entry that is at least the NTAMS pointer for the containing heap\n+\/\/ An entry that is at least the TAMS pointer for the containing heap\n@@ -78,1 +78,1 @@\n-\/\/ The stale reference cases are implicitly handled by the NTAMS\n+\/\/ The stale reference cases are implicitly handled by the TAMS\n@@ -90,1 +90,1 @@\n-  if (entry >= region->next_top_at_mark_start()) {\n+  if (entry >= region->top_at_mark_start()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SATBMarkQueueSet.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -393,1 +393,1 @@\n-      log_debug(gc, humongous)(\"Humongous region %u (object size \" SIZE_FORMAT \" @ \" PTR_FORMAT \") remset \" SIZE_FORMAT \" code roots \" SIZE_FORMAT \" marked %d reclaim candidate %d type array %d\",\n+      log_debug(gc, humongous)(\"Humongous region %u (object size %zu @ \" PTR_FORMAT \") remset %zu code roots %zu marked %d reclaim candidate %d type array %d\",\n@@ -399,1 +399,1 @@\n-                               _g1h->concurrent_mark()->next_mark_bitmap()->is_marked(hr->bottom()),\n+                               _g1h->concurrent_mark()->mark_bitmap()->is_marked(hr->bottom()),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -208,2 +208,2 @@\n-    assert(!cm->is_marked_in_prev_bitmap(obj) && !cm->is_marked_in_next_bitmap(obj),\n-           \"Eagerly reclaimed humongous region %u should not be marked at all but is in prev %s next %s\",\n+    assert(!cm->is_marked_in_bitmap(obj),\n+           \"Eagerly reclaimed humongous region %u should not be marked at all but is in bitmap %s\",\n@@ -211,2 +211,1 @@\n-           BOOL_TO_STR(cm->is_marked_in_prev_bitmap(obj)),\n-           BOOL_TO_STR(cm->is_marked_in_next_bitmap(obj)));\n+           BOOL_TO_STR(cm->is_marked_in_bitmap(obj)));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -382,0 +382,6 @@\n+  product(size_t, G1LogBackScanSkipGranularity, 16, EXPERIMENTAL,           \\\n+          \"Log of the granularity (step) size of the backwards scan table \" \\\n+          \"used for finding marks on the bitmap below parsable bottom \"     \\\n+          \"scrubbing.\")                                                     \\\n+          range(14, NOT_LP64(25) LP64_ONLY(29))                             \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/g1\/g1_globals.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -107,1 +107,0 @@\n-  _next_marked_bytes = 0;\n@@ -128,2 +127,0 @@\n-  zero_marked_bytes();\n-\n@@ -241,2 +238,4 @@\n-  _prev_top_at_mark_start(NULL), _next_top_at_mark_start(NULL),\n-  _prev_marked_bytes(0), _next_marked_bytes(0),\n+  _top_at_mark_start(NULL),\n+  _parsable_bottom(NULL),\n+  _garbage_bytes(0),\n+  _marked_bytes(0),\n@@ -277,4 +276,4 @@\n-  \/\/ We always recreate the prev marking info and we'll explicitly\n-  \/\/ mark all objects we find to be self-forwarded on the prev\n-  \/\/ bitmap. So all objects need to be below PTAMS.\n-  _prev_marked_bytes = 0;\n+  \/\/ We always scrub the region to make sure the entire region is\n+  \/\/ parsable after the self-forwarding point removal, and update _marked_bytes\n+  \/\/ at the end.\n+  _marked_bytes = 0;\n@@ -284,4 +283,3 @@\n-    \/\/ we find to be self-forwarded on the next bitmap. So all\n-    \/\/ objects need to be below NTAMS.\n-    _next_top_at_mark_start = top();\n-    _next_marked_bytes = 0;\n+    \/\/ we find to be self-forwarded in the marking bitmap. So all\n+    \/\/ objects need to be below TAMS.\n+    _top_at_mark_start = top();\n@@ -291,3 +289,2 @@\n-    \/\/ So all objects need to be above NTAMS.\n-    _next_top_at_mark_start = bottom();\n-    _next_marked_bytes = 0;\n+    \/\/ So all objects need to be above TAMS.\n+    _top_at_mark_start = bottom();\n@@ -300,2 +297,1 @@\n-  _prev_top_at_mark_start = top();\n-  _prev_marked_bytes = marked_bytes;\n+  _marked_bytes = marked_bytes;\n@@ -459,2 +455,2 @@\n-  st->print(\"|TAMS \" PTR_FORMAT \", \" PTR_FORMAT \"| %s \",\n-               p2i(prev_top_at_mark_start()), p2i(next_top_at_mark_start()), rem_set()->get_state_str());\n+  st->print(\"|TAMS \" PTR_FORMAT \"| PB \" PTR_FORMAT \"| %s \",\n+               p2i(top_at_mark_start()), p2i(parsable_bottom_acquire()), rem_set()->get_state_str());\n@@ -482,0 +478,1 @@\n+\n@@ -526,1 +523,2 @@\n-      if (!_g1h->is_in(obj) || _g1h->is_obj_dead_cond(obj, _vo)) {\n+      bool is_in_heap = _g1h->is_in(obj);\n+      if (!is_in_heap || _g1h->is_obj_dead_cond(obj, _vo)) {\n@@ -533,1 +531,1 @@\n-        if (!_g1h->is_in(obj)) {\n+        if (!is_in_heap) {\n@@ -767,1 +765,1 @@\n-  verify_rem_set(VerifyOption::G1UsePrevMarking, &failures);\n+  verify_rem_set(VerifyOption::G1UseConcMarking, &failures);\n@@ -793,1 +791,1 @@\n-    if (block_is_obj(p)) {\n+    if (block_is_obj(p, parsable_bottom())) {\n@@ -808,0 +806,19 @@\n+\n+void HeapRegion::fill_range_with_dead_objects(HeapWord* start, HeapWord* end) {\n+  size_t range_size = pointer_delta(end, start);\n+  if (range_size >= CollectedHeap::min_fill_size()) {\n+    \/\/ Fill the dead range with objects. G1 might need to create two objects if\n+    \/\/ the range is larger than half a region, which is the max_fill_size().\n+    CollectedHeap::fill_with_objects(start, range_size);\n+    HeapWord* current = start;\n+    do {\n+      \/\/ Update the BOT if the a threshold is crossed.\n+      size_t obj_size = cast_to_oop(current)->size();\n+      update_bot_for_block(current, current + obj_size);\n+\n+      \/\/ Advance to the next object.\n+      current += obj_size;\n+      guarantee(current <= end, \"Should never go past end\");\n+    } while (current != end);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.cpp","additions":42,"deletions":25,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -104,1 +104,1 @@\n-  HeapWord* pre_dummy_top() { return (_pre_dummy_top == NULL) ? top() : _pre_dummy_top; }\n+  HeapWord* pre_dummy_top() const { return (_pre_dummy_top == NULL) ? top() : _pre_dummy_top; }\n@@ -147,0 +147,7 @@\n+  \/\/ Find a live object spanning over the given address or starting at it. Returns\n+  \/\/ nullptr if there is no such object.\n+  inline HeapWord* prev_live_spanning_into_in_unparsable(const G1CMBitMap* bitmap, HeapWord* start) const;\n+\n+  \/\/ Finds a block spanning into start that is less than parsable_bottom and returns\n+  \/\/ its address. Nullptr if there is no such block.\n+  inline HeapWord* block_start_using_bitmap(HeapWord* start, HeapWord* pb) const;\n@@ -148,1 +155,1 @@\n-  HeapWord* block_start(const void* p);\n+  HeapWord* block_start(const void* addr, HeapWord* const pb);\n@@ -156,0 +163,5 @@\n+  \/\/ Create objects in the given range. The BOT will be updated if needed and\n+  \/\/ the created objects will have their header marked to show that they are\n+  \/\/ dead.\n+  void fill_range_with_dead_objects(HeapWord* start, HeapWord* end);\n+\n@@ -176,1 +188,3 @@\n-  bool block_is_obj(const HeapWord* p) const;\n+  bool block_is_obj(const HeapWord* p, HeapWord* pb) const;\n+\n+  bool obj_is_scrubbed(oop obj) const;\n@@ -178,4 +192,4 @@\n-  \/\/ Returns whether the given object is dead based on TAMS and bitmap.\n-  \/\/ An object is dead iff a) it was not allocated since the last mark (>TAMS), b) it\n-  \/\/ is not marked (bitmap).\n-  bool is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const;\n+  \/\/ Returns whether the given object is dead based on TAMS and mark word.\n+  \/\/ For an object to be considered dead it must be below TAMS and scrubbed.\n+  bool is_obj_dead(oop obj, HeapWord* pb) const;\n+  bool is_obj_dead_size_below_pb(oop obj, HeapWord* pb, size_t& block_size) const;\n@@ -186,0 +200,1 @@\n+  size_t block_size(const HeapWord* p, HeapWord* pb) const;\n@@ -193,1 +208,1 @@\n-    _bot_part.update();\n+    _bot_part.update(parsable_bottom());\n@@ -225,5 +240,12 @@\n-  \/\/ \"prev\" is the top at the start of the last completed marking.\n-  \/\/ \"next\" is the top at the start of the in-progress marking (if any.)\n-  HeapWord* _prev_top_at_mark_start;\n-  HeapWord* _next_top_at_mark_start;\n-\n+  HeapWord* _top_at_mark_start;\n+\n+  \/\/ The area above this limit is fully parsable. This limit\n+  \/\/ is equal to bottom except from Remark and until the region has been\n+  \/\/ scrubbed concurrently. The scrubbing ensures that all dead objects (with\n+  \/\/ possibly unloaded classes) have beenreplaced with filler objects that\n+  \/\/ are parsable. Below this limit the marking bitmap must be used to\n+  \/\/ determine size and liveness.\n+  HeapWord* volatile _parsable_bottom;\n+\n+  \/\/ Amount of dead data in the region.\n+  size_t _garbage_bytes;\n@@ -232,2 +254,1 @@\n-  size_t _prev_marked_bytes;    \/\/ Bytes known to be live via last completed marking.\n-  size_t _next_marked_bytes;    \/\/ Bytes known to be live via in-progress marking.\n+  size_t _marked_bytes;    \/\/ Bytes known to be live via last completed marking.\n@@ -236,4 +257,4 @@\n-    assert(_prev_marked_bytes == 0 &&\n-           _next_marked_bytes == 0,\n-           \"Must be called after zero_marked_bytes.\");\n-    _prev_top_at_mark_start = _next_top_at_mark_start = bottom();\n+    _top_at_mark_start = bottom();\n+    _parsable_bottom = bottom();\n+    _garbage_bytes = 0;\n+    _marked_bytes = 0;\n@@ -256,6 +277,5 @@\n-  \/\/ Returns whether the given object address refers to a dead object, and either the\n-  \/\/ size of the object (if live) or the size of the block (if dead) in size.\n-  \/\/ May\n-  \/\/ - only called with obj < top()\n-  \/\/ - not called on humongous objects or archive regions\n-  inline bool is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const;\n+  template <class Closure, bool is_gc_active>\n+  inline HeapWord* oops_on_memregion_iterate(MemRegion mr, Closure* cl);\n+\n+  template <class Closure, bool is_gc_active>\n+  inline HeapWord* oops_on_memregion_iterate_in_unparsable(MemRegion mr, HeapWord* pb, Closure* cl);\n@@ -272,2 +292,6 @@\n-                                                     Closure* cl,\n-                                                     G1CollectedHeap* g1h);\n+                                                     Closure* cl);\n+\n+  inline bool is_marked_in_bitmap(oop obj) const;\n+\n+  inline HeapWord* next_live_in_unparsable(G1CMBitMap* bitmap, const HeapWord* p, HeapWord* limit) const;\n+  inline HeapWord* next_live_in_unparsable(const HeapWord* p, HeapWord* limit) const;\n@@ -275,3 +299,0 @@\n-  \/\/ Returns the block size of the given (dead, potentially having its class unloaded) object\n-  \/\/ starting at p extending to at most the prev TAMS using the given mark bitmap.\n-  inline size_t block_size_using_bitmap(const HeapWord* p, const G1CMBitMap* const prev_bitmap) const;\n@@ -325,11 +346,3 @@\n-  size_t marked_bytes()    { return _prev_marked_bytes; }\n-  size_t live_bytes() {\n-    return (top() - prev_top_at_mark_start()) * HeapWordSize + marked_bytes();\n-  }\n-\n-  \/\/ The number of bytes counted in the next marking.\n-  size_t next_marked_bytes() { return _next_marked_bytes; }\n-  \/\/ The number of bytes live wrt the next marking.\n-  size_t next_live_bytes() {\n-    return\n-      (top() - next_top_at_mark_start()) * HeapWordSize + next_marked_bytes();\n+  size_t marked_bytes() const { return _marked_bytes; }\n+  size_t live_bytes() const {\n+    return byte_size(bottom(), top()) - garbage_bytes();\n@@ -339,5 +352,1 @@\n-  size_t garbage_bytes() {\n-    size_t used_at_mark_start_bytes =\n-      (prev_top_at_mark_start() - bottom()) * HeapWordSize;\n-    return used_at_mark_start_bytes - marked_bytes();\n-  }\n+  size_t garbage_bytes() const { return _garbage_bytes; }\n@@ -356,5 +365,1 @@\n-  size_t max_live_bytes() { return used() - garbage_bytes(); }\n-\n-  void add_to_marked_bytes(size_t incr_bytes) {\n-    _next_marked_bytes = _next_marked_bytes + incr_bytes;\n-  }\n+  size_t max_live_bytes() const { return used() - garbage_bytes(); }\n@@ -362,3 +367,0 @@\n-  void zero_marked_bytes()      {\n-    _prev_marked_bytes = _next_marked_bytes = 0;\n-  }\n@@ -366,2 +368,6 @@\n-  HeapWord* prev_top_at_mark_start() const { return _prev_top_at_mark_start; }\n-  HeapWord* next_top_at_mark_start() const { return _next_top_at_mark_start; }\n+  HeapWord* top_at_mark_start() const { return _top_at_mark_start; }\n+\n+  \/\/ Retrieve parsable bottom; since it may be modified concurrently, outside a\n+  \/\/ safepoint the _acquire method must be used.\n+  HeapWord* parsable_bottom() const;\n+  HeapWord* parsable_bottom_acquire() const;\n@@ -377,4 +383,18 @@\n-  \/\/ Notify the region that concurrent marking has finished. Copy the\n-  \/\/ (now finalized) next marking info fields into the prev marking\n-  \/\/ info fields.\n-  inline void note_end_of_marking();\n+  \/\/ Notify the region that concurrent marking has finished. Passes the number of\n+  \/\/ bytes between bottom and TAMS.\n+  inline void note_end_of_marking(size_t marked_bytes);\n+\n+  \/\/ Notify the region that scrubbing has completed.\n+  inline void note_end_of_scrubbing();\n+\n+  \/\/ During the concurrent scrubbing phase, can there be any areas with unloaded\n+  \/\/ classes in this region?\n+  \/\/ This set only includes old and open archive regions - humongous regions only\n+  \/\/ contain a single object which is either dead or live, contents of closed archive\n+  \/\/ regions never die (so is always contiguous), and young regions are never even\n+  \/\/ considered during concurrent scrub.\n+  bool needs_scrubbing() const { return is_old() || is_open_archive(); }\n+  \/\/ Same question as above, during full gc. Full gc needs to scrub any region that\n+  \/\/ might be skipped for compaction. This includes young generation regions as the\n+  \/\/ region relabeling to old happens later than scrubbing.\n+  bool needs_scrubbing_during_full_gc() const { return is_young() || needs_scrubbing(); }\n@@ -538,8 +558,6 @@\n-  \/\/ Determine if an object has been allocated since the last\n-  \/\/ mark performed by the collector. This returns true iff the object\n-  \/\/ is within the unmarked area of the region.\n-  bool obj_allocated_since_prev_marking(oop obj) const {\n-    return cast_from_oop<HeapWord*>(obj) >= prev_top_at_mark_start();\n-  }\n-  bool obj_allocated_since_next_marking(oop obj) const {\n-    return cast_from_oop<HeapWord*>(obj) >= next_top_at_mark_start();\n+  \/\/ Determine if an object is in the parsable or the to-be-scrubbed area.\n+  inline bool obj_in_parsable_area(const HeapWord* addr, HeapWord* pb) const;\n+  inline bool obj_in_scrubbing_area(oop obj, HeapWord* pb) const;\n+\n+  bool obj_allocated_since_marking_start(oop obj) const {\n+    return cast_from_oop<HeapWord*>(obj) >= top_at_mark_start();\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.hpp","additions":87,"deletions":69,"binary":false,"changes":156,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"g1ConcurrentRefine.hpp\"\n@@ -82,2 +84,2 @@\n-inline HeapWord* HeapRegion::block_start(const void* p) {\n-  return _bot_part.block_start(p);\n+inline HeapWord* HeapRegion::block_start(const void* addr, HeapWord* const pb) {\n+  return _bot_part.block_start(addr, pb);\n@@ -86,2 +88,2 @@\n-inline bool HeapRegion::is_obj_dead_with_size(const oop obj, const G1CMBitMap* const prev_bitmap, size_t* size) const {\n-  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+inline HeapWord* HeapRegion::prev_live_spanning_into_in_unparsable(const G1CMBitMap* const bitmap, HeapWord* const addr) const {\n+  assert(is_aligned(addr, BOTConstants::card_size_in_words()), \"Start not aligned: \" PTR_FORMAT, p2i(addr));\n@@ -89,5 +91,4 @@\n-  assert(addr < top(), \"must be\");\n-  assert(!is_closed_archive(),\n-         \"Closed archive regions should not have references into other regions\");\n-  assert(!is_humongous(), \"Humongous objects not handled here\");\n-  bool obj_is_dead = is_obj_dead(obj, prev_bitmap);\n+  HeapWord* result = bitmap->get_prev_marked_addr(bottom(), addr);\n+  if (result == nullptr) {\n+    return nullptr;\n+  }\n@@ -95,6 +96,6 @@\n-  if (ClassUnloading && obj_is_dead) {\n-    assert(!block_is_obj(addr), \"must be\");\n-    *size = block_size_using_bitmap(addr, prev_bitmap);\n-  } else {\n-    assert(block_is_obj(addr), \"must be\");\n-    *size = obj->size();\n+  assert(bitmap->is_marked(result), \"obj at \" PTR_FORMAT \" should be marked\", p2i(result));\n+\n+  \/\/ Check if that object spans into the start address.\n+  oop result_obj = cast_to_oop(result);\n+  if (result + result_obj->size() < addr) {\n+    result = nullptr;\n@@ -102,1 +103,28 @@\n-  return obj_is_dead;\n+  return result;\n+}\n+\n+inline HeapWord* HeapRegion::block_start_using_bitmap(HeapWord* const start, HeapWord* const pb) const {\n+  assert(is_aligned(start, BOTConstants::card_size_in_words()), \"Start not aligned: \" PTR_FORMAT, p2i(start));\n+  assert(start < pb, \"must be\");\n+\n+  G1CMBitMap* bitmap = G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap();\n+  HeapWord* result = prev_live_spanning_into_in_unparsable(bitmap, start);\n+\n+  if (result == nullptr) {\n+    \/\/ No live object spanning into this memory region. Find\n+    \/\/ the first live after start in the unparsable area.\n+    result = next_live_in_unparsable(bitmap, start, pb);\n+  }\n+  return result;\n+}\n+\n+inline bool HeapRegion::obj_in_scrubbing_area(oop obj, HeapWord* const pb) const {\n+  return !obj_in_parsable_area(cast_from_oop<HeapWord*>(obj), pb);\n+}\n+\n+inline bool HeapRegion::obj_in_parsable_area(const HeapWord* addr, HeapWord* const pb) const {\n+  return addr >= pb;\n+}\n+\n+inline bool HeapRegion::is_marked_in_bitmap(oop obj) const {\n+  return G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap()->is_marked(obj);\n@@ -105,1 +133,1 @@\n-inline bool HeapRegion::block_is_obj(const HeapWord* p) const {\n+inline bool HeapRegion::block_is_obj(const HeapWord* const p, HeapWord* const pb) const {\n@@ -108,0 +136,5 @@\n+\n+  if (obj_in_parsable_area(p, pb)) {\n+    return true;\n+  }\n+\n@@ -111,8 +144,6 @@\n-  \/\/ During a Full GC regions can be excluded from compaction due to high live ratio, and\n-  \/\/ because of this there can be stale objects for unloaded classes left in these regions.\n-  \/\/ During a concurrent cycle class unloading is done after marking is complete and objects\n-  \/\/ for the unloaded classes will be stale until the regions are collected.\n-  if (ClassUnloading) {\n-    return !G1CollectedHeap::heap()->is_obj_dead(cast_to_oop(p), this);\n-  }\n-  return true;\n+  \/\/ To make sure dead objects can be handled without always keeping an additional bitmap, we\n+  \/\/ scrub dead objects and create filler objects that are considered dead. We do this even if\n+  \/\/ class unloading is disabled to avoid special code.\n+  \/\/ From Remark until the region has been completely scrubbed obj_is_parsable will return false\n+  \/\/ and we have to use the bitmap to know if a block is a valid object.\n+  return is_marked_in_bitmap(cast_to_oop(p));\n@@ -121,6 +152,4 @@\n-inline size_t HeapRegion::block_size_using_bitmap(const HeapWord* addr, const G1CMBitMap* const prev_bitmap) const {\n-  assert(ClassUnloading,\n-         \"All blocks should be objects if class unloading isn't used, so this method should not be called. \"\n-         \"HR: [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \") \"\n-         \"addr: \" PTR_FORMAT,\n-         p2i(bottom()), p2i(top()), p2i(end()), p2i(addr));\n+inline bool HeapRegion::obj_is_scrubbed(const oop obj) const {\n+  Klass* k = obj->klass();\n+  return k == Universe::fillerArrayKlassObj() || k == vmClasses::FillerObject_klass();\n+}\n@@ -128,3 +157,2 @@\n-  \/\/ Old regions' dead objects may have dead classes\n-  \/\/ We need to find the next live object using the bitmap\n-  HeapWord* next = prev_bitmap->get_next_marked_addr(addr, prev_top_at_mark_start());\n+inline bool HeapRegion::is_obj_dead(const oop obj, HeapWord* const pb) const {\n+  assert(is_in_reserved(obj), \"Object \" PTR_FORMAT \" must be in region\", p2i(obj));\n@@ -132,2 +160,13 @@\n-  assert(next > addr, \"must get the next live object\");\n-  return pointer_delta(next, addr);\n+  \/\/ Objects in closed archive regions are always live.\n+  if (is_closed_archive()) {\n+    return false;\n+  }\n+\n+  \/\/ From Remark until a region has been concurrently scrubbed, parts of the\n+  \/\/ region is not guaranteed to be parsable. Use the bitmap for liveness.\n+  if (obj_in_scrubbing_area(obj, pb)) {\n+    return !is_marked_in_bitmap(obj);\n+  }\n+\n+  \/\/ This object is in the parsable part of the heap, live unless scrubbed.\n+  return obj_is_scrubbed(obj);\n@@ -136,1 +175,1 @@\n-inline bool HeapRegion::is_obj_dead(const oop obj, const G1CMBitMap* const prev_bitmap) const {\n+inline bool HeapRegion::is_obj_dead_size_below_pb(const oop obj, HeapWord* const pb, size_t& block_size) const {\n@@ -138,3 +177,25 @@\n-  return !obj_allocated_since_prev_marking(obj) &&\n-         !prev_bitmap->is_marked(obj) &&\n-         !is_closed_archive();\n+  assert(!is_closed_archive(), \"never walk CA regions for cross-references\");\n+  assert(obj_in_scrubbing_area(obj, pb), \"must be\");\n+\n+  G1CMBitMap* bitmap = G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap();\n+  bool is_live = bitmap->is_marked(obj);\n+  if (is_live) {\n+    block_size = obj->size();\n+  } else {\n+    HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+    block_size = pointer_delta(next_live_in_unparsable(bitmap, addr, pb), addr);\n+  }\n+  return !is_live;\n+}\n+\n+inline HeapWord* HeapRegion::next_live_in_unparsable(G1CMBitMap* const bitmap, const HeapWord* p, HeapWord* const limit) const {\n+  return bitmap->get_next_marked_addr(p, limit);\n+}\n+\n+inline HeapWord* HeapRegion::next_live_in_unparsable(const HeapWord* p, HeapWord* const limit) const {\n+  G1CMBitMap* bitmap = G1CollectedHeap::heap()->concurrent_mark()->mark_bitmap();\n+  return next_live_in_unparsable(bitmap, p, limit);\n+}\n+\n+inline size_t HeapRegion::block_size(const HeapWord* p) const {\n+  return block_size(p, parsable_bottom());\n@@ -143,2 +204,2 @@\n-inline size_t HeapRegion::block_size(const HeapWord* addr) const {\n-  assert(addr < top(), \"precondition\");\n+inline size_t HeapRegion::block_size(const HeapWord* p, HeapWord* const pb) const {\n+  assert(p < top(), \"precondition\");\n@@ -146,2 +207,2 @@\n-  if (block_is_obj(addr)) {\n-    return cast_to_oop(addr)->size();\n+  if (!block_is_obj(p, pb)) {\n+    return pointer_delta(next_live_in_unparsable(p, pb), p);\n@@ -150,1 +211,1 @@\n-  return block_size_using_bitmap(addr, G1CollectedHeap::heap()->concurrent_mark()->prev_mark_bitmap());\n+  return cast_to_oop(p)->size();\n@@ -163,2 +224,1 @@\n-  \/\/ We treat all objects as being above PTAMS.\n-  zero_marked_bytes();\n+  \/\/ But all objects are live, we get this by setting TAMS to bottom.\n@@ -177,4 +237,2 @@\n-  _prev_top_at_mark_start = top(); \/\/ Keep existing top and usage.\n-  _prev_marked_bytes = used();\n-  _next_top_at_mark_start = bottom();\n-  _next_marked_bytes = 0;\n+  _marked_bytes = used();\n+  _top_at_mark_start = bottom();\n@@ -186,0 +244,3 @@\n+  \/\/ Everything above bottom() is parsable and live.\n+  _parsable_bottom = bottom();\n+\n@@ -243,0 +304,9 @@\n+inline HeapWord* HeapRegion::parsable_bottom() const {\n+  assert(!is_init_completed() || SafepointSynchronize::is_at_safepoint(), \"only during initialization or safepoint\");\n+  return _parsable_bottom;\n+}\n+\n+inline HeapWord* HeapRegion::parsable_bottom_acquire() const {\n+  return Atomic::load_acquire(&_parsable_bottom);\n+}\n+\n@@ -244,1 +314,1 @@\n-  _next_marked_bytes = 0;\n+  assert(!is_closed_archive() || top_at_mark_start() == bottom(), \"CA region's TAMS must always be at bottom\");\n@@ -246,1 +316,1 @@\n-    _next_top_at_mark_start = top();\n+    _top_at_mark_start = top();\n@@ -248,1 +318,0 @@\n-  assert(!is_closed_archive() || next_top_at_mark_start() == bottom(), \"CA region's nTAMS must always be at bottom\");\n@@ -252,5 +321,21 @@\n-inline void HeapRegion::note_end_of_marking() {\n-  _prev_top_at_mark_start = _next_top_at_mark_start;\n-  _next_top_at_mark_start = bottom();\n-  _prev_marked_bytes = _next_marked_bytes;\n-  _next_marked_bytes = 0;\n+inline void HeapRegion::note_end_of_marking(size_t marked_bytes) {\n+  assert_at_safepoint();\n+\n+  size_t new_garbage_bytes = byte_size(bottom(), _top_at_mark_start) - marked_bytes;\n+  assert(new_garbage_bytes >= _garbage_bytes, \"impossible\");\n+  _garbage_bytes = new_garbage_bytes;\n+\n+  _marked_bytes = marked_bytes;\n+\n+  \/\/ We know that humongous regions do not need scrubbing as they are contiguous\n+  \/\/ and there is only one object in them at a time. So do not bother moving\n+  \/\/ _parsable_bottom.\n+  assert(needs_scrubbing() || parsable_bottom() == bottom(), \"Regions that are not scrubbed should not move parsable_bottom\");\n+  if (needs_scrubbing()) {\n+    _parsable_bottom = _top_at_mark_start;\n+  }\n+  _top_at_mark_start = bottom();\n+}\n+\n+inline void HeapRegion::note_end_of_scrubbing() {\n+  Atomic::release_store(&_parsable_bottom, _bottom);\n@@ -265,2 +350,1 @@\n-                                                        Closure* cl,\n-                                                        G1CollectedHeap* g1h) {\n+                                                        Closure* cl) {\n@@ -285,1 +369,2 @@\n-  if (g1h->is_obj_dead(obj, sr)) {\n+  HeapWord* const pb = is_gc_active ? sr->parsable_bottom() : sr->parsable_bottom_acquire();\n+  if (sr->is_obj_dead(obj, pb)) {\n@@ -311,18 +396,3 @@\n-template <bool is_gc_active, class Closure>\n-HeapWord* HeapRegion::oops_on_memregion_seq_iterate_careful(MemRegion mr,\n-                                                            Closure* cl) {\n-  assert(MemRegion(bottom(), end()).contains(mr), \"Card region not in heap region\");\n-  G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-\n-  \/\/ Special handling for humongous regions.\n-  if (is_humongous()) {\n-    return do_oops_on_memregion_in_humongous<Closure, is_gc_active>(mr, cl, g1h);\n-  }\n-  assert(is_old() || is_archive(), \"Wrongly trying to iterate over region %u type %s\", _hrm_index, get_type_str());\n-\n-  \/\/ Because mr has been trimmed to what's been allocated in this\n-  \/\/ region, the parts of the heap that are examined here are always\n-  \/\/ parsable; there's no need to use klass_or_null to detect\n-  \/\/ in-progress allocation.\n-\n-  \/\/ Cache the boundaries of the memory region in some const locals\n+template <class Closure, bool is_gc_active>\n+inline HeapWord* HeapRegion::oops_on_memregion_iterate_in_unparsable(MemRegion mr, HeapWord* const pb, Closure* cl) {\n+  \/\/ Cache the boundaries of the area to scan in some locals.\n@@ -330,1 +400,2 @@\n-  HeapWord* const end = mr.end();\n+  \/\/ Only scan until parsable_bottom.\n+  HeapWord* const end = MIN2(mr.end(), pb);\n@@ -333,1 +404,20 @@\n-  HeapWord* cur = block_start(start);\n+  HeapWord* cur;\n+  if (is_gc_active) {\n+    \/\/ If we are at a safepoint the BOT is stable (i.e. not potentially being modified\n+    \/\/ concurrently) and valid, so we can simply use block_start() to find the object\n+    \/\/ start. The object at cur (and others in that area) might be dead though.\n+    cur = block_start(start, pb);\n+\n+    assert(cur <= start, \"cur: \" PTR_FORMAT \", start: \" PTR_FORMAT, p2i(cur), p2i(start));\n+  } else {\n+    \/\/ We are during concurrent refinement, at this point the BOT is not stable from\n+    \/\/ [bottom, pb) and we need to get the object spanning into mr by searching\n+    \/\/ the bitmap.\n+    cur = block_start_using_bitmap(start, pb);\n+    \/\/ We may not find any live object spanning into or covering this region until pb. In\n+    \/\/ that case, we are done.\n+    if (cur == pb) {\n+      return cur;\n+    }\n+    assert(cur < pb, \"cur: \" PTR_FORMAT \", end\/pb: \" PTR_FORMAT, p2i(cur), p2i(end));\n+  }\n@@ -335,1 +425,0 @@\n-  const G1CMBitMap* const bitmap = g1h->concurrent_mark()->prev_mark_bitmap();\n@@ -342,2 +431,2 @@\n-    size_t size;\n-    bool is_dead = is_obj_dead_with_size(obj, bitmap, &size);\n+    size_t block_size;\n+    bool is_dead = is_obj_dead_size_below_pb(obj, pb, block_size);\n@@ -346,1 +435,1 @@\n-    cur += size;\n+    cur += block_size;\n@@ -367,0 +456,81 @@\n+\/\/ Applies cl to all reference fields of live objects in mr in non-humongous regions.\n+\/\/\n+\/\/ For performance, the strategy here is to divide the work into two parts: areas\n+\/\/ below parsable_bottom (unparsable) and above parsable_bottom. The unparsable parts\n+\/\/ use the bitmap to locate live objects.\n+template <class Closure, bool is_gc_active>\n+inline HeapWord* HeapRegion::oops_on_memregion_iterate(MemRegion mr, Closure* cl) {\n+  \/\/ Cache the boundaries of the memory region in some const locals\n+  HeapWord* const start = mr.start();\n+  HeapWord* const end = mr.end();\n+\n+  \/\/ Snapshot the region's parsable_bottom.\n+  HeapWord* const pb = is_gc_active ? parsable_bottom() : parsable_bottom_acquire();\n+\n+  \/\/ Find the obj that extends onto mr.start()\n+  HeapWord* cur;\n+  if (obj_in_parsable_area(start, pb)) {\n+    cur = block_start(start, pb);\n+  } else {\n+    cur = oops_on_memregion_iterate_in_unparsable<Closure, is_gc_active>(mr, pb, cl);\n+    \/\/ We might have scanned beyond end at this point because of imprecise iteration.\n+    if (cur >= end) {\n+      return cur;\n+    }\n+    \/\/ Parsable_bottom is always the start of a valid parsable object, so we must either\n+    \/\/ have stopped at parsable_bottom, or already iterated beyond end. The\n+    \/\/ latter case is handled above.\n+    assert(cur == pb, \"must be cur \" PTR_FORMAT \" pb \" PTR_FORMAT, p2i(cur), p2i(pb));\n+  }\n+  assert(cur < top(), \"must be cur \" PTR_FORMAT \" top \" PTR_FORMAT, p2i(cur), p2i(top()));\n+\n+  \/\/ All objects >= pb are parsable. So we can just take object sizes directly.\n+  while (true) {\n+    oop obj = cast_to_oop(cur);\n+    assert(oopDesc::is_oop(obj, true), \"Not an oop at \" PTR_FORMAT, p2i(cur));\n+    assert(obj->klass_or_null() != NULL,\n+           \"Unparsable heap at \" PTR_FORMAT, p2i(cur));\n+\n+    bool is_precise = false;\n+\n+    cur += obj->size();\n+    \/\/ Process live object's references.\n+\n+    \/\/ Non-objArrays are usually marked imprecise at the object\n+    \/\/ start, in which case we need to iterate over them in full.\n+    \/\/ objArrays are precisely marked, but can still be iterated\n+    \/\/ over in full if completely covered.\n+    if (!obj->is_objArray() || (cast_from_oop<HeapWord*>(obj) >= start && cur <= end)) {\n+      obj->oop_iterate(cl);\n+    } else {\n+      obj->oop_iterate(cl, mr);\n+      is_precise = true;\n+    }\n+    if (cur >= end) {\n+      return is_precise ? end : cur;\n+    }\n+  }\n+}\n+\n+template <bool is_gc_active, class Closure>\n+HeapWord* HeapRegion::oops_on_memregion_seq_iterate_careful(MemRegion mr,\n+                                                            Closure* cl) {\n+  assert(MemRegion(bottom(), top()).contains(mr), \"Card region not in heap region\");\n+\n+  \/\/ Special handling for humongous regions.\n+  if (is_humongous()) {\n+    return do_oops_on_memregion_in_humongous<Closure, is_gc_active>(mr, cl);\n+  }\n+  assert(is_old() || is_archive(), \"Wrongly trying to iterate over region %u type %s\", _hrm_index, get_type_str());\n+\n+  \/\/ Because mr has been trimmed to what's been allocated in this\n+  \/\/ region, the objects in these parts of the heap have non-NULL\n+  \/\/ klass pointers. There's no need to use klass_or_null to detect\n+  \/\/ in-progress allocation.\n+  \/\/ We might be in the progress of scrubbing this region and in this\n+  \/\/ case there might be objects that have their classes unloaded and\n+  \/\/ therefore needs to be scanned using the bitmap.\n+\n+  return oops_on_memregion_iterate<Closure, is_gc_active>(mr, cl);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegion.inline.hpp","additions":257,"deletions":87,"binary":false,"changes":344,"status":"modified"},{"patch":"@@ -70,2 +70,1 @@\n-  _prev_bitmap_mapper(NULL),\n-  _next_bitmap_mapper(NULL),\n+  _bitmap_mapper(NULL),\n@@ -76,2 +75,1 @@\n-                                   G1RegionToSpaceMapper* prev_bitmap,\n-                                   G1RegionToSpaceMapper* next_bitmap,\n+                                   G1RegionToSpaceMapper* bitmap,\n@@ -85,2 +83,1 @@\n-  _prev_bitmap_mapper = prev_bitmap;\n-  _next_bitmap_mapper = next_bitmap;\n+  _bitmap_mapper = bitmap;\n@@ -192,2 +189,1 @@\n-  _prev_bitmap_mapper->commit_regions(index, num_regions, pretouch_workers);\n-  _next_bitmap_mapper->commit_regions(index, num_regions, pretouch_workers);\n+  _bitmap_mapper->commit_regions(index, num_regions, pretouch_workers);\n@@ -219,2 +215,1 @@\n-  _prev_bitmap_mapper->uncommit_regions(start, num_regions);\n-  _next_bitmap_mapper->uncommit_regions(start, num_regions);\n+  _bitmap_mapper->uncommit_regions(start, num_regions);\n@@ -273,2 +268,1 @@\n-  _prev_bitmap_mapper->signal_mapping_changed(start, num_regions);\n-  _next_bitmap_mapper->signal_mapping_changed(start, num_regions);\n+  _bitmap_mapper->signal_mapping_changed(start, num_regions);\n@@ -285,2 +279,1 @@\n-    _prev_bitmap_mapper->committed_size() +\n-    _next_bitmap_mapper->committed_size() +\n+    _bitmap_mapper->committed_size() +\n@@ -292,2 +285,1 @@\n-    _prev_bitmap_mapper->reserved_size() +\n-    _next_bitmap_mapper->reserved_size() +\n+    _bitmap_mapper->reserved_size() +\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionManager.cpp","additions":8,"deletions":16,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -126,2 +126,1 @@\n-  G1RegionToSpaceMapper* _prev_bitmap_mapper;\n-  G1RegionToSpaceMapper* _next_bitmap_mapper;\n+  G1RegionToSpaceMapper* _bitmap_mapper;\n@@ -165,2 +164,1 @@\n-                  G1RegionToSpaceMapper* prev_bitmap,\n-                  G1RegionToSpaceMapper* next_bitmap,\n+                  G1RegionToSpaceMapper* bitmap,\n","filename":"src\/hotspot\/share\/gc\/g1\/heapRegionManager.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,0 +32,6 @@\n+\/\/ Closure for iteration over set bits of a bitmap.\n+class MarkBitMapClosure {\n+public:\n+  virtual bool do_addr(HeapWord* const addr) = 0;\n+};\n+\n@@ -56,2 +62,0 @@\n-public:\n-  static size_t compute_size(size_t heap_size);\n@@ -60,0 +64,3 @@\n+\n+public:\n+  static size_t compute_size(size_t heap_size);\n@@ -80,0 +87,1 @@\n+  inline bool iterate(MarkBitMapClosure* cl, MemRegion mr);\n@@ -84,1 +92,6 @@\n-                                        const HeapWord* limit) const;\n+                                        HeapWord* limit) const;\n+\n+  \/\/ Return the address corresponding to the previous marked bit at or before\n+  \/\/ \"addr\", and after or at \"limit\". If there is no such bit, returns nullptr.\n+  inline HeapWord* get_prev_marked_addr(HeapWord* limit,\n+                                        const HeapWord* addr) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/markBitMap.hpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/oop.inline.hpp\"\n@@ -35,2 +36,22 @@\n-inline HeapWord* MarkBitMap::get_next_marked_addr(const HeapWord* addr,\n-                                                const HeapWord* limit) const {\n+inline bool MarkBitMap::iterate(MarkBitMapClosure* cl, MemRegion mr) {\n+  assert(!mr.is_empty(), \"Does not support empty memregion to iterate over\");\n+  assert(_covered.contains(mr),\n+         \"Given MemRegion from \" PTR_FORMAT \" to \" PTR_FORMAT \" not contained in heap area\",\n+         p2i(mr.start()), p2i(mr.end()));\n+\n+  BitMap::idx_t const end_offset = addr_to_offset(mr.end());\n+  BitMap::idx_t offset = _bm.get_next_one_offset(addr_to_offset(mr.start()), end_offset);\n+\n+  while (offset < end_offset) {\n+    HeapWord* const addr = offset_to_addr(offset);\n+    if (!cl->do_addr(addr)) {\n+      return false;\n+    }\n+    size_t const obj_size = cast_to_oop(addr)->size();\n+    offset = _bm.get_next_one_offset(offset + (obj_size >> _shifter), end_offset);\n+  }\n+  return true;\n+}\n+\n+inline HeapWord* MarkBitMap::get_next_marked_addr(const HeapWord* const addr,\n+                                                  HeapWord* const limit) const {\n@@ -45,0 +66,10 @@\n+inline HeapWord* MarkBitMap::get_prev_marked_addr(HeapWord* const limit,\n+                                                  const HeapWord* const addr) const {\n+  assert(limit != NULL, \"limit must not be NULL\");\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  size_t const addr_offset = addr_to_offset(align_up(addr, HeapWordSize << _shifter));\n+  size_t const limit_offset = addr_to_offset(limit);\n+  size_t const nextOffset = _bm.get_prev_one_offset(limit_offset, addr_offset);\n+  return nextOffset != BitMap::NotFound ? offset_to_addr(nextOffset) : nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/markBitMap.inline.hpp","additions":33,"deletions":2,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -34,4 +34,3 @@\n-\n-  \/\/ Use \"prev\" mark bitmap information using pTAMS.\n-  G1UsePrevMarking = Default,\n-  \/\/ Use \"next\" mark bitmap information from full gc marking. This does not\n+  \/\/ Use mark bitmap information (from concurrent marking) using TAMS.\n+  G1UseConcMarking = Default,\n+  \/\/ Use mark bitmap information from full gc marking. This does not\n@@ -39,1 +38,1 @@\n-  G1UseFullMarking = G1UsePrevMarking + 1\n+  G1UseFullMarking = G1UseConcMarking + 1\n","filename":"src\/hotspot\/share\/gc\/shared\/verifyOption.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,6 +78,6 @@\n-  G1CMBitMap* bitmap = heap->concurrent_mark()->next_mark_bitmap();\n-  bitmap->mark(region->bottom());\n-  bitmap->mark(region->bottom() + MARK_OFFSET_1);\n-  bitmap->mark(region->bottom() + MARK_OFFSET_2);\n-  bitmap->mark(region->bottom() + MARK_OFFSET_3);\n-  bitmap->mark(region->end());\n+  G1CMBitMap* bitmap = heap->concurrent_mark()->mark_bitmap();\n+  bitmap->par_mark(region->bottom());\n+  bitmap->par_mark(region->bottom() + MARK_OFFSET_1);\n+  bitmap->par_mark(region->bottom() + MARK_OFFSET_2);\n+  bitmap->par_mark(region->bottom() + MARK_OFFSET_3);\n+  bitmap->par_mark(region->end());\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_heapRegion.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,1 @@\n-        \/\/ [0.048s][info ][pagesize     ] Next Bitmap: ... page_size=4K ...\n+        \/\/ [0.048s][info ][pagesize     ] Mark Bitmap: ... page_size=4K ...\n@@ -104,3 +104,2 @@\n-    static void checkBitmaps(OutputAnalyzer output, long expectedPageSize) throws Exception {\n-        checkSize(output, expectedPageSize, \"Prev Bitmap: .*page_size=([^ ]+)\");\n-        checkSize(output, expectedPageSize, \"Next Bitmap: .*page_size=([^ ]+)\");\n+    static void checkBitmap(OutputAnalyzer output, long expectedPageSize) throws Exception {\n+        checkSize(output, expectedPageSize, \"Mark Bitmap: .*page_size=([^ ]+)\");\n@@ -127,1 +126,1 @@\n-            checkBitmaps(output, (bitmapShouldUseLargePages ? largePageSize : smallPageSize));\n+            checkBitmap(output, (bitmapShouldUseLargePages ? largePageSize : smallPageSize));\n@@ -130,1 +129,1 @@\n-            checkBitmaps(output, smallPageSize);\n+            checkBitmap(output, smallPageSize);\n@@ -146,1 +145,1 @@\n-        checkBitmaps(output, smallPageSize);\n+        checkBitmap(output, smallPageSize);\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestLargePageUseForAuxMemory.java","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"}]}