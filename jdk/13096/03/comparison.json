{"files":[{"patch":"@@ -36,1 +36,1 @@\n-define_pd_global(bool, TrapBasedNullChecks,  false);\n+define_pd_global(bool, TrapBasedNullChecks,     false);\n@@ -39,0 +39,2 @@\n+define_pd_global(bool, DelayCompilerStubsGeneration, COMPILER2_OR_JVMCI);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -573,1 +573,1 @@\n-      (Interpreter::code() != NULL || StubRoutines::code1() != NULL)) {\n+      (Interpreter::code() != NULL || StubRoutines::final_stubs_code() != NULL)) {\n@@ -581,1 +581,1 @@\n-    if (Interpreter::code() != NULL)\n+    if (Interpreter::code() != NULL) {\n@@ -585,1 +585,2 @@\n-    if (StubRoutines::code1() != NULL)\n+    }\n+    if (StubRoutines::initial_stubs_code() != NULL) {\n@@ -587,1 +588,2 @@\n-                  StubRoutines::code1()->code_begin(), StubRoutines::code1()->code_end(),\n+                  StubRoutines::initial_stubs_code()->code_begin(),\n+                  StubRoutines::initial_stubs_code()->code_end(),\n@@ -589,1 +591,2 @@\n-    if (StubRoutines::code2() != NULL)\n+    }\n+    if (StubRoutines::final_stubs_code() != NULL) {\n@@ -591,1 +594,2 @@\n-                  StubRoutines::code2()->code_begin(), StubRoutines::code2()->code_end(),\n+                  StubRoutines::final_stubs_code()->code_begin(),\n+                  StubRoutines::final_stubs_code()->code_end(),\n@@ -593,0 +597,1 @@\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -8000,1 +8000,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -8026,0 +8026,6 @@\n+\n+    \/\/ Initialize table for copy memory (arraycopy) check.\n+    if (UnsafeCopyMemory::_table == nullptr) {\n+      UnsafeCopyMemory::create_table(8);\n+    }\n+\n@@ -8050,1 +8056,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -8060,1 +8066,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -8083,4 +8089,0 @@\n-    if (UseSVE == 0) {\n-      StubRoutines::aarch64::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    }\n-\n@@ -8090,2 +8092,22 @@\n-    \/\/ countPositives stub for large arrays.\n-    StubRoutines::aarch64::_count_positives = generate_count_positives(StubRoutines::aarch64::_count_positives_long);\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      StubRoutines::aarch64::_method_entry_barrier = generate_method_entry_barrier();\n+    }\n+\n+    StubRoutines::aarch64::_spin_wait = generate_spin_wait();\n+\n+#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n+\n+    generate_atomic_entry_points();\n+\n+#endif \/\/ LINUX\n+\n+    StubRoutines::aarch64::set_completed(); \/\/ Inidicate that arraycopy and zero_blocks stubs are generated\n+  }\n+\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n+\n+    if (UseSVE == 0) {\n+      StubRoutines::aarch64::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n+    }\n@@ -8098,0 +8120,6 @@\n+    \/\/ byte_array_inflate stub for large arrays.\n+    StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();\n+\n+    \/\/ countPositives stub for large arrays.\n+    StubRoutines::aarch64::_count_positives = generate_count_positives(StubRoutines::aarch64::_count_positives_long);\n+\n@@ -8102,7 +8130,0 @@\n-    \/\/ byte_array_inflate stub for large arrays.\n-    StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();\n-\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::aarch64::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n@@ -8195,10 +8216,1 @@\n-\n-    StubRoutines::aarch64::_spin_wait = generate_spin_wait();\n-\n-#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n-\n-    generate_atomic_entry_points();\n-\n-#endif \/\/ LINUX\n-\n-    StubRoutines::aarch64::set_completed();\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -8208,8 +8220,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -8219,6 +8241,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 8\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":58,"deletions":40,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,2 +38,5 @@\n-  code_size1 = 19000,          \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 45000           \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 10000,\n+  _continuation_stubs_code_size =  2000,\n+  _compiler_stubs_code_size     = 30000,\n+  _final_stubs_code_size        = 20000\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+define_pd_global(bool,  DelayCompilerStubsGeneration, false); \/\/ No need - only few compiler's stubs\n+\n","filename":"src\/hotspot\/cpu\/arm\/globals_arm.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3080,1 +3080,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -3097,0 +3097,4 @@\n+    if (UnsafeCopyMemory::_table == nullptr) {\n+      UnsafeCopyMemory::create_table(32);\n+    }\n+\n@@ -3109,1 +3113,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -3119,1 +3123,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -3122,5 +3126,0 @@\n-#ifdef COMPILER2\n-    \/\/ Generate partial_subtype_check first here since its code depends on\n-    \/\/ UseZeroBaseCompressedOops which is defined after heap initialization.\n-    StubRoutines::Arm::_partial_subtype_check                = generate_partial_subtype_check();\n-#endif\n@@ -3147,0 +3146,8 @@\n+  }\n+\n+  void generate_compiler_stubs() {\n+#ifdef COMPILER2\n+    \/\/ Generate partial_subtype_check first here since its code depends on\n+    \/\/ UseZeroBaseCompressedOops which is defined after heap initialization.\n+    StubRoutines::Arm::_partial_subtype_check                = generate_partial_subtype_check();\n+\n@@ -3157,0 +3164,1 @@\n+#endif \/\/ COMPILER2\n@@ -3159,1 +3167,0 @@\n-\n@@ -3161,8 +3168,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1();\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -3172,6 +3189,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 32\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == nullptr) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/arm\/stubGenerator_arm.cpp","additions":36,"deletions":23,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,2 +33,5 @@\n-  code_size1 =  9000,           \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 22000            \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      =  9000,\n+  _continuation_stubs_code_size =  2000,\n+  _compiler_stubs_code_size     = 22000,\n+  _final_stubs_code_size        = 22000\n","filename":"src\/hotspot\/cpu\/arm\/stubRoutines_arm.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,2 @@\n+define_pd_global(bool, DelayCompilerStubsGeneration, COMPILER2_OR_JVMCI);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/globals_ppc.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4653,1 +4653,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -4666,0 +4666,4 @@\n+    if (UnsafeCopyMemory::_table == NULL) {\n+      UnsafeCopyMemory::create_table(8);\n+    }\n+\n@@ -4687,1 +4691,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -4697,1 +4701,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -4718,0 +4722,4 @@\n+  }\n+\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n@@ -4766,0 +4774,1 @@\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -4769,8 +4778,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -4780,6 +4799,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 8\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":32,"deletions":17,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,2 +36,5 @@\n-  code_size1 = 20000,          \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 24000           \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 20000,\n+  _continuation_stubs_code_size =  2000,\n+  _compiler_stubs_code_size     = 24000,\n+  _final_stubs_code_size        = 24000\n","filename":"src\/hotspot\/cpu\/ppc\/stubRoutines_ppc.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,2 @@\n+define_pd_global(bool, DelayCompilerStubsGeneration, COMPILER2_OR_JVMCI);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4006,1 +4006,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -4017,0 +4017,4 @@\n+    if (UnsafeCopyMemory::_table == NULL) {\n+      UnsafeCopyMemory::create_table(8);\n+    }\n+\n@@ -4034,1 +4038,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -4044,1 +4048,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -4070,0 +4074,10 @@\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      StubRoutines::riscv::_method_entry_barrier = generate_method_entry_barrier();\n+    }\n+\n+    StubRoutines::riscv::set_completed();\n+  }\n+\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n@@ -4099,1 +4113,1 @@\n-#endif\n+#endif \/\/ COMPILER2\n@@ -4104,7 +4118,1 @@\n-\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::riscv::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n-\n-    StubRoutines::riscv::set_completed();\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -4114,8 +4122,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -4125,7 +4143,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 8\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":39,"deletions":26,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,2 +39,5 @@\n-  code_size1 = 19000,          \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 28000           \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 19000,\n+  _continuation_stubs_code_size =  2000,\n+  _compiler_stubs_code_size     = 28000,\n+  _final_stubs_code_size        = 28000\n","filename":"src\/hotspot\/cpu\/riscv\/stubRoutines_riscv.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,2 @@\n+define_pd_global(bool,  DelayCompilerStubsGeneration, COMPILER2_OR_JVMCI);\n+\n","filename":"src\/hotspot\/cpu\/s390\/globals_s390.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3090,1 +3090,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -3128,1 +3128,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -3140,1 +3140,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -3155,0 +3155,1 @@\n+  }\n@@ -3156,0 +3157,2 @@\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n@@ -3218,0 +3221,1 @@\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -3221,9 +3225,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    _stub_count = (phase == 0) ? 0x100 : 0x200;\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -3266,2 +3279,2 @@\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/s390\/stubGenerator_s390.cpp","additions":27,"deletions":14,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,3 +35,5 @@\n-  \/\/ TODO: May be able to shrink this a lot\n-  code_size1 = 20000,      \/\/ Simply increase if too small (assembler will crash if too small).\n-  code_size2 = 20000       \/\/ Simply increase if too small (assembler will crash if too small).\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 20000,\n+  _continuation_stubs_code_size =  2000,\n+  _compiler_stubs_code_size     = 20000,\n+  _final_stubs_code_size        = 20000\n","filename":"src\/hotspot\/cpu\/s390\/stubRoutines_s390.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+define_pd_global(bool, DelayCompilerStubsGeneration, COMPILER2_OR_JVMCI);\n+\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -748,1 +748,1 @@\n-      (Interpreter::code() != nullptr || StubRoutines::code1() != nullptr)) {\n+      (Interpreter::code() != nullptr || StubRoutines::final_stubs_code() != nullptr)) {\n@@ -755,1 +755,1 @@\n-    if (Interpreter::code() != nullptr)\n+    if (Interpreter::code() != nullptr) {\n@@ -759,1 +759,2 @@\n-    if (StubRoutines::code1() != nullptr)\n+    }\n+    if (StubRoutines::initial_stubs_code() != nullptr) {\n@@ -761,1 +762,2 @@\n-                  StubRoutines::code1()->code_begin(), StubRoutines::code1()->code_end(),\n+                  StubRoutines::initial_stubs_code()->code_begin(),\n+                  StubRoutines::initial_stubs_code()->code_end(),\n@@ -763,1 +765,2 @@\n-    if (StubRoutines::code2() != nullptr)\n+    }\n+    if (StubRoutines::final_stubs_code() != nullptr) {\n@@ -765,1 +768,2 @@\n-                  StubRoutines::code2()->code_begin(), StubRoutines::code2()->code_end(),\n+                  StubRoutines::final_stubs_code()->code_begin(),\n+                  StubRoutines::final_stubs_code()->code_end(),\n@@ -767,0 +771,1 @@\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -799,1 +799,1 @@\n-      (Interpreter::code() != nullptr || StubRoutines::code1() != nullptr)) {\n+      (Interpreter::code() != nullptr || StubRoutines::final_stubs_code() != nullptr)) {\n@@ -808,1 +808,1 @@\n-    if (Interpreter::code() != nullptr)\n+    if (Interpreter::code() != nullptr) {\n@@ -810,1 +810,2 @@\n-                  Interpreter::code()->code_start(), Interpreter::code()->code_end(),\n+                  Interpreter::code()->code_start(),\n+                  Interpreter::code()->code_end(),\n@@ -812,1 +813,2 @@\n-    if (StubRoutines::code1() != nullptr)\n+    }\n+    if (StubRoutines::initial_stubs_code() != nullptr) {\n@@ -814,1 +816,2 @@\n-                  StubRoutines::code1()->code_begin(), StubRoutines::code1()->code_end(),\n+                  StubRoutines::initial_stubs_code()->code_begin(),\n+                  StubRoutines::initial_stubs_code()->code_end(),\n@@ -816,1 +819,2 @@\n-    if (StubRoutines::code2() != nullptr)\n+    }\n+    if (StubRoutines::final_stubs_code() != nullptr) {\n@@ -818,1 +822,2 @@\n-                  StubRoutines::code2()->code_begin(), StubRoutines::code2()->code_end(),\n+                  StubRoutines::final_stubs_code()->code_begin(),\n+                  StubRoutines::final_stubs_code()->code_end(),\n@@ -820,0 +825,1 @@\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -4065,1 +4065,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -4082,0 +4082,5 @@\n+    \/\/ Initialize table for copy memory (arraycopy) check.\n+    if (UnsafeCopyMemory::_table == nullptr) {\n+      UnsafeCopyMemory::create_table(16);\n+    }\n+\n@@ -4140,1 +4145,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -4150,1 +4155,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -4159,2 +4164,16 @@\n-    \/\/------------------------------------------------------------------------------------------------------------------------\n-    \/\/ entry points that are platform specific\n+    \/\/ support for verify_oop (must happen after universe_init)\n+    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n+\n+    \/\/ arraycopy stubs used by compilers\n+    generate_arraycopy_stubs();\n+\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != nullptr) {\n+      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+    }\n+  }\n+\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n+\n+    \/\/ entry points that are C2\/JVMCI specific\n@@ -4193,6 +4212,0 @@\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n@@ -4232,5 +4245,1 @@\n-\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != nullptr) {\n-      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -4241,8 +4250,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -4252,6 +4271,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 16\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == nullptr) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":45,"deletions":30,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -3865,1 +3865,1 @@\n-void StubGenerator::generate_initial() {\n+void StubGenerator::generate_initial_stubs() {\n@@ -3871,0 +3871,5 @@\n+  \/\/ Initialize table for unsafe copy memeory check.\n+  if (UnsafeCopyMemory::_table == nullptr) {\n+    UnsafeCopyMemory::create_table(16);\n+  }\n+\n@@ -3920,4 +3925,0 @@\n-  if (UsePoly1305Intrinsics) {\n-    StubRoutines::_poly1305_processBlocks = generate_poly1305_processBlocks();\n-  }\n-\n@@ -3931,4 +3932,0 @@\n-  if (UseAdler32Intrinsics) {\n-     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n-  }\n-\n@@ -3948,1 +3945,1 @@\n-void StubGenerator::generate_phase1() {\n+void StubGenerator::generate_continuation_stubs() {\n@@ -3958,2 +3955,2 @@\n-void StubGenerator::generate_all() {\n-  \/\/ Generates all stubs and initializes the entry points\n+void StubGenerator::generate_final_stubs() {\n+  \/\/ Generates the rest of stubs and initializes the entry points\n@@ -3982,1 +3979,27 @@\n-  \/\/ entry points that are platform specific\n+  \/\/ support for verify_oop (must happen after universe_init)\n+  if (VerifyOops) {\n+    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  }\n+\n+  \/\/ data cache line writeback\n+  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n+  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n+\n+  \/\/ arraycopy stubs used by compilers\n+  generate_arraycopy_stubs();\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n+    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+  }\n+\n+  if (UseVectorizedMismatchIntrinsic) {\n+    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+  }\n+}\n+\n+void StubGenerator::generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n+\n+  \/\/ Entry points that are C2 compiler specific.\n+\n@@ -4014,12 +4037,0 @@\n-  \/\/ support for verify_oop (must happen after universe_init)\n-  if (VerifyOops) {\n-    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n-  }\n-\n-  \/\/ data cache line writeback\n-  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-\n-  \/\/ arraycopy stubs used by compilers\n-  generate_arraycopy_stubs();\n-\n@@ -4032,0 +4043,8 @@\n+  if (UseAdler32Intrinsics) {\n+     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+  }\n+\n+  if (UsePoly1305Intrinsics) {\n+    StubRoutines::_poly1305_processBlocks = generate_poly1305_processBlocks();\n+  }\n+\n@@ -4036,0 +4055,1 @@\n+\n@@ -4042,0 +4062,1 @@\n+\n@@ -4055,0 +4076,1 @@\n+\n@@ -4087,4 +4109,0 @@\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  if (bs_nm != nullptr) {\n-    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-  }\n@@ -4178,0 +4196,2 @@\n+#endif \/\/ COMPILER2_OR_JVMCI\n+}\n@@ -4179,3 +4199,19 @@\n-  if (UseVectorizedMismatchIntrinsic) {\n-    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n-  }\n+StubGenerator::StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    DEBUG_ONLY( _regs_in_thread = false; )\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -4184,5 +4220,2 @@\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == nullptr) {\n-    UnsafeCopyMemory::create_table(16);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":70,"deletions":37,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -553,3 +553,4 @@\n-  void generate_initial();\n-  void generate_phase1();\n-  void generate_all();\n+  void generate_initial_stubs();\n+  void generate_continuation_stubs();\n+  void generate_compiler_stubs();\n+  void generate_final_stubs();\n@@ -558,10 +559,1 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    DEBUG_ONLY( _regs_in_thread = false; )\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n-  }\n+  StubGenerator(CodeBuffer* code, StubsKind kind);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":5,"deletions":13,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -35,2 +35,7 @@\n-  code_size1 = 20000 LP64_ONLY(+10000),                    \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 35300 LP64_ONLY(+45000) WINDOWS_ONLY(+2048) \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 20000 WINDOWS_ONLY(+1000),\n+  _continuation_stubs_code_size =  1000 LP64_ONLY(+1000),\n+  \/\/ AVX512 intrinsics add more code in 64-bit VM,\n+  \/\/ Windows have more code to save\/restore registers\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+30000) WINDOWS_ONLY(+2000),\n+  _final_stubs_code_size        = 10000 LP64_ONLY(+20000) WINDOWS_ONLY(+2000)\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,2 @@\n+define_pd_global(bool,  DelayCompilerStubsGeneration, false); \/\/ Don't have compiler's stubs\n+\n","filename":"src\/hotspot\/cpu\/zero\/globals_zero.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -179,1 +179,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -200,1 +200,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -225,5 +225,5 @@\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n-      generate_initial();\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    if (kind == Initial_stubs) {\n+      generate_initial_stubs();\n+    } else if (kind == Final_stubs) {\n+      generate_final_stubs();\n@@ -234,2 +234,2 @@\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/zero\/stubGenerator_zero.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,3 +43,7 @@\n-    code_size1 = 0,      \/\/ The assembler will fail with a guarantee\n-    code_size2 = 0       \/\/ if these are too small.  Simply increase\n-  };                     \/\/ them if that happens.\n+    \/\/ The assembler will fail with a guarantee if these are too small.\n+    \/\/ Simply increase them if that happens.\n+    _initial_stubs_code_size      = 0,\n+    _continuation_stubs_code_size = 0,\n+    _compiler_stubs_code_size     = 0,\n+    _final_stubs_code_size        = 0\n+  };\n","filename":"src\/hotspot\/cpu\/zero\/stubRoutines_zero.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -557,1 +557,1 @@\n-  if (StubRoutines::code1() != nullptr) {\n+  if (StubRoutines::initial_stubs_code() != nullptr) {\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/os_windows_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+void compiler_stubs_init(bool in_compiler_thread);\n+\n@@ -63,1 +65,1 @@\n-\n+  compiler_stubs_init(true \/* in_compiler_thread *\/); \/\/ generate compiler's intrinsics stubs\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -182,0 +182,1 @@\n+  LOG_TAG(stubs) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+\n+void compiler_stubs_init(bool in_compiler_thread);\n+\n@@ -77,0 +80,2 @@\n+  compiler_stubs_init(true \/* in_compiler_thread *\/); \/\/ generate compiler's intrinsics stubs\n+\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -7284,1 +7284,1 @@\n-  assert(stubAddr != nullptr, \"Stub is generated\");\n+  assert(stubAddr != nullptr, \"Stub %s is not generated\", stubName);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -360,1 +360,1 @@\n-  product(bool, UseVectorizedHashCodeIntrinsic, false, DIAGNOSTIC,           \\\n+  product(bool, UseVectorizedHashCodeIntrinsic, false, DIAGNOSTIC,          \\\n@@ -369,0 +369,3 @@\n+  product_pd(bool, DelayCompilerStubsGeneration, DIAGNOSTIC,                \\\n+          \"Use Compiler thread for compiler's stubs generation\")            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,3 +69,3 @@\n-void stubRoutines_init1();\n-void stubRoutines_initContinuationStubs();\n-jint universe_init();          \/\/ depends on codeCache_init and stubRoutines_init\n+void initial_stubs_init();\n+\n+jint universe_init();           \/\/ depends on codeCache_init and initial_stubs_init\n@@ -74,2 +74,4 @@\n-void interpreter_init_stub();  \/\/ before any methods loaded\n-void interpreter_init_code();  \/\/ after methods loaded, but before they are linked\n+void continuations_init();      \/\/ depends on flags (UseCompressedOops) and barrier sets\n+void continuation_stubs_init(); \/\/ depend on continuations_init\n+void interpreter_init_stub();   \/\/ before any methods loaded\n+void interpreter_init_code();   \/\/ after methods loaded, but before they are linked\n@@ -78,1 +80,1 @@\n-void universe2_init();  \/\/ dependent on codeCache_init and stubRoutines_init, loads primordial classes\n+void universe2_init();  \/\/ dependent on codeCache_init and initial_stubs_init, loads primordial classes\n@@ -92,4 +94,3 @@\n-void javaClasses_init();  \/\/ must happen after vtable initialization\n-void stubRoutines_init2(); \/\/ note: StubRoutines need 2-phase init\n-\n-void continuations_init(); \/\/ depends on flags (UseCompressedOops) and barrier sets\n+void javaClasses_init();    \/\/ must happen after vtable initialization\n+void compiler_stubs_init(bool in_compiler_thread); \/\/ compiler's StubRoutines stubs\n+void final_stubs_init();    \/\/ final StubRoutines stubs\n@@ -122,1 +123,1 @@\n-  stubRoutines_init1();\n+  initial_stubs_init();\n@@ -124,1 +125,1 @@\n-                                  \/\/ stubRoutines_init1 and metaspace_init.\n+                                  \/\/ initial_stubs_init and metaspace_init.\n@@ -137,4 +138,4 @@\n-  gc_barrier_stubs_init();  \/\/ depends on universe_init, must be before interpreter_init\n-  continuations_init(); \/\/ must precede continuation stub generation\n-  stubRoutines_initContinuationStubs(); \/\/ depends on continuations_init\n-  interpreter_init_stub();  \/\/ before methods get loaded\n+  gc_barrier_stubs_init();   \/\/ depends on universe_init, must be before interpreter_init\n+  continuations_init();      \/\/ must precede continuation stub generation\n+  continuation_stubs_init(); \/\/ depends on continuations_init\n+  interpreter_init_stub();   \/\/ before methods get loaded\n@@ -143,1 +144,1 @@\n-  VMRegImpl::set_regName(); \/\/ need this before generate_stubs (for printing oop maps).\n+  VMRegImpl::set_regName();  \/\/ need this before generate_stubs (for printing oop maps).\n@@ -145,3 +146,3 @@\n-  universe2_init();  \/\/ dependent on codeCache_init and stubRoutines_init1\n-  javaClasses_init();\/\/ must happen after vtable initialization, before referenceProcessor_init\n-  interpreter_init_code();  \/\/ after javaClasses_init and before any method gets linked\n+  universe2_init();          \/\/ dependent on codeCache_init and initial_stubs_init\n+  javaClasses_init();        \/\/ must happen after vtable initialization, before referenceProcessor_init\n+  interpreter_init_code();   \/\/ after javaClasses_init and before any method gets linked\n@@ -172,1 +173,2 @@\n-  stubRoutines_init2(); \/\/ note: StubRoutines need 2-phase init\n+  compiler_stubs_init(false \/* in_compiler_thread *\/); \/\/ compiler's intrinsics stubs\n+  final_stubs_init();    \/\/ final StubRoutines stubs\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":24,"deletions":22,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2886,5 +2886,5 @@\n-  \/\/ StubRoutines::code2() is initialized after this function can be called. As a result,\n-  \/\/ VerifyAdapterCalls and VerifyAdapterSharing can fail if we re-use code that generated\n-  \/\/ prior to StubRoutines::code2() being set. Checks refer to checks generated in an I2C\n-  \/\/ stub that ensure that an I2C stub is called from an interpreter frame.\n-  bool contains_all_checks = StubRoutines::code2() != nullptr;\n+  \/\/ StubRoutines::_final_stubs_code is initialized after this function can be called. As a result,\n+  \/\/ VerifyAdapterCalls and VerifyAdapterSharing can fail if we re-use code that generated prior\n+  \/\/ to all StubRoutines::_final_stubs_code being set. Checks refer to runtime range checks generated\n+  \/\/ in an I2C stub that ensure that an I2C stub is called from an interpreter frame or stubs.\n+  bool contains_all_checks = StubRoutines::final_stubs_code() != nullptr;\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,0 +54,5 @@\n+void StubCodeDesc::unfreeze() {\n+  assert(_frozen, \"repeated unfreeze operation\");\n+  _frozen = false;\n+}\n+\n@@ -86,0 +91,7 @@\n+  LogTarget(Debug, stubs) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    cdesc->print_on(&ls);\n+    ls.cr();\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubCodeGenerator.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  static void unfreeze();\n@@ -116,0 +117,15 @@\n+\n+  enum StubsKind {\n+    Initial_stubs,       \/\/ Stubs used by Runtime, Interpreter and compiled code.\n+                         \/\/ Have to be generated very early during VM startup.\n+\n+    Continuation_stubs,  \/\/ Stubs used by virtual threads.\n+                         \/\/ Generated after GC barriers initialization but before\n+                         \/\/ Interpreter initialization.\n+\n+    Compiler_stubs,      \/\/ Intrinsics and other stubs used only by compiled code.\n+                         \/\/ Can be generated by compiler (C2\/JVMCI) thread based on\n+                         \/\/ DelayCompilerStubsGeneration flag.\n+\n+    Final_stubs          \/\/ The rest of stubs. Generated at the end of VM init.\n+  };\n","filename":"src\/hotspot\/share\/runtime\/stubCodeGenerator.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -53,3 +53,4 @@\n-BufferBlob* StubRoutines::_code1                                = nullptr;\n-BufferBlob* StubRoutines::_code2                                = nullptr;\n-BufferBlob* StubRoutines::_code3                                = nullptr;\n+BufferBlob* StubRoutines::_initial_stubs_code                   = nullptr;\n+BufferBlob* StubRoutines::_final_stubs_code                     = nullptr;\n+BufferBlob* StubRoutines::_compiler_stubs_code                  = nullptr;\n+BufferBlob* StubRoutines::_continuation_stubs_code              = nullptr;\n@@ -190,1 +191,1 @@\n-extern void StubGenerator_generate(CodeBuffer* code, int phase); \/\/ only interface to generators\n+extern void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind); \/\/ only interface to generators\n@@ -217,16 +218,57 @@\n-void StubRoutines::initialize1() {\n-  if (_code1 == nullptr) {\n-    ResourceMark rm;\n-    TraceTime timer(\"StubRoutines generation 1\", TRACETIME_LOG(Info, startuptime));\n-    \/\/ Add extra space for large CodeEntryAlignment\n-    int max_aligned_stubs = 10;\n-    int size = code_size1 + CodeEntryAlignment * max_aligned_stubs;\n-    _code1 = BufferBlob::create(\"StubRoutines (1)\", size);\n-    if (_code1 == nullptr) {\n-      vm_exit_out_of_memory(code_size1, OOM_MALLOC_ERROR, \"CodeCache: no room for StubRoutines (1)\");\n-    }\n-    CodeBuffer buffer(_code1);\n-    StubGenerator_generate(&buffer, 0);\n-    \/\/ When new stubs added we need to make sure there is some space left\n-    \/\/ to catch situation when we should increase size again.\n-    assert(code_size1 == 0 || buffer.insts_remaining() > 200, \"increase code_size1\");\n+\n+static BufferBlob* initialize_stubs(StubCodeGenerator::StubsKind kind,\n+                                    int code_size, int max_aligned_stubs,\n+                                    const char* timer_msg,\n+                                    const char* buffer_name,\n+                                    const char* assert_msg) {\n+  ResourceMark rm;\n+  TraceTime timer(timer_msg, TRACETIME_LOG(Info, startuptime));\n+  \/\/ Add extra space for large CodeEntryAlignment\n+  int size = code_size + CodeEntryAlignment * max_aligned_stubs;\n+  BufferBlob* stubs_code = BufferBlob::create(buffer_name, size);\n+  if (stubs_code == nullptr) {\n+    vm_exit_out_of_memory(code_size, OOM_MALLOC_ERROR, \"CodeCache: no room for %s\", buffer_name);\n+  }\n+  CodeBuffer buffer(stubs_code);\n+  StubGenerator_generate(&buffer, kind);\n+  \/\/ When new stubs added we need to make sure there is some space left\n+  \/\/ to catch situation when we should increase size again.\n+  assert(code_size == 0 || buffer.insts_remaining() > 200, \"increase %s\", assert_msg);\n+\n+  LogTarget(Info, stubs) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.print_cr(\"%s\\t [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \"] used: %d, free: %d\",\n+                buffer_name, p2i(stubs_code->content_begin()), p2i(stubs_code->content_end()),\n+                buffer.total_content_size(), buffer.insts_remaining());\n+  }\n+  return stubs_code;\n+}\n+\n+void StubRoutines::initialize_initial_stubs() {\n+  if (_initial_stubs_code == nullptr) {\n+    _initial_stubs_code = initialize_stubs(StubCodeGenerator::Initial_stubs,\n+                                           _initial_stubs_code_size, 10,\n+                                           \"StubRoutines generation initial stubs\",\n+                                           \"StubRoutines (initial stubs)\",\n+                                           \"_initial_stubs_code_size\");\n+  }\n+}\n+\n+void StubRoutines::initialize_continuation_stubs() {\n+  if (_continuation_stubs_code == nullptr) {\n+    _continuation_stubs_code = initialize_stubs(StubCodeGenerator::Continuation_stubs,\n+                                           _continuation_stubs_code_size, 10,\n+                                           \"StubRoutines generation continuation stubs\",\n+                                           \"StubRoutines (continuation stubs)\",\n+                                           \"_continuation_stubs_code_size\");\n+  }\n+}\n+\n+void StubRoutines::initialize_compiler_stubs() {\n+  if (_compiler_stubs_code == nullptr) {\n+    _compiler_stubs_code = initialize_stubs(StubCodeGenerator::Compiler_stubs,\n+                                           _compiler_stubs_code_size, 100,\n+                                           \"StubRoutines generation compiler stubs\",\n+                                           \"StubRoutines (compiler stubs)\",\n+                                           \"_compiler_stubs_code_size\");\n@@ -273,32 +315,7 @@\n-void StubRoutines::initializeContinuationStubs() {\n-  if (_code3 == nullptr) {\n-    ResourceMark rm;\n-    TraceTime timer(\"StubRoutines generation 3\", TRACETIME_LOG(Info, startuptime));\n-    _code3 = BufferBlob::create(\"StubRoutines (3)\", code_size2);\n-    if (_code3 == nullptr) {\n-      vm_exit_out_of_memory(code_size2, OOM_MALLOC_ERROR, \"CodeCache: no room for StubRoutines (3)\");\n-    }\n-    CodeBuffer buffer(_code3);\n-    StubGenerator_generate(&buffer, 1);\n-    \/\/ When new stubs added we need to make sure there is some space left\n-    \/\/ to catch situation when we should increase size again.\n-    assert(code_size2 == 0 || buffer.insts_remaining() > 200, \"increase code_size3\");\n-  }\n-}\n-\n-void StubRoutines::initialize2() {\n-  if (_code2 == nullptr) {\n-    ResourceMark rm;\n-    TraceTime timer(\"StubRoutines generation 2\", TRACETIME_LOG(Info, startuptime));\n-    \/\/ Add extra space for large CodeEntryAlignment\n-    int max_aligned_stubs = 100;\n-    int size = code_size2 + CodeEntryAlignment * max_aligned_stubs;\n-    _code2 = BufferBlob::create(\"StubRoutines (2)\", size);\n-    if (_code2 == nullptr) {\n-      vm_exit_out_of_memory(code_size2, OOM_MALLOC_ERROR, \"CodeCache: no room for StubRoutines (2)\");\n-    }\n-    CodeBuffer buffer(_code2);\n-    StubGenerator_generate(&buffer, 2);\n-    \/\/ When new stubs added we need to make sure there is some space left\n-    \/\/ to catch situation when we should increase size again.\n-    assert(code_size2 == 0 || buffer.insts_remaining() > 200, \"increase code_size2\");\n+void StubRoutines::initialize_final_stubs() {\n+  if (_final_stubs_code == nullptr) {\n+    _final_stubs_code = initialize_stubs(StubCodeGenerator::Final_stubs,\n+                                         _final_stubs_code_size, 10,\n+                                         \"StubRoutines generation final stubs\",\n+                                         \"StubRoutines (final stubs)\",\n+                                         \"_final_stubs_code_size\");\n@@ -390,3 +407,18 @@\n-void stubRoutines_init1() { StubRoutines::initialize1(); }\n-void stubRoutines_init2() { StubRoutines::initialize2(); }\n-void stubRoutines_initContinuationStubs() { StubRoutines::initializeContinuationStubs(); }\n+void initial_stubs_init()      { StubRoutines::initialize_initial_stubs(); }\n+void continuation_stubs_init() { StubRoutines::initialize_continuation_stubs(); }\n+void final_stubs_init()        { StubRoutines::initialize_final_stubs(); }\n+\n+void compiler_stubs_init(bool in_compiler_thread) {\n+  if (in_compiler_thread && DelayCompilerStubsGeneration) {\n+    \/\/ Temporarily revert state of stubs generation because\n+    \/\/ it is called after final_stubs_init() finished\n+    \/\/ during compiler runtime initialization.\n+    \/\/ It is fine because these stubs are only used by\n+    \/\/ compiled code and compiler is not running yet.\n+    StubCodeDesc::unfreeze();\n+    StubRoutines::initialize_compiler_stubs();\n+    StubCodeDesc::freeze();\n+  } else if (!in_compiler_thread && !DelayCompilerStubsGeneration) {\n+    StubRoutines::initialize_compiler_stubs();\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":87,"deletions":55,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -151,3 +151,4 @@\n-  static BufferBlob* _code1;                               \/\/ code buffer for initial routines\n-  static BufferBlob* _code2;\n-  static BufferBlob* _code3;                               \/\/ code buffer for all other routines\n+  static BufferBlob* _initial_stubs_code;                  \/\/ code buffer for initial routines\n+  static BufferBlob* _continuation_stubs_code;             \/\/ code buffer for continuation stubs\n+  static BufferBlob* _compiler_stubs_code;                 \/\/ code buffer for C2 intrinsics\n+  static BufferBlob* _final_stubs_code;                    \/\/ code buffer for all other routines\n@@ -268,3 +269,4 @@\n-  static void    initialize1();                            \/\/ must happen before universe::genesis\n-  static void    initialize2();                            \/\/ must happen after  universe::genesis\n-  static void    initializeContinuationStubs();            \/\/ must happen after  universe::genesis\n+  static void    initialize_initial_stubs();               \/\/ must happen before universe::genesis\n+  static void    initialize_continuation_stubs();          \/\/ must happen after  universe::genesis\n+  static void    initialize_compiler_stubs();              \/\/ must happen after  universe::genesis\n+  static void    initialize_final_stubs();                 \/\/ must happen after  universe::genesis\n@@ -276,2 +278,4 @@\n-      (_code1 != nullptr && _code1->blob_contains(addr)) ||\n-      (_code2 != nullptr && _code2->blob_contains(addr)) ;\n+      (_initial_stubs_code      != nullptr && _initial_stubs_code->blob_contains(addr))  ||\n+      (_continuation_stubs_code != nullptr && _continuation_stubs_code->blob_contains(addr)) ||\n+      (_compiler_stubs_code     != nullptr && _compiler_stubs_code->blob_contains(addr)) ||\n+      (_final_stubs_code        != nullptr && _final_stubs_code->blob_contains(addr)) ;\n@@ -280,3 +284,4 @@\n-  static RuntimeBlob* code1() { return _code1; }\n-  static RuntimeBlob* code2() { return _code2; }\n-  static RuntimeBlob* code3() { return _code3; }\n+  static RuntimeBlob* initial_stubs_code()      { return _initial_stubs_code; }\n+  static RuntimeBlob* continuation_stubs_code() { return _continuation_stubs_code; }\n+  static RuntimeBlob* compiler_stubs_code()     { return _compiler_stubs_code; }\n+  static RuntimeBlob* final_stubs_code()        { return _final_stubs_code; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":16,"deletions":11,"binary":false,"changes":27,"status":"modified"}]}