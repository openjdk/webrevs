{"files":[{"patch":"@@ -2603,0 +2603,7 @@\n+  \/\/ Volatile load may be followed by Unsafe CAS.\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ sync();\n+  } else {\n+    __ lwsync();\n+  }\n+\n@@ -2964,3 +2971,18 @@\n-  Register Rco = noreg;\n-  if (UseCompressedOops && data->is_oop()) {\n-    Rco = __ encode_heap_oop(Rtmp, data->as_register());\n+  Register Robj = noreg;\n+  if (data->is_oop()) {\n+    if (UseCompressedOops) {\n+      Robj = __ encode_heap_oop(Rtmp, data->as_register());\n+    } else {\n+      Robj = data->as_register();\n+      if (Robj == dest->as_register()) { \/\/ May happen with ZGC.\n+        __ mr(Rtmp, Robj);\n+        Robj = Rtmp;\n+      }\n+    }\n+  }\n+\n+  \/\/ Volatile load may be followed by Unsafe OP.\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ sync();\n+  } else {\n+    __ lwsync();\n@@ -2986,0 +3008,1 @@\n+    assert_different_registers(Rptr, Rold, Robj);\n@@ -2987,1 +3010,0 @@\n-      assert_different_registers(Rptr, Rold, Rco);\n@@ -2989,1 +3011,1 @@\n-      __ stwcx_(Rco, Rptr);\n+      __ stwcx_(Robj, Rptr);\n@@ -2991,7 +3013,0 @@\n-      Register Robj = data->as_register();\n-      assert_different_registers(Rptr, Rold, Rtmp);\n-      assert_different_registers(Rptr, Robj, Rtmp);\n-      if (Robj == Rold) { \/\/ May happen with ZGC.\n-        __ mr(Rtmp, Robj);\n-        Robj = Rtmp;\n-      }\n@@ -3025,0 +3040,6 @@\n+\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ isync();\n+  } else {\n+    __ sync();\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":33,"deletions":12,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -642,7 +642,0 @@\n-  \/\/ Volatile load may be followed by Unsafe CAS.\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar();\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -673,8 +666,0 @@\n-\n-  \/\/ Volatile load may be followed by Unsafe CAS.\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar();\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -682,6 +667,0 @@\n-\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar_acquire();\n-  } else {\n-    __ membar();\n-  }\n@@ -697,8 +676,0 @@\n-\n-  \/\/ Volatile load may be followed by Unsafe CAS.\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar(); \/\/ To be safe. Unsafe semantics are unclear.\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -706,6 +677,0 @@\n-\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar_acquire();\n-  } else {\n-    __ membar();\n-  }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRGenerator_ppc.cpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n- * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2018, 2023, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n@@ -56,2 +56,7 @@\n-  \/\/ Due to the memory barriers emitted in ShenandoahBarrierSetC1::atomic_cmpxchg_at_resolved,\n-  \/\/ there is no need to specify stronger memory semantics.\n+  \/\/ Volatile load may be followed by Unsafe CAS.\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ sync();\n+  } else {\n+    __ lwsync();\n+  }\n+\n@@ -66,0 +71,6 @@\n+  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+    __ isync();\n+  } else {\n+    __ sync();\n+  }\n+\n@@ -83,8 +94,0 @@\n-    if (ShenandoahCASBarrier) {\n-      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        __ membar();\n-      } else {\n-        __ membar_release();\n-      }\n-    }\n-\n@@ -107,6 +110,0 @@\n-      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-        __ membar_acquire();\n-      } else {\n-        __ membar();\n-      }\n-\n@@ -128,6 +125,0 @@\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar();\n-  } else {\n-    __ membar_release();\n-  }\n-\n@@ -155,6 +146,0 @@\n-  if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-    __ membar_acquire();\n-  } else {\n-    __ membar();\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":15,"deletions":30,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -343,1 +343,2 @@\n-                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update());\n+                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(),\n+                noreg, need_restore ? nullptr : &slow_path);\n@@ -346,0 +347,1 @@\n+      __ bne(CCR0, slow_path);\n@@ -347,1 +349,0 @@\n-    __ bne(CCR0, slow_path);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zBarrierSetAssembler_ppc.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"}]}