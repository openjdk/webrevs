{"files":[{"patch":"@@ -181,3 +181,0 @@\n-    ld(Rmark, oopDesc::mark_offset_in_bytes(), Roop);\n-    andi_(R0, Rmark, markWord::monitor_value);\n-    bne(CCR0, slow_int);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,0 +39,11 @@\n+\n+void C2_MacroAssembler::fast_lock_lightweight(ConditionRegister flag, Register obj, Register box,\n+                                              Register tmp1, Register tmp2, Register tmp3) {\n+  compiler_fast_lock_lightweight_object(flag, obj, tmp1, tmp2, tmp3);\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(ConditionRegister flag, Register obj, Register box,\n+                                                Register tmp1, Register tmp2, Register tmp3) {\n+  compiler_fast_unlock_lightweight_object(flag, obj, tmp1, tmp2, tmp3);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/c2_MacroAssembler_ppc.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -31,0 +31,6 @@\n+  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in .ad file.\n+  void fast_lock_lightweight(ConditionRegister flag, Register obj, Register box,\n+                             Register tmp1, Register tmp2, Register tmp3);\n+  void fast_unlock_lightweight(ConditionRegister flag, Register obj, Register box,\n+                               Register tmp1, Register tmp2, Register tmp3);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/c2_MacroAssembler_ppc.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -973,3 +973,0 @@\n-    \/\/ Load markWord from object into header.\n-    ld(header, oopDesc::mark_offset_in_bytes(), object);\n-\n@@ -984,1 +981,1 @@\n-      lightweight_lock(object, \/* mark word *\/ header, tmp, slow_case);\n+      lightweight_lock(object, header, tmp, slow_case);\n@@ -987,0 +984,2 @@\n+      \/\/ Load markWord from object into header.\n+      ld(header, oopDesc::mark_offset_in_bytes(), object);\n@@ -1118,16 +1117,0 @@\n-      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n-      \/\/ must handle it.\n-      Register tmp = current_header;\n-      \/\/ First check for lock-stack underflow.\n-      lwz(tmp, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n-      cmplwi(CCR0, tmp, (unsigned)LockStack::start_offset());\n-      ble(CCR0, slow_case);\n-      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n-      addi(tmp, tmp, -oopSize);\n-      ldx(tmp, tmp, R16_thread);\n-      cmpd(CCR0, tmp, object);\n-      bne(CCR0, slow_case);\n-\n-      ld(header, oopDesc::mark_offset_in_bytes(), object);\n-      andi_(R0, header, markWord::monitor_value);\n-      bne(CCR0, slow_case);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":3,"deletions":20,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"register_ppc.hpp\"\n@@ -2186,0 +2187,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"uses fast_lock_lightweight\");\n@@ -2187,1 +2189,0 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT || flag == CCR0, \"bad condition register\");\n@@ -2211,1 +2212,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -2255,4 +2257,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_lock(oop, displaced_header, temp, failure);\n-    b(success);\n@@ -2276,4 +2274,2 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ Store a non-null value into the box.\n-    std(box, BasicLock::displaced_header_offset_in_bytes(), box);\n-  }\n+  \/\/ Store a non-null value into the box.\n+  std(box, BasicLock::displaced_header_offset_in_bytes(), box);\n@@ -2301,0 +2297,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"uses fast_unlock_lightweight\");\n@@ -2302,1 +2299,0 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT || flag == CCR0, \"bad condition register\");\n@@ -2324,1 +2320,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -2339,4 +2336,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_unlock(oop, current_header, failure);\n-    b(success);\n@@ -2382,0 +2375,270 @@\n+void MacroAssembler::compiler_fast_lock_lightweight_object(ConditionRegister flag, Register obj, Register tmp1,\n+                                                           Register tmp2, Register tmp3) {\n+  assert_different_registers(obj, tmp1, tmp2, tmp3);\n+  assert(flag == CCR0, \"bad condition register\");\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. MUST reach to with flag == NE\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == EQ\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(tmp1, obj);\n+    lwz(tmp1, in_bytes(Klass::access_flags_offset()), tmp1);\n+    testbitdi(flag, R0, tmp1, exact_log2(JVM_ACC_IS_VALUE_BASED_CLASS));\n+    bne(flag, slow_path);\n+  }\n+\n+  const Register mark = tmp1;\n+  const Register t = tmp3; \/\/ Usage of R0 allowed!\n+\n+  { \/\/ Lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST reach to with flag == EQ\n+    Label push;\n+\n+    const Register top = tmp2;\n+\n+    \/\/ Check if lock-stack is full.\n+    lwz(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+    cmplwi(flag, top, LockStack::end_offset() - 1);\n+    bgt(flag, slow_path);\n+\n+    \/\/ The underflow check is elided. The recursive check will always fail\n+    \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n+\n+    \/\/ Check if recursive.\n+    subi(t, top, oopSize);\n+    ldx(t, R16_thread, t);\n+    cmpd(flag, obj, t);\n+    beq(flag, push);\n+\n+    \/\/ Check for monitor (0b10) or locked (0b00).\n+    ld(mark, oopDesc::mark_offset_in_bytes(), obj);\n+    andi_(t, mark, markWord::lock_mask_in_place);\n+    cmpldi(flag, t, markWord::unlocked_value);\n+    bgt(flag, inflated);\n+    bne(flag, slow_path);\n+\n+    \/\/ Not inflated.\n+\n+    \/\/ Try to lock. Transition lock bits 0b00 => 0b01\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+    atomically_flip_locked_state(\/* is_unlock *\/ false, obj, mark, slow_path, MacroAssembler::MemBarAcq);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    stdx(obj, R16_thread, top);\n+    addi(top, top, oopSize);\n+    stw(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+    b(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register tagged_monitor = mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register owner_addr = tmp2;\n+\n+    \/\/ Compute owner address.\n+    addi(owner_addr, tagged_monitor, in_bytes(ObjectMonitor::owner_offset()) - monitor_tag);\n+\n+    \/\/ CAS owner (null => current thread).\n+    cmpxchgd(\/*flag=*\/flag,\n+            \/*current_value=*\/t,\n+            \/*compare_value=*\/(intptr_t)0,\n+            \/*exchange_value=*\/R16_thread,\n+            \/*where=*\/owner_addr,\n+            MacroAssembler::MemBarRel | MacroAssembler::MemBarAcq,\n+            MacroAssembler::cmpxchgx_hint_acquire_lock());\n+    beq(flag, locked);\n+\n+    \/\/ Check if recursive.\n+    cmpd(flag, t, R16_thread);\n+    bne(flag, slow_path);\n+\n+    \/\/ Recursive.\n+    ld(tmp1, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), owner_addr);\n+    addi(tmp1, tmp1, 1);\n+    std(tmp1, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), owner_addr);\n+  }\n+\n+  bind(locked);\n+  inc_held_monitor_count(tmp1);\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with flags == EQ.\n+  Label flag_correct;\n+  beq(flag, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with flags == NE.\n+  bne(flag, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of flag (NE vs EQ) to determine the continuation.\n+}\n+\n+void MacroAssembler::compiler_fast_unlock_lightweight_object(ConditionRegister flag, Register obj, Register tmp1,\n+                                                             Register tmp2, Register tmp3) {\n+  assert_different_registers(obj, tmp1, tmp2, tmp3);\n+  assert(flag == CCR0, \"bad condition register\");\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_load_monitor;\n+  \/\/ Finish fast unlock successfully. MUST reach to with flag == EQ.\n+  Label unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE.\n+  Label slow_path;\n+\n+  const Register mark = tmp1;\n+  const Register top = tmp2;\n+  const Register t = tmp3;\n+\n+  { \/\/ Lightweight unlock\n+    Label push_and_slow;\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    lwz(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+    subi(top, top, oopSize);\n+    ldx(t, R16_thread, top);\n+    cmpd(flag, obj, t);\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    bne(flag, inflated_load_monitor);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(li(t, 0);)\n+    DEBUG_ONLY(stdx(t, R16_thread, top);)\n+    stw(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+\n+    \/\/ The underflow check is elided. The recursive check will always fail\n+    \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n+\n+    \/\/ Check if recursive.\n+    subi(t, top, oopSize);\n+    ldx(t, R16_thread, t);\n+    cmpd(flag, obj, t);\n+    beq(flag, unlocked);\n+\n+    \/\/ Not recursive.\n+\n+    \/\/ Check for monitor (0b10).\n+    ld(mark, oopDesc::mark_offset_in_bytes(), obj);\n+    andi_(t, mark, markWord::monitor_value);\n+    bne(CCR0, inflated);\n+\n+#ifdef ASSERT\n+    \/\/ Check header not unlocked (0b01).\n+    Label not_unlocked;\n+    andi_(t, mark, markWord::unlocked_value);\n+    beq(CCR0, not_unlocked);\n+    stop(\"lightweight_unlock already unlocked\");\n+    bind(not_unlocked);\n+#endif\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    atomically_flip_locked_state(\/* is_unlock *\/ true, obj, mark, push_and_slow, MacroAssembler::MemBarRel);\n+    b(unlocked);\n+\n+    bind(push_and_slow);\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    DEBUG_ONLY(stdx(obj, R16_thread, top);)\n+    addi(top, top, oopSize);\n+    stw(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+    b(slow_path);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_load_monitor);\n+    ld(mark, oopDesc::mark_offset_in_bytes(), obj);\n+#ifdef ASSERT\n+    andi_(t, mark, markWord::monitor_value);\n+    bne(CCR0, inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    Label check_done;\n+    subi(top, top, oopSize);\n+    cmplwi(CCR0, top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    blt(CCR0, check_done);\n+    ldx(t, R16_thread, top);\n+    cmpd(flag, obj, t);\n+    bne(flag, inflated);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register monitor = mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+    \/\/ Untag the monitor.\n+    subi(monitor, mark, monitor_tag);\n+\n+    const Register recursions = tmp2;\n+    Label not_recursive;\n+\n+    \/\/ Check if recursive.\n+    ld(recursions, in_bytes(ObjectMonitor::recursions_offset()), monitor);\n+    addic_(recursions, recursions, -1);\n+    blt(CCR0, not_recursive);\n+\n+    \/\/ Recursive unlock.\n+    std(recursions, in_bytes(ObjectMonitor::recursions_offset()), monitor);\n+    crorc(CCR0, Assembler::equal, CCR0, Assembler::equal);\n+    b(unlocked);\n+\n+    bind(not_recursive);\n+\n+    Label release_;\n+    const Register t2 = tmp2;\n+\n+    \/\/ Check if the entry lists are empty.\n+    ld(t, in_bytes(ObjectMonitor::EntryList_offset()), monitor);\n+    ld(t2, in_bytes(ObjectMonitor::cxq_offset()), monitor);\n+    orr(t, t, t2);\n+    cmpdi(flag, t, 0);\n+    beq(flag, release_);\n+\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    std(R16_thread, in_bytes(ObjectMonitor::owner_offset()), monitor);\n+    b(slow_path);\n+\n+    bind(release_);\n+    \/\/ Set owner to null.\n+    release();\n+    \/\/ t contains 0\n+    std(t, in_bytes(ObjectMonitor::owner_offset()), monitor);\n+  }\n+\n+  bind(unlocked);\n+  dec_held_monitor_count(t);\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with flags == EQ.\n+  Label flag_correct;\n+  beq(flag, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with flags == NE.\n+  bne(flag, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of flag (NE vs EQ) to determine the continuation.\n+}\n+\n@@ -4001,2 +4264,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with CCR0 NE.\n-\/\/ Falls through upon success with CCR0 EQ.\n@@ -4005,3 +4266,2 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1: temporary register\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Label& slow) {\n+\/\/  - t1, t2: temporary register\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Label& slow) {\n@@ -4009,1 +4269,1 @@\n-  assert_different_registers(obj, hdr, t1);\n+  assert_different_registers(obj, t1, t2);\n@@ -4011,4 +4271,4 @@\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  lwz(t1, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n-  cmplwi(CCR0, t1, LockStack::end_offset() - 1);\n-  bgt(CCR0, slow);\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = R0;\n@@ -4016,5 +4276,4 @@\n-  \/\/ Quick check: Do not reserve cache line for atomic update if not unlocked.\n-  \/\/ (Similar to contention_hint in cmpxchg solutions.)\n-  xori(R0, hdr, markWord::unlocked_value); \/\/ flip unlocked bit\n-  andi_(R0, R0, markWord::lock_mask_in_place);\n-  bne(CCR0, slow); \/\/ failed if new header doesn't contain locked_value (which is 0)\n+  \/\/ Check if the lock-stack is full.\n+  lwz(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  cmplwi(CCR0, top, LockStack::end_offset());\n+  bge(CCR0, slow);\n@@ -4022,4 +4281,2 @@\n-  \/\/ Note: We're not publishing anything (like the displaced header in LM_LEGACY)\n-  \/\/ to other threads at this point. Hence, no release barrier, here.\n-  \/\/ (The obj has been written to the BasicObjectLock at obj_offset() within the own thread stack.)\n-  atomically_flip_locked_state(\/* is_unlock *\/ false, obj, hdr, slow, MacroAssembler::MemBarAcq);\n+  \/\/ The underflow check is elided. The recursive check will always fail\n+  \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n@@ -4027,0 +4284,16 @@\n+  \/\/ Check for recursion.\n+  subi(t, top, oopSize);\n+  ldx(t, R16_thread, t);\n+  cmpd(CCR0, obj, t);\n+  beq(CCR0, push);\n+\n+  \/\/ Check header for monitor (0b10) or locked (0b00).\n+  ld(mark, oopDesc::mark_offset_in_bytes(), obj);\n+  xori(t, mark, markWord::unlocked_value);\n+  andi_(t, t, markWord::lock_mask_in_place);\n+  bne(CCR0, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b00 => 0b01\n+  atomically_flip_locked_state(\/* is_unlock *\/ false, obj, mark, slow, MacroAssembler::MemBarAcq);\n+\n+  bind(push);\n@@ -4028,3 +4301,3 @@\n-  stdx(obj, t1, R16_thread);\n-  addi(t1, t1, oopSize);\n-  stw(t1, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  stdx(obj, R16_thread, top);\n+  addi(top, top, oopSize);\n+  stw(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n@@ -4034,2 +4307,0 @@\n-\/\/ Branches to slow upon failure, with CCR0 NE.\n-\/\/ Falls through upon success, with CCR0 EQ.\n@@ -4038,2 +4309,2 @@\n-\/\/ - hdr: the (pre-loaded) header of the object, will be destroyed\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Label& slow) {\n+\/\/  - t1: temporary register\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Label& slow) {\n@@ -4041,1 +4312,1 @@\n-  assert_different_registers(obj, hdr);\n+  assert_different_registers(obj, t1);\n@@ -4044,9 +4315,0 @@\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    andi_(R0, hdr, markWord::lock_mask_in_place);\n-    beq(CCR0, hdr_ok);\n-    stop(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n-  Register t1 = hdr; \/\/ Reuse in debug build.\n@@ -4062,1 +4324,1 @@\n-    bgt(CCR0, stack_ok);\n+    bge(CCR0, stack_ok);\n@@ -4066,10 +4328,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    addi(t1, t1, -oopSize);\n-    ldx(t1, t1, R16_thread);\n-    cmpd(CCR0, t1, obj);\n-    beq(CCR0, tos_ok);\n-    stop(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n@@ -4078,2 +4330,33 @@\n-  \/\/ Release the lock.\n-  atomically_flip_locked_state(\/* is_unlock *\/ true, obj, hdr, slow, MacroAssembler::MemBarRel);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = R0;\n+  Register t = R0;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  lwz(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  subi(top, top, oopSize);\n+  ldx(t, R16_thread, top);\n+  cmpd(CCR0, obj, t);\n+  bne(CCR0, slow);\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(li(t, 0);)\n+  DEBUG_ONLY(stdx(t, R16_thread, top);)\n+  stw(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+\n+  \/\/ The underflow check is elided. The recursive check will always fail\n+  \/\/ when the lock stack is empty because of the _bad_oop_sentinel field.\n+\n+  \/\/ Check if recursive.\n+  subi(t, top, oopSize);\n+  ldx(t, R16_thread, t);\n+  cmpd(CCR0, obj, t);\n+  beq(CCR0, unlocked);\n+\n+  \/\/ Use top as tmp\n+  t = top;\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ld(mark, oopDesc::mark_offset_in_bytes(), obj);\n+  andi_(t, mark, markWord::monitor_value);\n+  bne(CCR0, push_and_slow);\n@@ -4081,4 +4364,0 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  Register t2 = hdr;\n-  lwz(t2, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n-  addi(t2, t2, -oopSize);\n@@ -4086,2 +4365,6 @@\n-  li(R0, 0);\n-  stdx(R0, t2, R16_thread);\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  andi_(t, mark, markWord::unlocked_value);\n+  beq(CCR0, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -4089,1 +4372,15 @@\n-  stw(t2, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  atomically_flip_locked_state(\/* is_unlock *\/ true, obj, t, push_and_slow, MacroAssembler::MemBarRel);\n+  b(unlocked);\n+\n+  bind(push_and_slow);\n+\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  lwz(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  DEBUG_ONLY(stdx(obj, R16_thread, top);)\n+  addi(top, top, oopSize);\n+  stw(top, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":369,"deletions":72,"binary":false,"changes":441,"status":"modified"},{"patch":"@@ -615,2 +615,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register t1, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Label& slow);\n+  void lightweight_lock(Register obj, Register t1, Register t2, Label& slow);\n+  void lightweight_unlock(Register obj, Register t1, Label& slow);\n@@ -637,0 +637,6 @@\n+  void compiler_fast_lock_lightweight_object(ConditionRegister flag, Register oop, Register tmp1,\n+                                             Register tmp2, Register tmp3);\n+\n+  void compiler_fast_unlock_lightweight_object(ConditionRegister flag, Register oop, Register tmp1,\n+                                               Register tmp2, Register tmp3);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -12150,0 +12150,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12165,0 +12166,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12179,0 +12181,32 @@\n+instruct cmpFastLockLightweight(flagsRegCR0 crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set crx (FastLock oop box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+\n+  format %{ \"FASTLOCK  $oop, $box, $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($crx$$CondRegister, $oop$$Register, $box$$Register,\n+                             $tmp1$$Register, $tmp2$$Register, \/*tmp3*\/ R0);\n+    \/\/ If locking was successful, crx should indicate 'EQ'.\n+    \/\/ The compiler generates a branch to the runtime call to\n+    \/\/ _complete_monitor_locking_Java for the case where crx is 'NE'.\n+  %}\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct cmpFastUnlockLightweight(flagsRegCR0 crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set crx (FastUnlock oop box));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3);\n+\n+  format %{ \"FASTUNLOCK  $oop, $box, $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($crx$$CondRegister, $oop$$Register, $box$$Register,\n+                               $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+    \/\/ If unlocking was successful, crx should indicate 'EQ'.\n+    \/\/ The compiler generates a branch to the runtime call to\n+    \/\/ _complete_monitor_unlocking_Java for the case where crx is 'NE'.\n+  %}\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2407,2 +2407,7 @@\n-    \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n-    __ compiler_fast_lock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n+      __ compiler_fast_lock_lightweight_object(CCR0, r_oop, r_temp_1, r_temp_2, r_temp_3);\n+    } else {\n+      \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n+      __ compiler_fast_lock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    }\n@@ -2618,1 +2623,5 @@\n-    __ compiler_fast_unlock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      __ compiler_fast_unlock_lightweight_object(CCR0, r_oop, r_temp_1, r_temp_2, r_temp_3);\n+    } else {\n+      __ compiler_fast_unlock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -97,0 +97,1 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n","filename":"src\/hotspot\/cpu\/ppc\/vm_version_ppc.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}