{"files":[{"patch":"@@ -4832,0 +4832,26 @@\n+  template<int N>\n+  void vs_shl(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+              const VSeq<N>& v1, int shift) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+\n+    for (int i = 0; i < N; i++) {\n+      __ shl(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_ushr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, int shift) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+\n+    for (int i = 0; i < N; i++) {\n+      __ ushr(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n@@ -4856,0 +4882,23 @@\n+  template<int N>\n+  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const FloatRegister v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ andr(v[i], __ T16B, v1[i], v2);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_eor(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ eor(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n@@ -7143,9 +7192,0 @@\n-  \/\/ P256 Montgomery Multiplication.\n-  \/\/ Implements the method protected void mult(long[] a, long[] b, long[] r) {}\n-  \/\/ of the sun.security.util.math.intpoly.MontgomeryIntegerPolynomialP256 class\n-  \/\/\n-  \/\/ a (long[5]) = c_rarg0\n-  \/\/ b (long[5]) = c_rarg1\n-  \/\/ r (long[5]) = c_rarg2\n-  \/\/\n-  \/\/ Note that each arg represents a 256-bit integer broken into 52-bit limbs\n@@ -7160,4 +7200,0 @@\n-    const Register a = c_rarg0;\n-    const Register b = c_rarg1;\n-    const Register result = c_rarg2;\n-\n@@ -7173,13 +7209,20 @@\n-    \/\/ GPRs that are used throughout loop\n-    Register b_j = r3;\n-    Register mod_ptr = r4;\n-    Register limb_mask_scalar = r5;\n-    Register c_ptr = r6;\n-\n-    \/\/ These neon registers remain constant through the main loop\n-    FloatRegister limb_mask = v0;\n-    FloatRegister mask_32_vec = v1;\n-    FloatRegister b_lows = v2;\n-    FloatRegister b_highs = v3;\n-    FloatRegister mod_lows = v4;\n-    FloatRegister mod_highs = v5;\n+    \/\/ Registers that are used throughout entire routine\n+    const Register a = c_rarg0;\n+    const Register b = c_rarg1;\n+    const Register result = c_rarg2;\n+    Register limb_mask = r3;\n+    Register c_ptr = r4;\n+    Register mod_0 = r5;\n+    Register mod_1 = r6;\n+    Register mod_3 = r7;\n+    Register mod_4 = r10;\n+    Register b_0 = r11;\n+    Register b_1 = r12;\n+    Register b_2 = r13;\n+    Register b_3 = r14;\n+    Register b_4 = r15;\n+\n+    FloatRegister limb_mask_vec = v0;\n+    FloatRegister b_lows = v1;\n+    FloatRegister b_highs = v2;\n+    FloatRegister a_vals = v3;\n@@ -7191,2 +7234,2 @@\n-    \/\/ Allocate space on the stack for carry values and zero memory\n-    __ sub(sp, sp, 80);\n+    \/\/ Allocate space on the stack for carry values\n+    __ sub(sp, sp, 48);\n@@ -7194,4 +7237,0 @@\n-    __ eor(b_j, b_j, b_j); \/\/Create a 0 reg to clear memory\n-    for (int i = 0; i < 10; i++) {\n-      __ str(b_j, Address(sp, i * 8));\n-    }\n@@ -7200,2 +7239,2 @@\n-      __ mov(limb_mask_scalar, -UCONST64(1) >> (64 - shift2));\n-      __ dup(limb_mask, __ T2D, limb_mask_scalar);\n+    __ mov(limb_mask, -UCONST64(1) >> (64 - shift2));\n+    __ dup(limb_mask_vec, __ T2D, limb_mask);\n@@ -7203,1 +7242,1 @@\n-    \/\/ Calculate 32-bit mask\n+    \/\/Load input arrays and modulus\n@@ -7205,35 +7244,184 @@\n-      Register mask_32 = r7;\n-      __ mov(mask_32, (UCONST64(1) << 32) - 1);\n-      __ dup(mask_32_vec, __ T2D, mask_32);\n-    }\n-\n-    \/\/ Load modulus and input array b\n-    __ lea(mod_ptr, ExternalAddress((address)modulus));\n-    __ ld2(b_lows, b_highs, __ T4S, Address(b));\n-    __ ld2(mod_lows, mod_highs, __ T4S, Address(mod_ptr));\n-    __ ldr(b_j, Address(b, 32));\n-\n-    for (int i = 0; i < 5; i++) {\n-      Register c_idx = r10;\n-      Register mul_tmp = r11;\n-      Register scalar_ai = r12;\n-\n-      FloatRegister A = v6;\n-      FloatRegister B = v7;\n-      FloatRegister C = v8;\n-      FloatRegister D = v16;\n-      FloatRegister a_i = v17;\n-      FloatRegister n = v18;\n-      FloatRegister middle = v19;\n-      FloatRegister tmp = v20;\n-      FloatRegister modmul_low = v21;\n-      FloatRegister modmul_high = v22;\n-      FloatRegister c_01 = v23;\n-      FloatRegister c_23 = v24;\n-      FloatRegister low_34 = v25;\n-      FloatRegister low_01 = v26;\n-      FloatRegister low_23 = v27;\n-      FloatRegister low_4x = v28;\n-      FloatRegister high_01 = v29;\n-      FloatRegister high_23 = v30;\n-      FloatRegister high_4x = v31;\n+      Register a_ptr = r27;\n+      Register mod_ptr = r28;\n+      __ add(a_ptr, a, 24);\n+      __ lea(mod_ptr, ExternalAddress((address)modulus));\n+      __ ldr(b_0, Address(b));\n+      __ ldr(b_1, Address(b, 8));\n+      __ ldr(b_2, Address(b, 16));\n+      __ ldr(b_3, Address(b, 24));\n+      __ ldr(b_4, Address(b, 32));\n+      __ ldr(mod_0, __ post(mod_ptr, 8));\n+      __ ldr(mod_1, __ post(mod_ptr, 8));\n+      __ ldr(mod_3, __ post(mod_ptr, 8));\n+      __ ldr(mod_4, mod_ptr);\n+      __ ld1(a_vals, __ T2D, a_ptr);\n+      __ ld2(b_lows, b_highs, __ T4S, b);\n+    }\n+\n+    \/\/Regs used throughout the main \"loop\", which is partially unrolled here\n+    Register high = r19;\n+    Register low = r20;\n+    Register mul_ptr = r21;\n+    Register mod_high = r23;\n+    Register mod_low = r24;\n+    Register a_i = r25;\n+    Register c_i = r26;\n+    Register tmp = r27;\n+    Register n = r28;\n+\n+    VSeq<4> A(16);\n+    VSeq<4> B(20);\n+    VSeq<4> C(24);\n+    VSeq<4> D(28);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    __ sub(sp, sp, 128);\n+    __ mov(mul_ptr, sp);\n+\n+    __ umullv(A[0], __ T2D, b_lows, __ T2S, a_vals, __ S, 0);\n+    __ umull2v(A[1], __ T2D, b_lows, __ T4S, a_vals, __ S, 0);\n+    __ umullv(A[2], __ T2D, b_lows, __ T2S, a_vals, __ S, 2);\n+    __ umull2v(A[3], __ T2D, b_lows, __ T4S, a_vals, __ S, 2);\n+\n+    \/\/ Limb 0\n+    __ ldr(a_i, __ post(a, 8));\n+    __ umulh(high, a_i, b_0);\n+    __ mul(low, a_i, b_0);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+    __ andr(n, low, limb_mask);\n+\n+    __ umullv(B[0], __ T2D, b_highs, __ T2S, a_vals, __ S, 0);\n+    __ umull2v(B[1], __ T2D, b_highs, __ T4S, a_vals, __ S, 0);\n+    __ umullv(B[2], __ T2D, b_highs, __ T2S, a_vals, __ S, 2);\n+    __ umull2v(B[3], __ T2D, b_highs, __ T4S, a_vals, __ S, 2);\n+\n+    \/\/ Limb 0 cont\n+    __ umulh(mod_high, n, mod_0);\n+    __ mul(mod_low, n, mod_0);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ lsr(c_i, low, shift2);\n+    __ add(c_i, c_i, high);\n+\n+    __ umullv(C[0], __ T2D, b_lows, __ T2S, a_vals, __ S, 1);\n+    __ umull2v(C[1], __ T2D, b_lows, __ T4S, a_vals, __ S, 1);\n+    __ umullv(C[2], __ T2D, b_lows, __ T2S, a_vals, __ S, 3);\n+    __ umull2v(C[3], __ T2D, b_lows, __ T4S, a_vals, __ S, 3);\n+\n+    \/\/ Limb 1\n+    __ umulh(high, a_i, b_1);\n+    __ mul(low, a_i, b_1);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+\n+    __ umullv(D[0], __ T2D, b_highs, __ T2S, a_vals, __ S, 1);\n+    __ umull2v(D[1], __ T2D, b_highs, __ T4S, a_vals, __ S, 1);\n+    __ umullv(D[2], __ T2D, b_highs, __ T2S, a_vals, __ S, 3);\n+    __ umull2v(D[3], __ T2D, b_highs, __ T4S, a_vals, __ S, 3);\n+\n+    __ umulh(mod_high, n, mod_1);\n+    __ mul(mod_low, n, mod_1);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, c_ptr);\n+    __ mov(c_i, high);\n+\n+    vs_addv(B, __ T2D, B, C); \/\/ Store (B+C) in B\n+\n+    \/\/ Limb 2\n+    __ umulh(high, a_i, b_2);\n+    __ mul(low, a_i, b_2);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 8));\n+    __ mov(c_i, high);\n+\n+    vs_shl(D, __ T2D, D, 12);\n+\n+    \/\/ Limb 3\n+    __ umulh(high, a_i, b_3); \/\/compute next mult to avoid waiting for result\n+    __ mul(low, a_i, b_3);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+\n+    vs_ushr(C, __ T2D, B, 20); \/\/ Use C for ((B+C) >>> 20)\n+\n+    __ umulh(mod_high, n, mod_3);\n+    __ mul(mod_low, n, mod_3);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 16));\n+    __ mov(c_i, high);\n+\n+    vs_shl(B, __ T2D, B, 32);\n+\n+    \/\/ Limb 4\n+    __ umulh(high, a_i, b_4);\n+    __ mul(low, a_i, b_4);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+\n+    vs_addv(D, __ T2D, D, C);\n+\n+    __ umulh(mod_high, n, mod_4);\n+    __ mul(mod_low, n, mod_4);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 24));\n+    __ str(high, Address(c_ptr, 32));\n+\n+    vs_ushr(C, __ T2D, A, 52); \/\/ C now holds (A >>> 52)\n+    vs_andr(B, B, limb_mask_vec);\n+    vs_andr(A, A, limb_mask_vec);\n+    vs_addv(D, __ T2D, D, C);\n+    vs_addv(A, __ T2D, A, B);\n+\n+    vs_ushr(B, __ T2D, A, shift2);\n+    vs_andr(A, A, limb_mask_vec);\n+    vs_addv(D, __ T2D, D, B);\n+\n+    __ st1(A[0], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[0], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(A[1], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[1], __ T2D, __ post(mul_ptr, 16));\n+\n+    __ st1(A[2], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[2], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(A[3], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[3], __ T2D, mul_ptr);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ Loop 2 & 3\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n@@ -7241,0 +7429,1 @@\n+    for (int i = 0; i < 2; i++) {\n@@ -7242,195 +7431,279 @@\n-      __ ldr(scalar_ai, a);\n-      __ ld1(a_i, __ D, 0, __ post(a, 8));\n-\n-      \/\/ Start computing final multiply with GPR since it is not\n-      \/\/ worth it to vectorize a single mult\n-      __ mul(mul_tmp, scalar_ai, b_j);\n-      __ mov(low_4x, Assembler::D, 0, mul_tmp);\n-      __ umulh(mul_tmp, scalar_ai, b_j);\n-      __ mov(high_4x, Assembler::D, 0, mul_tmp);\n-\n-      \/\/ Iterate through b, multiplying each limb by a_i\n-      \/\/ storing low and high parts in separate vectors.\n-      \/\/ Compute high[i] = high[i] << shift1 | (low[i] >>> shift2)\n-      \/\/ and low[i] &= LIMB_MASK\n-\n-      \/\/ Calculus low_01 and high_01\n-      __ umullv(A, __ T2D, b_lows, __ T2S, a_i, __ S, 0);\n-      __ umullv(B, __ T2D, b_highs, __ T2S, a_i, __ S, 0);\n-      __ umullv(C, __ T2D, b_lows, __ T2S, a_i, __ S, 1);\n-      __ umullv(D, __ T2D, b_highs, __ T2S, a_i, __ S, 1);\n-\n-      __ andr(middle, __ T16B, B, mask_32_vec);\n-      __ ushr(tmp, __ T2D, A, 32);\n-      __ addv(middle, __ T2D, middle, tmp);\n-      __ addv(middle, __ T2D, middle, C);\n-\n-      __ shl(low_01, __ T2D, middle, 32);\n-      __ andr(tmp, __ T16B, A, mask_32_vec);\n-      __ orr(low_01, __ T16B, low_01, tmp);\n-\n-      __ ushr(high_01, __ T2D, middle, 32);\n-      __ addv(high_01, __ T2D, high_01, D);\n-      __ ushr(tmp, __ T2D, B, 32);\n-      __ addv(high_01, __ T2D, high_01, tmp);\n-\n-      __ shl(high_01, __ T2D, high_01, shift1);\n-      __ ushr(tmp, __ T2D, low_01, shift2);\n-      __ orr(high_01, __ T16B, high_01, tmp);\n-      __ andr(low_01, __ T16B, low_01, limb_mask);\n-\n-      \/\/ Calculate low_23 and high_23\n-      __ umull2v(A, __ T2D, b_lows, __ T4S, a_i, __ S, 0);\n-      __ umull2v(B, __ T2D, b_highs, __ T4S, a_i, __ S, 0);\n-      __ umull2v(C, __ T2D, b_lows, __ T4S, a_i, __ S, 1);\n-      __ umull2v(D, __ T2D, b_highs, __ T4S, a_i, __ S, 1);\n-\n-      __ andr(middle, __ T16B, B, mask_32_vec);\n-      __ ushr(tmp, __ T2D, A, 32);\n-      __ addv(middle, __ T2D, middle, tmp);\n-      __ addv(middle, __ T2D, middle, C);\n-\n-      __ shl(low_23, __ T2D, middle, 32);\n-      __ andr(tmp, __ T16B, A, mask_32_vec);\n-      __ orr(low_23, __ T16B, low_23, tmp);\n-\n-      __ ushr(high_23, __ T2D, middle, 32);\n-      __ addv(high_23, __ T2D, high_23, D);\n-      __ ushr(tmp, __ T2D, B, 32);\n-      __ addv(high_23, __ T2D, high_23, tmp);\n-\n-      __ shl(high_23, __ T2D, high_23, shift1);\n-      __ ushr(tmp, __ T2D, low_23, shift2);\n-      __ orr(high_23, __ T16B, high_23, tmp);\n-      __ andr(low_23, __ T16B, low_23, limb_mask);\n-\n-      \/\/ Finish computing high_4x\n-      __ shl(high_4x, __ T2D, high_4x, shift1);\n-      __ ushr(tmp, __ T2D, low_4x, shift2);\n-      __ orr(high_4x, __ T16B, high_4x, tmp);\n-      __ andr(low_4x, __ T16B, low_4x, limb_mask);\n-\n-      \/\/ low_0 += c_i\n-      \/\/ n = low_0 & limb_mask\n-      __ eor(c_01, __ T16B, c_01, c_01);\n-      __ ld1(c_01, __ D, 0, c_ptr);\n-      __ addv(low_01, __ T2D, low_01, c_01);\n-      __ andr(n, __ T16B, low_01, limb_mask);\n-\n-      \/\/ Iterate through the modulus, multiplying each limb by n and\n-      \/\/ storing low and high parts in separate vectors.\n-      \/\/ Compute high += modmul_high << shift1 | (modmul_low >>> shift2);\n-      \/\/ and low += modmul_low & LIMB_MASK\n-\n-      \/\/ Calculate modmul_low and modmul_high for modulus[0] and modulus[1]\n-      __ umullv(A, __ T2D, mod_lows, __ T2S, n, __ S, 0);\n-      __ umullv(B, __ T2D, mod_highs, __ T2S, n, __ S, 0);\n-      __ umullv(C, __ T2D, mod_lows, __ T2S, n, __ S, 1);\n-      __ umullv(D, __ T2D, mod_highs, __ T2S, n, __ S, 1);\n-\n-      __ andr(middle, __ T16B, B, mask_32_vec);\n-      __ ushr(tmp, __ T2D, A, 32);\n-      __ addv(middle, __ T2D, middle, tmp);\n-      __ addv(middle, __ T2D, middle, C);\n-\n-      __ shl(modmul_low, __ T2D, middle, 32);\n-      __ andr(tmp, __ T16B, A, mask_32_vec);\n-      __ orr(modmul_low, __ T16B, modmul_low, tmp);\n-\n-      __ ushr(modmul_high, __ T2D, middle, 32);\n-      __ addv(modmul_high, __ T2D, modmul_high, D);\n-      __ ushr(tmp, __ T2D, B, 32);\n-      __ addv(modmul_high, __ T2D, modmul_high, tmp);\n-\n-      __ shl(modmul_high, __ T2D, modmul_high, shift1);\n-      __ ushr(tmp, __ T2D, modmul_low, shift2);\n-      __ orr(modmul_high, __ T16B, modmul_high, tmp);\n-      __ addv(high_01, __ T2D, high_01, modmul_high);\n-      __ andr(modmul_low, __ T16B, modmul_low, limb_mask);\n-      __ addv(low_01, __ T2D, low_01, modmul_low);\n-\n-      \/\/ Calculate modmul_low and modmul_high for modulus[3] and modulus[4].\n-      \/\/ Can omit modulus[2] since it is 0\n-      __ umull2v(A, __ T2D, mod_lows, __ T4S, n, __ S, 0);\n-      __ umull2v(B, __ T2D, mod_highs, __ T4S, n, __ S, 0);\n-      __ umull2v(C, __ T2D, mod_lows, __ T4S, n, __ S, 1);\n-      __ umull2v(D, __ T2D, mod_highs, __ T4S, n, __ S, 1);\n-\n-      __ andr(middle, __ T16B, B, mask_32_vec);\n-      __ ushr(tmp, __ T2D, A, 32);\n-      __ addv(middle, __ T2D, middle, tmp);\n-      __ addv(middle, __ T2D, middle, C);\n-\n-      __ shl(modmul_low, __ T2D, middle, 32);\n-      __ andr(tmp, __ T16B, A, mask_32_vec);\n-      __ orr(modmul_low, __ T16B, modmul_low, tmp);\n-\n-      __ ushr(modmul_high, __ T2D, middle, 32);\n-      __ addv(modmul_high, __ T2D, modmul_high, D);\n-      __ ushr(tmp, __ T2D, B, 32);\n-      __ addv(modmul_high, __ T2D, modmul_high, tmp);\n-\n-      __ shl(modmul_high, __ T2D, modmul_high, shift1);\n-      __ ushr(tmp, __ T2D, modmul_low, shift2);\n-      __ orr(modmul_high, __ T16B, modmul_high, tmp);\n-      __ andr(modmul_low, __ T16B, modmul_low, limb_mask);\n-\n-      \/\/Need to shift around vectors to get right layout bc of no modulus[2]\n-      __ ins(low_34, __ D, low_23, 0, 1);\n-      __ ins(low_34, __ D, low_4x, 1, 0);\n-      __ addv(low_34, __ T2D, low_34, modmul_low);\n-\n-      __ eor(tmp, __ T16B, tmp, tmp);\n-      __ ins(tmp, __ D, modmul_high, 1, 0); \/\/ tmp = [0, nn3]\n-      __ addv(high_23, __ T2D, high_23, tmp);\n-      __ ins(tmp, __ D, modmul_high, 0, 1); \/\/ tmp = [nn4, nn3]\n-      __ addv(high_4x, __ T2D, high_4x, tmp);\n-\n-      \/\/ Compute carry values\n-      \/\/ c_i+1 += low_1 + high_0 + (low_0 >>> shift2)\n-      \/\/ c_i+2 += low_2 + high_1\n-      \/\/ c_i+3 += low_3 + high_2\n-      \/\/ c_i+4 += low_4 + high_3;\n-      \/\/ c_i+5 = high_4\n-      __ add(c_ptr, c_ptr, 8);\n-      __ ld1(c_01, c_23, __ T2D, c_ptr);\n-      __ add(c_idx, c_ptr, 32);\n-      __ st1(high_4x, __ D, 0, c_idx);\n-\n-      \/\/ Add high values to c\n-      __ addv(c_01, __ T2D, c_01, high_01);\n-      __ addv(c_23, __ T2D, c_23, high_23);\n-      __ addv(c_23, __ T2D, c_23, low_34);\n-\n-      \/\/ Reorder low vectors to enable simd ops\n-      __ ins(tmp, __ D, low_01, 0, 1);\n-      __ ins(tmp, __ D, low_23, 1, 0);\n-      __ addv(c_01, __ T2D, c_01, tmp);\n-\n-      \/\/ clear tmp_4x and put low_0 in first lane\n-      \/\/ Shift low_0 and add to c_i+1\n-      __ ushr(low_01, __ T2D, low_01, shift2);\n-      __ eor(tmp, __ T16B, tmp, tmp); \/\/zero out tmp\n-      __ ins(tmp, __ D, low_01, 0, 0);\n-      __ addv(c_01, __ T2D, c_01, tmp);\n-\n-      \/\/ Write back carry values to stack\n-      __ st1(c_01, c_23, __ T2D, c_ptr);\n-    }\n-\n-    \/\/ Final carry propagate and write result\n-    Register mod_j = r3; \/\/ b_j is not used after loop\n-    Register tmp = r6; \/\/ c_ptr is not used after loop\n-    Register c0 = r19;\n-    Register c1 = r20;\n-    Register c2 = r21;\n-    Register c3 = r22;\n-    Register c4 = r23;\n-    Register c5 = r24;\n-    Register c6 = r25;\n-    Register c7 = r26;\n-    Register c8 = r27;\n-    Register c9 = r28;\n-\n-    __ pop(callee_saved, sp); \/\/the callee saved registers overlap exactly with the carry values\n-\n+      __ ldr(a_i, __ post(a, 8));\n+      __ ldr(c_i, c_ptr); \/\/Load prior c_i\n+\n+      \/\/ Limb 0\n+      __ umulh(high, a_i, b_0);\n+      __ mul(low, a_i, b_0);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ add(low, low, c_i);\n+      __ ldr(c_i, Address(c_ptr, 8));\n+      __ andr(n, low, limb_mask);\n+      __ umulh(mod_high, n, mod_0);\n+      __ mul(mod_low, n, mod_0);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ lsr(tmp, low, shift2);\n+      __ add(c_i, c_i, tmp);\n+      __ add(c_i, c_i, high);\n+\n+      \/\/ Limb 1\n+      __ umulh(high, a_i, b_1);\n+      __ mul(low, a_i, b_1);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ umulh(mod_high, n, mod_1);\n+      __ mul(mod_low, n, mod_1);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ ldr(tmp, Address(c_ptr, 16));\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, c_ptr);\n+      __ add(c_i, tmp, high);\n+\n+      \/\/ Limb 2\n+      __ umulh(high, a_i, b_2);\n+      __ mul(low, a_i, b_2);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ ldr(tmp, Address(c_ptr, 24));\n+      __ andr(low, low, limb_mask);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, Address(c_ptr, 8));\n+      __ add(c_i, tmp, high);\n+\n+      \/\/ Limb 3\n+      __ umulh(high, a_i, b_3);\n+      __ mul(low, a_i, b_3);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ umulh(mod_high, n, mod_3);\n+      __ mul(mod_low, n, mod_3);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ ldr(tmp, Address(c_ptr, 32));\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, Address(c_ptr, 16));\n+      __ add(c_i, tmp, high);\n+\n+      \/\/ Limb 4\n+      __ umulh(high, a_i, b_4);\n+      __ mul(low, a_i, b_4);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ umulh(mod_high, n, mod_4);\n+      __ mul(mod_low, n, mod_4);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, Address(c_ptr, 24));\n+      __ str(high, Address(c_ptr, 32));\n+    }\n+\n+    Register low_1 = r21;\n+    Register high_1 = r22;\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ a[3]\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    __ ldr(low_1, Address(sp));\n+    __ ldr(high_1, Address(sp, 16));\n+\n+    __ ldr(low, Address(sp, 8));\n+    __ ldr(high, Address(sp, 24));\n+    __ ldr(a_i, __ post(a, 8));\n+    __ ldr(c_i, c_ptr);\n+\n+    \/\/ Limb 1\n+    __ add(low_1, low_1, c_i);\n+    __ ldr(c_i, Address(c_ptr, 8));\n+    __ andr(n, low_1, limb_mask);\n+    __ umulh(mod_high, n, mod_0);\n+    __ mul(mod_low, n, mod_0);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+    __ lsr(tmp, low_1, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high_1);\n+\n+    \/\/ Limb 2\n+    __ ldr(low_1, Address(sp, 32));\n+    __ ldr(high_1, Address(sp, 48));\n+    __ umulh(mod_high, n, mod_1);\n+    __ mul(mod_low, n, mod_1);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ ldr(tmp, Address(c_ptr, 16));\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, c_ptr);\n+    __ add(c_i, tmp, high);\n+\n+    \/\/ Limb 2\n+    __ ldr(low, Address(sp, 40));\n+    __ ldr(high, Address(sp, 56));\n+    __ ldr(tmp, Address(c_ptr, 24));\n+    __ add(c_i, c_i, low_1);\n+    __ str(c_i, Address(c_ptr, 8));\n+    __ add(c_i, tmp, high_1);\n+\n+    \/\/ Limb 3\n+    __ umulh(mod_high, n, mod_3);\n+    __ mul(mod_low, n, mod_3);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ ldr(tmp, Address(c_ptr, 32));\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 16));\n+    __ add(c_i, tmp, high);\n+\n+    \/\/ Limb 4\n+    __ ldr(low, Address(sp, 64));\n+    __ ldr(high, Address(sp, 80));\n+    __ umulh(high_1, a_i, b_4);\n+    __ mul(low_1, a_i, b_4);\n+    __ lsl(high_1, high_1, shift1);\n+    __ lsr(tmp, low_1, shift2);\n+    __ orr(high_1, high_1, tmp);\n+    __ andr(low_1, low_1, limb_mask);\n+    __ umulh(mod_high, n, mod_4);\n+    __ mul(mod_low, n, mod_4);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+    __ add(c_i, c_i, low_1);\n+    __ str(c_i, Address(c_ptr, 24));\n+    __ str(high_1, Address(c_ptr, 32));\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ a[4]\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    __ ldr(a_i, a);\n+    __ ldr(c_i, c_ptr);\n+\n+    \/\/ Limb 0\n+    __ ldr(low_1, Address(sp, 72));\n+    __ ldr(high_1, Address(sp, 88));\n+\n+    __ add(low, low, c_i);\n+    __ ldr(c_i, Address(c_ptr, 8));\n+    __ andr(n, low, limb_mask);\n+    __ umulh(mod_high, n, mod_0);\n+    __ mul(mod_low, n, mod_0);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ lsr(tmp, low, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high);\n+\n+    \/\/ Limb 1\n+    __ ldr(low, Address(sp, 96));\n+    __ ldr(high, Address(sp, 112));\n+    __ umulh(mod_high, n, mod_1);\n+    __ mul(mod_low, n, mod_1);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+\n+    Register c5 = r11; \/\/replace b_0\n+    __ add(c5, c_i, low_1);\n+    __ ldr(c_i, Address(c_ptr, 16));\n+    __ lsr(tmp, c5, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high_1);\n+\n+    \/\/ Limb 2\n+    __ ldr(low_1, Address(sp, 104));\n+    __ ldr(high_1, Address(sp, 120));\n+    Register c6 = r12;\n+    __ add(c6, c_i, low);\n+    __ ldr(c_i, Address(c_ptr, 24));\n+    __ lsr(tmp, c6, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high);\n+\n+    \/\/ Limb 3\n+    __ umulh(mod_high, n, mod_3);\n+    __ mul(mod_low, n, mod_3);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+\n+    Register c7 = r13;\n+    __ add(c7, c_i, low_1);\n+    __ ldr(c_i, Address(c_ptr, 32));\n+    __ lsr(tmp, c7, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high_1);\n+\n+    \/\/ Limb 4\n+    __ umulh(high, a_i, b_4);\n+    __ mul(low, a_i, b_4);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+    __ umulh(mod_high, n, mod_4);\n+    __ mul(mod_low, n, mod_4);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ Final carry propagate\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    \/\/ c5 += d1 + dd0 + (d0 >>> BITS_PER_LIMB);\n@@ -7442,8 +7715,5 @@\n-    __ lsr(tmp, c5, shift2);\n-    __ add(c6, c6, tmp);\n-    __ lsr(tmp, c6, shift2);\n-    __ add(c7, c7, tmp);\n-    __ lsr(tmp, c7, shift2);\n-    __ add(c8, c8, tmp);\n-    __ lsr(tmp, c8, shift2);\n-    __ add(c9, c9, tmp);\n+    Register c8 = r14;\n+    Register c9 = r15;\n+    __ add(c8, c_i, low);\n+    __ lsr(c9, c8, shift2);\n+    __ add(c9, c9, high);\n@@ -7451,4 +7721,4 @@\n-    __ andr(c5, c5, limb_mask_scalar);\n-    __ andr(c6, c6, limb_mask_scalar);\n-    __ andr(c7, c7, limb_mask_scalar);\n-    __ andr(c8, c8, limb_mask_scalar);\n+    __ andr(c5, c5, limb_mask);\n+    __ andr(c6, c6, limb_mask);\n+    __ andr(c7, c7, limb_mask);\n+    __ andr(c8, c8, limb_mask);\n@@ -7465,27 +7735,24 @@\n-    __ ldr(mod_j, Address(mod_ptr));\n-    __ sub(c0, c5, mod_j);\n-\n-    __ ldr(mod_j, Address(mod_ptr, 8));\n-    __ sub(c1, c6, mod_j);\n-    __ asr(tmp, c0, shift2);\n-    __ add(c1, c1, tmp);\n-\n-    \/\/ Modulus[2] is zero\n-    __ asr(c2, c1, shift2);\n-    __ add(c2, c2, c7);\n-\n-    __ ldr(mod_j, Address(mod_ptr, 16));\n-    __ sub(c3, c8, mod_j);\n-    __ asr(tmp, c2, shift2);\n-    __ add(c3, c3, tmp);\n-\n-    __ ldr(mod_j, Address(mod_ptr, 24));\n-    __ sub(c4, c9, mod_j);\n-    __ asr(tmp, c3, shift2);\n-    __ add(c4, c4, tmp);\n-\n-    \/\/ Apply limb mask\n-    __ andr(c0, c0, limb_mask_scalar);\n-    __ andr(c1, c1, limb_mask_scalar);\n-    __ andr(c2, c2, limb_mask_scalar);\n-    __ andr(c3, c3, limb_mask_scalar);\n+\n+    Register c0 = r19;\n+    Register c1 = r20;\n+    Register c2 = r21;\n+    Register c3 = r22;\n+    Register c4 = r23;\n+    Register tmp0 = r24;\n+    Register tmp1 = r25;\n+    Register tmp2 = r26;\n+    Register tmp3 = r27;\n+    Register tmp4 = r28;\n+\n+    __ sub(c0, c5, mod_0);\n+    __ sub(c1, c6, mod_1);\n+    __ sub(c3, c8, mod_3);\n+    __ sub(c4, c9, mod_4);\n+    __ add(c1, c1, c0, Assembler::ASR, shift2);\n+    __ andr(c0, c0, limb_mask);\n+    __ add(c2, c7, c1, Assembler::ASR, shift2);\n+    __ andr(c1, c1, limb_mask);\n+    __ add(c3, c3, c2, Assembler::ASR, shift2);\n+    __ andr(c2, c2, limb_mask);\n+    __ add(c4, c4, c3, Assembler::ASR, shift2);\n+    __ andr(c3, c3, limb_mask);\n@@ -7501,9 +7768,2 @@\n-    Register res_0 = r11;\n-    Register res_1 = r12;\n-    Register res_2 = r13;\n-    Register res_3 = r14;\n-    Register res_4 = r15;\n-    Register mask = r7;\n-    Register nmask = r10;\n-\n-    RegSet res = RegSet::range(r11, r15);\n+    Register mask = r5;\n+    Register nmask = r6;\n@@ -7513,2 +7773,1 @@\n-\n-    __ andr(res_0, c5, mask);\n+    __ andr(c5, c5, mask);\n@@ -7516,3 +7775,2 @@\n-    __ orr(res_0, res_0, tmp);\n-\n-    __ andr(res_1, c6, mask);\n+    __ orr(c5, c5, tmp);\n+    __ andr(c6, c6, mask);\n@@ -7520,3 +7778,2 @@\n-    __ orr(res_1, res_1, tmp);\n-\n-    __ andr(res_2, c7, mask);\n+    __ orr(c6, c6, tmp);\n+    __ andr(c7, c7, mask);\n@@ -7524,3 +7781,2 @@\n-    __ orr(res_2, res_2, tmp);\n-\n-    __ andr(res_3, c8, mask);\n+    __ orr(c7, c7, tmp);\n+    __ andr(c8, c8, mask);\n@@ -7528,3 +7784,2 @@\n-    __ orr(res_3, res_3, tmp);\n-\n-    __ andr(res_4, c9, mask);\n+    __ orr(c8, c8, tmp);\n+    __ andr(c9, c9, mask);\n@@ -7532,1 +7787,1 @@\n-    __ orr(res_4, res_4, tmp);\n+    __ orr(c9, c9, tmp);\n@@ -7534,5 +7789,5 @@\n-    __ str(res_0, result);\n-    __ str(res_1, Address(result, 8));\n-    __ str(res_2, Address(result, 16));\n-    __ str(res_3, Address(result, 24));\n-    __ str(res_4, Address(result, 32));\n+    __ str(c5, result);\n+    __ str(c6, Address(result, 8));\n+    __ str(c7, Address(result, 16));\n+    __ str(c8, Address(result, 24));\n+    __ str(c9, Address(result, 32));\n@@ -7541,0 +7796,1 @@\n+    __ add(sp, sp, 176);\n@@ -7542,1 +7798,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ leave();\n@@ -7549,0 +7805,322 @@\n+  address generate_intpoly_assign() {\n+    \/\/ KNOWN Lengths:\n+    \/\/   MontgomeryIntPolynP256:  5 = 4 + 1\n+    \/\/   IntegerPolynomial1305:   5 = 4 + 1\n+    \/\/   IntegerPolynomial25519: 10 = 8 + 2\n+    \/\/   IntegerPolynomialP256:  10 = 8 + 2\n+    \/\/   Curve25519OrderField:   10 = 8 + 2\n+    \/\/   Curve25519OrderField:   10 = 8 + 2\n+    \/\/   P256OrderField:         10 = 8 + 2\n+    \/\/   IntegerPolynomialP384:  14 = 8 + 4 + 2\n+    \/\/   P384OrderField:         14 = 8 + 4 + 2\n+    \/\/   IntegerPolynomial448:   16 = 8 + 8\n+    \/\/   Curve448OrderField:     16 = 8 + 8\n+    \/\/   Curve448OrderField:     16 = 8 + 8\n+    \/\/   IntegerPolynomialP521:  19 = 8 + 8 + 2 + 1\n+    \/\/   P521OrderField:         19 = 8 + 8 + 2 + 1\n+    \/\/ Special Cases 5, 10, 14, 16, 19\n+\n+    __ align(CodeEntryAlignment);\n+    StubId stub_id = StubId::stubgen_intpoly_assign_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    \/\/ Inputs\n+    const Register set = c_rarg0;\n+    const Register aLimbs = c_rarg1;\n+    const Register bLimbs = c_rarg2;\n+    const Register length = c_rarg3;\n+\n+    Label L_Length5, L_Length10, L_Length14, L_Length16, L_Length19, L_DefaultLoop, L_Done;\n+\n+    \/*\n+    int maskValue = -set;\n+    for (int i = 0; i < a.length; i++) {\n+        long dummyLimbs = maskValue & (a[i] ^ b[i]);\n+        a[i] = dummyLimbs ^ a[i];\n+    }\n+    *\/\n+    Register mask_scalar = r4;\n+    FloatRegister mask_vec = v0;\n+\n+    __ neg(mask_scalar, set);\n+    __ dup(mask_vec, __ T2D, mask_scalar);\n+\n+    __ push(r19, sp); \/\/needed for length = 5\n+\n+    __ cmp(length, (u1)5);\n+    __ br(Assembler::EQ, L_Length5);\n+    __ cmp(length, (u1)10);\n+    __ br(Assembler::EQ, L_Length10);\n+    __ cmp(length, (u1)14);\n+    __ br(Assembler::EQ, L_Length14);\n+    __ cmp(length, (u1)16);\n+    __ br(Assembler::EQ, L_Length16);\n+    __ cmp(length, (u1)19);\n+    __ br(Assembler::EQ, L_Length19);\n+\n+    \/\/ Length = 5\n+    \/\/ Use 5 GPRs (neon not faster with this few limbs)\n+    __ BIND(L_Length5);\n+    {\n+      Register a0 = r5;\n+      Register a1 = r6;\n+      Register a2 = r7;\n+      Register a3 = r10;\n+      Register a4 = r11;\n+      Register b0 = r12;\n+      Register b1 = r13;\n+      Register b2 = r14;\n+      Register b3 = r15;\n+      Register b4 = r19;\n+\n+      __ ldr(a0, aLimbs);\n+      __ ldr(a1, Address(aLimbs, 8));\n+      __ ldr(a2, Address(aLimbs, 16));\n+      __ ldr(a3, Address(aLimbs, 24));\n+      __ ldr(a4, Address(aLimbs, 32));\n+\n+      __ ldr(b0, bLimbs);\n+      __ ldr(b1, Address(bLimbs, 8));\n+      __ ldr(b2, Address(bLimbs, 16));\n+      __ ldr(b3, Address(bLimbs, 24));\n+      __ ldr(b4, Address(bLimbs, 32));\n+\n+      __ eor(b0, b0, a0);\n+      __ eor(b1, b1, a1);\n+      __ eor(b2, b2, a2);\n+      __ eor(b3, b3, a3);\n+      __ eor(b4, b4, a4);\n+\n+      __ andr(b0, b0, mask_scalar);\n+      __ andr(b1, b1, mask_scalar);\n+      __ andr(b2, b2, mask_scalar);\n+      __ andr(b3, b3, mask_scalar);\n+      __ andr(b4, b4, mask_scalar);\n+\n+      __ eor(a0, a0, b0);\n+      __ eor(a1, a1, b1);\n+      __ eor(a2, a2, b2);\n+      __ eor(a3, a3, b3);\n+      __ eor(a4, a4, b4);\n+\n+      __ str(a0, aLimbs);\n+      __ str(a1, Address(aLimbs, 8));\n+      __ str(a2, Address(aLimbs, 16));\n+      __ str(a3, Address(aLimbs, 24));\n+      __ str(a4, Address(aLimbs, 32));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 10\n+    \/\/ Split into 4 neon regs and 2 GPRs\n+    __ BIND(L_Length10);\n+    {\n+      Register a9 = r10;\n+      Register a10 = r11;\n+      Register b9 = r12;\n+      Register b10 = r13;\n+\n+      VSeq<4> a_vec(16);\n+      VSeq<4> b_vec(20);\n+\n+      __ ldr(a9, Address(aLimbs, 64));\n+      __ ldr(a10, Address(aLimbs, 72));\n+      __ ldr(b9, Address(bLimbs, 64));\n+      __ ldr(b10, Address(bLimbs, 72));\n+\n+      __ ldpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ ldpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+\n+      __ eor(b9, b9, a9);\n+      __ eor(b10, b10, a10);\n+\n+      __ ldpq(b_vec[0], b_vec[1], Address(bLimbs));\n+      __ ldpq(b_vec[2], b_vec[3], Address(bLimbs, 32));\n+\n+      __ andr(b9, b9, mask_scalar);\n+      __ andr(b10, b10, mask_scalar);\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+\n+      __ eor(a9, a9, b9);\n+      __ eor(a10, a10, b10);\n+\n+      vs_andr(b_vec, b_vec, mask_vec);\n+\n+      __ str(a9, Address(aLimbs, 64));\n+      __ str(a10, Address(aLimbs, 72));\n+\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ stpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ stpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 14\n+    \/\/ Split into 5 neon regs and 4 GPRs\n+    __ BIND(L_Length14);\n+    {\n+      Register a10 = r5;\n+      Register a11 = r6;\n+      Register a12 = r7;\n+      Register a13 = r8;\n+      Register b10 = r9;\n+      Register b11 = r10;\n+      Register b12 = r11;\n+      Register b13 = r12;\n+\n+      VSeq<5> a_vec(16);\n+      VSeq<5> b_vec(22);\n+\n+      __ ldr(a10, Address(aLimbs, 80));\n+      __ ldr(a11, Address(aLimbs, 88));\n+      __ ldr(a12, Address(aLimbs, 96));\n+      __ ldr(a13, Address(aLimbs, 104));\n+\n+      __ ldr(b10, Address(bLimbs, 80));\n+      __ ldr(b11, Address(bLimbs, 88));\n+      __ ldr(b12, Address(bLimbs, 96));\n+      __ ldr(b13, Address(bLimbs, 104));\n+\n+      __ ld1(a_vec[0], __ T2D, aLimbs);\n+      __ ldpq(a_vec[1], a_vec[2], Address(aLimbs, 16));\n+      __ ldpq(a_vec[3], a_vec[4], Address(aLimbs, 48));\n+\n+      __ eor(b10, b10, a10);\n+      __ eor(b11, b11, a11);\n+      __ eor(b12, b12, a12);\n+      __ eor(b13, b13, a13);\n+\n+      __ ld1(b_vec[0], __ T2D, bLimbs);\n+      __ ldpq(b_vec[1], b_vec[2], Address(bLimbs, 16));\n+      __ ldpq(b_vec[3], b_vec[4], Address(bLimbs, 48));\n+\n+      __ andr(b10, b10, mask_scalar);\n+      __ andr(b11, b11, mask_scalar);\n+      __ andr(b12, b12, mask_scalar);\n+      __ andr(b13, b13, mask_scalar);\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+\n+      __ eor(a10, a10, b10);\n+      __ eor(a11, a11, b11);\n+      __ eor(a12, a12, b12);\n+      __ eor(a13, a13, b13);\n+\n+      vs_andr(b_vec, b_vec, mask_vec);\n+\n+      __ str(a10, Address(aLimbs, 80));\n+      __ str(a11, Address(aLimbs, 88));\n+      __ str(a12, Address(aLimbs, 96));\n+      __ str(a13, Address(aLimbs, 104));\n+\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ st1(a_vec[0], __ T2D, aLimbs);\n+      __ stpq(a_vec[1], a_vec[2], Address(aLimbs, 16));\n+      __ stpq(a_vec[3], a_vec[4], Address(aLimbs, 48));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 16\n+    \/\/ Use 8 neon regs\n+    __ BIND(L_Length16);\n+    {\n+      VSeq<8> a_vec(16);\n+      VSeq<8> b_vec(24);\n+\n+      __ ldpq(a_vec[0], a_vec[1], aLimbs);\n+      __ ldpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ ldpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ ldpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+\n+      __ ldpq(b_vec[0], b_vec[1], bLimbs);\n+      __ ldpq(b_vec[2], b_vec[3], Address(bLimbs, 32));\n+      __ ldpq(b_vec[4], b_vec[5], Address(bLimbs, 64));\n+      __ ldpq(b_vec[6], b_vec[7], Address(bLimbs, 96));\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+      vs_andr(b_vec, b_vec, mask_vec);\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ stpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ stpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ stpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ stpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 19\n+    \/\/ Split into 8 neon regs and 3 GPRs\n+    __ BIND(L_Length19);\n+    {\n+      Register a17 = r10;\n+      Register a18 = r11;\n+      Register a19 = r12;\n+      Register b17 = r13;\n+      Register b18 = r14;\n+      Register b19 = r15;\n+\n+      VSeq<8> a_vec(16);\n+      VSeq<8> b_vec(24);\n+\n+      __ ldr(a17, Address(aLimbs, 128));\n+      __ ldr(a18, Address(aLimbs, 136));\n+      __ ldr(a19, Address(aLimbs, 144));\n+      __ ldr(b17, Address(bLimbs, 128));\n+      __ ldr(b18, Address(bLimbs, 136));\n+      __ ldr(b19, Address(bLimbs, 144));\n+\n+      __ ldpq(a_vec[0], a_vec[1], aLimbs);\n+      __ ldpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ ldpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ ldpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+\n+      __ eor(b17, b17, a17);\n+      __ eor(b18, b18, a18);\n+      __ eor(b19, b19, a19);\n+\n+      __ ldpq(b_vec[0], b_vec[1], bLimbs);\n+      __ ldpq(b_vec[2], b_vec[3], Address(bLimbs, 32));\n+      __ ldpq(b_vec[4], b_vec[5], Address(bLimbs, 64));\n+      __ ldpq(b_vec[6], b_vec[7], Address(bLimbs, 96));\n+\n+      __ andr(b17, b17, mask_scalar);\n+      __ andr(b18, b18, mask_scalar);\n+      __ andr(b19, b19, mask_scalar);\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+\n+      __ eor(a17, a17, b17);\n+      __ eor(a18, a18, b18);\n+      __ eor(a19, a19, b19);\n+\n+      vs_andr(b_vec, b_vec, mask_vec);\n+\n+      __ str(a17, Address(aLimbs, 128));\n+      __ str(a18, Address(aLimbs, 136));\n+      __ str(a19, Address(aLimbs, 144));\n+\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ stpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ stpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ stpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ stpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+    }\n+\n+    __ BIND(L_Done);\n+    __ pop(r19, sp);\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+    return start;\n+  }\n+\n@@ -12291,0 +12869,1 @@\n+      StubRoutines::_intpoly_assign = generate_intpoly_assign();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":913,"deletions":334,"binary":false,"changes":1247,"status":"modified"}]}