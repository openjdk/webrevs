{"files":[{"patch":"@@ -3153,0 +3153,14 @@\n+  \/\/Vector by element variant of UMULL\n+  void _umullv(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,\n+                SIMD_Arrangement Tb, FloatRegister Vm, SIMD_RegVariant Ts, int lane) {\n+    starti;\n+    int size = (Ta == T4S) ? 0b01 : 0b10;\n+    int q = (Tb == T4H || Tb == T2S) ? 0 : 1;\n+    int h = (size == 0b01) ? ((lane >> 2) & 1) : ((lane >> 1) & 1);\n+    int l = (size == 0b01) ? ((lane >> 1) & 1) : (lane & 1);\n+    assert(size == 0b10 ? lane < 4 : lane < 8, \"umullv assumes lane < 4 when using half-words and lane < 8 otherwise\");\n+    assert(Ts == H ? Vm->encoding() < 16 : Vm->encoding() < 32, \"umullv requires Vm to be in range V0..V15 when Ts is H\");\n+    f(0, 31), f(q, 30), f(0b101111, 29, 24), f(size, 23, 22), f(l, 21); \/\/f(m, 20);\n+    rf(Vm, 16), f(0b1010, 15, 12), f(h, 11), f(0, 10), rf(Vn, 5), rf(Vd, 0);\n+  }\n+\n@@ -3164,0 +3178,15 @@\n+  \/\/Vector by element variant of UMULL\n+  void umullv(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,\n+               SIMD_Arrangement Tb, FloatRegister Vm, SIMD_RegVariant Ts, int lane) {\n+    assert(Ta == T4S || Ta == T2D, \"umullv destination register must have arrangement T4S or T2D\");\n+    assert(Ta == T4S ? (Tb == T4H && Ts == H) : (Tb == T2S && Ts == S), \"umullv register arrangements must adhere to spec\");\n+    _umullv(Vd, Ta, Vn, Tb, Vm, Ts, lane);\n+  }\n+\n+  void umull2v(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,\n+               SIMD_Arrangement Tb, FloatRegister Vm, SIMD_RegVariant Ts, int lane) {\n+    assert(Ta == T4S || Ta == T2D, \"umullv destination register must have arrangement T4S or T2D\");\n+    assert(Ta == T4S ? (Tb == T8H && Ts == H) : (Tb == T4S && Ts == S), \"umullv register arrangements must adhere to spec\");\n+    _umullv(Vd, Ta, Vn, Tb, Vm, Ts, lane);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-  do_arch_blob(compiler, 70000)                                         \\\n+  do_arch_blob(compiler, 75000)                                         \\\n","filename":"src\/hotspot\/cpu\/aarch64\/stubDeclarations_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -7143,0 +7143,406 @@\n+  \/\/ P256 Montgomery Multiplication.\n+  \/\/ Implements the method protected void mult(long[] a, long[] b, long[] r) {}\n+  \/\/ of the sun.security.util.math.intpoly.MontgomeryIntegerPolynomialP256 class\n+  \/\/\n+  \/\/ a (long[5]) = c_rarg0\n+  \/\/ b (long[5]) = c_rarg1\n+  \/\/ r (long[5]) = c_rarg2\n+  \/\/\n+  \/\/ Note that each arg represents a 256-bit integer broken into 52-bit limbs\n+  address generate_intpoly_montgomeryMult_P256() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubId stub_id = StubId::stubgen_intpoly_montgomeryMult_P256_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    const Register a = c_rarg0;\n+    const Register b = c_rarg1;\n+    const Register result = c_rarg2;\n+\n+    \/\/Omit 3rd limb of modulus since it is 0\n+    static const int64_t modulus[5] = {\n+      0x000fffffffffffffL, 0x00000fffffffffffL,\n+      0x0000001000000000L, 0x0000ffffffff0000L\n+    };\n+\n+    int shift1 = 12; \/\/ 64 - bits per limb\n+    int shift2 = 52; \/\/ bits per limb\n+\n+    \/\/ GPRs that are used throughout loop\n+    Register b_j = r3;\n+    Register mod_ptr = r4;\n+    Register limb_mask_scalar = r5;\n+    Register c_ptr = r6;\n+\n+    \/\/ These neon registers remain constant through the main loop\n+    FloatRegister limb_mask = v0;\n+    FloatRegister mask_32_vec = v1;\n+    FloatRegister b_lows = v2;\n+    FloatRegister b_highs = v3;\n+    FloatRegister mod_lows = v4;\n+    FloatRegister mod_highs = v5;\n+\n+    \/\/ Push callee saved registers on to the stack\n+    RegSet callee_saved = RegSet::range(r19, r28);\n+    __ push(callee_saved, sp);\n+\n+    \/\/ Allocate space on the stack for carry values and zero memory\n+    __ sub(sp, sp, 80);\n+    __ mov(c_ptr, sp);\n+    __ eor(b_j, b_j, b_j); \/\/Create a 0 reg to clear memory\n+    for (int i = 0; i < 10; i++) {\n+      __ str(b_j, Address(sp, i * 8));\n+    }\n+\n+    \/\/ Calculate limb mask\n+      __ mov(limb_mask_scalar, -UCONST64(1) >> (64 - shift2));\n+      __ dup(limb_mask, __ T2D, limb_mask_scalar);\n+\n+    \/\/ Calculate 32-bit mask\n+    {\n+      Register mask_32 = r7;\n+      __ mov(mask_32, (UCONST64(1) << 32) - 1);\n+      __ dup(mask_32_vec, __ T2D, mask_32);\n+    }\n+\n+    \/\/ Load modulus and input array b\n+    __ lea(mod_ptr, ExternalAddress((address)modulus));\n+    __ ld2(b_lows, b_highs, __ T4S, Address(b));\n+    __ ld2(mod_lows, mod_highs, __ T4S, Address(mod_ptr));\n+    __ ldr(b_j, Address(b, 32));\n+\n+    for (int i = 0; i < 5; i++) {\n+      Register c_idx = r10;\n+      Register mul_tmp = r11;\n+      Register scalar_ai = r12;\n+\n+      FloatRegister A = v6;\n+      FloatRegister B = v7;\n+      FloatRegister C = v8;\n+      FloatRegister D = v16;\n+      FloatRegister a_i = v17;\n+      FloatRegister n = v18;\n+      FloatRegister middle = v19;\n+      FloatRegister tmp = v20;\n+      FloatRegister modmul_low = v21;\n+      FloatRegister modmul_high = v22;\n+      FloatRegister c_01 = v23;\n+      FloatRegister c_23 = v24;\n+      FloatRegister low_34 = v25;\n+      FloatRegister low_01 = v26;\n+      FloatRegister low_23 = v27;\n+      FloatRegister low_4x = v28;\n+      FloatRegister high_01 = v29;\n+      FloatRegister high_23 = v30;\n+      FloatRegister high_4x = v31;\n+\n+      \/\/ Load a_i and increment by 8 bytes\n+      __ ldr(scalar_ai, a);\n+      __ ld1(a_i, __ D, 0, __ post(a, 8));\n+\n+      \/\/ Start computing final multiply with GPR since it is not\n+      \/\/ worth it to vectorize a single mult\n+      __ mul(mul_tmp, scalar_ai, b_j);\n+      __ mov(low_4x, Assembler::D, 0, mul_tmp);\n+      __ umulh(mul_tmp, scalar_ai, b_j);\n+      __ mov(high_4x, Assembler::D, 0, mul_tmp);\n+\n+      \/\/ Iterate through b, multiplying each limb by a_i\n+      \/\/ storing low and high parts in separate vectors.\n+      \/\/ Compute high[i] = high[i] << shift1 | (low[i] >>> shift2)\n+      \/\/ and low[i] &= LIMB_MASK\n+\n+      \/\/ Calculus low_01 and high_01\n+      __ umullv(A, __ T2D, b_lows, __ T2S, a_i, __ S, 0);\n+      __ umullv(B, __ T2D, b_highs, __ T2S, a_i, __ S, 0);\n+      __ umullv(C, __ T2D, b_lows, __ T2S, a_i, __ S, 1);\n+      __ umullv(D, __ T2D, b_highs, __ T2S, a_i, __ S, 1);\n+\n+      __ andr(middle, __ T16B, B, mask_32_vec);\n+      __ ushr(tmp, __ T2D, A, 32);\n+      __ addv(middle, __ T2D, middle, tmp);\n+      __ addv(middle, __ T2D, middle, C);\n+\n+      __ shl(low_01, __ T2D, middle, 32);\n+      __ andr(tmp, __ T16B, A, mask_32_vec);\n+      __ orr(low_01, __ T16B, low_01, tmp);\n+\n+      __ ushr(high_01, __ T2D, middle, 32);\n+      __ addv(high_01, __ T2D, high_01, D);\n+      __ ushr(tmp, __ T2D, B, 32);\n+      __ addv(high_01, __ T2D, high_01, tmp);\n+\n+      __ shl(high_01, __ T2D, high_01, shift1);\n+      __ ushr(tmp, __ T2D, low_01, shift2);\n+      __ orr(high_01, __ T16B, high_01, tmp);\n+      __ andr(low_01, __ T16B, low_01, limb_mask);\n+\n+      \/\/ Calculate low_23 and high_23\n+      __ umull2v(A, __ T2D, b_lows, __ T4S, a_i, __ S, 0);\n+      __ umull2v(B, __ T2D, b_highs, __ T4S, a_i, __ S, 0);\n+      __ umull2v(C, __ T2D, b_lows, __ T4S, a_i, __ S, 1);\n+      __ umull2v(D, __ T2D, b_highs, __ T4S, a_i, __ S, 1);\n+\n+      __ andr(middle, __ T16B, B, mask_32_vec);\n+      __ ushr(tmp, __ T2D, A, 32);\n+      __ addv(middle, __ T2D, middle, tmp);\n+      __ addv(middle, __ T2D, middle, C);\n+\n+      __ shl(low_23, __ T2D, middle, 32);\n+      __ andr(tmp, __ T16B, A, mask_32_vec);\n+      __ orr(low_23, __ T16B, low_23, tmp);\n+\n+      __ ushr(high_23, __ T2D, middle, 32);\n+      __ addv(high_23, __ T2D, high_23, D);\n+      __ ushr(tmp, __ T2D, B, 32);\n+      __ addv(high_23, __ T2D, high_23, tmp);\n+\n+      __ shl(high_23, __ T2D, high_23, shift1);\n+      __ ushr(tmp, __ T2D, low_23, shift2);\n+      __ orr(high_23, __ T16B, high_23, tmp);\n+      __ andr(low_23, __ T16B, low_23, limb_mask);\n+\n+      \/\/ Finish computing high_4x\n+      __ shl(high_4x, __ T2D, high_4x, shift1);\n+      __ ushr(tmp, __ T2D, low_4x, shift2);\n+      __ orr(high_4x, __ T16B, high_4x, tmp);\n+      __ andr(low_4x, __ T16B, low_4x, limb_mask);\n+\n+      \/\/ low_0 += c_i\n+      \/\/ n = low_0 & limb_mask\n+      __ eor(c_01, __ T16B, c_01, c_01);\n+      __ ld1(c_01, __ D, 0, c_ptr);\n+      __ addv(low_01, __ T2D, low_01, c_01);\n+      __ andr(n, __ T16B, low_01, limb_mask);\n+\n+      \/\/ Iterate through the modulus, multiplying each limb by n and\n+      \/\/ storing low and high parts in separate vectors.\n+      \/\/ Compute high += modmul_high << shift1 | (modmul_low >>> shift2);\n+      \/\/ and low += modmul_low & LIMB_MASK\n+\n+      \/\/ Calculate modmul_low and modmul_high for modulus[0] and modulus[1]\n+      __ umullv(A, __ T2D, mod_lows, __ T2S, n, __ S, 0);\n+      __ umullv(B, __ T2D, mod_highs, __ T2S, n, __ S, 0);\n+      __ umullv(C, __ T2D, mod_lows, __ T2S, n, __ S, 1);\n+      __ umullv(D, __ T2D, mod_highs, __ T2S, n, __ S, 1);\n+\n+      __ andr(middle, __ T16B, B, mask_32_vec);\n+      __ ushr(tmp, __ T2D, A, 32);\n+      __ addv(middle, __ T2D, middle, tmp);\n+      __ addv(middle, __ T2D, middle, C);\n+\n+      __ shl(modmul_low, __ T2D, middle, 32);\n+      __ andr(tmp, __ T16B, A, mask_32_vec);\n+      __ orr(modmul_low, __ T16B, modmul_low, tmp);\n+\n+      __ ushr(modmul_high, __ T2D, middle, 32);\n+      __ addv(modmul_high, __ T2D, modmul_high, D);\n+      __ ushr(tmp, __ T2D, B, 32);\n+      __ addv(modmul_high, __ T2D, modmul_high, tmp);\n+\n+      __ shl(modmul_high, __ T2D, modmul_high, shift1);\n+      __ ushr(tmp, __ T2D, modmul_low, shift2);\n+      __ orr(modmul_high, __ T16B, modmul_high, tmp);\n+      __ addv(high_01, __ T2D, high_01, modmul_high);\n+      __ andr(modmul_low, __ T16B, modmul_low, limb_mask);\n+      __ addv(low_01, __ T2D, low_01, modmul_low);\n+\n+      \/\/ Calculate modmul_low and modmul_high for modulus[3] and modulus[4].\n+      \/\/ Can omit modulus[2] since it is 0\n+      __ umull2v(A, __ T2D, mod_lows, __ T4S, n, __ S, 0);\n+      __ umull2v(B, __ T2D, mod_highs, __ T4S, n, __ S, 0);\n+      __ umull2v(C, __ T2D, mod_lows, __ T4S, n, __ S, 1);\n+      __ umull2v(D, __ T2D, mod_highs, __ T4S, n, __ S, 1);\n+\n+      __ andr(middle, __ T16B, B, mask_32_vec);\n+      __ ushr(tmp, __ T2D, A, 32);\n+      __ addv(middle, __ T2D, middle, tmp);\n+      __ addv(middle, __ T2D, middle, C);\n+\n+      __ shl(modmul_low, __ T2D, middle, 32);\n+      __ andr(tmp, __ T16B, A, mask_32_vec);\n+      __ orr(modmul_low, __ T16B, modmul_low, tmp);\n+\n+      __ ushr(modmul_high, __ T2D, middle, 32);\n+      __ addv(modmul_high, __ T2D, modmul_high, D);\n+      __ ushr(tmp, __ T2D, B, 32);\n+      __ addv(modmul_high, __ T2D, modmul_high, tmp);\n+\n+      __ shl(modmul_high, __ T2D, modmul_high, shift1);\n+      __ ushr(tmp, __ T2D, modmul_low, shift2);\n+      __ orr(modmul_high, __ T16B, modmul_high, tmp);\n+      __ andr(modmul_low, __ T16B, modmul_low, limb_mask);\n+\n+      \/\/Need to shift around vectors to get right layout bc of no modulus[2]\n+      __ ins(low_34, __ D, low_23, 0, 1);\n+      __ ins(low_34, __ D, low_4x, 1, 0);\n+      __ addv(low_34, __ T2D, low_34, modmul_low);\n+\n+      __ eor(tmp, __ T16B, tmp, tmp);\n+      __ ins(tmp, __ D, modmul_high, 1, 0); \/\/ tmp = [0, nn3]\n+      __ addv(high_23, __ T2D, high_23, tmp);\n+      __ ins(tmp, __ D, modmul_high, 0, 1); \/\/ tmp = [nn4, nn3]\n+      __ addv(high_4x, __ T2D, high_4x, tmp);\n+\n+      \/\/ Compute carry values\n+      \/\/ c_i+1 += low_1 + high_0 + (low_0 >>> shift2)\n+      \/\/ c_i+2 += low_2 + high_1\n+      \/\/ c_i+3 += low_3 + high_2\n+      \/\/ c_i+4 += low_4 + high_3;\n+      \/\/ c_i+5 = high_4\n+      __ add(c_ptr, c_ptr, 8);\n+      __ ld1(c_01, c_23, __ T2D, c_ptr);\n+      __ add(c_idx, c_ptr, 32);\n+      __ st1(high_4x, __ D, 0, c_idx);\n+\n+      \/\/ Add high values to c\n+      __ addv(c_01, __ T2D, c_01, high_01);\n+      __ addv(c_23, __ T2D, c_23, high_23);\n+      __ addv(c_23, __ T2D, c_23, low_34);\n+\n+      \/\/ Reorder low vectors to enable simd ops\n+      __ ins(tmp, __ D, low_01, 0, 1);\n+      __ ins(tmp, __ D, low_23, 1, 0);\n+      __ addv(c_01, __ T2D, c_01, tmp);\n+\n+      \/\/ clear tmp_4x and put low_0 in first lane\n+      \/\/ Shift low_0 and add to c_i+1\n+      __ ushr(low_01, __ T2D, low_01, shift2);\n+      __ eor(tmp, __ T16B, tmp, tmp); \/\/zero out tmp\n+      __ ins(tmp, __ D, low_01, 0, 0);\n+      __ addv(c_01, __ T2D, c_01, tmp);\n+\n+      \/\/ Write back carry values to stack\n+      __ st1(c_01, c_23, __ T2D, c_ptr);\n+    }\n+\n+    \/\/ Final carry propagate and write result\n+    Register mod_j = r3; \/\/ b_j is not used after loop\n+    Register tmp = r6; \/\/ c_ptr is not used after loop\n+    Register c0 = r19;\n+    Register c1 = r20;\n+    Register c2 = r21;\n+    Register c3 = r22;\n+    Register c4 = r23;\n+    Register c5 = r24;\n+    Register c6 = r25;\n+    Register c7 = r26;\n+    Register c8 = r27;\n+    Register c9 = r28;\n+\n+    __ pop(callee_saved, sp); \/\/the callee saved registers overlap exactly with the carry values\n+\n+    \/\/ c6 += (c5 >>> BITS_PER_LIMB);\n+    \/\/ c7 += (c6 >>> BITS_PER_LIMB);\n+    \/\/ c8 += (c7 >>> BITS_PER_LIMB);\n+    \/\/ c9 += (c8 >>> BITS_PER_LIMB);\n+\n+    __ lsr(tmp, c5, shift2);\n+    __ add(c6, c6, tmp);\n+    __ lsr(tmp, c6, shift2);\n+    __ add(c7, c7, tmp);\n+    __ lsr(tmp, c7, shift2);\n+    __ add(c8, c8, tmp);\n+    __ lsr(tmp, c8, shift2);\n+    __ add(c9, c9, tmp);\n+\n+    __ andr(c5, c5, limb_mask_scalar);\n+    __ andr(c6, c6, limb_mask_scalar);\n+    __ andr(c7, c7, limb_mask_scalar);\n+    __ andr(c8, c8, limb_mask_scalar);\n+\n+    \/\/ c0 = c5 - modulus[0];\n+    \/\/ c1 = c6 - modulus[1] + (c0 >> BITS_PER_LIMB);\n+    \/\/ c0 &= LIMB_MASK;\n+    \/\/ c2 = c7 + (c1 >> BITS_PER_LIMB);\n+    \/\/ c1 &= LIMB_MASK;\n+    \/\/ c3 = c8 - modulus[3] + (c2 >> BITS_PER_LIMB);\n+    \/\/ c2 &= LIMB_MASK;\n+    \/\/ c4 = c9 - modulus[4] + (c3 >> BITS_PER_LIMB);\n+    \/\/ c3 &= LIMB_MASK;\n+    __ ldr(mod_j, Address(mod_ptr));\n+    __ sub(c0, c5, mod_j);\n+\n+    __ ldr(mod_j, Address(mod_ptr, 8));\n+    __ sub(c1, c6, mod_j);\n+    __ asr(tmp, c0, shift2);\n+    __ add(c1, c1, tmp);\n+\n+    \/\/ Modulus[2] is zero\n+    __ asr(c2, c1, shift2);\n+    __ add(c2, c2, c7);\n+\n+    __ ldr(mod_j, Address(mod_ptr, 16));\n+    __ sub(c3, c8, mod_j);\n+    __ asr(tmp, c2, shift2);\n+    __ add(c3, c3, tmp);\n+\n+    __ ldr(mod_j, Address(mod_ptr, 24));\n+    __ sub(c4, c9, mod_j);\n+    __ asr(tmp, c3, shift2);\n+    __ add(c4, c4, tmp);\n+\n+    \/\/ Apply limb mask\n+    __ andr(c0, c0, limb_mask_scalar);\n+    __ andr(c1, c1, limb_mask_scalar);\n+    __ andr(c2, c2, limb_mask_scalar);\n+    __ andr(c3, c3, limb_mask_scalar);\n+\n+    \/\/ Final write back\n+    \/\/ mask = c4 >> 63\n+    \/\/ r[0] = ((c5 & mask) | (c0 & ~mask));\n+    \/\/ r[1] = ((c6 & mask) | (c1 & ~mask));\n+    \/\/ r[2] = ((c7 & mask) | (c2 & ~mask));\n+    \/\/ r[3] = ((c8 & mask) | (c3 & ~mask));\n+    \/\/ r[4] = ((c9 & mask) | (c4 & ~mask));\n+\n+    Register res_0 = r11;\n+    Register res_1 = r12;\n+    Register res_2 = r13;\n+    Register res_3 = r14;\n+    Register res_4 = r15;\n+    Register mask = r7;\n+    Register nmask = r10;\n+\n+    RegSet res = RegSet::range(r11, r15);\n+\n+    __ asr(mask, c4, 63);\n+    __ mvn(nmask, mask);\n+\n+    __ andr(res_0, c5, mask);\n+    __ andr(tmp, c0, nmask);\n+    __ orr(res_0, res_0, tmp);\n+\n+    __ andr(res_1, c6, mask);\n+    __ andr(tmp, c1, nmask);\n+    __ orr(res_1, res_1, tmp);\n+\n+    __ andr(res_2, c7, mask);\n+    __ andr(tmp, c2, nmask);\n+    __ orr(res_2, res_2, tmp);\n+\n+    __ andr(res_3, c8, mask);\n+    __ andr(tmp, c3, nmask);\n+    __ orr(res_3, res_3, tmp);\n+\n+    __ andr(res_4, c9, mask);\n+    __ andr(tmp, c4, nmask);\n+    __ orr(res_4, res_4, tmp);\n+\n+    __ str(res_0, result);\n+    __ str(res_1, Address(result, 8));\n+    __ str(res_2, Address(result, 16));\n+    __ str(res_3, Address(result, 24));\n+    __ str(res_4, Address(result, 32));\n+\n+    \/\/ End intrinsic call\n+    __ pop(callee_saved, sp);\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -11883,0 +12289,4 @@\n+    if (UseIntPolyIntrinsics) {\n+      StubRoutines::_intpoly_montgomeryMult_P256 = generate_intpoly_montgomeryMult_P256();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":410,"deletions":0,"binary":false,"changes":410,"status":"modified"},{"patch":"@@ -447,0 +447,4 @@\n+  if (FLAG_IS_DEFAULT(UseIntPolyIntrinsics)) {\n+        UseIntPolyIntrinsics = true;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}