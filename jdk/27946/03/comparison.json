{"files":[{"patch":"@@ -3153,0 +3153,14 @@\n+  \/\/Vector by element variant of UMULL\n+  void _umullv(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,\n+                SIMD_Arrangement Tb, FloatRegister Vm, SIMD_RegVariant Ts, int lane) {\n+    starti;\n+    int size = (Ta == T4S) ? 0b01 : 0b10;\n+    int q = (Tb == T4H || Tb == T2S) ? 0 : 1;\n+    int h = (size == 0b01) ? ((lane >> 2) & 1) : ((lane >> 1) & 1);\n+    int l = (size == 0b01) ? ((lane >> 1) & 1) : (lane & 1);\n+    assert(size == 0b10 ? lane < 4 : lane < 8, \"umullv assumes lane < 4 when using half-words and lane < 8 otherwise\");\n+    assert(Ts == H ? Vm->encoding() < 16 : Vm->encoding() < 32, \"umullv requires Vm to be in range V0..V15 when Ts is H\");\n+    f(0, 31), f(q, 30), f(0b101111, 29, 24), f(size, 23, 22), f(l, 21); \/\/f(m, 20);\n+    rf(Vm, 16), f(0b1010, 15, 12), f(h, 11), f(0, 10), rf(Vn, 5), rf(Vd, 0);\n+  }\n+\n@@ -3164,0 +3178,15 @@\n+  \/\/Vector by element variant of UMULL\n+  void umullv(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,\n+               SIMD_Arrangement Tb, FloatRegister Vm, SIMD_RegVariant Ts, int lane) {\n+    assert(Ta == T4S || Ta == T2D, \"umullv destination register must have arrangement T4S or T2D\");\n+    assert(Ta == T4S ? (Tb == T4H && Ts == H) : (Tb == T2S && Ts == S), \"umullv register arrangements must adhere to spec\");\n+    _umullv(Vd, Ta, Vn, Tb, Vm, Ts, lane);\n+  }\n+\n+  void umull2v(FloatRegister Vd, SIMD_Arrangement Ta, FloatRegister Vn,\n+               SIMD_Arrangement Tb, FloatRegister Vm, SIMD_RegVariant Ts, int lane) {\n+    assert(Ta == T4S || Ta == T2D, \"umullv destination register must have arrangement T4S or T2D\");\n+    assert(Ta == T4S ? (Tb == T8H && Ts == H) : (Tb == T4S && Ts == S), \"umullv register arrangements must adhere to spec\");\n+    _umullv(Vd, Ta, Vn, Tb, Vm, Ts, lane);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-  do_arch_blob(compiler, 70000)                                         \\\n+  do_arch_blob(compiler, 75000)                                         \\\n","filename":"src\/hotspot\/cpu\/aarch64\/stubDeclarations_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4832,0 +4832,26 @@\n+  template<int N>\n+  void vs_shl(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+              const VSeq<N>& v1, int shift) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+\n+    for (int i = 0; i < N; i++) {\n+      __ shl(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_ushr(const VSeq<N>& v, Assembler::SIMD_Arrangement T,\n+               const VSeq<N>& v1, int shift) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+\n+    for (int i = 0; i < N; i++) {\n+      __ ushr(v[i], T, v1[i], shift);\n+    }\n+  }\n+\n@@ -4856,0 +4882,23 @@\n+  template<int N>\n+  void vs_andr(const VSeq<N>& v, const VSeq<N>& v1, const FloatRegister v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ andr(v[i], __ T16B, v1[i], v2);\n+    }\n+  }\n+\n+  template<int N>\n+  void vs_eor(const VSeq<N>& v, const VSeq<N>& v1, const VSeq<N>& v2) {\n+    \/\/ output must not be constant\n+    assert(N == 1  || !v.is_constant(), \"cannot output multiple values to a constant vector\");\n+    \/\/ output cannot overwrite pending inputs\n+    assert(!vs_write_before_read(v, v1), \"output overwrites input\");\n+    assert(!vs_write_before_read(v, v2), \"output overwrites input\");\n+    for (int i = 0; i < N; i++) {\n+      __ eor(v[i], __ T16B, v1[i], v2[i]);\n+    }\n+  }\n+\n@@ -7143,0 +7192,935 @@\n+  address generate_intpoly_montgomeryMult_P256() {\n+\n+    __ align(CodeEntryAlignment);\n+    StubId stub_id = StubId::stubgen_intpoly_montgomeryMult_P256_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    \/\/Omit 3rd limb of modulus since it is 0\n+    static const int64_t modulus[5] = {\n+      0x000fffffffffffffL, 0x00000fffffffffffL,\n+      0x0000001000000000L, 0x0000ffffffff0000L\n+    };\n+\n+    int shift1 = 12; \/\/ 64 - bits per limb\n+    int shift2 = 52; \/\/ bits per limb\n+\n+    \/\/ Registers that are used throughout entire routine\n+    const Register a = c_rarg0;\n+    const Register b = c_rarg1;\n+    const Register result = c_rarg2;\n+    Register limb_mask = r3;\n+    Register c_ptr = r4;\n+    Register mod_0 = r5;\n+    Register mod_1 = r6;\n+    Register mod_3 = r7;\n+    Register mod_4 = r10;\n+    Register b_0 = r11;\n+    Register b_1 = r12;\n+    Register b_2 = r13;\n+    Register b_3 = r14;\n+    Register b_4 = r15;\n+\n+    FloatRegister limb_mask_vec = v0;\n+    FloatRegister b_lows = v1;\n+    FloatRegister b_highs = v2;\n+    FloatRegister a_vals = v3;\n+\n+    \/\/ Push callee saved registers on to the stack\n+    RegSet callee_saved = RegSet::range(r19, r28);\n+    __ push(callee_saved, sp);\n+\n+    \/\/ Allocate space on the stack for carry values\n+    __ sub(sp, sp, 48);\n+    __ mov(c_ptr, sp);\n+\n+    \/\/ Calculate limb mask\n+    __ mov(limb_mask, -UCONST64(1) >> (64 - shift2));\n+    __ dup(limb_mask_vec, __ T2D, limb_mask);\n+\n+    \/\/Load input arrays and modulus\n+    {\n+      Register a_ptr = r27;\n+      Register mod_ptr = r28;\n+      __ add(a_ptr, a, 24);\n+      __ lea(mod_ptr, ExternalAddress((address)modulus));\n+      __ ldr(b_0, Address(b));\n+      __ ldr(b_1, Address(b, 8));\n+      __ ldr(b_2, Address(b, 16));\n+      __ ldr(b_3, Address(b, 24));\n+      __ ldr(b_4, Address(b, 32));\n+      __ ldr(mod_0, __ post(mod_ptr, 8));\n+      __ ldr(mod_1, __ post(mod_ptr, 8));\n+      __ ldr(mod_3, __ post(mod_ptr, 8));\n+      __ ldr(mod_4, mod_ptr);\n+      __ ld1(a_vals, __ T2D, a_ptr);\n+      __ ld2(b_lows, b_highs, __ T4S, b);\n+    }\n+\n+    \/\/Regs used throughout the main \"loop\", which is partially unrolled here\n+    Register high = r19;\n+    Register low = r20;\n+    Register mul_ptr = r21;\n+    Register mod_high = r23;\n+    Register mod_low = r24;\n+    Register a_i = r25;\n+    Register c_i = r26;\n+    Register tmp = r27;\n+    Register n = r28;\n+\n+    VSeq<4> A(16);\n+    VSeq<4> B(20);\n+    VSeq<4> C(24);\n+    VSeq<4> D(28);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    __ sub(sp, sp, 128);\n+    __ mov(mul_ptr, sp);\n+\n+    __ umullv(A[0], __ T2D, b_lows, __ T2S, a_vals, __ S, 0);\n+    __ umull2v(A[1], __ T2D, b_lows, __ T4S, a_vals, __ S, 0);\n+    __ umullv(A[2], __ T2D, b_lows, __ T2S, a_vals, __ S, 2);\n+    __ umull2v(A[3], __ T2D, b_lows, __ T4S, a_vals, __ S, 2);\n+\n+    \/\/ Limb 0\n+    __ ldr(a_i, __ post(a, 8));\n+    __ umulh(high, a_i, b_0);\n+    __ mul(low, a_i, b_0);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+    __ andr(n, low, limb_mask);\n+\n+    __ umullv(B[0], __ T2D, b_highs, __ T2S, a_vals, __ S, 0);\n+    __ umull2v(B[1], __ T2D, b_highs, __ T4S, a_vals, __ S, 0);\n+    __ umullv(B[2], __ T2D, b_highs, __ T2S, a_vals, __ S, 2);\n+    __ umull2v(B[3], __ T2D, b_highs, __ T4S, a_vals, __ S, 2);\n+\n+    \/\/ Limb 0 cont\n+    __ umulh(mod_high, n, mod_0);\n+    __ mul(mod_low, n, mod_0);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ lsr(c_i, low, shift2);\n+    __ add(c_i, c_i, high);\n+\n+    __ umullv(C[0], __ T2D, b_lows, __ T2S, a_vals, __ S, 1);\n+    __ umull2v(C[1], __ T2D, b_lows, __ T4S, a_vals, __ S, 1);\n+    __ umullv(C[2], __ T2D, b_lows, __ T2S, a_vals, __ S, 3);\n+    __ umull2v(C[3], __ T2D, b_lows, __ T4S, a_vals, __ S, 3);\n+\n+    \/\/ Limb 1\n+    __ umulh(high, a_i, b_1);\n+    __ mul(low, a_i, b_1);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+\n+    __ umullv(D[0], __ T2D, b_highs, __ T2S, a_vals, __ S, 1);\n+    __ umull2v(D[1], __ T2D, b_highs, __ T4S, a_vals, __ S, 1);\n+    __ umullv(D[2], __ T2D, b_highs, __ T2S, a_vals, __ S, 3);\n+    __ umull2v(D[3], __ T2D, b_highs, __ T4S, a_vals, __ S, 3);\n+\n+    __ umulh(mod_high, n, mod_1);\n+    __ mul(mod_low, n, mod_1);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, c_ptr);\n+    __ mov(c_i, high);\n+\n+    vs_addv(B, __ T2D, B, C); \/\/ Store (B+C) in B\n+\n+    \/\/ Limb 2\n+    __ umulh(high, a_i, b_2);\n+    __ mul(low, a_i, b_2);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 8));\n+    __ mov(c_i, high);\n+\n+    vs_shl(D, __ T2D, D, 12);\n+\n+    \/\/ Limb 3\n+    __ umulh(high, a_i, b_3); \/\/compute next mult to avoid waiting for result\n+    __ mul(low, a_i, b_3);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+\n+    vs_ushr(C, __ T2D, B, 20); \/\/ Use C for ((B+C) >>> 20)\n+\n+    __ umulh(mod_high, n, mod_3);\n+    __ mul(mod_low, n, mod_3);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 16));\n+    __ mov(c_i, high);\n+\n+    vs_shl(B, __ T2D, B, 32);\n+\n+    \/\/ Limb 4\n+    __ umulh(high, a_i, b_4);\n+    __ mul(low, a_i, b_4);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+\n+    vs_addv(D, __ T2D, D, C);\n+\n+    __ umulh(mod_high, n, mod_4);\n+    __ mul(mod_low, n, mod_4);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 24));\n+    __ str(high, Address(c_ptr, 32));\n+\n+    vs_ushr(C, __ T2D, A, 52); \/\/ C now holds (A >>> 52)\n+    vs_andr(B, B, limb_mask_vec);\n+    vs_andr(A, A, limb_mask_vec);\n+    vs_addv(D, __ T2D, D, C);\n+    vs_addv(A, __ T2D, A, B);\n+\n+    vs_ushr(B, __ T2D, A, shift2);\n+    vs_andr(A, A, limb_mask_vec);\n+    vs_addv(D, __ T2D, D, B);\n+\n+    __ st1(A[0], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[0], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(A[1], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[1], __ T2D, __ post(mul_ptr, 16));\n+\n+    __ st1(A[2], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[2], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(A[3], __ T2D, __ post(mul_ptr, 16));\n+    __ st1(D[3], __ T2D, mul_ptr);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ Loop 2 & 3\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    for (int i = 0; i < 2; i++) {\n+      \/\/ Load a_i and increment by 8 bytes\n+      __ ldr(a_i, __ post(a, 8));\n+      __ ldr(c_i, c_ptr); \/\/Load prior c_i\n+\n+      \/\/ Limb 0\n+      __ umulh(high, a_i, b_0);\n+      __ mul(low, a_i, b_0);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ add(low, low, c_i);\n+      __ ldr(c_i, Address(c_ptr, 8));\n+      __ andr(n, low, limb_mask);\n+      __ umulh(mod_high, n, mod_0);\n+      __ mul(mod_low, n, mod_0);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ lsr(tmp, low, shift2);\n+      __ add(c_i, c_i, tmp);\n+      __ add(c_i, c_i, high);\n+\n+      \/\/ Limb 1\n+      __ umulh(high, a_i, b_1);\n+      __ mul(low, a_i, b_1);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ umulh(mod_high, n, mod_1);\n+      __ mul(mod_low, n, mod_1);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ ldr(tmp, Address(c_ptr, 16));\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, c_ptr);\n+      __ add(c_i, tmp, high);\n+\n+      \/\/ Limb 2\n+      __ umulh(high, a_i, b_2);\n+      __ mul(low, a_i, b_2);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ ldr(tmp, Address(c_ptr, 24));\n+      __ andr(low, low, limb_mask);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, Address(c_ptr, 8));\n+      __ add(c_i, tmp, high);\n+\n+      \/\/ Limb 3\n+      __ umulh(high, a_i, b_3);\n+      __ mul(low, a_i, b_3);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ umulh(mod_high, n, mod_3);\n+      __ mul(mod_low, n, mod_3);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ ldr(tmp, Address(c_ptr, 32));\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, Address(c_ptr, 16));\n+      __ add(c_i, tmp, high);\n+\n+      \/\/ Limb 4\n+      __ umulh(high, a_i, b_4);\n+      __ mul(low, a_i, b_4);\n+      __ lsl(high, high, shift1);\n+      __ lsr(tmp, low, shift2);\n+      __ orr(high, high, tmp);\n+      __ andr(low, low, limb_mask);\n+      __ umulh(mod_high, n, mod_4);\n+      __ mul(mod_low, n, mod_4);\n+      __ lsl(mod_high, mod_high, shift1);\n+      __ lsr(tmp, mod_low, shift2);\n+      __ orr(mod_high, mod_high, tmp);\n+      __ andr(mod_low, mod_low, limb_mask);\n+      __ add(low, low, mod_low);\n+      __ add(high, high, mod_high);\n+      __ add(c_i, c_i, low);\n+      __ str(c_i, Address(c_ptr, 24));\n+      __ str(high, Address(c_ptr, 32));\n+    }\n+\n+    Register low_1 = r21;\n+    Register high_1 = r22;\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ a[3]\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    __ ldr(low_1, Address(sp));\n+    __ ldr(high_1, Address(sp, 16));\n+\n+    __ ldr(low, Address(sp, 8));\n+    __ ldr(high, Address(sp, 24));\n+    __ ldr(a_i, __ post(a, 8));\n+    __ ldr(c_i, c_ptr);\n+\n+    \/\/ Limb 1\n+    __ add(low_1, low_1, c_i);\n+    __ ldr(c_i, Address(c_ptr, 8));\n+    __ andr(n, low_1, limb_mask);\n+    __ umulh(mod_high, n, mod_0);\n+    __ mul(mod_low, n, mod_0);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+    __ lsr(tmp, low_1, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high_1);\n+\n+    \/\/ Limb 2\n+    __ ldr(low_1, Address(sp, 32));\n+    __ ldr(high_1, Address(sp, 48));\n+    __ umulh(mod_high, n, mod_1);\n+    __ mul(mod_low, n, mod_1);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ ldr(tmp, Address(c_ptr, 16));\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, c_ptr);\n+    __ add(c_i, tmp, high);\n+\n+    \/\/ Limb 2\n+    __ ldr(low, Address(sp, 40));\n+    __ ldr(high, Address(sp, 56));\n+    __ ldr(tmp, Address(c_ptr, 24));\n+    __ add(c_i, c_i, low_1);\n+    __ str(c_i, Address(c_ptr, 8));\n+    __ add(c_i, tmp, high_1);\n+\n+    \/\/ Limb 3\n+    __ umulh(mod_high, n, mod_3);\n+    __ mul(mod_low, n, mod_3);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ ldr(tmp, Address(c_ptr, 32));\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ add(c_i, c_i, low);\n+    __ str(c_i, Address(c_ptr, 16));\n+    __ add(c_i, tmp, high);\n+\n+    \/\/ Limb 4\n+    __ ldr(low, Address(sp, 64));\n+    __ ldr(high, Address(sp, 80));\n+    __ umulh(high_1, a_i, b_4);\n+    __ mul(low_1, a_i, b_4);\n+    __ lsl(high_1, high_1, shift1);\n+    __ lsr(tmp, low_1, shift2);\n+    __ orr(high_1, high_1, tmp);\n+    __ andr(low_1, low_1, limb_mask);\n+    __ umulh(mod_high, n, mod_4);\n+    __ mul(mod_low, n, mod_4);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+    __ add(c_i, c_i, low_1);\n+    __ str(c_i, Address(c_ptr, 24));\n+    __ str(high_1, Address(c_ptr, 32));\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ a[4]\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    __ ldr(a_i, a);\n+    __ ldr(c_i, c_ptr);\n+\n+    \/\/ Limb 0\n+    __ ldr(low_1, Address(sp, 72));\n+    __ ldr(high_1, Address(sp, 88));\n+\n+    __ add(low, low, c_i);\n+    __ ldr(c_i, Address(c_ptr, 8));\n+    __ andr(n, low, limb_mask);\n+    __ umulh(mod_high, n, mod_0);\n+    __ mul(mod_low, n, mod_0);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+    __ lsr(tmp, low, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high);\n+\n+    \/\/ Limb 1\n+    __ ldr(low, Address(sp, 96));\n+    __ ldr(high, Address(sp, 112));\n+    __ umulh(mod_high, n, mod_1);\n+    __ mul(mod_low, n, mod_1);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+\n+    Register c5 = r11; \/\/replace b_0\n+    __ add(c5, c_i, low_1);\n+    __ ldr(c_i, Address(c_ptr, 16));\n+    __ lsr(tmp, c5, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high_1);\n+\n+    \/\/ Limb 2\n+    __ ldr(low_1, Address(sp, 104));\n+    __ ldr(high_1, Address(sp, 120));\n+    Register c6 = r12;\n+    __ add(c6, c_i, low);\n+    __ ldr(c_i, Address(c_ptr, 24));\n+    __ lsr(tmp, c6, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high);\n+\n+    \/\/ Limb 3\n+    __ umulh(mod_high, n, mod_3);\n+    __ mul(mod_low, n, mod_3);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low_1, low_1, mod_low);\n+    __ add(high_1, high_1, mod_high);\n+\n+    Register c7 = r13;\n+    __ add(c7, c_i, low_1);\n+    __ ldr(c_i, Address(c_ptr, 32));\n+    __ lsr(tmp, c7, shift2);\n+    __ add(c_i, c_i, tmp);\n+    __ add(c_i, c_i, high_1);\n+\n+    \/\/ Limb 4\n+    __ umulh(high, a_i, b_4);\n+    __ mul(low, a_i, b_4);\n+    __ lsl(high, high, shift1);\n+    __ lsr(tmp, low, shift2);\n+    __ orr(high, high, tmp);\n+    __ andr(low, low, limb_mask);\n+    __ umulh(mod_high, n, mod_4);\n+    __ mul(mod_low, n, mod_4);\n+    __ lsl(mod_high, mod_high, shift1);\n+    __ lsr(tmp, mod_low, shift2);\n+    __ orr(mod_high, mod_high, tmp);\n+    __ andr(mod_low, mod_low, limb_mask);\n+    __ add(low, low, mod_low);\n+    __ add(high, high, mod_high);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ Final carry propagate\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    \/\/ c5 += d1 + dd0 + (d0 >>> BITS_PER_LIMB);\n+    \/\/ c6 += (c5 >>> BITS_PER_LIMB);\n+    \/\/ c7 += (c6 >>> BITS_PER_LIMB);\n+    \/\/ c8 += (c7 >>> BITS_PER_LIMB);\n+    \/\/ c9 += (c8 >>> BITS_PER_LIMB);\n+\n+    Register c8 = r14;\n+    Register c9 = r15;\n+    __ add(c8, c_i, low);\n+    __ lsr(c9, c8, shift2);\n+    __ add(c9, c9, high);\n+\n+    __ andr(c5, c5, limb_mask);\n+    __ andr(c6, c6, limb_mask);\n+    __ andr(c7, c7, limb_mask);\n+    __ andr(c8, c8, limb_mask);\n+\n+    \/\/ c0 = c5 - modulus[0];\n+    \/\/ c1 = c6 - modulus[1] + (c0 >> BITS_PER_LIMB);\n+    \/\/ c0 &= LIMB_MASK;\n+    \/\/ c2 = c7 + (c1 >> BITS_PER_LIMB);\n+    \/\/ c1 &= LIMB_MASK;\n+    \/\/ c3 = c8 - modulus[3] + (c2 >> BITS_PER_LIMB);\n+    \/\/ c2 &= LIMB_MASK;\n+    \/\/ c4 = c9 - modulus[4] + (c3 >> BITS_PER_LIMB);\n+    \/\/ c3 &= LIMB_MASK;\n+\n+    Register c0 = r19;\n+    Register c1 = r20;\n+    Register c2 = r21;\n+    Register c3 = r22;\n+    Register c4 = r23;\n+    Register tmp0 = r24;\n+    Register tmp1 = r25;\n+    Register tmp2 = r26;\n+    Register tmp3 = r27;\n+    Register tmp4 = r28;\n+\n+    __ sub(c0, c5, mod_0);\n+    __ sub(c1, c6, mod_1);\n+    __ sub(c3, c8, mod_3);\n+    __ sub(c4, c9, mod_4);\n+    __ add(c1, c1, c0, Assembler::ASR, shift2);\n+    __ andr(c0, c0, limb_mask);\n+    __ add(c2, c7, c1, Assembler::ASR, shift2);\n+    __ andr(c1, c1, limb_mask);\n+    __ add(c3, c3, c2, Assembler::ASR, shift2);\n+    __ andr(c2, c2, limb_mask);\n+    __ add(c4, c4, c3, Assembler::ASR, shift2);\n+    __ andr(c3, c3, limb_mask);\n+\n+    \/\/ Final write back\n+    \/\/ mask = c4 >> 63\n+    \/\/ r[0] = ((c5 & mask) | (c0 & ~mask));\n+    \/\/ r[1] = ((c6 & mask) | (c1 & ~mask));\n+    \/\/ r[2] = ((c7 & mask) | (c2 & ~mask));\n+    \/\/ r[3] = ((c8 & mask) | (c3 & ~mask));\n+    \/\/ r[4] = ((c9 & mask) | (c4 & ~mask));\n+\n+    Register mask = r5;\n+    Register nmask = r6;\n+\n+    __ asr(mask, c4, 63);\n+    __ mvn(nmask, mask);\n+    __ andr(c5, c5, mask);\n+    __ andr(tmp, c0, nmask);\n+    __ orr(c5, c5, tmp);\n+    __ andr(c6, c6, mask);\n+    __ andr(tmp, c1, nmask);\n+    __ orr(c6, c6, tmp);\n+    __ andr(c7, c7, mask);\n+    __ andr(tmp, c2, nmask);\n+    __ orr(c7, c7, tmp);\n+    __ andr(c8, c8, mask);\n+    __ andr(tmp, c3, nmask);\n+    __ orr(c8, c8, tmp);\n+    __ andr(c9, c9, mask);\n+    __ andr(tmp, c4, nmask);\n+    __ orr(c9, c9, tmp);\n+\n+    __ str(c5, result);\n+    __ str(c6, Address(result, 8));\n+    __ str(c7, Address(result, 16));\n+    __ str(c8, Address(result, 24));\n+    __ str(c9, Address(result, 32));\n+\n+    \/\/ End intrinsic call\n+    __ add(sp, sp, 176);\n+    __ pop(callee_saved, sp);\n+    __ leave();\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  address generate_intpoly_assign() {\n+    \/\/ KNOWN Lengths:\n+    \/\/   MontgomeryIntPolynP256:  5 = 4 + 1\n+    \/\/   IntegerPolynomial1305:   5 = 4 + 1\n+    \/\/   IntegerPolynomial25519: 10 = 8 + 2\n+    \/\/   IntegerPolynomialP256:  10 = 8 + 2\n+    \/\/   Curve25519OrderField:   10 = 8 + 2\n+    \/\/   Curve25519OrderField:   10 = 8 + 2\n+    \/\/   P256OrderField:         10 = 8 + 2\n+    \/\/   IntegerPolynomialP384:  14 = 8 + 4 + 2\n+    \/\/   P384OrderField:         14 = 8 + 4 + 2\n+    \/\/   IntegerPolynomial448:   16 = 8 + 8\n+    \/\/   Curve448OrderField:     16 = 8 + 8\n+    \/\/   Curve448OrderField:     16 = 8 + 8\n+    \/\/   IntegerPolynomialP521:  19 = 8 + 8 + 2 + 1\n+    \/\/   P521OrderField:         19 = 8 + 8 + 2 + 1\n+    \/\/ Special Cases 5, 10, 14, 16, 19\n+\n+    __ align(CodeEntryAlignment);\n+    StubId stub_id = StubId::stubgen_intpoly_assign_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    \/\/ Inputs\n+    const Register set = c_rarg0;\n+    const Register aLimbs = c_rarg1;\n+    const Register bLimbs = c_rarg2;\n+    const Register length = c_rarg3;\n+\n+    Label L_Length5, L_Length10, L_Length14, L_Length16, L_Length19, L_DefaultLoop, L_Done;\n+\n+    \/*\n+    int maskValue = -set;\n+    for (int i = 0; i < a.length; i++) {\n+        long dummyLimbs = maskValue & (a[i] ^ b[i]);\n+        a[i] = dummyLimbs ^ a[i];\n+    }\n+    *\/\n+    Register mask_scalar = r4;\n+    FloatRegister mask_vec = v0;\n+\n+    __ neg(mask_scalar, set);\n+    __ dup(mask_vec, __ T2D, mask_scalar);\n+\n+    __ push(r19, sp); \/\/needed for length = 5\n+\n+    __ cmp(length, (u1)5);\n+    __ br(Assembler::EQ, L_Length5);\n+    __ cmp(length, (u1)10);\n+    __ br(Assembler::EQ, L_Length10);\n+    __ cmp(length, (u1)14);\n+    __ br(Assembler::EQ, L_Length14);\n+    __ cmp(length, (u1)16);\n+    __ br(Assembler::EQ, L_Length16);\n+    __ cmp(length, (u1)19);\n+    __ br(Assembler::EQ, L_Length19);\n+\n+    \/\/ Length = 5\n+    \/\/ Use 5 GPRs (neon not faster with this few limbs)\n+    __ BIND(L_Length5);\n+    {\n+      Register a0 = r5;\n+      Register a1 = r6;\n+      Register a2 = r7;\n+      Register a3 = r10;\n+      Register a4 = r11;\n+      Register b0 = r12;\n+      Register b1 = r13;\n+      Register b2 = r14;\n+      Register b3 = r15;\n+      Register b4 = r19;\n+\n+      __ ldr(a0, aLimbs);\n+      __ ldr(a1, Address(aLimbs, 8));\n+      __ ldr(a2, Address(aLimbs, 16));\n+      __ ldr(a3, Address(aLimbs, 24));\n+      __ ldr(a4, Address(aLimbs, 32));\n+\n+      __ ldr(b0, bLimbs);\n+      __ ldr(b1, Address(bLimbs, 8));\n+      __ ldr(b2, Address(bLimbs, 16));\n+      __ ldr(b3, Address(bLimbs, 24));\n+      __ ldr(b4, Address(bLimbs, 32));\n+\n+      __ eor(b0, b0, a0);\n+      __ eor(b1, b1, a1);\n+      __ eor(b2, b2, a2);\n+      __ eor(b3, b3, a3);\n+      __ eor(b4, b4, a4);\n+\n+      __ andr(b0, b0, mask_scalar);\n+      __ andr(b1, b1, mask_scalar);\n+      __ andr(b2, b2, mask_scalar);\n+      __ andr(b3, b3, mask_scalar);\n+      __ andr(b4, b4, mask_scalar);\n+\n+      __ eor(a0, a0, b0);\n+      __ eor(a1, a1, b1);\n+      __ eor(a2, a2, b2);\n+      __ eor(a3, a3, b3);\n+      __ eor(a4, a4, b4);\n+\n+      __ str(a0, aLimbs);\n+      __ str(a1, Address(aLimbs, 8));\n+      __ str(a2, Address(aLimbs, 16));\n+      __ str(a3, Address(aLimbs, 24));\n+      __ str(a4, Address(aLimbs, 32));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 10\n+    \/\/ Split into 4 neon regs and 2 GPRs\n+    __ BIND(L_Length10);\n+    {\n+      Register a9 = r10;\n+      Register a10 = r11;\n+      Register b9 = r12;\n+      Register b10 = r13;\n+\n+      VSeq<4> a_vec(16);\n+      VSeq<4> b_vec(20);\n+\n+      __ ldr(a9, Address(aLimbs, 64));\n+      __ ldr(a10, Address(aLimbs, 72));\n+      __ ldr(b9, Address(bLimbs, 64));\n+      __ ldr(b10, Address(bLimbs, 72));\n+\n+      __ ldpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ ldpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+\n+      __ eor(b9, b9, a9);\n+      __ eor(b10, b10, a10);\n+\n+      __ ldpq(b_vec[0], b_vec[1], Address(bLimbs));\n+      __ ldpq(b_vec[2], b_vec[3], Address(bLimbs, 32));\n+\n+      __ andr(b9, b9, mask_scalar);\n+      __ andr(b10, b10, mask_scalar);\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+\n+      __ eor(a9, a9, b9);\n+      __ eor(a10, a10, b10);\n+\n+      vs_andr(b_vec, b_vec, mask_vec);\n+\n+      __ str(a9, Address(aLimbs, 64));\n+      __ str(a10, Address(aLimbs, 72));\n+\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ stpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ stpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 14\n+    \/\/ Split into 5 neon regs and 4 GPRs\n+    __ BIND(L_Length14);\n+    {\n+      Register a10 = r5;\n+      Register a11 = r6;\n+      Register a12 = r7;\n+      Register a13 = r8;\n+      Register b10 = r9;\n+      Register b11 = r10;\n+      Register b12 = r11;\n+      Register b13 = r12;\n+\n+      VSeq<5> a_vec(16);\n+      VSeq<5> b_vec(22);\n+\n+      __ ldr(a10, Address(aLimbs, 80));\n+      __ ldr(a11, Address(aLimbs, 88));\n+      __ ldr(a12, Address(aLimbs, 96));\n+      __ ldr(a13, Address(aLimbs, 104));\n+\n+      __ ldr(b10, Address(bLimbs, 80));\n+      __ ldr(b11, Address(bLimbs, 88));\n+      __ ldr(b12, Address(bLimbs, 96));\n+      __ ldr(b13, Address(bLimbs, 104));\n+\n+      __ ld1(a_vec[0], __ T2D, aLimbs);\n+      __ ldpq(a_vec[1], a_vec[2], Address(aLimbs, 16));\n+      __ ldpq(a_vec[3], a_vec[4], Address(aLimbs, 48));\n+\n+      __ eor(b10, b10, a10);\n+      __ eor(b11, b11, a11);\n+      __ eor(b12, b12, a12);\n+      __ eor(b13, b13, a13);\n+\n+      __ ld1(b_vec[0], __ T2D, bLimbs);\n+      __ ldpq(b_vec[1], b_vec[2], Address(bLimbs, 16));\n+      __ ldpq(b_vec[3], b_vec[4], Address(bLimbs, 48));\n+\n+      __ andr(b10, b10, mask_scalar);\n+      __ andr(b11, b11, mask_scalar);\n+      __ andr(b12, b12, mask_scalar);\n+      __ andr(b13, b13, mask_scalar);\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+\n+      __ eor(a10, a10, b10);\n+      __ eor(a11, a11, b11);\n+      __ eor(a12, a12, b12);\n+      __ eor(a13, a13, b13);\n+\n+      vs_andr(b_vec, b_vec, mask_vec);\n+\n+      __ str(a10, Address(aLimbs, 80));\n+      __ str(a11, Address(aLimbs, 88));\n+      __ str(a12, Address(aLimbs, 96));\n+      __ str(a13, Address(aLimbs, 104));\n+\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ st1(a_vec[0], __ T2D, aLimbs);\n+      __ stpq(a_vec[1], a_vec[2], Address(aLimbs, 16));\n+      __ stpq(a_vec[3], a_vec[4], Address(aLimbs, 48));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 16\n+    \/\/ Use 8 neon regs\n+    __ BIND(L_Length16);\n+    {\n+      VSeq<8> a_vec(16);\n+      VSeq<8> b_vec(24);\n+\n+      __ ldpq(a_vec[0], a_vec[1], aLimbs);\n+      __ ldpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ ldpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ ldpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+\n+      __ ldpq(b_vec[0], b_vec[1], bLimbs);\n+      __ ldpq(b_vec[2], b_vec[3], Address(bLimbs, 32));\n+      __ ldpq(b_vec[4], b_vec[5], Address(bLimbs, 64));\n+      __ ldpq(b_vec[6], b_vec[7], Address(bLimbs, 96));\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+      vs_andr(b_vec, b_vec, mask_vec);\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ stpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ stpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ stpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ stpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+\n+      __ b(L_Done);\n+    }\n+\n+    \/\/ Length = 19\n+    \/\/ Split into 8 neon regs and 3 GPRs\n+    __ BIND(L_Length19);\n+    {\n+      Register a17 = r10;\n+      Register a18 = r11;\n+      Register a19 = r12;\n+      Register b17 = r13;\n+      Register b18 = r14;\n+      Register b19 = r15;\n+\n+      VSeq<8> a_vec(16);\n+      VSeq<8> b_vec(24);\n+\n+      __ ldr(a17, Address(aLimbs, 128));\n+      __ ldr(a18, Address(aLimbs, 136));\n+      __ ldr(a19, Address(aLimbs, 144));\n+      __ ldr(b17, Address(bLimbs, 128));\n+      __ ldr(b18, Address(bLimbs, 136));\n+      __ ldr(b19, Address(bLimbs, 144));\n+\n+      __ ldpq(a_vec[0], a_vec[1], aLimbs);\n+      __ ldpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ ldpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ ldpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+\n+      __ eor(b17, b17, a17);\n+      __ eor(b18, b18, a18);\n+      __ eor(b19, b19, a19);\n+\n+      __ ldpq(b_vec[0], b_vec[1], bLimbs);\n+      __ ldpq(b_vec[2], b_vec[3], Address(bLimbs, 32));\n+      __ ldpq(b_vec[4], b_vec[5], Address(bLimbs, 64));\n+      __ ldpq(b_vec[6], b_vec[7], Address(bLimbs, 96));\n+\n+      __ andr(b17, b17, mask_scalar);\n+      __ andr(b18, b18, mask_scalar);\n+      __ andr(b19, b19, mask_scalar);\n+\n+      vs_eor(b_vec, b_vec, a_vec);\n+\n+      __ eor(a17, a17, b17);\n+      __ eor(a18, a18, b18);\n+      __ eor(a19, a19, b19);\n+\n+      vs_andr(b_vec, b_vec, mask_vec);\n+\n+      __ str(a17, Address(aLimbs, 128));\n+      __ str(a18, Address(aLimbs, 136));\n+      __ str(a19, Address(aLimbs, 144));\n+\n+      vs_eor(a_vec, a_vec, b_vec);\n+\n+      __ stpq(a_vec[0], a_vec[1], Address(aLimbs));\n+      __ stpq(a_vec[2], a_vec[3], Address(aLimbs, 32));\n+      __ stpq(a_vec[4], a_vec[5], Address(aLimbs, 64));\n+      __ stpq(a_vec[6], a_vec[7], Address(aLimbs, 96));\n+    }\n+\n+    __ BIND(L_Done);\n+    __ pop(r19, sp);\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mov(r0, zr); \/\/ return 0\n+    __ ret(lr);\n+    return start;\n+  }\n+\n@@ -11883,0 +12867,5 @@\n+    if (UseIntPolyIntrinsics) {\n+      StubRoutines::_intpoly_montgomeryMult_P256 = generate_intpoly_montgomeryMult_P256();\n+      StubRoutines::_intpoly_assign = generate_intpoly_assign();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":989,"deletions":0,"binary":false,"changes":989,"status":"modified"},{"patch":"@@ -447,0 +447,4 @@\n+  if (FLAG_IS_DEFAULT(UseIntPolyIntrinsics)) {\n+        UseIntPolyIntrinsics = true;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}