{"files":[{"patch":"@@ -27,1 +27,0 @@\n-#include \"jfr\/utilities\/jfrSpinlockHelper.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"utilities\/spinCriticalSection.hpp\"\n@@ -131,1 +131,1 @@\n-  JfrSpinlockHelper mutex(&_lock);\n+  SpinCriticalSection scs(&_lock);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/service\/jfrEventThrottler.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"jfr\/utilities\/jfrSpinlockHelper.hpp\"\n@@ -35,0 +34,1 @@\n+#include \"utilities\/spinCriticalSection.hpp\"\n@@ -345,1 +345,1 @@\n-  JfrSpinlockHelper mutex(&_lock);\n+  SpinCriticalSection scs(&_lock);\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrAdaptiveSampler.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-#include \"jfr\/utilities\/jfrSpinlockHelper.hpp\"\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrThreadLocal.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1,44 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_JFR_UTILITIES_JFRSPINLOCKHELPER_HPP\n-#define SHARE_JFR_UTILITIES_JFRSPINLOCKHELPER_HPP\n-\n-#include \"runtime\/javaThread.hpp\"\n-\n-class JfrSpinlockHelper {\n- private:\n-  volatile int* const _lock;\n-\n- public:\n-  JfrSpinlockHelper(volatile int* lock) : _lock(lock) {\n-    Thread::SpinAcquire(_lock);\n-  }\n-\n-  ~JfrSpinlockHelper() {\n-    Thread::SpinRelease(_lock);\n-  }\n-};\n-\n-#endif \/\/ SHARE_JFR_UTILITIES_JFRSPINLOCKHELPER_HPP\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrSpinlockHelper.hpp","additions":0,"deletions":44,"binary":false,"changes":44,"status":"deleted"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"utilities\/spinCriticalSection.hpp\"\n@@ -319,1 +320,1 @@\n-    if (AtomicAccess::cmpxchg(&_object_strong_lock, 0, 1) == 0) {\n+    auto setObjectStrongLambda = [&](OopHandle& object_strong, const WeakHandle& object) {\n@@ -322,1 +323,1 @@\n-        _object_strong = OopHandle(JavaThread::thread_oop_storage(), _object.resolve());\n+        object_strong = OopHandle(JavaThread::thread_oop_storage(), _object.resolve());\n@@ -324,2 +325,2 @@\n-      AtomicAccess::release_store(&_object_strong_lock, 0);\n-    }\n+    };\n+    SpinSingleSection<decltype(setObjectStrongLambda), OopHandle, WeakHandle> sss(&_object_strong_lock, setObjectStrongLambda, _object_strong, _object);\n@@ -1866,3 +1867,4 @@\n-  Thread::SpinAcquire(&_wait_set_lock);\n-  add_waiter(&node);\n-  Thread::SpinRelease(&_wait_set_lock);\n+  {\n+    SpinCriticalSection scs(&_wait_set_lock);\n+    add_waiter(&node);\n+  }\n@@ -1925,1 +1927,1 @@\n-      Thread::SpinAcquire(&_wait_set_lock);\n+      SpinCriticalSection scs(&_wait_set_lock);\n@@ -1930,1 +1932,0 @@\n-      Thread::SpinRelease(&_wait_set_lock);\n@@ -2039,16 +2040,17 @@\n-  Thread::SpinAcquire(&_wait_set_lock);\n-  ObjectWaiter* iterator = dequeue_waiter();\n-  if (iterator != nullptr) {\n-    guarantee(iterator->TState == ObjectWaiter::TS_WAIT, \"invariant\");\n-\n-    if (iterator->is_vthread()) {\n-      oop vthread = iterator->vthread();\n-      java_lang_VirtualThread::set_notified(vthread, true);\n-      int old_state = java_lang_VirtualThread::state(vthread);\n-      \/\/ If state is not WAIT\/TIMED_WAIT then target could still be on\n-      \/\/ unmount transition, or wait could have already timed-out or target\n-      \/\/ could have been interrupted. In the first case, the target itself\n-      \/\/ will set the state to BLOCKED at the end of the unmount transition.\n-      \/\/ In the other cases the target would have been already unblocked so\n-      \/\/ there is nothing to do.\n-      if (old_state == java_lang_VirtualThread::WAIT ||\n+  {\n+    SpinCriticalSection scs(&_wait_set_lock);\n+    ObjectWaiter* iterator = dequeue_waiter();\n+    if (iterator != nullptr) {\n+      guarantee(iterator->TState == ObjectWaiter::TS_WAIT, \"invariant\");\n+\n+      if (iterator->is_vthread()) {\n+        oop vthread = iterator->vthread();\n+        java_lang_VirtualThread::set_notified(vthread, true);\n+        int old_state = java_lang_VirtualThread::state(vthread);\n+        \/\/ If state is not WAIT\/TIMED_WAIT then target could still be on\n+        \/\/ unmount transition, or wait could have already timed-out or target\n+        \/\/ could have been interrupted. In the first case, the target itself\n+        \/\/ will set the state to BLOCKED at the end of the unmount transition.\n+        \/\/ In the other cases the target would have been already unblocked so\n+        \/\/ there is nothing to do.\n+        if (old_state == java_lang_VirtualThread::WAIT ||\n@@ -2056,1 +2058,6 @@\n-        java_lang_VirtualThread::cmpxchg_state(vthread, old_state, java_lang_VirtualThread::BLOCKED);\n+          java_lang_VirtualThread::cmpxchg_state(vthread, old_state, java_lang_VirtualThread::BLOCKED);\n+        }\n+        \/\/ Increment counter *before* adding the vthread to the _entry_list.\n+        \/\/ Adding to _entry_list uses Atomic::cmpxchg() which already provides\n+        \/\/ a fence that prevents reordering of the stores.\n+        inc_unmounted_vthreads();\n@@ -2058,5 +2065,0 @@\n-      \/\/ Increment counter *before* adding the vthread to the _entry_list.\n-      \/\/ Adding to _entry_list uses Atomic::cmpxchg() which already provides\n-      \/\/ a fence that prevents reordering of the stores.\n-      inc_unmounted_vthreads();\n-    }\n@@ -2064,31 +2066,32 @@\n-    iterator->_notifier_tid = JFR_THREAD_ID(current);\n-    did_notify = true;\n-    add_to_entry_list(current, iterator);\n-\n-    \/\/ _wait_set_lock protects the wait queue, not the entry_list.  We could\n-    \/\/ move the add-to-entry_list operation, above, outside the critical section\n-    \/\/ protected by _wait_set_lock.  In practice that's not useful.  With the\n-    \/\/ exception of  wait() timeouts and interrupts the monitor owner\n-    \/\/ is the only thread that grabs _wait_set_lock.  There's almost no contention\n-    \/\/ on _wait_set_lock so it's not profitable to reduce the length of the\n-    \/\/ critical section.\n-\n-    if (!iterator->is_vthread()) {\n-      iterator->wait_reenter_begin(this);\n-\n-      \/\/ Read counter *after* adding the thread to the _entry_list.\n-      \/\/ Adding to _entry_list uses Atomic::cmpxchg() which already provides\n-      \/\/ a fence that prevents this load from floating up previous store.\n-      if (has_unmounted_vthreads()) {\n-        \/\/ Wake up the thread to alleviate some deadlock cases where the successor\n-        \/\/ that will be picked up when this thread releases the monitor is an unmounted\n-        \/\/ virtual thread that cannot run due to having run out of carriers. Upon waking\n-        \/\/ up, the thread will call reenter_internal() which will use timed-park in case\n-        \/\/ there is contention and there are still vthreads in the _entry_list.\n-        \/\/ If the target was interrupted or the wait timed-out at the same time, it could\n-        \/\/ have reached reenter_internal and read a false value of has_unmounted_vthreads()\n-        \/\/ before we added it to the _entry_list above. To deal with that case, we set _do_timed_park\n-        \/\/ which will be read by the target on the next loop iteration in reenter_internal.\n-        iterator->_do_timed_park = true;\n-        JavaThread* t = iterator->thread();\n-        t->_ParkEvent->unpark();\n+      iterator->_notifier_tid = JFR_THREAD_ID(current);\n+      did_notify = true;\n+      add_to_entry_list(current, iterator);\n+\n+      \/\/ _wait_set_lock protects the wait queue, not the entry_list.  We could\n+      \/\/ move the add-to-entry_list operation, above, outside the critical section\n+      \/\/ protected by _wait_set_lock.  In practice that's not useful.  With the\n+      \/\/ exception of  wait() timeouts and interrupts the monitor owner\n+      \/\/ is the only thread that grabs _wait_set_lock.  There's almost no contention\n+      \/\/ on _wait_set_lock so it's not profitable to reduce the length of the\n+      \/\/ critical section.\n+\n+      if (!iterator->is_vthread()) {\n+        iterator->wait_reenter_begin(this);\n+\n+        \/\/ Read counter *after* adding the thread to the _entry_list.\n+        \/\/ Adding to _entry_list uses Atomic::cmpxchg() which already provides\n+        \/\/ a fence that prevents this load from floating up previous store.\n+        if (has_unmounted_vthreads()) {\n+          \/\/ Wake up the thread to alleviate some deadlock cases where the successor\n+          \/\/ that will be picked up when this thread releases the monitor is an unmounted\n+          \/\/ virtual thread that cannot run due to having run out of carriers. Upon waking\n+          \/\/ up, the thread will call reenter_internal() which will use timed-park in case\n+          \/\/ there is contention and there are still vthreads in the _entry_list.\n+          \/\/ If the target was interrupted or the wait timed-out at the same time, it could\n+          \/\/ have reached reenter_internal and read a false value of has_unmounted_vthreads()\n+          \/\/ before we added it to the _entry_list above. To deal with that case, we set _do_timed_park\n+          \/\/ which will be read by the target on the next loop iteration in reenter_internal.\n+          iterator->_do_timed_park = true;\n+          JavaThread* t = iterator->thread();\n+          t->_ParkEvent->unpark();\n+        }\n@@ -2098,1 +2101,0 @@\n-  Thread::SpinRelease(&_wait_set_lock);\n@@ -2201,3 +2203,4 @@\n-  Thread::SpinAcquire(&_wait_set_lock);\n-  add_waiter(node);\n-  Thread::SpinRelease(&_wait_set_lock);\n+  {\n+    SpinCriticalSection scs(&_wait_set_lock);\n+    add_waiter(node);\n+  }\n@@ -2224,1 +2227,1 @@\n-    Thread::SpinAcquire(&_wait_set_lock);\n+    SpinCriticalSection scs(&_wait_set_lock);\n@@ -2229,1 +2232,0 @@\n-    Thread::SpinRelease(&_wait_set_lock);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":70,"deletions":68,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"utilities\/spinCriticalSection.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"utilities\/spinCriticalSection.hpp\"\n@@ -63,1 +64,0 @@\n-  Thread::SpinAcquire(&ListLock);\n@@ -65,3 +65,6 @@\n-    ev = FreeList;\n-    if (ev != nullptr) {\n-      FreeList = ev->FreeNext;\n+    SpinCriticalSection scs(&ListLock);\n+    {\n+      ev = FreeList;\n+      if (ev != nullptr) {\n+        FreeList = ev->FreeNext;\n+      }\n@@ -70,1 +73,0 @@\n-  Thread::SpinRelease(&ListLock);\n@@ -91,1 +93,0 @@\n-  Thread::SpinAcquire(&ListLock);\n@@ -93,0 +94,1 @@\n+    SpinCriticalSection scs(&ListLock);\n@@ -96,1 +98,0 @@\n-  Thread::SpinRelease(&ListLock);\n","filename":"src\/hotspot\/share\/runtime\/park.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -569,46 +569,0 @@\n-\n-\/\/ Ad-hoc mutual exclusion primitive: spin lock\n-\/\/\n-\/\/ We employ a spin lock _only for low-contention, fixed-length\n-\/\/ short-duration critical sections where we're concerned\n-\/\/ about native mutex_t or HotSpot Mutex:: latency.\n-\n-void Thread::SpinAcquire(volatile int * adr) {\n-  if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) {\n-    return;   \/\/ normal fast-path return\n-  }\n-\n-  \/\/ Slow-path : We've encountered contention -- Spin\/Yield\/Block strategy.\n-  int ctr = 0;\n-  int Yields = 0;\n-  for (;;) {\n-    while (*adr != 0) {\n-      ++ctr;\n-      if ((ctr & 0xFFF) == 0 || !os::is_MP()) {\n-        if (Yields > 5) {\n-          os::naked_short_sleep(1);\n-        } else {\n-          os::naked_yield();\n-          ++Yields;\n-        }\n-      } else {\n-        SpinPause();\n-      }\n-    }\n-    if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) return;\n-  }\n-}\n-\n-void Thread::SpinRelease(volatile int * adr) {\n-  assert(*adr != 0, \"invariant\");\n-  \/\/ Roach-motel semantics.\n-  \/\/ It's safe if subsequent LDs and STs float \"up\" into the critical section,\n-  \/\/ but prior LDs and STs within the critical section can't be allowed\n-  \/\/ to reorder or float past the ST that releases the lock.\n-  \/\/ Loads and stores in the critical section - which appear in program\n-  \/\/ order before the store that releases the lock - must also appear\n-  \/\/ before the store that releases the lock in memory visibility order.\n-  \/\/ So we need a #loadstore|#storestore \"release\" memory barrier before\n-  \/\/ the ST of 0 into the lock-word which releases the lock.\n-  AtomicAccess::release_store(adr, 0);\n-}\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -605,5 +605,0 @@\n-  \/\/ Low-level leaf-lock primitives used to implement synchronization.\n-  \/\/ Not for general synchronization use.\n-  static void SpinAcquire(volatile int * Lock);\n-  static void SpinRelease(volatile int * Lock);\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"runtime\/atomicAccess.hpp\"\n+#include \"utilities\/spinCriticalSection.hpp\"\n+\n+ \/\/ Ad-hoc mutual exclusion primitive: spin lock\n+ \/\/\n+ \/\/ We employ a spin lock _only for low-contention, fixed-length\n+ \/\/ short-duration critical sections where we're concerned\n+ \/\/ about native mutex_t or HotSpot Mutex:: latency.\n+void SpinCriticalSectionHelper::spin_acquire(volatile int* adr) {\n+  if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) {\n+    return;   \/\/ normal fast-path return\n+  }\n+\n+  \/\/ Slow-path : We've encountered contention -- Spin\/Yield\/Block strategy.\n+  int ctr = 0;\n+  int yields = 0;\n+  for (;;) {\n+    while (*adr != 0) {\n+      ++ctr;\n+      if ((ctr & 0xFFF) == 0 || !os::is_MP()) {\n+        if (yields > 5) {\n+          os::naked_short_sleep(1);\n+        }\n+        else {\n+          os::naked_yield();\n+          ++yields;\n+        }\n+      }\n+      else {\n+        SpinPause();\n+      }\n+    }\n+    if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) return;\n+  }\n+}\n+\n+void SpinCriticalSectionHelper::spin_release(volatile int* adr) {\n+  assert(*adr != 0, \"invariant\");\n+  \/\/ Roach-motel semantics.\n+  \/\/ It's safe if subsequent LDs and STs float \"up\" into the critical section,\n+  \/\/ but prior LDs and STs within the critical section can't be allowed\n+  \/\/ to reorder or float past the ST that releases the lock.\n+  \/\/ Loads and stores in the critical section - which appear in program\n+  \/\/ order before the store that releases the lock - must also appear\n+  \/\/ before the store that releases the lock in memory visibility order.\n+  \/\/ So we need a #loadstore|#storestore \"release\" memory barrier before\n+  \/\/ the ST of 0 into the lock-word which releases the lock.\n+  AtomicAccess::release_store(adr, 0);\n+}\n+\n+bool SpinCriticalSectionHelper::try_spin_acquire(volatile int* adr) {\n+  if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/spinCriticalSection.cpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_SPINCRITICALSECTION_HPP\n+#define SHARE_UTILITIES_SPINCRITICALSECTION_HPP\n+\n+#include \"runtime\/javaThread.hpp\"\n+\n+class SpinCriticalSectionHelper {\n+  friend class SpinCriticalSection;\n+  template<class Lambda, class...Args>\n+  friend class SpinSingleSection;\n+  \/\/ Low-level leaf-lock primitives used to implement synchronization.\n+  \/\/ Not for general synchronization use.\n+  static void spin_acquire(volatile int* Lock);\n+  static void spin_release(volatile int* Lock);\n+  static bool try_spin_acquire(volatile int* Lock);\n+};\n+\n+\/\/ Short critical section. To be used when having a\n+\/\/ mutex is considered to be expensive.\n+class SpinCriticalSection {\n+private:\n+  volatile int* const _lock;\n+public:\n+  SpinCriticalSection(volatile int* lock) : _lock(lock) {\n+    SpinCriticalSectionHelper::spin_acquire(_lock);\n+  }\n+  ~SpinCriticalSection() {\n+    SpinCriticalSectionHelper::spin_release(_lock);\n+  }\n+};\n+\n+template<class Lambda, class...Args>\n+class SpinSingleSection {\n+private:\n+  volatile int* const _lock;\n+  Thread* _lock_owner;\n+public:\n+  SpinSingleSection(volatile int* lock, Lambda& F, Args&... args) : _lock(lock), _lock_owner(nullptr) {\n+    if (SpinCriticalSectionHelper::try_spin_acquire(_lock)) {\n+      _lock_owner = Thread::current();\n+      F(args...);\n+    }\n+  }\n+  ~SpinSingleSection() {\n+    \/\/ It is safe to not have any atomic operations here,\n+    \/\/ as a thread either sees a nullptr or a pointer to a thread which\n+    \/\/ succeeded in locking the lock. Comparison will fail in both\n+    \/\/ cases if it is not a succeeded thread.\n+    if (_lock_owner == Thread::current()) {\n+      SpinCriticalSectionHelper::spin_release(_lock);\n+    }\n+  }\n+};\n+#endif \/\/SHARE_UTILITIES_SPINCRITICALSECTION_HPP\n","filename":"src\/hotspot\/share\/utilities\/spinCriticalSection.hpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"jfr\/utilities\/jfrSpinlockHelper.hpp\"\n@@ -44,0 +43,1 @@\n+#include \"utilities\/spinCriticalSection.hpp\"\n","filename":"test\/hotspot\/gtest\/jfr\/test_adaptiveSampler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}