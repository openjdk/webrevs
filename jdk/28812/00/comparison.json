{"files":[{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/callnode.hpp\"\n@@ -47,0 +48,2 @@\n+#include \"opto\/node.hpp\"\n+#include \"opto\/opcodes.hpp\"\n@@ -57,0 +60,1 @@\n+#include \"utilities\/tribool.hpp\"\n@@ -85,0 +89,93 @@\n+\/\/ Check whether an allocation has escaped at a certain control node ctl, the allocation does not\n+\/\/ escape at ctl if there is no node that make it escape at any transitive input of ctl\n+bool MemNode::check_not_escaped(PhaseValues* phase, Unique_Node_List& aliases, AllocateNode* alloc, Node* ctl) {\n+  if (!phase->is_IterGVN() || alloc == nullptr) {\n+    return false;\n+  }\n+  ciEnv* env = phase->C->env();\n+  if (env->should_retain_local_variables() || env->jvmti_can_walk_any_space()) {\n+    \/\/ JVMTI can modify local objects, so give up\n+    return false;\n+  }\n+\n+  Node* base = alloc->result_cast();\n+  assert(base != nullptr, \"must have a result cast\");\n+\n+  \/\/ Find all nodes that may alias base, if any of these nodes escapes, then we conservatively say\n+  \/\/ that base escapes\n+  assert(aliases.size() == 0, \"must not be computed yet\");\n+  aliases.push(base);\n+  for (uint wl_idx = 0; wl_idx < aliases.size(); wl_idx++) {\n+    Node* n = aliases.at(wl_idx);\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (out->is_ConstraintCast() || out->is_EncodeP() || out->is_DecodeN() ||\n+          out->is_Phi() || out->is_CMove()) {\n+        aliases.push(out);\n+      } else if (out->is_AddP()) {\n+        \/\/ Some runtime calls receive a derived pointer but not its base, so we consider these\n+        \/\/ derived pointers aliases, too\n+        aliases.push(out);\n+      }\n+    }\n+  }\n+\n+  \/\/ Walk the control graph to find a node that makes base escape\n+  ResourceMark rm;\n+  Unique_Node_List controls;\n+  controls.push(ctl);\n+  for (uint control_idx = 0; control_idx < controls.size(); control_idx++) {\n+    Node* n = controls.at(control_idx);\n+    if (n == alloc || n->is_Start()) {\n+      continue;\n+    }\n+\n+    \/\/ A call that receives an object as an argument makes that object escape\n+    if (n->is_Call() && !n->is_AbstractLock()) {\n+      const TypeTuple* d = n->as_Call()->tf()->domain();\n+      for (uint i = TypeFunc::Parms; i < d->cnt(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr && aliases.member(in)) {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (!out->is_Mem() || out->is_Load()) {\n+        continue;\n+      }\n+\n+      \/\/ If an object is stored to memory, then it escapes\n+      if (aliases.member(out->in(MemNode::ValueIn))) {\n+        return false;\n+      }\n+\n+      \/\/ Mismatched accesses can lie in a different alias class and are protected by memory\n+      \/\/ barriers, so we cannot be aggressive and walk past memory barriers if there is a\n+      \/\/ mismatched store into it. LoadStoreNodes are also lumped here because there is no\n+      \/\/ LoadStoreNode::is_mismatched_access.\n+      if (aliases.member(out->in(MemNode::Address)) && (!out->is_Store() || out->as_Store()->is_mismatched_access())) {\n+        return false;\n+      }\n+    }\n+\n+    if (n->is_Region()) {\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr) {\n+          controls.push(in);\n+        }\n+      }\n+    } else {\n+      Node* in = n->in(0);\n+      if (in != nullptr) {\n+        controls.push(in);\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -704,0 +801,13 @@\n+  if (is_Load() && alloc != nullptr) {\n+    \/\/ Get agressive for loads from freshly allocated objects\n+    cnt = 1000;\n+  }\n+\n+  \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n+  bool is_known_instance = addr_t != nullptr && addr_t->is_known_instance_field();\n+  \/\/ If alloc != nullptr and the allocated object has not escaped the current compilation unit, we\n+  \/\/ can be more aggressive, walk past calls and memory barriers to find a corresponding store\n+  TriBool has_not_escaped = is_known_instance ? TriBool(true) : (is_Load() ? TriBool() : TriBool(false));\n+  \/\/ If has_not_escaped and it is not empty, this is the set of all nodes that can alias base\n+  ResourceMark rm;\n+  Unique_Node_List aliases;\n@@ -736,8 +846,21 @@\n-      if (st_base != base &&\n-          detect_ptr_independence(base, alloc,\n-                                  st_base,\n-                                  AllocateNode::Ideal_allocation(st_base),\n-                                  phase)) {\n-        \/\/ Success:  The bases are provably independent.\n-        mem = mem->in(MemNode::Memory);\n-        continue;           \/\/ (a) advance through independent store memory\n+\n+      if (st_base != base) {\n+        bool known_independent = false;\n+        if (has_not_escaped && aliases.size() > 0) {\n+          known_independent = !aliases.member(st_base);\n+        } else if (detect_ptr_independence(base, alloc, st_base,\n+                                           AllocateNode::Ideal_allocation(st_base),\n+                                           phase)) {\n+          known_independent = true;\n+        } else if (has_not_escaped.is_default()) {\n+          has_not_escaped = check_not_escaped(phase, aliases, alloc, mem->in(0));\n+          if (has_not_escaped) {\n+            known_independent = !aliases.member(st_base);\n+          }\n+        }\n+\n+        if (known_independent) {\n+          \/\/ Success:  The bases are provably independent.\n+          mem = mem->in(MemNode::Memory);\n+          continue;\n+        }\n@@ -787,1 +910,0 @@\n-\n@@ -795,14 +917,7 @@\n-    } else if (addr_t != nullptr && addr_t->is_known_instance_field()) {\n-      \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n-      if (mem->is_Proj() && mem->in(0)->is_Call()) {\n-        \/\/ ArrayCopyNodes processed here as well.\n-        CallNode *call = mem->in(0)->as_Call();\n-        if (!call->may_modify(addr_t, phase)) {\n-          mem = call->in(TypeFunc::Memory);\n-          continue;         \/\/ (a) advance through independent call memory\n-        }\n-      } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n-        ArrayCopyNode* ac = nullptr;\n-        if (ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n-          break;\n-        }\n+    } else if (mem->is_MergeMem()) {\n+      int alias_idx = phase->C->get_alias_index(adr_type());\n+      mem = mem->as_MergeMem()->memory_at(alias_idx);\n+      continue;\n+    } else if (mem->is_Proj() && mem->in(0)->Opcode() == Op_SafePoint) {\n+      \/\/ Pure safepoints do not modify heap memory\n+      if (base != nullptr) {\n@@ -810,14 +925,41 @@\n-        continue;           \/\/ (a) advance through independent MemBar memory\n-      } else if (mem->is_ClearArray()) {\n-        if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n-          \/\/ (the call updated 'mem' value)\n-          continue;         \/\/ (a) advance through independent allocation memory\n-        } else {\n-          \/\/ Can not bypass initialization of the instance\n-          \/\/ we are looking for.\n-          return mem;\n-        }\n-      } else if (mem->is_MergeMem()) {\n-        int alias_idx = phase->C->get_alias_index(adr_type());\n-        mem = mem->as_MergeMem()->memory_at(alias_idx);\n-        continue;           \/\/ (a) advance through independent MergeMem memory\n+        continue;\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_Call()) {\n+      CallNode *call = mem->in(0)->as_Call();\n+      if (has_not_escaped.is_default()) {\n+        has_not_escaped = check_not_escaped(phase, aliases, alloc, call);\n+      }\n+      if (!has_not_escaped) {\n+        break;\n+      }\n+\n+      \/\/ We are more aggressive with known instances, for the others, if this call may modify base,\n+      \/\/ check_not_escaped would return false\n+      if (!is_known_instance || !call->may_modify(addr_t, phase)) {\n+        mem = call->in(TypeFunc::Memory);\n+        continue;         \/\/ (a) advance through independent call memory\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n+      if (has_not_escaped.is_default()) {\n+        has_not_escaped = check_not_escaped(phase, aliases, alloc, mem->in(0));\n+      }\n+      if (!has_not_escaped) {\n+        break;\n+      }\n+\n+      \/\/ We are more aggressive with known instances, for the others, if this call may modify base,\n+      \/\/ check_not_escaped would return false\n+      ArrayCopyNode* ac = nullptr;\n+      if (is_known_instance && ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n+        break;\n+      }\n+      mem = mem->in(0)->in(TypeFunc::Memory);\n+      continue;\n+    } else if (is_known_instance && mem->is_ClearArray()) {\n+      if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n+        \/\/ (the call updated 'mem' value)\n+        continue;         \/\/ (a) advance through independent allocation memory\n+      } else {\n+        \/\/ Can not bypass initialization of the instance\n+        \/\/ we are looking for.\n+        return mem;\n@@ -1885,0 +2027,5 @@\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+    return nullptr;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":184,"deletions":37,"binary":false,"changes":221,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  static bool check_not_escaped(PhaseValues* phase, Unique_Node_List& aliases, AllocateNode* alloc, Node* ctl);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,258 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.escapeAnalysis;\n+\n+import compiler.lib.ir_framework.*;\n+\n+import java.lang.invoke.VarHandle;\n+\n+\/**\n+ * @test\n+ * @bug 8373495\n+ * @summary Test that loads from a newly allocated object are aggressively folded if the object has not escaped\n+ * @library \/test\/lib \/\n+ * @run driver ${test.main.class}\n+ *\/\n+public class TestLoadFolding {\n+    public static class Point {\n+        int x;\n+        int y;\n+\n+        Point() {\n+            x = 1;\n+            y = 2;\n+        }\n+\n+        static final Point DEFAULT = new Point();\n+    }\n+\n+    static Point staticField;\n+\n+    public static void main(String[] args) {\n+        TestFramework.run();\n+    }\n+\n+    @Run(test = {\"test11\", \"test12\", \"test13\", \"test14\", \"test15\", \"test16\", \"test17\", \"test18\"})\n+    public void runPositiveTests() {\n+        test11();\n+        test12(false);\n+        test12(true);\n+        test13(false);\n+        test13(true);\n+        test14();\n+        test15(1, 16);\n+        test16(1, 16, false);\n+        test16(1, 16, true);\n+        test17(0);\n+        test18(0);\n+    }\n+\n+    @Run(test = {\"test01\", \"test02\", \"test03\", \"test04\", \"test05\"})\n+    public void runNegativeTests() {\n+        test01();\n+        test02(false);\n+        test02(true);\n+        test03(false);\n+        test03(true);\n+        test04(1, 16);\n+        test05(0);\n+    }\n+\n+    @DontInline\n+    static void escape(Object o) {}\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test11() {\n+        \/\/ p only escapes at return\n+        Point p = new Point();\n+        escape(null);\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test12(boolean b) {\n+        \/\/ p escapes in another branch\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        } else {\n+            escape(null);\n+            p.x += p.y;\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test13(boolean b) {\n+        \/\/ A Phi of p1 and Point.DEFAULT, but a store to Phi is after all the loads from p1\n+        Point p1 = new Point();\n+        Point p = b ? p1 : Point.DEFAULT;\n+        escape(null);\n+        p.x = p1.x + p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public int test14() {\n+        \/\/ Even if p escapes before the loads, if it is legal to execute the loads before the\n+        \/\/ store, then we can fold the loads\n+        Point p = new Point();\n+        escape(null);\n+        staticField = p;\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Point test15(int begin, int end) {\n+        \/\/ Fold the load that is a part of a cycle\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            p.x++;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Point test16(int begin, int end, boolean b) {\n+        \/\/ A cycle and a Phi, this time the store is at a different field\n+        Point p1 = new Point();\n+        \/\/ This store is not on a Phi involving p1, so it does not interfere\n+        Point.DEFAULT.y = 3;\n+        Point p = p1;\n+        for (int i = begin; i < end; i += 2) {\n+            if (b) {\n+                p = p1;\n+            } else {\n+                p = Point.DEFAULT;\n+            }\n+            b = !b;\n+\n+            p.x = p1.y + 3;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p1.x = p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test17(int idx) {\n+        \/\/ Array\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        int res = a[idx & 1];\n+        escape(null);\n+        res += a[0] + a[1];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test18(int idx) {\n+        \/\/ Array, even if we will give up if we encounter a[idx & 1] = 3, we meet a[0] = 4 first,\n+        \/\/ so the load int res = a[0] can still be folded\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        a[0] = 4;\n+        escape(null);\n+        int res = a[0];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC, \"1\"})\n+    public int test01() {\n+        Point p = new Point();\n+        staticField = p;\n+        \/\/ Actually, the only fence that requires the following loads to be executed after the\n+        \/\/ store is a fullFence\n+        VarHandle.fullFence();\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test02(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+            \/\/ p escaped, so the load must not be removed\n+            return p.x;\n+        } else {\n+            escape(null);\n+            return 0;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test03(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        }\n+        \/\/ p escaped, so the load must not be removed\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"> 0\", IRNode.ALLOC, \"1\"})\n+    public Point test04(int begin, int end) {\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            \/\/ p escaped here because this is a loop\n+            p.x++;\n+            escape(p);\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test05(int idx) {\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        \/\/ Cannot fold the loads because we do not know which element is written to by\n+        \/\/ a[idx & 1] = 3\n+        return a[0] + a[1];\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/escapeAnalysis\/TestLoadFolding.java","additions":258,"deletions":0,"binary":false,"changes":258,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -254,1 +254,1 @@\n-    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(failOn = {IRNode.LOAD_P_OF_CLASS, \"Outer\", IRNode.LOAD_N_OF_CLASS, \"Outer\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}