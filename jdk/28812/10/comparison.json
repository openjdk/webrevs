{"files":[{"patch":"@@ -586,0 +586,3 @@\n+  product(bool, DoLocalEscapeAnalysis, true, DIAGNOSTIC                     \\\n+          \"Perform local escape analysis during IGVN\")                      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+#include \"opto\/c2_globals.hpp\"\n+#include \"opto\/callnode.hpp\"\n@@ -47,0 +49,2 @@\n+#include \"opto\/node.hpp\"\n+#include \"opto\/opcodes.hpp\"\n@@ -57,0 +61,1 @@\n+#include \"utilities\/tribool.hpp\"\n@@ -704,0 +709,13 @@\n+  if (is_Load() && alloc != nullptr) {\n+    \/\/ Get agressive for loads from freshly allocated objects\n+    cnt = 1000;\n+  }\n+\n+  \/\/ If alloc != nullptr and the allocated object has not escaped the current compilation unit, we\n+  \/\/ can be more aggressive, walk past calls and memory barriers to find a corresponding store\n+  ResourceMark rm;\n+  bool is_known_instance = addr_t != nullptr && addr_t->is_known_instance_field();\n+  TriBool has_not_escaped = is_known_instance ? TriBool(true) : (is_Load() ? TriBool() : TriBool(false));\n+  LocalEA local_ea;\n+\n+  \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n@@ -712,2 +730,4 @@\n-      if (st_base == nullptr)\n-        break;              \/\/ inscrutable pointer\n+      if (st_base == nullptr) {\n+        \/\/ inscrutable pointer\n+        break;\n+      }\n@@ -715,2 +735,12 @@\n-      \/\/ For raw accesses it's not enough to prove that constant offsets don't intersect.\n-      \/\/ We need the bases to be the equal in order for the offset check to make sense.\n+      \/\/ If the bases are the same and the offsets are the same, it seems that this is the exact\n+      \/\/ store we are looking for, the caller will check if the type of the store matches\n+      if (st_base == base && st_offset == offset) {\n+        return mem;\n+      }\n+\n+      \/\/ If it is provable that the memory accessed by mem does not overlap the memory accessed by\n+      \/\/ this, we may walk past mem.\n+      \/\/ For raw accesses, 2 accesses are independent if they have the same base and the offset\n+      \/\/ says that they do not overlap.\n+      \/\/ For heap accesses, 2 accesses are independent if either the bases are provably different\n+      \/\/ at runtime or the offset says that the accesses do not overlap.\n@@ -718,0 +748,1 @@\n+        \/\/ Raw accesses can only be provably independent if they have the same base\n@@ -721,0 +752,3 @@\n+      \/\/ If the offsets say that the accesses do not overlap, then it is provable that mem and this\n+      \/\/ do not overlap. For example, a LoadI from Object+8 is independent from a StoreL into\n+      \/\/ Object+12, no matter what the bases are.\n@@ -733,1 +767,1 @@\n-          continue;           \/\/ (a) advance through independent store memory\n+          continue;\n@@ -736,8 +770,4 @@\n-      if (st_base != base &&\n-          detect_ptr_independence(base, alloc,\n-                                  st_base,\n-                                  AllocateNode::Ideal_allocation(st_base),\n-                                  phase)) {\n-        \/\/ Success:  The bases are provably independent.\n-        mem = mem->in(MemNode::Memory);\n-        continue;           \/\/ (a) advance through independent store memory\n+\n+      \/\/ Same base and overlapping offsets, it seems provable that the accesses overlap, give up\n+      if (st_base == base) {\n+        break;\n@@ -746,4 +776,20 @@\n-      \/\/ (b) At this point, if the bases or offsets do not agree, we lose,\n-      \/\/ since we have not managed to prove 'this' and 'mem' independent.\n-      if (st_base == base && st_offset == offset) {\n-        return mem;         \/\/ let caller handle steps (c), (d)\n+      \/\/ Try to prove that 2 different base nodes at compile time are different values at runtime\n+      bool known_independent = false;\n+      if (has_not_escaped && local_ea.aliases.size() > 0) {\n+        assert(!is_known_instance, \"aliases are not computed for known instances\");\n+\n+        \/\/ Since we are walking from a node to its input, if alloc is found that it has not escaped\n+        \/\/ at an earlier iteration, it must also be found that it has not escaped at the current\n+        \/\/ iteration\n+        assert(local_ea.not_escaped_controls.member(mem->in(0)), \"inconsistent\");\n+\n+        \/\/ If base is the result of an allocation that has not escaped, we can know all the nodes\n+        \/\/ that may have the same runtime value as base, these are the transitive outputs of base\n+        \/\/ along some chains that consist of ConstraintCasts, EncodePs, DecodeNs, Phis, and CMoves\n+        known_independent = !local_ea.aliases.member(st_base);\n+      } else if (detect_ptr_independence(base, alloc, st_base,\n+                                         AllocateNode::Ideal_allocation(st_base),\n+                                         phase)) {\n+        \/\/ detect_ptr_independence == true means that it can prove that base and st_base cannot\n+        \/\/ have the same runtime value\n+        known_independent = true;\n@@ -752,0 +798,4 @@\n+      if (known_independent) {\n+        mem = mem->in(MemNode::Memory);\n+        continue;\n+      }\n@@ -787,1 +837,0 @@\n-\n@@ -795,13 +844,18 @@\n-    } else if (addr_t != nullptr && addr_t->is_known_instance_field()) {\n-      \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n-      if (mem->is_Proj() && mem->in(0)->is_Call()) {\n-        \/\/ ArrayCopyNodes processed here as well.\n-        CallNode *call = mem->in(0)->as_Call();\n-        if (!call->may_modify(addr_t, phase)) {\n-          mem = call->in(TypeFunc::Memory);\n-          continue;         \/\/ (a) advance through independent call memory\n-        }\n-      } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n-        ArrayCopyNode* ac = nullptr;\n-        if (ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n-          break;\n+    } else if (mem->is_MergeMem()) {\n+      int alias_idx = phase->C->get_alias_index(adr_type());\n+      mem = mem->as_MergeMem()->memory_at(alias_idx);\n+      continue;\n+    } else if (mem->is_Proj() && mem->in(0)->is_Call()) {\n+      \/\/ We can walk past a call if we can prove that the call does not modify the memory we are\n+      \/\/ accessing, this is the case if the allocation has not escaped at this call\n+      CallNode* call = mem->in(0)->as_Call();\n+      if (has_not_escaped && !is_known_instance) {\n+        \/\/ Since we are walking from a node to its input, if alloc is found that it has not escaped\n+        \/\/ at an earlier iteration, it must also be found that it has not escaped at the current\n+        \/\/ iteration\n+        assert(local_ea.not_escaped_controls.member(call), \"inconsistent\");\n+      }\n+      if (has_not_escaped.is_default()) {\n+        LocalEA::EscapeStatus status = local_ea.check_escape_status(phase, alloc, call);\n+        if (status == LocalEA::DEAD_PATH) {\n+          return phase->C->top();\n@@ -809,10 +863,26 @@\n-        mem = mem->in(0)->in(TypeFunc::Memory);\n-        continue;           \/\/ (a) advance through independent MemBar memory\n-      } else if (mem->is_ClearArray()) {\n-        if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n-          \/\/ (the call updated 'mem' value)\n-          continue;         \/\/ (a) advance through independent allocation memory\n-        } else {\n-          \/\/ Can not bypass initialization of the instance\n-          \/\/ we are looking for.\n-          return mem;\n+\n+        has_not_escaped = (status == LocalEA::NOT_ESCAPED);\n+      }\n+      if (!has_not_escaped) {\n+        break;\n+      }\n+\n+      \/\/ We are more aggressive with known instances, for the others, if this call may modify base,\n+      \/\/ check_not_escaped would return false\n+      if (!is_known_instance || !call->may_modify(addr_t, phase)) {\n+        mem = call->in(TypeFunc::Memory);\n+        continue;         \/\/ (a) advance through independent call memory\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n+      \/\/ We can walk past a memory barrier if we can prove that the allocation has not escaped at\n+      \/\/ this barrier, hence it is invisible to other threads\n+      if (has_not_escaped && !is_known_instance) {\n+        \/\/ Since we are walking from a node to its input, if alloc is found that it has not escaped\n+        \/\/ at an earlier iteration, it must also be found that it has not escaped at the current\n+        \/\/ iteration\n+        assert(local_ea.not_escaped_controls.member(mem->in(0)), \"inconsistent\");\n+      }\n+      if (has_not_escaped.is_default()) {\n+        LocalEA::EscapeStatus status = local_ea.check_escape_status(phase, alloc, mem->in(0));\n+        if (status == LocalEA::DEAD_PATH) {\n+          return phase->C->top();\n@@ -820,4 +890,23 @@\n-      } else if (mem->is_MergeMem()) {\n-        int alias_idx = phase->C->get_alias_index(adr_type());\n-        mem = mem->as_MergeMem()->memory_at(alias_idx);\n-        continue;           \/\/ (a) advance through independent MergeMem memory\n+\n+        has_not_escaped = (status == LocalEA::NOT_ESCAPED);\n+      }\n+      if (!has_not_escaped) {\n+        break;\n+      }\n+\n+      \/\/ We are more aggressive with known instances, for the others, if this call may modify base,\n+      \/\/ check_not_escaped would return false\n+      ArrayCopyNode* ac = nullptr;\n+      if (is_known_instance && ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n+        break;\n+      }\n+      mem = mem->in(0)->in(TypeFunc::Memory);\n+      continue;\n+    } else if (is_known_instance && mem->is_ClearArray()) {\n+      if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n+        \/\/ (the call updated 'mem' value)\n+        continue;         \/\/ (a) advance through independent allocation memory\n+      } else {\n+        \/\/ Can not bypass initialization of the instance\n+        \/\/ we are looking for.\n+        return mem;\n@@ -1263,0 +1352,132 @@\n+\/\/ Check whether an allocation has escaped at a certain control node ctl, the allocation has not\n+\/\/ escaped at ctl if there is no node that:\n+\/\/ 1. Make the allocation escape.\n+\/\/ 2. Either:\n+\/\/   a. Has no control input.\n+\/\/   b. Has a control input that is ctl or a transitive control input of ctl.\n+\/\/\n+\/\/ In other word, alloc is determined that it has not escaped at ctl if all nodes that make alloc\n+\/\/ escape have a control input that is neither nullptr, ctl, nor a transitive control input of ctl.\n+MemNode::LocalEA::EscapeStatus MemNode::LocalEA::check_escape_status(PhaseValues* phase, AllocateNode* alloc, Node* ctl) {\n+  if (phase->type(ctl) == Type::TOP) {\n+    return DEAD_PATH;\n+  }\n+  if (!DoLocalEscapeAnalysis || !phase->is_IterGVN() || alloc == nullptr) {\n+    return ESCAPED;\n+  }\n+\n+  ciEnv* env = phase->C->env();\n+  if (env->should_retain_local_variables() || env->jvmti_can_walk_any_space()) {\n+    \/\/ JVMTI can modify local objects, so give up\n+    return ESCAPED;\n+  }\n+\n+  \/\/ Find all transitive control inputs of ctl that are not dead, if it is determined that alloc\n+  \/\/ has not escaped at ctl, then it must be the case that it has not escaped at all of these\n+  assert(not_escaped_controls.size() == 0, \"must not be computed yet\");\n+  Node* start = phase->C->start();\n+  not_escaped_controls.push(ctl);\n+  for (uint control_idx = 0; control_idx < not_escaped_controls.size(); control_idx++) {\n+    Node* n = not_escaped_controls.at(control_idx);\n+    assert(phase->type(n) == Type::CONTROL || phase->type(n)->base() == Type::Tuple, \"must be a control node %s\", n->Name());\n+    if (n == start) {\n+      continue;\n+    }\n+\n+    if (n->is_Region()) {\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr && phase->type(in) != Type::TOP) {\n+          not_escaped_controls.push(in);\n+        }\n+      }\n+    } else {\n+      Node* in = n->in(0);\n+      if (in != nullptr && phase->type(in) != Type::TOP) {\n+        not_escaped_controls.push(in);\n+      }\n+    }\n+  }\n+\n+  if (!not_escaped_controls.member(start)) {\n+    \/\/ If there is no control path from ctl to start, ctl is a dead path, give up\n+    not_escaped_controls.clear();\n+    return DEAD_PATH;\n+  }\n+\n+  Node* base = alloc->result_cast();\n+  assert(base != nullptr, \"must have a result cast\");\n+\n+  \/\/ Find all nodes that may escape alloc, and decide that it is provable that they must be\n+  \/\/ executed after ctl\n+  EscapeStatus res = NOT_ESCAPED;\n+  aliases.push(base);\n+  for (uint idx = 0; idx < aliases.size(); idx++) {\n+    Node* n = aliases.at(idx);\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (out->is_ConstraintCast() || out->is_EncodeP() || out->is_DecodeN() ||\n+          out->is_Phi() || out->is_CMove()) {\n+        \/\/ A node that may alias base, if any of these nodes escapes, then we conservatively say\n+        \/\/ that base escapes\n+        aliases.push(out);\n+        continue;\n+      } else if (out->is_AddP()) {\n+        \/\/ Some runtime calls receive a derived pointer but not its base, so we consider these\n+        \/\/ derived pointers aliases, too\n+        aliases.push(out);\n+        continue;\n+      }\n+\n+      Node* c = out->in(0);\n+      if (c != nullptr && !not_escaped_controls.member(c)) {\n+        \/\/ c is not a live transitive control input of ctl, so out is not executed before ctl,\n+        \/\/ which means it does not affect the escape status of alloc at ctl\n+        continue;\n+      }\n+\n+      if (out->is_Load()) {\n+        \/\/ A Load does not escape alloc\n+      } else if (out->is_Mem()) {\n+        \/\/ A Store or a LoadStore\n+        if (n == out->in(MemNode::ValueIn)) {\n+          \/\/ If an object is stored to memory, then it escapes\n+          res = ESCAPED;\n+          break;\n+        } else if (n == out->in(MemNode::Address) && (!out->is_Store() || out->as_Store()->is_mismatched_access())) {\n+          \/\/ Mismatched accesses can lie in a different alias class and are protected by memory\n+          \/\/ barriers, so we cannot be aggressive and walk past memory barriers if there is a\n+          \/\/ mismatched store into it. LoadStoreNodes are also lumped here because there is no\n+          \/\/ LoadStoreNode::is_mismatched_access.\n+          res = ESCAPED;\n+          break;\n+        }\n+      } else if (out->is_Call()) {\n+        if (!out->is_AbstractLock() && out->as_Call()->has_non_debug_use(n)) {\n+          \/\/ A call that receives an object as an argument makes that object escape\n+          res = ESCAPED;\n+          break;\n+        }\n+      } else if (out->is_SafePoint()) {\n+        \/\/ Non-call safepoints are pure control nodes\n+      } else if (out->Opcode() == Op_Blackhole) {\n+        \/\/ Blackhole does not escape an object (in the sense that it does not make its field values\n+        \/\/ unpredictable)\n+      } else {\n+        \/\/ Conservatively consider all other nodes to make alloc escape\n+        res = ESCAPED;\n+        break;\n+      }\n+    }\n+\n+    if (res == ESCAPED) {\n+      break;\n+    }\n+  }\n+\n+  if (res == ESCAPED) {\n+    not_escaped_controls.clear();\n+  }\n+  return res;\n+}\n+\n@@ -1885,0 +2106,5 @@\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+    return nullptr;\n+  }\n+\n@@ -1954,0 +2180,3 @@\n+  if (prev_mem != nullptr && prev_mem->is_top()) {\n+    return prev_mem;\n+  }\n@@ -3553,0 +3782,3 @@\n+      if (prev_mem != nullptr && prev_mem->is_top()) {\n+        return prev_mem;\n+      }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":277,"deletions":45,"binary":false,"changes":322,"status":"modified"},{"patch":"@@ -95,0 +95,59 @@\n+  \/\/ During igvn, in order to reason about the state of the memory a MemNode accesses, we can\n+  \/\/ inspect the escape status of that memory. A memory location is said to escape the compilation\n+  \/\/ unit if it is made visible outside the compilation unit, which is equivalent to the object\n+  \/\/ containing that memory visible outside the compilation unit. An object is in the not-escaped\n+  \/\/ state when it is just allocated, and stay so until its reference or a derived pointer of its\n+  \/\/ is stored into the memory, passed into a method invocation, or used as an input to a node we\n+  \/\/ do not know.\n+  \/\/\n+  \/\/ If an object has not escaped, it cannot be modified during a method invocation, and the\n+  \/\/ reordering of memory accesses into it is invisible to other threads. As a result, to find the\n+  \/\/ value that is stored at one of its fields, we can aggressively walk the memory graph past\n+  \/\/ CallNodes and MemBarNodes, this gives a much better chance to fold a load or a store.\n+  \/\/\n+  \/\/ This analysis is local, which means it only inspects a small part of the graph to determine\n+  \/\/ the escape status of an object. It is context-aware, which means it does not just try to find\n+  \/\/ a global escape status of an object, but it also tries to determine which node must observe\n+  \/\/ that the object has escaped. If an access into an object does not observe that the object has\n+  \/\/ escaped, it can try folding aggressively even though the object may escape afterwards.\n+  \/\/\n+  \/\/ It can be proved that if a node must observe that an object has escaped, then there is a path\n+  \/\/ of use-def edges from that node to the node which makes the object escape. This approach,\n+  \/\/ however, requires a global analysis as we need to walk the whole graph. As a result, we go for\n+  \/\/ a more conservative approach which inspects only the control subgraph of the IR graph. See\n+  \/\/ check_escape_status below.\n+  \/\/\n+  \/\/ Perform this local analysis during igvn has the advantage that it allows the folding to happen\n+  \/\/ earlier, before incremental inlining. This means that devirtualized method invocations can get\n+  \/\/ inlined. On the other hand, if the folding happens during the escape analysis phase, inlining\n+  \/\/ is already over.\n+  class LocalEA {\n+  public:\n+    \/\/ If a node p has not escaped from the compilation unit, this is the set of all nodes that can\n+    \/\/ alias p (i.e. may have the same value as p at runtime). This is the set of all nodes such\n+    \/\/ that for each of them, there is a path of def-use edges from p to it consisting of\n+    \/\/ ConstraintCasts, EncodePs, DecodeNs, Phis, CMoves, and AddPs.\n+    Unique_Node_List aliases;\n+\n+    \/\/ If it is known that a node p has not escaped at a control node c1, then it must be the case\n+    \/\/ that p has not escaped at all of the transitive control inputs of c1. Otherwise, there will\n+    \/\/ be a control flow following the path from a transitive input c2 of c1 to c1 in which p has\n+    \/\/ escaped at c2 but has also not escaped at a later point c1, which is impossible. As a\n+    \/\/ result, when p is determined that it has not escaped at a control node, we record that node\n+    \/\/ as well as all of its transitive control inputs here.\n+    Unique_Node_List not_escaped_controls;\n+\n+    \/\/ The result of the analysis whether an object has escaped at a control node\n+    enum EscapeStatus {\n+      ESCAPED,\n+      NOT_ESCAPED,\n+      DEAD_PATH\n+    };\n+\n+    LocalEA() : aliases(), not_escaped_controls() {}\n+\n+    \/\/ Check the escape status of an allocation alloc at a control node ctl. As this is called\n+    \/\/ during igvn, be prepared for non-canonical graph (dead paths, etc).\n+    EscapeStatus check_escape_status(PhaseValues* phase, AllocateNode* alloc, Node* ctl);\n+  };\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -0,0 +1,280 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.escapeAnalysis;\n+\n+import compiler.lib.ir_framework.*;\n+\n+import java.lang.invoke.VarHandle;\n+import java.util.function.Supplier;\n+\n+\/**\n+ * @test\n+ * @bug 8373495\n+ * @summary Test that loads from a newly allocated object are aggressively folded if the object has not escaped\n+ * @library \/test\/lib \/\n+ * @run driver ${test.main.class}\n+ *\/\n+public class TestLoadFolding {\n+    public static class Point {\n+        int x;\n+        int y;\n+\n+        Point() {\n+            x = 1;\n+            y = 2;\n+        }\n+\n+        static final Point DEFAULT = new Point();\n+    }\n+\n+    static Point staticField;\n+\n+    public static void main(String[] args) {\n+        var framework = new TestFramework();\n+        framework.setDefaultWarmup(1);\n+        framework.start();\n+    }\n+\n+    @DontInline\n+    static void escape(Object o) {}\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test11() {\n+        \/\/ p only escapes at return\n+        Point p = new Point();\n+        escape(null);\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test12(boolean b) {\n+        \/\/ p escapes in another branch\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        } else {\n+            escape(null);\n+            p.x += p.y;\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test13(boolean b) {\n+        \/\/ A Phi of p1 and Point.DEFAULT, but a store to Phi is after all the loads from p1\n+        Point p1 = new Point();\n+        Point p = b ? p1 : Point.DEFAULT;\n+        escape(null);\n+        p.x = p1.x + p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public int test14() {\n+        \/\/ Even if p escapes before the loads, if it is legal to execute the loads before the\n+        \/\/ store, then we can fold the loads\n+        Point p = new Point();\n+        escape(null);\n+        staticField = p;\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Point test15(int begin, int end) {\n+        \/\/ Fold the load that is a part of a cycle\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            p.x++;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Point test16(int begin, int end, boolean b) {\n+        \/\/ A cycle and a Phi, this time the store is at a different field\n+        Point p1 = new Point();\n+        \/\/ This store is not on a Phi involving p1, so it does not interfere\n+        Point.DEFAULT.y = 3;\n+        Point p = p1;\n+        for (int i = begin; i < end; i += 2) {\n+            if (b) {\n+                p = p1;\n+            } else {\n+                p = Point.DEFAULT;\n+            }\n+            b = !b;\n+\n+            p.x = p1.y + 3;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p1.x = p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test17(int idx) {\n+        \/\/ Array\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        int res = a[idx & 1];\n+        escape(null);\n+        res += a[0] + a[1];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test18(int idx) {\n+        \/\/ Array, even if we will give up if we encounter a[idx & 1] = 3, we meet a[0] = 4 first,\n+        \/\/ so the load int res = a[0] can still be folded\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        a[0] = 4;\n+        escape(null);\n+        int res = a[0];\n+        escape(a);\n+        return res;\n+    }\n+\n+    static class SupplierHolder {\n+        Supplier<String> f;\n+\n+        static final Supplier<String> DEFAULT_VALUE = () -> \"test\";\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.DYNAMIC_CALL_OF_METHOD, \"get\", IRNode.LOAD_OF_FIELD, \"f\", IRNode.CLASS_CHECK_TRAP}, counts = {IRNode.ALLOC, \"1\"})\n+    public String test19() {\n+        \/\/ Folding of the load o.f allows o.f.get to get devirtualized\n+        SupplierHolder o = new SupplierHolder();\n+        o.f = SupplierHolder.DEFAULT_VALUE;\n+        escape(null);\n+        String res = o.f.get();\n+        escape(o);\n+        return res;\n+    }\n+\n+    @Run(test = {\"test11\", \"test12\", \"test13\", \"test14\", \"test15\", \"test16\", \"test17\", \"test18\", \"test19\"})\n+    public void runPositiveTests() {\n+        test11();\n+        test12(false);\n+        test12(true);\n+        test13(false);\n+        test13(true);\n+        test14();\n+        test15(1, 16);\n+        test16(1, 16, false);\n+        test16(1, 16, true);\n+        test17(0);\n+        test18(0);\n+        test19();\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC, \"1\"})\n+    public int test01() {\n+        Point p = new Point();\n+        staticField = p;\n+        \/\/ Actually, the only fence that requires the following loads to be executed after the\n+        \/\/ store is a fullFence\n+        VarHandle.fullFence();\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test02(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+            \/\/ p escaped, so the load must not be removed\n+            return p.x;\n+        } else {\n+            escape(null);\n+            return 0;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test03(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        }\n+        \/\/ p escaped, so the load must not be removed\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"> 0\", IRNode.ALLOC, \"1\"})\n+    public Point test04(int begin, int end) {\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            \/\/ p escaped here because this is a loop\n+            p.x++;\n+            escape(p);\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test05(int idx) {\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        \/\/ Cannot fold the loads because we do not know which element is written to by\n+        \/\/ a[idx & 1] = 3\n+        return a[0] + a[1];\n+    }\n+\n+    @Run(test = {\"test01\", \"test02\", \"test03\", \"test04\", \"test05\"})\n+    public void runNegativeTests() {\n+        test01();\n+        test02(false);\n+        test02(true);\n+        test03(false);\n+        test03(true);\n+        test04(1, 16);\n+        test05(0);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/escapeAnalysis\/TestLoadFolding.java","additions":280,"deletions":0,"binary":false,"changes":280,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -254,1 +254,1 @@\n-    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"2\" }, phase = CompilePhase.FINAL_CODE)\n@@ -257,2 +257,4 @@\n-        Common.blackhole(o1);\n-        \/\/ This load is directly optimized away by C2.\n+        Common.outer = o1;\n+        \/\/ Prevent the field loads from getting folded by making o1 escape, the fullFence is\n+        \/\/ necessary because otherwise the loads can float above the store and get folded\n+        VarHandle.fullFence();\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"}]}