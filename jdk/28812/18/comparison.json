{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -586,0 +586,3 @@\n+  product(bool, DoLocalEscapeAnalysis, true, DIAGNOSTIC,                    \\\n+          \"Perform local escape analysis during IGVN\")                      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,2 @@\n+#include \"opto\/c2_globals.hpp\"\n+#include \"opto\/callnode.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"opto\/movenode.hpp\"\n@@ -47,0 +50,2 @@\n+#include \"opto\/node.hpp\"\n+#include \"opto\/opcodes.hpp\"\n@@ -693,0 +698,19 @@\n+  const TypePtr* adr_type = this->adr_type();\n+  if (adr_type == nullptr) {\n+    \/\/ This means the access is dead\n+    return phase->C->top();\n+  } else if (adr_type->base() == TypePtr::AnyPtr) {\n+    \/\/ Compile::get_alias_index will complain with these accesses\n+    if (adr_type->ptr() == TypePtr::Null) {\n+      \/\/ Access to null cannot happen, this means the access must be in a dead path\n+      return phase->C->top();\n+    } else {\n+      \/\/ Give up on a very wide access\n+      return nullptr;\n+    }\n+  }\n+\n+  int alias_idx = phase->C->get_alias_index(adr_type);\n+  assert(alias_idx != Compile::AliasIdxTop, \"must not be a dead node\");\n+  assert(alias_idx != Compile::AliasIdxBot || !phase->C->do_aliasing(), \"must not be a very wide access\");\n+\n@@ -703,1 +727,14 @@\n-  int cnt = 50;             \/\/ Cycle limiter\n+  \/\/ If base has not escaped the current compilation unit, we can be more aggressive, walk past\n+  \/\/ calls and memory barriers to find a corresponding store\n+  ResourceMark rm;\n+  bool is_known_instance = addr_t != nullptr && addr_t->is_known_instance_field();\n+  LocalEA local_ea(phase->is_IterGVN(), base);\n+  bool has_not_escaped = is_known_instance;\n+\n+  int cnt = 50; \/\/ Cycle limiter\n+  if (is_Load() && (local_ea.is_candidate() || is_known_instance)) {\n+    \/\/ Get agressive for loads from freshly allocated objects\n+    cnt = 1000;\n+  }\n+\n+  \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n@@ -712,2 +749,4 @@\n-      if (st_base == nullptr)\n-        break;              \/\/ inscrutable pointer\n+      if (st_base == nullptr) {\n+        \/\/ inscrutable pointer\n+        break;\n+      }\n@@ -715,2 +754,12 @@\n-      \/\/ For raw accesses it's not enough to prove that constant offsets don't intersect.\n-      \/\/ We need the bases to be the equal in order for the offset check to make sense.\n+      \/\/ If the bases are the same and the offsets are the same, it seems that this is the exact\n+      \/\/ store we are looking for, the caller will check if the type of the store matches\n+      if (st_base == base && st_offset == offset) {\n+        return mem;\n+      }\n+\n+      \/\/ If it is provable that the memory accessed by mem does not overlap the memory accessed by\n+      \/\/ this, we may walk past mem.\n+      \/\/ For raw accesses, 2 accesses are independent if they have the same base and the offsets\n+      \/\/ say that they do not overlap.\n+      \/\/ For heap accesses, 2 accesses are independent if either the bases are provably different\n+      \/\/ at runtime or the offsets say that the accesses do not overlap.\n@@ -718,0 +767,1 @@\n+        \/\/ Raw accesses can only be provably independent if they have the same base\n@@ -721,0 +771,3 @@\n+      \/\/ If the offsets say that the accesses do not overlap, then it is provable that mem and this\n+      \/\/ do not overlap. For example, a LoadI from Object+8 is independent from a StoreL into\n+      \/\/ Object+12, no matter what the bases are.\n@@ -733,1 +786,1 @@\n-          continue;           \/\/ (a) advance through independent store memory\n+          continue;\n@@ -736,8 +789,4 @@\n-      if (st_base != base &&\n-          detect_ptr_independence(base, alloc,\n-                                  st_base,\n-                                  AllocateNode::Ideal_allocation(st_base),\n-                                  phase)) {\n-        \/\/ Success:  The bases are provably independent.\n-        mem = mem->in(MemNode::Memory);\n-        continue;           \/\/ (a) advance through independent store memory\n+\n+      \/\/ Same base and overlapping offsets, it seems provable that the accesses overlap, give up\n+      if (st_base == base) {\n+        break;\n@@ -746,4 +795,18 @@\n-      \/\/ (b) At this point, if the bases or offsets do not agree, we lose,\n-      \/\/ since we have not managed to prove 'this' and 'mem' independent.\n-      if (st_base == base && st_offset == offset) {\n-        return mem;         \/\/ let caller handle steps (c), (d)\n+      \/\/ Try to prove that 2 different base nodes at compile time are different values at runtime\n+      bool known_independent = false;\n+      if (has_not_escaped && !is_known_instance) {\n+        \/\/ Since we are walking from a node to its input, if alloc is found that it has not escaped\n+        \/\/ at an earlier iteration, it must also be found that it has not escaped at the current\n+        \/\/ iteration\n+        assert(local_ea.not_escaped_controls().member(mem->in(0)), \"inconsistent\");\n+\n+        \/\/ If base is the result of an allocation that has not escaped, we can know all the nodes\n+        \/\/ that may have the same runtime value as base, these are the transitive outputs of base\n+        \/\/ along some chains that consist of ConstraintCasts, EncodePs, DecodeNs, Phis, and CMoves\n+        known_independent = !local_ea.aliases().member(st_base);\n+      } else if (detect_ptr_independence(base, alloc, st_base,\n+                                         AllocateNode::Ideal_allocation(st_base),\n+                                         phase)) {\n+        \/\/ detect_ptr_independence == true means that it can prove that base and st_base cannot\n+        \/\/ have the same runtime value\n+        known_independent = true;\n@@ -752,0 +815,4 @@\n+      if (known_independent) {\n+        mem = mem->in(MemNode::Memory);\n+        continue;\n+      }\n@@ -772,1 +839,0 @@\n-        int alias_idx = phase->C->get_alias_index(adr_type());\n@@ -787,1 +853,0 @@\n-\n@@ -795,8 +860,10 @@\n-    } else if (addr_t != nullptr && addr_t->is_known_instance_field()) {\n-      \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n-      if (mem->is_Proj() && mem->in(0)->is_Call()) {\n-        \/\/ ArrayCopyNodes processed here as well.\n-        CallNode *call = mem->in(0)->as_Call();\n-        if (!call->may_modify(addr_t, phase)) {\n-          mem = call->in(TypeFunc::Memory);\n-          continue;         \/\/ (a) advance through independent call memory\n+    } else if (mem->is_MergeMem()) {\n+      mem = mem->as_MergeMem()->memory_at(alias_idx);\n+      continue;\n+    } else if (mem->is_Proj() && mem->in(0)->is_Call()) {\n+      \/\/ We can walk past a call if we can prove that the call does not modify the memory we are\n+      \/\/ accessing, this is the case if the allocation has not escaped at this call\n+      CallNode* call = mem->in(0)->as_Call();\n+      if (!has_not_escaped) {\n+        if (!is_Load()) {\n+          return nullptr;\n@@ -804,4 +871,6 @@\n-      } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n-        ArrayCopyNode* ac = nullptr;\n-        if (ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n-          break;\n+\n+        LocalEA::EscapeStatus status = local_ea.check_escape_status(call);\n+        switch (status) {\n+          case LocalEA::ESCAPED:     return nullptr;\n+          case LocalEA::NOT_ESCAPED: has_not_escaped = true; break;\n+          case LocalEA::DEAD_PATH:   return phase->C->top();\n@@ -809,10 +878,28 @@\n-        mem = mem->in(0)->in(TypeFunc::Memory);\n-        continue;           \/\/ (a) advance through independent MemBar memory\n-      } else if (mem->is_ClearArray()) {\n-        if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n-          \/\/ (the call updated 'mem' value)\n-          continue;         \/\/ (a) advance through independent allocation memory\n-        } else {\n-          \/\/ Can not bypass initialization of the instance\n-          \/\/ we are looking for.\n-          return mem;\n+      } else if (!is_known_instance) {\n+        \/\/ Since we are walking from a node to its input, if alloc is found that it has not escaped\n+        \/\/ at an earlier iteration, it must also be found that it has not escaped at the current\n+        \/\/ iteration\n+        assert(local_ea.not_escaped_controls().member(call), \"inconsistent\");\n+      }\n+\n+      \/\/ We are more aggressive with known instances. For example, if base is an argument of call\n+      \/\/ and call is an invocation of unsafe_arraycopy, the global escape analyzer can consider\n+      \/\/ base not to escape, while the local escape analyzer will be more conservative.\n+      if (!is_known_instance || !call->may_modify(addr_t, phase)) {\n+        mem = call->in(TypeFunc::Memory);\n+        continue;         \/\/ (a) advance through independent call memory\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n+      \/\/ We can walk past a memory barrier if we can prove that the allocation has not escaped at\n+      \/\/ this barrier, hence it is invisible to other threads\n+      MemBarNode* membar = mem->in(0)->as_MemBar();\n+      if (!has_not_escaped) {\n+        if (!is_Load()) {\n+          return nullptr;\n+        }\n+\n+        LocalEA::EscapeStatus status = local_ea.check_escape_status(membar);\n+        switch (status) {\n+          case LocalEA::ESCAPED:     return nullptr;\n+          case LocalEA::NOT_ESCAPED: has_not_escaped = true; break;\n+          case LocalEA::DEAD_PATH:   return phase->C->top();\n@@ -820,4 +907,25 @@\n-      } else if (mem->is_MergeMem()) {\n-        int alias_idx = phase->C->get_alias_index(adr_type());\n-        mem = mem->as_MergeMem()->memory_at(alias_idx);\n-        continue;           \/\/ (a) advance through independent MergeMem memory\n+      } else if (!is_known_instance) {\n+        \/\/ Since we are walking from a node to its input, if alloc is found that it has not escaped\n+        \/\/ at an earlier iteration, it must also be found that it has not escaped at the current\n+        \/\/ iteration\n+        assert(local_ea.not_escaped_controls().member(membar), \"inconsistent\");\n+      }\n+\n+      \/\/ We are more aggressive with known instances. For example, if base is an argument of call\n+      \/\/ and call is an ArrayCopyNode, the global escape analyzer can consider base not to escape,\n+      \/\/ while the local escape analyzer will be more conservative. This case is about the trailing\n+      \/\/ memory barrier of an ArrayCopyNode.\n+      ArrayCopyNode* ac = nullptr;\n+      if (is_known_instance && ArrayCopyNode::may_modify(addr_t, membar, phase, ac)) {\n+        break;\n+      }\n+      mem = mem->in(0)->in(TypeFunc::Memory);\n+      continue;\n+    } else if (is_known_instance && mem->is_ClearArray()) {\n+      if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n+        \/\/ (the call updated 'mem' value)\n+        continue;         \/\/ (a) advance through independent allocation memory\n+      } else {\n+        \/\/ Can not bypass initialization of the instance\n+        \/\/ we are looking for.\n+        return mem;\n@@ -1263,0 +1371,208 @@\n+\/\/ Construct a LocalEA object that inspects a node for escape analysis. This constructor will\n+\/\/ calculate _is_candidate and _aliases.\n+MemNode::LocalEA::LocalEA(PhaseIterGVN* phase, Node* base) : _phase(phase), _is_candidate(true), _aliases(), _not_escaped_controls() {\n+  if (!DoLocalEscapeAnalysis || phase == nullptr || !phase->type(base)->isa_oopptr()) {\n+    _is_candidate = false;\n+    return;\n+  }\n+\n+  ciEnv* env = _phase->C->env();\n+  if (env->should_retain_local_variables() || env->jvmti_can_walk_any_space()) {\n+    \/\/ JVMTI can modify local objects, so give up\n+    _is_candidate = false;\n+    return;\n+  }\n+\n+  \/\/ Collect all nodes in _aliases.\n+  \/\/ Firstly, we traverse the graph from use to def, this visits all the allocations that may alias\n+  \/\/ base. Other nodes are collected as well, they are not important in this step as they should be\n+  \/\/ collected during the second step anyway.\n+  _aliases.push(base);\n+  for (uint idx = 0; idx < _aliases.size(); idx++) {\n+    Node* n = _aliases.at(idx);\n+    if (AllocateNode::Ideal_allocation(n) != nullptr) {\n+      continue;\n+    }\n+\n+    if (n->is_ConstraintCast() || n->is_DecodeN() || n->is_EncodeP()) {\n+      _aliases.push(n->in(1));\n+    } else if (n->is_Phi()) {\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr && !phase->type(in)->empty()) {\n+          _aliases.push(in);\n+        }\n+      }\n+    } else if (n->is_CMove()) {\n+      Node* if_false = n->in(CMoveNode::IfFalse);\n+      if (if_false != nullptr && !phase->type(if_false)->empty()) {\n+        _aliases.push(if_false);\n+      }\n+      Node* if_true = n->in(CMoveNode::IfTrue);\n+      if (if_true != nullptr && !phase->type(if_true)->empty()) {\n+        _aliases.push(if_true);\n+      }\n+    } else {\n+      \/\/ If base is not an allocation or a Phi of allocations (e.g. it is a Phi of an allocation\n+      \/\/ and a load), we cannot perform escape analysis. This branch is conservative.\n+      _is_candidate = false;\n+      _aliases.clear();\n+      return;\n+    }\n+  }\n+\n+  \/\/ Secondly, from the set of allocations that may alias base, collect all nodes that may alias\n+  \/\/ them, they may alias base as well. Actually, there may be cases that a may alias b and b may\n+  \/\/ alias c but a may not alias c, but we are conservative here.\n+  for (uint idx = 0; idx < _aliases.size(); idx++) {\n+    Node* n = _aliases.at(idx);\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (out->is_ConstraintCast() || out->is_EncodeP() || out->is_DecodeN() ||\n+          out->is_Phi() || out->is_CMove()) {\n+        _aliases.push(out);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Check whether the inspected node p has escaped at a certain control node ctl, p has not escaped\n+\/\/ at ctl if there is no node that:\n+\/\/ 1. Make p escape.\n+\/\/ 2. Either:\n+\/\/   a. Has no control input.\n+\/\/   b. Has a control input that is ctl or a transitive control input of ctl.\n+\/\/\n+\/\/ In other word, p is determined that it has not escaped at ctl if all nodes that make p escape\n+\/\/ have a control input that is neither nullptr, ctl, nor a transitive control input of ctl.\n+MemNode::LocalEA::EscapeStatus MemNode::LocalEA::check_escape_status(Node* ctl) {\n+  if (!_is_candidate) {\n+    return ESCAPED;\n+  }\n+  if (_phase->type(ctl) == Type::TOP) {\n+    return DEAD_PATH;\n+  }\n+\n+  \/\/ Find all transitive control inputs of ctl that are not dead, if it is determined that alloc\n+  \/\/ has not escaped at ctl, then it must be the case that it has not escaped at all of these\n+  assert(_not_escaped_controls.size() == 0, \"must not be computed yet\");\n+  Node* start = _phase->C->start();\n+  _not_escaped_controls.push(ctl);\n+  for (uint control_idx = 0; control_idx < _not_escaped_controls.size(); control_idx++) {\n+    Node* n = _not_escaped_controls.at(control_idx);\n+    assert(_phase->type(n) == Type::CONTROL || _phase->type(n)->base() == Type::Tuple, \"must be a control node %s\", n->Name());\n+    if (n == start) {\n+      continue;\n+    }\n+\n+    if (n->is_Region()) {\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr && _phase->type(in) != Type::TOP) {\n+          _not_escaped_controls.push(in);\n+        }\n+      }\n+    } else {\n+      Node* in = n->in(0);\n+      if (in != nullptr && _phase->type(in) != Type::TOP) {\n+        _not_escaped_controls.push(in);\n+      }\n+    }\n+  }\n+\n+  if (!_not_escaped_controls.member(start)) {\n+    \/\/ If there is no control path from ctl to start, ctl is a dead path, give up\n+    _not_escaped_controls.clear();\n+    return DEAD_PATH;\n+  }\n+\n+  \/\/ Find all nodes that may escape alloc, and decide that it is provable that they must be\n+  \/\/ executed after ctl\n+  ResourceMark rm;\n+  \/\/ If any of these nodes escapes, then we conservatively say that the inspected node p escapes\n+  Unique_Node_List dependencies;\n+  for (uint i = 0; i < _aliases.size(); i++) {\n+    \/\/ If any node that may alias p escapes, then p escapes\n+    dependencies.push(_aliases.at(i));\n+  }\n+  EscapeStatus res = NOT_ESCAPED;\n+  for (uint idx = 0; idx < dependencies.size(); idx++) {\n+    Node* n = dependencies.at(idx);\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (out->is_ConstraintCast() || out->is_EncodeP() || out->is_DecodeN() ||\n+          out->is_Phi() || out->is_CMove()) {\n+        \/\/ A node that may alias n, if it escapes, then n escapes, which leads to p escaping\n+        dependencies.push(out);\n+        continue;\n+      } else if (out->is_AddP()) {\n+        \/\/ Some runtime calls receive a derived pointer but not its base, so we consider these\n+        \/\/ derived pointers dependency, too\n+        dependencies.push(out);\n+        continue;\n+      }\n+\n+      Node* c = out->in(0);\n+      if (c != nullptr && !_not_escaped_controls.member(c)) {\n+        \/\/ c is not a live transitive control input of ctl, so out is not executed before ctl,\n+        \/\/ which means it does not affect the escape status of alloc at ctl\n+        continue;\n+      }\n+\n+      if (out->is_Load()) {\n+        \/\/ A Load does not escape alloc\n+      } else if (out->is_Mem()) {\n+        \/\/ A Store or a LoadStore\n+        if (n == out->in(MemNode::ValueIn)) {\n+          \/\/ You may wonder if we can reason about the escape status of the destination memory\n+          \/\/ here so that we can determine that an object has not escaped if the object into which\n+          \/\/ it is stored has not escaped. Unfortunately, this store breaks _aliases, because there\n+          \/\/ is now a memory that can alias the object we are analyzed, and a load from such memory\n+          \/\/ is not visited by this analysis. For example:\n+          \/\/   Object o = new Object;\n+          \/\/   Holder h = new Holder;\n+          \/\/   h.o = o;\n+          \/\/   do_something();\n+          \/\/   Object p = h.o;\n+          \/\/   escape(p);\n+          \/\/ Then, o escapes at escape(p), but we will not visit that. In order for this to work,\n+          \/\/ the constructor needs to be more conservative when it collects _aliases.\n+          res = ESCAPED;\n+          break;\n+        } else if (n == out->in(MemNode::Address) && (!out->is_Store() || out->as_Store()->is_mismatched_access())) {\n+          \/\/ Mismatched accesses can lie in a different alias class and are protected by memory\n+          \/\/ barriers, so we cannot be aggressive and walk past memory barriers if there is a\n+          \/\/ mismatched store into it. LoadStoreNodes are also lumped here because there is no\n+          \/\/ LoadStoreNode::is_mismatched_access.\n+          res = ESCAPED;\n+          break;\n+        }\n+      } else if (out->is_Call()) {\n+        if (!out->is_AbstractLock() && out->as_Call()->has_non_debug_use(n)) {\n+          \/\/ A call that receives an object as an argument makes that object escape\n+          res = ESCAPED;\n+          break;\n+        }\n+      } else if (out->is_SafePoint()) {\n+        \/\/ Non-call safepoints are pure control nodes\n+      } else if (out->Opcode() == Op_Blackhole) {\n+        \/\/ Blackhole does not escape an object (in the sense that it does not make its field values\n+        \/\/ unpredictable)\n+      } else {\n+        \/\/ Conservatively consider all other nodes to make alloc escape\n+        res = ESCAPED;\n+        break;\n+      }\n+    }\n+\n+    if (res == ESCAPED) {\n+      break;\n+    }\n+  }\n+\n+  if (res == ESCAPED) {\n+    _not_escaped_controls.clear();\n+  }\n+  return res;\n+}\n+\n@@ -1885,0 +2201,5 @@\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+    return nullptr;\n+  }\n+\n@@ -1888,1 +2209,1 @@\n-  if (can_reshape && (addr_t != nullptr)) {\n+  if (addr_t != nullptr) {\n@@ -1921,1 +2242,1 @@\n-  if (in(0) != nullptr && !adr_type()->isa_rawptr() && can_reshape) {\n+  if (in(0) != nullptr && !adr_type()->isa_rawptr()) {\n@@ -1953,0 +2274,2 @@\n+  \/\/ This performs complex analysis that requires a complete graph.\n+  assert(can_reshape, \"should be in IGVN\");\n@@ -1954,0 +2277,4 @@\n+  if (prev_mem != nullptr && prev_mem->is_top()) {\n+    \/\/ find_previous_store returns top when the access is dead\n+    return prev_mem;\n+  }\n@@ -3553,0 +3880,4 @@\n+      if (prev_mem != nullptr && prev_mem->is_top()) {\n+        \/\/ find_previous_store returns top when the access is dead\n+        return prev_mem;\n+      }\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":380,"deletions":49,"binary":false,"changes":429,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -95,0 +95,76 @@\n+  \/\/ During igvn, in order to reason about the state of the memory a MemNode accesses, we can\n+  \/\/ inspect the escape status of that memory. A memory location is said to escape the compilation\n+  \/\/ unit if it is made visible outside the compilation unit, which is equivalent to the object\n+  \/\/ containing that memory visible outside the compilation unit. An object is in the not-escaped\n+  \/\/ state when it is just allocated, and stay so until its reference or a derived pointer of its\n+  \/\/ is stored into the memory, passed into a method invocation, or used as an input to a node we\n+  \/\/ do not know.\n+  \/\/\n+  \/\/ If an object has not escaped, it cannot be modified during a method invocation, and the\n+  \/\/ reordering of memory accesses into it is invisible to other threads. As a result, to find the\n+  \/\/ value that is stored at one of its fields, we can aggressively walk the memory graph past\n+  \/\/ CallNodes and MemBarNodes, this gives a much better chance to fold a load or a store.\n+  \/\/\n+  \/\/ This analysis is local, which means it only inspects a small part of the graph to determine\n+  \/\/ the escape status of an object. It is context-aware, which means it does not just try to find\n+  \/\/ a global escape status of an object, but it also tries to determine which node must observe\n+  \/\/ that the object has escaped. If an access into an object does not observe that the object has\n+  \/\/ escaped, it can try folding aggressively even though the object may escape afterwards.\n+  \/\/\n+  \/\/ It can be proved that if a node must observe that an object has escaped, then there is a path\n+  \/\/ of use-def edges from that node to the node which makes the object escape. This approach,\n+  \/\/ however, requires a global analysis as we need to walk the whole graph. As a result, we go for\n+  \/\/ a more conservative approach which inspects only the control subgraph of the IR graph. See\n+  \/\/ check_escape_status below.\n+  \/\/\n+  \/\/ Perform this local analysis during igvn has the advantage that it allows the folding to happen\n+  \/\/ earlier, before incremental inlining. This means that devirtualized method invocations can get\n+  \/\/ inlined. On the other hand, if the folding happens during the escape analysis phase, inlining\n+  \/\/ is already over.\n+  class LocalEA {\n+  private:\n+    PhaseIterGVN* _phase;\n+\n+    \/\/ If the node being inspected p is eligible for escape analysis. For example, if p is the\n+    \/\/ result cast of an allocation, or if it is a Phi and its inputs are the result casts of some\n+    \/\/ allocations, then it is eligible for escape analysis. On the other hand, if p is a load from\n+    \/\/ memory, or the return value of a Java call, then we cannot do escape analysis on it.\n+    bool _is_candidate;\n+\n+    \/\/ If the node being inspected p has not escaped from the compilation unit, this is the set of\n+    \/\/ all nodes that can alias p (i.e. may have the same value as p at runtime). This is the set\n+    \/\/ of all allocations that may alias p (if p is the result cast of an allocation, then that is\n+    \/\/ the only allocation that may alias p, otherwise, if p is a Phi and its inputs are the result\n+    \/\/ casts of some allocations, then those inputs may alias p) and all nodes q such that, there\n+    \/\/ is an allocation alloc such that alloc may alias p and there is a path of def-use edges from\n+    \/\/ the result cast of alloc to q consisting of ConstraintCasts, EncodePs, DecodeNs, Phis, and\n+    \/\/ CMoves.\n+    Unique_Node_List _aliases;\n+\n+    \/\/ If it is known that the node being inspected p has not escaped at a control node c1, then it\n+    \/\/ must be the case that p has not escaped at all of the transitive control inputs of c1.\n+    \/\/ Otherwise, there will be a control flow following the path from a transitive input c2 of c1\n+    \/\/ to c1 in which p has escaped at c2 but has also not escaped at a later point c1, which is\n+    \/\/ impossible. As a result, when p is determined that it has not escaped at a control node, we\n+    \/\/ record that node as well as all of its transitive control inputs here.\n+    Unique_Node_List _not_escaped_controls;\n+\n+  public:\n+    LocalEA(PhaseIterGVN* phase, Node* base);\n+\n+    bool is_candidate() const { return _is_candidate; }\n+    const Unique_Node_List& aliases() const { return _aliases; }\n+    const Unique_Node_List& not_escaped_controls() const { return _not_escaped_controls; }\n+\n+    \/\/ The result of the analysis whether an object has escaped at a control node\n+    enum EscapeStatus {\n+      ESCAPED,\n+      NOT_ESCAPED,\n+      DEAD_PATH\n+    };\n+\n+    \/\/ Check the escape status of the node being inspected p at a control node ctl. As this is\n+    \/\/ called during igvn, be prepared for non-canonical graph (dead paths, etc).\n+    EscapeStatus check_escape_status(Node* ctl);\n+  };\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":77,"deletions":1,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -0,0 +1,487 @@\n+\/*\n+ * Copyright (c) 2026, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.escapeAnalysis;\n+\n+import compiler.lib.ir_framework.*;\n+import java.lang.invoke.VarHandle;\n+import java.util.function.Supplier;\n+import jdk.test.lib.Asserts;\n+\n+\/**\n+ * @test\n+ * @bug 8373495\n+ * @summary Test that loads from a newly allocated object are aggressively folded if the object has not escaped\n+ * @library \/test\/lib \/\n+ * @run driver ${test.main.class}\n+ *\/\n+public class TestLoadFolding {\n+    public static class Point {\n+        int x;\n+        int y;\n+\n+        Point() {\n+            this(1, 2);\n+        }\n+\n+        Point(int x, int y) {\n+            this.x = x;\n+            this.y = y;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            return o instanceof Point p && x == p.x && y == p.y;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Point[\" + x + \", \" + y + \"]\";\n+        }\n+    }\n+\n+    public static class PointHolder {\n+        Point p;\n+    }\n+\n+    public static void main(String[] args) {\n+        var framework = new TestFramework();\n+        framework.setDefaultWarmup(1);\n+        framework.addScenarios(new Scenario(0, \"-XX:+UnlockDiagnosticVMOptions\", \"-XX:-DoLocalEscapeAnalysis\"),\n+                new Scenario(1, \"-XX:+UnlockDiagnosticVMOptions\", \"-XX:+DoLocalEscapeAnalysis\"));\n+        framework.start();\n+    }\n+\n+    @DontInline\n+    static void escape(Object o) {}\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test101() {\n+        \/\/ p only escapes at return\n+        Point p = new Point();\n+        escape(null);\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test102(boolean b) {\n+        \/\/ p escapes in another branch\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        } else {\n+            escape(null);\n+            p.x += p.y;\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test103(Point p, boolean b) {\n+        \/\/ A Phi of p1 and p, but a store to Phi is after all the loads from p1\n+        Point p1 = new Point();\n+        if (b) {\n+            p = new Point();\n+        }\n+        escape(null);\n+        p.x = p1.x + p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public int test104(PointHolder h) {\n+        \/\/ Even if p escapes before the loads, if it is legal to execute the loads before the\n+        \/\/ store, then we can fold the loads\n+        Point p = new Point();\n+        escape(null);\n+        h.p = p;\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test105(int begin, int end) {\n+        \/\/ Fold the load that is a part of a cycle\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            p.x++;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test106(Point p2, int begin, int end, boolean b) {\n+        \/\/ A cycle and a Phi, this time the store is at a different field\n+        Point p1 = new Point();\n+        \/\/ This store is not on a Phi involving p1, so it does not interfere\n+        p2.y = 2;\n+        Point p = p1;\n+        for (int i = begin; i < end; i *= 2) {\n+            if (b) {\n+                p = p1;\n+            } else {\n+                p = p2;\n+            }\n+            b = !b;\n+\n+            p.x = p1.y + 3;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p1.x = p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test107(int idx) {\n+        \/\/ Array\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        int res = a[idx & 1];\n+        escape(null);\n+        res += a[0] + a[1];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test108(int idx) {\n+        \/\/ Array, even if we will give up if we encounter a[idx & 1] = 3, we meet a[0] = 4 first,\n+        \/\/ so the load int res = a[0] can still be folded\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        a[0] = 4;\n+        escape(null);\n+        int res = a[0];\n+        escape(a);\n+        return res;\n+    }\n+\n+    static class SupplierHolder {\n+        Supplier<String> f;\n+\n+        static final Supplier<String> DEFAULT_VALUE = () -> \"test\";\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"},\n+        failOn = {IRNode.DYNAMIC_CALL_OF_METHOD, \"get\", IRNode.LOAD_OF_FIELD, \"f\", IRNode.CLASS_CHECK_TRAP},\n+        counts = {IRNode.ALLOC, \"1\"})\n+    public String test109() {\n+        \/\/ Folding of the load o.f allows o.f.get to get devirtualized\n+        SupplierHolder o = new SupplierHolder();\n+        o.f = SupplierHolder.DEFAULT_VALUE;\n+        escape(null);\n+        String res = o.f.get();\n+        escape(o);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"2\"})\n+    public int test110(PointHolder h, boolean b) {\n+        \/\/ Inspect the escape status of a Phi\n+        Point p1 = new Point();\n+        Point p2 = new Point();\n+        Point p = b ? p1 : p2;\n+        p.x = 4;\n+        escape(null);\n+        h.p = p1;\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"2\"})\n+    public int test111(int begin, int end, boolean b) {\n+        \/\/ Inspect the escape status of a loop Phi\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            if (b) {\n+                p = new Point();\n+            }\n+        }\n+        p.x = 4;\n+        escape(null);\n+        int res = p.x;\n+        escape(p);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.ALLOC, \"1\"})\n+    public int test112() {\n+        \/\/ The object has been stored into memory but the destination does not escape\n+        PointHolder h = new PointHolder();\n+        Point p = new Point();\n+        h.p = p;\n+        VarHandle.fullFence();\n+        int res = p.x;\n+        escape(p);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.ALLOC, \"2\"})\n+    public int test113() {\n+        \/\/ The object has been stored into memory but the destination has not escaped\n+        PointHolder h = new PointHolder();\n+        Point p = new Point();\n+        h.p = p;\n+        VarHandle.fullFence();\n+        int res = p.x;\n+        escape(h);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.ALLOC, \"3\"})\n+    public int test114(boolean b) {\n+        \/\/ A Phi has been stored into memory but the destination has not escaped\n+        PointHolder h = new PointHolder();\n+        Point p1 = new Point();\n+        Point p2 = new Point();\n+        h.p = b ? p1 : p2;\n+        VarHandle.fullFence();\n+        int res = p1.x;\n+        escape(h);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"DoLocalEscapeAnalysis\", \"true\"}, counts = {IRNode.ALLOC, \"3\"})\n+    public int test115(boolean b) {\n+        \/\/ The object has been stored into a Phi but the destination has not escaped\n+        PointHolder h1 = new PointHolder();\n+        PointHolder h2 = new PointHolder();\n+        Point p = new Point();\n+        PointHolder h = b ? h1 : h2;\n+        h.p = p;\n+        VarHandle.fullFence();\n+        int res = p.x;\n+        escape(h1);\n+        return res;\n+    }\n+\n+    @Run(test = {\"test101\", \"test102\", \"test103\", \"test104\", \"test105\", \"test106\", \"test107\", \"test108\", \"test109\",\n+                 \"test110\", \"test111\", \"test112\", \"test113\", \"test114\", \"test115\"})\n+    public void runPositiveTests() {\n+        Asserts.assertEQ(new Point(3, 2), test101());\n+        Asserts.assertEQ(new Point(3, 2), test102(false));\n+        Asserts.assertEQ(new Point(1, 2), test102(true));\n+        Asserts.assertEQ(new Point(3, 2), test103(new Point(), false));\n+        Asserts.assertEQ(new Point(3, 2), test103(new Point(), true));\n+        Asserts.assertEQ(3, test104(new PointHolder()));\n+        Asserts.assertEQ(new Point(7, 2), test105(1, 16));\n+        Asserts.assertEQ(new Point(2, 2), test106(new Point(), 1, 16, false));\n+        Asserts.assertEQ(new Point(5, 2), test106(new Point(), 1, 16, true));\n+        Asserts.assertEQ(4, test107(0));\n+        Asserts.assertEQ(4, test108(0));\n+        Asserts.assertEQ(\"test\", test109());\n+        Asserts.assertEQ(4, test110(new PointHolder(), false));\n+        Asserts.assertEQ(4, test110(new PointHolder(), true));\n+        Asserts.assertEQ(4, test111(1, 16, false));\n+        Asserts.assertEQ(4, test111(1, 16, true));\n+        Asserts.assertEQ(1, test112());\n+        Asserts.assertEQ(1, test113());\n+        Asserts.assertEQ(1, test114(false));\n+        Asserts.assertEQ(1, test114(true));\n+        Asserts.assertEQ(1, test115(false));\n+        Asserts.assertEQ(1, test115(true));\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC, \"1\"})\n+    public int test001(PointHolder h) {\n+        Point p = new Point();\n+        h.p = p;\n+        \/\/ Actually, the only fence that requires the following loads to be executed after the\n+        \/\/ store is a fullFence\n+        VarHandle.fullFence();\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test002(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+            \/\/ p escaped, so the load must not be removed\n+            return p.x;\n+        } else {\n+            escape(null);\n+            return 0;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test003(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        }\n+        \/\/ p escaped, so the load must not be removed\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"> 0\", IRNode.ALLOC, \"1\"})\n+    public Point test004(int begin, int end) {\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            \/\/ p escaped here because this is a loop\n+            p.x++;\n+            escape(p);\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test005(int idx) {\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        \/\/ Cannot fold the loads because we do not know which element is written to by\n+        \/\/ a[idx & 1] = 3\n+        return a[0] + a[1];\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test006(Point p, boolean b) {\n+        \/\/ A Phi with an input ineligible for escape analysis\n+        if (b) {\n+            p = new Point();\n+        }\n+        escape(null);\n+        int res = p.x;\n+        escape(p);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"2\"})\n+    public int test007(boolean b) {\n+        \/\/ A Phi that escapes because an input escapes\n+        Point p1 = new Point();\n+        Point p2 = new Point();\n+        Point p = b ? p1 : p2;\n+        escape(p1);\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"2\"})\n+    public int test008() {\n+        \/\/ An object is stored into another object that escapes\n+        PointHolder h = new PointHolder();\n+        Point p = new Point();\n+        h.p = p;\n+        escape(h);\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"2\"})\n+    public int test009(PointHolder h, boolean b) {\n+        \/\/ An object is stored into a Phi that is ineligible for escape analysis\n+        if (b) {\n+            h = new PointHolder();\n+        }\n+        Point p = new Point();\n+        h.p = p;\n+        escape(null);\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"3\"})\n+    public int test010(boolean b) {\n+        \/\/ An object is stored into a Phi that escapes because one of its inputs escapes\n+        PointHolder h1 = new PointHolder();\n+        PointHolder h2 = new PointHolder();\n+        PointHolder h = b ? h1 : h2;\n+        Point p = new Point();\n+        h.p = p;\n+        escape(h1);\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"4\"})\n+    public int test011(boolean b1, boolean b2) {\n+        \/\/ A Phi escapes because one of its inputs is stored into a Phi, that in turn escapes\n+        \/\/ because one of its inputs escapes\n+        PointHolder h1 = new PointHolder();\n+        PointHolder h2 = new PointHolder();\n+        PointHolder h = b1 ? h1 : h2;\n+        Point p1 = new Point();\n+        Point p2 = new Point();\n+        Point p = b2 ? p1 : p2;\n+        h.p = p1;\n+        escape(h2);\n+        return p.x;\n+    }\n+\n+    @Run(test = {\"test001\", \"test002\", \"test003\", \"test004\", \"test005\", \"test006\", \"test007\", \"test008\", \"test009\",\n+                 \"test010\", \"test011\"})\n+    public void runNegativeTests() {\n+        Asserts.assertEQ(3, test001(new PointHolder()));\n+        Asserts.assertEQ(0, test002(false));\n+        Asserts.assertEQ(1, test002(true));\n+        Asserts.assertEQ(1, test003(false));\n+        Asserts.assertEQ(1, test003(true));\n+        Asserts.assertEQ(new Point(5, 2), test004(1, 16));\n+        Asserts.assertEQ(5, test005(0));\n+        Asserts.assertEQ(1, test006(new Point(), false));\n+        Asserts.assertEQ(1, test006(new Point(), true));\n+        Asserts.assertEQ(1, test007(false));\n+        Asserts.assertEQ(1, test007(true));\n+        Asserts.assertEQ(1, test008());\n+        Asserts.assertEQ(1, test009(new PointHolder(), false));\n+        Asserts.assertEQ(1, test009(new PointHolder(), true));\n+        Asserts.assertEQ(1, test010(false));\n+        Asserts.assertEQ(1, test010(true));\n+        Asserts.assertEQ(1, test011(false, false));\n+        Asserts.assertEQ(1, test011(false, true));\n+        Asserts.assertEQ(1, test011(true, false));\n+        Asserts.assertEQ(1, test011(true, true));\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/escapeAnalysis\/TestLoadFolding.java","additions":487,"deletions":0,"binary":false,"changes":487,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2026, Oracle and\/or its affiliates. All rights reserved.\n@@ -254,1 +254,1 @@\n-    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"2\" }, phase = CompilePhase.FINAL_CODE)\n@@ -257,2 +257,4 @@\n-        Common.blackhole(o1);\n-        \/\/ This load is directly optimized away by C2.\n+        Common.outer = o1;\n+        \/\/ Prevent the field loads from getting folded by making o1 escape, the fullFence is\n+        \/\/ necessary because otherwise the loads can float above the store and get folded\n+        VarHandle.fullFence();\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"}]}