{"files":[{"patch":"@@ -123,1 +123,2 @@\n-  \/\/ Walk the control graph to find a node that makes base escape\n+  \/\/ Find all control nodes from ctl to alloc, alloc must dominate ctl, which means all paths from\n+  \/\/ ctl must arrive at alloc\n@@ -129,1 +130,2 @@\n-    if (n == alloc || n->is_Start()) {\n+    assert(!n->is_Start(), \"alloc must dominate ctl\");\n+    if (n == alloc) {\n@@ -133,4 +135,2 @@\n-    \/\/ A call that receives an object as an argument makes that object escape\n-    if (n->is_Call() && !n->is_AbstractLock()) {\n-      const TypeTuple* d = n->as_Call()->tf()->domain();\n-      for (uint i = TypeFunc::Parms; i < d->cnt(); i++) {\n+    if (n->is_Region()) {\n+      for (uint i = 1; i < n->req(); i++) {\n@@ -138,2 +138,2 @@\n-        if (in != nullptr && aliases.member(in)) {\n-          return false;\n+        if (in != nullptr && !in->is_top()) {\n+          controls.push(in);\n@@ -142,0 +142,5 @@\n+    } else {\n+      Node* in = n->in(0);\n+      if (in != nullptr && !in->is_top()) {\n+        controls.push(in);\n+      }\n@@ -143,0 +148,1 @@\n+  }\n@@ -144,0 +150,3 @@\n+  \/\/ Find all nodes that may escape alloc, and see whether they may be between ctl and alloc\n+  for (uint idx = 0; idx < aliases.size(); idx++) {\n+    Node* n = aliases.at(idx);\n@@ -146,15 +155,10 @@\n-      if (!out->is_Mem() || out->is_Load()) {\n-        continue;\n-      }\n-\n-      \/\/ If an object is stored to memory, then it escapes\n-      if (aliases.member(out->in(MemNode::ValueIn))) {\n-        return false;\n-      }\n-\n-      \/\/ Mismatched accesses can lie in a different alias class and are protected by memory\n-      \/\/ barriers, so we cannot be aggressive and walk past memory barriers if there is a\n-      \/\/ mismatched store into it. LoadStoreNodes are also lumped here because there is no\n-      \/\/ LoadStoreNode::is_mismatched_access.\n-      if (aliases.member(out->in(MemNode::Address)) && (!out->is_Store() || out->as_Store()->is_mismatched_access())) {\n-        return false;\n+      Node* c = out->in(0);\n+      if (c != nullptr) {\n+        if (c->is_top()) {\n+          \/\/ out is a dead node, ignore it\n+          continue;\n+        } else if (!controls.member(c)) {\n+          \/\/ out is not executed before ctl, so it does not affect the escape status of alloc at\n+          \/\/ ctl\n+          continue;\n+        }\n@@ -162,1 +166,0 @@\n-    }\n@@ -164,5 +167,15 @@\n-    if (n->is_Region()) {\n-      for (uint i = 1; i < n->req(); i++) {\n-        Node* in = n->in(i);\n-        if (in != nullptr) {\n-          controls.push(in);\n+      if (aliases.member(out)) {\n+        \/\/ Just a node that may alias n, such as Phi, CMove, CastPP\n+      } else if (out->is_Load()) {\n+        \/\/ A Load does not escape alloc\n+      } else if (out->is_Mem()) {\n+        \/\/ A Store or a LoadStore\n+        if (n == out->in(MemNode::ValueIn)) {\n+          \/\/ If an object is stored to memory, then it escapes\n+          return false;\n+        } else if (n == out->in(MemNode::Address) && (!out->is_Store() || out->as_Store()->is_mismatched_access())) {\n+          \/\/ Mismatched accesses can lie in a different alias class and are protected by memory\n+          \/\/ barriers, so we cannot be aggressive and walk past memory barriers if there is a\n+          \/\/ mismatched store into it. LoadStoreNodes are also lumped here because there is no\n+          \/\/ LoadStoreNode::is_mismatched_access.\n+          return false;\n@@ -170,5 +183,13 @@\n-      }\n-    } else {\n-      Node* in = n->in(0);\n-      if (in != nullptr) {\n-        controls.push(in);\n+      } else if (out->is_Call()) {\n+        if (!out->is_AbstractLock() && out->as_Call()->has_non_debug_use(n)) {\n+          \/\/ A call that receives an object as an argument makes that object escape\n+          return false;\n+        }\n+      } else if (out->is_SafePoint()) {\n+        \/\/ Non-call safepoints are pure control nodes\n+      } else if (out->Opcode() == Op_Blackhole) {\n+        \/\/ Blackhole does not escape an object (in the sense that it does not make its field values\n+        \/\/ unpredictable)\n+      } else {\n+        \/\/ Conservatively consider all other nodes to make alloc escape\n+        return false;\n@@ -822,2 +843,10 @@\n-      if (st_base == nullptr)\n-        break;              \/\/ inscrutable pointer\n+      if (st_base == nullptr) {\n+        \/\/ inscrutable pointer\n+        break;\n+      }\n+\n+      \/\/ If the bases are the same and the offsets are the same, it seems that this is the exact\n+      \/\/ store we are looking for, the caller will check if the type of the store matches\n+      if (st_base == base && st_offset == offset) {\n+        return mem;\n+      }\n@@ -825,2 +854,6 @@\n-      \/\/ For raw accesses it's not enough to prove that constant offsets don't intersect.\n-      \/\/ We need the bases to be the equal in order for the offset check to make sense.\n+      \/\/ If it is provable that the memory accessed by mem does not overlap the memory accessed by\n+      \/\/ this, we may walk past mem.\n+      \/\/ For raw accesses, 2 accesses are independent if they have the same base and the offset\n+      \/\/ says that they do not overlap.\n+      \/\/ For heap accesses, 2 accesses are independent if either the bases are provably different\n+      \/\/ at runtime or the offset says that the accesses do not overlap.\n@@ -828,0 +861,1 @@\n+        \/\/ Raw accesses can only be provably independent if they have the same base\n@@ -831,0 +865,3 @@\n+      \/\/ If the offsets say that the accesses do not overlap, then it is provable that mem and this\n+      \/\/ do not overlap. For example, a LoadI from Object+8 is independent from a StoreL into\n+      \/\/ Object+12, no matter what the bases are.\n@@ -843,1 +880,1 @@\n-          continue;           \/\/ (a) advance through independent store memory\n+          continue;\n@@ -847,14 +884,4 @@\n-      if (st_base != base) {\n-        bool known_independent = false;\n-        if (has_not_escaped && aliases.size() > 0) {\n-          known_independent = !aliases.member(st_base);\n-        } else if (detect_ptr_independence(base, alloc, st_base,\n-                                           AllocateNode::Ideal_allocation(st_base),\n-                                           phase)) {\n-          known_independent = true;\n-        } else if (has_not_escaped.is_default()) {\n-          has_not_escaped = check_not_escaped(phase, aliases, alloc, mem->in(0));\n-          if (has_not_escaped) {\n-            known_independent = !aliases.member(st_base);\n-          }\n-        }\n+      \/\/ Same base and overlapping offsets, it seems provable that the accesses overlap, give up\n+      if (st_base == base) {\n+        break;\n+      }\n@@ -862,4 +889,27 @@\n-        if (known_independent) {\n-          \/\/ Success:  The bases are provably independent.\n-          mem = mem->in(MemNode::Memory);\n-          continue;\n+      \/\/ Try to prove that 2 different base nodes at compile time are different values at runtime\n+      bool known_independent = false;\n+      if (has_not_escaped && aliases.size() > 0) {\n+#ifdef ASSERT\n+        assert(!is_known_instance, \"aliases are not computed for known instances\");\n+        ResourceMark rm;\n+        Unique_Node_List verify_aliases;\n+        \/\/ Since we are walking from a node to its input, if alloc is found not to escape at an\n+        \/\/ earlier iteration, it must also be found not to escape at the current iteration\n+        assert(check_not_escaped(phase, verify_aliases, alloc, mem->in(0)), \"inconsistent\");\n+#endif \/\/ ASSERT\n+        \/\/ If base is the result of an allocation that has not escaped, we can know all the nodes\n+        \/\/ that may have the same runtime value as base, these are the transitive outputs of base\n+        \/\/ along some chains that consist of ConstraintCasts, EncodePs, DecodeNs, Phis, and CMoves\n+        known_independent = !aliases.member(st_base);\n+      } else if (detect_ptr_independence(base, alloc, st_base,\n+                                         AllocateNode::Ideal_allocation(st_base),\n+                                         phase)) {\n+        \/\/ detect_ptr_independence == true means that it can prove that base and st_base can not\n+        \/\/ have the same runtime value\n+        known_independent = true;\n+      } else if (has_not_escaped.is_default()) {\n+        \/\/ Both of the previous approaches fail, try to compute the set of all nodes that can have\n+        \/\/ the same runtime value as base and whether \n+        has_not_escaped = check_not_escaped(phase, aliases, alloc, mem->in(0));\n+        if (has_not_escaped) {\n+          known_independent = !aliases.member(st_base);\n@@ -869,4 +919,3 @@\n-      \/\/ (b) At this point, if the bases or offsets do not agree, we lose,\n-      \/\/ since we have not managed to prove 'this' and 'mem' independent.\n-      if (st_base == base && st_offset == offset) {\n-        return mem;         \/\/ let caller handle steps (c), (d)\n+      if (known_independent) {\n+        mem = mem->in(MemNode::Memory);\n+        continue;\n@@ -874,1 +923,0 @@\n-\n@@ -928,1 +976,12 @@\n-      CallNode *call = mem->in(0)->as_Call();\n+      \/\/ We can walk past a call if we can prove that the call does not modify the memory we are\n+      \/\/ accessing, this is the case if the allocation has not escaped at this call\n+      CallNode* call = mem->in(0)->as_Call();\n+#ifdef ASSERT\n+      if (has_not_escaped && !is_known_instance) {\n+        ResourceMark rm;\n+        Unique_Node_List verify_aliases;\n+        \/\/ Since we are walking from a node to its input, if alloc is found not to escape at an\n+        \/\/ earlier iteration, it must also be found not to escape at the current iteration\n+        assert(check_not_escaped(phase, verify_aliases, alloc, call), \"inconsistent\");\n+      }\n+#endif \/\/ ASSERT\n@@ -943,0 +1002,11 @@\n+      \/\/ We can walk past a memory barrier if we can prove that the allocation has not escaped at\n+      \/\/ this barrier, hence it is invisible to other threads\n+#ifdef ASSERT\n+      if (has_not_escaped && !is_known_instance) {\n+        ResourceMark rm;\n+        Unique_Node_List verify_aliases;\n+        \/\/ Since we are walking from a node to its input, if alloc is found not to escape at an\n+        \/\/ earlier iteration, it must also be found not to escape at the current iteration\n+        assert(check_not_escaped(phase, verify_aliases, alloc, mem->in(0)), \"inconsistent\");\n+      }\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":133,"deletions":63,"binary":false,"changes":196,"status":"modified"}]}