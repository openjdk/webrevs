{"files":[{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/callnode.hpp\"\n@@ -47,0 +48,2 @@\n+#include \"opto\/node.hpp\"\n+#include \"opto\/opcodes.hpp\"\n@@ -57,0 +60,1 @@\n+#include \"utilities\/tribool.hpp\"\n@@ -85,0 +89,122 @@\n+\/\/ Check whether an allocation has escaped at a certain control node ctl, the allocation does not\n+\/\/ escape at ctl if there is no node that:\n+\/\/ 1. Make the allocation escape.\n+\/\/ 2. Either:\n+\/\/   a. Has no control input.\n+\/\/   b. Has a control input that is a transitive control input of ctl.\n+\/\/\n+\/\/ In other word, alloc is determined not to escape at ctl if all nodes that make alloc escape have\n+\/\/ a control input that is not a transitive control input of ctl.\n+bool MemNode::check_not_escaped(PhaseValues* phase, Unique_Node_List& aliases, AllocateNode* alloc, Node* ctl) {\n+  if (!phase->is_IterGVN() || alloc == nullptr || phase->type(ctl) == Type::TOP) {\n+    return false;\n+  }\n+  ciEnv* env = phase->C->env();\n+  if (env->should_retain_local_variables() || env->jvmti_can_walk_any_space()) {\n+    \/\/ JVMTI can modify local objects, so give up\n+    return false;\n+  }\n+\n+  Node* base = alloc->result_cast();\n+  assert(base != nullptr, \"must have a result cast\");\n+\n+  \/\/ Find all nodes that may alias base, if any of these nodes escapes, then we conservatively say\n+  \/\/ that base escapes\n+  assert(aliases.size() == 0, \"must not be computed yet\");\n+  aliases.push(base);\n+  for (uint wl_idx = 0; wl_idx < aliases.size(); wl_idx++) {\n+    Node* n = aliases.at(wl_idx);\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      if (out->is_ConstraintCast() || out->is_EncodeP() || out->is_DecodeN() ||\n+          out->is_Phi() || out->is_CMove()) {\n+        aliases.push(out);\n+      } else if (out->is_AddP()) {\n+        \/\/ Some runtime calls receive a derived pointer but not its base, so we consider these\n+        \/\/ derived pointers aliases, too\n+        aliases.push(out);\n+      }\n+    }\n+  }\n+\n+  \/\/ Find all transitive control inputs of ctl that are not dead\n+  ResourceMark rm;\n+  Node* start = phase->C->start();\n+  Unique_Node_List controls;\n+  controls.push(ctl);\n+  for (uint control_idx = 0; control_idx < controls.size(); control_idx++) {\n+    Node* n = controls.at(control_idx);\n+    assert(phase->type(n) == Type::CONTROL || phase->type(n)->base() == Type::Tuple, \"must be a control node %s\", n->Name());\n+    if (n == start) {\n+      continue;\n+    }\n+\n+    if (n->is_Region()) {\n+      for (uint i = 1; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr && phase->type(in) != Type::TOP) {\n+          controls.push(in);\n+        }\n+      }\n+    } else {\n+      Node* in = n->in(0);\n+      if (in != nullptr && phase->type(in) != Type::TOP) {\n+        controls.push(in);\n+      }\n+    }\n+  }\n+\n+  if (!controls.member(start)) {\n+    \/\/ If there is no control path from ctl to start, ctl is a dead path, give up\n+    return false;\n+  }\n+\n+  \/\/ Find all nodes that may escape alloc, and decide that it is provable that they must be\n+  \/\/ executed after ctl\n+  for (uint idx = 0; idx < aliases.size(); idx++) {\n+    Node* n = aliases.at(idx);\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* out = n->fast_out(i);\n+      Node* c = out->in(0);\n+      if (c != nullptr && !controls.member(c)) {\n+        \/\/ c is not a live transitive control input of ctl, so out is not executed before ctl,\n+        \/\/ which means it does not affect the escape status of alloc at ctl\n+        continue;\n+      }\n+\n+      if (aliases.member(out)) {\n+        \/\/ Just a node that may alias n, such as Phi, CMove, CastPP\n+      } else if (out->is_Load()) {\n+        \/\/ A Load does not escape alloc\n+      } else if (out->is_Mem()) {\n+        \/\/ A Store or a LoadStore\n+        if (n == out->in(MemNode::ValueIn)) {\n+          \/\/ If an object is stored to memory, then it escapes\n+          return false;\n+        } else if (n == out->in(MemNode::Address) && (!out->is_Store() || out->as_Store()->is_mismatched_access())) {\n+          \/\/ Mismatched accesses can lie in a different alias class and are protected by memory\n+          \/\/ barriers, so we cannot be aggressive and walk past memory barriers if there is a\n+          \/\/ mismatched store into it. LoadStoreNodes are also lumped here because there is no\n+          \/\/ LoadStoreNode::is_mismatched_access.\n+          return false;\n+        }\n+      } else if (out->is_Call()) {\n+        if (!out->is_AbstractLock() && out->as_Call()->has_non_debug_use(n)) {\n+          \/\/ A call that receives an object as an argument makes that object escape\n+          return false;\n+        }\n+      } else if (out->is_SafePoint()) {\n+        \/\/ Non-call safepoints are pure control nodes\n+      } else if (out->Opcode() == Op_Blackhole) {\n+        \/\/ Blackhole does not escape an object (in the sense that it does not make its field values\n+        \/\/ unpredictable)\n+      } else {\n+        \/\/ Conservatively consider all other nodes to make alloc escape\n+        return false;\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -704,0 +830,13 @@\n+  if (is_Load() && alloc != nullptr) {\n+    \/\/ Get agressive for loads from freshly allocated objects\n+    cnt = 1000;\n+  }\n+\n+  \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n+  bool is_known_instance = addr_t != nullptr && addr_t->is_known_instance_field();\n+  \/\/ If alloc != nullptr and the allocated object has not escaped the current compilation unit, we\n+  \/\/ can be more aggressive, walk past calls and memory barriers to find a corresponding store\n+  TriBool has_not_escaped = is_known_instance ? TriBool(true) : (is_Load() ? TriBool() : TriBool(false));\n+  \/\/ If has_not_escaped and it is not empty, this is the set of all nodes that can alias base\n+  ResourceMark rm;\n+  Unique_Node_List aliases;\n@@ -712,2 +851,4 @@\n-      if (st_base == nullptr)\n-        break;              \/\/ inscrutable pointer\n+      if (st_base == nullptr) {\n+        \/\/ inscrutable pointer\n+        break;\n+      }\n@@ -715,2 +856,12 @@\n-      \/\/ For raw accesses it's not enough to prove that constant offsets don't intersect.\n-      \/\/ We need the bases to be the equal in order for the offset check to make sense.\n+      \/\/ If the bases are the same and the offsets are the same, it seems that this is the exact\n+      \/\/ store we are looking for, the caller will check if the type of the store matches\n+      if (st_base == base && st_offset == offset) {\n+        return mem;\n+      }\n+\n+      \/\/ If it is provable that the memory accessed by mem does not overlap the memory accessed by\n+      \/\/ this, we may walk past mem.\n+      \/\/ For raw accesses, 2 accesses are independent if they have the same base and the offset\n+      \/\/ says that they do not overlap.\n+      \/\/ For heap accesses, 2 accesses are independent if either the bases are provably different\n+      \/\/ at runtime or the offset says that the accesses do not overlap.\n@@ -718,0 +869,1 @@\n+        \/\/ Raw accesses can only be provably independent if they have the same base\n@@ -721,0 +873,3 @@\n+      \/\/ If the offsets say that the accesses do not overlap, then it is provable that mem and this\n+      \/\/ do not overlap. For example, a LoadI from Object+8 is independent from a StoreL into\n+      \/\/ Object+12, no matter what the bases are.\n@@ -733,1 +888,1 @@\n-          continue;           \/\/ (a) advance through independent store memory\n+          continue;\n@@ -736,8 +891,4 @@\n-      if (st_base != base &&\n-          detect_ptr_independence(base, alloc,\n-                                  st_base,\n-                                  AllocateNode::Ideal_allocation(st_base),\n-                                  phase)) {\n-        \/\/ Success:  The bases are provably independent.\n-        mem = mem->in(MemNode::Memory);\n-        continue;           \/\/ (a) advance through independent store memory\n+\n+      \/\/ Same base and overlapping offsets, it seems provable that the accesses overlap, give up\n+      if (st_base == base) {\n+        break;\n@@ -746,4 +897,28 @@\n-      \/\/ (b) At this point, if the bases or offsets do not agree, we lose,\n-      \/\/ since we have not managed to prove 'this' and 'mem' independent.\n-      if (st_base == base && st_offset == offset) {\n-        return mem;         \/\/ let caller handle steps (c), (d)\n+      \/\/ Try to prove that 2 different base nodes at compile time are different values at runtime\n+      bool known_independent = false;\n+      if (has_not_escaped && aliases.size() > 0) {\n+#ifdef ASSERT\n+        assert(!is_known_instance, \"aliases are not computed for known instances\");\n+        ResourceMark rm;\n+        Unique_Node_List verify_aliases;\n+        \/\/ Since we are walking from a node to its input, if alloc is found not to escape at an\n+        \/\/ earlier iteration, it must also be found not to escape at the current iteration\n+        assert(check_not_escaped(phase, verify_aliases, alloc, mem->in(0)), \"inconsistent\");\n+#endif \/\/ ASSERT\n+        \/\/ If base is the result of an allocation that has not escaped, we can know all the nodes\n+        \/\/ that may have the same runtime value as base, these are the transitive outputs of base\n+        \/\/ along some chains that consist of ConstraintCasts, EncodePs, DecodeNs, Phis, and CMoves\n+        known_independent = !aliases.member(st_base);\n+      } else if (detect_ptr_independence(base, alloc, st_base,\n+                                         AllocateNode::Ideal_allocation(st_base),\n+                                         phase)) {\n+        \/\/ detect_ptr_independence == true means that it can prove that base and st_base can not\n+        \/\/ have the same runtime value\n+        known_independent = true;\n+      } else if (has_not_escaped.is_default()) {\n+        \/\/ Both of the previous approaches fail, try to compute the set of all nodes that can have\n+        \/\/ the same runtime value as base and whether st_base is one of them\n+        has_not_escaped = check_not_escaped(phase, aliases, alloc, mem->in(0));\n+        if (has_not_escaped) {\n+          known_independent = !aliases.member(st_base);\n+        }\n@@ -752,0 +927,4 @@\n+      if (known_independent) {\n+        mem = mem->in(MemNode::Memory);\n+        continue;\n+      }\n@@ -787,1 +966,0 @@\n-\n@@ -795,29 +973,65 @@\n-    } else if (addr_t != nullptr && addr_t->is_known_instance_field()) {\n-      \/\/ Can't use optimize_simple_memory_chain() since it needs PhaseGVN.\n-      if (mem->is_Proj() && mem->in(0)->is_Call()) {\n-        \/\/ ArrayCopyNodes processed here as well.\n-        CallNode *call = mem->in(0)->as_Call();\n-        if (!call->may_modify(addr_t, phase)) {\n-          mem = call->in(TypeFunc::Memory);\n-          continue;         \/\/ (a) advance through independent call memory\n-        }\n-      } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n-        ArrayCopyNode* ac = nullptr;\n-        if (ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n-          break;\n-        }\n-        mem = mem->in(0)->in(TypeFunc::Memory);\n-        continue;           \/\/ (a) advance through independent MemBar memory\n-      } else if (mem->is_ClearArray()) {\n-        if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n-          \/\/ (the call updated 'mem' value)\n-          continue;         \/\/ (a) advance through independent allocation memory\n-        } else {\n-          \/\/ Can not bypass initialization of the instance\n-          \/\/ we are looking for.\n-          return mem;\n-        }\n-      } else if (mem->is_MergeMem()) {\n-        int alias_idx = phase->C->get_alias_index(adr_type());\n-        mem = mem->as_MergeMem()->memory_at(alias_idx);\n-        continue;           \/\/ (a) advance through independent MergeMem memory\n+    } else if (mem->is_MergeMem()) {\n+      int alias_idx = phase->C->get_alias_index(adr_type());\n+      mem = mem->as_MergeMem()->memory_at(alias_idx);\n+      continue;\n+    } else if (mem->is_Proj() && mem->in(0)->is_Call()) {\n+      \/\/ We can walk past a call if we can prove that the call does not modify the memory we are\n+      \/\/ accessing, this is the case if the allocation has not escaped at this call\n+      CallNode* call = mem->in(0)->as_Call();\n+#ifdef ASSERT\n+      if (has_not_escaped && !is_known_instance) {\n+        ResourceMark rm;\n+        Unique_Node_List verify_aliases;\n+        \/\/ Since we are walking from a node to its input, if alloc is found not to escape at an\n+        \/\/ earlier iteration, it must also be found not to escape at the current iteration\n+        assert(check_not_escaped(phase, verify_aliases, alloc, call), \"inconsistent\");\n+      }\n+#endif \/\/ ASSERT\n+      if (has_not_escaped.is_default()) {\n+        has_not_escaped = check_not_escaped(phase, aliases, alloc, call);\n+      }\n+      if (!has_not_escaped) {\n+        break;\n+      }\n+\n+      \/\/ We are more aggressive with known instances, for the others, if this call may modify base,\n+      \/\/ check_not_escaped would return false\n+      if (!is_known_instance || !call->may_modify(addr_t, phase)) {\n+        mem = call->in(TypeFunc::Memory);\n+        continue;         \/\/ (a) advance through independent call memory\n+      }\n+    } else if (mem->is_Proj() && mem->in(0)->is_MemBar()) {\n+      \/\/ We can walk past a memory barrier if we can prove that the allocation has not escaped at\n+      \/\/ this barrier, hence it is invisible to other threads\n+#ifdef ASSERT\n+      if (has_not_escaped && !is_known_instance) {\n+        ResourceMark rm;\n+        Unique_Node_List verify_aliases;\n+        \/\/ Since we are walking from a node to its input, if alloc is found not to escape at an\n+        \/\/ earlier iteration, it must also be found not to escape at the current iteration\n+        assert(check_not_escaped(phase, verify_aliases, alloc, mem->in(0)), \"inconsistent\");\n+      }\n+#endif \/\/ ASSERT\n+      if (has_not_escaped.is_default()) {\n+        has_not_escaped = check_not_escaped(phase, aliases, alloc, mem->in(0));\n+      }\n+      if (!has_not_escaped) {\n+        break;\n+      }\n+\n+      \/\/ We are more aggressive with known instances, for the others, if this call may modify base,\n+      \/\/ check_not_escaped would return false\n+      ArrayCopyNode* ac = nullptr;\n+      if (is_known_instance && ArrayCopyNode::may_modify(addr_t, mem->in(0)->as_MemBar(), phase, ac)) {\n+        break;\n+      }\n+      mem = mem->in(0)->in(TypeFunc::Memory);\n+      continue;\n+    } else if (is_known_instance && mem->is_ClearArray()) {\n+      if (ClearArrayNode::step_through(&mem, (uint)addr_t->instance_id(), phase)) {\n+        \/\/ (the call updated 'mem' value)\n+        continue;         \/\/ (a) advance through independent allocation memory\n+      } else {\n+        \/\/ Can not bypass initialization of the instance\n+        \/\/ we are looking for.\n+        return mem;\n@@ -1885,0 +2099,5 @@\n+  if (!can_reshape) {\n+    phase->record_for_igvn(this);\n+    return nullptr;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":266,"deletions":47,"binary":false,"changes":313,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  static bool check_not_escaped(PhaseValues* phase, Unique_Node_List& aliases, AllocateNode* alloc, Node* ctl);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,258 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.escapeAnalysis;\n+\n+import compiler.lib.ir_framework.*;\n+\n+import java.lang.invoke.VarHandle;\n+\n+\/**\n+ * @test\n+ * @bug 8373495\n+ * @summary Test that loads from a newly allocated object are aggressively folded if the object has not escaped\n+ * @library \/test\/lib \/\n+ * @run driver ${test.main.class}\n+ *\/\n+public class TestLoadFolding {\n+    public static class Point {\n+        int x;\n+        int y;\n+\n+        Point() {\n+            x = 1;\n+            y = 2;\n+        }\n+\n+        static final Point DEFAULT = new Point();\n+    }\n+\n+    static Point staticField;\n+\n+    public static void main(String[] args) {\n+        TestFramework.run();\n+    }\n+\n+    @Run(test = {\"test11\", \"test12\", \"test13\", \"test14\", \"test15\", \"test16\", \"test17\", \"test18\"})\n+    public void runPositiveTests() {\n+        test11();\n+        test12(false);\n+        test12(true);\n+        test13(false);\n+        test13(true);\n+        test14();\n+        test15(1, 16);\n+        test16(1, 16, false);\n+        test16(1, 16, true);\n+        test17(0);\n+        test18(0);\n+    }\n+\n+    @Run(test = {\"test01\", \"test02\", \"test03\", \"test04\", \"test05\"})\n+    public void runNegativeTests() {\n+        test01();\n+        test02(false);\n+        test02(true);\n+        test03(false);\n+        test03(true);\n+        test04(1, 16);\n+        test05(0);\n+    }\n+\n+    @DontInline\n+    static void escape(Object o) {}\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test11() {\n+        \/\/ p only escapes at return\n+        Point p = new Point();\n+        escape(null);\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test12(boolean b) {\n+        \/\/ p escapes in another branch\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        } else {\n+            escape(null);\n+            p.x += p.y;\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public Point test13(boolean b) {\n+        \/\/ A Phi of p1 and Point.DEFAULT, but a store to Phi is after all the loads from p1\n+        Point p1 = new Point();\n+        Point p = b ? p1 : Point.DEFAULT;\n+        escape(null);\n+        p.x = p1.x + p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC, \"1\"})\n+    public int test14() {\n+        \/\/ Even if p escapes before the loads, if it is legal to execute the loads before the\n+        \/\/ store, then we can fold the loads\n+        Point p = new Point();\n+        escape(null);\n+        staticField = p;\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Point test15(int begin, int end) {\n+        \/\/ Fold the load that is a part of a cycle\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            p.x++;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p.x += p.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ALLOC, \"1\"})\n+    public Point test16(int begin, int end, boolean b) {\n+        \/\/ A cycle and a Phi, this time the store is at a different field\n+        Point p1 = new Point();\n+        \/\/ This store is not on a Phi involving p1, so it does not interfere\n+        Point.DEFAULT.y = 3;\n+        Point p = p1;\n+        for (int i = begin; i < end; i += 2) {\n+            if (b) {\n+                p = p1;\n+            } else {\n+                p = Point.DEFAULT;\n+            }\n+            b = !b;\n+\n+            p.x = p1.y + 3;\n+            escape(null); \/\/ Force a memory Phi\n+        }\n+        p1.x = p1.y;\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test17(int idx) {\n+        \/\/ Array\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        int res = a[idx & 1];\n+        escape(null);\n+        res += a[0] + a[1];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.LOAD_I, counts = {IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test18(int idx) {\n+        \/\/ Array, even if we will give up if we encounter a[idx & 1] = 3, we meet a[0] = 4 first,\n+        \/\/ so the load int res = a[0] can still be folded\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        a[0] = 4;\n+        escape(null);\n+        int res = a[0];\n+        escape(a);\n+        return res;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC, \"1\"})\n+    public int test01() {\n+        Point p = new Point();\n+        staticField = p;\n+        \/\/ Actually, the only fence that requires the following loads to be executed after the\n+        \/\/ store is a fullFence\n+        VarHandle.fullFence();\n+        return p.x + p.y;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test02(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+            \/\/ p escaped, so the load must not be removed\n+            return p.x;\n+        } else {\n+            escape(null);\n+            return 0;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"1\", IRNode.ALLOC, \"1\"})\n+    public int test03(boolean b) {\n+        Point p = new Point();\n+        if (b) {\n+            escape(p);\n+        }\n+        \/\/ p escaped, so the load must not be removed\n+        return p.x;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"> 0\", IRNode.ALLOC, \"1\"})\n+    public Point test04(int begin, int end) {\n+        Point p = new Point();\n+        for (int i = begin; i < end; i *= 2) {\n+            \/\/ p escaped here because this is a loop\n+            p.x++;\n+            escape(p);\n+        }\n+        return p;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_I, \"2\", IRNode.ALLOC_ARRAY, \"1\"})\n+    public int test05(int idx) {\n+        int[] a = new int[2];\n+        a[0] = 1;\n+        a[1] = 2;\n+        escape(null);\n+        a[idx & 1] = 3;\n+        \/\/ Cannot fold the loads because we do not know which element is written to by\n+        \/\/ a[idx & 1] = 3\n+        return a[0] + a[1];\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/escapeAnalysis\/TestLoadFolding.java","additions":258,"deletions":0,"binary":false,"changes":258,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -254,1 +254,1 @@\n-    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"1\" }, phase = CompilePhase.FINAL_CODE)\n+    @IR(counts = { IRNode.Z_LOAD_P_WITH_BARRIER_FLAG, Common.ELIDED, \"2\" }, phase = CompilePhase.FINAL_CODE)\n@@ -257,2 +257,4 @@\n-        Common.blackhole(o1);\n-        \/\/ This load is directly optimized away by C2.\n+        Common.outer = o1;\n+        \/\/ Prevent the field loads from getting folded by making o1 escape, the fullFence is\n+        \/\/ necessary because otherwise the loads can float above the store and get folded\n+        VarHandle.fullFence();\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"}]}