{"files":[{"patch":"@@ -1716,0 +1716,52 @@\n+void MacroAssembler::crc32_vclmul_fold_16_bytes_vectorsize_16(VectorRegister vx, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register buf, Register tmp, const int STEP) {\n+  assert_different_registers(vx, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  vle64_v(vtmp4, buf); addi(buf, buf, STEP);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vtmp4);\n+  \/\/ high parts\n+  vslidedown_vi(vx, vtmp4, 1);\n+  vredxor_vs(vtmp1, vtmp2, vx);\n+  \/\/ merge low and high back\n+  vslideup_vi(vx, vtmp1, 1);\n+  vmv_x_s(tmp, vtmp3);\n+  vmv_s_x(vx, tmp);\n+}\n+\n+void MacroAssembler::crc32_vclmul_fold_16_bytes_vectorsize_16_2(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp) {\n+  assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vy);\n+  \/\/ high parts\n+  vslidedown_vi(vtmp4, vy, 1);\n+  vredxor_vs(vtmp1, vtmp2, vtmp4);\n+  \/\/ merge low and high back\n+  vslideup_vi(vx, vtmp1, 1);\n+  vmv_x_s(tmp, vtmp3);\n+  vmv_s_x(vx, tmp);\n+}\n+\n+void MacroAssembler::crc32_vclmul_fold_16_bytes_vectorsize_16_3(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp) {\n+  assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vy);\n+  \/\/ high parts\n+  vslidedown_vi(vtmp4, vy, 1);\n+  vredxor_vs(vtmp1, vtmp2, vtmp4);\n+  \/\/ merge low and high back\n+  vslideup_vi(vy, vtmp1, 1);\n+  vmv_x_s(tmp, vtmp3);\n+  vmv_s_x(vy, tmp);\n+}\n+\n@@ -1761,25 +1813,8 @@\n-    #define CRC32_VCLMUL_FOLD_16_BYTES(vx, vt, vtmp1, vtmp2, vtmp3, vtmp4) \\\n-      assert_different_registers(vx, vt, vtmp1, vtmp2, vtmp3, vtmp4); \\\n-      vclmul_vv(vtmp1, vx, vt); \\\n-      vclmulh_vv(vtmp2, vx, vt); \\\n-      vle64_v(vtmp4, buf); addi(buf, buf, STEP); \\\n-      \/* low parts *\/ \\\n-      vredxor_vs(vtmp3, vtmp1, vtmp4); \\\n-      \/* high parts *\/ \\\n-      vslidedown_vi(vx, vtmp4, 1); \\\n-      vredxor_vs(vtmp1, vtmp2, vx); \\\n-      \/* merge low and high back *\/ \\\n-      vslideup_vi(vx, vtmp1, 1); \\\n-      vmv_x_s(tmp2, vtmp3); \\\n-      vmv_s_x(vx, tmp2);\n-\n-    CRC32_VCLMUL_FOLD_16_BYTES(v0, v31, v8, v9, v10, v11);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v1, v31, v12, v13, v14, v15);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v2, v31, v16, v17, v18, v19);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v3, v31, v20, v21, v22, v23);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v4, v31, v24, v25, v26, v27);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v5, v31, v8, v9, v10, v11);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v6, v31, v12, v13, v14, v15);\n-    CRC32_VCLMUL_FOLD_16_BYTES(v7, v31, v16, v17, v18, v19);\n-\n-    #undef CRC32_VCLMUL_FOLD_16_BYTES\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v0, v31, v8, v9, v10, v11, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v1, v31, v12, v13, v14, v15, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v2, v31, v16, v17, v18, v19, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v3, v31, v20, v21, v22, v23, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v4, v31, v24, v25, v26, v27, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v5, v31, v8, v9, v10, v11, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v6, v31, v12, v13, v14, v15, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v7, v31, v16, v17, v18, v19, buf, tmp2, STEP);\n@@ -1797,20 +1832,4 @@\n-  #define CRC32_VCLMUL_FOLD_16_BYTES_2(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4) \\\n-    assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4); \\\n-    vclmul_vv(vtmp1, vx, vt); \\\n-    vclmulh_vv(vtmp2, vx, vt); \\\n-    \/* low parts *\/ \\\n-    vredxor_vs(vtmp3, vtmp1, vy); \\\n-    \/* high parts *\/ \\\n-    vslidedown_vi(vtmp4, vy, 1); \\\n-    vredxor_vs(vtmp1, vtmp2, vtmp4); \\\n-    \/* merge low and high back *\/ \\\n-    vslideup_vi(vx, vtmp1, 1); \\\n-    vmv_x_s(tmp2, vtmp3); \\\n-    vmv_s_x(vx, tmp2);\n-\n-  CRC32_VCLMUL_FOLD_16_BYTES_2(v0, v4, v31, v8, v9, v10, v11);\n-  CRC32_VCLMUL_FOLD_16_BYTES_2(v1, v5, v31, v12, v13, v14, v15);\n-  CRC32_VCLMUL_FOLD_16_BYTES_2(v2, v6, v31, v16, v17, v18, v19);\n-  CRC32_VCLMUL_FOLD_16_BYTES_2(v3, v7, v31, v20, v21, v22, v23);\n-\n-  #undef CRC32_VCLMUL_FOLD_16_BYTES_2\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v0, v4, v31, v8, v9, v10, v11, tmp2);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v1, v5, v31, v12, v13, v14, v15, tmp2);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v2, v6, v31, v16, v17, v18, v19, tmp2);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v3, v7, v31, v20, v21, v22, v23, tmp2);\n@@ -1821,14 +1840,0 @@\n-  #define CRC32_VCLMUL_FOLD_16_BYTES_3(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4) \\\n-    assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4); \\\n-    vclmul_vv(vtmp1, vx, vt); \\\n-    vclmulh_vv(vtmp2, vx, vt); \\\n-    \/* low parts *\/ \\\n-    vredxor_vs(vtmp3, vtmp1, vy); \\\n-    \/* high parts *\/ \\\n-    vslidedown_vi(vtmp4, vy, 1); \\\n-    vredxor_vs(vtmp1, vtmp2, vtmp4); \\\n-    \/* merge low and high back *\/ \\\n-    vslideup_vi(vy, vtmp1, 1); \\\n-    vmv_x_s(tmp2, vtmp3); \\\n-    vmv_s_x(vy, tmp2);\n-\n@@ -1837,1 +1842,1 @@\n-  CRC32_VCLMUL_FOLD_16_BYTES_3(v0, v3, v31, v8, v9, v10, v11);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_3(v0, v3, v31, v8, v9, v10, v11, tmp2);\n@@ -1841,1 +1846,1 @@\n-  CRC32_VCLMUL_FOLD_16_BYTES_3(v1, v3, v31, v12, v13, v14, v15);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_3(v1, v3, v31, v12, v13, v14, v15, tmp2);\n@@ -1845,1 +1850,1 @@\n-  CRC32_VCLMUL_FOLD_16_BYTES_3(v2, v3, v31, v16, v17, v18, v19);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_3(v2, v3, v31, v16, v17, v18, v19, tmp2);\n@@ -1857,0 +1862,16 @@\n+void MacroAssembler::crc32_vclmul_fold_to_16_bytes_vectorsize_32(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                            VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4) {\n+  assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vy);\n+  \/\/ high parts\n+  vslidedown_vi(vtmp4, vy, 1);\n+  vredxor_vs(vtmp1, vtmp2, vtmp4);\n+  \/\/ merge low and high back\n+  vslideup_vi(vy, vtmp1, 1);\n+  vmv_x_s(t1, vtmp3);\n+  vmv_s_x(vy, t1);\n+}\n+\n@@ -1992,14 +2013,0 @@\n-  #define CRC32_VCLMUL_FOLD_TO_16_BYTES(vx, vy, vt) \\\n-    assert_different_registers(vx, vy, vt, v28, v29, v30, v31); \\\n-    vclmul_vv(v28, vx, vt); \\\n-    vclmulh_vv(v29, vx, vt); \\\n-    \/* low parts *\/ \\\n-    vredxor_vs(v30, v28, vy); \\\n-    \/* high parts *\/ \\\n-    vslidedown_vi(v31, vy, 1); \\\n-    vredxor_vs(v28, v29, v31); \\\n-    \/* merge low and high back *\/ \\\n-    vslideup_vi(vy, v28, 1); \\\n-    vmv_x_s(t1, v30); \\\n-    vmv_s_x(vy, t1);\n-\n@@ -2008,1 +2015,1 @@\n-  CRC32_VCLMUL_FOLD_TO_16_BYTES(v4, v20, v8);\n+  crc32_vclmul_fold_to_16_bytes_vectorsize_32(v4, v20, v8, v28, v29, v30, v31);\n@@ -2012,1 +2019,1 @@\n-  CRC32_VCLMUL_FOLD_TO_16_BYTES(v16, v20, v8);\n+  crc32_vclmul_fold_to_16_bytes_vectorsize_32(v16, v20, v8, v28, v29, v30, v31);\n@@ -2016,1 +2023,1 @@\n-  CRC32_VCLMUL_FOLD_TO_16_BYTES(v18, v20, v8);\n+  crc32_vclmul_fold_to_16_bytes_vectorsize_32(v18, v20, v8, v28, v29, v30, v31);\n@@ -2025,1 +2032,0 @@\n-  #undef CRC32_VCLMUL_FOLD_TO_16_BYTES\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":86,"deletions":80,"binary":false,"changes":166,"status":"modified"},{"patch":"@@ -1309,0 +1309,5 @@\n+\n+#ifdef COMPILER2\n+  void vector_update_crc32(Register crc, Register buf, Register len,\n+                           Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5,\n+                           Register table0, Register table3);\n@@ -1312,0 +1317,2 @@\n+  void crc32_vclmul_fold_to_16_bytes_vectorsize_32(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                            VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4);\n@@ -1314,0 +1321,9 @@\n+  void crc32_vclmul_fold_16_bytes_vectorsize_16(VectorRegister vx, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register buf, Register tmp, const int STEP);\n+  void crc32_vclmul_fold_16_bytes_vectorsize_16_2(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp);\n+  void crc32_vclmul_fold_16_bytes_vectorsize_16_3(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp);\n@@ -1317,5 +1333,0 @@\n-#ifdef COMPILER2\n-  void vector_update_crc32(Register crc, Register buf, Register len,\n-                           Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5,\n-                           Register table0, Register table3);\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"}]}