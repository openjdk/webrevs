{"files":[{"patch":"@@ -118,0 +118,1 @@\n+  product(bool, UseZvbc, false, EXPERIMENTAL, \"Use Zvbc instructions\")           \\\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1715,0 +1715,353 @@\n+\n+void MacroAssembler::crc32_vclmul_fold_16_bytes_vectorsize_16(VectorRegister vx, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register buf, Register tmp, const int STEP) {\n+  assert_different_registers(vx, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  vle64_v(vtmp4, buf); addi(buf, buf, STEP);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vtmp4);\n+  \/\/ high parts\n+  vslidedown_vi(vx, vtmp4, 1);\n+  vredxor_vs(vtmp1, vtmp2, vx);\n+  \/\/ merge low and high back\n+  vslideup_vi(vx, vtmp1, 1);\n+  vmv_x_s(tmp, vtmp3);\n+  vmv_s_x(vx, tmp);\n+}\n+\n+void MacroAssembler::crc32_vclmul_fold_16_bytes_vectorsize_16_2(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp) {\n+  assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vy);\n+  \/\/ high parts\n+  vslidedown_vi(vtmp4, vy, 1);\n+  vredxor_vs(vtmp1, vtmp2, vtmp4);\n+  \/\/ merge low and high back\n+  vslideup_vi(vx, vtmp1, 1);\n+  vmv_x_s(tmp, vtmp3);\n+  vmv_s_x(vx, tmp);\n+}\n+\n+void MacroAssembler::crc32_vclmul_fold_16_bytes_vectorsize_16_3(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp) {\n+  assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vy);\n+  \/\/ high parts\n+  vslidedown_vi(vtmp4, vy, 1);\n+  vredxor_vs(vtmp1, vtmp2, vtmp4);\n+  \/\/ merge low and high back\n+  vslideup_vi(vy, vtmp1, 1);\n+  vmv_x_s(tmp, vtmp3);\n+  vmv_s_x(vy, tmp);\n+}\n+\n+void MacroAssembler::kernel_crc32_vclmul_fold_vectorsize_16(Register crc, Register buf, Register len,\n+                                              Register vclmul_table, Register tmp1, Register tmp2) {\n+  assert_different_registers(crc, buf, len, vclmul_table, tmp1, tmp2, t1);\n+  assert(MaxVectorSize == 16, \"sanity\");\n+\n+  const int TABLE_STEP = 16;\n+  const int STEP = 16;\n+  const int LOOP_STEP = 128;\n+  const int N = 2;\n+\n+  Register loop_step = t1;\n+\n+  \/\/ ======== preparation ========\n+\n+  mv(loop_step, LOOP_STEP);\n+  sub(len, len, loop_step);\n+\n+  vsetivli(zr, N, Assembler::e64, Assembler::m1, Assembler::mu, Assembler::tu);\n+  vle64_v(v0, buf); addi(buf, buf, STEP);\n+  vle64_v(v1, buf); addi(buf, buf, STEP);\n+  vle64_v(v2, buf); addi(buf, buf, STEP);\n+  vle64_v(v3, buf); addi(buf, buf, STEP);\n+  vle64_v(v4, buf); addi(buf, buf, STEP);\n+  vle64_v(v5, buf); addi(buf, buf, STEP);\n+  vle64_v(v6, buf); addi(buf, buf, STEP);\n+  vle64_v(v7, buf); addi(buf, buf, STEP);\n+\n+  vmv_v_x(v31, zr);\n+  vsetivli(zr, 1, Assembler::e32, Assembler::m1, Assembler::mu, Assembler::tu);\n+  vmv_s_x(v31, crc);\n+  vsetivli(zr, N, Assembler::e64, Assembler::m1, Assembler::mu, Assembler::tu);\n+  vxor_vv(v0, v0, v31);\n+\n+  \/\/ load table\n+  vle64_v(v31, vclmul_table);\n+\n+  Label L_16_bytes_loop;\n+  j(L_16_bytes_loop);\n+\n+\n+  \/\/ ======== folding 128 bytes in data buffer per round ========\n+\n+  align(OptoLoopAlignment);\n+  bind(L_16_bytes_loop);\n+  {\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v0, v31, v8, v9, v10, v11, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v1, v31, v12, v13, v14, v15, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v2, v31, v16, v17, v18, v19, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v3, v31, v20, v21, v22, v23, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v4, v31, v24, v25, v26, v27, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v5, v31, v8, v9, v10, v11, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v6, v31, v12, v13, v14, v15, buf, tmp2, STEP);\n+    crc32_vclmul_fold_16_bytes_vectorsize_16(v7, v31, v16, v17, v18, v19, buf, tmp2, STEP);\n+  }\n+  sub(len, len, loop_step);\n+  bge(len, loop_step, L_16_bytes_loop);\n+\n+\n+  \/\/ ======== folding into 64 bytes from 128 bytes in register ========\n+\n+  \/\/ load table\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v31, vclmul_table);\n+\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v0, v4, v31, v8, v9, v10, v11, tmp2);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v1, v5, v31, v12, v13, v14, v15, tmp2);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v2, v6, v31, v16, v17, v18, v19, tmp2);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_2(v3, v7, v31, v20, v21, v22, v23, tmp2);\n+\n+\n+  \/\/ ======== folding into 16 bytes from 64 bytes in register ========\n+\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v31, vclmul_table);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_3(v0, v3, v31, v8, v9, v10, v11, tmp2);\n+\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v31, vclmul_table);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_3(v1, v3, v31, v12, v13, v14, v15, tmp2);\n+\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v31, vclmul_table);\n+  crc32_vclmul_fold_16_bytes_vectorsize_16_3(v2, v3, v31, v16, v17, v18, v19, tmp2);\n+\n+  #undef FOLD_2_VCLMUL_3\n+\n+\n+  \/\/ ======== final: move result to scalar regsiters ========\n+\n+  vmv_x_s(tmp1, v3);\n+  vslidedown_vi(v1, v3, 1);\n+  vmv_x_s(tmp2, v1);\n+}\n+\n+void MacroAssembler::crc32_vclmul_fold_to_16_bytes_vectorsize_32(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                            VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4) {\n+  assert_different_registers(vx, vy, vt, vtmp1, vtmp2, vtmp3, vtmp4);\n+  vclmul_vv(vtmp1, vx, vt);\n+  vclmulh_vv(vtmp2, vx, vt);\n+  \/\/ low parts\n+  vredxor_vs(vtmp3, vtmp1, vy);\n+  \/\/ high parts\n+  vslidedown_vi(vtmp4, vy, 1);\n+  vredxor_vs(vtmp1, vtmp2, vtmp4);\n+  \/\/ merge low and high back\n+  vslideup_vi(vy, vtmp1, 1);\n+  vmv_x_s(t1, vtmp3);\n+  vmv_s_x(vy, t1);\n+}\n+\n+void MacroAssembler::kernel_crc32_vclmul_fold_vectorsize_32(Register crc, Register buf, Register len,\n+                                              Register vclmul_table, Register tmp1, Register tmp2) {\n+  assert_different_registers(crc, buf, len, vclmul_table, tmp1, tmp2, t1);\n+  assert(MaxVectorSize >= 32, \"sanity\");\n+\n+  \/\/ utility: load table\n+  #define CRC32_VCLMUL_LOAD_TABLE(vt, rt, vtmp, rtmp) \\\n+  vid_v(vtmp); \\\n+  mv(rtmp, 2); \\\n+  vremu_vx(vtmp, vtmp, rtmp); \\\n+  vsll_vi(vtmp, vtmp, 3); \\\n+  vluxei64_v(vt, rt, vtmp);\n+\n+  const int TABLE_STEP = 16;\n+  const int STEP = 128;  \/\/ 128 bytes per round\n+  const int N = 2 * 8;   \/\/ 2: 128-bits\/64-bits, 8: 8 pairs of double 64-bits\n+\n+  Register step = tmp2;\n+\n+\n+  \/\/ ======== preparation ========\n+\n+  mv(step, STEP);\n+  sub(len, len, step); \/\/ 2 rounds of folding with carry-less multiplication\n+\n+  vsetivli(zr, N, Assembler::e64, Assembler::m4, Assembler::mu, Assembler::tu);\n+  \/\/ load data\n+  vle64_v(v4, buf);\n+  add(buf, buf, step);\n+\n+  \/\/ load table\n+  CRC32_VCLMUL_LOAD_TABLE(v8, vclmul_table, v28, t1);\n+  \/\/ load mask,\n+  \/\/    v28 should already contains: 0, 8, 0, 8, ...\n+  vmseq_vi(v2, v28, 0);\n+  \/\/    now, v2 should contains: 101010...\n+  vmnand_mm(v1, v2, v2);\n+  \/\/    now, v1 should contains: 010101...\n+\n+  \/\/ initial crc\n+  vmv_v_x(v24, zr);\n+  vsetivli(zr, 1, Assembler::e32, Assembler::m4, Assembler::mu, Assembler::tu);\n+  vmv_s_x(v24, crc);\n+  vsetivli(zr, N, Assembler::e64, Assembler::m4, Assembler::mu, Assembler::tu);\n+  vxor_vv(v4, v4, v24);\n+\n+  Label L_128_bytes_loop;\n+  j(L_128_bytes_loop);\n+\n+\n+  \/\/ ======== folding 128 bytes in data buffer per round ========\n+\n+  align(OptoLoopAlignment);\n+  bind(L_128_bytes_loop);\n+  {\n+    \/\/ v4: data\n+    \/\/ v4: buf, reused\n+    \/\/ v8: table\n+    \/\/ v12: lows\n+    \/\/ v16: highs\n+    \/\/ v20: low_slides\n+    \/\/ v24: high_slides\n+    vclmul_vv(v12, v4, v8);\n+    vclmulh_vv(v16, v4, v8);\n+    vle64_v(v4, buf);\n+    add(buf, buf, step);\n+    \/\/ lows\n+    vslidedown_vi(v20, v12, 1);\n+    vmand_mm(v0, v2, v2);\n+    vxor_vv(v12, v12, v20, v0_t);\n+    \/\/ with buf data\n+    vxor_vv(v4, v4, v12, v0_t);\n+\n+    \/\/ highs\n+    vslideup_vi(v24, v16, 1);\n+    vmand_mm(v0, v1, v1);\n+    vxor_vv(v16, v16, v24, v0_t);\n+    \/\/ with buf data\n+    vxor_vv(v4, v4, v16, v0_t);\n+  }\n+  sub(len, len, step);\n+  bge(len, step, L_128_bytes_loop);\n+\n+\n+  \/\/ ======== folding into 64 bytes from 128 bytes in register ========\n+\n+  \/\/ load table\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  CRC32_VCLMUL_LOAD_TABLE(v8, vclmul_table, v28, t1);\n+\n+  \/\/ v4:  data, first (low) part, N\/2 of 64-bits\n+  \/\/ v20: data, second (high) part, N\/2 of 64-bits\n+  \/\/ v8:  table\n+  \/\/ v10: lows\n+  \/\/ v12: highs\n+  \/\/ v14: low_slides\n+  \/\/ v16: high_slides\n+\n+  \/\/ high part\n+  vslidedown_vi(v20, v4, N\/2);\n+\n+  vsetivli(zr, N\/2, Assembler::e64, Assembler::m2, Assembler::mu, Assembler::tu);\n+\n+  vclmul_vv(v10, v4, v8);\n+  vclmulh_vv(v12, v4, v8);\n+\n+  \/\/ lows\n+  vslidedown_vi(v14, v10, 1);\n+  vmand_mm(v0, v2, v2);\n+  vxor_vv(v10, v10, v14, v0_t);\n+  \/\/ with data part 2\n+  vxor_vv(v4, v20, v10, v0_t);\n+\n+  \/\/ highs\n+  vslideup_vi(v16, v12, 1);\n+  vmand_mm(v0, v1, v1);\n+  vxor_vv(v12, v12, v16, v0_t);\n+  \/\/ with data part 2\n+  vxor_vv(v4, v20, v12, v0_t);\n+\n+\n+  \/\/ ======== folding into 16 bytes from 64 bytes in register ========\n+\n+  \/\/ v4:  data, first part, 2 of 64-bits\n+  \/\/ v16: data, second part, 2 of 64-bits\n+  \/\/ v18: data, third part, 2 of 64-bits\n+  \/\/ v20: data, second part, 2 of 64-bits\n+  \/\/ v8:  table\n+\n+  vslidedown_vi(v16, v4, 2);\n+  vslidedown_vi(v18, v4, 4);\n+  vslidedown_vi(v20, v4, 6);\n+\n+  vsetivli(zr, 2, Assembler::e64, Assembler::m1, Assembler::mu, Assembler::tu);\n+\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v8, vclmul_table);\n+  crc32_vclmul_fold_to_16_bytes_vectorsize_32(v4, v20, v8, v28, v29, v30, v31);\n+\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v8, vclmul_table);\n+  crc32_vclmul_fold_to_16_bytes_vectorsize_32(v16, v20, v8, v28, v29, v30, v31);\n+\n+  addi(vclmul_table, vclmul_table, TABLE_STEP);\n+  vle64_v(v8, vclmul_table);\n+  crc32_vclmul_fold_to_16_bytes_vectorsize_32(v18, v20, v8, v28, v29, v30, v31);\n+\n+\n+  \/\/ ======== final: move result to scalar regsiters ========\n+\n+  vmv_x_s(tmp1, v20);\n+  vslidedown_vi(v4, v20, 1);\n+  vmv_x_s(tmp2, v4);\n+\n+  #undef CRC32_VCLMUL_LOAD_TABLE\n+}\n+\n+\/\/ For more details of the algorithm, please check the paper:\n+\/\/   \"Fast CRC Computation for Generic Polynomials Using PCLMULQDQ Instruction - Intel\"\n+\/\/\n+\/\/ Please also refer to the corresponding code in aarch64 or x86 ones.\n+\/\/\n+\/\/ As the riscv carry-less multiplication is a bit different from the other platforms,\n+\/\/ so the implementation itself is also a bit different from others.\n+\n+void MacroAssembler::kernel_crc32_vclmul_fold(Register crc, Register buf, Register len,\n+                        Register table0, Register table1, Register table2, Register table3,\n+                        Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5) {\n+  const int64_t single_table_size = 256;\n+  const int64_t table_num = 8;   \/\/ 4 for scalar, 4 for plain vector\n+  const ExternalAddress table_addr = StubRoutines::crc_table_addr();\n+  Register vclmul_table = tmp3;\n+\n+  la(vclmul_table, table_addr);\n+  add(vclmul_table, vclmul_table, table_num*single_table_size*sizeof(juint), tmp1);\n+  la(table0, table_addr);\n+\n+  if (MaxVectorSize == 16) {\n+    kernel_crc32_vclmul_fold_vectorsize_16(crc, buf, len, vclmul_table, tmp1, tmp2);\n+  } else {\n+    kernel_crc32_vclmul_fold_vectorsize_32(crc, buf, len, vclmul_table, tmp1, tmp2);\n+  }\n+\n+  mv(crc, zr);\n+  update_word_crc32(crc, tmp1, tmp3, tmp4, tmp5, table0, table1, table2, table3, false);\n+  update_word_crc32(crc, tmp1, tmp3, tmp4, tmp5, table0, table1, table2, table3, true);\n+  update_word_crc32(crc, tmp2, tmp3, tmp4, tmp5, table0, table1, table2, table3, false);\n+  update_word_crc32(crc, tmp2, tmp3, tmp4, tmp5, table0, table1, table2, table3, true);\n+}\n+\n@@ -1768,1 +2121,3 @@\n-    const int64_t tmp_limit = MaxVectorSize >= 32 ? unroll_words*3 : unroll_words*5;\n+    const int64_t tmp_limit =\n+            UseZvbc ? 128 * 3 \/\/ 3 rounds of folding with carry-less multiplication\n+                    : MaxVectorSize >= 32 ? unroll_words*3 : unroll_words*5;\n@@ -1830,1 +2185,7 @@\n-    vector_update_crc32(crc, buf, len, tmp1, tmp2, tmp3, tmp4, tmp6, table0, table3);\n+    if (UseZvbc) { \/\/ carry-less multiplication\n+      kernel_crc32_vclmul_fold(crc, buf, len,\n+                               table0, table1, table2, table3,\n+                               tmp1, tmp2, tmp3, tmp4, tmp6);\n+    } else { \/\/ plain vector instructions\n+      vector_update_crc32(crc, buf, len, tmp1, tmp2, tmp3, tmp4, tmp6, table0, table3);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":363,"deletions":2,"binary":false,"changes":365,"status":"modified"},{"patch":"@@ -1314,0 +1314,18 @@\n+  void kernel_crc32_vclmul_fold(Register crc, Register buf, Register len,\n+              Register table0, Register table1, Register table2, Register table3,\n+              Register tmp1, Register tmp2, Register tmp3, Register tmp4, Register tmp5);\n+  void crc32_vclmul_fold_to_16_bytes_vectorsize_32(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                            VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4);\n+  void kernel_crc32_vclmul_fold_vectorsize_32(Register crc, Register buf, Register len,\n+                                              Register vclmul_table, Register tmp1, Register tmp2);\n+  void crc32_vclmul_fold_16_bytes_vectorsize_16(VectorRegister vx, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register buf, Register tmp, const int STEP);\n+  void crc32_vclmul_fold_16_bytes_vectorsize_16_2(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp);\n+  void crc32_vclmul_fold_16_bytes_vectorsize_16_3(VectorRegister vx, VectorRegister vy, VectorRegister vt,\n+                      VectorRegister vtmp1, VectorRegister vtmp2, VectorRegister vtmp3, VectorRegister vtmp4,\n+                      Register tmp);\n+  void kernel_crc32_vclmul_fold_vectorsize_16(Register crc, Register buf, Register len,\n+                                              Register vclmul_table, Register tmp1, Register tmp2);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -482,1 +482,13 @@\n-    0xdbdabcaf\n+    0xdbdabcaf,\n+\n+    \/\/ CRC32 table for carry-less multiplication implementation\n+    0xe88ef372UL, 0x00000001UL,\n+    0x4a7fe880UL, 0x00000001UL,\n+    0x54442bd4UL, 0x00000001UL,\n+    0xc6e41596UL, 0x00000001UL,\n+    0x3db1ecdcUL, 0x00000000UL,\n+    0x74359406UL, 0x00000001UL,\n+    0xf1da05aaUL, 0x00000000UL,\n+    0x5a546366UL, 0x00000001UL,\n+    0x751997d0UL, 0x00000001UL,\n+    0xccaa009eUL, 0x00000000UL,\n","filename":"src\/hotspot\/cpu\/riscv\/stubRoutines_riscv.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -358,0 +358,8 @@\n+  \/\/ UseZvbc (depends on RVV).\n+  if (UseZvbc && !UseRVV) {\n+    if (!FLAG_IS_DEFAULT(UseZvbc)) {\n+      warning(\"Cannot enable UseZvbc on cpu without RVV support.\");\n+    }\n+    FLAG_SET_DEFAULT(UseZvbc, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -167,0 +167,1 @@\n+  decl(ext_Zvbc        , \"Zvbc\"        , RV_NO_FLAG_BIT, true , UPDATE_DEFAULT(UseZvbc))        \\\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -181,0 +181,3 @@\n+  if (is_set(RISCV_HWPROBE_KEY_IMA_EXT_0, RISCV_HWPROBE_EXT_ZVBC)) {\n+    VM_Version::ext_Zvbc.enable_feature();\n+  }\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/riscv_hwprobe.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}