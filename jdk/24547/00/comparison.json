{"files":[{"patch":"@@ -98,1 +98,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = ZGlobalsPointers::min_address_offset_request();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zAddress_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -95,1 +95,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = ZGlobalsPointers::min_address_offset_request();\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/zAddress_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = ZGlobalsPointers::min_address_offset_request();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zAddress_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * ZVirtualToPhysicalRatio);\n+  const size_t address_offset = ZGlobalsPointers::min_address_offset_request();\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zAddress_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -24,2 +24,4 @@\n-#include \"gc\/z\/zNUMA.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zCPU.inline.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n@@ -29,1 +31,3 @@\n-  _count = 1;\n+  _count = !FLAG_IS_DEFAULT(ZFakeNUMA)\n+      ? ZFakeNUMA\n+      : 1;\n@@ -33,0 +37,5 @@\n+  if (is_faked()) {\n+    \/\/ ZFakeNUMA testing, ignores _enabled\n+    return ZCPU::id() % ZFakeNUMA;\n+  }\n+\n","filename":"src\/hotspot\/os\/bsd\/gc\/z\/zNUMA_bsd.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/z\/zPhysicalMemory.inline.hpp\"\n@@ -100,1 +99,1 @@\n-bool ZPhysicalMemoryBacking::commit_inner(zoffset offset, size_t length) const {\n+bool ZPhysicalMemoryBacking::commit_inner(zbacking_offset offset, size_t length) const {\n@@ -105,1 +104,1 @@\n-                      untype(offset) \/ M, untype(to_zoffset_end(offset, length)) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(to_zbacking_offset_end(offset, length)) \/ M, length \/ M);\n@@ -119,1 +118,1 @@\n-size_t ZPhysicalMemoryBacking::commit(zoffset offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit(zbacking_offset offset, size_t length, uint32_t \/* numa_id - ignored *\/) const {\n@@ -127,2 +126,2 @@\n-  zoffset start = offset;\n-  zoffset end = offset + length;\n+  zbacking_offset start = offset;\n+  zbacking_offset end = offset + length;\n@@ -147,1 +146,1 @@\n-size_t ZPhysicalMemoryBacking::uncommit(zoffset offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::uncommit(zbacking_offset offset, size_t length) const {\n@@ -152,1 +151,1 @@\n-                      untype(offset) \/ M, untype(to_zoffset_end(offset, length)) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(to_zbacking_offset_end(offset, length)) \/ M, length \/ M);\n@@ -165,1 +164,1 @@\n-void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const {\n","filename":"src\/hotspot\/os\/bsd\/gc\/z\/zPhysicalMemoryBacking_bsd.cpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-  bool commit_inner(zoffset offset, size_t length) const;\n+  bool commit_inner(zbacking_offset offset, size_t length) const;\n@@ -43,2 +43,2 @@\n-  size_t commit(zoffset offset, size_t length) const;\n-  size_t uncommit(zoffset offset, size_t length) const;\n+  size_t commit(zbacking_offset offset, size_t length, uint32_t numa_id) const;\n+  size_t uncommit(zbacking_offset offset, size_t length) const;\n@@ -46,1 +46,1 @@\n-  void map(zaddress_unsafe addr, size_t size, zoffset offset) const;\n+  void map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const;\n","filename":"src\/hotspot\/os\/bsd\/gc\/z\/zPhysicalMemoryBacking_bsd.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -26,1 +27,1 @@\n-#include \"gc\/z\/zNUMA.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -35,0 +37,2 @@\n+\n+  \/\/ UseNUMA and is_faked() are mutually excluded in zArguments.cpp.\n@@ -37,1 +41,3 @@\n-      : 1;\n+      : !FLAG_IS_DEFAULT(ZFakeNUMA)\n+            ? ZFakeNUMA\n+            : 1;  \/\/ No NUMA nodes\n@@ -41,0 +47,5 @@\n+  if (is_faked()) {\n+    \/\/ ZFakeNUMA testing, ignores _enabled\n+    return ZCPU::id() % ZFakeNUMA;\n+  }\n+\n","filename":"src\/hotspot\/os\/linux\/gc\/z\/zNUMA_linux.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -391,1 +391,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(zoffset offset, size_t length, bool touch) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(zbacking_offset offset, size_t length, bool touch) const {\n@@ -442,1 +442,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(zbacking_offset offset, size_t length) const {\n@@ -471,1 +471,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_compat_pwrite(zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_compat_pwrite(zbacking_offset offset, size_t length) const {\n@@ -475,1 +475,1 @@\n-  for (zoffset pos = offset; pos < offset + length; pos += _block_size) {\n+  for (zbacking_offset pos = offset; pos < offset + length; pos += _block_size) {\n@@ -486,1 +486,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_compat(zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_compat(zbacking_offset offset, size_t length) const {\n@@ -500,1 +500,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_syscall(zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole_syscall(zbacking_offset offset, size_t length) const {\n@@ -512,1 +512,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole(zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_fill_hole(zbacking_offset offset, size_t length) const {\n@@ -539,1 +539,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate_punch_hole(zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate_punch_hole(zbacking_offset offset, size_t length) const {\n@@ -562,1 +562,1 @@\n-ZErrno ZPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, zbacking_offset offset, size_t length) const {\n@@ -564,1 +564,1 @@\n-  const zoffset offset0 = offset;\n+  const zbacking_offset offset0 = offset;\n@@ -572,1 +572,1 @@\n-  const zoffset offset1 = offset0 + length0;\n+  const zbacking_offset offset1 = offset0 + length0;\n@@ -583,1 +583,1 @@\n-ZErrno ZPhysicalMemoryBacking::fallocate(bool punch_hole, zoffset offset, size_t length) const {\n+ZErrno ZPhysicalMemoryBacking::fallocate(bool punch_hole, zbacking_offset offset, size_t length) const {\n@@ -599,1 +599,1 @@\n-bool ZPhysicalMemoryBacking::commit_inner(zoffset offset, size_t length) const {\n+bool ZPhysicalMemoryBacking::commit_inner(zbacking_offset offset, size_t length) const {\n@@ -601,1 +601,1 @@\n-                      untype(offset) \/ M, untype(to_zoffset_end(offset, length)) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(to_zbacking_offset_end(offset, length)) \/ M, length \/ M);\n@@ -630,16 +630,3 @@\n-static int offset_to_node(zoffset offset) {\n-  const GrowableArray<int>* mapping = os::Linux::numa_nindex_to_node();\n-  const size_t nindex = (untype(offset) >> ZGranuleSizeShift) % mapping->length();\n-  return mapping->at((int)nindex);\n-}\n-\n-size_t ZPhysicalMemoryBacking::commit_numa_interleaved(zoffset offset, size_t length) const {\n-  size_t committed = 0;\n-\n-  \/\/ Commit one granule at a time, so that each granule\n-  \/\/ can be allocated from a different preferred node.\n-  while (committed < length) {\n-    const zoffset granule_offset = offset + committed;\n-\n-    \/\/ Setup NUMA policy to allocate memory from a preferred node\n-    os::Linux::numa_set_preferred(offset_to_node(granule_offset));\n+size_t ZPhysicalMemoryBacking::commit_numa_preferred(zbacking_offset offset, size_t length, uint32_t numa_id) const {\n+  \/\/ Setup NUMA policy to allocate memory from a preferred node\n+  os::Linux::numa_set_preferred((int)numa_id);\n@@ -647,7 +634,1 @@\n-    if (!commit_inner(granule_offset, ZGranuleSize)) {\n-      \/\/ Failed\n-      break;\n-    }\n-\n-    committed += ZGranuleSize;\n-  }\n+  const size_t committed = commit_default(offset, length);\n@@ -661,1 +642,1 @@\n-size_t ZPhysicalMemoryBacking::commit_default(zoffset offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit_default(zbacking_offset offset, size_t length) const {\n@@ -669,2 +650,2 @@\n-  zoffset start = offset;\n-  zoffset end = offset + length;\n+  zbacking_offset start = offset;\n+  zbacking_offset_end end = to_zbacking_offset_end(offset, length);\n@@ -689,1 +670,1 @@\n-size_t ZPhysicalMemoryBacking::commit(zoffset offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::commit(zbacking_offset offset, size_t length, uint32_t numa_id) const {\n@@ -691,3 +672,3 @@\n-    \/\/ To get granule-level NUMA interleaving when using non-large pages,\n-    \/\/ we must explicitly interleave the memory at commit\/fallocate time.\n-    return commit_numa_interleaved(offset, length);\n+    \/\/ The memory is required to be preferred at the time it is paged in. As a\n+    \/\/ consequence we must prefer the memory when committing non-large pages.\n+    return commit_numa_preferred(offset, length, numa_id);\n@@ -699,1 +680,1 @@\n-size_t ZPhysicalMemoryBacking::uncommit(zoffset offset, size_t length) const {\n+size_t ZPhysicalMemoryBacking::uncommit(zbacking_offset offset, size_t length) const {\n@@ -701,1 +682,1 @@\n-                      untype(offset) \/ M, untype(to_zoffset_end(offset, length)) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(to_zbacking_offset_end(offset, length)) \/ M, length \/ M);\n@@ -712,1 +693,1 @@\n-void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const {\n","filename":"src\/hotspot\/os\/linux\/gc\/z\/zPhysicalMemoryBacking_linux.cpp","additions":28,"deletions":47,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,9 +51,9 @@\n-  ZErrno fallocate_compat_mmap_hugetlbfs(zoffset offset, size_t length, bool touch) const;\n-  ZErrno fallocate_compat_mmap_tmpfs(zoffset offset, size_t length) const;\n-  ZErrno fallocate_compat_pwrite(zoffset offset, size_t length) const;\n-  ZErrno fallocate_fill_hole_compat(zoffset offset, size_t length) const;\n-  ZErrno fallocate_fill_hole_syscall(zoffset offset, size_t length) const;\n-  ZErrno fallocate_fill_hole(zoffset offset, size_t length) const;\n-  ZErrno fallocate_punch_hole(zoffset offset, size_t length) const;\n-  ZErrno split_and_fallocate(bool punch_hole, zoffset offset, size_t length) const;\n-  ZErrno fallocate(bool punch_hole, zoffset offset, size_t length) const;\n+  ZErrno fallocate_compat_mmap_hugetlbfs(zbacking_offset offset, size_t length, bool touch) const;\n+  ZErrno fallocate_compat_mmap_tmpfs(zbacking_offset offset, size_t length) const;\n+  ZErrno fallocate_compat_pwrite(zbacking_offset offset, size_t length) const;\n+  ZErrno fallocate_fill_hole_compat(zbacking_offset offset, size_t length) const;\n+  ZErrno fallocate_fill_hole_syscall(zbacking_offset offset, size_t length) const;\n+  ZErrno fallocate_fill_hole(zbacking_offset offset, size_t length) const;\n+  ZErrno fallocate_punch_hole(zbacking_offset offset, size_t length) const;\n+  ZErrno split_and_fallocate(bool punch_hole, zbacking_offset offset, size_t length) const;\n+  ZErrno fallocate(bool punch_hole, zbacking_offset offset, size_t length) const;\n@@ -61,3 +61,3 @@\n-  bool commit_inner(zoffset offset, size_t length) const;\n-  size_t commit_numa_interleaved(zoffset offset, size_t length) const;\n-  size_t commit_default(zoffset offset, size_t length) const;\n+  bool commit_inner(zbacking_offset offset, size_t length) const;\n+  size_t commit_numa_preferred(zbacking_offset offset, size_t length, uint32_t numa_id) const;\n+  size_t commit_default(zbacking_offset offset, size_t length) const;\n@@ -72,2 +72,2 @@\n-  size_t commit(zoffset offset, size_t length) const;\n-  size_t uncommit(zoffset offset, size_t length) const;\n+  size_t commit(zbacking_offset offset, size_t length, uint32_t numa_id) const;\n+  size_t uncommit(zbacking_offset offset, size_t length) const;\n@@ -75,1 +75,1 @@\n-  void map(zaddress_unsafe addr, size_t size, zoffset offset) const;\n+  void map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const;\n","filename":"src\/hotspot\/os\/linux\/gc\/z\/zPhysicalMemoryBacking_linux.hpp","additions":16,"deletions":16,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+#include <sys\/mman.h>\n+\n+void ZVirtualMemoryReserver::pd_register_callbacks(ZVirtualMemoryRegistry* registry) {\n+  \/\/ Does nothing\n+}\n+\n+bool ZVirtualMemoryReserver::pd_reserve(zaddress_unsafe addr, size_t size) {\n+  void* const res = mmap((void*)untype(addr), size, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE|MAP_NORESERVE, -1, 0);\n+  if (res == MAP_FAILED) {\n+    \/\/ Failed to reserve memory\n+    return false;\n+  }\n+\n+  if (res != (void*)untype(addr)) {\n+    \/\/ Failed to reserve memory at the requested address\n+    munmap(res, size);\n+    return false;\n+  }\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+void ZVirtualMemoryReserver::pd_unreserve(zaddress_unsafe addr, size_t size) {\n+  const int res = munmap((void*)untype(addr), size);\n+  assert(res == 0, \"Failed to unmap memory\");\n+}\n","filename":"src\/hotspot\/os\/posix\/gc\/z\/zVirtualMemoryManager_posix.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n-#include \"logging\/log.hpp\"\n-\n-#include <sys\/mman.h>\n-#include <sys\/types.h>\n-\n-void ZVirtualMemoryManager::pd_initialize_before_reserve() {\n-  \/\/ Does nothing\n-}\n-\n-void ZVirtualMemoryManager::pd_register_callbacks(ZMemoryManager* manager) {\n-  \/\/ Does nothing\n-}\n-\n-bool ZVirtualMemoryManager::pd_reserve(zaddress_unsafe addr, size_t size) {\n-  void* const res = mmap((void*)untype(addr), size, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE|MAP_NORESERVE, -1, 0);\n-  if (res == MAP_FAILED) {\n-    \/\/ Failed to reserve memory\n-    return false;\n-  }\n-\n-  if (res != (void*)untype(addr)) {\n-    \/\/ Failed to reserve memory at the requested address\n-    munmap(res, size);\n-    return false;\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-void ZVirtualMemoryManager::pd_unreserve(zaddress_unsafe addr, size_t size) {\n-  const int res = munmap((void*)untype(addr), size);\n-  assert(res == 0, \"Failed to unmap memory\");\n-}\n","filename":"src\/hotspot\/os\/posix\/gc\/z\/zVirtualMemory_posix.cpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -27,0 +27,2 @@\n+void ZVirtualMemoryReserverImpl_initialize();\n+\n@@ -29,0 +31,1 @@\n+  ZVirtualMemoryReserverImpl_initialize();\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zInitialize_windows.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -24,1 +24,4 @@\n-#include \"gc\/z\/zNUMA.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zCPU.inline.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n@@ -28,1 +31,3 @@\n-  _count = 1;\n+  _count = !FLAG_IS_DEFAULT(ZFakeNUMA)\n+      ? ZFakeNUMA\n+      : 1;\n@@ -32,0 +37,5 @@\n+  if (is_faked()) {\n+    \/\/ ZFakeNUMA testing\n+    return ZCPU::id() % ZFakeNUMA;\n+  }\n+\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zNUMA_windows.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -36,3 +36,3 @@\n-  virtual size_t commit(zoffset offset, size_t size) = 0;\n-  virtual size_t uncommit(zoffset offset, size_t size) = 0;\n-  virtual void map(zaddress_unsafe addr, size_t size, zoffset offset) const = 0;\n+  virtual size_t commit(zbacking_offset offset, size_t size) = 0;\n+  virtual size_t uncommit(zbacking_offset offset, size_t size) = 0;\n+  virtual void map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const = 0;\n@@ -53,2 +53,8 @@\n-  HANDLE get_handle(zoffset offset) const {\n-    HANDLE const handle = _handles.get(offset);\n+  static zoffset to_zoffset(zbacking_offset offset) {\n+    \/\/ A zbacking_offset is always a valid zoffset\n+    return zoffset(untype(offset));\n+  }\n+\n+  HANDLE get_handle(zbacking_offset offset) const {\n+    const zoffset z_offset = to_zoffset(offset);\n+    HANDLE const handle = _handles.get(z_offset);\n@@ -59,1 +65,2 @@\n-  void put_handle(zoffset offset, HANDLE handle) {\n+  void put_handle(zbacking_offset offset, HANDLE handle) {\n+    const zoffset z_offset = to_zoffset(offset);\n@@ -61,2 +68,2 @@\n-    assert(_handles.get(offset) == 0, \"Should be cleared\");\n-    _handles.put(offset, handle);\n+    assert(_handles.get(z_offset) == 0, \"Should be cleared\");\n+    _handles.put(z_offset, handle);\n@@ -65,3 +72,4 @@\n-  void clear_handle(zoffset offset) {\n-    assert(_handles.get(offset) != 0, \"Should be set\");\n-    _handles.put(offset, 0);\n+  void clear_handle(zbacking_offset offset) {\n+    const zoffset z_offset = to_zoffset(offset);\n+    assert(_handles.get(z_offset) != 0, \"Should be set\");\n+    _handles.put(z_offset, 0);\n@@ -75,1 +83,1 @@\n-  size_t commit(zoffset offset, size_t size) {\n+  size_t commit(zbacking_offset offset, size_t size) {\n@@ -88,1 +96,1 @@\n-  size_t uncommit(zoffset offset, size_t size) {\n+  size_t uncommit(zbacking_offset offset, size_t size) {\n@@ -98,1 +106,1 @@\n-  void map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+  void map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const {\n@@ -152,1 +160,1 @@\n-  size_t commit(zoffset offset, size_t size) {\n+  size_t commit(zbacking_offset offset, size_t size) {\n@@ -170,1 +178,1 @@\n-  size_t uncommit(zoffset offset, size_t size) {\n+  size_t uncommit(zbacking_offset offset, size_t size) {\n@@ -184,1 +192,1 @@\n-  void map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+  void map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const {\n@@ -225,1 +233,1 @@\n-size_t ZPhysicalMemoryBacking::commit(zoffset offset, size_t length) {\n+size_t ZPhysicalMemoryBacking::commit(zbacking_offset offset, size_t length, uint32_t \/* numa_id - ignored *\/) {\n@@ -227,1 +235,1 @@\n-                      untype(offset) \/ M, untype(to_zoffset_end(offset, length)) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(to_zbacking_offset_end(offset, length)) \/ M, length \/ M);\n@@ -232,1 +240,1 @@\n-size_t ZPhysicalMemoryBacking::uncommit(zoffset offset, size_t length) {\n+size_t ZPhysicalMemoryBacking::uncommit(zbacking_offset offset, size_t length) {\n@@ -234,1 +242,1 @@\n-                      untype(offset) \/ M, untype(to_zoffset_end(offset, length)) \/ M, length \/ M);\n+                      untype(offset) \/ M, untype(to_zbacking_offset_end(offset, length)) \/ M, length \/ M);\n@@ -239,1 +247,1 @@\n-void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zoffset offset) const {\n+void ZPhysicalMemoryBacking::map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const {\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zPhysicalMemoryBacking_windows.cpp","additions":30,"deletions":22,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,2 +45,2 @@\n-  size_t commit(zoffset offset, size_t length);\n-  size_t uncommit(zoffset offset, size_t length);\n+  size_t commit(zbacking_offset offset, size_t length, uint32_t numa_id);\n+  size_t uncommit(zbacking_offset offset, size_t length);\n@@ -48,1 +48,1 @@\n-  void map(zaddress_unsafe addr, size_t size, zoffset offset) const;\n+  void map(zaddress_unsafe addr, size_t size, zbacking_offset offset) const;\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zPhysicalMemoryBacking_windows.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,230 @@\n+\/*\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zLargePages.inline.hpp\"\n+#include \"gc\/z\/zMapper_windows.hpp\"\n+#include \"gc\/z\/zSyscall_windows.hpp\"\n+#include \"gc\/z\/zValue.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class ZVirtualMemoryReserverImpl : public CHeapObj<mtGC> {\n+public:\n+  virtual void register_callbacks(ZVirtualMemoryRegistry* registry) {}\n+  virtual bool reserve(zaddress_unsafe addr, size_t size) = 0;\n+  virtual void unreserve(zaddress_unsafe addr, size_t size) = 0;\n+};\n+\n+\/\/ Implements small pages (paged) support using placeholder reservation.\n+\/\/\n+\/\/ When a memory area is available (kept by the virtual memory manager) a\n+\/\/ single placeholder is covering that memory area. When memory is\n+\/\/ removed from the registry the placeholder is split into granule\n+\/\/ sized placeholders to allow mapping operations on that granularity.\n+class ZVirtualMemoryReserverSmallPages : public ZVirtualMemoryReserverImpl {\n+private:\n+  class PlaceholderCallbacks : public AllStatic {\n+  private:\n+    static void split_placeholder(zoffset start, size_t size) {\n+      ZMapper::split_placeholder(ZOffset::address_unsafe(start), size);\n+    }\n+\n+    static void coalesce_placeholders(zoffset start, size_t size) {\n+      ZMapper::coalesce_placeholders(ZOffset::address_unsafe(start), size);\n+    }\n+\n+    \/\/ Turn the single placeholder covering the memory area into granule\n+    \/\/ sized placeholders.\n+    static void split_into_granule_sized_placeholders(zoffset start, size_t size) {\n+      assert(size >= ZGranuleSize, \"Must be at least one granule\");\n+      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n+\n+      \/\/ Don't call split_placeholder on the last granule, since it is already\n+      \/\/ a placeholder and the system call would therefore fail.\n+      const size_t limit = size - ZGranuleSize;\n+      for (size_t offset = 0; offset < limit; offset += ZGranuleSize) {\n+        split_placeholder(start + offset, ZGranuleSize);\n+      }\n+    }\n+\n+    static void coalesce_into_one_placeholder(zoffset start, size_t size) {\n+      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n+\n+      \/\/ Granule sized areas are already covered by a single placeholder\n+      if (size > ZGranuleSize) {\n+        coalesce_placeholders(start, size);\n+      }\n+    }\n+\n+    \/\/ Callback implementations\n+\n+    \/\/ Called when a memory area is going to be handed out to be used.\n+    \/\/\n+    \/\/ Splits the memory area into granule-sized placeholders.\n+    static void prepare_for_hand_out_callback(const ZVirtualMemory& area) {\n+      assert(is_aligned(area.size(), ZGranuleSize), \"Must be granule aligned\");\n+\n+      split_into_granule_sized_placeholders(area.start(), area.size());\n+    }\n+\n+    \/\/ Called when a memory area is handed back to the memory manager.\n+    \/\/\n+    \/\/ Combines the granule-sized placeholders into one placeholder.\n+    static void prepare_for_hand_back_callback(const ZVirtualMemory& area) {\n+      assert(is_aligned(area.size(), ZGranuleSize), \"Must be granule aligned\");\n+\n+      coalesce_into_one_placeholder(area.start(), area.size());\n+    }\n+\n+    \/\/ Called when inserting a memory area and it can be merged with an\n+    \/\/ existing, adjacent memory area.\n+    \/\/\n+    \/\/ Coalesces the underlying placeholders into one.\n+    static void grow_callback(const ZVirtualMemory& from, const ZVirtualMemory& to) {\n+      assert(is_aligned(from.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(is_aligned(to.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(from != to, \"Must have grown\");\n+      assert(to.contains(from), \"Must be within\");\n+\n+      coalesce_into_one_placeholder(to.start(), to.size());\n+    }\n+\n+    \/\/ Called when a memory area is removed from the front or back of an existing\n+    \/\/ memory area.\n+    \/\/\n+    \/\/ Splits the memory into two placeholders.\n+    static void shrink_callback(const ZVirtualMemory& from, const ZVirtualMemory& to) {\n+      assert(is_aligned(from.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(is_aligned(to.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(from != to, \"Must have shrunk\");\n+      assert(from.contains(to), \"Must be larger than what we try to split out\");\n+      assert(from.start() == to.start() || from.end() == to.end(),\n+             \"Only verified to work if we split a placeholder into two placeholders\");\n+\n+      \/\/ Split the area into two placeholders\n+      split_placeholder(to.start(), to.size());\n+    }\n+\n+  public:\n+    static ZVirtualMemoryRegistry::Callbacks callbacks() {\n+      \/\/ Each reserved virtual memory address area registered in _manager is\n+      \/\/ exactly covered by a single placeholder. Callbacks are installed so\n+      \/\/ that whenever a memory area changes, the corresponding placeholder\n+      \/\/ is adjusted.\n+      \/\/\n+      \/\/ The prepare_for_hand_out callback is called when virtual memory is\n+      \/\/ handed out to callers. The memory area is split into granule-sized\n+      \/\/ placeholders.\n+      \/\/\n+      \/\/ The prepare_for_hand_back callback is called when previously handed\n+      \/\/ out virtual memory is handed back  to the memory manager. The\n+      \/\/ returned memory area is then covered by a new single placeholder.\n+      \/\/\n+      \/\/ The grow callback is called when a virtual memory area grows. The\n+      \/\/ resulting memory area is then covered by a single placeholder.\n+      \/\/\n+      \/\/ The shrink callback is called when a virtual memory area is split into\n+      \/\/ two parts. The two resulting memory areas are then covered by two\n+      \/\/ separate placeholders.\n+      \/\/\n+      \/\/ See comment in zMapper_windows.cpp explaining why placeholders are\n+      \/\/ split into ZGranuleSize sized placeholders.\n+\n+      ZVirtualMemoryRegistry::Callbacks callbacks;\n+\n+      callbacks._prepare_for_hand_out = &prepare_for_hand_out_callback;\n+      callbacks._prepare_for_hand_back = &prepare_for_hand_back_callback;\n+      callbacks._grow = &grow_callback;\n+      callbacks._shrink = &shrink_callback;\n+\n+      return callbacks;\n+    }\n+  };\n+\n+  virtual void register_callbacks(ZVirtualMemoryRegistry* registry) {\n+    registry->register_callbacks(PlaceholderCallbacks::callbacks());\n+  }\n+\n+  virtual bool reserve(zaddress_unsafe addr, size_t size) {\n+    const zaddress_unsafe res = ZMapper::reserve(addr, size);\n+\n+    assert(res == addr || untype(res) == 0, \"Should not reserve other memory than requested\");\n+    return res == addr;\n+  }\n+\n+  virtual void unreserve(zaddress_unsafe addr, size_t size) {\n+    ZMapper::unreserve(addr, size);\n+  }\n+};\n+\n+\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n+\n+\/\/ ZPhysicalMemory layer needs access to the section\n+HANDLE ZAWESection;\n+\n+class ZVirtualMemoryReserverLargePages : public ZVirtualMemoryReserverImpl {\n+private:\n+  virtual bool reserve(zaddress_unsafe addr, size_t size) {\n+    const zaddress_unsafe res = ZMapper::reserve_for_shared_awe(ZAWESection, addr, size);\n+\n+    assert(res == addr || untype(res) == 0, \"Should not reserve other memory than requested\");\n+    return res == addr;\n+  }\n+\n+  virtual void unreserve(zaddress_unsafe addr, size_t size) {\n+    ZMapper::unreserve_for_shared_awe(addr, size);\n+  }\n+\n+public:\n+  ZVirtualMemoryReserverLargePages() {\n+    ZAWESection = ZMapper::create_shared_awe_section();\n+  }\n+};\n+\n+static ZVirtualMemoryReserverImpl* _impl = nullptr;\n+\n+void ZVirtualMemoryReserverImpl_initialize() {\n+  assert(_impl == nullptr, \"Should only initialize once\");\n+\n+  if (ZLargePages::is_enabled()) {\n+    _impl = new ZVirtualMemoryReserverLargePages();\n+  } else {\n+    _impl = new ZVirtualMemoryReserverSmallPages();\n+  }\n+}\n+\n+void ZVirtualMemoryReserver::pd_register_callbacks(ZVirtualMemoryRegistry* registry) {\n+  _impl->register_callbacks(registry);\n+}\n+\n+bool ZVirtualMemoryReserver::pd_reserve(zaddress_unsafe addr, size_t size) {\n+  return _impl->reserve(addr, size);\n+}\n+\n+void ZVirtualMemoryReserver::pd_unreserve(zaddress_unsafe addr, size_t size) {\n+  _impl->unreserve(addr, size);\n+}\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zVirtualMemoryManager_windows.cpp","additions":230,"deletions":0,"binary":false,"changes":230,"status":"added"},{"patch":"@@ -1,227 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"gc\/z\/zLargePages.inline.hpp\"\n-#include \"gc\/z\/zMapper_windows.hpp\"\n-#include \"gc\/z\/zSyscall_windows.hpp\"\n-#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-class ZVirtualMemoryManagerImpl : public CHeapObj<mtGC> {\n-public:\n-  virtual void initialize_before_reserve() {}\n-  virtual void register_callbacks(ZMemoryManager* manager) {}\n-  virtual bool reserve(zaddress_unsafe addr, size_t size) = 0;\n-  virtual void unreserve(zaddress_unsafe addr, size_t size) = 0;\n-};\n-\n-\/\/ Implements small pages (paged) support using placeholder reservation.\n-\/\/\n-\/\/ When a memory area is free (kept by the virtual memory manager) a\n-\/\/ single placeholder is covering that memory area. When memory is\n-\/\/ allocated from the manager the placeholder is split into granule\n-\/\/ sized placeholders to allow mapping operations on that granularity.\n-class ZVirtualMemoryManagerSmallPages : public ZVirtualMemoryManagerImpl {\n-private:\n-  class PlaceholderCallbacks : public AllStatic {\n-  private:\n-    static void split_placeholder(zoffset start, size_t size) {\n-      ZMapper::split_placeholder(ZOffset::address_unsafe(start), size);\n-    }\n-\n-    static void coalesce_placeholders(zoffset start, size_t size) {\n-      ZMapper::coalesce_placeholders(ZOffset::address_unsafe(start), size);\n-    }\n-\n-    \/\/ Turn the single placeholder covering the memory area into granule\n-    \/\/ sized placeholders.\n-    static void split_into_granule_sized_placeholders(zoffset start, size_t size) {\n-      assert(size >= ZGranuleSize, \"Must be at least one granule\");\n-      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n-\n-      \/\/ Don't call split_placeholder on the last granule, since it is already\n-      \/\/ a placeholder and the system call would therefore fail.\n-      const size_t limit = size - ZGranuleSize;\n-      for (size_t offset = 0; offset < limit; offset += ZGranuleSize) {\n-        split_placeholder(start + offset, ZGranuleSize);\n-      }\n-    }\n-\n-    static void coalesce_into_one_placeholder(zoffset start, size_t size) {\n-      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n-\n-      \/\/ Granule sized areas are already covered by a single placeholder\n-      if (size > ZGranuleSize) {\n-        coalesce_placeholders(start, size);\n-      }\n-    }\n-\n-    \/\/ Callback implementations\n-\n-    \/\/ Called when a memory area is going to be handed out to be used.\n-    \/\/\n-    \/\/ Splits the memory area into granule-sized placeholders.\n-    static void prepare_for_hand_out_callback(const ZMemory& area) {\n-      assert(is_aligned(area.size(), ZGranuleSize), \"Must be granule aligned\");\n-\n-      split_into_granule_sized_placeholders(area.start(), area.size());\n-    }\n-\n-    \/\/ Called when a memory area is handed back to the memory manager.\n-    \/\/\n-    \/\/ Combines the granule-sized placeholders into one placeholder.\n-    static void prepare_for_hand_back_callback(const ZMemory& area) {\n-      assert(is_aligned(area.size(), ZGranuleSize), \"Must be granule aligned\");\n-\n-      coalesce_into_one_placeholder(area.start(), area.size());\n-    }\n-\n-    \/\/ Called when inserting a memory area and it can be merged with an\n-    \/\/ existing, adjacent memory area.\n-    \/\/\n-    \/\/ Coalesces the underlying placeholders into one.\n-    static void grow_callback(const ZMemory& from, const ZMemory& to) {\n-      assert(is_aligned(from.size(), ZGranuleSize), \"Must be granule aligned\");\n-      assert(is_aligned(to.size(), ZGranuleSize), \"Must be granule aligned\");\n-      assert(from != to, \"Must have grown\");\n-      assert(to.contains(from), \"Must be within\");\n-\n-      coalesce_into_one_placeholder(to.start(), to.size());\n-    }\n-\n-    \/\/ Called when a memory area is removed from the front or back of an existing\n-    \/\/ memory area.\n-    \/\/\n-    \/\/ Splits the memory into two placeholders.\n-    static void shrink_callback(const ZMemory& from, const ZMemory& to) {\n-      assert(is_aligned(from.size(), ZGranuleSize), \"Must be granule aligned\");\n-      assert(is_aligned(to.size(), ZGranuleSize), \"Must be granule aligned\");\n-      assert(from != to, \"Must have shrunk\");\n-      assert(from.contains(to), \"Must be larger than what we try to split out\");\n-      assert(from.start() == to.start() || from.end() == to.end(),\n-             \"Only verified to work if we split a placeholder into two placeholders\");\n-\n-      \/\/ Split the area into two placeholders\n-      split_placeholder(to.start(), to.size());\n-    }\n-\n-  public:\n-    static ZMemoryManager::Callbacks callbacks() {\n-      \/\/ Each reserved virtual memory address area registered in _manager is\n-      \/\/ exactly covered by a single placeholder. Callbacks are installed so\n-      \/\/ that whenever a memory area changes, the corresponding placeholder\n-      \/\/ is adjusted.\n-      \/\/\n-      \/\/ The prepare_for_hand_out callback is called when virtual memory is\n-      \/\/ handed out to callers. The memory area is split into granule-sized\n-      \/\/ placeholders.\n-      \/\/\n-      \/\/ The prepare_for_hand_back callback is called when previously handed\n-      \/\/ out virtual memory is handed back  to the memory manager. The\n-      \/\/ returned memory area is then covered by a new single placeholder.\n-      \/\/\n-      \/\/ The grow callback is called when a virtual memory area grows. The\n-      \/\/ resulting memory area is then covered by a single placeholder.\n-      \/\/\n-      \/\/ The shrink callback is called when a virtual memory area is split into\n-      \/\/ two parts. The two resulting memory areas are then covered by two\n-      \/\/ separate placeholders.\n-      \/\/\n-      \/\/ See comment in zMapper_windows.cpp explaining why placeholders are\n-      \/\/ split into ZGranuleSize sized placeholders.\n-\n-      ZMemoryManager::Callbacks callbacks;\n-\n-      callbacks._prepare_for_hand_out = &prepare_for_hand_out_callback;\n-      callbacks._prepare_for_hand_back = &prepare_for_hand_back_callback;\n-      callbacks._grow = &grow_callback;\n-      callbacks._shrink = &shrink_callback;\n-\n-      return callbacks;\n-    }\n-  };\n-\n-  virtual void register_callbacks(ZMemoryManager* manager) {\n-    manager->register_callbacks(PlaceholderCallbacks::callbacks());\n-  }\n-\n-  virtual bool reserve(zaddress_unsafe addr, size_t size) {\n-    const zaddress_unsafe res = ZMapper::reserve(addr, size);\n-\n-    assert(res == addr || untype(res) == 0, \"Should not reserve other memory than requested\");\n-    return res == addr;\n-  }\n-\n-  virtual void unreserve(zaddress_unsafe addr, size_t size) {\n-    ZMapper::unreserve(addr, size);\n-  }\n-};\n-\n-\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n-\n-\/\/ ZPhysicalMemory layer needs access to the section\n-HANDLE ZAWESection;\n-\n-class ZVirtualMemoryManagerLargePages : public ZVirtualMemoryManagerImpl {\n-private:\n-  virtual void initialize_before_reserve() {\n-    ZAWESection = ZMapper::create_shared_awe_section();\n-  }\n-\n-  virtual bool reserve(zaddress_unsafe addr, size_t size) {\n-    const zaddress_unsafe res = ZMapper::reserve_for_shared_awe(ZAWESection, addr, size);\n-\n-    assert(res == addr || untype(res) == 0, \"Should not reserve other memory than requested\");\n-    return res == addr;\n-  }\n-\n-  virtual void unreserve(zaddress_unsafe addr, size_t size) {\n-    ZMapper::unreserve_for_shared_awe(addr, size);\n-  }\n-};\n-\n-static ZVirtualMemoryManagerImpl* _impl = nullptr;\n-\n-void ZVirtualMemoryManager::pd_initialize_before_reserve() {\n-  if (ZLargePages::is_enabled()) {\n-    _impl = new ZVirtualMemoryManagerLargePages();\n-  } else {\n-    _impl = new ZVirtualMemoryManagerSmallPages();\n-  }\n-  _impl->initialize_before_reserve();\n-}\n-\n-void ZVirtualMemoryManager::pd_register_callbacks(ZMemoryManager* manager) {\n-  _impl->register_callbacks(manager);\n-}\n-\n-bool ZVirtualMemoryManager::pd_reserve(zaddress_unsafe addr, size_t size) {\n-  return _impl->reserve(addr, size);\n-}\n-\n-void ZVirtualMemoryManager::pd_unreserve(zaddress_unsafe addr, size_t size) {\n-  _impl->unreserve(addr, size);\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zVirtualMemory_windows.cpp","additions":0,"deletions":227,"binary":false,"changes":227,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+#include \"gc\/z\/zNUMA.hpp\"\n@@ -34,0 +35,2 @@\n+#include \"gc\/z\/zValue.hpp\"\n+#include \"gc\/z\/zVirtualMemory.hpp\"\n@@ -64,0 +67,1 @@\n+typedef ZValue<ZPerNUMAStorage, ZPartition> ZPerNUMAZPartition;\n@@ -90,2 +94,7 @@\n-  volatile_nonstatic_field(ZPageAllocator,      _capacity,            size_t)                        \\\n-  volatile_nonstatic_field(ZPageAllocator,      _used,                size_t)                        \\\n+  nonstatic_field(ZPageAllocator,               _partitions,          ZPerNUMAZPartition)            \\\n+                                                                                                     \\\n+  static_field(ZNUMA,                           _count,               uint32_t)                      \\\n+  nonstatic_field(ZPerNUMAZPartition,           _addr,                const uintptr_t)               \\\n+                                                                                                     \\\n+  volatile_nonstatic_field(ZPartition,          _capacity,            size_t)                        \\\n+  nonstatic_field(ZPartition,                   _used,                size_t)                        \\\n@@ -100,2 +109,2 @@\n-  nonstatic_field(ZVirtualMemory,               _start,               const zoffset)                 \\\n-  nonstatic_field(ZVirtualMemory,               _end,                 const zoffset_end)             \\\n+  nonstatic_field(ZVirtualMemory,               _start,               const zoffset_end)             \\\n+  nonstatic_field(ZVirtualMemory,               _size,                const size_t)                  \\\n@@ -137,0 +146,3 @@\n+  declare_toplevel_type(ZPartition)                                                                  \\\n+  declare_toplevel_type(ZNUMA)                                                                       \\\n+  declare_toplevel_type(ZPerNUMAZPartition)                                                          \\\n","filename":"src\/hotspot\/share\/gc\/z\/vmStructs_z.hpp","additions":17,"deletions":5,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n@@ -39,0 +40,4 @@\n+size_t     ZBackingOffsetMax;\n+\n+uint32_t   ZBackingIndexMax;\n+\n@@ -148,0 +153,7 @@\n+\n+size_t ZGlobalsPointers::min_address_offset_request() {\n+  \/\/ See ZVirtualMemoryReserver for logic around setting up the heap for NUMA\n+  const size_t desired_for_heap = MaxHeapSize * ZVirtualToPhysicalRatio;\n+  const size_t desired_for_numa_multiplier = ZNUMA::count() > 1 ? 2 : 1;\n+  return round_up_power_of_2(desired_for_heap * desired_for_numa_multiplier);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,6 @@\n+\/\/ Describes the maximal offset inside the backing storage.\n+extern size_t    ZBackingOffsetMax;\n+\n+\/\/ Describes the maximal granule index inside the backing storage.\n+extern uint32_t  ZBackingIndexMax;\n+\n@@ -226,2 +232,6 @@\n-\/\/ - Physical memory offsets\n-enum class zoffset         : uintptr_t {};\n+enum class zoffset             : uintptr_t { invalid = UINTPTR_MAX };\n+\/\/ Offsets including end of offset range\n+enum class zoffset_end         : uintptr_t { invalid = UINTPTR_MAX };\n+\n+\/\/ - Physical memory segment offsets\n+enum class zbacking_offset     : uintptr_t {};\n@@ -229,1 +239,6 @@\n-enum class zoffset_end     : uintptr_t {};\n+enum class zbacking_offset_end : uintptr_t {};\n+\n+\/\/ - Physical memory segment indicies\n+enum class zbacking_index      : uint32_t { zero = 0, invalid = UINT32_MAX };\n+\/\/ Offsets including end of indicies range\n+enum class zbacking_index_end  : uint32_t { zero = 0, invalid = UINT32_MAX };\n@@ -232,1 +247,1 @@\n-enum class zpointer        : uintptr_t { null = 0 };\n+enum class zpointer            : uintptr_t { null = 0 };\n@@ -235,1 +250,1 @@\n-enum class zaddress        : uintptr_t { null = 0 };\n+enum class zaddress            : uintptr_t { null = 0 };\n@@ -310,0 +325,2 @@\n+\n+  static size_t min_address_offset_request();\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.hpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zGlobals.hpp\"\n@@ -33,0 +34,2 @@\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/checkedCast.hpp\"\n@@ -38,0 +41,107 @@\n+#include <type_traits>\n+\n+\/\/ Offset Operator Macro\n+\/\/ Creates operators for the offset, offset_end style types\n+\n+#define CREATE_ZOFFSET_OPERATORS(offset_type)                                             \\\n+                                                                                          \\\n+  \/* Arithmetic operators for offset_type *\/                                              \\\n+                                                                                          \\\n+inline offset_type operator+(offset_type offset, size_t size) {                           \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type>>(size);        \\\n+  return to_##offset_type(untype(offset) + size_value);                                   \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline offset_type& operator+=(offset_type& offset, size_t size) {                        \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type>>(size);        \\\n+  offset = to_##offset_type(untype(offset) + size_value);                                 \\\n+  return offset;                                                                          \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline offset_type operator-(offset_type offset, size_t size) {                           \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type>>(size);        \\\n+  return to_##offset_type(untype(offset) - size_value);                                   \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline size_t operator-(offset_type first, offset_type second) {                          \\\n+  return untype(first - untype(second));                                                  \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline offset_type& operator-=(offset_type& offset, size_t size) {                        \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type>>(size);        \\\n+  offset = to_##offset_type(untype(offset) - size_value);                                 \\\n+  return offset;                                                                          \\\n+}                                                                                         \\\n+                                                                                          \\\n+  \/* Arithmetic operators for offset_type##_end *\/                                        \\\n+                                                                                          \\\n+inline offset_type##_end operator+(offset_type##_end offset, size_t size) {               \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type##_end>>(size);  \\\n+  return to_##offset_type##_end(untype(offset) + size_value);                             \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline offset_type##_end& operator+=(offset_type##_end& offset, size_t size) {            \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type##_end>>(size);  \\\n+  offset = to_##offset_type##_end(untype(offset) + size_value);                           \\\n+  return offset;                                                                          \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline offset_type##_end operator-(offset_type##_end first, size_t size) {                \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type##_end>>(size);  \\\n+  return to_##offset_type##_end(untype(first) - size_value);                              \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline size_t operator-(offset_type##_end first, offset_type##_end second) {              \\\n+  return untype(first - untype(second));                                                  \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline offset_type##_end& operator-=(offset_type##_end& offset, size_t size) {            \\\n+  const auto size_value = checked_cast<std::underlying_type_t<offset_type##_end>>(size);  \\\n+  offset = to_##offset_type##_end(untype(offset) - size_value);                           \\\n+  return offset;                                                                          \\\n+}                                                                                         \\\n+                                                                                          \\\n+  \/* Arithmetic operators for offset_type cross offset_type##_end *\/                      \\\n+                                                                                          \\\n+inline size_t operator-(offset_type##_end first, offset_type second) {                    \\\n+  return untype(first - untype(second));                                                  \\\n+}                                                                                         \\\n+                                                                                          \\\n+  \/* Logical operators for offset_type cross offset_type##_end *\/                         \\\n+                                                                                          \\\n+inline bool operator!=(offset_type first, offset_type##_end second) {                     \\\n+  return untype(first) != untype(second);                                                 \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator!=(offset_type##_end first, offset_type second) {                     \\\n+  return untype(first) != untype(second);                                                 \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator==(offset_type first, offset_type##_end second) {                     \\\n+  return untype(first) == untype(second);                                                 \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator==(offset_type##_end first, offset_type second) {                     \\\n+  return untype(first) == untype(second);                                                 \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator<(offset_type##_end first, offset_type second) {                      \\\n+  return untype(first) < untype(second);                                                  \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator<(offset_type first, offset_type##_end second) {                      \\\n+  return untype(first) < untype(second);                                                  \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator<=(offset_type##_end first, offset_type second) {                     \\\n+  return untype(first) <= untype(second);                                                 \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator>(offset_type first, offset_type##_end second) {                      \\\n+  return untype(first) > untype(second);                                                  \\\n+}                                                                                         \\\n+                                                                                          \\\n+inline bool operator>=(offset_type first, offset_type##_end second) {                     \\\n+  return untype(first) >= untype(second);                                                 \\\n+}                                                                                         \\\n+\n@@ -62,25 +172,0 @@\n-inline zoffset operator+(zoffset offset, size_t size) {\n-  return to_zoffset(untype(offset) + size);\n-}\n-\n-inline zoffset& operator+=(zoffset& offset, size_t size) {\n-  offset = to_zoffset(untype(offset) + size);\n-  return offset;\n-}\n-\n-inline zoffset operator-(zoffset offset, size_t size) {\n-  const uintptr_t value = untype(offset) - size;\n-  return to_zoffset(value);\n-}\n-\n-inline size_t operator-(zoffset left, zoffset right) {\n-  const size_t diff = untype(left) - untype(right);\n-  assert(diff < ZAddressOffsetMax, \"Underflow\");\n-  return diff;\n-}\n-\n-inline zoffset& operator-=(zoffset& offset, size_t size) {\n-  offset = to_zoffset(untype(offset) - size);\n-  return offset;\n-}\n-\n@@ -112,2 +197,19 @@\n-inline bool operator!=(zoffset first, zoffset_end second) {\n-  return untype(first) != untype(second);\n+CREATE_ZOFFSET_OPERATORS(zoffset)\n+\n+\/\/ zbacking_offset functions\n+\n+inline uintptr_t untype(zbacking_offset offset) {\n+  const uintptr_t value = static_cast<uintptr_t>(offset);\n+  assert(value < ZBackingOffsetMax, \"must have no other bits\");\n+  return value;\n+}\n+\n+inline uintptr_t untype(zbacking_offset_end offset) {\n+  const uintptr_t value = static_cast<uintptr_t>(offset);\n+  assert(value <= ZBackingOffsetMax, \"must have no other bits\");\n+  return value;\n+}\n+\n+inline zbacking_offset to_zbacking_offset(uintptr_t value) {\n+  assert(value < ZBackingOffsetMax, \"must have no other bits\");\n+  return zbacking_offset(value);\n@@ -116,2 +218,3 @@\n-inline bool operator!=(zoffset_end first, zoffset second) {\n-  return untype(first) != untype(second);\n+inline zbacking_offset to_zbacking_offset(zbacking_offset_end offset) {\n+  const uintptr_t value = untype(offset);\n+  return to_zbacking_offset(value);\n@@ -120,2 +223,5 @@\n-inline bool operator==(zoffset first, zoffset_end second) {\n-  return untype(first) == untype(second);\n+inline zbacking_offset_end to_zbacking_offset_end(zbacking_offset start, size_t size) {\n+  const uintptr_t value = untype(start) + size;\n+  assert(value <= ZBackingOffsetMax, \"Overflow start: \" PTR_FORMAT \" size: \" PTR_FORMAT \" value: \" PTR_FORMAT,\n+                                     untype(start), size, value);\n+  return zbacking_offset_end(value);\n@@ -124,2 +230,3 @@\n-inline bool operator==(zoffset_end first, zoffset second) {\n-  return untype(first) == untype(second);\n+inline zbacking_offset_end to_zbacking_offset_end(uintptr_t value) {\n+  assert(value <= ZBackingOffsetMax, \"must have no other bits\");\n+  return zbacking_offset_end(value);\n@@ -128,2 +235,2 @@\n-inline bool operator<(zoffset_end first, zoffset second) {\n-  return untype(first) < untype(second);\n+inline zbacking_offset_end to_zbacking_offset_end(zbacking_offset offset) {\n+  return zbacking_offset_end(untype(offset));\n@@ -132,2 +239,8 @@\n-inline bool operator<(zoffset first, zoffset_end second) {\n-  return untype(first) < untype(second);\n+CREATE_ZOFFSET_OPERATORS(zbacking_offset)\n+\n+\/\/ zbacking_index functions\n+\n+inline uint32_t untype(zbacking_index index) {\n+  const uint32_t value = static_cast<uint32_t>(index);\n+  assert(value < ZBackingIndexMax, \"must have no other bits\");\n+  return value;\n@@ -136,2 +249,4 @@\n-inline bool operator<=(zoffset_end first, zoffset second) {\n-  return untype(first) <= untype(second);\n+inline uint32_t untype(zbacking_index_end index) {\n+  const uint32_t value = static_cast<uint32_t>(index);\n+  assert(value <= ZBackingIndexMax, \"must have no other bits\");\n+  return value;\n@@ -140,2 +255,3 @@\n-inline bool operator>(zoffset first, zoffset_end second) {\n-  return untype(first) > untype(second);\n+inline zbacking_index to_zbacking_index(uint32_t value) {\n+  assert(value < ZBackingIndexMax, \"must have no other bits\");\n+  return zbacking_index(value);\n@@ -144,2 +260,3 @@\n-inline bool operator>=(zoffset first, zoffset_end second) {\n-  return untype(first) >= untype(second);\n+inline zbacking_index to_zbacking_index(zbacking_index_end index) {\n+  const uint32_t value = untype(index);\n+  return to_zbacking_index(value);\n@@ -148,2 +265,6 @@\n-inline size_t operator-(zoffset_end first, zoffset second) {\n-  return untype(first) - untype(second);\n+inline zbacking_index_end to_zbacking_index_end(zbacking_index start, size_t size) {\n+  const uint32_t start_value = untype(start);\n+  const uint32_t value = start_value + checked_cast<uint32_t>(size);\n+  assert(value <= ZBackingIndexMax && start_value <= value,\n+         \"Overflow start: %x size: %zu value: %x\", start_value, size, value);\n+  return zbacking_index_end(value);\n@@ -152,2 +273,30 @@\n-inline zoffset_end operator-(zoffset_end first, size_t second) {\n-  return to_zoffset_end(untype(first) - second);\n+inline zbacking_index_end to_zbacking_index_end(uint32_t value) {\n+  assert(value <= ZBackingIndexMax, \"must have no other bits\");\n+  return zbacking_index_end(value);\n+}\n+\n+inline zbacking_index_end to_zbacking_index_end(zbacking_index index) {\n+  return zbacking_index_end(untype(index));\n+}\n+\n+CREATE_ZOFFSET_OPERATORS(zbacking_index)\n+\n+#undef CREATE_ZOFFSET_OPERATORS\n+\n+\/\/ zbacking_offset <-> zbacking_index conversion functions\n+\n+inline zbacking_index to_zbacking_index(zbacking_offset offset) {\n+  const uintptr_t value = untype(offset);\n+  assert(is_aligned(value, ZGranuleSize), \"must be granule aligned\");\n+  return to_zbacking_index((uint32_t)(value >> ZGranuleSizeShift));\n+}\n+\n+inline zbacking_offset to_zbacking_offset(zbacking_index index) {\n+  const uintptr_t value = untype(index);\n+  return to_zbacking_offset(value << ZGranuleSizeShift);\n+}\n+\n+\/\/ ZRange helper functions\n+\n+inline zoffset to_start_type(zoffset_end offset) {\n+  return to_zoffset(offset);\n@@ -156,2 +305,2 @@\n-inline size_t operator-(zoffset_end first, zoffset_end second) {\n-  return untype(first) - untype(second);\n+inline zbacking_index to_start_type(zbacking_index_end offset) {\n+  return to_zbacking_index(offset);\n@@ -160,3 +309,2 @@\n-inline zoffset_end& operator-=(zoffset_end& offset, size_t size) {\n-  offset = to_zoffset_end(untype(offset) - size);\n-  return offset;\n+inline zoffset_end to_end_type(zoffset start, size_t size) {\n+  return to_zoffset_end(start, size);\n@@ -165,3 +313,2 @@\n-inline zoffset_end& operator+=(zoffset_end& offset, size_t size) {\n-  offset = to_zoffset_end(untype(offset) + size);\n-  return offset;\n+inline zbacking_index_end to_end_type(zbacking_index start, size_t size) {\n+  return to_zbacking_index_end(start, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.inline.hpp","additions":203,"deletions":56,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -47,0 +49,10 @@\n+\n+void ZAddressSpaceLimit::print_limits() {\n+  const size_t limit = address_space_limit();\n+\n+  if (limit == SIZE_MAX) {\n+    log_info_p(gc, init)(\"Address Space Size: unlimited\");\n+  } else {\n+    log_info_p(gc, init)(\"Address Space Size: limited (\" EXACTFMT \")\", EXACTFMTARGS(limit));\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zAddressSpaceLimit.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,2 @@\n+\n+  static void print_limits();\n","filename":"src\/hotspot\/share\/gc\/z\/zAddressSpaceLimit.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,10 +34,8 @@\n-\/\/   7     2 1 0\n-\/\/  +-----+-+-+-+\n-\/\/  |00000|1|1|1|\n-\/\/  +-----+-+-+-+\n-\/\/  |     | | |\n-\/\/  |     | | * 0-0 Non-Blocking Flag (1-bit)\n-\/\/  |     | |\n-\/\/  |     | * 1-1 GC Relocation Flag (1-bit)\n-\/\/  |     |\n-\/\/  |     * 2-2 Low Address Flag (1-bit)\n+\/\/   7      1 0\n+\/\/  +------+-+-+\n+\/\/  |000000|1|1|\n+\/\/  +------+-+-+\n+\/\/  |      | |\n+\/\/  |      | * 0-0 Non-Blocking Flag (1-bit)\n+\/\/  |      |\n+\/\/  |      * 1-1 GC Relocation Flag (1-bit)\n@@ -45,1 +43,1 @@\n-\/\/  * 7-3 Unused (5-bits)\n+\/\/  * 7-2 Unused (6-bits)\n@@ -52,1 +50,0 @@\n-  typedef ZBitField<uint8_t, bool, 2, 1> field_low_address;\n@@ -68,4 +65,0 @@\n-  void set_low_address() {\n-    _flags |= field_low_address::encode(true);\n-  }\n-\n@@ -79,4 +72,0 @@\n-\n-  bool low_address() const {\n-    return field_low_address::decode(_flags);\n-  }\n","filename":"src\/hotspot\/share\/gc\/z\/zAllocationFlags.hpp","additions":10,"deletions":21,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-  if (FLAG_IS_DEFAULT(UseNUMA)) {\n+  if (FLAG_IS_DEFAULT(UseNUMA) && FLAG_IS_DEFAULT(ZFakeNUMA)) {\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+template<typename T> class ZArray;\n@@ -37,1 +38,40 @@\n-template <typename T> using ZArray = GrowableArrayCHeap<T, mtGC>;\n+template <typename T>\n+class ZArraySlice : public GrowableArrayView<T> {\n+  friend class ZArray<T>;\n+  friend class ZArray<std::remove_const_t<T>>;\n+  friend class ZArraySlice<std::remove_const_t<T>>;\n+  friend class ZArraySlice<const T>;\n+\n+private:\n+  ZArraySlice(T* data, int len);\n+\n+public:\n+  ZArraySlice<T> slice_front(int end);\n+  ZArraySlice<const T> slice_front(int end) const;\n+\n+  ZArraySlice<T> slice_back(int start);\n+  ZArraySlice<const T> slice_back(int start) const;\n+\n+  ZArraySlice<T> slice(int start, int end);\n+  ZArraySlice<const T> slice(int start, int end) const;\n+\n+  operator ZArraySlice<const T>() const;\n+};\n+\n+template <typename T>\n+class ZArray : public GrowableArrayCHeap<T, mtGC> {\n+public:\n+  using GrowableArrayCHeap<T, mtGC>::GrowableArrayCHeap;\n+\n+  ZArraySlice<T> slice_front(int end);\n+  ZArraySlice<const T> slice_front(int end) const;\n+\n+  ZArraySlice<T> slice_back(int start);\n+  ZArraySlice<const T> slice_back(int start) const;\n+\n+  ZArraySlice<T> slice(int start, int end);\n+  ZArraySlice<const T> slice(int start, int end) const;\n+\n+  operator ZArraySlice<T>();\n+  operator ZArraySlice<const T>() const;\n+};\n","filename":"src\/hotspot\/share\/gc\/z\/zArray.hpp","additions":42,"deletions":2,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,87 @@\n+template <typename T>\n+ZArraySlice<T>::ZArraySlice(T* data, int len)\n+  : GrowableArrayView<T>(data, len, len) {}\n+\n+template <typename T>\n+ZArraySlice<T> ZArraySlice<T>::slice_front(int end) {\n+  return slice(0, end);\n+}\n+\n+template <typename T>\n+ZArraySlice<const T> ZArraySlice<T>::slice_front(int end) const {\n+  return slice(0, end);\n+}\n+\n+template <typename T>\n+ZArraySlice<T> ZArraySlice<T>::slice_back(int start) {\n+  return slice(start, this->_len);\n+}\n+\n+template <typename T>\n+ZArraySlice<const T> ZArraySlice<T>::slice_back(int start) const {\n+  return slice(start, this->_len);\n+}\n+\n+template <typename T>\n+ZArraySlice<T> ZArraySlice<T>::slice(int start, int end) {\n+  assert(0 <= start && start <= end && end <= this->_len,\n+         \"slice called with invalid range (%d, %d) for length %d\", start, end, this->_len);\n+  return ZArraySlice<T>(this->_data + start, end - start);\n+}\n+\n+template <typename T>\n+ZArraySlice<const T> ZArraySlice<T>::slice(int start, int end) const {\n+  assert(0 <= start && start <= end && end <= this->_len,\n+         \"slice called with invalid range (%d, %d) for length %d\", start, end, this->_len);\n+  return ZArraySlice<const T>(this->_data + start, end - start);\n+}\n+\n+template <typename T>\n+ZArraySlice<T>::operator ZArraySlice<const T>() const {\n+  return slice(0, this->_len);\n+}\n+\n+template <typename T>\n+ZArraySlice<T> ZArray<T>::slice_front(int end) {\n+  return slice(0, end);\n+}\n+\n+template <typename T>\n+ZArraySlice<const T> ZArray<T>::slice_front(int end) const {\n+  return slice(0, end);\n+}\n+\n+template <typename T>\n+ZArraySlice<T> ZArray<T>::slice_back(int start) {\n+  return slice(start, this->_len);\n+}\n+\n+template <typename T>\n+ZArraySlice<const T> ZArray<T>::slice_back(int start) const {\n+  return slice(start, this->_len);\n+}\n+\n+template <typename T>\n+ZArraySlice<T> ZArray<T>::slice(int start, int end) {\n+  assert(0 <= start && start <= end && end <= this->_len,\n+         \"slice called with invalid range (%d, %d) for length %d\", start, end, this->_len);\n+  return ZArraySlice<T>(this->_data + start, end - start);\n+}\n+\n+template <typename T>\n+ZArraySlice<const T> ZArray<T>::slice(int start, int end) const {\n+  assert(0 <= start && start <= end && end <= this->_len,\n+         \"slice called with invalid range (%d, %d) for length %d\", start, end, this->_len);\n+  return ZArraySlice<const T>(this->_data + start, end - start);\n+}\n+\n+template <typename T>\n+ZArray<T>::operator ZArraySlice<T>() {\n+  return slice(0, this->_len);\n+}\n+\n+template <typename T>\n+ZArray<T>::operator ZArraySlice<const T>() const {\n+  return slice(0, this->_len);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zArray.inline.hpp","additions":88,"deletions":1,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -248,1 +249,1 @@\n-  const size_t initial_size = ZHeap::heap()->initial_capacity();\n+  const size_t initial_size = InitialHeapSize;\n@@ -358,0 +359,2 @@\n+  StreamAutoIndentor auto_indentor(st);\n+\n@@ -362,0 +365,2 @@\n+  StreamAutoIndentor auto_indentor(st);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zCollectedHeap.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-    const size_t freed = ZHeap::heap()->free_empty_pages(selector->empty_pages());\n+    const size_t freed = ZHeap::heap()->free_empty_pages(_id, selector->empty_pages());\n@@ -193,11 +193,0 @@\n-        \/\/ Note that the seqnum can change under our feet here as the page\n-        \/\/ can be concurrently freed and recycled by a concurrent generation\n-        \/\/ collection. However this property is stable across such transitions.\n-        \/\/ If it was not relocatable before recycling, then it won't be\n-        \/\/ relocatable after it gets recycled either, as the seqnum atomically\n-        \/\/ becomes allocating for the given generation. The opposite property\n-        \/\/ also holds: if the page is relocatable, then it can't have been\n-        \/\/ concurrently freed; if it was re-allocated it would not be\n-        \/\/ relocatable, and if it was not re-allocated we know that it was\n-        \/\/ allocated earlier than mark start of the current generation\n-        \/\/ collection.\n@@ -216,9 +205,8 @@\n-        \/\/ An active iterator blocks immediate recycle and delete of pages.\n-        \/\/ The intent it to allow the code that iterates over the pages to\n-        \/\/ safely read the properties of the pages without them being changed\n-        \/\/ by another thread. However, this function both iterates over the\n-        \/\/ pages AND frees\/recycles them. We \"yield\" the iterator, so that we\n-        \/\/ can perform immediate recycling (as long as no other thread is\n-        \/\/ iterating over the pages). The contract is that the pages that are\n-        \/\/ about to be freed are \"owned\" by this thread, and no other thread\n-        \/\/ will change their states.\n+        \/\/ An active iterator blocks immediate deletion of pages. The intent is\n+        \/\/ to allow the code that iterates over pages to safely read properties\n+        \/\/ of the pages without them being freed\/deleted. However, this\n+        \/\/ function both iterates over the pages AND frees them. We \"yield\" the\n+        \/\/ iterator, so that we can perform immediate deletion (as long as no\n+        \/\/ other thread is iterating over the pages). The contract is that the\n+        \/\/ pages that are about to be freed are \"owned\" by this thread, and no\n+        \/\/ other thread will change their states.\n@@ -937,1 +925,1 @@\n-  _page_allocator->promote_used(from_page->size());\n+  _page_allocator->promote_used(from_page, to_page);\n@@ -946,1 +934,1 @@\n-  _page_allocator->promote_used(from_page->size());\n+  _page_allocator->promote_used(from_page, to_page);\n","filename":"src\/hotspot\/share\/gc\/z\/zGeneration.cpp","additions":11,"deletions":23,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,0 +58,3 @@\n+\n+  const T* addr(zoffset offset) const;\n+  T* addr(zoffset offset);\n","filename":"src\/hotspot\/share\/gc\/z\/zGranuleMap.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -104,0 +104,11 @@\n+template <typename T>\n+inline const T* ZGranuleMap<T>::addr(zoffset offset) const {\n+  const size_t index = index_for_offset(offset);\n+  return _map + index;\n+}\n+\n+template <typename T>\n+inline T* ZGranuleMap<T>::addr(zoffset offset) {\n+  return const_cast<T*>(const_cast<const ZGranuleMap<T>*>(this)->addr(offset));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zGranuleMap.inline.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-    _serviceability(initial_capacity(), min_capacity(), max_capacity()),\n+    _serviceability(InitialHeapSize, min_capacity(), max_capacity()),\n@@ -97,4 +97,0 @@\n-size_t ZHeap::initial_capacity() const {\n-  return _page_allocator.initial_capacity();\n-}\n-\n@@ -243,1 +239,1 @@\n-  free_page(page, false \/* allow_defragment *\/);\n+  free_page(page);\n@@ -246,1 +242,1 @@\n-void ZHeap::free_page(ZPage* page, bool allow_defragment) {\n+void ZHeap::free_page(ZPage* page) {\n@@ -251,1 +247,1 @@\n-  _page_allocator.free_page(page, allow_defragment);\n+  _page_allocator.free_page(page);\n@@ -254,1 +250,1 @@\n-size_t ZHeap::free_empty_pages(const ZArray<ZPage*>* pages) {\n+size_t ZHeap::free_empty_pages(ZGenerationId id, const ZArray<ZPage*>* pages) {\n@@ -264,1 +260,1 @@\n-  _page_allocator.free_pages(pages);\n+  _page_allocator.free_pages(id, pages);\n@@ -322,4 +318,5 @@\n-  st->print_cr(\" ZHeap           used %zuM, capacity %zuM, max capacity %zuM\",\n-               used() \/ M,\n-               capacity() \/ M,\n-               max_capacity() \/ M);\n+  streamIndentor indentor(st, 1);\n+  _page_allocator.print_on(st);\n+\n+  \/\/ Metaspace printing prepends spaces instead of using outputStream indentation\n+  streamIndentor indentor_back(st, -1);\n@@ -330,1 +327,8 @@\n-  print_on(st);\n+  {\n+    streamIndentor indentor(st, 1);\n+    _page_allocator.print_on_error(st);\n+\n+    \/\/ Metaspace printing prepends spaces instead of using outputStream indentation\n+    streamIndentor indentor_back(st, -1);\n+    MetaspaceUtils::print_on(st);\n+  }\n@@ -337,0 +341,3 @@\n+  st->cr();\n+\n+  _page_allocator.print_extended_on_error(st);\n@@ -369,3 +376,6 @@\n-  ZPageTableIterator iter(&_page_table);\n-  for (ZPage* page; iter.next(&page);) {\n-    page->print_on(st);\n+  {\n+    streamIndentor indentor(st, 1);\n+    ZPageTableIterator iter(&_page_table);\n+    for (ZPage* page; iter.next(&page);) {\n+      page->print_on(st);\n+    }\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.cpp","additions":28,"deletions":18,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -70,1 +70,0 @@\n-  size_t initial_capacity() const;\n@@ -107,2 +106,2 @@\n-  void free_page(ZPage* page, bool allow_defragment);\n-  size_t free_empty_pages(const ZArray<ZPage*>* pages);\n+  void free_page(ZPage* page);\n+  size_t free_empty_pages(ZGenerationId id, const ZArray<ZPage*>* pages);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-  ZGlobalsPointers::initialize();\n@@ -62,0 +61,1 @@\n+  ZGlobalsPointers::initialize();\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,293 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZINTRUSIVERBTREE_HPP\n+#define SHARE_GC_Z_ZINTRUSIVERBTREE_HPP\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+enum class ZIntrusiveRBTreeDirection { LEFT, RIGHT };\n+\n+class ZIntrusiveRBTreeNode {\n+  template <typename Key, typename Compare>\n+  friend class ZIntrusiveRBTree;\n+\n+public:\n+  enum Color { RED = 0b0, BLACK = 0b1 };\n+\n+private:\n+  class ColoredNodePtr {\n+  private:\n+    static constexpr uintptr_t COLOR_MASK = 0b1;\n+    static constexpr uintptr_t NODE_MASK = ~COLOR_MASK;\n+\n+    uintptr_t _value;\n+\n+  public:\n+    ColoredNodePtr(ZIntrusiveRBTreeNode* node = nullptr, Color color = RED);\n+\n+    constexpr Color color() const;\n+    constexpr bool is_black() const;\n+    constexpr bool is_red() const;\n+\n+    ZIntrusiveRBTreeNode* node() const;\n+    ZIntrusiveRBTreeNode* red_node() const;\n+    ZIntrusiveRBTreeNode* black_node() const;\n+  };\n+\n+private:\n+  ColoredNodePtr _colored_parent;\n+  ZIntrusiveRBTreeNode* _left;\n+  ZIntrusiveRBTreeNode* _right;\n+\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  const ZIntrusiveRBTreeNode* find_next_node() const;\n+\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  const ZIntrusiveRBTreeNode* child() const;\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  ZIntrusiveRBTreeNode* child();\n+\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  ZIntrusiveRBTreeNode* const* child_addr() const;\n+\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  bool has_child() const;\n+\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  void update_child(ZIntrusiveRBTreeNode* new_child);\n+\n+  void link_node(ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode** insert_location);\n+\n+  void copy_parent_and_color(ZIntrusiveRBTreeNode* other);\n+  void update_parent_and_color(ZIntrusiveRBTreeNode* parent, Color color);\n+\n+  void update_parent(ZIntrusiveRBTreeNode* parent);\n+  void update_color(Color color);\n+\n+  void update_left_child(ZIntrusiveRBTreeNode* new_child);\n+  void update_right_child(ZIntrusiveRBTreeNode* new_child);\n+\n+  const ZIntrusiveRBTreeNode* parent() const;\n+  ZIntrusiveRBTreeNode* parent();\n+  const ZIntrusiveRBTreeNode* red_parent() const;\n+  ZIntrusiveRBTreeNode* red_parent();\n+  const ZIntrusiveRBTreeNode* black_parent() const;\n+  ZIntrusiveRBTreeNode* black_parent();\n+\n+  bool has_parent() const;\n+\n+  Color color() const;\n+  bool is_black() const;\n+  bool is_red() const;\n+  static bool is_black(ZIntrusiveRBTreeNode* node);\n+\n+  ZIntrusiveRBTreeNode* const* left_child_addr() const;\n+  ZIntrusiveRBTreeNode* const* right_child_addr() const;\n+\n+  const ZIntrusiveRBTreeNode* left_child() const;\n+  ZIntrusiveRBTreeNode* left_child();\n+  const ZIntrusiveRBTreeNode* right_child() const;\n+  ZIntrusiveRBTreeNode* right_child();\n+\n+  bool has_left_child() const;\n+  bool has_right_child() const;\n+\n+public:\n+  ZIntrusiveRBTreeNode();\n+\n+  const ZIntrusiveRBTreeNode* prev() const;\n+  ZIntrusiveRBTreeNode* prev();\n+  const ZIntrusiveRBTreeNode* next() const;\n+  ZIntrusiveRBTreeNode* next();\n+};\n+\n+template <typename Key, typename Compare>\n+class ZIntrusiveRBTree {\n+public:\n+  class FindCursor {\n+    friend class ZIntrusiveRBTree<Key, Compare>;\n+\n+  private:\n+    ZIntrusiveRBTreeNode** _insert_location;\n+    ZIntrusiveRBTreeNode* _parent;\n+    bool _left_most;\n+    bool _right_most;\n+    DEBUG_ONLY(uintptr_t _sequence_number;)\n+\n+    FindCursor(ZIntrusiveRBTreeNode** insert_location, ZIntrusiveRBTreeNode* parent, bool left_most, bool right_most DEBUG_ONLY(COMMA uintptr_t sequence_number));\n+    FindCursor();\n+\n+#ifdef ASSERT\n+    bool is_valid(uintptr_t sequence_number) const;\n+#endif\n+\n+  public:\n+    FindCursor(const FindCursor&) = default;\n+    FindCursor& operator=(const FindCursor&) = default;\n+\n+    bool is_valid() const;\n+    bool found() const;\n+    ZIntrusiveRBTreeNode* node() const;\n+    bool is_left_most() const;\n+    bool is_right_most() const;\n+    ZIntrusiveRBTreeNode* parent() const;\n+    ZIntrusiveRBTreeNode** insert_location() const;\n+  };\n+\n+private:\n+  ZIntrusiveRBTreeNode* _root_node;\n+  ZIntrusiveRBTreeNode* _left_most;\n+  ZIntrusiveRBTreeNode* _right_most;\n+  DEBUG_ONLY(uintptr_t _sequence_number;)\n+\n+  NONCOPYABLE(ZIntrusiveRBTree);\n+\n+#ifdef ASSERT\n+  template <bool swap_left_right>\n+  bool verify_node(ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode* left_child, ZIntrusiveRBTreeNode* right_child);\n+  template <bool swap_left_right>\n+  bool verify_node(ZIntrusiveRBTreeNode* parent);\n+  template <bool swap_left_right>\n+  bool verify_node(ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode* left_child);\n+  struct any_t {};\n+  template <bool swap_left_right>\n+  bool verify_node(ZIntrusiveRBTreeNode* parent, any_t, ZIntrusiveRBTreeNode* right_child);\n+#endif \/\/ ASSERT\n+\n+  ZIntrusiveRBTreeNode* const* root_node_addr() const;\n+\n+  void update_child_or_root(ZIntrusiveRBTreeNode* old_node, ZIntrusiveRBTreeNode* new_node, ZIntrusiveRBTreeNode* parent);\n+  void rotate_and_update_child_or_root(ZIntrusiveRBTreeNode* old_node, ZIntrusiveRBTreeNode* new_node, ZIntrusiveRBTreeNode::Color color);\n+\n+  template <ZIntrusiveRBTreeDirection PARENT_SIBLING_DIRECTION>\n+  void rebalance_insert_with_sibling(ZIntrusiveRBTreeNode* node, ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode* grand_parent);\n+  template <ZIntrusiveRBTreeDirection PARENT_SIBLING_DIRECTION>\n+  bool rebalance_insert_with_parent_sibling(ZIntrusiveRBTreeNode** node_addr, ZIntrusiveRBTreeNode** parent_addr, ZIntrusiveRBTreeNode* grand_parent);\n+  void rebalance_insert(ZIntrusiveRBTreeNode* new_node);\n+\n+  template <ZIntrusiveRBTreeDirection SIBLING_DIRECTION>\n+  bool rebalance_remove_with_sibling(ZIntrusiveRBTreeNode** node_addr, ZIntrusiveRBTreeNode** parent_addr);\n+  void rebalance_remove(ZIntrusiveRBTreeNode* rebalance_from);\n+\n+  FindCursor make_cursor(ZIntrusiveRBTreeNode* const* insert_location, ZIntrusiveRBTreeNode* parent, bool left_most, bool right_most) const;\n+  template <ZIntrusiveRBTreeDirection DIRECTION>\n+  FindCursor find_next(const FindCursor& cursor) const;\n+\n+public:\n+  ZIntrusiveRBTree();\n+\n+  ZIntrusiveRBTreeNode* first() const;\n+  ZIntrusiveRBTreeNode* last() const;\n+\n+  FindCursor root_cursor() const;\n+  FindCursor get_cursor(const ZIntrusiveRBTreeNode* node) const;\n+  FindCursor prev_cursor(const ZIntrusiveRBTreeNode* node) const;\n+  FindCursor next_cursor(const ZIntrusiveRBTreeNode* node) const;\n+  FindCursor prev(const FindCursor& cursor) const;\n+  FindCursor next(const FindCursor& cursor) const;\n+  FindCursor find(const Key& key) const;\n+\n+  void insert(ZIntrusiveRBTreeNode* new_node, const FindCursor& find_cursor);\n+  void replace(ZIntrusiveRBTreeNode* new_node, const FindCursor& find_cursor);\n+  void remove(const FindCursor& find_cursor);\n+\n+  void verify_tree();\n+\n+public:\n+  template <bool IsConst, bool Reverse>\n+  class IteratorImplementation;\n+\n+  using Iterator = IteratorImplementation<false, false>;\n+  using ConstIterator = IteratorImplementation<true, false>;\n+  using ReverseIterator = IteratorImplementation<false, true>;\n+  using ConstReverseIterator = IteratorImplementation<true, true>;\n+\n+  \/\/ remove and replace invalidate the iterators\n+  \/\/ however the iterators provide a remove and replace\n+  \/\/ function which does not invalidate that iterator nor\n+  \/\/ any end iterator\n+  Iterator begin();\n+  Iterator end();\n+  ConstIterator begin() const;\n+  ConstIterator end() const;\n+  ConstIterator cbegin() const;\n+  ConstIterator cend() const;\n+  ReverseIterator rbegin();\n+  ReverseIterator rend();\n+  ConstReverseIterator rbegin() const;\n+  ConstReverseIterator rend() const;\n+  ConstReverseIterator crbegin() const;\n+  ConstReverseIterator crend() const;\n+};\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+class ZIntrusiveRBTree<Key, Compare>::IteratorImplementation {\n+  friend IteratorImplementation<true, Reverse>;\n+\n+public:\n+  using difference_type   = std::ptrdiff_t;\n+  using value_type        = const ZIntrusiveRBTreeNode;\n+  using pointer           = value_type*;\n+  using reference         = value_type&;\n+\n+private:\n+  ZIntrusiveRBTree<Key, Compare>* _tree;\n+  const ZIntrusiveRBTreeNode* _node;\n+  bool _removed;\n+\n+  bool at_end() const;\n+\n+public:\n+  IteratorImplementation(ZIntrusiveRBTree<Key, Compare>& tree, pointer node);\n+  IteratorImplementation(const IteratorImplementation<IsConst, Reverse>&) = default;\n+  template <bool Enable = IsConst, ENABLE_IF(Enable)>\n+  IteratorImplementation(const IteratorImplementation<false, Reverse>& other);\n+\n+  reference operator*() const;\n+  pointer operator->();\n+  IteratorImplementation& operator--();\n+  IteratorImplementation operator--(int);\n+  IteratorImplementation& operator++();\n+  IteratorImplementation operator++(int);\n+\n+  template <bool Enable = !IsConst, ENABLE_IF(Enable)>\n+  void replace(ZIntrusiveRBTreeNode * new_node);\n+  template <bool Enable = !IsConst, ENABLE_IF(Enable)>\n+  void remove();\n+\n+  \/\/ Note: friend operator overloads defined inside class declaration because of problems with ADL\n+  friend bool operator==(const IteratorImplementation& a, const IteratorImplementation& b) {\n+    precond(a._tree == b._tree);\n+    return a._node == b._node;\n+  }\n+  friend bool operator!=(const IteratorImplementation& a, const IteratorImplementation& b) {\n+    precond(a._tree == b._tree);\n+    return a._node != b._node;\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZINTRUSIVERBTREE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zIntrusiveRBTree.hpp","additions":293,"deletions":0,"binary":false,"changes":293,"status":"added"},{"patch":"@@ -0,0 +1,1351 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZINTRUSIVERBTREE_INLINE_HPP\n+#define SHARE_GC_Z_ZINTRUSIVERBTREE_INLINE_HPP\n+\n+#include \"gc\/z\/zIntrusiveRBTree.hpp\"\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+static constexpr ZIntrusiveRBTreeDirection other(const ZIntrusiveRBTreeDirection& direction) {\n+  return direction == ZIntrusiveRBTreeDirection::LEFT ? ZIntrusiveRBTreeDirection::RIGHT : ZIntrusiveRBTreeDirection::LEFT;\n+}\n+\n+inline ZIntrusiveRBTreeNode::ColoredNodePtr::ColoredNodePtr(ZIntrusiveRBTreeNode* node, Color color)\n+  : _value(reinterpret_cast<uintptr_t>(node) | color) {}\n+\n+inline constexpr ZIntrusiveRBTreeNode::Color ZIntrusiveRBTreeNode::ColoredNodePtr::color() const {\n+  return static_cast<Color>(_value & COLOR_MASK);\n+}\n+\n+inline constexpr bool ZIntrusiveRBTreeNode::ColoredNodePtr::is_black() const {\n+  return color() == BLACK;\n+}\n+\n+inline constexpr bool ZIntrusiveRBTreeNode::ColoredNodePtr::is_red() const {\n+  return color() == RED;\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::ColoredNodePtr::node() const {\n+  return reinterpret_cast<ZIntrusiveRBTreeNode*>(_value & NODE_MASK);\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::ColoredNodePtr::red_node() const {\n+  precond(is_red());\n+  return reinterpret_cast<ZIntrusiveRBTreeNode*>(_value);\n+}\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::ColoredNodePtr::black_node() const {\n+  precond(is_black());\n+  return reinterpret_cast<ZIntrusiveRBTreeNode*>(_value ^ BLACK);\n+}\n+\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::find_next_node() const {\n+  constexpr ZIntrusiveRBTreeDirection OTHER_DIRECTION = other(DIRECTION);\n+  const ZIntrusiveRBTreeNode* node = this;\n+\n+  \/\/ Down the tree\n+  if (node->has_child<DIRECTION>()) {\n+    node = node->child<DIRECTION>();\n+    while (node->has_child<OTHER_DIRECTION>()) {\n+      node = node->child<OTHER_DIRECTION>();\n+    }\n+    return node;\n+  }\n+\n+  \/\/ Up the tree\n+  const ZIntrusiveRBTreeNode* parent = node->parent();\n+  while (parent != nullptr && node == parent->child<DIRECTION>()) {\n+    node = parent;\n+    parent = node->parent();\n+  }\n+  return parent;\n+}\n+\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::child() const {\n+  if (DIRECTION == ZIntrusiveRBTreeDirection::LEFT) {\n+    return _left;\n+  }\n+  assert(DIRECTION == ZIntrusiveRBTreeDirection::RIGHT, \"must be\");\n+  return _right;\n+}\n+\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::child() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->template child<DIRECTION>());\n+}\n+\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline ZIntrusiveRBTreeNode* const* ZIntrusiveRBTreeNode::child_addr() const {\n+  if (DIRECTION == ZIntrusiveRBTreeDirection::LEFT) {\n+    return &_left;\n+  }\n+  assert(DIRECTION == ZIntrusiveRBTreeDirection::RIGHT, \"must be\");\n+  return &_right;\n+}\n+\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline bool ZIntrusiveRBTreeNode::has_child() const {\n+  if (DIRECTION == ZIntrusiveRBTreeDirection::LEFT) {\n+    return _left != nullptr;\n+  }\n+  assert(DIRECTION == ZIntrusiveRBTreeDirection::RIGHT, \"must be\");\n+  return _right != nullptr;\n+}\n+\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline void ZIntrusiveRBTreeNode::update_child(ZIntrusiveRBTreeNode* new_child) {\n+  if (DIRECTION == ZIntrusiveRBTreeDirection::LEFT) {\n+    _left = new_child;\n+    return;\n+  }\n+  assert(DIRECTION == ZIntrusiveRBTreeDirection::RIGHT, \"must be\");\n+  _right = new_child;\n+}\n+\n+inline void ZIntrusiveRBTreeNode::link_node(ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode** insert_location) {\n+  \/\/ Newly linked node is always red\n+  _colored_parent = ColoredNodePtr(parent, RED);\n+  _left = nullptr;\n+  _right = nullptr;\n+\n+  \/\/ Link into location\n+  *insert_location = this;\n+}\n+\n+inline void ZIntrusiveRBTreeNode::copy_parent_and_color(ZIntrusiveRBTreeNode* other) {\n+  _colored_parent = other->_colored_parent;\n+}\n+\n+inline void ZIntrusiveRBTreeNode::update_parent_and_color(ZIntrusiveRBTreeNode* parent, Color color) {\n+  _colored_parent = ColoredNodePtr(parent, color);\n+}\n+\n+inline void ZIntrusiveRBTreeNode::update_parent(ZIntrusiveRBTreeNode* parent) {\n+  _colored_parent = ColoredNodePtr(parent, color());\n+}\n+\n+inline void ZIntrusiveRBTreeNode::update_color(Color color) {\n+  _colored_parent = ColoredNodePtr(parent(), color);\n+}\n+\n+inline void ZIntrusiveRBTreeNode::update_left_child(ZIntrusiveRBTreeNode* new_child) {\n+  update_child<ZIntrusiveRBTreeDirection::LEFT>(new_child);\n+}\n+\n+inline void ZIntrusiveRBTreeNode::update_right_child(ZIntrusiveRBTreeNode* new_child) {\n+  update_child<ZIntrusiveRBTreeDirection::RIGHT>(new_child);\n+}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::parent() const {\n+  return _colored_parent.node();\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::parent() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->parent());\n+}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::red_parent() const {\n+  return _colored_parent.red_node();\n+}\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::red_parent() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->red_parent());\n+}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::black_parent() const {\n+  return _colored_parent.black_node();\n+}\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::black_parent() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->black_parent());\n+}\n+\n+inline bool ZIntrusiveRBTreeNode::has_parent() const {\n+  return _colored_parent.node() != nullptr;\n+}\n+\n+inline ZIntrusiveRBTreeNode::Color ZIntrusiveRBTreeNode::color() const {\n+  return _colored_parent.color();\n+}\n+\n+inline bool ZIntrusiveRBTreeNode::is_black() const {\n+  return _colored_parent.is_black();\n+}\n+\n+inline bool ZIntrusiveRBTreeNode::is_red() const {\n+  return _colored_parent.is_red();\n+}\n+\n+inline bool ZIntrusiveRBTreeNode::is_black(ZIntrusiveRBTreeNode* node) {\n+  return node == nullptr || node->is_black();\n+}\n+\n+inline ZIntrusiveRBTreeNode* const* ZIntrusiveRBTreeNode::left_child_addr() const {\n+  return child_addr<ZIntrusiveRBTreeDirection::LEFT>();\n+}\n+\n+inline ZIntrusiveRBTreeNode* const* ZIntrusiveRBTreeNode::right_child_addr() const {\n+  return child_addr<ZIntrusiveRBTreeDirection::RIGHT>();\n+}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::left_child() const {\n+  return child<ZIntrusiveRBTreeDirection::LEFT>();\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::left_child() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->left_child());\n+}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::right_child() const {\n+  return child<ZIntrusiveRBTreeDirection::RIGHT>();\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::right_child() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->right_child());\n+}\n+\n+inline bool ZIntrusiveRBTreeNode::has_left_child() const {\n+  return has_child<ZIntrusiveRBTreeDirection::LEFT>();\n+}\n+\n+inline bool ZIntrusiveRBTreeNode::has_right_child() const {\n+  return has_child<ZIntrusiveRBTreeDirection::RIGHT>();\n+}\n+\n+inline ZIntrusiveRBTreeNode::ZIntrusiveRBTreeNode() {}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::prev() const {\n+  return find_next_node<ZIntrusiveRBTreeDirection::LEFT>();\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::prev() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->prev());\n+}\n+\n+inline const ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::next() const {\n+  return find_next_node<ZIntrusiveRBTreeDirection::RIGHT>();\n+}\n+\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTreeNode::next() {\n+  return const_cast<ZIntrusiveRBTreeNode*>(const_cast<const ZIntrusiveRBTreeNode*>(this)->next());\n+}\n+\n+#ifdef ASSERT\n+template <typename Key, typename Compare>\n+template <bool swap_left_right>\n+inline bool ZIntrusiveRBTree<Key, Compare>::verify_node(ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode* left_child, ZIntrusiveRBTreeNode* right_child) {\n+  if (swap_left_right) {\n+    ::swap(left_child, right_child);\n+  }\n+  assert(parent->left_child() == left_child, swap_left_right ? \"Bad child Swapped\" : \"Bad child\");\n+  assert(parent->right_child() == right_child, swap_left_right ? \"Bad child Swapped\" : \"Bad child\");\n+  if (left_child != nullptr) {\n+    assert(left_child->parent() == parent, swap_left_right ? \"Bad parent Swapped\" : \"Bad parent\");\n+  }\n+  if (right_child != nullptr) {\n+    assert(right_child->parent() == parent, swap_left_right ? \"Bad parent Swapped\" : \"Bad parent\");\n+  }\n+  return true;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool swap_left_right>\n+inline bool ZIntrusiveRBTree<Key, Compare>::verify_node(ZIntrusiveRBTreeNode* parent) {\n+  if (parent == nullptr) {\n+    return true;\n+  }\n+  if (swap_left_right) {\n+    return verify_node<swap_left_right>(parent, parent->right_child());\n+  }\n+  return verify_node<swap_left_right>(parent, parent->left_child());\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool swap_left_right>\n+inline bool ZIntrusiveRBTree<Key, Compare>::verify_node(ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode* left_child) {\n+  if (swap_left_right) {\n+    return verify_node<swap_left_right>(parent, left_child, parent->left_child());\n+  }\n+  return verify_node<swap_left_right>(parent, left_child, parent->right_child());\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool swap_left_right>\n+inline bool ZIntrusiveRBTree<Key, Compare>::verify_node(ZIntrusiveRBTreeNode* parent, any_t, ZIntrusiveRBTreeNode* right_child) {\n+  if (swap_left_right) {\n+    return verify_node<swap_left_right>(parent, parent->right_child(), right_child);\n+  }\n+  return verify_node<swap_left_right>(parent, parent->left_child(), right_child);\n+}\n+#endif \/\/ ASSERT\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTreeNode* const* ZIntrusiveRBTree<Key, Compare>::root_node_addr() const {\n+  return &_root_node;\n+}\n+\n+template <typename Key, typename Compare>\n+void ZIntrusiveRBTree<Key, Compare>::update_child_or_root(ZIntrusiveRBTreeNode* old_node, ZIntrusiveRBTreeNode* new_node, ZIntrusiveRBTreeNode* parent) {\n+  if (parent == nullptr) {\n+    \/\/ Update root\n+    _root_node = new_node;\n+    return;\n+  }\n+  if (old_node == parent->left_child()) {\n+    parent->update_left_child(new_node);\n+    return;\n+  }\n+  assert(old_node == parent->right_child(), \"must be\");\n+  parent->update_right_child(new_node);\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::rotate_and_update_child_or_root(ZIntrusiveRBTreeNode* old_node, ZIntrusiveRBTreeNode* new_node, ZIntrusiveRBTreeNode::Color color) {\n+  ZIntrusiveRBTreeNode* const parent = old_node->parent();\n+  new_node->copy_parent_and_color(old_node);\n+  old_node->update_parent_and_color(new_node, color);\n+  update_child_or_root(old_node, new_node, parent);\n+}\n+\n+template <typename Key, typename Compare>\n+template <ZIntrusiveRBTreeDirection PARENT_SIBLING_DIRECTION>\n+inline void ZIntrusiveRBTree<Key, Compare>::rebalance_insert_with_sibling(ZIntrusiveRBTreeNode* node, ZIntrusiveRBTreeNode* parent, ZIntrusiveRBTreeNode* grand_parent) {\n+  DEBUG_ONLY(const bool swap_left_right = PARENT_SIBLING_DIRECTION == ZIntrusiveRBTreeDirection::LEFT;)\n+  constexpr ZIntrusiveRBTreeDirection OTHER_DIRECTION = other(PARENT_SIBLING_DIRECTION);\n+  ZIntrusiveRBTreeNode* sibling = parent->template child<PARENT_SIBLING_DIRECTION>();\n+  DEBUG_ONLY(bool rotated_parent = false;)\n+  if (node == sibling) {\n+    DEBUG_ONLY(rotated_parent = true;)\n+    \/\/ Rotate up node through parent\n+    ZIntrusiveRBTreeNode* child = node->template child<OTHER_DIRECTION>();\n+\n+    \/\/\/\/ PRE\n+    \/\/\n+    \/\/      G          G\n+    \/\/     \/            \\\n+    \/\/    p      or      p\n+    \/\/     \\            \/\n+    \/\/      n          n\n+    \/\/     \/            \\\n+    \/\/   (c)            (c)\n+    \/\/\n+    \/\/\/\/\n+    precond(grand_parent->is_black());\n+    precond(parent->is_red());\n+    precond(node->is_red());\n+    precond(verify_node<swap_left_right>(grand_parent, parent));\n+    precond(verify_node<swap_left_right>(parent, any_t{}, node));\n+    precond(verify_node<swap_left_right>(node, child));\n+    precond(verify_node<swap_left_right>(child));\n+\n+    \/\/ Fix children\n+    parent->template update_child<PARENT_SIBLING_DIRECTION>(child);\n+    node->template update_child<OTHER_DIRECTION>(parent);\n+\n+    \/\/ Fix parents and colors\n+    if (child != nullptr) {\n+      child->update_parent_and_color(parent, ZIntrusiveRBTreeNode::BLACK);\n+    }\n+    parent->update_parent_and_color(node, ZIntrusiveRBTreeNode::RED);\n+\n+    \/\/\/\/ POST\n+    \/\/\n+    \/\/        G          G\n+    \/\/       \/            \\\n+    \/\/      n      or      n\n+    \/\/     \/                \\\n+    \/\/    p                  p\n+    \/\/     \\                \/\n+    \/\/     (C)            (C)\n+    \/\/\n+    \/\/\/\/\n+    postcond(grand_parent->is_black());\n+    postcond(parent->is_red());\n+    postcond(node->is_red());\n+    postcond(ZIntrusiveRBTreeNode::is_black(child));\n+    \/\/ The grand_parent is updated in the next rotation\n+    \/\/ postcond(verify_node<swap_left_right>(grand_parent, node));\n+    postcond(verify_node<swap_left_right>(node, parent));\n+    postcond(verify_node<swap_left_right>(parent, any_t{}, child));\n+    postcond(verify_node<swap_left_right>(child));\n+\n+    parent = node;\n+    sibling = parent->template child<PARENT_SIBLING_DIRECTION>();\n+    DEBUG_ONLY(node = parent->template child<OTHER_DIRECTION>();)\n+  }\n+\n+  \/\/\/\/ PRE\n+  \/\/\n+  \/\/        G        G\n+  \/\/       \/          \\\n+  \/\/      p     or     p\n+  \/\/     \/ \\          \/ \\\n+  \/\/    n  (s)      (s)  n\n+  \/\/\n+  \/\/\/\/\n+  precond(grand_parent->is_black());\n+  precond(parent->is_red());\n+  precond(node->is_red());\n+  precond(rotated_parent || verify_node<swap_left_right>(grand_parent, parent));\n+  precond(verify_node<swap_left_right>(parent, node, sibling));\n+  precond(verify_node<swap_left_right>(node));\n+  precond(verify_node<swap_left_right>(sibling));\n+\n+  \/\/ Rotate up parent through grand-parent\n+\n+  \/\/ Fix children\n+  grand_parent->template update_child<OTHER_DIRECTION>(sibling);\n+  parent->template update_child<PARENT_SIBLING_DIRECTION>(grand_parent);\n+\n+  \/\/ Fix parents and colors\n+  if (sibling != nullptr) {\n+    sibling->update_parent_and_color(grand_parent, ZIntrusiveRBTreeNode::BLACK);\n+  }\n+  rotate_and_update_child_or_root(grand_parent, parent, ZIntrusiveRBTreeNode::RED);\n+\n+  \/\/\/\/ POST\n+  \/\/\n+  \/\/      P          P\n+  \/\/     \/ \\        \/ \\\n+  \/\/    n   g  or  g   n\n+  \/\/       \/        \\\n+  \/\/     (S)        (S)\n+  \/\/\n+  \/\/\/\/\n+  postcond(parent->is_black());\n+  postcond(grand_parent->is_red());\n+  postcond(node->is_red());\n+  postcond(ZIntrusiveRBTreeNode::is_black(sibling));\n+  postcond(verify_node<swap_left_right>(parent, node, grand_parent));\n+  postcond(verify_node<swap_left_right>(node));\n+  postcond(verify_node<swap_left_right>(grand_parent, sibling));\n+  postcond(verify_node<swap_left_right>(sibling));\n+}\n+\n+template <typename Key, typename Compare>\n+template <ZIntrusiveRBTreeDirection PARENT_SIBLING_DIRECTION>\n+inline bool ZIntrusiveRBTree<Key, Compare>::rebalance_insert_with_parent_sibling(ZIntrusiveRBTreeNode** node_addr, ZIntrusiveRBTreeNode** parent_addr, ZIntrusiveRBTreeNode* grand_parent) {\n+  DEBUG_ONLY(const bool swap_left_right = PARENT_SIBLING_DIRECTION == ZIntrusiveRBTreeDirection::LEFT;)\n+  constexpr ZIntrusiveRBTreeDirection OTHER_DIRECTION = other(PARENT_SIBLING_DIRECTION);\n+  ZIntrusiveRBTreeNode* const parent_sibling = grand_parent->template child<PARENT_SIBLING_DIRECTION>();\n+  ZIntrusiveRBTreeNode*& node = *node_addr;\n+  ZIntrusiveRBTreeNode*& parent = *parent_addr;\n+  if (parent_sibling != nullptr && parent_sibling->is_red()) {\n+    \/\/\/\/ PRE\n+    \/\/\n+    \/\/       G          G\n+    \/\/      \/ \\        \/ \\\n+    \/\/     p   u  or  u   p\n+    \/\/    \/ \\            \/ \\\n+    \/\/   n | n          n | n\n+    \/\/\n+    \/\/\/\/\n+    precond(grand_parent->is_black());\n+    precond(parent_sibling->is_red());\n+    precond(parent->is_red());\n+    precond(node->is_red());\n+    precond(verify_node<swap_left_right>(grand_parent, parent, parent_sibling));\n+    precond(parent->left_child() == node || parent->right_child() == node);\n+    precond(verify_node<swap_left_right>(parent));\n+    precond(verify_node<swap_left_right>(parent_sibling));\n+    precond(verify_node<swap_left_right>(node));\n+\n+    \/\/ Flip colors of parent, parent sibling and grand parent\n+    parent_sibling->update_parent_and_color(grand_parent, ZIntrusiveRBTreeNode::BLACK);\n+    parent->update_parent_and_color(grand_parent, ZIntrusiveRBTreeNode::BLACK);\n+    ZIntrusiveRBTreeNode* grand_grand_parent = grand_parent->black_parent();\n+    grand_parent->update_parent_and_color(grand_grand_parent, ZIntrusiveRBTreeNode::RED);\n+\n+    \/\/\/\/ POST\n+    \/\/\n+    \/\/       g          g\n+    \/\/      \/ \\        \/ \\\n+    \/\/     P   U  or  U   P\n+    \/\/    \/ \\            \/ \\\n+    \/\/   n | n          n | n\n+    \/\/\n+    \/\/\/\/\n+    postcond(grand_parent->is_red());\n+    postcond(parent_sibling->is_black());\n+    postcond(parent->is_black());\n+    postcond(node->is_red());\n+    postcond(verify_node<swap_left_right>(grand_parent, parent, parent_sibling));\n+    postcond(parent->left_child() == node || parent->right_child() == node);\n+    postcond(verify_node<swap_left_right>(parent));\n+    postcond(verify_node<swap_left_right>(parent_sibling));\n+    postcond(verify_node<swap_left_right>(node));\n+\n+    \/\/ Recurse up the tree\n+    node = grand_parent;\n+    parent = grand_grand_parent;\n+    return false; \/\/ Not finished\n+  }\n+\n+  rebalance_insert_with_sibling<PARENT_SIBLING_DIRECTION>(node, parent, grand_parent);\n+  return true; \/\/ Finished\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::rebalance_insert(ZIntrusiveRBTreeNode* new_node) {\n+  ZIntrusiveRBTreeNode* node = new_node;\n+  ZIntrusiveRBTreeNode* parent = node->red_parent();\n+  for (;;) {\n+    precond(node->is_red());\n+    if (parent == nullptr) {\n+      \/\/ Recursive (or root) case\n+      node->update_parent_and_color(parent, ZIntrusiveRBTreeNode::BLACK);\n+      break;\n+    }\n+    if (parent->is_black()) {\n+      \/\/ Tree is balanced\n+      break;\n+    }\n+    ZIntrusiveRBTreeNode* grand_parent = parent->red_parent();\n+    if (parent == grand_parent->left_child() ? rebalance_insert_with_parent_sibling<ZIntrusiveRBTreeDirection::RIGHT>(&node, &parent, grand_parent)\n+                                            : rebalance_insert_with_parent_sibling<ZIntrusiveRBTreeDirection::LEFT>(&node, &parent, grand_parent)) {\n+      break;\n+    }\n+  }\n+}\n+\n+template <typename Key, typename Compare>\n+template <ZIntrusiveRBTreeDirection SIBLING_DIRECTION>\n+inline bool ZIntrusiveRBTree<Key, Compare>::rebalance_remove_with_sibling(ZIntrusiveRBTreeNode** node_addr, ZIntrusiveRBTreeNode** parent_addr) {\n+  DEBUG_ONLY(const bool swap_left_right = SIBLING_DIRECTION == ZIntrusiveRBTreeDirection::LEFT;)\n+  constexpr ZIntrusiveRBTreeDirection OTHER_DIRECTION = other(SIBLING_DIRECTION);\n+  ZIntrusiveRBTreeNode*& node = *node_addr;\n+  ZIntrusiveRBTreeNode*& parent = *parent_addr;\n+  ZIntrusiveRBTreeNode* sibling = parent->template child<SIBLING_DIRECTION>();\n+  if (sibling->is_red()) {\n+    ZIntrusiveRBTreeNode* sibling_child = sibling->template child<OTHER_DIRECTION>();\n+    \/\/\/\/ PRE\n+    \/\/\n+    \/\/     P          P\n+    \/\/    \/ \\        \/ \\\n+    \/\/   N   s  or  s   N\n+    \/\/      \/        \\\n+    \/\/     SC        SC\n+    \/\/\n+    \/\/\/\/\n+    precond(parent->is_black());\n+    precond(ZIntrusiveRBTreeNode::is_black(node));\n+    precond(sibling->is_red());\n+    precond(ZIntrusiveRBTreeNode::is_black(sibling_child));\n+    precond(verify_node<swap_left_right>(parent, node, sibling));\n+    precond(verify_node<swap_left_right>(node));\n+    precond(verify_node<swap_left_right>(sibling, sibling_child));\n+    precond(verify_node<swap_left_right>(sibling_child));\n+\n+    \/\/ Rotate sibling up through parent\n+\n+    \/\/ Fix children\n+    parent->template update_child<SIBLING_DIRECTION>(sibling_child);\n+    sibling->template update_child<OTHER_DIRECTION>(parent);\n+\n+    \/\/ Fix parents and colors\n+    sibling_child->update_parent_and_color(parent, ZIntrusiveRBTreeNode::BLACK);\n+    rotate_and_update_child_or_root(parent, sibling, ZIntrusiveRBTreeNode::RED);\n+\n+    \/\/\/\/ POST\n+    \/\/\n+    \/\/       S         S\n+    \/\/      \/           \\\n+    \/\/     p             p\n+    \/\/    \/ \\           \/ \\\n+    \/\/   N   SC        SC  N\n+    \/\/\n+    \/\/\/\/\n+    postcond(sibling->is_black());\n+    postcond(parent->is_red());\n+    postcond(ZIntrusiveRBTreeNode::is_black(node));\n+    postcond(ZIntrusiveRBTreeNode::is_black(sibling_child));\n+    postcond(verify_node<swap_left_right>(sibling, parent));\n+    postcond(verify_node<swap_left_right>(parent, node, sibling_child));\n+    postcond(verify_node<swap_left_right>(node));\n+    postcond(verify_node<swap_left_right>(sibling_child));\n+\n+    \/\/ node has a new sibling\n+    sibling = sibling_child;\n+  }\n+\n+  ZIntrusiveRBTreeNode* sibling_child = sibling->template child<SIBLING_DIRECTION>();\n+  DEBUG_ONLY(bool rotated_parent = false;)\n+  if (ZIntrusiveRBTreeNode::is_black(sibling_child)) {\n+    DEBUG_ONLY(rotated_parent = true;)\n+    ZIntrusiveRBTreeNode* sibling_other_child = sibling->template child<OTHER_DIRECTION>();\n+    if (ZIntrusiveRBTreeNode::is_black(sibling_other_child)) {\n+      \/\/\/\/ PRE\n+      \/\/\n+      \/\/    (p)        (p)\n+      \/\/    \/ \\        \/ \\\n+      \/\/   N   S  or  S   N\n+      \/\/\n+      \/\/\/\/\n+      precond(ZIntrusiveRBTreeNode::is_black(node));\n+      precond(sibling->is_black());\n+      precond(verify_node<swap_left_right>(parent, node, sibling));\n+\n+      \/\/ Flip sibling color to RED\n+      sibling->update_parent_and_color(parent, ZIntrusiveRBTreeNode::RED);\n+\n+      \/\/\/\/ POST\n+      \/\/\n+      \/\/    (p)        (p)\n+      \/\/    \/ \\        \/ \\\n+      \/\/   N   s  or  s   N\n+      \/\/\n+      \/\/\/\/\n+      postcond(ZIntrusiveRBTreeNode::is_black(node));\n+      postcond(sibling->is_red());\n+      postcond(verify_node<swap_left_right>(parent, node, sibling));\n+\n+      if (parent->is_black()) {\n+        \/\/ We did not introduce a RED-RED edge, if parent is\n+        \/\/ the root we are done, else recurse up the tree\n+        if (parent->parent() != nullptr) {\n+          node = parent;\n+          parent = node->parent();\n+          return false;\n+        }\n+        return true;\n+      }\n+      \/\/ Change RED-RED edge to BLACK-RED edge\n+      parent->update_color(ZIntrusiveRBTreeNode::BLACK);\n+      return true;\n+    }\n+\n+    ZIntrusiveRBTreeNode* sibling_grand_child = sibling_other_child->template child<SIBLING_DIRECTION>();\n+    \/\/\/\/ PRE\n+    \/\/\n+    \/\/    (p)          (p)\n+    \/\/    \/ \\          \/ \\\n+    \/\/   N   S        S   N\n+    \/\/      \/     or   \\\n+    \/\/    soc          soc\n+    \/\/      \\          \/\n+    \/\/     (sgc)     (sgc)\n+    \/\/\n+    \/\/\/\/\n+    precond(ZIntrusiveRBTreeNode::is_black(node));\n+    precond(sibling->is_black());\n+    precond(sibling_other_child->is_red());\n+    precond(verify_node<swap_left_right>(parent, node, sibling));\n+    precond(verify_node<swap_left_right>(node));\n+    precond(verify_node<swap_left_right>(sibling, sibling_other_child, sibling_child));\n+    precond(verify_node<swap_left_right>(sibling_other_child, any_t{}, sibling_grand_child));\n+    precond(verify_node<swap_left_right>(sibling_grand_child));\n+\n+    \/\/ Rotate sibling other child through the sibling\n+\n+    \/\/ Fix children\n+    sibling->template update_child<OTHER_DIRECTION>(sibling_grand_child);\n+    sibling_other_child->template update_child<SIBLING_DIRECTION>(sibling);\n+    parent->template update_child<SIBLING_DIRECTION>(sibling_other_child);\n+\n+    \/\/ Fix parents and colors\n+    if (sibling_grand_child != nullptr) {\n+      sibling_grand_child->update_parent_and_color(sibling, ZIntrusiveRBTreeNode::BLACK);\n+    }\n+    \/\/ Defer updating the sibling and sibling other child parents until\n+    \/\/ after we rotate below. This will also fix the any potential RED-RED\n+    \/\/ edge between parent and sibling_other_child\n+\n+    \/\/\/\/ POST\n+    \/\/\n+    \/\/    (p)            (p)\n+    \/\/    \/ \\            \/ \\\n+    \/\/   N  soc   or   soc  N\n+    \/\/      \/ \\        \/ \\\n+    \/\/    SGC  S      S  SGC\n+    \/\/\n+    \/\/\/\/\n+    postcond(ZIntrusiveRBTreeNode::is_black(node));\n+    postcond(sibling->is_black());\n+    postcond(sibling_other_child->is_red());\n+    postcond(ZIntrusiveRBTreeNode::is_black(sibling_grand_child));\n+    \/\/ Deferred\n+    \/\/ postcond(verify_node<swap_left_right>(parent, node, sibling_other_child));\n+    postcond(verify_node<swap_left_right>(node));\n+    \/\/ postcond(verify_node<swap_left_right>(sibling_other_child, sibling_grand_child, sibling));\n+    postcond(verify_node<swap_left_right>(sibling_grand_child));\n+    postcond(verify_node<swap_left_right>(sibling));\n+\n+    \/\/ node has a new sibling\n+    sibling_child = sibling;\n+    sibling = sibling_other_child;\n+  }\n+\n+  ZIntrusiveRBTreeNode* sibling_other_child = sibling->template child<OTHER_DIRECTION>();\n+  \/\/\/\/ PRE\n+  \/\/\n+  \/\/    (p)              (p)\n+  \/\/    \/ \\              \/ \\\n+  \/\/   N   S     or     S   N\n+  \/\/      \/ \\          \/ \\\n+  \/\/   (soc)(sc)    (sc)(soc)\n+  \/\/\n+  \/\/\/\/\n+  DEBUG_ONLY(ZIntrusiveRBTreeNode::Color parent_color = parent->color();)\n+  precond(ZIntrusiveRBTreeNode::is_black(node));\n+  precond(rotated_parent || sibling->is_black());\n+  DEBUG_ONLY(bool sibling_other_child_is_black = ZIntrusiveRBTreeNode::is_black(sibling_other_child);)\n+  precond(rotated_parent || verify_node<swap_left_right>(parent, node, sibling));\n+  precond(verify_node<swap_left_right>(node));\n+  precond(rotated_parent || verify_node<swap_left_right>(sibling, sibling_other_child, sibling_child));\n+  postcond(verify_node<swap_left_right>(sibling_other_child));\n+  postcond(verify_node<swap_left_right>(sibling_child));\n+\n+  \/\/ Rotate sibling through parent and fix colors\n+\n+  \/\/ Fix children\n+  parent->template update_child<SIBLING_DIRECTION>(sibling_other_child);\n+  sibling->template update_child<OTHER_DIRECTION>(parent);\n+\n+  \/\/ Fix parents and colors\n+  sibling_child->update_parent_and_color(sibling, ZIntrusiveRBTreeNode::BLACK);\n+  if (sibling_other_child != nullptr) {\n+    sibling_other_child->update_parent(parent);\n+  }\n+  rotate_and_update_child_or_root(parent, sibling, ZIntrusiveRBTreeNode::BLACK);\n+\n+  \/\/\/\/ POST\n+  \/\/\n+  \/\/      (s)           (s)\n+  \/\/      \/ \\           \/ \\\n+  \/\/     P   SC  or    SC  P\n+  \/\/    \/ \\               \/ \\\n+  \/\/   N (soc)         (soc) N\n+  \/\/\n+  \/\/\/\/\n+  postcond(sibling->color() == parent_color);\n+  postcond(parent->is_black());\n+  postcond(sibling_child->is_black());\n+  postcond(ZIntrusiveRBTreeNode::is_black(node));\n+  postcond(sibling_other_child_is_black == ZIntrusiveRBTreeNode::is_black(sibling_other_child));\n+  postcond(verify_node<swap_left_right>(sibling, parent, sibling_child));\n+  postcond(verify_node<swap_left_right>(parent, node, sibling_other_child));\n+  postcond(verify_node<swap_left_right>(sibling_child));\n+  postcond(verify_node<swap_left_right>(node));\n+  postcond(verify_node<swap_left_right>(sibling_other_child));\n+  return true;\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::rebalance_remove(ZIntrusiveRBTreeNode* rebalance_from) {\n+  ZIntrusiveRBTreeNode* node = nullptr;\n+  ZIntrusiveRBTreeNode* parent = rebalance_from;\n+\n+  for (;;) {\n+    precond(ZIntrusiveRBTreeNode::is_black(node));\n+    precond(parent != nullptr);\n+    if (node == parent->left_child() ? rebalance_remove_with_sibling<ZIntrusiveRBTreeDirection::RIGHT>(&node, &parent)\n+                                    : rebalance_remove_with_sibling<ZIntrusiveRBTreeDirection::LEFT>(&node, &parent)) {\n+      break;\n+    }\n+  }\n+}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTree<Key, Compare>::FindCursor::FindCursor(ZIntrusiveRBTreeNode** insert_location, ZIntrusiveRBTreeNode* parent, bool left_most, bool right_most DEBUG_ONLY(COMMA uintptr_t sequence_number))\n+  : _insert_location(insert_location),\n+    _parent(parent),\n+    _left_most(left_most),\n+    _right_most(right_most)\n+    DEBUG_ONLY(COMMA _sequence_number(sequence_number)) {}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTree<Key, Compare>::FindCursor::FindCursor()\n+  : _insert_location(nullptr),\n+    _parent(nullptr),\n+    _left_most(),\n+    _right_most()\n+    DEBUG_ONLY(COMMA _sequence_number()) {}\n+\n+#ifdef ASSERT\n+template <typename Key, typename Compare>\n+inline bool ZIntrusiveRBTree<Key, Compare>::FindCursor::is_valid(uintptr_t sequence_number) const {\n+  return is_valid() && _sequence_number == sequence_number;\n+}\n+#endif \/\/ ASSERT\n+\n+template <typename Key, typename Compare>\n+inline bool ZIntrusiveRBTree<Key, Compare>::FindCursor::is_valid() const {\n+  return insert_location() != nullptr;\n+}\n+\n+template <typename Key, typename Compare>\n+inline bool ZIntrusiveRBTree<Key, Compare>::FindCursor::found() const {\n+  return node() != nullptr;\n+}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTree<Key, Compare>::FindCursor::node() const {\n+  precond(is_valid());\n+  return *_insert_location == nullptr ? nullptr : *_insert_location;\n+}\n+\n+template <typename Key, typename Compare>\n+inline bool ZIntrusiveRBTree<Key, Compare>::FindCursor::is_left_most() const {\n+  precond(is_valid());\n+  return _left_most;\n+}\n+\n+template <typename Key, typename Compare>\n+inline bool ZIntrusiveRBTree<Key, Compare>::FindCursor::is_right_most() const {\n+  precond(is_valid());\n+  return _right_most;\n+}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTree<Key, Compare>::FindCursor::parent() const {\n+  precond(is_valid());\n+  return _parent;\n+}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTreeNode** ZIntrusiveRBTree<Key, Compare>::FindCursor::insert_location() const {\n+  return _insert_location;\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::make_cursor(ZIntrusiveRBTreeNode* const* insert_location, ZIntrusiveRBTreeNode* parent, bool left_most, bool right_most) const {\n+  return FindCursor(const_cast<ZIntrusiveRBTreeNode**>(insert_location), parent, left_most, right_most DEBUG_ONLY(COMMA _sequence_number));\n+}\n+\n+template <typename Key, typename Compare>\n+template <ZIntrusiveRBTreeDirection DIRECTION>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::find_next(const FindCursor& cursor) const {\n+  constexpr ZIntrusiveRBTreeDirection OTHER_DIRECTION = other(DIRECTION);\n+  if (cursor.found()) {\n+    ZIntrusiveRBTreeNode* const node = cursor.node();\n+    const ZIntrusiveRBTreeNode* const next_node = node->template find_next_node<DIRECTION>();\n+    if (next_node != nullptr) {\n+      return get_cursor(next_node);\n+    }\n+    const bool is_right_most = DIRECTION == ZIntrusiveRBTreeDirection::RIGHT && node == _right_most;\n+    const bool is_left_most = DIRECTION == ZIntrusiveRBTreeDirection::LEFT && node == _left_most;\n+    return make_cursor(node->template child_addr<DIRECTION>(), node, is_left_most, is_right_most);\n+  }\n+  ZIntrusiveRBTreeNode* const parent = cursor.parent();\n+  if (parent == nullptr) {\n+    assert(&_root_node == cursor.insert_location(), \"must be\");\n+    \/\/ tree is empty\n+    return FindCursor();\n+  }\n+  if (parent->template child_addr<OTHER_DIRECTION>() == cursor.insert_location()) {\n+    \/\/ Cursor at leaf in other direction, parent is next in direction\n+    return get_cursor(parent);\n+  }\n+  assert(parent->template child_addr<DIRECTION>() == cursor.insert_location(), \"must be\");\n+  \/\/ Cursor at leaf in direction, parent->next in direction is also cursors next in direction\n+  return get_cursor(parent->template find_next_node<DIRECTION>());\n+}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTree<Key, Compare>::ZIntrusiveRBTree()\n+  : _root_node(nullptr),\n+    _left_most(nullptr),\n+    _right_most(nullptr)\n+    DEBUG_ONLY(COMMA _sequence_number()) {}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTree<Key, Compare>::first() const {\n+  return _left_most;\n+}\n+\n+template <typename Key, typename Compare>\n+inline ZIntrusiveRBTreeNode* ZIntrusiveRBTree<Key, Compare>::last() const {\n+  return _right_most;\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::root_cursor() const {\n+  const bool is_left_most = _root_node == _left_most;\n+  const bool is_right_most = _root_node == _right_most;\n+  return make_cursor(&_root_node, nullptr, is_left_most, is_right_most);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::get_cursor(const ZIntrusiveRBTreeNode* node) const {\n+  if (node == nullptr) {\n+    \/\/ Return a invalid cursor\n+    return FindCursor();\n+  }\n+  const bool is_left_most = node == _left_most;\n+  const bool is_right_most = node == _right_most;\n+  if (node->has_parent()) {\n+    const ZIntrusiveRBTreeNode* const parent = node->parent();\n+    if (parent->left_child() == node) {\n+      return make_cursor(parent->left_child_addr(), nullptr, is_left_most, is_right_most);\n+    }\n+    assert(parent->right_child() == node, \"must be\");\n+      return make_cursor(parent->right_child_addr(), nullptr, is_left_most, is_right_most);\n+  }\n+  \/\/ No parent, root node\n+  return make_cursor(&_root_node, nullptr, is_left_most, is_right_most);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::prev_cursor(const ZIntrusiveRBTreeNode* node) const {\n+  return prev(get_cursor(node));\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::next_cursor(const ZIntrusiveRBTreeNode* node) const {\n+  return next(get_cursor(node));\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::prev(const FindCursor& cursor) const {\n+  return find_next<ZIntrusiveRBTreeDirection::LEFT>(cursor);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::next(const FindCursor& cursor) const {\n+  return find_next<ZIntrusiveRBTreeDirection::RIGHT>(cursor);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::FindCursor ZIntrusiveRBTree<Key, Compare>::find(const Key& key) const {\n+  Compare compare_fn;\n+  ZIntrusiveRBTreeNode* const* insert_location = root_node_addr();\n+  ZIntrusiveRBTreeNode* parent = nullptr;\n+  bool left_most = true;\n+  bool right_most = true;\n+  while (*insert_location != nullptr) {\n+    int result = compare_fn(key, *insert_location);\n+    if (result == 0) {\n+      assert(*insert_location != _left_most || left_most, \"must be\");\n+      assert(*insert_location != _right_most || right_most, \"must be\");\n+      return make_cursor(insert_location, parent, *insert_location == _left_most, *insert_location == _right_most);\n+    }\n+    parent = *insert_location;\n+    if (result < 0) {\n+      insert_location = parent->left_child_addr();\n+      \/\/ We took one step to the left, cannot be right_most.\n+      right_most = false;\n+    } else {\n+      insert_location = parent->right_child_addr();\n+      \/\/ We took one step to the right, cannot be left_most.\n+      left_most = false;\n+    }\n+  }\n+  return make_cursor(insert_location, parent, left_most, right_most);\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::insert(ZIntrusiveRBTreeNode* new_node, const FindCursor& find_cursor) {\n+  precond(find_cursor.is_valid(_sequence_number));\n+  precond(!find_cursor.found());\n+  DEBUG_ONLY(_sequence_number++;)\n+\n+  \/\/ Link in the new node\n+  new_node->link_node(find_cursor.parent(), find_cursor.insert_location());\n+\n+  \/\/ Keep track of first and last node(s)\n+  if (find_cursor.is_left_most()) {\n+    _left_most = new_node;\n+  }\n+  if (find_cursor.is_right_most()) {\n+    _right_most = new_node;\n+  }\n+\n+  rebalance_insert(new_node);\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::replace(ZIntrusiveRBTreeNode* new_node, const FindCursor& find_cursor) {\n+  precond(find_cursor.is_valid(_sequence_number));\n+  precond(find_cursor.found());\n+  DEBUG_ONLY(_sequence_number++;)\n+\n+  const ZIntrusiveRBTreeNode* const node = find_cursor.node();\n+\n+  if (new_node != node) {\n+    \/\/ Node has changed\n+\n+    \/\/ Copy the node to new location\n+    *new_node = *node;\n+\n+    \/\/ Update insert location\n+    *find_cursor.insert_location() = new_node;\n+\n+    \/\/ Update children's parent\n+    if (new_node->has_left_child()) {\n+      new_node->left_child()->update_parent(new_node);\n+    }\n+    if (new_node->has_right_child()) {\n+      new_node->right_child()->update_parent(new_node);\n+    }\n+\n+    \/\/ Keep track of first and last node(s)\n+    if (find_cursor.is_left_most()) {\n+      assert(_left_most == node, \"must be\");\n+      _left_most = new_node;\n+    }\n+    if (find_cursor.is_right_most()) {\n+      assert(_right_most == node, \"must be\");\n+      _right_most = new_node;\n+    }\n+  }\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::remove(const FindCursor& find_cursor) {\n+  precond(find_cursor.is_valid(_sequence_number));\n+  precond(find_cursor.found());\n+  DEBUG_ONLY(_sequence_number++;)\n+\n+  ZIntrusiveRBTreeNode* const node = find_cursor.node();\n+  ZIntrusiveRBTreeNode* const parent = node->parent();\n+\n+  \/\/ Keep track of first and last node(s)\n+  if (find_cursor.is_left_most()) {\n+    assert(_left_most == node, \"must be\");\n+    _left_most = _left_most->next();\n+  }\n+  if (find_cursor.is_right_most()) {\n+    assert(_right_most == node, \"must be\");\n+    _right_most = _right_most->prev();\n+  }\n+\n+  ZIntrusiveRBTreeNode* rebalance_from = nullptr;\n+\n+  if (!node->has_left_child() && !node->has_right_child()) {\n+    \/\/ No children\n+\n+    \/\/ Remove node\n+    update_child_or_root(node, nullptr, parent);\n+    if (node->is_black()) {\n+      \/\/ We unbalanced the tree\n+      rebalance_from = parent;\n+    }\n+  } else if (!node->has_left_child() || !node->has_right_child()) {\n+    assert(node->has_right_child() || node->has_left_child(), \"must be\");\n+    \/\/ Only one child\n+    ZIntrusiveRBTreeNode* child = node->has_left_child() ? node->left_child() : node->right_child();\n+\n+    \/\/ Let child take nodes places\n+    update_child_or_root(node, child, parent);\n+\n+    \/\/ And update parent and color\n+    child->copy_parent_and_color(node);\n+  } else {\n+    assert(node->has_left_child() && node->has_right_child(), \"must be\");\n+    \/\/ Find next node and let it take the nodes place\n+    \/\/ This asymmetry always swap next instead of prev,\n+    \/\/ I wonder how this behaves w.r.t. our mapped cache\n+    \/\/ strategy of mostly removing from the left side of\n+    \/\/ the tree\n+\n+    \/\/ This will never walk up the tree, hope the compiler sees this.\n+    ZIntrusiveRBTreeNode* next_node = node->next();\n+\n+    ZIntrusiveRBTreeNode* next_node_parent = next_node->parent();\n+    ZIntrusiveRBTreeNode* next_node_child = next_node->right_child();\n+    if (next_node_parent != node) {\n+      \/\/ Not the direct descendant, adopt node's child\n+      ZIntrusiveRBTreeNode* node_child = node->right_child();\n+      next_node->update_right_child(node_child);\n+      node_child->update_parent(next_node);\n+\n+      \/\/ And let parent adopt their grand child\n+      next_node_parent->update_left_child(next_node_child);\n+    } else {\n+      next_node_parent = next_node;\n+    }\n+    \/\/ Adopt node's other child\n+    ZIntrusiveRBTreeNode* node_child = node->left_child();\n+    next_node->update_left_child(node_child);\n+    node_child->update_parent(next_node);\n+\n+    update_child_or_root(node, next_node, parent);\n+\n+    \/\/ Update parent(s) and colors\n+    if (next_node_child != nullptr) {\n+      next_node_child->update_parent_and_color(next_node_parent, ZIntrusiveRBTreeNode::BLACK);\n+    } else if (next_node->is_black()) {\n+      rebalance_from = next_node_parent;\n+    }\n+    next_node->copy_parent_and_color(node);\n+  }\n+\n+  if (rebalance_from == nullptr) {\n+    \/\/ Removal did not unbalance the tree\n+    return;\n+  }\n+\n+  rebalance_remove(rebalance_from);\n+}\n+\n+template <typename Key, typename Compare>\n+inline void ZIntrusiveRBTree<Key, Compare>::verify_tree() {\n+  \/\/ Properties:\n+  \/\/  (a) Node's are either BLACK or RED\n+  \/\/  (b) All nullptr children are counted as BLACK\n+  \/\/  (c) Compare::operator(Node*, Node*) <=> 0 is transitive\n+  \/\/ Invariants:\n+  \/\/  (1) Root node is BLACK\n+  \/\/  (2) All RED nodes only have BLACK children\n+  \/\/  (3) Every simple path from the root to a leaf\n+  \/\/      contains the same amount of BLACK nodes\n+  \/\/  (4) A node's children must have that node as\n+  \/\/      its parent\n+  \/\/  (5) Each node N in the sub-tree formed from a\n+  \/\/      node A's child must:\n+  \/\/        if left child:  Compare::operator(A, N) < 0\n+  \/\/        if right child: Compare::operator(A, N) > 0\n+  \/\/\n+  \/\/ Note: 1-4 may not hold during a call to insert\n+  \/\/       and remove.\n+\n+  \/\/ Helpers\n+  const auto is_leaf = [](ZIntrusiveRBTreeNode* node) {\n+    return node == nullptr;\n+  };\n+  const auto is_black = [&](ZIntrusiveRBTreeNode* node) {\n+    return is_leaf(node) || node->is_black();\n+  };\n+  const auto is_red = [&](ZIntrusiveRBTreeNode* node) {\n+    return !is_black(node);\n+  };\n+\n+  \/\/ Verify (1)\n+  ZIntrusiveRBTreeNode* const root_node = _root_node;\n+  guarantee(is_black(root_node), \"Invariant (1)\");\n+\n+  \/\/ Verify (2)\n+  const auto verify_2 = [&](ZIntrusiveRBTreeNode* node) {\n+    guarantee(!is_red(node) || is_black(node->left_child()), \"Invariant (2)\");\n+    guarantee(!is_red(node) || is_black(node->right_child()), \"Invariant (2)\");\n+  };\n+\n+  \/\/ Verify (3)\n+  size_t first_simple_path_black_nodes_traversed = 0;\n+  const auto verify_3 = [&](ZIntrusiveRBTreeNode* node, size_t black_nodes_traversed) {\n+    if (!is_leaf(node)) { return; }\n+    if (first_simple_path_black_nodes_traversed == 0) {\n+      first_simple_path_black_nodes_traversed = black_nodes_traversed;\n+    }\n+    guarantee(first_simple_path_black_nodes_traversed == black_nodes_traversed, \"Invariant (3)\");\n+  };\n+\n+  \/\/ Verify (4)\n+  const auto verify_4 = [&](ZIntrusiveRBTreeNode* node) {\n+    if (is_leaf(node)) { return; }\n+    guarantee(!node->has_left_child() || node->left_child()->parent() == node, \"Invariant (4)\");\n+    guarantee(!node->has_right_child() || node->right_child()->parent() == node, \"Invariant (4)\");\n+  };\n+  guarantee(root_node == nullptr || root_node->parent() == nullptr, \"Invariant (4)\");\n+\n+  \/\/ Verify (5)\n+  const auto verify_5 = [&](ZIntrusiveRBTreeNode* node) {\n+    \/\/ Because of the transitive property of Compare (c) we simply check\n+    \/\/ this that (5) hold for each parent child pair.\n+    if (is_leaf(node)) { return; }\n+    Compare compare_fn;\n+    guarantee(!node->has_left_child() || compare_fn(node->left_child(), node) < 0, \"Invariant (5)\");\n+    guarantee(!node->has_right_child() || compare_fn(node->right_child(), node) > 0, \"Invariant (5)\");\n+  };\n+\n+  \/\/ Walk every simple path by recursively descending the tree from the root\n+  const auto recursive_walk = [&](auto&& recurse, ZIntrusiveRBTreeNode* node, size_t black_nodes_traversed) {\n+    if (is_black(node)) { black_nodes_traversed++; }\n+    verify_2(node);\n+    verify_3(node, black_nodes_traversed);\n+    verify_4(node);\n+    verify_5(node);\n+    if (is_leaf(node)) { return; }\n+    recurse(recurse, node->left_child(), black_nodes_traversed);\n+    recurse(recurse, node->right_child(), black_nodes_traversed);\n+  };\n+  recursive_walk(recursive_walk, root_node, 0);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::Iterator ZIntrusiveRBTree<Key, Compare>::begin() {\n+  return Iterator(*this, first());\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::Iterator ZIntrusiveRBTree<Key, Compare>::end() {\n+  return Iterator(*this, nullptr);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstIterator ZIntrusiveRBTree<Key, Compare>::begin() const {\n+  return cbegin();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstIterator ZIntrusiveRBTree<Key, Compare>::end() const {\n+  return cend();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstIterator ZIntrusiveRBTree<Key, Compare>::cbegin() const {\n+  return const_cast<ZIntrusiveRBTree<Key, Compare>*>(this)->begin();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstIterator ZIntrusiveRBTree<Key, Compare>::cend() const {\n+  return const_cast<ZIntrusiveRBTree<Key, Compare>*>(this)->end();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ReverseIterator ZIntrusiveRBTree<Key, Compare>::rbegin() {\n+  return ReverseIterator(*this, last());\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ReverseIterator ZIntrusiveRBTree<Key, Compare>::rend() {\n+  return ReverseIterator(*this, nullptr);\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstReverseIterator ZIntrusiveRBTree<Key, Compare>::rbegin() const {\n+  return crbegin();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstReverseIterator ZIntrusiveRBTree<Key, Compare>::rend() const {\n+  return crend();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstReverseIterator ZIntrusiveRBTree<Key, Compare>::crbegin() const {\n+  return const_cast<ZIntrusiveRBTree<Key, Compare>*>(this)->rbegin();\n+}\n+\n+template <typename Key, typename Compare>\n+inline typename ZIntrusiveRBTree<Key, Compare>::ConstReverseIterator ZIntrusiveRBTree<Key, Compare>::crend() const {\n+  return const_cast<ZIntrusiveRBTree<Key, Compare>*>(this)->rend();\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline bool ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::at_end() const {\n+  return _node == nullptr;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::IteratorImplementation(ZIntrusiveRBTree<Key, Compare>& tree, pointer node)\n+: _tree(&tree),\n+  _node(node),\n+  _removed(false) {}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+template <bool Enable, ENABLE_IF_SDEFN(Enable)>\n+inline ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::IteratorImplementation(const IteratorImplementation<false, Reverse>& other)\n+: _tree(other._tree),\n+  _node(other._node),\n+  _removed(false) {}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline typename ZIntrusiveRBTree<Key, Compare>::template IteratorImplementation<IsConst, Reverse>::reference ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::operator*() const {\n+  precond(!_removed);\n+  return *_node;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline typename ZIntrusiveRBTree<Key, Compare>::template IteratorImplementation<IsConst, Reverse>::pointer ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::operator->() {\n+  precond(!_removed);\n+  return _node;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline typename ZIntrusiveRBTree<Key, Compare>::template IteratorImplementation<IsConst, Reverse>& ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::operator--() {\n+  if (_removed) {\n+    _removed = false;\n+  } else if (Reverse) {\n+    precond(_node != _tree->last());\n+    _node = at_end() ? _tree->first() : _node->next();\n+  } else {\n+    precond(_node != _tree->first());\n+    _node = at_end() ? _tree->last() : _node->prev();\n+  }\n+  return *this;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline typename ZIntrusiveRBTree<Key, Compare>::template IteratorImplementation<IsConst, Reverse> ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::operator--(int) {\n+  IteratorImplementation tmp = *this;\n+  --(*this);\n+  return tmp;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline typename ZIntrusiveRBTree<Key, Compare>::template IteratorImplementation<IsConst, Reverse>& ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::operator++() {\n+  if (_removed) {\n+    _removed = false;\n+  } else if (Reverse) {\n+    precond(!at_end());\n+    _node = _node->prev();\n+  } else {\n+    precond(!at_end());\n+    _node = _node->next();\n+  }\n+  return *this;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+inline typename ZIntrusiveRBTree<Key, Compare>::template IteratorImplementation<IsConst, Reverse> ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::operator++(int) {\n+  IteratorImplementation tmp = *this;\n+  ++(*this);\n+  return tmp;\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+template <bool Enable, ENABLE_IF_SDEFN(Enable)>\n+void ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::replace(ZIntrusiveRBTreeNode* new_node) {\n+  precond(!_removed);\n+  precond(!at_end());\n+  FindCursor cursor = _tree->get_cursor(_node);\n+  _node = new_node;\n+  _tree->replace(new_node, cursor);\n+}\n+\n+template <typename Key, typename Compare>\n+template <bool IsConst, bool Reverse>\n+template <bool Enable, ENABLE_IF_SDEFN(Enable)>\n+void ZIntrusiveRBTree<Key, Compare>::IteratorImplementation<IsConst, Reverse>::remove() {\n+  precond(!_removed);\n+  precond(!at_end());\n+  FindCursor cursor = _tree->get_cursor(_node);\n+  ++(*this);\n+  _removed = true;\n+  _tree->remove(cursor);\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZINTRUSIVERBTREE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zIntrusiveRBTree.inline.hpp","additions":1351,"deletions":0,"binary":false,"changes":1351,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -49,1 +50,6 @@\n-  ~ZListNode();\n+  ~ZListNode() {\n+    \/\/ Implementation placed here to make it easier easier to embed ZListNode\n+    \/\/ instances without having to include zListNode.inline.hpp.\n+    assert(_next == this, \"Should not be in a list\");\n+    assert(_prev == this, \"Should not be in a list\");\n+  }\n@@ -62,0 +68,1 @@\n+  void verify_head_error_reporter_safe() const;\n@@ -71,0 +78,3 @@\n+  size_t size_error_reporter_safe() const;\n+  bool is_empty_error_reporter_safe() const;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zList.hpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"utilities\/vmError.hpp\"\n@@ -36,5 +37,0 @@\n-template <typename T>\n-inline ZListNode<T>::~ZListNode() {\n-  verify_links_unlinked();\n-}\n-\n@@ -65,0 +61,10 @@\n+template <typename T>\n+inline void ZList<T>::verify_head_error_reporter_safe() const {\n+  if (VMError::is_error_reported() && VMError::is_error_reported_in_current_thread()) {\n+    \/\/ Do not verify if this thread is in the process of reporting an error.\n+    return;\n+  }\n+\n+  verify_head();\n+}\n+\n@@ -100,0 +106,11 @@\n+template <typename T>\n+inline size_t ZList<T>::size_error_reporter_safe() const {\n+  verify_head_error_reporter_safe();\n+  return _size;\n+}\n+\n+template <typename T>\n+inline bool ZList<T>::is_empty_error_reporter_safe() const {\n+  return size_error_reporter_safe() == 0;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zList.inline.hpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -37,7 +37,4 @@\n-static size_t bitmap_size(uint32_t size, size_t NumSegments) {\n-  \/\/ We need at least one bit per segment\n-  return MAX2<size_t>(size, NumSegments) * 2;\n-}\n-\n-ZLiveMap::ZLiveMap(uint32_t size)\n-  : _seqnum(0),\n+ZLiveMap::ZLiveMap(uint32_t object_max_count)\n+  : _segment_size((object_max_count == 1 ? 1u : (object_max_count \/ NumSegments)) * BitsPerObject),\n+    _segment_shift(log2i_exact(_segment_size)),\n+    _seqnum(0),\n@@ -48,3 +45,1 @@\n-    _bitmap_size(bitmap_size(size, NumSegments)),\n-    _bitmap(0),\n-    _segment_shift(log2i_exact(segment_size())) {}\n+    _bitmap(0) {}\n@@ -52,3 +47,3 @@\n-void ZLiveMap::allocate_bitmap() {\n-  if (_bitmap.size() != _bitmap_size) {\n-    _bitmap.initialize(_bitmap_size, false \/* clear *\/);\n+void ZLiveMap::initialize_bitmap() {\n+  if (_bitmap.size() == 0) {\n+    _bitmap.initialize(size_t(_segment_size) * size_t(NumSegments), false \/* clear *\/);\n@@ -74,4 +69,0 @@\n-      \/\/ We lazily initialize the bitmap the first time the page is\n-      \/\/ marked, i.e. a bit is about to be set for the first time.\n-      allocate_bitmap();\n-\n@@ -82,0 +73,4 @@\n+      \/\/ We lazily initialize the bitmap the first time the page is marked, i.e.\n+      \/\/ a bit is about to be set for the first time.\n+      initialize_bitmap();\n+\n@@ -128,1 +123,1 @@\n-  if (segment_size() \/ BitsPerWord >= 32) {\n+  if (_segment_size \/ BitsPerWord >= 32) {\n@@ -138,10 +133,0 @@\n-\n-void ZLiveMap::resize(uint32_t size) {\n-  const size_t new_bitmap_size = bitmap_size(size, NumSegments);\n-  _bitmap_size = new_bitmap_size;\n-  _segment_shift = log2i_exact(segment_size());\n-\n-  if (_bitmap.size() != 0 && _bitmap.size() != new_bitmap_size) {\n-    _bitmap.reinitialize(new_bitmap_size, false \/* clear *\/);\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.cpp","additions":13,"deletions":28,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -38,1 +38,5 @@\n-  static const size_t NumSegments = 64;\n+  static const uint32_t NumSegments = 64;\n+  static const uint32_t BitsPerObject = 2;\n+\n+  const uint32_t    _segment_size;\n+  const int         _segment_shift;\n@@ -45,1 +49,0 @@\n-  size_t            _bitmap_size;\n@@ -47,1 +50,0 @@\n-  int               _segment_shift;\n@@ -55,2 +57,0 @@\n-  BitMap::idx_t segment_size() const;\n-\n@@ -69,1 +69,1 @@\n-  void allocate_bitmap();\n+  void initialize_bitmap();\n@@ -80,1 +80,1 @@\n-  ZLiveMap(uint32_t size);\n+  ZLiveMap(uint32_t object_max_count);\n@@ -84,1 +84,0 @@\n-  void resize(uint32_t size);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.hpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -90,4 +90,0 @@\n-inline BitMap::idx_t ZLiveMap::segment_size() const {\n-  return _bitmap_size \/ NumSegments;\n-}\n-\n@@ -128,1 +124,1 @@\n-  return segment_size() * segment;\n+  return segment * _segment_size;\n@@ -132,1 +128,1 @@\n-  return segment_start(segment) + segment_size();\n+  return segment_start(segment) + _segment_size;\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.inline.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,629 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zIntrusiveRBTree.inline.hpp\"\n+#include \"gc\/z\/zList.inline.hpp\"\n+#include \"gc\/z\/zMappedCache.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+class ZMappedCacheEntry {\n+private:\n+  ZVirtualMemory                  _vmem;\n+  ZMappedCache::TreeNode          _tree_node;\n+  ZMappedCache::SizeClassListNode _size_class_list_node;\n+\n+public:\n+  ZMappedCacheEntry(ZVirtualMemory vmem)\n+    : _vmem(vmem),\n+      _tree_node(),\n+      _size_class_list_node() {}\n+\n+  static ZMappedCacheEntry* cast_to_entry(ZMappedCache::TreeNode* tree_node);\n+  static const ZMappedCacheEntry* cast_to_entry(const ZMappedCache::TreeNode* tree_node);\n+  static ZMappedCacheEntry* cast_to_entry(ZMappedCache::SizeClassListNode* list_node);\n+\n+  zoffset start() const {\n+    return _vmem.start();\n+  }\n+\n+  zoffset_end end() const {\n+    return _vmem.end();\n+  }\n+\n+  ZVirtualMemory vmem() const {\n+    return _vmem;\n+  }\n+\n+  ZMappedCache::TreeNode* node_addr() {\n+    return &_tree_node;\n+  }\n+\n+  void update_start(ZVirtualMemory vmem) {\n+    precond(vmem.end() == end());\n+\n+    _vmem = vmem;\n+  }\n+\n+  ZMappedCache::ZSizeClassListNode* size_class_node() {\n+    return &_size_class_list_node;\n+  }\n+};\n+\n+ZMappedCacheEntry* ZMappedCacheEntry::cast_to_entry(ZMappedCache::TreeNode* tree_node) {\n+  return const_cast<ZMappedCacheEntry*>(ZMappedCacheEntry::cast_to_entry(const_cast<const ZMappedCache::TreeNode*>(tree_node)));\n+}\n+\n+const ZMappedCacheEntry* ZMappedCacheEntry::cast_to_entry(const ZMappedCache::TreeNode* tree_node) {\n+  return (const ZMappedCacheEntry*)((uintptr_t)tree_node - offset_of(ZMappedCacheEntry, _tree_node));\n+}\n+\n+ZMappedCacheEntry* ZMappedCacheEntry::cast_to_entry(ZMappedCache::SizeClassListNode* list_node) {\n+  const size_t offset = offset_of(ZMappedCacheEntry, _size_class_list_node);\n+  return (ZMappedCacheEntry*)((uintptr_t)list_node - offset);\n+}\n+\n+static void* entry_address_for_zoffset_end(zoffset_end offset) {\n+  STATIC_ASSERT(is_aligned(ZCacheLineSize, alignof(ZMappedCacheEntry)));;\n+\n+  \/\/ This spreads out the location of the entries in an effort to combat hyper alignment.\n+  \/\/ Verify if this is an efficient and worthwhile optimization.\n+\n+  constexpr size_t aligned_entry_size = align_up(sizeof(ZMappedCacheEntry), ZCacheLineSize);\n+\n+  \/\/ Do not use the last location\n+  constexpr size_t number_of_locations = ZGranuleSize \/ aligned_entry_size - 1;\n+  const size_t granule_index = untype(offset) >> ZGranuleSizeShift;\n+  const size_t index = granule_index % number_of_locations;\n+  const uintptr_t end_addr = untype(offset) + ZAddressHeapBase;\n+\n+  return reinterpret_cast<void*>(end_addr - aligned_entry_size * (index + 1));\n+}\n+\n+static ZMappedCacheEntry* create_entry(const ZVirtualMemory& vmem) {\n+  precond(vmem.size() >= ZGranuleSize);\n+\n+  void* placement_addr = entry_address_for_zoffset_end(vmem.end());\n+  ZMappedCacheEntry* entry = new (placement_addr) ZMappedCacheEntry(vmem);\n+\n+  postcond(entry->start() == vmem.start());\n+  postcond(entry->end() == vmem.end());\n+\n+  return entry;\n+}\n+\n+int ZMappedCache::EntryCompare::operator()(ZMappedCache::TreeNode* a, ZMappedCache::TreeNode* b) {\n+  const ZVirtualMemory vmem_a = ZMappedCacheEntry::cast_to_entry(a)->vmem();\n+  const ZVirtualMemory vmem_b = ZMappedCacheEntry::cast_to_entry(b)->vmem();\n+\n+  if (vmem_a.end() < vmem_b.start()) { return -1; }\n+  if (vmem_b.end() < vmem_a.start()) { return 1; }\n+\n+  return 0; \/\/ Overlapping\n+}\n+\n+int ZMappedCache::EntryCompare::operator()(zoffset key, ZMappedCache::TreeNode* node) {\n+  const ZVirtualMemory vmem = ZMappedCacheEntry::cast_to_entry(node)->vmem();\n+\n+  if (key < vmem.start()) { return -1; }\n+  if (key > vmem.end()) { return 1; }\n+\n+  return 0; \/\/ Containing\n+}\n+\n+int ZMappedCache::size_class_index(size_t size) {\n+  \/\/ Returns the size class index of for size, or -1 if smaller than the smallest size class.\n+  const int size_class_power = log2i_graceful(size) - (int)ZGranuleSizeShift;\n+\n+  if (size_class_power < MinSizeClassShift) {\n+    \/\/ Allocation is smaller than the smallest size class minimum size.\n+    return -1;\n+  }\n+\n+  return MIN2(size_class_power, MaxSizeClassShift) - MinSizeClassShift;\n+}\n+\n+int ZMappedCache::guaranteed_size_class_index(size_t size) {\n+  \/\/ Returns the size class index of the smallest size class which can always\n+  \/\/ accommodate a size allocation, or -1 otherwise.\n+  const int size_class_power = log2i_ceil(size) - (int)ZGranuleSizeShift;\n+\n+  if (size_class_power > MaxSizeClassShift) {\n+    \/\/ Allocation is larger than the largest size class minimum size.\n+    return -1;\n+  }\n+\n+  return MAX2(size_class_power, MinSizeClassShift) - MinSizeClassShift;\n+}\n+\n+void ZMappedCache::tree_insert(const Tree::FindCursor& cursor, const ZVirtualMemory& vmem) {\n+  ZMappedCacheEntry* const entry = create_entry(vmem);\n+\n+  \/\/ Insert creates a new entry\n+  _entry_count += 1;\n+\n+  \/\/ Insert in tree\n+  _tree.insert(entry->node_addr(), cursor);\n+\n+  \/\/ Insert in size-class lists\n+  const size_t size = vmem.size();\n+  const int index = size_class_index(size);\n+  if (index != -1) {\n+    _size_class_lists[index].insert_first(entry->size_class_node());\n+  }\n+}\n+\n+void ZMappedCache::tree_remove(const Tree::FindCursor& cursor, const ZVirtualMemory& vmem) {\n+  ZMappedCacheEntry* entry = ZMappedCacheEntry::cast_to_entry(cursor.node());\n+\n+  \/\/ Remove destroys an old entry\n+  _entry_count -= 1;\n+\n+  \/\/ Remove from tree\n+  _tree.remove(cursor);\n+\n+  \/\/ Insert in size-class lists\n+  const size_t size = vmem.size();\n+  const int index = size_class_index(size);\n+  if (index != -1) {\n+    _size_class_lists[index].remove(entry->size_class_node());\n+  }\n+\n+  \/\/ Destroy entry\n+  entry->~ZMappedCacheEntry();\n+}\n+\n+void ZMappedCache::tree_replace(const Tree::FindCursor& cursor, const ZVirtualMemory& vmem) {\n+  ZMappedCacheEntry* const entry = create_entry(vmem);\n+\n+  ZMappedCache::TreeNode* const node = cursor.node();\n+  ZMappedCacheEntry* const old_entry = ZMappedCacheEntry::cast_to_entry(node);\n+  assert(old_entry->end() != vmem.end(), \"should not replace, use update\");\n+\n+  \/\/ Replace in tree\n+  _tree.replace(entry->node_addr(), cursor);\n+\n+  \/\/ Replace in size-class lists\n+\n+  \/\/ Remove old\n+  const size_t old_size = old_entry->vmem().size();\n+  const int old_index = size_class_index(old_size);\n+  if (old_index != -1) {\n+    _size_class_lists[old_index].remove(old_entry->size_class_node());\n+  }\n+\n+  \/\/ Insert new\n+  const size_t new_size = vmem.size();\n+  const int new_index = size_class_index(new_size);\n+  if (new_index != -1) {\n+    _size_class_lists[new_index].insert_first(entry->size_class_node());\n+  }\n+\n+  \/\/ Destroy old entry\n+  old_entry->~ZMappedCacheEntry();\n+}\n+\n+void ZMappedCache::tree_update(ZMappedCacheEntry* entry, const ZVirtualMemory& vmem) {\n+  assert(entry->end() == vmem.end(), \"must be\");\n+\n+  \/\/ Remove or add to size-class lists if required\n+\n+  const size_t old_size = entry->vmem().size();\n+  const size_t new_size = vmem.size();\n+  const int old_index = size_class_index(old_size);\n+  const int new_index = size_class_index(new_size);\n+\n+  if (old_index != new_index) {\n+    \/\/ Size class changed\n+\n+    \/\/ Remove old\n+    if (old_index != -1) {\n+      _size_class_lists[old_index].remove(entry->size_class_node());\n+    }\n+\n+    \/\/ Insert new\n+    if (new_index != -1) {\n+      _size_class_lists[new_index].insert_first(entry->size_class_node());\n+    }\n+  }\n+\n+  \/\/ And update entry\n+  entry->update_start(vmem);\n+}\n+\n+template <ZMappedCache::RemovalStrategy strategy, typename SelectFunction>\n+ZVirtualMemory ZMappedCache::remove_vmem(ZMappedCacheEntry* const entry, size_t min_size, SelectFunction select) {\n+  ZVirtualMemory vmem = entry->vmem();\n+  const size_t size = vmem.size();\n+\n+  if (size < min_size) {\n+    \/\/ Do not select this, smaller than min_size\n+    return ZVirtualMemory();\n+  }\n+\n+  \/\/ Query how much to remove\n+  const size_t to_remove = select(size);\n+  assert(to_remove <= size, \"must not remove more than size\");\n+\n+  if (to_remove == 0) {\n+    \/\/ Nothing to remove\n+    return ZVirtualMemory();\n+  }\n+\n+  if (to_remove != size) {\n+    \/\/ Partial removal\n+    if (strategy == RemovalStrategy::LowestAddress) {\n+      const size_t unused_size = size - to_remove;\n+      const ZVirtualMemory unused_vmem = vmem.shrink_from_back(unused_size);\n+      tree_update(entry, unused_vmem);\n+\n+    } else {\n+      assert(strategy == RemovalStrategy::HighestAddress, \"must be LowestAddress or HighestAddress\");\n+\n+      const size_t unused_size = size - to_remove;\n+      const ZVirtualMemory unused_vmem = vmem.shrink_from_front(unused_size);\n+\n+      auto cursor = _tree.get_cursor(entry->node_addr());\n+      assert(cursor.is_valid(), \"must be\");\n+      tree_replace(cursor, unused_vmem);\n+    }\n+\n+  } else {\n+    \/\/ Whole removal\n+    auto cursor = _tree.get_cursor(entry->node_addr());\n+    assert(cursor.is_valid(), \"must be\");\n+    tree_remove(cursor, vmem);\n+  }\n+\n+  \/\/ Update statistics\n+  _size -= to_remove;\n+  _min = MIN2(_size, _min);\n+\n+  postcond(to_remove == vmem.size());\n+  return vmem;\n+}\n+\n+template <typename SelectFunction, typename ConsumeFunction>\n+bool ZMappedCache::try_remove_vmem_size_class(size_t min_size, SelectFunction select, ConsumeFunction consume) {\n+new_max_size:\n+  \/\/ Query the max select size possible given the size of the cache\n+  const size_t max_size = select(_size);\n+\n+  if (max_size < min_size) {\n+    \/\/ Never select less than min_size\n+    return false;\n+  }\n+\n+  \/\/ Start scanning from max_size guaranteed size class to the largest size class\n+  const int guaranteed_index = guaranteed_size_class_index(max_size);\n+  for (int index = guaranteed_index; index != -1 && index < NumSizeClasses; ++index) {\n+    ZList<ZSizeClassListNode>& list = _size_class_lists[index];\n+    if (!list.is_empty()) {\n+      ZMappedCacheEntry* const entry = ZMappedCacheEntry::cast_to_entry(list.first());\n+\n+      \/\/ Because this is guaranteed, select should always succeed\n+      const ZVirtualMemory vmem = remove_vmem<RemovalStrategy::LowestAddress>(entry, min_size, select);\n+      assert(!vmem.is_null(), \"select must succeed\");\n+\n+      if (consume(vmem)) {\n+        \/\/ consume is satisfied\n+        return true;\n+      }\n+\n+      \/\/ Continue with a new max_size\n+      goto new_max_size;\n+    }\n+  }\n+\n+  \/\/ Consume the rest starting at max_size's size class to min_size's size class\n+  const int max_size_index = size_class_index(max_size);\n+  const int min_size_index = size_class_index(min_size);\n+  const int lowest_index = MAX2(min_size_index, 0);\n+\n+  for (int index = max_size_index; index >= lowest_index; --index) {\n+    ZListIterator<ZSizeClassListNode> iter(&_size_class_lists[index]);\n+    for (ZSizeClassListNode* list_node; iter.next(&list_node);) {\n+      ZMappedCacheEntry* const entry = ZMappedCacheEntry::cast_to_entry(list_node);\n+\n+      \/\/ Try remove\n+      const ZVirtualMemory vmem = remove_vmem<RemovalStrategy::LowestAddress>(entry, min_size, select);\n+\n+      if (!vmem.is_null() && consume(vmem)) {\n+        \/\/ Found a vmem and consume is satisfied\n+        return true;\n+      }\n+    }\n+  }\n+\n+  \/\/ consume was not satisfied\n+  return false;\n+}\n+\n+template <ZMappedCache::RemovalStrategy strategy, typename SelectFunction, typename ConsumeFunction>\n+void ZMappedCache::scan_remove_vmem(size_t min_size, SelectFunction select, ConsumeFunction consume) {\n+  if (strategy == RemovalStrategy::SizeClasses) {\n+    if (try_remove_vmem_size_class(min_size, select, consume)) {\n+      \/\/ Satisfied using size classes\n+      return;\n+    }\n+\n+    if (size_class_index(min_size) != -1) {\n+      \/\/ There exists a size class for our min size. All possibilities must have\n+      \/\/ been exhausted, do not scan the tree.\n+      return;\n+    }\n+\n+    \/\/ Fallthrough to tree scan\n+  }\n+\n+  if (strategy == RemovalStrategy::HighestAddress) {\n+    \/\/ Scan whole tree starting at the highest address\n+    for (ZMappedCache::TreeNode* node = _tree.last(); node != nullptr; node = node->prev()) {\n+      ZMappedCacheEntry* const entry = ZMappedCacheEntry::cast_to_entry(node);\n+\n+      const ZVirtualMemory vmem = remove_vmem<RemovalStrategy::HighestAddress>(entry, min_size, select);\n+\n+      if (!vmem.is_null() && consume(vmem)) {\n+        \/\/ Found a vmem and consume is satisfied.\n+        return;\n+      }\n+    }\n+\n+  } else {\n+    assert(strategy == RemovalStrategy::SizeClasses || strategy == RemovalStrategy::LowestAddress, \"unknown strategy\");\n+\n+    \/\/ Scan whole tree starting at the lowest address\n+    for (ZMappedCache::TreeNode* node = _tree.first(); node != nullptr; node = node->next()) {\n+      ZMappedCacheEntry* const entry = ZMappedCacheEntry::cast_to_entry(node);\n+\n+      const ZVirtualMemory vmem = remove_vmem<RemovalStrategy::LowestAddress>(entry, min_size, select);\n+\n+      if (!vmem.is_null() && consume(vmem)) {\n+        \/\/ Found a vmem and consume is satisfied.\n+        return;\n+      }\n+    }\n+  }\n+}\n+\n+template <ZMappedCache::RemovalStrategy strategy, typename SelectFunction, typename ConsumeFunction>\n+void ZMappedCache::scan_remove_vmem(SelectFunction select, ConsumeFunction consume) {\n+  \/\/ Scan without a min_size\n+  scan_remove_vmem<strategy>(0, select, consume);\n+}\n+\n+template <ZMappedCache::RemovalStrategy strategy>\n+size_t ZMappedCache::remove_discontiguous_with_strategy(size_t size, ZArray<ZVirtualMemory>* out) {\n+  precond(size > 0);\n+  precond(is_aligned(size, ZGranuleSize));\n+\n+  size_t remaining = size;\n+\n+  const auto select_size_fn = [&](size_t vmem_size) {\n+    \/\/ Select at most remaining\n+    return MIN2(remaining, vmem_size);\n+  };\n+\n+  const auto consume_vmem_fn = [&](ZVirtualMemory vmem) {\n+    const size_t vmem_size = vmem.size();\n+    out->append(vmem);\n+\n+    assert(vmem_size <= remaining, \"consumed to much\");\n+\n+    \/\/ Track remaining, and stop when it reaches zero\n+    remaining -= vmem_size;\n+\n+    return remaining == 0;\n+  };\n+\n+  scan_remove_vmem<strategy>(select_size_fn, consume_vmem_fn);\n+\n+  return size - remaining;\n+}\n+\n+ZMappedCache::ZMappedCache()\n+  : _tree(),\n+    _entry_count(0),\n+    _size_class_lists{},\n+    _size(0),\n+    _min(_size) {}\n+\n+void ZMappedCache::insert(const ZVirtualMemory& vmem) {\n+  _size += vmem.size();\n+\n+  Tree::FindCursor current_cursor = _tree.find(vmem.start());\n+  Tree::FindCursor next_cursor = _tree.next(current_cursor);\n+\n+  const bool extends_left = current_cursor.found();\n+  const bool extends_right = next_cursor.is_valid() && next_cursor.found() &&\n+                             ZMappedCacheEntry::cast_to_entry(next_cursor.node())->start() == vmem.end();\n+\n+  if (extends_left && extends_right) {\n+    ZMappedCacheEntry* next_entry = ZMappedCacheEntry::cast_to_entry(next_cursor.node());\n+\n+    const ZVirtualMemory left_vmem = ZMappedCacheEntry::cast_to_entry(current_cursor.node())->vmem();\n+    const ZVirtualMemory right_vmem = next_entry->vmem();\n+    assert(left_vmem.adjacent_to(vmem), \"must be\");\n+    assert(vmem.adjacent_to(right_vmem), \"must be\");\n+\n+    ZVirtualMemory new_vmem = left_vmem;\n+    new_vmem.grow_from_back(vmem.size());\n+    new_vmem.grow_from_back(right_vmem.size());\n+\n+    \/\/ Remove current (left vmem)\n+    tree_remove(current_cursor, left_vmem);\n+\n+    \/\/ And update next's start\n+    tree_update(next_entry, new_vmem);\n+\n+    return;\n+  }\n+\n+  if (extends_left) {\n+    const ZVirtualMemory left_vmem = ZMappedCacheEntry::cast_to_entry(current_cursor.node())->vmem();\n+    assert(left_vmem.adjacent_to(vmem), \"must be\");\n+\n+    ZVirtualMemory new_vmem = left_vmem;\n+    new_vmem.grow_from_back(vmem.size());\n+\n+    tree_replace(current_cursor, new_vmem);\n+\n+    return;\n+  }\n+\n+  if (extends_right) {\n+    ZMappedCacheEntry* next_entry = ZMappedCacheEntry::cast_to_entry(next_cursor.node());\n+\n+    const ZVirtualMemory right_vmem = next_entry->vmem();\n+    assert(vmem.adjacent_to(right_vmem), \"must be\");\n+\n+    ZVirtualMemory new_vmem = vmem;\n+    new_vmem.grow_from_back(right_vmem.size());\n+\n+    \/\/ Update next's start\n+    tree_update(next_entry, new_vmem);\n+\n+    return;\n+  }\n+\n+  tree_insert(current_cursor, vmem);\n+}\n+\n+ZVirtualMemory ZMappedCache::remove_contiguous(size_t size) {\n+  precond(size > 0);\n+  precond(is_aligned(size, ZGranuleSize));\n+\n+  ZVirtualMemory result;\n+\n+  const auto select_size_fn = [&](size_t) {\n+    \/\/ We always select the size\n+    return size;\n+  };\n+\n+  const auto consume_vmem_fn = [&](ZVirtualMemory vmem) {\n+    assert(result.is_null(), \"only consume once\");\n+    assert(vmem.size() == size, \"wrong size consumed\");\n+\n+    result = vmem;\n+\n+    \/\/ Only require one vmem\n+    return true;\n+  };\n+\n+  if (size == ZPageSizeSmall) {\n+    \/\/ Small page allocations allocate at the lowest possible address\n+    scan_remove_vmem<RemovalStrategy::LowestAddress>(size, select_size_fn, consume_vmem_fn);\n+  } else {\n+    \/\/ Other sizes uses approximate best fit size classes first\n+    scan_remove_vmem<RemovalStrategy::SizeClasses>(size, select_size_fn, consume_vmem_fn);\n+  }\n+\n+  return result;\n+}\n+\n+size_t ZMappedCache::remove_discontiguous(size_t size, ZArray<ZVirtualMemory>* out) {\n+  return remove_discontiguous_with_strategy<RemovalStrategy::SizeClasses>(size, out);\n+}\n+\n+size_t ZMappedCache::reset_min() {\n+  const size_t old_min = _min;\n+\n+  _min = _size;\n+\n+  return old_min;\n+}\n+\n+size_t ZMappedCache::remove_from_min(size_t max_size, ZArray<ZVirtualMemory>* out) {\n+  const size_t size = MIN2(_min, max_size);\n+\n+  if (size == 0) {\n+    return 0;\n+  }\n+\n+  return remove_discontiguous_with_strategy<RemovalStrategy::HighestAddress>(size, out);\n+}\n+\n+void ZMappedCache::print_on(outputStream* st) const {\n+  \/\/ This may be called from error printing where we may not hold the lock, so\n+  \/\/ values may be inconsistent. As such we read the _entry_count only once. And\n+  \/\/ use is_empty_error_reporter_safe and size_error_reporter_safe on the size\n+  \/\/ class lists.\n+  const size_t entry_count = Atomic::load(&_entry_count);\n+\n+  st->print(\"Cache\");\n+  st->fill_to(17);\n+  st->print_cr(\"%zuM (%zu)\", _size \/ M, entry_count);\n+\n+  if (entry_count == 0) {\n+    \/\/ Empty cache, skip printing size classes\n+    return;\n+  }\n+\n+  \/\/ Aggregate the number of size class entries\n+  size_t size_class_entry_count = 0;\n+  for (int index = 0; index < NumSizeClasses; ++index) {\n+    size_class_entry_count += _size_class_lists[index].size_error_reporter_safe();\n+  }\n+\n+  \/\/ Print information on size classes\n+  streamIndentor indentor(st, 1);\n+\n+  st->print(\"size classes\");\n+  st->fill_to(17);\n+\n+  \/\/ Print the number of entries smaller than the min size class's size\n+  const size_t small_entry_size_count = entry_count - size_class_entry_count;\n+  bool first = true;\n+  if (small_entry_size_count != 0) {\n+    st->print(EXACTFMT \" (%zu)\", EXACTFMTARGS(ZGranuleSize), small_entry_size_count);\n+    first = false;\n+  }\n+\n+  for (int index = 0; index < NumSizeClasses; ++index) {\n+    const ZList<ZSizeClassListNode>& list = _size_class_lists[index];\n+    if (!list.is_empty_error_reporter_safe()) {\n+      const int shift = index + MinSizeClassShift + (int)ZGranuleSizeShift;\n+      const size_t size = (size_t)1 << shift;\n+\n+      st->print(\"%s\" EXACTFMT \" (%zu)\", first ? \"\" : \", \", EXACTFMTARGS(size), list.size_error_reporter_safe());\n+      first = false;\n+    }\n+  }\n+\n+  st->cr();\n+}\n+\n+void ZMappedCache::print_extended_on(outputStream* st) const {\n+  \/\/ Print the ranges and size of all nodes in the tree\n+  for (ZMappedCache::TreeNode* node = _tree.first(); node != nullptr; node = node->next()) {\n+    const ZVirtualMemory vmem = ZMappedCacheEntry::cast_to_entry(node)->vmem();\n+\n+    st->print_cr(PTR_FORMAT \" \" PTR_FORMAT \" \" EXACTFMT,\n+                 untype(vmem.start()), untype(vmem.end()), EXACTFMTARGS(vmem.size()));\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.cpp","additions":629,"deletions":0,"binary":false,"changes":629,"status":"added"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZMAPPEDCACHE_HPP\n+#define SHARE_GC_Z_ZMAPPEDCACHE_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zArray.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zIntrusiveRBTree.hpp\"\n+#include \"gc\/z\/zList.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+class ZMappedCacheEntry;\n+class ZVirtualMemory;\n+\n+class ZMappedCache {\n+  friend class ZMappedCacheEntry;\n+\n+private:\n+  struct EntryCompare {\n+    int operator()(ZIntrusiveRBTreeNode* a, ZIntrusiveRBTreeNode* b);\n+    int operator()(zoffset key, ZIntrusiveRBTreeNode* node);\n+  };\n+\n+  struct ZSizeClassListNode {\n+    ZListNode<ZSizeClassListNode> _node;\n+  };\n+\n+  using Tree              = ZIntrusiveRBTree<zoffset, EntryCompare>;\n+  using TreeNode          = ZIntrusiveRBTreeNode;\n+  using SizeClassList     = ZList<ZSizeClassListNode>;\n+  using SizeClassListNode = ZSizeClassListNode;\n+\n+  \/\/ Maintain size class lists from 4MB to 16GB\n+  static constexpr int MaxLongArraySizeClassShift = 3 \/* 8 byte *\/ + 31 \/* max length *\/;\n+  static constexpr int MinSizeClassShift = 1;\n+  static constexpr int MaxSizeClassShift = MaxLongArraySizeClassShift - ZGranuleSizeShift;\n+  static constexpr int NumSizeClasses = MaxSizeClassShift - MinSizeClassShift + 1;\n+\n+  Tree          _tree;\n+  size_t        _entry_count;\n+  SizeClassList _size_class_lists[NumSizeClasses];\n+  size_t        _size;\n+  size_t        _min;\n+\n+  static int size_class_index(size_t size);\n+  static int guaranteed_size_class_index(size_t size);\n+\n+  void tree_insert(const Tree::FindCursor& cursor, const ZVirtualMemory& vmem);\n+  void tree_remove(const Tree::FindCursor& cursor, const ZVirtualMemory& vmem);\n+  void tree_replace(const Tree::FindCursor& cursor, const ZVirtualMemory& vmem);\n+  void tree_update(ZMappedCacheEntry* entry, const ZVirtualMemory& vmem);\n+\n+  enum class RemovalStrategy {\n+    LowestAddress,\n+    HighestAddress,\n+    SizeClasses,\n+  };\n+\n+  template <RemovalStrategy strategy, typename SelectFunction>\n+  ZVirtualMemory remove_vmem(ZMappedCacheEntry* const entry, size_t min_size, SelectFunction select);\n+\n+  template <typename SelectFunction, typename ConsumeFunction>\n+  bool try_remove_vmem_size_class(size_t min_size, SelectFunction select, ConsumeFunction consume);\n+\n+  template <RemovalStrategy strategy, typename SelectFunction, typename ConsumeFunction>\n+  void scan_remove_vmem(size_t min_size, SelectFunction select, ConsumeFunction consume);\n+\n+  template <RemovalStrategy strategy, typename SelectFunction, typename ConsumeFunction>\n+  void scan_remove_vmem(SelectFunction select, ConsumeFunction consume);\n+\n+  template <RemovalStrategy strategy>\n+  size_t remove_discontiguous_with_strategy(size_t size, ZArray<ZVirtualMemory>* out);\n+\n+public:\n+  ZMappedCache();\n+\n+  void insert(const ZVirtualMemory& vmem);\n+\n+  ZVirtualMemory remove_contiguous(size_t size);\n+  size_t remove_discontiguous(size_t size, ZArray<ZVirtualMemory>* out);\n+\n+  size_t reset_min();\n+  size_t remove_from_min(size_t max_size, ZArray<ZVirtualMemory>* out);\n+\n+  void print_on(outputStream* st) const;\n+  void print_extended_on(outputStream* st) const;\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZMAPPEDCACHE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.hpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -1,284 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/z\/zList.inline.hpp\"\n-#include \"gc\/z\/zLock.inline.hpp\"\n-#include \"gc\/z\/zMemory.inline.hpp\"\n-\n-void ZMemoryManager::shrink_from_front(ZMemory* area, size_t size) {\n-  if (_callbacks._shrink != nullptr) {\n-    const ZMemory* from = area;\n-    const ZMemory to(area->start() + size, area->size() - size);\n-    _callbacks._shrink(*from, to);\n-  }\n-  area->shrink_from_front(size);\n-}\n-\n-void ZMemoryManager::shrink_from_back(ZMemory* area, size_t size) {\n-  if (_callbacks._shrink != nullptr) {\n-    const ZMemory* from = area;\n-    const ZMemory to(area->start(), area->size() - size);\n-    _callbacks._shrink(*from, to);\n-  }\n-  area->shrink_from_back(size);\n-}\n-\n-void ZMemoryManager::grow_from_front(ZMemory* area, size_t size) {\n-  if (_callbacks._grow != nullptr) {\n-    const ZMemory* from = area;\n-    const ZMemory to(area->start() - size, area->size() + size);\n-    _callbacks._grow(*from, to);\n-  }\n-  area->grow_from_front(size);\n-}\n-\n-void ZMemoryManager::grow_from_back(ZMemory* area, size_t size) {\n-  if (_callbacks._grow != nullptr) {\n-    const ZMemory* from = area;\n-    const ZMemory to(area->start(), area->size() + size);\n-    _callbacks._grow(*from, to);\n-  }\n-  area->grow_from_back(size);\n-}\n-\n-ZMemoryManager::Callbacks::Callbacks()\n-  : _prepare_for_hand_out(nullptr),\n-    _prepare_for_hand_back(nullptr),\n-    _grow(nullptr),\n-    _shrink(nullptr) {}\n-\n-ZMemoryManager::ZMemoryManager()\n-  : _freelist(),\n-    _callbacks() {}\n-\n-bool ZMemoryManager::free_is_contiguous() const {\n-  return _freelist.size() == 1;\n-}\n-\n-void ZMemoryManager::register_callbacks(const Callbacks& callbacks) {\n-  _callbacks = callbacks;\n-}\n-\n-zoffset ZMemoryManager::peek_low_address() const {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  const ZMemory* const area = _freelist.first();\n-  if (area != nullptr) {\n-    return area->start();\n-  }\n-\n-  \/\/ Out of memory\n-  return zoffset(UINTPTR_MAX);\n-}\n-\n-zoffset_end ZMemoryManager::peak_high_address_end() const {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  const ZMemory* const area = _freelist.last();\n-  if (area != nullptr) {\n-    return area->end();\n-  }\n-\n-  \/\/ Out of memory\n-  return zoffset_end(UINTPTR_MAX);\n-}\n-\n-zoffset ZMemoryManager::alloc_low_address(size_t size) {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  ZListIterator<ZMemory> iter(&_freelist);\n-  for (ZMemory* area; iter.next(&area);) {\n-    if (area->size() >= size) {\n-      zoffset start;\n-\n-      if (area->size() == size) {\n-        \/\/ Exact match, remove area\n-        start = area->start();\n-        _freelist.remove(area);\n-        delete area;\n-      } else {\n-        \/\/ Larger than requested, shrink area\n-        start = area->start();\n-        shrink_from_front(area, size);\n-      }\n-\n-      if (_callbacks._prepare_for_hand_out != nullptr) {\n-        _callbacks._prepare_for_hand_out(ZMemory(start, size));\n-      }\n-\n-      return start;\n-    }\n-  }\n-\n-  \/\/ Out of memory\n-  return zoffset(UINTPTR_MAX);\n-}\n-\n-zoffset ZMemoryManager::alloc_low_address_at_most(size_t size, size_t* allocated) {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  ZMemory* const area = _freelist.first();\n-  if (area != nullptr) {\n-    const zoffset start = area->start();\n-\n-    if (area->size() <= size) {\n-      \/\/ Smaller than or equal to requested, remove area\n-      _freelist.remove(area);\n-      *allocated = area->size();\n-      delete area;\n-    } else {\n-      \/\/ Larger than requested, shrink area\n-      shrink_from_front(area, size);\n-      *allocated = size;\n-    }\n-\n-    if (_callbacks._prepare_for_hand_out != nullptr) {\n-      _callbacks._prepare_for_hand_out(ZMemory(start, *allocated));\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/ Out of memory\n-  *allocated = 0;\n-  return zoffset(UINTPTR_MAX);\n-}\n-\n-zoffset ZMemoryManager::alloc_high_address(size_t size) {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  ZListReverseIterator<ZMemory> iter(&_freelist);\n-  for (ZMemory* area; iter.next(&area);) {\n-    if (area->size() >= size) {\n-      zoffset start;\n-\n-      if (area->size() == size) {\n-        \/\/ Exact match, remove area\n-        start = area->start();\n-        _freelist.remove(area);\n-        delete area;\n-      } else {\n-        \/\/ Larger than requested, shrink area\n-        shrink_from_back(area, size);\n-        start = to_zoffset(area->end());\n-      }\n-\n-      if (_callbacks._prepare_for_hand_out != nullptr) {\n-        _callbacks._prepare_for_hand_out(ZMemory(start, size));\n-      }\n-\n-      return start;\n-    }\n-  }\n-\n-  \/\/ Out of memory\n-  return zoffset(UINTPTR_MAX);\n-}\n-\n-void ZMemoryManager::move_into(zoffset start, size_t size) {\n-  assert(start != zoffset(UINTPTR_MAX), \"Invalid address\");\n-  const zoffset_end end = to_zoffset_end(start, size);\n-\n-  ZListIterator<ZMemory> iter(&_freelist);\n-  for (ZMemory* area; iter.next(&area);) {\n-    if (start < area->start()) {\n-      ZMemory* const prev = _freelist.prev(area);\n-      if (prev != nullptr && start == prev->end()) {\n-        if (end == area->start()) {\n-          \/\/ Merge with prev and current area\n-          grow_from_back(prev, size + area->size());\n-          _freelist.remove(area);\n-          delete area;\n-        } else {\n-          \/\/ Merge with prev area\n-          grow_from_back(prev, size);\n-        }\n-      } else if (end == area->start()) {\n-        \/\/ Merge with current area\n-        grow_from_front(area, size);\n-      } else {\n-        \/\/ Insert new area before current area\n-        assert(end < area->start(), \"Areas must not overlap\");\n-        ZMemory* const new_area = new ZMemory(start, size);\n-        _freelist.insert_before(area, new_area);\n-      }\n-\n-      \/\/ Done\n-      return;\n-    }\n-  }\n-\n-  \/\/ Insert last\n-  ZMemory* const last = _freelist.last();\n-  if (last != nullptr && start == last->end()) {\n-    \/\/ Merge with last area\n-    grow_from_back(last, size);\n-  } else {\n-    \/\/ Insert new area last\n-    ZMemory* const new_area = new ZMemory(start, size);\n-    _freelist.insert_last(new_area);\n-  }\n-}\n-\n-void ZMemoryManager::free(zoffset start, size_t size) {\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  if (_callbacks._prepare_for_hand_back != nullptr) {\n-    _callbacks._prepare_for_hand_back(ZMemory(start, size));\n-  }\n-\n-  move_into(start, size);\n-}\n-\n-void ZMemoryManager::register_range(zoffset start, size_t size) {\n-  \/\/ Note that there's no need to call the _prepare_for_hand_back when memory\n-  \/\/ is added the first time. We don't have to undo the effects of a previous\n-  \/\/ _prepare_for_hand_out callback.\n-\n-  \/\/ No need to lock during initialization.\n-\n-  move_into(start, size);\n-}\n-\n-bool ZMemoryManager::unregister_first(zoffset* start_out, size_t* size_out) {\n-  \/\/ Note that this doesn't hand out memory to be used, so we don't call the\n-  \/\/ _prepare_for_hand_out callback.\n-\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  if (_freelist.is_empty()) {\n-    return false;\n-  }\n-\n-  \/\/ Don't invoke the _prepare_for_hand_out callback\n-\n-  ZMemory* const area = _freelist.remove_first();\n-\n-  \/\/ Return the range\n-  *start_out = area->start();\n-  *size_out  = area->size();\n-\n-  delete area;\n-\n-  return true;\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.cpp","additions":0,"deletions":284,"binary":false,"changes":284,"status":"deleted"},{"patch":"@@ -1,104 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZMEMORY_HPP\n-#define SHARE_GC_Z_ZMEMORY_HPP\n-\n-#include \"gc\/z\/zAddress.hpp\"\n-#include \"gc\/z\/zList.hpp\"\n-#include \"gc\/z\/zLock.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class ZMemory : public CHeapObj<mtGC> {\n-  friend class ZList<ZMemory>;\n-\n-private:\n-  zoffset            _start;\n-  zoffset_end        _end;\n-  ZListNode<ZMemory> _node;\n-\n-public:\n-  ZMemory(zoffset start, size_t size);\n-\n-  zoffset start() const;\n-  zoffset_end end() const;\n-  size_t size() const;\n-\n-  bool operator==(const ZMemory& other) const;\n-  bool operator!=(const ZMemory& other) const;\n-\n-  bool contains(const ZMemory& other) const;\n-\n-  void shrink_from_front(size_t size);\n-  void shrink_from_back(size_t size);\n-  void grow_from_front(size_t size);\n-  void grow_from_back(size_t size);\n-};\n-\n-class ZMemoryManager {\n-  friend class ZVirtualMemoryManagerTest;\n-\n-public:\n-  typedef void (*CallbackPrepare)(const ZMemory& area);\n-  typedef void (*CallbackResize)(const ZMemory& from, const ZMemory& to);\n-\n-  struct Callbacks {\n-    CallbackPrepare _prepare_for_hand_out;\n-    CallbackPrepare _prepare_for_hand_back;\n-    CallbackResize  _grow;\n-    CallbackResize  _shrink;\n-\n-    Callbacks();\n-  };\n-\n-private:\n-  mutable ZLock  _lock;\n-  ZList<ZMemory> _freelist;\n-  Callbacks      _callbacks;\n-\n-  void shrink_from_front(ZMemory* area, size_t size);\n-  void shrink_from_back(ZMemory* area, size_t size);\n-  void grow_from_front(ZMemory* area, size_t size);\n-  void grow_from_back(ZMemory* area, size_t size);\n-\n-  void move_into(zoffset start, size_t size);\n-\n-public:\n-  ZMemoryManager();\n-\n-  bool free_is_contiguous() const;\n-\n-  void register_callbacks(const Callbacks& callbacks);\n-\n-  zoffset peek_low_address() const;\n-  zoffset_end peak_high_address_end() const;\n-  zoffset alloc_low_address(size_t size);\n-  zoffset alloc_low_address_at_most(size_t size, size_t* allocated);\n-  zoffset alloc_high_address(size_t size);\n-\n-  void free(zoffset start, size_t size);\n-  void register_range(zoffset start, size_t size);\n-  bool unregister_first(zoffset* start_out, size_t* size_out);\n-};\n-\n-#endif \/\/ SHARE_GC_Z_ZMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.hpp","additions":0,"deletions":104,"binary":false,"changes":104,"status":"deleted"},{"patch":"@@ -1,80 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZMEMORY_INLINE_HPP\n-#define SHARE_GC_Z_ZMEMORY_INLINE_HPP\n-\n-#include \"gc\/z\/zMemory.hpp\"\n-\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"gc\/z\/zList.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline ZMemory::ZMemory(zoffset start, size_t size)\n-  : _start(start),\n-    _end(to_zoffset_end(start, size)) {}\n-\n-inline zoffset ZMemory::start() const {\n-  return _start;\n-}\n-\n-inline zoffset_end ZMemory::end() const {\n-  return _end;\n-}\n-\n-inline size_t ZMemory::size() const {\n-  return end() - start();\n-}\n-\n-inline bool ZMemory::operator==(const ZMemory& other) const {\n-  return _start == other._start && _end == other._end;\n-}\n-\n-inline bool ZMemory::operator!=(const ZMemory& other) const {\n-  return !operator==(other);\n-}\n-\n-inline bool ZMemory::contains(const ZMemory& other) const {\n-  return _start <= other._start && other.end() <= end();\n-}\n-\n-inline void ZMemory::shrink_from_front(size_t size) {\n-  assert(this->size() > size, \"Too small\");\n-  _start += size;\n-}\n-\n-inline void ZMemory::shrink_from_back(size_t size) {\n-  assert(this->size() > size, \"Too small\");\n-  _end -= size;\n-}\n-\n-inline void ZMemory::grow_from_front(size_t size) {\n-  assert(size_t(start()) >= size, \"Too big\");\n-  _start -= size;\n-}\n-\n-inline void ZMemory::grow_from_back(size_t size) {\n-  _end += size;\n-}\n-\n-#endif \/\/ SHARE_GC_Z_ZMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.inline.hpp","additions":0,"deletions":80,"binary":false,"changes":80,"status":"deleted"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n@@ -63,1 +62,1 @@\n-void ZNMT::commit(zoffset offset, size_t size) {\n+void ZNMT::commit(zbacking_offset offset, size_t size) {\n@@ -67,1 +66,1 @@\n-void ZNMT::uncommit(zoffset offset, size_t size) {\n+void ZNMT::uncommit(zbacking_offset offset, size_t size) {\n@@ -71,1 +70,1 @@\n-void ZNMT::map(zaddress_unsafe addr, size_t size, zoffset offset) {\n+void ZNMT::map(zaddress_unsafe addr, size_t size, zbacking_offset offset) {\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,2 +29,0 @@\n-#include \"gc\/z\/zMemory.hpp\"\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n@@ -35,1 +33,0 @@\n-#include \"utilities\/nativeCallStack.hpp\"\n@@ -47,2 +44,2 @@\n-  static void commit(zoffset offset, size_t size);\n-  static void uncommit(zoffset offset, size_t size);\n+  static void commit(zbacking_offset offset, size_t size);\n+  static void uncommit(zbacking_offset offset, size_t size);\n@@ -50,1 +47,1 @@\n-  static void map(zaddress_unsafe addr, size_t size, zoffset offset);\n+  static void map(zaddress_unsafe addr, size_t size, zbacking_offset offset);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.hpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -25,1 +26,2 @@\n-#include \"gc\/z\/zNUMA.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n@@ -34,0 +36,1 @@\n+\n@@ -35,0 +38,1 @@\n+    assert(!is_faked(), \"Currently not supported\");\n@@ -36,0 +40,3 @@\n+\n+  } else if (is_faked()) {\n+    log_info_p(gc, init)(\"Fake NUMA Nodes: %u\", count());\n@@ -40,0 +47,4 @@\n+  if (is_faked()) {\n+    return \"Faked\";\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zNUMA.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zGlobals.hpp\"\n@@ -31,0 +32,4 @@\n+  friend class VMStructs;\n+  friend class ZNUMATest;\n+  friend class ZTest;\n+\n@@ -39,0 +44,1 @@\n+\n@@ -40,0 +46,1 @@\n+  static bool is_faked();\n@@ -46,0 +53,2 @@\n+  static size_t calculate_share(uint32_t numa_id, size_t total, size_t granule = ZGranuleSize, uint32_t ignore_count = 0);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zNUMA.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -29,0 +29,4 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n@@ -33,0 +37,4 @@\n+inline bool ZNUMA::is_faked() {\n+  return ZFakeNUMA > 1;\n+}\n+\n@@ -37,0 +45,16 @@\n+inline size_t ZNUMA::calculate_share(uint32_t numa_id, size_t total, size_t granule, uint32_t ignore_count) {\n+  assert(total % granule == 0, \"total must be divisible by granule\");\n+  assert(ignore_count < count(), \"must not ignore all nodes\");\n+  assert(numa_id < count() - ignore_count, \"numa_id must be in bounds\");\n+\n+  const uint32_t num_nodes = count() - ignore_count;\n+  const size_t base_share = ((total \/ num_nodes) \/ granule) * granule;\n+\n+  const size_t extra_share_nodes = (total - base_share * num_nodes) \/ granule;\n+  if (numa_id < extra_share_nodes) {\n+    return base_share + granule;\n+  }\n+\n+  return base_share;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zNUMA.inline.hpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -141,4 +141,4 @@\n-    \/\/ When a new medium page is required, we synchronize the allocation\n-    \/\/ of the new page using a lock. This is to avoid having multiple\n-    \/\/ threads requesting a medium page from the page cache when we know\n-    \/\/ only one of the will succeed in installing the page at this layer.\n+    \/\/ When a new medium page is required, we synchronize the allocation of the\n+    \/\/ new page using a lock. This is to avoid having multiple threads allocate\n+    \/\/ medium pages when we know only one of them will succeed in installing\n+    \/\/ the page at this layer.\n","filename":"src\/hotspot\/share\/gc\/z\/zObjectAllocator.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"gc\/z\/zList.inline.hpp\"\n@@ -28,1 +27,1 @@\n-#include \"gc\/z\/zPhysicalMemory.inline.hpp\"\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -30,1 +29,0 @@\n-#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n@@ -33,1 +31,0 @@\n-#include \"utilities\/growableArray.hpp\"\n@@ -35,1 +32,1 @@\n-ZPage::ZPage(ZPageType type, const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem)\n+ZPage::ZPage(ZPageType type, ZPageAge age, const ZVirtualMemory& vmem, ZMultiPartitionTracker* multi_partition_tracker, uint32_t partition_id)\n@@ -37,5 +34,5 @@\n-    _generation_id(ZGenerationId::young),\n-    _age(ZPageAge::eden),\n-    _numa_id((uint8_t)-1),\n-    _seqnum(0),\n-    _seqnum_other(0),\n+    _generation_id(\/* set in reset *\/),\n+    _age(\/* set in reset *\/),\n+    _seqnum(\/* set in reset *\/),\n+    _seqnum_other(\/* set in reset *\/),\n+    _single_partition_id(partition_id),\n@@ -46,3 +43,1 @@\n-    _last_used(0),\n-    _physical(pmem),\n-    _node() {\n+    _multi_partition_tracker(multi_partition_tracker) {\n@@ -50,2 +45,0 @@\n-  assert(!_physical.is_null(), \"Should not be null\");\n-  assert(_virtual.size() == _physical.size(), \"Virtual\/Physical size mismatch\");\n@@ -56,0 +49,5 @@\n+  reset(age);\n+\n+  if (is_old()) {\n+    remset_alloc();\n+  }\n@@ -58,1 +56,8 @@\n-ZPage* ZPage::clone_limited() const {\n+ZPage::ZPage(ZPageType type, ZPageAge age, const ZVirtualMemory& vmem, uint32_t partition_id)\n+  : ZPage(type, age, vmem, nullptr \/* multi_partition_tracker *\/, partition_id) {}\n+\n+ZPage::ZPage(ZPageType type, ZPageAge age, const ZVirtualMemory& vmem, ZMultiPartitionTracker* multi_partition_tracker)\n+  : ZPage(type, age, vmem, multi_partition_tracker, -1u \/* partition_id *\/) {}\n+\n+ZPage* ZPage::clone_for_promotion() const {\n+  assert(_age != ZPageAge::old, \"must be used for promotion\");\n@@ -61,1 +66,1 @@\n-  ZPage* const page = new ZPage(_type, _virtual, _physical);\n+  ZPage* const page = new ZPage(_type, ZPageAge::old, _virtual, _multi_partition_tracker, _single_partition_id);\n@@ -88,5 +93,1 @@\n-void ZPage::remset_delete() {\n-  _remembered_set.delete_all();\n-}\n-\n-void ZPage::reset(ZPageAge age) {\n+ZPage* ZPage::reset(ZPageAge age) {\n@@ -94,1 +95,0 @@\n-  _last_used = 0;\n@@ -101,0 +101,2 @@\n+\n+  return this;\n@@ -111,53 +113,0 @@\n-void ZPage::reset_type_and_size(ZPageType type) {\n-  _type = type;\n-  _livemap.resize(object_max_count());\n-}\n-\n-ZPage* ZPage::retype(ZPageType type) {\n-  assert(_type != type, \"Invalid retype\");\n-  reset_type_and_size(type);\n-  return this;\n-}\n-\n-ZPage* ZPage::split(size_t split_of_size) {\n-  return split(type_from_size(split_of_size), split_of_size);\n-}\n-\n-ZPage* ZPage::split_with_pmem(ZPageType type, const ZPhysicalMemory& pmem) {\n-  \/\/ Resize this page\n-  const ZVirtualMemory vmem = _virtual.split(pmem.size());\n-  assert(vmem.end() == _virtual.start(), \"Should be consecutive\");\n-\n-  reset_type_and_size(type_from_size(_virtual.size()));\n-\n-  log_trace(gc, page)(\"Split page [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-      untype(vmem.start()),\n-      untype(vmem.end()),\n-      untype(_virtual.end()));\n-\n-  \/\/ Create new page\n-  return new ZPage(type, vmem, pmem);\n-}\n-\n-ZPage* ZPage::split(ZPageType type, size_t split_of_size) {\n-  assert(_virtual.size() > split_of_size, \"Invalid split\");\n-\n-  const ZPhysicalMemory pmem = _physical.split(split_of_size);\n-\n-  return split_with_pmem(type, pmem);\n-}\n-\n-ZPage* ZPage::split_committed() {\n-  \/\/ Split any committed part of this page into a separate page,\n-  \/\/ leaving this page with only uncommitted physical memory.\n-  const ZPhysicalMemory pmem = _physical.split_committed();\n-  if (pmem.is_null()) {\n-    \/\/ Nothing committed\n-    return nullptr;\n-  }\n-\n-  assert(!_physical.is_null(), \"Should not be null\");\n-\n-  return split_with_pmem(type_from_size(pmem.size()), pmem);\n-}\n-\n@@ -218,2 +167,2 @@\n-void ZPage::print_on_msg(outputStream* out, const char* msg) const {\n-  out->print_cr(\" %-6s  \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %s\/%-4u %s%s%s\",\n+void ZPage::print_on_msg(outputStream* st, const char* msg) const {\n+  st->print_cr(\"%-6s  \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %s\/%-4u %s%s%s%s\",\n@@ -223,1 +172,0 @@\n-                is_allocating()  ? \" Allocating \" : \"\",\n@@ -225,1 +173,3 @@\n-                msg == nullptr ? \"\" : msg);\n+                is_allocating()  ? \" Allocating\"  : \"\",\n+                is_allocating() && msg != nullptr ? \" \" : \"\",\n+                msg != nullptr ? msg : \"\");\n@@ -228,2 +178,2 @@\n-void ZPage::print_on(outputStream* out) const {\n-  print_on_msg(out, nullptr);\n+void ZPage::print_on(outputStream* st) const {\n+  print_on_msg(st, nullptr);\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.cpp","additions":32,"deletions":82,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,0 @@\n-#include \"gc\/z\/zList.hpp\"\n@@ -32,1 +31,0 @@\n-#include \"gc\/z\/zPhysicalMemory.hpp\"\n@@ -36,0 +34,1 @@\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -38,0 +37,1 @@\n+class ZMultiPartitionTracker;\n@@ -41,1 +41,0 @@\n-  friend class ZList<ZPage>;\n@@ -45,15 +44,12 @@\n-  ZPageType            _type;\n-  ZGenerationId        _generation_id;\n-  ZPageAge             _age;\n-  uint8_t              _numa_id;\n-  uint32_t             _seqnum;\n-  uint32_t             _seqnum_other;\n-  ZVirtualMemory       _virtual;\n-  volatile zoffset_end _top;\n-  ZLiveMap             _livemap;\n-  ZRememberedSet       _remembered_set;\n-  uint64_t             _last_used;\n-  ZPhysicalMemory      _physical;\n-  ZListNode<ZPage>     _node;\n-\n-  ZPageType type_from_size(size_t size) const;\n+  const ZPageType               _type;\n+  ZGenerationId                 _generation_id;\n+  ZPageAge                      _age;\n+  uint32_t                      _seqnum;\n+  uint32_t                      _seqnum_other;\n+  const uint32_t                _single_partition_id;\n+  const ZVirtualMemory          _virtual;\n+  volatile zoffset_end          _top;\n+  ZLiveMap                      _livemap;\n+  ZRememberedSet                _remembered_set;\n+  ZMultiPartitionTracker* const _multi_partition_tracker;\n+\n@@ -74,1 +70,1 @@\n-  ZPage* split_with_pmem(ZPageType type, const ZPhysicalMemory& pmem);\n+  ZPage(ZPageType type, ZPageAge age, const ZVirtualMemory& vmem, ZMultiPartitionTracker* multi_partition_tracker, uint32_t partition_id);\n@@ -77,1 +73,2 @@\n-  ZPage(ZPageType type, const ZVirtualMemory& vmem, const ZPhysicalMemory& pmem);\n+  ZPage(ZPageType type, ZPageAge age, const ZVirtualMemory& vmem, uint32_t partition_id);\n+  ZPage(ZPageType type, ZPageAge age, const ZVirtualMemory& vmem, ZMultiPartitionTracker* multi_partition_tracker);\n@@ -79,1 +76,1 @@\n-  ZPage* clone_limited() const;\n+  ZPage* clone_for_promotion() const;\n@@ -102,2 +99,0 @@\n-  const ZPhysicalMemory& physical_memory() const;\n-  ZPhysicalMemory& physical_memory();\n@@ -105,1 +100,4 @@\n-  uint8_t numa_id();\n+  uint32_t single_partition_id() const;\n+  bool is_multi_partition() const;\n+  ZMultiPartitionTracker* multi_partition_tracker() const;\n+\n@@ -112,4 +110,1 @@\n-  uint64_t last_used() const;\n-  void set_last_used();\n-\n-  void reset(ZPageAge age);\n+  ZPage* reset(ZPageAge age);\n@@ -118,6 +113,0 @@\n-  void reset_type_and_size(ZPageType type);\n-\n-  ZPage* retype(ZPageType type);\n-  ZPage* split(size_t split_of_size);\n-  ZPage* split(ZPageType type, size_t split_of_size);\n-  ZPage* split_committed();\n@@ -159,1 +148,0 @@\n-  void remset_delete();\n@@ -196,2 +184,2 @@\n-  void print_on_msg(outputStream* out, const char* msg) const;\n-  void print_on(outputStream* out) const;\n+  void print_on_msg(outputStream* st, const char* msg) const;\n+  void print_on(outputStream* st) const;\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.hpp","additions":26,"deletions":38,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -33,2 +33,0 @@\n-#include \"gc\/z\/zNUMA.hpp\"\n-#include \"gc\/z\/zPhysicalMemory.inline.hpp\"\n@@ -36,1 +34,0 @@\n-#include \"gc\/z\/zUtils.inline.hpp\"\n@@ -42,1 +39,0 @@\n-#include \"utilities\/checkedCast.hpp\"\n@@ -45,10 +41,0 @@\n-inline ZPageType ZPage::type_from_size(size_t size) const {\n-  if (size == ZPageSizeSmall) {\n-    return ZPageType::small;\n-  } else if (size == ZPageSizeMedium) {\n-    return ZPageType::medium;\n-  } else {\n-    return ZPageType::large;\n-  }\n-}\n-\n@@ -173,2 +159,2 @@\n-inline const ZPhysicalMemory& ZPage::physical_memory() const {\n-  return _physical;\n+inline uint32_t ZPage::single_partition_id() const {\n+  return _single_partition_id;\n@@ -177,2 +163,2 @@\n-inline ZPhysicalMemory& ZPage::physical_memory() {\n-  return _physical;\n+inline bool ZPage::is_multi_partition() const {\n+  return _multi_partition_tracker != nullptr;\n@@ -181,6 +167,2 @@\n-inline uint8_t ZPage::numa_id() {\n-  if (_numa_id == (uint8_t)-1) {\n-    _numa_id = checked_cast<uint8_t>(ZNUMA::memory_id(untype(ZOffset::address(start()))));\n-  }\n-\n-  return _numa_id;\n+inline ZMultiPartitionTracker* ZPage::multi_partition_tracker() const {\n+  return _multi_partition_tracker;\n@@ -205,8 +187,0 @@\n-inline uint64_t ZPage::last_used() const {\n-  return _last_used;\n-}\n-\n-inline void ZPage::set_last_used() {\n-  _last_used = (uint64_t)ceil(os::elapsedTime());\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.inline.hpp","additions":6,"deletions":32,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zAllocationFlags.hpp\"\n@@ -34,0 +36,2 @@\n+#include \"gc\/z\/zMappedCache.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n@@ -37,1 +41,2 @@\n-#include \"gc\/z\/zPageCache.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n+#include \"gc\/z\/zPhysicalMemoryManager.hpp\"\n@@ -42,1 +47,3 @@\n-#include \"gc\/z\/zUnmapper.hpp\"\n+#include \"gc\/z\/zValue.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.inline.hpp\"\n@@ -46,0 +53,2 @@\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memTag.hpp\"\n@@ -50,0 +59,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -52,0 +62,6 @@\n+#include \"utilities\/ticks.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+\n+#include <cmath>\n+\n+class ZMemoryAllocation;\n@@ -54,1 +70,1 @@\n-static const ZStatCounter       ZCounterPageCacheFlush(\"Memory\", \"Page Cache Flush\", ZStatUnitBytesPerSecond);\n+static const ZStatCounter       ZCounterMappedCacheHarvest(\"Memory\", \"Mapped Cache Harvest\", ZStatUnitBytesPerSecond);\n@@ -58,6 +74,8 @@\n-ZSafePageRecycle::ZSafePageRecycle(ZPageAllocator* page_allocator)\n-  : _page_allocator(page_allocator),\n-    _unsafe_to_recycle() {}\n-\n-void ZSafePageRecycle::activate() {\n-  _unsafe_to_recycle.activate();\n+static void check_numa_mismatch(const ZVirtualMemory& vmem, uint32_t desired_id) {\n+  if (ZNUMA::is_enabled()) {\n+    \/\/ Check if memory ended up on desired NUMA node or not\n+    const uint32_t actual_id = ZNUMA::memory_id(untype(ZOffset::address(vmem.start())));\n+    if (actual_id != desired_id) {\n+      log_debug(gc, heap)(\"NUMA Mismatch: desired %d, actual %d\", desired_id, actual_id);\n+    }\n+  }\n@@ -66,4 +84,23 @@\n-void ZSafePageRecycle::deactivate() {\n-  auto delete_function = [&](ZPage* page) {\n-    _page_allocator->safe_destroy_page(page);\n-  };\n+class ZMemoryAllocation : public CHeapObj<mtGC> {\n+private:\n+  const size_t           _size;\n+  ZPartition*            _partition;\n+  ZVirtualMemory         _satisfied_from_cache_vmem;\n+  ZArray<ZVirtualMemory> _partial_vmems;\n+  int                    _num_harvested;\n+  size_t                 _harvested;\n+  size_t                 _increased_capacity;\n+  size_t                 _committed_capacity;\n+  bool                   _commit_failed;\n+\n+  explicit ZMemoryAllocation(const ZMemoryAllocation& other)\n+    : ZMemoryAllocation(other._size) {\n+    \/\/ Transfer the partition\n+    set_partition(other._partition);\n+\n+    \/\/ Reserve space for the partial vmems\n+    _partial_vmems.reserve(other._partial_vmems.length() + (other._satisfied_from_cache_vmem.is_null() ? 1 : 0));\n+\n+    \/\/ Transfer the claimed capacity\n+    transfer_claimed_capacity(other);\n+  }\n@@ -71,2 +108,5 @@\n-  _unsafe_to_recycle.deactivate_and_apply(delete_function);\n-}\n+  ZMemoryAllocation(const ZMemoryAllocation& a1, const ZMemoryAllocation& a2)\n+    : ZMemoryAllocation(a1._size + a2._size) {\n+    \/\/ Transfer the partition\n+    assert(a1._partition == a2._partition, \"only merge with same partition\");\n+    set_partition(a1._partition);\n@@ -74,5 +114,8 @@\n-ZPage* ZSafePageRecycle::register_and_clone_if_activated(ZPage* page) {\n-  if (!_unsafe_to_recycle.is_activated()) {\n-    \/\/ The page has no concurrent readers.\n-    \/\/ Recycle original page.\n-    return page;\n+    \/\/ Reserve space for the partial vmems\n+    const int num_vmems_a1 = a1._partial_vmems.length() + (a1._satisfied_from_cache_vmem.is_null() ? 1 : 0);\n+    const int num_vmems_a2 = a2._partial_vmems.length() + (a2._satisfied_from_cache_vmem.is_null() ? 1 : 0);\n+    _partial_vmems.reserve(num_vmems_a1 + num_vmems_a2);\n+\n+    \/\/ Transfer the claimed capacity\n+    transfer_claimed_capacity(a1);\n+    transfer_claimed_capacity(a2);\n@@ -81,2 +124,6 @@\n-  \/\/ The page could have concurrent readers.\n-  \/\/ It would be unsafe to recycle this page at this point.\n+  void transfer_claimed_capacity(const ZMemoryAllocation& from) {\n+    assert(from._committed_capacity == 0, \"Unexpected value %zu\", from._committed_capacity);\n+    assert(!from._commit_failed, \"Unexpected value\");\n+\n+    \/\/ Transfer increased capacity\n+    _increased_capacity += from._increased_capacity;\n@@ -84,9 +131,12 @@\n-  \/\/ As soon as the page is added to _unsafe_to_recycle, it\n-  \/\/ must not be used again. Hence, the extra double-checked\n-  \/\/ locking to only clone the page if it is believed to be\n-  \/\/ unsafe to recycle the page.\n-  ZPage* const cloned_page = page->clone_limited();\n-  if (!_unsafe_to_recycle.add_if_activated(page)) {\n-    \/\/ It became safe to recycle the page after the is_activated check\n-    delete cloned_page;\n-    return page;\n+    \/\/ Transfer satisfying vmem or partial mappings\n+    const ZVirtualMemory vmem = from._satisfied_from_cache_vmem;\n+    if (!vmem.is_null()) {\n+      assert(_partial_vmems.is_empty(), \"Must either have result or partial vmems\");\n+      _partial_vmems.push(vmem);\n+      _num_harvested += 1;\n+      _harvested += vmem.size();\n+    } else {\n+      _partial_vmems.appendAll(&from._partial_vmems);\n+      _num_harvested += from._num_harvested;\n+      _harvested += from._harvested;\n+    }\n@@ -95,4 +145,246 @@\n-  \/\/ The original page has been registered to be deleted by another thread.\n-  \/\/ Recycle the cloned page.\n-  return cloned_page;\n-}\n+public:\n+  explicit ZMemoryAllocation(size_t size)\n+    : _size(size),\n+      _partition(nullptr),\n+      _satisfied_from_cache_vmem(),\n+      _partial_vmems(0),\n+      _num_harvested(0),\n+      _harvested(0),\n+      _increased_capacity(0),\n+      _committed_capacity(0),\n+      _commit_failed(false) {}\n+\n+  void reset_for_retry() {\n+    assert(_satisfied_from_cache_vmem.is_null(), \"Incompatible with reset\");\n+\n+    _partition = nullptr;\n+    _partial_vmems.clear();\n+    _num_harvested = 0;\n+    _harvested = 0;\n+    _increased_capacity = 0;\n+    _committed_capacity = 0;\n+    _commit_failed = false;\n+  }\n+\n+  size_t size() const {\n+    return _size;\n+  }\n+\n+  ZPartition& partition() const {\n+    assert(_partition != nullptr, \"Should have been initialized\");\n+    return *_partition;\n+  }\n+\n+  void set_partition(ZPartition* partition) {\n+    assert(_partition == nullptr, \"Should be initialized only once\");\n+    _partition = partition;\n+  }\n+\n+  ZVirtualMemory satisfied_from_cache_vmem() const {\n+    return _satisfied_from_cache_vmem;\n+  }\n+\n+  void set_satisfied_from_cache_vmem(ZVirtualMemory vmem) {\n+    precond(_satisfied_from_cache_vmem.is_null());\n+    precond(vmem.size() == size());\n+    precond(_partial_vmems.is_empty());\n+\n+    _satisfied_from_cache_vmem = vmem;\n+  }\n+\n+  ZArray<ZVirtualMemory>* partial_vmems() {\n+    return &_partial_vmems;\n+  }\n+\n+  const ZArray<ZVirtualMemory>* partial_vmems() const {\n+    return &_partial_vmems;\n+  }\n+\n+  int num_harvested() const {\n+    return _num_harvested;\n+  }\n+\n+  size_t harvested() const {\n+    return _harvested;\n+  }\n+\n+  void set_harvested(int num_harvested, size_t harvested) {\n+    _num_harvested = num_harvested;\n+    _harvested = harvested;\n+  }\n+\n+  size_t increased_capacity() const {\n+    return _increased_capacity;\n+  }\n+\n+  void set_increased_capacity(size_t increased_capacity) {\n+    _increased_capacity = increased_capacity;\n+  }\n+\n+  size_t committed_capacity() const {\n+    return _committed_capacity;\n+  }\n+\n+  void set_committed_capacity(size_t committed_capacity) {\n+    assert(_committed_capacity == 0, \"Should only commit once\");\n+    _committed_capacity = committed_capacity;\n+    _commit_failed = committed_capacity != _increased_capacity;\n+  }\n+\n+  bool commit_failed() const {\n+    return _commit_failed;\n+  }\n+\n+  static void destroy(ZMemoryAllocation* allocation) {\n+    delete allocation;\n+  }\n+\n+  static void merge(const ZMemoryAllocation& allocation, ZMemoryAllocation** merge_location) {\n+    ZMemoryAllocation* const other_allocation = *merge_location;\n+    if (other_allocation == nullptr) {\n+      \/\/ First allocation, allocate new partition\n+      *merge_location = new ZMemoryAllocation(allocation);\n+    } else {\n+      \/\/ Merge with other allocation\n+      *merge_location = new ZMemoryAllocation(allocation, *other_allocation);\n+\n+      \/\/ Delete old allocation\n+      delete other_allocation;\n+    }\n+  }\n+};\n+\n+class ZSinglePartitionAllocation {\n+private:\n+  ZMemoryAllocation _allocation;\n+\n+public:\n+  ZSinglePartitionAllocation(size_t size)\n+    : _allocation(size) {}\n+\n+  size_t size() const {\n+    return _allocation.size();\n+  }\n+\n+  ZMemoryAllocation* allocation() {\n+    return &_allocation;\n+  }\n+\n+  const ZMemoryAllocation* allocation() const {\n+    return &_allocation;\n+  }\n+\n+  void reset_for_retry() {\n+    _allocation.reset_for_retry();\n+  }\n+};\n+\n+class ZMultiPartitionAllocation : public StackObj {\n+private:\n+  const size_t               _size;\n+  ZArray<ZMemoryAllocation*> _allocations;\n+\n+public:\n+  ZMultiPartitionAllocation(size_t size)\n+    : _size(size),\n+      _allocations(0) {}\n+\n+  ~ZMultiPartitionAllocation() {\n+    for (ZMemoryAllocation* allocation : _allocations) {\n+      ZMemoryAllocation::destroy(allocation);\n+    }\n+  }\n+\n+  void initialize() {\n+    precond(_allocations.is_empty());\n+\n+    \/\/ The multi-partition allocation creates at most one allocation per partition.\n+    const int length = (int)ZNUMA::count();\n+\n+    _allocations.reserve(length);\n+  }\n+\n+  void reset_for_retry() {\n+    for (ZMemoryAllocation* allocation : _allocations) {\n+      ZMemoryAllocation::destroy(allocation);\n+    }\n+    _allocations.clear();\n+  }\n+\n+  size_t size() const {\n+    return _size;\n+  }\n+\n+  ZArray<ZMemoryAllocation*>* allocations() {\n+    return &_allocations;\n+  }\n+\n+  const ZArray<ZMemoryAllocation*>* allocations() const {\n+    return &_allocations;\n+  }\n+\n+  void register_allocation(const ZMemoryAllocation& allocation) {\n+    ZMemoryAllocation** const slot = allocation_slot(allocation.partition().numa_id());\n+\n+    ZMemoryAllocation::merge(allocation, slot);\n+  }\n+\n+  ZMemoryAllocation** allocation_slot(uint32_t numa_id) {\n+    \/\/ Try to find an existing allocation for numa_id\n+    for (int i = 0; i < _allocations.length(); ++i) {\n+      ZMemoryAllocation** const slot_addr = _allocations.adr_at(i);\n+      ZMemoryAllocation* const allocation = *slot_addr;\n+      if (allocation->partition().numa_id() == numa_id) {\n+        \/\/ Found an existing slot\n+        return slot_addr;\n+      }\n+    }\n+\n+    \/\/ Push an empty slot for the numa_id\n+    _allocations.push(nullptr);\n+\n+    \/\/ Return the address of the slot\n+    return &_allocations.last();\n+  }\n+\n+  int sum_num_harvested_vmems() const {\n+    int total = 0;\n+\n+    for (const ZMemoryAllocation* allocation : _allocations) {\n+      total += allocation->num_harvested();\n+    }\n+\n+    return total;\n+  }\n+\n+  size_t sum_harvested() const {\n+    size_t total = 0;\n+\n+    for (const ZMemoryAllocation* allocation : _allocations) {\n+      total += allocation->harvested();\n+    }\n+\n+    return total;\n+  }\n+\n+  size_t sum_committed_increased_capacity() const {\n+    size_t total = 0;\n+\n+    for (const ZMemoryAllocation* allocation : _allocations) {\n+      total += allocation->committed_capacity();\n+    }\n+\n+    return total;\n+  }\n+};\n+\n+struct ZPageAllocationStats {\n+  int    _num_harvested_vmems;\n+  size_t _total_harvested;\n+  size_t _total_committed_capacity;\n+\n+  ZPageAllocationStats(int num_harvested_vmems, size_t total_harvested, size_t total_committed_capacity)\n+    : _num_harvested_vmems(num_harvested_vmems),\n+      _total_harvested(total_harvested),\n+      _total_committed_capacity(total_committed_capacity) {}\n+};\n@@ -107,0 +399,2 @@\n+  const ZPageAge             _age;\n+  const Ticks                _start_timestamp;\n@@ -109,3 +403,4 @@\n-  size_t                     _flushed;\n-  size_t                     _committed;\n-  ZList<ZPage>               _pages;\n+  const uint32_t             _initiating_numa_id;\n+  bool                       _is_multi_partition;\n+  ZSinglePartitionAllocation _single_partition_allocation;\n+  ZMultiPartitionAllocation  _multi_partition_allocation;\n@@ -116,1 +411,1 @@\n-  ZPageAllocation(ZPageType type, size_t size, ZAllocationFlags flags)\n+  ZPageAllocation(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age)\n@@ -120,0 +415,2 @@\n+      _age(age),\n+      _start_timestamp(Ticks::now()),\n@@ -122,3 +419,4 @@\n-      _flushed(0),\n-      _committed(0),\n-      _pages(),\n+      _initiating_numa_id(ZNUMA::id()),\n+      _is_multi_partition(false),\n+      _single_partition_allocation(size),\n+      _multi_partition_allocation(size),\n@@ -128,0 +426,6 @@\n+  void reset_for_retry() {\n+    _is_multi_partition = false;\n+    _single_partition_allocation.reset_for_retry();\n+    _multi_partition_allocation.reset_for_retry();\n+  }\n+\n@@ -140,0 +444,4 @@\n+  ZPageAge age() const {\n+    return _age;\n+  }\n+\n@@ -148,2 +456,6 @@\n-  size_t flushed() const {\n-    return _flushed;\n+  uint32_t initiating_numa_id() const {\n+    return _initiating_numa_id;\n+  }\n+\n+  bool is_multi_partition() const {\n+    return _is_multi_partition;\n@@ -152,2 +464,4 @@\n-  void set_flushed(size_t flushed) {\n-    _flushed = flushed;\n+  void initiate_multi_partition_allocation() {\n+    assert(!_is_multi_partition, \"Reinitialization?\");\n+    _is_multi_partition = true;\n+    _multi_partition_allocation.initialize();\n@@ -156,2 +470,4 @@\n-  size_t committed() const {\n-    return _committed;\n+  ZMultiPartitionAllocation* multi_partition_allocation() {\n+    assert(_is_multi_partition, \"multi-partition allocation must be initiated\");\n+\n+    return &_multi_partition_allocation;\n@@ -160,2 +476,4 @@\n-  void set_committed(size_t committed) {\n-    _committed = committed;\n+  const ZMultiPartitionAllocation* multi_partition_allocation() const {\n+    assert(_is_multi_partition, \"multi-partition allocation must be initiated\");\n+\n+    return &_multi_partition_allocation;\n@@ -164,2 +482,18 @@\n-  bool wait() {\n-    return _stall_result.get();\n+  ZSinglePartitionAllocation* single_partition_allocation() {\n+    assert(!_is_multi_partition, \"multi-partition allocation must not have been initiated\");\n+\n+    return &_single_partition_allocation;\n+  }\n+\n+  const ZSinglePartitionAllocation* single_partition_allocation() const {\n+    assert(!_is_multi_partition, \"multi-partition allocation must not have been initiated\");\n+\n+    return &_single_partition_allocation;\n+  }\n+\n+  ZVirtualMemory satisfied_from_cache_vmem() const {\n+    precond(!_is_multi_partition);\n+\n+    const ZMemoryAllocation* const allocation = _single_partition_allocation.allocation();\n+\n+    return allocation->satisfied_from_cache_vmem();\n@@ -168,2 +502,2 @@\n-  ZList<ZPage>* pages() {\n-    return &_pages;\n+  bool wait() {\n+    return _stall_result.get();\n@@ -179,25 +513,0 @@\n-};\n-\n-ZPageAllocator::ZPageAllocator(size_t min_capacity,\n-                               size_t initial_capacity,\n-                               size_t soft_max_capacity,\n-                               size_t max_capacity)\n-  : _lock(),\n-    _cache(),\n-    _virtual(max_capacity),\n-    _physical(max_capacity),\n-    _min_capacity(min_capacity),\n-    _initial_capacity(initial_capacity),\n-    _max_capacity(max_capacity),\n-    _current_max_capacity(max_capacity),\n-    _capacity(0),\n-    _claimed(0),\n-    _used(0),\n-    _used_generations{0, 0},\n-    _collection_stats{{0, 0}, {0, 0}},\n-    _stalled(),\n-    _unmapper(new ZUnmapper(this)),\n-    _uncommitter(new ZUncommitter(this)),\n-    _safe_destroy(),\n-    _safe_recycle(this),\n-    _initialized(false) {\n@@ -205,2 +514,12 @@\n-  if (!_virtual.is_initialized() || !_physical.is_initialized()) {\n-    return;\n+  ZPageAllocationStats stats() const {\n+    if (_is_multi_partition) {\n+      return ZPageAllocationStats(\n+          _multi_partition_allocation.sum_num_harvested_vmems(),\n+          _multi_partition_allocation.sum_harvested(),\n+          _multi_partition_allocation.sum_committed_increased_capacity());\n+    } else {\n+      return ZPageAllocationStats(\n+          _single_partition_allocation.allocation()->num_harvested(),\n+          _single_partition_allocation.allocation()->harvested(),\n+          _single_partition_allocation.allocation()->committed_capacity());\n+    }\n@@ -209,8 +528,16 @@\n-  log_info_p(gc, init)(\"Min Capacity: %zuM\", min_capacity \/ M);\n-  log_info_p(gc, init)(\"Initial Capacity: %zuM\", initial_capacity \/ M);\n-  log_info_p(gc, init)(\"Max Capacity: %zuM\", max_capacity \/ M);\n-  log_info_p(gc, init)(\"Soft Max Capacity: %zuM\", soft_max_capacity \/ M);\n-  if (ZPageSizeMedium > 0) {\n-    log_info_p(gc, init)(\"Medium Page Size: %zuM\", ZPageSizeMedium \/ M);\n-  } else {\n-    log_info_p(gc, init)(\"Medium Page Size: N\/A\");\n+  void send_event(bool successful) {\n+    EventZPageAllocation event;\n+\n+    Ticks end_timestamp = Ticks::now();\n+    const ZPageAllocationStats st = stats();\n+\n+    event.commit(_start_timestamp,\n+                 end_timestamp,\n+                 (u8)_type,\n+                 _size,\n+                 st._total_harvested,\n+                 st._total_committed_capacity,\n+                 (unsigned)st._num_harvested_vmems,\n+                 _is_multi_partition,\n+                 successful,\n+                 _flags.non_blocking());\n@@ -218,7 +545,1 @@\n-  log_info_p(gc, init)(\"Pre-touch: %s\", AlwaysPreTouch ? \"Enabled\" : \"Disabled\");\n-\n-  \/\/ Warn if system limits could stop us from reaching max capacity\n-  _physical.warn_commit_limits(max_capacity);\n-\n-  \/\/ Check if uncommit should and can be enabled\n-  _physical.try_enable_uncommit(min_capacity, max_capacity);\n+};\n@@ -226,2 +547,2 @@\n-  \/\/ Successfully initialized\n-  _initialized = true;\n+const ZVirtualMemoryManager& ZPartition::virtual_memory_manager() const {\n+  return _page_allocator->_virtual;\n@@ -230,2 +551,2 @@\n-bool ZPageAllocator::is_initialized() const {\n-  return _initialized;\n+ZVirtualMemoryManager& ZPartition::virtual_memory_manager() {\n+  return _page_allocator->_virtual;\n@@ -234,4 +555,3 @@\n-class ZPreTouchTask : public ZTask {\n-private:\n-  volatile uintptr_t _current;\n-  const uintptr_t    _end;\n+const ZPhysicalMemoryManager& ZPartition::physical_memory_manager() const {\n+  return _page_allocator->_physical;\n+}\n@@ -239,5 +559,3 @@\n-  static void pretouch(zaddress zaddr, size_t size) {\n-    const uintptr_t addr = untype(zaddr);\n-    const size_t page_size = ZLargePages::is_explicit() ? ZGranuleSize : os::vm_page_size();\n-    os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n-  }\n+ZPhysicalMemoryManager& ZPartition::physical_memory_manager() {\n+  return _page_allocator->_physical;\n+}\n@@ -245,5 +563,1 @@\n-public:\n-  ZPreTouchTask(zoffset start, zoffset_end end)\n-    : ZTask(\"ZPreTouchTask\"),\n-      _current(untype(start)),\n-      _end(untype(end)) {}\n+#ifdef ASSERT\n@@ -251,2 +565,2 @@\n-  virtual void work() {\n-    const size_t size = ZGranuleSize;\n+void ZPartition::verify_virtual_memory_multi_partition_association(const ZVirtualMemory& vmem) const {\n+  const ZVirtualMemoryManager& manager = virtual_memory_manager();\n@@ -254,7 +568,4 @@\n-    for (;;) {\n-      \/\/ Claim an offset for this thread\n-      const uintptr_t claimed = Atomic::fetch_then_add(&_current, size);\n-      if (claimed >= _end) {\n-        \/\/ Done\n-        break;\n-      }\n+  assert(manager.is_in_multi_partition(vmem),\n+         \"Virtual memory must be associated with the extra space \"\n+         \"actual: %u\", virtual_memory_manager().lookup_partition_id(vmem));\n+}\n@@ -262,3 +573,2 @@\n-      \/\/ At this point we know that we have a valid zoffset \/ zaddress.\n-      const zoffset offset = to_zoffset(claimed);\n-      const zaddress addr = ZOffset::address(offset);\n+void ZPartition::verify_virtual_memory_association(const ZVirtualMemory& vmem, bool check_multi_partition) const {\n+  const ZVirtualMemoryManager& manager = virtual_memory_manager();\n@@ -266,3 +576,4 @@\n-      \/\/ Pre-touch the granule\n-      pretouch(addr, size);\n-    }\n+  if (check_multi_partition && manager.is_in_multi_partition(vmem)) {\n+    \/\/ We allow claim\/free\/commit physical operation in multi-partition allocations\n+    \/\/ to use virtual memory associated with the extra space.\n+    return;\n@@ -270,1 +581,0 @@\n-};\n@@ -272,4 +582,5 @@\n-bool ZPageAllocator::prime_cache(ZWorkers* workers, size_t size) {\n-  ZAllocationFlags flags;\n-  flags.set_non_blocking();\n-  flags.set_low_address();\n+  const uint32_t vmem_numa_id = virtual_memory_manager().lookup_partition_id(vmem);\n+  assert(_numa_id == vmem_numa_id,\n+         \"Virtual memory must be associated with the current partition \"\n+         \"expected: %u, actual: %u\", _numa_id, vmem_numa_id);\n+}\n@@ -277,3 +588,3 @@\n-  ZPage* const page = alloc_page(ZPageType::large, size, flags, ZPageAge::eden);\n-  if (page == nullptr) {\n-    return false;\n+void ZPartition::verify_virtual_memory_association(const ZArray<ZVirtualMemory>* vmems) const {\n+  for (const ZVirtualMemory& vmem : *vmems) {\n+    verify_virtual_memory_association(vmem);\n@@ -281,0 +592,1 @@\n+}\n@@ -282,5 +594,5 @@\n-  if (AlwaysPreTouch) {\n-    \/\/ Pre-touch page\n-    ZPreTouchTask task(page->start(), page->end());\n-    workers->run_all(&task);\n-  }\n+void ZPartition::verify_memory_allocation_association(const ZMemoryAllocation* allocation) const {\n+  assert(this == &allocation->partition(),\n+         \"Memory allocation must be associated with the current partition \"\n+         \"expected: %u, actual: %u\", _numa_id, allocation->partition().numa_id());\n+}\n@@ -288,1 +600,1 @@\n-  free_page(page, false \/* allow_defragment *\/);\n+#endif \/\/ ASSERT\n@@ -290,2 +602,14 @@\n-  return true;\n-}\n+ZPartition::ZPartition(uint32_t numa_id, ZPageAllocator* page_allocator)\n+  : _page_allocator(page_allocator),\n+    _cache(),\n+    _uncommitter(numa_id, this),\n+    _min_capacity(ZNUMA::calculate_share(numa_id, page_allocator->min_capacity())),\n+    _max_capacity(ZNUMA::calculate_share(numa_id, page_allocator->max_capacity())),\n+    _current_max_capacity(_max_capacity),\n+    _capacity(0),\n+    _claimed(0),\n+    _used(0),\n+    _last_commit(0.0),\n+    _last_uncommit(0.0),\n+    _to_uncommit(0),\n+    _numa_id(numa_id) {}\n@@ -293,2 +617,2 @@\n-size_t ZPageAllocator::initial_capacity() const {\n-  return _initial_capacity;\n+uint32_t ZPartition::numa_id() const {\n+  return _numa_id;\n@@ -297,2 +621,2 @@\n-size_t ZPageAllocator::min_capacity() const {\n-  return _min_capacity;\n+size_t ZPartition::available() const {\n+  return _current_max_capacity - _used - _claimed;\n@@ -301,3 +625,2 @@\n-size_t ZPageAllocator::max_capacity() const {\n-  return _max_capacity;\n-}\n+size_t ZPartition::increase_capacity(size_t size) {\n+  const size_t increased = MIN2(size, _current_max_capacity - _capacity);\n@@ -305,5 +628,10 @@\n-size_t ZPageAllocator::soft_max_capacity() const {\n-  \/\/ Note that SoftMaxHeapSize is a manageable flag\n-  const size_t soft_max_capacity = Atomic::load(&SoftMaxHeapSize);\n-  const size_t current_max_capacity = Atomic::load(&_current_max_capacity);\n-  return MIN2(soft_max_capacity, current_max_capacity);\n+  if (increased > 0) {\n+    \/\/ Update atomically since we have concurrent readers\n+    Atomic::add(&_capacity, increased);\n+\n+    _last_commit = os::elapsedTime();\n+    _last_uncommit = 0;\n+    _cache.reset_min();\n+  }\n+\n+  return increased;\n@@ -312,2 +640,15 @@\n-size_t ZPageAllocator::capacity() const {\n-  return Atomic::load(&_capacity);\n+void ZPartition::decrease_capacity(size_t size, bool set_max_capacity) {\n+  \/\/ Update capacity atomically since we have concurrent readers\n+  Atomic::sub(&_capacity, size);\n+\n+  \/\/ Adjust current max capacity to avoid further attempts to increase capacity\n+  if (set_max_capacity) {\n+    const size_t current_max_capacity_before = _current_max_capacity;\n+    Atomic::store(&_current_max_capacity, _capacity);\n+\n+    log_debug_p(gc)(\"Forced to lower max partition (%u) capacity from \"\n+                    \"%zuM(%.0f%%) to %zuM(%.0f%%)\",\n+                    _numa_id,\n+                    current_max_capacity_before \/ M, percent_of(current_max_capacity_before, _max_capacity),\n+                    _current_max_capacity \/ M, percent_of(_current_max_capacity, _max_capacity));\n+  }\n@@ -316,2 +657,5 @@\n-size_t ZPageAllocator::used() const {\n-  return Atomic::load(&_used);\n+void ZPartition::increase_used(size_t size) {\n+  \/\/ The partition usage tracking is only read and updated under the page\n+  \/\/ allocator lock. Usage statistics for generations and GC cycles are\n+  \/\/ collected on the ZPageAllocator level.\n+  _used += size;\n@@ -320,2 +664,5 @@\n-size_t ZPageAllocator::used_generation(ZGenerationId id) const {\n-  return Atomic::load(&_used_generations[(int)id]);\n+void ZPartition::decrease_used(size_t size) {\n+  \/\/ The partition usage tracking is only read and updated under the page\n+  \/\/ allocator lock. Usage statistics for generations and GC cycles are\n+  \/\/ collected on the ZPageAllocator level.\n+  _used -= size;\n@@ -324,4 +671,694 @@\n-size_t ZPageAllocator::unused() const {\n-  const ssize_t capacity = (ssize_t)Atomic::load(&_capacity);\n-  const ssize_t used = (ssize_t)Atomic::load(&_used);\n-  const ssize_t claimed = (ssize_t)Atomic::load(&_claimed);\n+void ZPartition::free_memory(const ZVirtualMemory& vmem) {\n+  const size_t size = vmem.size();\n+\n+  \/\/ Cache the vmem\n+  _cache.insert(vmem);\n+\n+  \/\/ Update accounting\n+  decrease_used(size);\n+}\n+\n+void ZPartition::claim_from_cache_or_increase_capacity(ZMemoryAllocation* allocation) {\n+  const size_t size = allocation->size();\n+  ZArray<ZVirtualMemory>* const out = allocation->partial_vmems();\n+\n+  \/\/ We are guaranteed to succeed the claiming of capacity here\n+  assert(available() >= size, \"Must be\");\n+\n+  \/\/ Associate the allocation with this partition.\n+  allocation->set_partition(this);\n+\n+  \/\/ Try to allocate one contiguous vmem\n+  ZVirtualMemory vmem = _cache.remove_contiguous(size);\n+  if (!vmem.is_null()) {\n+    \/\/ Found a satisfying vmem in the cache\n+    allocation->set_satisfied_from_cache_vmem(vmem);\n+\n+    \/\/ Done\n+    return;\n+  }\n+\n+  \/\/ Try increase capacity\n+  const size_t increased_capacity = increase_capacity(size);\n+\n+  allocation->set_increased_capacity(increased_capacity);\n+\n+  if (increased_capacity == size) {\n+    \/\/ Capacity increase covered the entire request, done.\n+    return;\n+  }\n+\n+  \/\/ Could not increase capacity enough to satisfy the allocation completely.\n+  \/\/ Try removing multiple vmems from the mapped cache.\n+  const size_t remaining = size - increased_capacity;\n+  const size_t harvested = _cache.remove_discontiguous(remaining, out);\n+  const int num_harvested = out->length();\n+\n+  allocation->set_harvested(num_harvested, harvested);\n+\n+  assert(harvested + increased_capacity == size,\n+         \"Mismatch harvested: %zu increased_capacity: %zu size: %zu\",\n+         harvested, increased_capacity, size);\n+\n+  return;\n+}\n+\n+bool ZPartition::claim_capacity(ZMemoryAllocation* allocation) {\n+  const size_t size = allocation->size();\n+\n+  if (available() < size) {\n+    \/\/ Out of memory\n+    return false;\n+  }\n+\n+  claim_from_cache_or_increase_capacity(allocation);\n+\n+  \/\/ Updated used statistics\n+  increase_used(size);\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+size_t ZPartition::uncommit(uint64_t* timeout) {\n+  ZArray<ZVirtualMemory> flushed_vmems;\n+  size_t flushed = 0;\n+\n+  {\n+    \/\/ We need to join the suspendible thread set while manipulating capacity\n+    \/\/ and used, to make sure GC safepoints will have a consistent view.\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZLocker<ZLock> locker(&_page_allocator->_lock);\n+\n+    const double now = os::elapsedTime();\n+    const double time_since_last_commit = std::floor(now - _last_commit);\n+    const double time_since_last_uncommit = std::floor(now - _last_uncommit);\n+\n+    if (time_since_last_commit < double(ZUncommitDelay)) {\n+      \/\/ We have committed within the delay, stop uncommitting.\n+      *timeout = uint64_t(double(ZUncommitDelay) - time_since_last_commit);\n+      return 0;\n+    }\n+\n+    \/\/ We flush out and uncommit chunks at a time (~0.8% of the max capacity,\n+    \/\/ but at least one granule and at most 256M), in case demand for memory\n+    \/\/ increases while we are uncommitting.\n+    const size_t limit_upper_bound = MAX2(ZGranuleSize, align_down(256 * M \/ ZNUMA::count(), ZGranuleSize));\n+    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, ZGranuleSize), limit_upper_bound);\n+\n+    if (limit == 0) {\n+      \/\/ This may occur if the current max capacity for this partition is 0\n+\n+      \/\/ Set timeout to ZUncommitDelay\n+      *timeout = ZUncommitDelay;\n+      return 0;\n+    }\n+\n+    if (time_since_last_uncommit < double(ZUncommitDelay)) {\n+      \/\/ We are in the uncommit phase\n+      const size_t num_uncommits_left = _to_uncommit \/ limit;\n+      const double time_left = double(ZUncommitDelay) - time_since_last_uncommit;\n+      if (time_left < *timeout * num_uncommits_left) {\n+        \/\/ Running out of time, speed up.\n+        uint64_t new_timeout = uint64_t(std::floor(time_left \/ double(num_uncommits_left + 1)));\n+        *timeout = new_timeout;\n+      }\n+    } else {\n+      \/\/ We are about to start uncommitting\n+      _to_uncommit = _cache.reset_min();\n+      _last_uncommit = now;\n+\n+      const size_t split = _to_uncommit \/ limit + 1;\n+      uint64_t new_timeout = ZUncommitDelay \/ split;\n+      *timeout = new_timeout;\n+    }\n+\n+    \/\/ Never uncommit below min capacity.\n+    const size_t retain = MAX2(_used, _min_capacity);\n+    const size_t release = _capacity - retain;\n+    const size_t flush = MIN3(release, limit, _to_uncommit);\n+\n+    if (flush == 0) {\n+      \/\/ Nothing to flush\n+      return 0;\n+    }\n+\n+    \/\/ Flush memory from the mapped cache to uncommit\n+    flushed = _cache.remove_from_min(flush, &flushed_vmems);\n+    if (flushed == 0) {\n+      \/\/ Nothing flushed\n+      return 0;\n+    }\n+\n+    \/\/ Record flushed memory as claimed and how much we've flushed for this partition\n+    Atomic::add(&_claimed, flushed);\n+    _to_uncommit -= flushed;\n+  }\n+\n+  \/\/ Unmap and uncommit flushed memory\n+  for (const ZVirtualMemory vmem : flushed_vmems) {\n+    unmap_virtual(vmem);\n+    uncommit_physical(vmem);\n+    free_physical(vmem);\n+    free_virtual(vmem);\n+  }\n+\n+  {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZLocker<ZLock> locker(&_page_allocator->_lock);\n+\n+    \/\/ Adjust claimed and capacity to reflect the uncommit\n+    Atomic::sub(&_claimed, flushed);\n+    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n+  }\n+\n+  return flushed;\n+}\n+\n+void ZPartition::sort_segments_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Sort physical segments\n+  manager.sort_segments_physical(vmem);\n+}\n+\n+void ZPartition::claim_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Alloc physical memory\n+  manager.alloc(vmem, _numa_id);\n+}\n+\n+void ZPartition::free_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Free physical memory\n+  manager.free(vmem, _numa_id);\n+}\n+\n+size_t ZPartition::commit_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Commit physical memory\n+  return manager.commit(vmem, _numa_id);\n+}\n+\n+size_t ZPartition::uncommit_physical(const ZVirtualMemory& vmem) {\n+  assert(ZUncommit, \"should not uncommit when uncommit is disabled\");\n+  verify_virtual_memory_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Uncommit physical memory\n+  return manager.uncommit(vmem);\n+}\n+\n+void ZPartition::map_virtual(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Map virtual memory to physical memory\n+  manager.map(vmem, _numa_id);\n+}\n+\n+void ZPartition::unmap_virtual(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Unmap virtual memory from physical memory\n+  manager.unmap(vmem);\n+}\n+\n+void ZPartition::map_virtual_from_multi_partition(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_multi_partition_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Sort physical segments\n+  manager.sort_segments_physical(vmem);\n+\n+  \/\/ Map virtual memory to physical memory\n+  manager.map(vmem, _numa_id);\n+}\n+\n+void ZPartition::unmap_virtual_from_multi_partition(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_multi_partition_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Unmap virtual memory from physical memory\n+  manager.unmap(vmem);\n+}\n+\n+ZVirtualMemory ZPartition::claim_virtual(size_t size) {\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  return manager.remove_from_low(size, _numa_id);\n+}\n+\n+size_t ZPartition::claim_virtual(size_t size, ZArray<ZVirtualMemory>* vmems_out) {\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  return manager.remove_from_low_many_at_most(size, _numa_id, vmems_out);\n+}\n+\n+void ZPartition::free_virtual(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  \/\/ Free virtual memory\n+  manager.insert(vmem, _numa_id);\n+}\n+\n+void ZPartition::free_and_claim_virtual_from_low_many(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  \/\/ Shuffle virtual memory\n+  manager.insert_and_remove_from_low_many(vmem, _numa_id, vmems_out);\n+}\n+\n+ZVirtualMemory ZPartition::free_and_claim_virtual_from_low_exact_or_many(size_t size, ZArray<ZVirtualMemory>* vmems_in_out) {\n+  verify_virtual_memory_association(vmems_in_out);\n+\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  \/\/ Shuffle virtual memory\n+  return manager.insert_and_remove_from_low_exact_or_many(size, _numa_id, vmems_in_out);\n+}\n+\n+static void pretouch_memory(zoffset start, size_t size) {\n+  \/\/ At this point we know that we have a valid zoffset \/ zaddress.\n+  const zaddress zaddr = ZOffset::address(start);\n+  const uintptr_t addr = untype(zaddr);\n+  const size_t page_size = ZLargePages::is_explicit() ? ZGranuleSize : os::vm_page_size();\n+  os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n+}\n+\n+class ZPreTouchTask : public ZTask {\n+private:\n+  volatile uintptr_t _current;\n+  const uintptr_t    _end;\n+\n+public:\n+  ZPreTouchTask(zoffset start, zoffset_end end)\n+    : ZTask(\"ZPreTouchTask\"),\n+      _current(untype(start)),\n+      _end(untype(end)) {}\n+\n+  virtual void work() {\n+    const size_t size = ZGranuleSize;\n+\n+    for (;;) {\n+      \/\/ Claim an offset for this thread\n+      const uintptr_t claimed = Atomic::fetch_then_add(&_current, size);\n+      if (claimed >= _end) {\n+        \/\/ Done\n+        break;\n+      }\n+\n+      \/\/ At this point we know that we have a valid zoffset \/ zaddress.\n+      const zoffset offset = to_zoffset(claimed);\n+\n+      \/\/ Pre-touch the granule\n+      pretouch_memory(offset, size);\n+    }\n+  }\n+};\n+\n+bool ZPartition::prime(ZWorkers* workers, size_t size) {\n+  if (size == 0) {\n+    return true;\n+  }\n+\n+  \/\/ Claim virtual memory\n+  const ZVirtualMemory vmem = claim_virtual(size);\n+\n+  \/\/ Increase capacity\n+  increase_capacity(size);\n+\n+  \/\/ Claim the backing physical memory\n+  claim_physical(vmem);\n+\n+  \/\/ Commit the claimed physical memory\n+  const size_t committed = commit_physical(vmem);\n+\n+  if (committed != vmem.size()) {\n+    \/\/ This is a failure state. We do not cleanup the maybe partially committed memory.\n+    return false;\n+  }\n+\n+  map_virtual(vmem);\n+\n+  check_numa_mismatch(vmem, _numa_id);\n+\n+  if (AlwaysPreTouch) {\n+    \/\/ Pre-touch memory\n+    ZPreTouchTask task(vmem.start(), vmem.end());\n+    workers->run_all(&task);\n+  }\n+\n+  \/\/ We don't have to take a lock here as no other threads will access the cache\n+  \/\/ until we're finished\n+  _cache.insert(vmem);\n+\n+  return true;\n+}\n+\n+ZVirtualMemory ZPartition::prepare_harvested_and_claim_virtual(ZMemoryAllocation* allocation) {\n+  verify_memory_allocation_association(allocation);\n+\n+  \/\/ Unmap virtual memory\n+  for (const ZVirtualMemory vmem : *allocation->partial_vmems()) {\n+    unmap_virtual(vmem);\n+  }\n+\n+  const size_t harvested = allocation->harvested();\n+  const int granule_count = (int)(harvested >> ZGranuleSizeShift);\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Stash segments\n+  ZArray<zbacking_index> stash(granule_count);\n+  manager.stash_segments(*allocation->partial_vmems(), &stash);\n+\n+  \/\/ Shuffle virtual memory. We attempt to allocate enough memory to cover the\n+  \/\/ entire allocation size, not just for the harvested memory.\n+  const ZVirtualMemory result = free_and_claim_virtual_from_low_exact_or_many(allocation->size(), allocation->partial_vmems());\n+\n+  \/\/ Restore segments\n+  if (!result.is_null()) {\n+    \/\/ Got exact match. Restore stashed physical segments for the harvested part.\n+    manager.restore_segments(result.first_part(harvested), stash);\n+  } else {\n+    \/\/ Got many partial vmems\n+    manager.restore_segments(*allocation->partial_vmems(), stash);\n+  }\n+\n+  if (result.is_null()) {\n+    \/\/ Before returning harvested memory to the cache it must be mapped.\n+    for (const ZVirtualMemory vmem : *allocation->partial_vmems()) {\n+      map_virtual(vmem);\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+void ZPartition::copy_physical_segments_to_partition(const ZVirtualMemory& at, const ZVirtualMemory& from) {\n+  verify_virtual_memory_association(at);\n+  verify_virtual_memory_association(from, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Copy segments\n+  manager.copy_physical_segments(at, from);\n+}\n+\n+void ZPartition::copy_physical_segments_from_partition(const ZVirtualMemory& at, const ZVirtualMemory& to) {\n+  verify_virtual_memory_association(at);\n+  verify_virtual_memory_association(to, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+\n+  \/\/ Copy segments\n+  manager.copy_physical_segments(to, at);\n+}\n+\n+void ZPartition::commit_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  assert(allocation->increased_capacity() > 0, \"Nothing to commit\");\n+\n+  const size_t already_committed = allocation->harvested();\n+\n+  const ZVirtualMemory already_committed_vmem = vmem.first_part(already_committed);\n+  const ZVirtualMemory to_be_committed_vmem = vmem.last_part(already_committed);\n+\n+  \/\/ Try to commit the uncommitted physical memory\n+  const size_t committed = commit_physical(to_be_committed_vmem);\n+\n+  \/\/ Keep track of the committed amount\n+  allocation->set_committed_capacity(committed);\n+}\n+\n+void ZPartition::map_memory(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  sort_segments_physical(vmem);\n+  map_virtual(vmem);\n+\n+  check_numa_mismatch(vmem, allocation->partition().numa_id());\n+}\n+\n+void ZPartition::free_memory_alloc_failed(ZMemoryAllocation* allocation) {\n+  verify_memory_allocation_association(allocation);\n+\n+  \/\/ Only decrease the overall used and not the generation used,\n+  \/\/ since the allocation failed and generation used wasn't bumped.\n+  decrease_used(allocation->size());\n+\n+  size_t freed = 0;\n+\n+  \/\/ Free mapped memory\n+  for (const ZVirtualMemory vmem : *allocation->partial_vmems()) {\n+    freed += vmem.size();\n+    _cache.insert(vmem);\n+  }\n+  assert(allocation->harvested() + allocation->committed_capacity() == freed, \"must have freed all\");\n+\n+  \/\/ Adjust capacity to reflect the failed capacity increase\n+  const size_t remaining = allocation->size() - freed;\n+  if (remaining > 0) {\n+    const bool set_max_capacity = allocation->commit_failed();\n+    decrease_capacity(remaining, set_max_capacity);\n+  }\n+}\n+\n+void ZPartition::threads_do(ThreadClosure* tc) const {\n+  tc->do_thread(const_cast<ZUncommitter*>(&_uncommitter));\n+}\n+\n+void ZPartition::print_on(outputStream* st) const {\n+  st->print(\"Partition %u\", _numa_id);\n+  st->fill_to(17);\n+  st->print_cr(\"used %zuM, capacity %zuM, max capacity %zuM\",\n+               _used \/ M, _capacity \/ M, _max_capacity \/ M);\n+\n+  streamIndentor indentor(st, 1);\n+  print_cache_on(st);\n+}\n+\n+void ZPartition::print_cache_on(outputStream* st) const {\n+  _cache.print_on(st);\n+}\n+\n+void ZPartition::print_extended_on_error(outputStream* st) const {\n+  st->print_cr(\"Partition %u\", _numa_id);\n+\n+  streamIndentor indentor(st, 1);\n+\n+  _cache.print_extended_on(st);\n+}\n+\n+class ZMultiPartitionTracker : CHeapObj<mtGC> {\n+private:\n+  struct Element {\n+    ZVirtualMemory _vmem;\n+    ZPartition*    _partition;\n+  };\n+\n+  ZArray<Element> _map;\n+\n+  ZMultiPartitionTracker(int capacity)\n+    : _map(capacity) {}\n+\n+  const ZArray<Element>* map() const {\n+    return &_map;\n+  }\n+\n+  ZArray<Element>* map() {\n+    return &_map;\n+  }\n+\n+public:\n+  void prepare_memory_for_free(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out) const {\n+    \/\/ Remap memory back to original partition\n+    for (const Element partial_allocation : *map()) {\n+      ZVirtualMemory remaining_vmem = partial_allocation._vmem;\n+      ZPartition& partition = *partial_allocation._partition;\n+\n+      const size_t size = remaining_vmem.size();\n+\n+      \/\/ Allocate new virtual address ranges\n+      const int start_index = vmems_out->length();\n+      const size_t claimed_virtual = partition.claim_virtual(remaining_vmem.size(), vmems_out);\n+\n+      \/\/ We are holding memory associated with this partition, and we do not\n+      \/\/ overcommit virtual memory claiming. So virtual memory must always\n+      \/\/ be available.\n+      assert(claimed_virtual == size, \"must succeed\");\n+\n+      \/\/ Remap to the newly allocated virtual address ranges\n+      for (const ZVirtualMemory& to_vmem : vmems_out->slice_back(start_index)) {\n+        const ZVirtualMemory from_vmem = remaining_vmem.shrink_from_front(to_vmem.size());\n+\n+        \/\/ Copy physical segments\n+        partition.copy_physical_segments_to_partition(to_vmem, from_vmem);\n+\n+        \/\/ Unmap from_vmem\n+        partition.unmap_virtual_from_multi_partition(from_vmem);\n+\n+        \/\/ Map to_vmem\n+        partition.map_virtual(to_vmem);\n+      }\n+      assert(remaining_vmem.size() == 0, \"must have mapped all claimed virtual memory\");\n+    }\n+  }\n+\n+  static void destroy(const ZMultiPartitionTracker* tracker) {\n+    delete tracker;\n+  }\n+\n+  static ZMultiPartitionTracker* create(const ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+    const ZArray<ZMemoryAllocation*>* const partial_allocations = multi_partition_allocation->allocations();\n+\n+    ZMultiPartitionTracker* const tracker = new ZMultiPartitionTracker(partial_allocations->length());\n+\n+    ZVirtualMemory remaining = vmem;\n+\n+    \/\/ Each partial allocation is mapped to the virtual memory in order\n+    for (ZMemoryAllocation* partial_allocation : *partial_allocations) {\n+      \/\/ Track each separate vmem's partition\n+      const ZVirtualMemory partial_vmem = remaining.shrink_from_front(partial_allocation->size());\n+      ZPartition* const partition = &partial_allocation->partition();\n+      tracker->map()->push({partial_vmem, partition});\n+    }\n+\n+    return tracker;\n+  }\n+};\n+\n+ZPageAllocator::ZPageAllocator(size_t min_capacity,\n+                               size_t initial_capacity,\n+                               size_t soft_max_capacity,\n+                               size_t max_capacity)\n+  : _lock(),\n+    _virtual(max_capacity),\n+    _physical(max_capacity),\n+    _min_capacity(min_capacity),\n+    _max_capacity(max_capacity),\n+    _used(0),\n+    _used_generations{0,0},\n+    _collection_stats{{0, 0},{0, 0}},\n+    _partitions(ZValueIdTagType{}, this),\n+    _stalled(),\n+    _safe_destroy(),\n+    _initialized(false) {\n+\n+  if (!_virtual.is_initialized() || !_physical.is_initialized()) {\n+    return;\n+  }\n+\n+  log_info_p(gc, init)(\"Min Capacity: %zuM\", min_capacity \/ M);\n+  log_info_p(gc, init)(\"Initial Capacity: %zuM\", initial_capacity \/ M);\n+  log_info_p(gc, init)(\"Max Capacity: %zuM\", max_capacity \/ M);\n+  log_info_p(gc, init)(\"Soft Max Capacity: %zuM\", soft_max_capacity \/ M);\n+  if (ZPageSizeMedium > 0) {\n+    log_info_p(gc, init)(\"Medium Page Size: %zuM\", ZPageSizeMedium \/ M);\n+  } else {\n+    log_info_p(gc, init)(\"Medium Page Size: N\/A\");\n+  }\n+  log_info_p(gc, init)(\"Pre-touch: %s\", AlwaysPreTouch ? \"Enabled\" : \"Disabled\");\n+\n+  \/\/ Warn if system limits could stop us from reaching max capacity\n+  _physical.warn_commit_limits(max_capacity);\n+\n+  \/\/ Check if uncommit should and can be enabled\n+  _physical.try_enable_uncommit(min_capacity, max_capacity);\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+bool ZPageAllocator::is_initialized() const {\n+  return _initialized;\n+}\n+\n+bool ZPageAllocator::prime_cache(ZWorkers* workers, size_t size) {\n+  ZPartitionIterator iter = partition_iterator();\n+  for (ZPartition* partition; iter.next(&partition);) {\n+    const uint32_t numa_id = partition->numa_id();\n+    const size_t to_prime = ZNUMA::calculate_share(numa_id, size);\n+\n+    if (!partition->prime(workers, to_prime)) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+size_t ZPageAllocator::min_capacity() const {\n+  return _min_capacity;\n+}\n+\n+size_t ZPageAllocator::max_capacity() const {\n+  return _max_capacity;\n+}\n+\n+size_t ZPageAllocator::soft_max_capacity() const {\n+  const size_t current_max_capacity = ZPageAllocator::current_max_capacity();\n+  const size_t soft_max_heapsize = Atomic::load(&SoftMaxHeapSize);\n+  return MIN2(soft_max_heapsize, current_max_capacity);\n+}\n+\n+size_t ZPageAllocator::current_max_capacity() const {\n+  size_t current_max_capacity = 0;\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    current_max_capacity += Atomic::load(&partition->_current_max_capacity);\n+  }\n+\n+  return current_max_capacity;\n+}\n+\n+size_t ZPageAllocator::capacity() const {\n+  size_t capacity = 0;\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    capacity += Atomic::load(&partition->_capacity);\n+  }\n+\n+  return capacity;\n+}\n+\n+size_t ZPageAllocator::used() const {\n+  return Atomic::load(&_used);\n+}\n+\n+size_t ZPageAllocator::used_generation(ZGenerationId id) const {\n+  return Atomic::load(&_used_generations[(int)id]);\n+}\n+\n+size_t ZPageAllocator::unused() const {\n+  const ssize_t used = (ssize_t)ZPageAllocator::used();\n+  ssize_t capacity = 0;\n+  ssize_t claimed = 0;\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    capacity += (ssize_t)Atomic::load(&partition->_capacity);\n+    claimed += (ssize_t)Atomic::load(&partition->_claimed);\n+  }\n+\n@@ -334,0 +1371,1 @@\n+\n@@ -337,1 +1375,1 @@\n-                             _capacity,\n+                             capacity(),\n@@ -350,2 +1388,21 @@\n-  _collection_stats[(int)id]._used_high = _used;\n-  _collection_stats[(int)id]._used_low = _used;\n+#ifdef ASSERT\n+  {\n+    \/\/ We may free without safepoint synchronization, take the lock to get\n+    \/\/ consistent values.\n+    ZLocker<ZLock> locker(&_lock);\n+    size_t total_used = 0;\n+\n+    ZPartitionIterator iter(&_partitions);\n+    for (ZPartition* partition; iter.next(&partition);) {\n+      total_used += partition->_used;\n+    }\n+\n+    assert(total_used == _used, \"Must be consistent at safepoint %zu == %zu\", total_used, _used);\n+  }\n+#endif\n+\n+  \/\/ Read once, we may have concurrent writers.\n+  const size_t used = Atomic::load(&_used);\n+\n+  _collection_stats[(int)id]._used_high = used;\n+  _collection_stats[(int)id]._used_low = used;\n@@ -354,2 +1411,4 @@\n-size_t ZPageAllocator::increase_capacity(size_t size) {\n-  const size_t increased = MIN2(size, _current_max_capacity - _capacity);\n+void ZPageAllocator::increase_used_generation(ZGenerationId id, size_t size) {\n+  \/\/ Update atomically since we have concurrent readers and writers\n+  Atomic::add(&_used_generations[(int)id], size, memory_order_relaxed);\n+}\n@@ -357,3 +1416,14 @@\n-  if (increased > 0) {\n-    \/\/ Update atomically since we have concurrent readers\n-    Atomic::add(&_capacity, increased);\n+void ZPageAllocator::decrease_used_generation(ZGenerationId id, size_t size) {\n+  \/\/ Update atomically since we have concurrent readers and writers\n+  Atomic::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n+}\n+\n+void ZPageAllocator::promote_used(const ZPage* from, const ZPage* to) {\n+  assert(from->start() == to->start(), \"pages start at same offset\");\n+  assert(from->size() == to->size(),   \"pages are the same size\");\n+  assert(from->age() != ZPageAge::old, \"must be promotion\");\n+  assert(to->age() == ZPageAge::old,   \"must be promotion\");\n+\n+  decrease_used_generation(ZGenerationId::young, to->size());\n+  increase_used_generation(ZGenerationId::old, to->size());\n+}\n@@ -361,6 +1431,3 @@\n-    \/\/ Record time of last commit. When allocation, we prefer increasing\n-    \/\/ the capacity over flushing the cache. That means there could be\n-    \/\/ expired pages in the cache at this time. However, since we are\n-    \/\/ increasing the capacity we are obviously in need of committed\n-    \/\/ memory and should therefore not be uncommitting memory.\n-    _cache.set_last_commit();\n+static void check_out_of_memory_during_initialization() {\n+  if (!is_init_completed()) {\n+    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\", \"Java heap too small\");\n@@ -368,0 +1435,1 @@\n+}\n@@ -369,1 +1437,34 @@\n-  return increased;\n+ZPage* ZPageAllocator::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age) {\n+  EventZPageAllocation event;\n+\n+  ZPageAllocation allocation(type, size, flags, age);\n+\n+  \/\/ Allocate the page\n+  ZPage* const page = alloc_page_inner(&allocation);\n+  if (page == nullptr) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Update allocation statistics. Exclude gc relocations to avoid\n+  \/\/ artificial inflation of the allocation rate during relocation.\n+  if (!flags.gc_relocation() && is_init_completed()) {\n+    \/\/ Note that there are two allocation rate counters, which have\n+    \/\/ different purposes and are sampled at different frequencies.\n+    ZStatInc(ZCounterMutatorAllocationRate, size);\n+    ZStatMutatorAllocRate::sample_allocation(size);\n+  }\n+\n+  const ZPageAllocationStats stats = allocation.stats();\n+  const int num_harvested_vmems = stats._num_harvested_vmems;\n+  const size_t harvested = stats._total_harvested;\n+  const size_t committed = stats._total_committed_capacity;\n+\n+  if (harvested > 0) {\n+    ZStatInc(ZCounterMappedCacheHarvest, harvested);\n+    log_debug(gc, heap)(\"Mapped Cache Harvested: %zuM (%d)\", harvested \/ M, num_harvested_vmems);\n+  }\n+\n+  \/\/ Send event for successful allocation\n+  allocation.send_event(true \/* successful *\/);\n+\n+  return page;\n@@ -372,3 +1473,3 @@\n-void ZPageAllocator::decrease_capacity(size_t size, bool set_max_capacity) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::sub(&_capacity, size);\n+bool ZPageAllocator::alloc_page_stall(ZPageAllocation* allocation) {\n+  ZStatTimer timer(ZCriticalPhaseAllocationStall);\n+  EventZAllocationStall event;\n@@ -376,6 +1477,2 @@\n-  if (set_max_capacity) {\n-    \/\/ Adjust current max capacity to avoid further attempts to increase capacity\n-    log_error_p(gc)(\"Forced to lower max Java heap size from \"\n-                    \"%zuM(%.0f%%) to %zuM(%.0f%%)\",\n-                    _current_max_capacity \/ M, percent_of(_current_max_capacity, _max_capacity),\n-                    _capacity \/ M, percent_of(_capacity, _max_capacity));\n+  \/\/ We can only block if the VM is fully initialized\n+  check_out_of_memory_during_initialization();\n@@ -383,2 +1480,16 @@\n-    \/\/ Update atomically since we have concurrent readers\n-    Atomic::store(&_current_max_capacity, _capacity);\n+  \/\/ Start asynchronous minor GC\n+  const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, 0);\n+  ZDriver::minor()->collect(request);\n+\n+  \/\/ Wait for allocation to complete or fail\n+  const bool result = allocation->wait();\n+\n+  {\n+    \/\/ Guard deletion of underlying semaphore. This is a workaround for\n+    \/\/ a bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n+    \/\/ the semaphore immediately after returning from sem_wait(). The\n+    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n+    \/\/ thread have returned from sem_wait(). To avoid this race we are\n+    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n+    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n+    ZLocker<ZLock> locker(&_lock);\n@@ -386,0 +1497,5 @@\n+\n+  \/\/ Send event\n+  event.commit((u8)allocation->type(), allocation->size());\n+\n+  return result;\n@@ -388,8 +1504,2 @@\n-void ZPageAllocator::increase_used(size_t size) {\n-  \/\/ We don't track generation usage here because this page\n-  \/\/ could be allocated by a thread that satisfies a stalling\n-  \/\/ allocation. The stalled thread can wake up and potentially\n-  \/\/ realize that the page alloc should be undone. If the alloc\n-  \/\/ and the undo gets separated by a safepoint, the generation\n-  \/\/ statistics could se a decreasing used value between mark\n-  \/\/ start and mark end.\n+ZPage* ZPageAllocator::alloc_page_inner(ZPageAllocation* allocation) {\n+retry:\n@@ -397,2 +1507,12 @@\n-  \/\/ Update atomically since we have concurrent readers\n-  const size_t used = Atomic::add(&_used, size);\n+  \/\/ Claim the capacity needed for this allocation.\n+  \/\/\n+  \/\/ The claimed capacity comes from memory already mapped in the cache, or\n+  \/\/ from increasing the capacity. The increased capacity allows us to allocate\n+  \/\/ physical memory from the physical memory manager later on.\n+  \/\/\n+  \/\/ Note that this call might block in a safepoint if the non-blocking flag is\n+  \/\/ not set.\n+  if (!claim_capacity_or_stall(allocation)) {\n+    \/\/ Out of memory\n+    return nullptr;\n+  }\n@@ -400,4 +1520,51 @@\n-  \/\/ Update used high\n-  for (auto& stats : _collection_stats) {\n-    if (used > stats._used_high) {\n-      stats._used_high = used;\n+  \/\/ If the entire claimed capacity came from claiming a single vmem from the\n+  \/\/ mapped cache then the allocation has been satisfied and we are done.\n+  const ZVirtualMemory cached_vmem = satisfied_from_cache_vmem(allocation);\n+  if (!cached_vmem.is_null()) {\n+    return create_page(allocation, cached_vmem);\n+  }\n+\n+  \/\/ We couldn't find a satisfying vmem in the cache, so we need to build one.\n+\n+  \/\/ Claim virtual memory, either from remapping harvested vmems from the\n+  \/\/ mapped cache or by claiming it straight from the virtual memory manager.\n+  const ZVirtualMemory vmem = claim_virtual_memory(allocation);\n+  if (vmem.is_null()) {\n+    log_error(gc)(\"Out of address space\");\n+    free_after_alloc_page_failed(allocation);\n+\n+    \/\/ Crash in debug builds for more information\n+    DEBUG_ONLY(fatal(\"Out of address space\");)\n+\n+    return nullptr;\n+  }\n+\n+  \/\/ Claim physical memory for the increased capacity. The previous claiming of\n+  \/\/ capacity guarantees that this will succeed.\n+  claim_physical_for_increased_capacity(allocation, vmem);\n+\n+  \/\/ Commit memory for the increased capacity and map the entire vmem.\n+  if (!commit_and_map(allocation, vmem)) {\n+    free_after_alloc_page_failed(allocation);\n+    goto retry;\n+  }\n+\n+  return create_page(allocation, vmem);\n+}\n+\n+bool ZPageAllocator::claim_capacity_or_stall(ZPageAllocation* allocation) {\n+  {\n+    ZLocker<ZLock> locker(&_lock);\n+\n+    \/\/ Try to claim memory\n+    if (claim_capacity(allocation)) {\n+      \/\/ Keep track of usage\n+      increase_used(allocation->size());\n+\n+      return true;\n+    }\n+\n+    \/\/ Failed to claim memory\n+    if (allocation->flags().non_blocking()) {\n+      \/\/ Don't stall\n+      return false;\n@@ -405,0 +1572,3 @@\n+\n+    \/\/ Enqueue allocation request\n+    _stalled.insert_last(allocation);\n@@ -406,0 +1576,3 @@\n+\n+  \/\/ Stall\n+  return alloc_page_stall(allocation);\n@@ -408,3 +1581,4 @@\n-void ZPageAllocator::decrease_used(size_t size) {\n-  \/\/ Update atomically since we have concurrent readers\n-  const size_t used = Atomic::sub(&_used, size);\n+bool ZPageAllocator::claim_capacity(ZPageAllocation* allocation) {\n+  const uint32_t start_numa_id = allocation->initiating_numa_id();\n+  const uint32_t start_partition = start_numa_id;\n+  const uint32_t num_partitions = _partitions.count();\n@@ -412,4 +1586,7 @@\n-  \/\/ Update used low\n-  for (auto& stats : _collection_stats) {\n-    if (used < stats._used_low) {\n-      stats._used_low = used;\n+  \/\/ Round robin single-partition claiming\n+\n+  for (uint32_t i = 0; i < num_partitions; ++i) {\n+    const uint32_t partition_id = (start_partition + i) % num_partitions;\n+\n+    if (claim_capacity_single_partition(allocation->single_partition_allocation(), partition_id)) {\n+      return true;\n@@ -418,0 +1595,16 @@\n+\n+  if (!is_multi_partition_enabled() || sum_available() < allocation->size()) {\n+    \/\/ Multi-partition claiming is not possible\n+    return false;\n+  }\n+\n+  \/\/ Multi-partition claiming\n+\n+  \/\/ Flip allocation to multi-partition allocation\n+  allocation->initiate_multi_partition_allocation();\n+\n+  ZMultiPartitionAllocation* const multi_partition_allocation = allocation->multi_partition_allocation();\n+\n+  claim_capacity_multi_partition(multi_partition_allocation, start_partition);\n+\n+  return true;\n@@ -420,3 +1613,4 @@\n-void ZPageAllocator::increase_used_generation(ZGenerationId id, size_t size) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::add(&_used_generations[(int)id], size, memory_order_relaxed);\n+bool ZPageAllocator::claim_capacity_single_partition(ZSinglePartitionAllocation* single_partition_allocation, uint32_t partition_id) {\n+  ZPartition& partition = _partitions.get(partition_id);\n+\n+  return partition.claim_capacity(single_partition_allocation->allocation());\n@@ -425,3 +1619,62 @@\n-void ZPageAllocator::decrease_used_generation(ZGenerationId id, size_t size) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n+void ZPageAllocator::claim_capacity_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, uint32_t start_partition) {\n+  const size_t size = multi_partition_allocation->size();\n+  const uint32_t num_partitions = _partitions.count();\n+  const size_t split_size = align_up(size \/ num_partitions, ZGranuleSize);\n+\n+  size_t remaining = size;\n+\n+  const auto do_claim_one_partition = [&](ZPartition& partition, bool claim_evenly) {\n+    if (remaining == 0) {\n+      \/\/ All memory claimed\n+      return false;\n+    }\n+\n+    const size_t max_alloc_size = claim_evenly ? MIN2(split_size, remaining) : remaining;\n+\n+    \/\/ This guarantees that claim_physical below will succeed\n+    const size_t alloc_size = MIN2(max_alloc_size, partition.available());\n+\n+    \/\/ Skip over empty allocations\n+    if (alloc_size == 0) {\n+      \/\/ Continue\n+      return true;\n+    }\n+\n+    ZMemoryAllocation partial_allocation(alloc_size);\n+\n+    \/\/ Claim capacity for this allocation - this should succeed\n+    const bool result = partition.claim_capacity(&partial_allocation);\n+    assert(result, \"Should have succeeded\");\n+\n+    \/\/ Register allocation\n+    multi_partition_allocation->register_allocation(partial_allocation);\n+\n+    \/\/ Update remaining\n+    remaining -= alloc_size;\n+\n+    \/\/ Continue\n+    return true;\n+  };\n+\n+  \/\/ Loops over every partition and claims memory\n+  const auto do_claim_each_partition = [&](bool claim_evenly) {\n+    for (uint32_t i = 0; i < num_partitions; ++i) {\n+      const uint32_t partition_id = (start_partition + i) % num_partitions;\n+      ZPartition& partition = _partitions.get(partition_id);\n+\n+      if (!do_claim_one_partition(partition, claim_evenly)) {\n+        \/\/ All memory claimed\n+        break;\n+      }\n+    }\n+  };\n+\n+  \/\/ Try to claim from multiple partitions\n+\n+  \/\/ Try to claim up to split_size on each partition\n+  do_claim_each_partition(true  \/* claim_evenly *\/);\n+\n+  \/\/ Try claim the remaining\n+  do_claim_each_partition(false \/* claim_evenly *\/);\n+\n+  assert(remaining == 0, \"Must have claimed capacity for the whole allocation\");\n@@ -430,3 +1683,8 @@\n-void ZPageAllocator::promote_used(size_t size) {\n-  decrease_used_generation(ZGenerationId::young, size);\n-  increase_used_generation(ZGenerationId::old, size);\n+ZVirtualMemory ZPageAllocator::satisfied_from_cache_vmem(const ZPageAllocation* allocation) const {\n+  if (allocation->is_multi_partition()) {\n+    \/\/ Multi-partition allocations are always harvested and\/or committed, so\n+    \/\/ there's never a satisfying vmem from the caches.\n+    return {};\n+  }\n+\n+  return allocation->satisfied_from_cache_vmem();\n@@ -435,3 +1693,10 @@\n-bool ZPageAllocator::commit_page(ZPage* page) {\n-  \/\/ Commit physical memory\n-  return _physical.commit(page->physical_memory());\n+ZVirtualMemory ZPageAllocator::claim_virtual_memory(ZPageAllocation* allocation) {\n+  \/\/ Note: that the single-partition performs \"shuffling\" of already harvested\n+  \/\/ vmem(s), while the multi-partition searches for available virtual memory\n+  \/\/ area without shuffling.\n+\n+  if (allocation->is_multi_partition()) {\n+    return claim_virtual_memory_multi_partition(allocation->multi_partition_allocation());\n+  } else {\n+    return claim_virtual_memory_single_partition(allocation->single_partition_allocation());\n+  }\n@@ -440,3 +1705,11 @@\n-void ZPageAllocator::uncommit_page(ZPage* page) {\n-  if (!ZUncommit) {\n-    return;\n+ZVirtualMemory ZPageAllocator::claim_virtual_memory_single_partition(ZSinglePartitionAllocation* single_partition_allocation) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+  ZPartition& partition = allocation->partition();\n+\n+  if (allocation->harvested() > 0) {\n+    \/\/ We claim virtual memory from the harvested vmems and perhaps also\n+    \/\/ allocate more to match the allocation request.\n+    return partition.prepare_harvested_and_claim_virtual(allocation);\n+  } else {\n+    \/\/ Just try to claim virtual memory\n+    return partition.claim_virtual(allocation->size());\n@@ -444,0 +1717,1 @@\n+}\n@@ -445,2 +1719,34 @@\n-  \/\/ Uncommit physical memory\n-  _physical.uncommit(page->physical_memory());\n+ZVirtualMemory ZPageAllocator::claim_virtual_memory_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation) {\n+  const size_t size = multi_partition_allocation->size();\n+\n+  const ZVirtualMemory vmem = _virtual.remove_from_low_multi_partition(size);\n+  if (!vmem.is_null()) {\n+    \/\/ Copy claimed multi-partition vmems, we leave the old vmems mapped until\n+    \/\/ after we have committed. In case committing fails we can simply\n+    \/\/ reinsert the initial vmems.\n+    copy_claimed_physical_multi_partition(multi_partition_allocation, vmem);\n+  }\n+\n+  return vmem;\n+}\n+\n+void ZPageAllocator::copy_claimed_physical_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  \/\/ Start at the new dest offset\n+  ZVirtualMemory remaining_dest_vmem = vmem;\n+\n+  for (const ZMemoryAllocation* partial_allocation : *multi_partition_allocation->allocations()) {\n+    \/\/ Split off the partial allocation's destination vmem\n+    ZVirtualMemory partial_dest_vmem = remaining_dest_vmem.shrink_from_front(partial_allocation->size());\n+\n+    \/\/ Get the partial allocation's partition\n+    ZPartition& partition = partial_allocation->partition();\n+\n+    \/\/ Copy all physical segments from the partition to the destination vmem\n+    for (const ZVirtualMemory from_vmem : *partial_allocation->partial_vmems()) {\n+      \/\/ Split off destination\n+      const ZVirtualMemory to_vmem = partial_dest_vmem.shrink_from_front(from_vmem.size());\n+\n+      \/\/ Copy physical segments\n+      partition.copy_physical_segments_from_partition(from_vmem, to_vmem);\n+    }\n+  }\n@@ -449,3 +1755,8 @@\n-void ZPageAllocator::map_page(const ZPage* page) const {\n-  \/\/ Map physical memory\n-  _physical.map(page->start(), page->physical_memory());\n+void ZPageAllocator::claim_physical_for_increased_capacity(ZPageAllocation* allocation, const ZVirtualMemory& vmem) {\n+  assert(allocation->size() == vmem.size(), \"vmem should be the final entry\");\n+\n+  if (allocation->is_multi_partition()) {\n+    claim_physical_for_increased_capacity_multi_partition(allocation->multi_partition_allocation(), vmem);\n+  } else {\n+    claim_physical_for_increased_capacity_single_partition(allocation->single_partition_allocation(), vmem);\n+  }\n@@ -454,3 +1765,2 @@\n-void ZPageAllocator::unmap_page(const ZPage* page) const {\n-  \/\/ Unmap physical memory\n-  _physical.unmap(page->start(), page->size());\n+void ZPageAllocator::claim_physical_for_increased_capacity_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  claim_physical_for_increased_capacity(single_partition_allocation->allocation(), vmem);\n@@ -459,3 +1769,7 @@\n-void ZPageAllocator::safe_destroy_page(ZPage* page) {\n-  \/\/ Destroy page safely\n-  _safe_destroy.schedule_delete(page);\n+void ZPageAllocator::claim_physical_for_increased_capacity_multi_partition(const ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZVirtualMemory remaining = vmem;\n+\n+  for (ZMemoryAllocation* allocation : *multi_partition_allocation->allocations()) {\n+    const ZVirtualMemory partial = remaining.shrink_from_front(allocation->size());\n+    claim_physical_for_increased_capacity(allocation, partial);\n+  }\n@@ -464,3 +1778,4 @@\n-void ZPageAllocator::destroy_page(ZPage* page) {\n-  \/\/ Free virtual memory\n-  _virtual.free(page->virtual_memory());\n+void ZPageAllocator::claim_physical_for_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  \/\/ The previously harvested memory is memory that has already been committed\n+  \/\/ and mapped. The rest of the vmem gets physical memory assigned here and\n+  \/\/ will be committed in a subsequent function.\n@@ -468,2 +1783,3 @@\n-  \/\/ Free physical memory\n-  _physical.free(page->physical_memory());\n+  const size_t already_committed = allocation->harvested();\n+  const size_t non_committed = allocation->size() - already_committed;\n+  const size_t increased_capacity = allocation->increased_capacity();\n@@ -471,2 +1787,89 @@\n-  \/\/ Destroy page safely\n-  safe_destroy_page(page);\n+  assert(non_committed == increased_capacity,\n+         \"Mismatch non_committed: \" PTR_FORMAT \" increased_capacity: \" PTR_FORMAT,\n+         non_committed, increased_capacity);\n+\n+  if (non_committed > 0) {\n+    ZPartition& partition = allocation->partition();\n+    ZVirtualMemory non_committed_vmem = vmem.last_part(already_committed);\n+    partition.claim_physical(non_committed_vmem);\n+  }\n+}\n+\n+bool ZPageAllocator::commit_and_map(ZPageAllocation* allocation, const ZVirtualMemory& vmem) {\n+  assert(allocation->size() == vmem.size(), \"vmem should be the final entry\");\n+\n+  if (allocation->is_multi_partition()) {\n+    return commit_and_map_multi_partition(allocation->multi_partition_allocation(), vmem);\n+  } else {\n+    return commit_and_map_single_partition(allocation->single_partition_allocation(), vmem);\n+  }\n+}\n+\n+bool ZPageAllocator::commit_and_map_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  const bool commit_successful = commit_single_partition(single_partition_allocation, vmem);\n+\n+  \/\/ Map the vmem\n+  map_committed_single_partition(single_partition_allocation, vmem);\n+\n+  if (commit_successful) {\n+    return true;\n+  }\n+\n+  \/\/ Commit failed\n+  cleanup_failed_commit_single_partition(single_partition_allocation, vmem);\n+\n+  return false;\n+}\n+\n+bool ZPageAllocator::commit_and_map_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  if (commit_multi_partition(multi_partition_allocation, vmem)) {\n+    \/\/ Commit successful\n+\n+    \/\/ Unmap harvested vmems\n+    unmap_harvested_multi_partition(multi_partition_allocation);\n+\n+    \/\/ Map the vmem\n+    map_committed_multi_partition(multi_partition_allocation, vmem);\n+\n+    return true;\n+  }\n+\n+  \/\/ Commit failed\n+  cleanup_failed_commit_multi_partition(multi_partition_allocation, vmem);\n+\n+  return false;\n+}\n+\n+void ZPageAllocator::commit(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  ZPartition& partition = allocation->partition();\n+\n+  if (allocation->increased_capacity() > 0) {\n+    \/\/ Commit memory\n+    partition.commit_increased_capacity(allocation, vmem);\n+  }\n+}\n+\n+bool ZPageAllocator::commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+\n+  commit(allocation, vmem);\n+\n+  return !allocation->commit_failed();\n+}\n+\n+bool ZPageAllocator::commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  bool commit_failed = false;\n+  ZVirtualMemory remaining = vmem;\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    \/\/ Split off the partial allocation's memory range\n+    const ZVirtualMemory partial_vmem = remaining.shrink_from_front(allocation->size());\n+\n+    commit(allocation, partial_vmem);\n+\n+    \/\/ Keep track if any partial allocation failed to commit\n+    commit_failed |= allocation->commit_failed();\n+  }\n+\n+  assert(remaining.size() == 0, \"all memory must be accounted for\");\n+\n+  return !commit_failed;\n@@ -475,8 +1878,12 @@\n-bool ZPageAllocator::should_defragment(const ZPage* page) const {\n-  \/\/ A small page can end up at a high address (second half of the address space)\n-  \/\/ if we've split a larger page or we have a constrained address space. To help\n-  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n-  \/\/ a lower address is available.\n-  return page->type() == ZPageType::small &&\n-         page->start() >= to_zoffset(_virtual.reserved() \/ 2) &&\n-         page->start() > _virtual.lowest_available_address();\n+void ZPageAllocator::unmap_harvested_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation) {\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    ZPartition& partition = allocation->partition();\n+    ZArray<ZVirtualMemory>* const partial_vmems = allocation->partial_vmems();\n+\n+    \/\/ Unmap harvested vmems\n+    while (!partial_vmems->is_empty()) {\n+      const ZVirtualMemory to_unmap = partial_vmems->pop();\n+      partition.unmap_virtual(to_unmap);\n+      partition.free_virtual(to_unmap);\n+    }\n+  }\n@@ -485,6 +1892,12 @@\n-ZPage* ZPageAllocator::defragment_page(ZPage* page) {\n-  \/\/ Harvest the physical memory (which is committed)\n-  ZPhysicalMemory pmem;\n-  ZPhysicalMemory& old_pmem = page->physical_memory();\n-  pmem.add_segments(old_pmem);\n-  old_pmem.remove_segments();\n+void ZPageAllocator::map_committed_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+  ZPartition& partition = allocation->partition();\n+\n+  const size_t total_committed = allocation->harvested() + allocation->committed_capacity();\n+  const ZVirtualMemory total_committed_vmem = vmem.first_part(total_committed);\n+\n+  if (total_committed_vmem.size() > 0)  {\n+    \/\/ Map all the committed memory\n+    partition.map_memory(allocation, total_committed_vmem);\n+  }\n+}\n@@ -492,1 +1905,4 @@\n-  _unmapper->unmap_and_destroy_page(page);\n+void ZPageAllocator::map_committed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZVirtualMemory remaining = vmem;\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    assert(!allocation->commit_failed(), \"Sanity check\");\n@@ -494,2 +1910,1 @@\n-  \/\/ Allocate new virtual memory at a low address\n-  const ZVirtualMemory vmem = _virtual.alloc(pmem.size(), true \/* force_low_address *\/);\n+    ZPartition& partition = allocation->partition();\n@@ -497,3 +1912,2 @@\n-  \/\/ Create the new page and map it\n-  ZPage* new_page = new ZPage(ZPageType::small, vmem, pmem);\n-  map_page(new_page);\n+    \/\/ Split off the partial allocation's memory range\n+    const ZVirtualMemory to_vmem = remaining.shrink_from_front(allocation->size());\n@@ -501,2 +1915,3 @@\n-  \/\/ Update statistics\n-  ZStatInc(ZCounterDefragment);\n+    \/\/ Map the partial_allocation to partial_vmem\n+    partition.map_virtual_from_multi_partition(to_vmem);\n+  }\n@@ -504,1 +1919,1 @@\n-  return new_page;\n+  assert(remaining.size() == 0, \"all memory must be accounted for\");\n@@ -507,4 +1922,2 @@\n-bool ZPageAllocator::is_alloc_allowed(size_t size) const {\n-  const size_t available = _current_max_capacity - _used - _claimed;\n-  return available >= size;\n-}\n+void ZPageAllocator::cleanup_failed_commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n@@ -512,5 +1925,1 @@\n-bool ZPageAllocator::alloc_page_common_inner(ZPageType type, size_t size, ZList<ZPage>* pages) {\n-  if (!is_alloc_allowed(size)) {\n-    \/\/ Out of memory\n-    return false;\n-  }\n+  assert(allocation->commit_failed(), \"Must have failed to commit\");\n@@ -518,7 +1927,4 @@\n-  \/\/ Try allocate from the page cache\n-  ZPage* const page = _cache.alloc_page(type, size);\n-  if (page != nullptr) {\n-    \/\/ Success\n-    pages->insert_last(page);\n-    return true;\n-  }\n+  const size_t committed = allocation->committed_capacity();\n+  const ZVirtualMemory non_harvested_vmem = vmem.last_part(allocation->harvested());\n+  const ZVirtualMemory committed_vmem = non_harvested_vmem.first_part(committed);\n+  const ZVirtualMemory non_committed_vmem = non_harvested_vmem.last_part(committed);\n@@ -526,7 +1932,5 @@\n-  \/\/ Try increase capacity\n-  const size_t increased = increase_capacity(size);\n-  if (increased < size) {\n-    \/\/ Could not increase capacity enough to satisfy the allocation\n-    \/\/ completely. Flush the page cache to satisfy the remainder.\n-    const size_t remaining = size - increased;\n-    _cache.flush_for_allocation(remaining, pages);\n+  if (committed_vmem.size() > 0) {\n+    \/\/ Register the committed and mapped memory. We insert the committed\n+    \/\/ memory into partial_vmems so that it will be inserted into the cache\n+    \/\/ in a subsequent step.\n+    allocation->partial_vmems()->append(committed_vmem);\n@@ -535,2 +1939,4 @@\n-  \/\/ Success\n-  return true;\n+  \/\/ Free the virtual and physical memory we fetched to use but failed to commit\n+  ZPartition& partition = allocation->partition();\n+  partition.free_physical(non_committed_vmem);\n+  partition.free_virtual(non_committed_vmem);\n@@ -539,5 +1945,5 @@\n-bool ZPageAllocator::alloc_page_common(ZPageAllocation* allocation) {\n-  const ZPageType type = allocation->type();\n-  const size_t size = allocation->size();\n-  const ZAllocationFlags flags = allocation->flags();\n-  ZList<ZPage>* const pages = allocation->pages();\n+void ZPageAllocator::cleanup_failed_commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZVirtualMemory remaining = vmem;\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    \/\/ Split off the partial allocation's memory range\n+    const ZVirtualMemory partial_vmem = remaining.shrink_from_front(allocation->size());\n@@ -545,4 +1951,5 @@\n-  if (!alloc_page_common_inner(type, size, pages)) {\n-    \/\/ Out of memory\n-    return false;\n-  }\n+    if (allocation->harvested() == allocation->size()) {\n+      \/\/ Everything is harvested, the mappings are already in the partial_vmems,\n+      \/\/ nothing to cleanup.\n+      continue;\n+    }\n@@ -550,2 +1957,4 @@\n-  \/\/ Updated used statistics\n-  increase_used(size);\n+    const size_t committed = allocation->committed_capacity();\n+    const ZVirtualMemory non_harvested_vmem = vmem.last_part(allocation->harvested());\n+    const ZVirtualMemory committed_vmem = non_harvested_vmem.first_part(committed);\n+    const ZVirtualMemory non_committed_vmem = non_harvested_vmem.last_part(committed);\n@@ -553,3 +1962,1 @@\n-  \/\/ Success\n-  return true;\n-}\n+    ZPartition& partition = allocation->partition();\n@@ -557,5 +1964,6 @@\n-static void check_out_of_memory_during_initialization() {\n-  if (!is_init_completed()) {\n-    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\", \"Java heap too small\");\n-  }\n-}\n+    if (allocation->commit_failed()) {\n+      \/\/ Free the physical memory we failed to commit. Virtual memory is later\n+      \/\/ freed for the entire multi-partition allocation after all memory\n+      \/\/ allocations have been visited.\n+      partition.free_physical(non_committed_vmem);\n+    }\n@@ -563,3 +1971,4 @@\n-bool ZPageAllocator::alloc_page_stall(ZPageAllocation* allocation) {\n-  ZStatTimer timer(ZCriticalPhaseAllocationStall);\n-  EventZAllocationStall event;\n+    if (committed_vmem.size() == 0) {\n+      \/\/ Nothing committed, nothing more to cleanup\n+      continue;\n+    }\n@@ -567,2 +1976,2 @@\n-  \/\/ We can only block if the VM is fully initialized\n-  check_out_of_memory_during_initialization();\n+    \/\/ Remove the harvested part\n+    const ZVirtualMemory non_harvest_vmem = partial_vmem.last_part(allocation->harvested());\n@@ -570,3 +1979,1 @@\n-  \/\/ Start asynchronous minor GC\n-  const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, 0);\n-  ZDriver::minor()->collect(request);\n+    ZArray<ZVirtualMemory>* const partial_vmems = allocation->partial_vmems();\n@@ -574,2 +1981,2 @@\n-  \/\/ Wait for allocation to complete or fail\n-  const bool result = allocation->wait();\n+    \/\/ Keep track of the start index\n+    const int start_index = partial_vmems->length();\n@@ -577,10 +1984,2 @@\n-  {\n-    \/\/ Guard deletion of underlying semaphore. This is a workaround for\n-    \/\/ a bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n-    \/\/ the semaphore immediately after returning from sem_wait(). The\n-    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n-    \/\/ thread have returned from sem_wait(). To avoid this race we are\n-    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n-    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n-    ZLocker<ZLock> locker(&_lock);\n-  }\n+    \/\/ Claim virtual memory for the committed part\n+    const size_t claimed_virtual = partition.claim_virtual(committed, partial_vmems);\n@@ -588,2 +1987,4 @@\n-  \/\/ Send event\n-  event.commit((u8)allocation->type(), allocation->size());\n+    \/\/ We are holding memory associated with this partition, and we do not\n+    \/\/ overcommit virtual memory claiming. So virtual memory must always be\n+    \/\/ available.\n+    assert(claimed_virtual == committed, \"must succeed\");\n@@ -591,2 +1992,1 @@\n-  return result;\n-}\n+    \/\/ Associate and map the physical memory with the partial vmems\n@@ -594,3 +1994,3 @@\n-bool ZPageAllocator::alloc_page_or_stall(ZPageAllocation* allocation) {\n-  {\n-    ZLocker<ZLock> locker(&_lock);\n+    ZVirtualMemory remaining_committed_vmem = committed_vmem;\n+    for (const ZVirtualMemory& to_vmem : partial_vmems->slice_back(start_index)) {\n+      const ZVirtualMemory from_vmem = remaining_committed_vmem.shrink_from_front(to_vmem.size());\n@@ -598,4 +1998,2 @@\n-    if (alloc_page_common(allocation)) {\n-      \/\/ Success\n-      return true;\n-    }\n+      \/\/ Copy physical mappings\n+      partition.copy_physical_segments_to_partition(to_vmem, from_vmem);\n@@ -603,4 +2001,2 @@\n-    \/\/ Failed\n-    if (allocation->flags().non_blocking()) {\n-      \/\/ Don't stall\n-      return false;\n+      \/\/ Map memory\n+      partition.map_virtual(to_vmem);\n@@ -609,2 +2005,1 @@\n-    \/\/ Enqueue allocation request\n-    _stalled.insert_last(allocation);\n+    assert(remaining_committed_vmem.size() == 0, \"all memory must be accounted for\");\n@@ -613,2 +2008,4 @@\n-  \/\/ Stall\n-  return alloc_page_stall(allocation);\n+  assert(remaining.size() == 0, \"all memory must be accounted for\");\n+\n+  \/\/ Free the unused virtual memory\n+  _virtual.insert_multi_partition(vmem);\n@@ -617,2 +2014,3 @@\n-ZPage* ZPageAllocator::alloc_page_create(ZPageAllocation* allocation) {\n-  const size_t size = allocation->size();\n+void ZPageAllocator::free_after_alloc_page_failed(ZPageAllocation* allocation) {\n+  \/\/ Send event for failed allocation\n+  allocation->send_event(false \/* successful *\/);\n@@ -620,9 +2018,1 @@\n-  \/\/ Allocate virtual memory. To make error handling a lot more straight\n-  \/\/ forward, we allocate virtual memory before destroying flushed pages.\n-  \/\/ Flushed pages are also unmapped and destroyed asynchronously, so we\n-  \/\/ can't immediately reuse that part of the address space anyway.\n-  const ZVirtualMemory vmem = _virtual.alloc(size, allocation->flags().low_address());\n-  if (vmem.is_null()) {\n-    log_error(gc)(\"Out of address space\");\n-    return nullptr;\n-  }\n+  ZLocker<ZLock> locker(&_lock);\n@@ -630,2 +2020,2 @@\n-  ZPhysicalMemory pmem;\n-  size_t flushed = 0;\n+  \/\/ Free memory\n+  free_memory_alloc_failed(allocation);\n@@ -633,4 +2023,9 @@\n-  \/\/ Harvest physical memory from flushed pages\n-  ZListRemoveIterator<ZPage> iter(allocation->pages());\n-  for (ZPage* page; iter.next(&page);) {\n-    flushed += page->size();\n+  \/\/ Keep track of usage\n+  decrease_used(allocation->size());\n+\n+  \/\/ Reset allocation for a potential retry\n+  allocation->reset_for_retry();\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n+}\n@@ -638,4 +2033,3 @@\n-    \/\/ Harvest flushed physical memory\n-    ZPhysicalMemory& fmem = page->physical_memory();\n-    pmem.add_segments(fmem);\n-    fmem.remove_segments();\n+void ZPageAllocator::free_memory_alloc_failed(ZPageAllocation* allocation) {\n+  \/\/ The current max capacity may be decreased, store the value before freeing memory\n+  const size_t current_max_capacity_before = current_max_capacity();\n@@ -643,2 +2037,4 @@\n-    \/\/ Unmap and destroy page\n-    _unmapper->unmap_and_destroy_page(page);\n+  if (allocation->is_multi_partition()) {\n+    free_memory_alloc_failed_multi_partition(allocation->multi_partition_allocation());\n+  } else {\n+    free_memory_alloc_failed_single_partition(allocation->single_partition_allocation());\n@@ -647,2 +2043,1 @@\n-  if (flushed > 0) {\n-    allocation->set_flushed(flushed);\n+  const size_t current_max_capacity_after = current_max_capacity();\n@@ -650,3 +2045,5 @@\n-    \/\/ Update statistics\n-    ZStatInc(ZCounterPageCacheFlush, flushed);\n-    log_debug(gc, heap)(\"Page Cache Flushed: %zuM\", flushed \/ M);\n+  if (current_max_capacity_before != current_max_capacity_after) {\n+    log_error_p(gc)(\"Forced to lower max Java heap size from \"\n+                    \"%zuM(%.0f%%) to %zuM(%.0f%%)\",\n+                    current_max_capacity_before \/ M, percent_of(current_max_capacity_before, _max_capacity),\n+                    current_max_capacity_after \/ M, percent_of(current_max_capacity_after, _max_capacity));\n@@ -654,0 +2051,5 @@\n+}\n+\n+void ZPageAllocator::free_memory_alloc_failed_single_partition(ZSinglePartitionAllocation* single_partition_allocation) {\n+  free_memory_alloc_failed(single_partition_allocation->allocation());\n+}\n@@ -655,7 +2057,3 @@\n-  \/\/ Allocate any remaining physical memory. Capacity and used has\n-  \/\/ already been adjusted, we just need to fetch the memory, which\n-  \/\/ is guaranteed to succeed.\n-  if (flushed < size) {\n-    const size_t remaining = size - flushed;\n-    allocation->set_committed(remaining);\n-    _physical.alloc(pmem, remaining);\n+void ZPageAllocator::free_memory_alloc_failed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation) {\n+  for (ZMemoryAllocation* allocation : *multi_partition_allocation->allocations()) {\n+    free_memory_alloc_failed(allocation);\n@@ -663,0 +2061,4 @@\n+}\n+\n+void ZPageAllocator::free_memory_alloc_failed(ZMemoryAllocation* allocation) {\n+  ZPartition& partition = allocation->partition();\n@@ -664,2 +2066,1 @@\n-  \/\/ Create new page\n-  return new ZPage(allocation->type(), vmem, pmem);\n+  partition.free_memory_alloc_failed(allocation);\n@@ -668,6 +2069,10 @@\n-bool ZPageAllocator::is_alloc_satisfied(ZPageAllocation* allocation) const {\n-  \/\/ The allocation is immediately satisfied if the list of pages contains\n-  \/\/ exactly one page, with the type and size that was requested. However,\n-  \/\/ even if the allocation is immediately satisfied we might still want to\n-  \/\/ return false here to force the page to be remapped to fight address\n-  \/\/ space fragmentation.\n+ZPage* ZPageAllocator::create_page(ZPageAllocation* allocation, const ZVirtualMemory& vmem) {\n+  \/\/ We don't track generation usage when claiming capacity, because this page\n+  \/\/ could have been allocated by a thread that satisfies a stalling allocation.\n+  \/\/ The stalled thread can wake up and potentially realize that the page alloc\n+  \/\/ should be undone. If the alloc and the undo gets separated by a safepoint,\n+  \/\/ the generation statistics could se a decreasing used value between mark\n+  \/\/ start and mark end. At this point an allocation will be successful, so we\n+  \/\/ update the generation usage.\n+  const ZGenerationId id = allocation->age() == ZPageAge::old ? ZGenerationId::old : ZGenerationId::young;\n+  increase_used_generation(id, allocation->size());\n@@ -675,4 +2080,2 @@\n-  if (allocation->pages()->size() != 1) {\n-    \/\/ Not a single page\n-    return false;\n-  }\n+  const ZPageType type = allocation->type();\n+  const ZPageAge age = allocation->age();\n@@ -680,5 +2083,5 @@\n-  const ZPage* const page = allocation->pages()->first();\n-  if (page->type() != allocation->type() ||\n-      page->size() != allocation->size()) {\n-    \/\/ Wrong type or size\n-    return false;\n+  if (allocation->is_multi_partition()) {\n+    const ZMultiPartitionAllocation* const multi_partition_allocation = allocation->multi_partition_allocation();\n+    ZMultiPartitionTracker* const tracker = ZMultiPartitionTracker::create(multi_partition_allocation, vmem);\n+\n+    return new ZPage(type, age, vmem, tracker);\n@@ -687,2 +2090,4 @@\n-  \/\/ Allocation immediately satisfied\n-  return true;\n+  const ZSinglePartitionAllocation* const single_partition_allocation = allocation->single_partition_allocation();\n+  const uint32_t partition_id = single_partition_allocation->allocation()->partition().numa_id();\n+\n+  return new ZPage(type, age, vmem, partition_id);\n@@ -691,5 +2096,5 @@\n-ZPage* ZPageAllocator::alloc_page_finalize(ZPageAllocation* allocation) {\n-  \/\/ Fast path\n-  if (is_alloc_satisfied(allocation)) {\n-    return allocation->pages()->remove_first();\n-  }\n+void ZPageAllocator::prepare_memory_for_free(ZPage* page, ZArray<ZVirtualMemory>* vmems) {\n+  \/\/ Extract memory and destroy the page\n+  const ZVirtualMemory vmem = page->virtual_memory();\n+  const ZPageType page_type = page->type();\n+  const ZMultiPartitionTracker* const tracker = page->multi_partition_tracker();\n@@ -697,6 +2102,1 @@\n-  \/\/ Slow path\n-  ZPage* const page = alloc_page_create(allocation);\n-  if (page == nullptr) {\n-    \/\/ Out of address space\n-    return nullptr;\n-  }\n+  safe_destroy_page(page);\n@@ -704,6 +2104,6 @@\n-  \/\/ Commit page\n-  if (commit_page(page)) {\n-    \/\/ Success\n-    map_page(page);\n-    return page;\n-  }\n+  \/\/ Multi-partition memory is always remapped\n+  if (tracker != nullptr) {\n+    tracker->prepare_memory_for_free(vmem, vmems);\n+\n+    \/\/ Free the virtual memory\n+    _virtual.insert_multi_partition(vmem);\n@@ -711,5 +2111,4 @@\n-  \/\/ Failed or partially failed. Split of any successfully committed\n-  \/\/ part of the page into a new page and insert it into list of pages,\n-  \/\/ so that it will be re-inserted into the page cache.\n-  ZPage* const committed_page = page->split_committed();\n-  destroy_page(page);\n+    \/\/ Destroy the tracker\n+    ZMultiPartitionTracker::destroy(tracker);\n+    return;\n+  }\n@@ -717,3 +2116,4 @@\n-  if (committed_page != nullptr) {\n-    map_page(committed_page);\n-    allocation->pages()->insert_last(committed_page);\n+  \/\/ Try to remap and defragment if page is large\n+  if (page_type == ZPageType::large) {\n+    remap_and_defragment(vmem, vmems);\n+    return;\n@@ -722,1 +2122,2 @@\n-  return nullptr;\n+  \/\/ Leave the memory untouched\n+  vmems->append(vmem);\n@@ -725,2 +2126,2 @@\n-ZPage* ZPageAllocator::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age) {\n-  EventZPageAllocation event;\n+void ZPageAllocator::remap_and_defragment(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out) {\n+  ZPartition& partition = partition_from_vmem(vmem);\n@@ -728,11 +2129,4 @@\n-retry:\n-  ZPageAllocation allocation(type, size, flags);\n-\n-  \/\/ Allocate one or more pages from the page cache. If the allocation\n-  \/\/ succeeds but the returned pages don't cover the complete allocation,\n-  \/\/ then finalize phase is allowed to allocate the remaining memory\n-  \/\/ directly from the physical memory manager. Note that this call might\n-  \/\/ block in a safepoint if the non-blocking flag is not set.\n-  if (!alloc_page_or_stall(&allocation)) {\n-    \/\/ Out of memory\n-    return nullptr;\n+  \/\/ If no lower address can be found, don't remap\/defrag\n+  if (_virtual.lowest_available_address(partition.numa_id()) > vmem.start()) {\n+    vmems_out->append(vmem);\n+    return;\n@@ -741,7 +2135,1 @@\n-  ZPage* const page = alloc_page_finalize(&allocation);\n-  if (page == nullptr) {\n-    \/\/ Failed to commit or map. Clean up and retry, in the hope that\n-    \/\/ we can still allocate by flushing the page cache (more aggressively).\n-    free_pages_alloc_failed(&allocation);\n-    goto retry;\n-  }\n+  ZStatInc(ZCounterDefragment);\n@@ -749,5 +2137,2 @@\n-  \/\/ The generation's used is tracked here when the page is handed out\n-  \/\/ to the allocating thread. The overall heap \"used\" is tracked in\n-  \/\/ the lower-level allocation code.\n-  const ZGenerationId id = age == ZPageAge::old ? ZGenerationId::old : ZGenerationId::young;\n-  increase_used_generation(id, size);\n+  \/\/ Synchronously unmap the virtual memory\n+  partition.unmap_virtual(vmem);\n@@ -755,9 +2140,3 @@\n-  \/\/ Reset page. This updates the page's sequence number and must\n-  \/\/ be done after we potentially blocked in a safepoint (stalled)\n-  \/\/ where the global sequence number was updated.\n-  page->reset(age);\n-  page->reset_top_for_allocation();\n-  page->reset_livemap();\n-  if (age == ZPageAge::old) {\n-    page->remset_alloc();\n-  }\n+  \/\/ Stash segments\n+  ZArray<zbacking_index> stash(vmem.granule_count());\n+  _physical.stash_segments(vmem, &stash);\n@@ -765,7 +2144,15 @@\n-  \/\/ Update allocation statistics. Exclude gc relocations to avoid\n-  \/\/ artificial inflation of the allocation rate during relocation.\n-  if (!flags.gc_relocation() && is_init_completed()) {\n-    \/\/ Note that there are two allocation rate counters, which have\n-    \/\/ different purposes and are sampled at different frequencies.\n-    ZStatInc(ZCounterMutatorAllocationRate, size);\n-    ZStatMutatorAllocRate::sample_allocation(size);\n+  \/\/ Shuffle vmem - put new vmems in vmems_out\n+  const int start_index = vmems_out->length();\n+  partition.free_and_claim_virtual_from_low_many(vmem, vmems_out);\n+\n+  \/\/ The output array may contain results from other defragmentations as well,\n+  \/\/ so we only operate on the result(s) we just got.\n+  ZArraySlice<ZVirtualMemory> defragmented_vmems = vmems_out->slice_back(start_index);\n+\n+  \/\/ Restore segments\n+  _physical.restore_segments(defragmented_vmems, stash);\n+\n+  \/\/ Map and pre-touch\n+  for (const ZVirtualMemory& claimed_vmem : defragmented_vmems) {\n+    partition.map_virtual(claimed_vmem);\n+    pretouch_memory(claimed_vmem.start(), claimed_vmem.size());\n@@ -773,0 +2160,1 @@\n+}\n@@ -774,3 +2162,2 @@\n-  \/\/ Send event\n-  event.commit((u8)type, size, allocation.flushed(), allocation.committed(),\n-               page->physical_memory().nsegments(), flags.non_blocking());\n+void ZPageAllocator::free_memory(ZArray<ZVirtualMemory>* vmems) {\n+  ZLocker<ZLock> locker(&_lock);\n@@ -778,1 +2165,13 @@\n-  return page;\n+  \/\/ Free the vmems\n+  for (const ZVirtualMemory vmem : *vmems) {\n+    ZPartition& partition = partition_from_vmem(vmem);\n+\n+    \/\/ Free the vmem\n+    partition.free_memory(vmem);\n+\n+    \/\/ Keep track of usage\n+    decrease_used(vmem.size());\n+  }\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n@@ -789,1 +2188,1 @@\n-    if (!alloc_page_common(allocation)) {\n+    if (!claim_capacity(allocation)) {\n@@ -794,0 +2193,3 @@\n+    \/\/ Keep track of usage\n+    increase_used(allocation->size());\n+\n@@ -802,15 +2204,2 @@\n-ZPage* ZPageAllocator::prepare_to_recycle(ZPage* page, bool allow_defragment) {\n-  \/\/ Make sure we have a page that is safe to recycle\n-  ZPage* const to_recycle = _safe_recycle.register_and_clone_if_activated(page);\n-\n-  \/\/ Defragment the page before recycle if allowed and needed\n-  if (allow_defragment && should_defragment(to_recycle)) {\n-    return defragment_page(to_recycle);\n-  }\n-\n-  \/\/ Remove the remset before recycling\n-  if (to_recycle->is_old() && to_recycle == page) {\n-    to_recycle->remset_delete();\n-  }\n-\n-  return to_recycle;\n+bool ZPageAllocator::is_multi_partition_enabled() const {\n+  return _virtual.is_multi_partition_enabled();\n@@ -819,6 +2208,2 @@\n-void ZPageAllocator::recycle_page(ZPage* page) {\n-  \/\/ Set time when last used\n-  page->set_last_used();\n-\n-  \/\/ Cache page\n-  _cache.free_page(page);\n+const ZPartition& ZPageAllocator::partition_from_partition_id(uint32_t numa_id) const {\n+  return _partitions.get(numa_id);\n@@ -827,18 +2212,2 @@\n-void ZPageAllocator::free_page(ZPage* page, bool allow_defragment) {\n-  const ZGenerationId generation_id = page->generation_id();\n-\n-  \/\/ Prepare page for recycling before taking the lock\n-  ZPage* const to_recycle = prepare_to_recycle(page, allow_defragment);\n-\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  \/\/ Update used statistics\n-  const size_t size = to_recycle->size();\n-  decrease_used(size);\n-  decrease_used_generation(generation_id, size);\n-\n-  \/\/ Free page\n-  recycle_page(to_recycle);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n+ZPartition& ZPageAllocator::partition_from_partition_id(uint32_t numa_id) {\n+  return _partitions.get(numa_id);\n@@ -847,14 +2216,3 @@\n-void ZPageAllocator::free_pages(const ZArray<ZPage*>* pages) {\n-  ZArray<ZPage*> to_recycle_pages;\n-\n-  size_t young_size = 0;\n-  size_t old_size = 0;\n-\n-  \/\/ Prepare pages for recycling before taking the lock\n-  ZArrayIterator<ZPage*> pages_iter(pages);\n-  for (ZPage* page; pages_iter.next(&page);) {\n-    if (page->is_young()) {\n-      young_size += page->size();\n-    } else {\n-      old_size += page->size();\n-    }\n+ZPartition& ZPageAllocator::partition_from_vmem(const ZVirtualMemory& vmem) {\n+  return partition_from_partition_id(_virtual.lookup_partition_id(vmem));\n+}\n@@ -862,2 +2220,2 @@\n-    \/\/ Prepare to recycle\n-    ZPage* const to_recycle = prepare_to_recycle(page, true \/* allow_defragment *\/);\n+size_t ZPageAllocator::sum_available() const {\n+  size_t total = 0;\n@@ -865,2 +2223,3 @@\n-    \/\/ Register for recycling\n-    to_recycle_pages.push(to_recycle);\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    total += partition->available();\n@@ -869,1 +2228,2 @@\n-  ZLocker<ZLock> locker(&_lock);\n+  return total;\n+}\n@@ -871,4 +2231,3 @@\n-  \/\/ Update used statistics\n-  decrease_used(young_size + old_size);\n-  decrease_used_generation(ZGenerationId::young, young_size);\n-  decrease_used_generation(ZGenerationId::old, old_size);\n+void ZPageAllocator::increase_used(size_t size) {\n+  \/\/ Update atomically since we have concurrent readers\n+  const size_t used = Atomic::add(&_used, size);\n@@ -876,4 +2235,5 @@\n-  \/\/ Free pages\n-  ZArrayIterator<ZPage*> iter(&to_recycle_pages);\n-  for (ZPage* page; iter.next(&page);) {\n-    recycle_page(page);\n+  \/\/ Update used high\n+  for (auto& stats : _collection_stats) {\n+    if (used > stats._used_high) {\n+      stats._used_high = used;\n+    }\n@@ -881,3 +2241,0 @@\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n@@ -886,11 +2243,3 @@\n-void ZPageAllocator::free_pages_alloc_failed(ZPageAllocation* allocation) {\n-  \/\/ The page(s) in the allocation are either taken from the cache or a newly\n-  \/\/ created, mapped and commited ZPage. These page(s) have not been inserted in\n-  \/\/ the page table, nor allocated a remset, so prepare_to_recycle is not required.\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  \/\/ Only decrease the overall used and not the generation used,\n-  \/\/ since the allocation failed and generation used wasn't bumped.\n-  decrease_used(allocation->size());\n-\n-  size_t freed = 0;\n+void ZPageAllocator::decrease_used(size_t size) {\n+  \/\/ Update atomically since we have concurrent readers\n+  const size_t used = Atomic::sub(&_used, size);\n@@ -898,5 +2247,5 @@\n-  \/\/ Free any allocated\/flushed pages\n-  ZListRemoveIterator<ZPage> iter(allocation->pages());\n-  for (ZPage* page; iter.next(&page);) {\n-    freed += page->size();\n-    recycle_page(page);\n+  \/\/ Update used low\n+  for (auto& stats : _collection_stats) {\n+    if (used < stats._used_low) {\n+      stats._used_low = used;\n+    }\n@@ -904,7 +2253,0 @@\n-\n-  \/\/ Adjust capacity and used to reflect the failed capacity increase\n-  const size_t remaining = allocation->size() - freed;\n-  decrease_capacity(remaining, true \/* set_max_capacity *\/);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n@@ -913,5 +2255,4 @@\n-size_t ZPageAllocator::uncommit(uint64_t* timeout) {\n-  \/\/ We need to join the suspendible thread set while manipulating capacity and\n-  \/\/ used, to make sure GC safepoints will have a consistent view.\n-  ZList<ZPage> pages;\n-  size_t flushed;\n+void ZPageAllocator::safe_destroy_page(ZPage* page) {\n+  \/\/ Destroy page safely\n+  _safe_destroy.schedule_delete(page);\n+}\n@@ -919,3 +2260,4 @@\n-  {\n-    SuspendibleThreadSetJoiner sts_joiner;\n-    ZLocker<ZLock> locker(&_lock);\n+void ZPageAllocator::free_page(ZPage* page) {\n+  \/\/ Extract the id from the page\n+  const ZGenerationId id = page->generation_id();\n+  const size_t size = page->size();\n@@ -923,7 +2265,3 @@\n-    \/\/ Never uncommit below min capacity. We flush out and uncommit chunks at\n-    \/\/ a time (~0.8% of the max capacity, but at least one granule and at most\n-    \/\/ 256M), in case demand for memory increases while we are uncommitting.\n-    const size_t retain = MAX2(_used, _min_capacity);\n-    const size_t release = _capacity - retain;\n-    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, ZGranuleSize), 256 * M);\n-    const size_t flush = MIN2(release, limit);\n+  \/\/ Extract vmems and destroy the page\n+  ZArray<ZVirtualMemory> vmems;\n+  prepare_memory_for_free(page, &vmems);\n@@ -931,6 +2269,2 @@\n-    \/\/ Flush pages to uncommit\n-    flushed = _cache.flush_for_uncommit(flush, &pages, timeout);\n-    if (flushed == 0) {\n-      \/\/ Nothing flushed\n-      return 0;\n-    }\n+  \/\/ Updated used statistics\n+  decrease_used_generation(id, size);\n@@ -938,3 +2272,3 @@\n-    \/\/ Record flushed pages as claimed\n-    Atomic::add(&_claimed, flushed);\n-  }\n+  \/\/ Free the extracted vmems\n+  free_memory(&vmems);\n+}\n@@ -942,7 +2276,6 @@\n-  \/\/ Unmap, uncommit, and destroy flushed pages\n-  ZListRemoveIterator<ZPage> iter(&pages);\n-  for (ZPage* page; iter.next(&page);) {\n-    unmap_page(page);\n-    uncommit_page(page);\n-    destroy_page(page);\n-  }\n+void ZPageAllocator::free_pages(ZGenerationId id, const ZArray<ZPage*>* pages) {\n+  \/\/ Prepare memory from pages to be cached\n+  ZArray<ZVirtualMemory> vmems;\n+  for (ZPage* page : *pages) {\n+    assert(page->generation_id() == id, \"All pages must be from the same generation\");\n+    const size_t size = page->size();\n@@ -950,3 +2283,2 @@\n-  {\n-    SuspendibleThreadSetJoiner sts_joiner;\n-    ZLocker<ZLock> locker(&_lock);\n+    \/\/ Extract vmems and destroy the page\n+    prepare_memory_for_free(page, &vmems);\n@@ -954,3 +2286,2 @@\n-    \/\/ Adjust claimed and capacity to reflect the uncommit\n-    Atomic::sub(&_claimed, flushed);\n-    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n+    \/\/ Updated used statistics\n+    decrease_used_generation(id, size);\n@@ -959,1 +2290,2 @@\n-  return flushed;\n+  \/\/ Free the extracted vmems\n+  free_memory(&vmems);\n@@ -970,8 +2302,0 @@\n-void ZPageAllocator::enable_safe_recycle() const {\n-  _safe_recycle.activate();\n-}\n-\n-void ZPageAllocator::disable_safe_recycle() const {\n-  _safe_recycle.deactivate();\n-}\n-\n@@ -1048,0 +2372,8 @@\n+ZPartitionConstIterator ZPageAllocator::partition_iterator() const {\n+  return ZPartitionConstIterator(&_partitions);\n+}\n+\n+ZPartitionIterator ZPageAllocator::partition_iterator() {\n+  return ZPartitionIterator(&_partitions);\n+}\n+\n@@ -1049,2 +2381,81 @@\n-  tc->do_thread(_unmapper);\n-  tc->do_thread(_uncommitter);\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    partition->threads_do(tc);\n+  }\n+}\n+\n+void ZPageAllocator::print_on(outputStream* st) const {\n+  ZLocker<ZLock> lock(&_lock);\n+  print_on_inner(st);\n+}\n+\n+static bool try_lock_on_error(ZLock* lock) {\n+  if (VMError::is_error_reported() && VMError::is_error_reported_in_current_thread()) {\n+    return lock->try_lock();\n+  }\n+\n+  lock->lock();\n+\n+  return true;\n+}\n+\n+void ZPageAllocator::print_extended_on_error(outputStream* st) const {\n+  st->print_cr(\"ZMappedCache:\");\n+\n+  streamIndentor indentor(st, 1);\n+\n+  if (!try_lock_on_error(&_lock)) {\n+    \/\/ We can't print without taking the lock since printing the contents of\n+    \/\/ the cache requires iterating over the nodes in the cache's tree, which\n+    \/\/ is not thread-safe.\n+    st->print_cr(\"<Skipped>\");\n+\n+    return;\n+  }\n+\n+  \/\/ Print each partition's cache content\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    partition->print_extended_on_error(st);\n+  }\n+\n+  _lock.unlock();\n+}\n+\n+void ZPageAllocator::print_on_error(outputStream* st) const {\n+  const bool locked = try_lock_on_error(&_lock);\n+\n+  if (!locked) {\n+    st->print_cr(\"<Without lock>\");\n+  }\n+\n+  \/\/ Print information even though we have not successfully taken the lock.\n+  \/\/ This is thread-safe, but may produce inconsistent results.\n+  print_on_inner(st);\n+\n+  if (locked) {\n+    _lock.unlock();\n+  }\n+}\n+\n+void ZPageAllocator::print_on_inner(outputStream* st) const {\n+  \/\/ Print total usage\n+  st->print(\"ZHeap\");\n+  st->fill_to(17);\n+  st->print_cr(\"used %zuM, capacity %zuM, max capacity %zuM\",\n+               used() \/ M, capacity() \/ M, max_capacity() \/ M);\n+\n+  \/\/ Print per-partition\n+\n+  streamIndentor indentor(st, 1);\n+\n+  if (_partitions.count() == 1) {\n+    \/\/ The summary printing is redundant if we only have one partition\n+    _partitions.get(0).print_cache_on(st);\n+    return;\n+  }\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    partition->print_on(st);\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":2030,"deletions":619,"binary":false,"changes":2649,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zGranuleMap.hpp\"\n@@ -31,0 +34,2 @@\n+#include \"gc\/z\/zMappedCache.hpp\"\n+#include \"gc\/z\/zPage.hpp\"\n@@ -32,1 +37,0 @@\n-#include \"gc\/z\/zPageCache.hpp\"\n@@ -34,1 +38,1 @@\n-#include \"gc\/z\/zPhysicalMemory.hpp\"\n+#include \"gc\/z\/zPhysicalMemoryManager.hpp\"\n@@ -36,1 +40,4 @@\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n+#include \"gc\/z\/zUncommitter.hpp\"\n+#include \"gc\/z\/zValue.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.hpp\"\n+#include \"utilities\/ostream.hpp\"\n@@ -40,0 +47,2 @@\n+class ZMemoryAllocation;\n+class ZMultiPartitionAllocation;\n@@ -43,0 +52,3 @@\n+class ZSegmentStash;\n+class ZSinglePartitionAllocation;\n+class ZVirtualMemory;\n@@ -44,2 +56,0 @@\n-class ZUncommitter;\n-class ZUnmapper;\n@@ -47,1 +57,4 @@\n-class ZSafePageRecycle {\n+class ZPartition {\n+  friend class VMStructs;\n+  friend class ZPageAllocator;\n+\n@@ -49,2 +62,24 @@\n-  ZPageAllocator*        _page_allocator;\n-  ZActivatedArray<ZPage> _unsafe_to_recycle;\n+  ZPageAllocator* const _page_allocator;\n+  ZMappedCache          _cache;\n+  ZUncommitter          _uncommitter;\n+  const size_t          _min_capacity;\n+  const size_t          _max_capacity;\n+  volatile size_t       _current_max_capacity;\n+  volatile size_t       _capacity;\n+  volatile size_t       _claimed;\n+  size_t                _used;\n+  double                _last_commit;\n+  double                _last_uncommit;\n+  size_t                _to_uncommit;\n+  const uint32_t        _numa_id;\n+\n+  const ZVirtualMemoryManager& virtual_memory_manager() const;\n+  ZVirtualMemoryManager& virtual_memory_manager();\n+\n+  const ZPhysicalMemoryManager& physical_memory_manager() const;\n+  ZPhysicalMemoryManager& physical_memory_manager();\n+\n+  void verify_virtual_memory_multi_partition_association(const ZVirtualMemory& vmem) const NOT_DEBUG_RETURN;\n+  void verify_virtual_memory_association(const ZVirtualMemory& vmem, bool check_multi_partition = false) const NOT_DEBUG_RETURN;\n+  void verify_virtual_memory_association(const ZArray<ZVirtualMemory>* vmems) const NOT_DEBUG_RETURN;\n+  void verify_memory_allocation_association(const ZMemoryAllocation* allocation) const NOT_DEBUG_RETURN;\n@@ -53,1 +88,18 @@\n-  ZSafePageRecycle(ZPageAllocator* page_allocator);\n+  ZPartition(uint32_t numa_id, ZPageAllocator* page_allocator);\n+\n+  uint32_t numa_id() const;\n+\n+  size_t available() const;\n+\n+  size_t increase_capacity(size_t size);\n+  void decrease_capacity(size_t size, bool set_max_capacity);\n+\n+  void increase_used(size_t size);\n+  void decrease_used(size_t size);\n+\n+  void free_memory(const ZVirtualMemory& vmem);\n+\n+  void claim_from_cache_or_increase_capacity(ZMemoryAllocation* allocation);\n+  bool claim_capacity(ZMemoryAllocation* allocation);\n+\n+  size_t uncommit(uint64_t* timeout);\n@@ -55,2 +107,1 @@\n-  void activate();\n-  void deactivate();\n+  void sort_segments_physical(const ZVirtualMemory& vmem);\n@@ -58,1 +109,35 @@\n-  ZPage* register_and_clone_if_activated(ZPage* page);\n+  void claim_physical(const ZVirtualMemory& vmem);\n+  void free_physical(const ZVirtualMemory& vmem);\n+  size_t commit_physical(const ZVirtualMemory& vmem);\n+  size_t uncommit_physical(const ZVirtualMemory& vmem);\n+\n+  void map_virtual(const ZVirtualMemory& vmem);\n+  void unmap_virtual(const ZVirtualMemory& vmem);\n+\n+  void map_virtual_from_multi_partition(const ZVirtualMemory& vmem);\n+  void unmap_virtual_from_multi_partition(const ZVirtualMemory& vmem);\n+\n+  ZVirtualMemory claim_virtual(size_t size);\n+  size_t claim_virtual(size_t size, ZArray<ZVirtualMemory>* vmems_out);\n+  void free_virtual(const ZVirtualMemory& vmem);\n+\n+  void free_and_claim_virtual_from_low_many(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out);\n+  ZVirtualMemory free_and_claim_virtual_from_low_exact_or_many(size_t size, ZArray<ZVirtualMemory>* vmems_in_out);\n+\n+  bool prime(ZWorkers* workers, size_t size);\n+\n+  ZVirtualMemory prepare_harvested_and_claim_virtual(ZMemoryAllocation* allocation);\n+\n+  void copy_physical_segments_to_partition(const ZVirtualMemory& at, const ZVirtualMemory& from);\n+  void copy_physical_segments_from_partition(const ZVirtualMemory& at, const ZVirtualMemory& to);\n+\n+  void commit_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n+  void map_memory(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n+\n+  void free_memory_alloc_failed(ZMemoryAllocation* allocation);\n+\n+  void threads_do(ThreadClosure* tc) const;\n+\n+  void print_on(outputStream* st) const;\n+  void print_cache_on(outputStream* st) const;\n+  void print_extended_on_error(outputStream* st) const;\n@@ -61,0 +146,3 @@\n+using ZPartitionIterator = ZPerNUMAIterator<ZPartition>;\n+using ZPartitionConstIterator = ZPerNUMAConstIterator<ZPartition>;\n+\n@@ -63,1 +151,2 @@\n-  friend class ZUnmapper;\n+  friend class ZMultiPartitionTracker;\n+  friend class ZPartition;\n@@ -67,12 +156,7 @@\n-  mutable ZLock              _lock;\n-  ZPageCache                 _cache;\n-  ZVirtualMemoryManager      _virtual;\n-  ZPhysicalMemoryManager     _physical;\n-  const size_t               _min_capacity;\n-  const size_t               _initial_capacity;\n-  const size_t               _max_capacity;\n-  volatile size_t            _current_max_capacity;\n-  volatile size_t            _capacity;\n-  volatile size_t            _claimed;\n-  volatile size_t            _used;\n-  size_t                     _used_generations[2];\n+  mutable ZLock               _lock;\n+  ZVirtualMemoryManager       _virtual;\n+  ZPhysicalMemoryManager      _physical;\n+  const size_t                _min_capacity;\n+  const size_t                _max_capacity;\n+  volatile size_t             _used;\n+  volatile size_t             _used_generations[2];\n@@ -80,9 +164,7 @@\n-    size_t                   _used_high;\n-    size_t                   _used_low;\n-  } _collection_stats[2];\n-  ZList<ZPageAllocation>     _stalled;\n-  ZUnmapper*                 _unmapper;\n-  ZUncommitter*              _uncommitter;\n-  mutable ZSafeDelete<ZPage> _safe_destroy;\n-  mutable ZSafePageRecycle   _safe_recycle;\n-  bool                       _initialized;\n+    size_t _used_high;\n+    size_t _used_low;\n+  }                           _collection_stats[2];\n+  ZPerNUMA<ZPartition>        _partitions;\n+  ZList<ZPageAllocation>      _stalled;\n+  mutable ZSafeDelete<ZPage>  _safe_destroy;\n+  bool                        _initialized;\n@@ -90,2 +172,2 @@\n-  size_t increase_capacity(size_t size);\n-  void decrease_capacity(size_t size, bool set_max_capacity);\n+  bool alloc_page_stall(ZPageAllocation* allocation);\n+  ZPage* alloc_page_inner(ZPageAllocation* allocation);\n@@ -93,2 +175,4 @@\n-  void increase_used(size_t size);\n-  void decrease_used(size_t size);\n+  bool claim_capacity_or_stall(ZPageAllocation* allocation);\n+  bool claim_capacity(ZPageAllocation* allocation);\n+  bool claim_capacity_single_partition(ZSinglePartitionAllocation* single_partition_allocation, uint32_t partition_id);\n+  void claim_capacity_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, uint32_t start_partition);\n@@ -96,2 +180,1 @@\n-  void increase_used_generation(ZGenerationId id, size_t size);\n-  void decrease_used_generation(ZGenerationId id, size_t size);\n+  ZVirtualMemory satisfied_from_cache_vmem(const ZPageAllocation* allocation) const;\n@@ -99,2 +182,3 @@\n-  bool commit_page(ZPage* page);\n-  void uncommit_page(ZPage* page);\n+  ZVirtualMemory claim_virtual_memory(ZPageAllocation* allocation);\n+  ZVirtualMemory claim_virtual_memory_single_partition(ZSinglePartitionAllocation* single_partition_allocation);\n+  ZVirtualMemory claim_virtual_memory_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation);\n@@ -102,2 +186,1 @@\n-  void map_page(const ZPage* page) const;\n-  void unmap_page(const ZPage* page) const;\n+  void copy_claimed_physical_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n@@ -105,1 +188,4 @@\n-  void destroy_page(ZPage* page);\n+  void claim_physical_for_increased_capacity(ZPageAllocation* allocation, const ZVirtualMemory& vmem);\n+  void claim_physical_for_increased_capacity_single_partition(ZSinglePartitionAllocation* allocation, const ZVirtualMemory& vmem);\n+  void claim_physical_for_increased_capacity_multi_partition(const ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n+  void claim_physical_for_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n@@ -107,2 +193,3 @@\n-  bool should_defragment(const ZPage* page) const;\n-  ZPage* defragment_page(ZPage* page);\n+  bool commit_and_map(ZPageAllocation* allocation, const ZVirtualMemory& vmem);\n+  bool commit_and_map_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  bool commit_and_map_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n@@ -110,1 +197,3 @@\n-  bool is_alloc_allowed(size_t size) const;\n+  void commit(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n+  bool commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  bool commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n@@ -112,8 +201,20 @@\n-  bool alloc_page_common_inner(ZPageType type, size_t size, ZList<ZPage>* pages);\n-  bool alloc_page_common(ZPageAllocation* allocation);\n-  bool alloc_page_stall(ZPageAllocation* allocation);\n-  bool alloc_page_or_stall(ZPageAllocation* allocation);\n-  bool is_alloc_satisfied(ZPageAllocation* allocation) const;\n-  ZPage* alloc_page_create(ZPageAllocation* allocation);\n-  ZPage* alloc_page_finalize(ZPageAllocation* allocation);\n-  void free_pages_alloc_failed(ZPageAllocation* allocation);\n+  void unmap_harvested_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation);\n+\n+  void map_committed_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  void map_committed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n+\n+  void cleanup_failed_commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  void cleanup_failed_commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n+\n+  void free_after_alloc_page_failed(ZPageAllocation* allocation);\n+\n+  void free_memory_alloc_failed(ZPageAllocation* allocation);\n+  void free_memory_alloc_failed_single_partition(ZSinglePartitionAllocation* single_partition_allocation);\n+  void free_memory_alloc_failed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation);\n+  void free_memory_alloc_failed(ZMemoryAllocation* allocation);\n+\n+  ZPage* create_page(ZPageAllocation* allocation, const ZVirtualMemory& vmem);\n+\n+  void prepare_memory_for_free(ZPage* page, ZArray<ZVirtualMemory>* vmems);\n+  void remap_and_defragment(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out);\n+  void free_memory(ZArray<ZVirtualMemory>* vmems);\n@@ -123,1 +224,10 @@\n-  size_t uncommit(uint64_t* timeout);\n+  bool is_multi_partition_enabled() const;\n+\n+  const ZPartition& partition_from_partition_id(uint32_t partition_id) const;\n+  ZPartition&       partition_from_partition_id(uint32_t partition_id);\n+  ZPartition&       partition_from_vmem(const ZVirtualMemory& vmem);\n+\n+  size_t sum_available() const;\n+\n+  void increase_used(size_t size);\n+  void decrease_used(size_t size);\n@@ -128,0 +238,2 @@\n+  void print_on_inner(outputStream* st) const;\n+\n@@ -138,1 +250,0 @@\n-  size_t initial_capacity() const;\n@@ -142,0 +253,1 @@\n+  size_t current_max_capacity() const;\n@@ -147,1 +259,4 @@\n-  void promote_used(size_t size);\n+  void increase_used_generation(ZGenerationId id, size_t size);\n+  void decrease_used_generation(ZGenerationId id, size_t size);\n+\n+  void promote_used(const ZPage* from, const ZPage* to);\n@@ -154,2 +269,0 @@\n-  ZPage* prepare_to_recycle(ZPage* page, bool allow_defragment);\n-  void recycle_page(ZPage* page);\n@@ -157,2 +270,2 @@\n-  void free_page(ZPage* page, bool allow_defragment);\n-  void free_pages(const ZArray<ZPage*>* pages);\n+  void free_page(ZPage* page);\n+  void free_pages(ZGenerationId id, const ZArray<ZPage*>* pages);\n@@ -163,3 +276,0 @@\n-  void enable_safe_recycle() const;\n-  void disable_safe_recycle() const;\n-\n@@ -171,0 +281,3 @@\n+  ZPartitionConstIterator partition_iterator() const;\n+  ZPartitionIterator partition_iterator();\n+\n@@ -172,0 +285,4 @@\n+\n+  void print_on(outputStream* st) const;\n+  void print_extended_on_error(outputStream* st) const;\n+  void print_on_error(outputStream* st) const;\n@@ -176,12 +293,12 @@\n-  size_t _min_capacity;\n-  size_t _max_capacity;\n-  size_t _soft_max_capacity;\n-  size_t _capacity;\n-  size_t _used;\n-  size_t _used_high;\n-  size_t _used_low;\n-  size_t _used_generation;\n-  size_t _freed;\n-  size_t _promoted;\n-  size_t _compacted;\n-  size_t _allocation_stalls;\n+  const size_t _min_capacity;\n+  const size_t _max_capacity;\n+  const size_t _soft_max_capacity;\n+  const size_t _capacity;\n+  const size_t _used;\n+  const size_t _used_high;\n+  const size_t _used_low;\n+  const size_t _used_generation;\n+  const size_t _freed;\n+  const size_t _promoted;\n+  const size_t _compacted;\n+  const size_t _allocation_stalls;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":196,"deletions":79,"binary":false,"changes":275,"status":"modified"},{"patch":"@@ -1,332 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"gc\/z\/zList.inline.hpp\"\n-#include \"gc\/z\/zNUMA.inline.hpp\"\n-#include \"gc\/z\/zPage.inline.hpp\"\n-#include \"gc\/z\/zPageCache.hpp\"\n-#include \"gc\/z\/zStat.hpp\"\n-#include \"gc\/z\/zValue.inline.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-\n-static const ZStatCounter ZCounterPageCacheHitL1(\"Memory\", \"Page Cache Hit L1\", ZStatUnitOpsPerSecond);\n-static const ZStatCounter ZCounterPageCacheHitL2(\"Memory\", \"Page Cache Hit L2\", ZStatUnitOpsPerSecond);\n-static const ZStatCounter ZCounterPageCacheHitL3(\"Memory\", \"Page Cache Hit L3\", ZStatUnitOpsPerSecond);\n-static const ZStatCounter ZCounterPageCacheMiss(\"Memory\", \"Page Cache Miss\", ZStatUnitOpsPerSecond);\n-\n-class ZPageCacheFlushClosure : public StackObj {\n-  friend class ZPageCache;\n-\n-protected:\n-  const size_t _requested;\n-  size_t       _flushed;\n-\n-public:\n-  ZPageCacheFlushClosure(size_t requested);\n-  virtual bool do_page(const ZPage* page) = 0;\n-};\n-\n-ZPageCacheFlushClosure::ZPageCacheFlushClosure(size_t requested)\n-  : _requested(requested),\n-    _flushed(0) {}\n-\n-ZPageCache::ZPageCache()\n-  : _small(),\n-    _medium(),\n-    _large(),\n-    _last_commit(0) {}\n-\n-ZPage* ZPageCache::alloc_small_page() {\n-  const uint32_t numa_id = ZNUMA::id();\n-  const uint32_t numa_count = ZNUMA::count();\n-\n-  \/\/ Try NUMA local page cache\n-  ZPage* const l1_page = _small.get(numa_id).remove_first();\n-  if (l1_page != nullptr) {\n-    ZStatInc(ZCounterPageCacheHitL1);\n-    return l1_page;\n-  }\n-\n-  \/\/ Try NUMA remote page cache(s)\n-  uint32_t remote_numa_id = numa_id + 1;\n-  const uint32_t remote_numa_count = numa_count - 1;\n-  for (uint32_t i = 0; i < remote_numa_count; i++) {\n-    if (remote_numa_id == numa_count) {\n-      remote_numa_id = 0;\n-    }\n-\n-    ZPage* const l2_page = _small.get(remote_numa_id).remove_first();\n-    if (l2_page != nullptr) {\n-      ZStatInc(ZCounterPageCacheHitL2);\n-      return l2_page;\n-    }\n-\n-    remote_numa_id++;\n-  }\n-\n-  return nullptr;\n-}\n-\n-ZPage* ZPageCache::alloc_medium_page() {\n-  ZPage* const page = _medium.remove_first();\n-  if (page != nullptr) {\n-    ZStatInc(ZCounterPageCacheHitL1);\n-    return page;\n-  }\n-\n-  return nullptr;\n-}\n-\n-ZPage* ZPageCache::alloc_large_page(size_t size) {\n-  \/\/ Find a page with the right size\n-  ZListIterator<ZPage> iter(&_large);\n-  for (ZPage* page; iter.next(&page);) {\n-    if (size == page->size()) {\n-      \/\/ Page found\n-      _large.remove(page);\n-      ZStatInc(ZCounterPageCacheHitL1);\n-      return page;\n-    }\n-  }\n-\n-  return nullptr;\n-}\n-\n-ZPage* ZPageCache::alloc_oversized_medium_page(size_t size) {\n-  if (size <= ZPageSizeMedium) {\n-    return _medium.remove_first();\n-  }\n-\n-  return nullptr;\n-}\n-\n-ZPage* ZPageCache::alloc_oversized_large_page(size_t size) {\n-  \/\/ Find a page that is large enough\n-  ZListIterator<ZPage> iter(&_large);\n-  for (ZPage* page; iter.next(&page);) {\n-    if (size <= page->size()) {\n-      \/\/ Page found\n-      _large.remove(page);\n-      return page;\n-    }\n-  }\n-\n-  return nullptr;\n-}\n-\n-ZPage* ZPageCache::alloc_oversized_page(size_t size) {\n-  ZPage* page = alloc_oversized_large_page(size);\n-  if (page == nullptr) {\n-    page = alloc_oversized_medium_page(size);\n-  }\n-\n-  if (page != nullptr) {\n-    ZStatInc(ZCounterPageCacheHitL3);\n-  }\n-\n-  return page;\n-}\n-\n-ZPage* ZPageCache::alloc_page(ZPageType type, size_t size) {\n-  ZPage* page;\n-\n-  \/\/ Try allocate exact page\n-  if (type == ZPageType::small) {\n-    page = alloc_small_page();\n-  } else if (type == ZPageType::medium) {\n-    page = alloc_medium_page();\n-  } else {\n-    page = alloc_large_page(size);\n-  }\n-\n-  if (page == nullptr) {\n-    \/\/ Try allocate potentially oversized page\n-    ZPage* const oversized = alloc_oversized_page(size);\n-    if (oversized != nullptr) {\n-      if (size < oversized->size()) {\n-        \/\/ Split oversized page\n-        page = oversized->split(type, size);\n-\n-        \/\/ Cache remainder\n-        free_page(oversized);\n-      } else {\n-        \/\/ Re-type correctly sized page\n-        page = oversized->retype(type);\n-      }\n-    }\n-  }\n-\n-  if (page == nullptr) {\n-    ZStatInc(ZCounterPageCacheMiss);\n-  }\n-\n-  return page;\n-}\n-\n-void ZPageCache::free_page(ZPage* page) {\n-  const ZPageType type = page->type();\n-  if (type == ZPageType::small) {\n-    _small.get(page->numa_id()).insert_first(page);\n-  } else if (type == ZPageType::medium) {\n-    _medium.insert_first(page);\n-  } else {\n-    _large.insert_first(page);\n-  }\n-}\n-\n-bool ZPageCache::flush_list_inner(ZPageCacheFlushClosure* cl, ZList<ZPage>* from, ZList<ZPage>* to) {\n-  ZPage* const page = from->last();\n-  if (page == nullptr || !cl->do_page(page)) {\n-    \/\/ Don't flush page\n-    return false;\n-  }\n-\n-  \/\/ Flush page\n-  from->remove(page);\n-  to->insert_last(page);\n-  return true;\n-}\n-\n-void ZPageCache::flush_list(ZPageCacheFlushClosure* cl, ZList<ZPage>* from, ZList<ZPage>* to) {\n-  while (flush_list_inner(cl, from, to));\n-}\n-\n-void ZPageCache::flush_per_numa_lists(ZPageCacheFlushClosure* cl, ZPerNUMA<ZList<ZPage> >* from, ZList<ZPage>* to) {\n-  const uint32_t numa_count = ZNUMA::count();\n-  uint32_t numa_done = 0;\n-  uint32_t numa_next = 0;\n-\n-  \/\/ Flush lists round-robin\n-  while (numa_done < numa_count) {\n-    ZList<ZPage>* const numa_list = from->addr(numa_next);\n-    if (++numa_next == numa_count) {\n-      numa_next = 0;\n-    }\n-\n-    if (flush_list_inner(cl, numa_list, to)) {\n-      \/\/ Not done\n-      numa_done = 0;\n-    } else {\n-      \/\/ Done\n-      numa_done++;\n-    }\n-  }\n-}\n-\n-void ZPageCache::flush(ZPageCacheFlushClosure* cl, ZList<ZPage>* to) {\n-  \/\/ Prefer flushing large, then medium and last small pages\n-  flush_list(cl, &_large, to);\n-  flush_list(cl, &_medium, to);\n-  flush_per_numa_lists(cl, &_small, to);\n-\n-  if (cl->_flushed > cl->_requested) {\n-    \/\/ Overflushed, re-insert part of last page into the cache\n-    const size_t overflushed = cl->_flushed - cl->_requested;\n-    ZPage* const reinsert = to->last()->split(overflushed);\n-    free_page(reinsert);\n-    cl->_flushed -= overflushed;\n-  }\n-}\n-\n-class ZPageCacheFlushForAllocationClosure : public ZPageCacheFlushClosure {\n-public:\n-  ZPageCacheFlushForAllocationClosure(size_t requested)\n-    : ZPageCacheFlushClosure(requested) {}\n-\n-  virtual bool do_page(const ZPage* page) {\n-    if (_flushed < _requested) {\n-      \/\/ Flush page\n-      _flushed += page->size();\n-      return true;\n-    }\n-\n-    \/\/ Don't flush page\n-    return false;\n-  }\n-};\n-\n-void ZPageCache::flush_for_allocation(size_t requested, ZList<ZPage>* to) {\n-  ZPageCacheFlushForAllocationClosure cl(requested);\n-  flush(&cl, to);\n-}\n-\n-class ZPageCacheFlushForUncommitClosure : public ZPageCacheFlushClosure {\n-private:\n-  const uint64_t _now;\n-  uint64_t*      _timeout;\n-\n-public:\n-  ZPageCacheFlushForUncommitClosure(size_t requested, uint64_t now, uint64_t* timeout)\n-    : ZPageCacheFlushClosure(requested),\n-      _now(now),\n-      _timeout(timeout) {\n-    \/\/ Set initial timeout\n-    *_timeout = ZUncommitDelay;\n-  }\n-\n-  virtual bool do_page(const ZPage* page) {\n-    const uint64_t expires = page->last_used() + ZUncommitDelay;\n-    if (expires > _now) {\n-      \/\/ Don't flush page, record shortest non-expired timeout\n-      *_timeout = MIN2(*_timeout, expires - _now);\n-      return false;\n-    }\n-\n-    if (_flushed >= _requested) {\n-      \/\/ Don't flush page, requested amount flushed\n-      return false;\n-    }\n-\n-    \/\/ Flush page\n-    _flushed += page->size();\n-    return true;\n-  }\n-};\n-\n-size_t ZPageCache::flush_for_uncommit(size_t requested, ZList<ZPage>* to, uint64_t* timeout) {\n-  const uint64_t now = (uint64_t)os::elapsedTime();\n-  const uint64_t expires = _last_commit + ZUncommitDelay;\n-  if (expires > now) {\n-    \/\/ Delay uncommit, set next timeout\n-    *timeout = expires - now;\n-    return 0;\n-  }\n-\n-  if (requested == 0) {\n-    \/\/ Nothing to flush, set next timeout\n-    *timeout = ZUncommitDelay;\n-    return 0;\n-  }\n-\n-  ZPageCacheFlushForUncommitClosure cl(requested, now, timeout);\n-  flush(&cl, to);\n-\n-  return cl._flushed;\n-}\n-\n-void ZPageCache::set_last_commit() {\n-  _last_commit = (uint64_t)ceil(os::elapsedTime());\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zPageCache.cpp","additions":0,"deletions":332,"binary":false,"changes":332,"status":"deleted"},{"patch":"@@ -1,66 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZPAGECACHE_HPP\n-#define SHARE_GC_Z_ZPAGECACHE_HPP\n-\n-#include \"gc\/z\/zList.hpp\"\n-#include \"gc\/z\/zPage.hpp\"\n-#include \"gc\/z\/zPageType.hpp\"\n-#include \"gc\/z\/zValue.hpp\"\n-\n-class ZPageCacheFlushClosure;\n-\n-class ZPageCache {\n-private:\n-  ZPerNUMA<ZList<ZPage> > _small;\n-  ZList<ZPage>            _medium;\n-  ZList<ZPage>            _large;\n-  uint64_t                _last_commit;\n-\n-  ZPage* alloc_small_page();\n-  ZPage* alloc_medium_page();\n-  ZPage* alloc_large_page(size_t size);\n-\n-  ZPage* alloc_oversized_medium_page(size_t size);\n-  ZPage* alloc_oversized_large_page(size_t size);\n-  ZPage* alloc_oversized_page(size_t size);\n-\n-  bool flush_list_inner(ZPageCacheFlushClosure* cl, ZList<ZPage>* from, ZList<ZPage>* to);\n-  void flush_list(ZPageCacheFlushClosure* cl, ZList<ZPage>* from, ZList<ZPage>* to);\n-  void flush_per_numa_lists(ZPageCacheFlushClosure* cl, ZPerNUMA<ZList<ZPage> >* from, ZList<ZPage>* to);\n-  void flush(ZPageCacheFlushClosure* cl, ZList<ZPage>* to);\n-\n-public:\n-  ZPageCache();\n-\n-  ZPage* alloc_page(ZPageType type, size_t size);\n-  void free_page(ZPage* page);\n-\n-  void flush_for_allocation(size_t requested, ZList<ZPage>* to);\n-  size_t flush_for_uncommit(size_t requested, ZList<ZPage>* to, uint64_t* timeout);\n-\n-  void set_last_commit();\n-};\n-\n-#endif \/\/ SHARE_GC_Z_ZPAGECACHE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPageCache.hpp","additions":0,"deletions":66,"binary":false,"changes":66,"status":"deleted"},{"patch":"@@ -84,1 +84,0 @@\n-  _page_allocator->enable_safe_recycle();\n@@ -88,1 +87,0 @@\n-  _page_allocator->disable_safe_recycle();\n@@ -97,1 +95,0 @@\n-  _page_allocator->enable_safe_recycle();\n@@ -101,1 +98,0 @@\n-  _page_allocator->disable_safe_recycle();\n","filename":"src\/hotspot\/share\/gc\/z\/zPageTable.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -106,1 +106,0 @@\n-  _page_allocator->disable_safe_recycle();\n@@ -110,1 +109,0 @@\n-  _page_allocator->enable_safe_recycle();\n","filename":"src\/hotspot\/share\/gc\/z\/zPageTable.inline.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,386 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"gc\/z\/zArray.inline.hpp\"\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"gc\/z\/zLargePages.inline.hpp\"\n-#include \"gc\/z\/zList.inline.hpp\"\n-#include \"gc\/z\/zNMT.hpp\"\n-#include \"gc\/z\/zNUMA.inline.hpp\"\n-#include \"gc\/z\/zPhysicalMemory.inline.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/init.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-ZPhysicalMemory::ZPhysicalMemory()\n-  : _segments() {}\n-\n-ZPhysicalMemory::ZPhysicalMemory(const ZPhysicalMemorySegment& segment)\n-  : _segments() {\n-  _segments.append(segment);\n-}\n-\n-ZPhysicalMemory::ZPhysicalMemory(const ZPhysicalMemory& pmem)\n-  : _segments(pmem.nsegments()) {\n-  _segments.appendAll(&pmem._segments);\n-}\n-\n-const ZPhysicalMemory& ZPhysicalMemory::operator=(const ZPhysicalMemory& pmem) {\n-  \/\/ Check for self-assignment\n-  if (this == &pmem) {\n-    return *this;\n-  }\n-\n-  \/\/ Free and copy segments\n-  _segments.clear_and_deallocate();\n-  _segments.reserve(pmem.nsegments());\n-  _segments.appendAll(&pmem._segments);\n-\n-  return *this;\n-}\n-\n-size_t ZPhysicalMemory::size() const {\n-  size_t size = 0;\n-\n-  for (int i = 0; i < _segments.length(); i++) {\n-    size += _segments.at(i).size();\n-  }\n-\n-  return size;\n-}\n-\n-void ZPhysicalMemory::insert_segment(int index, zoffset start, size_t size, bool committed) {\n-  _segments.insert_before(index, ZPhysicalMemorySegment(start, size, committed));\n-}\n-\n-void ZPhysicalMemory::replace_segment(int index, zoffset start, size_t size, bool committed) {\n-  _segments.at_put(index, ZPhysicalMemorySegment(start, size, committed));\n-}\n-\n-void ZPhysicalMemory::remove_segment(int index) {\n-  _segments.remove_at(index);\n-}\n-\n-void ZPhysicalMemory::add_segments(const ZPhysicalMemory& pmem) {\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    add_segment(pmem.segment(i));\n-  }\n-}\n-\n-void ZPhysicalMemory::remove_segments() {\n-  _segments.clear_and_deallocate();\n-}\n-\n-static bool is_mergable(const ZPhysicalMemorySegment& before, const ZPhysicalMemorySegment& after) {\n-  return before.end() == after.start() && before.is_committed() == after.is_committed();\n-}\n-\n-void ZPhysicalMemory::add_segment(const ZPhysicalMemorySegment& segment) {\n-  \/\/ Insert segments in address order, merge segments when possible\n-  for (int i = _segments.length(); i > 0; i--) {\n-    const int current = i - 1;\n-\n-    if (_segments.at(current).end() <= segment.start()) {\n-      if (is_mergable(_segments.at(current), segment)) {\n-        if (current + 1 < _segments.length() && is_mergable(segment, _segments.at(current + 1))) {\n-          \/\/ Merge with end of current segment and start of next segment\n-          const zoffset start = _segments.at(current).start();\n-          const size_t size = _segments.at(current).size() + segment.size() + _segments.at(current + 1).size();\n-          replace_segment(current, start, size, segment.is_committed());\n-          remove_segment(current + 1);\n-          return;\n-        }\n-\n-        \/\/ Merge with end of current segment\n-        const zoffset start = _segments.at(current).start();\n-        const size_t size = _segments.at(current).size() + segment.size();\n-        replace_segment(current, start, size, segment.is_committed());\n-        return;\n-      } else if (current + 1 < _segments.length() && is_mergable(segment, _segments.at(current + 1))) {\n-        \/\/ Merge with start of next segment\n-        const zoffset start = segment.start();\n-        const size_t size = segment.size() + _segments.at(current + 1).size();\n-        replace_segment(current + 1, start, size, segment.is_committed());\n-        return;\n-      }\n-\n-      \/\/ Insert after current segment\n-      insert_segment(current + 1, segment.start(), segment.size(), segment.is_committed());\n-      return;\n-    }\n-  }\n-\n-  if (_segments.length() > 0 && is_mergable(segment, _segments.at(0))) {\n-    \/\/ Merge with start of first segment\n-    const zoffset start = segment.start();\n-    const size_t size = segment.size() + _segments.at(0).size();\n-    replace_segment(0, start, size, segment.is_committed());\n-    return;\n-  }\n-\n-  \/\/ Insert before first segment\n-  insert_segment(0, segment.start(), segment.size(), segment.is_committed());\n-}\n-\n-bool ZPhysicalMemory::commit_segment(int index, size_t size) {\n-  assert(size <= _segments.at(index).size(), \"Invalid size\");\n-  assert(!_segments.at(index).is_committed(), \"Invalid state\");\n-\n-  if (size == _segments.at(index).size()) {\n-    \/\/ Completely committed\n-    _segments.at(index).set_committed(true);\n-    return true;\n-  }\n-\n-  if (size > 0) {\n-    \/\/ Partially committed, split segment\n-    insert_segment(index + 1, _segments.at(index).start() + size, _segments.at(index).size() - size, false \/* committed *\/);\n-    replace_segment(index, _segments.at(index).start(), size, true \/* committed *\/);\n-  }\n-\n-  return false;\n-}\n-\n-bool ZPhysicalMemory::uncommit_segment(int index, size_t size) {\n-  assert(size <= _segments.at(index).size(), \"Invalid size\");\n-  assert(_segments.at(index).is_committed(), \"Invalid state\");\n-\n-  if (size == _segments.at(index).size()) {\n-    \/\/ Completely uncommitted\n-    _segments.at(index).set_committed(false);\n-    return true;\n-  }\n-\n-  if (size > 0) {\n-    \/\/ Partially uncommitted, split segment\n-    insert_segment(index + 1, _segments.at(index).start() + size, _segments.at(index).size() - size, true \/* committed *\/);\n-    replace_segment(index, _segments.at(index).start(), size, false \/* committed *\/);\n-  }\n-\n-  return false;\n-}\n-\n-ZPhysicalMemory ZPhysicalMemory::split(size_t size) {\n-  ZPhysicalMemory pmem;\n-  int nsegments = 0;\n-\n-  for (int i = 0; i < _segments.length(); i++) {\n-    const ZPhysicalMemorySegment& segment = _segments.at(i);\n-    if (pmem.size() < size) {\n-      if (pmem.size() + segment.size() <= size) {\n-        \/\/ Transfer segment\n-        pmem.add_segment(segment);\n-      } else {\n-        \/\/ Split segment\n-        const size_t split_size = size - pmem.size();\n-        pmem.add_segment(ZPhysicalMemorySegment(segment.start(), split_size, segment.is_committed()));\n-        _segments.at_put(nsegments++, ZPhysicalMemorySegment(segment.start() + split_size, segment.size() - split_size, segment.is_committed()));\n-      }\n-    } else {\n-      \/\/ Keep segment\n-      _segments.at_put(nsegments++, segment);\n-    }\n-  }\n-\n-  _segments.trunc_to(nsegments);\n-\n-  return pmem;\n-}\n-\n-ZPhysicalMemory ZPhysicalMemory::split_committed() {\n-  ZPhysicalMemory pmem;\n-  int nsegments = 0;\n-\n-  for (int i = 0; i < _segments.length(); i++) {\n-    const ZPhysicalMemorySegment& segment = _segments.at(i);\n-    if (segment.is_committed()) {\n-      \/\/ Transfer segment\n-      pmem.add_segment(segment);\n-    } else {\n-      \/\/ Keep segment\n-      _segments.at_put(nsegments++, segment);\n-    }\n-  }\n-\n-  _segments.trunc_to(nsegments);\n-\n-  return pmem;\n-}\n-\n-ZPhysicalMemoryManager::ZPhysicalMemoryManager(size_t max_capacity)\n-  : _backing(max_capacity) {\n-  \/\/ Make the whole range free\n-  _manager.register_range(zoffset(0), max_capacity);\n-}\n-\n-bool ZPhysicalMemoryManager::is_initialized() const {\n-  return _backing.is_initialized();\n-}\n-\n-void ZPhysicalMemoryManager::warn_commit_limits(size_t max_capacity) const {\n-  _backing.warn_commit_limits(max_capacity);\n-}\n-\n-void ZPhysicalMemoryManager::try_enable_uncommit(size_t min_capacity, size_t max_capacity) {\n-  assert(!is_init_completed(), \"Invalid state\");\n-\n-  \/\/ If uncommit is not explicitly disabled, max capacity is greater than\n-  \/\/ min capacity, and uncommit is supported by the platform, then uncommit\n-  \/\/ will be enabled.\n-  if (!ZUncommit) {\n-    log_info_p(gc, init)(\"Uncommit: Disabled\");\n-    return;\n-  }\n-\n-  if (max_capacity == min_capacity) {\n-    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (-Xms equals -Xmx)\");\n-    FLAG_SET_ERGO(ZUncommit, false);\n-    return;\n-  }\n-\n-  \/\/ Test if uncommit is supported by the operating system by committing\n-  \/\/ and then uncommitting a granule.\n-  ZPhysicalMemory pmem(ZPhysicalMemorySegment(zoffset(0), ZGranuleSize, false \/* committed *\/));\n-  if (!commit(pmem) || !uncommit(pmem)) {\n-    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (Not supported by operating system)\");\n-    FLAG_SET_ERGO(ZUncommit, false);\n-    return;\n-  }\n-\n-  log_info_p(gc, init)(\"Uncommit: Enabled\");\n-  log_info_p(gc, init)(\"Uncommit Delay: %zus\", ZUncommitDelay);\n-}\n-\n-void ZPhysicalMemoryManager::alloc(ZPhysicalMemory& pmem, size_t size) {\n-  assert(is_aligned(size, ZGranuleSize), \"Invalid size\");\n-\n-  \/\/ Allocate segments\n-  while (size > 0) {\n-    size_t allocated = 0;\n-    const zoffset start = _manager.alloc_low_address_at_most(size, &allocated);\n-    assert(start != zoffset(UINTPTR_MAX), \"Allocation should never fail\");\n-    pmem.add_segment(ZPhysicalMemorySegment(start, allocated, false \/* committed *\/));\n-    size -= allocated;\n-  }\n-}\n-\n-void ZPhysicalMemoryManager::free(const ZPhysicalMemory& pmem) {\n-  \/\/ Free segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const ZPhysicalMemorySegment& segment = pmem.segment(i);\n-    _manager.free(segment.start(), segment.size());\n-  }\n-}\n-\n-bool ZPhysicalMemoryManager::commit(ZPhysicalMemory& pmem) {\n-  \/\/ Commit segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const ZPhysicalMemorySegment& segment = pmem.segment(i);\n-    if (segment.is_committed()) {\n-      \/\/ Segment already committed\n-      continue;\n-    }\n-\n-    \/\/ Commit segment\n-    const size_t committed = _backing.commit(segment.start(), segment.size());\n-\n-    \/\/ Register with NMT\n-    if (committed > 0) {\n-      ZNMT::commit(segment.start(), committed);\n-    }\n-\n-    \/\/ Register committed segment\n-    if (!pmem.commit_segment(i, committed)) {\n-      \/\/ Failed or partially failed\n-      return false;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-bool ZPhysicalMemoryManager::uncommit(ZPhysicalMemory& pmem) {\n-  \/\/ Commit segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const ZPhysicalMemorySegment& segment = pmem.segment(i);\n-    if (!segment.is_committed()) {\n-      \/\/ Segment already uncommitted\n-      continue;\n-    }\n-\n-    \/\/ Uncommit segment\n-    const size_t uncommitted = _backing.uncommit(segment.start(), segment.size());\n-\n-    \/\/ Unregister with NMT\n-    if (uncommitted > 0) {\n-      ZNMT::uncommit(segment.start(), uncommitted);\n-    }\n-\n-    \/\/ Deregister uncommitted segment\n-    if (!pmem.uncommit_segment(i, uncommitted)) {\n-      \/\/ Failed or partially failed\n-      return false;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-\/\/ Map virtual memory to physcial memory\n-void ZPhysicalMemoryManager::map(zoffset offset, const ZPhysicalMemory& pmem) const {\n-  const zaddress_unsafe addr = ZOffset::address_unsafe(offset);\n-\n-  size_t size = 0;\n-\n-  \/\/ Map segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const ZPhysicalMemorySegment& segment = pmem.segment(i);\n-    _backing.map(addr + size, segment.size(), segment.start());\n-    size += segment.size();\n-  }\n-\n-  \/\/ Setup NUMA interleaving for large pages\n-  if (ZNUMA::is_enabled() && ZLargePages::is_explicit()) {\n-    \/\/ To get granule-level NUMA interleaving when using large pages,\n-    \/\/ we simply let the kernel interleave the memory for us at page\n-    \/\/ fault time.\n-    os::numa_make_global((char*)addr, size);\n-  }\n-}\n-\n-\/\/ Unmap virtual memory from physical memory\n-void ZPhysicalMemoryManager::unmap(zoffset offset, size_t size) const {\n-  const zaddress_unsafe addr = ZOffset::address_unsafe(offset);\n-\n-  _backing.unmap(addr, size);\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.cpp","additions":0,"deletions":386,"binary":false,"changes":386,"status":"deleted"},{"patch":"@@ -1,105 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZPHYSICALMEMORY_HPP\n-#define SHARE_GC_Z_ZPHYSICALMEMORY_HPP\n-\n-#include \"gc\/z\/zAddress.hpp\"\n-#include \"gc\/z\/zArray.hpp\"\n-#include \"gc\/z\/zMemory.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include OS_HEADER(gc\/z\/zPhysicalMemoryBacking)\n-\n-class ZPhysicalMemorySegment : public CHeapObj<mtGC> {\n-private:\n-  zoffset     _start;\n-  zoffset_end _end;\n-  bool        _committed;\n-\n-public:\n-  ZPhysicalMemorySegment();\n-  ZPhysicalMemorySegment(zoffset start, size_t size, bool committed);\n-\n-  zoffset start() const;\n-  zoffset_end end() const;\n-  size_t size() const;\n-\n-  bool is_committed() const;\n-  void set_committed(bool committed);\n-};\n-\n-class ZPhysicalMemory {\n-private:\n-  ZArray<ZPhysicalMemorySegment> _segments;\n-\n-  void insert_segment(int index, zoffset start, size_t size, bool committed);\n-  void replace_segment(int index, zoffset start, size_t size, bool committed);\n-  void remove_segment(int index);\n-\n-public:\n-  ZPhysicalMemory();\n-  ZPhysicalMemory(const ZPhysicalMemorySegment& segment);\n-  ZPhysicalMemory(const ZPhysicalMemory& pmem);\n-  const ZPhysicalMemory& operator=(const ZPhysicalMemory& pmem);\n-\n-  bool is_null() const;\n-  size_t size() const;\n-\n-  int nsegments() const;\n-  const ZPhysicalMemorySegment& segment(int index) const;\n-\n-  void add_segments(const ZPhysicalMemory& pmem);\n-  void remove_segments();\n-\n-  void add_segment(const ZPhysicalMemorySegment& segment);\n-  bool commit_segment(int index, size_t size);\n-  bool uncommit_segment(int index, size_t size);\n-\n-  ZPhysicalMemory split(size_t size);\n-  ZPhysicalMemory split_committed();\n-};\n-\n-class ZPhysicalMemoryManager {\n-private:\n-  ZPhysicalMemoryBacking _backing;\n-  ZMemoryManager         _manager;\n-\n-public:\n-  ZPhysicalMemoryManager(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  void warn_commit_limits(size_t max_capacity) const;\n-  void try_enable_uncommit(size_t min_capacity, size_t max_capacity);\n-\n-  void alloc(ZPhysicalMemory& pmem, size_t size);\n-  void free(const ZPhysicalMemory& pmem);\n-\n-  bool commit(ZPhysicalMemory& pmem);\n-  bool uncommit(ZPhysicalMemory& pmem);\n-\n-  void map(zoffset offset, const ZPhysicalMemory& pmem) const;\n-  void unmap(zoffset offset, size_t size) const;\n-};\n-\n-#endif \/\/ SHARE_GC_Z_ZPHYSICALMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.hpp","additions":0,"deletions":105,"binary":false,"changes":105,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZPHYSICALMEMORY_INLINE_HPP\n-#define SHARE_GC_Z_ZPHYSICALMEMORY_INLINE_HPP\n-\n-#include \"gc\/z\/zPhysicalMemory.hpp\"\n-\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline ZPhysicalMemorySegment::ZPhysicalMemorySegment()\n-  : _start(zoffset(UINTPTR_MAX)),\n-    _end(zoffset_end(UINTPTR_MAX)),\n-    _committed(false) {}\n-\n-inline ZPhysicalMemorySegment::ZPhysicalMemorySegment(zoffset start, size_t size, bool committed)\n-  : _start(start),\n-    _end(to_zoffset_end(start, size)),\n-    _committed(committed) {}\n-\n-inline zoffset ZPhysicalMemorySegment::start() const {\n-  return _start;\n-}\n-\n-inline zoffset_end ZPhysicalMemorySegment::end() const {\n-  return _end;\n-}\n-\n-inline size_t ZPhysicalMemorySegment::size() const {\n-  return _end - _start;\n-}\n-\n-inline bool ZPhysicalMemorySegment::is_committed() const {\n-  return _committed;\n-}\n-\n-inline void ZPhysicalMemorySegment::set_committed(bool committed) {\n-  _committed = committed;\n-}\n-\n-inline bool ZPhysicalMemory::is_null() const {\n-  return _segments.length() == 0;\n-}\n-\n-inline int ZPhysicalMemory::nsegments() const {\n-  return _segments.length();\n-}\n-\n-inline const ZPhysicalMemorySegment& ZPhysicalMemory::segment(int index) const {\n-  return _segments.at(index);\n-}\n-\n-#endif \/\/ SHARE_GC_Z_ZPHYSICALMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.inline.hpp","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -0,0 +1,376 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zArray.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zLargePages.inline.hpp\"\n+#include \"gc\/z\/zList.inline.hpp\"\n+#include \"gc\/z\/zNMT.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"gc\/z\/zPhysicalMemoryManager.hpp\"\n+#include \"gc\/z\/zRangeRegistry.inline.hpp\"\n+#include \"gc\/z\/zUtils.inline.hpp\"\n+#include \"gc\/z\/zValue.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/init.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+ZPhysicalMemoryManager::ZPhysicalMemoryManager(size_t max_capacity)\n+  : _backing(max_capacity),\n+    _physical_mappings(ZAddressOffsetMax) {\n+  assert(is_aligned(max_capacity, ZGranuleSize), \"must be granule aligned\");\n+\n+  \/\/ Setup backing storage limits\n+  ZBackingOffsetMax = max_capacity;\n+  ZBackingIndexMax = checked_cast<uint32_t>(max_capacity >> ZGranuleSizeShift);\n+\n+  \/\/ Install capacity into the registry\n+  const size_t num_segments_total = max_capacity >> ZGranuleSizeShift;\n+  zbacking_index_end next_index = zbacking_index_end::zero;\n+  uint32_t numa_id;\n+  ZPerNUMAIterator<ZBackingIndexRegistry> iter(&_partition_registries);\n+  for (ZBackingIndexRegistry* registry; iter.next(&registry, &numa_id);) {\n+    const size_t num_segments = ZNUMA::calculate_share(numa_id, num_segments_total, 1 \/* granule *\/);\n+\n+    if (num_segments == 0) {\n+      \/\/ If the capacity consist of less granules than the number of partitions,\n+      \/\/ some partitions will be empty.\n+      break;\n+    }\n+\n+    const zbacking_index index = to_zbacking_index(next_index);\n+\n+    \/\/ Insert the next number of segment indices into id's partition's registry\n+    registry->insert({index, num_segments});\n+\n+    \/\/ Advance to next index by the inserted number of segment indices\n+    next_index += num_segments;\n+  }\n+\n+  assert(untype(next_index) == ZBackingIndexMax, \"must insert all capacity\");\n+}\n+\n+bool ZPhysicalMemoryManager::is_initialized() const {\n+  return _backing.is_initialized();\n+}\n+\n+void ZPhysicalMemoryManager::warn_commit_limits(size_t max_capacity) const {\n+  _backing.warn_commit_limits(max_capacity);\n+}\n+\n+void ZPhysicalMemoryManager::try_enable_uncommit(size_t min_capacity, size_t max_capacity) {\n+  assert(!is_init_completed(), \"Invalid state\");\n+\n+  \/\/ If uncommit is not explicitly disabled, max capacity is greater than\n+  \/\/ min capacity, and uncommit is supported by the platform, then uncommit\n+  \/\/ will be enabled.\n+  if (!ZUncommit) {\n+    log_info_p(gc, init)(\"Uncommit: Disabled\");\n+    return;\n+  }\n+\n+  if (max_capacity == min_capacity) {\n+    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (-Xms equals -Xmx)\");\n+    FLAG_SET_ERGO(ZUncommit, false);\n+    return;\n+  }\n+\n+  \/\/ Test if uncommit is supported by the operating system by committing\n+  \/\/ and then uncommitting a granule.\n+  const ZVirtualMemory vmem(zoffset(0), ZGranuleSize);\n+  if (!commit(vmem, (uint32_t)-1) || !uncommit(vmem)) {\n+    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (Not supported by operating system)\");\n+    FLAG_SET_ERGO(ZUncommit, false);\n+    return;\n+  }\n+\n+  log_info_p(gc, init)(\"Uncommit: Enabled\");\n+  log_info_p(gc, init)(\"Uncommit Delay: %zus\", ZUncommitDelay);\n+}\n+\n+void ZPhysicalMemoryManager::alloc(const ZVirtualMemory& vmem, uint32_t numa_id) {\n+  zbacking_index* const pmem = _physical_mappings.addr(vmem.start());\n+  const size_t size = vmem.size();\n+\n+  assert(is_aligned(size, ZGranuleSize), \"Invalid size\");\n+\n+  size_t current_segment = 0;\n+  size_t remaining_segments = size >> ZGranuleSizeShift;\n+\n+  while (remaining_segments != 0) {\n+    \/\/ Allocate a range of backing segment indices\n+    ZBackingIndexRegistry& registry = _partition_registries.get(numa_id);\n+    const ZBackingIndexRange range = registry.remove_from_low_at_most(remaining_segments);\n+    assert(!range.is_null(), \"Allocation should never fail\");\n+\n+    const size_t num_allocated_segments = range.size();\n+\n+    \/\/ Insert backing segment indices in pmem\n+    const zbacking_index start_i = range.start();\n+    for (size_t i = 0; i < num_allocated_segments; i++) {\n+      pmem[current_segment + i] = start_i + i;\n+    }\n+\n+    \/\/ Advance by number of allocated segments\n+    remaining_segments -= num_allocated_segments;\n+    current_segment += num_allocated_segments;\n+  }\n+}\n+\n+template <typename ReturnType>\n+struct IterateInvoker {\n+  template<typename Function>\n+  bool operator()(Function function, zbacking_offset segment_start, size_t segment_size) const {\n+    return function(segment_start, segment_size);\n+  }\n+};\n+\n+template<>\n+struct IterateInvoker<void> {\n+  template<typename Function>\n+  bool operator()(Function function, zbacking_offset segment_start, size_t segment_size) const {\n+    function(segment_start, segment_size);\n+    return true;\n+  }\n+};\n+\n+template<typename Function>\n+bool for_each_segment_apply(const zbacking_index* pmem, size_t size, Function function) {\n+  IterateInvoker<decltype(function(zbacking_offset{}, size_t{}))> invoker;\n+\n+  \/\/ Total number of segment indices\n+  const size_t num_segments = size >> ZGranuleSizeShift;\n+\n+  \/\/ Apply the function over all zbacking_offset ranges consisting of consecutive indices\n+  for (size_t i = 0; i < num_segments; i++) {\n+    const size_t start_i = i;\n+\n+    \/\/ Find index corresponding to the last index in the consecutive range starting at start_i\n+    while (i + 1 < num_segments && to_zbacking_index_end(pmem[i], 1) == pmem[i + 1]) {\n+      i++;\n+    }\n+\n+    const size_t last_i = i;\n+\n+    \/\/ [start_i, last_i] now forms a consecutive range of indicies in pmem\n+    const size_t num_indicies = last_i - start_i + 1;\n+    const zbacking_offset start = to_zbacking_offset(pmem[start_i]);\n+    const size_t size = num_indicies * ZGranuleSize;\n+\n+    \/\/ Invoke function on zbacking_offset Range [start, start + size[\n+    if (!invoker(function, start, size)) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+void ZPhysicalMemoryManager::free(const ZVirtualMemory& vmem, uint32_t numa_id) {\n+  zbacking_index* const pmem = _physical_mappings.addr(vmem.start());\n+  const size_t size = vmem.size();\n+\n+  \/\/ Free segments\n+  for_each_segment_apply(pmem, size, [&](zbacking_offset segment_start, size_t segment_size) {\n+    const size_t num_segments = segment_size >> ZGranuleSizeShift;\n+    const zbacking_index index = to_zbacking_index(segment_start);\n+\n+    \/\/ Insert the free segment indices\n+    _partition_registries.get(numa_id).insert({index, num_segments});\n+  });\n+}\n+\n+size_t ZPhysicalMemoryManager::commit(const ZVirtualMemory& vmem, uint32_t numa_id) {\n+  zbacking_index* const pmem = _physical_mappings.addr(vmem.start());\n+  const size_t size = vmem.size();\n+\n+  size_t total_committed = 0;\n+\n+  \/\/ Commit segments\n+  for_each_segment_apply(pmem, size, [&](zbacking_offset segment_start, size_t segment_size) {\n+    \/\/ Commit segment\n+    const size_t committed = _backing.commit(segment_start, segment_size, numa_id);\n+\n+    total_committed += committed;\n+\n+    \/\/ Register with NMT\n+    if (committed > 0) {\n+      ZNMT::commit(segment_start, committed);\n+    }\n+\n+    return segment_size == committed;\n+  });\n+\n+  \/\/ Success\n+  return total_committed;\n+}\n+\n+size_t ZPhysicalMemoryManager::uncommit(const ZVirtualMemory& vmem) {\n+  zbacking_index* const pmem = _physical_mappings.addr(vmem.start());\n+  const size_t size = vmem.size();\n+\n+  size_t total_uncommitted = 0;\n+\n+  \/\/ Uncommit segments\n+  for_each_segment_apply(pmem, size, [&](zbacking_offset segment_start, size_t segment_size) {\n+    \/\/ Uncommit segment\n+    const size_t uncommitted = _backing.uncommit(segment_start, segment_size);\n+\n+    total_uncommitted += uncommitted;\n+\n+    \/\/ Unregister with NMT\n+    if (uncommitted > 0) {\n+      ZNMT::uncommit(segment_start, uncommitted);\n+    }\n+\n+    return segment_size == uncommitted;\n+  });\n+\n+  \/\/ Success\n+  return total_uncommitted;\n+}\n+\n+\/\/ Map virtual memory to physical memory\n+void ZPhysicalMemoryManager::map(const ZVirtualMemory& vmem, uint32_t numa_id) const {\n+  const zbacking_index* const pmem = _physical_mappings.addr(vmem.start());\n+  const zaddress_unsafe addr = ZOffset::address_unsafe(vmem.start());\n+  const size_t size = vmem.size();\n+\n+  size_t mapped = 0;\n+\n+  for_each_segment_apply(pmem, size, [&](zbacking_offset segment_start, size_t segment_size) {\n+    _backing.map(addr + mapped, segment_size, segment_start);\n+    mapped += segment_size;\n+  });\n+\n+  postcond(mapped == size);\n+\n+  \/\/ Setup NUMA preferred for large pages\n+  if (ZNUMA::is_enabled() && ZLargePages::is_explicit()) {\n+    os::numa_make_local((char*)addr, size, (int)numa_id);\n+  }\n+}\n+\n+\/\/ Unmap virtual memory from physical memory\n+void ZPhysicalMemoryManager::unmap(const ZVirtualMemory& vmem) const {\n+  const zaddress_unsafe addr = ZOffset::address_unsafe(vmem.start());\n+  const size_t size = vmem.size();\n+  _backing.unmap(addr, size);\n+}\n+\n+void ZPhysicalMemoryManager::copy_physical_segments(const ZVirtualMemory& to, const ZVirtualMemory& from) {\n+  assert(to.size() == from.size(), \"must be of the same size\");\n+\n+  zbacking_index* const dest = _physical_mappings.addr(to.start());\n+  const zbacking_index* const src = _physical_mappings.addr(from.start());\n+  const int granule_count = from.granule_count();\n+\n+  ZUtils::copy_disjoint(dest, src, granule_count);\n+}\n+\n+static void sort_zbacking_index_array(zbacking_index* array, int count) {\n+  ZUtils::sort(array, count, [](const zbacking_index* e1, const zbacking_index* e2) {\n+    return *e1 < *e2 ? -1 : 1;\n+  });\n+}\n+\n+void ZPhysicalMemoryManager::sort_segments_physical(const ZVirtualMemory& vmem) {\n+  zbacking_index* const pmem = _physical_mappings.addr(vmem.start());\n+  const int granule_count = vmem.granule_count();\n+\n+  \/\/ Sort physical segments\n+  sort_zbacking_index_array(pmem, granule_count);\n+}\n+\n+void ZPhysicalMemoryManager::copy_to_stash(ZArraySlice<zbacking_index> stash, const ZVirtualMemory& vmem) const {\n+  zbacking_index* const dest = stash.adr_at(0);\n+  const zbacking_index* const src = _physical_mappings.addr(vmem.start());\n+  const int granule_count = vmem.granule_count();\n+\n+  \/\/ Check bounds\n+  assert(granule_count <= stash.length(), \"Copy overflow %d <= %d\", granule_count, stash.length());\n+\n+  \/\/ Copy to stash\n+  ZUtils::copy_disjoint(dest, src, granule_count);\n+}\n+\n+void ZPhysicalMemoryManager::copy_from_stash(const ZArraySlice<const zbacking_index> stash, const ZVirtualMemory& vmem) {\n+  zbacking_index* const dest = _physical_mappings.addr(vmem.start());\n+  const zbacking_index* const src = stash.adr_at(0);\n+  const int granule_count = vmem.granule_count();\n+\n+  \/\/ Check bounds\n+  assert(granule_count <= stash.length(), \"Copy overflow %d <= %d\", granule_count, stash.length());\n+\n+  \/\/ Copy from stash\n+  ZUtils::copy_disjoint(dest, src, granule_count);\n+}\n+\n+void ZPhysicalMemoryManager::stash_segments(const ZVirtualMemory& vmem, ZArray<zbacking_index>* stash_out) const {\n+  precond(stash_out->is_empty());\n+\n+  stash_out->at_grow(vmem.granule_count() - 1);\n+  copy_to_stash(*stash_out, vmem);\n+  sort_zbacking_index_array(stash_out->adr_at(0), stash_out->length());\n+}\n+\n+void ZPhysicalMemoryManager::restore_segments(const ZVirtualMemory& vmem, const ZArray<zbacking_index>& stash) {\n+  assert(vmem.granule_count() == stash.length(), \"Must match stash size\");\n+\n+  copy_from_stash(stash, vmem);\n+}\n+\n+void ZPhysicalMemoryManager::stash_segments(const ZArraySlice<const ZVirtualMemory>& vmems, ZArray<zbacking_index>* stash_out) const {\n+  precond(stash_out->is_empty());\n+\n+  int stash_index = 0;\n+  for (const ZVirtualMemory& vmem : vmems) {\n+    const int granule_count = vmem.granule_count();\n+    stash_out->at_grow(stash_index + vmem.granule_count() - 1);\n+    copy_to_stash(stash_out->slice_back(stash_index), vmem);\n+    stash_index += granule_count;\n+  }\n+\n+  sort_zbacking_index_array(stash_out->adr_at(0), stash_out->length());\n+\n+}\n+\n+void ZPhysicalMemoryManager::restore_segments(const ZArraySlice<const ZVirtualMemory>& vmems, const ZArray<zbacking_index>& stash) {\n+  int stash_index = 0;\n+\n+  for (const ZVirtualMemory& vmem : vmems) {\n+    copy_from_stash(stash.slice_back(stash_index), vmem);\n+    stash_index += vmem.granule_count();\n+  }\n+\n+  assert(stash_index == stash.length(), \"Must have emptied the stash\");\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemoryManager.cpp","additions":376,"deletions":0,"binary":false,"changes":376,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZPHYSICALMEMORYMANAGER_HPP\n+#define SHARE_GC_Z_ZPHYSICALMEMORYMANAGER_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zArray.hpp\"\n+#include \"gc\/z\/zGranuleMap.hpp\"\n+#include \"gc\/z\/zRange.hpp\"\n+#include \"gc\/z\/zRangeRegistry.hpp\"\n+#include \"gc\/z\/zValue.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include OS_HEADER(gc\/z\/zPhysicalMemoryBacking)\n+\n+class ZVirtualMemory;\n+\n+using ZBackingIndexRange = ZRange<zbacking_index, zbacking_index_end>;\n+\n+class ZPhysicalMemoryManager {\n+private:\n+  using ZBackingIndexRegistry = ZRangeRegistry<ZBackingIndexRange>;\n+\n+  ZPhysicalMemoryBacking          _backing;\n+  ZPerNUMA<ZBackingIndexRegistry> _partition_registries;\n+  ZGranuleMap<zbacking_index>     _physical_mappings;\n+\n+  void copy_to_stash(ZArraySlice<zbacking_index> stash, const ZVirtualMemory& vmem) const;\n+  void copy_from_stash(const ZArraySlice<const zbacking_index> stash, const ZVirtualMemory& vmem);\n+\n+public:\n+  ZPhysicalMemoryManager(size_t max_capacity);\n+\n+  bool is_initialized() const;\n+\n+  void warn_commit_limits(size_t max_capacity) const;\n+  void try_enable_uncommit(size_t min_capacity, size_t max_capacity);\n+\n+  void alloc(const ZVirtualMemory& vmem, uint32_t numa_id);\n+  void free(const ZVirtualMemory& vmem, uint32_t numa_id);\n+\n+  size_t commit(const ZVirtualMemory& vmem, uint32_t numa_id);\n+  size_t uncommit(const ZVirtualMemory& vmem);\n+\n+  void map(const ZVirtualMemory& vmem, uint32_t numa_id) const;\n+  void unmap(const ZVirtualMemory& vmem) const;\n+\n+  void copy_physical_segments(const ZVirtualMemory& to, const ZVirtualMemory& from);\n+\n+  void sort_segments_physical(const ZVirtualMemory& vmem);\n+\n+  void stash_segments(const ZVirtualMemory& vmem, ZArray<zbacking_index>* stash_out) const;\n+  void restore_segments(const ZVirtualMemory& vmem, const ZArray<zbacking_index>& stash);\n+\n+  void stash_segments(const ZArraySlice<const ZVirtualMemory>& vmems, ZArray<zbacking_index>* stash_out) const;\n+  void restore_segments(const ZArraySlice<const ZVirtualMemory>& vmems, const ZArray<zbacking_index>& stash);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZPHYSICALMEMORYMANAGER_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemoryManager.hpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZRANGE_HPP\n+#define SHARE_GC_Z_ZRANGE_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename Start, typename End>\n+class ZRange {\n+  friend class VMStructs;\n+\n+public:\n+  using offset     = Start;\n+  using offset_end = End;\n+\n+private:\n+  End    _start;\n+  size_t _size;\n+\n+  \/\/ Used internally to create a ZRange.\n+  \/\/\n+  \/\/ The end parameter is only used for verification and to distinguish\n+  \/\/ the constructors if End == Start.\n+  ZRange(End start, size_t size, End end);\n+\n+public:\n+  ZRange();\n+  ZRange(Start start, size_t size);\n+\n+  bool is_null() const;\n+\n+  Start start() const;\n+  End end() const;\n+\n+  size_t size() const;\n+\n+  bool operator==(const ZRange& other) const;\n+  bool operator!=(const ZRange& other) const;\n+\n+  bool contains(const ZRange& other) const;\n+\n+  void grow_from_front(size_t size);\n+  void grow_from_back(size_t size);\n+\n+  ZRange shrink_from_front(size_t size);\n+  ZRange shrink_from_back(size_t size);\n+\n+  ZRange partition(size_t offset, size_t partition_size) const;\n+  ZRange first_part(size_t split_offset) const;\n+  ZRange last_part(size_t split_offset) const;\n+\n+  bool adjacent_to(const ZRange& other) const;\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZRANGE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRange.hpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,144 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZRANGE_INLINE_HPP\n+#define SHARE_GC_Z_ZRANGE_INLINE_HPP\n+\n+#include \"gc\/z\/zRange.hpp\"\n+\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End>::ZRange(End start, size_t size, End end)\n+  : _start(start),\n+    _size(size) {\n+  postcond(this->end() == end);\n+}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End>::ZRange()\n+  : _start(End::invalid),\n+    _size(0) {}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End>::ZRange(Start start, size_t size)\n+  : _start(to_end_type(start, 0)),\n+    _size(size) {}\n+\n+template <typename Start, typename End>\n+inline bool ZRange<Start, End>::is_null() const {\n+  return _start == End::invalid;\n+}\n+\n+template <typename Start, typename End>\n+inline Start ZRange<Start, End>::start() const {\n+  return to_start_type(_start);\n+}\n+\n+template <typename Start, typename End>\n+inline End ZRange<Start, End>::end() const {\n+  return _start + _size;\n+}\n+\n+template <typename Start, typename End>\n+inline size_t ZRange<Start, End>::size() const {\n+  return _size;\n+}\n+\n+template <typename Start, typename End>\n+inline bool ZRange<Start, End>::operator==(const ZRange& other) const {\n+  precond(!is_null());\n+  precond(!other.is_null());\n+\n+  return _start == other._start && _size == other._size;\n+}\n+\n+template <typename Start, typename End>\n+inline bool ZRange<Start, End>::operator!=(const ZRange& other) const {\n+  return !operator==(other);\n+}\n+\n+template <typename Start, typename End>\n+inline bool ZRange<Start, End>::contains(const ZRange& other) const {\n+  precond(!is_null());\n+  precond(!other.is_null());\n+\n+  return _start <= other._start && other.end() <= end();\n+}\n+\n+template <typename Start, typename End>\n+inline void ZRange<Start, End>::grow_from_front(size_t size) {\n+  precond(size_t(start()) >= size);\n+\n+  _start -= size;\n+  _size  += size;\n+}\n+\n+template <typename Start, typename End>\n+inline void ZRange<Start, End>::grow_from_back(size_t size) {\n+  _size += size;\n+}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End> ZRange<Start, End>::shrink_from_front(size_t size) {\n+  precond(this->size() >= size);\n+\n+  _start += size;\n+  _size  -= size;\n+\n+  return ZRange(_start - size, size, _start);\n+}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End> ZRange<Start, End>::shrink_from_back(size_t size) {\n+  precond(this->size() >= size);\n+\n+  _size -= size;\n+\n+  return ZRange(end(), size, end() + size);\n+}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End> ZRange<Start, End>::partition(size_t offset, size_t partition_size) const {\n+  precond(size() - offset >= partition_size);\n+\n+  return ZRange(_start + offset, partition_size, _start + offset + partition_size);\n+}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End> ZRange<Start, End>::first_part(size_t split_offset) const {\n+  return partition(0, split_offset);\n+}\n+\n+template <typename Start, typename End>\n+inline ZRange<Start, End> ZRange<Start, End>::last_part(size_t split_offset) const {\n+  return partition(split_offset, size() - split_offset);\n+}\n+\n+template <typename Start, typename End>\n+inline bool ZRange<Start, End>::adjacent_to(const ZRange<Start, End>& other) const {\n+  return end() == other.start() || other.end() == start();\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZRANGE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRange.inline.hpp","additions":144,"deletions":0,"binary":false,"changes":144,"status":"added"},{"patch":"@@ -0,0 +1,150 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZRANGEREGISTRY_HPP\n+#define SHARE_GC_Z_ZRANGEREGISTRY_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zList.hpp\"\n+#include \"gc\/z\/zLock.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T>\n+class ZArray;\n+\n+template <typename Range>\n+class ZRangeRegistry {\n+  friend class ZVirtualMemoryManagerTest;\n+\n+private:\n+  \/\/ The node type for the list of Ranges\n+  class Node;\n+\n+public:\n+  using offset     = typename Range::offset;\n+  using offset_end = typename Range::offset_end;\n+\n+  typedef void (*CallbackPrepare)(const Range& range);\n+  typedef void (*CallbackResize)(const Range& from, const Range& to);\n+\n+  struct Callbacks {\n+    CallbackPrepare _prepare_for_hand_out;\n+    CallbackPrepare _prepare_for_hand_back;\n+    CallbackResize  _grow;\n+    CallbackResize  _shrink;\n+\n+    Callbacks();\n+  };\n+\n+private:\n+  mutable ZLock _lock;\n+  ZList<Node>   _list;\n+  Callbacks     _callbacks;\n+  Range         _limits;\n+\n+  void move_into(const Range& range);\n+\n+  void insert_inner(const Range& range);\n+  void register_inner(const Range& range);\n+\n+  void grow_from_front(Range* range, size_t size);\n+  void grow_from_back(Range* range, size_t size);\n+\n+  Range shrink_from_front(Range* range, size_t size);\n+  Range shrink_from_back(Range* range, size_t size);\n+\n+  Range remove_from_low_inner(size_t size);\n+  Range remove_from_low_at_most_inner(size_t size);\n+\n+  size_t remove_from_low_many_at_most_inner(size_t size, ZArray<Range>* out);\n+\n+  bool check_limits(const Range& range) const;\n+\n+public:\n+  ZRangeRegistry();\n+\n+  void register_callbacks(const Callbacks& callbacks);\n+\n+  void register_range(const Range& range);\n+  bool unregister_first(Range* out);\n+\n+  bool is_empty() const;\n+  bool is_contiguous() const;\n+\n+  void anchor_limits();\n+  bool limits_contain(const Range& range) const;\n+\n+  offset peek_low_address() const;\n+  offset_end peak_high_address_end() const;\n+\n+  void insert(const Range& range);\n+\n+  void insert_and_remove_from_low_many(const Range& range, ZArray<Range>* out);\n+  Range insert_and_remove_from_low_exact_or_many(size_t size, ZArray<Range>* in_out);\n+\n+  Range remove_from_low(size_t size);\n+  Range remove_from_low_at_most(size_t size);\n+  size_t remove_from_low_many_at_most(size_t size, ZArray<Range>* out);\n+  Range remove_from_high(size_t size);\n+\n+  void transfer_from_low(ZRangeRegistry* other, size_t size);\n+};\n+\n+template <typename Range>\n+class ZRangeRegistry<Range>::Node : public CHeapObj<mtGC> {\n+  friend class ZList<Node>;\n+\n+private:\n+  using offset     = typename Range::offset;\n+  using offset_end = typename Range::offset_end;\n+\n+  Range           _range;\n+  ZListNode<Node> _node;\n+\n+public:\n+  Node(offset start, size_t size)\n+    : _range(start, size),\n+      _node() {}\n+\n+  Node(const Range& other)\n+    : Node(other.start(), other.size()) {}\n+\n+  Range* range() {\n+    return &_range;\n+  }\n+\n+  offset start() const {\n+    return _range.start();\n+  }\n+\n+  offset_end end() const {\n+    return _range.end();\n+  }\n+\n+  size_t size() const {\n+    return _range.size();\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZRANGEREGISTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRangeRegistry.hpp","additions":150,"deletions":0,"binary":false,"changes":150,"status":"added"},{"patch":"@@ -0,0 +1,469 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZRANGEREGISTRY_INLINE_HPP\n+#define SHARE_GC_Z_ZRANGEREGISTRY_INLINE_HPP\n+\n+#include \"gc\/z\/zRangeRegistry.hpp\"\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zList.inline.hpp\"\n+#include \"gc\/z\/zLock.inline.hpp\"\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::move_into(const Range& range) {\n+  assert(!range.is_null(), \"Invalid range\");\n+  assert(check_limits(range), \"Range outside limits\");\n+\n+  const offset start = range.start();\n+  const offset_end end = range.end();\n+  const size_t size = range.size();\n+\n+  ZListIterator<Node> iter(&_list);\n+  for (Node* node; iter.next(&node);) {\n+    if (node->start() < start) {\n+      continue;\n+    }\n+\n+    Node* const prev = _list.prev(node);\n+    if (prev != nullptr && start == prev->end()) {\n+      if (end == node->start()) {\n+        \/\/ Merge with prev and current ranges\n+        grow_from_back(prev->range(), size);\n+        grow_from_back(prev->range(), node->size());\n+        _list.remove(node);\n+        delete node;\n+      } else {\n+        \/\/ Merge with prev range\n+        grow_from_back(prev->range(), size);\n+      }\n+    } else if (end == node->start()) {\n+      \/\/ Merge with current range\n+      grow_from_front(node->range(), size);\n+    } else {\n+      \/\/ Insert range before current range\n+      assert(end < node->start(), \"Areas must not overlap\");\n+      Node* const new_node = new Node(start, size);\n+      _list.insert_before(node, new_node);\n+    }\n+\n+    \/\/ Done\n+    return;\n+  }\n+\n+  \/\/ Insert last\n+  Node* const last = _list.last();\n+  if (last != nullptr && start == last->end()) {\n+    \/\/ Merge with last range\n+    grow_from_back(last->range(), size);\n+  } else {\n+    \/\/ Insert new node last\n+    Node* const new_node = new Node(start, size);\n+    _list.insert_last(new_node);\n+  }\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::insert_inner(const Range& range) {\n+  if (_callbacks._prepare_for_hand_back != nullptr) {\n+    _callbacks._prepare_for_hand_back(range);\n+  }\n+  move_into(range);\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::register_inner(const Range& range) {\n+  move_into(range);\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::grow_from_front(Range* range, size_t size) {\n+  if (_callbacks._grow != nullptr) {\n+    const Range from = *range;\n+    const Range to = Range(from.start() - size, from.size() + size);\n+    _callbacks._grow(from, to);\n+  }\n+  range->grow_from_front(size);\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::grow_from_back(Range* range, size_t size) {\n+  if (_callbacks._grow != nullptr) {\n+    const Range from = *range;\n+    const Range to = Range(from.start(), from.size() + size);\n+    _callbacks._grow(from, to);\n+  }\n+  range->grow_from_back(size);\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::shrink_from_front(Range* range, size_t size) {\n+  if (_callbacks._shrink != nullptr) {\n+    const Range from = *range;\n+    const Range to = from.last_part(size);\n+    _callbacks._shrink(from, to);\n+  }\n+  return range->shrink_from_front(size);\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::shrink_from_back(Range* range, size_t size) {\n+  if (_callbacks._shrink != nullptr) {\n+    const Range from = *range;\n+    const Range to = from.first_part(from.size() - size);\n+    _callbacks._shrink(from, to);\n+  }\n+  return range->shrink_from_back(size);\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::remove_from_low_inner(size_t size) {\n+  ZListIterator<Node> iter(&_list);\n+  for (Node* node; iter.next(&node);) {\n+    if (node->size() >= size) {\n+      Range range;\n+\n+      if (node->size() == size) {\n+        \/\/ Exact match, remove range\n+        _list.remove(node);\n+        range = *node->range();\n+        delete node;\n+      } else {\n+        \/\/ Larger than requested, shrink range\n+        range = shrink_from_front(node->range(), size);\n+      }\n+\n+      if (_callbacks._prepare_for_hand_out != nullptr) {\n+        _callbacks._prepare_for_hand_out(range);\n+      }\n+\n+      return range;\n+    }\n+  }\n+\n+  \/\/ Out of memory\n+  return Range();\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::remove_from_low_at_most_inner(size_t size) {\n+  Node* const node = _list.first();\n+  if (node == nullptr) {\n+    \/\/ List is empty\n+    return Range();\n+  }\n+\n+  Range range;\n+\n+  if (node->size() <= size) {\n+    \/\/ Smaller than or equal to requested, remove range\n+    _list.remove(node);\n+    range = *node->range();\n+    delete node;\n+  } else {\n+    \/\/ Larger than requested, shrink range\n+    range = shrink_from_front(node->range(), size);\n+  }\n+\n+  if (_callbacks._prepare_for_hand_out) {\n+    _callbacks._prepare_for_hand_out(range);\n+  }\n+\n+  return range;\n+}\n+\n+template <typename Range>\n+size_t ZRangeRegistry<Range>::remove_from_low_many_at_most_inner(size_t size, ZArray<Range>* out) {\n+  size_t to_remove = size;\n+\n+  while (to_remove > 0) {\n+    const Range range = remove_from_low_at_most_inner(to_remove);\n+\n+    if (range.is_null()) {\n+      \/\/ The requested amount is not available\n+      return size - to_remove;\n+    }\n+\n+    to_remove -= range.size();\n+    out->append(range);\n+  }\n+\n+  return size;\n+}\n+\n+template <typename Range>\n+ZRangeRegistry<Range>::Callbacks::Callbacks()\n+  : _prepare_for_hand_out(nullptr),\n+    _prepare_for_hand_back(nullptr),\n+    _grow(nullptr),\n+    _shrink(nullptr) {}\n+\n+template <typename Range>\n+ZRangeRegistry<Range>::ZRangeRegistry()\n+  : _list(),\n+    _callbacks(),\n+    _limits() {}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::register_callbacks(const Callbacks& callbacks) {\n+  _callbacks = callbacks;\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::register_range(const Range& range) {\n+  ZLocker<ZLock> locker(&_lock);\n+  register_inner(range);\n+}\n+\n+template <typename Range>\n+bool ZRangeRegistry<Range>::unregister_first(Range* out) {\n+  \/\/ Unregistering a range doesn't call a \"prepare_to_hand_out\" callback\n+  \/\/ because the range is unregistered and not handed out to be used.\n+\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  if (_list.is_empty()) {\n+    return false;\n+  }\n+\n+  \/\/ Don't invoke the \"prepare_to_hand_out\" callback\n+\n+  Node* const node = _list.remove_first();\n+\n+  \/\/ Return the range\n+  *out = *node->range();\n+\n+  delete node;\n+\n+  return true;\n+}\n+\n+template <typename Range>\n+inline bool ZRangeRegistry<Range>::is_empty() const {\n+  return _list.is_empty();\n+}\n+\n+template <typename Range>\n+bool ZRangeRegistry<Range>::is_contiguous() const {\n+  return _list.size() == 1;\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::anchor_limits() {\n+  assert(_limits.is_null(), \"Should only anchor limits once\");\n+\n+  if (_list.is_empty()) {\n+    return;\n+  }\n+\n+  const offset start = _list.first()->start();\n+  const size_t size = _list.last()->end() - start;\n+\n+  _limits = Range(start, size);\n+}\n+\n+template <typename Range>\n+bool ZRangeRegistry<Range>::limits_contain(const Range& range) const {\n+  if (_limits.is_null() || range.is_null()) {\n+    return false;\n+  }\n+\n+  return range.start() >= _limits.start() && range.end() <= _limits.end();\n+}\n+\n+template <typename Range>\n+bool ZRangeRegistry<Range>::check_limits(const Range& range) const {\n+  if (_limits.is_null()) {\n+    \/\/ Limits not anchored\n+    return true;\n+  }\n+\n+  \/\/ Otherwise, check that other is within the limits\n+  return limits_contain(range);\n+}\n+\n+template <typename Range>\n+typename ZRangeRegistry<Range>::offset ZRangeRegistry<Range>::peek_low_address() const {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  const Node* const node = _list.first();\n+  if (node != nullptr) {\n+    return node->start();\n+  }\n+\n+  \/\/ Out of memory\n+  return offset::invalid;\n+}\n+\n+template <typename Range>\n+typename ZRangeRegistry<Range>::offset_end ZRangeRegistry<Range>::peak_high_address_end() const {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  const Node* const node = _list.last();\n+  if (node != nullptr) {\n+    return node->end();\n+  }\n+\n+  \/\/ Out of memory\n+  return offset_end::invalid;\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::insert(const Range& range) {\n+  ZLocker<ZLock> locker(&_lock);\n+  insert_inner(range);\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::insert_and_remove_from_low_many(const Range& range, ZArray<Range>* out) {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  const size_t size = range.size();\n+\n+  \/\/ Insert the range\n+  insert_inner(range);\n+\n+  \/\/ Remove (hopefully) at a lower address\n+  const size_t removed = remove_from_low_many_at_most_inner(size, out);\n+\n+  \/\/ This should always succeed since we freed the same amount.\n+  assert(removed == size, \"must succeed\");\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::insert_and_remove_from_low_exact_or_many(size_t size, ZArray<Range>* in_out) {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  size_t inserted = 0;\n+\n+  \/\/ Insert everything\n+  ZArrayIterator<Range> iter(in_out);\n+  for (Range mem; iter.next(&mem);) {\n+    insert_inner(mem);\n+    inserted += mem.size();\n+  }\n+\n+  \/\/ Clear stored memory so that we can populate it below\n+  in_out->clear();\n+\n+  \/\/ Try to find and remove a contiguous chunk\n+  Range range = remove_from_low_inner(size);\n+  if (!range.is_null()) {\n+    return range;\n+  }\n+\n+  \/\/ Failed to find a contiguous chunk, split it up into smaller chunks and\n+  \/\/ only remove up to as much that has been inserted.\n+  size_t removed = remove_from_low_many_at_most_inner(inserted, in_out);\n+  assert(removed == inserted, \"Should be able to get back as much as we previously inserted\");\n+  return Range();\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::remove_from_low(size_t size) {\n+  ZLocker<ZLock> locker(&_lock);\n+  Range range = remove_from_low_inner(size);\n+  return range;\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::remove_from_low_at_most(size_t size) {\n+  ZLocker<ZLock> lock(&_lock);\n+  Range range = remove_from_low_at_most_inner(size);\n+  return range;\n+}\n+\n+template <typename Range>\n+size_t ZRangeRegistry<Range>::remove_from_low_many_at_most(size_t size, ZArray<Range>* out) {\n+  ZLocker<ZLock> lock(&_lock);\n+  return remove_from_low_many_at_most_inner(size, out);\n+}\n+\n+template <typename Range>\n+Range ZRangeRegistry<Range>::remove_from_high(size_t size) {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  ZListReverseIterator<Node> iter(&_list);\n+  for (Node* node; iter.next(&node);) {\n+    if (node->size() >= size) {\n+      Range range;\n+\n+      if (node->size() == size) {\n+        \/\/ Exact match, remove range\n+        _list.remove(node);\n+        range = *node->range();\n+        delete node;\n+      } else {\n+        \/\/ Larger than requested, shrink range\n+        range = shrink_from_back(node->range(), size);\n+      }\n+\n+      if (_callbacks._prepare_for_hand_out != nullptr) {\n+        _callbacks._prepare_for_hand_out(range);\n+      }\n+\n+      return range;\n+    }\n+  }\n+\n+  \/\/ Out of memory\n+  return Range();\n+}\n+\n+template <typename Range>\n+void ZRangeRegistry<Range>::transfer_from_low(ZRangeRegistry* other, size_t size) {\n+  assert(other->_list.is_empty(), \"Should only be used for initialization\");\n+\n+  ZLocker<ZLock> locker(&_lock);\n+  size_t to_move = size;\n+\n+  ZListIterator<Node> iter(&_list);\n+  for (Node* node; iter.next(&node);) {\n+    Node* to_transfer;\n+\n+    if (node->size() <= to_move) {\n+      \/\/ Smaller than or equal to requested, remove range\n+      _list.remove(node);\n+      to_transfer = node;\n+    } else {\n+      \/\/ Larger than requested, shrink range\n+      const Range range = shrink_from_front(node->range(), to_move);\n+      to_transfer = new Node(range);\n+    }\n+\n+    \/\/ Insert into the other list\n+    \/\/\n+    \/\/ The from list is sorted, the other list starts empty, and the inserts\n+    \/\/ come in sort order, so we can insert_last here.\n+    other->_list.insert_last(to_transfer);\n+\n+    to_move -= to_transfer->size();\n+    if (to_move == 0) {\n+      break;\n+    }\n+  }\n+\n+  assert(to_move == 0, \"Should have transferred requested size\");\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZRANGEREGISTRY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zRangeRegistry.inline.hpp","additions":469,"deletions":0,"binary":false,"changes":469,"status":"added"},{"patch":"@@ -413,1 +413,1 @@\n-    ZHeap::heap()->free_page(page, true \/* allow_defragment *\/);\n+    ZHeap::heap()->free_page(page);\n@@ -844,1 +844,3 @@\n-    ZPage* const to_page = promotion ? from_page->clone_limited() : from_page;\n+    ZPage* const to_page = promotion\n+        ? from_page->clone_for_promotion()\n+        : from_page->reset(to_age);\n@@ -847,1 +849,0 @@\n-    to_page->reset(to_age);\n@@ -849,3 +850,0 @@\n-    if (promotion) {\n-      to_page->remset_alloc();\n-    }\n@@ -1014,1 +1012,1 @@\n-      ZHeap::heap()->free_page(page, true \/* allow_defragment *\/);\n+      ZHeap::heap()->free_page(page);\n@@ -1263,1 +1261,3 @@\n-      ZPage* const new_page = promotion ? prev_page->clone_limited() : prev_page;\n+      ZPage* const new_page = promotion\n+          ? prev_page->clone_for_promotion()\n+          : prev_page->reset(to_age);\n@@ -1266,1 +1266,0 @@\n-      new_page->reset(to_age);\n@@ -1268,3 +1267,0 @@\n-      if (promotion) {\n-        new_page->remset_alloc();\n-      }\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":8,"deletions":12,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -476,1 +476,0 @@\n-    _remembered->_page_allocator->enable_safe_recycle();\n@@ -480,1 +479,0 @@\n-    _remembered->_page_allocator->disable_safe_recycle();\n","filename":"src\/hotspot\/share\/gc\/z\/zRemembered.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,6 +57,0 @@\n-void ZRememberedSet::delete_all() {\n-  assert(is_initialized(), \"precondition\");\n-  _bitmap[0].resize(0);\n-  _bitmap[1].resize(0);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zRememberedSet.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -117,1 +117,0 @@\n-  void delete_all();\n","filename":"src\/hotspot\/share\/gc\/z\/zRememberedSet.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,2 +34,3 @@\n-ZUncommitter::ZUncommitter(ZPageAllocator* page_allocator)\n-  : _page_allocator(page_allocator),\n+ZUncommitter::ZUncommitter(uint32_t id, ZPartition* partition)\n+  : _id(id),\n+    _partition(partition),\n@@ -38,1 +39,1 @@\n-  set_name(\"ZUncommitter\");\n+  set_name(\"ZUncommitter#%u\", id);\n@@ -49,1 +50,1 @@\n-    log_debug(gc, heap)(\"Uncommit Timeout: \" UINT64_FORMAT \"s\", timeout);\n+    log_debug(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"s\", _id, timeout);\n@@ -66,1 +67,1 @@\n-    size_t uncommitted = 0;\n+    size_t total_uncommitted = 0;\n@@ -70,2 +71,2 @@\n-      const size_t flushed = _page_allocator->uncommit(&timeout);\n-      if (flushed == 0) {\n+      const size_t uncommitted = _partition->uncommit(&timeout);\n+      if (uncommitted == 0) {\n@@ -76,1 +77,1 @@\n-      uncommitted += flushed;\n+      total_uncommitted += uncommitted;\n@@ -79,1 +80,1 @@\n-    if (uncommitted > 0) {\n+    if (total_uncommitted > 0) {\n@@ -81,3 +82,3 @@\n-      ZStatInc(ZCounterUncommit, uncommitted);\n-      log_info(gc, heap)(\"Uncommitted: %zuM(%.0f%%)\",\n-                         uncommitted \/ M, percent_of(uncommitted, ZHeap::heap()->max_capacity()));\n+      ZStatInc(ZCounterUncommit, total_uncommitted);\n+      log_info(gc, heap)(\"Uncommitter (%u) Uncommitted: %zuM(%.0f%%)\",\n+                         _id, total_uncommitted \/ M, percent_of(total_uncommitted, ZHeap::heap()->max_capacity()));\n@@ -86,1 +87,1 @@\n-      event.commit(uncommitted);\n+      event.commit(total_uncommitted);\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.cpp","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-class ZPageAllocator;\n+class ZPartition;\n@@ -34,1 +34,2 @@\n-  ZPageAllocator* const  _page_allocator;\n+  const uint32_t         _id;\n+  ZPartition* const      _partition;\n@@ -46,1 +47,1 @@\n-  ZUncommitter(ZPageAllocator* page_allocator);\n+  ZUncommitter(uint32_t id, ZPartition* partition);\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1,129 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/z\/zList.inline.hpp\"\n-#include \"gc\/z\/zLock.inline.hpp\"\n-#include \"gc\/z\/zPage.inline.hpp\"\n-#include \"gc\/z\/zPageAllocator.hpp\"\n-#include \"gc\/z\/zUnmapper.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"runtime\/globals.hpp\"\n-\n-ZUnmapper::ZUnmapper(ZPageAllocator* page_allocator)\n-  : _page_allocator(page_allocator),\n-    _lock(),\n-    _queue(),\n-    _enqueued_bytes(0),\n-    _warned_sync_unmapping(false),\n-    _stop(false) {\n-  set_name(\"ZUnmapper\");\n-  create_and_start();\n-}\n-\n-ZPage* ZUnmapper::dequeue() {\n-  ZLocker<ZConditionLock> locker(&_lock);\n-\n-  for (;;) {\n-    if (_stop) {\n-      return nullptr;\n-    }\n-\n-    ZPage* const page = _queue.remove_first();\n-    if (page != nullptr) {\n-      _enqueued_bytes -= page->size();\n-      return page;\n-    }\n-\n-    _lock.wait();\n-  }\n-}\n-\n-bool ZUnmapper::try_enqueue(ZPage* page) {\n-  \/\/ Enqueue for asynchronous unmap and destroy\n-  ZLocker<ZConditionLock> locker(&_lock);\n-  if (is_saturated()) {\n-    \/\/ The unmapper thread is lagging behind and is unable to unmap memory fast enough\n-    if (!_warned_sync_unmapping) {\n-      _warned_sync_unmapping = true;\n-      log_warning_p(gc)(\"WARNING: Encountered synchronous unmapping because asynchronous unmapping could not keep up\");\n-    }\n-    log_debug(gc, unmap)(\"Synchronous unmapping %zuM page\", page->size() \/ M);\n-    return false;\n-  }\n-\n-  log_trace(gc, unmap)(\"Asynchronous unmapping %zuM page (%zuM \/ %zuM enqueued)\",\n-                       page->size() \/ M, _enqueued_bytes \/ M, queue_capacity() \/ M);\n-\n-  _queue.insert_last(page);\n-  _enqueued_bytes += page->size();\n-  _lock.notify_all();\n-\n-  return true;\n-}\n-\n-size_t ZUnmapper::queue_capacity() const {\n-  return align_up((size_t)(_page_allocator->max_capacity() * ZAsyncUnmappingLimit \/ 100.0), ZGranuleSize);\n-}\n-\n-bool ZUnmapper::is_saturated() const {\n-  return _enqueued_bytes >= queue_capacity();\n-}\n-\n-void ZUnmapper::do_unmap_and_destroy_page(ZPage* page) const {\n-  EventZUnmap event;\n-  const size_t unmapped = page->size();\n-\n-  \/\/ Unmap and destroy\n-  _page_allocator->unmap_page(page);\n-  _page_allocator->destroy_page(page);\n-\n-  \/\/ Send event\n-  event.commit(unmapped);\n-}\n-\n-void ZUnmapper::unmap_and_destroy_page(ZPage* page) {\n-  if (!try_enqueue(page)) {\n-    \/\/ Synchronously unmap and destroy\n-    do_unmap_and_destroy_page(page);\n-  }\n-}\n-\n-void ZUnmapper::run_thread() {\n-  for (;;) {\n-    ZPage* const page = dequeue();\n-    if (page == nullptr) {\n-      \/\/ Stop\n-      return;\n-    }\n-\n-    do_unmap_and_destroy_page(page);\n-  }\n-}\n-\n-void ZUnmapper::terminate() {\n-  ZLocker<ZConditionLock> locker(&_lock);\n-  _stop = true;\n-  _lock.notify_all();\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zUnmapper.cpp","additions":0,"deletions":129,"binary":false,"changes":129,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_ZUNMAPPER_HPP\n-#define SHARE_GC_Z_ZUNMAPPER_HPP\n-\n-#include \"gc\/z\/zList.hpp\"\n-#include \"gc\/z\/zLock.hpp\"\n-#include \"gc\/z\/zThread.hpp\"\n-\n-class ZPage;\n-class ZPageAllocator;\n-\n-class ZUnmapper : public ZThread {\n-private:\n-  ZPageAllocator* const _page_allocator;\n-  ZConditionLock        _lock;\n-  ZList<ZPage>          _queue;\n-  size_t                _enqueued_bytes;\n-  bool                  _warned_sync_unmapping;\n-  bool                  _stop;\n-\n-  ZPage* dequeue();\n-  bool try_enqueue(ZPage* page);\n-  size_t queue_capacity() const;\n-  bool is_saturated() const;\n-  void do_unmap_and_destroy_page(ZPage* page) const;\n-\n-protected:\n-  virtual void run_thread();\n-  virtual void terminate();\n-\n-public:\n-  ZUnmapper(ZPageAllocator* page_allocator);\n-\n-  void unmap_and_destroy_page(ZPage* page);\n-};\n-\n-#endif \/\/ SHARE_GC_Z_ZUNMAPPER_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zUnmapper.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,10 @@\n+  template <typename T>\n+  static void copy_disjoint(T* dest, const T* src, size_t count);\n+  template <typename T>\n+  static void copy_disjoint(T* dest, const T* src, int count);\n+\n+  \/\/ Sort\n+  template <typename T, typename Comparator>\n+  static void sort(T* array, size_t count, Comparator comparator);\n+  template <typename T, typename Comparator>\n+  static void sort(T* array, int count, Comparator comparator);\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -72,0 +72,31 @@\n+template <typename T>\n+inline void ZUtils::copy_disjoint(T* dest, const T* src, size_t count) {\n+  memcpy(dest, src, sizeof(T) * count);\n+}\n+\n+template <typename T>\n+inline void ZUtils::copy_disjoint(T* dest, const T* src, int count) {\n+  assert(count >= 0, \"must be positive %d\", count);\n+\n+  copy_disjoint(dest, src, static_cast<size_t>(count));\n+}\n+\n+template <typename T, typename Comparator>\n+inline void ZUtils::sort(T* array, size_t count, Comparator comparator) {\n+  using SortType = int(const void*, const void*);\n+  using ComparatorType = int(const T*, const T*);\n+\n+  static constexpr bool IsComparatorCompatible = std::is_assignable<ComparatorType*&, Comparator>::value;\n+  static_assert(IsComparatorCompatible, \"Incompatible Comparator, must decay to plain function pointer\");\n+\n+  \/\/ We rely on ABI compatibility between ComparatorType and SortType\n+  qsort(array, count, sizeof(T), reinterpret_cast<SortType*>(static_cast<ComparatorType*>(comparator)));\n+}\n+\n+template <typename T, typename Comparator>\n+inline void ZUtils::sort(T* array, int count, Comparator comparator) {\n+  assert(count >= 0, \"must be positive %d\", count);\n+\n+  sort(array, static_cast<size_t>(count), comparator);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":32,"deletions":1,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -79,0 +79,2 @@\n+struct ZValueIdTagType {};\n+\n@@ -81,0 +83,2 @@\n+  friend class VMStructs;\n+\n@@ -89,0 +93,2 @@\n+  template <typename... Args>\n+  ZValue(ZValueIdTagType, Args&&... args);\n@@ -98,0 +104,2 @@\n+\n+  uint32_t count() const;\n@@ -109,0 +117,3 @@\n+template<typename S, typename T>\n+class ZValueConstIterator;\n+\n@@ -111,0 +122,2 @@\n+  friend class ZValueConstIterator<S, T>;\n+\n@@ -117,0 +130,1 @@\n+  ZValueIterator(const ZValueIterator&) = default;\n@@ -119,0 +133,1 @@\n+  bool next(T** value, uint32_t* value_id);\n@@ -133,0 +148,2 @@\n+  ZValueConstIterator(const ZValueIterator<S, T>& other);\n+  ZValueConstIterator(const ZValueConstIterator&) = default;\n","filename":"src\/hotspot\/share\/gc\/z\/zValue.hpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"gc\/z\/zNUMA.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n@@ -145,0 +145,12 @@\n+template <typename S, typename T>\n+template <typename... Args>\n+inline ZValue<S, T>::ZValue(ZValueIdTagType, Args&&... args)\n+  : _addr(S::alloc(sizeof(T))) {\n+  \/\/ Initialize all instances\n+  uint32_t value_id;\n+  ZValueIterator<S, T> iter(this);\n+  for (T* addr; iter.next(&addr, &value_id);) {\n+    ::new (addr) T(value_id, args...);\n+  }\n+}\n+\n@@ -178,0 +190,5 @@\n+template <typename S, typename T>\n+uint32_t ZValue<S, T>::count() const {\n+  return S::count();\n+}\n+\n@@ -195,0 +212,9 @@\n+template <typename S, typename T>\n+inline bool ZValueIterator<S, T>::next(T** value, uint32_t* value_id) {\n+  if (_value_id < S::count()) {\n+    *value_id = _value_id;\n+    *value = _value->addr(_value_id++);\n+    return true;\n+  }\n+  return false;\n+}\n@@ -201,0 +227,5 @@\n+template <typename S, typename T>\n+inline ZValueConstIterator<S, T>::ZValueConstIterator(const ZValueIterator<S, T>& other)\n+  : _value(other._value),\n+    _value_id(other._value_id) {}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zValue.inline.hpp","additions":33,"deletions":2,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1,257 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/z\/zAddress.inline.hpp\"\n-#include \"gc\/z\/zAddressSpaceLimit.hpp\"\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"gc\/z\/zInitialize.hpp\"\n-#include \"gc\/z\/zNMT.hpp\"\n-#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-ZVirtualMemoryManager::ZVirtualMemoryManager(size_t max_capacity)\n-  : _manager(),\n-    _reserved(0),\n-    _initialized(false) {\n-\n-  assert(max_capacity <= ZAddressOffsetMax, \"Too large max_capacity\");\n-\n-  \/\/ Initialize platform specific parts before reserving address space\n-  pd_initialize_before_reserve();\n-\n-  \/\/ Register the Windows callbacks\n-  pd_register_callbacks(&_manager);\n-\n-  \/\/ Reserve address space\n-  if (!reserve(max_capacity)) {\n-    ZInitialize::error_d(\"Failed to reserve enough address space for Java heap\");\n-    return;\n-  }\n-\n-  \/\/ Set ZAddressOffsetMax to the highest address end available after reservation\n-  ZAddressOffsetMax = untype(highest_available_address_end());\n-\n-  \/\/ Successfully initialized\n-  _initialized = true;\n-}\n-\n-#ifdef ASSERT\n-size_t ZVirtualMemoryManager::force_reserve_discontiguous(size_t size) {\n-  const size_t min_range = calculate_min_range(size);\n-  const size_t max_range = MAX2(align_down(size \/ ZForceDiscontiguousHeapReservations, ZGranuleSize), min_range);\n-  size_t reserved = 0;\n-\n-  \/\/ Try to reserve ZForceDiscontiguousHeapReservations number of virtual memory\n-  \/\/ ranges. Starting with higher addresses.\n-  uintptr_t end = ZAddressOffsetMax;\n-  while (reserved < size && end >= max_range) {\n-    const size_t remaining = size - reserved;\n-    const size_t reserve_size = MIN2(max_range, remaining);\n-    const uintptr_t reserve_start = end - reserve_size;\n-\n-    if (reserve_contiguous(to_zoffset(reserve_start), reserve_size)) {\n-      reserved += reserve_size;\n-    }\n-\n-    end -= reserve_size * 2;\n-  }\n-\n-  \/\/ If (reserved < size) attempt to reserve the rest via normal divide and conquer\n-  uintptr_t start = 0;\n-  while (reserved < size && start < ZAddressOffsetMax) {\n-    const size_t remaining = MIN2(size - reserved, ZAddressOffsetMax - start);\n-    reserved += reserve_discontiguous(to_zoffset(start), remaining, min_range);\n-    start += remaining;\n-  }\n-\n-  return reserved;\n-}\n-#endif\n-\n-size_t ZVirtualMemoryManager::reserve_discontiguous(zoffset start, size_t size, size_t min_range) {\n-  if (size < min_range) {\n-    \/\/ Too small\n-    return 0;\n-  }\n-\n-  assert(is_aligned(size, ZGranuleSize), \"Misaligned\");\n-\n-  if (reserve_contiguous(start, size)) {\n-    return size;\n-  }\n-\n-  const size_t half = size \/ 2;\n-  if (half < min_range) {\n-    \/\/ Too small\n-    return 0;\n-  }\n-\n-  \/\/ Divide and conquer\n-  const size_t first_part = align_down(half, ZGranuleSize);\n-  const size_t second_part = size - first_part;\n-  const size_t first_size = reserve_discontiguous(start, first_part, min_range);\n-  const size_t second_size = reserve_discontiguous(start + first_part, second_part, min_range);\n-  return first_size + second_size;\n-}\n-\n-size_t ZVirtualMemoryManager::calculate_min_range(size_t size) {\n-  \/\/ Don't try to reserve address ranges smaller than 1% of the requested size.\n-  \/\/ This avoids an explosion of reservation attempts in case large parts of the\n-  \/\/ address space is already occupied.\n-  return align_up(size \/ ZMaxVirtualReservations, ZGranuleSize);\n-}\n-\n-size_t ZVirtualMemoryManager::reserve_discontiguous(size_t size) {\n-  const size_t min_range = calculate_min_range(size);\n-  uintptr_t start = 0;\n-  size_t reserved = 0;\n-\n-  \/\/ Reserve size somewhere between [0, ZAddressOffsetMax)\n-  while (reserved < size && start < ZAddressOffsetMax) {\n-    const size_t remaining = MIN2(size - reserved, ZAddressOffsetMax - start);\n-    reserved += reserve_discontiguous(to_zoffset(start), remaining, min_range);\n-    start += remaining;\n-  }\n-\n-  return reserved;\n-}\n-\n-bool ZVirtualMemoryManager::reserve_contiguous(zoffset start, size_t size) {\n-  assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned 0x%zx\", size);\n-\n-  \/\/ Reserve address views\n-  const zaddress_unsafe addr = ZOffset::address_unsafe(start);\n-\n-  \/\/ Reserve address space\n-  if (!pd_reserve(addr, size)) {\n-    return false;\n-  }\n-\n-  \/\/ Register address views with native memory tracker\n-  ZNMT::reserve(addr, size);\n-\n-  \/\/ Make the address range free\n-  _manager.register_range(start, size);\n-\n-  return true;\n-}\n-\n-bool ZVirtualMemoryManager::reserve_contiguous(size_t size) {\n-  \/\/ Allow at most 8192 attempts spread evenly across [0, ZAddressOffsetMax)\n-  const size_t unused = ZAddressOffsetMax - size;\n-  const size_t increment = MAX2(align_up(unused \/ 8192, ZGranuleSize), ZGranuleSize);\n-\n-  for (uintptr_t start = 0; start + size <= ZAddressOffsetMax; start += increment) {\n-    if (reserve_contiguous(to_zoffset(start), size)) {\n-      \/\/ Success\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Failed\n-  return false;\n-}\n-\n-bool ZVirtualMemoryManager::reserve(size_t max_capacity) {\n-  const size_t limit = MIN2(ZAddressOffsetMax, ZAddressSpaceLimit::heap());\n-  const size_t size = MIN2(max_capacity * ZVirtualToPhysicalRatio, limit);\n-\n-  auto do_reserve = [&]() {\n-#ifdef ASSERT\n-    if (ZForceDiscontiguousHeapReservations > 0) {\n-      return force_reserve_discontiguous(size);\n-    }\n-#endif\n-\n-    \/\/ Prefer a contiguous address space\n-    if (reserve_contiguous(size)) {\n-      return size;\n-    }\n-\n-    \/\/ Fall back to a discontiguous address space\n-    return reserve_discontiguous(size);\n-  };\n-\n-  const size_t reserved = do_reserve();\n-\n-  const bool contiguous = _manager.free_is_contiguous();\n-\n-  log_info_p(gc, init)(\"Address Space Type: %s\/%s\/%s\",\n-                       (contiguous ? \"Contiguous\" : \"Discontiguous\"),\n-                       (limit == ZAddressOffsetMax ? \"Unrestricted\" : \"Restricted\"),\n-                       (reserved == size ? \"Complete\" : \"Degraded\"));\n-  log_info_p(gc, init)(\"Address Space Size: %zuM\", reserved \/ M);\n-\n-  \/\/ Record reserved\n-  _reserved = reserved;\n-\n-  return reserved >= max_capacity;\n-}\n-\n-void ZVirtualMemoryManager::unreserve(zoffset start, size_t size) {\n-  const zaddress_unsafe addr = ZOffset::address_unsafe(start);\n-\n-  \/\/ Unregister the reserved memory from NMT\n-  ZNMT::unreserve(addr, size);\n-\n-  \/\/ Unreserve address space\n-  pd_unreserve(addr, size);\n-}\n-\n-void ZVirtualMemoryManager::unreserve_all() {\n-  zoffset start;\n-  size_t size;\n-\n-  while (_manager.unregister_first(&start, &size)) {\n-    unreserve(start, size);\n-  }\n-}\n-\n-bool ZVirtualMemoryManager::is_initialized() const {\n-  return _initialized;\n-}\n-\n-ZVirtualMemory ZVirtualMemoryManager::alloc(size_t size, bool force_low_address) {\n-  zoffset start;\n-\n-  \/\/ Small pages are allocated at low addresses, while medium\/large pages\n-  \/\/ are allocated at high addresses (unless forced to be at a low address).\n-  if (force_low_address || size <= ZPageSizeSmall) {\n-    start = _manager.alloc_low_address(size);\n-  } else {\n-    start = _manager.alloc_high_address(size);\n-  }\n-\n-  if (start == zoffset(UINTPTR_MAX)) {\n-    return ZVirtualMemory();\n-  }\n-\n-  return ZVirtualMemory(start, size);\n-}\n-\n-void ZVirtualMemoryManager::free(const ZVirtualMemory& vmem) {\n-  _manager.free(vmem.start(), vmem.size());\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.cpp","additions":0,"deletions":257,"binary":false,"changes":257,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,8 +28,2 @@\n-#include \"gc\/z\/zMemory.hpp\"\n-\n-class ZVirtualMemory {\n-  friend class VMStructs;\n-\n-private:\n-  zoffset     _start;\n-  zoffset_end _end;\n+#include \"gc\/z\/zRange.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -37,0 +31,1 @@\n+class ZVirtualMemory : public ZRange<zoffset, zoffset_end> {\n@@ -40,0 +35,1 @@\n+  ZVirtualMemory(const ZRange<zoffset, zoffset_end>& range);\n@@ -41,48 +37,1 @@\n-  bool is_null() const;\n-  zoffset start() const;\n-  zoffset_end end() const;\n-  size_t size() const;\n-\n-  ZVirtualMemory split(size_t size);\n-};\n-\n-class ZVirtualMemoryManager {\n-  friend class ZMapperTest;\n-  friend class ZVirtualMemoryManagerTest;\n-\n-private:\n-  static size_t calculate_min_range(size_t size);\n-\n-  ZMemoryManager _manager;\n-  size_t         _reserved;\n-  bool           _initialized;\n-\n-  \/\/ Platform specific implementation\n-  void pd_initialize_before_reserve();\n-  void pd_register_callbacks(ZMemoryManager* manager);\n-  bool pd_reserve(zaddress_unsafe addr, size_t size);\n-  void pd_unreserve(zaddress_unsafe addr, size_t size);\n-\n-  bool reserve_contiguous(zoffset start, size_t size);\n-  bool reserve_contiguous(size_t size);\n-  size_t reserve_discontiguous(zoffset start, size_t size, size_t min_range);\n-  size_t reserve_discontiguous(size_t size);\n-  bool reserve(size_t max_capacity);\n-\n-  void unreserve(zoffset start, size_t size);\n-\n-  DEBUG_ONLY(size_t force_reserve_discontiguous(size_t size);)\n-\n-public:\n-  ZVirtualMemoryManager(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  size_t reserved() const;\n-  zoffset lowest_available_address() const;\n-  zoffset_end highest_available_address_end() const;\n-\n-  ZVirtualMemory alloc(size_t size, bool force_low_address);\n-  void free(const ZVirtualMemory& vmem);\n-\n-  void unreserve_all();\n+  int granule_count() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.hpp","additions":6,"deletions":57,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,5 @@\n-#include \"gc\/z\/zMemory.inline.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zRange.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n@@ -32,2 +36,1 @@\n-  : _start(zoffset(UINTPTR_MAX)),\n-    _end(zoffset_end(UINTPTR_MAX)) {}\n+  : ZRange() {}\n@@ -36,13 +39,4 @@\n-  : _start(start),\n-    _end(to_zoffset_end(start, size)) {}\n-\n-inline bool ZVirtualMemory::is_null() const {\n-  return _start == zoffset(UINTPTR_MAX);\n-}\n-\n-inline zoffset ZVirtualMemory::start() const {\n-  return _start;\n-}\n-\n-inline zoffset_end ZVirtualMemory::end() const {\n-  return _end;\n+  : ZRange(start, size) {\n+  \/\/ ZVirtualMemory is only used for ZGranuleSize multiple ranges\n+  assert(is_aligned(untype(start), ZGranuleSize), \"must be multiple of ZGranuleSize\");\n+  assert(is_aligned(size, ZGranuleSize), \"must be multiple of ZGranuleSize\");\n@@ -51,3 +45,2 @@\n-inline size_t ZVirtualMemory::size() const {\n-  return _end - _start;\n-}\n+inline ZVirtualMemory::ZVirtualMemory(const ZRange<zoffset, zoffset_end>& range)\n+  : ZVirtualMemory(range.start(), range.size()) {}\n@@ -55,4 +48,2 @@\n-inline ZVirtualMemory ZVirtualMemory::split(size_t size) {\n-  _start += size;\n-  return ZVirtualMemory(_start - size, size);\n-}\n+inline int ZVirtualMemory::granule_count() const {\n+  const size_t granule_count = size() >> ZGranuleSizeShift;\n@@ -60,7 +51,2 @@\n-inline size_t ZVirtualMemoryManager::reserved() const {\n-  return _reserved;\n-}\n-\n-inline zoffset ZVirtualMemoryManager::lowest_available_address() const {\n-  return _manager.peek_low_address();\n-}\n+  assert(granule_count <= static_cast<size_t>(std::numeric_limits<int>::max()),\n+         \"must not overflow an int %zu\", granule_count);\n@@ -68,2 +54,1 @@\n-inline zoffset_end ZVirtualMemoryManager::highest_available_address_end() const {\n-  return _manager.peak_high_address_end();\n+  return static_cast<int>(granule_count);\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.inline.hpp","additions":18,"deletions":33,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -0,0 +1,357 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/gcLogPrecious.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zAddressSpaceLimit.hpp\"\n+#include \"gc\/z\/zArray.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zInitialize.hpp\"\n+#include \"gc\/z\/zNMT.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"gc\/z\/zValue.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+ZVirtualMemoryReserver::ZVirtualMemoryReserver(size_t size)\n+  : _registry(),\n+    _reserved(reserve(size)) {}\n+\n+void ZVirtualMemoryReserver::initialize_partition_registry(ZVirtualMemoryRegistry* partition_registry, size_t size) {\n+  assert(partition_registry->is_empty(), \"Should be empty when initializing\");\n+\n+  \/\/ Registers the Windows callbacks\n+  pd_register_callbacks(partition_registry);\n+\n+  _registry.transfer_from_low(partition_registry, size);\n+\n+  \/\/ Set the limits according to the virtual memory given to this partition\n+  partition_registry->anchor_limits();\n+}\n+\n+void ZVirtualMemoryReserver::unreserve(const ZVirtualMemory& vmem) {\n+  const zaddress_unsafe addr = ZOffset::address_unsafe(vmem.start());\n+\n+  \/\/ Unregister the reserved memory from NMT\n+  ZNMT::unreserve(addr, vmem.size());\n+\n+  \/\/ Unreserve address space\n+  pd_unreserve(addr, vmem.size());\n+}\n+\n+void ZVirtualMemoryReserver::unreserve_all() {\n+  for (ZVirtualMemory vmem; _registry.unregister_first(&vmem);) {\n+    unreserve(vmem);\n+  }\n+}\n+\n+bool ZVirtualMemoryReserver::is_empty() const {\n+  return _registry.is_empty();\n+}\n+\n+bool ZVirtualMemoryReserver::is_contiguous() const {\n+  return _registry.is_contiguous();\n+}\n+\n+size_t ZVirtualMemoryReserver::reserved() const {\n+  return _reserved;\n+}\n+\n+zoffset_end ZVirtualMemoryReserver::highest_available_address_end() const {\n+  return _registry.peak_high_address_end();\n+}\n+\n+#ifdef ASSERT\n+size_t ZVirtualMemoryReserver::force_reserve_discontiguous(size_t size) {\n+  const size_t min_range = calculate_min_range(size);\n+  const size_t max_range = MAX2(align_down(size \/ ZForceDiscontiguousHeapReservations, ZGranuleSize), min_range);\n+  size_t reserved = 0;\n+\n+  \/\/ Try to reserve ZForceDiscontiguousHeapReservations number of virtual memory\n+  \/\/ ranges. Starting with higher addresses.\n+  uintptr_t end = ZAddressOffsetMax;\n+  while (reserved < size && end >= max_range) {\n+    const size_t remaining = size - reserved;\n+    const size_t reserve_size = MIN2(max_range, remaining);\n+    const uintptr_t reserve_start = end - reserve_size;\n+\n+    if (reserve_contiguous(to_zoffset(reserve_start), reserve_size)) {\n+      reserved += reserve_size;\n+    }\n+\n+    end -= reserve_size * 2;\n+  }\n+\n+  \/\/ If (reserved < size) attempt to reserve the rest via normal divide and conquer\n+  uintptr_t start = 0;\n+  while (reserved < size && start < ZAddressOffsetMax) {\n+    const size_t remaining = MIN2(size - reserved, ZAddressOffsetMax - start);\n+    reserved += reserve_discontiguous(to_zoffset(start), remaining, min_range);\n+    start += remaining;\n+  }\n+\n+  return reserved;\n+}\n+#endif\n+\n+size_t ZVirtualMemoryReserver::reserve_discontiguous(zoffset start, size_t size, size_t min_range) {\n+  if (size < min_range) {\n+    \/\/ Too small\n+    return 0;\n+  }\n+\n+  assert(is_aligned(size, ZGranuleSize), \"Misaligned\");\n+\n+  if (reserve_contiguous(start, size)) {\n+    return size;\n+  }\n+\n+  const size_t half = size \/ 2;\n+  if (half < min_range) {\n+    \/\/ Too small\n+    return 0;\n+  }\n+\n+  \/\/ Divide and conquer\n+  const size_t first_part = align_down(half, ZGranuleSize);\n+  const size_t second_part = size - first_part;\n+  const size_t first_size = reserve_discontiguous(start, first_part, min_range);\n+  const size_t second_size = reserve_discontiguous(start + first_part, second_part, min_range);\n+  return first_size + second_size;\n+}\n+\n+size_t ZVirtualMemoryReserver::calculate_min_range(size_t size) {\n+  \/\/ Don't try to reserve address ranges smaller than 1% of the requested size.\n+  \/\/ This avoids an explosion of reservation attempts in case large parts of the\n+  \/\/ address space is already occupied.\n+  return align_up(size \/ ZMaxVirtualReservations, ZGranuleSize);\n+}\n+\n+size_t ZVirtualMemoryReserver::reserve_discontiguous(size_t size) {\n+  const size_t min_range = calculate_min_range(size);\n+  uintptr_t start = 0;\n+  size_t reserved = 0;\n+\n+  \/\/ Reserve size somewhere between [0, ZAddressOffsetMax)\n+  while (reserved < size && start < ZAddressOffsetMax) {\n+    const size_t remaining = MIN2(size - reserved, ZAddressOffsetMax - start);\n+    reserved += reserve_discontiguous(to_zoffset(start), remaining, min_range);\n+    start += remaining;\n+  }\n+\n+  return reserved;\n+}\n+\n+bool ZVirtualMemoryReserver::reserve_contiguous(zoffset start, size_t size) {\n+  assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned 0x%zx\", size);\n+\n+  \/\/ Reserve address views\n+  const zaddress_unsafe addr = ZOffset::address_unsafe(start);\n+\n+  \/\/ Reserve address space\n+  if (!pd_reserve(addr, size)) {\n+    return false;\n+  }\n+\n+  \/\/ Register address views with native memory tracker\n+  ZNMT::reserve(addr, size);\n+\n+  \/\/ Register the memory reservation\n+  _registry.register_range({start, size});\n+\n+  return true;\n+}\n+\n+bool ZVirtualMemoryReserver::reserve_contiguous(size_t size) {\n+  \/\/ Allow at most 8192 attempts spread evenly across [0, ZAddressOffsetMax)\n+  const size_t unused = ZAddressOffsetMax - size;\n+  const size_t increment = MAX2(align_up(unused \/ 8192, ZGranuleSize), ZGranuleSize);\n+\n+  for (uintptr_t start = 0; start + size <= ZAddressOffsetMax; start += increment) {\n+    if (reserve_contiguous(to_zoffset(start), size)) {\n+      \/\/ Success\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Failed\n+  return false;\n+}\n+\n+size_t ZVirtualMemoryReserver::reserve(size_t size) {\n+  \/\/ Register Windows callbacks\n+  pd_register_callbacks(&_registry);\n+\n+  \/\/ Reserve address space\n+\n+#ifdef ASSERT\n+  if (ZForceDiscontiguousHeapReservations > 0) {\n+    return force_reserve_discontiguous(size);\n+  }\n+#endif\n+\n+  \/\/ Prefer a contiguous address space\n+  if (reserve_contiguous(size)) {\n+    return size;\n+  }\n+\n+  \/\/ Fall back to a discontiguous address space\n+  return reserve_discontiguous(size);\n+}\n+\n+ZVirtualMemoryManager::ZVirtualMemoryManager(size_t max_capacity)\n+  : _partition_registries(),\n+    _multi_partition_registry(),\n+    _is_multi_partition_enabled(false),\n+    _initialized(false) {\n+\n+  assert(max_capacity <= ZAddressOffsetMax, \"Too large max_capacity\");\n+\n+  ZAddressSpaceLimit::print_limits();\n+\n+  const size_t limit = MIN2(ZAddressOffsetMax, ZAddressSpaceLimit::heap());\n+\n+  const size_t desired_for_partitions = max_capacity * ZVirtualToPhysicalRatio;\n+  const size_t desired_for_multi_partition = ZNUMA::count() > 1 ? desired_for_partitions : 0;\n+\n+  const size_t desired = desired_for_partitions + desired_for_multi_partition;\n+  const size_t requested = desired <= limit\n+      ? desired\n+      : MIN2(desired_for_partitions, limit);\n+\n+  \/\/ Reserve virtual memory for the heap\n+  ZVirtualMemoryReserver reserver(requested);\n+\n+  const size_t reserved = reserver.reserved();\n+  const bool is_contiguous = reserver.is_contiguous();\n+\n+  log_debug_p(gc, init)(\"Reserved Space: limit \" EXACTFMT \", desired \" EXACTFMT \", requested \" EXACTFMT,\n+                        EXACTFMTARGS(limit), EXACTFMTARGS(desired), EXACTFMTARGS(requested));\n+\n+  if (reserved < max_capacity) {\n+    ZInitialize::error_d(\"Failed to reserve \" EXACTFMT \" address space for Java heap\", EXACTFMTARGS(max_capacity));\n+    return;\n+  }\n+\n+  \/\/ Set ZAddressOffsetMax to the highest address end available after reservation\n+  ZAddressOffsetMax = untype(reserver.highest_available_address_end());\n+\n+  const size_t size_for_partitions = MIN2(reserved, desired_for_partitions);\n+\n+  \/\/ Divide size_for_partitions virtual memory over the NUMA nodes\n+  initialize_partitions(&reserver, size_for_partitions);\n+\n+  \/\/ Set up multi-partition or unreserve the surplus memory\n+  if (desired_for_multi_partition > 0 && reserved == desired) {\n+    \/\/ Enough left to setup the multi-partition memory reservation\n+    reserver.initialize_partition_registry(&_multi_partition_registry, desired_for_multi_partition);\n+    _is_multi_partition_enabled = true;\n+  } else {\n+    \/\/ Failed to reserve enough memory for multi-partition, unreserve unused memory\n+    reserver.unreserve_all();\n+  }\n+\n+  assert(reserver.is_empty(), \"Must have handled all reserved memory\");\n+\n+  log_info_p(gc, init)(\"Reserved Space Type: %s\/%s\/%s\",\n+                       (is_contiguous ? \"Contiguous\" : \"Discontiguous\"),\n+                       (requested == desired ? \"Unrestricted\" : \"Restricted\"),\n+                       (reserved == desired ? \"Complete\" : ((reserved < desired_for_partitions) ? \"Degraded\"  : \"NUMA-Degraded\")));\n+  log_info_p(gc, init)(\"Reserved Space Size: \" EXACTFMT, EXACTFMTARGS(reserved));\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+void ZVirtualMemoryManager::initialize_partitions(ZVirtualMemoryReserver* reserver, size_t size_for_partitions) {\n+  precond(is_aligned(size_for_partitions, ZGranuleSize));\n+\n+  \/\/ If the capacity consist of less granules than the number of partitions\n+  \/\/ some partitions will be empty. Distribute these shares on the none empty\n+  \/\/ partitions.\n+  const uint32_t first_empty_numa_id = MIN2(static_cast<uint32_t>(size_for_partitions >> ZGranuleSizeShift), ZNUMA::count());\n+  const uint32_t ignore_count = ZNUMA::count() - first_empty_numa_id;\n+\n+  \/\/ Install reserved memory into registry(s)\n+  uint32_t numa_id;\n+  ZPerNUMAIterator<ZVirtualMemoryRegistry> iter(&_partition_registries);\n+  for (ZVirtualMemoryRegistry* registry; iter.next(&registry, &numa_id);) {\n+    if (numa_id == first_empty_numa_id) {\n+      break;\n+    }\n+\n+    \/\/ Calculate how much reserved memory this partition gets\n+    const size_t reserved_for_partition = ZNUMA::calculate_share(numa_id, size_for_partitions, ZGranuleSize, ignore_count);\n+\n+    \/\/ Transfer reserved memory\n+    reserver->initialize_partition_registry(registry, reserved_for_partition);\n+  }\n+}\n+\n+bool ZVirtualMemoryManager::is_initialized() const {\n+  return _initialized;\n+}\n+\n+ZVirtualMemoryRegistry& ZVirtualMemoryManager::registry(uint32_t partition_id) {\n+  return _partition_registries.get(partition_id);\n+}\n+\n+const ZVirtualMemoryRegistry& ZVirtualMemoryManager::registry(uint32_t partition_id) const {\n+  return _partition_registries.get(partition_id);\n+}\n+\n+zoffset ZVirtualMemoryManager::lowest_available_address(uint32_t partition_id) const {\n+  return registry(partition_id).peek_low_address();\n+}\n+\n+void ZVirtualMemoryManager::insert(const ZVirtualMemory& vmem, uint32_t partition_id) {\n+  assert(partition_id == lookup_partition_id(vmem), \"wrong partition_id for vmem\");\n+  registry(partition_id).insert(vmem);\n+}\n+\n+void ZVirtualMemoryManager::insert_multi_partition(const ZVirtualMemory& vmem) {\n+  _multi_partition_registry.insert(vmem);\n+}\n+\n+size_t ZVirtualMemoryManager::remove_from_low_many_at_most(size_t size, uint32_t partition_id, ZArray<ZVirtualMemory>* vmems_out) {\n+  return registry(partition_id).remove_from_low_many_at_most(size, vmems_out);\n+}\n+\n+ZVirtualMemory ZVirtualMemoryManager::remove_from_low(size_t size, uint32_t partition_id) {\n+  return registry(partition_id).remove_from_low(size);\n+}\n+\n+ZVirtualMemory ZVirtualMemoryManager::remove_from_low_multi_partition(size_t size) {\n+  return _multi_partition_registry.remove_from_low(size);\n+}\n+\n+void ZVirtualMemoryManager::insert_and_remove_from_low_many(const ZVirtualMemory& vmem, uint32_t partition_id, ZArray<ZVirtualMemory>* vmems_out) {\n+  registry(partition_id).insert_and_remove_from_low_many(vmem, vmems_out);\n+}\n+\n+ZVirtualMemory ZVirtualMemoryManager::insert_and_remove_from_low_exact_or_many(size_t size, uint32_t partition_id, ZArray<ZVirtualMemory>* vmems_in_out) {\n+  return registry(partition_id).insert_and_remove_from_low_exact_or_many(size, vmems_in_out);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemoryManager.cpp","additions":357,"deletions":0,"binary":false,"changes":357,"status":"added"},{"patch":"@@ -0,0 +1,109 @@\n+\/*\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZVIRTUALMEMORYMANAGER_HPP\n+#define SHARE_GC_Z_ZVIRTUALMEMORYMANAGER_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zArray.hpp\"\n+#include \"gc\/z\/zRangeRegistry.hpp\"\n+#include \"gc\/z\/zValue.hpp\"\n+#include \"gc\/z\/zVirtualMemory.hpp\"\n+\n+using ZVirtualMemoryRegistry = ZRangeRegistry<ZVirtualMemory>;\n+\n+class ZVirtualMemoryReserver {\n+  friend class ZMapperTest;\n+  friend class ZVirtualMemoryManagerTest;\n+\n+private:\n+\n+  ZVirtualMemoryRegistry _registry;\n+  const size_t           _reserved;\n+\n+  static size_t calculate_min_range(size_t size);\n+\n+  \/\/ Platform specific implementation\n+  void pd_register_callbacks(ZVirtualMemoryRegistry* registry);\n+  bool pd_reserve(zaddress_unsafe addr, size_t size);\n+  void pd_unreserve(zaddress_unsafe addr, size_t size);\n+\n+  bool reserve_contiguous(zoffset start, size_t size);\n+  bool reserve_contiguous(size_t size);\n+  size_t reserve_discontiguous(zoffset start, size_t size, size_t min_range);\n+  size_t reserve_discontiguous(size_t size);\n+\n+  size_t reserve(size_t size);\n+  void unreserve(const ZVirtualMemory& vmem);\n+\n+  DEBUG_ONLY(size_t force_reserve_discontiguous(size_t size);)\n+\n+public:\n+  ZVirtualMemoryReserver(size_t size);\n+\n+  void initialize_partition_registry(ZVirtualMemoryRegistry* partition_registry, size_t size);\n+\n+  void unreserve_all();\n+\n+  bool is_empty() const;\n+  bool is_contiguous() const;\n+\n+  size_t reserved() const;\n+\n+  zoffset_end highest_available_address_end() const;\n+};\n+\n+class ZVirtualMemoryManager {\n+private:\n+  ZPerNUMA<ZVirtualMemoryRegistry> _partition_registries;\n+  ZVirtualMemoryRegistry           _multi_partition_registry;\n+  bool                             _is_multi_partition_enabled;\n+  bool                             _initialized;\n+\n+  ZVirtualMemoryRegistry& registry(uint32_t partition_id);\n+  const ZVirtualMemoryRegistry& registry(uint32_t partition_id) const;\n+\n+public:\n+  ZVirtualMemoryManager(size_t max_capacity);\n+\n+  void initialize_partitions(ZVirtualMemoryReserver* reserver, size_t reserved);\n+\n+  bool is_initialized() const;\n+  bool is_multi_partition_enabled() const;\n+  bool is_in_multi_partition(const ZVirtualMemory& vmem) const;\n+\n+  uint32_t lookup_partition_id(const ZVirtualMemory& vmem) const;\n+  zoffset lowest_available_address(uint32_t partition_id) const;\n+\n+  void insert(const ZVirtualMemory& vmem, uint32_t partition_id);\n+  void insert_multi_partition(const ZVirtualMemory& vmem);\n+\n+  size_t remove_from_low_many_at_most(size_t size, uint32_t partition_id, ZArray<ZVirtualMemory>* vmems_out);\n+  ZVirtualMemory remove_from_low(size_t size, uint32_t partition_id);\n+  ZVirtualMemory remove_from_low_multi_partition(size_t size);\n+\n+  void insert_and_remove_from_low_many(const ZVirtualMemory& vmem, uint32_t partition_id, ZArray<ZVirtualMemory>* vmems_out);\n+  ZVirtualMemory insert_and_remove_from_low_exact_or_many(size_t size, uint32_t partition_id, ZArray<ZVirtualMemory>* vmems_in_out);\n+};\n+\n+#endif \/\/ SHARE_GC_Z_ZVIRTUALMEMORYMANAGER_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemoryManager.hpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"added"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_Z_ZVIRTUALMEMORYMANAGER_INLINE_HPP\n+#define SHARE_GC_Z_ZVIRTUALMEMORYMANAGER_INLINE_HPP\n+\n+#include \"gc\/z\/zVirtualMemoryManager.hpp\"\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"gc\/z\/zRangeRegistry.inline.hpp\"\n+\n+\n+inline bool ZVirtualMemoryManager::is_multi_partition_enabled() const {\n+  return _is_multi_partition_enabled;\n+}\n+\n+inline bool ZVirtualMemoryManager::is_in_multi_partition(const ZVirtualMemory& vmem) const {\n+  return _multi_partition_registry.limits_contain(vmem);\n+}\n+\n+inline uint32_t ZVirtualMemoryManager::lookup_partition_id(const ZVirtualMemory& vmem) const {\n+  const uint32_t num_partitions = _partition_registries.count();\n+  for (uint32_t partition_id = 0; partition_id < num_partitions; partition_id++) {\n+    if (registry(partition_id).limits_contain(vmem)) {\n+      return partition_id;\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+}\n+\n+#endif \/\/ SHARE_GC_Z_ZVIRTUALMEMORYMANAGER_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemoryManager.inline.hpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -71,7 +71,0 @@\n-  product(double, ZAsyncUnmappingLimit, 100.0, DIAGNOSTIC,                  \\\n-          \"Specify the max amount (percentage of max heap size) of async \"  \\\n-          \"unmapping that can be in-flight before unmapping requests are \"  \\\n-          \"temporarily forced to be synchronous instead. \"                  \\\n-          \"The default means after an amount of pages proportional to the \" \\\n-          \"max capacity is enqueued, we resort to synchronous unmapping.\")  \\\n-                                                                            \\\n@@ -121,0 +114,5 @@\n+  develop(uint, ZFakeNUMA, 1,                                               \\\n+          \"ZFakeNUMA is used to test the internal NUMA memory support \"     \\\n+          \"without the need for UseNUMA\")                                   \\\n+          range(1, 16)                                                      \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/z\/z_globals.hpp","additions":6,"deletions":8,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1158,6 +1158,8 @@\n-     <Field type=\"ZPageTypeType\" name=\"type\" label=\"Type\" \/>\n-     <Field type=\"ulong\" contentType=\"bytes\" name=\"size\" label=\"Size\" \/>\n-     <Field type=\"ulong\" contentType=\"bytes\" name=\"flushed\" label=\"Flushed\" \/>\n-     <Field type=\"ulong\" contentType=\"bytes\" name=\"committed\" label=\"Committed\" \/>\n-     <Field type=\"uint\" name=\"segments\" label=\"Segments\" \/>\n-     <Field type=\"boolean\" name=\"nonBlocking\" label=\"Non-blocking\" \/>\n+    <Field type=\"ZPageTypeType\" name=\"type\" label=\"Type\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"size\" label=\"Size\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"harvested\" label=\"Harvested\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"committed\" label=\"Committed\" \/>\n+    <Field type=\"uint\" name=\"numHarvested\" label=\"Number of harvested vmems\" \/>\n+    <Field type=\"boolean\" name=\"multiPartition\" label=\"Multi-partition allocation\" \/>\n+    <Field type=\"boolean\" name=\"successful\" label=\"Successful allocation\" \/>\n+    <Field type=\"boolean\" name=\"nonBlocking\" label=\"Non-blocking\" \/>\n@@ -1201,4 +1203,0 @@\n-  <Event name=\"ZUnmap\" category=\"Java Virtual Machine, GC, Detailed\" label=\"ZGC Unmap\" description=\"Unmapping of memory\" thread=\"true\">\n-    <Field type=\"ulong\" contentType=\"bytes\" name=\"unmapped\" label=\"Unmapped\" \/>\n-  <\/Event>\n-\n","filename":"src\/hotspot\/share\/jfr\/metadata\/metadata.xml","additions":9,"deletions":11,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -331,0 +331,6 @@\n+const char* VMError::get_filename_only() {\n+  char separator = os::file_separator()[0];\n+  const char* p = strrchr(_filename, separator);\n+  return p ? p + 1 : _filename;\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"memory\/allStatic.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -111,5 +113,1 @@\n-  static const char* get_filename_only() {\n-    char separator = os::file_separator()[0];\n-    const char* p = strrchr(_filename, separator);\n-    return p ? p+1 : _filename;\n-  }\n+  static const char* get_filename_only();\n","filename":"src\/hotspot\/share\/utilities\/vmError.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.z;\n+\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+\/\/ Mirror class for ZNUMA\n+\n+public class ZNUMA {\n+\n+    private static CIntegerField countField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"ZNUMA\");\n+\n+        countField = type.getCIntegerField(\"_count\");\n+    }\n+\n+    public static long count() {\n+        return countField.getValue();\n+    }\n+}\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZNUMA.java","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n@@ -39,2 +40,2 @@\n-    private static CIntegerField capacityField;\n-    private static CIntegerField usedField;\n+    private static long partitionsOffset;\n+    private static long numaCount;\n@@ -50,2 +51,7 @@\n-        capacityField = type.getCIntegerField(\"_capacity\");\n-        usedField = type.getCIntegerField(\"_used\");\n+        partitionsOffset = type.getAddressField(\"_partitions\").getOffset();\n+        numaCount = ZNUMA.count();\n+    }\n+\n+    private ZPerNUMAZPartition partitions() {\n+        Address partitionsAddr = addr.addOffsetTo(partitionsOffset);\n+        return VMObjectFactory.newObject(ZPerNUMAZPartition.class, partitionsAddr);\n@@ -59,1 +65,5 @@\n-        return capacityField.getValue(addr);\n+        long total_capacity = 0;\n+        for (int id = 0; id < numaCount; id++) {\n+          total_capacity += partitions().value(id).capacity();\n+        }\n+        return total_capacity;\n@@ -63,1 +73,5 @@\n-        return usedField.getValue(addr);\n+        long total_used = 0;\n+        for (int id = 0; id < numaCount; id++) {\n+          total_used += partitions().value(id).used();\n+        }\n+        return total_used;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZPageAllocator.java","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.z;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+\/\/ Mirror class for ZPartition\n+\n+public class ZPartition extends VMObject {\n+\n+    private static CIntegerField capacityField;\n+    private static CIntegerField usedField;\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"ZPartition\");\n+\n+        capacityField = type.getCIntegerField(\"_capacity\");\n+        usedField = type.getCIntegerField(\"_used\");\n+    }\n+\n+    public ZPartition(Address addr) {\n+        super(addr);\n+    }\n+\n+    public long capacity() {\n+        return capacityField.getValue(addr);\n+    }\n+\n+    public long used() {\n+        return usedField.getValue(addr);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZPartition.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -0,0 +1,61 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.z;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.runtime.VMObjectFactory;\n+import sun.jvm.hotspot.types.AddressField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+\/\/ Mirror class for ZPerNUMA<ZPartition>\n+\n+public class ZPerNUMAZPartition extends VMObject {\n+\n+    private static AddressField addrField;\n+    private static long valueOffset = 4096; \/\/ 4k\n+\n+    static {\n+        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"ZPerNUMAZPartition\");\n+        addrField = type.getAddressField(\"_addr\");\n+    }\n+\n+    public ZPartition value(long id) {\n+        Address valueArrayAddr = addrField.getValue(addr);\n+        Address partitionAddr = valueArrayAddr.addOffsetTo(id * valueOffset);\n+        return VMObjectFactory.newObject(ZPartition.class, partitionAddr);\n+    }\n+\n+    public ZPerNUMAZPartition(Address addr) {\n+        super(addr);\n+    }\n+}\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/z\/ZPerNUMAZPartition.java","additions":61,"deletions":0,"binary":false,"changes":61,"status":"added"},{"patch":"@@ -886,5 +886,0 @@\n-    <event name=\"jdk.ZUnmap\">\n-      <setting name=\"enabled\">true<\/setting>\n-      <setting name=\"threshold\">0 ms<\/setting>\n-    <\/event>\n-\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/default.jfc","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -886,5 +886,0 @@\n-    <event name=\"jdk.ZUnmap\">\n-      <setting name=\"enabled\">true<\/setting>\n-      <setting name=\"threshold\">0 ms<\/setting>\n-    <\/event>\n-\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/profile.jfc","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,1 +25,3 @@\n-#include \"unittest.hpp\"\n+#include \"zunittest.hpp\"\n+\n+class ZArrayTest : public ZTest {};\n@@ -83,0 +85,113 @@\n+\n+TEST_F(ZArrayTest, slice) {\n+  ZArray<int> a0(0);\n+  ZArray<int> a10(10);\n+  ZArray<int> ar(10 + abs(random() % 10));\n+\n+  \/\/ Add elements\n+  for (int i = 0; i < ar.capacity(); ++i) {\n+    const auto append = [&](ZArray<int>& a) {\n+      if (i < a.capacity()) {\n+        a.append(i);\n+      }\n+    };\n+\n+    append(a0);\n+    append(a10);\n+    append(ar);\n+  }\n+\n+  {\n+    const auto reverse_test = [](const ZArray<int>& original) {\n+      ZArray<int> a(original.capacity());\n+      a.appendAll(&original);\n+\n+      const auto reverse = [](ZArraySlice<int> slice, auto reverse) -> ZArraySlice<int> {\n+        const auto swap_elements = [](ZArraySlice<int> s1, ZArraySlice<int> s2) {\n+          ASSERT_EQ(s1.length(), s2.length());\n+          for (int i = 0; i < s1.length(); ++i) {\n+            ::swap(s1.at(i), s2.at(i));\n+          }\n+        };\n+\n+        const int length = slice.length();\n+        if (length > 1) {\n+          const int middle = length \/ 2;\n+          swap_elements(\n+            reverse(slice.slice_front(middle), reverse),\n+            reverse(slice.slice_back(length - middle), reverse)\n+          );\n+        }\n+        return slice;\n+      };\n+\n+      const auto check_reversed = [](ZArraySlice<const int> original, ZArraySlice<int> reversed) {\n+        ASSERT_EQ(original.length(), reversed.length());\n+        for (int e : original) {\n+          ASSERT_EQ(e, reversed.pop());\n+        }\n+      };\n+\n+      ZArraySlice<int> a_reversed = reverse(a, reverse);\n+      check_reversed(original, a_reversed);\n+    };\n+\n+    reverse_test(a0);\n+    reverse_test(a10);\n+    reverse_test(ar);\n+  }\n+\n+  {\n+    const auto sort_test = [&](const ZArray<int>& original) {\n+      ZArray<int> a(original.capacity());\n+      a.appendAll(&original);\n+\n+      const auto shuffle = [&](ZArraySlice<int> slice) {\n+        for (int i = 1; i < slice.length(); ++i) {\n+          const ptrdiff_t random_index = random() % (i + 1);\n+          ::swap(slice.at(i), slice.at(random_index));\n+        }\n+      };\n+\n+      const auto qsort = [](ZArraySlice<int> slice, auto qsort) -> void {\n+        const auto partition = [](ZArraySlice<int> slice) {\n+          const int p = slice.last();\n+          int pi = 0;\n+          for (int i = 0; i < slice.length() - 1; ++i) {\n+            if (slice.at(i) < p) {\n+              ::swap(slice.at(i), slice.at(pi++));\n+            }\n+          }\n+          ::swap(slice.at(pi), slice.last());\n+          return pi;\n+        };\n+\n+        if (slice.length() > 1) {\n+          const int pi = partition(slice);\n+          qsort(slice.slice_front(pi), qsort);\n+          qsort(slice.slice_back(pi + 1), qsort);\n+        }\n+      };\n+\n+      const auto verify = [](ZArraySlice<const int> slice) {\n+        for (int i = 0; i < slice.length(); ++i) {\n+          int e = slice.at(i);\n+          for (int l : slice.slice_front(i)) {\n+            ASSERT_GE(e, l);\n+          }\n+          for (int g : slice.slice_back(i)) {\n+            ASSERT_LE(e, g);\n+          }\n+        }\n+      };\n+\n+      shuffle(a);\n+      qsort(a, qsort);\n+      verify(a);\n+    };\n+\n+    sort_test(a0);\n+    sort_test(a10);\n+    sort_test(ar);\n+  }\n+}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zArray.cpp","additions":116,"deletions":1,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n@@ -224,4 +225,1 @@\n-    const ZPhysicalMemory pmem(ZPhysicalMemorySegment(zoffset(0), ZPageSizeSmall, true));\n-    ZPage page(ZPageType::small, vmem, pmem);\n-\n-    page.reset(ZPageAge::eden);\n+    ZPage page(ZPageType::small, ZPageAge::eden, vmem, 0u);\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zForwarding.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,379 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/z\/zIntrusiveRBTree.inline.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/arena.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"unittest.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"zunittest.hpp\"\n+\n+#include <limits>\n+\n+struct ZTestEntryCompare {\n+  int operator()(const ZIntrusiveRBTreeNode* a, const ZIntrusiveRBTreeNode* b);\n+  int operator()(int key, const ZIntrusiveRBTreeNode* entry);\n+};\n+\n+class ZTestEntry : public ArenaObj {\n+  friend class ZIntrusiveRBTree<int, ZTestEntryCompare>;\n+\n+public:\n+  using ZTree = ZIntrusiveRBTree<int, ZTestEntryCompare>;\n+private:\n+  const int   _id;\n+  ZIntrusiveRBTreeNode _node;\n+\n+public:\n+  ZTestEntry(int id)\n+    : _id(id),\n+      _node() {}\n+\n+  int id() const {\n+    return _id;\n+  }\n+\n+  static ZIntrusiveRBTreeNode* cast_to_inner(ZTestEntry* element) {\n+    return &element->_node;\n+  }\n+  static const ZTestEntry* cast_to_outer(const ZIntrusiveRBTreeNode* node) {\n+    return (ZTestEntry*)((uintptr_t)node - offset_of(ZTestEntry, _node));\n+  }\n+\n+};\n+\n+int ZTestEntryCompare::operator()(const ZIntrusiveRBTreeNode* a, const ZIntrusiveRBTreeNode* b) {\n+  return ZTestEntry::cast_to_outer(a)->id() - ZTestEntry::cast_to_outer(b)->id();\n+}\n+int ZTestEntryCompare::operator()(int key, const ZIntrusiveRBTreeNode* entry) {\n+  return key - ZTestEntry::cast_to_outer(entry)->id();\n+}\n+\n+class ZTreeTest : public ZTest {\n+public:\n+  void shuffle_array(ZTestEntry** beg, ZTestEntry** end);\n+  void reverse_array(ZTestEntry** beg, ZTestEntry** end);\n+};\n+\n+class ResettableArena : public Arena {\n+public:\n+  using Arena::Arena;\n+\n+  void reset_arena() {\n+    if (_chunk != _first) {\n+      set_size_in_bytes(_chunk->length());\n+      Chunk::next_chop(_first);\n+    }\n+    _chunk = _first;\n+    _hwm = _chunk->bottom();\n+    _max = _chunk->top();\n+  }\n+};\n+\n+TEST_F(ZTreeTest, test_random) {\n+  constexpr size_t sizes[] = {1, 2, 4, 8, 16, 1024, 1024 * 1024};\n+  constexpr size_t num_sizes = ARRAY_SIZE(sizes);\n+  constexpr size_t iterations_multiplier = 4;\n+  constexpr size_t max_allocation_size = sizes[num_sizes - 1] * iterations_multiplier * sizeof(ZTestEntry);\n+  ResettableArena arena{MemTag::mtTest, Arena::Tag::tag_other, max_allocation_size};\n+  for (size_t s : sizes) {\n+    ZTestEntry::ZTree tree;\n+    const size_t num_iterations = s * iterations_multiplier;\n+    for (size_t i = 0; i < num_iterations; i++) {\n+      if (i % s == 0) {\n+        tree.verify_tree();\n+      }\n+      int id = random() % s;\n+      auto cursor = tree.find(id);\n+      if (cursor.found()) {\n+        \/\/ Replace or Remove\n+        if (i % 2 == 0) {\n+          \/\/ Replace\n+          if (i % 4 == 0) {\n+            \/\/ Replace with new\n+            tree.replace(ZTestEntry::cast_to_inner(new (&arena) ZTestEntry(id)), cursor);\n+          } else {\n+            \/\/ Replace with same\n+            tree.replace(cursor.node(), cursor);\n+          }\n+        } else {\n+          \/\/ Remove\n+          tree.remove(cursor);\n+        }\n+      } else {\n+        \/\/ Insert\n+        tree.insert(ZTestEntry::cast_to_inner(new (&arena) ZTestEntry(id)), cursor);\n+      }\n+    }\n+    tree.verify_tree();\n+    arena.reset_arena();\n+  }\n+}\n+\n+void ZTreeTest::reverse_array(ZTestEntry** beg, ZTestEntry** end) {\n+  if (beg == end) {\n+    return;\n+  }\n+\n+  ZTestEntry** first = beg;\n+  ZTestEntry** last = end - 1;\n+  while (first < last) {\n+    ::swap(*first, *last);\n+    first++;\n+    last--;\n+  }\n+}\n+\n+void ZTreeTest::shuffle_array(ZTestEntry** beg, ZTestEntry** end) {\n+  if (beg == end) {\n+    return;\n+  }\n+\n+  for (ZTestEntry** first = beg + 1; first != end; first++) {\n+    const ptrdiff_t distance = first - beg;\n+    ASSERT_GE(distance, 0);\n+    const ptrdiff_t random_index = random() % (distance + 1);\n+    ::swap(*first, *(beg + random_index));\n+  }\n+}\n+\n+TEST_F(ZTreeTest, test_insert) {\n+  Arena arena(MemTag::mtTest);\n+  constexpr size_t num_entries = 1024;\n+  ZTestEntry* forward[num_entries]{};\n+  ZTestEntry* reverse[num_entries]{};\n+  ZTestEntry* shuffle[num_entries]{};\n+  for (size_t i = 0; i < num_entries; i++) {\n+    const int id = static_cast<int>(i);\n+    forward[i] = new (&arena) ZTestEntry(id);\n+    reverse[i] = new (&arena) ZTestEntry(id);\n+    shuffle[i] = new (&arena) ZTestEntry(id);\n+  }\n+  reverse_array(reverse, reverse + num_entries);\n+  shuffle_array(shuffle, shuffle + num_entries);\n+\n+  ZTestEntry::ZTree forward_tree;\n+  auto cursor = forward_tree.root_cursor();\n+  for (size_t i = 0; i < num_entries; i++) {\n+    ASSERT_TRUE(cursor.is_valid());\n+    ASSERT_FALSE(cursor.found());\n+    ZIntrusiveRBTreeNode* const new_node = ZTestEntry::cast_to_inner(forward[i]);\n+    forward_tree.insert(new_node, cursor);\n+    cursor = forward_tree.next_cursor(new_node);\n+  }\n+  forward_tree.verify_tree();\n+\n+  ZTestEntry::ZTree reverse_tree;\n+  cursor = reverse_tree.root_cursor();\n+  for (size_t i = 0; i < num_entries; i++) {\n+    ASSERT_TRUE(cursor.is_valid());\n+    ASSERT_FALSE(cursor.found());\n+    ZIntrusiveRBTreeNode* const new_node = ZTestEntry::cast_to_inner(reverse[i]);\n+    reverse_tree.insert(new_node, cursor);\n+    cursor = reverse_tree.prev_cursor(new_node);\n+  }\n+  reverse_tree.verify_tree();\n+\n+  ZTestEntry::ZTree shuffle_tree;\n+  for (size_t i = 0; i < num_entries; i++) {\n+    cursor = shuffle_tree.find(reverse[i]->id());\n+    ASSERT_TRUE(cursor.is_valid());\n+    ASSERT_FALSE(cursor.found());\n+    ZIntrusiveRBTreeNode* const new_node = ZTestEntry::cast_to_inner(reverse[i]);\n+    shuffle_tree.insert(new_node, cursor);\n+  }\n+  shuffle_tree.verify_tree();\n+\n+  ZTestEntryCompare compare_fn;\n+  const ZIntrusiveRBTreeNode* forward_node = forward_tree.first();\n+  const ZIntrusiveRBTreeNode* reverse_node = reverse_tree.first();\n+  const ZIntrusiveRBTreeNode* shuffle_node = shuffle_tree.first();\n+  size_t count = 0;\n+  while (true) {\n+    count++;\n+    ASSERT_EQ(compare_fn(forward_node, reverse_node), 0);\n+    ASSERT_EQ(compare_fn(forward_node, shuffle_node), 0);\n+    ASSERT_EQ(compare_fn(reverse_node, shuffle_node), 0);\n+    const ZIntrusiveRBTreeNode* forward_next_node = forward_node->next();\n+    const ZIntrusiveRBTreeNode* reverse_next_node = reverse_node->next();\n+    const ZIntrusiveRBTreeNode* shuffle_next_node = shuffle_node->next();\n+    if (forward_next_node == nullptr) {\n+      ASSERT_EQ(forward_next_node, reverse_next_node);\n+      ASSERT_EQ(forward_next_node, shuffle_next_node);\n+      ASSERT_EQ(forward_node, forward_tree.last());\n+      ASSERT_EQ(reverse_node, reverse_tree.last());\n+      ASSERT_EQ(shuffle_node, shuffle_tree.last());\n+      break;\n+    }\n+    ASSERT_LT(compare_fn(forward_node, forward_next_node), 0);\n+    ASSERT_LT(compare_fn(reverse_node, reverse_next_node), 0);\n+    ASSERT_LT(compare_fn(shuffle_node, shuffle_next_node), 0);\n+    forward_node = forward_next_node;\n+    reverse_node = reverse_next_node;\n+    shuffle_node = shuffle_next_node;\n+  }\n+  ASSERT_EQ(count, num_entries);\n+}\n+\n+TEST_F(ZTreeTest, test_replace) {\n+  Arena arena(MemTag::mtTest);\n+  constexpr size_t num_entries = 1024;\n+  ZTestEntry::ZTree tree;\n+  auto cursor = tree.root_cursor();\n+  for (size_t i = 0; i < num_entries; i++) {\n+    ASSERT_TRUE(cursor.is_valid());\n+    ASSERT_FALSE(cursor.found());\n+    const int id = static_cast<int>(i) * 2 + 1;\n+    ZIntrusiveRBTreeNode* const new_node = ZTestEntry::cast_to_inner(new (&arena) ZTestEntry(id));\n+    tree.insert(new_node, cursor);\n+    cursor = tree.next_cursor(new_node);\n+  }\n+  tree.verify_tree();\n+\n+  size_t i = 0;\n+  for (auto it = tree.begin(), end = tree.end(); it != end; ++it) {\n+    auto& node = *it;\n+    if (i % (num_entries \/ 4)) {\n+      tree.verify_tree();\n+    }\n+    switch (i++ % 4) {\n+      case 0: {\n+        \/\/ Decrement\n+        ZTestEntry* new_entry = new (&arena) ZTestEntry(ZTestEntry::cast_to_outer(&node)->id() - 1);\n+        it.replace(ZTestEntry::cast_to_inner(new_entry));\n+      } break;\n+      case 1: break;\n+      case 2: {\n+        \/\/ Increment\n+        ZTestEntry* new_entry = new (&arena) ZTestEntry(ZTestEntry::cast_to_outer(&node)->id() + 1);\n+        it.replace(ZTestEntry::cast_to_inner(new_entry));\n+      } break;\n+      case 3: break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+  tree.verify_tree();\n+\n+  int last_id = std::numeric_limits<int>::min();\n+  for (auto& node : tree) {\n+    int id = ZTestEntry::cast_to_outer(&node)->id();\n+    ASSERT_LT(last_id, id);\n+    last_id = id;\n+  }\n+  tree.verify_tree();\n+\n+  last_id = std::numeric_limits<int>::min();\n+  for (auto it = tree.begin(), end = tree.end(); it != end; ++it) {\n+    int id = ZTestEntry::cast_to_outer(&*it)->id();\n+    ASSERT_LT(last_id, id);\n+    last_id = id;\n+  }\n+  tree.verify_tree();\n+\n+  last_id = std::numeric_limits<int>::min();\n+  for (auto it = tree.cbegin(), end = tree.cend(); it != end; ++it) {\n+    int id = ZTestEntry::cast_to_outer(&*it)->id();\n+    ASSERT_LT(last_id, id);\n+    last_id = id;\n+  }\n+  tree.verify_tree();\n+\n+  last_id = std::numeric_limits<int>::max();\n+  for (auto it = tree.rbegin(), end = tree.rend(); it != end; ++it) {\n+    int id = ZTestEntry::cast_to_outer(&*it)->id();\n+    ASSERT_GT(last_id, id);\n+    last_id = id;\n+  }\n+  tree.verify_tree();\n+\n+  last_id = std::numeric_limits<int>::max();\n+  for (auto it = tree.crbegin(), end = tree.crend(); it != end; ++it) {\n+    int id = ZTestEntry::cast_to_outer(&*it)->id();\n+    ASSERT_GT(last_id, id);\n+    last_id = id;\n+  }\n+  tree.verify_tree();\n+}\n+\n+TEST_F(ZTreeTest, test_remove) {\n+  Arena arena(MemTag::mtTest);\n+  constexpr int num_entries = 1024;\n+  ZTestEntry::ZTree tree;\n+  int id = 0;\n+  tree.insert(ZTestEntry::cast_to_inner(new (&arena) ZTestEntry(++id)), tree.root_cursor());\n+  for (auto& node : tree) {\n+    if (ZTestEntry::cast_to_outer(&node)->id() == num_entries) {\n+      break;\n+    }\n+    auto cursor = tree.next_cursor(&node);\n+    ZIntrusiveRBTreeNode* const new_node = ZTestEntry::cast_to_inner(new (&arena) ZTestEntry(++id));\n+    tree.insert(new_node, cursor);\n+  }\n+  tree.verify_tree();\n+  ASSERT_EQ(ZTestEntry::cast_to_outer(tree.last())->id(), num_entries);\n+\n+  int i = 0;\n+  int removed = 0;\n+  for (auto it = tree.begin(), end = tree.end(); it != end; ++it) {\n+    if (i++ % 2 == 0) {\n+      it.remove();\n+      ++removed;\n+    }\n+  }\n+  tree.verify_tree();\n+\n+  int count = 0;\n+  for (auto it = tree.cbegin(), end = tree.cend(); it != end; ++it) {\n+    ++count;\n+  }\n+  ASSERT_EQ(count, num_entries - removed);\n+  tree.verify_tree();\n+\n+  for (auto it = tree.rbegin(), end = tree.rend(); it != end; ++it) {\n+    if (i++ % 2 == 0) {\n+      it.remove();\n+      ++removed;\n+    }\n+  }\n+  tree.verify_tree();\n+\n+  count = 0;\n+  for (auto it = tree.cbegin(), end = tree.cend(); it != end; ++it) {\n+    ++count;\n+  }\n+  ASSERT_EQ(count, num_entries - removed);\n+  tree.verify_tree();\n+\n+  for (auto it = tree.begin(), end = tree.end(); it != end; ++it) {\n+    it.remove();\n+    removed++;\n+  }\n+  tree.verify_tree();\n+\n+  ASSERT_EQ(removed, num_entries);\n+  ASSERT_EQ(tree.last(), nullptr);\n+  ASSERT_EQ(tree.first(), nullptr);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zIntrusiveRBTree.cpp","additions":379,"deletions":0,"binary":false,"changes":379,"status":"added"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -29,3 +28,0 @@\n-#include \"gc\/z\/zMapper_windows.hpp\"\n-#include \"gc\/z\/zMemory.inline.hpp\"\n-#include \"gc\/z\/zSyscall_windows.hpp\"\n@@ -33,0 +29,1 @@\n+#include \"gc\/z\/zVirtualMemoryManager.inline.hpp\"\n@@ -42,2 +39,2 @@\n-  ZVirtualMemoryManager* _vmm;\n-  ZMemoryManager*        _va;\n+  ZVirtualMemoryReserver* _reserver;\n+  ZVirtualMemoryRegistry* _registry;\n@@ -49,1 +46,1 @@\n-      GTEST_SKIP() << \"Requires Windows version 1803 or later\";\n+      GTEST_SKIP() << \"OS not supported\";\n@@ -52,11 +49,3 @@\n-    \/\/ Fake a ZVirtualMemoryManager\n-    _vmm = (ZVirtualMemoryManager*)os::malloc(sizeof(ZVirtualMemoryManager), mtTest);\n-    _vmm = ::new (_vmm) ZVirtualMemoryManager(ReservationSize);\n-\n-    \/\/ Construct its internal ZMemoryManager\n-    _va = new (&_vmm->_manager) ZMemoryManager();\n-\n-    \/\/ Reserve address space for the test\n-    if (_vmm->reserved() != ReservationSize) {\n-      GTEST_SKIP() << \"Failed to reserve address space\";\n-    }\n+    _reserver = (ZVirtualMemoryReserver*)os::malloc(sizeof(ZVirtualMemoryManager), mtTest);\n+    _reserver = ::new (_reserver) ZVirtualMemoryReserver(ReservationSize);\n+    _registry = &_reserver->_registry;\n@@ -72,3 +61,3 @@\n-    _vmm->unreserve_all();\n-    _vmm->~ZVirtualMemoryManager();\n-    os::free(_vmm);\n+    _reserver->unreserve_all();\n+    _reserver->~ZVirtualMemoryReserver();\n+    os::free(_reserver);\n@@ -78,3 +67,3 @@\n-    zoffset bottom = _va->alloc_low_address(ZGranuleSize);\n-    zoffset middle = _va->alloc_low_address(ZGranuleSize);\n-    zoffset top    = _va->alloc_low_address(ZGranuleSize);\n+    ZVirtualMemory bottom = _registry->remove_from_low(ZGranuleSize);\n+    ZVirtualMemory middle = _registry->remove_from_low(ZGranuleSize);\n+    ZVirtualMemory top    = _registry->remove_from_low(ZGranuleSize);\n@@ -82,3 +71,3 @@\n-    ASSERT_EQ(bottom, zoffset(0));\n-    ASSERT_EQ(middle, bottom + 1 * ZGranuleSize);\n-    ASSERT_EQ(top,    bottom + 2 * ZGranuleSize);\n+    ASSERT_EQ(bottom, ZVirtualMemory(bottom.start(),                    ZGranuleSize));\n+    ASSERT_EQ(middle, ZVirtualMemory(bottom.start() + 1 * ZGranuleSize, ZGranuleSize));\n+    ASSERT_EQ(top,    ZVirtualMemory(bottom.start() + 2 * ZGranuleSize, ZGranuleSize));\n@@ -87,1 +76,1 @@\n-    ZMapper::unreserve(ZOffset::address_unsafe(middle), ZGranuleSize);\n+    _reserver->unreserve(middle);\n@@ -90,2 +79,2 @@\n-    ZMapper::unreserve(ZOffset::address_unsafe(bottom), ZGranuleSize);\n-    ZMapper::unreserve(ZOffset::address_unsafe(top), ZGranuleSize);\n+    _reserver->unreserve(bottom);\n+    _reserver->unreserve(top);\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zMapper_windows.cpp","additions":19,"deletions":30,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -1,86 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/z\/zGlobals.hpp\"\n-#include \"gc\/z\/zMemory.inline.hpp\"\n-#include \"zunittest.hpp\"\n-\n-TEST(ZMemory, accessors) {\n-  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n-\n-  {\n-    ZMemory mem(zoffset(0), ZGranuleSize);\n-\n-    EXPECT_EQ(mem.start(), zoffset(0));\n-    EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize));\n-    EXPECT_EQ(mem.size(), ZGranuleSize);\n-  }\n-\n-\n-  {\n-    ZMemory mem(zoffset(ZGranuleSize), ZGranuleSize);\n-\n-    EXPECT_EQ(mem.start(), zoffset(ZGranuleSize));\n-    EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize + ZGranuleSize));\n-    EXPECT_EQ(mem.size(), ZGranuleSize);\n-  }\n-\n-  {\n-    \/\/ Max area - check end boundary\n-    ZMemory mem(zoffset(0), ZAddressOffsetMax);\n-\n-    EXPECT_EQ(mem.start(), zoffset(0));\n-    EXPECT_EQ(mem.end(), zoffset_end(ZAddressOffsetMax));\n-    EXPECT_EQ(mem.size(), ZAddressOffsetMax);\n-  }\n-}\n-\n-TEST(ZMemory, resize) {\n-  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n-\n-  ZMemory mem(zoffset(ZGranuleSize * 2), ZGranuleSize * 2) ;\n-\n-  mem.shrink_from_front(ZGranuleSize);\n-  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 3));\n-  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 4));\n-  EXPECT_EQ(mem.size(),            ZGranuleSize * 1);\n-  mem.grow_from_front(ZGranuleSize);\n-\n-  mem.shrink_from_back(ZGranuleSize);\n-  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 2));\n-  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 3));\n-  EXPECT_EQ(mem.size(),            ZGranuleSize * 1);\n-  mem.grow_from_back(ZGranuleSize);\n-\n-  mem.grow_from_front(ZGranuleSize);\n-  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 1));\n-  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 4));\n-  EXPECT_EQ(mem.size(),            ZGranuleSize * 3);\n-  mem.shrink_from_front(ZGranuleSize);\n-\n-  mem.grow_from_back(ZGranuleSize);\n-  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 2));\n-  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 5));\n-  EXPECT_EQ(mem.size(),            ZGranuleSize * 3);\n-  mem.shrink_from_back(ZGranuleSize);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zMemory.cpp","additions":0,"deletions":86,"binary":false,"changes":86,"status":"deleted"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"zunittest.hpp\"\n+\n+using namespace testing;\n+\n+#ifdef ASSERT\n+\n+class ZNUMATest : public ZTest {\n+protected:\n+  const uint32_t nodes = 4;\n+\n+  uint32_t _original_count;\n+  uint     _original_ZFakeNUMA;\n+\n+public:\n+  virtual void SetUp() {\n+    _original_count = ZNUMA::_count;\n+    _original_ZFakeNUMA = ZFakeNUMA;\n+\n+    \/\/ Setup number of NUMA nodes through faking\n+    ZFakeNUMA = nodes;\n+    ZNUMA::_count = nodes;\n+  }\n+\n+  virtual void TearDown() {\n+    ZNUMA::_count = _original_count;\n+    ZFakeNUMA = _original_ZFakeNUMA;\n+  }\n+};\n+\n+TEST_F(ZNUMATest, calculate_share) {\n+  {\n+    \/\/ Test even spread\n+    const size_t total = nodes * ZGranuleSize;\n+\n+    for (uint32_t numa_id = 0; numa_id < nodes; ++numa_id) {\n+      EXPECT_EQ(ZNUMA::calculate_share(numa_id, total), ZGranuleSize);\n+    }\n+  }\n+\n+  {\n+    \/\/ Test not enough for every node (WITHOUT ignore_count)\n+    const size_t total = (nodes - 1) * ZGranuleSize;\n+\n+    for (uint32_t numa_id = 0; numa_id < (nodes - 1); ++numa_id) {\n+      EXPECT_EQ(ZNUMA::calculate_share(numa_id, total), ZGranuleSize);\n+    }\n+    EXPECT_EQ(ZNUMA::calculate_share(nodes - 1, total), (size_t)0);\n+  }\n+\n+  {\n+    \/\/ Test not enough for every node (WITH ignore_count)\n+    const size_t ignore_count = 2;\n+    const size_t total = nodes * ZGranuleSize;\n+\n+    for (uint32_t numa_id = 0; numa_id < (nodes - ignore_count); ++numa_id) {\n+      EXPECT_EQ(ZNUMA::calculate_share(numa_id, total, ZGranuleSize, ignore_count), nodes * ZGranuleSize \/ (nodes - ignore_count));\n+    }\n+  }\n+\n+  {\n+    \/\/ Test no size\n+    const size_t total = 0;\n+\n+    for (uint32_t numa_id = 0; numa_id < (nodes - 1); ++numa_id) {\n+      EXPECT_EQ(ZNUMA::calculate_share(numa_id, total), (size_t)0);\n+    }\n+  }\n+\n+  {\n+    \/\/ Test one more than even\n+    const size_t total = (nodes + 1) * ZGranuleSize;\n+\n+    EXPECT_EQ(ZNUMA::calculate_share(0, total), ZGranuleSize * 2);\n+    for (uint32_t numa_id = 1; numa_id < nodes; ++numa_id) {\n+      EXPECT_EQ(ZNUMA::calculate_share(numa_id, total), ZGranuleSize);\n+    }\n+  }\n+\n+  {\n+    \/\/ Test one less than even\n+    const size_t total = (nodes * 2 - 1) * ZGranuleSize;\n+\n+    for (uint32_t numa_id = 0; numa_id < (nodes - 1); ++numa_id) {\n+      EXPECT_EQ(ZNUMA::calculate_share(numa_id, total), 2 * ZGranuleSize);\n+    }\n+    EXPECT_EQ(ZNUMA::calculate_share(nodes - 1, total), ZGranuleSize);\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zNUMA.cpp","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -1,218 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/z\/zPhysicalMemory.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-class ZAddressOffsetMaxSetter {\n-private:\n-  const size_t _old_max;\n-  const size_t _old_mask;\n-\n-public:\n-  ZAddressOffsetMaxSetter()\n-    : _old_max(ZAddressOffsetMax),\n-      _old_mask(ZAddressOffsetMask) {\n-    ZAddressOffsetMax = size_t(16) * G * 1024;\n-    ZAddressOffsetMask = ZAddressOffsetMax - 1;\n-  }\n-  ~ZAddressOffsetMaxSetter() {\n-    ZAddressOffsetMax = _old_max;\n-    ZAddressOffsetMask = _old_mask;\n-  }\n-};\n-\n-TEST(ZPhysicalMemoryTest, copy) {\n-  ZAddressOffsetMaxSetter setter;\n-\n-  const ZPhysicalMemorySegment seg0(zoffset(0), 100, true);\n-  const ZPhysicalMemorySegment seg1(zoffset(200), 100, true);\n-\n-  ZPhysicalMemory pmem0;\n-  pmem0.add_segment(seg0);\n-  EXPECT_EQ(pmem0.nsegments(), 1);\n-  EXPECT_EQ(pmem0.segment(0).size(), 100u);\n-\n-  ZPhysicalMemory pmem1;\n-  pmem1.add_segment(seg0);\n-  pmem1.add_segment(seg1);\n-  EXPECT_EQ(pmem1.nsegments(), 2);\n-  EXPECT_EQ(pmem1.segment(0).size(), 100u);\n-  EXPECT_EQ(pmem1.segment(1).size(), 100u);\n-\n-  ZPhysicalMemory pmem2(pmem0);\n-  EXPECT_EQ(pmem2.nsegments(), 1);\n-  EXPECT_EQ(pmem2.segment(0).size(), 100u);\n-\n-  pmem2 = pmem1;\n-  EXPECT_EQ(pmem2.nsegments(), 2);\n-  EXPECT_EQ(pmem2.segment(0).size(), 100u);\n-  EXPECT_EQ(pmem2.segment(1).size(), 100u);\n-}\n-\n-TEST(ZPhysicalMemoryTest, add) {\n-  ZAddressOffsetMaxSetter setter;\n-\n-  const ZPhysicalMemorySegment seg0(zoffset(0), 1, true);\n-  const ZPhysicalMemorySegment seg1(zoffset(1), 1, true);\n-  const ZPhysicalMemorySegment seg2(zoffset(2), 1, true);\n-  const ZPhysicalMemorySegment seg3(zoffset(3), 1, true);\n-  const ZPhysicalMemorySegment seg4(zoffset(4), 1, true);\n-  const ZPhysicalMemorySegment seg5(zoffset(5), 1, true);\n-  const ZPhysicalMemorySegment seg6(zoffset(6), 1, true);\n-\n-  ZPhysicalMemory pmem0;\n-  EXPECT_EQ(pmem0.nsegments(), 0);\n-  EXPECT_EQ(pmem0.is_null(), true);\n-\n-  ZPhysicalMemory pmem1;\n-  pmem1.add_segment(seg0);\n-  pmem1.add_segment(seg1);\n-  pmem1.add_segment(seg2);\n-  pmem1.add_segment(seg3);\n-  pmem1.add_segment(seg4);\n-  pmem1.add_segment(seg5);\n-  pmem1.add_segment(seg6);\n-  EXPECT_EQ(pmem1.nsegments(), 1);\n-  EXPECT_EQ(pmem1.segment(0).size(), 7u);\n-  EXPECT_EQ(pmem1.is_null(), false);\n-\n-  ZPhysicalMemory pmem2;\n-  pmem2.add_segment(seg0);\n-  pmem2.add_segment(seg1);\n-  pmem2.add_segment(seg2);\n-  pmem2.add_segment(seg4);\n-  pmem2.add_segment(seg5);\n-  pmem2.add_segment(seg6);\n-  EXPECT_EQ(pmem2.nsegments(), 2);\n-  EXPECT_EQ(pmem2.segment(0).size(), 3u);\n-  EXPECT_EQ(pmem2.segment(1).size(), 3u);\n-  EXPECT_EQ(pmem2.is_null(), false);\n-\n-  ZPhysicalMemory pmem3;\n-  pmem3.add_segment(seg0);\n-  pmem3.add_segment(seg2);\n-  pmem3.add_segment(seg3);\n-  pmem3.add_segment(seg4);\n-  pmem3.add_segment(seg6);\n-  EXPECT_EQ(pmem3.nsegments(), 3);\n-  EXPECT_EQ(pmem3.segment(0).size(), 1u);\n-  EXPECT_EQ(pmem3.segment(1).size(), 3u);\n-  EXPECT_EQ(pmem3.segment(2).size(), 1u);\n-  EXPECT_EQ(pmem3.is_null(), false);\n-\n-  ZPhysicalMemory pmem4;\n-  pmem4.add_segment(seg0);\n-  pmem4.add_segment(seg2);\n-  pmem4.add_segment(seg4);\n-  pmem4.add_segment(seg6);\n-  EXPECT_EQ(pmem4.nsegments(), 4);\n-  EXPECT_EQ(pmem4.segment(0).size(), 1u);\n-  EXPECT_EQ(pmem4.segment(1).size(), 1u);\n-  EXPECT_EQ(pmem4.segment(2).size(), 1u);\n-  EXPECT_EQ(pmem4.segment(3).size(), 1u);\n-  EXPECT_EQ(pmem4.is_null(), false);\n-}\n-\n-TEST(ZPhysicalMemoryTest, remove) {\n-  ZAddressOffsetMaxSetter setter;\n-\n-  ZPhysicalMemory pmem;\n-\n-  pmem.add_segment(ZPhysicalMemorySegment(zoffset(10), 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(zoffset(30), 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(zoffset(50), 10, true));\n-  EXPECT_EQ(pmem.nsegments(), 3);\n-  EXPECT_EQ(pmem.size(), 30u);\n-  EXPECT_FALSE(pmem.is_null());\n-\n-  pmem.remove_segments();\n-  EXPECT_EQ(pmem.nsegments(), 0);\n-  EXPECT_EQ(pmem.size(), 0u);\n-  EXPECT_TRUE(pmem.is_null());\n-}\n-\n-TEST(ZPhysicalMemoryTest, split) {\n-  ZAddressOffsetMaxSetter setter;\n-\n-  ZPhysicalMemory pmem;\n-\n-  pmem.add_segment(ZPhysicalMemorySegment(zoffset(0), 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(zoffset(10), 10, true));\n-  pmem.add_segment(ZPhysicalMemorySegment(zoffset(30), 10, true));\n-  EXPECT_EQ(pmem.nsegments(), 2);\n-  EXPECT_EQ(pmem.size(), 30u);\n-\n-  ZPhysicalMemory pmem0 = pmem.split(1);\n-  EXPECT_EQ(pmem0.nsegments(), 1);\n-  EXPECT_EQ(pmem0.size(), 1u);\n-  EXPECT_EQ(pmem.nsegments(), 2);\n-  EXPECT_EQ(pmem.size(), 29u);\n-\n-  ZPhysicalMemory pmem1 = pmem.split(25);\n-  EXPECT_EQ(pmem1.nsegments(), 2);\n-  EXPECT_EQ(pmem1.size(), 25u);\n-  EXPECT_EQ(pmem.nsegments(), 1);\n-  EXPECT_EQ(pmem.size(), 4u);\n-\n-  ZPhysicalMemory pmem2 = pmem.split(4);\n-  EXPECT_EQ(pmem2.nsegments(), 1);\n-  EXPECT_EQ(pmem2.size(), 4u);\n-  EXPECT_EQ(pmem.nsegments(), 0);\n-  EXPECT_EQ(pmem.size(), 0u);\n-}\n-\n-TEST(ZPhysicalMemoryTest, split_committed) {\n-  ZAddressOffsetMaxSetter setter;\n-\n-  ZPhysicalMemory pmem0;\n-  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(0), 10, true));\n-  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(10), 10, false));\n-  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(20), 10, true));\n-  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(30), 10, false));\n-  EXPECT_EQ(pmem0.nsegments(), 4);\n-  EXPECT_EQ(pmem0.size(), 40u);\n-\n-  ZPhysicalMemory pmem1 = pmem0.split_committed();\n-  EXPECT_EQ(pmem0.nsegments(), 2);\n-  EXPECT_EQ(pmem0.size(), 20u);\n-  EXPECT_EQ(pmem1.nsegments(), 2);\n-  EXPECT_EQ(pmem1.size(), 20u);\n-}\n-\n-TEST(ZPhysicalMemoryTest, limits) {\n-  ZAddressOffsetMaxSetter setter;\n-\n-  const size_t HalfZAddressOffsetMax = ZAddressOffsetMax >> 1;\n-  ZPhysicalMemory pmem0;\n-  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(0), HalfZAddressOffsetMax, true));\n-  pmem0.add_segment(ZPhysicalMemorySegment(zoffset(HalfZAddressOffsetMax), HalfZAddressOffsetMax, false));\n-  EXPECT_EQ(pmem0.nsegments(), 2);\n-  EXPECT_EQ(pmem0.size(), ZAddressOffsetMax);\n-\n-  ZPhysicalMemory pmem1 = pmem0.split_committed();\n-  EXPECT_EQ(pmem0.nsegments(), 1);\n-  EXPECT_EQ(pmem0.size(), HalfZAddressOffsetMax);\n-  EXPECT_EQ(pmem1.nsegments(), 1);\n-  EXPECT_EQ(pmem1.size(), HalfZAddressOffsetMax);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zPhysicalMemory.cpp","additions":0,"deletions":218,"binary":false,"changes":218,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -24,0 +24,1 @@\n+#include \"gc\/z\/zGlobals.hpp\"\n@@ -27,1 +28,1 @@\n-TEST(ZVirtualMemory, split) {\n+TEST(ZVirtualMemory, is_null) {\n@@ -30,1 +31,15 @@\n-  ZVirtualMemory vmem(zoffset(0), 10);\n+  ZVirtualMemory mem;\n+  EXPECT_TRUE(mem.is_null());\n+}\n+\n+TEST(ZVirtualMemory, accessors) {\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n+\n+  {\n+    ZVirtualMemory mem(zoffset(0), ZGranuleSize);\n+\n+    EXPECT_EQ(mem.start(), zoffset(0));\n+    EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize));\n+    EXPECT_EQ(mem.size(), ZGranuleSize);\n+    EXPECT_EQ(mem.granule_count(), 1);\n+  }\n@@ -32,3 +47,87 @@\n-  ZVirtualMemory vmem0 = vmem.split(0);\n-  EXPECT_EQ(vmem0.size(), 0u);\n-  EXPECT_EQ(vmem.size(), 10u);\n+  {\n+    ZVirtualMemory mem(zoffset(ZGranuleSize), ZGranuleSize);\n+\n+    EXPECT_EQ(mem.start(), zoffset(ZGranuleSize));\n+    EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize + ZGranuleSize));\n+    EXPECT_EQ(mem.size(), ZGranuleSize);\n+    EXPECT_EQ(mem.granule_count(), 1);\n+  }\n+\n+  {\n+    \/\/ Max area - check end boundary\n+    ZVirtualMemory mem(zoffset(0), ZAddressOffsetMax);\n+\n+    EXPECT_EQ(mem.start(), zoffset(0));\n+    EXPECT_EQ(mem.end(), zoffset_end(ZAddressOffsetMax));\n+    EXPECT_EQ(mem.size(), ZAddressOffsetMax);\n+    EXPECT_EQ(mem.granule_count(), (int)(ZAddressOffsetMax >> ZGranuleSizeShift));\n+  }\n+}\n+\n+TEST(ZVirtualMemory, resize) {\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n+\n+  ZVirtualMemory mem(zoffset(ZGranuleSize * 2), ZGranuleSize * 2) ;\n+\n+  mem.shrink_from_front(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 3));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 4));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 1);\n+  mem.grow_from_front(ZGranuleSize);\n+\n+  mem.shrink_from_back(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 2));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 3));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 1);\n+  mem.grow_from_back(ZGranuleSize);\n+\n+  mem.grow_from_front(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 1));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 4));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 3);\n+  mem.shrink_from_front(ZGranuleSize);\n+\n+  mem.grow_from_back(ZGranuleSize);\n+  EXPECT_EQ(mem.start(),   zoffset(ZGranuleSize * 2));\n+  EXPECT_EQ(mem.end(), zoffset_end(ZGranuleSize * 5));\n+  EXPECT_EQ(mem.size(),            ZGranuleSize * 3);\n+  mem.shrink_from_back(ZGranuleSize);\n+}\n+\n+TEST(ZVirtualMemory, shrink_from_front) {\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n+\n+  ZVirtualMemory mem(zoffset(0), ZGranuleSize * 10);\n+\n+  ZVirtualMemory mem0 = mem.shrink_from_front(0);\n+  EXPECT_EQ(mem0.size(), 0u);\n+  EXPECT_EQ(mem.size(), ZGranuleSize * 10);\n+\n+  ZVirtualMemory mem1 = mem.shrink_from_front(ZGranuleSize * 5);\n+  EXPECT_EQ(mem1.size(), ZGranuleSize * 5);\n+  EXPECT_EQ(mem.size(), ZGranuleSize * 5);\n+\n+  ZVirtualMemory mem2 = mem.shrink_from_front(ZGranuleSize * 5);\n+  EXPECT_EQ(mem2.size(), ZGranuleSize * 5);\n+  EXPECT_EQ(mem.size(), 0u);\n+\n+  ZVirtualMemory mem3 = mem.shrink_from_front(0);\n+  EXPECT_EQ(mem3.size(), 0u);\n+}\n+\n+TEST(ZVirtualMemory, shrink_from_back) {\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n+\n+  ZVirtualMemory mem(zoffset(0), ZGranuleSize * 10);\n+\n+  ZVirtualMemory mem1 = mem.shrink_from_back(ZGranuleSize * 5);\n+  EXPECT_EQ(mem1.size(), ZGranuleSize * 5);\n+  EXPECT_EQ(mem.size(), ZGranuleSize * 5);\n+\n+  ZVirtualMemory mem2 = mem.shrink_from_back(ZGranuleSize * 5);\n+  EXPECT_EQ(mem2.size(), ZGranuleSize * 5);\n+  EXPECT_EQ(mem.size(), 0u);\n+}\n+\n+TEST(ZVirtualMemory, adjacent_to) {\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n@@ -36,3 +135,3 @@\n-  ZVirtualMemory vmem1 = vmem.split(5);\n-  EXPECT_EQ(vmem1.size(), 5u);\n-  EXPECT_EQ(vmem.size(), 5u);\n+  ZVirtualMemory mem0(zoffset(0), ZGranuleSize);\n+  ZVirtualMemory mem1(zoffset(ZGranuleSize), ZGranuleSize);\n+  ZVirtualMemory mem2(zoffset(ZGranuleSize * 2), ZGranuleSize);\n@@ -40,3 +139,4 @@\n-  ZVirtualMemory vmem2 = vmem.split(5);\n-  EXPECT_EQ(vmem2.size(), 5u);\n-  EXPECT_EQ(vmem.size(), 0u);\n+  EXPECT_TRUE(mem0.adjacent_to(mem1));\n+  EXPECT_TRUE(mem1.adjacent_to(mem0));\n+  EXPECT_TRUE(mem1.adjacent_to(mem2));\n+  EXPECT_TRUE(mem2.adjacent_to(mem1));\n@@ -44,2 +144,2 @@\n-  ZVirtualMemory vmem3 = vmem.split(0);\n-  EXPECT_EQ(vmem3.size(), 0u);\n+  EXPECT_FALSE(mem0.adjacent_to(mem2));\n+  EXPECT_FALSE(mem2.adjacent_to(mem0));\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zVirtualMemory.cpp","additions":114,"deletions":14,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/z\/zMemory.inline.hpp\"\n@@ -32,1 +31,1 @@\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.inline.hpp\"\n@@ -38,1 +37,1 @@\n-#define ASSERT_ALLOC_OK(offset) ASSERT_NE(offset, zoffset(UINTPTR_MAX))\n+#define ASSERT_REMOVAL_OK(range, sz) ASSERT_FALSE(range.is_null()); ASSERT_EQ(range.size(), (sz))\n@@ -42,2 +41,2 @@\n-  ZMemoryManager::Callbacks* _callbacks;\n-  ZMemoryManager::Callbacks  _saved;\n+  ZVirtualMemoryRegistry::Callbacks* _callbacks;\n+  ZVirtualMemoryRegistry::Callbacks  _saved;\n@@ -46,1 +45,1 @@\n-  ZCallbacksResetter(ZMemoryManager::Callbacks* callbacks)\n+  ZCallbacksResetter(ZVirtualMemoryRegistry::Callbacks* callbacks)\n@@ -60,2 +59,2 @@\n-  ZMemoryManager*        _va;\n-  ZVirtualMemoryManager* _vmm;\n+  ZVirtualMemoryReserver* _reserver;\n+  ZVirtualMemoryRegistry* _registry;\n@@ -70,3 +69,3 @@\n-    void* vmr_mem = os::malloc(sizeof(ZVirtualMemoryManager), mtTest);\n-    _vmm = ::new (vmr_mem) ZVirtualMemoryManager(ReservationSize);\n-    _va = &_vmm->_manager;\n+    _reserver = (ZVirtualMemoryReserver*)os::malloc(sizeof(ZVirtualMemoryManager), mtTest);\n+    _reserver = ::new (_reserver) ZVirtualMemoryReserver(ReservationSize);\n+    _registry = &_reserver->_registry;\n@@ -82,3 +81,3 @@\n-    _vmm->unreserve_all();\n-    _vmm->~ZVirtualMemoryManager();\n-    os::free(_vmm);\n+    _reserver->unreserve_all();\n+    _reserver->~ZVirtualMemoryReserver();\n+    os::free(_reserver);\n@@ -97,1 +96,1 @@\n-    \/\/ then we won't be able to allocate 4 consecutive granules and the code\n+    \/\/ then we won't be able to reserve 4 consecutive granules and the code\n@@ -116,1 +115,1 @@\n-    if (_vmm->reserved() < 4 * ZGranuleSize || !_va->free_is_contiguous()) {\n+    if (_reserver->reserved() < 4 * ZGranuleSize || !_registry->is_contiguous()) {\n@@ -118,1 +117,1 @@\n-          << (_vmm->reserved() >> ZGranuleSizeShift) << \" * ZGranuleSize\";\n+          << (_reserver->reserved() >> ZGranuleSizeShift) << \" * ZGranuleSize\";\n@@ -122,1 +121,1 @@\n-    const zoffset base_offset = _vmm->lowest_available_address();\n+    const zoffset base_offset = _registry->peek_low_address();\n@@ -125,1 +124,1 @@\n-    _vmm->unreserve_all();\n+    _reserver->unreserve_all();\n@@ -143,1 +142,1 @@\n-      \/\/ lead to a subsequent failure when _vmm->alloc tried to split off the\n+      \/\/ lead to a subsequent failure when _vmr->remove* tried to split off the\n@@ -156,1 +155,1 @@\n-      const size_t reserved = _vmm->reserve_discontiguous(base_offset, 4 * ZGranuleSize, ZGranuleSize);\n+      const size_t reserved = _reserver->reserve_discontiguous(base_offset, 4 * ZGranuleSize, ZGranuleSize);\n@@ -168,3 +167,2 @@\n-      const ZVirtualMemory vmem = _vmm->alloc(2 * ZGranuleSize, true);\n-      ASSERT_EQ(vmem.start(), base_offset);\n-      ASSERT_EQ(vmem.size(), 2 * ZGranuleSize);\n+      const ZVirtualMemory vmem = _registry->remove_from_low(2 * ZGranuleSize);\n+      ASSERT_EQ(vmem, ZVirtualMemory(base_offset, 2 * ZGranuleSize));\n@@ -174,2 +172,2 @@\n-      _vmm->unreserve(base_offset, ZGranuleSize);\n-      _vmm->unreserve(base_offset + ZGranuleSize, ZGranuleSize);\n+      _reserver->unreserve(vmem.first_part(ZGranuleSize));\n+      _reserver->unreserve(vmem.last_part(ZGranuleSize));\n@@ -179,4 +177,3 @@\n-    const ZVirtualMemory vmem = _vmm->alloc(ZGranuleSize, true);\n-    ASSERT_EQ(vmem.start(), base_offset + 2 * ZGranuleSize);\n-    ASSERT_EQ(vmem.size(), ZGranuleSize);\n-    _vmm->unreserve(vmem.start(), vmem.size());\n+    const ZVirtualMemory vmem = _registry->remove_from_low(ZGranuleSize);\n+    ASSERT_EQ(vmem, ZVirtualMemory(base_offset + 2 * ZGranuleSize, ZGranuleSize));\n+    _reserver->unreserve(vmem);\n@@ -188,4 +185,5 @@\n-  void test_alloc_low_address() {\n-    \/\/ Verify that we get a placeholder for the first granule\n-    zoffset bottom = _va->alloc_low_address(ZGranuleSize);\n-    ASSERT_ALLOC_OK(bottom);\n+  void test_remove_from_low() {\n+    {\n+      \/\/ Verify that we get a placeholder for the first granule\n+      const ZVirtualMemory removed = _registry->remove_from_low(ZGranuleSize);\n+      ASSERT_REMOVAL_OK(removed, ZGranuleSize);\n@@ -193,1 +191,2 @@\n-    _va->free(bottom, ZGranuleSize);\n+      _registry->insert(removed);\n+    }\n@@ -195,3 +194,4 @@\n-    \/\/ Alloc something larger than a granule and free it\n-    bottom = _va->alloc_low_address(ZGranuleSize * 3);\n-    ASSERT_ALLOC_OK(bottom);\n+    {\n+      \/\/ Remove something larger than a granule and then insert it\n+      const ZVirtualMemory removed = _registry->remove_from_low(3 * ZGranuleSize);\n+      ASSERT_REMOVAL_OK(removed, 3 * ZGranuleSize);\n@@ -199,1 +199,2 @@\n-    _va->free(bottom, ZGranuleSize * 3);\n+      _registry->insert(removed);\n+    }\n@@ -201,3 +202,4 @@\n-    \/\/ Free with more memory allocated\n-    bottom = _va->alloc_low_address(ZGranuleSize);\n-    ASSERT_ALLOC_OK(bottom);\n+    {\n+      \/\/ Insert with more memory removed\n+      const ZVirtualMemory removed = _registry->remove_from_low(ZGranuleSize);\n+      ASSERT_REMOVAL_OK(removed, ZGranuleSize);\n@@ -205,2 +207,2 @@\n-    zoffset next = _va->alloc_low_address(ZGranuleSize);\n-    ASSERT_ALLOC_OK(next);\n+      ZVirtualMemory next = _registry->remove_from_low(ZGranuleSize);\n+      ASSERT_REMOVAL_OK(next, ZGranuleSize);\n@@ -208,2 +210,3 @@\n-    _va->free(bottom, ZGranuleSize);\n-    _va->free(next, ZGranuleSize);\n+      _registry->insert(removed);\n+      _registry->insert(next);\n+    }\n@@ -212,4 +215,5 @@\n-  void test_alloc_high_address() {\n-    \/\/ Verify that we get a placeholder for the last granule\n-    zoffset high = _va->alloc_high_address(ZGranuleSize);\n-    ASSERT_ALLOC_OK(high);\n+  void test_remove_from_high() {\n+    {\n+      \/\/ Verify that we get a placeholder for the last granule\n+      const ZVirtualMemory high = _registry->remove_from_high(ZGranuleSize);\n+      ASSERT_REMOVAL_OK(high, ZGranuleSize);\n@@ -217,2 +221,2 @@\n-    zoffset prev = _va->alloc_high_address(ZGranuleSize);\n-    ASSERT_ALLOC_OK(prev);\n+      const ZVirtualMemory prev = _registry->remove_from_high(ZGranuleSize);\n+      ASSERT_REMOVAL_OK(prev, ZGranuleSize);\n@@ -220,2 +224,3 @@\n-    _va->free(high, ZGranuleSize);\n-    _va->free(prev, ZGranuleSize);\n+      _registry->insert(high);\n+      _registry->insert(prev);\n+    }\n@@ -223,3 +228,4 @@\n-    \/\/ Alloc something larger than a granule and return it\n-    high = _va->alloc_high_address(ZGranuleSize * 2);\n-    ASSERT_ALLOC_OK(high);\n+    {\n+      \/\/ Remove something larger than a granule and return it\n+      const ZVirtualMemory high = _registry->remove_from_high(2 * ZGranuleSize);\n+      ASSERT_REMOVAL_OK(high, 2 * ZGranuleSize);\n@@ -227,1 +233,2 @@\n-    _va->free(high, ZGranuleSize * 2);\n+      _registry->insert(high);\n+    }\n@@ -230,4 +237,10 @@\n-  void test_alloc_whole_area() {\n-    \/\/ Alloc the whole reservation\n-    zoffset bottom = _va->alloc_low_address(ReservationSize);\n-    ASSERT_ALLOC_OK(bottom);\n+  void test_remove_whole() {\n+    \/\/ Need a local variable to appease gtest\n+    const size_t reservation_size = ReservationSize;\n+\n+    \/\/ Remove the whole reservation\n+    const ZVirtualMemory reserved = _registry->remove_from_low(reservation_size);\n+    ASSERT_REMOVAL_OK(reserved, reservation_size);\n+\n+    const ZVirtualMemory first(reserved.start(), 4 * ZGranuleSize);\n+    const ZVirtualMemory second(reserved.start() + 6 * ZGranuleSize, 6 * ZGranuleSize);\n@@ -235,3 +248,3 @@\n-    \/\/ Free two chunks and then allocate them again\n-    _va->free(bottom, ZGranuleSize * 4);\n-    _va->free(bottom + ZGranuleSize * 6, ZGranuleSize * 6);\n+    \/\/ Insert two chunks and then remove them again\n+    _registry->insert(first);\n+    _registry->insert(second);\n@@ -239,2 +252,2 @@\n-    zoffset offset = _va->alloc_low_address(ZGranuleSize * 4);\n-    ASSERT_ALLOC_OK(offset);\n+    const ZVirtualMemory removed_first = _registry->remove_from_low(first.size());\n+    ASSERT_EQ(removed_first, first);\n@@ -242,2 +255,2 @@\n-    offset = _va->alloc_low_address(ZGranuleSize * 6);\n-    ASSERT_ALLOC_OK(offset);\n+    const ZVirtualMemory removed_second = _registry->remove_from_low(second.size());\n+    ASSERT_EQ(removed_second, second);\n@@ -245,2 +258,2 @@\n-    \/\/ Now free it all, and verify it can be re-allocated\n-    _va->free(bottom, ReservationSize);\n+    \/\/ Now insert it all, and verify it can be re-removed\n+    _registry->insert(reserved);\n@@ -248,2 +261,2 @@\n-    bottom = _va->alloc_low_address(ReservationSize);\n-    ASSERT_ALLOC_OK(bottom);\n+    const ZVirtualMemory removed_reserved = _registry->remove_from_low(reservation_size);\n+    ASSERT_EQ(removed_reserved, reserved);\n@@ -251,1 +264,1 @@\n-    _va->free(bottom, ReservationSize);\n+    _registry->insert(reserved);\n@@ -259,2 +272,2 @@\n-TEST_VM_F(ZVirtualMemoryManagerTest, test_alloc_low_address) {\n-  test_alloc_low_address();\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_remove_from_low) {\n+  test_remove_from_low();\n@@ -263,2 +276,2 @@\n-TEST_VM_F(ZVirtualMemoryManagerTest, test_alloc_high_address) {\n-  test_alloc_high_address();\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_remove_from_high) {\n+  test_remove_from_high();\n@@ -267,2 +280,2 @@\n-TEST_VM_F(ZVirtualMemoryManagerTest, test_alloc_whole_area) {\n-  test_alloc_whole_area();\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_remove_whole) {\n+  test_remove_whole();\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zVirtualMemoryManager.cpp","additions":93,"deletions":80,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -30,0 +30,4 @@\n+#include \"gc\/z\/zNUMA.hpp\"\n+#include \"gc\/z\/zRangeRegistry.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n@@ -32,0 +36,6 @@\n+#include <ostream>\n+\n+inline std::ostream& operator<<(std::ostream& str, const ZVirtualMemory& vmem) {\n+  return str << \"ZVirtualMemory{start=\" << (void*)untype(vmem.start()) << \", size=\" << vmem.size() << \"}\";\n+}\n+\n@@ -55,0 +65,1 @@\n+  unsigned int _rand_seed;\n@@ -58,1 +69,2 @@\n-    : _zaddress_offset_max_setter(ZAddressOffsetMax) {\n+    : _zaddress_offset_max_setter(ZAddressOffsetMax),\n+      _rand_seed(static_cast<unsigned int>(::testing::UnitTest::GetInstance()->random_seed())) {\n@@ -67,0 +79,1 @@\n+      ZNUMA::pd_initialize();\n@@ -77,0 +90,6 @@\n+  int random() {\n+    const int next_seed = os::next_random(_rand_seed);\n+    _rand_seed = static_cast<unsigned int>(next_seed);\n+    return next_seed;\n+  }\n+\n","filename":"test\/hotspot\/gtest\/gc\/z\/zunittest.hpp","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,2 @@\n-# Quiet all SA tests\n+# Quiet the majority of SA tests\n+# We run serviceability\/sa\/TestUniverse.java as a sanity check for minimal functionality\n@@ -105,1 +106,0 @@\n-serviceability\/sa\/TestUniverse.java                           8307393   generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package gc.z;\n+\n+\/*\n+ * @test TestMappedCacheHarvest\n+ * @requires vm.gc.Z\n+ * @summary Test ZGC mapped cache harvesting\n+ * @library \/test\/lib\n+ * @run driver gc.z.TestMappedCacheHarvest\n+ *\/\n+\n+import java.util.ArrayList;\n+import java.lang.instrument.Instrumentation;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestMappedCacheHarvest {\n+    static class Test {\n+        private static int M = 1024 * 1024;\n+\n+        private static int LARGE = 33 * M;\n+        private static int MEDIUM = 3 * M;\n+\n+        public volatile static byte[] tmp;\n+\n+        private static long freeMem() {\n+            long currentlyCommitted = Runtime.getRuntime().totalMemory();\n+            long currentlyFree = Runtime.getRuntime().freeMemory();\n+            long max = Runtime.getRuntime().maxMemory();\n+            return max - (currentlyCommitted - currentlyFree);\n+        }\n+\n+        public static void main(String[] args) throws Exception {\n+            int iter = 0;\n+            ArrayList<byte[]> keep = new ArrayList<>();\n+            try {\n+                \/\/ Interleave ever growing different sized allocation while\n+                \/\/ keeping the smaller allocations alive until the heap is full\n+                \/\/ and having the larger allocations be transient.\n+                while (freeMem() > LARGE) {\n+                    tmp = new byte[LARGE];\n+                    keep.add(new byte[MEDIUM]);\n+\n+                    System.gc();\n+                    LARGE += 2*M;\n+                    if (freeMem() < LARGE) {\n+                        \/\/ Release the keep to see if we can continue\n+                        keep = new ArrayList<>();\n+                        System.gc();\n+                        MEDIUM += 2*M;\n+                    }\n+                }\n+                System.out.println(\"Last large size: \" + LARGE \/ M + \"M (\" + iter + \") Free mem: \" +\n+                        Runtime.getRuntime().freeMemory() \/ M + \"M\" );\n+            } catch (OutOfMemoryError oome) {\n+                keep = null;\n+                System.out.println(\"Premature OOME: large size: \" + LARGE \/ M + \"M (\" + iter + \") Free mem: \" +\n+                        Runtime.getRuntime().freeMemory() \/ M + \"M\" );\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        ProcessTools.executeTestJava(\n+            \"-XX:+UseZGC\",\n+            \"-Xms128M\",\n+            \"-Xmx128M\",\n+            \"-Xlog:gc,gc+init,gc+heap=debug\",\n+            Test.class.getName())\n+                .outputTo(System.out)\n+                .errorTo(System.out)\n+                .shouldContain(\"Mapped Cache Harvested:\")\n+                .shouldNotContain(\"Out of address space\")\n+                .shouldHaveExitValue(0);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestMappedCacheHarvest.java","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -1,82 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.z;\n-\n-\/*\n- * @test TestPageCacheFlush\n- * @requires vm.gc.Z\n- * @summary Test ZGC page cache flushing\n- * @library \/test\/lib\n- * @run driver gc.z.TestPageCacheFlush\n- *\/\n-\n-import java.util.LinkedList;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestPageCacheFlush {\n-    static class Test {\n-        private static final int K = 1024;\n-        private static final int M = K * K;\n-        private static volatile LinkedList<byte[]> keepAlive;\n-\n-        public static void fillPageCache(int size) {\n-            System.out.println(\"Begin allocate (\" + size + \")\");\n-\n-            keepAlive = new LinkedList<>();\n-\n-            try {\n-                for (;;) {\n-                    keepAlive.add(new byte[size]);\n-                }\n-            } catch (OutOfMemoryError e) {\n-                keepAlive = null;\n-                System.gc();\n-            }\n-\n-            System.out.println(\"End allocate (\" + size + \")\");\n-        }\n-\n-        public static void main(String[] args) throws Exception {\n-            \/\/ Allocate small objects to fill the page cache with small pages\n-            fillPageCache(10 * K);\n-\n-            \/\/ Allocate large objects to provoke page cache flushing to rebuild\n-            \/\/ cached small pages into large pages\n-            fillPageCache(10 * M);\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        ProcessTools.executeTestJava(\n-            \"-XX:+UseZGC\",\n-            \"-Xms128M\",\n-            \"-Xmx128M\",\n-            \"-Xlog:gc,gc+init,gc+heap=debug\",\n-            Test.class.getName())\n-                .outputTo(System.out)\n-                .errorTo(System.out)\n-                .shouldContain(\"Page Cache Flushed:\")\n-                .shouldHaveExitValue(0);\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestPageCacheFlush.java","additions":0,"deletions":82,"binary":false,"changes":82,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.z.TestUncommit\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=5 gc.z.TestUncommit\n@@ -38,1 +38,1 @@\n-    private static final int delay = 10 * 1000; \/\/ milliseconds\n+    private static final int delay = 5 * 1000; \/\/ milliseconds\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestUncommit.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,1 +59,1 @@\n-            oa.shouldContain(\"Address Space Type: Discontiguous\");\n+            oa.shouldContain(\"Reserved Space Type: Discontiguous\");\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZForceDiscontiguousHeapReservations.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -86,1 +86,1 @@\n-            oa.shouldContain(\"Address Space Type: Discontiguous\");\n+            oa.shouldContain(\"Reserved Space Type: Discontiguous\");\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZNMT.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -82,0 +82,3 @@\n+            expStrings.add(\"used\");\n+            expStrings.add(\" capacity \");\n+            expStrings.add(\"max capacity\");\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/TestUniverse.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,70 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package jdk.jfr.event.gc.detailed;\n-\n-import java.util.List;\n-\n-import static gc.testlibrary.Allocation.blackHole;\n-import jdk.jfr.Recording;\n-import jdk.jfr.consumer.RecordedEvent;\n-import jdk.test.lib.jfr.EventNames;\n-import jdk.test.lib.jfr.Events;\n-\n-\/**\n- * @test id=Z\n- * @requires vm.hasJFR & vm.gc.Z\n- * @requires vm.flagless\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestZUnmapEvent\n- *\/\n-\n-public class TestZUnmapEvent {\n-    public static void main(String[] args) throws Exception {\n-        try (Recording recording = new Recording()) {\n-            \/\/ Activate the event we are interested in and start recording\n-            recording.enable(EventNames.ZUnmap);\n-            recording.start();\n-\n-            \/\/ Allocate non-large objects, to fill page cache with non-large pages\n-            for (int i = 0; i < 128; i++) {\n-                blackHole(new byte[256 * 1024]);\n-            }\n-\n-            \/\/ Allocate large objects, to provoke page cache flushing and unmapping\n-            for (int i = 0; i < 10; i++) {\n-                blackHole(new byte[7 * 1024 * 1024]);\n-            }\n-\n-            \/\/ Wait for unmap to happen\n-            Thread.sleep(10 * 1000);\n-\n-            recording.stop();\n-\n-            \/\/ Verify recording\n-            List<RecordedEvent> events = Events.fromRecording(recording);\n-            System.out.println(\"Events: \" + events.size());\n-            Events.hasEvents(events);\n-        }\n-    }\n-}\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZUnmapEvent.java","additions":0,"deletions":70,"binary":false,"changes":70,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -156,1 +156,0 @@\n-    public static final String ZUnmap = PREFIX + \"ZUnmap\";\n","filename":"test\/lib\/jdk\/test\/lib\/jfr\/EventNames.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"}]}