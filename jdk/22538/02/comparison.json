{"files":[{"patch":"@@ -1049,1 +1049,4 @@\n-  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup(),\n+                              \"cleanup early.\");\n+  ShenandoahHeap::heap()->recycle_trash();\n@@ -1181,1 +1184,4 @@\n-  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup(),\n+                              \"cleanup complete.\");\n+  ShenandoahHeap::heap()->recycle_trash();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -732,1 +732,0 @@\n-  _trash_regions(NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, max_regions, mtGC)),\n@@ -1005,1 +1004,1 @@\n-  try_recycle_trashed(r);\n+  r->recycle_under_lock();\n@@ -1216,1 +1215,1 @@\n-    try_recycle_trashed(r);\n+    r->recycle_under_lock();\n@@ -1258,3 +1257,15 @@\n-void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion* r) {\n-  if (r->is_trash()) {\n-    r->recycle();\n+class ShenandoahRecycleTrashedRegionTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator _regions;\n+public:\n+  ShenandoahRecycleTrashedRegionTask() :\n+    WorkerTask(\"Shenandoah Recycle Trashed Regions\") {}\n+\n+  void work(uint worker_id) {\n+    ShenandoahHeapRegion* region = nullptr;\n+    while ((region = _regions.next()) != nullptr) {\n+      if (!region->is_trash()) {\n+        continue;\n+      }\n+      region->try_recycle();\n+    }\n@@ -1262,1 +1273,1 @@\n-}\n+};\n@@ -1267,7 +1278,0 @@\n-  size_t count = 0;\n-  for (size_t i = 0; i < _heap->num_regions(); i++) {\n-    ShenandoahHeapRegion* r = _heap->get_region(i);\n-    if (r->is_trash()) {\n-      _trash_regions[count++] = r;\n-    }\n-  }\n@@ -1275,40 +1279,5 @@\n-  size_t total_batches = 0;\n-  jlong batch_start_time = 0;\n-  jlong recycle_trash_start_time = os::javaTimeNanos();    \/\/ This value will be treated as the initial batch_start_time\n-  jlong batch_end_time = recycle_trash_start_time;\n-  \/\/ Process as many batches as can be processed within 10 us.\n-  static constexpr jlong deadline_ns = 10000;               \/\/ 10 us\n-  size_t idx = 0;\n-  jlong predicted_next_batch_end_time;\n-  jlong batch_process_time_estimate = 0;\n-  while (idx < count) {\n-    if (idx > 0) {\n-      os::naked_yield(); \/\/ Yield to allow allocators to take the lock, except on the first iteration\n-    }\n-    \/\/ Avoid another call to javaTimeNanos() if we already know time at which last batch ended\n-    batch_start_time = batch_end_time;\n-    const jlong deadline = batch_start_time + deadline_ns;\n-\n-    ShenandoahHeapLocker locker(_heap->lock());\n-    do {\n-      \/\/ Measurements on typical 2024 hardware suggest it typically requires between 1400 and 2000 ns to process a batch of\n-      \/\/ 32 regions, assuming low contention with other threads.  Sometimes this goes higher, when mutator threads\n-      \/\/ are contending for CPU cores and\/or the heap lock.  On this hardware with a 10 us deadline, we expect 3-6 batches\n-      \/\/ to be processed between yields most of the time.\n-      \/\/\n-      \/\/ Note that deadline is enforced since the end of previous batch.  In the case that yield() or acquisition of heap lock\n-      \/\/ takes a \"long time\", we will have less time to process regions, but we will always process at least one batch between\n-      \/\/ yields.  Yielding more frequently when there is heavy contention for the heap lock or for CPU cores is considered the\n-      \/\/ right thing to do.\n-      const size_t REGIONS_PER_BATCH = 32;\n-      size_t max_idx = MIN2(count, idx + REGIONS_PER_BATCH);\n-      while (idx < max_idx) {\n-        try_recycle_trashed(_trash_regions[idx++]);\n-      }\n-      total_batches++;\n-      batch_end_time = os::javaTimeNanos();\n-      \/\/ Estimate includes historic combination of yield times and heap lock acquisition times.\n-      batch_process_time_estimate = (batch_end_time - recycle_trash_start_time) \/ total_batches;\n-      predicted_next_batch_end_time = batch_end_time + batch_process_time_estimate;\n-    } while ((idx < count) && (predicted_next_batch_end_time < deadline));\n-  }\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+\n+  ShenandoahRecycleTrashedRegionTask task;\n+  heap->workers()->run_task(&task);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":23,"deletions":54,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -289,1 +289,0 @@\n-  ShenandoahHeapRegion** _trash_regions;\n@@ -355,1 +354,0 @@\n-  void try_recycle_trashed(ShenandoahHeapRegion *r);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -544,1 +544,1 @@\n-      r->recycle();\n+      r->recycle_under_lock();\n@@ -993,1 +993,1 @@\n-      r->recycle();\n+      r->recycle_under_lock();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -885,2 +885,1 @@\n-  _affiliated_region_count++;\n-  return _affiliated_region_count;\n+  return Atomic::add(&_affiliated_region_count, (size_t) 1);\n@@ -890,1 +889,5 @@\n-  shenandoah_assert_heaplocked_or_safepoint();\n+  \/\/ Assertions only hold true for Java threads since they call this method under heap lock.\n+  bool const is_java_thread = Thread::current()->is_Java_thread();\n+  if (is_java_thread) {\n+    shenandoah_assert_heaplocked_or_safepoint();\n+  }\n@@ -894,5 +897,7 @@\n-  _affiliated_region_count--;\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n-         \"used + humongous cannot exceed regions\");\n-  return _affiliated_region_count;\n+  auto affiliated_region_count = Atomic::sub(&_affiliated_region_count, (size_t) 1);\n+  if (is_java_thread) {\n+    assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+           (used() + _humongous_waste <= affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+           \"used + humongous cannot exceed regions\");\n+  }\n+  return affiliated_region_count;\n@@ -903,2 +908,1 @@\n-  _affiliated_region_count += delta;\n-  return _affiliated_region_count;\n+  return Atomic::add(&_affiliated_region_count, delta);\n@@ -909,1 +913,1 @@\n-  assert(_affiliated_region_count >= delta, \"Affiliated region count cannot be negative\");\n+  assert(Atomic::load(&_affiliated_region_count) >= delta, \"Affiliated region count cannot be negative\");\n@@ -911,1 +915,1 @@\n-  _affiliated_region_count -= delta;\n+  auto const affiliated_region_count = Atomic::sub(&_affiliated_region_count, delta);\n@@ -913,1 +917,1 @@\n-         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         (_used + _humongous_waste <= affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n@@ -915,1 +919,1 @@\n-  return _affiliated_region_count;\n+  return affiliated_region_count;\n@@ -920,2 +924,2 @@\n-  _affiliated_region_count = num_regions;\n-  _used = num_bytes;\n+  Atomic::store(&_affiliated_region_count, num_regions);\n+  Atomic::store(&_used, num_bytes);\n@@ -950,1 +954,1 @@\n-  return _affiliated_region_count;\n+  return Atomic::load(&_affiliated_region_count);\n@@ -955,1 +959,2 @@\n-  if (_affiliated_region_count > result) {\n+  auto const used_regions = this->used_regions();\n+  if (used_regions > result) {\n@@ -958,1 +963,1 @@\n-    result -= _affiliated_region_count;\n+    result -= used_regions;\n@@ -964,1 +969,1 @@\n-  return _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes();\n+  return used_regions() * ShenandoahHeapRegion::region_size_bytes();\n@@ -998,1 +1003,1 @@\n-         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         (used_regions_size() >= used()),\n@@ -1022,1 +1027,1 @@\n-         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         (used_regions_size() >= used()),\n@@ -1027,1 +1032,1 @@\n-         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() <= _max_capacity),\n+         (used_regions_size() <= _max_capacity),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":28,"deletions":23,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  size_t _affiliated_region_count;\n+  volatile size_t _affiliated_region_count;\n@@ -134,1 +134,1 @@\n-  size_t used() const override { return _used; }\n+  size_t used() const override { return Atomic::load(&_used); }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -601,1 +601,1 @@\n-  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r) const;\n@@ -604,1 +604,1 @@\n-  inline ShenandoahAffiliation region_affiliation(size_t index);\n+  inline ShenandoahAffiliation region_affiliation(size_t index) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -374,1 +374,1 @@\n-  switch (_affiliations[index]) {\n+  switch (region_affiliation(index)) {\n@@ -385,1 +385,1 @@\n-    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, _affiliations[index], index);\n+    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, region_affiliation(index), index);\n@@ -391,1 +391,1 @@\n-  return is_in_reserved(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::YOUNG_GENERATION);\n+  return is_in_reserved(p) && (region_affiliation(heap_region_index_containing(p)) == ShenandoahAffiliation::YOUNG_GENERATION);\n@@ -395,1 +395,1 @@\n-  return is_in_reserved(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::OLD_GENERATION);\n+  return is_in_reserved(p) && (region_affiliation(heap_region_index_containing(p)) == ShenandoahAffiliation::OLD_GENERATION);\n@@ -402,2 +402,2 @@\n-inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) {\n-  return (ShenandoahAffiliation) _affiliations[r->index()];\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) const {\n+  return region_affiliation(r->index());\n@@ -422,1 +422,1 @@\n-  if ((orig_affiliation == ShenandoahAffiliation::FREE) || (new_affiliation == ShenandoahAffiliation::FREE)) {\n+  if (orig_affiliation == ShenandoahAffiliation::FREE) {\n@@ -431,1 +431,1 @@\n-  _affiliations[r->index()] = (uint8_t) new_affiliation;\n+  Atomic::store(_affiliations + r->index(), (uint8_t) new_affiliation);\n@@ -434,2 +434,2 @@\n-inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(size_t index) {\n-  return (ShenandoahAffiliation) _affiliations[index];\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(size_t index) const {\n+  return (ShenandoahAffiliation) Atomic::load(_affiliations + index);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  _recycling.unset();\n@@ -332,1 +333,0 @@\n-  shenandoah_assert_heaplocked();\n@@ -572,4 +572,0 @@\n-void ShenandoahHeapRegion::recycle() {\n-  shenandoah_assert_heaplocked();\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahGeneration* generation = heap->generation_for(affiliation());\n@@ -577,2 +573,2 @@\n-  heap->decrease_used(generation, used());\n-  generation->decrement_affiliated_region_count();\n+void ShenandoahHeapRegion::recycle_internal() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -583,1 +579,0 @@\n-\n@@ -585,1 +580,0 @@\n-\n@@ -587,0 +581,3 @@\n+  if (ZapUnusedHeapArea) {\n+    SpaceMangler::mangle_region(MemRegion(bottom(), end()));\n+  }\n@@ -589,1 +586,0 @@\n-\n@@ -591,2 +587,40 @@\n-  if (ZapUnusedHeapArea) {\n-    SpaceMangler::mangle_region(MemRegion(bottom(), end()));\n+}\n+\n+void ShenandoahHeapRegion::recycle_under_lock() {\n+  shenandoah_assert_heaplocked();\n+  if (is_trash() && _recycling.try_set()) {\n+    if (is_trash()) {\n+      ShenandoahHeap* heap = ShenandoahHeap::heap();\n+      ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+\n+      heap->decrease_used(generation, used());\n+      generation->decrement_affiliated_region_count();\n+\n+      recycle_internal();\n+      _recycling.unset();\n+    }\n+  } else {\n+    \/\/ Ensure recycling is unset before returning to mutator to continue memory allocation.\n+    while (_recycling.is_set()) {\n+      if (os::is_MP()) {\n+        SpinPause();\n+      } else {\n+        os::naked_yield();\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahHeapRegion::try_recycle() {\n+  shenandoah_assert_not_heaplocked();\n+  if (_recycling.try_set()) {\n+    \/\/ Double check region state after win the race to set recycling flag\n+    if (is_trash()) {\n+      ShenandoahHeap* heap = ShenandoahHeap::heap();\n+      ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+      heap->decrease_used(generation, used());\n+      generation->decrement_affiliated_region_count();\n+\n+      recycle_internal();\n+    }\n+    _recycling.unset();\n@@ -798,1 +832,1 @@\n-    evt.set_from(_state);\n+    evt.set_from(state());\n@@ -802,1 +836,1 @@\n-  _state = to;\n+  Atomic::store(&_state, to);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":48,"deletions":14,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -168,0 +168,1 @@\n+  void recycle_internal();\n@@ -192,4 +193,7 @@\n-  bool is_empty_uncommitted()      const { return _state == _empty_uncommitted; }\n-  bool is_empty_committed()        const { return _state == _empty_committed; }\n-  bool is_regular()                const { return _state == _regular; }\n-  bool is_humongous_continuation() const { return _state == _humongous_cont; }\n+  bool is_empty_uncommitted()      const { return state() == _empty_uncommitted; }\n+  bool is_empty_committed()        const { return state() == _empty_committed; }\n+  bool is_regular()                const { return state() == _regular; }\n+  bool is_humongous_continuation() const { return state() == _humongous_cont; }\n+\n+  bool is_empty_state(RegionState state) const { return state == _empty_committed || state == _empty_uncommitted; }\n+  bool is_humongous_start_state(RegionState state) const { return state == _humongous_start || state == _pinned_humongous_start; }\n@@ -198,5 +202,5 @@\n-  bool is_empty()                  const { return is_empty_committed() || is_empty_uncommitted(); }\n-  bool is_active()                 const { return !is_empty() && !is_trash(); }\n-  bool is_trash()                  const { return _state == _trash; }\n-  bool is_humongous_start()        const { return _state == _humongous_start || _state == _pinned_humongous_start; }\n-  bool is_humongous()              const { return is_humongous_start() || is_humongous_continuation(); }\n+  bool is_empty()                  const { return this->is_empty_state(this->state()); }\n+  bool is_active()                 const { auto cur_state = state(); return !is_empty_state(cur_state) && cur_state != _trash; }\n+  bool is_trash()                  const { return state() == _trash; }\n+  bool is_humongous_start()        const { return is_humongous_start_state(state()); }\n+  bool is_humongous()              const { auto cur_state = state(); return is_humongous_start_state(cur_state) || cur_state == _humongous_cont; }\n@@ -204,3 +208,3 @@\n-  bool is_cset()                   const { return _state == _cset   || _state == _pinned_cset; }\n-  bool is_pinned()                 const { return _state == _pinned || _state == _pinned_cset || _state == _pinned_humongous_start; }\n-  bool is_regular_pinned()         const { return _state == _pinned; }\n+  bool is_cset()                   const { auto cur_state = state(); return cur_state == _cset || cur_state == _pinned_cset; }\n+  bool is_pinned()                 const { auto cur_state = state(); return cur_state == _pinned || cur_state == _pinned_cset || cur_state == _pinned_humongous_start; }\n+  bool is_regular_pinned()         const { return state() == _pinned; }\n@@ -213,2 +217,2 @@\n-  bool is_alloc_allowed()          const { return is_empty() || is_regular() || _state == _pinned; }\n-  bool is_stw_move_allowed()       const { return is_regular() || _state == _cset || (ShenandoahHumongousMoves && _state == _humongous_start); }\n+  bool is_alloc_allowed()          const { auto cur_state = state(); return is_empty_state(cur_state) || cur_state == _regular || cur_state == _pinned; }\n+  bool is_stw_move_allowed()       const { auto cur_state = state(); return cur_state == _regular || cur_state == _cset || (ShenandoahHumongousMoves && cur_state == _humongous_start); }\n@@ -216,2 +220,2 @@\n-  RegionState state()              const { return _state; }\n-  int  state_ordinal()             const { return region_state_to_ordinal(_state); }\n+  RegionState state()              const { return Atomic::load(&_state); }\n+  int  state_ordinal()             const { return region_state_to_ordinal(state()); }\n@@ -246,1 +250,1 @@\n-  RegionState _state;\n+  volatile RegionState _state;\n@@ -264,0 +268,2 @@\n+  ShenandoahSharedFlag _recycling;\n+\n@@ -379,1 +385,3 @@\n-  void recycle();\n+  void recycle_under_lock();\n+\n+  void try_recycle();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":26,"deletions":18,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -77,0 +77,4 @@\n+\n+uint ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup() {\n+  return ConcGCThreads;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -67,0 +67,3 @@\n+\n+  \/\/ Calculate workers for concurrent cleanup\n+  static uint calc_workers_for_conc_cleanup();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  nonstatic_field(ShenandoahHeapRegion, _state,                    ShenandoahHeapRegion::RegionState) \\\n+  volatile_nonstatic_field(ShenandoahHeapRegion, _state,           ShenandoahHeapRegion::RegionState) \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/vmStructs_shenandoah.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}