{"files":[{"patch":"@@ -2927,18 +2927,0 @@\n-void Compile::gather_nodes_for_merge_stores(PhaseIterGVN &igvn) {\n-  ResourceMark rm;\n-  Unique_Node_List worklist;\n-  worklist.push(root());\n-  for (uint i = 0; i < worklist.size(); i++) {\n-    Node* n = worklist[i];\n-    int opc = n->Opcode();\n-    if (opc == Op_StoreB || opc == Op_StoreC || opc == Op_StoreI) {\n-      igvn._worklist.push(n);\n-    }\n-    for (uint i = 0; i < n->len(); i++) {\n-      if (n->in(i) != nullptr) {\n-        worklist.push(n->in(i));\n-      }\n-    }\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1228,2 +1228,0 @@\n-  void gather_nodes_for_merge_stores(PhaseIterGVN &igvn);\n-\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2688,278 +2688,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ Change back-to-back Store(, p, x) -> Store(m, p, y) to Store(m, p, x).\n-\/\/ When a store immediately follows a relevant allocation\/initialization,\n-\/\/ try to capture it into the initialization, or hoist it above.\n-Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  Node* p = MemNode::Ideal_common(phase, can_reshape);\n-  if (p)  return (p == NodeSentinel) ? nullptr : p;\n-\n-  Node* mem     = in(MemNode::Memory);\n-  Node* address = in(MemNode::Address);\n-  Node* value   = in(MemNode::ValueIn);\n-  \/\/ Back-to-back stores to same address?  Fold em up.  Generally\n-  \/\/ unsafe if I have intervening uses...  Also disallowed for StoreCM\n-  \/\/ since they must follow each StoreP operation.  Redundant StoreCMs\n-  \/\/ are eliminated just before matching in final_graph_reshape.\n-  {\n-    Node* st = mem;\n-    \/\/ If Store 'st' has more than one use, we cannot fold 'st' away.\n-    \/\/ For example, 'st' might be the final state at a conditional\n-    \/\/ return.  Or, 'st' might be used by some node which is live at\n-    \/\/ the same time 'st' is live, which might be unschedulable.  So,\n-    \/\/ require exactly ONE user until such time as we clone 'mem' for\n-    \/\/ each of 'mem's uses (thus making the exactly-1-user-rule hold\n-    \/\/ true).\n-    while (st->is_Store() && st->outcnt() == 1 && st->Opcode() != Op_StoreCM) {\n-      \/\/ Looking at a dead closed cycle of memory?\n-      assert(st != st->in(MemNode::Memory), \"dead loop in StoreNode::Ideal\");\n-      assert(Opcode() == st->Opcode() ||\n-             st->Opcode() == Op_StoreVector ||\n-             Opcode() == Op_StoreVector ||\n-             st->Opcode() == Op_StoreVectorScatter ||\n-             Opcode() == Op_StoreVectorScatter ||\n-             phase->C->get_alias_index(adr_type()) == Compile::AliasIdxRaw ||\n-             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreI) || \/\/ expanded ClearArrayNode\n-             (Opcode() == Op_StoreI && st->Opcode() == Op_StoreL) || \/\/ initialization by arraycopy\n-             (is_mismatched_access() || st->as_Store()->is_mismatched_access()),\n-             \"no mismatched stores, except on raw memory: %s %s\", NodeClassNames[Opcode()], NodeClassNames[st->Opcode()]);\n-\n-      if (st->in(MemNode::Address)->eqv_uncast(address) &&\n-          st->as_Store()->memory_size() <= this->memory_size()) {\n-        Node* use = st->raw_out(0);\n-        if (phase->is_IterGVN()) {\n-          phase->is_IterGVN()->rehash_node_delayed(use);\n-        }\n-        \/\/ It's OK to do this in the parser, since DU info is always accurate,\n-        \/\/ and the parser always refers to nodes via SafePointNode maps.\n-        use->set_req_X(MemNode::Memory, st->in(MemNode::Memory), phase);\n-        return this;\n-      }\n-      st = st->in(MemNode::Memory);\n-    }\n-  }\n-\n-\n-  \/\/ Capture an unaliased, unconditional, simple store into an initializer.\n-  \/\/ Or, if it is independent of the allocation, hoist it above the allocation.\n-  if (ReduceFieldZeroing && \/*can_reshape &&*\/\n-      mem->is_Proj() && mem->in(0)->is_Initialize()) {\n-    InitializeNode* init = mem->in(0)->as_Initialize();\n-    intptr_t offset = init->can_capture_store(this, phase, can_reshape);\n-    if (offset > 0) {\n-      Node* moved = init->capture_store(this, offset, phase, can_reshape);\n-      \/\/ If the InitializeNode captured me, it made a raw copy of me,\n-      \/\/ and I need to disappear.\n-      if (moved != nullptr) {\n-        \/\/ %%% hack to ensure that Ideal returns a new node:\n-        mem = MergeMemNode::make(mem);\n-        return mem;             \/\/ fold me away\n-      }\n-    }\n-  }\n-\n-  \/\/ Fold reinterpret cast into memory operation:\n-  \/\/    StoreX mem (MoveY2X v) => StoreY mem v\n-  if (value->is_Move()) {\n-    const Type* vt = value->in(1)->bottom_type();\n-    if (has_reinterpret_variant(vt)) {\n-      if (phase->C->post_loop_opts_phase()) {\n-        return convert_to_reinterpret_store(*phase, value->in(1), vt);\n-      } else {\n-        phase->C->record_for_post_loop_opts_igvn(this); \/\/ attempt the transformation once loop opts are over\n-      }\n-    }\n-  }\n-\n-#ifdef VM_LITTLE_ENDIAN\n-  if (MergeStores && UseUnalignedAccesses) {\n-    if (phase->C->post_loop_opts_phase()) {\n-      Node* progress = Ideal_merge_primitive_array_stores(phase);\n-      if (progress != nullptr) { return progress; }\n-    } else {\n-      phase->C->record_for_post_loop_opts_igvn(this);\n-    }\n-  }\n-#endif\n-\n-  return nullptr;                  \/\/ No further progress\n-}\n-\n-\/\/ Link together multiple stores (B\/S\/C\/I) into a longer one.\n-Node* StoreNode::Ideal_merge_primitive_array_stores(PhaseGVN* phase) {\n-  int opc = Opcode();\n-  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n-    return nullptr;\n-  }\n-\n-  \/\/ Only merge stores on arrays.\n-  if (adr_type()->isa_aryptr() == nullptr) {\n-    return nullptr;\n-  }\n-\n-  \/\/ If we can merge with use, then we must process use first.\n-  StoreNode* use = can_merge_primitive_array_store_with_use(phase, true);\n-  if (use != nullptr) {\n-    return nullptr;\n-  }\n-\n-  \/\/ Check if we can merge with at least one def.\n-  StoreNode* def = can_merge_primitive_array_store_with_def(phase, true);\n-  if (def == nullptr) {\n-    return nullptr;\n-  }\n-\n-  \/\/ Now we know we can merge at least two stores.\n-  ResourceMark rm;\n-  Node_List merge_list;\n-  merge_list.push(this);\n-\n-  uint merge_list_max_size = 8 \/ memory_size();\n-  assert(merge_list_max_size >= 2 &&\n-         merge_list_max_size <= 8 &&\n-         is_power_of_2(merge_list_max_size),\n-         \"must be 2, 4 or 8\");\n-\n-  \/\/ Collect list of stores\n-  while (def != nullptr && merge_list.size() <= merge_list_max_size) {\n-    merge_list.push(def);\n-    def = def->can_merge_primitive_array_store_with_def(phase, true);\n-  }\n-\n-  int pow2size = round_down_power_of_2(merge_list.size());\n-  assert(pow2size >= 2, \"must be merging at least 2 stores\");\n-\n-  \/\/ Create \/ find new value:\n-  Node* new_value = nullptr;\n-  int new_memory_size = memory_size() * pow2size;\n-  if (in(MemNode::ValueIn)->Opcode() == Op_ConI) {\n-    \/\/ Collect all constants\n-    jlong con = 0;\n-    jlong bits_per_store = memory_size() * 8;\n-    jlong mask = (((jlong)1) << bits_per_store) - 1;\n-    for (int i = 0; i < pow2size; i++) {\n-      jlong con_i = merge_list.at(i)->in(MemNode::ValueIn)->get_int();\n-      con = con << bits_per_store;\n-      con = con | (mask & con_i);\n-    }\n-    new_value = phase->longcon(con);\n-  } else {\n-    Node* first = merge_list.at(pow2size-1);\n-    new_value = first->in(MemNode::ValueIn);\n-    Node* base_last;\n-    jint shift_last;\n-    bool is_true = is_con_RShift(in(MemNode::ValueIn), base_last, shift_last);\n-    assert(is_true, \"must detect con RShift\");\n-    if (new_value != base_last && new_value->Opcode() == Op_ConvL2I) {\n-      \/\/ look through\n-      new_value = new_value->in(1);\n-    }\n-    if (new_value != base_last) {\n-      \/\/ new_value is not the base\n-      return nullptr;\n-    }\n-  }\n-\n-  if (phase->type(new_value)->isa_long() != nullptr && new_memory_size <= 4) {\n-    new_value = phase->transform(new ConvL2INode(new_value));\n-  }\n-\n-  assert((phase->type(new_value)->isa_int() != nullptr && new_memory_size <= 4) ||\n-         (phase->type(new_value)->isa_long() != nullptr && new_memory_size == 8),\n-         \"new_value is either int or long, and new_memory_size is small enough\");\n-\n-  Node* first = merge_list.at(pow2size-1);\n-  Node* new_ctrl = in(MemNode::Control); \/\/ must take last: after all RangeChecks\n-  Node* new_mem  = first->in(MemNode::Memory);\n-  Node* new_adr  = first->in(MemNode::Address);\n-  const TypePtr* new_adr_type = adr_type();\n-  BasicType bt = T_ILLEGAL;\n-  switch (new_memory_size) {\n-    case 2: bt = T_SHORT; break;\n-    case 4: bt = T_INT;   break;\n-    case 8: bt = T_LONG;  break;\n-  }\n-\n-  StoreNode* new_store = StoreNode::make(*phase, new_ctrl, new_mem, new_adr,\n-                                         new_adr_type, new_value, bt, MemNode::unordered);\n-  \/\/ Marking the store mismatched is sufficient to prevent reordering, since array stores\n-  \/\/ are all on the same slice. Hence, we need no barriers.\n-  new_store->set_mismatched_access();\n-\n-  \/\/ Constants above may now also be be packed -> put candidate on worklist\n-  phase->is_IterGVN()->_worklist.push(new_mem);\n-\n-#ifdef ASSERT\n-  if (TraceMergeStores) {\n-    stringStream ss;\n-    ss.print_cr(\"[TraceMergeStores]: Replace\");\n-    for (int i = pow2size - 1; i >= 0; i--) {\n-      merge_list.at(i)->dump(\"\\n\", false, &ss);\n-    }\n-    ss.print_cr(\"[TraceMergeStores]: with\");\n-    new_store->dump(\"\\n\", false, &ss);\n-    tty->print(\"%s\", ss.as_string());\n-  }\n-#endif\n-\n-  return new_store;\n-}\n-\n-StoreNode* StoreNode::can_merge_primitive_array_store_with_use(PhaseGVN* phase, bool check_def) {\n-  int opc = Opcode();\n-  assert(opc == Op_StoreB || opc == Op_StoreC || opc == Op_StoreI, \"precondition\");\n-\n-  \/\/ Uses should be:\n-  \/\/ 1) the other StoreNode\n-  \/\/ 2) optionally a MergeMem from the uncommon trap\n-  if (outcnt() > 2) {\n-    return nullptr;\n-  }\n-\n-  StoreNode* use_store = nullptr;\n-  MergeMemNode* merge_mem = nullptr;\n-  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n-    Node* use = fast_out(i);\n-    if (use->Opcode() == opc && use_store == nullptr) {\n-      use_store = use->as_Store();\n-    } else if (use->is_MergeMem() && merge_mem == nullptr) {\n-      merge_mem = use->as_MergeMem();\n-    } else {\n-      return nullptr;\n-    }\n-  }\n-  if (use_store == nullptr) {\n-    return nullptr;\n-  }\n-  if (merge_mem != nullptr) {\n-    \/\/ Check that merge_mem only leads to the uncommon trap between\n-    \/\/ the two stores.\n-    if (merge_mem->outcnt() != 1) {\n-      return nullptr;\n-    }\n-    Node* ctrl_s1 = use_store->in(MemNode::Control);\n-    Node* ctrl_s2 = this->in(MemNode::Control);\n-    if (!ctrl_s1->is_IfProj() ||\n-        !ctrl_s1->in(0)->is_RangeCheck() ||\n-        ctrl_s1->in(0)->outcnt() != 2) {\n-      return nullptr;\n-    }\n-    ProjNode* other_proj = ctrl_s1->as_IfProj()->other_if_proj();\n-    Node* trap = other_proj->is_uncommon_trap_proj(Deoptimization::Reason_range_check);\n-    if (trap != merge_mem->unique_out() ||\n-        ctrl_s1->in(0)->in(0) != ctrl_s2) {\n-      return nullptr;\n-    }\n-  }\n-\n-  \/\/ Having checked \"def -> use\", we now check \"use -> def\".\n-  if (check_def) {\n-    StoreNode* use_def = use_store->can_merge_primitive_array_store_with_def(phase, false);\n-    if (use_def == nullptr) {\n-      return nullptr;\n-    }\n-    assert(use_def == this, \"def of use is this\");\n-  }\n-\n-  return use_store;\n-}\n-\n@@ -3082,1 +2804,1 @@\n-  bool is_adjacent_to(const ArrayPointer& other, const jlong data_size) const {\n+  bool is_adjacent_to_and_before(const ArrayPointer& other, const jlong data_size) const {\n@@ -3128,0 +2850,445 @@\n+\/\/ Link together multiple stores (B\/S\/C\/I) into a longer one.\n+\/\/\n+\/\/ Example: _store = StoreB[i+3]\n+\/\/\n+\/\/   RangeCheck[i+0]           RangeCheck[i+0]\n+\/\/   StoreB[i+0]\n+\/\/   RangeCheck[i+1]           RangeCheck[i+1]\n+\/\/   StoreB[i+1]         -->   pass:             fail:\n+\/\/   StoreB[i+2]               StoreI[i+0]       StoreB[i+0]\n+\/\/   StoreB[i+3]\n+\/\/\n+\/\/ The 4 StoreB are merged into a single StoreI node. We have to be careful with RangeCheck[i+1]: before\n+\/\/ the optimization, if this RangeCheck[i+1] fails, then we execute only StoreB[i+0], and then trap. After\n+\/\/ the optimization, the new StoreI[i+0] is on the passing path of RangeCheck[i+1], and StoreB[i+0] on the\n+\/\/ failing path.\n+class MergePrimitiveArrayStores : public StackObj {\n+private:\n+  PhaseGVN* _phase;\n+  StoreNode* _store;\n+\n+public:\n+  MergePrimitiveArrayStores(PhaseGVN* phase, StoreNode* store) : _phase(phase), _store(store) {}\n+\n+  StoreNode* run();\n+\n+private:\n+  class Status {\n+  private:\n+    StoreNode* _found_store;\n+    bool       _found_range_check;\n+\n+  public:\n+    Status(StoreNode* found_store, bool found_range_check) : _found_store(found_store), _found_range_check(found_range_check) {}\n+    static Status make_failure() { return Status(nullptr, false); }\n+    StoreNode* found_store() const { return _found_store; }\n+    bool found_range_check() const { return _found_range_check; }\n+  };\n+\n+  bool is_compatible_store(const StoreNode* other_store) const;\n+  bool is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const; \/\/ TODO static\n+  Status find_adjacent_use_store(const StoreNode* def_store) const;\n+  Status find_adjacent_def_store(const StoreNode* use_store) const;\n+  Status find_use_store(const StoreNode* def_store) const;\n+  Status find_def_store(const StoreNode* use_store) const;\n+  Status find_use_store_unidirectional(const StoreNode* def_store) const;\n+  Status find_def_store_unidirectional(const StoreNode* use_store) const;\n+};\n+\n+StoreNode* MergePrimitiveArrayStores::run() {\n+  \/\/ Check for B\/S\/C\/I\n+  int opc = _store->Opcode();\n+  if (opc != Op_StoreB && opc != Op_StoreC && opc != Op_StoreI) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Only merge stores on arrays.\n+  if (_store->adr_type()->isa_aryptr() == nullptr) {\n+    return nullptr;\n+  }\n+\n+  \/\/ TODO more checks about array and element type?\n+\n+  \/\/ If we can merge with use, then we must process use first.\n+  Status status_use = find_adjacent_use_store(_store);\n+  if (status_use.found_store() != nullptr) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Check if we can merge with at least one def.\n+  Status status_def = find_adjacent_def_store(_store);\n+  if (status_def.found_store() == nullptr) {\n+    return nullptr;\n+  }\n+\n+  tty->print_cr(\"maybe!\");\n+  _store->dump();\n+  return nullptr;\n+}\n+\n+\/\/ Check compatibility between _store and other_store.\n+bool MergePrimitiveArrayStores::is_compatible_store(const StoreNode* other_store) const {\n+  int opc = _store->Opcode();\n+  assert(opc == Op_StoreB || opc == Op_StoreC || opc == Op_StoreI, \"precondition\");\n+  assert(_store->adr_type()->isa_aryptr() != nullptr, \"must be array store\");\n+\n+  if (other_store == nullptr ||\n+      _store->Opcode() != other_store->Opcode() ||\n+      other_store->adr_type()->isa_aryptr() == nullptr) {\n+    return false;\n+  }\n+\n+  \/\/ Check that the size of the stores, and the array elements are all the same.\n+  const TypeAryPtr* ary_t1 = _store->adr_type()->is_aryptr();\n+  const TypeAryPtr* ary_t2 = other_store->adr_type()->is_aryptr();\n+  int size1 = type2aelembytes(ary_t1->elem()->array_element_basic_type());\n+  int size2 = type2aelembytes(ary_t2->elem()->array_element_basic_type());\n+  if (size1 != size2 ||\n+      size1 != _store->memory_size() ||\n+      _store->memory_size() != other_store->memory_size()) {\n+    return false;\n+  }\n+  return true;\n+}\n+\n+bool MergePrimitiveArrayStores::is_adjacent_pair(const StoreNode* use_store, const StoreNode* def_store) const {\n+  \/\/ TODO value adjacent\n+  {\n+    ResourceMark rm;\n+    ArrayPointer array_pointer_use = ArrayPointer::make(_phase, use_store->in(MemNode::Address));\n+    ArrayPointer array_pointer_def = ArrayPointer::make(_phase, def_store->in(MemNode::Address));\n+    if (!array_pointer_def.is_adjacent_to_and_before(array_pointer_use, use_store->memory_size())) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_use_store(const StoreNode* def_store) const {\n+  Status status_use = find_use_store(def_store);\n+  StoreNode* use_store = status_use.found_store();\n+  if (use_store != nullptr && !is_adjacent_pair(use_store, def_store)) {\n+    return Status::make_failure();\n+  }\n+  return status_use;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_adjacent_def_store(const StoreNode* use_store) const {\n+  Status status_def = find_def_store(use_store);\n+  StoreNode* def_store = status_def.found_store();\n+  if (def_store != nullptr && !is_adjacent_pair(use_store, def_store)) {\n+    return Status::make_failure();\n+  }\n+  return status_def;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store(const StoreNode* def_store) const {\n+  Status status_use = find_use_store_unidirectional(def_store);\n+\n+#ifdef ASSERT\n+  StoreNode* use_store = status_use.found_store();\n+  if (use_store != nullptr) {\n+    Status status_def = find_def_store_unidirectional(use_store);\n+    assert(status_def.found_store() == def_store &&\n+           status_def.found_range_check() == status_use.found_range_check(),\n+           \"find_use_store and find_def_store must be symmetric\");\n+  }\n+#endif\n+\n+  return status_use;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store(const StoreNode* use_store) const {\n+  Status status_def = find_def_store_unidirectional(use_store);\n+\n+#ifdef ASSERT\n+  StoreNode* def_store = status_def.found_store();\n+  if (def_store != nullptr) {\n+    Status status_use = find_use_store_unidirectional(def_store);\n+    assert(status_use.found_store() == use_store &&\n+           status_use.found_range_check() == status_def.found_range_check(),\n+           \"find_use_store and find_def_store must be symmetric\");\n+  }\n+#endif\n+\n+  return status_def;\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_use_store_unidirectional(const StoreNode* def_store) const {\n+  assert(is_compatible_store(def_store), \"precondition: must be compatible with _store\");\n+\n+  \/\/ Uses should be:\n+  \/\/ 1) the other StoreNode\n+  \/\/ 2) optionally a MergeMem from the uncommon trap\n+  if (def_store->outcnt() > 2) {\n+    return Status::make_failure();\n+  }\n+\n+  StoreNode* use_store = nullptr;\n+  MergeMemNode* merge_mem = nullptr;\n+  for (DUIterator_Fast imax, i = def_store->fast_outs(imax); i < imax; i++) {\n+    Node* use = def_store->fast_out(i);\n+    if (use_store == nullptr && is_compatible_store(use->isa_Store())) {\n+      use_store = use->as_Store();\n+    } else if (use->is_MergeMem() && merge_mem == nullptr) {\n+      merge_mem = use->as_MergeMem();\n+    } else {\n+      return Status::make_failure();\n+    }\n+  }\n+  if (use_store == nullptr) {\n+    return Status::make_failure();\n+  }\n+  if (merge_mem != nullptr) {\n+    \/\/ Check that merge_mem only leads to the uncommon trap between\n+    \/\/ the two stores.\n+    if (merge_mem->outcnt() != 1) {\n+      return Status::make_failure();\n+    }\n+    Node* ctrl_s1 = use_store->in(MemNode::Control);\n+    Node* ctrl_s2 = def_store->in(MemNode::Control);\n+    if (!ctrl_s1->is_IfProj() ||\n+        !ctrl_s1->in(0)->is_RangeCheck() ||\n+        ctrl_s1->in(0)->outcnt() != 2) {\n+      return Status::make_failure();\n+    }\n+    ProjNode* other_proj = ctrl_s1->as_IfProj()->other_if_proj();\n+    Node* trap = other_proj->is_uncommon_trap_proj(Deoptimization::Reason_range_check);\n+    if (trap != merge_mem->unique_out() ||\n+        ctrl_s1->in(0)->in(0) != ctrl_s2) {\n+      return Status::make_failure();\n+    }\n+  }\n+\n+  bool found_range_check = merge_mem != nullptr;\n+  return Status(use_store, found_range_check);\n+}\n+\n+MergePrimitiveArrayStores::Status MergePrimitiveArrayStores::find_def_store_unidirectional(const StoreNode* use_store) const {\n+  assert(is_compatible_store(use_store), \"precondition: must be compatible with _store\");\n+\n+  StoreNode* def_store = use_store->in(MemNode::Memory)->isa_Store();\n+  if (!is_compatible_store(def_store)) {\n+    return Status::make_failure();\n+  }\n+\n+  \/\/ TODO consider unifying it, maybe it is even necessary for correctness.\n+  \/\/ Check ctrl compatibility\n+  Node* ctrl_use = use_store->in(MemNode::Control);\n+  Node* ctrl_def = def_store->in(MemNode::Control);\n+  if (ctrl_use != ctrl_def) {\n+    \/\/ See if we can bypass a RangeCheck\n+    if (!ctrl_use->is_IfProj() ||\n+        !ctrl_use->in(0)->is_RangeCheck() ||\n+        ctrl_use->in(0)->outcnt() != 2) {\n+      return Status::make_failure();\n+    }\n+    ProjNode* other_proj = ctrl_use->as_IfProj()->other_if_proj();\n+    if (other_proj->is_uncommon_trap_proj(Deoptimization::Reason_range_check) == nullptr ||\n+        ctrl_use->in(0)->in(0) != ctrl_def) {\n+      return Status::make_failure();\n+    }\n+    \/\/ Success, we skipped a RangeCheck.\n+    return Status(def_store, true);\n+  }\n+\n+  \/\/ Success, no RangeCheck is in between.\n+  return Status(def_store, false);\n+}\n+\n+\/\/------------------------------Ideal------------------------------------------\n+\/\/ Change back-to-back Store(, p, x) -> Store(m, p, y) to Store(m, p, x).\n+\/\/ When a store immediately follows a relevant allocation\/initialization,\n+\/\/ try to capture it into the initialization, or hoist it above.\n+Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  Node* p = MemNode::Ideal_common(phase, can_reshape);\n+  if (p)  return (p == NodeSentinel) ? nullptr : p;\n+\n+  Node* mem     = in(MemNode::Memory);\n+  Node* address = in(MemNode::Address);\n+  Node* value   = in(MemNode::ValueIn);\n+  \/\/ Back-to-back stores to same address?  Fold em up.  Generally\n+  \/\/ unsafe if I have intervening uses...  Also disallowed for StoreCM\n+  \/\/ since they must follow each StoreP operation.  Redundant StoreCMs\n+  \/\/ are eliminated just before matching in final_graph_reshape.\n+  {\n+    Node* st = mem;\n+    \/\/ If Store 'st' has more than one use, we cannot fold 'st' away.\n+    \/\/ For example, 'st' might be the final state at a conditional\n+    \/\/ return.  Or, 'st' might be used by some node which is live at\n+    \/\/ the same time 'st' is live, which might be unschedulable.  So,\n+    \/\/ require exactly ONE user until such time as we clone 'mem' for\n+    \/\/ each of 'mem's uses (thus making the exactly-1-user-rule hold\n+    \/\/ true).\n+    while (st->is_Store() && st->outcnt() == 1 && st->Opcode() != Op_StoreCM) {\n+      \/\/ Looking at a dead closed cycle of memory?\n+      assert(st != st->in(MemNode::Memory), \"dead loop in StoreNode::Ideal\");\n+      assert(Opcode() == st->Opcode() ||\n+             st->Opcode() == Op_StoreVector ||\n+             Opcode() == Op_StoreVector ||\n+             st->Opcode() == Op_StoreVectorScatter ||\n+             Opcode() == Op_StoreVectorScatter ||\n+             phase->C->get_alias_index(adr_type()) == Compile::AliasIdxRaw ||\n+             (Opcode() == Op_StoreL && st->Opcode() == Op_StoreI) || \/\/ expanded ClearArrayNode\n+             (Opcode() == Op_StoreI && st->Opcode() == Op_StoreL) || \/\/ initialization by arraycopy\n+             (is_mismatched_access() || st->as_Store()->is_mismatched_access()),\n+             \"no mismatched stores, except on raw memory: %s %s\", NodeClassNames[Opcode()], NodeClassNames[st->Opcode()]);\n+\n+      if (st->in(MemNode::Address)->eqv_uncast(address) &&\n+          st->as_Store()->memory_size() <= this->memory_size()) {\n+        Node* use = st->raw_out(0);\n+        if (phase->is_IterGVN()) {\n+          phase->is_IterGVN()->rehash_node_delayed(use);\n+        }\n+        \/\/ It's OK to do this in the parser, since DU info is always accurate,\n+        \/\/ and the parser always refers to nodes via SafePointNode maps.\n+        use->set_req_X(MemNode::Memory, st->in(MemNode::Memory), phase);\n+        return this;\n+      }\n+      st = st->in(MemNode::Memory);\n+    }\n+  }\n+\n+\n+  \/\/ Capture an unaliased, unconditional, simple store into an initializer.\n+  \/\/ Or, if it is independent of the allocation, hoist it above the allocation.\n+  if (ReduceFieldZeroing && \/*can_reshape &&*\/\n+      mem->is_Proj() && mem->in(0)->is_Initialize()) {\n+    InitializeNode* init = mem->in(0)->as_Initialize();\n+    intptr_t offset = init->can_capture_store(this, phase, can_reshape);\n+    if (offset > 0) {\n+      Node* moved = init->capture_store(this, offset, phase, can_reshape);\n+      \/\/ If the InitializeNode captured me, it made a raw copy of me,\n+      \/\/ and I need to disappear.\n+      if (moved != nullptr) {\n+        \/\/ %%% hack to ensure that Ideal returns a new node:\n+        mem = MergeMemNode::make(mem);\n+        return mem;             \/\/ fold me away\n+      }\n+    }\n+  }\n+\n+  \/\/ Fold reinterpret cast into memory operation:\n+  \/\/    StoreX mem (MoveY2X v) => StoreY mem v\n+  if (value->is_Move()) {\n+    const Type* vt = value->in(1)->bottom_type();\n+    if (has_reinterpret_variant(vt)) {\n+      if (phase->C->post_loop_opts_phase()) {\n+        return convert_to_reinterpret_store(*phase, value->in(1), vt);\n+      } else {\n+        phase->C->record_for_post_loop_opts_igvn(this); \/\/ attempt the transformation once loop opts are over\n+      }\n+    }\n+  }\n+\n+#ifdef VM_LITTLE_ENDIAN\n+  if (MergeStores && UseUnalignedAccesses) {\n+    if (phase->C->post_loop_opts_phase()) {\n+      MergePrimitiveArrayStores merge(phase, this);\n+      Node* progress = merge.run();\n+      if (progress != nullptr) { return progress; }\n+    } else {\n+      phase->C->record_for_post_loop_opts_igvn(this);\n+    }\n+  }\n+#endif\n+\n+  return nullptr;                  \/\/ No further progress\n+}\n+\n+\/\/  \/\/ Now we know we can merge at least two stores.\n+\/\/  ResourceMark rm;\n+\/\/  Node_List merge_list;\n+\/\/  merge_list.push(this);\n+\/\/\n+\/\/  uint merge_list_max_size = 8 \/ memory_size();\n+\/\/  assert(merge_list_max_size >= 2 &&\n+\/\/         merge_list_max_size <= 8 &&\n+\/\/         is_power_of_2(merge_list_max_size),\n+\/\/         \"must be 2, 4 or 8\");\n+\/\/\n+\/\/  \/\/ Collect list of stores\n+\/\/  while (def != nullptr && merge_list.size() <= merge_list_max_size) {\n+\/\/    merge_list.push(def);\n+\/\/    def = def->can_merge_primitive_array_store_with_def(phase, true);\n+\/\/  }\n+\/\/\n+\/\/  int pow2size = round_down_power_of_2(merge_list.size());\n+\/\/  assert(pow2size >= 2, \"must be merging at least 2 stores\");\n+\/\/\n+\/\/  \/\/ Create \/ find new value:\n+\/\/  Node* new_value = nullptr;\n+\/\/  int new_memory_size = memory_size() * pow2size;\n+\/\/  if (in(MemNode::ValueIn)->Opcode() == Op_ConI) {\n+\/\/    \/\/ Collect all constants\n+\/\/    jlong con = 0;\n+\/\/    jlong bits_per_store = memory_size() * 8;\n+\/\/    jlong mask = (((jlong)1) << bits_per_store) - 1;\n+\/\/    for (int i = 0; i < pow2size; i++) {\n+\/\/      jlong con_i = merge_list.at(i)->in(MemNode::ValueIn)->get_int();\n+\/\/      con = con << bits_per_store;\n+\/\/      con = con | (mask & con_i);\n+\/\/    }\n+\/\/    new_value = phase->longcon(con);\n+\/\/  } else {\n+\/\/    Node* first = merge_list.at(pow2size-1);\n+\/\/    new_value = first->in(MemNode::ValueIn);\n+\/\/    Node* base_last;\n+\/\/    jint shift_last;\n+\/\/    bool is_true = is_con_RShift(in(MemNode::ValueIn), base_last, shift_last);\n+\/\/    assert(is_true, \"must detect con RShift\");\n+\/\/    if (new_value != base_last && new_value->Opcode() == Op_ConvL2I) {\n+\/\/      \/\/ look through\n+\/\/      new_value = new_value->in(1);\n+\/\/    }\n+\/\/    if (new_value != base_last) {\n+\/\/      \/\/ new_value is not the base\n+\/\/      return nullptr;\n+\/\/    }\n+\/\/  }\n+\/\/\n+\/\/  if (phase->type(new_value)->isa_long() != nullptr && new_memory_size <= 4) {\n+\/\/    new_value = phase->transform(new ConvL2INode(new_value));\n+\/\/  }\n+\/\/\n+\/\/  assert((phase->type(new_value)->isa_int() != nullptr && new_memory_size <= 4) ||\n+\/\/         (phase->type(new_value)->isa_long() != nullptr && new_memory_size == 8),\n+\/\/         \"new_value is either int or long, and new_memory_size is small enough\");\n+\/\/\n+\/\/  Node* first = merge_list.at(pow2size-1);\n+\/\/  Node* new_ctrl = in(MemNode::Control); \/\/ must take last: after all RangeChecks\n+\/\/  Node* new_mem  = first->in(MemNode::Memory);\n+\/\/  Node* new_adr  = first->in(MemNode::Address);\n+\/\/  const TypePtr* new_adr_type = adr_type();\n+\/\/  BasicType bt = T_ILLEGAL;\n+\/\/  switch (new_memory_size) {\n+\/\/    case 2: bt = T_SHORT; break;\n+\/\/    case 4: bt = T_INT;   break;\n+\/\/    case 8: bt = T_LONG;  break;\n+\/\/  }\n+\/\/\n+\/\/  StoreNode* new_store = StoreNode::make(*phase, new_ctrl, new_mem, new_adr,\n+\/\/                                         new_adr_type, new_value, bt, MemNode::unordered);\n+\/\/  \/\/ Marking the store mismatched is sufficient to prevent reordering, since array stores\n+\/\/  \/\/ are all on the same slice. Hence, we need no barriers.\n+\/\/  new_store->set_mismatched_access();\n+\/\/\n+\/\/  \/\/ Constants above may now also be be packed -> put candidate on worklist\n+\/\/  phase->is_IterGVN()->_worklist.push(new_mem);\n+\/\/\n+\/\/#ifdef ASSERT\n+\/\/  if (TraceMergeStores) {\n+\/\/    stringStream ss;\n+\/\/    ss.print_cr(\"[TraceMergeStores]: Replace\");\n+\/\/    for (int i = pow2size - 1; i >= 0; i--) {\n+\/\/      merge_list.at(i)->dump(\"\\n\", false, &ss);\n+\/\/    }\n+\/\/    ss.print_cr(\"[TraceMergeStores]: with\");\n+\/\/    new_store->dump(\"\\n\", false, &ss);\n+\/\/    tty->print(\"%s\", ss.as_string());\n+\/\/  }\n+\/\/#endif\n+\/\/\n+\/\/  return new_store;\n+\/\/}\n+\n@@ -3211,9 +3378,1 @@\n-    if (!array_pointer2.is_adjacent_to(array_pointer1, s1->memory_size())) {\n-      return nullptr;\n-    }\n-  }\n-\n-  \/\/ Having checked \"use -> def\", we now check \"def -> use\".\n-  if (check_use) {\n-    StoreNode* def_use = s2->can_merge_primitive_array_store_with_use(phase, false);\n-    if (def_use == nullptr) {\n+    if (!array_pointer2.is_adjacent_to_and_before(array_pointer1, s1->memory_size())) {\n@@ -3222,1 +3381,0 @@\n-    assert(def_use == this, \"use of def is this\");\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":447,"deletions":289,"binary":false,"changes":736,"status":"modified"},{"patch":"@@ -577,2 +577,0 @@\n-  Node* Ideal_merge_primitive_array_stores(PhaseGVN* phase);\n-  StoreNode* can_merge_primitive_array_store_with_use(PhaseGVN* phase, bool check_def);\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"}]}