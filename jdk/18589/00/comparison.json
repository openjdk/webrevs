{"files":[{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -217,1 +217,0 @@\n-      genMarkSweep.cpp \\\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1,544 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"classfile\/javaClasses.hpp\"\n-#include \"classfile\/stringTable.hpp\"\n-#include \"classfile\/symbolTable.hpp\"\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"classfile\/vmSymbols.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/serial\/cardTableRS.hpp\"\n-#include \"gc\/serial\/defNewGeneration.hpp\"\n-#include \"gc\/serial\/generation.hpp\"\n-#include \"gc\/serial\/genMarkSweep.hpp\"\n-#include \"gc\/serial\/markSweep.inline.hpp\"\n-#include \"gc\/serial\/serialGcRefProcProxyTask.hpp\"\n-#include \"gc\/serial\/serialHeap.hpp\"\n-#include \"gc\/shared\/classUnloadingContext.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"gc\/shared\/gcHeapSummary.hpp\"\n-#include \"gc\/shared\/gcTimer.hpp\"\n-#include \"gc\/shared\/gcTrace.hpp\"\n-#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n-#include \"gc\/shared\/modRefBarrierSet.hpp\"\n-#include \"gc\/shared\/preservedMarks.inline.hpp\"\n-#include \"gc\/shared\/referencePolicy.hpp\"\n-#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n-#include \"gc\/shared\/space.inline.hpp\"\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n-#include \"gc\/shared\/weakProcessor.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/instanceRefKlass.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"prims\/jvmtiExport.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/prefetch.inline.hpp\"\n-#include \"runtime\/synchronizer.hpp\"\n-#include \"runtime\/vmThread.hpp\"\n-#include \"utilities\/copy.hpp\"\n-#include \"utilities\/events.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n-#if INCLUDE_JVMCI\n-#include \"jvmci\/jvmci.hpp\"\n-#endif\n-\n-class DeadSpacer : StackObj {\n-  size_t _allowed_deadspace_words;\n-  bool _active;\n-  ContiguousSpace* _space;\n-\n-public:\n-  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n-    size_t ratio = _space->allowed_dead_ratio();\n-    _active = ratio > 0;\n-\n-    if (_active) {\n-      \/\/ We allow some amount of garbage towards the bottom of the space, so\n-      \/\/ we don't start compacting before there is a significant gain to be made.\n-      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n-      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n-      if ((MarkSweep::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n-        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n-      } else {\n-        _active = false;\n-      }\n-    }\n-  }\n-\n-  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n-    if (!_active) {\n-      return false;\n-    }\n-\n-    size_t dead_length = pointer_delta(dead_end, dead_start);\n-    if (_allowed_deadspace_words >= dead_length) {\n-      _allowed_deadspace_words -= dead_length;\n-      CollectedHeap::fill_with_object(dead_start, dead_length);\n-      oop obj = cast_to_oop(dead_start);\n-      \/\/ obj->set_mark(obj->mark().set_marked());\n-\n-      assert(dead_length == obj->size(), \"bad filler object size\");\n-      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n-                                        p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n-\n-      return true;\n-    } else {\n-      _active = false;\n-      return false;\n-    }\n-  }\n-};\n-\n-\/\/ Implement the \"compaction\" part of the mark-compact GC algorithm.\n-class Compacter {\n-  \/\/ There are four spaces in total, but only the first three can be used after\n-  \/\/ compact. IOW, old and eden\/from must be enough for all live objs\n-  static constexpr uint max_num_spaces = 4;\n-\n-  struct CompactionSpace {\n-    ContiguousSpace* _space;\n-    \/\/ Will be the new top after compaction is complete.\n-    HeapWord* _compaction_top;\n-    \/\/ The first dead word in this contiguous space. It's an optimization to\n-    \/\/ skip large chunk of live objects at the beginning.\n-    HeapWord* _first_dead;\n-\n-    void init(ContiguousSpace* space) {\n-      _space = space;\n-      _compaction_top = space->bottom();\n-      _first_dead = nullptr;\n-    }\n-  };\n-\n-  CompactionSpace _spaces[max_num_spaces];\n-  \/\/ The num of spaces to be compacted, i.e. containing live objs.\n-  uint _num_spaces;\n-\n-  uint _index;\n-\n-  HeapWord* get_compaction_top(uint index) const {\n-    return _spaces[index]._compaction_top;\n-  }\n-\n-  HeapWord* get_first_dead(uint index) const {\n-    return _spaces[index]._first_dead;\n-  }\n-\n-  ContiguousSpace* get_space(uint index) const {\n-    return _spaces[index]._space;\n-  }\n-\n-  void record_first_dead(uint index, HeapWord* first_dead) {\n-    assert(_spaces[index]._first_dead == nullptr, \"should write only once\");\n-    _spaces[index]._first_dead = first_dead;\n-  }\n-\n-  HeapWord* alloc(size_t words) {\n-    while (true) {\n-      if (words <= pointer_delta(_spaces[_index]._space->end(),\n-                                 _spaces[_index]._compaction_top)) {\n-        HeapWord* result = _spaces[_index]._compaction_top;\n-        _spaces[_index]._compaction_top += words;\n-        if (_index == 0) {\n-          \/\/ old-gen requires BOT update\n-          static_cast<TenuredSpace*>(_spaces[0]._space)->update_for_block(result, result + words);\n-        }\n-        return result;\n-      }\n-\n-      \/\/ out-of-memory in this space\n-      _index++;\n-      assert(_index < max_num_spaces - 1, \"the last space should not be used\");\n-    }\n-  }\n-\n-  static void prefetch_read_scan(void* p) {\n-    if (PrefetchScanIntervalInBytes >= 0) {\n-      Prefetch::read(p, PrefetchScanIntervalInBytes);\n-    }\n-  }\n-\n-  static void prefetch_write_scan(void* p) {\n-    if (PrefetchScanIntervalInBytes >= 0) {\n-      Prefetch::write(p, PrefetchScanIntervalInBytes);\n-    }\n-  }\n-\n-  static void prefetch_write_copy(void* p) {\n-    if (PrefetchCopyIntervalInBytes >= 0) {\n-      Prefetch::write(p, PrefetchCopyIntervalInBytes);\n-    }\n-  }\n-\n-  static void forward_obj(oop obj, HeapWord* new_addr) {\n-    prefetch_write_scan(obj);\n-    if (cast_from_oop<HeapWord*>(obj) != new_addr) {\n-      obj->forward_to(cast_to_oop(new_addr));\n-    } else {\n-      assert(obj->is_gc_marked(), \"inv\");\n-      \/\/ This obj will stay in-place. Fix the markword.\n-      obj->init_mark();\n-    }\n-  }\n-\n-  static HeapWord* find_next_live_addr(HeapWord* start, HeapWord* end) {\n-    for (HeapWord* i_addr = start; i_addr < end; \/* empty *\/) {\n-      prefetch_read_scan(i_addr);\n-      oop obj = cast_to_oop(i_addr);\n-      if (obj->is_gc_marked()) {\n-        return i_addr;\n-      }\n-      i_addr += obj->size();\n-    }\n-    return end;\n-  };\n-\n-  static size_t relocate(HeapWord* addr) {\n-    \/\/ Prefetch source and destination\n-    prefetch_read_scan(addr);\n-\n-    oop obj = cast_to_oop(addr);\n-    oop new_obj = obj->forwardee();\n-    HeapWord* new_addr = cast_from_oop<HeapWord*>(new_obj);\n-    assert(addr != new_addr, \"inv\");\n-    prefetch_write_copy(new_addr);\n-\n-    size_t obj_size = obj->size();\n-    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n-    new_obj->init_mark();\n-\n-    return obj_size;\n-  }\n-\n-public:\n-  explicit Compacter(SerialHeap* heap) {\n-    \/\/ In this order so that heap is compacted towards old-gen.\n-    _spaces[0].init(heap->old_gen()->space());\n-    _spaces[1].init(heap->young_gen()->eden());\n-    _spaces[2].init(heap->young_gen()->from());\n-\n-    bool is_promotion_failed = (heap->young_gen()->from()->next_compaction_space() != nullptr);\n-    if (is_promotion_failed) {\n-      _spaces[3].init(heap->young_gen()->to());\n-      _num_spaces = 4;\n-    } else {\n-      _num_spaces = 3;\n-    }\n-    _index = 0;\n-  }\n-\n-  void phase2_calculate_new_addr() {\n-    for (uint i = 0; i < _num_spaces; ++i) {\n-      ContiguousSpace* space = get_space(i);\n-      HeapWord* cur_addr = space->bottom();\n-      HeapWord* top = space->top();\n-\n-      bool record_first_dead_done = false;\n-\n-      DeadSpacer dead_spacer(space);\n-\n-      while (cur_addr < top) {\n-        oop obj = cast_to_oop(cur_addr);\n-        size_t obj_size = obj->size();\n-        if (obj->is_gc_marked()) {\n-          HeapWord* new_addr = alloc(obj_size);\n-          forward_obj(obj, new_addr);\n-          cur_addr += obj_size;\n-        } else {\n-          \/\/ Skipping the current known-unmarked obj\n-          HeapWord* next_live_addr = find_next_live_addr(cur_addr + obj_size, top);\n-          if (dead_spacer.insert_deadspace(cur_addr, next_live_addr)) {\n-            \/\/ Register space for the filler obj\n-            alloc(pointer_delta(next_live_addr, cur_addr));\n-          } else {\n-            if (!record_first_dead_done) {\n-              record_first_dead(i, cur_addr);\n-              record_first_dead_done = true;\n-            }\n-            *(HeapWord**)cur_addr = next_live_addr;\n-          }\n-          cur_addr = next_live_addr;\n-        }\n-      }\n-\n-      if (!record_first_dead_done) {\n-        record_first_dead(i, top);\n-      }\n-    }\n-  }\n-\n-  void phase3_adjust_pointers() {\n-    for (uint i = 0; i < _num_spaces; ++i) {\n-      ContiguousSpace* space = get_space(i);\n-      HeapWord* cur_addr = space->bottom();\n-      HeapWord* const top = space->top();\n-      HeapWord* const first_dead = get_first_dead(i);\n-\n-      while (cur_addr < top) {\n-        prefetch_write_scan(cur_addr);\n-        if (cur_addr < first_dead || cast_to_oop(cur_addr)->is_gc_marked()) {\n-          size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_addr));\n-          cur_addr += size;\n-        } else {\n-          assert(*(HeapWord**)cur_addr > cur_addr, \"forward progress\");\n-          cur_addr = *(HeapWord**)cur_addr;\n-        }\n-      }\n-    }\n-  }\n-\n-  void phase4_compact() {\n-    for (uint i = 0; i < _num_spaces; ++i) {\n-      ContiguousSpace* space = get_space(i);\n-      HeapWord* cur_addr = space->bottom();\n-      HeapWord* top = space->top();\n-\n-      \/\/ Check if the first obj inside this space is forwarded.\n-      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n-        \/\/ Jump over consecutive (in-place) live-objs-chunk\n-        cur_addr = get_first_dead(i);\n-      }\n-\n-      while (cur_addr < top) {\n-        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n-          cur_addr = *(HeapWord**) cur_addr;\n-          continue;\n-        }\n-        cur_addr += relocate(cur_addr);\n-      }\n-\n-      \/\/ Reset top and unused memory\n-      space->set_top(get_compaction_top(i));\n-      if (ZapUnusedHeapArea) {\n-        space->mangle_unused_area();\n-      }\n-    }\n-  }\n-};\n-\n-void GenMarkSweep::phase1_mark(bool clear_all_softrefs) {\n-  \/\/ Recursively traverse all live objects and mark them\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 1: Mark live objects\", _gc_timer);\n-\n-  SerialHeap* gch = SerialHeap::heap();\n-\n-  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n-\n-  ref_processor()->start_discovery(clear_all_softrefs);\n-\n-  {\n-    StrongRootsScope srs(0);\n-\n-    CLDClosure* weak_cld_closure = ClassUnloading ? nullptr : &follow_cld_closure;\n-    MarkingCodeBlobClosure mark_code_closure(&follow_root_closure, !CodeBlobToOopClosure::FixRelocations, true);\n-    gch->process_roots(SerialHeap::SO_None,\n-                       &follow_root_closure,\n-                       &follow_cld_closure,\n-                       weak_cld_closure,\n-                       &mark_code_closure);\n-  }\n-\n-  \/\/ Process reference objects found during marking\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Reference Processing\", gc_timer());\n-\n-    ReferenceProcessorPhaseTimes pt(_gc_timer, ref_processor()->max_num_queues());\n-    SerialGCRefProcProxyTask task(is_alive, keep_alive, follow_stack_closure);\n-    const ReferenceProcessorStats& stats = ref_processor()->process_discovered_references(task, pt);\n-    pt.print_all_references();\n-    gc_tracer()->report_gc_reference_stats(stats);\n-  }\n-\n-  \/\/ This is the point where the entire marking should have completed.\n-  assert(_marking_stack.is_empty(), \"Marking should have completed\");\n-\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Weak Processing\", gc_timer());\n-    WeakProcessor::weak_oops_do(&is_alive, &do_nothing_cl);\n-  }\n-\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", gc_timer());\n-\n-    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n-\n-    bool unloading_occurred;\n-    {\n-      CodeCache::UnlinkingScope scope(&is_alive);\n-\n-      \/\/ Unload classes and purge the SystemDictionary.\n-      unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n-\n-      \/\/ Unload nmethods.\n-      CodeCache::do_unloading(unloading_occurred);\n-    }\n-\n-    {\n-      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n-      \/\/ Release unloaded nmethod's memory.\n-      ctx->purge_nmethods();\n-    }\n-    {\n-      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", gc_timer());\n-      gch->prune_unlinked_nmethods();\n-    }\n-    {\n-      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n-      ctx->free_code_blobs();\n-    }\n-\n-    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n-    Klass::clean_weak_klass_links(unloading_occurred);\n-\n-    \/\/ Clean JVMCI metadata handles.\n-    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n-  }\n-\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Report Object Count\", gc_timer());\n-    gc_tracer()->report_object_count_after_gc(&is_alive, nullptr);\n-  }\n-}\n-\n-void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n-\n-  SerialHeap* gch = SerialHeap::heap();\n-#ifdef ASSERT\n-  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n-    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n-  }\n-#endif\n-\n-  gch->trace_heap_before_gc(_gc_tracer);\n-\n-  \/\/ Increment the invocation count\n-  _total_invocations++;\n-\n-  \/\/ Capture used regions for old-gen to reestablish old-to-young invariant\n-  \/\/ after full-gc.\n-  gch->old_gen()->save_used_region();\n-\n-  allocate_stacks();\n-\n-  phase1_mark(clear_all_softrefs);\n-\n-  Compacter compacter{gch};\n-\n-  {\n-    \/\/ Now all live objects are marked, compute the new object addresses.\n-    GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n-\n-    compacter.phase2_calculate_new_addr();\n-  }\n-\n-  \/\/ Don't add any more derived pointers during phase3\n-#if COMPILER2_OR_JVMCI\n-  assert(DerivedPointerTable::is_active(), \"Sanity\");\n-  DerivedPointerTable::set_active(false);\n-#endif\n-\n-  {\n-    \/\/ Adjust the pointers to reflect the new locations\n-    GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n-\n-    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n-\n-    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-    gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                       &adjust_pointer_closure,\n-                       &adjust_cld_closure,\n-                       &adjust_cld_closure,\n-                       &code_closure);\n-\n-    WeakProcessor::oops_do(&adjust_pointer_closure);\n-\n-    adjust_marks();\n-    compacter.phase3_adjust_pointers();\n-  }\n-\n-  {\n-    \/\/ All pointers are now adjusted, move objects accordingly\n-    GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n-\n-    compacter.phase4_compact();\n-  }\n-\n-  restore_marks();\n-\n-  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n-  \/\/ (Should this be in general part?)\n-  gch->save_marks();\n-\n-  deallocate_stacks();\n-\n-  MarkSweep::_string_dedup_requests->flush();\n-\n-  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n-  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n-\n-  gch->prune_scavengable_nmethods();\n-\n-  \/\/ Update heap occupancy information which is used as\n-  \/\/ input to soft ref clearing policy at the next gc.\n-  Universe::heap()->update_capacity_and_used_at_gc();\n-\n-  \/\/ Signal that we have completed a visit to all live objects.\n-  Universe::heap()->record_whole_heap_examined_timestamp();\n-\n-  gch->trace_heap_after_gc(_gc_tracer);\n-}\n-\n-void GenMarkSweep::allocate_stacks() {\n-  void* scratch = nullptr;\n-  size_t num_words;\n-  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-  young_gen->contribute_scratch(scratch, num_words);\n-\n-  if (scratch != nullptr) {\n-    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n-  } else {\n-    _preserved_count_max = 0;\n-  }\n-\n-  _preserved_marks = (PreservedMark*)scratch;\n-  _preserved_count = 0;\n-\n-  _preserved_overflow_stack_set.init(1);\n-}\n-\n-void GenMarkSweep::deallocate_stacks() {\n-  if (_preserved_count_max != 0) {\n-    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-    young_gen->reset_scratch();\n-  }\n-\n-  _preserved_overflow_stack_set.reclaim();\n-  _marking_stack.clear();\n-  _objarray_stack.clear(true);\n-}\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":0,"deletions":544,"binary":false,"changes":544,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SERIAL_GENMARKSWEEP_HPP\n-#define SHARE_GC_SERIAL_GENMARKSWEEP_HPP\n-\n-#include \"gc\/serial\/markSweep.hpp\"\n-\n-class GenMarkSweep : public MarkSweep {\n- public:\n-  static void invoke_at_safepoint(bool clear_all_softrefs);\n-\n- private:\n-  \/\/ Mark live objects\n-  static void phase1_mark(bool clear_all_softrefs);\n-\n-  \/\/ Temporary data structures for traversal and storing\/restoring marks\n-  static void allocate_stacks();\n-  static void deallocate_stacks();\n-};\n-\n-#endif \/\/ SHARE_GC_SERIAL_GENMARKSWEEP_HPP\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.hpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,7 @@\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n@@ -27,0 +34,3 @@\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/serial\/cardTableRS.hpp\"\n+#include \"gc\/serial\/defNewGeneration.hpp\"\n@@ -28,0 +38,3 @@\n+#include \"gc\/serial\/serialGcRefProcProxyTask.hpp\"\n+#include \"gc\/serial\/serialHeap.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -29,0 +42,1 @@\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n@@ -31,0 +45,1 @@\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n@@ -32,0 +47,6 @@\n+#include \"gc\/shared\/modRefBarrierSet.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/space.inline.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/weakProcessor.hpp\"\n@@ -36,0 +57,1 @@\n+#include \"oops\/instanceRefKlass.hpp\"\n@@ -40,0 +62,3 @@\n+#include \"runtime\/prefetch.inline.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/events.hpp\"\n@@ -41,0 +66,3 @@\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n@@ -65,0 +93,274 @@\n+class DeadSpacer : StackObj {\n+  size_t _allowed_deadspace_words;\n+  bool _active;\n+  ContiguousSpace* _space;\n+\n+public:\n+  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n+    size_t ratio = _space->allowed_dead_ratio();\n+    _active = ratio > 0;\n+\n+    if (_active) {\n+      \/\/ We allow some amount of garbage towards the bottom of the space, so\n+      \/\/ we don't start compacting before there is a significant gain to be made.\n+      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n+      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n+      if ((MarkSweep::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n+        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n+      } else {\n+        _active = false;\n+      }\n+    }\n+  }\n+\n+  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n+    if (!_active) {\n+      return false;\n+    }\n+\n+    size_t dead_length = pointer_delta(dead_end, dead_start);\n+    if (_allowed_deadspace_words >= dead_length) {\n+      _allowed_deadspace_words -= dead_length;\n+      CollectedHeap::fill_with_object(dead_start, dead_length);\n+      oop obj = cast_to_oop(dead_start);\n+      \/\/ obj->set_mark(obj->mark().set_marked());\n+\n+      assert(dead_length == obj->size(), \"bad filler object size\");\n+      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n+                                        p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n+\n+      return true;\n+    } else {\n+      _active = false;\n+      return false;\n+    }\n+  }\n+};\n+\n+\/\/ Implement the \"compaction\" part of the mark-compact GC algorithm.\n+class Compacter {\n+  \/\/ There are four spaces in total, but only the first three can be used after\n+  \/\/ compact. IOW, old and eden\/from must be enough for all live objs\n+  static constexpr uint max_num_spaces = 4;\n+\n+  struct CompactionSpace {\n+    ContiguousSpace* _space;\n+    \/\/ Will be the new top after compaction is complete.\n+    HeapWord* _compaction_top;\n+    \/\/ The first dead word in this contiguous space. It's an optimization to\n+    \/\/ skip large chunk of live objects at the beginning.\n+    HeapWord* _first_dead;\n+\n+    void init(ContiguousSpace* space) {\n+      _space = space;\n+      _compaction_top = space->bottom();\n+      _first_dead = nullptr;\n+    }\n+  };\n+\n+  CompactionSpace _spaces[max_num_spaces];\n+  \/\/ The num of spaces to be compacted, i.e. containing live objs.\n+  uint _num_spaces;\n+\n+  uint _index;\n+\n+  HeapWord* get_compaction_top(uint index) const {\n+    return _spaces[index]._compaction_top;\n+  }\n+\n+  HeapWord* get_first_dead(uint index) const {\n+    return _spaces[index]._first_dead;\n+  }\n+\n+  ContiguousSpace* get_space(uint index) const {\n+    return _spaces[index]._space;\n+  }\n+\n+  void record_first_dead(uint index, HeapWord* first_dead) {\n+    assert(_spaces[index]._first_dead == nullptr, \"should write only once\");\n+    _spaces[index]._first_dead = first_dead;\n+  }\n+\n+  HeapWord* alloc(size_t words) {\n+    while (true) {\n+      if (words <= pointer_delta(_spaces[_index]._space->end(),\n+                                 _spaces[_index]._compaction_top)) {\n+        HeapWord* result = _spaces[_index]._compaction_top;\n+        _spaces[_index]._compaction_top += words;\n+        if (_index == 0) {\n+          \/\/ old-gen requires BOT update\n+          static_cast<TenuredSpace*>(_spaces[0]._space)->update_for_block(result, result + words);\n+        }\n+        return result;\n+      }\n+\n+      \/\/ out-of-memory in this space\n+      _index++;\n+      assert(_index < max_num_spaces - 1, \"the last space should not be used\");\n+    }\n+  }\n+\n+  static void prefetch_read_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::read(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n+\n+  static void prefetch_write_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n+\n+  static void prefetch_write_copy(void* p) {\n+    if (PrefetchCopyIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchCopyIntervalInBytes);\n+    }\n+  }\n+\n+  static void forward_obj(oop obj, HeapWord* new_addr) {\n+    prefetch_write_scan(obj);\n+    if (cast_from_oop<HeapWord*>(obj) != new_addr) {\n+      obj->forward_to(cast_to_oop(new_addr));\n+    } else {\n+      assert(obj->is_gc_marked(), \"inv\");\n+      \/\/ This obj will stay in-place. Fix the markword.\n+      obj->init_mark();\n+    }\n+  }\n+\n+  static HeapWord* find_next_live_addr(HeapWord* start, HeapWord* end) {\n+    for (HeapWord* i_addr = start; i_addr < end; \/* empty *\/) {\n+      prefetch_read_scan(i_addr);\n+      oop obj = cast_to_oop(i_addr);\n+      if (obj->is_gc_marked()) {\n+        return i_addr;\n+      }\n+      i_addr += obj->size();\n+    }\n+    return end;\n+  };\n+\n+  static size_t relocate(HeapWord* addr) {\n+    \/\/ Prefetch source and destination\n+    prefetch_read_scan(addr);\n+\n+    oop obj = cast_to_oop(addr);\n+    oop new_obj = obj->forwardee();\n+    HeapWord* new_addr = cast_from_oop<HeapWord*>(new_obj);\n+    assert(addr != new_addr, \"inv\");\n+    prefetch_write_copy(new_addr);\n+\n+    size_t obj_size = obj->size();\n+    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    new_obj->init_mark();\n+\n+    return obj_size;\n+  }\n+\n+public:\n+  explicit Compacter(SerialHeap* heap) {\n+    \/\/ In this order so that heap is compacted towards old-gen.\n+    _spaces[0].init(heap->old_gen()->space());\n+    _spaces[1].init(heap->young_gen()->eden());\n+    _spaces[2].init(heap->young_gen()->from());\n+\n+    bool is_promotion_failed = (heap->young_gen()->from()->next_compaction_space() != nullptr);\n+    if (is_promotion_failed) {\n+      _spaces[3].init(heap->young_gen()->to());\n+      _num_spaces = 4;\n+    } else {\n+      _num_spaces = 3;\n+    }\n+    _index = 0;\n+  }\n+\n+  void phase2_calculate_new_addr() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      bool record_first_dead_done = false;\n+\n+      DeadSpacer dead_spacer(space);\n+\n+      while (cur_addr < top) {\n+        oop obj = cast_to_oop(cur_addr);\n+        size_t obj_size = obj->size();\n+        if (obj->is_gc_marked()) {\n+          HeapWord* new_addr = alloc(obj_size);\n+          forward_obj(obj, new_addr);\n+          cur_addr += obj_size;\n+        } else {\n+          \/\/ Skipping the current known-unmarked obj\n+          HeapWord* next_live_addr = find_next_live_addr(cur_addr + obj_size, top);\n+          if (dead_spacer.insert_deadspace(cur_addr, next_live_addr)) {\n+            \/\/ Register space for the filler obj\n+            alloc(pointer_delta(next_live_addr, cur_addr));\n+          } else {\n+            if (!record_first_dead_done) {\n+              record_first_dead(i, cur_addr);\n+              record_first_dead_done = true;\n+            }\n+            *(HeapWord**)cur_addr = next_live_addr;\n+          }\n+          cur_addr = next_live_addr;\n+        }\n+      }\n+\n+      if (!record_first_dead_done) {\n+        record_first_dead(i, top);\n+      }\n+    }\n+  }\n+\n+  void phase3_adjust_pointers() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* const top = space->top();\n+      HeapWord* const first_dead = get_first_dead(i);\n+\n+      while (cur_addr < top) {\n+        prefetch_write_scan(cur_addr);\n+        if (cur_addr < first_dead || cast_to_oop(cur_addr)->is_gc_marked()) {\n+          size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_addr));\n+          cur_addr += size;\n+        } else {\n+          assert(*(HeapWord**)cur_addr > cur_addr, \"forward progress\");\n+          cur_addr = *(HeapWord**)cur_addr;\n+        }\n+      }\n+    }\n+  }\n+\n+  void phase4_compact() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      \/\/ Check if the first obj inside this space is forwarded.\n+      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+        \/\/ Jump over consecutive (in-place) live-objs-chunk\n+        cur_addr = get_first_dead(i);\n+      }\n+\n+      while (cur_addr < top) {\n+        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+          cur_addr = *(HeapWord**) cur_addr;\n+          continue;\n+        }\n+        cur_addr += relocate(cur_addr);\n+      }\n+\n+      \/\/ Reset top and unused memory\n+      space->set_top(get_compaction_top(i));\n+      if (ZapUnusedHeapArea) {\n+        space->mangle_unused_area();\n+      }\n+    }\n+  }\n+};\n+\n@@ -160,0 +462,113 @@\n+void MarkSweep::phase1_mark(bool clear_all_softrefs) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Phase 1: Mark live objects\", _gc_timer);\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+\n+  ref_processor()->start_discovery(clear_all_softrefs);\n+\n+  {\n+    StrongRootsScope srs(0);\n+\n+    CLDClosure* weak_cld_closure = ClassUnloading ? nullptr : &follow_cld_closure;\n+    MarkingCodeBlobClosure mark_code_closure(&follow_root_closure, !CodeBlobToOopClosure::FixRelocations, true);\n+    gch->process_roots(SerialHeap::SO_None,\n+                       &follow_root_closure,\n+                       &follow_cld_closure,\n+                       weak_cld_closure,\n+                       &mark_code_closure);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Reference Processing\", gc_timer());\n+\n+    ReferenceProcessorPhaseTimes pt(_gc_timer, ref_processor()->max_num_queues());\n+    SerialGCRefProcProxyTask task(is_alive, keep_alive, follow_stack_closure);\n+    const ReferenceProcessorStats& stats = ref_processor()->process_discovered_references(task, pt);\n+    pt.print_all_references();\n+    gc_tracer()->report_gc_reference_stats(stats);\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  assert(_marking_stack.is_empty(), \"Marking should have completed\");\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Weak Processing\", gc_timer());\n+    WeakProcessor::weak_oops_do(&is_alive, &do_nothing_cl);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", gc_timer());\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(&is_alive);\n+\n+      \/\/ Unload classes and purge the SystemDictionary.\n+      unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", gc_timer());\n+      gch->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_code_blobs();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Report Object Count\", gc_timer());\n+    gc_tracer()->report_object_count_after_gc(&is_alive, nullptr);\n+  }\n+}\n+\n+void MarkSweep::allocate_stacks() {\n+  void* scratch = nullptr;\n+  size_t num_words;\n+  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+  young_gen->contribute_scratch(scratch, num_words);\n+\n+  if (scratch != nullptr) {\n+    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n+  } else {\n+    _preserved_count_max = 0;\n+  }\n+\n+  _preserved_marks = (PreservedMark*)scratch;\n+  _preserved_count = 0;\n+\n+  _preserved_overflow_stack_set.init(1);\n+}\n+\n+void MarkSweep::deallocate_stacks() {\n+  if (_preserved_count_max != 0) {\n+    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+    young_gen->reset_scratch();\n+  }\n+\n+  _preserved_overflow_stack_set.reclaim();\n+  _marking_stack.clear();\n+  _objarray_stack.clear(true);\n+}\n+\n@@ -238,0 +653,89 @@\n+\n+void MarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+#ifdef ASSERT\n+  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n+    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+  }\n+#endif\n+\n+  gch->trace_heap_before_gc(_gc_tracer);\n+\n+  \/\/ Increment the invocation count\n+  _total_invocations++;\n+\n+  \/\/ Capture used regions for old-gen to reestablish old-to-young invariant\n+  \/\/ after full-gc.\n+  gch->old_gen()->save_used_region();\n+\n+  allocate_stacks();\n+\n+  phase1_mark(clear_all_softrefs);\n+\n+  Compacter compacter{gch};\n+\n+  {\n+    \/\/ Now all live objects are marked, compute the new object addresses.\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n+\n+    compacter.phase2_calculate_new_addr();\n+  }\n+\n+  \/\/ Don't add any more derived pointers during phase3\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_active(), \"Sanity\");\n+  DerivedPointerTable::set_active(false);\n+#endif\n+\n+  {\n+    \/\/ Adjust the pointers to reflect the new locations\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n+\n+    WeakProcessor::oops_do(&adjust_pointer_closure);\n+\n+    adjust_marks();\n+    compacter.phase3_adjust_pointers();\n+  }\n+\n+  {\n+    \/\/ All pointers are now adjusted, move objects accordingly\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n+\n+    compacter.phase4_compact();\n+  }\n+\n+  restore_marks();\n+\n+  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n+  \/\/ (Should this be in general part?)\n+  gch->save_marks();\n+\n+  deallocate_stacks();\n+\n+  MarkSweep::_string_dedup_requests->flush();\n+\n+  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n+  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+\n+  gch->prune_scavengable_nmethods();\n+\n+  \/\/ Update heap occupancy information which is used as\n+  \/\/ input to soft ref clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+\n+  gch->trace_heap_after_gc(_gc_tracer);\n+}\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":505,"deletions":1,"binary":false,"changes":506,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -131,0 +131,2 @@\n+  static void invoke_at_safepoint(bool clear_all_softrefs);\n+\n@@ -155,0 +157,7 @@\n+  \/\/ Mark live objects\n+  static void phase1_mark(bool clear_all_softrefs);\n+\n+  \/\/ Temporary data structures for traversal and storing\/restoring marks\n+  static void allocate_stacks();\n+  static void deallocate_stacks();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-#include \"gc\/serial\/genMarkSweep.hpp\"\n@@ -564,1 +563,1 @@\n-    GCTraceCPUTime tcpu(GenMarkSweep::gc_tracer());\n+    GCTraceCPUTime tcpu(MarkSweep::gc_tracer());\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  friend class GenMarkSweep;\n+  friend class MarkSweep;\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,1 @@\n-#include \"gc\/serial\/genMarkSweep.hpp\"\n+#include \"gc\/serial\/markSweep.hpp\"\n@@ -447,1 +447,1 @@\n-  STWGCTimer* gc_timer = GenMarkSweep::gc_timer();\n+  STWGCTimer* gc_timer = MarkSweep::gc_timer();\n@@ -450,1 +450,1 @@\n-  SerialOldTracer* gc_tracer = GenMarkSweep::gc_tracer();\n+  SerialOldTracer* gc_tracer = MarkSweep::gc_tracer();\n@@ -455,1 +455,1 @@\n-  GenMarkSweep::invoke_at_safepoint(clear_all_soft_refs);\n+  MarkSweep::invoke_at_safepoint(clear_all_soft_refs);\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"}]}