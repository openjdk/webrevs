{"files":[{"patch":"@@ -44,1 +44,1 @@\n-\/\/ - TenuredGeneration             - tenured (old object) space (markSweepCompact)\n+\/\/ - TenuredGeneration             - tenured (old object) space (mark-compact)\n","filename":"src\/hotspot\/share\/gc\/serial\/generation.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,741 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"classfile\/javaClasses.hpp\"\n-#include \"classfile\/stringTable.hpp\"\n-#include \"classfile\/symbolTable.hpp\"\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"classfile\/vmSymbols.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"compiler\/compileBroker.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/serial\/cardTableRS.hpp\"\n-#include \"gc\/serial\/defNewGeneration.hpp\"\n-#include \"gc\/serial\/markSweep.inline.hpp\"\n-#include \"gc\/serial\/serialGcRefProcProxyTask.hpp\"\n-#include \"gc\/serial\/serialHeap.hpp\"\n-#include \"gc\/shared\/classUnloadingContext.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"gc\/shared\/gcHeapSummary.hpp\"\n-#include \"gc\/shared\/gcTimer.hpp\"\n-#include \"gc\/shared\/gcTrace.hpp\"\n-#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/modRefBarrierSet.hpp\"\n-#include \"gc\/shared\/referencePolicy.hpp\"\n-#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n-#include \"gc\/shared\/space.inline.hpp\"\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n-#include \"gc\/shared\/weakProcessor.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/access.inline.hpp\"\n-#include \"oops\/compressedOops.inline.hpp\"\n-#include \"oops\/instanceRefKlass.hpp\"\n-#include \"oops\/methodData.hpp\"\n-#include \"oops\/objArrayKlass.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"oops\/typeArrayOop.inline.hpp\"\n-#include \"runtime\/prefetch.inline.hpp\"\n-#include \"utilities\/copy.hpp\"\n-#include \"utilities\/events.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n-#if INCLUDE_JVMCI\n-#include \"jvmci\/jvmci.hpp\"\n-#endif\n-\n-uint                    MarkSweep::_total_invocations = 0;\n-\n-Stack<oop, mtGC>              MarkSweep::_marking_stack;\n-Stack<ObjArrayTask, mtGC>     MarkSweep::_objarray_stack;\n-\n-PreservedMarksSet       MarkSweep::_preserved_overflow_stack_set(false \/* in_c_heap *\/);\n-size_t                  MarkSweep::_preserved_count = 0;\n-size_t                  MarkSweep::_preserved_count_max = 0;\n-PreservedMark*          MarkSweep::_preserved_marks = nullptr;\n-STWGCTimer*             MarkSweep::_gc_timer        = nullptr;\n-SerialOldTracer*        MarkSweep::_gc_tracer       = nullptr;\n-\n-AlwaysTrueClosure   MarkSweep::_always_true_closure;\n-ReferenceProcessor* MarkSweep::_ref_processor;\n-\n-StringDedup::Requests*  MarkSweep::_string_dedup_requests = nullptr;\n-\n-MarkSweep::FollowRootClosure  MarkSweep::follow_root_closure;\n-\n-MarkAndPushClosure MarkSweep::mark_and_push_closure(ClassLoaderData::_claim_stw_fullgc_mark);\n-CLDToOopClosure    MarkSweep::follow_cld_closure(&mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n-\n-class DeadSpacer : StackObj {\n-  size_t _allowed_deadspace_words;\n-  bool _active;\n-  ContiguousSpace* _space;\n-\n-public:\n-  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n-    size_t ratio = _space->allowed_dead_ratio();\n-    _active = ratio > 0;\n-\n-    if (_active) {\n-      \/\/ We allow some amount of garbage towards the bottom of the space, so\n-      \/\/ we don't start compacting before there is a significant gain to be made.\n-      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n-      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n-      if ((MarkSweep::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n-        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n-      } else {\n-        _active = false;\n-      }\n-    }\n-  }\n-\n-  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n-    if (!_active) {\n-      return false;\n-    }\n-\n-    size_t dead_length = pointer_delta(dead_end, dead_start);\n-    if (_allowed_deadspace_words >= dead_length) {\n-      _allowed_deadspace_words -= dead_length;\n-      CollectedHeap::fill_with_object(dead_start, dead_length);\n-      oop obj = cast_to_oop(dead_start);\n-      \/\/ obj->set_mark(obj->mark().set_marked());\n-\n-      assert(dead_length == obj->size(), \"bad filler object size\");\n-      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n-                                        p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n-\n-      return true;\n-    } else {\n-      _active = false;\n-      return false;\n-    }\n-  }\n-};\n-\n-\/\/ Implement the \"compaction\" part of the mark-compact GC algorithm.\n-class Compacter {\n-  \/\/ There are four spaces in total, but only the first three can be used after\n-  \/\/ compact. IOW, old and eden\/from must be enough for all live objs\n-  static constexpr uint max_num_spaces = 4;\n-\n-  struct CompactionSpace {\n-    ContiguousSpace* _space;\n-    \/\/ Will be the new top after compaction is complete.\n-    HeapWord* _compaction_top;\n-    \/\/ The first dead word in this contiguous space. It's an optimization to\n-    \/\/ skip large chunk of live objects at the beginning.\n-    HeapWord* _first_dead;\n-\n-    void init(ContiguousSpace* space) {\n-      _space = space;\n-      _compaction_top = space->bottom();\n-      _first_dead = nullptr;\n-    }\n-  };\n-\n-  CompactionSpace _spaces[max_num_spaces];\n-  \/\/ The num of spaces to be compacted, i.e. containing live objs.\n-  uint _num_spaces;\n-\n-  uint _index;\n-\n-  HeapWord* get_compaction_top(uint index) const {\n-    return _spaces[index]._compaction_top;\n-  }\n-\n-  HeapWord* get_first_dead(uint index) const {\n-    return _spaces[index]._first_dead;\n-  }\n-\n-  ContiguousSpace* get_space(uint index) const {\n-    return _spaces[index]._space;\n-  }\n-\n-  void record_first_dead(uint index, HeapWord* first_dead) {\n-    assert(_spaces[index]._first_dead == nullptr, \"should write only once\");\n-    _spaces[index]._first_dead = first_dead;\n-  }\n-\n-  HeapWord* alloc(size_t words) {\n-    while (true) {\n-      if (words <= pointer_delta(_spaces[_index]._space->end(),\n-                                 _spaces[_index]._compaction_top)) {\n-        HeapWord* result = _spaces[_index]._compaction_top;\n-        _spaces[_index]._compaction_top += words;\n-        if (_index == 0) {\n-          \/\/ old-gen requires BOT update\n-          static_cast<TenuredSpace*>(_spaces[0]._space)->update_for_block(result, result + words);\n-        }\n-        return result;\n-      }\n-\n-      \/\/ out-of-memory in this space\n-      _index++;\n-      assert(_index < max_num_spaces - 1, \"the last space should not be used\");\n-    }\n-  }\n-\n-  static void prefetch_read_scan(void* p) {\n-    if (PrefetchScanIntervalInBytes >= 0) {\n-      Prefetch::read(p, PrefetchScanIntervalInBytes);\n-    }\n-  }\n-\n-  static void prefetch_write_scan(void* p) {\n-    if (PrefetchScanIntervalInBytes >= 0) {\n-      Prefetch::write(p, PrefetchScanIntervalInBytes);\n-    }\n-  }\n-\n-  static void prefetch_write_copy(void* p) {\n-    if (PrefetchCopyIntervalInBytes >= 0) {\n-      Prefetch::write(p, PrefetchCopyIntervalInBytes);\n-    }\n-  }\n-\n-  static void forward_obj(oop obj, HeapWord* new_addr) {\n-    prefetch_write_scan(obj);\n-    if (cast_from_oop<HeapWord*>(obj) != new_addr) {\n-      obj->forward_to(cast_to_oop(new_addr));\n-    } else {\n-      assert(obj->is_gc_marked(), \"inv\");\n-      \/\/ This obj will stay in-place. Fix the markword.\n-      obj->init_mark();\n-    }\n-  }\n-\n-  static HeapWord* find_next_live_addr(HeapWord* start, HeapWord* end) {\n-    for (HeapWord* i_addr = start; i_addr < end; \/* empty *\/) {\n-      prefetch_read_scan(i_addr);\n-      oop obj = cast_to_oop(i_addr);\n-      if (obj->is_gc_marked()) {\n-        return i_addr;\n-      }\n-      i_addr += obj->size();\n-    }\n-    return end;\n-  };\n-\n-  static size_t relocate(HeapWord* addr) {\n-    \/\/ Prefetch source and destination\n-    prefetch_read_scan(addr);\n-\n-    oop obj = cast_to_oop(addr);\n-    oop new_obj = obj->forwardee();\n-    HeapWord* new_addr = cast_from_oop<HeapWord*>(new_obj);\n-    assert(addr != new_addr, \"inv\");\n-    prefetch_write_copy(new_addr);\n-\n-    size_t obj_size = obj->size();\n-    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n-    new_obj->init_mark();\n-\n-    return obj_size;\n-  }\n-\n-public:\n-  explicit Compacter(SerialHeap* heap) {\n-    \/\/ In this order so that heap is compacted towards old-gen.\n-    _spaces[0].init(heap->old_gen()->space());\n-    _spaces[1].init(heap->young_gen()->eden());\n-    _spaces[2].init(heap->young_gen()->from());\n-\n-    bool is_promotion_failed = (heap->young_gen()->from()->next_compaction_space() != nullptr);\n-    if (is_promotion_failed) {\n-      _spaces[3].init(heap->young_gen()->to());\n-      _num_spaces = 4;\n-    } else {\n-      _num_spaces = 3;\n-    }\n-    _index = 0;\n-  }\n-\n-  void phase2_calculate_new_addr() {\n-    for (uint i = 0; i < _num_spaces; ++i) {\n-      ContiguousSpace* space = get_space(i);\n-      HeapWord* cur_addr = space->bottom();\n-      HeapWord* top = space->top();\n-\n-      bool record_first_dead_done = false;\n-\n-      DeadSpacer dead_spacer(space);\n-\n-      while (cur_addr < top) {\n-        oop obj = cast_to_oop(cur_addr);\n-        size_t obj_size = obj->size();\n-        if (obj->is_gc_marked()) {\n-          HeapWord* new_addr = alloc(obj_size);\n-          forward_obj(obj, new_addr);\n-          cur_addr += obj_size;\n-        } else {\n-          \/\/ Skipping the current known-unmarked obj\n-          HeapWord* next_live_addr = find_next_live_addr(cur_addr + obj_size, top);\n-          if (dead_spacer.insert_deadspace(cur_addr, next_live_addr)) {\n-            \/\/ Register space for the filler obj\n-            alloc(pointer_delta(next_live_addr, cur_addr));\n-          } else {\n-            if (!record_first_dead_done) {\n-              record_first_dead(i, cur_addr);\n-              record_first_dead_done = true;\n-            }\n-            *(HeapWord**)cur_addr = next_live_addr;\n-          }\n-          cur_addr = next_live_addr;\n-        }\n-      }\n-\n-      if (!record_first_dead_done) {\n-        record_first_dead(i, top);\n-      }\n-    }\n-  }\n-\n-  void phase3_adjust_pointers() {\n-    for (uint i = 0; i < _num_spaces; ++i) {\n-      ContiguousSpace* space = get_space(i);\n-      HeapWord* cur_addr = space->bottom();\n-      HeapWord* const top = space->top();\n-      HeapWord* const first_dead = get_first_dead(i);\n-\n-      while (cur_addr < top) {\n-        prefetch_write_scan(cur_addr);\n-        if (cur_addr < first_dead || cast_to_oop(cur_addr)->is_gc_marked()) {\n-          size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_addr));\n-          cur_addr += size;\n-        } else {\n-          assert(*(HeapWord**)cur_addr > cur_addr, \"forward progress\");\n-          cur_addr = *(HeapWord**)cur_addr;\n-        }\n-      }\n-    }\n-  }\n-\n-  void phase4_compact() {\n-    for (uint i = 0; i < _num_spaces; ++i) {\n-      ContiguousSpace* space = get_space(i);\n-      HeapWord* cur_addr = space->bottom();\n-      HeapWord* top = space->top();\n-\n-      \/\/ Check if the first obj inside this space is forwarded.\n-      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n-        \/\/ Jump over consecutive (in-place) live-objs-chunk\n-        cur_addr = get_first_dead(i);\n-      }\n-\n-      while (cur_addr < top) {\n-        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n-          cur_addr = *(HeapWord**) cur_addr;\n-          continue;\n-        }\n-        cur_addr += relocate(cur_addr);\n-      }\n-\n-      \/\/ Reset top and unused memory\n-      space->set_top(get_compaction_top(i));\n-      if (ZapUnusedHeapArea) {\n-        space->mangle_unused_area();\n-      }\n-    }\n-  }\n-};\n-\n-template <class T> void MarkSweep::KeepAliveClosure::do_oop_work(T* p) {\n-  mark_and_push(p);\n-}\n-\n-void MarkSweep::push_objarray(oop obj, size_t index) {\n-  ObjArrayTask task(obj, index);\n-  assert(task.is_valid(), \"bad ObjArrayTask\");\n-  _objarray_stack.push(task);\n-}\n-\n-void MarkSweep::follow_array(objArrayOop array) {\n-  mark_and_push_closure.do_klass(array->klass());\n-  \/\/ Don't push empty arrays to avoid unnecessary work.\n-  if (array->length() > 0) {\n-    MarkSweep::push_objarray(array, 0);\n-  }\n-}\n-\n-void MarkSweep::follow_object(oop obj) {\n-  assert(obj->is_gc_marked(), \"should be marked\");\n-  if (obj->is_objArray()) {\n-    \/\/ Handle object arrays explicitly to allow them to\n-    \/\/ be split into chunks if needed.\n-    MarkSweep::follow_array((objArrayOop)obj);\n-  } else {\n-    obj->oop_iterate(&mark_and_push_closure);\n-  }\n-}\n-\n-void MarkSweep::follow_array_chunk(objArrayOop array, int index) {\n-  const int len = array->length();\n-  const int beg_index = index;\n-  assert(beg_index < len || len == 0, \"index too large\");\n-\n-  const int stride = MIN2(len - beg_index, (int) ObjArrayMarkingStride);\n-  const int end_index = beg_index + stride;\n-\n-  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n-\n-  if (end_index < len) {\n-    MarkSweep::push_objarray(array, end_index); \/\/ Push the continuation.\n-  }\n-}\n-\n-void MarkSweep::follow_stack() {\n-  do {\n-    while (!_marking_stack.is_empty()) {\n-      oop obj = _marking_stack.pop();\n-      assert (obj->is_gc_marked(), \"p must be marked\");\n-      follow_object(obj);\n-    }\n-    \/\/ Process ObjArrays one at a time to avoid marking stack bloat.\n-    if (!_objarray_stack.is_empty()) {\n-      ObjArrayTask task = _objarray_stack.pop();\n-      follow_array_chunk(objArrayOop(task.obj()), task.index());\n-    }\n-  } while (!_marking_stack.is_empty() || !_objarray_stack.is_empty());\n-}\n-\n-MarkSweep::FollowStackClosure MarkSweep::follow_stack_closure;\n-\n-void MarkSweep::FollowStackClosure::do_void() { follow_stack(); }\n-\n-template <class T> void MarkSweep::follow_root(T* p) {\n-  assert(!Universe::heap()->is_in(p),\n-         \"roots shouldn't be things within the heap\");\n-  T heap_oop = RawAccess<>::oop_load(p);\n-  if (!CompressedOops::is_null(heap_oop)) {\n-    oop obj = CompressedOops::decode_not_null(heap_oop);\n-    if (!obj->mark().is_marked()) {\n-      mark_object(obj);\n-      follow_object(obj);\n-    }\n-  }\n-  follow_stack();\n-}\n-\n-void MarkSweep::FollowRootClosure::do_oop(oop* p)       { follow_root(p); }\n-void MarkSweep::FollowRootClosure::do_oop(narrowOop* p) { follow_root(p); }\n-\n-\/\/ We preserve the mark which should be replaced at the end and the location\n-\/\/ that it will go.  Note that the object that this markWord belongs to isn't\n-\/\/ currently at that address but it will be after phase4\n-void MarkSweep::preserve_mark(oop obj, markWord mark) {\n-  \/\/ We try to store preserved marks in the to space of the new generation since\n-  \/\/ this is storage which should be available.  Most of the time this should be\n-  \/\/ sufficient space for the marks we need to preserve but if it isn't we fall\n-  \/\/ back to using Stacks to keep track of the overflow.\n-  if (_preserved_count < _preserved_count_max) {\n-    _preserved_marks[_preserved_count++] = PreservedMark(obj, mark);\n-  } else {\n-    _preserved_overflow_stack_set.get()->push_always(obj, mark);\n-  }\n-}\n-\n-void MarkSweep::phase1_mark(bool clear_all_softrefs) {\n-  \/\/ Recursively traverse all live objects and mark them\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 1: Mark live objects\", _gc_timer);\n-\n-  SerialHeap* gch = SerialHeap::heap();\n-\n-  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n-\n-  ref_processor()->start_discovery(clear_all_softrefs);\n-\n-  {\n-    StrongRootsScope srs(0);\n-\n-    CLDClosure* weak_cld_closure = ClassUnloading ? nullptr : &follow_cld_closure;\n-    MarkingCodeBlobClosure mark_code_closure(&follow_root_closure, !CodeBlobToOopClosure::FixRelocations, true);\n-    gch->process_roots(SerialHeap::SO_None,\n-                       &follow_root_closure,\n-                       &follow_cld_closure,\n-                       weak_cld_closure,\n-                       &mark_code_closure);\n-  }\n-\n-  \/\/ Process reference objects found during marking\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Reference Processing\", gc_timer());\n-\n-    ReferenceProcessorPhaseTimes pt(_gc_timer, ref_processor()->max_num_queues());\n-    SerialGCRefProcProxyTask task(is_alive, keep_alive, follow_stack_closure);\n-    const ReferenceProcessorStats& stats = ref_processor()->process_discovered_references(task, pt);\n-    pt.print_all_references();\n-    gc_tracer()->report_gc_reference_stats(stats);\n-  }\n-\n-  \/\/ This is the point where the entire marking should have completed.\n-  assert(_marking_stack.is_empty(), \"Marking should have completed\");\n-\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Weak Processing\", gc_timer());\n-    WeakProcessor::weak_oops_do(&is_alive, &do_nothing_cl);\n-  }\n-\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", gc_timer());\n-\n-    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n-\n-    bool unloading_occurred;\n-    {\n-      CodeCache::UnlinkingScope scope(&is_alive);\n-\n-      \/\/ Unload classes and purge the SystemDictionary.\n-      unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n-\n-      \/\/ Unload nmethods.\n-      CodeCache::do_unloading(unloading_occurred);\n-    }\n-\n-    {\n-      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n-      \/\/ Release unloaded nmethod's memory.\n-      ctx->purge_nmethods();\n-    }\n-    {\n-      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", gc_timer());\n-      gch->prune_unlinked_nmethods();\n-    }\n-    {\n-      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n-      ctx->free_code_blobs();\n-    }\n-\n-    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n-    Klass::clean_weak_klass_links(unloading_occurred);\n-\n-    \/\/ Clean JVMCI metadata handles.\n-    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n-  }\n-\n-  {\n-    GCTraceTime(Debug, gc, phases) tm_m(\"Report Object Count\", gc_timer());\n-    gc_tracer()->report_object_count_after_gc(&is_alive, nullptr);\n-  }\n-}\n-\n-void MarkSweep::allocate_stacks() {\n-  void* scratch = nullptr;\n-  size_t num_words;\n-  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-  young_gen->contribute_scratch(scratch, num_words);\n-\n-  if (scratch != nullptr) {\n-    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n-  } else {\n-    _preserved_count_max = 0;\n-  }\n-\n-  _preserved_marks = (PreservedMark*)scratch;\n-  _preserved_count = 0;\n-\n-  _preserved_overflow_stack_set.init(1);\n-}\n-\n-void MarkSweep::deallocate_stacks() {\n-  if (_preserved_count_max != 0) {\n-    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-    young_gen->reset_scratch();\n-  }\n-\n-  _preserved_overflow_stack_set.reclaim();\n-  _marking_stack.clear();\n-  _objarray_stack.clear(true);\n-}\n-\n-void MarkSweep::mark_object(oop obj) {\n-  if (StringDedup::is_enabled() &&\n-      java_lang_String::is_instance(obj) &&\n-      SerialStringDedup::is_candidate_from_mark(obj)) {\n-    _string_dedup_requests->add(obj);\n-  }\n-\n-  \/\/ some marks may contain information we need to preserve so we store them away\n-  \/\/ and overwrite the mark.  We'll restore it at the end of markSweep.\n-  markWord mark = obj->mark();\n-  obj->set_mark(markWord::prototype().set_marked());\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n-\n-  if (obj->mark_must_be_preserved(mark)) {\n-    preserve_mark(obj, mark);\n-  }\n-}\n-\n-template <class T> void MarkSweep::mark_and_push(T* p) {\n-  T heap_oop = RawAccess<>::oop_load(p);\n-  if (!CompressedOops::is_null(heap_oop)) {\n-    oop obj = CompressedOops::decode_not_null(heap_oop);\n-    if (!obj->mark().is_marked()) {\n-      mark_object(obj);\n-      _marking_stack.push(obj);\n-    }\n-  }\n-}\n-\n-template <typename T>\n-void MarkAndPushClosure::do_oop_work(T* p)            { MarkSweep::mark_and_push(p); }\n-void MarkAndPushClosure::do_oop(      oop* p)         { do_oop_work(p); }\n-void MarkAndPushClosure::do_oop(narrowOop* p)         { do_oop_work(p); }\n-\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n-void MarkSweep::adjust_marks() {\n-  \/\/ adjust the oops we saved earlier\n-  for (size_t i = 0; i < _preserved_count; i++) {\n-    PreservedMarks::adjust_preserved_mark(_preserved_marks + i);\n-  }\n-\n-  \/\/ deal with the overflow stack\n-  _preserved_overflow_stack_set.get()->adjust_during_full_gc();\n-}\n-\n-void MarkSweep::restore_marks() {\n-  log_trace(gc)(\"Restoring \" SIZE_FORMAT \" marks\", _preserved_count + _preserved_overflow_stack_set.get()->size());\n-\n-  \/\/ restore the marks we saved earlier\n-  for (size_t i = 0; i < _preserved_count; i++) {\n-    _preserved_marks[i].set_mark();\n-  }\n-\n-  \/\/ deal with the overflow\n-  _preserved_overflow_stack_set.restore(nullptr);\n-}\n-\n-MarkSweep::IsAliveClosure   MarkSweep::is_alive;\n-\n-bool MarkSweep::IsAliveClosure::do_object_b(oop p) { return p->is_gc_marked(); }\n-\n-MarkSweep::KeepAliveClosure MarkSweep::keep_alive;\n-\n-void MarkSweep::KeepAliveClosure::do_oop(oop* p)       { MarkSweep::KeepAliveClosure::do_oop_work(p); }\n-void MarkSweep::KeepAliveClosure::do_oop(narrowOop* p) { MarkSweep::KeepAliveClosure::do_oop_work(p); }\n-\n-void MarkSweep::initialize() {\n-  MarkSweep::_gc_timer = new STWGCTimer();\n-  MarkSweep::_gc_tracer = new SerialOldTracer();\n-  MarkSweep::_string_dedup_requests = new StringDedup::Requests();\n-\n-  \/\/ The Full GC operates on the entire heap so all objects should be subject\n-  \/\/ to discovery, hence the _always_true_closure.\n-  MarkSweep::_ref_processor = new ReferenceProcessor(&_always_true_closure);\n-  mark_and_push_closure.set_ref_discoverer(_ref_processor);\n-}\n-\n-void MarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n-\n-  SerialHeap* gch = SerialHeap::heap();\n-#ifdef ASSERT\n-  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n-    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n-  }\n-#endif\n-\n-  gch->trace_heap_before_gc(_gc_tracer);\n-\n-  \/\/ Increment the invocation count\n-  _total_invocations++;\n-\n-  \/\/ Capture used regions for old-gen to reestablish old-to-young invariant\n-  \/\/ after full-gc.\n-  gch->old_gen()->save_used_region();\n-\n-  allocate_stacks();\n-\n-  phase1_mark(clear_all_softrefs);\n-\n-  Compacter compacter{gch};\n-\n-  {\n-    \/\/ Now all live objects are marked, compute the new object addresses.\n-    GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n-\n-    compacter.phase2_calculate_new_addr();\n-  }\n-\n-  \/\/ Don't add any more derived pointers during phase3\n-#if COMPILER2_OR_JVMCI\n-  assert(DerivedPointerTable::is_active(), \"Sanity\");\n-  DerivedPointerTable::set_active(false);\n-#endif\n-\n-  {\n-    \/\/ Adjust the pointers to reflect the new locations\n-    GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n-\n-    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n-\n-    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-    gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                       &adjust_pointer_closure,\n-                       &adjust_cld_closure,\n-                       &adjust_cld_closure,\n-                       &code_closure);\n-\n-    WeakProcessor::oops_do(&adjust_pointer_closure);\n-\n-    adjust_marks();\n-    compacter.phase3_adjust_pointers();\n-  }\n-\n-  {\n-    \/\/ All pointers are now adjusted, move objects accordingly\n-    GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n-\n-    compacter.phase4_compact();\n-  }\n-\n-  restore_marks();\n-\n-  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n-  \/\/ (Should this be in general part?)\n-  gch->save_marks();\n-\n-  deallocate_stacks();\n-\n-  MarkSweep::_string_dedup_requests->flush();\n-\n-  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n-  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n-\n-  gch->prune_scavengable_nmethods();\n-\n-  \/\/ Update heap occupancy information which is used as\n-  \/\/ input to soft ref clearing policy at the next gc.\n-  Universe::heap()->update_capacity_and_used_at_gc();\n-\n-  \/\/ Signal that we have completed a visit to all live objects.\n-  Universe::heap()->record_whole_heap_examined_timestamp();\n-\n-  gch->trace_heap_after_gc(_gc_tracer);\n-}\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":0,"deletions":741,"binary":false,"changes":741,"status":"deleted"},{"patch":"@@ -1,65 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SERIAL_MARKSWEEP_INLINE_HPP\n-#define SHARE_GC_SERIAL_MARKSWEEP_INLINE_HPP\n-\n-#include \"gc\/serial\/markSweep.hpp\"\n-\n-#include \"classfile\/classLoaderData.inline.hpp\"\n-#include \"classfile\/javaClasses.inline.hpp\"\n-#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n-#include \"gc\/serial\/serialStringDedup.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/markWord.hpp\"\n-#include \"oops\/access.inline.hpp\"\n-#include \"oops\/compressedOops.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n-\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n-  T heap_oop = RawAccess<>::oop_load(p);\n-  if (!CompressedOops::is_null(heap_oop)) {\n-    oop obj = CompressedOops::decode_not_null(heap_oop);\n-    assert(Universe::heap()->is_in(obj), \"should be in heap\");\n-\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n-      assert(is_object_aligned(new_obj), \"oop must be aligned\");\n-      RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n-    }\n-  }\n-}\n-\n-template <typename T>\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n-inline void AdjustPointerClosure::do_oop(oop* p)       { do_oop_work(p); }\n-inline void AdjustPointerClosure::do_oop(narrowOop* p) { do_oop_work(p); }\n-\n-inline size_t MarkSweep::adjust_pointers(oop obj) {\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n-}\n-\n-#endif \/\/ SHARE_GC_SERIAL_MARKSWEEP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":0,"deletions":65,"binary":false,"changes":65,"status":"deleted"},{"patch":"@@ -0,0 +1,741 @@\n+\/*\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"compiler\/compileBroker.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/serial\/cardTableRS.hpp\"\n+#include \"gc\/serial\/defNewGeneration.hpp\"\n+#include \"gc\/serial\/serialFullGC.inline.hpp\"\n+#include \"gc\/serial\/serialGcRefProcProxyTask.hpp\"\n+#include \"gc\/serial\/serialHeap.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/modRefBarrierSet.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/space.inline.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/weakProcessor.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/instanceRefKlass.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"oops\/objArrayKlass.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"runtime\/prefetch.inline.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+uint                    SerialFullGC::_total_invocations = 0;\n+\n+Stack<oop, mtGC>              SerialFullGC::_marking_stack;\n+Stack<ObjArrayTask, mtGC>     SerialFullGC::_objarray_stack;\n+\n+PreservedMarksSet       SerialFullGC::_preserved_overflow_stack_set(false \/* in_c_heap *\/);\n+size_t                  SerialFullGC::_preserved_count = 0;\n+size_t                  SerialFullGC::_preserved_count_max = 0;\n+PreservedMark*          SerialFullGC::_preserved_marks = nullptr;\n+STWGCTimer*             SerialFullGC::_gc_timer        = nullptr;\n+SerialOldTracer*        SerialFullGC::_gc_tracer       = nullptr;\n+\n+AlwaysTrueClosure   SerialFullGC::_always_true_closure;\n+ReferenceProcessor* SerialFullGC::_ref_processor;\n+\n+StringDedup::Requests*  SerialFullGC::_string_dedup_requests = nullptr;\n+\n+SerialFullGC::FollowRootClosure  SerialFullGC::follow_root_closure;\n+\n+MarkAndPushClosure SerialFullGC::mark_and_push_closure(ClassLoaderData::_claim_stw_fullgc_mark);\n+CLDToOopClosure    SerialFullGC::follow_cld_closure(&mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+CLDToOopClosure    SerialFullGC::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+\n+class DeadSpacer : StackObj {\n+  size_t _allowed_deadspace_words;\n+  bool _active;\n+  ContiguousSpace* _space;\n+\n+public:\n+  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n+    size_t ratio = _space->allowed_dead_ratio();\n+    _active = ratio > 0;\n+\n+    if (_active) {\n+      \/\/ We allow some amount of garbage towards the bottom of the space, so\n+      \/\/ we don't start compacting before there is a significant gain to be made.\n+      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n+      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n+      if ((SerialFullGC::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n+        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n+      } else {\n+        _active = false;\n+      }\n+    }\n+  }\n+\n+  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n+    if (!_active) {\n+      return false;\n+    }\n+\n+    size_t dead_length = pointer_delta(dead_end, dead_start);\n+    if (_allowed_deadspace_words >= dead_length) {\n+      _allowed_deadspace_words -= dead_length;\n+      CollectedHeap::fill_with_object(dead_start, dead_length);\n+      oop obj = cast_to_oop(dead_start);\n+      \/\/ obj->set_mark(obj->mark().set_marked());\n+\n+      assert(dead_length == obj->size(), \"bad filler object size\");\n+      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n+                                        p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n+\n+      return true;\n+    } else {\n+      _active = false;\n+      return false;\n+    }\n+  }\n+};\n+\n+\/\/ Implement the \"compaction\" part of the mark-compact GC algorithm.\n+class Compacter {\n+  \/\/ There are four spaces in total, but only the first three can be used after\n+  \/\/ compact. IOW, old and eden\/from must be enough for all live objs\n+  static constexpr uint max_num_spaces = 4;\n+\n+  struct CompactionSpace {\n+    ContiguousSpace* _space;\n+    \/\/ Will be the new top after compaction is complete.\n+    HeapWord* _compaction_top;\n+    \/\/ The first dead word in this contiguous space. It's an optimization to\n+    \/\/ skip large chunk of live objects at the beginning.\n+    HeapWord* _first_dead;\n+\n+    void init(ContiguousSpace* space) {\n+      _space = space;\n+      _compaction_top = space->bottom();\n+      _first_dead = nullptr;\n+    }\n+  };\n+\n+  CompactionSpace _spaces[max_num_spaces];\n+  \/\/ The num of spaces to be compacted, i.e. containing live objs.\n+  uint _num_spaces;\n+\n+  uint _index;\n+\n+  HeapWord* get_compaction_top(uint index) const {\n+    return _spaces[index]._compaction_top;\n+  }\n+\n+  HeapWord* get_first_dead(uint index) const {\n+    return _spaces[index]._first_dead;\n+  }\n+\n+  ContiguousSpace* get_space(uint index) const {\n+    return _spaces[index]._space;\n+  }\n+\n+  void record_first_dead(uint index, HeapWord* first_dead) {\n+    assert(_spaces[index]._first_dead == nullptr, \"should write only once\");\n+    _spaces[index]._first_dead = first_dead;\n+  }\n+\n+  HeapWord* alloc(size_t words) {\n+    while (true) {\n+      if (words <= pointer_delta(_spaces[_index]._space->end(),\n+                                 _spaces[_index]._compaction_top)) {\n+        HeapWord* result = _spaces[_index]._compaction_top;\n+        _spaces[_index]._compaction_top += words;\n+        if (_index == 0) {\n+          \/\/ old-gen requires BOT update\n+          static_cast<TenuredSpace*>(_spaces[0]._space)->update_for_block(result, result + words);\n+        }\n+        return result;\n+      }\n+\n+      \/\/ out-of-memory in this space\n+      _index++;\n+      assert(_index < max_num_spaces - 1, \"the last space should not be used\");\n+    }\n+  }\n+\n+  static void prefetch_read_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::read(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n+\n+  static void prefetch_write_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n+\n+  static void prefetch_write_copy(void* p) {\n+    if (PrefetchCopyIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchCopyIntervalInBytes);\n+    }\n+  }\n+\n+  static void forward_obj(oop obj, HeapWord* new_addr) {\n+    prefetch_write_scan(obj);\n+    if (cast_from_oop<HeapWord*>(obj) != new_addr) {\n+      obj->forward_to(cast_to_oop(new_addr));\n+    } else {\n+      assert(obj->is_gc_marked(), \"inv\");\n+      \/\/ This obj will stay in-place. Fix the markword.\n+      obj->init_mark();\n+    }\n+  }\n+\n+  static HeapWord* find_next_live_addr(HeapWord* start, HeapWord* end) {\n+    for (HeapWord* i_addr = start; i_addr < end; \/* empty *\/) {\n+      prefetch_read_scan(i_addr);\n+      oop obj = cast_to_oop(i_addr);\n+      if (obj->is_gc_marked()) {\n+        return i_addr;\n+      }\n+      i_addr += obj->size();\n+    }\n+    return end;\n+  };\n+\n+  static size_t relocate(HeapWord* addr) {\n+    \/\/ Prefetch source and destination\n+    prefetch_read_scan(addr);\n+\n+    oop obj = cast_to_oop(addr);\n+    oop new_obj = obj->forwardee();\n+    HeapWord* new_addr = cast_from_oop<HeapWord*>(new_obj);\n+    assert(addr != new_addr, \"inv\");\n+    prefetch_write_copy(new_addr);\n+\n+    size_t obj_size = obj->size();\n+    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    new_obj->init_mark();\n+\n+    return obj_size;\n+  }\n+\n+public:\n+  explicit Compacter(SerialHeap* heap) {\n+    \/\/ In this order so that heap is compacted towards old-gen.\n+    _spaces[0].init(heap->old_gen()->space());\n+    _spaces[1].init(heap->young_gen()->eden());\n+    _spaces[2].init(heap->young_gen()->from());\n+\n+    bool is_promotion_failed = (heap->young_gen()->from()->next_compaction_space() != nullptr);\n+    if (is_promotion_failed) {\n+      _spaces[3].init(heap->young_gen()->to());\n+      _num_spaces = 4;\n+    } else {\n+      _num_spaces = 3;\n+    }\n+    _index = 0;\n+  }\n+\n+  void phase2_calculate_new_addr() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      bool record_first_dead_done = false;\n+\n+      DeadSpacer dead_spacer(space);\n+\n+      while (cur_addr < top) {\n+        oop obj = cast_to_oop(cur_addr);\n+        size_t obj_size = obj->size();\n+        if (obj->is_gc_marked()) {\n+          HeapWord* new_addr = alloc(obj_size);\n+          forward_obj(obj, new_addr);\n+          cur_addr += obj_size;\n+        } else {\n+          \/\/ Skipping the current known-unmarked obj\n+          HeapWord* next_live_addr = find_next_live_addr(cur_addr + obj_size, top);\n+          if (dead_spacer.insert_deadspace(cur_addr, next_live_addr)) {\n+            \/\/ Register space for the filler obj\n+            alloc(pointer_delta(next_live_addr, cur_addr));\n+          } else {\n+            if (!record_first_dead_done) {\n+              record_first_dead(i, cur_addr);\n+              record_first_dead_done = true;\n+            }\n+            *(HeapWord**)cur_addr = next_live_addr;\n+          }\n+          cur_addr = next_live_addr;\n+        }\n+      }\n+\n+      if (!record_first_dead_done) {\n+        record_first_dead(i, top);\n+      }\n+    }\n+  }\n+\n+  void phase3_adjust_pointers() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* const top = space->top();\n+      HeapWord* const first_dead = get_first_dead(i);\n+\n+      while (cur_addr < top) {\n+        prefetch_write_scan(cur_addr);\n+        if (cur_addr < first_dead || cast_to_oop(cur_addr)->is_gc_marked()) {\n+          size_t size = SerialFullGC::adjust_pointers(cast_to_oop(cur_addr));\n+          cur_addr += size;\n+        } else {\n+          assert(*(HeapWord**)cur_addr > cur_addr, \"forward progress\");\n+          cur_addr = *(HeapWord**)cur_addr;\n+        }\n+      }\n+    }\n+  }\n+\n+  void phase4_compact() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      \/\/ Check if the first obj inside this space is forwarded.\n+      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+        \/\/ Jump over consecutive (in-place) live-objs-chunk\n+        cur_addr = get_first_dead(i);\n+      }\n+\n+      while (cur_addr < top) {\n+        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+          cur_addr = *(HeapWord**) cur_addr;\n+          continue;\n+        }\n+        cur_addr += relocate(cur_addr);\n+      }\n+\n+      \/\/ Reset top and unused memory\n+      space->set_top(get_compaction_top(i));\n+      if (ZapUnusedHeapArea) {\n+        space->mangle_unused_area();\n+      }\n+    }\n+  }\n+};\n+\n+template <class T> void SerialFullGC::KeepAliveClosure::do_oop_work(T* p) {\n+  mark_and_push(p);\n+}\n+\n+void SerialFullGC::push_objarray(oop obj, size_t index) {\n+  ObjArrayTask task(obj, index);\n+  assert(task.is_valid(), \"bad ObjArrayTask\");\n+  _objarray_stack.push(task);\n+}\n+\n+void SerialFullGC::follow_array(objArrayOop array) {\n+  mark_and_push_closure.do_klass(array->klass());\n+  \/\/ Don't push empty arrays to avoid unnecessary work.\n+  if (array->length() > 0) {\n+    SerialFullGC::push_objarray(array, 0);\n+  }\n+}\n+\n+void SerialFullGC::follow_object(oop obj) {\n+  assert(obj->is_gc_marked(), \"should be marked\");\n+  if (obj->is_objArray()) {\n+    \/\/ Handle object arrays explicitly to allow them to\n+    \/\/ be split into chunks if needed.\n+    SerialFullGC::follow_array((objArrayOop)obj);\n+  } else {\n+    obj->oop_iterate(&mark_and_push_closure);\n+  }\n+}\n+\n+void SerialFullGC::follow_array_chunk(objArrayOop array, int index) {\n+  const int len = array->length();\n+  const int beg_index = index;\n+  assert(beg_index < len || len == 0, \"index too large\");\n+\n+  const int stride = MIN2(len - beg_index, (int) ObjArrayMarkingStride);\n+  const int end_index = beg_index + stride;\n+\n+  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n+\n+  if (end_index < len) {\n+    SerialFullGC::push_objarray(array, end_index); \/\/ Push the continuation.\n+  }\n+}\n+\n+void SerialFullGC::follow_stack() {\n+  do {\n+    while (!_marking_stack.is_empty()) {\n+      oop obj = _marking_stack.pop();\n+      assert (obj->is_gc_marked(), \"p must be marked\");\n+      follow_object(obj);\n+    }\n+    \/\/ Process ObjArrays one at a time to avoid marking stack bloat.\n+    if (!_objarray_stack.is_empty()) {\n+      ObjArrayTask task = _objarray_stack.pop();\n+      follow_array_chunk(objArrayOop(task.obj()), task.index());\n+    }\n+  } while (!_marking_stack.is_empty() || !_objarray_stack.is_empty());\n+}\n+\n+SerialFullGC::FollowStackClosure SerialFullGC::follow_stack_closure;\n+\n+void SerialFullGC::FollowStackClosure::do_void() { follow_stack(); }\n+\n+template <class T> void SerialFullGC::follow_root(T* p) {\n+  assert(!Universe::heap()->is_in(p),\n+         \"roots shouldn't be things within the heap\");\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    if (!obj->mark().is_marked()) {\n+      mark_object(obj);\n+      follow_object(obj);\n+    }\n+  }\n+  follow_stack();\n+}\n+\n+void SerialFullGC::FollowRootClosure::do_oop(oop* p)       { follow_root(p); }\n+void SerialFullGC::FollowRootClosure::do_oop(narrowOop* p) { follow_root(p); }\n+\n+\/\/ We preserve the mark which should be replaced at the end and the location\n+\/\/ that it will go.  Note that the object that this markWord belongs to isn't\n+\/\/ currently at that address but it will be after phase4\n+void SerialFullGC::preserve_mark(oop obj, markWord mark) {\n+  \/\/ We try to store preserved marks in the to space of the new generation since\n+  \/\/ this is storage which should be available.  Most of the time this should be\n+  \/\/ sufficient space for the marks we need to preserve but if it isn't we fall\n+  \/\/ back to using Stacks to keep track of the overflow.\n+  if (_preserved_count < _preserved_count_max) {\n+    _preserved_marks[_preserved_count++] = PreservedMark(obj, mark);\n+  } else {\n+    _preserved_overflow_stack_set.get()->push_always(obj, mark);\n+  }\n+}\n+\n+void SerialFullGC::phase1_mark(bool clear_all_softrefs) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Phase 1: Mark live objects\", _gc_timer);\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+\n+  ref_processor()->start_discovery(clear_all_softrefs);\n+\n+  {\n+    StrongRootsScope srs(0);\n+\n+    CLDClosure* weak_cld_closure = ClassUnloading ? nullptr : &follow_cld_closure;\n+    MarkingCodeBlobClosure mark_code_closure(&follow_root_closure, !CodeBlobToOopClosure::FixRelocations, true);\n+    gch->process_roots(SerialHeap::SO_None,\n+                       &follow_root_closure,\n+                       &follow_cld_closure,\n+                       weak_cld_closure,\n+                       &mark_code_closure);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Reference Processing\", gc_timer());\n+\n+    ReferenceProcessorPhaseTimes pt(_gc_timer, ref_processor()->max_num_queues());\n+    SerialGCRefProcProxyTask task(is_alive, keep_alive, follow_stack_closure);\n+    const ReferenceProcessorStats& stats = ref_processor()->process_discovered_references(task, pt);\n+    pt.print_all_references();\n+    gc_tracer()->report_gc_reference_stats(stats);\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  assert(_marking_stack.is_empty(), \"Marking should have completed\");\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Weak Processing\", gc_timer());\n+    WeakProcessor::weak_oops_do(&is_alive, &do_nothing_cl);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", gc_timer());\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(&is_alive);\n+\n+      \/\/ Unload classes and purge the SystemDictionary.\n+      unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", gc_timer());\n+      gch->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_code_blobs();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Report Object Count\", gc_timer());\n+    gc_tracer()->report_object_count_after_gc(&is_alive, nullptr);\n+  }\n+}\n+\n+void SerialFullGC::allocate_stacks() {\n+  void* scratch = nullptr;\n+  size_t num_words;\n+  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+  young_gen->contribute_scratch(scratch, num_words);\n+\n+  if (scratch != nullptr) {\n+    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n+  } else {\n+    _preserved_count_max = 0;\n+  }\n+\n+  _preserved_marks = (PreservedMark*)scratch;\n+  _preserved_count = 0;\n+\n+  _preserved_overflow_stack_set.init(1);\n+}\n+\n+void SerialFullGC::deallocate_stacks() {\n+  if (_preserved_count_max != 0) {\n+    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+    young_gen->reset_scratch();\n+  }\n+\n+  _preserved_overflow_stack_set.reclaim();\n+  _marking_stack.clear();\n+  _objarray_stack.clear(true);\n+}\n+\n+void SerialFullGC::mark_object(oop obj) {\n+  if (StringDedup::is_enabled() &&\n+      java_lang_String::is_instance(obj) &&\n+      SerialStringDedup::is_candidate_from_mark(obj)) {\n+    _string_dedup_requests->add(obj);\n+  }\n+\n+  \/\/ some marks may contain information we need to preserve so we store them away\n+  \/\/ and overwrite the mark.  We'll restore it at the end of serial full GC.\n+  markWord mark = obj->mark();\n+  obj->set_mark(markWord::prototype().set_marked());\n+\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n+  if (obj->mark_must_be_preserved(mark)) {\n+    preserve_mark(obj, mark);\n+  }\n+}\n+\n+template <class T> void SerialFullGC::mark_and_push(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    if (!obj->mark().is_marked()) {\n+      mark_object(obj);\n+      _marking_stack.push(obj);\n+    }\n+  }\n+}\n+\n+template <typename T>\n+void MarkAndPushClosure::do_oop_work(T* p)            { SerialFullGC::mark_and_push(p); }\n+void MarkAndPushClosure::do_oop(      oop* p)         { do_oop_work(p); }\n+void MarkAndPushClosure::do_oop(narrowOop* p)         { do_oop_work(p); }\n+\n+AdjustPointerClosure SerialFullGC::adjust_pointer_closure;\n+\n+void SerialFullGC::adjust_marks() {\n+  \/\/ adjust the oops we saved earlier\n+  for (size_t i = 0; i < _preserved_count; i++) {\n+    PreservedMarks::adjust_preserved_mark(_preserved_marks + i);\n+  }\n+\n+  \/\/ deal with the overflow stack\n+  _preserved_overflow_stack_set.get()->adjust_during_full_gc();\n+}\n+\n+void SerialFullGC::restore_marks() {\n+  log_trace(gc)(\"Restoring \" SIZE_FORMAT \" marks\", _preserved_count + _preserved_overflow_stack_set.get()->size());\n+\n+  \/\/ restore the marks we saved earlier\n+  for (size_t i = 0; i < _preserved_count; i++) {\n+    _preserved_marks[i].set_mark();\n+  }\n+\n+  \/\/ deal with the overflow\n+  _preserved_overflow_stack_set.restore(nullptr);\n+}\n+\n+SerialFullGC::IsAliveClosure   SerialFullGC::is_alive;\n+\n+bool SerialFullGC::IsAliveClosure::do_object_b(oop p) { return p->is_gc_marked(); }\n+\n+SerialFullGC::KeepAliveClosure SerialFullGC::keep_alive;\n+\n+void SerialFullGC::KeepAliveClosure::do_oop(oop* p)       { SerialFullGC::KeepAliveClosure::do_oop_work(p); }\n+void SerialFullGC::KeepAliveClosure::do_oop(narrowOop* p) { SerialFullGC::KeepAliveClosure::do_oop_work(p); }\n+\n+void SerialFullGC::initialize() {\n+  SerialFullGC::_gc_timer = new STWGCTimer();\n+  SerialFullGC::_gc_tracer = new SerialOldTracer();\n+  SerialFullGC::_string_dedup_requests = new StringDedup::Requests();\n+\n+  \/\/ The Full GC operates on the entire heap so all objects should be subject\n+  \/\/ to discovery, hence the _always_true_closure.\n+  SerialFullGC::_ref_processor = new ReferenceProcessor(&_always_true_closure);\n+  mark_and_push_closure.set_ref_discoverer(_ref_processor);\n+}\n+\n+void SerialFullGC::invoke_at_safepoint(bool clear_all_softrefs) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+#ifdef ASSERT\n+  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n+    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+  }\n+#endif\n+\n+  gch->trace_heap_before_gc(_gc_tracer);\n+\n+  \/\/ Increment the invocation count\n+  _total_invocations++;\n+\n+  \/\/ Capture used regions for old-gen to reestablish old-to-young invariant\n+  \/\/ after full-gc.\n+  gch->old_gen()->save_used_region();\n+\n+  allocate_stacks();\n+\n+  phase1_mark(clear_all_softrefs);\n+\n+  Compacter compacter{gch};\n+\n+  {\n+    \/\/ Now all live objects are marked, compute the new object addresses.\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n+\n+    compacter.phase2_calculate_new_addr();\n+  }\n+\n+  \/\/ Don't add any more derived pointers during phase3\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_active(), \"Sanity\");\n+  DerivedPointerTable::set_active(false);\n+#endif\n+\n+  {\n+    \/\/ Adjust the pointers to reflect the new locations\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n+\n+    WeakProcessor::oops_do(&adjust_pointer_closure);\n+\n+    adjust_marks();\n+    compacter.phase3_adjust_pointers();\n+  }\n+\n+  {\n+    \/\/ All pointers are now adjusted, move objects accordingly\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n+\n+    compacter.phase4_compact();\n+  }\n+\n+  restore_marks();\n+\n+  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n+  \/\/ (Should this be in general part?)\n+  gch->save_marks();\n+\n+  deallocate_stacks();\n+\n+  SerialFullGC::_string_dedup_requests->flush();\n+\n+  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n+  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+\n+  gch->prune_scavengable_nmethods();\n+\n+  \/\/ Update heap occupancy information which is used as\n+  \/\/ input to soft ref clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+\n+  gch->trace_heap_after_gc(_gc_tracer);\n+}\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":741,"deletions":0,"binary":false,"changes":741,"status":"added"},{"patch":"@@ -25,2 +25,2 @@\n-#ifndef SHARE_GC_SERIAL_MARKSWEEP_HPP\n-#define SHARE_GC_SERIAL_MARKSWEEP_HPP\n+#ifndef SHARE_GC_SERIAL_SERIALFULLGC_HPP\n+#define SHARE_GC_SERIAL_SERIALFULLGC_HPP\n@@ -44,1 +44,1 @@\n-\/\/ MarkSweep takes care of global mark-compact garbage collection for a\n+\/\/ Serial full GC takes care of global mark-compact garbage collection for a\n@@ -56,1 +56,1 @@\n-class MarkSweep : AllStatic {\n+class SerialFullGC : AllStatic {\n@@ -95,1 +95,1 @@\n-  \/\/ Total invocations of a MarkSweep collection\n+  \/\/ Total invocations of serial full GC\n@@ -199,1 +199,1 @@\n-#endif \/\/ SHARE_GC_SERIAL_MARKSWEEP_HPP\n+#endif \/\/ SHARE_GC_SERIAL_SERIALFULLGC_HPP\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"previous_filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","status":"renamed"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SERIAL_SERIALFULLGC_INLINE_HPP\n+#define SHARE_GC_SERIAL_SERIALFULLGC_INLINE_HPP\n+\n+#include \"gc\/serial\/serialFullGC.hpp\"\n+\n+#include \"classfile\/classLoaderData.inline.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n+#include \"gc\/serial\/serialStringDedup.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+\n+template <class T> inline void SerialFullGC::adjust_pointer(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    assert(Universe::heap()->is_in(obj), \"should be in heap\");\n+\n+    if (obj->is_forwarded()) {\n+      oop new_obj = obj->forwardee();\n+      assert(is_object_aligned(new_obj), \"oop must be aligned\");\n+      RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+    }\n+  }\n+}\n+\n+template <typename T>\n+void AdjustPointerClosure::do_oop_work(T* p)           { SerialFullGC::adjust_pointer(p); }\n+inline void AdjustPointerClosure::do_oop(oop* p)       { do_oop_work(p); }\n+inline void AdjustPointerClosure::do_oop(narrowOop* p) { do_oop_work(p); }\n+\n+inline size_t SerialFullGC::adjust_pointers(oop obj) {\n+  return obj->oop_iterate_size(&SerialFullGC::adjust_pointer_closure);\n+}\n+\n+#endif \/\/ SHARE_GC_SERIAL_SERIALFULLGC_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.inline.hpp","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"gc\/serial\/markSweep.hpp\"\n+#include \"gc\/serial\/serialFullGC.hpp\"\n@@ -252,1 +252,1 @@\n-  MarkSweep::initialize();\n+  SerialFullGC::initialize();\n@@ -563,1 +563,1 @@\n-    GCTraceCPUTime tcpu(MarkSweep::gc_tracer());\n+    GCTraceCPUTime tcpu(SerialFullGC::gc_tracer());\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  friend class MarkSweep;\n+  friend class SerialFullGC;\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"gc\/serial\/markSweep.hpp\"\n+#include \"gc\/serial\/serialFullGC.hpp\"\n@@ -447,1 +447,1 @@\n-  STWGCTimer* gc_timer = MarkSweep::gc_timer();\n+  STWGCTimer* gc_timer = SerialFullGC::gc_timer();\n@@ -450,1 +450,1 @@\n-  SerialOldTracer* gc_tracer = MarkSweep::gc_tracer();\n+  SerialOldTracer* gc_tracer = SerialFullGC::gc_tracer();\n@@ -455,1 +455,1 @@\n-  MarkSweep::invoke_at_safepoint(clear_all_soft_refs);\n+  SerialFullGC::invoke_at_safepoint(clear_all_soft_refs);\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  friend class MarkSweep;\n+  friend class SerialFullGC;\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -259,1 +259,0 @@\n-\/\/ Used only for markSweep, scavenging\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -87,1 +87,1 @@\n-\/\/  MarkSweep::invoke(0, \"Debugging\");\n+\/\/  Universe::heap()->collect();\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}