{"files":[{"patch":"@@ -6765,0 +6765,21 @@\n+void Assembler::sha512msg1(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sha512() && VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xCC, (0xC0 | encode));\n+}\n+\n+void Assembler::sha512msg2(XMMRegister dst, XMMRegister src) {\n+  assert(VM_Version::supports_sha512() && VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), 0, src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xCD, (0xC0 | encode));\n+}\n+\n+void Assembler::sha512rnds2(XMMRegister dst, XMMRegister nds, XMMRegister src) {\n+  assert(VM_Version::supports_sha512() && VM_Version::supports_avx(), \"\");\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xCB, (0xC0 | encode));\n+}\n+\n@@ -11675,0 +11696,13 @@\n+void Assembler::vbroadcasti128(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(vector_len == AVX_256bit, \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T4, \/* input_size_in_bits *\/ EVEX_32bit);\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x5A);\n+  emit_operand(dst, src, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2342,0 +2342,3 @@\n+  void sha512rnds2(XMMRegister dst, XMMRegister nds, XMMRegister src);\n+  void sha512msg1(XMMRegister dst, XMMRegister src);\n+  void sha512msg2(XMMRegister dst, XMMRegister src);\n@@ -3031,0 +3034,1 @@\n+  void vbroadcasti128(XMMRegister dst, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3485,0 +3485,11 @@\n+void MacroAssembler::vbroadcasti128(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::vbroadcasti128(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vbroadcasti128(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1121,0 +1121,1 @@\n+  void sha512_update_ni_x1(Register arg_hash, Register arg_msg, Register ofs, Register limit, bool multi_block);\n@@ -1219,0 +1220,3 @@\n+  using Assembler::vbroadcasti128;\n+  void vbroadcasti128(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1522,0 +1522,179 @@\n+\/\/Implemented using Intel IpSec implementation (intel-ipsec-mb on github)\n+void MacroAssembler::sha512_update_ni_x1(Register arg_hash, Register arg_msg, Register ofs, Register limit, bool multi_block) {\n+    Label done_hash, block_loop;\n+    address K512_W = StubRoutines::x86::k512_W_addr();\n+\n+    vbroadcasti128(xmm15, ExternalAddress(StubRoutines::x86::pshuffle_byte_flip_mask_addr_sha512()), Assembler::AVX_256bit, r10);\n+\n+    \/\/load current hash value and transform\n+    vmovdqu(xmm0, Address(arg_hash));\n+    vmovdqu(xmm1, Address(arg_hash, 32));\n+    \/\/ymm0 = D C B A, ymm1 = H G F E\n+    vperm2i128(xmm2, xmm0, xmm1, 0x20);\n+    vperm2i128(xmm3, xmm0, xmm1, 0x31);\n+    \/\/ymm2 = F E B A, ymm3 = H G D C\n+    vpermq(xmm13, xmm2, 0x1b, Assembler::AVX_256bit);\n+    vpermq(xmm14, xmm3, 0x1b, Assembler::AVX_256bit);\n+    \/\/ymm13 = A B E F, ymm14 = C D G H\n+\n+    lea(rax, ExternalAddress(K512_W));\n+    align(32);\n+    bind(block_loop);\n+    vmovdqu(xmm11, xmm13);\/\/ABEF\n+    vmovdqu(xmm12, xmm14);\/\/CDGH\n+\n+    \/\/R0 - R3\n+    vmovdqu(xmm0, Address(arg_msg, 0 * 32));\n+    vpshufb(xmm3, xmm0, xmm15, Assembler::AVX_256bit);\/\/ymm0 \/ ymm3 = W[0..3]\n+    vpaddq(xmm0, xmm3, Address(rax, 0 * 32), Assembler::AVX_256bit);\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+\n+    \/\/R4 - R7\n+    vmovdqu(xmm0, Address(arg_msg, 1 * 32));\n+    vpshufb(xmm4, xmm0, xmm15, Assembler::AVX_256bit);\/\/ymm0 \/ ymm4 = W[4..7]\n+    vpaddq(xmm0, xmm4, Address(rax, 1 * 32), Assembler::AVX_256bit);\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+    sha512msg1(xmm3, xmm4); \/\/ymm3 = W[0..3] + S0(W[1..4])\n+\n+    \/\/R8 - R11\n+    vmovdqu(xmm0, Address(arg_msg, 2 * 32));\n+    vpshufb(xmm5, xmm0, xmm15, Assembler::AVX_256bit);\/\/ymm0 \/ ymm5 = W[8..11]\n+    vpaddq(xmm0, xmm5, Address(rax, 2 * 32), Assembler::AVX_256bit);\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+    sha512msg1(xmm4, xmm5);\/\/ymm4 = W[4..7] + S0(W[5..8])\n+\n+    \/\/R12 - R15\n+    vmovdqu(xmm0, Address(arg_msg, 3 * 32));\n+    vpshufb(xmm6, xmm0, xmm15, Assembler::AVX_256bit); \/\/ymm0 \/ ymm6 = W[12..15]\n+    vpaddq(xmm0, xmm6, Address(rax, 3 * 32), Assembler::AVX_256bit);\n+    vpermq(xmm8, xmm6, 0x1b, Assembler::AVX_256bit); \/\/ymm8 = W[12] W[13] W[14] W[15]\n+    vpermq(xmm9, xmm5, 0x39, Assembler::AVX_256bit); \/\/ymm9 = W[8]  W[11] W[10] W[9]\n+    vpblendd(xmm8, xmm8, xmm9, 0x3f, Assembler::AVX_256bit); \/\/ymm8 = W[12] W[11] W[10] W[9]\n+    vpaddq(xmm3, xmm3, xmm8, Assembler::AVX_256bit);\n+    sha512msg2(xmm3, xmm6);\/\/W[16..19] = xmm3 + W[9..12] + S1(W[14..17])\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+    sha512msg1(xmm5, xmm6); \/\/ymm5 = W[8..11] + S0(W[9..12])\n+\n+    \/\/R16 - R19, R32 - R35, R48 - R51\n+    for (int i = 4, j = 3; j > 0; j--) {\n+      vpaddq(xmm0, xmm3, Address(rax, i * 32), Assembler::AVX_256bit);\n+      vpermq(xmm8, xmm3, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[16] W[17] W[18] W[19]\n+      vpermq(xmm9, xmm6, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[12] W[15] W[14] W[13]\n+      vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/xmm7 = W[16] W[15] W[14] W[13]\n+      vpaddq(xmm4, xmm4, xmm7, Assembler::AVX_256bit);\/\/ymm4 = W[4..7] + S0(W[5..8]) + W[13..16]\n+      sha512msg2(xmm4, xmm3);\/\/ymm4 += S1(W[14..17])\n+      sha512rnds2(xmm12, xmm11, xmm0);\n+      vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+      sha512rnds2(xmm11, xmm12, xmm0);\n+      sha512msg1(xmm6, xmm3); \/\/ymm6 = W[12..15] + S0(W[13..16])\n+      i += 1;\n+      \/\/R20 - R23, R36 - R39, R52 - R55\n+      vpaddq(xmm0, xmm4, Address(rax, i * 32), Assembler::AVX_256bit);\n+      vpermq(xmm8, xmm4, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[20] W[21] W[22] W[23]\n+      vpermq(xmm9, xmm3, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[16] W[19] W[18] W[17]\n+      vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/ymm7 = W[20] W[19] W[18] W[17]\n+      vpaddq(xmm5, xmm5, xmm7, Assembler::AVX_256bit);\/\/ymm5 = W[8..11] + S0(W[9..12]) + W[17..20]\n+      sha512msg2(xmm5, xmm4);\/\/ymm5 += S1(W[18..21])\n+      sha512rnds2(xmm12, xmm11, xmm0);\n+      vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+      sha512rnds2(xmm11, xmm12, xmm0);\n+      sha512msg1(xmm3, xmm4); \/\/ymm3 = W[16..19] + S0(W[17..20])\n+      i += 1;\n+      \/\/R24 - R27, R40 - R43, R56 - R59\n+      vpaddq(xmm0, xmm5, Address(rax, i * 32), Assembler::AVX_256bit);\n+      vpermq(xmm8, xmm5, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[24] W[25] W[26] W[27]\n+      vpermq(xmm9, xmm4, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[20] W[23] W[22] W[21]\n+      vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/ymm7 = W[24] W[23] W[22] W[21]\n+      vpaddq(xmm6, xmm6, xmm7, Assembler::AVX_256bit);\/\/ymm6 = W[12..15] + S0(W[13..16]) + W[21..24]\n+      sha512msg2(xmm6, xmm5);\/\/ymm6 += S1(W[22..25])\n+      sha512rnds2(xmm12, xmm11, xmm0);\n+      vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+      sha512rnds2(xmm11, xmm12, xmm0);\n+      sha512msg1(xmm4, xmm5);\/\/ymm4 = W[20..23] + S0(W[21..24])\n+      i += 1;\n+      \/\/R28 - R31, R44 - R47, R60 - R63\n+      vpaddq(xmm0, xmm6, Address(rax, i * 32), Assembler::AVX_256bit);\n+      vpermq(xmm8, xmm6, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[28] W[29] W[30] W[31]\n+      vpermq(xmm9, xmm5, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[24] W[27] W[26] W[25]\n+      vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/ymm7 = W[28] W[27] W[26] W[25]\n+      vpaddq(xmm3, xmm3, xmm7, Assembler::AVX_256bit);\/\/ymm3 = W[16..19] + S0(W[17..20]) + W[25..28]\n+      sha512msg2(xmm3, xmm6); \/\/ymm3 += S1(W[26..29])\n+      sha512rnds2(xmm12, xmm11, xmm0);\n+      vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+      sha512rnds2(xmm11, xmm12, xmm0);\n+      sha512msg1(xmm5, xmm6);\/\/ymm5 = W[24..27] + S0(W[25..28])\n+      i += 1;\n+    }\n+    \/\/R64 - R67\n+    vpaddq(xmm0, xmm3, Address(rax, 16 * 32), Assembler::AVX_256bit);\n+    vpermq(xmm8, xmm3, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[64] W[65] W[66] W[67]\n+    vpermq(xmm9, xmm6, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[60] W[63] W[62] W[61]\n+    vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/ymm7 = W[64] W[63] W[62] W[61]\n+    vpaddq(xmm4, xmm4, xmm7, Assembler::AVX_256bit);\/\/ymm4 = W[52..55] + S0(W[53..56]) + W[61..64]\n+    sha512msg2(xmm4, xmm3);\/\/ymm4 += S1(W[62..65])\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+    sha512msg1(xmm6, xmm3);\/\/ymm6 = W[60..63] + S0(W[61..64])\n+\n+    \/\/R68 - R71\n+    vpaddq(xmm0, xmm4, Address(rax, 17 * 32), Assembler::AVX_256bit);\n+    vpermq(xmm8, xmm4, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[68] W[69] W[70] W[71]\n+    vpermq(xmm9, xmm3, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[64] W[67] W[66] W[65]\n+    vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/ymm7 = W[68] W[67] W[66] W[65]\n+    vpaddq(xmm5, xmm5, xmm7, Assembler::AVX_256bit);\/\/ymm5 = W[56..59] + S0(W[57..60]) + W[65..68]\n+    sha512msg2(xmm5, xmm4);\/\/ymm5 += S1(W[66..69])\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+\n+    \/\/R72 - R75\n+    vpaddq(xmm0, xmm5, Address(rax, 18 * 32), Assembler::AVX_256bit);\n+    vpermq(xmm8, xmm5, 0x1b, Assembler::AVX_256bit);\/\/ymm8 = W[72] W[73] W[74] W[75]\n+    vpermq(xmm9, xmm4, 0x39, Assembler::AVX_256bit);\/\/ymm9 = W[68] W[71] W[70] W[69]\n+    vpblendd(xmm7, xmm8, xmm9, 0x3f, Assembler::AVX_256bit);\/\/ymm7 = W[72] W[71] W[70] W[69]\n+    vpaddq(xmm6, xmm6, xmm7, Assembler::AVX_256bit);\/\/ymm6 = W[60..63] + S0(W[61..64]) + W[69..72]\n+    sha512msg2(xmm6, xmm5);\/\/ymm6 += S1(W[70..73])\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+\n+    \/\/R76 - R79\n+    vpaddq(xmm0, xmm6, Address(rax, 19 * 32), Assembler::AVX_256bit);\n+    sha512rnds2(xmm12, xmm11, xmm0);\n+    vperm2i128(xmm0, xmm0, xmm0, 0x01);\n+    sha512rnds2(xmm11, xmm12, xmm0);\n+\n+    \/\/update hash value\n+    vpaddq(xmm14, xmm14, xmm12, Assembler::AVX_256bit);\n+    vpaddq(xmm13, xmm13, xmm11, Assembler::AVX_256bit);\n+\n+    if (multi_block) {\n+      addptr(arg_msg, 4 * 32);\n+      addptr(ofs, 128);\n+      cmpptr(ofs, limit);\n+      jcc(Assembler::belowEqual, block_loop);\n+      movptr(rax, ofs); \/\/return ofs\n+    }\n+\n+    \/\/store the hash value back in memory\n+    \/\/xmm13 = ABEF\n+    \/\/xmm14 = CDGH\n+    vperm2i128(xmm1, xmm13, xmm14, 0x31);\n+    vperm2i128(xmm2, xmm13, xmm14, 0x20);\n+    vpermq(xmm1, xmm1, 0xb1, Assembler::AVX_256bit);\/\/ymm1 = D C B A\n+    vpermq(xmm2, xmm2, 0xb1, Assembler::AVX_256bit);\/\/ymm2 = H G F E\n+    vmovdqu(Address(arg_hash, 0 * 32), xmm1);\n+    vmovdqu(Address(arg_hash, 1 * 32), xmm2);\n+\n+    bind(done_hash);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_sha.cpp","additions":179,"deletions":0,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -1560,1 +1560,1 @@\n-  assert(VM_Version::supports_bmi2(), \"\");\n+  assert(VM_Version::supports_bmi2() || VM_Version::supports_sha512(), \"\");\n@@ -1570,11 +1570,0 @@\n-  const XMMRegister msg = xmm0;\n-  const XMMRegister state0 = xmm1;\n-  const XMMRegister state1 = xmm2;\n-  const XMMRegister msgtmp0 = xmm3;\n-  const XMMRegister msgtmp1 = xmm4;\n-  const XMMRegister msgtmp2 = xmm5;\n-  const XMMRegister msgtmp3 = xmm6;\n-  const XMMRegister msgtmp4 = xmm7;\n-\n-  const XMMRegister shuf_mask = xmm8;\n-\n@@ -1583,3 +1572,16 @@\n-  __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-  buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-\n+  if (VM_Version::supports_sha512()) {\n+      __ sha512_update_ni_x1(state, buf, ofs, limit, multi_block);\n+  } else {\n+    const XMMRegister msg = xmm0;\n+    const XMMRegister state0 = xmm1;\n+    const XMMRegister state1 = xmm2;\n+    const XMMRegister msgtmp0 = xmm3;\n+    const XMMRegister msgtmp1 = xmm4;\n+    const XMMRegister msgtmp2 = xmm5;\n+    const XMMRegister msgtmp3 = xmm6;\n+    const XMMRegister msgtmp4 = xmm7;\n+\n+    const XMMRegister shuf_mask = xmm8;\n+    __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":17,"deletions":15,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -1061,0 +1061,1 @@\n+    _features &= ~CPU_SHA512;\n@@ -1305,1 +1306,1 @@\n-  if (UseSHA && supports_avx2() && supports_bmi2()) {\n+  if (UseSHA && supports_avx2() && (supports_bmi2() || supports_sha512())) {\n@@ -3008,0 +3009,2 @@\n+    if (sefsl1_cpuid7_eax.bits.sha512 != 0)\n+      result |= CPU_SHA512;\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -286,1 +286,2 @@\n-      uint32_t             : 23,\n+      uint32_t    sha512   : 1,\n+                           : 22,\n@@ -418,1 +419,2 @@\n-    decl(APX_F,             \"apx_f\",             60) \/* Intel Advanced Performance Extensions*\/\n+    decl(APX_F,             \"apx_f\",             60) \/* Intel Advanced Performance Extensions*\/\\\n+    decl(SHA512,            \"sha512\",            61) \/* SHA512 instructions*\/\n@@ -760,0 +762,1 @@\n+  static bool supports_sha512()       { return (_features & CPU_SHA512) != 0; }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -259,0 +259,1 @@\n+        SHA512,\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/amd64\/AMD64.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -106,1 +106,1 @@\n-            checkLongValue(\"VM_Version::CPU_SHA\",\n+            checkLongValue(\"VM_Version::CPU_SHA \",\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbLongConstant.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}