{"files":[{"patch":"@@ -1001,1 +1001,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,5 +36,14 @@\n-ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n-  _heap(heap),\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+static const char* partition_name(ShenandoahFreeSetPartitionId t) {\n+  switch (t) {\n+    case NotFree: return \"NotFree\";\n+    case Mutator: return \"Mutator\";\n+    case Collector: return \"Collector\";\n+    default: return \"Unrecognized\";\n+  }\n+}\n+\n+ShenandoahRegionPartition::ShenandoahRegionPartition(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes()),\n+    _free_set(free_set),\n+    _membership(NEW_C_HEAP_ARRAY(ShenandoahFreeSetPartitionId, max_regions, mtGC))\n@@ -41,1 +51,1 @@\n-  clear_internal();\n+  make_all_regions_unavailable();\n@@ -44,3 +54,204 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n-  shenandoah_assert_heaplocked();\n-  _used += num_bytes;\n+ShenandoahRegionPartition::~ShenandoahRegionPartition() {\n+  FREE_C_HEAP_ARRAY(ShenandoahFreeSetPartitionId, _membership);\n+}\n+\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+inline bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n+  }\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+inline bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+void ShenandoahRegionPartition::make_all_regions_unavailable() {\n+  for (size_t idx = 0; idx < _max; idx++) {\n+    _membership[idx] = NotFree;\n+  }\n+\n+  for (size_t partition_id = 0; partition_id < NumPartitions; partition_id++) {\n+    _leftmosts[partition_id] = _max;\n+    _rightmosts[partition_id] = 0;\n+    _leftmosts_empty[partition_id] = _max;\n+    _rightmosts_empty[partition_id] = 0;\n+    _capacity[partition_id] = 0;\n+    _used[partition_id] = 0;\n+  }\n+\n+  _region_counts[Mutator] = _region_counts[Collector] = 0;\n+  _region_counts[NotFree] = _max;\n+}\n+\n+void ShenandoahRegionPartition::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"Partition must be valid\");\n+  _used[which_partition] += bytes;\n+  assert (_used[which_partition] <= _capacity[which_partition],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used[which_partition], _capacity[which_partition], bytes);\n+}\n+\n+inline void ShenandoahRegionPartition::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, size_t idx) {\n+  if (idx == _leftmosts[partition]) {\n+    while ((_leftmosts[partition] < _max) && !in_partition(_leftmosts[partition], partition)) {\n+      _leftmosts[partition]++;\n+    }\n+    if (_leftmosts_empty[partition] < _leftmosts[partition]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[partition] = _leftmosts[partition];\n+    }\n+  }\n+  if (idx == _rightmosts[partition]) {\n+    while (_rightmosts[partition] > 0 && !in_partition(_rightmosts[partition], partition)) {\n+      _rightmosts[partition]--;\n+    }\n+    if (_rightmosts_empty[partition] > _rightmosts[partition]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[partition] = _rightmosts[partition];\n+    }\n+  }\n+}\n+\n+inline void ShenandoahRegionPartition::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                           size_t idx, size_t region_available) {\n+  if (region_available == _region_size_bytes) {\n+    if (_leftmosts_empty[partition] > idx) {\n+      _leftmosts_empty[partition] = idx;\n+    }\n+    if (_rightmosts_empty[partition] < idx) {\n+      _rightmosts_empty[partition] = idx;\n+    }\n+  }\n+  if (_leftmosts[partition] > idx) {\n+    _leftmosts[partition] = idx;\n+  }\n+  if (_rightmosts[partition] < idx) {\n+    _rightmosts[partition] = idx;\n+  }\n+}\n+\n+\/\/ Remove this region from its free partition, but leave its capacity and used as part of the original free partition's totals.\n+\/\/ When retiring a region, add any remnant of available memory within the region to the used total for the original free partition.\n+void ShenandoahRegionPartition::retire_within_partition(size_t idx, size_t used_bytes) {\n+  ShenandoahFreeSetPartitionId orig_partition = membership(idx);\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (orig_partition > NotFree && orig_partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+\n+  if (used_bytes < _region_size_bytes) {\n+    \/\/ Count the alignment pad remnant of memory as used when we retire this region\n+    increase_used(orig_partition, _region_size_bytes - used_bytes);\n+  }\n+\n+  _membership[idx] = NotFree;\n+  shrink_interval_if_boundary_modified(orig_partition, idx);\n+\n+  _region_counts[orig_partition]--;\n+  _region_counts[NotFree]++;\n+}\n+\n+void ShenandoahRegionPartition::make_free(size_t idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  _membership[idx] = which_partition;\n+  _capacity[which_partition] += _region_size_bytes;\n+  _used[which_partition] += _region_size_bytes - available;\n+  expand_interval_if_boundary_modified(which_partition, idx, available);\n+\n+  _region_counts[NotFree]--;\n+  _region_counts[which_partition]++;\n+}\n+\n+void ShenandoahRegionPartition::move_to_partition(size_t idx, ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert ((new_partition > NotFree) && (new_partition < NumPartitions), \"New partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  ShenandoahFreeSetPartitionId orig_partition = _membership[idx];\n+  assert ((orig_partition > NotFree) && (orig_partition < NumPartitions), \"Cannot move free unless already free\");\n+\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild:         Mutator => Collector\n+  \/\/  During flip_to_gc:      Mutator empty => Collector\n+  \/\/ At start of update refs: Collector => Mutator\n+  assert (((available <= _region_size_bytes) &&\n+           (((orig_partition == Mutator) && (new_partition == Collector)) ||\n+            ((orig_partition == Collector) && (new_partition == Mutator)))) ||\n+          ((available == _region_size_bytes) &&\n+           ((orig_partition == Mutator) && (new_partition == Collector))), \"Unexpected movement between partitions\");\n+\n+\n+  size_t used = _region_size_bytes - available;\n+  _membership[idx] = new_partition;\n+  _capacity[orig_partition] -= _region_size_bytes;\n+  _used[orig_partition] -= used;\n+  shrink_interval_if_boundary_modified(orig_partition, idx);\n+\n+  _capacity[new_partition] += _region_size_bytes;;\n+  _used[new_partition] += used;\n+  expand_interval_if_boundary_modified(new_partition, idx, available);\n+\n+  _region_counts[orig_partition]--;\n+  _region_counts[new_partition]++;\n+}\n+\n+inline ShenandoahFreeSetPartitionId ShenandoahRegionPartition::membership(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  return _membership[idx];\n+}\n+\n+  \/\/ Returns true iff region idx is in the test_partition free_partition.  Before returning true, asserts that the free\n+  \/\/ partition is not empty.  Requires that test_partition != NotFree or NumPartitions.\n+inline bool ShenandoahRegionPartition::in_partition(size_t idx, ShenandoahFreeSetPartitionId test_partition) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  if (_membership[idx] == test_partition) {\n+    assert (test_partition == NotFree || _free_set->alloc_capacity(idx) > 0,\n+            \"Free region \" SIZE_FORMAT \", belonging to %s free partition, must have alloc capacity\",\n+            idx, partition_name(test_partition));\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+inline size_t ShenandoahRegionPartition::leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free partition must be valid\");\n+  size_t idx = _leftmosts[which_partition];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    assert (in_partition(idx, which_partition), \"left-most region must be free\");\n+    return idx;\n+  }\n+}\n+\n+inline size_t ShenandoahRegionPartition::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free partition must be valid\");\n+  size_t idx = _rightmosts[which_partition];\n+  assert ((_leftmosts[which_partition] == _max) || in_partition(idx, which_partition), \"right-most region must be free\");\n+  return idx;\n+}\n@@ -48,2 +259,3 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+inline bool ShenandoahRegionPartition::is_empty(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free partition must be valid\");\n+  return (leftmost(which_partition) > rightmost(which_partition));\n@@ -52,4 +264,123 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+size_t ShenandoahRegionPartition::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free partition must be valid\");\n+  for (size_t idx = _leftmosts_empty[which_partition]; idx < _max; idx++) {\n+    if ((membership(idx) == which_partition) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _leftmosts_empty[which_partition] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_partition] = _max;\n+  _rightmosts_empty[which_partition] = 0;\n+  return _max;\n+}\n+\n+inline size_t ShenandoahRegionPartition::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free partition must be valid\");\n+  for (intptr_t idx = _rightmosts_empty[which_partition]; idx >= 0; idx--) {\n+    if ((membership(idx) == which_partition) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _rightmosts_empty[which_partition] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_partition] = _max;\n+  _rightmosts_empty[which_partition] = 0;\n+  return 0;\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahRegionPartition::assert_bounds() {\n+\n+  size_t leftmosts[NumPartitions];\n+  size_t rightmosts[NumPartitions];\n+  size_t empty_leftmosts[NumPartitions];\n+  size_t empty_rightmosts[NumPartitions];\n+\n+  for (int i = 0; i < NumPartitions; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = 0;\n+    empty_rightmosts[i] = 0;\n+  }\n+\n+  for (size_t i = 0; i < _max; i++) {\n+    ShenandoahFreeSetPartitionId partition = membership(i);\n+    switch (partition) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < leftmosts[partition]) {\n+          leftmosts[partition] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[partition])) {\n+          empty_leftmosts[partition] = i;\n+        }\n+        if (i > rightmosts[partition]) {\n+          rightmosts[partition] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[partition])) {\n+          empty_rightmosts[partition] = i;\n+        }\n+        break;\n+      }\n+\n+      case NumPartitions:\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Mutator),  _max);\n+  assert (rightmost(Mutator) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Mutator),  _max);\n+\n+  assert (leftmost(Mutator) == _max || in_partition(leftmost(Mutator), Mutator),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Mutator));\n+  assert (leftmost(Mutator) == _max || in_partition(rightmost(Mutator), Mutator),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Mutator));\n+\n+  \/\/ If Mutator partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  size_t beg_off = leftmosts[Mutator];\n+  size_t end_off = rightmosts[Mutator];\n+  assert (beg_off >= leftmost(Mutator),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Mutator));\n+  assert (end_off <= rightmost(Mutator),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Mutator));\n+\n+  beg_off = empty_leftmosts[Mutator];\n+  end_off = empty_rightmosts[Mutator];\n+  assert (beg_off >= leftmost_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Mutator));\n+  assert (end_off <= rightmost_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Collector),  _max);\n+  assert (rightmost(Collector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Collector),  _max);\n+\n+  assert (leftmost(Collector) == _max || in_partition(leftmost(Collector), Collector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Collector));\n+  assert (leftmost(Collector) == _max || in_partition(rightmost(Collector), Collector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Collector));\n+\n+  \/\/ If Collector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[Collector];\n+  end_off = rightmosts[Collector];\n+  assert (beg_off >= leftmost(Collector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Collector));\n+  assert (end_off <= rightmost(Collector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Collector));\n+\n+  beg_off = empty_leftmosts[Collector];\n+  end_off = empty_rightmosts[Collector];\n+  assert (beg_off >= leftmost_empty(Collector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Collector));\n+  assert (end_off <= rightmost_empty(Collector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Collector));\n@@ -57,0 +388,1 @@\n+#endif\n@@ -58,4 +390,5 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n+  _heap(heap),\n+  _partitions(max_regions, this)\n+{\n+  clear_internal();\n@@ -65,0 +398,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -67,2 +402,1 @@\n-  \/\/ Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,\n-  \/\/ we would find the region to allocate at right away.\n+  \/\/ Leftmost and rightmost bounds provide enough caching to quickly find a region from which to allocate.\n@@ -70,3 +404,3 @@\n-  \/\/ Allocations are biased: new application allocs go to beginning of the heap, and GC allocs\n-  \/\/ go to the end. This makes application allocation faster, because we would clear lots\n-  \/\/ of regions from the beginning most of the time.\n+  \/\/ Allocations are biased: GC allocations are taken from the high end of the heap.  Regular (and TLAB)\n+  \/\/ mutator allocations are taken from the middle of heap, below the memory reserved for Collector.\n+  \/\/ Humongous mutator allocations are taken from the bottom of the heap.\n@@ -74,2 +408,3 @@\n-  \/\/ Free set maintains mutator and collector views, and normally they allocate in their views only,\n-  \/\/ unless we special cases for stealing and mixed allocations.\n+  \/\/ Free set maintains mutator and collector partitions.  Mutator can only allocate from the\n+  \/\/ Mutator partition.  Collector prefers to allocate from the Collector partition, but may steal\n+  \/\/ regions from the Mutator partition if the Collector partition has been depleted.\n@@ -80,1 +415,0 @@\n-\n@@ -82,5 +416,13 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n+      \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+      if (!_partitions.is_empty(Mutator)) {\n+        \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+        int leftmost = (int) _partitions.leftmost(Mutator);\n+        for (int idx = (int) _partitions.rightmost(Mutator); idx >= leftmost; idx--) {\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (_partitions.in_partition(idx, Mutator)) {\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n@@ -90,1 +432,0 @@\n-\n@@ -95,0 +436,2 @@\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.\n+\n@@ -97,1 +440,0 @@\n-\n@@ -99,1 +441,1 @@\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+      for (size_t c = _partitions.rightmost(Collector) + 1; c > _partitions.leftmost(Collector); c--) {\n@@ -101,1 +443,1 @@\n-        if (is_collector_free(idx)) {\n+        if (_partitions.in_partition(idx, Collector)) {\n@@ -114,2 +456,2 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n+      \/\/ Try to steal an empty region from the mutator view.\n+      for (size_t c = _partitions.rightmost_empty(Mutator) + 1; c > _partitions.leftmost_empty(Mutator); c--) {\n@@ -117,1 +459,1 @@\n-        if (is_mutator_free(idx)) {\n+        if (_partitions.in_partition(idx, Mutator)) {\n@@ -123,0 +465,1 @@\n+              log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n@@ -129,4 +472,2 @@\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n-      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n-      \/\/ allocations.\n-\n+      \/\/ No dice. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+      \/\/ due to GC allocations would expose unparsable mutator allocations.\n@@ -138,1 +479,0 @@\n-\n@@ -143,4 +483,2 @@\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n-\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n@@ -150,0 +488,1 @@\n+  HeapWord* result = nullptr;\n@@ -151,1 +490,0 @@\n-\n@@ -154,2 +492,4 @@\n-  HeapWord* result = nullptr;\n-  size_t size = req.size();\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  }\n@@ -157,0 +497,1 @@\n+  \/\/ req.size() is in words, r->free() is in bytes.\n@@ -158,0 +499,2 @@\n+    \/\/ This is a GCLAB or a TLAB allocation\n+    size_t adjusted_size = req.size();\n@@ -159,2 +502,2 @@\n-    if (size > free) {\n-      size = free;\n+    if (adjusted_size > free) {\n+      adjusted_size = free;\n@@ -162,3 +505,11 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+    if (adjusted_size >= req.min_size()) {\n+      result = r->allocate(adjusted_size, req.type());\n+      log_debug(gc, free)(\"Allocated \" SIZE_FORMAT \" words (adjusted from \" SIZE_FORMAT \") for %s @\" PTR_FORMAT\n+                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n+                          adjusted_size, req.size(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n+                          partition_name(_partitions.membership(r->index())),  r->index(), r->free());\n+      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+      req.set_actual_size(adjusted_size);\n+    } else {\n+      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                          \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n@@ -167,0 +518,1 @@\n+    size_t size = req.size();\n@@ -168,0 +520,8 @@\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      log_debug(gc, free)(\"Allocated \" SIZE_FORMAT \" words for %s @\" PTR_FORMAT\n+                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n+                          size, ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n+                          partition_name(_partitions.membership(r->index())),  r->index(), r->free());\n+      req.set_actual_size(size);\n+    }\n@@ -173,5 +533,3 @@\n-      increase_used(size * HeapWordSize);\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n+      _partitions.increase_used(Mutator, req.actual_size() * HeapWordSize);\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n@@ -179,1 +537,2 @@\n-    if (req.is_gc_alloc()) {\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.\n@@ -184,7 +543,3 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n-    \/\/\n-    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+  if (alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n+    \/\/ Regardless of whether this allocation succeeded, if the remaining memory is less than PLAB:min_size(), retire this region.\n+    \/\/ Note that retire_within_partition() increases used to account for waste.\n@@ -192,8 +547,4 @@\n-    \/\/ Record the remainder as allocation waste\n-    if (req.is_mutator_alloc()) {\n-      size_t waste = r->free();\n-      if (waste > 0) {\n-        increase_used(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n-      }\n-    }\n+    \/\/ Note that a previous implementation of this function would retire a region following any failure to\n+    \/\/ allocate within.  This was observed to result in large amounts of available memory being ignored\n+    \/\/ following a failed shared allocation request.  TLAB requests will generally downsize to absorb all\n+    \/\/ memory available within the region even if this is less than the desired size.\n@@ -201,8 +552,3 @@\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n-    }\n-    assert_bounds();\n+    size_t idx = r->index();\n+    _partitions.retire_within_partition(idx, r->used());\n+    _partitions.assert_bounds();\n@@ -213,32 +559,0 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n-}\n-\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n-\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n-  }\n-}\n-\n@@ -251,2 +565,2 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (num > _partitions.count(Mutator)) {\n@@ -259,1 +573,1 @@\n-  size_t beg = _mutator_leftmost;\n+  size_t beg = _partitions.leftmost(Mutator);\n@@ -263,1 +577,1 @@\n-    if (end >= _max) {\n+    if (end >= _partitions.max()) {\n@@ -270,1 +584,1 @@\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!_partitions.in_partition(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n@@ -308,0 +622,1 @@\n+    r->set_update_watermark(r->bottom());\n@@ -310,15 +625,2 @@\n-    _mutator_free_bitmap.clear_bit(r->index());\n-  }\n-\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-\n-  if (remainder != 0) {\n-    \/\/ Record this remainder as allocation waste\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n-  }\n-\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n+    \/\/ While individual regions report their true use, all humongous regions are marked used in the free partition.\n+    _partitions.retire_within_partition(r->index(), ShenandoahHeapRegion::region_size_bytes());\n@@ -326,1 +628,0 @@\n-  assert_bounds();\n@@ -328,0 +629,3 @@\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _partitions.increase_used(Mutator, total_humongous_size);\n+  _partitions.assert_bounds();\n@@ -332,17 +636,0 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n-  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n-}\n-\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n-  if (r->is_trash()) {\n-    \/\/ This would be recycled on allocation path\n-    return ShenandoahHeapRegion::region_size_bytes();\n-  } else {\n-    return r->free();\n-  }\n-}\n-\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n-  return alloc_capacity(r) == 0;\n-}\n-\n@@ -373,1 +660,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_partitions.in_partition(idx, Mutator), \"Should be in mutator view\");\n@@ -376,6 +663,3 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n-\n-  _capacity -= alloc_capacity(r);\n+  size_t region_capacity = alloc_capacity(r);\n+  _partitions.move_to_partition(idx, Collector, region_capacity);\n+  _partitions.assert_bounds();\n@@ -383,4 +667,2 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -395,8 +677,1 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _partitions.make_all_regions_unavailable();\n@@ -405,4 +680,5 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n-\n+\/\/ This function places all regions that have allocation capacity into the mutator_partition.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector partition with the intent of packing collector memory into the\n+\/\/  highest (rightmost) addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &cset_regions) {\n+  cset_regions = 0;\n@@ -411,0 +687,5 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n+      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      cset_regions++;\n+    }\n@@ -412,1 +693,2 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free partition\");\n+      assert(_partitions.in_partition(idx, NotFree), \"We are about to make region free; it should not be free already\");\n@@ -414,2 +696,18 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      size_t ac = alloc_capacity(region);\n+      if (ac > PLAB::min_size() * HeapWordSize) {\n+        _partitions.make_free(idx, Mutator, ac);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator partition\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      } else {\n+        assert(_partitions.membership(idx) == NotFree,\n+               \"Region \" SIZE_FORMAT \" should not be in free partition because capacity is \" SIZE_FORMAT, idx, ac);\n+      }\n+    } else {\n+      assert(_partitions.membership(idx) == NotFree,\n+             \"Region \" SIZE_FORMAT \" should not be in free partition because alloc is not allowed and not is trash\", idx);\n+    }\n+  }\n+}\n@@ -417,2 +715,22 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+\/\/ Move no more than max_xfer_regions from the existing Collector free partitions to the Mutator free partition.\n+\/\/ This is called from outside the heap lock at the start of update refs.  At this point, we no longer\n+\/\/ need to reserve memory within for evacuation.  (We will create a new reserve after update refs finishes,\n+\/\/ setting aside some of the memory that was reclaimed by the most recent GC.  This new reserve will satisfy\n+\/\/ the evacuation needs of the next GC pass.)\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator_partition(size_t max_xfer_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t collector_empty_xfer = 0;\n+  size_t collector_not_empty_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost_empty(Collector) <= _partitions.rightmost_empty(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _partitions.leftmost_empty(Collector);\n+         (max_xfer_regions > 0) && (idx <= _partitions.rightmost_empty(Collector)); idx++) {\n+      if (_partitions.in_partition(idx, Collector) && can_allocate_from(idx)) {\n+        _partitions.move_to_partition(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        collector_empty_xfer += region_size_bytes;\n+      }\n+    }\n+  }\n@@ -420,2 +738,11 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+  \/\/ If there are any non-empty regions within Collector partition, we can also move them to the Mutator free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost(Collector) <= _partitions.rightmost(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _partitions.leftmost(Collector);\n+         (max_xfer_regions > 0) && (idx <= _partitions.rightmost(Collector)); idx++) {\n+      size_t alloc_capacity = this->alloc_capacity(idx);\n+      if (_partitions.in_partition(idx, Collector) && (alloc_capacity > 0)) {\n+        _partitions.move_to_partition(idx, Mutator, alloc_capacity);\n+        max_xfer_regions--;\n+        collector_not_empty_xfer += alloc_capacity;\n+      }\n@@ -425,3 +752,4 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n-  size_t reserved = 0;\n+  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n+  log_info(gc, free)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free partition from Collector Reserve\",\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer));\n+}\n@@ -429,2 +757,0 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n@@ -432,7 +758,64 @@\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n+\/\/ Overwrite arguments to represent the number of regions to be reclaimed from the cset\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &cset_regions) {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all partitions.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the mutator partition.\n+  find_regions_with_alloc_capacity(cset_regions);\n+}\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t cset_regions) {\n+  shenandoah_assert_heaplocked();\n+\n+  \/\/ Our desire is to reserve this much memory for future evacuation.  We may end up reserving less, if\n+  \/\/ memory is in short supply.\n+\n+  size_t reserve = _heap->max_capacity() * ShenandoahEvacReserve \/ 100;\n+  size_t available_in_collector_partition = _partitions.capacity_of(Collector) - _partitions.used_by(Collector);\n+  size_t additional_reserve;\n+  if (available_in_collector_partition < reserve) {\n+    additional_reserve = reserve - available_in_collector_partition;\n+  } else {\n+    additional_reserve = 0;\n+  }\n+\n+  reserve_regions(reserve);\n+  _partitions.assert_bounds();\n+  log_status();\n+}\n+\n+void ShenandoahFreeSet::rebuild() {\n+  size_t cset_regions;\n+  prepare_to_rebuild(cset_regions);\n+  finish_rebuild(cset_regions);\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator partition, move some of these regions from\n+\/\/ the mutator partition into the collector partition in order to assure that the memory available for allocations within\n+\/\/ the collector partition is at least to_reserve.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (!_partitions.in_partition(idx, Mutator)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free partition implies has capacity\");\n+\n+    bool move_to_collector = _partitions.capacity_of(Collector) < to_reserve;\n+    if (!move_to_collector) {\n+      \/\/ We've satisfied to_reserve\n+      break;\n+    }\n+\n+    if (move_to_collector) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n+      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+      \/\/ ephemeral objects.\n+      _partitions.move_to_partition(idx, Collector, ac);\n+      log_debug(gc,free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n@@ -442,2 +825,7 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  if (LogTarget(Info, gc, free)::is_enabled()) {\n+    size_t reserve = _partitions.capacity_of(Collector);\n+    if (reserve < to_reserve) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+    }\n+  }\n@@ -449,1 +837,54 @@\n-  LogTarget(Info, gc, ergo) lt;\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  if (LogTarget(Debug, gc, free)::is_enabled()) {\n+#define BUFFER_SIZE 80\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t consumed_collector = 0;\n+    size_t available_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_mutator = 0;\n+\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_debug(gc, free)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free H:humongous _:retired\");\n+    log_debug(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                        \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+                        _partitions.leftmost(Mutator), _partitions.rightmost(Mutator),\n+                        _partitions.leftmost(Collector), _partitions.rightmost(Collector));\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_debug(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_partitions.in_partition(i, Mutator)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_mutator += capacity;\n+        consumed_mutator += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'M': 'm';\n+      } else if (_partitions.in_partition(i, Collector)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_collector += capacity;\n+        consumed_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'C': 'c';\n+      } else if (r->is_humongous()) {\n+        buffer[idx] = 'h';\n+      } else {\n+        buffer[idx] = '_';\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_debug(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+  }\n+#endif\n+\n+  LogTarget(Info, gc, free) lt;\n@@ -464,2 +905,2 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (size_t idx = _partitions.leftmost(Mutator); idx <= _partitions.rightmost(Mutator); idx++) {\n+        if (_partitions.in_partition(idx, Mutator)) {\n@@ -468,1 +909,0 @@\n-\n@@ -470,1 +910,0 @@\n-\n@@ -481,1 +920,0 @@\n-\n@@ -484,1 +922,0 @@\n-\n@@ -493,0 +930,5 @@\n+      \/\/ Since certain regions that belonged to the Mutator free partition at the time of most recent rebuild may have been\n+      \/\/ retired, the sum of used and capacities within regions that are still in the Mutator free partition may not match\n+      \/\/ my internally tracked values of used() and free().\n+      assert(free == total_free, \"Free memory should match\");\n+\n@@ -494,1 +936,1 @@\n-               byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(free),          proper_unit_for_byte_size(free),\n@@ -509,2 +951,2 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_partitions.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _partitions.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -515,0 +957,2 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _partitions.count(Mutator));\n@@ -520,0 +964,1 @@\n+      size_t total_used = 0;\n@@ -521,2 +966,2 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (size_t idx = _partitions.leftmost(Collector); idx <= _partitions.rightmost(Collector); idx++) {\n+        if (_partitions.in_partition(idx, Collector)) {\n@@ -527,0 +972,1 @@\n+          total_used += r->used();\n@@ -529,4 +975,4 @@\n-\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n-                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -539,1 +985,0 @@\n-  assert_bounds();\n@@ -541,0 +986,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -563,1 +1009,1 @@\n-  \/\/ Deliberately not locked, this method is unsafe when free set is modified.\n+  \/\/ Deliberately not locked, this method is unsafe when free partition is modified.\n@@ -565,2 +1011,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n+  for (size_t index = _partitions.leftmost(Mutator); index <= _partitions.rightmost(Mutator); index++) {\n+    if (index < _partitions.max() && _partitions.in_partition(index, Mutator)) {\n@@ -579,3 +1025,3 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _partitions.count(Mutator));\n+  for (size_t index = _partitions.leftmost(Mutator); index <= _partitions.rightmost(Mutator); index++) {\n+    if (_partitions.in_partition(index, Mutator)) {\n@@ -585,3 +1031,3 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(Collector));\n+  for (size_t index = _partitions.leftmost(Collector); index <= _partitions.rightmost(Collector); index++) {\n+    if (_partitions.in_partition(index, Collector)) {\n@@ -619,2 +1065,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _partitions.leftmost(Mutator); index <= _partitions.rightmost(Mutator); index++) {\n+    if (_partitions.in_partition(index, Mutator)) {\n@@ -657,2 +1103,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _partitions.leftmost(Mutator); index <= _partitions.rightmost(Mutator); index++) {\n+    if (_partitions.in_partition(index, Mutator)) {\n@@ -683,27 +1129,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":675,"deletions":256,"binary":false,"changes":931,"status":"modified"},{"patch":"@@ -0,0 +1,1 @@\n+\n@@ -3,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,1 +33,14 @@\n-class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+\/\/ Each ShenandoahHeapRegion is associated with a ShenandoahFreeSetPartitionId.\n+enum ShenandoahFreeSetPartitionId : uint8_t {\n+  NotFree,                      \/\/ Region has been retired and is not in any free set: there is no available memory.\n+  Mutator,                      \/\/ Region is in the Mutator free set: available memory is available to mutators.\n+  Collector,                    \/\/ Region is in the Collector free set: available memory is reserved for evacuations.\n+\n+  NumPartitions                 \/\/ This value represents the size of an array that may be indexed by NotFree, Mutator, Collector.\n+};\n+\n+\n+\/\/ This class implements partitioning of regions into distinct sets.  Each ShenandoahHeapRegion is either in the Mutator free set,\n+\/\/ the Collector free set, or in neither free set (NotFree).\n+class ShenandoahRegionPartition {\n+\n@@ -33,4 +48,80 @@\n-  ShenandoahHeap* const _heap;\n-  CHeapBitMap _mutator_free_bitmap;\n-  CHeapBitMap _collector_free_bitmap;\n-  size_t _max;\n+  const size_t _max;            \/\/ The maximum number of heap regions\n+  const size_t _region_size_bytes;\n+  const ShenandoahFreeSet* _free_set;\n+  ShenandoahFreeSetPartitionId* const _membership;\n+\n+  \/\/ For each type, we track an interval outside of which a region affiliated with that partition is guaranteed\n+  \/\/ not to be found. This makes searches for free space more efficient.  For each partition p, _leftmosts[p]\n+  \/\/ represents its least index, and its _rightmosts[p] its greatest index. Empty intervals are indicated by the\n+  \/\/ canonical [_max, 0].\n+  size_t _leftmosts[NumPartitions];\n+  size_t _rightmosts[NumPartitions];\n+\n+  \/\/ Allocation for humongous objects needs to find regions that are entirely empty.  For each partion p, _leftmosts[p]\n+  \/\/ represents the first region belonging to this partition that is completely empty and _rightmosts[p] represents the\n+  \/\/ last region that is completely empty.  If there are no completely empty regions in this partition, this is represented\n+  \/\/ by canonical [_max, 0].\n+  size_t _leftmosts_empty[NumPartitions];\n+  size_t _rightmosts_empty[NumPartitions];\n+\n+  \/\/ For each partition p, _capacity[p] represents the total amount of memory within the partition at the time\n+  \/\/ of the most recent rebuild, _used[p] represents the total amount of memory that has been consumed within this\n+  \/\/ partition (either already consumed as of the rebuild, or consumed since the rebuild).  _capacity[p] and _used[p]\n+  \/\/ are denoted in bytes.  Note that some regions that had been assigned to a particular partition at rebuild time\n+  \/\/ may have been retired following the rebuild.  The tallies for these regions are still reflected in _capacity[p]\n+  \/\/ and _used[p], even though the region may have been removed from the free set.\n+  size_t _capacity[NumPartitions];\n+  size_t _used[NumPartitions];\n+  size_t _region_counts[NumPartitions];\n+\n+  inline void shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, size_t idx);\n+  inline void expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, size_t idx, size_t capacity);\n+\n+public:\n+  ShenandoahRegionPartition(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahRegionPartition();\n+\n+  \/\/ Make all regions NotFree and reset all bounds\n+  void make_all_regions_unavailable();\n+\n+  \/\/ Retire region idx from within its partition.  Requires that region idx is in in Mutator or Collector partitions.\n+  \/\/ Moves this region to the NotFree partition.  Any remnant of available memory at the time of retirement is added to the\n+  \/\/ original partition's total of used bytes.\n+  void retire_within_partition(size_t idx, size_t used_bytes);\n+\n+  \/\/ Place region idx into free set which_partition.  Requires that idx is currently NotFree.\n+  void make_free(size_t idx, ShenandoahFreeSetPartitionId which_partition, size_t region_capacity);\n+\n+  \/\/ Place region idx into free partition new_partition.  Requires that idx is currently not NotFree.\n+  void move_to_partition(size_t idx, ShenandoahFreeSetPartitionId new_partition, size_t region_capacity);\n+\n+  \/\/ Returns the ShenandoahFreeSetPartitionId affiliation of region idx, NotFree if this region is not currently free.\n+  \/\/ This does not enforce that free_set membership implies allocation capacity.\n+  inline ShenandoahFreeSetPartitionId membership(size_t idx) const;\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumPartitions.\n+  inline bool in_partition(size_t idx, ShenandoahFreeSetPartitionId which_partition) const;\n+\n+  \/\/ The following four methods return the left-most and right-most bounds on ranges of regions representing\n+  \/\/ the requested set.  The _empty variants represent bounds on the range that holds completely empty\n+  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.  A\n+  \/\/ return value of -1 from leftmost() or leftmost_empty() denotes that the corresponding set is empty.\n+  \/\/ In other words:\n+  \/\/   if the requested which_partition is empty:\n+  \/\/     leftmost() and leftmost_empty() return _max, rightmost() and rightmost_empty() return 0\n+  \/\/   otherwise, expect the following:\n+  \/\/     0 <= leftmost <= leftmost_empty <= rightmost_empty <= rightmost < _max\n+  inline size_t leftmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  inline size_t rightmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  size_t leftmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+  size_t rightmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+\n+  inline bool is_empty(ShenandoahFreeSetPartitionId which_partition) const;\n+\n+  inline void increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+\n+  inline size_t capacity_of(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[which_partition];\n+  }\n@@ -38,4 +129,9 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n-  size_t _mutator_leftmost, _mutator_rightmost;\n-  size_t _collector_leftmost, _collector_rightmost;\n+  inline size_t used_by(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _used[which_partition];\n+  }\n+\n+  inline void set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free set must be valid\");\n+    _capacity[which_partition] = value;\n+  }\n@@ -43,2 +139,4 @@\n-  size_t _capacity;\n-  size_t _used;\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition > NotFree && which_partition < NumPartitions, \"selected free set must be valid\");\n+    _used[which_partition] = value;\n+  }\n@@ -46,1 +144,26 @@\n-  void assert_bounds() const NOT_DEBUG_RETURN;\n+  inline size_t max() const { return _max; }\n+\n+  inline size_t count(ShenandoahFreeSetPartitionId which_partition) const { return _region_counts[which_partition]; }\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n+  \/\/ Valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and rightmost_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= lefmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n@@ -48,2 +171,4 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+private:\n+  ShenandoahHeap* const _heap;\n+  ShenandoahRegionPartition _partitions;\n@@ -52,0 +177,5 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object or LAB  which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.\n+  \/\/\n+  \/\/ Precondition: req.size() <= ShenandoahHeapRegion::humongous_threshold_words().\n@@ -53,0 +183,5 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a humongous object which will span multiple contiguous heap\n+  \/\/ regions.\n+  \/\/\n+  \/\/ Precondition: req.size() > ShenandoahHeapRegion::humongous_threshold_words().\n@@ -56,0 +191,1 @@\n+  void clear_internal();\n@@ -57,3 +193,1 @@\n-  void recompute_bounds();\n-  void adjust_bounds();\n-  bool touches_bounds(size_t num) const;\n+  void try_recycle_trashed(ShenandoahHeapRegion *r);\n@@ -61,2 +195,2 @@\n-  void increase_used(size_t amount);\n-  void clear_internal();\n+  inline bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  inline bool can_allocate_from(size_t idx) const;\n@@ -64,2 +198,1 @@\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+  inline bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -67,1 +200,2 @@\n-  void try_recycle_trashed(ShenandoahHeapRegion *r);\n+  void find_regions_with_alloc_capacity(size_t &cset_regions);\n+  void reserve_regions(size_t to_reserve);\n@@ -69,3 +203,2 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  void prepare_to_rebuild(size_t &cset_regions);\n+  void finish_rebuild(size_t cset_regions);\n@@ -76,0 +209,4 @@\n+  \/\/ Public because ShenandoahRegionPartition assertions require access.\n+  inline size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  inline size_t alloc_capacity(size_t idx) const;\n+\n@@ -79,1 +216,7 @@\n-  void recycle_trash();\n+  \/\/ After we have finished evacuation, we no longer need to hold regions in reserve for the Collector.\n+  \/\/ Call this method at the start of update refs to make more memory available to the Mutator.  This\n+  \/\/ benefits workloads that do not consume all of the evacuation waste reserve.\n+  \/\/\n+  \/\/ Note that we plan to replenish the Collector reserve at the end of update refs, at which time all\n+  \/\/ of the regions recycled from the collection set will be available.\n+  void move_regions_from_collector_to_mutator_partition(size_t cset_regions);\n@@ -81,0 +224,1 @@\n+  void recycle_trash();\n@@ -83,5 +227,5 @@\n-  size_t capacity()  const { return _capacity; }\n-  size_t used()      const { return _used;     }\n-  size_t available() const {\n-    assert(_used <= _capacity, \"must use less than capacity\");\n-    return _capacity - _used;\n+  inline size_t capacity()  const { return _partitions.capacity_of(Mutator); }\n+  inline size_t used()      const { return _partitions.used_by(Mutator);     }\n+  inline size_t available() const {\n+    assert(used() <= capacity(), \"must use less than capacity\");\n+    return capacity() - used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":175,"deletions":31,"binary":false,"changes":206,"status":"modified"},{"patch":"@@ -1068,0 +1068,2 @@\n+\n+    \/\/ Since Full GC directly manipulates top of certain regions, certain ShenandoahFreeSet abstractions may have been corrupted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -377,1 +377,0 @@\n-\n@@ -2061,1 +2060,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2064,1 +2063,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2070,1 +2069,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2072,0 +2071,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_regions_from_collector_to_mutator_partition(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"}]}