{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.inline.hpp\"\n@@ -35,5 +37,229 @@\n-ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n-  _heap(heap),\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+static const char* partition_name(ShenandoahFreeSetPartitionId t) {\n+  switch (t) {\n+    case NotFree: return \"NotFree\";\n+    case Mutator: return \"Mutator\";\n+    case Collector: return \"Collector\";\n+    default: return \"Unrecognized\";\n+  }\n+}\n+\n+size_t ShenandoahSimpleBitMap::count_leading_ones(ssize_t start_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+  size_t array_idx = start_idx \/ _bits_per_array_element;\n+  size_t element_bits = _bitmap[array_idx];\n+  size_t bit_number = start_idx % _bits_per_array_element;\n+  size_t the_bit = ((size_t) 0x01) << bit_number;\n+  size_t omit_mask = the_bit - 1;\n+  size_t mask = ((size_t) ((ssize_t) -1)) & omit_mask;\n+\n+  if ((element_bits & mask) == mask) {\n+    size_t counted_ones = _bits_per_array_element - bit_number;\n+    return counted_ones + count_leading_ones(start_idx + counted_ones);\n+  } else {\n+    size_t counted_ones;\n+    for (counted_ones = 0; element_bits & the_bit; counted_ones++) {\n+      the_bit <<= 1;\n+    }\n+    return counted_ones;\n+  }\n+}\n+\n+\/\/ Count consecutive ones in reverse order, starting from last_idx.  Requires that there is at least one zero\n+\/\/ between last_idx and index value zero, inclusive.\n+size_t ShenandoahSimpleBitMap::count_trailing_ones(ssize_t last_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  size_t array_idx = last_idx \/ _bits_per_array_element;\n+  size_t element_bits = _bitmap[array_idx];\n+  size_t bit_number = last_idx % _bits_per_array_element;\n+  size_t the_bit = ((size_t) 0x01) << bit_number;\n+\n+  \/\/ All ones from bit 0 to the_bit\n+  size_t mask = (the_bit * 2) - 1;\n+  if ((element_bits & mask) == mask) {\n+    size_t counted_ones = bit_number + 1;\n+    return counted_ones + count_trailing_ones(last_idx - counted_ones);\n+  } else {\n+    size_t counted_ones;\n+    for (counted_ones = 0; element_bits & the_bit; counted_ones++) {\n+      the_bit >>= 1;\n+    }\n+    return counted_ones;\n+  }\n+}\n+\n+bool ShenandoahSimpleBitMap::is_forward_consecutive_ones(ssize_t start_idx, ssize_t count) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition: start_idx: \" SSIZE_FORMAT \", count: \" SSIZE_FORMAT,\n+         start_idx, count);\n+  assert(start_idx + count <= (ssize_t) _num_bits, \"precondition\");\n+  size_t array_idx = start_idx \/ _bits_per_array_element;\n+  size_t bit_number = start_idx % _bits_per_array_element;\n+  size_t the_bit = ((size_t) 0x01) << bit_number;\n+  size_t element_bits = _bitmap[array_idx];\n+\n+  if ((ssize_t) (_bits_per_array_element - bit_number) >= count) {\n+    \/\/ All relevant bits reside within this array element\n+    size_t overreach_mask = ((size_t) 0x1 << (bit_number + count)) - 1;\n+    size_t exclude_mask = ((size_t) 0x1 << bit_number) - 1;\n+    size_t exact_mask = overreach_mask & ~exclude_mask;\n+    return (element_bits & exact_mask) == exact_mask? true: false;\n+  } else {\n+    \/\/ Need to exactly match all relevant bits of this array element, plus relevant bits of following array elements\n+    size_t overreach_mask = (size_t) (ssize_t) - 1;\n+    size_t exclude_mask = ((size_t) 0x1 << bit_number) - 1;\n+    size_t exact_mask = overreach_mask & ~exclude_mask;\n+    if ((element_bits & exact_mask) == exact_mask) {\n+      size_t matched_bits = _bits_per_array_element - bit_number;\n+      return is_forward_consecutive_ones(start_idx + matched_bits, count - matched_bits);\n+    } else {\n+      return false;\n+    }\n+  }\n+}\n+\n+bool ShenandoahSimpleBitMap::is_backward_consecutive_ones(ssize_t last_idx, ssize_t count) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  assert(last_idx - count >= -1, \"precondition\");\n+  size_t array_idx = last_idx \/ _bits_per_array_element;\n+  size_t bit_number = last_idx % _bits_per_array_element;\n+  size_t the_bit = ((size_t) 0x01) << bit_number;\n+  size_t element_bits = _bitmap[array_idx];\n+\n+  if ((ssize_t) (bit_number + 1) >= count) {\n+    \/\/ All relevant bits reside within this array element\n+    size_t overreach_mask = ((size_t) 0x1 << (bit_number + 1)) - 1;\n+    size_t exclude_mask = ((size_t) 0x1 << (bit_number + 1 - count)) - 1;\n+    size_t exact_mask = overreach_mask & ~exclude_mask;\n+    return (element_bits & exact_mask) == exact_mask? true: false;\n+  } else {\n+    \/\/ Need to exactly match all relevant bits of this array element, plus relevant bits of following array elements\n+    size_t exact_mask = ((size_t) 0x1 << (bit_number + 1)) - 1;\n+    if ((element_bits & exact_mask) == exact_mask) {\n+      size_t matched_bits = bit_number + 1;\n+      return is_backward_consecutive_ones(last_idx - matched_bits, count - matched_bits);\n+    } else {\n+      return false;\n+    }\n+  }\n+}\n+\n+\n+ssize_t ShenandoahSimpleBitMap::find_next_consecutive_bits(size_t num_bits, ssize_t start_idx, ssize_t boundary_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+\n+  \/\/ Stop looking if there are not num_bits remaining in probe space.\n+  ssize_t start_boundary = boundary_idx - num_bits;\n+  size_t array_idx = start_idx \/ _bits_per_array_element;\n+  size_t bit_number = start_idx % _bits_per_array_element;\n+  size_t element_bits = _bitmap[array_idx];\n+  if (bit_number > 0) {\n+    size_t mask_out = (((size_t) 0x01) << bit_number) - 1;\n+    element_bits &= ~mask_out;\n+  }\n+  while (start_idx <= start_boundary) {\n+    if (!element_bits) {\n+      \/\/ move to the next element\n+      start_idx += _bits_per_array_element - bit_number;\n+      array_idx++;\n+      bit_number = 0;\n+      element_bits = _bitmap[array_idx];\n+    } else if (is_forward_consecutive_ones(start_idx, num_bits)) {\n+      return start_idx;\n+    } else {\n+      \/\/ There is at least one zero bit in this span.  Align the next probe at the start of trailing ones for probed span.\n+      size_t trailing_ones = count_trailing_ones(start_idx + num_bits - 1);\n+      start_idx += num_bits - trailing_ones;\n+      array_idx = start_idx \/ _bits_per_array_element;\n+      element_bits = _bitmap[array_idx];\n+      bit_number = start_idx % _bits_per_array_element;\n+      if (bit_number > 0) {\n+        size_t mask_out = (((size_t) 0x01) << bit_number) - 1;\n+        element_bits &= ~mask_out;\n+      }\n+    }\n+  }\n+  \/\/ No match found.\n+  return boundary_idx;\n+}\n+\n+ssize_t ShenandoahSimpleBitMap::find_prev_consecutive_bits(size_t num_bits, ssize_t last_idx, ssize_t boundary_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+\n+  \/\/ Stop looking if there are not num_bits remaining in probe space.\n+  ssize_t last_boundary = boundary_idx + num_bits;\n+  ssize_t array_idx = last_idx \/ _bits_per_array_element;\n+  size_t bit_number = last_idx % _bits_per_array_element;\n+  size_t element_bits = _bitmap[array_idx];\n+  if (bit_number < _bits_per_array_element - 1) {\n+    size_t mask_in = (((size_t) 0x1) << (bit_number + 1)) - 1;\n+    element_bits &= mask_in;\n+  }\n+  while (last_idx >= last_boundary) {\n+    if (!element_bits) {\n+      \/\/ move to the previous element\n+      last_idx -= bit_number + 1;\n+      array_idx--;\n+      bit_number = _bits_per_array_element - 1;\n+      element_bits = _bitmap[array_idx];\n+    } else if (is_backward_consecutive_ones(last_idx, num_bits)) {\n+      return last_idx + 1 - num_bits;\n+    } else {\n+      \/\/ There is at least one zero bit in this span.  Align the next probe at the end of leading ones for probed span.\n+      size_t leading_ones = count_leading_ones(last_idx - (num_bits - 1));\n+      last_idx -= num_bits - leading_ones;\n+      array_idx = last_idx \/ _bits_per_array_element;\n+      bit_number = last_idx % _bits_per_array_element;\n+      element_bits = _bitmap[array_idx];\n+      if (bit_number < _bits_per_array_element - 1){\n+        size_t mask_in = (((size_t) 0x1) << (bit_number + 1)) - 1;\n+        element_bits &= mask_in;\n+      }\n+    }\n+  }\n+  \/\/ No match found.\n+  return boundary_idx;\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_all() const {\n+  log_info(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts[Mutator], _rightmosts[Mutator], _leftmosts[Collector], _rightmosts[Collector]);\n+  log_info(gc)(\"Empty Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts_empty[Mutator], _rightmosts_empty[Mutator],\n+               _leftmosts_empty[Collector], _rightmosts_empty[Collector]);\n+\n+#ifdef _LP64\n+  log_info(gc)(\"%6s: %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"NotFree Bits\");\n+#else\n+  log_info(gc)(\"%6s: %10s %10s %10s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"NotFree Bits\");\n+#endif\n+  dump_bitmap_range(0, _max-1);\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_range(ssize_t start_idx, ssize_t end_idx) const {\n+  assert((start_idx >= 0) && (start_idx < (ssize_t) _max), \"precondition\");\n+  assert((end_idx >= 0) && (end_idx < (ssize_t) _max), \"precondition\");\n+  ssize_t aligned_start = _membership[Mutator].aligned_index(start_idx);\n+  ssize_t aligned_end = _membership[Mutator].aligned_index(end_idx);\n+  ssize_t alignment = _membership[Mutator].alignment();\n+  while (aligned_start <= aligned_end) {\n+    dump_bitmap_row(aligned_start);\n+    aligned_start += alignment;\n+  }\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_row(ssize_t idx) const {\n+  assert((idx >= 0) && (idx < (ssize_t) _max), \"precondition\");\n+  ssize_t aligned_idx = _membership[Mutator].aligned_index(idx);\n+  size_t mutator_bits = _membership[Mutator].bits_at(aligned_idx);\n+  size_t collector_bits = _membership[Collector].bits_at(aligned_idx);\n+  size_t free_bits = mutator_bits | collector_bits;\n+  size_t notfree_bits =  ~free_bits;\n+  log_info(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+               aligned_idx, mutator_bits, collector_bits, notfree_bits);\n+}\n+\n+ShenandoahRegionPartitions::ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes()),\n+    _free_set(free_set),\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) }\n@@ -41,1 +267,1 @@\n-  clear_internal();\n+  make_all_regions_unavailable();\n@@ -44,3 +270,193 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n-  shenandoah_assert_heaplocked();\n-  _used += num_bytes;\n+ShenandoahRegionPartitions::~ShenandoahRegionPartitions() {\n+}\n+\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+inline bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n+  }\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+inline bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+inline ssize_t ShenandoahRegionPartitions:: leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  ssize_t idx = _leftmosts[which_partition];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    \/\/ _membership[which_partition].is_set(idx) may not be true if we are shrinking the interval\n+    return idx;\n+  }\n+}\n+\n+inline ssize_t ShenandoahRegionPartitions::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  ssize_t idx = _rightmosts[which_partition];\n+  \/\/ _membership[which_partition].is_set(idx) may not be true if we are shrinking the interval\n+  return idx;\n+}\n+\n+void ShenandoahRegionPartitions::make_all_regions_unavailable() {\n+  for (size_t partition_id = 0; partition_id < NumPartitions; partition_id++) {\n+    _membership[partition_id].clear_all();\n+    _leftmosts[partition_id] = _max;\n+    _rightmosts[partition_id] = -1;\n+    _leftmosts_empty[partition_id] = _max;\n+    _rightmosts_empty[partition_id] = -1;;\n+    _capacity[partition_id] = 0;\n+    _used[partition_id] = 0;\n+  }\n+  _region_counts[Mutator] = _region_counts[Collector] = 0;\n+}\n+\n+void ShenandoahRegionPartitions::establish_intervals(ssize_t mutator_leftmost, ssize_t mutator_rightmost,\n+                                                     ssize_t mutator_leftmost_empty, ssize_t mutator_rightmost_empty,\n+                                                     size_t mutator_region_count, size_t mutator_used) {\n+  _region_counts[Mutator] = mutator_region_count;\n+  _leftmosts[Mutator] = mutator_leftmost;\n+  _rightmosts[Mutator] = mutator_rightmost;\n+  _leftmosts_empty[Mutator] = mutator_leftmost_empty;\n+  _rightmosts_empty[Mutator] = mutator_rightmost_empty;\n+\n+  _region_counts[Mutator] = mutator_region_count;\n+  _used[Mutator] = mutator_used;\n+  _capacity[Mutator] = mutator_region_count * _region_size_bytes;\n+\n+  _leftmosts[Collector] = _max;\n+  _rightmosts[Collector] = -1;\n+  _leftmosts_empty[Collector] = _max;\n+  _rightmosts_empty[Collector] = -1;\n+\n+  _region_counts[Collector] = 0;\n+  _used[Collector] = 0;\n+  _capacity[Collector] = 0;\n+}\n+\n+void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _used[which_partition] += bytes;\n+  assert (_used[which_partition] <= _capacity[which_partition],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used[which_partition], _capacity[which_partition], bytes);\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_range_modifies_either_boundary(\n+  ShenandoahFreeSetPartitionId partition, ssize_t low_idx, ssize_t high_idx) {\n+  assert((low_idx <= high_idx) && (low_idx >= 0) && (high_idx < _max), \"Range must span legal index values\");\n+  if (low_idx == leftmost(partition)) {\n+    assert (!_membership[partition].is_set(low_idx), \"Do not shrink interval if region not removed\");\n+    if (high_idx + 1 == _max) {\n+      _leftmosts[partition] = _max;\n+    } else {\n+      _leftmosts[partition] = find_index_of_next_available_region(partition, high_idx + 1);\n+    }\n+    if (leftmost_empty(partition) < leftmost(partition)) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[partition] = leftmost(partition);\n+    }\n+  }\n+  if (high_idx == rightmost(partition)) {\n+    assert (!_membership[partition].is_set(high_idx), \"Do not shrink interval if region not removed\");\n+    if (low_idx == 0) {\n+      _rightmosts[partition] = -1;\n+    } else {\n+      _rightmosts[partition] = find_index_of_previous_available_region(partition, low_idx - 1);\n+    }\n+    if (rightmost_empty(partition) > rightmost(partition)) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[partition] = rightmost(partition);\n+    }\n+  }\n+  if (leftmost(partition) > rightmost(partition)) {\n+    _leftmosts[partition] = _max;\n+    _rightmosts[partition] = -1;\n+    _leftmosts_empty[partition] = _max;\n+    _rightmosts_empty[partition] = -1;\n+  }\n+\n+\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx) {\n+  assert((idx >= 0) && (idx < _max), \"Range must span legal index values\");\n+  if (idx == leftmost(partition)) {\n+    assert (!_membership[partition].is_set(idx), \"Do not shrink interval if region not removed\");\n+    if (idx + 1 == _max) {\n+      _leftmosts[partition] = _max;\n+    } else {\n+      _leftmosts[partition] = find_index_of_next_available_region(partition, idx + 1);\n+    }\n+    if (leftmost_empty(partition) < leftmost(partition)) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[partition] = leftmost(partition);\n+    }\n+  }\n+  if (idx == rightmost(partition)) {\n+    assert (!_membership[partition].is_set(idx), \"Do not shrink interval if region not removed\");\n+    if (idx == 0) {\n+      _rightmosts[partition] = -1;\n+    } else {\n+      _rightmosts[partition] = find_index_of_previous_available_region(partition, idx - 1);\n+    }\n+    if (rightmost_empty(partition) > rightmost(partition)) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[partition] = rightmost(partition);\n+    }\n+  }\n+  if (leftmost(partition) > rightmost(partition)) {\n+    _leftmosts[partition] = _max;\n+    _rightmosts[partition] = -1;\n+    _leftmosts_empty[partition] = _max;\n+    _rightmosts_empty[partition] = -1;\n+  }\n+}\n+\n+inline void ShenandoahRegionPartitions::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                                             ssize_t idx, size_t region_available) {\n+  if (leftmost(partition) > idx) {\n+    _leftmosts[partition] = idx;\n+  }\n+  if (rightmost(partition) < idx) {\n+    _rightmosts[partition] = idx;\n+  }\n+  if (region_available == _region_size_bytes) {\n+    if (leftmost_empty(partition) > idx) {\n+      _leftmosts_empty[partition] = idx;\n+    }\n+    if (rightmost_empty(partition) < idx) {\n+      _rightmosts_empty[partition] = idx;\n+    }\n+  }\n+}\n+\n+\/\/ Remove the consecutive regions between low_idx and high_idx inclusive from partition since all of these will be subsumed\n+\/\/ by a humongous object.  The entirety of each retired region is assumed to equal the region size.\n+void ShenandoahRegionPartitions::retire_range_from_partition(\n+  ShenandoahFreeSetPartitionId partition, ssize_t low_idx, ssize_t high_idx) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert ((low_idx < _max) && (high_idx < _max), \"Both indices are sane: \" SIZE_FORMAT \" and \" SIZE_FORMAT \" < \" SIZE_FORMAT,\n+          low_idx, high_idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n@@ -48,2 +464,85 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+  for (ssize_t idx = low_idx; idx <= high_idx; idx++) {\n+    assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+    _membership[partition].clear_bit(idx);\n+  }\n+  _region_counts[partition] -= high_idx + 1 - low_idx;\n+  shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx);\n+}\n+\n+\/\/ Remove this region from its free partition, but leave its capacity and used as part of the original free partition's totals.\n+\/\/ When retiring a region, add any remnant of available memory within the region to the used total for the original free partition.\n+void ShenandoahRegionPartitions::retire_from_partition(ShenandoahFreeSetPartitionId partition, ssize_t idx, size_t used_bytes) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+  assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+\n+  if (used_bytes < _region_size_bytes) {\n+    \/\/ Count the alignment pad remnant of memory as used when we retire this region\n+    increase_used(partition, _region_size_bytes - used_bytes);\n+  }\n+  _membership[partition].clear_bit(idx);\n+  shrink_interval_if_boundary_modified(partition, idx);\n+  _region_counts[partition]--;\n+}\n+\n+void ShenandoahRegionPartitions::make_free(ssize_t idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (membership(idx) == NotFree, \"Cannot make free if already free\");\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  _membership[which_partition].set_bit(idx);\n+  _capacity[which_partition] += _region_size_bytes;\n+  _used[which_partition] += _region_size_bytes - available;\n+  expand_interval_if_boundary_modified(which_partition, idx, available);\n+\n+  _region_counts[which_partition]++;\n+}\n+\n+void ShenandoahRegionPartitions::move_from_partition_to_partition(ssize_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                                  ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (orig_partition < NumPartitions, \"Original partition must be valid\");\n+  assert (new_partition < NumPartitions, \"New partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild:         Mutator => Collector\n+  \/\/  During flip_to_gc:      Mutator empty => Collector\n+  \/\/ At start of update refs: Collector => Mutator\n+  assert (((available <= _region_size_bytes) &&\n+           (((orig_partition == Mutator) && (new_partition == Collector)) ||\n+            ((orig_partition == Collector) && (new_partition == Mutator)))) ||\n+          ((available == _region_size_bytes) &&\n+           ((orig_partition == Mutator) && (new_partition == Collector))), \"Unexpected movement between partitions\");\n+\n+\n+  size_t used = _region_size_bytes - available;\n+\n+  _membership[orig_partition].clear_bit(idx);\n+  _membership[new_partition].set_bit(idx);\n+\n+  _capacity[orig_partition] -= _region_size_bytes;\n+  _used[orig_partition] -= used;\n+  shrink_interval_if_boundary_modified(orig_partition, idx);\n+\n+  _capacity[new_partition] += _region_size_bytes;;\n+  _used[new_partition] += used;\n+  expand_interval_if_boundary_modified(new_partition, idx, available);\n+\n+  _region_counts[orig_partition]--;\n+  _region_counts[new_partition]++;\n+}\n+\n+const char* ShenandoahRegionPartitions::partition_membership_name(ssize_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeSetPartitionId result = NotFree;\n+  for (uint partition_id = 0; partition_id < NumPartitions; partition_id++) {\n+    if (_membership[partition_id].is_set(idx)) {\n+      assert(result == NotFree, \"Region should reside in only one partition\");\n+      result = (ShenandoahFreeSetPartitionId) partition_id;\n+    }\n+  }\n+  return partition_name(result);\n@@ -52,4 +551,210 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+\n+#ifdef ASSERT\n+inline ShenandoahFreeSetPartitionId ShenandoahRegionPartitions::membership(ssize_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeSetPartitionId result = NotFree;\n+  for (uint partition_id = 0; partition_id < NumPartitions; partition_id++) {\n+    if (_membership[partition_id].is_set(idx)) {\n+      assert(result == NotFree, \"Region should reside in only one partition\");\n+      result = (ShenandoahFreeSetPartitionId) partition_id;\n+    }\n+  }\n+  return result;\n+}\n+\n+\/\/ Returns true iff region idx is in the test_partition, which must not equal NotFree.\n+inline bool ShenandoahRegionPartitions::partition_id_matches(ssize_t idx, ShenandoahFreeSetPartitionId test_partition) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (test_partition < NotFree, \"must be a valid partition\");\n+\n+  return membership(idx) == test_partition;\n+}\n+#endif\n+\n+inline bool ShenandoahRegionPartitions::is_empty(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  return (leftmost(which_partition) > rightmost(which_partition));\n+}\n+\n+  \/\/ Return the index of the next available region >= start_index, or maximum_regions if not found.\n+inline ssize_t ShenandoahRegionPartitions::find_index_of_next_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, ssize_t start_index) const {\n+  ssize_t rightmost_idx = rightmost(which_partition);\n+  ssize_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  if (start_index < leftmost_idx) {\n+    start_index = leftmost_idx;\n+  }\n+  ssize_t result = _membership[which_partition].find_next_set_bit(start_index, rightmost_idx + 1);\n+  return (result > rightmost_idx)? _max: result;\n+}\n+\n+\/\/ Return the index of the previous available region <= last_index, or -1 if not found.\n+inline ssize_t ShenandoahRegionPartitions::find_index_of_previous_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, ssize_t last_index) const {\n+  ssize_t rightmost_idx = rightmost(which_partition);\n+  ssize_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  if (last_index > rightmost_idx) {\n+    last_index = rightmost_idx;\n+  }\n+  ssize_t result = _membership[which_partition].find_prev_set_bit(last_index, -1);\n+  return (result < leftmost_idx)? -1: result;\n+}\n+\n+\/\/ Return the index of the next available cluster of cluster_size regions >= start_index, or maximum_regions if not found.\n+inline ssize_t ShenandoahRegionPartitions::find_index_of_next_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, ssize_t start_index, size_t cluster_size) const {\n+  ssize_t rightmost_idx = rightmost(which_partition);\n+  ssize_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  ssize_t result = _membership[which_partition].find_next_consecutive_bits(cluster_size, start_index, rightmost_idx + 1);\n+  return (result > rightmost_idx)? _max: result;\n+}\n+\n+\/\/ Return the index of the previous available cluster of cluster_size regions <= last_index, or -1 if not found.\n+inline ssize_t ShenandoahRegionPartitions::find_index_of_previous_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, ssize_t last_index, size_t cluster_size) const {\n+  ssize_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  ssize_t result = _membership[which_partition].find_prev_consecutive_bits(cluster_size, last_index, leftmost_idx - 1);\n+  return (result <= leftmost_idx)? -1: result;\n+}\n+\n+ssize_t ShenandoahRegionPartitions::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  ssize_t max_regions = _max;\n+  if (_leftmosts_empty[which_partition] == _max) {\n+    return _max;\n+  }\n+  for (ssize_t idx = find_index_of_next_available_region(which_partition, _leftmosts_empty[which_partition]);\n+       idx < max_regions; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_prev_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _leftmosts_empty[which_partition] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_next_available_region(which_partition, idx + 1);\n+  }\n+  _leftmosts_empty[which_partition] = _max;\n+  _rightmosts_empty[which_partition] = -1;\n+  return _max;\n+}\n+\n+ssize_t ShenandoahRegionPartitions::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  if (_rightmosts_empty[which_partition] < 0) {\n+    return -1;\n+  }\n+  for (ssize_t idx = find_index_of_previous_available_region(which_partition, _rightmosts_empty[which_partition]); idx >= 0; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_prev_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _rightmosts_empty[which_partition] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_previous_available_region(which_partition, idx - 1);\n+  }\n+  _leftmosts_empty[which_partition] = _max;\n+  _rightmosts_empty[which_partition] = -1;\n+  return -1;\n+}\n+\n+\n+#ifdef ASSERT\n+void ShenandoahRegionPartitions::assert_bounds() {\n+\n+  ssize_t leftmosts[NumPartitions];\n+  ssize_t rightmosts[NumPartitions];\n+  ssize_t empty_leftmosts[NumPartitions];\n+  ssize_t empty_rightmosts[NumPartitions];\n+\n+  for (int i = 0; i < NumPartitions; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = -1;\n+    empty_rightmosts[i] = -1;\n+  }\n+\n+  for (ssize_t i = 0; i < _max; i++) {\n+    ShenandoahFreeSetPartitionId partition = membership(i);\n+    switch (partition) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < leftmosts[partition]) {\n+          leftmosts[partition] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[partition])) {\n+          empty_leftmosts[partition] = i;\n+        }\n+        if (i > rightmosts[partition]) {\n+          rightmosts[partition] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[partition])) {\n+          empty_rightmosts[partition] = i;\n+        }\n+        break;\n+      }\n+\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(Mutator) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, leftmost(Mutator),  _max);\n+  assert (rightmost(Mutator) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, rightmost(Mutator),  _max);\n+\n+  assert (leftmost(Mutator) == _max || partition_id_matches(leftmost(Mutator), Mutator),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(Mutator));\n+  assert (leftmost(Mutator) == _max || partition_id_matches(rightmost(Mutator), Mutator),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(Mutator));\n+\n+  \/\/ If Mutator partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  ssize_t beg_off = leftmosts[Mutator];\n+  ssize_t end_off = rightmosts[Mutator];\n+  assert (beg_off >= leftmost(Mutator),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT, beg_off, leftmost(Mutator));\n+  assert (end_off <= rightmost(Mutator),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,  end_off, rightmost(Mutator));\n+\n+  beg_off = empty_leftmosts[Mutator];\n+  end_off = empty_rightmosts[Mutator];\n+  assert (beg_off >= leftmost_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT, beg_off, leftmost_empty(Mutator));\n+  assert (end_off <= rightmost_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,  end_off, rightmost_empty(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(Collector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, leftmost(Collector),  _max);\n+  assert (rightmost(Collector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, rightmost(Collector),  _max);\n+\n+  assert (leftmost(Collector) == _max || partition_id_matches(leftmost(Collector), Collector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(Collector));\n+  assert (leftmost(Collector) == _max || partition_id_matches(rightmost(Collector), Collector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(Collector));\n+\n+  \/\/ If Collector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[Collector];\n+  end_off = rightmosts[Collector];\n+  assert (beg_off >= leftmost(Collector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT, beg_off, leftmost(Collector));\n+  assert (end_off <= rightmost(Collector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,  end_off, rightmost(Collector));\n+\n+  beg_off = empty_leftmosts[Collector];\n+  end_off = empty_rightmosts[Collector];\n+  assert (beg_off >= _leftmosts_empty[Collector],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT, beg_off, leftmost_empty(Collector));\n+  assert (end_off <= _rightmosts_empty[Collector],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,  end_off, rightmost_empty(Collector));\n@@ -57,0 +762,1 @@\n+#endif\n@@ -58,4 +764,7 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n+  _heap(heap),\n+  _partitions(max_regions, this),\n+  _right_to_left_bias(false),\n+  _alloc_bias_weight(0)\n+{\n+  clear_internal();\n@@ -65,0 +774,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -67,2 +778,1 @@\n-  \/\/ Leftmost and rightmost bounds provide enough caching to walk bitmap efficiently. Normally,\n-  \/\/ we would find the region to allocate at right away.\n+  \/\/ Leftmost and rightmost bounds provide enough caching to quickly find a region from which to allocate.\n@@ -70,3 +780,3 @@\n-  \/\/ Allocations are biased: new application allocs go to beginning of the heap, and GC allocs\n-  \/\/ go to the end. This makes application allocation faster, because we would clear lots\n-  \/\/ of regions from the beginning most of the time.\n+  \/\/ Allocations are biased: GC allocations are taken from the high end of the heap.  Regular (and TLAB)\n+  \/\/ mutator allocations are taken from the middle of heap, below the memory reserved for Collector.\n+  \/\/ Humongous mutator allocations are taken from the bottom of the heap.\n@@ -74,2 +784,3 @@\n-  \/\/ Free set maintains mutator and collector views, and normally they allocate in their views only,\n-  \/\/ unless we special cases for stealing and mixed allocations.\n+  \/\/ Free set maintains mutator and collector partitions.  Mutator can only allocate from the\n+  \/\/ Mutator partition.  Collector prefers to allocate from the Collector partition, but may steal\n+  \/\/ regions from the Mutator partition if the Collector partition has been depleted.\n@@ -80,1 +791,0 @@\n-\n@@ -82,5 +792,55 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n+      if (_alloc_bias_weight-- <= 0) {\n+        \/\/ We have observed that regions not collected in previous GC cycle tend to congregate at one end or the other\n+        \/\/ of the heap.  Typically, these are the more recently engaged regions and the objects in these regions have not\n+        \/\/ yet had a chance to die (and\/or are treated as floating garbage).  If we use the same allocation bias on each\n+        \/\/ GC pass, these \"most recently\" engaged regions for GC pass N will also be the \"most recently\" engaged regions\n+        \/\/ for GC pass N+1, and the relatively large amount of live data and\/or floating garbage introduced\n+        \/\/ during the most recent GC pass may once again prevent the region from being collected.  We have found that\n+        \/\/ alternating the allocation behavior between GC passes improves evacuation performance by 3-7% on certain\n+        \/\/ benchmarks.  In the best case, this has the effect of consuming these partially consumed regions before\n+        \/\/ the start of the next mark cycle so all of their garbage can be efficiently reclaimed.\n+        \/\/\n+        \/\/ First, finish consuming regions that are already partially consumed so as to more tightly limit ranges of\n+        \/\/ available regions.  Other potential benefits:\n+        \/\/  1. Eventual collection set has fewer regions because we have packed newly allocated objects into fewer regions\n+        \/\/  2. We preserve the \"empty\" regions longer into the GC cycle, reducing likelihood of allocation failures\n+        \/\/     late in the GC cycle.\n+        ssize_t non_empty_on_left = _partitions.leftmost_empty(Mutator) - _partitions.leftmost(Mutator);\n+        ssize_t non_empty_on_right = _partitions.rightmost(Mutator) - _partitions.rightmost_empty(Mutator);\n+        _right_to_left_bias = (non_empty_on_right > non_empty_on_left);\n+        _alloc_bias_weight = _InitialAllocBiasWeight;\n+      }\n+      if (_right_to_left_bias) {\n+        \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+        if (!_partitions.is_empty(Mutator)) {\n+          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+          ssize_t leftmost = _partitions.leftmost(Mutator);\n+          for (ssize_t idx = _partitions.rightmost(Mutator); idx >= leftmost; ) {\n+            assert(_partitions.in_free_set(Mutator, idx), \"Boundaries or find_prev_set_bit failed: \" SSIZE_FORMAT, idx);\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n+            idx = _partitions.find_index_of_previous_available_region(Mutator, idx - 1);\n+          }\n+        }\n+      } else {\n+        \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n+        \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n+        \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n+        if (!_partitions.is_empty(Mutator)) {\n+          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+          ssize_t rightmost = _partitions.rightmost(Mutator);\n+          for (ssize_t idx = _partitions.leftmost(Mutator); idx <= rightmost; ) {\n+            assert(_partitions.in_free_set(Mutator, idx), \"Boundaries or find_prev_set_bit failed: \" SSIZE_FORMAT, idx);\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n+            idx = _partitions.find_index_of_next_available_region(Mutator, idx + 1);\n@@ -90,1 +850,0 @@\n-\n@@ -95,2 +854,1 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.\n@@ -98,0 +856,1 @@\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n@@ -99,7 +858,6 @@\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n-          }\n+      ssize_t leftmost_collector = _partitions.leftmost(Collector);\n+      for (ssize_t idx = _partitions.rightmost(Collector); idx >= leftmost_collector; ) {\n+        assert(_partitions.in_free_set(Collector, idx), \"Boundaries or find_prev_set_bit failed: \" SSIZE_FORMAT, idx);\n+        HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n@@ -107,0 +865,1 @@\n+        idx = _partitions.find_index_of_previous_available_region(Collector, idx - 1);\n@@ -114,11 +873,11 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_mutator_free(idx)) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (can_allocate_from(r)) {\n-            flip_to_gc(r);\n-            HeapWord *result = try_allocate_in(r, req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n-            }\n+      \/\/ Try to steal an empty region from the mutator view.\n+      ssize_t leftmost_mutator_empty = _partitions.leftmost_empty(Mutator);\n+      for (ssize_t idx = _partitions.rightmost_empty(Mutator); idx >= leftmost_mutator_empty; ) {\n+        assert(_partitions.in_free_set(Mutator, idx), \"Boundaries or find_prev_set_bit failed: \" SSIZE_FORMAT, idx);\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        if (can_allocate_from(r)) {\n+          flip_to_gc(r);\n+          HeapWord *result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            log_debug(gc)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+            return result;\n@@ -127,0 +886,1 @@\n+        idx = _partitions.find_index_of_previous_available_region(Mutator, idx - 1);\n@@ -129,4 +889,2 @@\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n-      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n-      \/\/ allocations.\n-\n+      \/\/ No dice. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+      \/\/ due to GC allocations would expose unparsable mutator allocations.\n@@ -138,1 +896,0 @@\n-\n@@ -143,4 +900,2 @@\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n-\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n@@ -150,0 +905,1 @@\n+  HeapWord* result = nullptr;\n@@ -151,1 +907,0 @@\n-\n@@ -154,2 +909,4 @@\n-  HeapWord* result = nullptr;\n-  size_t size = req.size();\n+  if (in_new_region) {\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  }\n@@ -157,0 +914,1 @@\n+  \/\/ req.size() is in words, r->free() is in bytes.\n@@ -158,0 +916,2 @@\n+    \/\/ This is a GCLAB or a TLAB allocation\n+    size_t adjusted_size = req.size();\n@@ -159,2 +919,2 @@\n-    if (size > free) {\n-      size = free;\n+    if (adjusted_size > free) {\n+      adjusted_size = free;\n@@ -162,3 +922,11 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+    if (adjusted_size >= req.min_size()) {\n+      result = r->allocate(adjusted_size, req.type());\n+      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words (adjusted from \" SIZE_FORMAT \") for %s @\" PTR_FORMAT\n+                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n+                          adjusted_size, req.size(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n+                          _partitions.partition_membership_name(r->index()), r->index(), r->free());\n+      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+      req.set_actual_size(adjusted_size);\n+    } else {\n+      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                          \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n@@ -167,0 +935,1 @@\n+    size_t size = req.size();\n@@ -168,0 +937,8 @@\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      log_debug(gc)(\"Allocated \" SIZE_FORMAT \" words for %s @\" PTR_FORMAT\n+                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n+                          size, ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n+                          _partitions.partition_membership_name(r->index()),  r->index(), r->free());\n+      req.set_actual_size(size);\n+    }\n@@ -173,5 +950,3 @@\n-      increase_used(size * HeapWordSize);\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n+      _partitions.increase_used(Mutator, req.actual_size() * HeapWordSize);\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n@@ -179,1 +954,2 @@\n-    if (req.is_gc_alloc()) {\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.\n@@ -184,7 +960,2 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n-    \/\/\n-    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+  static const size_t min_capacity = (size_t) (ShenandoahHeapRegion::region_size_bytes() * (1.0 - 1.0 \/ ShenandoahEvacWaste));\n+  size_t ac = alloc_capacity(r);\n@@ -192,8 +963,3 @@\n-    \/\/ Record the remainder as allocation waste\n-    if (req.is_mutator_alloc()) {\n-      size_t waste = r->free();\n-      if (waste > 0) {\n-        increase_used(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n-      }\n-    }\n+  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size() * HeapWordSize)) {\n+    \/\/ Regardless of whether this allocation succeeded, if the remaining memory is less than PLAB:min_size(), retire this region.\n+    \/\/ Note that retire_from_partition() increases used to account for waste.\n@@ -201,26 +967,2 @@\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n-    }\n-    assert_bounds();\n-  }\n-  return result;\n-}\n-\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n-}\n-\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n+    \/\/ Also, if this allocation request failed and the consumed within this region * ShenandoahEvacWaste > region size,\n+    \/\/ then retire the region so that subsequent searches can find available memory more quickly.\n@@ -228,14 +970,3 @@\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n+    size_t idx = r->index();\n+    _partitions.retire_from_partition(req.is_mutator_alloc()? Mutator: Collector, idx, r->used());\n+    _partitions.assert_bounds();\n@@ -243,0 +974,1 @@\n+  return result;\n@@ -246,0 +978,1 @@\n+  assert(req.is_mutator_alloc(), \"All humongous allocations are performed by mutator\");\n@@ -249,1 +982,1 @@\n-  size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n+  ssize_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n@@ -251,2 +984,2 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (num > (ssize_t) _partitions.count(Mutator)) {\n@@ -256,0 +989,4 @@\n+  ssize_t start_range = _partitions.leftmost_empty(Mutator);\n+  ssize_t end_range = _partitions.rightmost_empty(Mutator) + 1;\n+  ssize_t last_possible_start = end_range - num;\n+\n@@ -258,3 +995,6 @@\n-\n-  size_t beg = _mutator_leftmost;\n-  size_t end = beg;\n+  ssize_t beg = _partitions.find_index_of_next_available_cluster_of_regions(Mutator, start_range, num);\n+  if (beg > last_possible_start) {\n+    \/\/ Hit the end, goodbye\n+    return nullptr;\n+  }\n+  ssize_t end = beg;\n@@ -263,11 +1003,25 @@\n-    if (end >= _max) {\n-      \/\/ Hit the end, goodbye\n-      return nullptr;\n-    }\n-\n-    \/\/ If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.\n-    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n-      end++;\n-      beg = end;\n-      continue;\n+    \/\/ We've confirmed num contiguous regions belonging to Mutator partition, so no need to confirm membership.\n+    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.  If we can extend\n+    \/\/ the existing range, we can exploit that certain regions are already known to be in the Mutator free set.\n+    while (!can_allocate_from(_heap->get_region(end))) {\n+      \/\/ region[end] is not empty, so we restart our search after region[end]\n+      ssize_t slide_delta = end + 1 - beg;\n+      if (beg + slide_delta > last_possible_start) {\n+        \/\/ no room to slide\n+        return nullptr;\n+      }\n+      for (ssize_t span_end = beg + num; slide_delta > 0; slide_delta--) {\n+        if (!_partitions.in_free_set(Mutator, span_end)) {\n+          beg = _partitions.find_index_of_next_available_cluster_of_regions(Mutator, span_end + 1, num);\n+          break;\n+        } else {\n+          beg++;\n+          span_end++;\n+        }\n+      }\n+      \/\/ Here, either beg identifies a range of num regions all of which are in the Mutator free set, or beg > last_possible_start\n+      if (beg > last_possible_start) {\n+        \/\/ Hit the end, goodbye\n+        return nullptr;\n+      }\n+      end = beg;\n@@ -287,1 +1041,1 @@\n-  for (size_t i = beg; i <= end; i++) {\n+  for (ssize_t i = beg; i <= end; i++) {\n@@ -308,0 +1062,1 @@\n+    r->set_update_watermark(r->bottom());\n@@ -309,11 +1064,0 @@\n-\n-    _mutator_free_bitmap.clear_bit(r->index());\n-  }\n-\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-\n-  if (remainder != 0) {\n-    \/\/ Record this remainder as allocation waste\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n@@ -321,0 +1065,1 @@\n+  _partitions.retire_range_from_partition(Mutator, beg, end);\n@@ -322,6 +1067,3 @@\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n-\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _partitions.increase_used(Mutator, total_humongous_size);\n+  _partitions.assert_bounds();\n@@ -332,17 +1074,0 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n-  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n-}\n-\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n-  if (r->is_trash()) {\n-    \/\/ This would be recycled on allocation path\n-    return ShenandoahHeapRegion::region_size_bytes();\n-  } else {\n-    return r->free();\n-  }\n-}\n-\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n-  return alloc_capacity(r) == 0;\n-}\n-\n@@ -373,1 +1098,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_partitions.partition_id_matches(idx, Mutator), \"Should be in mutator view\");\n@@ -376,4 +1101,3 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  size_t ac = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, Mutator, Collector, ac);\n+  _partitions.assert_bounds();\n@@ -381,6 +1105,2 @@\n-  _capacity -= alloc_capacity(r);\n-\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -395,8 +1115,1 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _partitions.make_all_regions_unavailable();\n@@ -405,3 +1118,18 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n+\/\/ This function places all regions that have allocation capacity into the mutator_partition, identifying regions\n+\/\/ that have no allocation capacity as NotFree.  Subsequently, we will move some of the mutator regions into the\n+\/\/ collector partition with the intent of packing collector memory into the highest (rightmost) addresses of the\n+\/\/ heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &cset_regions) {\n+\n+  cset_regions = 0;\n+  clear_internal();\n+  size_t region_size_bytes = _partitions.region_size_bytes();\n+  size_t max_regions = _partitions.max_regions();\n+\n+  size_t mutator_leftmost = max_regions;\n+  size_t mutator_rightmost = 0;\n+  size_t mutator_leftmost_empty = max_regions;\n+  size_t mutator_rightmost_empty = 0;\n+\n+  size_t mutator_regions = 0;\n+  size_t mutator_used = 0;\n@@ -411,0 +1139,5 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n+      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      cset_regions++;\n+    }\n@@ -412,1 +1145,0 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n@@ -414,2 +1146,4 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      size_t ac = alloc_capacity(region);\n+      if (ac > PLAB::min_size() * HeapWordSize) {\n+        _partitions.raw_set_membership(idx, Mutator);\n@@ -417,2 +1151,16 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+        if (idx < mutator_leftmost) {\n+          mutator_leftmost = idx;\n+        }\n+        if (idx > mutator_rightmost) {\n+          mutator_rightmost = idx;\n+        }\n+        if (ac == region_size_bytes) {\n+          if (idx < mutator_leftmost_empty) {\n+            mutator_leftmost_empty = idx;\n+          }\n+          if (idx > mutator_rightmost_empty) {\n+            mutator_rightmost_empty = idx;\n+          }\n+        }\n+        mutator_regions++;\n+        mutator_used += (region_size_bytes - ac);\n@@ -420,2 +1168,5 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+        log_debug(gc)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator partition\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n@@ -424,0 +1175,3 @@\n+  _partitions.establish_intervals(mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                                  mutator_regions, mutator_used);\n+}\n@@ -425,3 +1179,26 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n-  size_t reserved = 0;\n+\/\/ Move no more than max_xfer_regions from the existing Collector partition to the Mutator partition.\n+\/\/\n+\/\/ This is called from outside the heap lock at the start of update refs.  At this point, we no longer\n+\/\/ need to reserve memory for evacuation.  (We will create a new reserve after update refs finishes,\n+\/\/ setting aside some of the memory that was reclaimed by the most recent GC.  This new reserve will satisfy\n+\/\/ the evacuation needs of the next GC pass.)\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t collector_empty_xfer = 0;\n+  size_t collector_not_empty_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost_empty(Collector) <= _partitions.rightmost_empty(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ssize_t rightmost = _partitions.rightmost_empty(Collector);\n+    for (ssize_t idx = _partitions.leftmost_empty(Collector); (max_xfer_regions > 0) && (idx <= rightmost); ) {\n+      assert(_partitions.in_free_set(Collector, idx), \"Boundaries or find_next_set_bit failed: \" SSIZE_FORMAT, idx);\n+      \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+      if (can_allocate_from(idx)) {\n+        _partitions.move_from_partition_to_partition(idx, Collector, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        collector_empty_xfer += region_size_bytes;\n+      }\n+      idx = _partitions.find_index_of_next_available_region(Collector, idx + 1);\n+    }\n+  }\n@@ -429,2 +1206,15 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n+  \/\/ If there are any non-empty regions within Collector partition, we can also move them to the Mutator free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost(Collector) <= _partitions.rightmost(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ssize_t rightmost = _partitions.rightmost(Collector);\n+    for (ssize_t idx = _partitions.leftmost(Collector); (max_xfer_regions > 0) && (idx <= rightmost); ) {\n+      assert(_partitions.in_free_set(Collector, idx), \"Boundaries or find_next_set_bit failed: \" SSIZE_FORMAT, idx);\n+      size_t ac = alloc_capacity(idx);\n+      if (ac > 0) {\n+        _partitions.move_from_partition_to_partition(idx, Collector, Mutator, ac);\n+        max_xfer_regions--;\n+        collector_not_empty_xfer += ac;\n+      }\n+      idx = _partitions.find_index_of_next_available_region(Collector, idx + 1);\n+    }\n+  }\n@@ -432,7 +1222,70 @@\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n+  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n+  log_info(gc)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free partition from Collector Reserve\",\n+               byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer));\n+}\n+\n+\n+\/\/ Overwrite arguments to represent the number of regions to be reclaimed from the cset\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &cset_regions) {\n+  shenandoah_assert_heaplocked();\n+\n+  log_debug(gc)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the mutator partition.\n+  find_regions_with_alloc_capacity(cset_regions);\n+}\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t cset_regions) {\n+  shenandoah_assert_heaplocked();\n+\n+  \/\/ Our desire is to reserve this much memory for future evacuation.  We may end up reserving less, if\n+  \/\/ memory is in short supply.\n+\n+  size_t reserve = _heap->max_capacity() * ShenandoahEvacReserve \/ 100;\n+  size_t available_in_collector_partition = _partitions.capacity_of(Collector) - _partitions.used_by(Collector);\n+  size_t additional_reserve;\n+  if (available_in_collector_partition < reserve) {\n+    additional_reserve = reserve - available_in_collector_partition;\n+  } else {\n+    additional_reserve = 0;\n+  }\n+\n+  reserve_regions(reserve);\n+  _partitions.assert_bounds();\n+  log_status();\n+}\n+\n+void ShenandoahFreeSet::rebuild() {\n+  size_t cset_regions;\n+  prepare_to_rebuild(cset_regions);\n+  finish_rebuild(cset_regions);\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator partition, move some of these regions from\n+\/\/ the mutator partition into the collector partition in order to assure that the memory available for allocations within\n+\/\/ the collector partition is at least to_reserve.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+\n+    if (!_partitions.in_free_set(Mutator, idx)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free partition implies has capacity\");\n+\n+    bool move_to_collector = _partitions.available_in(Collector) < to_reserve;\n+    if (!move_to_collector) {\n+      \/\/ We've satisfied to_reserve\n+      break;\n+    }\n+\n+    if (move_to_collector) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n+      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+      \/\/ ephemeral objects.\n+      _partitions.move_from_partition_to_partition(idx, Mutator, Collector, ac);\n+      log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n@@ -442,2 +1295,7 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  if (LogTarget(Info, gc, free)::is_enabled()) {\n+    size_t reserve = _partitions.capacity_of(Collector);\n+    if (reserve < to_reserve) {\n+      log_debug(gc)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                    PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+    }\n+  }\n@@ -449,1 +1307,54 @@\n-  LogTarget(Info, gc, ergo) lt;\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  if (LogTarget(Debug, gc, free)::is_enabled()) {\n+#define BUFFER_SIZE 80\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t consumed_collector = 0;\n+    size_t available_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_mutator = 0;\n+\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_debug(gc)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free H:humongous _:retired\");\n+    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                        \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+                        _partitions.leftmost(Mutator), _partitions.rightmost(Mutator),\n+                        _partitions.leftmost(Collector), _partitions.rightmost(Collector));\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_debug(gc)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_partitions.in_free_set(Mutator, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_mutator += capacity;\n+        consumed_mutator += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'M': 'm';\n+      } else if (_partitions.in_free_set(Collector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_collector += capacity;\n+        consumed_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'C': 'c';\n+      } else if (r->is_humongous()) {\n+        buffer[idx] = 'h';\n+      } else {\n+        buffer[idx] = '_';\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_debug(gc)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+  }\n+#endif\n+\n+  LogTarget(Info, gc, free) lt;\n@@ -455,1 +1366,1 @@\n-      size_t last_idx = 0;\n+      ssize_t last_idx = 0;\n@@ -464,2 +1375,2 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (ssize_t idx = _partitions.leftmost(Mutator); idx <= _partitions.rightmost(Mutator); idx++) {\n+        if (_partitions.in_free_set(Mutator, idx)) {\n@@ -468,1 +1379,0 @@\n-\n@@ -470,1 +1380,0 @@\n-\n@@ -481,1 +1390,0 @@\n-\n@@ -484,1 +1392,0 @@\n-\n@@ -493,0 +1400,5 @@\n+      \/\/ Since certain regions that belonged to the Mutator free partition at the time of most recent rebuild may have been\n+      \/\/ retired, the sum of used and capacities within regions that are still in the Mutator free partition may not match\n+      \/\/ my internally tracked values of used() and free().\n+      assert(free == total_free, \"Free memory should match\");\n+\n@@ -494,1 +1406,1 @@\n-               byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(free),          proper_unit_for_byte_size(free),\n@@ -509,2 +1421,2 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_partitions.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _partitions.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -515,0 +1427,2 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _partitions.count(Mutator));\n@@ -520,0 +1434,1 @@\n+      size_t total_used = 0;\n@@ -521,2 +1436,2 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (ssize_t idx = _partitions.leftmost(Collector); idx <= _partitions.rightmost(Collector); idx++) {\n+        if (_partitions.in_free_set(Collector, idx)) {\n@@ -527,0 +1442,1 @@\n+          total_used += r->used();\n@@ -529,4 +1445,4 @@\n-\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n-                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -539,1 +1455,0 @@\n-  assert_bounds();\n@@ -541,0 +1456,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -562,16 +1478,0 @@\n-size_t ShenandoahFreeSet::unsafe_peek_free() const {\n-  \/\/ Deliberately not locked, this method is unsafe when free set is modified.\n-\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->free() >= MinTLABSize) {\n-        return r->free();\n-      }\n-    }\n-  }\n-\n-  \/\/ It appears that no regions left\n-  return 0;\n-}\n-\n@@ -579,5 +1479,6 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      _heap->get_region(index)->print_on(out);\n-    }\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _partitions.count(Mutator));\n+  ssize_t rightmost = _partitions.rightmost(Mutator);\n+  for (ssize_t index = _partitions.leftmost(Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(Mutator, index), \"Boundaries or find_next_set_bit failed: \" SSIZE_FORMAT, index);\n+    _heap->get_region(index)->print_on(out);\n+    index = _partitions.find_index_of_next_available_region(Mutator, index + 1);\n@@ -585,5 +1486,6 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n-      _heap->get_region(index)->print_on(out);\n-    }\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(Collector));\n+  rightmost = _partitions.rightmost(Collector);\n+  for (ssize_t index = _partitions.leftmost(Collector); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(Collector, index), \"Boundaries or find_next_set_bit failed: \" SSIZE_FORMAT, index);\n+    _heap->get_region(index)->print_on(out);\n+    index = _partitions.find_index_of_next_available_region(Collector, index + 1);\n@@ -619,8 +1521,9 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      size_t used = r->used();\n-      squared += used * used;\n-      linear += used;\n-      count++;\n-    }\n+  ssize_t rightmost = _partitions.rightmost(Mutator);\n+  for (ssize_t index = _partitions.leftmost(Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(Mutator, index), \"Boundaries or find_next_set_bit failed: \" SSIZE_FORMAT, index);\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    size_t used = r->used();\n+    squared += used * used;\n+    linear += used;\n+    count++;\n+    index = _partitions.find_index_of_next_available_region(Mutator, index + 1);\n@@ -651,1 +1554,1 @@\n-  size_t last_idx = 0;\n+  ssize_t last_idx = 0;\n@@ -657,10 +1560,8 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->is_empty()) {\n-        free += ShenandoahHeapRegion::region_size_bytes();\n-        if (last_idx + 1 == index) {\n-          empty_contig++;\n-        } else {\n-          empty_contig = 1;\n-        }\n+  ssize_t rightmost = _partitions.rightmost(Mutator);\n+  for (ssize_t index = _partitions.leftmost(Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(Mutator, index), \"Boundaries or find_next_set_bit failed: \" SSIZE_FORMAT, index);\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    if (r->is_empty()) {\n+      free += ShenandoahHeapRegion::region_size_bytes();\n+      if (last_idx + 1 == index) {\n+        empty_contig++;\n@@ -668,1 +1569,1 @@\n-        empty_contig = 0;\n+        empty_contig = 1;\n@@ -670,3 +1571,2 @@\n-\n-      max_contig = MAX2(max_contig, empty_contig);\n-      last_idx = index;\n+    } else {\n+      empty_contig = 0;\n@@ -674,0 +1574,3 @@\n+    max_contig = MAX2(max_contig, empty_contig);\n+    last_idx = index;\n+    index = _partitions.find_index_of_next_available_region(Mutator, index + 1);\n@@ -683,27 +1586,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1196,"deletions":320,"binary":false,"changes":1516,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,351 @@\n+\/\/ The API and internal implementation of ShenandoahSimpleBitMap and ShenandoahRegionPartitions use ssize_t to\n+\/\/ represent index, even though index is \"inherently\" unsigned.  There are two reasons for this choice:\n+\/\/  1. We use -1 as a sentinel value to represent empty partitions.  This same value may be used to represent\n+\/\/     failure to find a previous set bit or previous range of set bits.\n+\/\/  2. Certain loops are written most naturally if the iterator, which may hold the sentinel -1 value, can be\n+\/\/     declared as signed and the terminating condition can be < 0.\n+\n+\n+\n+\/\/ ShenandoahSimpleBitMap resembles CHeapBitMap but adds missing support for find_next_consecutive_bits() and\n+\/\/ find_prev_contiguous_bits.  An alternative refactoring of code would subclass CHeapBitMap, but this might\n+\/\/ break abstraction rules, because efficient implementation requires assumptions about superclass internals that\n+\/\/ might be violatee through future software maintenance.\n+class ShenandoahSimpleBitMap {\n+  static const size_t _bits_per_array_element = HeapWordSize * 8;\n+\n+  const ssize_t _num_bits;\n+  const size_t _num_words;\n+  size_t* const _bitmap;\n+\n+public:\n+  ShenandoahSimpleBitMap(size_t num_bits) :\n+      _num_bits(num_bits),\n+      _num_words((num_bits + (_bits_per_array_element - 1)) \/ _bits_per_array_element),\n+      _bitmap(NEW_C_HEAP_ARRAY(size_t, _num_words, mtGC))\n+  {\n+    clear_all();\n+  }\n+\n+  ~ShenandoahSimpleBitMap() {\n+    if (_bitmap != nullptr) {\n+      FREE_C_HEAP_ARRAY(size_t, _bitmap);\n+    }\n+  }\n+  void clear_all() {\n+    for (size_t i = 0; i < _num_words; i++) {\n+      _bitmap[i] = 0;\n+    }\n+  }\n+\n+private:\n+\n+  \/\/ Count consecutive ones in forward order, starting from start_idx.  Requires that there is at least one zero\n+  \/\/ between start_idx and index value (_num_bits - 1), inclusive.\n+  size_t count_leading_ones(ssize_t start_idx) const;\n+\n+  \/\/ Count consecutive ones in reverse order, starting from last_idx.  Requires that there is at least one zero\n+  \/\/ between last_idx and index value zero, inclusive.\n+  size_t count_trailing_ones(ssize_t last_idx) const;\n+\n+  bool is_forward_consecutive_ones(ssize_t start_idx, ssize_t count) const;\n+  bool is_backward_consecutive_ones(ssize_t last_idx, ssize_t count) const;\n+\n+public:\n+\n+  inline ssize_t aligned_index(ssize_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    ssize_t array_idx = idx \/ _bits_per_array_element;\n+    return array_idx * _bits_per_array_element;\n+  }\n+\n+  inline ssize_t alignment() const {\n+    return _bits_per_array_element;\n+  }\n+\n+  \/\/ For testing\n+  inline ssize_t number_of_bits() const {\n+    return _num_bits;\n+  }\n+\n+  inline size_t bits_at(ssize_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    ssize_t array_idx = idx \/ _bits_per_array_element;\n+    return _bitmap[array_idx];\n+  }\n+\n+  inline void set_bit(ssize_t idx) {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    size_t array_idx = idx \/ _bits_per_array_element;\n+    size_t bit_number = idx % _bits_per_array_element;\n+    size_t the_bit = ((size_t) 0x01) << bit_number;\n+    _bitmap[array_idx] |= the_bit;\n+  }\n+\n+  inline void clear_bit(ssize_t idx) {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    assert(idx >= 0, \"precondition\");\n+    size_t array_idx = idx \/ _bits_per_array_element;\n+    size_t bit_number = idx % _bits_per_array_element;\n+    size_t the_bit = ((size_t) 0x01) << bit_number;\n+    _bitmap[array_idx] &= ~the_bit;\n+  }\n+\n+  inline bool is_set(ssize_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    assert(idx >= 0, \"precondition\");\n+    size_t array_idx = idx \/ _bits_per_array_element;\n+    size_t bit_number = idx % _bits_per_array_element;\n+    size_t the_bit = ((size_t) 0x01) << bit_number;\n+    return (_bitmap[array_idx] & the_bit)? true: false;\n+  }\n+\n+  \/\/ Return the index of the first set bit which is greater or equal to start_idx.  If not found, return _num_bits.\n+  inline ssize_t find_next_set_bit(ssize_t start_idx) const;\n+\n+  \/\/ Return the index of the first set bit which is greater or equal to start_idx and less than boundary_idx.\n+  \/\/ If not found, return boundary_idx\n+  inline ssize_t find_next_set_bit(ssize_t start_idx, ssize_t boundary_idx) const;\n+\n+  \/\/ Return the index of the last set bit which is less or equal to start_idx.  If not found, return -1.\n+  inline ssize_t find_prev_set_bit(ssize_t last_idx) const;\n+\n+  \/\/ Return the index of the last set bit which is less or equal to start_idx and greater than boundary_idx.\n+  \/\/ If not found, return boundary_idx.\n+  inline ssize_t find_prev_set_bit(ssize_t last_idx, ssize_t boundary_idx) const;\n+\n+  \/\/ Return the smallest index at which a run of num_bits consecutive ones is found, where return value is >= start_idx\n+  \/\/ and return value < _num_bits.  If no run of num_bits consecutive ones is found within the target range, return _num_bits.\n+  inline ssize_t find_next_consecutive_bits(size_t num_bits, ssize_t start_idx) const;\n+\n+  \/\/ Return the smallest index at which a run of num_bits consecutive ones is found, where return value is >= start_idx\n+  \/\/ and return value < boundary_idx.  If no run of num_bits consecutive ones is found within the target range,\n+  \/\/ return boundary_idx.\n+  ssize_t find_next_consecutive_bits(size_t num_bits, ssize_t start_idx, ssize_t boundary_idx) const;\n+\n+  \/\/ Return the largest index at which a run of num_bits consecutive ones is found, where return value is <= last_idx and > -1.\n+  \/\/ If no run of num_bits consecutive ones is found within the target range, return -1.\n+  inline ssize_t find_prev_consecutive_bits(size_t num_bits, ssize_t last_idx) const;\n+\n+  \/\/ Return the largest index at which a run of num_bits consecutive ones is found, where return value is <= last_idx and > -1.\n+  \/\/ If no run of num_bits consecutive ones is found within the target range, return -1.\n+  ssize_t find_prev_consecutive_bits(size_t num_bits, ssize_t last_idx, ssize_t boundary_idx) const;\n+};\n+\n+\/\/ Each ShenandoahHeapRegion is associated with a ShenandoahFreeSetPartitionId.\n+enum ShenandoahFreeSetPartitionId : uint8_t {\n+  Mutator,                      \/\/ Region is in the Mutator free set: available memory is available to mutators.\n+  Collector,                    \/\/ Region is in the Collector free set: available memory is reserved for evacuations.\n+  NotFree                       \/\/ Region is in no free set: it has no available memory\n+};\n+\n+\/\/ We do not maintain counts, capacity, or used for regions that are not free.  Informally, if a region is NotFree, it is\n+\/\/ in no partition.  NumPartitions represents the size of an array that may be indexed by Mutator or Collector.\n+#define NumPartitions NotFree\n+\n+\/\/ ShenandoahRegionPartitions provides an abstraction to help organize the implementation of ShenandoahFreeSet.  This\n+\/\/ class implements partitioning of regions into distinct sets.  Each ShenandoahHeapRegion is either in the Mutator free set,\n+\/\/ the Collector free set, or in neither free set (NotFree).  When we speak of a \"free partition\", we mean partitions that\n+\/\/ for which the ShenandoahFreeSetPartitionId is not equal to NotFree.\n+class ShenandoahRegionPartitions {\n+\n+private:\n+  const ssize_t _max;           \/\/ The maximum number of heap regions\n+  const size_t _region_size_bytes;\n+  const ShenandoahFreeSet* _free_set;\n+  \/\/ For each partition, we maintain a bitmap of which regions are affiliated with his partition.\n+  ShenandoahSimpleBitMap _membership[NumPartitions];\n+\n+  \/\/ For each partition, we track an interval outside of which a region affiliated with that partition is guaranteed\n+  \/\/ not to be found. This makes searches for free space more efficient.  For each partition p, _leftmosts[p]\n+  \/\/ represents its least index, and its _rightmosts[p] its greatest index. Empty intervals are indicated by the\n+  \/\/ canonical [_max, -1].\n+  ssize_t _leftmosts[NumPartitions];\n+  ssize_t _rightmosts[NumPartitions];\n+\n+  \/\/ Allocation for humongous objects needs to find regions that are entirely empty.  For each partion p, _leftmosts_empty[p]\n+  \/\/ represents the first region belonging to this partition that is completely empty and _rightmosts_empty[p] represents the\n+  \/\/ last region that is completely empty.  If there is no completely empty region in this partition, this is represented\n+  \/\/ by the canonical [_max, -1].\n+  ssize_t _leftmosts_empty[NumPartitions];\n+  ssize_t _rightmosts_empty[NumPartitions];\n+\n+  \/\/ For each partition p, _capacity[p] represents the total amount of memory within the partition at the time\n+  \/\/ of the most recent rebuild, _used[p] represents the total amount of memory that has been allocated within this\n+  \/\/ partition (either already allocated as of the rebuild, or allocated since the rebuild).  _capacity[p] and _used[p]\n+  \/\/ are denoted in bytes.  Note that some regions that had been assigned to a particular partition at rebuild time\n+  \/\/ may have been retired following the rebuild.  The tallies for these regions are still reflected in _capacity[p]\n+  \/\/ and _used[p], even though the region may have been removed from the free set.\n+  size_t _capacity[NumPartitions];\n+  size_t _used[NumPartitions];\n+  size_t _region_counts[NumPartitions];\n+\n+  inline void shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx);\n+  inline void shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n+                                                                ssize_t low_idx, ssize_t high_idx);\n+  inline void expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx, size_t capacity);\n+\n+  void dump_bitmap_row(ssize_t idx) const;\n+  void dump_bitmap_range(ssize_t start_idx, ssize_t end_idx) const;\n+  void dump_bitmap_all() const;\n+\n+public:\n+  ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahRegionPartitions();\n+\n+  \/\/ Remove all regions from all partitions and reset all bounds\n+  void make_all_regions_unavailable();\n+\n+  \/\/ Set the partition id for a particular region without adjusting interval bounds or usage\/capacity tallies\n+  inline void raw_set_membership(size_t idx, ShenandoahFreeSetPartitionId p) {\n+    _membership[p].set_bit(idx);\n+  }\n+\n+  \/\/ Set the Mutator intervals, usage, and capacity according to arguments.  Reset the Collector intervals, used, capacity\n+  \/\/ to represent empty Collector free set.\n+  void establish_intervals(ssize_t mutator_leftmost, ssize_t mutator_rightmost,\n+                           ssize_t mutator_leftmost_empty, ssize_t mutator_rightmost_empty,\n+                           size_t mutator_region_count, size_t mutator_used);\n+\n+  \/\/ Retire region idx from within partition.  Requires that region idx is in in the Mutator or Collector partitions.\n+  \/\/ Hereafter, identifies this region as NotFree.  Any remnant of available memory at the time of retirement is added to the\n+  \/\/ original partition's total of used bytes.\n+  void retire_from_partition(ShenandoahFreeSetPartitionId p, ssize_t idx, size_t used_bytes);\n+\n+  \/\/ Retire all regions between low_idx and high_idx inclusive from within partition.  Requires that each region idx is\n+  \/\/ in the same Mutator or Collector partition.  Hereafter, identifies each region as NotFree.   Assumes that each region\n+  \/\/ is now considered fully used, since the region is presumably used to represent a humongous object.\n+  void retire_range_from_partition(ShenandoahFreeSetPartitionId partition, ssize_t low_idx, ssize_t high_idx);\n+\n+  \/\/ Place region idx into free set which_partition.  Requires that idx is currently NotFree.\n+  void make_free(ssize_t idx, ShenandoahFreeSetPartitionId which_partition, size_t region_capacity);\n+\n+  \/\/ Place region idx into free partition new_partition, adjusting used and capacity totals for the original and new partition\n+  \/\/ given that available bytes can still be allocated within this region.  Requires that idx is currently not NotFree.\n+  void move_from_partition_to_partition(ssize_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                        ShenandoahFreeSetPartitionId new_partition, size_t available);\n+\n+  const char* partition_membership_name(ssize_t idx) const;\n+\n+  \/\/ Return the index of the next available region >= start_index, or maximum_regions if not found.\n+  inline ssize_t find_index_of_next_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t start_index) const;\n+\n+  \/\/ Return the index of the previous available region <= last_index, or -1 if not found.\n+  inline ssize_t find_index_of_previous_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t last_index) const;\n+\n+  \/\/ Return the index of the next available cluster of cluster_size regions >= start_index, or maximum_regions if not found.\n+  inline ssize_t find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                 ssize_t start_index, size_t cluster_size) const;\n+\n+  \/\/ Return the index of the previous available cluster of cluster_size regions <= last_index, or -1 if not found.\n+  inline ssize_t find_index_of_previous_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                     ssize_t last_index, size_t cluster_size) const;\n+\n+  inline bool in_free_set(ShenandoahFreeSetPartitionId which_partition, ssize_t idx) const {\n+    return _membership[which_partition].is_set(idx);\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Returns the ShenandoahFreeSetPartitionId affiliation of region idx, NotFree if this region is not currently in any partition.\n+  \/\/ This does not enforce that free_set membership implies allocation capacity.\n+  inline ShenandoahFreeSetPartitionId membership(ssize_t idx) const;\n+\n+  \/\/ Returns true iff region idx's membership is which_partition.  If which_partition represents a free set, asserts\n+  \/\/ that the region has allocation capacity.\n+  inline bool partition_id_matches(ssize_t idx, ShenandoahFreeSetPartitionId which_partition) const;\n+#endif\n+\n+  inline size_t max_regions() const { return _max; }\n+\n+  inline size_t region_size_bytes() const { return _region_size_bytes; };\n+\n+  \/\/ The following four methods return the left-most and right-most bounds on ranges of regions representing\n+  \/\/ the requested set.  The _empty variants represent bounds on the range that holds completely empty\n+  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.\n+  \/\/   if the requested which_partition is empty:\n+  \/\/     leftmost() and leftmost_empty() return _max, rightmost() and rightmost_empty() return 0\n+  \/\/   otherwise, expect the following:\n+  \/\/     0 <= leftmost <= leftmost_empty <= rightmost_empty <= rightmost < _max\n+  inline ssize_t leftmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  inline ssize_t rightmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  ssize_t leftmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+  ssize_t rightmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+\n+  inline bool is_empty(ShenandoahFreeSetPartitionId which_partition) const;\n+\n+  inline void increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+\n+  inline size_t capacity_of(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[which_partition];\n+  }\n+\n+  inline size_t used_by(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _used[which_partition];\n+  }\n+\n+  inline size_t available_in(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[which_partition] - _used[which_partition];\n+  }\n+\n+  inline void set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _capacity[which_partition] = value;\n+  }\n+\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _used[which_partition] = value;\n+  }\n+\n+  inline size_t count(ShenandoahFreeSetPartitionId which_partition) const { return _region_counts[which_partition]; }\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n+  \/\/ Valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and rightmost_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= leftmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n+\n+\/\/ Publicly, ShenandoahFreeSet represents memory that is available to mutator threads.  The public capacity(), used(),\n+\/\/ and available() methods represent this public notion of memory that is under control of the mutator.  Separately,\n+\/\/ ShenandoahFreeSet also represents memory available to garbage collection activities for compaction purposes.\n+\/\/\n+\/\/ The Shenandoah garbage collector evacuates live objects out of specific regions that are identified as members of the\n+\/\/ collection set (cset).\n+\/\/\n+\/\/ The ShenandoahFreeSet endeavors to congregrate survivor objects (objects that have been evacuated at least once) at the\n+\/\/ high end of memory.  New mutator allocations are taken from the low end of memory.  Within the mutator's range of regions,\n+\/\/ humongous allocations are taken from the lowest addresses, and LAB (local allocation buffers) and regular shared allocations\n+\/\/ are taken from the higher address of the mutator's range of regions.  This approach allows longer lasting survivor regions\n+\/\/ to congregate at the top of the heap and longer lasting humongous regions to congregate at the bottom of the heap, with\n+\/\/ short-lived frequently evacuated regions occupying the middle of the heap.\n+\/\/\n+\/\/ Mutator and garbage collection activities tend to scramble the content of regions.  Twice, during each GC pass, we rebuild\n+\/\/ the free set in an effort to restore the efficient segregation of Collector and Mutator regions:\n+\/\/\n+\/\/  1. At the start of evacuation, we know exactly how much memory is going to be evacuated, and this guides our\n+\/\/     sizing of the Collector free set.\n+\/\/\n+\/\/  2. At the end of GC, we have reclaimed all of the memory that was spanned by the cset.  We rebuild here to make\n+\/\/     sure there is enough memory reserved at the high end of memory to hold the objects that might need to be evacuated\n+\/\/     during the next GC pass.\n+\n@@ -34,3 +386,1 @@\n-  CHeapBitMap _mutator_free_bitmap;\n-  CHeapBitMap _collector_free_bitmap;\n-  size_t _max;\n+  ShenandoahRegionPartitions _partitions;\n@@ -38,4 +388,4 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n-  size_t _mutator_leftmost, _mutator_rightmost;\n-  size_t _collector_leftmost, _collector_rightmost;\n+  \/\/ Mutator allocations are biased from left-to-right or from right-to-left based on which end of mutator range\n+  \/\/ is most likely to hold partially used regions.  In general, we want to finish consuming partially used\n+  \/\/ regions and retire them in order to reduce the regions that must be searched for each allocation request.\n+  bool _right_to_left_bias;\n@@ -43,2 +393,4 @@\n-  size_t _capacity;\n-  size_t _used;\n+  \/\/ We re-evaluate the left-to-right allocation bias whenever _alloc_bias_weight is less than zero.  Each time\n+  \/\/ we allocate an object, we decrement the count of this value.  Each time we re-evaluate whether to allocate\n+  \/\/ from right-to-left or left-to-right, we reset the value of this counter to _InitialAllocBiasWeight.\n+  ssize_t _alloc_bias_weight;\n@@ -46,4 +398,1 @@\n-  void assert_bounds() const NOT_DEBUG_RETURN;\n-\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+  const ssize_t _InitialAllocBiasWeight = 256;\n@@ -52,0 +401,5 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object or LAB  which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.\n+  \/\/\n+  \/\/ Precondition: req.size() <= ShenandoahHeapRegion::humongous_threshold_words().\n@@ -53,0 +407,6 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a humongous object which spans one or more regions that\n+  \/\/ were previously empty.  Regions that represent humongous objects are entirely dedicated to the humongous\n+  \/\/ object.  No other objects are packed into these regions.\n+  \/\/\n+  \/\/ Precondition: req.size() > ShenandoahHeapRegion::humongous_threshold_words().\n@@ -56,6 +416,0 @@\n-\n-  void recompute_bounds();\n-  void adjust_bounds();\n-  bool touches_bounds(size_t num) const;\n-\n-  void increase_used(size_t amount);\n@@ -63,0 +417,1 @@\n+  void try_recycle_trashed(ShenandoahHeapRegion *r);\n@@ -64,2 +419,2 @@\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+  inline bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  inline bool can_allocate_from(size_t idx) const;\n@@ -67,1 +422,4 @@\n-  void try_recycle_trashed(ShenandoahHeapRegion *r);\n+  inline bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+\n+  void find_regions_with_alloc_capacity(size_t &cset_regions);\n+  void reserve_regions(size_t to_reserve);\n@@ -69,3 +427,2 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  void prepare_to_rebuild(size_t &cset_regions);\n+  void finish_rebuild(size_t cset_regions);\n@@ -76,0 +433,4 @@\n+  \/\/ Public because ShenandoahRegionPartitions assertions require access.\n+  inline size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  inline size_t alloc_capacity(size_t idx) const;\n+\n@@ -79,1 +440,14 @@\n-  void recycle_trash();\n+#ifdef KELVIN_BAD_CODE\n+  void dump_bitmaps();\n+#endif\n+\n+  \/\/ Move up to cset_regions number of regions from being available to the collector to being available to the mutator.\n+  \/\/\n+  \/\/ Typical usage: At the end of evacuation, when the collector no longer needs the regions that had been reserved\n+  \/\/ for evacuation, invoke this to make regions available for mutator allocations.\n+  \/\/\n+  \/\/ Note that we plan to replenish the Collector reserve at the end of update refs, at which time all\n+  \/\/ of the regions recycled from the collection set will be available.  If the very unlikely event that there\n+  \/\/ are fewer regions in the collection set than remain in the collector set, we limit the transfer in order\n+  \/\/ to assure that the replenished Collector reserve can be sufficiently large.\n+  void move_regions_from_collector_to_mutator(size_t cset_regions);\n@@ -81,0 +455,1 @@\n+  void recycle_trash();\n@@ -83,5 +458,5 @@\n-  size_t capacity()  const { return _capacity; }\n-  size_t used()      const { return _used;     }\n-  size_t available() const {\n-    assert(_used <= _capacity, \"must use less than capacity\");\n-    return _capacity - _used;\n+  inline size_t capacity()  const { return _partitions.capacity_of(Mutator); }\n+  inline size_t used()      const { return _partitions.used_by(Mutator);     }\n+  inline size_t available() const {\n+    assert(used() <= capacity(), \"must use less than capacity\");\n+    return capacity() - used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":406,"deletions":31,"binary":false,"changes":437,"status":"modified"},{"patch":"@@ -0,0 +1,118 @@\n+\/*\n+ * Copyright (c) 2016, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHFREESET_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHFREESET_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+\n+inline ssize_t ShenandoahSimpleBitMap::find_next_set_bit(ssize_t start_idx, ssize_t boundary_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+  assert((boundary_idx > start_idx) && (boundary_idx <= _num_bits), \"precondition\");\n+  do {\n+    size_t array_idx = start_idx \/ _bits_per_array_element;\n+    size_t bit_number = start_idx % _bits_per_array_element;\n+    size_t element_bits = _bitmap[array_idx];\n+    if (bit_number > 0) {\n+      size_t mask_out = (((size_t) 0x01) << bit_number) - 1;\n+      element_bits &= ~mask_out;\n+    }\n+    if (element_bits) {\n+      \/\/ The next set bit is here\n+      size_t the_bit = ((size_t) 0x01) << bit_number;\n+      while (bit_number < _bits_per_array_element) {\n+        if (element_bits & the_bit) {\n+          ssize_t candidate_result = (array_idx * _bits_per_array_element) + bit_number;\n+          if (candidate_result < boundary_idx) return candidate_result;\n+          else return boundary_idx;\n+        } else {\n+          the_bit <<= 1;\n+          bit_number++;\n+        }\n+      }\n+      assert(false, \"should not reach here\");\n+    } else {\n+      \/\/ Next bit is not here.  Try the next array element\n+      start_idx += _bits_per_array_element - bit_number;\n+    }\n+  } while (start_idx < boundary_idx);\n+  return boundary_idx;\n+}\n+\n+inline ssize_t ShenandoahSimpleBitMap::find_next_set_bit(ssize_t start_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+  return find_next_set_bit(start_idx, _num_bits);\n+}\n+\n+inline ssize_t ShenandoahSimpleBitMap::find_prev_set_bit(ssize_t last_idx, ssize_t boundary_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  assert((boundary_idx >= -1) && (boundary_idx < last_idx), \"precondition\");\n+  do {\n+    ssize_t array_idx = last_idx \/ _bits_per_array_element;\n+    size_t bit_number = last_idx % _bits_per_array_element;\n+    size_t element_bits = _bitmap[array_idx];\n+    if (bit_number < _bits_per_array_element - 1){\n+      size_t mask_in = (((size_t) 0x1) << (bit_number + 1)) - 1;\n+      element_bits &= mask_in;\n+    }\n+    if (element_bits) {\n+      \/\/ The prev set bit is here\n+      size_t the_bit = ((size_t) 0x01) << bit_number;\n+\n+      for (ssize_t bit_iterator = bit_number; bit_iterator >= 0; bit_iterator--) {\n+        if (element_bits & the_bit) {\n+          ssize_t candidate_result = (array_idx * _bits_per_array_element) + bit_number;\n+          if (candidate_result > boundary_idx) return candidate_result;\n+          else return boundary_idx;\n+        } else {\n+          the_bit >>= 1;\n+          bit_number--;\n+        }\n+      }\n+      assert(false, \"should not reach here\");\n+    } else {\n+      \/\/ Next bit is not here.  Try the previous array element\n+      last_idx -= (bit_number + 1);\n+    }\n+  } while (last_idx > boundary_idx);\n+  return boundary_idx;\n+}\n+\n+inline ssize_t ShenandoahSimpleBitMap::find_prev_set_bit(ssize_t last_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  return find_prev_set_bit(last_idx, -1);\n+}\n+\n+inline ssize_t ShenandoahSimpleBitMap::find_next_consecutive_bits(size_t num_bits, ssize_t start_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+  return find_next_consecutive_bits(num_bits, start_idx, _num_bits);\n+}\n+\n+inline ssize_t ShenandoahSimpleBitMap::find_prev_consecutive_bits(size_t num_bits, ssize_t last_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  return find_prev_consecutive_bits(num_bits, last_idx, (ssize_t) -1);\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHFREESET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.inline.hpp","additions":118,"deletions":0,"binary":false,"changes":118,"status":"added"},{"patch":"@@ -912,1 +912,0 @@\n-    _heap->free_set()->clear();\n@@ -1089,1 +1088,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -377,1 +377,0 @@\n-\n@@ -2124,1 +2123,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2127,1 +2126,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2133,1 +2132,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2135,0 +2134,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_regions_from_collector_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,395 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.inline.hpp\"\n+\n+#include \"utilities\/ostream.hpp\"\n+\n+#include \"utilities\/vmassert_uninstall.hpp\"\n+#include <iostream>\n+#include \"utilities\/vmassert_reinstall.hpp\"\n+\n+#include \"unittest.hpp\"\n+\n+\n+class ShenandoahSimpleBitMapTest: public ::testing::Test {\n+protected:\n+  const ssize_t SMALL_BITMAP_SIZE =  512;\n+  const ssize_t LARGE_BITMAP_SIZE = 4096;\n+\n+  ShenandoahSimpleBitMap bm_small;\n+  ShenandoahSimpleBitMap bm_large;\n+\n+  ShenandoahSimpleBitMapTest():\n+      bm_small(SMALL_BITMAP_SIZE),\n+      bm_large(LARGE_BITMAP_SIZE) {\n+  }\n+};\n+\n+class BasicShenandoahSimpleBitMapTest: public ShenandoahSimpleBitMapTest {\n+protected:\n+\n+  \/\/ set_bits[] is an array of indexes holding bits that are supposed to be set, in increasing order.\n+  void verifyBitMapState(ShenandoahSimpleBitMap& bm, ssize_t size, ssize_t set_bits[], ssize_t num_set_bits) {\n+\n+    \/\/ Verify number of bits\n+    ASSERT_EQ(bm.number_of_bits(), size);\n+\n+    ssize_t set_bit_index = 0;\n+    \/\/ Check that is_set(idx) for every possible idx\n+    for (ssize_t i = 0; i < size; i++) {\n+      bool is_set = bm_small.is_set(i);\n+      bool intended_value = false;;\n+      if (set_bit_index < num_set_bits) {\n+        if (set_bits[set_bit_index] == i) {\n+          intended_value = true;\n+          set_bit_index++;\n+        }\n+      }\n+      ASSERT_EQ(is_set, intended_value);\n+    }\n+\n+    \/\/ Check that bits_at(array_idx) matches intended value for every valid array_idx value\n+    set_bit_index = 0;\n+    ssize_t alignment = bm_small.alignment();\n+    ssize_t small_words = size \/ alignment;\n+    for (ssize_t i = 0; i < small_words; i += alignment) {\n+      size_t bits = bm_small.bits_at(i);\n+      for (ssize_t b = 0; b < alignment; b++) {\n+        ssize_t bit_value = i * alignment + b;\n+        bool intended_value = false;;\n+        if (set_bit_index < num_set_bits) {\n+          if (set_bits[set_bit_index] == bit_value) {\n+            intended_value = true;\n+          }\n+        }\n+        size_t bit_mask = ((size_t) 0x01) << b;\n+        bool is_set = (bits & bit_mask) != 0;\n+        ASSERT_EQ(is_set, intended_value);\n+      }\n+    }\n+\n+    \/\/ Make sure find_next_set_bit() works correctly\n+    ssize_t probe_point = 0;\n+    for (ssize_t i = 0; i < num_set_bits; i++) {\n+      ssize_t next_expected_bit = set_bits[i];\n+      probe_point = bm.find_next_set_bit(probe_point);\n+      ASSERT_EQ(probe_point, next_expected_bit);\n+      probe_point++;            \/\/ Prepare to look beyond the most recent bit.\n+    }\n+    probe_point = bm.find_next_set_bit(probe_point);\n+    ASSERT_EQ(probe_point, size); \/\/ Verify that last failed search returns sentinel value: num bits in bit map\n+\n+    \/\/ Confirm that find_next_set_bit() with a bounded search space works correctly\n+    \/\/ Limit this search to the first 3\/4 of the full bit map\n+    ssize_t boundary_idx = 3 * size \/ 4;\n+    probe_point = 0;\n+    for (ssize_t i = 0; i < num_set_bits; i++) {\n+      ssize_t next_expected_bit = set_bits[i];\n+      probe_point = bm.find_next_set_bit(probe_point, boundary_idx);\n+      if (next_expected_bit >= boundary_idx) {\n+        \/\/ Verify that last failed search returns sentinel value: boundary_idx\n+        ASSERT_EQ(probe_point, boundary_idx);\n+        break;\n+      } else {\n+        ASSERT_EQ(probe_point, next_expected_bit);\n+        probe_point++;            \/\/ Prepare to look beyond the most recent bit.\n+      }\n+    }\n+    if (probe_point < boundary_idx) {\n+      \/\/ In case there are no set bits in the last 1\/4 of bit map, confirm that last failed search returns sentinel: boundary_idx\n+      probe_point = bm.find_next_set_bit(probe_point, boundary_idx);\n+      ASSERT_EQ(probe_point, boundary_idx);\n+    }\n+\n+    \/\/ Make sure find_prev_set_bit() works correctly\n+    probe_point = size - 1;\n+    for (ssize_t i = num_set_bits - 1; i >= 0; i--) {\n+      ssize_t next_expected_bit = set_bits[i];\n+      probe_point = bm.find_prev_set_bit(probe_point);\n+      ASSERT_EQ(probe_point, next_expected_bit);\n+      probe_point--;            \/\/ Prepare to look before the most recent bit.\n+    }\n+    probe_point = bm.find_prev_set_bit(probe_point);\n+    ASSERT_EQ(probe_point, -1); \/\/ Verify that last failed search returns sentinel value: -1\n+\n+    \/\/ Confirm that find_prev_set_bit() with a bounded search space works correctly\n+    \/\/ Limit this search to the last 3\/4 of the full bit map\n+    boundary_idx = size \/ 4;\n+    probe_point = size - 1;\n+    for (ssize_t i = num_set_bits - 1; i >= 0; i--) {\n+      ssize_t next_expected_bit = set_bits[i];\n+      probe_point = bm.find_next_set_bit(probe_point, boundary_idx);\n+      if (next_expected_bit <= boundary_idx) {\n+        \/\/ Verify that last failed search returns sentinel value: boundary_idx\n+        ASSERT_EQ(probe_point, boundary_idx);\n+        break;\n+      } else {\n+        ASSERT_EQ(probe_point, next_expected_bit);\n+        probe_point--;            \/\/ Prepare to look beyond the most recent bit.\n+      }\n+    }\n+    if (probe_point >= boundary_idx) {\n+      probe_point = bm.find_next_set_bit(probe_point, boundary_idx);\n+        \/\/ Verify that last failed search returns sentinel value: boundary_idx\n+      ASSERT_EQ(probe_point, boundary_idx);\n+    }\n+\n+    \/\/ What's the longest cluster of consecutive bits\n+    ssize_t previous_value = -2;\n+    ssize_t longest_run = 0;\n+    ssize_t current_run = 0;\n+    for (ssize_t i = 0; i < num_set_bits; i++) {\n+      ssize_t next_expected_bit = set_bits[i];\n+      if (next_expected_bit == previous_value + 1) {\n+        current_run++;\n+      } else {\n+        previous_value = next_expected_bit;\n+        current_run = 1;\n+      }\n+      if (current_run > longest_run) {\n+        longest_run = current_run;\n+      }\n+      previous_value = next_expected_bit;\n+    }\n+\n+    \/\/ Confirm that find_next_consecutive_bits() works for each cluster size known to have at least one match\n+    for (ssize_t cluster_size = 1; cluster_size <= longest_run; cluster_size++) {\n+\n+      \/\/ Verify that find_next_consecutive_bits() works\n+      ssize_t bit_idx = 0;\n+      ssize_t probe_point = 0;\n+      while (probe_point <= size - cluster_size) {\n+        bool cluster_found = false;\n+        while (!cluster_found && (bit_idx <= num_set_bits - cluster_size)) {\n+          cluster_found = true;\n+          for (ssize_t i = 1; i < cluster_size; i++) {\n+            if (set_bits[bit_idx] + i != set_bits[bit_idx + i]) {\n+              cluster_found = false;\n+              break;\n+            }\n+          }\n+        }\n+        if (cluster_found) {\n+          ssize_t next_expected_cluster = bit_idx;\n+          probe_point = bm.find_next_consecutive_bits(cluster_size, probe_point);\n+          ASSERT_EQ(next_expected_cluster, probe_point);\n+          probe_point++;\n+          bit_idx++;\n+        } else {\n+          bit_idx++;\n+        }\n+      }\n+      \/\/ Confirm that the last request, which fails to find a cluster, returns sentinel value: num_bits\n+      probe_point = bm.find_next_consecutive_bits(cluster_size, probe_point);\n+      ASSERT_EQ(probe_point, size);\n+\n+      \/\/ Repeat the above experiment, using 3\/4 size as the search boundary_idx\n+      bit_idx = 0;\n+      probe_point = 0;\n+      boundary_idx = 4 * size \/ 4;\n+      while (probe_point <= boundary_idx - cluster_size) {\n+        bool cluster_found = false;\n+        while (!cluster_found && (bit_idx <= num_set_bits - cluster_size)) {\n+          cluster_found = true;\n+          for (int i = 1; i < cluster_size; i++) {\n+            if (set_bits[bit_idx] + i != set_bits[bit_idx + i]) {\n+              cluster_found = false;\n+              break;\n+            }\n+          }\n+        }\n+        if (cluster_found) {\n+          ssize_t next_expected_cluster = set_bits[bit_idx];\n+          probe_point = bm.find_next_consecutive_bits(cluster_size, probe_point, boundary_idx);\n+          ASSERT_EQ(next_expected_cluster, probe_point);\n+          probe_point++;\n+          bit_idx++;\n+        } else {\n+          bit_idx++;\n+        }\n+      }\n+      \/\/ Confirm that the last request, which fails to find a cluster, returns sentinel value: boundary_idx\n+      probe_point = bm.find_prev_consecutive_bits(cluster_size, probe_point, boundary_idx);\n+      ASSERT_EQ(probe_point, boundary_idx);\n+\n+      \/\/ Verify that find_prev_consecutive_bits() works\n+      bit_idx = num_set_bits - 1;\n+      probe_point = size - 1;\n+      while (probe_point >= cluster_size - 1) {\n+        bool cluster_found = false;\n+        while (!cluster_found && (bit_idx - cluster_size >= -1)) {\n+          cluster_found = true;\n+          for (int i = 1; i < cluster_size; i++) {\n+            if (set_bits[bit_idx] - i != set_bits[bit_idx - i]) {\n+              cluster_found = false;\n+              break;\n+            }\n+          }\n+        }\n+        if (cluster_found) {\n+          ssize_t next_expected_cluster = set_bits[bit_idx];\n+          probe_point = bm.find_prev_consecutive_bits(cluster_size, probe_point);\n+          ASSERT_EQ(next_expected_cluster, probe_point);\n+          probe_point--;\n+          bit_idx--;\n+        } else {\n+          bit_idx--;\n+        }\n+      }\n+      \/\/ Confirm that the last request, which fails to find a cluster, returns sentinel value: -1\n+      probe_point = bm.find_prev_consecutive_bits(cluster_size, probe_point);\n+      ASSERT_EQ(probe_point, -1);\n+\n+      \/\/ Verify that find_prev_consecutive_bits() works with the search range bounded at 1\/4 size\n+      bit_idx = num_set_bits - 1;\n+      probe_point = size - 1;\n+      boundary_idx = size \/ 4;\n+      while (probe_point >= boundary_idx - 1 + cluster_size) {\n+        bool cluster_found = false;\n+        while (!cluster_found && (bit_idx - cluster_size >= -1)) {\n+          cluster_found = true;\n+          for (int i = 1; i < cluster_size; i++) {\n+            if (set_bits[bit_idx] - i != set_bits[bit_idx - i]) {\n+              cluster_found = false;\n+              break;\n+            }\n+          }\n+        }\n+        if (cluster_found) {\n+          ssize_t next_expected_cluster = set_bits[bit_idx];\n+          probe_point = bm.find_prev_consecutive_bits(cluster_size, probe_point, boundary_idx);\n+          ASSERT_EQ(next_expected_cluster, probe_point);\n+          probe_point--;\n+          bit_idx--;\n+        } else {\n+          bit_idx--;\n+        }\n+      }\n+      \/\/ Confirm that the last request, which fails to find a cluster, returns sentinel value: boundary_idx\n+      probe_point = bm.find_prev_consecutive_bits(cluster_size, probe_point, boundary_idx);\n+      ASSERT_EQ(probe_point, boundary_idx);\n+    }\n+\n+    \/\/ Confirm that find_next_consecutive_bits() works for each cluster sizes known not to have any matches\n+    probe_point = bm.find_next_consecutive_bits(longest_run + 1, 0);\n+    ASSERT_EQ(probe_point, size);  \/\/ Confirm: failed search returns sentinel: size\n+\n+    probe_point = bm.find_prev_consecutive_bits(longest_run + 1, size);\n+    ASSERT_EQ(probe_point, -1);    \/\/ Confirm: failed search returns sentinel: -1\n+\n+    boundary_idx = 3 * size \/ 4;\n+    probe_point = bm.find_next_consecutive_bits(longest_run + 1, 0, boundary_idx);\n+    ASSERT_EQ(probe_point, boundary_idx); \/\/ Confirm: failed search returns sentinel: boundary_idx\n+\n+    boundary_idx = size \/ 4;\n+    probe_point = bm.find_prev_consecutive_bits(longest_run + 1, size, boundary_idx);\n+    ASSERT_EQ(probe_point, -1);           \/\/ Confirm: failed search returns sentinel: -1\n+  }\n+\n+\n+  BasicShenandoahSimpleBitMapTest() {\n+\n+    \/\/ Initial state of each bitmap is all bits are clear.  Confirm this:\n+    ssize_t set_bits_0[1] = { 0 };\n+    verifyBitMapState(bm_small, SMALL_BITMAP_SIZE, set_bits_0, 0);\n+    verifyBitMapState(bm_large, LARGE_BITMAP_SIZE, set_bits_0, 0);\n+\n+    bm_small.set_bit(5);\n+    bm_small.set_bit(63);\n+    bm_small.set_bit(128);\n+    ssize_t set_bits_1[3] = { 5, 63, 128 };\n+    verifyBitMapState(bm_small, SMALL_BITMAP_SIZE, set_bits_1, 3);\n+\n+    bm_large.set_bit(5);\n+    bm_large.set_bit(63);\n+    bm_large.set_bit(128);\n+    verifyBitMapState(bm_large, LARGE_BITMAP_SIZE, set_bits_1, 3);\n+\n+    \/\/ Test some consecutive bits\n+    bm_small.set_bit(140);\n+    bm_small.set_bit(141);\n+    bm_small.set_bit(142);\n+\n+    bm_small.set_bit(253);\n+    bm_small.set_bit(254);\n+    bm_small.set_bit(255);\n+\n+    bm_small.set_bit(271);\n+    bm_small.set_bit(272);\n+\n+    bm_small.set_bit(320);\n+    bm_small.set_bit(321);\n+    bm_small.set_bit(322);\n+\n+    bm_small.set_bit(361);\n+\n+    ssize_t set_bits_2[15] = { 5, 63, 128, 140, 141, 142, 253, 254, 255, 271, 272, 320, 321, 322, 361 };\n+    verifyBitMapState(bm_small, SMALL_BITMAP_SIZE, set_bits_2, 15);\n+\n+    bm_large.set_bit(140);\n+    bm_large.set_bit(141);\n+    bm_large.set_bit(142);\n+\n+    bm_large.set_bit(1021);\n+    bm_large.set_bit(1022);\n+    bm_large.set_bit(1023);\n+\n+    bm_large.set_bit(1051);\n+\n+    bm_large.set_bit(1280);\n+    bm_large.set_bit(1281);\n+    bm_large.set_bit(1282);\n+\n+    bm_large.set_bit(1300);\n+    bm_large.set_bit(1301);\n+    bm_large.set_bit(1302);\n+\n+    ssize_t set_bits_3[16] = { 5, 63, 128, 140, 141, 142, 1021, 1022, 1023, 1051, 1280, 1281, 1282, 1300, 1301, 1302 };\n+    verifyBitMapState(bm_large, LARGE_BITMAP_SIZE, set_bits_3, 16);\n+\n+    \/\/ Test clear_bit\n+    bm_small.clear_bit(141);\n+    bm_small.clear_bit(253);\n+    ssize_t set_bits_4[13] = { 5, 63, 128, 140, 142, 254, 255, 271, 272, 320, 321, 322, 361 };\n+    verifyBitMapState(bm_small, SMALL_BITMAP_SIZE, set_bits_2, 13);\n+\n+    bm_large.clear_bit(5);\n+    bm_large.clear_bit(63);\n+    bm_large.clear_bit(128);\n+    bm_large.clear_bit(141);\n+    ssize_t set_bits_5[12] = { 140, 142, 1021, 1022, 1023, 1051, 1280, 1281, 1282, 1300, 1301, 1302 };\n+    verifyBitMapState(bm_large, LARGE_BITMAP_SIZE, set_bits_5, 12);\n+\n+    \/\/ Test clear_all()\n+    bm_small.clear_all();\n+    bm_large.clear_all();\n+\n+  }\n+};\n+\n+TEST(BasicShenandoahSimpleBitMapTest, minimum_test) {\n+}\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahSimpleBitMap.cpp","additions":395,"deletions":0,"binary":false,"changes":395,"status":"added"}]}