{"files":[{"patch":"@@ -1001,1 +1001,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -35,5 +37,14 @@\n-ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n-  _heap(heap),\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+static const char* free_memory_type_name(ShenandoahFreeMemoryType t) {\n+  switch (t) {\n+    case NotFree: return \"NotFree\";\n+    case Mutator: return \"Mutator\";\n+    case Collector: return \"Collector\";\n+    case NumFreeSets: return \"NumFreeSets\";\n+    default: return \"Unrecognized\";\n+  }\n+}\n+\n+ShenandoahSetsOfFree::ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _free_set(free_set),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes())\n@@ -41,0 +52,1 @@\n+  _membership = NEW_C_HEAP_ARRAY(ShenandoahFreeMemoryType, max_regions, mtGC);\n@@ -44,3 +56,134 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n-  shenandoah_assert_heaplocked();\n-  _used += num_bytes;\n+ShenandoahSetsOfFree::~ShenandoahSetsOfFree() {\n+  FREE_C_HEAP_ARRAY(ShenandoahFreeMemoryType, _membership);\n+}\n+\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+inline bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n+  }\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+inline bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+void ShenandoahSetsOfFree::clear_internal() {\n+  for (size_t idx = 0; idx < _max; idx++) {\n+    _membership[idx] = NotFree;\n+  }\n+\n+  for (size_t idx = 0; idx < NumFreeSets; idx++) {\n+    _leftmosts[idx] = _max;\n+    _rightmosts[idx] = 0;\n+    _leftmosts_empty[idx] = _max;\n+    _rightmosts_empty[idx] = 0;\n+    _capacity_of[idx] = 0;\n+    _used_by[idx] = 0;\n+  }\n+\n+  _region_counts[Mutator] = 0;\n+  _region_counts[Collector] = 0;\n+  _region_counts[NotFree] = _max;\n+}\n+\n+void ShenandoahSetsOfFree::clear_all() {\n+  clear_internal();\n+}\n+\n+void ShenandoahSetsOfFree::increase_used(ShenandoahFreeMemoryType which_set, size_t bytes) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"Set must correspond to a valid freeset\");\n+  _used_by[which_set] += bytes;\n+  assert (_used_by[which_set] <= _capacity_of[which_set],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used_by[which_set], _capacity_of[which_set], bytes);\n+}\n+\n+inline void ShenandoahSetsOfFree::shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx) {\n+  if (idx == _leftmosts[set]) {\n+    while ((_leftmosts[set] < _max) && !in_free_set(_leftmosts[set], set)) {\n+      _leftmosts[set]++;\n+    }\n+    if (_leftmosts_empty[set] < _leftmosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[set] = _leftmosts[set];\n+    }\n+  }\n+  if (idx == _rightmosts[set]) {\n+    while (_rightmosts[set] > 0 && !in_free_set(_rightmosts[set], set)) {\n+      _rightmosts[set]--;\n+    }\n+    if (_rightmosts_empty[set] > _rightmosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[set] = _rightmosts[set];\n+    }\n+  }\n+}\n+\n+inline void ShenandoahSetsOfFree::expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t region_available) {\n+  if (region_available == _region_size_bytes) {\n+    if (_leftmosts_empty[set] > idx) {\n+      _leftmosts_empty[set] = idx;\n+    }\n+    if (_rightmosts_empty[set] < idx) {\n+      _rightmosts_empty[set] = idx;\n+    }\n+  }\n+  if (_leftmosts[set] > idx) {\n+    _leftmosts[set] = idx;\n+  }\n+  if (_rightmosts[set] < idx) {\n+    _rightmosts[set] = idx;\n+  }\n+}\n+\n+\/\/ Remove this region from its free set, but leave its capacity and used as part of the original free set's totals.\n+\/\/ When retiring a region, add any remnant of available memory within the region to the used total for the original free set.\n+void ShenandoahSetsOfFree::retire_within_free_set(size_t idx, size_t used_bytes) {\n+  ShenandoahFreeMemoryType orig_set = membership(idx);\n+\n+  \/\/ Note: we may remove from free set even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (orig_set > NotFree && orig_set < NumFreeSets, \"Cannot remove from free sets if not already free\");\n+\n+  if (used_bytes < _region_size_bytes) {\n+    \/\/ Count the alignment pad remnant of memory as used when we retire this region\n+    increase_used(orig_set, _region_size_bytes - used_bytes);\n+  }\n+\n+  _membership[idx] = NotFree;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[NotFree]++;\n+}\n+\n+void ShenandoahSetsOfFree::make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  _membership[idx] = which_set;\n+  _capacity_of[which_set] += _region_size_bytes;\n+  _used_by[which_set] += _region_size_bytes - available;\n+  expand_bounds_maybe(which_set, idx, available);\n@@ -48,2 +191,2 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+  _region_counts[NotFree]--;\n+  _region_counts[which_set]++;\n@@ -52,4 +195,31 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+void ShenandoahSetsOfFree::move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert ((new_set > NotFree) && (new_set < NumFreeSets), \"New set must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  ShenandoahFreeMemoryType orig_set = _membership[idx];\n+  assert ((orig_set > NotFree) && (orig_set < NumFreeSets), \"Cannot move free unless already free\");\n+\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild:         Mutator => Collector\n+  \/\/  During flip_to_gc:      Mutator empty => Collector\n+  \/\/ At start of update refs: Collector => Mutator\n+  assert (((available <= _region_size_bytes) &&\n+           (((orig_set == Mutator) && (new_set == Collector)) ||\n+            ((orig_set == Collector) && (new_set == Mutator)))) ||\n+          ((available == _region_size_bytes) &&\n+           ((orig_set == Mutator) && (new_set == Collector))), \"Unexpected movement between sets\");\n+\n+\n+  size_t used = _region_size_bytes - available;\n+  _membership[idx] = new_set;\n+  _capacity_of[orig_set] -= _region_size_bytes;\n+  _used_by[orig_set] -= used;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _capacity_of[new_set] += _region_size_bytes;;\n+  _used_by[new_set] += used;\n+  expand_bounds_maybe(new_set, idx, available);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[new_set]++;\n@@ -58,4 +228,171 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+inline ShenandoahFreeMemoryType ShenandoahSetsOfFree::membership(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  return _membership[idx];\n+}\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+inline bool ShenandoahSetsOfFree::in_free_set(size_t idx, ShenandoahFreeMemoryType test_set) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  if (_membership[idx] == test_set) {\n+    assert (test_set == NotFree || _free_set->alloc_capacity(idx) > 0,\n+            \"Free region \" SIZE_FORMAT \", belonging to %s free set, must have alloc capacity\",\n+            idx, free_memory_type_name(test_set));\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::leftmost(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _leftmosts[which_set];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    assert (in_free_set(idx, which_set), \"left-most region must be free\");\n+    return idx;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::rightmost(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _rightmosts[which_set];\n+  assert ((_leftmosts[which_set] == _max) || in_free_set(idx, which_set), \"right-most region must be free\");\n+  return idx;\n+}\n+\n+inline bool ShenandoahSetsOfFree::is_empty(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return (leftmost(which_set) > rightmost(which_set));\n+}\n+\n+size_t ShenandoahSetsOfFree::leftmost_empty(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (size_t idx = _leftmosts_empty[which_set]; idx < _max; idx++) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _leftmosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_set] = _max;\n+  _rightmosts_empty[which_set] = 0;\n+  return _max;\n+}\n+\n+inline size_t ShenandoahSetsOfFree::rightmost_empty(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (intptr_t idx = _rightmosts_empty[which_set]; idx >= 0; idx--) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _rightmosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_set] = _max;\n+  _rightmosts_empty[which_set] = 0;\n+  return 0;\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahSetsOfFree::assert_bounds() {\n+\n+  size_t leftmosts[NumFreeSets];\n+  size_t rightmosts[NumFreeSets];\n+  size_t empty_leftmosts[NumFreeSets];\n+  size_t empty_rightmosts[NumFreeSets];\n+\n+  for (int i = 0; i < NumFreeSets; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = 0;\n+    empty_rightmosts[i] = 0;\n+  }\n+\n+  for (size_t i = 0; i < _max; i++) {\n+    ShenandoahFreeMemoryType set = membership(i);\n+    switch (set) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < leftmosts[set]) {\n+          leftmosts[set] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[set])) {\n+          empty_leftmosts[set] = i;\n+        }\n+        if (i > rightmosts[set]) {\n+          rightmosts[set] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[set])) {\n+          empty_rightmosts[set] = i;\n+        }\n+        break;\n+      }\n+\n+      case NumFreeSets:\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Mutator),  _max);\n+  assert (rightmost(Mutator) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Mutator),  _max);\n+\n+  assert (leftmost(Mutator) == _max || in_free_set(leftmost(Mutator), Mutator),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Mutator));\n+  assert (leftmost(Mutator) == _max || in_free_set(rightmost(Mutator), Mutator),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Mutator));\n+\n+  \/\/ If Mutator set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  size_t beg_off = leftmosts[Mutator];\n+  size_t end_off = rightmosts[Mutator];\n+  assert (beg_off >= leftmost(Mutator),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Mutator));\n+  assert (end_off <= rightmost(Mutator),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Mutator));\n+\n+  beg_off = empty_leftmosts[Mutator];\n+  end_off = empty_rightmosts[Mutator];\n+  assert (beg_off >= leftmost_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Mutator));\n+  assert (end_off <= rightmost_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Collector),  _max);\n+  assert (rightmost(Collector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Collector),  _max);\n+\n+  assert (leftmost(Collector) == _max || in_free_set(leftmost(Collector), Collector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Collector));\n+  assert (leftmost(Collector) == _max || in_free_set(rightmost(Collector), Collector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Collector));\n+\n+  \/\/ If Collector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = leftmosts[Collector];\n+  end_off = rightmosts[Collector];\n+  assert (beg_off >= leftmost(Collector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Collector));\n+  assert (end_off <= rightmost(Collector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Collector));\n+\n+  beg_off = empty_leftmosts[Collector];\n+  end_off = empty_rightmosts[Collector];\n+  assert (beg_off >= leftmost_empty(Collector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Collector));\n+  assert (end_off <= rightmost_empty(Collector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Collector));\n+}\n+#endif\n+\n+ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n+  _heap(heap),\n+  _free_sets(max_regions, this)\n+{\n+  clear_internal();\n@@ -65,0 +402,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -77,0 +416,2 @@\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+\n@@ -80,1 +421,0 @@\n-\n@@ -82,5 +422,13 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n+      \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+      if (!_free_sets.is_empty(Mutator)) {\n+        \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+        int leftmost = (int) _free_sets.leftmost(Mutator);\n+        for (int idx = (int) _free_sets.rightmost(Mutator); idx >= leftmost; idx--) {\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (_free_sets.in_free_set(idx, Mutator)) {\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n@@ -90,1 +438,0 @@\n-\n@@ -95,0 +442,3 @@\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n+      \/\/ the relevant evac_expended rather than used value.\n+\n@@ -97,1 +447,0 @@\n-\n@@ -99,1 +448,1 @@\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+      for (size_t c = _free_sets.rightmost(Collector) + 1; c > _free_sets.leftmost(Collector); c--) {\n@@ -101,1 +450,1 @@\n-        if (is_collector_free(idx)) {\n+        if (_free_sets.in_free_set(idx, Collector)) {\n@@ -114,2 +463,2 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n+      \/\/ Try to steal an empty region from the mutator view.\n+      for (size_t c = _free_sets.rightmost_empty(Mutator) + 1; c > _free_sets.leftmost_empty(Mutator); c--) {\n@@ -117,1 +466,1 @@\n-        if (is_mutator_free(idx)) {\n+        if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -123,0 +472,1 @@\n+              log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n@@ -129,4 +479,2 @@\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n-      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n-      \/\/ allocations.\n-\n+      \/\/ No dice. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+      \/\/ due to GC allocations would expose unparsable mutator allocations.\n@@ -138,1 +486,0 @@\n-\n@@ -143,4 +490,2 @@\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n-\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n@@ -150,0 +495,1 @@\n+  HeapWord* result = nullptr;\n@@ -151,1 +497,0 @@\n-\n@@ -154,2 +499,4 @@\n-  HeapWord* result = nullptr;\n-  size_t size = req.size();\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  }\n@@ -157,0 +504,1 @@\n+  \/\/ req.size() is in words, r->free() is in bytes.\n@@ -158,0 +506,2 @@\n+    \/\/ This is a GCLAB or a TLAB allocation\n+    size_t adjusted_size = req.size();\n@@ -159,2 +509,2 @@\n-    if (size > free) {\n-      size = free;\n+    if (adjusted_size > free) {\n+      adjusted_size = free;\n@@ -162,3 +512,11 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+    if (adjusted_size >= req.min_size()) {\n+      result = r->allocate(adjusted_size, req.type());\n+      log_debug(gc, free)(\"Allocated \" SIZE_FORMAT \" words (adjusted from \" SIZE_FORMAT \") for %s @\" PTR_FORMAT\n+                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n+                          adjusted_size, req.size(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n+                          free_memory_type_name(_free_sets.membership(r->index())),  r->index(), r->free());\n+      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+      req.set_actual_size(adjusted_size);\n+    } else {\n+      log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                          \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n@@ -167,0 +525,1 @@\n+    size_t size = req.size();\n@@ -168,0 +527,8 @@\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      log_debug(gc, free)(\"Allocated \" SIZE_FORMAT \" words for %s @\" PTR_FORMAT\n+                          \" from %s region \" SIZE_FORMAT \", free bytes remaining: \" SIZE_FORMAT,\n+                          size, ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(result),\n+                          free_memory_type_name(_free_sets.membership(r->index())),  r->index(), r->free());\n+      req.set_actual_size(size);\n+    }\n@@ -173,5 +540,3 @@\n-      increase_used(size * HeapWordSize);\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n+      _free_sets.increase_used(Mutator, req.actual_size() * HeapWordSize);\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n@@ -179,1 +544,2 @@\n-    if (req.is_gc_alloc()) {\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.\n@@ -184,7 +550,3 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n-    \/\/\n-    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+  if (alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n+    \/\/ Regardless of whether this allocation succeeded, if the remaining memory is less than PLAB:min_size(), retire this region.\n+    \/\/ Note that retire_within_free_set() increased used to account for waste.\n@@ -192,8 +554,4 @@\n-    \/\/ Record the remainder as allocation waste\n-    if (req.is_mutator_alloc()) {\n-      size_t waste = r->free();\n-      if (waste > 0) {\n-        increase_used(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n-      }\n-    }\n+    \/\/ Note that a previous implementation of this function would retire a region following any failure to\n+    \/\/ allocate within.  This was observed to result in large amounts of available memory being ignored\n+    \/\/ following a failed shared allocation request.  TLAB requests will generally downsize to absorb all\n+    \/\/ memory available within the region even if this is less than the desired size.\n@@ -201,8 +559,3 @@\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n-    }\n-    assert_bounds();\n+    size_t idx = r->index();\n+    _free_sets.retire_within_free_set(idx, r->used());\n+    _free_sets.assert_bounds();\n@@ -213,32 +566,0 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n-}\n-\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n-\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n-  }\n-}\n-\n@@ -251,2 +572,2 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (num > _free_sets.count(Mutator)) {\n@@ -259,1 +580,1 @@\n-  size_t beg = _mutator_leftmost;\n+  size_t beg = _free_sets.leftmost(Mutator);\n@@ -263,1 +584,1 @@\n-    if (end >= _max) {\n+    if (end >= _free_sets.max()) {\n@@ -270,1 +591,1 @@\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!_free_sets.in_free_set(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n@@ -308,0 +629,1 @@\n+    r->set_update_watermark(r->bottom());\n@@ -310,1 +632,2 @@\n-    _mutator_free_bitmap.clear_bit(r->index());\n+    \/\/ While individual regions report their true use, all humongous regions are marked used in the free set.\n+    _free_sets.retire_within_free_set(r->index(), ShenandoahHeapRegion::region_size_bytes());\n@@ -313,15 +636,3 @@\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-\n-  if (remainder != 0) {\n-    \/\/ Record this remainder as allocation waste\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n-  }\n-\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n-\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _free_sets.increase_used(Mutator, total_humongous_size);\n+  _free_sets.assert_bounds();\n@@ -332,17 +643,0 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n-  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n-}\n-\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n-  if (r->is_trash()) {\n-    \/\/ This would be recycled on allocation path\n-    return ShenandoahHeapRegion::region_size_bytes();\n-  } else {\n-    return r->free();\n-  }\n-}\n-\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n-  return alloc_capacity(r) == 0;\n-}\n-\n@@ -373,1 +667,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n@@ -376,6 +670,3 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n-\n-  _capacity -= alloc_capacity(r);\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, Collector, region_capacity);\n+  _free_sets.assert_bounds();\n@@ -383,4 +674,2 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -395,8 +684,1 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _free_sets.clear_all();\n@@ -405,4 +687,5 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n-\n+\/\/ This function places all regions that have allocation capacity into the mutator_set.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector set with the intent of packing collector memory into the\n+\/\/  highest (rightmost) addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &cset_regions) {\n+  cset_regions = 0;\n@@ -411,0 +694,5 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection set but have not yet been \"cleaned up\".\n+      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      cset_regions++;\n+    }\n@@ -412,1 +700,2 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n+      assert(_free_sets.in_free_set(idx, NotFree), \"We are about to make region free; it should not be free already\");\n@@ -414,2 +703,18 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      size_t ac = alloc_capacity(region);\n+      if (ac > PLAB::min_size() * HeapWordSize) {\n+        _free_sets.make_free(idx, Mutator, ac);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      } else {\n+        assert(_free_sets.membership(idx) == NotFree,\n+               \"Region \" SIZE_FORMAT \" should not be in free set because capacity is \" SIZE_FORMAT, idx, ac);\n+      }\n+    } else {\n+      assert(_free_sets.membership(idx) == NotFree,\n+             \"Region \" SIZE_FORMAT \" should not be in free set because alloc is not allowed and not is trash\", idx);\n+    }\n+  }\n+}\n@@ -417,2 +722,22 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+\/\/ Move no more than max_xfer_regions from the existing Collector free sets to the Mutator free set.\n+\/\/ This is called from outside the heap lock at the start of update refs.  At this point, we no longer\n+\/\/ need to reserve memory within for evacuation.  (We will create a new reserve after update refs finishes,\n+\/\/ setting aside some of the memory that was reclaimed by the most recent GC.  This new reserve will satisfy\n+\/\/ the evacuation needs of the next GC pass.)\n+void ShenandoahFreeSet::move_collector_sets_to_mutator(size_t max_xfer_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t collector_empty_xfer = 0;\n+  size_t collector_not_empty_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(Collector) <= _free_sets.rightmost_empty(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(Collector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(Collector)); idx++) {\n+      if (_free_sets.in_free_set(idx, Collector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        collector_empty_xfer += region_size_bytes;\n+      }\n+    }\n+  }\n@@ -420,2 +745,10 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+  \/\/ If there are any non-empty regions within Collector set, we can also move them to the Mutator free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost(Collector) <= _free_sets.rightmost(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost(Collector); (max_xfer_regions > 0) && (idx <= _free_sets.rightmost(Collector)); idx++) {\n+      size_t alloc_capacity = this->alloc_capacity(idx);\n+      if (_free_sets.in_free_set(idx, Collector) && (alloc_capacity > 0)) {\n+        _free_sets.move_to_set(idx, Mutator, alloc_capacity);\n+        max_xfer_regions--;\n+        collector_not_empty_xfer += alloc_capacity;\n+      }\n@@ -425,3 +758,4 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n-  size_t reserved = 0;\n+  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n+  log_info(gc, free)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve\",\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer));\n+}\n@@ -429,2 +763,0 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n@@ -432,7 +764,65 @@\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n+\/\/ Overwrite arguments to represent the number of regions to be reclaimed from the cset\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &cset_regions) {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.\n+  find_regions_with_alloc_capacity(cset_regions);\n+}\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t cset_regions) {\n+  shenandoah_assert_heaplocked();\n+\n+  \/\/ Our desire is to reserve this much memory for future evacuation.  We may end up reserving less, if\n+  \/\/ memory is in short supply.\n+\n+  size_t reserve = _heap->max_capacity() * ShenandoahEvacReserve \/ 100;\n+  size_t available_in_collector_set = _free_sets.capacity_of(Collector) - _free_sets.used_by(Collector);\n+  size_t additional_reserve;\n+  if (available_in_collector_set < reserve) {\n+    additional_reserve = reserve - available_in_collector_set;\n+  } else {\n+    additional_reserve = 0;\n+  }\n+\n+  reserve_regions(reserve);\n+  _free_sets.assert_bounds();\n+  log_status();\n+}\n+\n+void ShenandoahFreeSet::rebuild() {\n+  size_t cset_regions;\n+  prepare_to_rebuild(cset_regions);\n+  finish_rebuild(cset_regions);\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator set, move some of these regions from\n+\/\/ the mutator set into the collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (!_free_sets.in_free_set(idx, Mutator)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+\n+    bool move_to_collector = _free_sets.capacity_of(Collector) < to_reserve;\n+    if (!move_to_collector) {\n+      \/\/ We've satisfied to_reserve\n+      break;\n+    }\n+\n+    if (move_to_collector) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n+      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+      \/\/ ephemeral objects.\n+      _free_sets.move_to_set(idx, Collector, ac);\n+      log_debug(gc,free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n@@ -442,2 +832,7 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  if (LogTarget(Info, gc, free)::is_enabled()) {\n+    size_t reserve = _free_sets.capacity_of(Collector);\n+    if (reserve < to_reserve) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+    }\n+  }\n@@ -449,1 +844,54 @@\n-  LogTarget(Info, gc, ergo) lt;\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  if (LogTarget(Debug, gc, free)::is_enabled()) {\n+#define BUFFER_SIZE 80\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t consumed_collector = 0;\n+    size_t available_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_mutator = 0;\n+\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_debug(gc, free)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free H:humongous _:retired\");\n+    log_debug(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                        \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+                        _free_sets.leftmost(Mutator), _free_sets.rightmost(Mutator),\n+                        _free_sets.leftmost(Collector), _free_sets.rightmost(Collector));\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_debug(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_free_sets.in_free_set(i, Mutator)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_mutator += capacity;\n+        consumed_mutator += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'M': 'm';\n+      } else if (_free_sets.in_free_set(i, Collector)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_collector += capacity;\n+        consumed_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'C': 'c';\n+      } else if (r->is_humongous()) {\n+        buffer[idx] = 'h';\n+      } else {\n+        buffer[idx] = '_';\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_debug(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+  }\n+#endif\n+\n+  LogTarget(Info, gc, free) lt;\n@@ -464,2 +912,2 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (size_t idx = _free_sets.leftmost(Mutator); idx <= _free_sets.rightmost(Mutator); idx++) {\n+        if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -468,1 +916,0 @@\n-\n@@ -470,1 +917,0 @@\n-\n@@ -481,1 +927,0 @@\n-\n@@ -484,1 +929,0 @@\n-\n@@ -493,0 +937,5 @@\n+      \/\/ Since certain regions that belonged to the Mutator free set at the time of most recent rebuild may have been retired,\n+      \/\/ the sum of used and capacities within regions that are still in the Mutator free set may not match my internally tracked\n+      \/\/ values of used() and free().\n+      assert(free == total_free, \"Free memory should match\");\n+\n@@ -494,1 +943,1 @@\n-               byte_size_in_proper_unit(total_free),    proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(free),          proper_unit_for_byte_size(free),\n@@ -509,2 +958,2 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_free_sets.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _free_sets.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -515,0 +964,2 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _free_sets.count(Mutator));\n@@ -520,0 +971,1 @@\n+      size_t total_used = 0;\n@@ -521,2 +973,2 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (size_t idx = _free_sets.leftmost(Collector); idx <= _free_sets.rightmost(Collector); idx++) {\n+        if (_free_sets.in_free_set(idx, Collector)) {\n@@ -527,0 +979,1 @@\n+          total_used += r->used();\n@@ -529,4 +982,4 @@\n-\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n-                  byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -539,1 +992,0 @@\n-  assert_bounds();\n@@ -541,0 +993,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -565,2 +1018,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (index < _free_sets.max() && _free_sets.in_free_set(index, Mutator)) {\n@@ -579,3 +1032,3 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Mutator));\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -585,3 +1038,3 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Collector));\n+  for (size_t index = _free_sets.leftmost(Collector); index <= _free_sets.rightmost(Collector); index++) {\n+    if (_free_sets.in_free_set(index, Collector)) {\n@@ -619,2 +1072,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -657,2 +1110,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -683,27 +1136,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":673,"deletions":247,"binary":false,"changes":920,"status":"modified"},{"patch":"@@ -0,0 +1,1 @@\n+\n@@ -3,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,1 +33,9 @@\n-class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+enum ShenandoahFreeMemoryType : uint8_t {\n+  NotFree,\n+  Mutator,\n+  Collector,\n+  NumFreeSets\n+};\n+\n+class ShenandoahSetsOfFree {\n+\n@@ -33,4 +43,19 @@\n-  ShenandoahHeap* const _heap;\n-  CHeapBitMap _mutator_free_bitmap;\n-  CHeapBitMap _collector_free_bitmap;\n-  size_t _max;\n+  size_t _max;                  \/\/ The maximum number of heap regions\n+  ShenandoahFreeSet* _free_set;\n+  size_t _region_size_bytes;\n+  ShenandoahFreeMemoryType* _membership;\n+  size_t _leftmosts[NumFreeSets];\n+  size_t _rightmosts[NumFreeSets];\n+  size_t _leftmosts_empty[NumFreeSets];\n+  size_t _rightmosts_empty[NumFreeSets];\n+\n+  \/\/ _capacity_of and _used_by are denoted in bytes\n+  size_t _capacity_of[NumFreeSets];\n+  size_t _used_by[NumFreeSets];\n+  size_t _region_counts[NumFreeSets];\n+\n+  inline void shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx);\n+  inline void expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t capacity);\n+\n+  \/\/ Restore all state variables to initial default state.\n+  void clear_internal();\n@@ -38,4 +63,59 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n-  size_t _mutator_leftmost, _mutator_rightmost;\n-  size_t _collector_leftmost, _collector_rightmost;\n+public:\n+  ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahSetsOfFree();\n+\n+  \/\/ Make all regions NotFree and reset all bounds\n+  void clear_all();\n+\n+  \/\/ Retire region idx from within its free set.  Requires that idx is in a free set.  The free set's original capacity\n+  \/\/ and usage are unaffected, but this region is no longer considered to be part of the free set insofar as future\n+  \/\/ allocation requests are concerned.  Any remnant of available memory at the time of retirement is added to the\n+  \/\/ original free set's total of used bytes.\n+  void retire_within_free_set(size_t idx, size_t used_bytes);\n+\n+  \/\/ Place region idx into free set which_set.  Requires that idx is currently NotFree.\n+  void make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t region_capacity);\n+\n+  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFree.\n+  void move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t region_capacity);\n+\n+  \/\/ Returns the ShenandoahFreeMemoryType affiliation of region idx, or NotFree if this region is not currently free.  This does\n+  \/\/ not enforce that free_set membership implies allocation capacity.\n+  inline ShenandoahFreeMemoryType membership(size_t idx) const;\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+  inline bool in_free_set(size_t idx, ShenandoahFreeMemoryType which_set) const;\n+\n+  \/\/ The following four methods return the left-most and right-most bounds on ranges of regions representing\n+  \/\/ the requested set.  The _empty variants represent bounds on the range that holds completely empty\n+  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.  A\n+  \/\/ return value of -1 from leftmost() or leftmost_empty() denotes that the corresponding set is empty.\n+  \/\/ In other words:\n+  \/\/   if the requested which_set is empty:\n+  \/\/     leftmost() and leftmost_empty() return _max, rightmost() and rightmost_empty() return 0\n+  \/\/   otherwise, expect the following:\n+  \/\/     0 <= leftmost <= leftmost_empty <= rightmost_empty <= rightmost < _max\n+  inline size_t leftmost(ShenandoahFreeMemoryType which_set) const;\n+  inline size_t rightmost(ShenandoahFreeMemoryType which_set) const;\n+  size_t leftmost_empty(ShenandoahFreeMemoryType which_set);\n+  size_t rightmost_empty(ShenandoahFreeMemoryType which_set);\n+\n+  inline bool is_empty(ShenandoahFreeMemoryType which_set) const;\n+\n+  inline void increase_used(ShenandoahFreeMemoryType which_set, size_t bytes);\n+\n+  inline size_t capacity_of(ShenandoahFreeMemoryType which_set) const {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _capacity_of[which_set];\n+  }\n+\n+  inline size_t used_by(ShenandoahFreeMemoryType which_set) const {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _used_by[which_set];\n+  }\n+\n+  inline void set_capacity_of(ShenandoahFreeMemoryType which_set, size_t value) {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    _capacity_of[which_set] = value;\n+  }\n@@ -43,2 +123,4 @@\n-  size_t _capacity;\n-  size_t _used;\n+  inline void set_used_by(ShenandoahFreeMemoryType which_set, size_t value) {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    _used_by[which_set] = value;\n+  }\n@@ -46,1 +128,26 @@\n-  void assert_bounds() const NOT_DEBUG_RETURN;\n+  inline size_t max() const { return _max; }\n+\n+  inline size_t count(ShenandoahFreeMemoryType which_set) const { return _region_counts[which_set]; }\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n+  \/\/ Valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and rightmost_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= lefmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n@@ -48,2 +155,4 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+private:\n+  ShenandoahHeap* const _heap;\n+  ShenandoahSetsOfFree _free_sets;\n@@ -52,0 +161,7 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.  The req.size() value is known to be less than or\n+  \/\/ equal to ShenandoahHeapRegion::humongous_threshold_words().  The caller of allocate_single is responsible\n+  \/\/ for registering the resulting object and setting the remembered set card values as appropriate.  The\n+  \/\/ most common case is that we are allocating a PLAB in which case object registering and card dirtying\n+  \/\/ is managed after the PLAB is divided into individual objects.\n@@ -56,0 +172,1 @@\n+  void clear_internal();\n@@ -57,3 +174,1 @@\n-  void recompute_bounds();\n-  void adjust_bounds();\n-  bool touches_bounds(size_t num) const;\n+  void try_recycle_trashed(ShenandoahHeapRegion *r);\n@@ -61,2 +176,2 @@\n-  void increase_used(size_t amount);\n-  void clear_internal();\n+  inline bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  inline bool can_allocate_from(size_t idx) const;\n@@ -64,2 +179,1 @@\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+  inline bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -67,1 +181,2 @@\n-  void try_recycle_trashed(ShenandoahHeapRegion *r);\n+  void find_regions_with_alloc_capacity(size_t &cset_regions);\n+  void reserve_regions(size_t to_reserve);\n@@ -69,3 +184,2 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  void prepare_to_rebuild(size_t &cset_regions);\n+  void finish_rebuild(size_t cset_regions);\n@@ -76,0 +190,4 @@\n+  \/\/ Public because ShenandoahSetsOfFree assertions require access.\n+  inline size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  inline size_t alloc_capacity(size_t idx) const;\n+\n@@ -79,1 +197,7 @@\n-  void recycle_trash();\n+  \/\/ After we have finished evacuation, we no longer need to hold regions in reserve for the Collector.\n+  \/\/ Call this method at the start of update refs to make more memory available to the Mutator.  This\n+  \/\/ benefits workloads that do not consume all of the evacuation waste reserve.\n+  \/\/\n+  \/\/ Note that we plan to replenish the Collector reserve at the end of update refs, at which time all\n+  \/\/ of the regions recycled from the collection set will be available.\n+  void move_collector_sets_to_mutator(size_t cset_regions);\n@@ -81,0 +205,1 @@\n+  void recycle_trash();\n@@ -83,5 +208,5 @@\n-  size_t capacity()  const { return _capacity; }\n-  size_t used()      const { return _used;     }\n-  size_t available() const {\n-    assert(_used <= _capacity, \"must use less than capacity\");\n-    return _capacity - _used;\n+  inline size_t capacity()  const { return _free_sets.capacity_of(Mutator); }\n+  inline size_t used()      const { return _free_sets.used_by(Mutator);     }\n+  inline size_t available() const {\n+    assert(used() <= capacity(), \"must use less than capacity\");\n+    return capacity() - used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":156,"deletions":31,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -1068,0 +1068,2 @@\n+\n+    \/\/ Since Full GC directly manipulates top of certain regions, certain ShenandoahFreeSet abstractions may have been corrupted.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -377,1 +377,0 @@\n-\n@@ -2061,1 +2060,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2064,1 +2063,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2070,1 +2069,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2072,0 +2071,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"}]}