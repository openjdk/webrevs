{"files":[{"patch":"@@ -1337,0 +1337,1 @@\n+  INSN(vslideup_vi,   0b1010111, 0b011, 0b001110);\n@@ -1692,1 +1693,0 @@\n-#undef patch_VArith\n@@ -1734,2 +1734,2 @@\n-#define INSN(NAME, op, lumop, vm, mop, nf)                                           \\\n-  void NAME(VectorRegister Vd, Register Rs1, uint32_t width = 0, bool mew = false) { \\\n+#define INSN(NAME, op, width, lumop, vm, mop, mew, nf)                               \\\n+  void NAME(VectorRegister Vd, Register Rs1) {                                       \\\n@@ -1741,1 +1741,16 @@\n-  INSN(vl1re8_v, 0b0000111, 0b01000, 0b1, 0b00, g1);\n+  INSN(vl1re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl2re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl4re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl8re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g8);\n@@ -1752,0 +1767,3 @@\n+  INSN(vs2r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vs4r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vs8r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g8);\n@@ -1797,0 +1815,1 @@\n+  INSN( vluxei8_v, 0b0000111, 0b000, 0b01, 0b0);\n@@ -1800,0 +1819,1 @@\n+  INSN( vsuxei8_v, 0b0100111, 0b000, 0b01, 0b0);\n@@ -1823,0 +1843,49 @@\n+\/\/ ====================================\n+\/\/ RISC-V Vector Crypto Extension\n+\/\/ ====================================\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);                            \\\n+  }\n+\n+  \/\/ Vector Bit-manipulation used in Cryptography (Zvkb) Extension\n+  INSN(vandn_vv,   0b1010111, 0b000, 0b000001);\n+  INSN(vandn_vx,   0b1010111, 0b100, 0b000001);\n+  INSN(vandn_vi,   0b1010111, 0b011, 0b000001);\n+  INSN(vclmul_vv,  0b1010111, 0b010, 0b001100);\n+  INSN(vclmul_vx,  0b1010111, 0b110, 0b001100);\n+  INSN(vclmulh_vv, 0b1010111, 0b010, 0b001101);\n+  INSN(vclmulh_vx, 0b1010111, 0b110, 0b001101);\n+  INSN(vror_vv,    0b1010111, 0b000, 0b010100);\n+  INSN(vror_vx,    0b1010111, 0b100, 0b010100);\n+  INSN(vrol_vv,    0b1010111, 0b000, 0b010101);\n+  INSN(vrol_vx,    0b1010111, 0b100, 0b010101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Bit-manipulation used in Cryptography (Zvkb) Extension\n+  INSN(vbrev8_v, 0b1010111, 0b010, 0b01000, 0b010010);\n+  INSN(vrev8_v,  0b1010111, 0b010, 0b01001, 0b010010);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);      \\\n+  }\n+\n+  \/\/ Vector SHA-2 Secure Hash (Zvknh[ab]) Extension\n+  INSN(vsha2ms_vv,  0b1110111, 0b010, 0b1, 0b101101);\n+  INSN(vsha2ch_vv,  0b1110111, 0b010, 0b1, 0b101110);\n+  INSN(vsha2cl_vv,  0b1110111, 0b010, 0b1, 0b101111);\n+\n+#undef INSN\n+\n+#undef patch_VArith\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":73,"deletions":4,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -116,0 +116,2 @@\n+  product(bool, UseZvkn, false, EXPERIMENTAL,                                    \\\n+          \"Use Zvkn group extension, Zvkned, Zvknhb, Zvkb, Zvkt\")                \\\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1361,0 +1361,10 @@\n+  inline void vmsltu_vi(VectorRegister Vd, VectorRegister Vs2, uint32_t imm, VectorMask vm = unmasked) {\n+    guarantee(imm >= 1 && imm <= 16, \"imm is invalid\");\n+    vmsleu_vi(Vd, Vs2, imm-1, vm);\n+  }\n+\n+  inline void vmsgeu_vi(VectorRegister Vd, VectorRegister Vs2, uint32_t imm, VectorMask vm = unmasked) {\n+    guarantee(imm >= 1 && imm <= 16, \"imm is invalid\");\n+    vmsgtu_vi(Vd, Vs2, imm-1, vm);\n+  }\n+\n@@ -1376,0 +1386,4 @@\n+  inline void vnot_v(VectorRegister Vd, VectorRegister Vs, VectorMask vm = unmasked) {\n+    vxor_vi(Vd, Vs, -1, vm);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -3662,0 +3662,1 @@\n+\n@@ -3664,0 +3665,386 @@\n+#undef __\n+#define __ this->\n+  class Sha2Generator : public MacroAssembler {\n+    StubCodeGenerator* _cgen;\n+   public:\n+      Sha2Generator(MacroAssembler* masm, StubCodeGenerator* cgen) : MacroAssembler(masm->code()), _cgen(cgen) {}\n+      address generate_sha256_implCompress(bool multi_block) {\n+        return generate_sha2_implCompress(Assembler::e32, multi_block);\n+      }\n+      address generate_sha512_implCompress(bool multi_block) {\n+        return generate_sha2_implCompress(Assembler::e64, multi_block);\n+      }\n+   private:\n+\n+    void vl1reXX_v(Assembler::SEW vset_sew, VectorRegister vr, Register sr) {\n+      if (vset_sew == Assembler::e32) __ vl1re32_v(vr, sr);\n+      else                            __ vl1re64_v(vr, sr);\n+    }\n+\n+    void vleXX_v(Assembler::SEW vset_sew, VectorRegister vr, Register sr) {\n+      if (vset_sew == Assembler::e32) __ vle32_v(vr, sr);\n+      else                            __ vle64_v(vr, sr);\n+    }\n+\n+    void vseXX_v(Assembler::SEW vset_sew, VectorRegister vr, Register sr) {\n+      if (vset_sew == Assembler::e32) __ vse32_v(vr, sr);\n+      else                            __ vse64_v(vr, sr);\n+    }\n+\n+    \/\/ Overview of the logic in each \"quad round\".\n+    \/\/\n+    \/\/ The code below repeats 16\/20 times the logic implementing four rounds\n+    \/\/ of the SHA-256\/512 core loop as documented by NIST. 16\/20 \"quad rounds\"\n+    \/\/ to implementing the 64\/80 single rounds.\n+    \/\/\n+    \/\/    \/\/ Load four word (u32\/64) constants (K[t+3], K[t+2], K[t+1], K[t+0])\n+    \/\/    \/\/ Output:\n+    \/\/    \/\/   vTmp1 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n+    \/\/    vl1reXX.v vTmp1, ofs\n+    \/\/\n+    \/\/    \/\/ Increment word constant address by stride (16\/32 bytes, 4*4B\/8B, 128b\/256b)\n+    \/\/    addi ofs, ofs, 16\/32\n+    \/\/\n+    \/\/    \/\/ Add constants to message schedule words:\n+    \/\/    \/\/  Input\n+    \/\/    \/\/    vTmp1 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n+    \/\/    \/\/    vW0 = {W[t+3], W[t+2], W[t+1], W[t+0]}; \/\/ Vt0 = W[3:0];\n+    \/\/    \/\/  Output\n+    \/\/    \/\/    vTmp0 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    vadd.vv vTmp0, vTmp1, vW0\n+    \/\/\n+    \/\/    \/\/  2 rounds of working variables updates.\n+    \/\/    \/\/     vState1[t+4] <- vState1[t], vState0[t], vTmp0[t]\n+    \/\/    \/\/  Input:\n+    \/\/    \/\/    vState1 = {c[t],d[t],g[t],h[t]}   \" = vState1[t] \"\n+    \/\/    \/\/    vState0 = {a[t],b[t],e[t],f[t]}\n+    \/\/    \/\/    vTmp0 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    \/\/  Output:\n+    \/\/    \/\/    vState1 = {f[t+2],e[t+2],b[t+2],a[t+2]}  \" = vState0[t+2] \"\n+    \/\/    \/\/        = {h[t+4],g[t+4],d[t+4],c[t+4]}  \" = vState1[t+4] \"\n+    \/\/    vsha2cl.vv vState1, vState0, vTmp0\n+    \/\/\n+    \/\/    \/\/  2 rounds of working variables updates.\n+    \/\/    \/\/     vState0[t+4] <- vState0[t], vState0[t+2], vTmp0[t]\n+    \/\/    \/\/  Input\n+    \/\/    \/\/   vState0 = {a[t],b[t],e[t],f[t]}       \" = vState0[t] \"\n+    \/\/    \/\/       = {h[t+2],g[t+2],d[t+2],c[t+2]}   \" = vState1[t+2] \"\n+    \/\/    \/\/   vState1 = {f[t+2],e[t+2],b[t+2],a[t+2]}   \" = vState0[t+2] \"\n+    \/\/    \/\/   vTmp0 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    \/\/  Output:\n+    \/\/    \/\/   vState0 = {f[t+4],e[t+4],b[t+4],a[t+4]}   \" = vState0[t+4] \"\n+    \/\/    vsha2ch.vv vState0, vState1, vTmp0\n+    \/\/\n+    \/\/    \/\/ Combine 2QW into 1QW\n+    \/\/    \/\/\n+    \/\/    \/\/ To generate the next 4 words, \"new_vW0\"\/\"vTmp0\" from vW0-vW3, vsha2ms needs\n+    \/\/    \/\/     vW0[0..3], vW1[0], vW2[1..3], vW3[0, 2..3]\n+    \/\/    \/\/ and it can only take 3 vectors as inputs. Hence we need to combine\n+    \/\/    \/\/ vW1[0] and vW2[1..3] in a single vector.\n+    \/\/    \/\/\n+    \/\/    \/\/ vmerge Vt4, Vt1, Vt2, V0\n+    \/\/    \/\/ Input\n+    \/\/    \/\/  V0 = mask \/\/ first word from vW2, 1..3 words from vW1\n+    \/\/    \/\/  vW2 = {Wt-8, Wt-7, Wt-6, Wt-5}\n+    \/\/    \/\/  vW1 = {Wt-12, Wt-11, Wt-10, Wt-9}\n+    \/\/    \/\/ Output\n+    \/\/    \/\/  Vt4 = {Wt-12, Wt-7, Wt-6, Wt-5}\n+    \/\/    vmerge.vvm vTmp0, vW2, vW1, v0\n+    \/\/\n+    \/\/    \/\/ Generate next Four Message Schedule Words (hence allowing for 4 more rounds)\n+    \/\/    \/\/ Input\n+    \/\/    \/\/  vW0 = {W[t+ 3], W[t+ 2], W[t+ 1], W[t+ 0]}     W[ 3: 0]\n+    \/\/    \/\/  vW3 = {W[t+15], W[t+14], W[t+13], W[t+12]}     W[15:12]\n+    \/\/    \/\/  vTmp0 = {W[t+11], W[t+10], W[t+ 9], W[t+ 4]}     W[11: 9,4]\n+    \/\/    \/\/ Output (next four message schedule words)\n+    \/\/    \/\/  vW0 = {W[t+19],  W[t+18],  W[t+17],  W[t+16]}  W[19:16]\n+    \/\/    vsha2ms.vv vW0, vTmp0, vW3\n+    \/\/\n+    \/\/ BEFORE\n+    \/\/  vW0 - vW3 hold the message schedule words (initially the block words)\n+    \/\/    vW0 = W[ 3: 0]   \"oldest\"\n+    \/\/    vW1 = W[ 7: 4]\n+    \/\/    vW2 = W[11: 8]\n+    \/\/    vW3 = W[15:12]   \"newest\"\n+    \/\/\n+    \/\/  vt6 - vt7 hold the working state variables\n+    \/\/    vState0 = {a[t],b[t],e[t],f[t]}   \/\/ initially {H5,H4,H1,H0}\n+    \/\/    vState1 = {c[t],d[t],g[t],h[t]}   \/\/ initially {H7,H6,H3,H2}\n+    \/\/\n+    \/\/ AFTER\n+    \/\/  vW0 - vW3 hold the message schedule words (initially the block words)\n+    \/\/    vW1 = W[ 7: 4]   \"oldest\"\n+    \/\/    vW2 = W[11: 8]\n+    \/\/    vW3 = W[15:12]\n+    \/\/    vW0 = W[19:16]   \"newest\"\n+    \/\/\n+    \/\/  vState0 and vState1 hold the working state variables\n+    \/\/    vState0 = {a[t+4],b[t+4],e[t+4],f[t+4]}\n+    \/\/    vState1 = {c[t+4],d[t+4],g[t+4],h[t+4]}\n+    \/\/\n+    \/\/  The group of vectors vW0,vW1,vW2,vW3 is \"rotated\" by one in each quad-round,\n+    \/\/  hence the uses of those vectors rotate in each round, and we get back to the\n+    \/\/  initial configuration every 4 quad-rounds. We could avoid those changes at\n+    \/\/  the cost of moving those vectors at the end of each quad-rounds.\n+    void sha2_quad_round(Assembler::SEW vset_sew, VectorRegister rot1, VectorRegister rot2, VectorRegister rot3, VectorRegister rot4,\n+                         Register scalarconst, VectorRegister vtemp, VectorRegister vtemp2, VectorRegister v_abef, VectorRegister v_cdgh,\n+                         bool gen_words = true, bool step_const = true) {\n+      __ vl1reXX_v(vset_sew, vtemp, scalarconst);\n+      if (step_const) {\n+        __ addi(scalarconst, scalarconst, vset_sew == Assembler::e32 ? 16 : 32);\n+      }\n+      __ vadd_vv(vtemp2, vtemp, rot1);\n+      __ vsha2cl_vv(v_cdgh, v_abef, vtemp2);\n+      __ vsha2ch_vv(v_abef, v_cdgh, vtemp2);\n+      if (gen_words) {\n+        __ vmerge_vvm(vtemp2, rot3, rot2);\n+        __ vsha2ms_vv(rot1, vtemp2, rot4);\n+      }\n+    }\n+\n+    const char* stub_name(Assembler::SEW vset_sew, bool multi_block) {\n+      if (vset_sew == Assembler::e32 && !multi_block) return \"sha256_implCompress\";\n+      if (vset_sew == Assembler::e32 &&  multi_block) return \"sha256_implCompressMB\";\n+      if (vset_sew == Assembler::e64 && !multi_block) return \"sha512_implCompress\";\n+      if (vset_sew == Assembler::e64 &&  multi_block) return \"sha512_implCompressMB\";\n+      ShouldNotReachHere();\n+      return \"bad name lookup\";\n+    }\n+\n+    \/\/ Arguments:\n+    \/\/\n+    \/\/ Inputs:\n+    \/\/   c_rarg0   - byte[]  source+offset\n+    \/\/   c_rarg1   - int[]   SHA.state\n+    \/\/   c_rarg2   - int     offset\n+    \/\/   c_rarg3   - int     limit\n+    \/\/\n+    address generate_sha2_implCompress(Assembler::SEW vset_sew, bool multi_block) {\n+      alignas(64) static const uint32_t round_consts_256[64] = {\n+        0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n+        0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n+        0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n+        0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n+        0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n+        0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n+        0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n+        0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n+        0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n+        0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n+        0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n+        0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n+        0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n+        0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n+        0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n+        0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n+      };\n+      alignas(64) static const uint64_t round_consts_512[80] = {\n+        0x428a2f98d728ae22l, 0x7137449123ef65cdl, 0xb5c0fbcfec4d3b2fl,\n+        0xe9b5dba58189dbbcl, 0x3956c25bf348b538l, 0x59f111f1b605d019l,\n+        0x923f82a4af194f9bl, 0xab1c5ed5da6d8118l, 0xd807aa98a3030242l,\n+        0x12835b0145706fbel, 0x243185be4ee4b28cl, 0x550c7dc3d5ffb4e2l,\n+        0x72be5d74f27b896fl, 0x80deb1fe3b1696b1l, 0x9bdc06a725c71235l,\n+        0xc19bf174cf692694l, 0xe49b69c19ef14ad2l, 0xefbe4786384f25e3l,\n+        0x0fc19dc68b8cd5b5l, 0x240ca1cc77ac9c65l, 0x2de92c6f592b0275l,\n+        0x4a7484aa6ea6e483l, 0x5cb0a9dcbd41fbd4l, 0x76f988da831153b5l,\n+        0x983e5152ee66dfabl, 0xa831c66d2db43210l, 0xb00327c898fb213fl,\n+        0xbf597fc7beef0ee4l, 0xc6e00bf33da88fc2l, 0xd5a79147930aa725l,\n+        0x06ca6351e003826fl, 0x142929670a0e6e70l, 0x27b70a8546d22ffcl,\n+        0x2e1b21385c26c926l, 0x4d2c6dfc5ac42aedl, 0x53380d139d95b3dfl,\n+        0x650a73548baf63del, 0x766a0abb3c77b2a8l, 0x81c2c92e47edaee6l,\n+        0x92722c851482353bl, 0xa2bfe8a14cf10364l, 0xa81a664bbc423001l,\n+        0xc24b8b70d0f89791l, 0xc76c51a30654be30l, 0xd192e819d6ef5218l,\n+        0xd69906245565a910l, 0xf40e35855771202al, 0x106aa07032bbd1b8l,\n+        0x19a4c116b8d2d0c8l, 0x1e376c085141ab53l, 0x2748774cdf8eeb99l,\n+        0x34b0bcb5e19b48a8l, 0x391c0cb3c5c95a63l, 0x4ed8aa4ae3418acbl,\n+        0x5b9cca4f7763e373l, 0x682e6ff3d6b2b8a3l, 0x748f82ee5defb2fcl,\n+        0x78a5636f43172f60l, 0x84c87814a1f0ab72l, 0x8cc702081a6439ecl,\n+        0x90befffa23631e28l, 0xa4506cebde82bde9l, 0xbef9a3f7b2c67915l,\n+        0xc67178f2e372532bl, 0xca273eceea26619cl, 0xd186b8c721c0c207l,\n+        0xeada7dd6cde0eb1el, 0xf57d4f7fee6ed178l, 0x06f067aa72176fbal,\n+        0x0a637dc5a2c898a6l, 0x113f9804bef90dael, 0x1b710b35131c471bl,\n+        0x28db77f523047d84l, 0x32caab7b40c72493l, 0x3c9ebe0a15c9bebcl,\n+        0x431d67c49c100d4cl, 0x4cc5d4becb3e42b6l, 0x597f299cfc657e2al,\n+        0x5fcb6fab3ad6faecl, 0x6c44198c4a475817l\n+      };\n+      const int const_add = vset_sew == Assembler::e32 ? 16 : 32;\n+\n+      __ align(CodeEntryAlignment);\n+      StubCodeMark mark(_cgen, \"StubRoutines\", stub_name(vset_sew, multi_block));\n+      address start = __ pc();\n+\n+      Register buf   = c_rarg0;\n+      Register state = c_rarg1;\n+      Register ofs   = c_rarg2;\n+      Register limit = c_rarg3;\n+      Register consts =  t2; \/\/ caller saved\n+      Register state_c = x28; \/\/ caller saved\n+      VectorRegister vindex = v1;\n+      VectorRegister vW0 = v2;\n+      VectorRegister vW1 = v4;\n+      VectorRegister vW2 = v6;\n+      VectorRegister vW3 = v8;\n+      VectorRegister vState0 = v10;\n+      VectorRegister vState1 = v12;\n+      VectorRegister vHash0  = v14;\n+      VectorRegister vHash1  = v16;\n+      VectorRegister vTmp0   = v18;\n+      VectorRegister vTmp1   = v20;\n+\n+      Label multi_block_loop;\n+\n+      __ enter();\n+\n+      address constant_table = vset_sew == Assembler::e32 ? (address)round_consts_256 : (address)round_consts_512;\n+      la(consts, ExternalAddress(constant_table));\n+\n+      \/\/ Register use in this function:\n+      \/\/\n+      \/\/ VECTORS\n+      \/\/  vW0 - vW3 (512\/1024-bits \/ 4*128\/256 bits \/ 4*4*32\/65 bits), hold the message\n+      \/\/             schedule words (Wt). They start with the message block\n+      \/\/             content (W0 to W15), then further words in the message\n+      \/\/             schedule generated via vsha2ms from previous Wt.\n+      \/\/   Initially:\n+      \/\/     vW0 = W[  3:0] = { W3,  W2,  W1,  W0}\n+      \/\/     vW1 = W[  7:4] = { W7,  W6,  W5,  W4}\n+      \/\/     vW2 = W[ 11:8] = {W11, W10,  W9,  W8}\n+      \/\/     vW3 = W[15:12] = {W15, W14, W13, W12}\n+      \/\/\n+      \/\/  vState0 - vState1 hold the working state variables (a, b, ..., h)\n+      \/\/    vState0 = {f[t],e[t],b[t],a[t]}\n+      \/\/    vState1 = {h[t],g[t],d[t],c[t]}\n+      \/\/   Initially:\n+      \/\/    vState0 = {H5i-1, H4i-1, H1i-1 , H0i-1}\n+      \/\/    vState1 = {H7i-i, H6i-1, H3i-1 , H2i-1}\n+      \/\/\n+      \/\/  v0 = masks for vrgather\/vmerge. Single value during the 16 rounds.\n+      \/\/\n+      \/\/  vTmp0 = temporary, Wt+Kt\n+      \/\/  vTmp1 = temporary, Kt\n+      \/\/\n+      \/\/  vHash0\/vHash1 = hold the initial values of the hash, byte-swapped.\n+      \/\/\n+      \/\/ During most of the function the vector state is configured so that each\n+      \/\/ vector is interpreted as containing four 32\/64 bits (e32\/e64) elements (128\/256 bits).\n+\n+      \/\/ Set vectors as 4 * 32\/64 bits\n+      \/\/\n+      \/\/ e32\/e64: vector of 32b\/64b\/4B\/8B elements\n+      \/\/ m1: LMUL=1\n+      \/\/ ta: tail agnostic (don't care about those lanes)\n+      \/\/ ma: mask agnostic (don't care about those lanes)\n+      \/\/ x0 is not written, we known the number of vector elements.\n+\n+      if (vset_sew == Assembler::e32 && MaxVectorSize == 16) {\n+        __ vsetivli(x0, 4, vset_sew, Assembler::m2, Assembler::ma, Assembler::ta);\n+      } else {\n+        __ vsetivli(x0, 4, vset_sew, Assembler::m1, Assembler::ma, Assembler::ta);\n+      }\n+      \/\/ Splat indexes in vindex if SEW = e64, but don't hurt anything.\n+      int64_t indexes = vset_sew == Assembler::e32 ? 0x00041014ul : 0x00082028ul;\n+      __ li(t0, indexes);\n+      __ vmv_s_x(vindex, t0);\n+\n+      \/\/ Step-over a,b, so we are pointing to c.\n+      \/\/ const_add is equal to 4x state variable, div by 2 is thus 2, a,b\n+      __ addi(state_c, state, const_add\/2);\n+\n+      \/\/ Use index-load to get {f,e,b,a},{h,g,d,c}\n+      __ vluxei8_v(vState0, state, vindex);\n+      __ vluxei8_v(vState1, state_c, vindex);\n+\n+      __ bind(multi_block_loop);\n+\n+      \/\/ Capture the initial H values in vHash0 and vHash1 to allow for computing\n+      \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n+      __ vmv_v_v(vHash0, vState0);\n+      __ vmv_v_v(vHash1, vState1);\n+\n+      \/\/ Load the 512\/1024-bits of the message block in vW0-vW3 and perform\n+      \/\/ an endian swap on each 4\/8 bytes element.\n+      \/\/\n+      \/\/ If Zvkb is not implemented one can use vrgather\n+      \/\/ with an index sequence to byte-swap.\n+      \/\/  sequence = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n+      \/\/   <https:\/\/oeis.org\/A004444> gives us \"N ^ 3\" as a nice formula to generate\n+      \/\/  this sequence. 'vid' gives us the N.\n+      __ vleXX_v(vset_sew, vW0, buf);\n+      __ vrev8_v(vW0, vW0);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v(vset_sew, vW1, buf);\n+      __ vrev8_v(vW1, vW1);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v(vset_sew, vW2, buf);\n+      __ vrev8_v(vW2, vW2);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v(vset_sew, vW3, buf);\n+      __ vrev8_v(vW3, vW3);\n+      __ addi(buf, buf, const_add);\n+\n+      \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n+      __ vid_v(v0);\n+      __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n+\n+      VectorRegister rotation_regs[] = {vW0, vW1, vW2, vW3};\n+      int rot_pos = 0;\n+      \/\/ Quad-round #0 (+0, vW0->vW1->vW2->vW3) ... #11 (+3, vW3->vW0->vW1->vW2)\n+      const int qr_end = vset_sew == Assembler::e32 ? 12 : 16;\n+      for (int i = 0; i < qr_end; i++) {\n+        sha2_quad_round(vset_sew,\n+                   rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   vTmp1, vTmp0, vState0, vState1);\n+        ++rot_pos;\n+      }\n+      \/\/ Quad-round #12 (+0, vW0->vW1->vW2->vW3) ... #15 (+3, vW3->vW0->vW1->vW2)\n+      \/\/ Note that we stop generating new message schedule words (Wt, vW0-13)\n+      \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n+      const int qr_c_end = qr_end + 4;\n+      for (int i = qr_end; i < qr_c_end; i++) {\n+        sha2_quad_round(vset_sew,\n+                   rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   vTmp1, vTmp0, vState0, vState1, false, i < (qr_c_end-1));\n+        ++rot_pos;\n+      }\n+\n+      \/\/--------------------------------------------------------------------------------\n+      \/\/ Compute the updated hash value H'\n+      \/\/   H' = H + {h',g',...,b',a'}\n+      \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n+      \/\/      = {h+h',g+g',...,b+b',a+a'}\n+\n+      \/\/ H' = H+{a',b',c',...,h'}\n+      __ vadd_vv(vState0, vHash0, vState0);\n+      __ vadd_vv(vState1, vHash1, vState1);\n+\n+      if (multi_block) {\n+        int total_adds = vset_sew == Assembler::e32 ? 240 : 608;\n+        __ addi(consts, consts, -total_adds);\n+        __ add(ofs, ofs, vset_sew == Assembler::e32 ? 64 : 128);\n+        __ ble(ofs, limit, multi_block_loop);\n+        __ mv(c_rarg0, ofs); \/\/ return ofs\n+      }\n+\n+      \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n+      \/\/  vState0 = {f,e,b,a}\n+      \/\/  vState1 = {h,g,d,c}\n+      __ vsuxei8_v(vState0, state,   vindex);\n+      __ vsuxei8_v(vState1, state_c, vindex);\n+\n+      __ leave();\n+      __ ret();\n+\n+      return start;\n+    }\n+  };\n+#undef __\n+#define __ masm->\n+\n@@ -4865,0 +5252,12 @@\n+    if (UseSHA256Intrinsics) {\n+      Sha2Generator sha2(_masm, this);\n+      StubRoutines::_sha256_implCompress   = sha2.generate_sha256_implCompress(false);\n+      StubRoutines::_sha256_implCompressMB = sha2.generate_sha256_implCompress(true);\n+    }\n+\n+    if (UseSHA512Intrinsics) {\n+      Sha2Generator sha2(_masm, this);\n+      StubRoutines::_sha512_implCompress   = sha2.generate_sha512_implCompress(false);\n+      StubRoutines::_sha512_implCompressMB = sha2.generate_sha512_implCompress(true);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":399,"deletions":0,"binary":false,"changes":399,"status":"modified"},{"patch":"@@ -159,3 +159,19 @@\n-  if (UseSHA256Intrinsics) {\n-    warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n+  if (UseZvkn) {\n+    if (!ext_V.enabled()) {\n+      FLAG_SET_DEFAULT(UseZvkn, false);\n+      warning(\"Cannot enable Zvkn on cpu without RVV support.\");\n+    }\n+    if (FLAG_IS_DEFAULT(UseRVV)) {\n+      FLAG_SET_DEFAULT(UseRVV, true);\n+    }\n+    if (UseRVV) {\n+      if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+        FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+      }\n+      if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+        FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+      }\n+    } else {\n+      FLAG_SET_DEFAULT(UseZvkn, false);\n+      warning(\"Cannot enabled Zvkn when RVV is disabled.\");\n+    }\n@@ -163,4 +179,9 @@\n-\n-  if (UseSHA512Intrinsics) {\n-    warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n+  if (!UseZvkn) {\n+    if (UseSHA256Intrinsics) {\n+      warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU, UseZvkn needed.\");\n+      FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n+    }\n+    if (UseSHA512Intrinsics) {\n+      warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU, UseZvkn needed.\");\n+      FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"}]}