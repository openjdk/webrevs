{"files":[{"patch":"@@ -1335,0 +1335,1 @@\n+  INSN(vslideup_vi,   0b1010111, 0b011, 0b001110);\n@@ -1689,1 +1690,0 @@\n-#undef patch_VArith\n@@ -1731,2 +1731,2 @@\n-#define INSN(NAME, op, lumop, vm, mop, nf)                                           \\\n-  void NAME(VectorRegister Vd, Register Rs1, uint32_t width = 0, bool mew = false) { \\\n+#define INSN(NAME, op, width, lumop, vm, mop, mew, nf)                               \\\n+  void NAME(VectorRegister Vd, Register Rs1) {                                       \\\n@@ -1738,1 +1738,16 @@\n-  INSN(vl1re8_v, 0b0000111, 0b01000, 0b1, 0b00, g1);\n+  INSN(vl1re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl1re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g1);\n+  INSN(vl2re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl2re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vl4re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl4re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vl8re8_v,  0b0000111, 0b000, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re16_v, 0b0000111, 0b101, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re32_v, 0b0000111, 0b110, 0b01000, 0b1, 0b00, 0b0, g8);\n+  INSN(vl8re64_v, 0b0000111, 0b111, 0b01000, 0b1, 0b00, 0b0, g8);\n@@ -1749,0 +1764,3 @@\n+  INSN(vs2r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g2);\n+  INSN(vs4r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g4);\n+  INSN(vs8r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g8);\n@@ -1820,0 +1838,49 @@\n+\/\/ ====================================\n+\/\/ RISC-V Vector Crypto Extension\n+\/\/ ====================================\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);                            \\\n+  }\n+\n+  \/\/ Vector Bit-manipulation used in Cryptography (Zvkb) Extension\n+  INSN(vandn_vv,   0b1010111, 0b000, 0b000001);\n+  INSN(vandn_vx,   0b1010111, 0b100, 0b000001);\n+  INSN(vandn_vi,   0b1010111, 0b011, 0b000001);\n+  INSN(vclmul_vv,  0b1010111, 0b010, 0b001100);\n+  INSN(vclmul_vx,  0b1010111, 0b110, 0b001100);\n+  INSN(vclmulh_vv, 0b1010111, 0b010, 0b001101);\n+  INSN(vclmulh_vx, 0b1010111, 0b110, 0b001101);\n+  INSN(vror_vv,    0b1010111, 0b000, 0b010100);\n+  INSN(vror_vx,    0b1010111, 0b100, 0b010100);\n+  INSN(vrol_vv,    0b1010111, 0b000, 0b010101);\n+  INSN(vrol_vx,    0b1010111, 0b100, 0b010101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Bit-manipulation used in Cryptography (Zvkb) Extension\n+  INSN(vbrev8_v, 0b1010111, 0b010, 0b01000, 0b010010);\n+  INSN(vrev8_v,  0b1010111, 0b010, 0b01001, 0b010010);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n+    patch_VArith(op, Vd, funct3, Vs1->raw_encoding(), Vs2, vm, funct6);      \\\n+  }\n+\n+  \/\/ Vector SHA-2 Secure Hash (Zvknh[ab]) Extension\n+  INSN(vsha2ms_vv,  0b1110111, 0b010, 0b1, 0b101101);\n+  INSN(vsha2ch_vv,  0b1110111, 0b010, 0b1, 0b101110);\n+  INSN(vsha2cl_vv,  0b1110111, 0b010, 0b1, 0b101111);\n+\n+#undef INSN\n+\n+#undef patch_VArith\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":71,"deletions":4,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -115,0 +115,2 @@\n+  product(bool, UseZvkn, false, EXPERIMENTAL,                                    \\\n+          \"Use Zvkn group extension, Zvkned, Zvknhb, Zvkb, Zvkt\")                \\\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1359,0 +1359,8 @@\n+  inline void vmsltu_vi(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {\n+    vmsleu_vi(Vd, Vs2, imm-1, vm);\n+  }\n+\n+  inline void vmsgeu_vi(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {\n+    vmsgtu_vi(Vd, Vs2, imm-1, vm);\n+  }\n+\n@@ -1374,0 +1382,4 @@\n+  inline void vnot_v(VectorRegister Vd, VectorRegister Vs, VectorMask vm = unmasked) {\n+    vxor_vi(Vd, Vs, -1, vm);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3662,0 +3662,1 @@\n+\n@@ -3664,0 +3665,408 @@\n+#undef __\n+#define __ this->\n+  class Sha2Generator : public MacroAssembler {\n+    StubCodeGenerator* _cgen;\n+   public:\n+      Sha2Generator(MacroAssembler* masm, StubCodeGenerator* cgen) : MacroAssembler(masm->code()), _cgen(cgen) {}\n+      address generate_sha256_implCompress(bool multi_block) {\n+        return generate_sha2_implCompress<Assembler::e32>(multi_block);\n+      }\n+      address generate_sha512_implCompress(bool multi_block) {\n+        return generate_sha2_implCompress<Assembler::e64>(multi_block);\n+      }\n+   private:\n+\n+    template<Assembler::SEW T>\n+    void vl1reXX_v(VectorRegister vr, Register sr) {\n+      if (T == Assembler::e32) __ vl1re32_v(vr, sr);\n+      else                     __ vl1re64_v(vr, sr);\n+    }\n+\n+    template<Assembler::SEW T>\n+    void vleXX_v(VectorRegister vr, Register sr) {\n+      if (T == Assembler::e32) __ vle32_v(vr, sr);\n+      else                     __ vle64_v(vr, sr);\n+    }\n+\n+    template<Assembler::SEW T>\n+    void vseXX_v(VectorRegister vr, Register sr) {\n+      if (T == Assembler::e32) __ vse32_v(vr, sr);\n+      else                     __ vse64_v(vr, sr);\n+    }\n+\n+    \/\/ Overview of the logic in each \"quad round\".\n+    \/\/\n+    \/\/ The code below repeats 16\/20 times the logic implementing four rounds\n+    \/\/ of the SHA-256\/512 core loop as documented by NIST. 16\/20 \"quad rounds\"\n+    \/\/ to implementing the 64\/80 single rounds.\n+    \/\/\n+    \/\/    \/\/ Load four word (u32\/64) constants (K[t+3], K[t+2], K[t+1], K[t+0])\n+    \/\/    \/\/ Output:\n+    \/\/    \/\/   v15 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n+    \/\/    vl1reXX.v v15, ofs\n+    \/\/\n+    \/\/    \/\/ Increment word contant address by stride (16\/32 bytes, 4*4B\/8B, 128b\/256b)\n+    \/\/    addi ofs, ofs, 16\/32\n+    \/\/\n+    \/\/    \/\/ Add constants to message schedule words:\n+    \/\/    \/\/  Input\n+    \/\/    \/\/    v15 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n+    \/\/    \/\/    v10 = {W[t+3], W[t+2], W[t+1], W[t+0]}; \/\/ Vt0 = W[3:0];\n+    \/\/    \/\/  Output\n+    \/\/    \/\/    v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    vadd.vv v14, v15, v10\n+    \/\/\n+    \/\/    \/\/  2 rounds of working variables updates.\n+    \/\/    \/\/     v17[t+4] <- v17[t], v16[t], v14[t]\n+    \/\/    \/\/  Input:\n+    \/\/    \/\/    v17 = {c[t],d[t],g[t],h[t]}   \" = v17[t] \"\n+    \/\/    \/\/    v16 = {a[t],b[t],e[t],f[t]}\n+    \/\/    \/\/    v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    \/\/  Output:\n+    \/\/    \/\/    v17 = {f[t+2],e[t+2],b[t+2],a[t+2]}  \" = v16[t+2] \"\n+    \/\/    \/\/        = {h[t+4],g[t+4],d[t+4],c[t+4]}  \" = v17[t+4] \"\n+    \/\/    vsha2cl.vv v17, v16, v14\n+    \/\/\n+    \/\/    \/\/  2 rounds of working variables updates.\n+    \/\/    \/\/     v16[t+4] <- v16[t], v16[t+2], v14[t]\n+    \/\/    \/\/  Input\n+    \/\/    \/\/   v16 = {a[t],b[t],e[t],f[t]}       \" = v16[t] \"\n+    \/\/    \/\/       = {h[t+2],g[t+2],d[t+2],c[t+2]}   \" = v17[t+2] \"\n+    \/\/    \/\/   v17 = {f[t+2],e[t+2],b[t+2],a[t+2]}   \" = v16[t+2] \"\n+    \/\/    \/\/   v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n+    \/\/    \/\/  Output:\n+    \/\/    \/\/   v16 = {f[t+4],e[t+4],b[t+4],a[t+4]}   \" = v16[t+4] \"\n+    \/\/    vsha2ch.vv v16, v17, v14\n+    \/\/\n+    \/\/    \/\/ Combine 2QW into 1QW\n+    \/\/    \/\/\n+    \/\/    \/\/ To generate the next 4 words, \"new_v10\"\/\"v14\" from v10-v13, vsha2ms needs\n+    \/\/    \/\/     v10[0..3], v11[0], v12[1..3], v13[0, 2..3]\n+    \/\/    \/\/ and it can only take 3 vectors as inputs. Hence we need to combine\n+    \/\/    \/\/ v11[0] and v12[1..3] in a single vector.\n+    \/\/    \/\/\n+    \/\/    \/\/ vmerge Vt4, Vt1, Vt2, V0\n+    \/\/    \/\/ Input\n+    \/\/    \/\/  V0 = mask \/\/ first word from v12, 1..3 words from v11\n+    \/\/    \/\/  V12 = {Wt-8, Wt-7, Wt-6, Wt-5}\n+    \/\/    \/\/  V11 = {Wt-12, Wt-11, Wt-10, Wt-9}\n+    \/\/    \/\/ Output\n+    \/\/    \/\/  Vt4 = {Wt-12, Wt-7, Wt-6, Wt-5}\n+    \/\/    vmerge.vvm v14, v12, v11, v0\n+    \/\/\n+    \/\/    \/\/ Generate next Four Message Schedule Words (hence allowing for 4 more rounds)\n+    \/\/    \/\/ Input\n+    \/\/    \/\/  V10 = {W[t+ 3], W[t+ 2], W[t+ 1], W[t+ 0]}     W[ 3: 0]\n+    \/\/    \/\/  V13 = {W[t+15], W[t+14], W[t+13], W[t+12]}     W[15:12]\n+    \/\/    \/\/  V14 = {W[t+11], W[t+10], W[t+ 9], W[t+ 4]}     W[11: 9,4]\n+    \/\/    \/\/ Output (next four message schedule words)\n+    \/\/    \/\/  v10 = {W[t+19],  W[t+18],  W[t+17],  W[t+16]}  W[19:16]\n+    \/\/    vsha2ms.vv v10, v14, v13\n+    \/\/\n+    \/\/ BEFORE\n+    \/\/  v10 - v13 hold the message schedule words (initially the block words)\n+    \/\/    v10 = W[ 3: 0]   \"oldest\"\n+    \/\/    v11 = W[ 7: 4]\n+    \/\/    v12 = W[11: 8]\n+    \/\/    v13 = W[15:12]   \"newest\"\n+    \/\/\n+    \/\/  vt6 - vt7 hold the working state variables\n+    \/\/    v16 = {a[t],b[t],e[t],f[t]}   \/\/ initially {H5,H4,H1,H0}\n+    \/\/    v17 = {c[t],d[t],g[t],h[t]}   \/\/ initially {H7,H6,H3,H2}\n+    \/\/\n+    \/\/ AFTER\n+    \/\/  v10 - v13 hold the message schedule words (initially the block words)\n+    \/\/    v11 = W[ 7: 4]   \"oldest\"\n+    \/\/    v12 = W[11: 8]\n+    \/\/    v13 = W[15:12]\n+    \/\/    v10 = W[19:16]   \"newest\"\n+    \/\/\n+    \/\/  v16 and v17 hold the working state variables\n+    \/\/    v16 = {a[t+4],b[t+4],e[t+4],f[t+4]}\n+    \/\/    v17 = {c[t+4],d[t+4],g[t+4],h[t+4]}\n+    \/\/\n+    \/\/  The group of vectors v10,v11,v12,v13 is \"rotated\" by one in each quad-round,\n+    \/\/  hence the uses of those vectors rotate in each round, and we get back to the\n+    \/\/  initial configuration every 4 quad-rounds. We could avoid those changes at\n+    \/\/  the cost of moving those vectors at the end of each quad-rounds.\n+    template<Assembler::SEW vset_sew>\n+    void sha2_quad_round(VectorRegister rot1, VectorRegister rot2, VectorRegister rot3, VectorRegister rot4,\n+                         Register scalarconst, VectorRegister vtemp, VectorRegister vtemp2, VectorRegister vtemp3, VectorRegister vtemp4,\n+                         bool gen_words = true, bool step_const = true) {\n+      __ vl1reXX_v<vset_sew>(vtemp, scalarconst);\n+      if (step_const) {\n+        __ addi(scalarconst, scalarconst, vset_sew == Assembler::e32 ? 16 : 32);\n+      }\n+      __ vadd_vv(vtemp2, vtemp, rot1);\n+      __ vsha2cl_vv(vtemp4, vtemp3, vtemp2);\n+      __ vsha2ch_vv(vtemp3, vtemp4, vtemp2);\n+      if ((vset_sew == Assembler::e64 && step_const) || gen_words) {\n+        __ vmerge_vvm(vtemp2, rot3, rot2);\n+      }\n+      if (gen_words) {\n+        __ vsha2ms_vv(rot1, vtemp2, rot4);\n+      }\n+    }\n+\n+    template<Assembler::SEW vset_sew>\n+    const char* stub_name(bool multi_block) {\n+      if (vset_sew == Assembler::e32 && !multi_block) return \"sha256_implCompress\";\n+      if (vset_sew == Assembler::e32 &&  multi_block) return \"sha256_implCompressMB\";\n+      if (vset_sew == Assembler::e64 && !multi_block) return \"sha512_implCompress\";\n+      if (vset_sew == Assembler::e64 &&  multi_block) return \"sha512_implCompressMB\";\n+      return \"bad name lookup\";\n+    }\n+\n+    \/\/ Arguments:\n+    \/\/\n+    \/\/ Inputs:\n+    \/\/   c_rarg0   - byte[]  source+offset\n+    \/\/   c_rarg1   - int[]   SHA.state\n+    \/\/   c_rarg2   - int     offset\n+    \/\/   c_rarg3   - int     limit\n+    \/\/\n+    template<Assembler::SEW vset_sew>\n+    address generate_sha2_implCompress(bool multi_block) {\n+      alignas(64) static const uint32_t round_consts_256[64] = {\n+        0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n+        0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n+        0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n+        0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n+        0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n+        0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n+        0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n+        0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n+        0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n+        0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n+        0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n+        0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n+        0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n+        0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n+        0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n+        0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n+      };\n+      alignas(64) static const uint64_t round_consts_512[80] = {\n+        0x428a2f98d728ae22l, 0x7137449123ef65cdl, 0xb5c0fbcfec4d3b2fl,\n+        0xe9b5dba58189dbbcl, 0x3956c25bf348b538l, 0x59f111f1b605d019l,\n+        0x923f82a4af194f9bl, 0xab1c5ed5da6d8118l, 0xd807aa98a3030242l,\n+        0x12835b0145706fbel, 0x243185be4ee4b28cl, 0x550c7dc3d5ffb4e2l,\n+        0x72be5d74f27b896fl, 0x80deb1fe3b1696b1l, 0x9bdc06a725c71235l,\n+        0xc19bf174cf692694l, 0xe49b69c19ef14ad2l, 0xefbe4786384f25e3l,\n+        0x0fc19dc68b8cd5b5l, 0x240ca1cc77ac9c65l, 0x2de92c6f592b0275l,\n+        0x4a7484aa6ea6e483l, 0x5cb0a9dcbd41fbd4l, 0x76f988da831153b5l,\n+        0x983e5152ee66dfabl, 0xa831c66d2db43210l, 0xb00327c898fb213fl,\n+        0xbf597fc7beef0ee4l, 0xc6e00bf33da88fc2l, 0xd5a79147930aa725l,\n+        0x06ca6351e003826fl, 0x142929670a0e6e70l, 0x27b70a8546d22ffcl,\n+        0x2e1b21385c26c926l, 0x4d2c6dfc5ac42aedl, 0x53380d139d95b3dfl,\n+        0x650a73548baf63del, 0x766a0abb3c77b2a8l, 0x81c2c92e47edaee6l,\n+        0x92722c851482353bl, 0xa2bfe8a14cf10364l, 0xa81a664bbc423001l,\n+        0xc24b8b70d0f89791l, 0xc76c51a30654be30l, 0xd192e819d6ef5218l,\n+        0xd69906245565a910l, 0xf40e35855771202al, 0x106aa07032bbd1b8l,\n+        0x19a4c116b8d2d0c8l, 0x1e376c085141ab53l, 0x2748774cdf8eeb99l,\n+        0x34b0bcb5e19b48a8l, 0x391c0cb3c5c95a63l, 0x4ed8aa4ae3418acbl,\n+        0x5b9cca4f7763e373l, 0x682e6ff3d6b2b8a3l, 0x748f82ee5defb2fcl,\n+        0x78a5636f43172f60l, 0x84c87814a1f0ab72l, 0x8cc702081a6439ecl,\n+        0x90befffa23631e28l, 0xa4506cebde82bde9l, 0xbef9a3f7b2c67915l,\n+        0xc67178f2e372532bl, 0xca273eceea26619cl, 0xd186b8c721c0c207l,\n+        0xeada7dd6cde0eb1el, 0xf57d4f7fee6ed178l, 0x06f067aa72176fbal,\n+        0x0a637dc5a2c898a6l, 0x113f9804bef90dael, 0x1b710b35131c471bl,\n+        0x28db77f523047d84l, 0x32caab7b40c72493l, 0x3c9ebe0a15c9bebcl,\n+        0x431d67c49c100d4cl, 0x4cc5d4becb3e42b6l, 0x597f299cfc657e2al,\n+        0x5fcb6fab3ad6faecl, 0x6c44198c4a475817l\n+      };\n+      constexpr int const_add = vset_sew == Assembler::e32 ? 16 : 32;\n+\n+      __ align(CodeEntryAlignment);\n+      StubCodeMark mark(_cgen, \"StubRoutines\", stub_name<vset_sew>(multi_block));\n+      address start = __ pc();\n+\n+      Register buf   = c_rarg0;\n+      Register state = c_rarg1;\n+      Register ofs   = c_rarg2;\n+      Register limit = c_rarg3;\n+      Register consts = t2;\n+\n+      RegSet saved_regs(t2);\n+\n+      Label multi_block_loop;\n+\n+      __ enter();\n+\n+      __ push_reg(saved_regs, sp);\n+      \/\/ Register use in this function:\n+      \/\/\n+      \/\/ VECTORS\n+      \/\/  v10 - v13 (512\/1024-bits \/ 4*128\/256 bits \/ 4*4*32\/65 bits), hold the message\n+      \/\/             schedule words (Wt). They start with the message block\n+      \/\/             content (W0 to W15), then further words in the message\n+      \/\/             schedule generated via vsha2ms from previous Wt.\n+      \/\/   Initially:\n+      \/\/     v10 = W[  3:0] = { W3,  W2,  W1,  W0}\n+      \/\/     v11 = W[  7:4] = { W7,  W6,  W5,  W4}\n+      \/\/     v12 = W[ 11:8] = {W11, W10,  W9,  W8}\n+      \/\/     v13 = W[15:12] = {W15, W14, W13, W12}\n+      \/\/\n+      \/\/  v16 - v17 hold the working state variables (a, b, ..., h)\n+      \/\/    v16 = {f[t],e[t],b[t],a[t]}\n+      \/\/    v17 = {h[t],g[t],d[t],c[t]}\n+      \/\/   Initially:\n+      \/\/    v16 = {H5i-1, H4i-1, H1i-1 , H0i-1}\n+      \/\/    v17 = {H7i-i, H6i-1, H3i-1 , H2i-1}\n+      \/\/\n+      \/\/  v0 = masks for vrgather\/vmerge. Single value during the 16 rounds.\n+      \/\/\n+      \/\/  v14 = temporary, Wt+Kt\n+      \/\/  v15 = temporary, Kt\n+      \/\/\n+      \/\/  v18\/v19 = temporaries, in the epilogue, to re-arrange\n+      \/\/            and byte-swap v16\/v17\n+      \/\/\n+      \/\/  v26\/v27 = hold the initial values of the hash, byte-swapped.\n+      \/\/\n+      \/\/  v30\/v31 = used to generate masks, vrgather indices.\n+      \/\/\n+      \/\/ During most of the function the vector state is configured so that each\n+      \/\/ vector is interpreted as containing four 32\/64 bits (e32\/e64) elements (128\/256 bits).\n+\n+      \/\/ Set vectors as 4 * 32\/64 bits\n+      \/\/\n+      \/\/ e32\/e64: vector of 32b\/64b\/4B\/8B elements\n+      \/\/ m1: LMUL=1\n+      \/\/ ta: tail agnostic (don't care about those lanes)\n+      \/\/ ma: mask agnostic (don't care about those lanes)\n+      \/\/ x0 is not written, we known the number of vector elements.\n+      __ vsetivli(x0, 4, vset_sew, Assembler::m1, Assembler::ma, Assembler::ta);\n+\n+      \/\/ Load H[0..8] to produce\n+      \/\/  v16 = {a,b,e,f}\n+      \/\/  v17 = {c,d,g,h}\n+      __ vleXX_v<vset_sew>(v16, state);                \/\/ v16 = {d,c,b,a}\n+      __ addi(state, state, const_add);\n+      __ vleXX_v<vset_sew>(v17, state);                \/\/ v17 = {h,g,f,e}\n+\n+      __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n+      __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n+      __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {a,b,c,d}\n+      __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {e,f,g,h}\n+      __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n+      \/\/ Copy elements [3..2] of v26 ({d,c}) into elements [3..2] of v17.\n+      __ vslideup_vi(v17, v26, 2);                     \/\/ v17 = {c,d,_,_}\n+      \/\/ Merge elements [1..0] of v27 ({g,h}) into elements [1..0] of v17\n+      __ vmerge_vvm(v17, v17, v27);                    \/\/ v17 = {c,d,g,h}\n+      \/\/ Copy elements [1..0] of v27 ({f,e}) into elements [1..0] of v16.\n+      __ vslidedown_vi(v16, v27, 2);                   \/\/ v16 = {_,_,e,f}\n+      \/\/ Merge elements [3..2] of v26 ({a,b}) into elements [3..2] of v16\n+      __ vmerge_vvm(v16, v26, v16);                    \/\/ v16 = {a,b,e,f}\n+\n+      __ bind(multi_block_loop);\n+\n+      \/\/ Capture the initial H values in v26 and v27 to allow for computing\n+      \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n+      __ vmv_v_v(v26, v16);\n+      __ vmv_v_v(v27, v17);\n+\n+      \/\/ Load the 512\/1024-bits of the message block in v10-v13 and perform\n+      \/\/ an endian swap on each 4\/8 bytes element.\n+      \/\/\n+      \/\/ If Zvkb is not implemented one can use vrgather\n+      \/\/ with an index sequence to byte-swap.\n+      \/\/  sequence = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n+      \/\/   <https:\/\/oeis.org\/A004444> gives us \"N ^ 3\" as a nice formula to generate\n+      \/\/  this sequence. 'vid' gives us the N.\n+      __ vleXX_v<vset_sew>(v10, buf);\n+      __ vrev8_v(v10, v10);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v<vset_sew>(v11, buf);\n+      __ vrev8_v(v11, v11);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v<vset_sew>(v12, buf);\n+      __ vrev8_v(v12, v12);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v<vset_sew>(v13, buf);\n+      __ vrev8_v(v13, v13);\n+      __ addi(buf, buf, const_add);\n+\n+      \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n+      __ vid_v(v0);\n+      __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n+\n+      address constant_table = vset_sew == Assembler::e32 ? (address)round_consts_256 : (address)round_consts_512;\n+      la(consts, ExternalAddress(constant_table));\n+\n+      VectorRegister rotation_regs[] = {v10, v11, v12, v13};\n+      int rot_pos = 0;\n+      \/\/ Quad-round #0 (+0, v10->v11->v12->v13) ... #11 (+3, v13->v10->v11->v12)\n+      constexpr int qr_end = vset_sew == Assembler::e32 ? 12 : 16;\n+      for (int i = 0; i < qr_end; i++) {\n+        sha2_quad_round<vset_sew>\n+                  (rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   v15, v14, v16, v17);\n+        ++rot_pos;\n+      }\n+      \/\/ Quad-round #12 (+0, v10->v11->v12->v13) ... #15 (+3, v13->v10->v11->v12)\n+      \/\/ Note that we stop generating new message schedule words (Wt, v10-13)\n+      \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n+      constexpr int qr_c_end = qr_end + 4;\n+      for (int i = qr_end; i < qr_c_end; i++) {\n+        sha2_quad_round<vset_sew>\n+                  (rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   v15, v14, v16, v17, false, i < (qr_c_end-1));\n+        ++rot_pos;\n+      }\n+\n+      \/\/--------------------------------------------------------------------------------\n+      \/\/ Compute the updated hash value H'\n+      \/\/   H' = H + {h',g',...,b',a'}\n+      \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n+      \/\/      = {h+h',g+g',...,b+b',a+a'}\n+\n+      \/\/ H' = H+{a',b',c',...,h'}\n+      __ vadd_vv(v16, v26, v16);\n+      __ vadd_vv(v17, v27, v17);\n+\n+      if (multi_block) {\n+        __ add(ofs, ofs, vset_sew == Assembler::e32 ? 64 : 128);\n+        __ ble(ofs, limit, multi_block_loop);\n+        __ mv(c_rarg0, ofs); \/\/ return ofs\n+      }\n+\n+      \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n+      \/\/  v16 = {f,e,b,a}\n+      \/\/  v17 = {h,g,d,c}\n+      __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n+      __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n+      __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {f,e,b,a}\n+      __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {h,g,d,c}\n+      __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n+      \/\/ Copy elements [3..2] of v26 ({f,e}) into elements [1..0] of v17.\n+      __ vslidedown_vi(v17, v26, 2);                   \/\/ v17 = {_,_,f,e}\n+      \/\/ Merge elements [3..2] of v27 ({g,h}) into elements [3..2] of v17\n+      __ vmerge_vvm(v17, v27, v17);                    \/\/ v17 = {h,g,f,e}\n+      \/\/ Copy elements [1..0] of v27 ({c,d}) into elements [3..2] of v16.\n+      __ vslideup_vi(v16, v27, 2);                     \/\/ v16 = {d,c,_,_}\n+      \/\/ Merge elements [1..0] of v26 ({a,b}) into elements [1..0] of v16\n+      __ vmerge_vvm(v16, v16, v26);                    \/\/ v16 = {d,c,b,a}\n+\n+      \/\/ Save the hash\n+      __ vseXX_v<vset_sew>(v17, state);\n+      __ addi(state, state, -const_add);\n+      __ vseXX_v<vset_sew>(v16, state);\n+\n+      __ pop_reg(saved_regs, sp);\n+      __ leave();\n+      __ ret();\n+\n+      return start;\n+    }\n+  };\n+#undef __\n+#define __ masm->\n+\n@@ -4861,0 +5270,12 @@\n+    if (UseSHA256Intrinsics) {\n+      Sha2Generator sha2(_masm, this);\n+      StubRoutines::_sha256_implCompress   = sha2.generate_sha256_implCompress(false);\n+      StubRoutines::_sha256_implCompressMB = sha2.generate_sha256_implCompress(true);\n+    }\n+\n+    if (UseSHA512Intrinsics) {\n+      Sha2Generator sha2(_masm, this);\n+      StubRoutines::_sha512_implCompress   = sha2.generate_sha512_implCompress(false);\n+      StubRoutines::_sha512_implCompressMB = sha2.generate_sha512_implCompress(true);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":421,"deletions":0,"binary":false,"changes":421,"status":"modified"},{"patch":"@@ -159,3 +159,19 @@\n-  if (UseSHA256Intrinsics) {\n-    warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n+  if (UseZvkn) {\n+    if (!ext_V.enabled()) {\n+      FLAG_SET_DEFAULT(UseZvkn, false);\n+      warning(\"Cannot enable Zvkn on cpu without RVV support.\");\n+    }\n+    if (FLAG_IS_DEFAULT(UseRVV)) {\n+      FLAG_SET_DEFAULT(UseRVV, true);\n+    }\n+    if (UseRVV) {\n+      if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+        FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+      }\n+      if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+        FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+      }\n+    } else {\n+      FLAG_SET_DEFAULT(UseZvkn, false);\n+      warning(\"Cannot enabled Zvkn when RVV is disabled.\");\n+    }\n@@ -163,4 +179,9 @@\n-\n-  if (UseSHA512Intrinsics) {\n-    warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n+  if (!UseZvkn) {\n+    if (UseSHA256Intrinsics) {\n+      warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU, UseZvkn needed.\");\n+      FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n+    }\n+    if (UseSHA512Intrinsics) {\n+      warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU, UseZvkn needed.\");\n+      FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"}]}