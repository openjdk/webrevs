{"files":[{"patch":"@@ -115,3 +115,2 @@\n-  product(bool, UseZvkb, false, EXPERIMENTAL, \"Use Zvkb instructions\")           \\\n-  product(bool, UseZvknha, false, EXPERIMENTAL, \"Use Zvknha instructions\")       \\\n-  product(bool, UseZvknhb, false, EXPERIMENTAL, \"Use Zvknhb instructions\")       \\\n+  product(bool, UseZvkn, false, EXPERIMENTAL,                                    \\\n+          \"Use Zvkn group extension, Zvkned, Zvknhb, Zvkb, Zvkt\")                \\\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3662,33 +3662,0 @@\n-#endif \/\/ COMPILER2\n-\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - byte[]  source+offset\n-  \/\/   c_rarg1   - int[]   SHA.state\n-  \/\/   c_rarg2   - int     offset\n-  \/\/   c_rarg3   - int     limit\n-  \/\/\n-  address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    static const uint32_t round_consts[64] = {\n-      0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n-      0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n-      0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n-      0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n-      0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n-      0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n-      0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n-      0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n-      0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n-      0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n-      0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n-      0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n-      0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n-      0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n-      0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n-      0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n-    };\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n@@ -3696,5 +3663,1 @@\n-    Register buf   = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs   = c_rarg2;\n-    Register limit = c_rarg3;\n-    Register consts = t0;\n+#endif \/\/ COMPILER2\n@@ -3702,1 +3665,13 @@\n-    Label multi_block_loop;\n+#undef __\n+#define __ this->\n+  class Sha2Generator : public MacroAssembler {\n+    StubCodeGenerator* _cgen;\n+   public:\n+      Sha2Generator(MacroAssembler* masm, StubCodeGenerator* cgen) : MacroAssembler(masm->code()), _cgen(cgen) {}\n+      address generate_sha256_implCompress(bool multi_block) {\n+        return generate_sha2_implCompress<Assembler::e32>(multi_block);\n+      }\n+      address generate_sha512_implCompress(bool multi_block) {\n+        return generate_sha2_implCompress<Assembler::e64>(multi_block);\n+      }\n+   private:\n@@ -3704,1 +3679,5 @@\n-    __ enter();\n+    template<Assembler::SEW T>\n+    void vl1reXX_v(VectorRegister vr, Register sr) {\n+      if (T == Assembler::e32) __ vl1re32_v(vr, sr);\n+      else                     __ vl1re64_v(vr, sr);\n+    }\n@@ -3706,34 +3685,5 @@\n-    \/\/ Register use in this function:\n-    \/\/\n-    \/\/ VECTORS\n-    \/\/  v10 - v13 (512-bits \/ 4*128 bits \/ 4*4*32 bits), hold the message\n-    \/\/             schedule words (Wt). They start with the message block\n-    \/\/             content (W0 to W15), then further words in the message\n-    \/\/             schedule generated via vsha2ms from previous Wt.\n-    \/\/   Initially:\n-    \/\/     v10 = W[  3:0] = { W3,  W2,  W1,  W0}\n-    \/\/     v11 = W[  7:4] = { W7,  W6,  W5,  W4}\n-    \/\/     v12 = W[ 11:8] = {W11, W10,  W9,  W8}\n-    \/\/     v13 = W[15:12] = {W15, W14, W13, W12}\n-    \/\/\n-    \/\/  v16 - v17 hold the working state variables (a, b, ..., h)\n-    \/\/    v16 = {a[t],b[t],e[t],f[t]}\n-    \/\/    v17 = {c[t],d[t],g[t],h[t]}\n-    \/\/   Initially:\n-    \/\/    v16 = {H5i-1, H4i-1, H1i-1 , H0i-1}\n-    \/\/    v17 = {H7i-i, H6i-1, H3i-1 , H2i-1}\n-    \/\/\n-    \/\/  v0 = masks for vrgather\/vmerge. Single value during the 16 rounds.\n-    \/\/\n-    \/\/  v14 = temporary, Wt+Kt\n-    \/\/  v15 = temporary, Kt\n-    \/\/\n-    \/\/  v18\/v19 = temporaries, in the epilogue, to re-arrange\n-    \/\/            and byte-swap v16\/v17\n-    \/\/\n-    \/\/  v26\/v27 = hold the initial values of the hash, byte-swapped.\n-    \/\/\n-    \/\/  v30\/v31 = used to generate masks, vrgather indices.\n-    \/\/\n-    \/\/ During most of the function the vector state is configured so that each\n-    \/\/ vector is interpreted as containing four 32 bits (e32) elements (128 bits).\n+    template<Assembler::SEW T>\n+    void vleXX_v(VectorRegister vr, Register sr) {\n+      if (T == Assembler::e32) __ vle32_v(vr, sr);\n+      else                     __ vle64_v(vr, sr);\n+    }\n@@ -3741,77 +3691,5 @@\n-    \/\/ Set vectors as 4 * 32 bits\n-    \/\/\n-    \/\/ e32: vector of 32b\/4B elements\n-    \/\/ m1: LMUL=1\n-    \/\/ ta: tail agnostic (don't care about those lanes)\n-    \/\/ ma: mask agnostic (don't care about those lanes)\n-    \/\/ x0 is not written, we known the number of vector elements, 8.\n-    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1, Assembler::ma, Assembler::ta);\n-\n-    \/\/ Load H[0..8] to produce\n-    \/\/  v16 = {a,b,e,f}\n-    \/\/  v17 = {c,d,g,h}\n-    __ vle32_v(v16, state);                          \/\/ v16 = {d,c,b,a}\n-    __ addi(state, state, 16);\n-    __ vle32_v(v17, state);                          \/\/ v17 = {h,g,f,e}\n-\n-    __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n-    __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n-    __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {a,b,c,d}\n-    __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {e,f,g,h}\n-    __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n-    \/\/ Copy elements [3..2] of v26 ({d,c}) into elements [3..2] of v17.\n-    __ vslideup_vi(v17, v26, 2);                     \/\/ v17 = {c,d,_,_}\n-    \/\/ Merge elements [1..0] of v27 ({g,h}) into elements [1..0] of v17\n-    __ vmerge_vvm(v17, v17, v27);                    \/\/ v17 = {c,d,g,h}\n-    \/\/ Copy elements [1..0] of v27 ({f,e}) into elements [1..0] of v16.\n-    __ vslidedown_vi(v16, v27, 2);                   \/\/ v16 = {_,_,e,f}\n-    \/\/ Merge elements [3..2] of v26 ({a,b}) into elements [3..2] of v16\n-    __ vmerge_vvm(v16, v26, v16);                    \/\/ v16 = {a,b,e,f}\n-\n-    __ bind(multi_block_loop);\n-\n-    \/\/ Capture the initial H values in v26 and v27 to allow for computing\n-    \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n-    __ vmv_v_v(v26, v16);\n-    __ vmv_v_v(v27, v17);\n-\n-    \/\/ Load the 512-bits of the message block in v10-v13 and perform\n-    \/\/ an endian swap on each 4 bytes element.\n-    \/\/\n-    \/\/ If Zvkb is not implemented, one can use vrgather with the right index\n-    \/\/ sequence. It requires loading in separate registers since the destination\n-    \/\/ of vrgather cannot overlap the source.\n-    \/\/    \/\/ We generate the lane (byte) index sequence\n-    \/\/    \/\/    v24 = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n-    \/\/    \/\/ <https:\/\/oeis.org\/a104444> gives us \"N ^ 3\" as a nice formula to generate\n-    \/\/    \/\/ this sequence. 'vid' gives us the N.\n-    \/\/    \/\/\n-    \/\/    \/\/ We switch the vector type to SEW=8 temporarily.\n-    \/\/    vsetivli x0, 16, e8, m1, ta, ma\n-    \/\/    vid.v v24\n-    \/\/    vxor.vi v24, v24, 0x3\n-    \/\/    \/\/ Byteswap the bytes in each word of the text.\n-    \/\/    vrgather.vv v10, v20, v24\n-    \/\/    vrgather.vv v11, v21, v24\n-    \/\/    vrgather.vv v12, v22, v24\n-    \/\/    vrgather.vv v13, v23, v24\n-    \/\/    \/\/ Switch back to SEW=32\n-    \/\/    vsetivli x0, 4, e32, m1, ta, ma\n-    __ vle32_v(v10, buf);\n-    __ vrev8_v(v10, v10);\n-    __ addi(buf, buf, 16);\n-    __ vle32_v(v11, buf);\n-    __ vrev8_v(v11, v11);\n-    __ addi(buf, buf, 16);\n-    __ vle32_v(v12, buf);\n-    __ vrev8_v(v12, v12);\n-    __ addi(buf, buf, 16);\n-    __ vle32_v(v13, buf);\n-    __ vrev8_v(v13, v13);\n-    __ addi(buf, buf, 16);\n-\n-    \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n-    __ vid_v(v0);\n-    __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n-\n-    __ la(consts, ExternalAddress((address)round_consts));\n+    template<Assembler::SEW T>\n+    void vseXX_v(VectorRegister vr, Register sr) {\n+      if (T == Assembler::e32) __ vse32_v(vr, sr);\n+      else                     __ vse64_v(vr, sr);\n+    }\n@@ -3821,3 +3699,3 @@\n-    \/\/ The code below repeats 16 times the logic implementing four rounds\n-    \/\/ of the SHA-256 core loop as documented by NIST. 16 \"quad rounds\"\n-    \/\/ to implementing the 64 single rounds.\n+    \/\/ The code below repeats 16\/20 times the logic implementing four rounds\n+    \/\/ of the SHA-256\/512 core loop as documented by NIST. 16\/20 \"quad rounds\"\n+    \/\/ to implementing the 64\/80 single rounds.\n@@ -3825,1 +3703,1 @@\n-    \/\/    \/\/ Load four word (u32) constants (K[t+3], K[t+2], K[t+1], K[t+0])\n+    \/\/    \/\/ Load four word (u32\/64) constants (K[t+3], K[t+2], K[t+1], K[t+0])\n@@ -3828,1 +3706,1 @@\n-    \/\/    vl1re32.v v15, ofs\n+    \/\/    vl1reXX.v v15, ofs\n@@ -3830,2 +3708,2 @@\n-    \/\/    \/\/ Increment word contant address by stride (16 bytes, 4*4B, 128b)\n-    \/\/    addi ofs, ofs, 16\n+    \/\/    \/\/ Increment word contant address by stride (16\/32 bytes, 4*4B\/8B, 128b\/256b)\n+    \/\/    addi ofs, ofs, 16\/32\n@@ -3914,156 +3792,17 @@\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 0 (+0, v10->v11->v12->v13)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13); \/\/ Generate W[19:16]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 1 (+1, v11->v12->v13->v10)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10); \/\/ Generate W[23:20]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 2 (+2, v12->v13->v10->v11)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11); \/\/ Generate W[27:24]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 3 (+3, v13->v10->v11->v12)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12); \/\/ Generate W[31:28]\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 4 (+0, v10->v11->v12->v13)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13); \/\/ Generate W[35:32]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 5 (+1, v11->v12->v13->v10)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10); \/\/ Generate W[39:36]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 6 (+2, v12->v13->v10->v11)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11); \/\/ Generate W[43:40]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 7 (+3, v13->v10->v11->v12)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12); \/\/ Generate W[47:44]\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 8 (+0, v10->v11->v12->v13)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13); \/\/ Generate W[51:48]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 9 (+1, v11->v12->v13->v10)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10); \/\/ Generate W[55:52]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 10 (+2, v12->v13->v10->v11)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11); \/\/ Generate W[59:56]\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 11 (+3, v13->v10->v11->v12)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12); \/\/ Generate W[63:60]\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 12 (+0, v10->v11->v12->v13)\n-    \/\/ Note that we stop generating new message schedule words (Wt, v10-13)\n-    \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 13 (+1, v11->v12->v13->v10)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 14 (+2, v12->v13->v10->v11)\n-    __ vl1re32_v(v15, consts);\n-    __ addi(consts, consts, 16);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 15 (+3, v13->v10->v11->v12)\n-    __ vl1re32_v(v15, consts);\n-    \/\/ No consts increment needed\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Compute the updated hash value H'\n-    \/\/   H' = H + {h',g',...,b',a'}\n-    \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n-    \/\/      = {h+h',g+g',...,b+b',a+a'}\n-\n-    __ vadd_vv(v16, v26, v16);\n-    __ vadd_vv(v17, v27, v17);\n-\n-    if (multi_block) {\n-      __ add(ofs, ofs, 64);\n-      __ ble(ofs, limit, multi_block_loop);\n-      __ mv(c_rarg0, ofs); \/\/ return ofs\n+    template<Assembler::SEW vset_sew>\n+    void sha2_quad_round(VectorRegister rot1, VectorRegister rot2, VectorRegister rot3, VectorRegister rot4,\n+                         Register scalarconst, VectorRegister vtemp, VectorRegister vtemp2, VectorRegister vtemp3, VectorRegister vtemp4,\n+                         bool gen_words = true, bool step_const = true) {\n+      __ vl1reXX_v<vset_sew>(vtemp, scalarconst);\n+      if (step_const) {\n+        __ addi(scalarconst, scalarconst, vset_sew == Assembler::e32 ? 16 : 32);\n+      }\n+      __ vadd_vv(vtemp2, vtemp, rot1);\n+      __ vsha2cl_vv(vtemp4, vtemp3, vtemp2);\n+      __ vsha2ch_vv(vtemp3, vtemp4, vtemp2);\n+      if ((vset_sew == Assembler::e64 && step_const) || gen_words) {\n+        __ vmerge_vvm(vtemp2, rot3, rot2);\n+      }\n+      if (gen_words) {\n+        __ vsha2ms_vv(rot1, vtemp2, rot4);\n+      }\n@@ -4072,114 +3811,8 @@\n-    \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n-    \/\/  v16 = {f,e,b,a}\n-    \/\/  v17 = {h,g,d,c}\n-    __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n-    __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n-    __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {f,e,b,a}\n-    __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {h,g,d,c}\n-    __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n-    \/\/ Copy elements [3..2] of v26 ({f,e}) into elements [1..0] of v17.\n-    __ vslidedown_vi(v17, v26, 2);                   \/\/ v17 = {_,_,f,e}\n-    \/\/ Merge elements [3..2] of v27 ({g,h}) into elements [3..2] of v17\n-    __ vmerge_vvm(v17, v27, v17);                    \/\/ v17 = {h,g,f,e}\n-    \/\/ Copy elements [1..0] of v27 ({c,d}) into elements [3..2] of v16.\n-    __ vslideup_vi(v16, v27, 2);                     \/\/ v16 = {d,c,_,_}\n-    \/\/ Merge elements [1..0] of v26 ({a,b}) into elements [1..0] of v16\n-    __ vmerge_vvm(v16, v16, v26);                    \/\/ v16 = {d,c,b,a}\n-\n-    \/\/ Save the hash\n-    __ vse32_v(v17, state);\n-    __ addi(state, state, -16);\n-    __ vse32_v(v16, state);\n-\n-    __ leave();\n-    __ ret();\n-\n-    return start;\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - byte[]  source+offset\n-  \/\/   c_rarg1   - int[]   SHA.state\n-  \/\/   c_rarg2   - int     offset\n-  \/\/   c_rarg3   - int     limit\n-  \/\/\n-  address generate_sha512_implCompress(bool multi_block, const char *name) {\n-    static const uint64_t round_consts[80] = {\n-      0x428a2f98d728ae22l, 0x7137449123ef65cdl, 0xb5c0fbcfec4d3b2fl,\n-      0xe9b5dba58189dbbcl, 0x3956c25bf348b538l, 0x59f111f1b605d019l,\n-      0x923f82a4af194f9bl, 0xab1c5ed5da6d8118l, 0xd807aa98a3030242l,\n-      0x12835b0145706fbel, 0x243185be4ee4b28cl, 0x550c7dc3d5ffb4e2l,\n-      0x72be5d74f27b896fl, 0x80deb1fe3b1696b1l, 0x9bdc06a725c71235l,\n-      0xc19bf174cf692694l, 0xe49b69c19ef14ad2l, 0xefbe4786384f25e3l,\n-      0x0fc19dc68b8cd5b5l, 0x240ca1cc77ac9c65l, 0x2de92c6f592b0275l,\n-      0x4a7484aa6ea6e483l, 0x5cb0a9dcbd41fbd4l, 0x76f988da831153b5l,\n-      0x983e5152ee66dfabl, 0xa831c66d2db43210l, 0xb00327c898fb213fl,\n-      0xbf597fc7beef0ee4l, 0xc6e00bf33da88fc2l, 0xd5a79147930aa725l,\n-      0x06ca6351e003826fl, 0x142929670a0e6e70l, 0x27b70a8546d22ffcl,\n-      0x2e1b21385c26c926l, 0x4d2c6dfc5ac42aedl, 0x53380d139d95b3dfl,\n-      0x650a73548baf63del, 0x766a0abb3c77b2a8l, 0x81c2c92e47edaee6l,\n-      0x92722c851482353bl, 0xa2bfe8a14cf10364l, 0xa81a664bbc423001l,\n-      0xc24b8b70d0f89791l, 0xc76c51a30654be30l, 0xd192e819d6ef5218l,\n-      0xd69906245565a910l, 0xf40e35855771202al, 0x106aa07032bbd1b8l,\n-      0x19a4c116b8d2d0c8l, 0x1e376c085141ab53l, 0x2748774cdf8eeb99l,\n-      0x34b0bcb5e19b48a8l, 0x391c0cb3c5c95a63l, 0x4ed8aa4ae3418acbl,\n-      0x5b9cca4f7763e373l, 0x682e6ff3d6b2b8a3l, 0x748f82ee5defb2fcl,\n-      0x78a5636f43172f60l, 0x84c87814a1f0ab72l, 0x8cc702081a6439ecl,\n-      0x90befffa23631e28l, 0xa4506cebde82bde9l, 0xbef9a3f7b2c67915l,\n-      0xc67178f2e372532bl, 0xca273eceea26619cl, 0xd186b8c721c0c207l,\n-      0xeada7dd6cde0eb1el, 0xf57d4f7fee6ed178l, 0x06f067aa72176fbal,\n-      0x0a637dc5a2c898a6l, 0x113f9804bef90dael, 0x1b710b35131c471bl,\n-      0x28db77f523047d84l, 0x32caab7b40c72493l, 0x3c9ebe0a15c9bebcl,\n-      0x431d67c49c100d4cl, 0x4cc5d4becb3e42b6l, 0x597f299cfc657e2al,\n-      0x5fcb6fab3ad6faecl, 0x6c44198c4a475817l\n-    };\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf   = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs   = c_rarg2;\n-    Register limit = c_rarg3;\n-    Register consts = t0;\n-\n-    Label multi_block_loop;\n-\n-    __ enter();\n-\n-    \/\/ Register use in this function:\n-    \/\/\n-    \/\/ VECTORS\n-    \/\/  v10 - v13 (1024-bits \/ 4*256 bits \/ 4*4*64 bits), hold the message\n-    \/\/             schedule words (Wt). They start with the message block\n-    \/\/             content (W0 to W15), then further words in the message\n-    \/\/             schedule generated via vsha2ms from previous Wt.\n-    \/\/   Initially:\n-    \/\/     v10 = W[  3:0] = { W3,  W2,  W1,  W0}\n-    \/\/     v11 = W[  7:4] = { W7,  W6,  W5,  W4}\n-    \/\/     v12 = W[ 11:8] = {W11, W10,  W9,  W8}\n-    \/\/     v13 = W[15:12] = {W15, W14, W13, W12}\n-    \/\/\n-    \/\/  v16 - v17 hold the working state variables (a, b, ..., h)\n-    \/\/    v16 = {f[t],e[t],b[t],a[t]}\n-    \/\/    v17 = {h[t],g[t],d[t],c[t]}\n-    \/\/   Initially:\n-    \/\/    v16 = {H5i-1, H4i-1, H1i-1 , H0i-1}\n-    \/\/    v17 = {H7i-i, H6i-1, H3i-1 , H2i-1}\n-    \/\/\n-    \/\/  v0 = masks for vrgather\/vmerge. Single value during the 16 rounds.\n-    \/\/\n-    \/\/  v14 = temporary, Wt+Kt\n-    \/\/  v15 = temporary, Kt\n-    \/\/\n-    \/\/  v18\/v19 = temporaries, in the epilogue, to re-arrange\n-    \/\/            and byte-swap v16\/v17\n-    \/\/\n-    \/\/  v26\/v27 = hold the initial values of the hash, byte-swapped.\n-    \/\/\n-    \/\/  v30\/v31 = used to generate masks, vrgather indices.\n-    \/\/\n-    \/\/ During most of the function the vector state is configured so that each\n-    \/\/ vector is interpreted as containing four 64 bits (e64) elements (256 bits).\n+    template<Assembler::SEW vset_sew>\n+    const char* stub_name(bool multi_block) {\n+      if (vset_sew == Assembler::e32 && !multi_block) return \"sha256_implCompress\";\n+      if (vset_sew == Assembler::e32 &&  multi_block) return \"sha256_implCompressMB\";\n+      if (vset_sew == Assembler::e64 && !multi_block) return \"sha512_implCompress\";\n+      if (vset_sew == Assembler::e64 &&  multi_block) return \"sha512_implCompressMB\";\n+      return \"bad name lookup\";\n+    }\n@@ -4187,1 +3820,1 @@\n-    \/\/ Set vectors as 4 * 64\n+    \/\/ Arguments:\n@@ -4189,37 +3822,5 @@\n-    \/\/ e64: vector of 64b\/8B elements\n-    \/\/ m1: LMUL=1\n-    \/\/ ta: tail agnostic (don't care about those lanes)\n-    \/\/ ma: mask agnostic (don't care about those lanes)\n-    \/\/ x0 is not written, we known the number of vector elements, 2.\n-    __ vsetivli(x0, 4, Assembler::e64, Assembler::m1, Assembler::ma, Assembler::ta);\n-\n-    \/\/ Load H[0..8] to produce\n-    \/\/  v16 = {a,b,e,f}\n-    \/\/  v17 = {c,d,g,h}\n-    __ vle64_v(v16, state);                          \/\/ v16 = {d,c,b,a}\n-    __ addi(state, state, 32);\n-    __ vle64_v(v17, state);                          \/\/ v17 = {h,g,f,e}\n-\n-    __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n-    __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n-    __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {a,b,c,d}\n-    __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {e,f,g,h}\n-    __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n-    \/\/ Copy elements [3..2] of v26 ({d,c}) into elements [3..2] of v17.\n-    __ vslideup_vi(v17, v26, 2);                     \/\/ v17 = {c,d,_,_}\n-    \/\/ Merge elements [1..0] of v27 ({g,h}) into elements [1..0] of v17\n-    __ vmerge_vvm(v17, v17, v27);                    \/\/ v17 = {c,d,g,h}\n-    \/\/ Copy elements [1..0] of v27 ({f,e}) into elements [1..0] of v16.\n-    __ vslidedown_vi(v16, v27, 2);                   \/\/ v16 = {_,_,e,f}\n-    \/\/ Merge elements [3..2] of v26 ({a,b}) into elements [3..2] of v16\n-    __ vmerge_vvm(v16, v26, v16);                    \/\/ v16 = {a,b,e,f}\n-\n-    __ bind(multi_block_loop);\n-\n-    \/\/ Capture the initial H values in v26 and v27 to allow for computing\n-    \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n-    __ vmv_v_v(v26, v16);\n-    __ vmv_v_v(v27, v17);\n-\n-    \/\/ Load the 1024-bits of the message block in v10-v13 and perform\n-    \/\/ an endian swap on each 8 bytes element.\n+    \/\/ Inputs:\n+    \/\/   c_rarg0   - byte[]  source+offset\n+    \/\/   c_rarg1   - int[]   SHA.state\n+    \/\/   c_rarg2   - int     offset\n+    \/\/   c_rarg3   - int     limit\n@@ -4227,23 +3828,102 @@\n-    \/\/ If Zvkb is not implemented, similar to SHA-256, one can use vrgather\n-    \/\/ with an index sequence to byte-swap.\n-    \/\/  sequence = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n-    \/\/   <https:\/\/oeis.org\/A004444> gives us \"N ^ 3\" as a nice formula to generate\n-    \/\/  this sequence. 'vid' gives us the N.\n-    __ vle64_v(v10, buf);\n-    __ vrev8_v(v10, v10);\n-    __ add(buf, buf, 32);\n-    __ vle64_v(v11, buf);\n-    __ vrev8_v(v11, v11);\n-    __ add(buf, buf, 32);\n-    __ vle64_v(v12, buf);\n-    __ vrev8_v(v12, v12);\n-    __ add(buf, buf, 32);\n-    __ vle64_v(v13, buf);\n-    __ vrev8_v(v13, v13);\n-    __ add(buf, buf, 32);\n-\n-    \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n-    __ vid_v(v0);\n-    __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n-\n-    __ la(consts, ExternalAddress((address)round_consts));\n+    template<Assembler::SEW vset_sew>\n+    address generate_sha2_implCompress(bool multi_block) {\n+      alignas(64) static const uint32_t round_consts_256[64] = {\n+        0x428a2f98, 0x71374491, 0xb5c0fbcf, 0xe9b5dba5,\n+        0x3956c25b, 0x59f111f1, 0x923f82a4, 0xab1c5ed5,\n+        0xd807aa98, 0x12835b01, 0x243185be, 0x550c7dc3,\n+        0x72be5d74, 0x80deb1fe, 0x9bdc06a7, 0xc19bf174,\n+        0xe49b69c1, 0xefbe4786, 0x0fc19dc6, 0x240ca1cc,\n+        0x2de92c6f, 0x4a7484aa, 0x5cb0a9dc, 0x76f988da,\n+        0x983e5152, 0xa831c66d, 0xb00327c8, 0xbf597fc7,\n+        0xc6e00bf3, 0xd5a79147, 0x06ca6351, 0x14292967,\n+        0x27b70a85, 0x2e1b2138, 0x4d2c6dfc, 0x53380d13,\n+        0x650a7354, 0x766a0abb, 0x81c2c92e, 0x92722c85,\n+        0xa2bfe8a1, 0xa81a664b, 0xc24b8b70, 0xc76c51a3,\n+        0xd192e819, 0xd6990624, 0xf40e3585, 0x106aa070,\n+        0x19a4c116, 0x1e376c08, 0x2748774c, 0x34b0bcb5,\n+        0x391c0cb3, 0x4ed8aa4a, 0x5b9cca4f, 0x682e6ff3,\n+        0x748f82ee, 0x78a5636f, 0x84c87814, 0x8cc70208,\n+        0x90befffa, 0xa4506ceb, 0xbef9a3f7, 0xc67178f2,\n+      };\n+      alignas(64) static const uint64_t round_consts_512[80] = {\n+        0x428a2f98d728ae22l, 0x7137449123ef65cdl, 0xb5c0fbcfec4d3b2fl,\n+        0xe9b5dba58189dbbcl, 0x3956c25bf348b538l, 0x59f111f1b605d019l,\n+        0x923f82a4af194f9bl, 0xab1c5ed5da6d8118l, 0xd807aa98a3030242l,\n+        0x12835b0145706fbel, 0x243185be4ee4b28cl, 0x550c7dc3d5ffb4e2l,\n+        0x72be5d74f27b896fl, 0x80deb1fe3b1696b1l, 0x9bdc06a725c71235l,\n+        0xc19bf174cf692694l, 0xe49b69c19ef14ad2l, 0xefbe4786384f25e3l,\n+        0x0fc19dc68b8cd5b5l, 0x240ca1cc77ac9c65l, 0x2de92c6f592b0275l,\n+        0x4a7484aa6ea6e483l, 0x5cb0a9dcbd41fbd4l, 0x76f988da831153b5l,\n+        0x983e5152ee66dfabl, 0xa831c66d2db43210l, 0xb00327c898fb213fl,\n+        0xbf597fc7beef0ee4l, 0xc6e00bf33da88fc2l, 0xd5a79147930aa725l,\n+        0x06ca6351e003826fl, 0x142929670a0e6e70l, 0x27b70a8546d22ffcl,\n+        0x2e1b21385c26c926l, 0x4d2c6dfc5ac42aedl, 0x53380d139d95b3dfl,\n+        0x650a73548baf63del, 0x766a0abb3c77b2a8l, 0x81c2c92e47edaee6l,\n+        0x92722c851482353bl, 0xa2bfe8a14cf10364l, 0xa81a664bbc423001l,\n+        0xc24b8b70d0f89791l, 0xc76c51a30654be30l, 0xd192e819d6ef5218l,\n+        0xd69906245565a910l, 0xf40e35855771202al, 0x106aa07032bbd1b8l,\n+        0x19a4c116b8d2d0c8l, 0x1e376c085141ab53l, 0x2748774cdf8eeb99l,\n+        0x34b0bcb5e19b48a8l, 0x391c0cb3c5c95a63l, 0x4ed8aa4ae3418acbl,\n+        0x5b9cca4f7763e373l, 0x682e6ff3d6b2b8a3l, 0x748f82ee5defb2fcl,\n+        0x78a5636f43172f60l, 0x84c87814a1f0ab72l, 0x8cc702081a6439ecl,\n+        0x90befffa23631e28l, 0xa4506cebde82bde9l, 0xbef9a3f7b2c67915l,\n+        0xc67178f2e372532bl, 0xca273eceea26619cl, 0xd186b8c721c0c207l,\n+        0xeada7dd6cde0eb1el, 0xf57d4f7fee6ed178l, 0x06f067aa72176fbal,\n+        0x0a637dc5a2c898a6l, 0x113f9804bef90dael, 0x1b710b35131c471bl,\n+        0x28db77f523047d84l, 0x32caab7b40c72493l, 0x3c9ebe0a15c9bebcl,\n+        0x431d67c49c100d4cl, 0x4cc5d4becb3e42b6l, 0x597f299cfc657e2al,\n+        0x5fcb6fab3ad6faecl, 0x6c44198c4a475817l\n+      };\n+      constexpr int const_add = vset_sew == Assembler::e32 ? 16 : 32;\n+\n+      __ align(CodeEntryAlignment);\n+      StubCodeMark mark(_cgen, \"StubRoutines\", stub_name<vset_sew>(multi_block));\n+      address start = __ pc();\n+\n+      Register buf   = c_rarg0;\n+      Register state = c_rarg1;\n+      Register ofs   = c_rarg2;\n+      Register limit = c_rarg3;\n+      Register consts = t2;\n+\n+      RegSet saved_regs(t2);\n+\n+      Label multi_block_loop;\n+\n+      __ enter();\n+\n+      __ push_reg(saved_regs, sp);\n+      \/\/ Register use in this function:\n+      \/\/\n+      \/\/ VECTORS\n+      \/\/  v10 - v13 (512\/1024-bits \/ 4*128\/256 bits \/ 4*4*32\/65 bits), hold the message\n+      \/\/             schedule words (Wt). They start with the message block\n+      \/\/             content (W0 to W15), then further words in the message\n+      \/\/             schedule generated via vsha2ms from previous Wt.\n+      \/\/   Initially:\n+      \/\/     v10 = W[  3:0] = { W3,  W2,  W1,  W0}\n+      \/\/     v11 = W[  7:4] = { W7,  W6,  W5,  W4}\n+      \/\/     v12 = W[ 11:8] = {W11, W10,  W9,  W8}\n+      \/\/     v13 = W[15:12] = {W15, W14, W13, W12}\n+      \/\/\n+      \/\/  v16 - v17 hold the working state variables (a, b, ..., h)\n+      \/\/    v16 = {f[t],e[t],b[t],a[t]}\n+      \/\/    v17 = {h[t],g[t],d[t],c[t]}\n+      \/\/   Initially:\n+      \/\/    v16 = {H5i-1, H4i-1, H1i-1 , H0i-1}\n+      \/\/    v17 = {H7i-i, H6i-1, H3i-1 , H2i-1}\n+      \/\/\n+      \/\/  v0 = masks for vrgather\/vmerge. Single value during the 16 rounds.\n+      \/\/\n+      \/\/  v14 = temporary, Wt+Kt\n+      \/\/  v15 = temporary, Kt\n+      \/\/\n+      \/\/  v18\/v19 = temporaries, in the epilogue, to re-arrange\n+      \/\/            and byte-swap v16\/v17\n+      \/\/\n+      \/\/  v26\/v27 = hold the initial values of the hash, byte-swapped.\n+      \/\/\n+      \/\/  v30\/v31 = used to generate masks, vrgather indices.\n+      \/\/\n+      \/\/ During most of the function the vector state is configured so that each\n+      \/\/ vector is interpreted as containing four 32\/64 bits (e32\/e64) elements (128\/256 bits).\n@@ -4251,95 +3931,93 @@\n-    \/\/ Overview of the logic in each \"quad round\".\n-    \/\/\n-    \/\/ The code below repeats 20 times the logic implementing four rounds\n-    \/\/ of the SHA-512 core loop as documented by NIST. 20 \"quad rounds\"\n-    \/\/ to implementing the 80 single rounds.\n-    \/\/\n-    \/\/    \/\/ Load four word (u64) constants (K[t+3], K[t+2], K[t+1], K[t+0])\n-    \/\/    \/\/ Output:\n-    \/\/    \/\/   v15 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n-    \/\/    vl1re32.v v15, (a2)\n-    \/\/\n-    \/\/    \/\/ Increment word contant address by stride (32 bytes, 4*8B, 256b)\n-    \/\/    addi consts, consts, 32\n-    \/\/\n-    \/\/    \/\/ Add constants to message schedule words:\n-    \/\/    \/\/  Input\n-    \/\/    \/\/    v15 = {K[t+3], K[t+2], K[t+1], K[t+0]}\n-    \/\/    \/\/    v10 = {W[t+3], W[t+2], W[t+1], W[t+0]}; \/\/ Vt0 = W[3:0];\n-    \/\/    \/\/  Output\n-    \/\/    \/\/    v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n-    \/\/    vadd.vv v14, v15, v10\n-    \/\/\n-    \/\/    \/\/  2 rounds of working variables updates.\n-    \/\/    \/\/     v17[t+4] <- v17[t], v16[t], v14[t]\n-    \/\/    \/\/  Input:\n-    \/\/    \/\/    v17 = {c[t],d[t],g[t],h[t]}   \" = v17[t] \"\n-    \/\/    \/\/    v16 = {a[t],b[t],e[t],f[t]}\n-    \/\/    \/\/    v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n-    \/\/    \/\/  Output:\n-    \/\/    \/\/    v17 = {f[t+2],e[t+2],b[t+2],a[t+2]}  \" = v16[t+2] \"\n-    \/\/    \/\/        = {h[t+4],g[t+4],d[t+4],c[t+4]}  \" = v17[t+4] \"\n-    \/\/    vsha2cl.vv v17, v16, v14\n-    \/\/\n-    \/\/    \/\/  2 rounds of working variables updates.\n-    \/\/    \/\/     v16[t+4] <- v16[t], v16[t+2], v14[t]\n-    \/\/    \/\/  Input\n-    \/\/    \/\/   v16 = {a[t],b[t],e[t],f[t]}       \" = v16[t] \"\n-    \/\/    \/\/       = {h[t+2],g[t+2],d[t+2],c[t+2]}   \" = v17[t+2] \"\n-    \/\/    \/\/   v17 = {f[t+2],e[t+2],b[t+2],a[t+2]}   \" = v16[t+2] \"\n-    \/\/    \/\/   v14 = {W[t+3]+K[t+3], W[t+2]+K[t+2], W[t+1]+K[t+1], W[t+0]+K[t+0]}\n-    \/\/    \/\/  Output:\n-    \/\/    \/\/   v16 = {f[t+4],e[t+4],b[t+4],a[t+4]}   \" = v16[t+4] \"\n-    \/\/    vsha2ch.vv v16, v17, v14\n-    \/\/\n-    \/\/    \/\/ Combine 2QW into 1QW\n-    \/\/    \/\/\n-    \/\/    \/\/ To generate the next 4 words, \"new_v10\"\/\"v14\" from v10-v13, vsha2ms needs\n-    \/\/    \/\/     v10[0..3], v11[0], v12[1..3], v13[0, 2..3]\n-    \/\/    \/\/ and it can only take 3 vectors as inputs. Hence we need to combine\n-    \/\/    \/\/ v11[0] and v12[1..3] in a single vector.\n-    \/\/    \/\/\n-    \/\/    \/\/ vmerge Vt4, Vt1, Vt2, V0\n-    \/\/    \/\/ Input\n-    \/\/    \/\/  V0 = mask \/\/ first word from v12, 1..3 words from v11\n-    \/\/    \/\/  V12 = {Wt-8, Wt-7, Wt-6, Wt-5}\n-    \/\/    \/\/  V11 = {Wt-12, Wt-11, Wt-10, Wt-9}\n-    \/\/    \/\/ Output\n-    \/\/    \/\/  Vt4 = {Wt-12, Wt-7, Wt-6, Wt-5}\n-    \/\/    vmerge.vvm v14, v12, v11, v0\n-    \/\/\n-    \/\/    \/\/ Generate next Four Message Schedule Words (hence allowing for 4 more rounds)\n-    \/\/    \/\/ Input\n-    \/\/    \/\/  V10 = {W[t+ 3], W[t+ 2], W[t+ 1], W[t+ 0]}     W[ 3: 0]\n-    \/\/    \/\/  V13 = {W[t+15], W[t+14], W[t+13], W[t+12]}     W[15:12]\n-    \/\/    \/\/  V14 = {W[t+11], W[t+10], W[t+ 9], W[t+ 4]}     W[11: 9,4]\n-    \/\/    \/\/ Output (next four message schedule words)\n-    \/\/    \/\/  v10 = {W[t+19],  W[t+18],  W[t+17],  W[t+16]}  W[19:16]\n-    \/\/    vsha2ms.vv v10, v14, v13\n-    \/\/\n-    \/\/ BEFORE\n-    \/\/  v10 - v13 hold the message schedule words (initially the block words)\n-    \/\/    v10 = W[ 3: 0]   \"oldest\"\n-    \/\/    v11 = W[ 7: 4]\n-    \/\/    v12 = W[11: 8]\n-    \/\/    v13 = W[15:12]   \"newest\"\n-    \/\/\n-    \/\/  vt6 - vt7 hold the working state variables\n-    \/\/    v16 = {a[t],b[t],e[t],f[t]}   \/\/ initially {H5,H4,H1,H0}\n-    \/\/    v17 = {c[t],d[t],g[t],h[t]}   \/\/ initially {H7,H6,H3,H2}\n-    \/\/\n-    \/\/ AFTER\n-    \/\/  v10 - v13 hold the message schedule words (initially the block words)\n-    \/\/    v11 = W[ 7: 4]   \"oldest\"\n-    \/\/    v12 = W[11: 8]\n-    \/\/    v13 = W[15:12]\n-    \/\/    v10 = W[19:16]   \"newest\"\n-    \/\/\n-    \/\/  v16 and v17 hold the working state variables\n-    \/\/    v16 = {a[t+4],b[t+4],e[t+4],f[t+4]}\n-    \/\/    v17 = {c[t+4],d[t+4],g[t+4],h[t+4]}\n-    \/\/\n-    \/\/  The group of vectors v10,v11,v12,v13 is \"rotated\" by one in each quad-round,\n-    \/\/  hence the uses of those vectors rotate in each round, and we get back to the\n-    \/\/  initial configuration every 4 quad-rounds. We could avoid those changes at\n-    \/\/  the cost of moving those vectors at the end of each quad-rounds.\n+      \/\/ Set vectors as 4 * 32\/64 bits\n+      \/\/\n+      \/\/ e32\/e64: vector of 32b\/64b\/4B\/8B elements\n+      \/\/ m1: LMUL=1\n+      \/\/ ta: tail agnostic (don't care about those lanes)\n+      \/\/ ma: mask agnostic (don't care about those lanes)\n+      \/\/ x0 is not written, we known the number of vector elements.\n+      __ vsetivli(x0, 4, vset_sew, Assembler::m1, Assembler::ma, Assembler::ta);\n+\n+      \/\/ Load H[0..8] to produce\n+      \/\/  v16 = {a,b,e,f}\n+      \/\/  v17 = {c,d,g,h}\n+      __ vleXX_v<vset_sew>(v16, state);                \/\/ v16 = {d,c,b,a}\n+      __ addi(state, state, const_add);\n+      __ vleXX_v<vset_sew>(v17, state);                \/\/ v17 = {h,g,f,e}\n+\n+      __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n+      __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n+      __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {a,b,c,d}\n+      __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {e,f,g,h}\n+      __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n+      \/\/ Copy elements [3..2] of v26 ({d,c}) into elements [3..2] of v17.\n+      __ vslideup_vi(v17, v26, 2);                     \/\/ v17 = {c,d,_,_}\n+      \/\/ Merge elements [1..0] of v27 ({g,h}) into elements [1..0] of v17\n+      __ vmerge_vvm(v17, v17, v27);                    \/\/ v17 = {c,d,g,h}\n+      \/\/ Copy elements [1..0] of v27 ({f,e}) into elements [1..0] of v16.\n+      __ vslidedown_vi(v16, v27, 2);                   \/\/ v16 = {_,_,e,f}\n+      \/\/ Merge elements [3..2] of v26 ({a,b}) into elements [3..2] of v16\n+      __ vmerge_vvm(v16, v26, v16);                    \/\/ v16 = {a,b,e,f}\n+\n+      __ bind(multi_block_loop);\n+\n+      \/\/ Capture the initial H values in v26 and v27 to allow for computing\n+      \/\/ the resulting H', since H' = H+{a',b',c',...,h'}.\n+      __ vmv_v_v(v26, v16);\n+      __ vmv_v_v(v27, v17);\n+\n+      \/\/ Load the 512\/1024-bits of the message block in v10-v13 and perform\n+      \/\/ an endian swap on each 4\/8 bytes element.\n+      \/\/\n+      \/\/ If Zvkb is not implemented one can use vrgather\n+      \/\/ with an index sequence to byte-swap.\n+      \/\/  sequence = [3 2 1 0   7 6 5 4  11 10 9 8   15 14 13 12]\n+      \/\/   <https:\/\/oeis.org\/A004444> gives us \"N ^ 3\" as a nice formula to generate\n+      \/\/  this sequence. 'vid' gives us the N.\n+      __ vleXX_v<vset_sew>(v10, buf);\n+      __ vrev8_v(v10, v10);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v<vset_sew>(v11, buf);\n+      __ vrev8_v(v11, v11);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v<vset_sew>(v12, buf);\n+      __ vrev8_v(v12, v12);\n+      __ addi(buf, buf, const_add);\n+      __ vleXX_v<vset_sew>(v13, buf);\n+      __ vrev8_v(v13, v13);\n+      __ addi(buf, buf, const_add);\n+\n+      \/\/ Set v0 up for the vmerge that replaces the first word (idx==0)\n+      __ vid_v(v0);\n+      __ vmseq_vi(v0, v0, 0x0);  \/\/ v0.mask[i] = (i == 0 ? 1 : 0)\n+\n+      address constant_table = vset_sew == Assembler::e32 ? (address)round_consts_256 : (address)round_consts_512;\n+      la(consts, ExternalAddress(constant_table));\n+\n+      VectorRegister rotation_regs[] = {v10, v11, v12, v13};\n+      int rot_pos = 0;\n+      \/\/ Quad-round #0 (+0, v10->v11->v12->v13) ... #11 (+3, v13->v10->v11->v12)\n+      constexpr int qr_end = vset_sew == Assembler::e32 ? 12 : 16;\n+      for (int i = 0; i < qr_end; i++) {\n+        sha2_quad_round<vset_sew>\n+                  (rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   v15, v14, v16, v17);\n+        ++rot_pos;\n+      }\n+      \/\/ Quad-round #12 (+0, v10->v11->v12->v13) ... #15 (+3, v13->v10->v11->v12)\n+      \/\/ Note that we stop generating new message schedule words (Wt, v10-13)\n+      \/\/ as we already generated all the words we end up consuming (i.e., W[63:60]).\n+      constexpr int qr_c_end = qr_end + 4;\n+      for (int i = qr_end; i < qr_c_end; i++) {\n+        sha2_quad_round<vset_sew>\n+                  (rotation_regs[(rot_pos + 0) & 0x3],\n+                   rotation_regs[(rot_pos + 1) & 0x3],\n+                   rotation_regs[(rot_pos + 2) & 0x3],\n+                   rotation_regs[(rot_pos + 3) & 0x3],\n+                   consts,\n+                   v15, v14, v16, v17, false, i < (qr_c_end-1));\n+        ++rot_pos;\n+      }\n@@ -4347,191 +4025,5 @@\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 0 (+0, v10->v11->v12->v13)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 1 (+1, v11->v12->v13->v10)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 2 (+2, v12->v13->v10->v11)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 3 (+3, v13->v10->v11->v12)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12);\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 4 (+0, v10->v11->v12->v13)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 5 (+1, v11->v12->v13->v10)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 6 (+2, v12->v13->v10->v11)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 7 (+3, v13->v10->v11->v12)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12);\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 8 (+0, v10->v11->v12->v13)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 9 (+1, v11->v12->v13->v10)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 10 (+2, v12->v13->v10->v11)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 11 (+3, v13->v10->v11->v12)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12);\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 12 (+0, v10->v11->v12->v13)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    __ vsha2ms_vv(v10, v14, v13);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 13 (+1, v11->v12->v13->v10)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    __ vsha2ms_vv(v11, v14, v10);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 14 (+2, v12->v13->v10->v11)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    __ vsha2ms_vv(v12, v14, v11);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 15 (+3, v13->v10->v11->v12)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v11, v10);\n-    __ vsha2ms_vv(v13, v14, v12);\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 16 (+0, v10->v11->v12->v13)\n-    \/\/ Note that we stop generating new message schedule words (Wt, v10-13)\n-    \/\/ as we already generated all the words we end up consuming (i.e., W[79:76]).\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v10);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v12, v11);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 17 (+1, v11->v12->v13->v10)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v11);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v13, v12);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 18 (+2, v12->v13->v10->v11)\n-    __ vl1re64_v(v15, consts);\n-    __ addi(consts, consts, 32);\n-    __ vadd_vv(v14, v15, v12);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-    __ vmerge_vvm(v14, v10, v13);\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Quad-round 19 (+3, v13->v10->v11->v12)\n-    __ vl1re64_v(v15, consts);\n-    \/\/ No consts increment needed.\n-    __ vadd_vv(v14, v15, v13);\n-    __ vsha2cl_vv(v17, v16, v14);\n-    __ vsha2ch_vv(v16, v17, v14);\n-\n-    \/\/--------------------------------------------------------------------------------\n-    \/\/ Compute the updated hash value H'\n-    \/\/   H' = H + {h',g',...,b',a'}\n-    \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n-    \/\/      = {h+h',g+g',...,b+b',a+a'}\n-\n-    \/\/ H' = H+{a',b',c',...,h'}\n-    __ vadd_vv(v16, v26, v16);\n-    __ vadd_vv(v17, v27, v17);\n+      \/\/--------------------------------------------------------------------------------\n+      \/\/ Compute the updated hash value H'\n+      \/\/   H' = H + {h',g',...,b',a'}\n+      \/\/      = {h,g,...,b,a} + {h',g',...,b',a'}\n+      \/\/      = {h+h',g+g',...,b+b',a+a'}\n@@ -4539,5 +4031,3 @@\n-    if (multi_block) {\n-      __ add(ofs, ofs, 128);\n-      __ ble(ofs, limit, multi_block_loop);\n-      __ mv(c_rarg0, ofs); \/\/ return ofs\n-    }\n+      \/\/ H' = H+{a',b',c',...,h'}\n+      __ vadd_vv(v16, v26, v16);\n+      __ vadd_vv(v17, v27, v17);\n@@ -4545,21 +4035,5 @@\n-    \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n-    \/\/  v16 = {f,e,b,a}\n-    \/\/  v17 = {h,g,d,c}\n-    __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n-    __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n-    __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {f,e,b,a}\n-    __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {h,g,d,c}\n-    __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n-    \/\/ Copy elements [3..2] of v26 ({f,e}) into elements [1..0] of v17.\n-    __ vslidedown_vi(v17, v26, 2);                   \/\/ v17 = {_,_,f,e}\n-    \/\/ Merge elements [3..2] of v27 ({g,h}) into elements [3..2] of v17\n-    __ vmerge_vvm(v17, v27, v17);                    \/\/ v17 = {h,g,f,e}\n-    \/\/ Copy elements [1..0] of v27 ({c,d}) into elements [3..2] of v16.\n-    __ vslideup_vi(v16, v27, 2);                     \/\/ v16 = {d,c,_,_}\n-    \/\/ Merge elements [1..0] of v26 ({a,b}) into elements [1..0] of v16\n-    __ vmerge_vvm(v16, v16, v26);                    \/\/ v16 = {d,c,b,a}\n-\n-    \/\/ Save the  hash\n-    __ vse64_v(v17, state);\n-    __ addi(state, state, -32);\n-    __ vse64_v(v16, state);\n+      if (multi_block) {\n+        __ add(ofs, ofs, vset_sew == Assembler::e32 ? 64 : 128);\n+        __ ble(ofs, limit, multi_block_loop);\n+        __ mv(c_rarg0, ofs); \/\/ return ofs\n+      }\n@@ -4567,2 +4041,25 @@\n-    __ leave();\n-    __ ret();\n+      \/\/ Store H[0..8] = {a,b,c,d,e,f,g,h} from\n+      \/\/  v16 = {f,e,b,a}\n+      \/\/  v17 = {h,g,d,c}\n+      __ vid_v(v30);                                   \/\/ v30 = {3,2,1,0}\n+      __ vxor_vi(v30, v30, 0x3);                       \/\/ v30 = {0,1,2,3}\n+      __ vrgather_vv(v26, v16, v30);                   \/\/ v26 = {f,e,b,a}\n+      __ vrgather_vv(v27, v17, v30);                   \/\/ v27 = {h,g,d,c}\n+      __ vmsgeu_vi(v0, v30, 2);                        \/\/ v0  = {f,f,t,t}\n+      \/\/ Copy elements [3..2] of v26 ({f,e}) into elements [1..0] of v17.\n+      __ vslidedown_vi(v17, v26, 2);                   \/\/ v17 = {_,_,f,e}\n+      \/\/ Merge elements [3..2] of v27 ({g,h}) into elements [3..2] of v17\n+      __ vmerge_vvm(v17, v27, v17);                    \/\/ v17 = {h,g,f,e}\n+      \/\/ Copy elements [1..0] of v27 ({c,d}) into elements [3..2] of v16.\n+      __ vslideup_vi(v16, v27, 2);                     \/\/ v16 = {d,c,_,_}\n+      \/\/ Merge elements [1..0] of v26 ({a,b}) into elements [1..0] of v16\n+      __ vmerge_vvm(v16, v16, v26);                    \/\/ v16 = {d,c,b,a}\n+\n+      \/\/ Save the hash\n+      __ vseXX_v<vset_sew>(v17, state);\n+      __ addi(state, state, -const_add);\n+      __ vseXX_v<vset_sew>(v16, state);\n+\n+      __ pop_reg(saved_regs, sp);\n+      __ leave();\n+      __ ret();\n@@ -4570,2 +4067,5 @@\n-    return start;\n-  }\n+      return start;\n+    }\n+  };\n+#undef __\n+#define __ masm->\n@@ -5771,2 +5271,3 @@\n-      StubRoutines::_sha256_implCompress   = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true,  \"sha256_implCompressMB\");\n+      Sha2Generator sha2(_masm, this);\n+      StubRoutines::_sha256_implCompress   = sha2.generate_sha256_implCompress(false);\n+      StubRoutines::_sha256_implCompressMB = sha2.generate_sha256_implCompress(true);\n@@ -5774,0 +5275,1 @@\n+\n@@ -5775,2 +5277,3 @@\n-      StubRoutines::_sha512_implCompress   = generate_sha512_implCompress(false, \"sha512_implCompress\");\n-      StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true,  \"sha512_implCompressMB\");\n+      Sha2Generator sha2(_masm, this);\n+      StubRoutines::_sha512_implCompress   = sha2.generate_sha512_implCompress(false);\n+      StubRoutines::_sha512_implCompressMB = sha2.generate_sha512_implCompress(true);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":312,"deletions":809,"binary":false,"changes":1121,"status":"modified"},{"patch":"@@ -159,3 +159,18 @@\n-  if (UseZvknha && UseZvkb) {\n-    if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n-      FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+  if (UseZvkn) {\n+    if (!ext_V.enabled()) {\n+      FLAG_SET_DEFAULT(UseZvkn, false);\n+      warning(\"Cannot enable Zvkn on cpu without RVV support.\");\n+    }\n+    if (FLAG_IS_DEFAULT(UseRVV)) {\n+      FLAG_SET_DEFAULT(UseRVV, true);\n+    }\n+    if (UseRVV) {\n+      if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+        FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+      }\n+      if (FLAG_IS_DEFAULT(UseSHA256Intrinsics)) {\n+        FLAG_SET_DEFAULT(UseSHA256Intrinsics, true);\n+      }\n+    } else {\n+      FLAG_SET_DEFAULT(UseZvkn, false);\n+      warning(\"Cannot enabled Zvkn when RVV is disabled.\");\n@@ -163,3 +178,0 @@\n-  } else if (UseSHA256Intrinsics) {\n-    warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n@@ -167,4 +179,8 @@\n-\n-  if (UseZvknhb && UseZvkb) {\n-    if (FLAG_IS_DEFAULT(UseSHA512Intrinsics)) {\n-      FLAG_SET_DEFAULT(UseSHA512Intrinsics, true);\n+  if (!UseZvkn) {\n+    if (UseSHA256Intrinsics) {\n+      warning(\"Intrinsics for SHA-224 and SHA-256 crypto hash functions not available on this CPU, UseZvkn needed.\");\n+      FLAG_SET_DEFAULT(UseSHA256Intrinsics, false);\n+    }\n+    if (UseSHA512Intrinsics) {\n+      warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU, UseZvkn needed.\");\n+      FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n@@ -172,3 +188,0 @@\n-  } else if (UseSHA512Intrinsics) {\n-    warning(\"Intrinsics for SHA-384 and SHA-512 crypto hash functions not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseSHA512Intrinsics, false);\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":26,"deletions":13,"binary":false,"changes":39,"status":"modified"}]}