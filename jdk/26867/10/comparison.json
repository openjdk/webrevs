{"files":[{"patch":"@@ -472,1 +472,1 @@\n-          constraint(SoftMaxHeapSizeConstraintFunc,AfterMemoryInit)         \\\n+          constraint(SoftMaxHeapSizeConstraintFunc,AfterErgo)               \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -91,1 +91,1 @@\n-                     \"%s, Max Old Evacuation: %zu%s, Actual Free: %zu%s.\",\n+                     \"%s, Max Old Evacuation: %zu%s, Max Either Evacuation: %zu%s, Actual Free: %zu%s.\",\n@@ -94,0 +94,1 @@\n+                     byte_size_in_proper_unit(unaffiliated_young_memory), proper_unit_for_byte_size(unaffiliated_young_memory),\n@@ -136,1 +137,0 @@\n-\n@@ -138,1 +138,1 @@\n-    heap->generation_sizer()->force_transfer_to_old(regions_transferred_to_old);\n+    assert(young_evac_reserve > regions_transferred_to_old * region_size_bytes, \"young reserve cannot be negative\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -605,1 +605,2 @@\n-  size_t old_used = _old_generation->used() + _old_generation->get_humongous_waste();\n+  \/\/ used() includes humongous waste\n+  size_t old_used = _old_generation->used();\n@@ -609,2 +610,1 @@\n-         \"Old used (%zu, %zu) must not be more than heap capacity (%zu)\",\n-         _old_generation->used(), _old_generation->get_humongous_waste(), _heap->capacity());\n+         \"Old used (%zu) must not be more than heap capacity (%zu)\", _old_generation->used(), _heap->capacity());\n@@ -682,1 +682,2 @@\n-    const size_t current_usage = _old_generation->used() + _old_generation->get_humongous_waste();\n+    \/\/ _old_generation->used() includes humongous waste.\n+    const size_t current_usage = _old_generation->used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1153,0 +1153,3 @@\n+  bool is_generational = heap->mode()->is_generational();\n+  ShenandoahGenerationalHeap* const gen_heap = is_generational? ShenandoahGenerationalHeap::heap(): nullptr;\n+\n@@ -1176,1 +1179,1 @@\n-  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+  if (is_generational && heap->is_concurrent_old_mark_in_progress()) {\n@@ -1196,1 +1199,1 @@\n-    ShenandoahGenerationalHeap::heap()->set_aging_cycle(false);\n+    gen_heap->set_aging_cycle(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -54,2 +54,2 @@\n-  idx_t _idx;\n-  idx_t _end;\n+  index_type _idx;\n+  index_type _end;\n@@ -59,1 +59,2 @@\n-  explicit ShenandoahLeftRightIterator(ShenandoahRegionPartitions* partitions, ShenandoahFreeSetPartitionId partition, bool use_empty = false)\n+  explicit ShenandoahLeftRightIterator(ShenandoahRegionPartitions* partitions,\n+                                       ShenandoahFreeSetPartitionId partition, bool use_empty = false)\n@@ -73,1 +74,1 @@\n-  idx_t current() const {\n+  index_type current() const {\n@@ -77,1 +78,1 @@\n-  idx_t next() {\n+  index_type next() {\n@@ -85,2 +86,2 @@\n-  idx_t _idx;\n-  idx_t _end;\n+  index_type _idx;\n+  index_type _end;\n@@ -90,1 +91,2 @@\n-  explicit ShenandoahRightLeftIterator(ShenandoahRegionPartitions* partitions, ShenandoahFreeSetPartitionId partition, bool use_empty = false)\n+  explicit ShenandoahRightLeftIterator(ShenandoahRegionPartitions* partitions,\n+                                       ShenandoahFreeSetPartitionId partition, bool use_empty = false)\n@@ -104,1 +106,1 @@\n-  idx_t current() const {\n+  index_type current() const {\n@@ -108,1 +110,1 @@\n-  idx_t next() {\n+  index_type next() {\n@@ -138,6 +140,6 @@\n-void ShenandoahRegionPartitions::dump_bitmap_range(idx_t start_region_idx, idx_t end_region_idx) const {\n-  assert((start_region_idx >= 0) && (start_region_idx < (idx_t) _max), \"precondition\");\n-  assert((end_region_idx >= 0) && (end_region_idx < (idx_t) _max), \"precondition\");\n-  idx_t aligned_start = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(start_region_idx);\n-  idx_t aligned_end = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(end_region_idx);\n-  idx_t alignment = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].alignment();\n+void ShenandoahRegionPartitions::dump_bitmap_range(index_type start_region_idx, index_type end_region_idx) const {\n+  assert((start_region_idx >= 0) && (start_region_idx < (index_type) _max), \"precondition\");\n+  assert((end_region_idx >= 0) && (end_region_idx < (index_type) _max), \"precondition\");\n+  index_type aligned_start = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(start_region_idx);\n+  index_type aligned_end = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(end_region_idx);\n+  index_type alignment = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].alignment();\n@@ -150,3 +152,3 @@\n-void ShenandoahRegionPartitions::dump_bitmap_row(idx_t region_idx) const {\n-  assert((region_idx >= 0) && (region_idx < (idx_t) _max), \"precondition\");\n-  idx_t aligned_idx = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(region_idx);\n+void ShenandoahRegionPartitions::dump_bitmap_row(index_type region_idx) const {\n+  assert((region_idx >= 0) && (region_idx < (index_type) _max), \"precondition\");\n+  index_type aligned_idx = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(region_idx);\n@@ -169,0 +171,1 @@\n+  initialize_old_collector();\n@@ -172,0 +175,51 @@\n+void ShenandoahFreeSet::account_for_pip_regions(size_t mutator_regions, size_t mutator_bytes,\n+                                                size_t collector_regions, size_t collector_bytes) {\n+  shenandoah_assert_heaplocked();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ We have removed all of these regions from their respective partition. Each pip region is \"in\" the NotFree partition.\n+  \/\/ We want to account for all pip pad memory as if it had been consumed from within the Mutator partition.\n+  \/\/\n+  \/\/ After we finish promote in place, the pad memory will be deallocated and made available within the OldCollector\n+  \/\/ region.  At that time, we will transfer the used memory from the Mutator partition to the OldCollector parttion,\n+  \/\/ and then we will unallocate the pad memory.\n+\n+\n+  _partitions.decrease_region_counts(ShenandoahFreeSetPartitionId::Mutator, mutator_regions);\n+  _partitions.decrease_region_counts(ShenandoahFreeSetPartitionId::Collector, collector_regions);\n+\n+  \/\/ Increase used by remnant fill objects placed in both Mutator and Collector partitions\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, mutator_bytes);\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, collector_bytes);\n+\n+  \/\/ Now transfer all of the memory contained within Collector pip regions from the Collector to the Mutator.\n+  \/\/ Each of these regions is treated as fully used, even though some of the region's memory may be artifically used,\n+  \/\/ to be recycled and put into allocatable OldCollector partition after the region has been promoted in place.\n+  _partitions.transfer_used_capacity_from_to(ShenandoahFreeSetPartitionId::Collector, ShenandoahFreeSetPartitionId::Mutator,\n+                                             collector_regions);\n+\n+  \/\/ Conservatively, act as if we've promoted from both Mutator and Collector partitions\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ false, \/* CollectorEmptiesChanged *\/ false,\n+                             \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  recompute_total_young_used<\/* UsedByMutatorChanged *\/ true, \/*UsedByCollectorChanged *\/ true>();\n+  recompute_total_global_used<\/* UsedByMutatorChanged *\/ true, \/* UsedByCollectorChanged *\/ true,\n+                              \/* UsedByOldCollectorChanged *\/ false>();\n+}\n+\n+ShenandoahFreeSetPartitionId ShenandoahFreeSet::prepare_to_promote_in_place(size_t idx, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  size_t min_remnant_size = PLAB::min_size() * HeapWordSize;\n+  ShenandoahFreeSetPartitionId p =  _partitions.membership(idx);\n+  if (bytes >= min_remnant_size) {\n+    assert((p == ShenandoahFreeSetPartitionId::Mutator) || (p == ShenandoahFreeSetPartitionId::Collector),\n+           \"PIP region must be associated with young\");\n+    _partitions.raw_clear_membership(idx, p);\n+  } else {\n+    assert(p == ShenandoahFreeSetPartitionId::NotFree, \"We did not fill this region and do not need to adjust used\");\n+  }\n+  return p;\n+}\n+\n@@ -199,1 +253,49 @@\n-inline idx_t ShenandoahRegionPartitions::leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n+\/\/ This is used for unit testing.  Do not use in production code.\n+void ShenandoahFreeSet::resize_old_collector_capacity(size_t regions) {\n+  shenandoah_assert_heaplocked();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t original_old_regions = _partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector) \/ region_size_bytes;\n+  size_t unaffiliated_mutator_regions = _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator);\n+  size_t unaffiliated_collector_regions = _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector);\n+  size_t unaffiliated_old_collector_regions = _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector);\n+  if (regions > original_old_regions) {\n+    size_t regions_to_transfer = regions - original_old_regions;\n+    if (regions_to_transfer <= unaffiliated_mutator_regions + unaffiliated_collector_regions) {\n+      size_t regions_from_mutator =\n+        (regions_to_transfer > unaffiliated_mutator_regions)? unaffiliated_mutator_regions: regions_to_transfer;\n+      regions_to_transfer -= regions_from_mutator;\n+      size_t regions_from_collector = regions_to_transfer;\n+      if (regions_from_mutator > 0) {\n+        transfer_empty_regions_from_to(ShenandoahFreeSetPartitionId::Mutator, ShenandoahFreeSetPartitionId::OldCollector,\n+                                       regions_from_mutator);\n+      }\n+      if (regions_from_collector > 0) {\n+        transfer_empty_regions_from_to(ShenandoahFreeSetPartitionId::Collector, ShenandoahFreeSetPartitionId::OldCollector,\n+                                       regions_from_mutator);\n+      }\n+    } else {\n+      fatal(\"Could not resize old for unit test\");\n+    }\n+  } else if (regions < original_old_regions) {\n+    size_t regions_to_transfer = original_old_regions - regions;\n+    if (regions_to_transfer <= unaffiliated_old_collector_regions) {\n+      transfer_empty_regions_from_to(ShenandoahFreeSetPartitionId::OldCollector, ShenandoahFreeSetPartitionId::Mutator,\n+                                     regions_to_transfer);\n+    } else {\n+      fatal(\"Could not resize old for unit test\");\n+    }\n+  }\n+  \/\/ else, old generation is already appropriately sized\n+}\n+\n+void ShenandoahFreeSet::reset_bytes_allocated_since_gc_start(size_t initial_bytes_allocated) {\n+  shenandoah_assert_heaplocked();\n+  _mutator_bytes_allocated_since_gc_start = initial_bytes_allocated;\n+}\n+\n+void ShenandoahFreeSet::increase_bytes_allocated(size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  _mutator_bytes_allocated_since_gc_start += bytes;\n+}\n+\n+inline index_type ShenandoahRegionPartitions::leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n@@ -201,1 +303,1 @@\n-  idx_t idx = _leftmosts[int(which_partition)];\n+  index_type idx = _leftmosts[int(which_partition)];\n@@ -212,1 +314,1 @@\n-inline idx_t ShenandoahRegionPartitions::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n+inline index_type ShenandoahRegionPartitions::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n@@ -214,1 +316,1 @@\n-  idx_t idx = _rightmosts[int(which_partition)];\n+  index_type idx = _rightmosts[int(which_partition)];\n@@ -221,0 +323,6 @@\n+void ShenandoahRegionPartitions::initialize_old_collector() {\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = 0;\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = 0;\n+  _empty_region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = 0;\n+}\n+\n@@ -230,0 +338,2 @@\n+    _region_counts[partition_id] = 0;\n+    _empty_region_counts[partition_id] = 0;\n@@ -231,0 +341,1 @@\n+    _humongous_waste[partition_id] = 0;\n@@ -233,1 +344,0 @@\n-  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n@@ -236,3 +346,5 @@\n-void ShenandoahRegionPartitions::establish_mutator_intervals(idx_t mutator_leftmost, idx_t mutator_rightmost,\n-                                                             idx_t mutator_leftmost_empty, idx_t mutator_rightmost_empty,\n-                                                             size_t mutator_region_count, size_t mutator_used) {\n+void ShenandoahRegionPartitions::establish_mutator_intervals(index_type mutator_leftmost, index_type mutator_rightmost,\n+                                                             index_type mutator_leftmost_empty, index_type mutator_rightmost_empty,\n+                                                             size_t total_mutator_regions, size_t empty_mutator_regions,\n+                                                             size_t mutator_region_count, size_t mutator_used,\n+                                                             size_t mutator_humongous_waste_bytes) {\n@@ -248,1 +360,2 @@\n-  _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count * _region_size_bytes;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] = total_mutator_regions * _region_size_bytes;\n+  _humongous_waste[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_humongous_waste_bytes;\n@@ -252,0 +365,2 @@\n+  _empty_region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = empty_mutator_regions;\n+\n@@ -260,0 +375,1 @@\n+  _humongous_waste[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n@@ -261,0 +377,2 @@\n+\n+  _empty_region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n@@ -263,4 +381,8 @@\n-void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n-                                                                   idx_t old_collector_leftmost_empty,\n-                                                                   idx_t old_collector_rightmost_empty,\n-                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(index_type old_collector_leftmost,\n+                                                                   index_type old_collector_rightmost,\n+                                                                   index_type old_collector_leftmost_empty,\n+                                                                   index_type old_collector_rightmost_empty,\n+                                                                   size_t total_old_collector_region_count,\n+                                                                   size_t old_collector_empty, size_t old_collector_regions,\n+                                                                   size_t old_collector_used,\n+                                                                   size_t old_collector_humongous_waste_bytes) {\n@@ -274,1 +396,1 @@\n-  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_regions;\n@@ -276,1 +398,2 @@\n-  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = total_old_collector_region_count * _region_size_bytes;\n+  _humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_humongous_waste_bytes;\n@@ -279,0 +402,2 @@\n+\n+  _empty_region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_empty;\n@@ -292,2 +417,102 @@\n-inline void ShenandoahRegionPartitions::shrink_interval_if_range_modifies_either_boundary(\n-  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+void ShenandoahRegionPartitions::decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  assert (_used[int(which_partition)] >= bytes, \"Must not use less than zero after decrease\");\n+  _used[int(which_partition)] -= bytes;\n+  _available[int(which_partition)] += bytes;\n+}\n+\n+void ShenandoahRegionPartitions::increase_humongous_waste(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _humongous_waste[int(which_partition)] += bytes;\n+}\n+\n+size_t ShenandoahRegionPartitions::get_humongous_waste(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  return _humongous_waste[int(which_partition)];;\n+}\n+\n+void ShenandoahRegionPartitions::set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+  _capacity[int(which_partition)] = value;\n+  _available[int(which_partition)] = value - _used[int(which_partition)];\n+}\n+\n+\n+void ShenandoahRegionPartitions::increase_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _capacity[int(which_partition)] += bytes;\n+  _available[int(which_partition)] += bytes;\n+}\n+\n+void ShenandoahRegionPartitions::transfer_used_capacity_from_to(ShenandoahFreeSetPartitionId from_partition,\n+                                                                ShenandoahFreeSetPartitionId to_partition, size_t regions) {\n+  shenandoah_assert_heaplocked();\n+  size_t bytes = regions * ShenandoahHeapRegion::region_size_bytes();\n+  assert (from_partition < NumPartitions, \"Partition must be valid\");\n+  assert (to_partition < NumPartitions, \"Partition must be valid\");\n+  assert(_capacity[int(from_partition)] >= bytes, \"Cannot remove more capacity bytes than are present\");\n+  assert(_used[int(from_partition)] >= bytes, \"Cannot transfer used bytes that are not used\");\n+\n+  \/\/ available is unaffected by transfer\n+  _capacity[int(from_partition)] -= bytes;\n+  _used[int(from_partition)] -= bytes;\n+  _capacity[int(to_partition)] += bytes;\n+  _used[int(to_partition)] += bytes;\n+}\n+\n+void ShenandoahRegionPartitions::decrease_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  assert(_capacity[int(which_partition)] >= bytes, \"Cannot remove more capacity bytes than are present\");\n+  assert(_available[int(which_partition)] >= bytes, \"Cannot shrink capacity unless capacity is unused\");\n+  _capacity[int(which_partition)] -= bytes;\n+  _available[int(which_partition)] -= bytes;\n+}\n+\n+void ShenandoahRegionPartitions::increase_available(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _available[int(which_partition)] += bytes;\n+}\n+\n+void ShenandoahRegionPartitions::decrease_available(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  assert(_available[int(which_partition)] >= bytes, \"Cannot remove more available bytes than are present\");\n+  _available[int(which_partition)] -= bytes;\n+}\n+\n+size_t ShenandoahRegionPartitions::get_available(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  return _available[int(which_partition)];;\n+}\n+\n+void ShenandoahRegionPartitions::increase_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+  _region_counts[int(which_partition)] += regions;\n+}\n+\n+void ShenandoahRegionPartitions::decrease_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+  assert(_region_counts[int(which_partition)] >= regions, \"Cannot remove more regions than are present\");\n+  _region_counts[int(which_partition)] -= regions;\n+}\n+\n+void ShenandoahRegionPartitions::increase_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+  _empty_region_counts[int(which_partition)] += regions;\n+}\n+\n+void ShenandoahRegionPartitions::decrease_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions) {\n+  assert(_empty_region_counts[int(which_partition)] >= regions, \"Cannot remove more regions than are present\");\n+  _empty_region_counts[int(which_partition)] -= regions;\n+}\n+\n+void ShenandoahRegionPartitions::one_region_is_no_longer_empty(ShenandoahFreeSetPartitionId partition) {\n+  decrease_empty_region_counts(partition, (size_t) 1);\n+}\n+\n+\/\/ All members of partition between low_idx and high_idx inclusive have been removed.\n+void ShenandoahRegionPartitions::shrink_interval_if_range_modifies_either_boundary(\n+  ShenandoahFreeSetPartitionId partition, index_type low_idx, index_type high_idx, size_t num_regions) {\n@@ -295,0 +520,2 @@\n+  size_t span = high_idx + 1 - low_idx;\n+  bool regions_are_contiguous = (span == num_regions);\n@@ -298,1 +525,5 @@\n-      _leftmosts[int(partition)] = _max;\n+      if (regions_are_contiguous) {\n+        _leftmosts[int(partition)] = _max;\n+      } else {\n+        _leftmosts[int(partition)] = find_index_of_next_available_region(partition, low_idx + 1);\n+      }\n@@ -300,1 +531,5 @@\n-      _leftmosts[int(partition)] = find_index_of_next_available_region(partition, high_idx + 1);\n+      if (regions_are_contiguous) {\n+        _leftmosts[int(partition)] = find_index_of_next_available_region(partition, high_idx + 1);\n+      } else {\n+        _leftmosts[int(partition)] = find_index_of_next_available_region(partition, low_idx + 1);\n+      }\n@@ -310,1 +545,5 @@\n-      _rightmosts[int(partition)] = -1;\n+      if (regions_are_contiguous) {\n+        _rightmosts[int(partition)] = -1;\n+      } else {\n+        _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, high_idx - 1);\n+      }\n@@ -312,1 +551,5 @@\n-      _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, low_idx - 1);\n+      if (regions_are_contiguous) {\n+        _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, low_idx - 1);\n+      } else {\n+        _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, high_idx - 1);\n+      }\n@@ -327,2 +570,45 @@\n-inline void ShenandoahRegionPartitions::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, idx_t idx) {\n-  shrink_interval_if_range_modifies_either_boundary(partition, idx, idx);\n+void ShenandoahRegionPartitions::establish_interval(ShenandoahFreeSetPartitionId partition, index_type low_idx,\n+                                                    index_type high_idx, index_type low_empty_idx, index_type high_empty_idx) {\n+#ifdef ASSERT\n+  assert (partition < NumPartitions, \"invalid partition\");\n+  if (low_idx != max()) {\n+    assert((low_idx <= high_idx) && (low_idx >= 0) && (high_idx < _max), \"Range must span legal index values\");\n+    assert (in_free_set(partition, low_idx), \"Must be in partition of established interval\");\n+    assert (in_free_set(partition, high_idx), \"Must be in partition of established interval\");\n+  }\n+  if (low_empty_idx != max()) {\n+    ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(low_empty_idx);\n+    assert (in_free_set(partition, low_empty_idx) && (r->is_trash() || r->free() == _region_size_bytes),\n+            \"Must be empty and in partition of established interval\");\n+    r = ShenandoahHeap::heap()->get_region(high_empty_idx);\n+    assert (in_free_set(partition, high_empty_idx), \"Must be in partition of established interval\");\n+  }\n+#endif\n+\n+  _leftmosts[int(partition)] = low_idx;\n+  _rightmosts[int(partition)] = high_idx;\n+  _leftmosts_empty[int(partition)] = low_empty_idx;\n+  _rightmosts_empty[int(partition)] = high_empty_idx;\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                                             index_type idx) {\n+  shrink_interval_if_range_modifies_either_boundary(partition, idx, idx, 1);\n+}\n+\n+\/\/ Some members of partition between low_idx and high_idx inclusive have been added.\n+void ShenandoahRegionPartitions::\n+expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition, index_type low_idx, index_type high_idx,\n+                                                  index_type low_empty_idx, index_type high_empty_idx) {\n+  if (_leftmosts[int(partition)] > low_idx) {\n+    _leftmosts[int(partition)] = low_idx;\n+  }\n+  if (_rightmosts[int(partition)] < high_idx) {\n+    _rightmosts[int(partition)] = high_idx;\n+  }\n+  if (_leftmosts_empty[int(partition)] > low_empty_idx) {\n+    _leftmosts_empty[int(partition)] = low_empty_idx;\n+  }\n+  if (_rightmosts_empty[int(partition)] < high_empty_idx) {\n+    _rightmosts_empty[int(partition)] = high_empty_idx;\n+  }\n@@ -331,2 +617,2 @@\n-inline void ShenandoahRegionPartitions::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n-                                                                             idx_t idx, size_t region_available) {\n+void ShenandoahRegionPartitions::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                                      index_type idx, size_t region_available) {\n@@ -350,1 +636,1 @@\n-  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+  ShenandoahFreeSetPartitionId partition, index_type low_idx, index_type high_idx) {\n@@ -357,1 +643,3 @@\n-  for (idx_t idx = low_idx; idx <= high_idx; idx++) {\n+  for (index_type idx = low_idx; idx <= high_idx; idx++) {\n+#ifdef ASSERT\n+    ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n@@ -359,0 +647,2 @@\n+    assert(r->is_empty() || r->is_trash(), \"Region must be empty or trash\");\n+#endif\n@@ -361,2 +651,4 @@\n-  _region_counts[int(partition)] -= high_idx + 1 - low_idx;\n-  shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx);\n+  size_t num_regions = high_idx + 1 - low_idx;\n+  decrease_region_counts(partition, num_regions);\n+  decrease_empty_region_counts(partition, num_regions);\n+  shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx, num_regions);\n@@ -365,1 +657,2 @@\n-void ShenandoahRegionPartitions::retire_from_partition(ShenandoahFreeSetPartitionId partition, idx_t idx, size_t used_bytes) {\n+size_t ShenandoahRegionPartitions::retire_from_partition(ShenandoahFreeSetPartitionId partition,\n+                                                         index_type idx, size_t used_bytes) {\n@@ -367,0 +660,1 @@\n+  size_t waste_bytes = 0;\n@@ -374,1 +668,3 @@\n-    increase_used(partition, _region_size_bytes - used_bytes);\n+    size_t fill_padding = _region_size_bytes - used_bytes;\n+    waste_bytes = fill_padding;\n+    increase_used(partition, fill_padding);\n@@ -377,0 +673,1 @@\n+  decrease_region_counts(partition, 1);\n@@ -378,1 +675,10 @@\n-  _region_counts[int(partition)]--;\n+\n+  \/\/ This region is fully used, whether or not top() equals end().  It\n+  \/\/ is retired and no more memory will be allocated from within it.\n+\n+  return waste_bytes;\n+}\n+\n+void ShenandoahRegionPartitions::unretire_to_partition(ShenandoahHeapRegion* r, ShenandoahFreeSetPartitionId which_partition) {\n+  shenandoah_assert_heaplocked();\n+  make_free(r->index(), which_partition, r->free());\n@@ -381,1 +687,4 @@\n-void ShenandoahRegionPartitions::make_free(idx_t idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n+\n+\/\/ The caller is responsible for increasing capacity and available and used in which_partition, and decreasing the\n+\/\/ same quantities for the original partition\n+void ShenandoahRegionPartitions::make_free(index_type idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n@@ -389,3 +698,0 @@\n-  _capacity[int(which_partition)] += _region_size_bytes;\n-  _used[int(which_partition)] += _region_size_bytes - available;\n-  _available[int(which_partition)] += available;\n@@ -393,1 +699,0 @@\n-  _region_counts[int(which_partition)]++;\n@@ -412,3 +717,4 @@\n-\n-void ShenandoahRegionPartitions::move_from_partition_to_partition(idx_t idx, ShenandoahFreeSetPartitionId orig_partition,\n-                                                                  ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+\/\/ Do not adjust capacities, available, or used.  Return used delta.\n+size_t ShenandoahRegionPartitions::\n+move_from_partition_to_partition_with_deferred_accounting(index_type idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                          ShenandoahFreeSetPartitionId new_partition, size_t available) {\n@@ -452,0 +758,2 @@\n+  return used;\n+}\n@@ -453,0 +761,7 @@\n+void ShenandoahRegionPartitions::move_from_partition_to_partition(index_type idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                                  ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+  size_t used = move_from_partition_to_partition_with_deferred_accounting(idx, orig_partition, new_partition, available);\n+\n+  \/\/ We decreased used, which increases available, but then we decrease available by full region size below\n+  decrease_used(orig_partition, used);\n+  _region_counts[int(orig_partition)]--;\n@@ -454,2 +769,1 @@\n-  _used[int(orig_partition)] -= used;\n-  _available[int(orig_partition)] -= available;\n+  _available[int(orig_partition)] -= _region_size_bytes;\n@@ -458,3 +772,5 @@\n-  _capacity[int(new_partition)] += _region_size_bytes;;\n-  _used[int(new_partition)] += used;\n-  _available[int(new_partition)] += available;\n+  _capacity[int(new_partition)] += _region_size_bytes;\n+  _available[int(new_partition)] += _region_size_bytes;\n+  _region_counts[int(new_partition)]++;\n+  \/\/ We increased availableby full region size above, but decrease it by used within this region now.\n+  increase_used(new_partition, used);\n@@ -463,2 +779,4 @@\n-  _region_counts[int(orig_partition)]--;\n-  _region_counts[int(new_partition)]++;\n+  if (available == _region_size_bytes) {\n+    _empty_region_counts[int(orig_partition)]--;\n+    _empty_region_counts[int(new_partition)]++;\n+  }\n@@ -467,1 +785,1 @@\n-const char* ShenandoahRegionPartitions::partition_membership_name(idx_t idx) const {\n+const char* ShenandoahRegionPartitions::partition_membership_name(index_type idx) const {\n@@ -471,12 +789,0 @@\n-inline ShenandoahFreeSetPartitionId ShenandoahRegionPartitions::membership(idx_t idx) const {\n-  assert (idx < _max, \"index is sane: %zu < %zu\", idx, _max);\n-  ShenandoahFreeSetPartitionId result = ShenandoahFreeSetPartitionId::NotFree;\n-  for (uint partition_id = 0; partition_id < UIntNumPartitions; partition_id++) {\n-    if (_membership[partition_id].is_set(idx)) {\n-      assert(result == ShenandoahFreeSetPartitionId::NotFree, \"Region should reside in only one partition\");\n-      result = (ShenandoahFreeSetPartitionId) partition_id;\n-    }\n-  }\n-  return result;\n-}\n-\n@@ -484,1 +790,1 @@\n-inline bool ShenandoahRegionPartitions::partition_id_matches(idx_t idx, ShenandoahFreeSetPartitionId test_partition) const {\n+inline bool ShenandoahRegionPartitions::partition_id_matches(index_type idx, ShenandoahFreeSetPartitionId test_partition) const {\n@@ -497,4 +803,4 @@\n-inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_region(\n-  ShenandoahFreeSetPartitionId which_partition, idx_t start_index) const {\n-  idx_t rightmost_idx = rightmost(which_partition);\n-  idx_t leftmost_idx = leftmost(which_partition);\n+inline index_type ShenandoahRegionPartitions::find_index_of_next_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, index_type start_index) const {\n+  index_type rightmost_idx = rightmost(which_partition);\n+  index_type leftmost_idx = leftmost(which_partition);\n@@ -505,1 +811,1 @@\n-  idx_t result = _membership[int(which_partition)].find_first_set_bit(start_index, rightmost_idx + 1);\n+  index_type result = _membership[int(which_partition)].find_first_set_bit(start_index, rightmost_idx + 1);\n@@ -513,4 +819,4 @@\n-inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_region(\n-  ShenandoahFreeSetPartitionId which_partition, idx_t last_index) const {\n-  idx_t rightmost_idx = rightmost(which_partition);\n-  idx_t leftmost_idx = leftmost(which_partition);\n+inline index_type ShenandoahRegionPartitions::find_index_of_previous_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, index_type last_index) const {\n+  index_type rightmost_idx = rightmost(which_partition);\n+  index_type leftmost_idx = leftmost(which_partition);\n@@ -522,1 +828,1 @@\n-  idx_t result = _membership[int(which_partition)].find_last_set_bit(-1, last_index);\n+  index_type result = _membership[int(which_partition)].find_last_set_bit(-1, last_index);\n@@ -530,4 +836,4 @@\n-inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_cluster_of_regions(\n-  ShenandoahFreeSetPartitionId which_partition, idx_t start_index, size_t cluster_size) const {\n-  idx_t rightmost_idx = rightmost(which_partition);\n-  idx_t leftmost_idx = leftmost(which_partition);\n+inline index_type ShenandoahRegionPartitions::find_index_of_next_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, index_type start_index, size_t cluster_size) const {\n+  index_type rightmost_idx = rightmost(which_partition);\n+  index_type leftmost_idx = leftmost(which_partition);\n@@ -535,1 +841,2 @@\n-  idx_t result = _membership[int(which_partition)].find_first_consecutive_set_bits(start_index, rightmost_idx + 1, cluster_size);\n+  index_type result =\n+    _membership[int(which_partition)].find_first_consecutive_set_bits(start_index, rightmost_idx + 1, cluster_size);\n@@ -543,3 +850,3 @@\n-inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_cluster_of_regions(\n-  ShenandoahFreeSetPartitionId which_partition, idx_t last_index, size_t cluster_size) const {\n-  idx_t leftmost_idx = leftmost(which_partition);\n+inline index_type ShenandoahRegionPartitions::find_index_of_previous_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, index_type last_index, size_t cluster_size) const {\n+  index_type leftmost_idx = leftmost(which_partition);\n@@ -548,1 +855,1 @@\n-  idx_t result = _membership[int(which_partition)].find_last_consecutive_set_bits(leftmost_idx - 1, last_index, cluster_size);\n+  index_type result = _membership[int(which_partition)].find_last_consecutive_set_bits(leftmost_idx - 1, last_index, cluster_size);\n@@ -556,1 +863,1 @@\n-idx_t ShenandoahRegionPartitions::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+index_type ShenandoahRegionPartitions::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n@@ -558,1 +865,1 @@\n-  idx_t max_regions = _max;\n+  index_type max_regions = _max;\n@@ -562,1 +869,1 @@\n-  for (idx_t idx = find_index_of_next_available_region(which_partition, _leftmosts_empty[int(which_partition)]);\n+  for (index_type idx = find_index_of_next_available_region(which_partition, _leftmosts_empty[int(which_partition)]);\n@@ -576,1 +883,1 @@\n-idx_t ShenandoahRegionPartitions::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+index_type ShenandoahRegionPartitions::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n@@ -581,1 +888,1 @@\n-  for (idx_t idx = find_index_of_previous_available_region(which_partition, _rightmosts_empty[int(which_partition)]);\n+  for (index_type idx = find_index_of_previous_available_region(which_partition, _rightmosts_empty[int(which_partition)]);\n@@ -597,1 +904,1 @@\n-void ShenandoahRegionPartitions::assert_bounds() {\n+void ShenandoahRegionPartitions::assert_bounds(bool validate_totals) {\n@@ -599,4 +906,16 @@\n-  idx_t leftmosts[UIntNumPartitions];\n-  idx_t rightmosts[UIntNumPartitions];\n-  idx_t empty_leftmosts[UIntNumPartitions];\n-  idx_t empty_rightmosts[UIntNumPartitions];\n+  size_t capacities[UIntNumPartitions];\n+  size_t used[UIntNumPartitions];\n+  size_t regions[UIntNumPartitions];\n+  size_t humongous_waste[UIntNumPartitions];\n+\n+  \/\/ We don't know whether young retired regions belonged to Mutator or Collector before they were retired.\n+  \/\/ We just tally the total, and divide it to make matches work if possible.\n+  size_t young_retired_regions = 0;\n+  size_t young_retired_used = 0;\n+  size_t young_retired_capacity = 0;\n+  size_t young_humongous_waste = 0;\n+\n+  index_type leftmosts[UIntNumPartitions];\n+  index_type rightmosts[UIntNumPartitions];\n+  index_type empty_leftmosts[UIntNumPartitions];\n+  index_type empty_rightmosts[UIntNumPartitions];\n@@ -609,0 +928,4 @@\n+    capacities[i] = 0;\n+    used[i] = 0;\n+    regions[i] = 0;\n+    humongous_waste[i] = 0;\n@@ -611,1 +934,1 @@\n-  for (idx_t i = 0; i < _max; i++) {\n+  for (index_type i = 0; i < _max; i++) {\n@@ -613,0 +936,1 @@\n+    size_t capacity = _free_set->alloc_capacity(i);\n@@ -615,1 +939,36 @@\n-        break;\n+      {\n+        assert(!validate_totals || (capacity != _region_size_bytes), \"Should not be retired if empty\");\n+        ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(i);\n+        if (r->is_humongous()) {\n+          if (r->is_old()) {\n+            regions[int(ShenandoahFreeSetPartitionId::OldCollector)]++;\n+            used[int(ShenandoahFreeSetPartitionId::OldCollector)] += _region_size_bytes;\n+            capacities[int(ShenandoahFreeSetPartitionId::OldCollector)] += _region_size_bytes;\n+            humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)] += capacity;\n+          } else {\n+            assert(r->is_young(), \"Must be young if not old\");\n+            young_retired_regions++;\n+            \/\/ Count entire region as used even if there is some waste.\n+            young_retired_used += _region_size_bytes;\n+            young_retired_capacity += _region_size_bytes;\n+            young_humongous_waste += capacity;\n+          }\n+        } else {\n+          assert(r->is_cset() || (capacity < PLAB::min_size() * HeapWordSize),\n+                 \"Expect retired remnant size to be smaller than min plab size\");\n+          \/\/ This region has been retired already or it is in the cset.  In either case, we set capacity to zero\n+          \/\/ so that the entire region will be counted as used.  We count young cset regions as \"retired\".\n+          capacity = 0;\n+          if (r->is_old()) {\n+            regions[int(ShenandoahFreeSetPartitionId::OldCollector)]++;\n+            used[int(ShenandoahFreeSetPartitionId::OldCollector)] += _region_size_bytes - capacity;\n+            capacities[int(ShenandoahFreeSetPartitionId::OldCollector)] += _region_size_bytes;\n+          } else {\n+            assert(r->is_young(), \"Must be young if not old\");\n+            young_retired_regions++;\n+            young_retired_used += _region_size_bytes - capacity;\n+            young_retired_capacity += _region_size_bytes;\n+          }\n+        }\n+      }\n+      break;\n@@ -621,2 +980,0 @@\n-        size_t capacity = _free_set->alloc_capacity(i);\n-        bool is_empty = (capacity == _region_size_bytes);\n@@ -624,0 +981,5 @@\n+        bool is_empty = (capacity == _region_size_bytes);\n+        regions[int(partition)]++;\n+        used[int(partition)] += _region_size_bytes - capacity;\n+        capacities[int(partition)] += _region_size_bytes;\n+\n@@ -659,2 +1021,2 @@\n-  idx_t beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n-  idx_t end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  index_type beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  index_type end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n@@ -662,1 +1024,1 @@\n-          \"free regions before the leftmost: %zd, bound %zd\",\n+          \"Mutator free regions before the leftmost: %zd, bound %zd\",\n@@ -665,1 +1027,1 @@\n-          \"free regions past the rightmost: %zd, bound %zd\",\n+          \"Mutator free regions past the rightmost: %zd, bound %zd\",\n@@ -671,1 +1033,1 @@\n-          \"free empty regions before the leftmost: %zd, bound %zd\",\n+          \"Mutator free empty regions before the leftmost: %zd, bound %zd\",\n@@ -674,1 +1036,1 @@\n-          \"free empty regions past the rightmost: %zd, bound %zd\",\n+          \"Mutator free empty regions past the rightmost: %zd, bound %zd\",\n@@ -685,1 +1047,1 @@\n-          \"leftmost region should be free: %zd\",  leftmost(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector leftmost region should be free: %zd\",  leftmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -688,1 +1050,1 @@\n-          \"rightmost region should be free: %zd\", rightmost(ShenandoahFreeSetPartitionId::Collector));\n+          \"Collector rightmost region should be free: %zd\", rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -695,1 +1057,1 @@\n-          \"free regions before the leftmost: %zd, bound %zd\",\n+          \"Collector free regions before the leftmost: %zd, bound %zd\",\n@@ -698,1 +1060,1 @@\n-          \"free regions past the rightmost: %zd, bound %zd\",\n+          \"Collector free regions past the rightmost: %zd, bound %zd\",\n@@ -704,1 +1066,1 @@\n-          \"free empty regions before the leftmost: %zd, bound %zd\",\n+          \"Collector free empty regions before the leftmost: %zd, bound %zd\",\n@@ -707,1 +1069,1 @@\n-          \"free empty regions past the rightmost: %zd, bound %zd\",\n+          \"Collector free empty regions past the rightmost: %zd, bound %zd\",\n@@ -711,1 +1073,1 @@\n-  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: %zd < %zd\",\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"OldCollector leftmost in bounds: %zd < %zd\",\n@@ -713,1 +1075,1 @@\n-  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: %zd < %zd\",\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"OldCollector rightmost in bounds: %zd < %zd\",\n@@ -719,1 +1081,1 @@\n-          \"leftmost region should be free: %zd\",  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+          \"OldCollector leftmost region should be free: %zd\",  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -723,1 +1085,1 @@\n-          \"rightmost region should be free: %zd\", rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+          \"OldCollector rightmost region should be free: %zd\", rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -730,1 +1092,1 @@\n-          \"free regions before the leftmost: %zd, bound %zd\",\n+          \"OldCollector free regions before the leftmost: %zd, bound %zd\",\n@@ -733,1 +1095,1 @@\n-          \"free regions past the rightmost: %zd, bound %zd\",\n+          \"OldCollector free regions past the rightmost: %zd, bound %zd\",\n@@ -739,1 +1101,1 @@\n-          \"free empty regions before the leftmost: %zd, bound %zd\",\n+          \"OldCollector free empty regions before the leftmost: %zd, bound %zd\",\n@@ -742,1 +1104,1 @@\n-          \"free empty regions past the rightmost: %zd, bound %zd\",\n+          \"OldCollector free empty regions past the rightmost: %zd, bound %zd\",\n@@ -744,0 +1106,81 @@\n+\n+  if (validate_totals) {\n+    \/\/ young_retired_regions need to be added to either Mutator or Collector partitions, 100% used.\n+    \/\/ Give enough of young_retired_regions, young_retired_capacity, young_retired_user\n+    \/\/  to the Mutator partition to top it off so that it matches the running totals.\n+    \/\/\n+    \/\/ Give any remnants to the Collector partition.  After topping off the Collector partition, its values\n+    \/\/  should also match running totals.\n+\n+    assert(young_retired_regions * _region_size_bytes == young_retired_capacity, \"sanity\");\n+    assert(young_retired_capacity == young_retired_used, \"sanity\");\n+\n+\n+    assert(capacities[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+           == _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old collector capacities must match\");\n+    assert(used[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+           == _used[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old collector used must match\");\n+    assert(regions[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+           == _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] \/ _region_size_bytes, \"Old collector regions must match\");\n+    assert(_capacity[int(ShenandoahFreeSetPartitionId::OldCollector)]\n+           >= _used[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old Collector capacity must be >= used\");\n+    assert(_available[int(ShenandoahFreeSetPartitionId::OldCollector)] ==\n+           (_capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] - _used[int(ShenandoahFreeSetPartitionId::OldCollector)]),\n+           \"Old Collector available must equal capacity minus used\");\n+    assert(_humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)] ==\n+           humongous_waste[int(ShenandoahFreeSetPartitionId::OldCollector)], \"Old Collector humongous waste must match\");\n+\n+    assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] >= capacities[int(ShenandoahFreeSetPartitionId::Mutator)],\n+           \"Capacity total must be >= counted tally\");\n+    size_t mutator_capacity_shortfall =\n+      _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] - capacities[int(ShenandoahFreeSetPartitionId::Mutator)];\n+    assert(mutator_capacity_shortfall <= young_retired_capacity, \"sanity\");\n+    capacities[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_capacity_shortfall;\n+    young_retired_capacity -= mutator_capacity_shortfall;\n+    capacities[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_capacity;\n+\n+\n+    assert(_used[int(ShenandoahFreeSetPartitionId::Mutator)] >= used[int(ShenandoahFreeSetPartitionId::Mutator)],\n+           \"Used total must be >= counted tally\");\n+    size_t mutator_used_shortfall =\n+      _used[int(ShenandoahFreeSetPartitionId::Mutator)] - used[int(ShenandoahFreeSetPartitionId::Mutator)];\n+    assert(mutator_used_shortfall <= young_retired_used, \"sanity\");\n+    used[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_used_shortfall;\n+    young_retired_used -= mutator_used_shortfall;\n+    used[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_used;\n+\n+    assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes\n+           >= regions[int(ShenandoahFreeSetPartitionId::Mutator)], \"Region total must be >= counted tally\");\n+    size_t mutator_regions_shortfall = (_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes\n+                                        - regions[int(ShenandoahFreeSetPartitionId::Mutator)]);\n+    assert(mutator_regions_shortfall <= young_retired_regions, \"sanity\");\n+    regions[int(ShenandoahFreeSetPartitionId::Mutator)] += mutator_regions_shortfall;\n+    young_retired_regions -= mutator_regions_shortfall;\n+    regions[int(ShenandoahFreeSetPartitionId::Collector)] += young_retired_regions;\n+\n+    assert(capacities[int(ShenandoahFreeSetPartitionId::Collector)] == _capacity[int(ShenandoahFreeSetPartitionId::Collector)],\n+           \"Collector capacities must match\");\n+    assert(used[int(ShenandoahFreeSetPartitionId::Collector)] == _used[int(ShenandoahFreeSetPartitionId::Collector)],\n+           \"Collector used must match\");\n+    assert(regions[int(ShenandoahFreeSetPartitionId::Collector)]\n+           == _capacity[int(ShenandoahFreeSetPartitionId::Collector)] \/ _region_size_bytes, \"Collector regions must match\");\n+    assert(_capacity[int(ShenandoahFreeSetPartitionId::Collector)] >= _used[int(ShenandoahFreeSetPartitionId::Collector)],\n+           \"Collector Capacity must be >= used\");\n+    assert(_available[int(ShenandoahFreeSetPartitionId::Collector)] ==\n+           (_capacity[int(ShenandoahFreeSetPartitionId::Collector)] - _used[int(ShenandoahFreeSetPartitionId::Collector)]),\n+           \"Collector Available must equal capacity minus used\");\n+\n+    assert(capacities[int(ShenandoahFreeSetPartitionId::Mutator)] == _capacity[int(ShenandoahFreeSetPartitionId::Mutator)],\n+           \"Mutator capacities must match\");\n+    assert(used[int(ShenandoahFreeSetPartitionId::Mutator)] == _used[int(ShenandoahFreeSetPartitionId::Mutator)],\n+           \"Mutator used must match\");\n+    assert(regions[int(ShenandoahFreeSetPartitionId::Mutator)]\n+           == _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] \/ _region_size_bytes, \"Mutator regions must match\");\n+    assert(_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] >= _used[int(ShenandoahFreeSetPartitionId::Mutator)],\n+           \"Mutator capacity must be >= used\");\n+    assert(_available[int(ShenandoahFreeSetPartitionId::Mutator)] ==\n+           (_capacity[int(ShenandoahFreeSetPartitionId::Mutator)] - _used[int(ShenandoahFreeSetPartitionId::Mutator)]),\n+           \"Mutator available must equal capacity minus used\");\n+    assert(_humongous_waste[int(ShenandoahFreeSetPartitionId::Mutator)] == young_humongous_waste,\n+           \"Mutator humongous waste must match\");\n+  }\n@@ -750,1 +1193,13 @@\n-  _alloc_bias_weight(0)\n+  _total_humongous_waste(0),\n+  _alloc_bias_weight(0),\n+  _total_young_used(0),\n+  _total_old_used(0),\n+  _total_global_used(0),\n+  _young_affiliated_regions(0),\n+  _old_affiliated_regions(0),\n+  _global_affiliated_regions(0),\n+  _young_unaffiliated_regions(0),\n+  _global_unaffiliated_regions(0),\n+  _total_young_regions(0),\n+  _total_global_regions(0),\n+  _mutator_bytes_allocated_since_gc_start(0)\n@@ -755,0 +1210,1 @@\n+\/\/ was pip_pad_bytes\n@@ -758,3 +1214,5 @@\n-  size_t idx = region->index();\n-  size_t capacity = alloc_capacity(region);\n-  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+  size_t region_size_bytes =  ShenandoahHeapRegion::region_size_bytes();\n+  size_t available_in_region = alloc_capacity(region);\n+  size_t region_index = region->index();\n+  ShenandoahFreeSetPartitionId p = _partitions.membership(region_index);\n+  assert(_partitions.membership(region_index) == ShenandoahFreeSetPartitionId::NotFree,\n@@ -762,4 +1220,46 @@\n-  if (capacity >= plab_min_size_in_bytes) {\n-    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n-    _heap->old_generation()->augment_promoted_reserve(capacity);\n-  }\n+\n+  \/\/ If region had been retired, its end-of-region alignment pad had been counted as used within the Mutator partition\n+  size_t used_while_awaiting_pip = region_size_bytes;\n+  size_t used_after_pip = region_size_bytes;\n+  if (available_in_region >= plab_min_size_in_bytes) {\n+    used_after_pip -= available_in_region;\n+  } else {\n+    if (available_in_region >= ShenandoahHeap::min_fill_size() * HeapWordSize) {\n+      size_t fill_words = available_in_region \/ HeapWordSize;\n+      ShenandoahHeap::heap()->old_generation()->card_scan()->register_object(region->top());\n+      region->allocate_fill(fill_words);\n+    }\n+    available_in_region = 0;\n+  }\n+\n+  assert(p == ShenandoahFreeSetPartitionId::NotFree, \"pip region must be NotFree\");\n+  assert(region->is_young(), \"pip region must be young\");\n+\n+  \/\/ Though this region may have been promoted in place from the Collector region, its usage is now accounted within\n+  \/\/ the Mutator partition.\n+  _partitions.decrease_used(ShenandoahFreeSetPartitionId::Mutator, used_while_awaiting_pip);\n+\n+  \/\/ decrease capacity adjusts available\n+  _partitions.decrease_capacity(ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+  _partitions.increase_capacity(ShenandoahFreeSetPartitionId::OldCollector, region_size_bytes);\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, used_after_pip);\n+  region->set_affiliation(ShenandoahAffiliation::OLD_GENERATION);\n+  if (available_in_region > 0) {\n+    assert(available_in_region >= plab_min_size_in_bytes, \"enforced above\");\n+    _partitions.increase_region_counts(ShenandoahFreeSetPartitionId::OldCollector, 1);\n+    \/\/ make_free() adjusts bounds for OldCollector partition\n+    _partitions.make_free(region_index, ShenandoahFreeSetPartitionId::OldCollector, available_in_region);\n+    _heap->old_generation()->augment_promoted_reserve(available_in_region);\n+    assert(available_in_region != region_size_bytes, \"Nothing to promote in place\");\n+  }\n+  \/\/ else, leave this region as NotFree\n+\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+  \/\/ Conservatively, assume that pip regions came from both Mutator and Collector\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ false, \/* CollectorEmptiesChanged *\/ false,\n+                             \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ true,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+  _partitions.assert_bounds(true);\n@@ -784,1 +1284,1 @@\n-  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+  for (index_type idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n@@ -869,1 +1369,1 @@\n-    idx_t non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n+    index_type non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n@@ -871,1 +1371,1 @@\n-    idx_t non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n+    index_type non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n@@ -880,1 +1380,1 @@\n-  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+  for (index_type idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n@@ -949,1 +1449,1 @@\n-  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+  for (index_type idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n@@ -1037,1 +1537,0 @@\n-\n@@ -1049,2 +1548,0 @@\n-    _heap->generation_for(r->affiliation())->increment_affiliated_region_count();\n-\n@@ -1123,0 +1620,1 @@\n+      increase_bytes_allocated(req.actual_size() * HeapWordSize);\n@@ -1131,1 +1629,1 @@\n-        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, (req.actual_size() + req.waste()) * HeapWordSize);\n@@ -1135,1 +1633,1 @@\n-        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, (req.actual_size() + req.waste()) * HeapWordSize);\n@@ -1140,1 +1638,0 @@\n-  static const size_t min_capacity = (size_t) (ShenandoahHeapRegion::region_size_bytes() * (1.0 - 1.0 \/ ShenandoahEvacWaste));\n@@ -1142,2 +1639,22 @@\n-\n-  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size() * HeapWordSize)) {\n+  ShenandoahFreeSetPartitionId orig_partition;\n+  ShenandoahGeneration* request_generation = nullptr;\n+  if (req.is_mutator_alloc()) {\n+    request_generation = _heap->mode()->is_generational()? _heap->young_generation(): _heap->global_generation();\n+    orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+  } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+    request_generation = _heap->mode()->is_generational()? _heap->young_generation(): _heap->global_generation();\n+    orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+  } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+    request_generation = _heap->old_generation();\n+    orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+  } else {\n+    assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+    if (req.is_old()) {\n+      request_generation = _heap->old_generation();\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+    } else {\n+      request_generation = _heap->mode()->is_generational()? _heap->young_generation(): _heap->global_generation();\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    }\n+  }\n+  if (alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n@@ -1151,14 +1668,32 @@\n-    ShenandoahFreeSetPartitionId orig_partition;\n-    if (req.is_mutator_alloc()) {\n-      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n-    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n-      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n-    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n-      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n-    } else {\n-      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n-      if (req.is_old()) {\n-        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n-      } else {\n-        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n-      }\n+    if ((result != nullptr) && in_new_region) {\n+      _partitions.one_region_is_no_longer_empty(orig_partition);\n+    }\n+    size_t waste_bytes = _partitions.retire_from_partition(orig_partition, idx, r->used());\n+    if (req.is_mutator_alloc() && (waste_bytes > 0)) {\n+      increase_bytes_allocated(waste_bytes);\n+    }\n+  } else if ((result != nullptr) && in_new_region) {\n+    _partitions.one_region_is_no_longer_empty(orig_partition);\n+  }\n+\n+  switch (orig_partition) {\n+  case ShenandoahFreeSetPartitionId::Mutator:\n+    recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                         \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ false>();\n+    if (in_new_region) {\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                                 \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ false,\n+                                 \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ false,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+    }\n+    break;\n+  case ShenandoahFreeSetPartitionId::Collector:\n+    recompute_total_used<\/* UsedByMutatorChanged *\/ false,\n+                         \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ false>();\n+    if (in_new_region) {\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ false, \/* CollectorEmptiesChanged *\/ true,\n+                                 \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ false,\n+                                 \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ false,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n@@ -1166,2 +1701,15 @@\n-    _partitions.retire_from_partition(orig_partition, idx, r->used());\n-    _partitions.assert_bounds();\n+    break;\n+  case ShenandoahFreeSetPartitionId::OldCollector:\n+    recompute_total_used<\/* UsedByMutatorChanged *\/ false,\n+                         \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+    if (in_new_region) {\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ false, \/* CollectorEmptiesChanged *\/ false,\n+                                 \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ false,\n+                                 \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ false,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+    }\n+    break;\n+  case ShenandoahFreeSetPartitionId::NotFree:\n+  default:\n+    assert(false, \"won't happen\");\n@@ -1169,0 +1717,1 @@\n+  _partitions.assert_bounds(true);\n@@ -1177,1 +1726,1 @@\n-  idx_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n+  index_type num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n@@ -1183,1 +1732,1 @@\n-  if (num > (idx_t) _partitions.count(ShenandoahFreeSetPartitionId::Mutator)) {\n+  if (num > (index_type) _partitions.count(ShenandoahFreeSetPartitionId::Mutator)) {\n@@ -1187,3 +1736,3 @@\n-  idx_t start_range = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n-  idx_t end_range = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator) + 1;\n-  idx_t last_possible_start = end_range - num;\n+  index_type start_range = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+  index_type end_range = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator) + 1;\n+  index_type last_possible_start = end_range - num;\n@@ -1193,1 +1742,1 @@\n-  idx_t beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+  index_type beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n@@ -1199,1 +1748,1 @@\n-  idx_t end = beg;\n+  index_type end = beg;\n@@ -1207,1 +1756,1 @@\n-      idx_t slide_delta = end + 1 - beg;\n+      index_type slide_delta = end + 1 - beg;\n@@ -1212,1 +1761,1 @@\n-      for (idx_t span_end = beg + num; slide_delta > 0; slide_delta--) {\n+      for (index_type span_end = beg + num; slide_delta > 0; slide_delta--) {\n@@ -1238,12 +1787,14 @@\n-  size_t remainder = words_size & ShenandoahHeapRegion::region_size_words_mask();\n-  \/\/ Initialize regions:\n-  for (idx_t i = beg; i <= end; i++) {\n-    ShenandoahHeapRegion* r = _heap->get_region(i);\n-    r->try_recycle_under_lock();\n-\n-    assert(i == beg || _heap->get_region(i - 1)->index() + 1 == r->index(), \"Should be contiguous\");\n-    assert(r->is_empty(), \"Should be empty\");\n-\n-    r->set_affiliation(req.affiliation());\n-\n-    if (is_humongous) {\n+  size_t total_used = 0;\n+  const size_t used_words_in_last_region = words_size & ShenandoahHeapRegion::region_size_words_mask();\n+  size_t waste_bytes;\n+  \/\/ Retire regions from free partition and initialize them.\n+  if (is_humongous) {\n+    \/\/ Humongous allocation retires all regions at once: no allocation is possible anymore.\n+    \/\/ retire_range_from_partition() will adjust bounds on Mutator free set if appropriate and will recompute affiliated.\n+    _partitions.retire_range_from_partition(ShenandoahFreeSetPartitionId::Mutator, beg, end);\n+    for (index_type i = beg; i <= end; i++) {\n+      ShenandoahHeapRegion* r = _heap->get_region(i);\n+      assert(i == beg || _heap->get_region(i - 1)->index() + 1 == r->index(), \"Should be contiguous\");\n+      r->try_recycle_under_lock();\n+      assert(r->is_empty(), \"Should be empty\");\n+      r->set_affiliation(req.affiliation());\n@@ -1255,10 +1806,7 @@\n-    } else {\n-      r->make_regular_allocation(req.affiliation());\n-    }\n-\n-    \/\/ Trailing region may be non-full, record the remainder there\n-    size_t used_words;\n-    if ((i == end) && (remainder != 0)) {\n-      used_words = remainder;\n-    } else {\n-      used_words = ShenandoahHeapRegion::region_size_words();\n+      if ((i == end) && (used_words_in_last_region > 0)) {\n+        r->set_top(r->bottom() + used_words_in_last_region);\n+      } else {\n+        \/\/ if used_words_in_last_region is zero, then the end region is fully consumed.\n+        r->set_top(r->end());\n+      }\n+      r->set_update_watermark(r->bottom());\n@@ -1266,9 +1814,0 @@\n-    r->set_update_watermark(r->bottom());\n-    r->set_top(r->bottom() + used_words);\n-  }\n-  generation->increase_affiliated_region_count(num);\n-\n-  size_t total_used = 0;\n-  if (is_humongous) {\n-    \/\/ Humongous allocation retires all regions at once: no allocation is possible anymore.\n-    _partitions.retire_range_from_partition(ShenandoahFreeSetPartitionId::Mutator, beg, end);\n@@ -1276,0 +1815,2 @@\n+    waste_bytes =\n+      (used_words_in_last_region == 0)? 0: ShenandoahHeapRegion::region_size_bytes() - used_words_in_last_region * HeapWordSize;\n@@ -1278,1 +1819,2 @@\n-    for (idx_t i = beg; i <= end; i++) {\n+    waste_bytes = 0;\n+    for (index_type i = beg; i <= end; i++) {\n@@ -1280,2 +1822,10 @@\n-      if (r->free() < PLAB::min_size() * HeapWordSize) {\n-        _partitions.retire_from_partition(ShenandoahFreeSetPartitionId::Mutator, i, r->used());\n+      assert(i == beg || _heap->get_region(i - 1)->index() + 1 == r->index(), \"Should be contiguous\");\n+      assert(r->is_empty(), \"Should be empty\");\n+      r->try_recycle_under_lock();\n+      r->set_affiliation(req.affiliation());\n+      r->make_regular_allocation(req.affiliation());\n+      if ((i == end) && (used_words_in_last_region > 0)) {\n+        r->set_top(r->bottom() + used_words_in_last_region);\n+      } else {\n+        \/\/ if used_words_in_last_region is zero, then the end region is fully consumed.\n+        r->set_top(r->end());\n@@ -1283,0 +1833,1 @@\n+      r->set_update_watermark(r->bottom());\n@@ -1284,0 +1835,12 @@\n+      if  (r->free() < PLAB::min_size() * HeapWordSize) {\n+        \/\/ retire_from_partition() will adjust bounds on Mutator free set if appropriate and will recompute affiliated.\n+        \/\/ It also increases used for the waste bytes, which includes bytes filled at retirement and bytes too small\n+        \/\/ to be filled.  Only the last iteration may have non-zero waste_bytes.\n+        waste_bytes += _partitions.retire_from_partition(ShenandoahFreeSetPartitionId::Mutator, i, r->used());\n+      }\n+    }\n+    _partitions.decrease_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator, num);\n+    if (waste_bytes > 0) {\n+      \/\/ For humongous allocations, waste_bytes are included in total_used.  Since this is not humongous,\n+      \/\/ we need to account separately for the waste_bytes.\n+      increase_bytes_allocated(waste_bytes);\n@@ -1287,2 +1850,1 @@\n-  _partitions.assert_bounds();\n-\n+  increase_bytes_allocated(total_used);\n@@ -1290,2 +1852,7 @@\n-  if (remainder != 0 && is_humongous) {\n-    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  \/\/ If !is_humongous, the \"waste\" is made availabe for new allocation\n+  if (waste_bytes > 0) {\n+    req.set_waste(waste_bytes \/ HeapWordSize);\n+    if (is_humongous) {\n+      _partitions.increase_humongous_waste(ShenandoahFreeSetPartitionId::Mutator, waste_bytes);\n+      _total_humongous_waste += waste_bytes;\n+    }\n@@ -1293,0 +1860,10 @@\n+\n+  recompute_total_young_used<\/* UsedByMutatorChanged *\/ true, \/*UsedByCollectorChanged *\/ false>();\n+  recompute_total_global_used<\/* UsedByMutatorChanged *\/ true, \/*UsedByCollectorChanged *\/ false,\n+                              \/* UsedByOldCollectorChanged *\/ true>();\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                             \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ false,\n+                             \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ false,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  _partitions.assert_bounds(true);\n@@ -1297,0 +1874,91 @@\n+private:\n+  static const ssize_t SentinelUsed = -1;\n+  static const ssize_t SentinelIndex = -1;\n+  static const size_t MaxSavedRegions = 128;\n+\n+  ShenandoahRegionPartitions* _partitions;\n+  volatile size_t _recycled_region_count;\n+  ssize_t _region_indices[MaxSavedRegions];\n+  ssize_t _region_used[MaxSavedRegions];\n+\n+  void get_lock_and_flush_buffer(size_t region_count, size_t overflow_region_used, size_t overflow_region_index) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahHeapLocker locker(heap->lock());\n+    size_t recycled_regions = AtomicAccess::load(&_recycled_region_count);\n+    size_t region_tallies[int(ShenandoahRegionPartitions::NumPartitions)];\n+    size_t used_byte_tallies[int(ShenandoahRegionPartitions::NumPartitions)];\n+    for (int p = 0; p < int(ShenandoahRegionPartitions::NumPartitions); p++) {\n+      region_tallies[p] = 0;\n+      used_byte_tallies[p] = 0;\n+    }\n+    ShenandoahFreeSetPartitionId p = _partitions->membership(overflow_region_index);\n+    used_byte_tallies[int(p)] += overflow_region_used;\n+    if (region_count <= recycled_regions) {\n+      \/\/ _recycled_region_count has not been decremented after I incremented it to obtain region_count, so I will\n+      \/\/ try to flush the buffer.\n+\n+      \/\/ Multiple worker threads may attempt to flush this buffer.  The first thread to acquire the lock does the work.\n+      \/\/ _recycled_region_count is only decreased while holding the heap lock.\n+      if (region_count > recycled_regions) {\n+        region_count = recycled_regions;\n+      }\n+      for (size_t i = 0; i < region_count; i++) {\n+        ssize_t used;\n+        \/\/ wait for other threads to finish updating their entries within the region buffer before processing entry\n+        do {\n+          used = _region_used[i];\n+        } while (used == SentinelUsed);\n+        ssize_t index;\n+        do {\n+          index = _region_indices[i];\n+        } while (index == SentinelIndex);\n+\n+        ShenandoahFreeSetPartitionId p = _partitions->membership(index);\n+        assert(p != ShenandoahFreeSetPartitionId::NotFree, \"Trashed regions should be in a free partition\");\n+        used_byte_tallies[int(p)] += used;\n+        region_tallies[int(p)]++;\n+      }\n+      if (region_count > 0) {\n+        for (size_t i = 0; i < MaxSavedRegions; i++) {\n+          _region_indices[i] = SentinelIndex;\n+          _region_used[i] = SentinelUsed;\n+        }\n+      }\n+\n+      \/\/ The almost last thing we do before releasing the lock is to set the _recycled_region_count to 0.  What happens next?\n+      \/\/\n+      \/\/  1. Any worker thread that attempted to buffer a new region while we were flushing the buffer will have seen\n+      \/\/     that _recycled_region_count > MaxSavedRegions. All such worker threads will first wait for the lock, then\n+      \/\/     discover that the _recycled_region_count is zero, then, while holding the lock, they will process the\n+      \/\/     region so it doesn't have to be placed into the buffer.  This handles the large majority of cases.\n+      \/\/\n+      \/\/  2. However, there's a race that can happen, which will result in someewhat different behavior.  Suppose\n+      \/\/     this thread resets _recycled_region_count to 0.  Then some other worker thread increments _recycled_region_count\n+      \/\/     in order to stores its region into the buffer and suppose this happens before all of the other worker threads\n+      \/\/     which are waiting to acquire the heap lock have finished their efforts to flush the buffer.  If this happens,\n+      \/\/     then the workers who are waiting to acquire the heap lock and flush the buffer will find that _recycled_region_count\n+      \/\/     has decreased from the value it held when they last tried to increment its value.  In this case, these worker\n+      \/\/     threads will process their overflow region while holding the lock, but they will not attempt to process regions\n+      \/\/     newly placed into the buffer.  Otherwise, confusion could result.\n+      \/\/\n+      \/\/ Assumption: all worker threads who are attempting to acquire lock and flush buffer will finish their efforts before\n+      \/\/             the buffer once again overflows.\n+      \/\/ How could we avoid depending on this assumption?\n+      \/\/   1. Let MaxSavedRegions be as large as number of regions, or at least as large as the collection set.\n+      \/\/   2. Keep a count of how many times the buffer has been flushed per instantation of the\n+      \/\/      ShenandoahRecycleTrashedRegionClosure object, and only consult\/update this value while holding the heap lock.\n+      \/\/      Need to think about how this helps resolve the race.\n+      _recycled_region_count = 0;\n+    } else {\n+      \/\/ Some other thread has already processed the buffer, resetting _recycled_region_count to zero. Its current value\n+      \/\/ may be greater than zero because other workers may have accumulated entries into the buffer. But it is \"extremely\"\n+      \/\/ unlikely that it will overflow again before all waiting workers have had a chance to clear their state. While I've\n+      \/\/ got the heap lock, I'll go ahead and update the global state for my overflow region. I'll let other heap regions\n+      \/\/ accumulate in the buffer to be processed when the buffer is once again full.\n+      region_count = 0;\n+    }\n+    for (size_t p = 0; p < int(ShenandoahRegionPartitions::NumPartitions); p++) {\n+      _partitions->decrease_used(ShenandoahFreeSetPartitionId(p), used_byte_tallies[p]);\n+    }\n+  }\n+\n@@ -1298,1 +1966,8 @@\n-  ShenandoahRecycleTrashedRegionClosure(): ShenandoahHeapRegionClosure() {}\n+  ShenandoahRecycleTrashedRegionClosure(ShenandoahRegionPartitions* p): ShenandoahHeapRegionClosure() {\n+    _partitions = p;\n+    _recycled_region_count = 0;\n+    for (size_t i = 0; i < MaxSavedRegions; i++) {\n+      _region_indices[i] = SentinelIndex;\n+      _region_used[i] = SentinelUsed;\n+    }\n+  }\n@@ -1316,1 +1991,1 @@\n-  ShenandoahRecycleTrashedRegionClosure closure;\n+  ShenandoahRecycleTrashedRegionClosure closure(&_partitions);\n@@ -1320,0 +1995,28 @@\n+bool ShenandoahFreeSet::transfer_one_region_from_mutator_to_old_collector(size_t idx, size_t alloc_capacity) {\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahYoungGeneration* young_gen = gen_heap->young_generation();\n+  ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  assert(alloc_capacity == region_size_bytes, \"Region must be empty\");\n+  if ((young_unaffiliated_regions() > 0) &&\n+      ((_partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector) + region_size_bytes)\n+       <= gen_heap->generation_sizer()->max_size_for(old_gen)) &&\n+      ((total_young_regions() - 1) * region_size_bytes >= gen_heap->generation_sizer()->min_size_for(young_gen))) {\n+    _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                 ShenandoahFreeSetPartitionId::OldCollector, alloc_capacity);\n+    gen_heap->old_generation()->augment_evacuation_reserve(alloc_capacity);\n+    recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                         \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+    \/\/ Transferred region is unaffilliated, empty\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                               \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+    _partitions.assert_bounds(true);\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n@@ -1327,1 +2030,1 @@\n-  const size_t region_capacity = alloc_capacity(r);\n+  const size_t region_alloc_capacity = alloc_capacity(r);\n@@ -1329,6 +2032,1 @@\n-  bool transferred = gen_heap->generation_sizer()->transfer_to_old(1);\n-  if (transferred) {\n-    _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n-                                                 ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n-    _partitions.assert_bounds();\n-    _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+  if (transfer_one_region_from_mutator_to_old_collector(idx, region_alloc_capacity)) {\n@@ -1346,1 +2044,1 @@\n-    idx_t unusable_trash = -1;\n+    index_type unusable_trash = -1;\n@@ -1364,4 +2062,9 @@\n-                                                   ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n-\n-      _partitions.assert_bounds();\n-\n+                                                   ShenandoahFreeSetPartitionId::OldCollector, region_alloc_capacity);\n+      \/\/ Should have no effect on used, since flipped regions are trashed: zero used *\/\n+      \/\/ Transferred regions are not affiliated, because they are empty (trash)\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                                 \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                                 \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+      _partitions.assert_bounds(true);\n@@ -1372,1 +2075,1 @@\n-      _heap->old_generation()->set_evacuation_reserve(reserve - unusable_capacity + region_capacity);\n+      _heap->old_generation()->set_evacuation_reserve(reserve - unusable_capacity + region_alloc_capacity);\n@@ -1390,2 +2093,9 @@\n-  _partitions.assert_bounds();\n-\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+  \/\/ Transfer only affects unaffiliated regions, which stay in young\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                             \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+  _partitions.assert_bounds(true);\n@@ -1403,1 +2113,7 @@\n-\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                             \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ true,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n@@ -1410,1 +2126,1 @@\n-void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_trashed_regions, size_t &old_trashed_regions,\n@@ -1413,0 +2129,1 @@\n+  \/\/ This resets all state information, removing all regions from all sets.\n@@ -1418,2 +2135,5 @@\n-  old_cset_regions = 0;\n-  young_cset_regions = 0;\n+  old_trashed_regions = 0;\n+  young_trashed_regions = 0;\n+\n+  size_t old_cset_regions = 0;\n+  size_t young_cset_regions = 0;\n@@ -1422,1 +2142,1 @@\n-  size_t max_regions = _partitions.max_regions();\n+  size_t max_regions = _partitions.max();\n@@ -1428,2 +2148,0 @@\n-  size_t mutator_regions = 0;\n-  size_t mutator_used = 0;\n@@ -1435,1 +2153,6 @@\n-  size_t old_collector_regions = 0;\n+\n+  size_t mutator_empty = 0;\n+  size_t old_collector_empty = 0;\n+\n+  \/\/ These two variables represent the total used within each partition, including humongous waste and retired regions\n+  size_t mutator_used = 0;\n@@ -1438,0 +2161,17 @@\n+  \/\/ These two variables represent memory that is wasted within humongous regions due to alignment padding\n+  size_t mutator_humongous_waste = 0;\n+  size_t old_collector_humongous_waste = 0;\n+\n+  \/\/ These two variables track regions that have allocatable memory\n+  size_t mutator_regions = 0;\n+  size_t old_collector_regions = 0;\n+\n+  \/\/ These two variables track regions that are not empty within each partition\n+  size_t affiliated_mutator_regions = 0;\n+  size_t affiliated_old_collector_regions = 0;\n+\n+  \/\/ These two variables represent the total capacity of each partition, including retired regions\n+  size_t total_mutator_regions = 0;\n+  size_t total_old_collector_regions = 0;\n+\n+  bool is_generational = _heap->mode()->is_generational();\n@@ -1442,2 +2182,2 @@\n-      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n-      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      \/\/ Trashed regions represent immediate garbage identified by final mark and regions that had been in the collection\n+      \/\/ partition but have not yet been \"cleaned up\" following update refs.\n@@ -1445,1 +2185,1 @@\n-        old_cset_regions++;\n+        old_trashed_regions++;\n@@ -1448,1 +2188,1 @@\n-        young_cset_regions++;\n+        young_trashed_regions++;\n@@ -1463,1 +2203,1 @@\n-      if (ac > PLAB::min_size() * HeapWordSize) {\n+      if (ac >= PLAB::min_size() * HeapWordSize) {\n@@ -1474,0 +2214,1 @@\n+            mutator_empty++;\n@@ -1480,0 +2221,2 @@\n+          } else {\n+            affiliated_mutator_regions++;\n@@ -1482,0 +2225,1 @@\n+          total_mutator_regions++;\n@@ -1492,8 +2236,2 @@\n-          if (ac == region_size_bytes) {\n-            if (idx < old_collector_leftmost_empty) {\n-              old_collector_leftmost_empty = idx;\n-            }\n-            if (idx > old_collector_rightmost_empty) {\n-              old_collector_rightmost_empty = idx;\n-            }\n-          }\n+          assert(ac != region_size_bytes, \"Empty regions should be in mutator partition\");\n+          affiliated_old_collector_regions++;\n@@ -1501,1 +2239,45 @@\n-          old_collector_used += (region_size_bytes - ac);\n+          total_old_collector_regions++;\n+          old_collector_used += region_size_bytes - ac;\n+        }\n+      } else {\n+        \/\/ This region does not have enough free to be part of the free set.  Count all of its memory as used.\n+        assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree, \"Region should have been retired\");\n+        if (region->is_old()) {\n+          old_collector_used += region_size_bytes;\n+          total_old_collector_regions++;\n+          affiliated_old_collector_regions++;\n+        } else {\n+          mutator_used += region_size_bytes;\n+          total_mutator_regions++;\n+          affiliated_mutator_regions++;\n+        }\n+      }\n+    } else {\n+      \/\/ This region does not allow allocation (it is retired or is humongous or is in cset).\n+      \/\/ Retired and humongous regions generally have no alloc capacity, but cset regions may have large alloc capacity.\n+      if (region->is_cset()) {\n+        if (region->is_old()) {\n+          old_cset_regions++;\n+        } else {\n+          young_cset_regions++;\n+        }\n+      } else {\n+        assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree, \"Region should have been retired\");\n+        size_t ac = alloc_capacity(region);\n+        size_t humongous_waste_bytes = 0;\n+        if (region->is_humongous_start()) {\n+          oop obj = cast_to_oop(region->bottom());\n+          size_t byte_size = obj->size() * HeapWordSize;\n+          size_t region_span = ShenandoahHeapRegion::required_regions(byte_size);\n+          humongous_waste_bytes = region_span * ShenandoahHeapRegion::region_size_bytes() - byte_size;\n+        }\n+        if (region->is_old()) {\n+          old_collector_used += region_size_bytes;\n+          total_old_collector_regions++;\n+          old_collector_humongous_waste += humongous_waste_bytes;\n+          affiliated_old_collector_regions++;\n+        } else {\n+          mutator_used += region_size_bytes;\n+          total_mutator_regions++;\n+          mutator_humongous_waste += humongous_waste_bytes;\n+          affiliated_mutator_regions++;\n@@ -1506,0 +2288,9 @@\n+  \/\/ At the start of evacuation, the cset regions are not counted as part of Mutator or OldCollector partitions.\n+\n+  \/\/ At the end of GC, when we rebuild rebuild freeset (which happens before we have recycled the collection set), we treat\n+  \/\/ all cset regions as part of capacity, as fully available, as unaffiliated.  We place trashed regions into the Mutator\n+  \/\/ partition.\n+\n+  \/\/ No need to update generation sizes here.  These are the sizes already recognized by the generations.  These\n+  \/\/ adjustments allow the freeset tallies to match the generation tallies.\n+\n@@ -1514,1 +2305,0 @@\n-\n@@ -1523,0 +2313,3 @@\n+  log_debug(gc, free)(\"  total_mutator_regions: %zu, total_old_collector_regions: %zu\"\n+                      \", mutator_empty: %zu, old_collector_empty: %zu\",\n+                      total_mutator_regions, total_old_collector_regions, mutator_empty, old_collector_empty);\n@@ -1524,0 +2317,2 @@\n+  index_type rightmost_idx = (mutator_leftmost == max_regions)? -1: (index_type) mutator_rightmost;\n+  index_type rightmost_empty_idx = (mutator_leftmost_empty == max_regions)? -1: (index_type) mutator_rightmost_empty;\n@@ -1525,2 +2320,0 @@\n-  idx_t rightmost_idx = (mutator_leftmost == max_regions)? -1: (idx_t) mutator_rightmost;\n-  idx_t rightmost_empty_idx = (mutator_leftmost_empty == max_regions)? -1: (idx_t) mutator_rightmost_empty;\n@@ -1528,5 +2321,28 @@\n-                                          mutator_regions, mutator_used);\n-  rightmost_idx = (old_collector_leftmost == max_regions)? -1: (idx_t) old_collector_rightmost;\n-  rightmost_empty_idx = (old_collector_leftmost_empty == max_regions)? -1: (idx_t) old_collector_rightmost_empty;\n-  _partitions.establish_old_collector_intervals(old_collector_leftmost, rightmost_idx, old_collector_leftmost_empty,\n-                                                rightmost_empty_idx, old_collector_regions, old_collector_used);\n+                                          total_mutator_regions + young_cset_regions, mutator_empty, mutator_regions,\n+                                          mutator_used + young_cset_regions * region_size_bytes, mutator_humongous_waste);\n+  rightmost_idx = (old_collector_leftmost == max_regions)? -1: (index_type) old_collector_rightmost;\n+  rightmost_empty_idx = (old_collector_leftmost_empty == max_regions)? -1: (index_type) old_collector_rightmost_empty;\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, rightmost_idx,\n+                                                old_collector_leftmost_empty, rightmost_empty_idx,\n+                                                total_old_collector_regions + old_cset_regions,\n+                                                old_collector_empty, old_collector_regions,\n+                                                old_collector_used + old_cset_regions * region_size_bytes,\n+                                                old_collector_humongous_waste);\n+  _total_humongous_waste = mutator_humongous_waste + old_collector_humongous_waste;\n+  _total_young_regions = total_mutator_regions + young_cset_regions;\n+  _total_global_regions = _total_young_regions + total_old_collector_regions + old_cset_regions;\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                             \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ true,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  _partitions.assert_bounds(true);\n+#ifdef ASSERT\n+  if (_heap->mode()->is_generational()) {\n+    assert(young_affiliated_regions() == _heap->young_generation()->get_affiliated_region_count(), \"sanity\");\n+  } else {\n+    assert(young_affiliated_regions() == _heap->global_generation()->get_affiliated_region_count(), \"sanity\");\n+  }\n+#endif\n@@ -1541,0 +2357,107 @@\n+void ShenandoahFreeSet::transfer_humongous_regions_from_mutator_to_old_collector(size_t xfer_regions,\n+                                                                                 size_t humongous_waste_bytes) {\n+  shenandoah_assert_heaplocked();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  _partitions.decrease_humongous_waste(ShenandoahFreeSetPartitionId::Mutator, humongous_waste_bytes);\n+  _partitions.decrease_used(ShenandoahFreeSetPartitionId::Mutator, xfer_regions * region_size_bytes);\n+  _partitions.decrease_capacity(ShenandoahFreeSetPartitionId::Mutator, xfer_regions * region_size_bytes);\n+\n+  _partitions.increase_capacity(ShenandoahFreeSetPartitionId::OldCollector, xfer_regions * region_size_bytes);\n+  _partitions.increase_humongous_waste(ShenandoahFreeSetPartitionId::OldCollector, humongous_waste_bytes);\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, xfer_regions * region_size_bytes);\n+\n+  \/\/ _total_humongous_waste, _total_global_regions are unaffected by transfer\n+  _total_young_regions -= xfer_regions;\n+  recompute_total_young_used<\/* UsedByMutatorChanged *\/ true, \/* UsedByCollectorChanged *\/ false>();\n+  recompute_total_old_used<\/* UsedByOldCollectorChanged *\/ true>();\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ false, \/* CollectorEmptiesChanged *\/ false,\n+                             \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+  _partitions.assert_bounds(true);\n+  \/\/ global_used is unaffected by this transfer\n+\n+  \/\/ No need to adjust ranges because humongous regions are not allocatable\n+}\n+\n+void ShenandoahFreeSet::transfer_empty_regions_from_to(ShenandoahFreeSetPartitionId source,\n+                                                       ShenandoahFreeSetPartitionId dest,\n+                                                       size_t num_regions) {\n+  assert(dest != source, \"precondition\");\n+  shenandoah_assert_heaplocked();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t transferred_regions = 0;\n+  size_t used_transfer = 0;\n+  index_type source_low_idx = _partitions.max();\n+  index_type source_high_idx = -1;\n+  index_type dest_low_idx = _partitions.max();\n+  index_type dest_high_idx = -1;\n+  ShenandoahLeftRightIterator iterator(&_partitions, source, true);\n+  for (index_type idx = iterator.current(); transferred_regions < num_regions && iterator.has_next(); idx = iterator.next()) {\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      if (idx < source_low_idx) {\n+        source_low_idx = idx;\n+      }\n+      if (idx > source_high_idx) {\n+        source_high_idx = idx;\n+      }\n+      if (idx < dest_low_idx) {\n+        dest_low_idx = idx;\n+      }\n+      if (idx > dest_high_idx) {\n+        dest_high_idx = idx;\n+      }\n+      used_transfer += _partitions.move_from_partition_to_partition_with_deferred_accounting(idx, source, dest, region_size_bytes);\n+      transferred_regions++;\n+    }\n+  }\n+\n+  \/\/ All transferred regions are empty.\n+  assert(used_transfer == 0, \"empty regions should have no used\");\n+  _partitions.expand_interval_if_range_modifies_either_boundary(dest, dest_low_idx,\n+                                                                dest_high_idx, dest_low_idx, dest_high_idx);\n+  _partitions.shrink_interval_if_range_modifies_either_boundary(source, source_low_idx, source_high_idx,\n+                                                                transferred_regions);\n+\n+  _partitions.decrease_region_counts(source, transferred_regions);\n+  _partitions.decrease_empty_region_counts(source, transferred_regions);\n+  _partitions.decrease_capacity(source, transferred_regions * region_size_bytes);\n+\n+  _partitions.increase_capacity(dest, transferred_regions * region_size_bytes);\n+  _partitions.increase_region_counts(dest, transferred_regions);\n+  _partitions.increase_empty_region_counts(dest, transferred_regions);\n+\n+  \/\/ Since only empty regions are transferred, no need to recompute_total_used()\n+  if (source == ShenandoahFreeSetPartitionId::OldCollector) {\n+    assert((dest == ShenandoahFreeSetPartitionId::Collector) || (dest == ShenandoahFreeSetPartitionId::Mutator), \"sanity\");\n+    _total_young_regions += transferred_regions;\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                               \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ true,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  } else {\n+    assert((source == ShenandoahFreeSetPartitionId::Collector) || (source == ShenandoahFreeSetPartitionId::Mutator), \"sanity\");\n+    if (dest == ShenandoahFreeSetPartitionId::OldCollector) {\n+      _total_young_regions -= transferred_regions;\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                                 \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                                 \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ true,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+    } else {\n+      assert((dest == ShenandoahFreeSetPartitionId::Collector) || (dest == ShenandoahFreeSetPartitionId::Mutator), \"sanity\");\n+      \/\/ No adjustments to total_young_regions if transferring within young\n+      recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                                 \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                                 \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                                 \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                                 \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+    }\n+  }\n+  _partitions.assert_bounds(true);\n+}\n+\n@@ -1548,0 +2471,5 @@\n+  size_t used_transfer = 0;\n+  index_type collector_low_idx = _partitions.max();\n+  index_type collector_high_idx = -1;\n+  index_type mutator_low_idx = _partitions.max();\n+  index_type mutator_high_idx = -1;\n@@ -1549,1 +2477,1 @@\n-  for (idx_t idx = iterator.current(); transferred_regions < max_xfer_regions && iterator.has_next(); idx = iterator.next()) {\n+  for (index_type idx = iterator.current(); transferred_regions < max_xfer_regions && iterator.has_next(); idx = iterator.next()) {\n@@ -1552,1 +2480,15 @@\n-      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      if (idx < collector_low_idx) {\n+        collector_low_idx = idx;\n+      }\n+      if (idx > collector_high_idx) {\n+        collector_high_idx = idx;\n+      }\n+      if (idx < mutator_low_idx) {\n+        mutator_low_idx = idx;\n+      }\n+      if (idx > mutator_high_idx) {\n+        mutator_high_idx = idx;\n+      }\n+      used_transfer += _partitions.move_from_partition_to_partition_with_deferred_accounting(idx, which_collector,\n+                                                                                             ShenandoahFreeSetPartitionId::Mutator,\n+                                                                                             region_size_bytes);\n@@ -1557,0 +2499,30 @@\n+  \/\/ All transferred regions are empty.\n+  assert(used_transfer == 0, \"empty regions should have no used\");\n+  _partitions.expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::Mutator, mutator_low_idx,\n+                                                                mutator_high_idx, mutator_low_idx, mutator_high_idx);\n+  _partitions.shrink_interval_if_range_modifies_either_boundary(which_collector, collector_low_idx, collector_high_idx,\n+                                                                transferred_regions);\n+\n+  _partitions.decrease_region_counts(which_collector, transferred_regions);\n+  _partitions.decrease_empty_region_counts(which_collector, transferred_regions);\n+  _partitions.decrease_capacity(which_collector, transferred_regions * region_size_bytes);\n+\n+  _partitions.increase_capacity(ShenandoahFreeSetPartitionId::Mutator, transferred_regions * region_size_bytes);\n+  _partitions.increase_region_counts(ShenandoahFreeSetPartitionId::Mutator, transferred_regions);\n+  _partitions.increase_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator, transferred_regions);\n+\n+  if (which_collector == ShenandoahFreeSetPartitionId::OldCollector) {\n+    _total_young_regions += transferred_regions;\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                               \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  } else {\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ true,\n+                               \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+  }\n+  _partitions.assert_bounds(true);\n@@ -1561,3 +2533,3 @@\n-size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n-                                                                                       size_t max_xfer_regions,\n-                                                                                       size_t& bytes_transferred) {\n+size_t ShenandoahFreeSet::\n+transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                             size_t max_xfer_regions, size_t& bytes_transferred) {\n@@ -1565,0 +2537,1 @@\n+  size_t region_size_bytes = _partitions.region_size_bytes();\n@@ -1566,0 +2539,6 @@\n+  size_t used_transfer = 0;\n+  index_type collector_low_idx = _partitions.max();\n+  index_type collector_high_idx = -1;\n+  index_type mutator_low_idx = _partitions.max();\n+  index_type mutator_high_idx = -1;\n+\n@@ -1567,1 +2546,1 @@\n-  for (idx_t idx = iterator.current(); transferred_regions < max_xfer_regions && iterator.has_next(); idx = iterator.next()) {\n+  for (index_type idx = iterator.current(); transferred_regions < max_xfer_regions && iterator.has_next(); idx = iterator.next()) {\n@@ -1570,1 +2549,16 @@\n-      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      if (idx < collector_low_idx) {\n+        collector_low_idx = idx;\n+      }\n+      if (idx > collector_high_idx) {\n+        collector_high_idx = idx;\n+      }\n+      if (idx < mutator_low_idx) {\n+        mutator_low_idx = idx;\n+      }\n+      if (idx > mutator_high_idx) {\n+        mutator_high_idx = idx;\n+      }\n+      assert (ac < region_size_bytes, \"Move empty regions with different function\");\n+      used_transfer += _partitions.move_from_partition_to_partition_with_deferred_accounting(idx, which_collector,\n+                                                                                             ShenandoahFreeSetPartitionId::Mutator,\n+                                                                                             ac);\n@@ -1575,0 +2569,35 @@\n+  \/\/ _empty_region_counts is unaffected, because we transfer only non-empty regions here.\n+\n+  _partitions.decrease_used(which_collector, used_transfer);\n+  _partitions.expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                mutator_low_idx, mutator_high_idx, _partitions.max(), -1);\n+  _partitions.shrink_interval_if_range_modifies_either_boundary(which_collector, collector_low_idx, collector_high_idx,\n+                                                                transferred_regions);\n+\n+  _partitions.decrease_region_counts(which_collector, transferred_regions);\n+  _partitions.decrease_capacity(which_collector, transferred_regions * region_size_bytes);\n+  _partitions.increase_capacity(ShenandoahFreeSetPartitionId::Mutator, transferred_regions * region_size_bytes);\n+  _partitions.increase_region_counts(ShenandoahFreeSetPartitionId::Mutator, transferred_regions);\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, used_transfer);\n+\n+  if (which_collector == ShenandoahFreeSetPartitionId::OldCollector) {\n+    _total_young_regions += transferred_regions;\n+  }\n+  \/\/ _total_global_regions unaffected by transfer\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+  \/\/ All transfers are affiliated\n+  if (which_collector == ShenandoahFreeSetPartitionId::OldCollector) {\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollectorEmptiesChanged *\/ false,\n+                               \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ false, \/* OldCollectorSizeChanged *\/ true,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+  } else {\n+    recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollecteorEmptiesChanged *\/true,\n+                               \/* OldCollectorEmptiesChanged *\/ false, \/* MutatorSizeChanged *\/ true,\n+                               \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ false,\n+                               \/* AffiliatedChangesAreYoungNeutral *\/ true, \/* AffiliatedChangesAreGlobalNeutral *\/ true,\n+                               \/* UnaffiliatedChangesAreYoungNeutral *\/ true>();\n+  }\n+  _partitions.assert_bounds(true);\n@@ -1601,3 +2630,0 @@\n-    if (old_collector_regions > 0) {\n-      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n-    }\n@@ -1623,1 +2649,0 @@\n-\n@@ -1625,1 +2650,1 @@\n-void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_trashed_regions, size_t &old_trashed_regions,\n@@ -1628,2 +2653,0 @@\n-  \/\/ This resets all state information, removing all regions from all sets.\n-  clear();\n@@ -1634,29 +2657,2 @@\n-  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n-}\n-\n-void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n-  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n-    ShenandoahOldGeneration* old_gen = heap->old_generation();\n-    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-\n-    size_t original_old_capacity = old_gen->max_capacity();\n-    size_t new_old_capacity = old_region_count * region_size_bytes;\n-    size_t new_young_capacity = young_region_count * region_size_bytes;\n-    old_gen->set_capacity(new_old_capacity);\n-    young_gen->set_capacity(new_young_capacity);\n-\n-    if (new_old_capacity > original_old_capacity) {\n-      size_t region_count = (new_old_capacity - original_old_capacity) \/ region_size_bytes;\n-      log_info(gc, ergo)(\"Transfer %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n-                         region_count, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_old_capacity));\n-    } else if (new_old_capacity < original_old_capacity) {\n-      size_t region_count = (original_old_capacity - new_old_capacity) \/ region_size_bytes;\n-      log_info(gc, ergo)(\"Transfer %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n-                         region_count, old_gen->name(), young_gen->name(), PROPERFMTARGS(new_young_capacity));\n-    }\n-    \/\/ This balances generations, so clear any pending request to balance.\n-    old_gen->set_region_balance(0);\n-  }\n+  find_regions_with_alloc_capacity(young_trashed_regions, old_trashed_regions,\n+                                   first_old_region, last_old_region, old_region_count);\n@@ -1665,1 +2661,1 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+void ShenandoahFreeSet::finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t old_region_count,\n@@ -1671,1 +2667,1 @@\n-    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+    compute_young_and_old_reserves(young_trashed_regions, old_trashed_regions, have_evacuation_reserves,\n@@ -1678,1 +2674,1 @@\n-  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ Move some of the mutator regions into the Collector and OldCollector partitions in order to satisfy\n@@ -1680,3 +2676,5 @@\n-  reserve_regions(young_reserve, old_reserve, old_region_count);\n-  size_t young_region_count = _heap->num_regions() - old_region_count;\n-  establish_generation_sizes(young_region_count, old_region_count);\n+  size_t young_used_regions, old_used_regions, young_used_bytes, old_used_bytes;\n+  reserve_regions(young_reserve, old_reserve, old_region_count, young_used_regions, old_used_regions,\n+                  young_used_bytes, old_used_bytes);\n+  _total_young_regions = _heap->num_regions() - old_region_count;\n+  _total_global_regions = _heap->num_regions();\n@@ -1684,1 +2682,1 @@\n-  _partitions.assert_bounds();\n+  _partitions.assert_bounds(true);\n@@ -1688,1 +2686,14 @@\n-void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+\/**\n+ * Set young_reserve_result and old_reserve_result to the number of bytes that we desire to set aside to hold the\n+ * results of evacuation to young and old collector spaces respectively during the next evacuation phase.  Overwrite\n+ * old_generation region balance in case the original value is incompatible with the current reality.\n+ *\n+ * These values are determined by how much memory is currently available within each generation, which is\n+ * represented by:\n+ *  1. Memory currently available within old and young\n+ *  2. Trashed regions currently residing in young and old, which will become available momentarily\n+ *  3. The value of old_generation->get_region_balance() which represents the number of regions that we plan\n+ *     to transfer from old generation to young generation.  Prior to each invocation of compute_young_and_old_reserves(),\n+ *     this value should computed by ShenandoahGenerationalHeap::compute_old_generation_balance().\n+ *\/\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_trashed_regions, size_t old_trashed_regions,\n@@ -1692,0 +2703,1 @@\n+  shenandoah_assert_heaplocked();\n@@ -1693,1 +2705,0 @@\n-\n@@ -1702,2 +2713,3 @@\n-  old_unaffiliated_regions += old_cset_regions;\n-  young_unaffiliated_regions += young_cset_regions;\n+  old_unaffiliated_regions += old_trashed_regions;\n+  old_available += old_trashed_regions * region_size_bytes;\n+  young_unaffiliated_regions += young_trashed_regions;\n@@ -1706,2 +2718,3 @@\n-  \/\/ The generation region transfers take place after we rebuild.\n-  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  \/\/ The generation region transfers take place after we rebuild.  old_region_balance represents number of regions\n+  \/\/ to transfer from old to young.\n+  ssize_t old_region_balance = old_generation->get_region_balance();\n@@ -1711,1 +2724,3 @@\n-      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions),\n+             \"Cannot transfer %zd regions that are affiliated (old_trashed: %zu, old_unaffiliated: %zu)\",\n+             old_region_balance, old_trashed_regions, old_unaffiliated_regions);\n@@ -1713,1 +2728,2 @@\n-      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions),\n+             \"Cannot transfer regions that are affiliated\");\n@@ -1734,3 +2750,12 @@\n-    assert(old_reserve_result <= old_available,\n-           \"Cannot reserve (%zu + %zu) more OLD than is available: %zu\",\n-           promoted_reserve, old_evac_reserve, old_available);\n+    if (old_reserve_result > old_available) {\n+      \/\/ Try to transfer memory from young to old.\n+      size_t old_deficit = old_reserve_result - old_available;\n+      size_t old_region_deficit = (old_deficit + region_size_bytes - 1) \/ region_size_bytes;\n+      if (young_unaffiliated_regions < old_region_deficit) {\n+        old_region_deficit = young_unaffiliated_regions;\n+      }\n+      young_unaffiliated_regions -= old_region_deficit;\n+      old_unaffiliated_regions += old_region_deficit;\n+      old_region_balance -= old_region_deficit;\n+      old_generation->set_region_balance(old_region_balance);\n+    }\n@@ -1766,7 +2791,4 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n-  for (size_t i = _heap->num_regions(); i > 0; i--) {\n-    size_t idx = i - 1;\n-    ShenandoahHeapRegion* r = _heap->get_region(idx);\n-    if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n-      continue;\n-    }\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count,\n+                                        size_t &young_used_regions, size_t &old_used_regions,\n+                                        size_t &young_used_bytes, size_t &old_used_bytes) {\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -1774,3 +2796,4 @@\n-    size_t ac = alloc_capacity(r);\n-    assert (ac > 0, \"Membership in free set implies has capacity\");\n-    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n+  young_used_regions = 0;\n+  old_used_regions = 0;\n+  young_used_bytes = 0;\n+  old_used_bytes = 0;\n@@ -1778,2 +2801,4 @@\n-    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n-    bool move_to_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector) < to_reserve;\n+  index_type mutator_low_idx = _partitions.max();\n+  index_type mutator_high_idx = -1;\n+  index_type mutator_empty_low_idx = _partitions.max();\n+  index_type mutator_empty_high_idx = -1;\n@@ -1781,4 +2806,19 @@\n-    if (!move_to_collector && !move_to_old_collector) {\n-      \/\/ We've satisfied both to_reserve and to_reserved_old\n-      break;\n-    }\n+  index_type collector_low_idx = _partitions.max();\n+  index_type collector_high_idx = -1;\n+  index_type collector_empty_low_idx = _partitions.max();\n+  index_type collector_empty_high_idx = -1;\n+\n+  index_type old_collector_low_idx = _partitions.max();\n+  index_type old_collector_high_idx = -1;\n+  index_type old_collector_empty_low_idx = _partitions.max();\n+  index_type old_collector_empty_high_idx = -1;\n+\n+  size_t used_to_collector = 0;\n+  size_t used_to_old_collector = 0;\n+  size_t regions_to_collector = 0;\n+  size_t regions_to_old_collector = 0;\n+  size_t empty_regions_to_collector = 0;\n+  size_t empty_regions_to_old_collector = 0;\n+\n+  size_t old_collector_available = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector);;\n+  size_t collector_available = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector);\n@@ -1786,9 +2826,85 @@\n-    if (move_to_old_collector) {\n-      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n-      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n-      \/\/ be collected in the near future.\n-      if (r->is_trash() || !r->is_affiliated()) {\n-        \/\/ OLD regions that have available memory are already in the old_collector free set.\n-        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n-                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n-        log_trace(gc, free)(\"  Shifting region %zu from mutator_free to old_collector_free\", idx);\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    index_type idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n+      \/\/ Note: trashed regions have region_size_bytes alloc capacity.\n+      size_t ac = alloc_capacity(r);\n+      assert (ac > 0, \"Membership in free set implies has capacity\");\n+      assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n+\n+      bool move_to_old_collector = old_collector_available < to_reserve_old;\n+      bool move_to_collector = collector_available < to_reserve;\n+\n+      if (move_to_old_collector) {\n+        \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+        \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+        \/\/ be collected in the near future.\n+        if (r->is_trash() || !r->is_affiliated()) {\n+          \/\/ OLD regions that have available memory are already in the old_collector free set.\n+          assert(r->is_empty() || r->is_trash(), \"Not affiliated implies region %zu is empty\", r->index());\n+          if (idx < old_collector_low_idx) {\n+            old_collector_low_idx = idx;\n+          }\n+          if (idx > old_collector_high_idx) {\n+            old_collector_high_idx = idx;\n+          }\n+          if (idx < old_collector_empty_low_idx) {\n+            old_collector_empty_low_idx = idx;\n+          }\n+          if (idx > old_collector_empty_high_idx) {\n+            old_collector_empty_high_idx = idx;\n+          }\n+          used_to_old_collector +=\n+            _partitions.move_from_partition_to_partition_with_deferred_accounting(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                                                  ShenandoahFreeSetPartitionId::OldCollector, ac);\n+          old_collector_available += ac;\n+          regions_to_old_collector++;\n+          empty_regions_to_old_collector++;\n+\n+          log_trace(gc, free)(\"  Shifting region %zu from mutator_free to old_collector_free\", idx);\n+          log_trace(gc, free)(\"  Shifted Mutator range [%zd, %zd],\"\n+                              \"  Old Collector range [%zd, %zd]\",\n+                              _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                              _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                              _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                              _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+          old_region_count++;\n+          continue;\n+        }\n+      }\n+\n+      if (move_to_collector) {\n+        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+        \/\/ they were entirely empty.  This has the effect of causing new Mutator allocation to reside next to objects\n+        \/\/ that have already survived at least one GC, mixing ephemeral with longer-lived objects in the same region.\n+        \/\/ Any objects that have survived a GC are less likely to immediately become garbage, so a region that contains\n+        \/\/ survivor objects is less likely to be selected for the collection set.  This alternative implementation allows\n+        \/\/ survivor regions to continue accumulating other survivor objects, and makes it more likely that ephemeral objects\n+        \/\/ occupy regions comprised entirely of ephemeral objects.  These regions are highly likely to be included in the next\n+        \/\/ collection set, and they are easily evacuated because they have low density of live objects.\n+        if (idx < collector_low_idx) {\n+          collector_low_idx = idx;\n+        }\n+        if (idx > collector_high_idx) {\n+          collector_high_idx = idx;\n+        }\n+        if (ac == region_size_bytes) {\n+          if (idx < collector_empty_low_idx) {\n+            collector_empty_low_idx = idx;\n+          }\n+          if (idx > collector_empty_high_idx) {\n+            collector_empty_high_idx = idx;\n+          }\n+          empty_regions_to_collector++;\n+        }\n+        used_to_collector +=\n+          _partitions.move_from_partition_to_partition_with_deferred_accounting(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                                                ShenandoahFreeSetPartitionId::Collector, ac);\n+        collector_available += ac;\n+        regions_to_collector++;\n+        if (ac != region_size_bytes) {\n+          young_used_regions++;\n+          young_used_bytes = region_size_bytes - ac;\n+        }\n+\n+        log_trace(gc, free)(\"  Shifting region %zu from mutator_free to collector_free\", idx);\n@@ -1796,1 +2912,1 @@\n-                            \"  Old Collector range [%zd, %zd]\",\n+                            \"  Collector range [%zd, %zd]\",\n@@ -1799,4 +2915,2 @@\n-                            _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n-                            _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n-\n-        old_region_count++;\n+                            _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                            _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -1805,20 +2919,0 @@\n-    }\n-\n-    if (move_to_collector) {\n-      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n-      \/\/ they were entirely empty.  This has the effect of causing new Mutator allocation to reside next to objects\n-      \/\/ that have already survived at least one GC, mixing ephemeral with longer-lived objects in the same region.\n-      \/\/ Any objects that have survived a GC are less likely to immediately become garbage, so a region that contains\n-      \/\/ survivor objects is less likely to be selected for the collection set.  This alternative implementation allows\n-      \/\/ survivor regions to continue accumulating other survivor objects, and makes it more likely that ephemeral objects\n-      \/\/ occupy regions comprised entirely of ephemeral objects.  These regions are highly likely to be included in the next\n-      \/\/ collection set, and they are easily evacuated because they have low density of live objects.\n-      _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n-                                                   ShenandoahFreeSetPartitionId::Collector, ac);\n-      log_trace(gc, free)(\"  Shifting region %zu from mutator_free to collector_free\", idx);\n-      log_trace(gc, free)(\"  Shifted Mutator range [%zd, %zd],\"\n-                          \"  Collector range [%zd, %zd]\",\n-                          _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n-                          _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n-                          _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n-                          _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -1826,0 +2920,60 @@\n+      \/\/ Mutator region is not moved to Collector or OldCollector. Still, do the accounting.\n+      if (idx < mutator_low_idx) {\n+        mutator_low_idx = idx;\n+      }\n+      if (idx > mutator_high_idx) {\n+        mutator_high_idx = idx;\n+      }\n+      if ((ac == region_size_bytes) && (idx < mutator_empty_low_idx)) {\n+        mutator_empty_low_idx = idx;\n+      }\n+      if ((ac == region_size_bytes) && (idx > mutator_empty_high_idx)) {\n+        mutator_empty_high_idx = idx;\n+      }\n+      if (ac != region_size_bytes) {\n+        young_used_regions++;\n+        young_used_bytes += region_size_bytes - ac;\n+      }\n+    } else {\n+      \/\/ Region is not in Mutator partition. Do the accounting.\n+      ShenandoahFreeSetPartitionId p = _partitions.membership(idx);\n+      size_t ac = alloc_capacity(r);\n+      assert(ac != region_size_bytes, \"Empty regions should be in Mutator partion at entry to reserve_regions\");\n+      if (p == ShenandoahFreeSetPartitionId::Collector) {\n+        if (ac != region_size_bytes) {\n+          young_used_regions++;\n+          young_used_bytes = region_size_bytes - ac;\n+        }\n+        \/\/ else, unaffiliated region has no used\n+      } else if (p == ShenandoahFreeSetPartitionId::OldCollector) {\n+        if (ac != region_size_bytes) {\n+          old_used_regions++;\n+          old_used_bytes = region_size_bytes - ac;\n+        }\n+        \/\/ else, unaffiliated region has no used\n+      } else if (p == ShenandoahFreeSetPartitionId::NotFree) {\n+        \/\/ This region has been retired\n+        if (r->is_old()) {\n+          old_used_regions++;\n+          old_used_bytes += region_size_bytes - ac;\n+        } else {\n+          assert(r->is_young(), \"Retired region should be old or young\");\n+          young_used_regions++;\n+          young_used_bytes += region_size_bytes - ac;\n+        }\n+      } else {\n+        assert(p == ShenandoahFreeSetPartitionId::OldCollector, \"Not mutator and not NotFree, so must be OldCollector\");\n+        assert(!r->is_empty(), \"Empty regions should be in Mutator partition at entry to reserve_regions\");\n+        if (idx < old_collector_low_idx) {\n+          old_collector_low_idx = idx;\n+        }\n+        if (idx > old_collector_high_idx) {\n+          old_collector_high_idx = idx;\n+        }\n+        if (idx < old_collector_empty_low_idx) {\n+          old_collector_empty_low_idx = idx;\n+        }\n+        if (idx > old_collector_empty_high_idx) {\n+          old_collector_empty_high_idx = idx;\n+        }\n+      }\n@@ -1829,0 +2983,41 @@\n+  _partitions.decrease_used(ShenandoahFreeSetPartitionId::Mutator, used_to_old_collector + used_to_collector);\n+  _partitions.decrease_region_counts(ShenandoahFreeSetPartitionId::Mutator, regions_to_old_collector + regions_to_collector);\n+  _partitions.decrease_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator,\n+                                           empty_regions_to_old_collector + empty_regions_to_collector);\n+  \/\/ decrease_capacity() also decreases available\n+  _partitions.decrease_capacity(ShenandoahFreeSetPartitionId::Mutator,\n+                                (regions_to_old_collector + regions_to_collector) * region_size_bytes);\n+  \/\/ increase_capacity() also increases available\n+  _partitions.increase_capacity(ShenandoahFreeSetPartitionId::Collector, regions_to_collector * region_size_bytes);\n+  _partitions.increase_region_counts(ShenandoahFreeSetPartitionId::Collector, regions_to_collector);\n+  _partitions.increase_empty_region_counts(ShenandoahFreeSetPartitionId::Collector, empty_regions_to_collector);\n+  \/\/ increase_capacity() also increases available\n+  _partitions.increase_capacity(ShenandoahFreeSetPartitionId::OldCollector, regions_to_old_collector * region_size_bytes);\n+  _partitions.increase_region_counts(ShenandoahFreeSetPartitionId::OldCollector, regions_to_old_collector);\n+  _partitions.increase_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector, empty_regions_to_old_collector);\n+\n+  if (used_to_collector > 0) {\n+    _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, used_to_collector);\n+  }\n+\n+  if (used_to_old_collector > 0) {\n+    _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, used_to_old_collector);\n+  }\n+\n+  _partitions.expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::Collector,\n+                                                                collector_low_idx, collector_high_idx,\n+                                                                collector_empty_low_idx, collector_empty_high_idx);\n+  _partitions.expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::OldCollector,\n+                                                                old_collector_low_idx, old_collector_high_idx,\n+                                                                old_collector_empty_low_idx, old_collector_empty_high_idx);\n+  _partitions.establish_interval(ShenandoahFreeSetPartitionId::Mutator,\n+                                 mutator_low_idx, mutator_high_idx, mutator_empty_low_idx, mutator_empty_high_idx);\n+\n+  recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                       \/* UsedByCollectorChanged *\/ true, \/* UsedByOldCollectorChanged *\/ true>();\n+  recompute_total_affiliated<\/* MutatorEmptiesChanged *\/ true, \/* CollecteorEmptiesChanged *\/true,\n+                             \/* OldCollectorEmptiesChanged *\/ true, \/* MutatorSizeChanged *\/ true,\n+                             \/* CollectorSizeChanged *\/ true, \/* OldCollectorSizeChanged *\/ true,\n+                             \/* AffiliatedChangesAreYoungNeutral *\/ false, \/* AffiliatedChangesAreGlobalNeutral *\/ false,\n+                             \/* UnaffiliatedChangesAreYoungNeutral *\/ false>();\n+  _partitions.assert_bounds(true);\n@@ -1847,3 +3042,3 @@\n-  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n-  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n-  idx_t middle = (left_idx + right_idx) \/ 2;\n+  index_type left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  index_type right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  index_type middle = (left_idx + right_idx) \/ 2;\n@@ -1853,1 +3048,1 @@\n-  for (idx_t index = left_idx; index < middle; index++) {\n+  for (index_type index = left_idx; index < middle; index++) {\n@@ -1859,1 +3054,1 @@\n-  for (idx_t index = middle; index <= right_idx; index++) {\n+  for (index_type index = middle; index <= right_idx; index++) {\n@@ -1961,1 +3156,1 @@\n-      idx_t last_idx = 0;\n+      index_type last_idx = 0;\n@@ -1969,0 +3164,1 @@\n+      size_t total_trashed_free = 0;\n@@ -1970,1 +3166,1 @@\n-      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+      for (index_type idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n@@ -1976,1 +3172,3 @@\n-          if (r->is_empty()) {\n+          size_t used_in_region = r->used();\n+          if (r->is_empty() || r->is_trash()) {\n+            used_in_region = 0;\n@@ -1986,1 +3184,1 @@\n-          total_used += r->used();\n+          total_used += used_in_region;\n@@ -1994,0 +3192,2 @@\n+      \/\/ capacity() is capacity of mutator\n+      \/\/ used() is used of mutator\n@@ -1995,1 +3195,0 @@\n-\n@@ -1999,1 +3198,1 @@\n-      assert(free == total_free, \"Free memory should match\");\n+      assert(free == total_free, \"Free memory (%zu) should match calculated memory (%zu)\", free, total_free);\n@@ -2033,1 +3232,1 @@\n-      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n+      for (index_type idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n@@ -2054,1 +3253,1 @@\n-      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+      for (index_type idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n@@ -2072,0 +3271,21 @@\n+void ShenandoahFreeSet::decrease_humongous_waste_for_regular_bypass(ShenandoahHeapRegion*r, size_t waste) {\n+  shenandoah_assert_heaplocked();\n+  assert(_partitions.membership(r->index()) == ShenandoahFreeSetPartitionId::NotFree, \"Humongous regions should be NotFree\");\n+  ShenandoahFreeSetPartitionId p =\n+    r->is_old()? ShenandoahFreeSetPartitionId::OldCollector: ShenandoahFreeSetPartitionId::Mutator;\n+  _partitions.decrease_humongous_waste(p, waste);\n+  if (waste >= PLAB::min_size() * HeapWordSize) {\n+    _partitions.decrease_used(p, waste);\n+    _partitions.unretire_to_partition(r, p);\n+    if (r->is_old()) {\n+      recompute_total_used<\/* UsedByMutatorChanged *\/ false,\n+                           \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ true>();\n+    } else {\n+      recompute_total_used<\/* UsedByMutatorChanged *\/ true,\n+                           \/* UsedByCollectorChanged *\/ false, \/* UsedByOldCollectorChanged *\/ false>();\n+    }\n+  }\n+  _total_humongous_waste -= waste;\n+}\n+\n+\n@@ -2101,1 +3321,1 @@\n-  for (idx_t index = mutator.current(); mutator.has_next(); index = mutator.next()) {\n+  for (index_type index = mutator.current(); mutator.has_next(); index = mutator.next()) {\n@@ -2107,1 +3327,1 @@\n-  for (idx_t index = collector.current(); collector.has_next(); index = collector.next()) {\n+  for (index_type index = collector.current(); collector.has_next(); index = collector.next()) {\n@@ -2113,1 +3333,1 @@\n-    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+    for (index_type index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n@@ -2127,1 +3347,1 @@\n-  for (idx_t index = iterator.current(); iterator.has_next(); index = iterator.next()) {\n+  for (index_type index = iterator.current(); iterator.has_next(); index = iterator.next()) {\n@@ -2143,1 +3363,1 @@\n-  idx_t last_idx = 0;\n+  index_type last_idx = 0;\n@@ -2149,1 +3369,1 @@\n-  for (idx_t index = iterator.current(); iterator.has_next(); index = iterator.next()) {\n+  for (index_type index = iterator.current(); iterator.has_next(); index = iterator.next()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1614,"deletions":394,"binary":false,"changes":2008,"status":"modified"},{"patch":"@@ -38,2 +38,6 @@\n-                                \/\/    available memory is reserved for old evacuations and for promotions..\n-  NotFree                       \/\/ Region is in no free set: it has no available memory\n+                                \/\/    available memory is reserved for old evacuations and for promotions.\n+  NotFree                       \/\/ Region is in no free set: it has no available memory.  Consult region affiliation\n+                                \/\/    to determine whether this retired region is young or old.  If young, the region\n+                                \/\/    is considered to be part of the Mutator partition.  (When we retire from the\n+                                \/\/    Collector partition, we decrease total_region_count for Collector and increaese\n+                                \/\/    for Mutator, making similar adjustments to used (net impact on available is neutral).\n@@ -48,1 +52,1 @@\n-private:\n+public:\n@@ -55,1 +59,2 @@\n-  const ssize_t _max;           \/\/ The maximum number of heap regions\n+private:\n+  const index_type _max;           \/\/ The maximum number of heap regions\n@@ -60,1 +65,0 @@\n-\n@@ -65,2 +69,2 @@\n-  ssize_t _leftmosts[UIntNumPartitions];\n-  ssize_t _rightmosts[UIntNumPartitions];\n+  index_type _leftmosts[UIntNumPartitions];\n+  index_type _rightmosts[UIntNumPartitions];\n@@ -72,9 +76,26 @@\n-  ssize_t _leftmosts_empty[UIntNumPartitions];\n-  ssize_t _rightmosts_empty[UIntNumPartitions];\n-\n-  \/\/ For each partition p, _capacity[p] represents the total amount of memory within the partition at the time\n-  \/\/ of the most recent rebuild, _used[p] represents the total amount of memory that has been allocated within this\n-  \/\/ partition (either already allocated as of the rebuild, or allocated since the rebuild).  _capacity[p] and _used[p]\n-  \/\/ are denoted in bytes.  Note that some regions that had been assigned to a particular partition at rebuild time\n-  \/\/ may have been retired following the rebuild.  The tallies for these regions are still reflected in _capacity[p]\n-  \/\/ and _used[p], even though the region may have been removed from the free set.\n+  index_type _leftmosts_empty[UIntNumPartitions];\n+  index_type _rightmosts_empty[UIntNumPartitions];\n+\n+  \/\/ For each partition p:\n+  \/\/  _capacity[p] represents the total amount of memory within the partition, including retired regions, as adjusted\n+  \/\/                       by transfers of memory between partitions\n+  \/\/  _used[p] represents the total amount of memory that has been allocated within this partition (either already\n+  \/\/                       allocated as of the rebuild, or allocated since the rebuild).\n+  \/\/  _available[p] represents the total amount of memory that can be allocated within partition p, calculated from\n+  \/\/                       _capacity[p] minus _used[p], where the difference is computed and assigned under heap lock\n+  \/\/\n+  \/\/  _region_counts[p] represents the number of regions associated with the partition which currently have available memory.\n+  \/\/                       When a region is retired from partition p, _region_counts[p] is decremented.\n+  \/\/  total_region_counts[p] is _capacity[p] \/ RegionSizeBytes.\n+  \/\/  _empty_region_counts[p] is number of regions associated with p which are entirely empty\n+  \/\/\n+  \/\/ capacity and used values are expressed in bytes.\n+  \/\/\n+  \/\/ When a region is retired, the used[p] is increased to account for alignment waste.  capacity is unaffected.\n+  \/\/\n+  \/\/ When a region is \"flipped\", we adjust capacities and region counts for original and destination partitions.  We also\n+  \/\/ adjust used values when flipping from mutator to collector.  Flip to old collector does not need to adjust used because\n+  \/\/ only empty regions can be flipped to old collector.\n+  \/\/\n+  \/\/ All memory quantities (capacity, available, used) are represented in bytes.\n+\n@@ -82,0 +103,1 @@\n+\n@@ -84,0 +106,11 @@\n+\n+  \/\/ Measured in bytes.\n+  size_t _allocated_since_gc_start[UIntNumPartitions];\n+\n+  \/\/ Some notes:\n+  \/\/  total_region_counts[p] is _capacity[p] \/ region_size_bytes\n+  \/\/  retired_regions[p] is total_region_counts[p] - _region_counts[p]\n+  \/\/  _empty_region_counts[p] <= _region_counts[p] <= total_region_counts[p]\n+  \/\/  affiliated regions is total_region_counts[p] - empty_region_counts[p]\n+  \/\/  used_regions is affilaited_regions * region_size_bytes\n+  \/\/  _available[p] is _capacity[p] - _used[p]\n@@ -85,0 +118,5 @@\n+  size_t _empty_region_counts[UIntNumPartitions];\n+\n+  \/\/ Humongous waste, in bytes, can exist in Mutator partition for recently allocated humongous objects\n+  \/\/ and in OldCollector partition for humongous objects that have been promoted in place.\n+  size_t _humongous_waste[UIntNumPartitions];\n@@ -90,8 +128,0 @@\n-  \/\/ Shrink the intervals associated with partition when region idx is removed from this free set\n-  inline void shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx);\n-\n-  \/\/ Shrink the intervals associated with partition when regions low_idx through high_idx inclusive are removed from this free set\n-  inline void shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n-                                                                ssize_t low_idx, ssize_t high_idx);\n-  inline void expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx, size_t capacity);\n-\n@@ -104,2 +134,2 @@\n-  void dump_bitmap_row(ssize_t region_idx) const;\n-  void dump_bitmap_range(ssize_t start_region_idx, ssize_t end_region_idx) const;\n+  void dump_bitmap_row(index_type region_idx) const;\n+  void dump_bitmap_range(index_type start_region_idx, index_type end_region_idx) const;\n@@ -114,0 +144,5 @@\n+  inline index_type max() const { return _max; }\n+\n+  \/\/ At initialization, reset OldCollector tallies\n+  void initialize_old_collector();\n+\n@@ -122,0 +157,7 @@\n+  \/\/ Clear the partition id for a particular region without adjusting interval bounds or usage\/capacity tallies\n+  inline void raw_clear_membership(size_t idx, ShenandoahFreeSetPartitionId p) {\n+    _membership[int(p)].clear_bit(idx);\n+  }\n+\n+  inline void one_region_is_no_longer_empty(ShenandoahFreeSetPartitionId partition);\n+\n@@ -125,3 +167,4 @@\n-  void establish_mutator_intervals(ssize_t mutator_leftmost, ssize_t mutator_rightmost,\n-                                   ssize_t mutator_leftmost_empty, ssize_t mutator_rightmost_empty,\n-                                   size_t mutator_region_count, size_t mutator_used);\n+  void establish_mutator_intervals(index_type mutator_leftmost, index_type mutator_rightmost,\n+                                   index_type mutator_leftmost_empty, index_type mutator_rightmost_empty,\n+                                   size_t total_mutator_regions, size_t empty_mutator_regions,\n+                                   size_t mutator_region_count, size_t mutator_used, size_t mutator_humongous_words_waste);\n@@ -132,3 +175,20 @@\n-  void establish_old_collector_intervals(ssize_t old_collector_leftmost, ssize_t old_collector_rightmost,\n-                                         ssize_t old_collector_leftmost_empty, ssize_t old_collector_rightmost_empty,\n-                                         size_t old_collector_region_count, size_t old_collector_used);\n+  void establish_old_collector_intervals(index_type old_collector_leftmost, index_type old_collector_rightmost,\n+                                         index_type old_collector_leftmost_empty, index_type old_collector_rightmost_empty,\n+                                         size_t total_old_collector_region_count, size_t old_collector_empty,\n+                                         size_t old_collector_regions, size_t old_collector_used,\n+                                         size_t old_collector_humongous_words_waste);\n+\n+  void establish_interval(ShenandoahFreeSetPartitionId partition, index_type low_idx, index_type high_idx,\n+                          index_type low_empty_idx, index_type high_empty_idx);\n+\n+  \/\/ Shrink the intervals associated with partition when region idx is removed from this free set\n+  inline void shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, index_type idx);\n+\n+  \/\/ Shrink the intervals associated with partition when regions low_idx through high_idx inclusive are removed from this free set\n+  void shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n+                                                         index_type low_idx, index_type high_idx, size_t num_regions);\n+\n+  void expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, index_type idx, size_t capacity);\n+  void expand_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n+                                                         index_type low_idx, index_type high_idx,\n+                                                         index_type low_empty_idx, index_type high_empty_idx);\n@@ -139,1 +199,2 @@\n-  void retire_from_partition(ShenandoahFreeSetPartitionId p, ssize_t idx, size_t used_bytes);\n+  \/\/ Return the number of waste bytes (if any).\n+  size_t retire_from_partition(ShenandoahFreeSetPartitionId p, index_type idx, size_t used_bytes);\n@@ -144,1 +205,3 @@\n-  void retire_range_from_partition(ShenandoahFreeSetPartitionId partition, ssize_t low_idx, ssize_t high_idx);\n+  void retire_range_from_partition(ShenandoahFreeSetPartitionId partition, index_type low_idx, index_type high_idx);\n+\n+  void unretire_to_partition(ShenandoahHeapRegion* region, ShenandoahFreeSetPartitionId which_partition);\n@@ -147,1 +210,1 @@\n-  void make_free(ssize_t idx, ShenandoahFreeSetPartitionId which_partition, size_t region_capacity);\n+  void make_free(index_type idx, ShenandoahFreeSetPartitionId which_partition, size_t region_capacity);\n@@ -149,3 +212,8 @@\n-  \/\/ Place region idx into free partition new_partition, adjusting used and capacity totals for the original and new partition\n-  \/\/ given that available bytes can still be allocated within this region.  Requires that idx is currently not NotFree.\n-  void move_from_partition_to_partition(ssize_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+  \/\/ Place region idx into free partition new_partition, not adjusting used and capacity totals for the original and new partition.\n+  \/\/ available represents bytes that can still be allocated within this region.  Requires that idx is currently not NotFree.\n+  size_t move_from_partition_to_partition_with_deferred_accounting(index_type idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                                   ShenandoahFreeSetPartitionId new_partition, size_t available);\n+\n+  \/\/ Place region idx into free partition new_partition, adjusting used and capacity totals for the original and new partition.\n+  \/\/ available represents bytes that can still be allocated within this region.  Requires that idx is currently not NotFree.\n+  void move_from_partition_to_partition(index_type idx, ShenandoahFreeSetPartitionId orig_partition,\n@@ -154,1 +222,4 @@\n-  const char* partition_membership_name(ssize_t idx) const;\n+  void transfer_used_capacity_from_to(ShenandoahFreeSetPartitionId from_partition, ShenandoahFreeSetPartitionId to_partition,\n+                                      size_t regions);\n+\n+  const char* partition_membership_name(index_type idx) const;\n@@ -157,1 +228,2 @@\n-  inline ssize_t find_index_of_next_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t start_index) const;\n+  inline index_type find_index_of_next_available_region(ShenandoahFreeSetPartitionId which_partition,\n+                                                        index_type start_index) const;\n@@ -160,1 +232,2 @@\n-  inline ssize_t find_index_of_previous_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t last_index) const;\n+  inline index_type find_index_of_previous_available_region(ShenandoahFreeSetPartitionId which_partition,\n+                                                            index_type last_index) const;\n@@ -163,2 +236,2 @@\n-  inline ssize_t find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n-                                                                 ssize_t start_index, size_t cluster_size) const;\n+  inline index_type find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                               index_type start_index, size_t cluster_size) const;\n@@ -167,2 +240,2 @@\n-  inline ssize_t find_index_of_previous_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n-                                                                     ssize_t last_index, size_t cluster_size) const;\n+  inline index_type find_index_of_previous_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                   index_type last_index, size_t cluster_size) const;\n@@ -170,1 +243,1 @@\n-  inline bool in_free_set(ShenandoahFreeSetPartitionId which_partition, ssize_t idx) const {\n+  inline bool in_free_set(ShenandoahFreeSetPartitionId which_partition, index_type idx) const {\n@@ -176,1 +249,11 @@\n-  inline ShenandoahFreeSetPartitionId membership(ssize_t idx) const;\n+  inline ShenandoahFreeSetPartitionId membership(index_type idx) const {\n+    assert (idx < _max, \"index is sane: %zu < %zu\", idx, _max);\n+    ShenandoahFreeSetPartitionId result = ShenandoahFreeSetPartitionId::NotFree;\n+    for (uint partition_id = 0; partition_id < UIntNumPartitions; partition_id++) {\n+      if (_membership[partition_id].is_set(idx)) {\n+        assert(result == ShenandoahFreeSetPartitionId::NotFree, \"Region should reside in only one partition\");\n+        result = (ShenandoahFreeSetPartitionId) partition_id;\n+      }\n+    }\n+    return result;\n+  }\n@@ -181,1 +264,1 @@\n-  inline bool partition_id_matches(ssize_t idx, ShenandoahFreeSetPartitionId which_partition) const;\n+  inline bool partition_id_matches(index_type idx, ShenandoahFreeSetPartitionId which_partition) const;\n@@ -184,2 +267,0 @@\n-  inline size_t max_regions() const { return _max; }\n-\n@@ -195,4 +276,4 @@\n-  inline ssize_t leftmost(ShenandoahFreeSetPartitionId which_partition) const;\n-  inline ssize_t rightmost(ShenandoahFreeSetPartitionId which_partition) const;\n-  ssize_t leftmost_empty(ShenandoahFreeSetPartitionId which_partition);\n-  ssize_t rightmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+  inline index_type leftmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  inline index_type rightmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  index_type leftmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+  index_type rightmost_empty(ShenandoahFreeSetPartitionId which_partition);\n@@ -202,0 +283,25 @@\n+  inline void increase_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n+  inline void decrease_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n+  inline size_t get_region_counts(ShenandoahFreeSetPartitionId which_partition) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _region_counts[int(which_partition)];\n+  }\n+\n+  inline void increase_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n+  inline void decrease_empty_region_counts(ShenandoahFreeSetPartitionId which_partition, size_t regions);\n+  inline size_t get_empty_region_counts(ShenandoahFreeSetPartitionId which_partition) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _empty_region_counts[int(which_partition)];\n+  }\n+\n+  inline void increase_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline void decrease_capacity(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline size_t get_capacity(ShenandoahFreeSetPartitionId which_partition) {\n+    assert (which_partition < NumPartitions, \"Partition must be valid\");\n+    return _capacity[int(which_partition)];\n+  }\n+\n+  inline void increase_available(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline void decrease_available(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline size_t get_available(ShenandoahFreeSetPartitionId which_partition);\n+\n@@ -203,0 +309,15 @@\n+  inline void decrease_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline size_t get_used(ShenandoahFreeSetPartitionId which_partition) {\n+    assert (which_partition < NumPartitions, \"Partition must be valid\");\n+    return _used[int(which_partition)];\n+  }\n+\n+  inline void increase_humongous_waste(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+  inline void decrease_humongous_waste(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+    shenandoah_assert_heaplocked();\n+    assert (which_partition < NumPartitions, \"Partition must be valid\");\n+    assert(_humongous_waste[int(which_partition)] >= bytes, \"Cannot decrease waste beyond what is there\");\n+    _humongous_waste[int(which_partition)] -= bytes;\n+  }\n+\n+  inline size_t get_humongous_waste(ShenandoahFreeSetPartitionId which_partition);\n@@ -230,1 +351,1 @@\n-           partition_membership_name(ssize_t(which_partition)));\n+           partition_membership_name(index_type(which_partition)));\n@@ -234,0 +355,7 @@\n+  \/\/ Returns bytes of humongous waste\n+  inline size_t humongous_waste(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    \/\/ This may be called with or without the global heap lock.  Changes to _humongous_waste[] are always made with heap lock.\n+    return _humongous_waste[int(which_partition)];\n+  }\n+\n@@ -246,1 +374,1 @@\n-           partition_membership_name(ssize_t(which_partition)));\n+           partition_membership_name(index_type(which_partition)));\n@@ -251,6 +379,1 @@\n-  inline void set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n-    shenandoah_assert_heaplocked();\n-    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n-    _capacity[int(which_partition)] = value;\n-    _available[int(which_partition)] = value - _used[int(which_partition)];\n-  }\n+  inline void set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value);\n@@ -287,1 +410,1 @@\n-  void assert_bounds() NOT_DEBUG_RETURN;\n+  void assert_bounds(bool validate_totals) NOT_DEBUG_RETURN;\n@@ -319,0 +442,2 @@\n+  size_t _total_humongous_waste;\n+\n@@ -333,0 +458,96 @@\n+  \/\/ bytes used by young\n+  size_t _total_young_used;\n+  template<bool UsedByMutatorChanged, bool UsedByCollectorChanged>\n+  inline void recompute_total_young_used() {\n+    if (UsedByMutatorChanged || UsedByCollectorChanged) {\n+      shenandoah_assert_heaplocked();\n+      _total_young_used = (_partitions.used_by(ShenandoahFreeSetPartitionId::Mutator) +\n+                           _partitions.used_by(ShenandoahFreeSetPartitionId::Collector));\n+    }\n+  }\n+\n+  \/\/ bytes used by old\n+  size_t _total_old_used;\n+  template<bool UsedByOldCollectorChanged>\n+  inline void recompute_total_old_used() {\n+    if (UsedByOldCollectorChanged) {\n+      shenandoah_assert_heaplocked();\n+      _total_old_used =_partitions.used_by(ShenandoahFreeSetPartitionId::OldCollector);\n+    }\n+  }\n+\n+  \/\/ bytes used by global\n+  size_t _total_global_used;\n+  \/\/ Prerequisite: _total_young_used and _total_old_used are valid\n+  template<bool UsedByMutatorChanged, bool UsedByCollectorChanged, bool UsedByOldCollectorChanged>\n+  inline void recompute_total_global_used() {\n+    if (UsedByMutatorChanged || UsedByCollectorChanged || UsedByOldCollectorChanged) {\n+      shenandoah_assert_heaplocked();\n+      _total_global_used = _total_young_used + _total_old_used;\n+    }\n+  }\n+\n+  template<bool UsedByMutatorChanged, bool UsedByCollectorChanged, bool UsedByOldCollectorChanged>\n+  inline void recompute_total_used() {\n+    recompute_total_young_used<UsedByMutatorChanged, UsedByCollectorChanged>();\n+    recompute_total_old_used<UsedByOldCollectorChanged>();\n+    recompute_total_global_used<UsedByMutatorChanged, UsedByCollectorChanged, UsedByOldCollectorChanged>();\n+  }\n+\n+  size_t _young_affiliated_regions;\n+  size_t _old_affiliated_regions;\n+  size_t _global_affiliated_regions;\n+\n+  size_t _young_unaffiliated_regions;\n+  size_t _global_unaffiliated_regions;\n+\n+  size_t _total_young_regions;\n+  size_t _total_global_regions;\n+\n+  size_t _mutator_bytes_allocated_since_gc_start;\n+\n+  \/\/ If only affiliation changes are promote-in-place and generation sizes have not changed,\n+  \/\/    we have AffiliatedChangesAreGlobalNeutral\n+  \/\/ If only affiliation changes are non-empty regions moved from Mutator to Collector and young size has not changed,\n+  \/\/    we have AffiliatedChangesAreYoungNeutral\n+  \/\/ If only unaffiliated changes are empty regions from Mutator to\/from Collector, we have UnaffiliatedChangesAreYoungNeutral\n+  template<bool MutatorEmptiesChanged, bool CollectorEmptiesChanged, bool OldCollectorEmptiesChanged,\n+           bool MutatorSizeChanged, bool CollectorSizeChanged, bool OldCollectorSizeChanged,\n+           bool AffiliatedChangesAreYoungNeutral, bool AffiliatedChangesAreGlobalNeutral,\n+           bool UnaffiliatedChangesAreYoungNeutral>\n+  inline void recompute_total_affiliated() {\n+    shenandoah_assert_heaplocked();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    if (!UnaffiliatedChangesAreYoungNeutral && (MutatorEmptiesChanged || CollectorEmptiesChanged)) {\n+      _young_unaffiliated_regions = (_partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Mutator) +\n+                                     _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::Collector));\n+    }\n+    if (!AffiliatedChangesAreYoungNeutral &&\n+        (MutatorSizeChanged || CollectorSizeChanged || MutatorEmptiesChanged || CollectorEmptiesChanged)) {\n+      _young_affiliated_regions = ((_partitions.get_capacity(ShenandoahFreeSetPartitionId::Mutator) +\n+                                    _partitions.get_capacity(ShenandoahFreeSetPartitionId::Collector)) \/ region_size_bytes -\n+                                   _young_unaffiliated_regions);\n+    }\n+    if (OldCollectorSizeChanged || OldCollectorEmptiesChanged) {\n+      _old_affiliated_regions = (_partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector) \/ region_size_bytes -\n+                                 _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector));\n+    }\n+    if (!AffiliatedChangesAreGlobalNeutral &&\n+        (MutatorEmptiesChanged || CollectorEmptiesChanged || OldCollectorEmptiesChanged)) {\n+      _global_unaffiliated_regions =\n+        _young_unaffiliated_regions + _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector);\n+    }\n+    if (!AffiliatedChangesAreGlobalNeutral &&\n+        (MutatorSizeChanged || CollectorSizeChanged || MutatorEmptiesChanged || CollectorEmptiesChanged ||\n+         OldCollectorSizeChanged || OldCollectorEmptiesChanged)) {\n+      _global_affiliated_regions = _young_affiliated_regions + _old_affiliated_regions;\n+    }\n+#ifdef ASSERT\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      assert(_young_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_young_used, \"sanity\");\n+      assert(_old_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_old_used, \"sanity\");\n+    }\n+    assert(_global_affiliated_regions * ShenandoahHeapRegion::region_size_bytes() >= _total_global_used, \"sanity\");\n+#endif\n+  }\n+\n@@ -350,0 +571,2 @@\n+  bool transfer_one_region_from_mutator_to_old_collector(size_t idx, size_t alloc_capacity);\n+\n@@ -377,1 +600,2 @@\n-  HeapWord* allocate_with_affiliation(Iter& iterator, ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+  HeapWord* allocate_with_affiliation(Iter& iterator, ShenandoahAffiliation affiliation,\n+                                      ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -395,0 +619,4 @@\n+  void transfer_empty_regions_from_to(ShenandoahFreeSetPartitionId source_partition,\n+                                      ShenandoahFreeSetPartitionId dest_partition,\n+                                      size_t num_regions);\n+\n@@ -402,1 +630,0 @@\n-\n@@ -405,3 +632,0 @@\n-\n-  \/\/ Set max_capacity for young and old generations\n-  void establish_generation_sizes(size_t young_region_count, size_t old_region_count);\n@@ -418,0 +642,15 @@\n+  inline size_t max_regions() const { return _partitions.max(); }\n+  ShenandoahFreeSetPartitionId membership(size_t index) const { return _partitions.membership(index); }\n+  inline void shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n+                                                                index_type low_idx, index_type high_idx, size_t num_regions) {\n+    return _partitions.shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx, num_regions);\n+  }\n+\n+  void reset_bytes_allocated_since_gc_start(size_t initial_bytes_allocated);\n+\n+  void increase_bytes_allocated(size_t bytes);\n+\n+  inline size_t get_bytes_allocated_since_gc_start() const {\n+    return _mutator_bytes_allocated_since_gc_start;\n+  }\n+\n@@ -422,0 +661,57 @@\n+  \/\/ Return bytes used by old\n+  inline size_t old_used() {\n+    return _total_old_used;\n+  }\n+\n+  ShenandoahFreeSetPartitionId prepare_to_promote_in_place(size_t idx, size_t bytes);\n+  void account_for_pip_regions(size_t mutator_regions, size_t mutator_bytes, size_t collector_regions, size_t collector_bytes);\n+\n+  \/\/ This is used for unit testing.  Not for preoduction.  Invokes exit() if old cannot be resized.\n+  void resize_old_collector_capacity(size_t desired_regions);\n+\n+  \/\/ Return bytes used by young\n+  inline size_t young_used() {\n+    return _total_young_used;\n+  }\n+\n+  \/\/ Return bytes used by global\n+  inline size_t global_used() {\n+    return _total_global_used;\n+  }\n+\n+  size_t global_unaffiliated_regions() {\n+    return _global_unaffiliated_regions;\n+  }\n+\n+  size_t young_unaffiliated_regions() {\n+    return _young_unaffiliated_regions;\n+  }\n+\n+  size_t old_unaffiliated_regions() {\n+    return _partitions.get_empty_region_counts(ShenandoahFreeSetPartitionId::OldCollector);\n+  }\n+\n+  size_t young_affiliated_regions() {\n+    return _young_affiliated_regions;\n+  }\n+\n+  size_t old_affiliated_regions() {\n+    return _old_affiliated_regions;\n+  }\n+\n+  size_t global_affiliated_regions() {\n+    return _global_affiliated_regions;\n+  }\n+\n+  size_t total_young_regions() {\n+    return _total_young_regions;\n+  }\n+\n+  size_t total_old_regions() {\n+    return _partitions.get_capacity(ShenandoahFreeSetPartitionId::OldCollector) \/ ShenandoahHeapRegion::region_size_bytes();\n+  }\n+\n+  size_t total_global_regions() {\n+    return _total_global_regions;\n+  }\n+\n@@ -467,0 +763,2 @@\n+  void transfer_humongous_regions_from_mutator_to_old_collector(size_t xfer_regions, size_t humongous_waste_words);\n+\n@@ -485,0 +783,6 @@\n+  inline size_t total_humongous_waste() const      { return _total_humongous_waste; }\n+  inline size_t humongous_waste_in_mutator() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::Mutator); }\n+  inline size_t humongous_waste_in_old() const { return _partitions.humongous_waste(ShenandoahFreeSetPartitionId::OldCollector); }\n+\n+  void decrease_humongous_waste_for_regular_bypass(ShenandoahHeapRegion* r, size_t waste);\n+\n@@ -542,1 +846,2 @@\n-  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count);\n+  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count,\n+                       size_t &young_used_regions, size_t &old_used_regions, size_t &young_used_bytes, size_t &old_used_bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":374,"deletions":69,"binary":false,"changes":443,"status":"modified"},{"patch":"@@ -242,1 +242,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n@@ -256,8 +255,1 @@\n-    result = phase5_epilog();\n-  }\n-  if (heap->mode()->is_generational()) {\n-    LogTarget(Info, gc, ergo) lt;\n-    if (lt.is_enabled()) {\n-      LogStream ls(lt);\n-      result.print_on(\"Full GC\", &ls);\n-    }\n+    phase5_epilog();\n@@ -996,17 +988,0 @@\n-\n-  void update_generation_usage() {\n-    if (_is_generational) {\n-      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n-      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n-    } else {\n-      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n-      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n-      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n-    }\n-\n-    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n-    \/\/ for non-generational modes except that there are no old regions.\n-    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n-                                                _old_usage + _young_usage,\n-                                                _old_humongous_waste + _young_humongous_waste);\n-  }\n@@ -1131,1 +1106,1 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahFullGC::phase5_epilog() {\n+void ShenandoahFullGC::phase5_epilog() {\n@@ -1134,1 +1109,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n@@ -1149,6 +1123,0 @@\n-    post_compact.update_generation_usage();\n-\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalFullGC::balance_generations_after_gc(heap);\n-    }\n-\n@@ -1180,1 +1148,0 @@\n-    result = ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n@@ -1183,1 +1150,0 @@\n-  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":2,"deletions":36,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -85,2 +85,1 @@\n-  ShenandoahGenerationalHeap::TransferResult phase5_epilog();\n-\n+  void phase5_epilog();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -150,12 +150,0 @@\n-size_t ShenandoahGeneration::bytes_allocated_since_gc_start() const {\n-  return AtomicAccess::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-void ShenandoahGeneration::reset_bytes_allocated_since_gc_start(size_t initial_bytes_allocated) {\n-  AtomicAccess::store(&_bytes_allocated_since_gc_start, initial_bytes_allocated);\n-}\n-\n-void ShenandoahGeneration::increase_allocated(size_t bytes) {\n-  AtomicAccess::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n-}\n-\n@@ -163,0 +151,1 @@\n+  shenandoah_assert_heaplocked();\n@@ -274,1 +263,1 @@\n-  const size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_generation->available_with_reserve());\n+  size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_generation->available_with_reserve());\n@@ -354,0 +343,5 @@\n+  \/\/ If any regions have been selected for promotion in place, this has the effect of decreasing available within mutator\n+  \/\/ and collector partitions, due to padding of remnant memory within each promoted in place region.  This will affect\n+  \/\/ young_evacuation_reserve but not old_evacuation_reserve or consumed_by_advance_promotion.  So recompute.\n+  young_evacuation_reserve = MIN2(young_evacuation_reserve, young_generation->available_with_reserve());\n+\n@@ -437,1 +431,3 @@\n-  assert(old_available >= unaffiliated_old, \"Unaffiliated old is a subset of old available\");\n+  assert(old_available >= unaffiliated_old,\n+         \"Unaffiliated old (%zu is %zu * %zu) is a subset of old available (%zu)\",\n+         unaffiliated_old, unaffiliated_old_regions, region_size_bytes, old_available);\n@@ -467,1 +463,0 @@\n-\n@@ -469,4 +464,0 @@\n-    bool result = ShenandoahGenerationalHeap::cast(heap)->generation_sizer()->transfer_to_young(regions_to_xfer);\n-    assert(excess_old >= regions_to_xfer * region_size_bytes,\n-           \"Cannot transfer (%zu, %zu) more than excess old (%zu)\",\n-           regions_to_xfer, region_size_bytes, excess_old);\n@@ -474,2 +465,2 @@\n-    log_debug(gc, ergo)(\"%s transferred %zu excess regions to young before start of evacuation\",\n-                       result? \"Successfully\": \"Unsuccessfully\", regions_to_xfer);\n+    log_debug(gc, ergo)(\"Before start of evacuation, total_promotion reserve is young_advance_promoted_reserve: %zu \"\n+                        \"plus excess: old: %zu\", young_advance_promoted_reserve_used, excess_old);\n@@ -533,0 +524,2 @@\n+  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+  ShenandoahFreeSet* free_set = heap->free_set();\n@@ -550,1 +543,1 @@\n-  const size_t num_regions = heap->num_regions();\n+  const index_type num_regions = heap->num_regions();\n@@ -555,1 +548,17 @@\n-  for (size_t i = 0; i < num_regions; i++) {\n+  ShenandoahFreeSet* freeset = heap->free_set();\n+\n+  \/\/ Any region that is to be promoted in place needs to be retired from its Collector or Mutator partition.\n+  index_type pip_low_collector_idx = freeset->max_regions();\n+  index_type pip_high_collector_idx = -1;\n+  index_type pip_low_mutator_idx = freeset->max_regions();\n+  index_type pip_high_mutator_idx = -1;\n+  size_t collector_regions_to_pip = 0;\n+  size_t mutator_regions_to_pip = 0;\n+\n+  size_t pip_mutator_regions = 0;\n+  size_t pip_collector_regions = 0;\n+  size_t pip_mutator_bytes = 0;\n+  size_t pip_collector_bytes = 0;\n+\n+  size_t min_remnant_size = PLAB::min_size() * HeapWordSize;\n+  for (index_type i = 0; i < num_regions; i++) {\n@@ -563,1 +572,1 @@\n-        \/\/ We prefer to promote this region in place because is has a small amount of garbage and a large usage.\n+        \/\/ We prefer to promote this region in place because it has a small amount of garbage and a large usage.\n@@ -572,4 +581,5 @@\n-\n-          size_t remnant_size = r->free() \/ HeapWordSize;\n-          if (remnant_size > ShenandoahHeap::min_fill_size()) {\n-            ShenandoahHeap::fill_with_object(original_top, remnant_size);\n+          size_t remnant_bytes = r->free();\n+          size_t remnant_words = remnant_bytes \/ HeapWordSize;\n+          assert(ShenandoahHeap::min_fill_size() <= PLAB::min_size(), \"Implementation makes invalid assumptions\");\n+          if (remnant_words >= ShenandoahHeap::min_fill_size()) {\n+            ShenandoahHeap::fill_with_object(original_top, remnant_words);\n@@ -580,1 +590,33 @@\n-            promote_in_place_pad += remnant_size * HeapWordSize;\n+            \/\/ The region r is either in the Mutator or Collector partition if remnant_words > heap()->plab_min_size.\n+            \/\/ Otherwise, the region is in the NotFree partition.\n+            ShenandoahFreeSetPartitionId p = free_set->membership(i);\n+            if (p == ShenandoahFreeSetPartitionId::Mutator) {\n+              mutator_regions_to_pip++;\n+              if (i < pip_low_mutator_idx) {\n+                pip_low_mutator_idx = i;\n+              }\n+              if (i > pip_high_mutator_idx) {\n+                pip_high_mutator_idx = i;\n+              }\n+              pip_mutator_regions++;\n+              pip_mutator_bytes += remnant_bytes;\n+            } else if (p == ShenandoahFreeSetPartitionId::Collector) {\n+              collector_regions_to_pip++;\n+              if (i < pip_low_collector_idx) {\n+                pip_low_collector_idx = i;\n+              }\n+              if (i > pip_high_collector_idx) {\n+                pip_high_collector_idx = i;\n+              }\n+              pip_collector_regions++;\n+              pip_collector_bytes += remnant_bytes;\n+            } else {\n+              assert((p == ShenandoahFreeSetPartitionId::NotFree) && (remnant_words < heap->plab_min_size()),\n+                     \"Should be NotFree if not in Collector or Mutator partitions\");\n+              \/\/ In this case, the memory is already counted as used and the region has already been retired.  There is\n+              \/\/ no need for further adjustments to used.  Further, the remnant memory for this region will not be\n+              \/\/ unallocated or made available to OldCollector after pip.\n+              remnant_bytes = 0;\n+            }\n+            promote_in_place_pad += remnant_bytes;\n+            free_set->prepare_to_promote_in_place(i, remnant_bytes);\n@@ -582,2 +624,5 @@\n-            \/\/ Since the remnant is so small that it cannot be filled, we don't have to worry about any accidental\n-            \/\/ allocations occurring within this region before the region is promoted in place.\n+            \/\/ Since the remnant is so small that this region has already been retired, we don't have to worry about any\n+            \/\/ accidental allocations occurring within this region before the region is promoted in place.\n+\n+            \/\/ This region was already not in the Collector or Mutator set, so no need to remove it.\n+            assert(free_set->membership(i) == ShenandoahFreeSetPartitionId::NotFree, \"sanity\");\n@@ -623,0 +668,17 @@\n+\n+  if (pip_mutator_regions + pip_collector_regions > 0) {\n+    freeset->account_for_pip_regions(pip_mutator_regions, pip_mutator_bytes, pip_collector_regions, pip_collector_bytes);\n+  }\n+\n+  \/\/ Retire any regions that have been selected for promote in place\n+  if (collector_regions_to_pip > 0) {\n+    freeset->shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::Collector,\n+                                                               pip_low_collector_idx, pip_high_collector_idx,\n+                                                               collector_regions_to_pip);\n+  }\n+  if (mutator_regions_to_pip > 0) {\n+    freeset->shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId::Mutator,\n+                                                               pip_low_mutator_idx, pip_high_mutator_idx,\n+                                                               mutator_regions_to_pip);\n+  }\n+\n@@ -753,1 +815,7 @@\n-    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+      gen_heap->compute_old_generation_balance(young_cset_regions, old_cset_regions);\n+    }\n+\n@@ -755,1 +823,1 @@\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n@@ -805,1 +873,1 @@\n-  _used(0), _bytes_allocated_since_gc_start(0),\n+  _used(0),\n@@ -807,0 +875,1 @@\n+  _free_set(nullptr),\n@@ -825,0 +894,5 @@\n+void ShenandoahGeneration::post_initialize(ShenandoahHeap* heap) {\n+  _free_set = heap->free_set();\n+  assert(_free_set != nullptr, \"bad initialization order\");\n+}\n+\n@@ -852,22 +926,3 @@\n-size_t ShenandoahGeneration::increment_affiliated_region_count() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-  \/\/ During full gc, multiple GC worker threads may change region affiliations without a lock.  No lock is enforced\n-  \/\/ on read and write of _affiliated_region_count.  At the end of full gc, a single thread overwrites the count with\n-  \/\/ a coherent value.\n-  return AtomicAccess::add(&_affiliated_region_count, (size_t) 1);\n-}\n-\n-size_t ShenandoahGeneration::decrement_affiliated_region_count() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-  \/\/ During full gc, multiple GC worker threads may change region affiliations without a lock.  No lock is enforced\n-  \/\/ on read and write of _affiliated_region_count.  At the end of full gc, a single thread overwrites the count with\n-  \/\/ a coherent value.\n-  auto affiliated_region_count = AtomicAccess::sub(&_affiliated_region_count, (size_t) 1);\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (used() + _humongous_waste <= affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n-         \"used + humongous cannot exceed regions\");\n-  return affiliated_region_count;\n-}\n-\n-size_t ShenandoahGeneration::decrement_affiliated_region_count_without_lock() {\n-  return AtomicAccess::sub(&_affiliated_region_count, (size_t) 1);\n+size_t ShenandoahGeneration::used() const {\n+  assert(_type == ShenandoahGenerationType::NON_GEN, \"OO sanity\");\n+  return _free_set->global_used();\n@@ -876,8 +931,0 @@\n-size_t ShenandoahGeneration::increase_affiliated_region_count(size_t delta) {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-  return AtomicAccess::add(&_affiliated_region_count, delta);\n-}\n-\n-size_t ShenandoahGeneration::decrease_affiliated_region_count(size_t delta) {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-  assert(AtomicAccess::load(&_affiliated_region_count) >= delta, \"Affiliated region count cannot be negative\");\n@@ -885,5 +932,4 @@\n-  auto const affiliated_region_count = AtomicAccess::sub(&_affiliated_region_count, delta);\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (_used + _humongous_waste <= affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n-         \"used + humongous cannot exceed regions\");\n-  return affiliated_region_count;\n+size_t ShenandoahGeneration::bytes_allocated_since_gc_start() const {\n+  assert(_type == ShenandoahGenerationType::NON_GEN, \"OO sanity\");\n+  assert(!ShenandoahHeap::heap()->mode()->is_generational(), \"NON_GEN implies not generational\");\n+  return _free_set->get_bytes_allocated_since_gc_start();\n@@ -892,5 +938,3 @@\n-void ShenandoahGeneration::establish_usage(size_t num_regions, size_t num_bytes, size_t humongous_waste) {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at a safepoint\");\n-  AtomicAccess::store(&_affiliated_region_count, num_regions);\n-  AtomicAccess::store(&_used, num_bytes);\n-  _humongous_waste = humongous_waste;\n+size_t ShenandoahGeneration::get_affiliated_region_count() const {\n+  assert(_type == ShenandoahGenerationType::NON_GEN, \"OO sanity\");\n+  return _free_set->global_affiliated_regions();\n@@ -899,2 +943,3 @@\n-void ShenandoahGeneration::increase_used(size_t bytes) {\n-  AtomicAccess::add(&_used, bytes);\n+size_t ShenandoahGeneration::get_humongous_waste() const {\n+  assert(_type == ShenandoahGenerationType::NON_GEN, \"OO sanity\");\n+  return _free_set->total_humongous_waste();\n@@ -903,4 +948,3 @@\n-void ShenandoahGeneration::increase_humongous_waste(size_t bytes) {\n-  if (bytes > 0) {\n-    AtomicAccess::add(&_humongous_waste, bytes);\n-  }\n+size_t ShenandoahGeneration::used_regions() const {\n+  assert(_type == ShenandoahGenerationType::NON_GEN, \"OO sanity\");\n+  return _free_set->global_affiliated_regions();\n@@ -909,5 +953,14 @@\n-void ShenandoahGeneration::decrease_humongous_waste(size_t bytes) {\n-  if (bytes > 0) {\n-    assert(ShenandoahHeap::heap()->is_full_gc_in_progress() || (_humongous_waste >= bytes),\n-           \"Waste (%zu) cannot be negative (after subtracting %zu)\", _humongous_waste, bytes);\n-    AtomicAccess::sub(&_humongous_waste, bytes);\n+size_t ShenandoahGeneration::max_capacity() const {\n+  size_t total_regions;\n+  switch (_type) {\n+  case ShenandoahGenerationType::OLD:\n+    total_regions = _free_set->total_old_regions();\n+    break;\n+  case ShenandoahGenerationType::YOUNG:\n+    total_regions = _free_set->total_young_regions();\n+    break;\n+  case ShenandoahGenerationType::GLOBAL:\n+  case ShenandoahGenerationType::NON_GEN:\n+  default:\n+    total_regions = _free_set->total_global_regions();\n+    break;\n@@ -915,10 +968,1 @@\n-}\n-\n-void ShenandoahGeneration::decrease_used(size_t bytes) {\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (_used >= bytes), \"cannot reduce bytes used by generation below zero\");\n-  AtomicAccess::sub(&_used, bytes);\n-}\n-\n-size_t ShenandoahGeneration::used_regions() const {\n-  return AtomicAccess::load(&_affiliated_region_count);\n+  return total_regions * ShenandoahHeapRegion::region_size_bytes();\n@@ -928,6 +972,13 @@\n-  size_t result = max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n-  auto const used_regions = this->used_regions();\n-  if (used_regions > result) {\n-    result = 0;\n-  } else {\n-    result -= used_regions;\n+  size_t free_regions;\n+  switch (_type) {\n+  case ShenandoahGenerationType::OLD:\n+    free_regions = _free_set->old_unaffiliated_regions();\n+    break;\n+  case ShenandoahGenerationType::YOUNG:\n+    free_regions = _free_set->young_unaffiliated_regions();\n+    break;\n+  case ShenandoahGenerationType::GLOBAL:\n+  case ShenandoahGenerationType::NON_GEN:\n+  default:\n+    free_regions = _free_set->global_unaffiliated_regions();\n+    break;\n@@ -935,1 +986,1 @@\n-  return result;\n+  return free_regions;\n@@ -939,1 +990,15 @@\n-  return used_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  size_t used_regions;\n+  switch (_type) {\n+  case ShenandoahGenerationType::OLD:\n+    used_regions = _free_set->old_affiliated_regions();\n+    break;\n+  case ShenandoahGenerationType::YOUNG:\n+    used_regions = _free_set->young_affiliated_regions();\n+    break;\n+  case ShenandoahGenerationType::GLOBAL:\n+  case ShenandoahGenerationType::NON_GEN:\n+  default:\n+    used_regions = _free_set->global_affiliated_regions();\n+    break;\n+  }\n+  return used_regions * ShenandoahHeapRegion::region_size_bytes();\n@@ -943,1 +1008,2 @@\n-  return available(max_capacity());\n+  size_t result = available(max_capacity());\n+  return result;\n@@ -948,1 +1014,2 @@\n-  return available(max_capacity());\n+  size_t result = available(max_capacity());\n+  return result;\n@@ -952,1 +1019,2 @@\n-  return available(ShenandoahHeap::heap()->soft_max_capacity());\n+  size_t result = available(ShenandoahHeap::heap()->soft_max_capacity());\n+  return result;\n@@ -956,49 +1024,3 @@\n-  size_t in_use = used() + get_humongous_waste();\n-  return in_use > capacity ? 0 : capacity - in_use;\n-}\n-\n-size_t ShenandoahGeneration::increase_capacity(size_t increment) {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-\n-  \/\/ We do not enforce that new capacity >= heap->max_size_for(this).  The maximum generation size is treated as a rule of thumb\n-  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n-  \/\/ in place.\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (_max_capacity + increment <= ShenandoahHeap::heap()->max_capacity()), \"Generation cannot be larger than heap size\");\n-  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n-  _max_capacity += increment;\n-\n-  \/\/ This detects arithmetic wraparound on _used\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (used_regions_size() >= used()),\n-         \"Affiliated regions must hold more than what is currently used\");\n-  return _max_capacity;\n-}\n-\n-size_t ShenandoahGeneration::set_capacity(size_t byte_size) {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-  _max_capacity = byte_size;\n-  return _max_capacity;\n-}\n-\n-size_t ShenandoahGeneration::decrease_capacity(size_t decrement) {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-\n-  \/\/ We do not enforce that new capacity >= heap->min_size_for(this).  The minimum generation size is treated as a rule of thumb\n-  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n-  \/\/ in place.\n-  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n-  assert(_max_capacity >= decrement, \"Generation capacity cannot be negative\");\n-\n-  _max_capacity -= decrement;\n-\n-  \/\/ This detects arithmetic wraparound on _used\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (used_regions_size() >= used()),\n-         \"Affiliated regions must hold more than what is currently used\");\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (_used <= _max_capacity), \"Cannot use more than capacity\");\n-  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n-         (used_regions_size() <= _max_capacity),\n-         \"Cannot use more than capacity\");\n-  return _max_capacity;\n+  size_t in_use = used();\n+  size_t result = in_use > capacity ? 0 : capacity - in_use;\n+  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":177,"deletions":155,"binary":false,"changes":332,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -43,1 +44,0 @@\n-\n@@ -72,1 +72,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -74,1 +73,1 @@\n-\n+  ShenandoahFreeSet* _free_set;\n@@ -102,0 +101,1 @@\n+  \/\/ Return available assuming that we can allocate no more than capacity bytes within this generation.\n@@ -110,3 +110,4 @@\n-  bool is_young() const  { return _type == YOUNG; }\n-  bool is_old() const    { return _type == OLD; }\n-  bool is_global() const { return _type == GLOBAL || _type == NON_GEN; }\n+  inline bool is_young() const                 { return _type == YOUNG; }\n+  inline bool is_old() const                   { return _type == OLD; }\n+  inline bool is_global() const                { return _type == GLOBAL || _type == NON_GEN; }\n+  inline ShenandoahGenerationType type() const { return _type; }\n@@ -119,2 +120,0 @@\n-  inline ShenandoahGenerationType type() const { return _type; }\n-\n@@ -127,1 +126,4 @@\n-  size_t max_capacity() const override      { return _max_capacity; }\n+  virtual void post_initialize(ShenandoahHeap* heap);\n+\n+  size_t max_capacity() const override;\n+\n@@ -131,1 +133,3 @@\n-  size_t used() const override { return AtomicAccess::load(&_used); }\n+  virtual size_t used() const override;\n+  virtual size_t get_affiliated_region_count() const;\n+\n@@ -135,1 +139,3 @@\n-    return used() + get_humongous_waste();\n+    \/\/ In the current implementation, used() includes humongous waste\n+    size_t result = used();\n+    return result;\n@@ -144,11 +150,1 @@\n-  size_t bytes_allocated_since_gc_start() const override;\n-  void reset_bytes_allocated_since_gc_start(size_t initial_bytes_allocated);\n-  void increase_allocated(size_t bytes);\n-\n-  \/\/ These methods change the capacity of the generation by adding or subtracting the given number of bytes from the current\n-  \/\/ capacity, returning the capacity of the generation following the change.\n-  size_t increase_capacity(size_t increment);\n-  size_t decrease_capacity(size_t decrement);\n-\n-  \/\/ Set the capacity of the generation, returning the value set\n-  size_t set_capacity(size_t byte_size);\n+  virtual size_t bytes_allocated_since_gc_start() const override;\n@@ -215,22 +211,1 @@\n-  \/\/ Return the updated value of affiliated_region_count\n-  size_t increment_affiliated_region_count();\n-\n-  \/\/ Return the updated value of affiliated_region_count\n-  size_t decrement_affiliated_region_count();\n-  \/\/ Same as decrement_affiliated_region_count, but w\/o the need to hold heap lock before being called.\n-  size_t decrement_affiliated_region_count_without_lock();\n-\n-  \/\/ Return the updated value of affiliated_region_count\n-  size_t increase_affiliated_region_count(size_t delta);\n-\n-  \/\/ Return the updated value of affiliated_region_count\n-  size_t decrease_affiliated_region_count(size_t delta);\n-\n-  void establish_usage(size_t num_regions, size_t num_bytes, size_t humongous_waste);\n-\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-\n-  void increase_humongous_waste(size_t bytes);\n-  void decrease_humongous_waste(size_t bytes);\n-  size_t get_humongous_waste() const { return _humongous_waste; }\n+  virtual size_t get_humongous_waste() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":19,"deletions":44,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -118,28 +118,0 @@\n-bool ShenandoahGenerationSizer::transfer_regions(ShenandoahGeneration* src, ShenandoahGeneration* dst, size_t regions) const {\n-  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n-\n-  if (src->free_unaffiliated_regions() < regions) {\n-    \/\/ Source does not have enough free regions for this transfer. The caller should have\n-    \/\/ already capped the transfer based on available unaffiliated regions.\n-    return false;\n-  }\n-\n-  if (dst->max_capacity() + bytes_to_transfer > max_size_for(dst)) {\n-    \/\/ This transfer would cause the destination generation to grow above its configured maximum size.\n-    return false;\n-  }\n-\n-  if (src->max_capacity() - bytes_to_transfer < min_size_for(src)) {\n-    \/\/ This transfer would cause the source generation to shrink below its configured minimum size.\n-    return false;\n-  }\n-\n-  src->decrease_capacity(bytes_to_transfer);\n-  dst->increase_capacity(bytes_to_transfer);\n-  const size_t new_size = dst->max_capacity();\n-  log_info(gc, ergo)(\"Transfer %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n-                     regions, src->name(), dst->name(), PROPERFMTARGS(new_size));\n-  return true;\n-}\n-\n-\n@@ -174,28 +146,0 @@\n-\n-\/\/ Returns true iff transfer is successful\n-bool ShenandoahGenerationSizer::transfer_to_old(size_t regions) const {\n-  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n-  return transfer_regions(heap->young_generation(), heap->old_generation(), regions);\n-}\n-\n-\/\/ This is used when promoting humongous or highly utilized regular regions in place.  It is not required in this situation\n-\/\/ that the transferred regions be unaffiliated.\n-void ShenandoahGenerationSizer::force_transfer_to_old(size_t regions) const {\n-  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n-  ShenandoahGeneration* old_gen = heap->old_generation();\n-  ShenandoahGeneration* young_gen = heap->young_generation();\n-  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n-\n-  young_gen->decrease_capacity(bytes_to_transfer);\n-  old_gen->increase_capacity(bytes_to_transfer);\n-  const size_t new_size = old_gen->max_capacity();\n-  log_info(gc, ergo)(\"Forcing transfer of %zu region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n-                     regions, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_size));\n-}\n-\n-\n-bool ShenandoahGenerationSizer::transfer_to_young(size_t regions) const {\n-  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n-  return transfer_regions(heap->old_generation(), heap->young_generation(), regions);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.cpp","additions":0,"deletions":56,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -54,5 +54,2 @@\n-  \/\/ This will attempt to transfer regions from the `src` generation to `dst` generation.\n-  \/\/ If the transfer would violate the configured minimum size for the source or the configured\n-  \/\/ maximum size of the destination, it will not perform the transfer and will return false.\n-  \/\/ Returns true if the transfer is performed.\n-  bool transfer_regions(ShenandoahGeneration* src, ShenandoahGeneration* dst, size_t regions) const;\n+public:\n+  ShenandoahGenerationSizer();\n@@ -66,3 +63,0 @@\n-public:\n-  ShenandoahGenerationSizer();\n-\n@@ -84,7 +78,0 @@\n-\n-  \/\/ True if transfer succeeds, else false. See transfer_regions.\n-  bool transfer_to_young(size_t regions) const;\n-  bool transfer_to_old(size_t regions) const;\n-\n-  \/\/ force transfer is used when we promote humongous objects.  May violate min\/max limits on generation sizes\n-  void force_transfer_to_old(size_t regions) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.hpp","additions":2,"deletions":15,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -248,1 +248,3 @@\n-  _heap->reset_bytes_allocated_since_gc_start();\n+  if (gc_mode() != servicing_old) {\n+    _heap->reset_bytes_allocated_since_gc_start();\n+  }\n@@ -292,0 +294,3 @@\n+    \/\/ Report current free set state at the end of cycle if normal completion.\n+    \/\/ Do not report if cancelled, since we may not have rebuilt free set and content is unreliable.\n+    _heap->free_set()->log_status_under_lock();\n@@ -294,3 +299,0 @@\n-  \/\/ Report current free set state at the end of cycle, whether\n-  \/\/ it is a normal completion, or the abort.\n-  _heap->free_set()->log_status_under_lock();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -177,1 +178,1 @@\n-    const size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+    const size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n@@ -227,0 +228,7 @@\n+    \/\/ pip_unpadded is memory too small to be filled above original top\n+    size_t pip_unpadded = (region->end() - region->top()) * HeapWordSize;\n+    assert((region->top() == region->end())\n+           || (pip_unpadded == (size_t) ((region->end() - region->top()) * HeapWordSize)), \"Invariant\");\n+    assert(pip_unpadded < ShenandoahHeap::min_fill_size() * HeapWordSize, \"Sanity\");\n+    size_t pip_pad_bytes = (region->top() - region->get_top_before_promote()) * HeapWordSize;\n+    assert((pip_unpadded == 0) || (pip_pad_bytes == 0), \"Only one of pip_unpadded and pip_pad_bytes is non-zero\");\n@@ -231,2 +239,2 @@\n-\n-    size_t region_used = region->used();\n+    size_t region_to_be_used_in_old = region->used();\n+    assert(region_to_be_used_in_old + pip_pad_bytes + pip_unpadded == region_size_bytes, \"invariant\");\n@@ -244,6 +252,0 @@\n-    young_gen->decrease_used(region_used);\n-    young_gen->decrement_affiliated_region_count();\n-\n-    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n-    _heap->generation_sizer()->force_transfer_to_old(1);\n-    region->set_affiliation(OLD_GENERATION);\n@@ -251,2 +253,6 @@\n-    old_gen->increment_affiliated_region_count();\n-    old_gen->increase_used(region_used);\n+    size_t available_in_region = region->free();\n+    size_t plab_min_size_in_bytes = _heap->plab_min_size() * HeapWordSize;\n+    if (available_in_region < plab_min_size_in_bytes) {\n+      \/\/ The available memory in young had been retired.  Retire it in old also.\n+      region_to_be_used_in_old += available_in_region;\n+    }\n@@ -256,0 +262,1 @@\n+    region->set_affiliation(OLD_GENERATION);\n@@ -271,1 +278,2 @@\n-  const size_t humongous_waste = spanned_regions * ShenandoahHeapRegion::region_size_bytes() - obj->size() * HeapWordSize;\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  const size_t humongous_waste = spanned_regions * region_size_bytes - obj->size() * HeapWordSize;\n@@ -285,7 +293,0 @@\n-    young_gen->decrease_used(used_bytes);\n-    young_gen->decrease_humongous_waste(humongous_waste);\n-    young_gen->decrease_affiliated_region_count(spanned_regions);\n-\n-    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n-    _heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n-\n@@ -303,3 +304,2 @@\n-    old_gen->increase_affiliated_region_count(spanned_regions);\n-    old_gen->increase_used(used_bytes);\n-    old_gen->increase_humongous_waste(humongous_waste);\n+    ShenandoahFreeSet* freeset = _heap->free_set();\n+    freeset->transfer_humongous_regions_from_mutator_to_old_collector(spanned_regions, humongous_waste);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -108,27 +108,0 @@\n-void ShenandoahGenerationalFullGC::balance_generations_after_gc(ShenandoahHeap* heap) {\n-  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::cast(heap);\n-  ShenandoahOldGeneration* const old_gen = gen_heap->old_generation();\n-\n-  size_t old_usage = old_gen->used_regions_size();\n-  size_t old_capacity = old_gen->max_capacity();\n-\n-  assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must align with region size\");\n-  assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must align with region size\");\n-\n-  if (old_capacity > old_usage) {\n-    size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n-    gen_heap->generation_sizer()->transfer_to_young(excess_old_regions);\n-  } else if (old_capacity < old_usage) {\n-    size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n-    gen_heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n-  }\n-\n-  log_info(gc, ergo)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n-               PROPERFMTARGS(gen_heap->young_generation()->used()),\n-               PROPERFMTARGS(old_gen->used()));\n-}\n-\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set() {\n-  return ShenandoahGenerationalHeap::heap()->balance_generations();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -48,6 +48,0 @@\n-  \/\/ Full GC may have promoted regions and may have temporarily violated constraints on the usage and\n-  \/\/ capacity of the old generation. This method will balance the accounting of regions between the\n-  \/\/ young and old generations. This is somewhat vestigial, but the outcome of this method is used\n-  \/\/ when rebuilding the free sets.\n-  static void balance_generations_after_gc(ShenandoahHeap* heap);\n-\n@@ -59,8 +53,0 @@\n-  \/\/ Rebuilding the free set may have resulted in regions being pulled in to the old generation\n-  \/\/ evacuation reserve. For this reason, we must update the usage and capacity of the generations\n-  \/\/ again. In the distant past, the free set did not know anything about generations, so we had\n-  \/\/ a layer built above it to represent how much young\/old memory was available. This layer is\n-  \/\/ redundant and adds complexity. We would like to one day remove it. Until then, we must keep it\n-  \/\/ synchronized with the free set's view of things.\n-  static ShenandoahGenerationalHeap::TransferResult balance_generations_after_rebuilding_free_set();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.hpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -122,0 +123,6 @@\n+void ShenandoahGenerationalHeap::post_initialize_heuristics() {\n+  ShenandoahHeap::post_initialize_heuristics();\n+  _young_generation->post_initialize(this);\n+  _old_generation->post_initialize(this);\n+}\n+\n@@ -579,29 +586,0 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalHeap::balance_generations() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-\n-  ShenandoahOldGeneration* old_gen = old_generation();\n-  const ssize_t old_region_balance = old_gen->get_region_balance();\n-  old_gen->set_region_balance(0);\n-\n-  if (old_region_balance > 0) {\n-    const auto old_region_surplus = checked_cast<size_t>(old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_young(old_region_surplus);\n-    return TransferResult {\n-      success, old_region_surplus, \"young\"\n-    };\n-  }\n-\n-  if (old_region_balance < 0) {\n-    const auto old_region_deficit = checked_cast<size_t>(-old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_old(old_region_deficit);\n-    if (!success) {\n-      old_gen->handle_failed_transfer();\n-    }\n-    return TransferResult {\n-      success, old_region_deficit, \"old\"\n-    };\n-  }\n-\n-  return TransferResult {true, 0, \"none\"};\n-}\n-\n@@ -611,1 +589,4 @@\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+\/\/\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old based on either:\n+\/\/  1. an assumption that we will be able to replenish memory \"borrowed\" from young at the end of collection, or\n+\/\/  2. there is sufficient excess in the allocation runway during GC idle cycles\n@@ -639,3 +620,3 @@\n-  const double max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n-                                 bound_on_old_reserve: MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent) \/ double(100 - ShenandoahOldEvacRatioPercent),\n-                                                            bound_on_old_reserve);\n+  const double max_old_reserve = ((ShenandoahOldEvacRatioPercent == 100)? bound_on_old_reserve:\n+                                  MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent)\n+                                       \/ double(100 - ShenandoahOldEvacRatioPercent), bound_on_old_reserve));\n@@ -650,1 +631,2 @@\n-    const double max_evac_need = (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    const double max_evac_need =\n+      (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n@@ -653,1 +635,2 @@\n-    const double old_fragmented_available = double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n+    const double old_fragmented_available =\n+      double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n@@ -700,0 +683,1 @@\n+  ShenandoahHeapLocker locker(lock());\n@@ -1060,9 +1044,0 @@\n-\n-  \/\/ We defer generation resizing actions until after cset regions have been recycled.\n-  TransferResult result = balance_generations();\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Degenerated GC\", &ls);\n-  }\n-\n@@ -1090,14 +1065,1 @@\n-\n-  TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(lock());\n-\n-    result = balance_generations();\n-    reset_generation_reserves();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Concurrent GC\", &ls);\n-  }\n+  reset_generation_reserves();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":20,"deletions":58,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+  void post_initialize_heuristics() override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,0 +43,41 @@\n+size_t ShenandoahGlobalGeneration::used() const {\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_generational = heap->mode()->is_generational();\n+  assert((is_generational && (type() == ShenandoahGenerationType::GLOBAL)) ||\n+         (!is_generational && (type() == ShenandoahGenerationType::NON_GEN)), \"OO sanity\");\n+#endif\n+  return _free_set->global_used();\n+}\n+\n+size_t ShenandoahGlobalGeneration::bytes_allocated_since_gc_start() const {\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_generational = heap->mode()->is_generational();\n+  assert((is_generational && (type() == ShenandoahGenerationType::GLOBAL)) ||\n+         (!is_generational && (type() == ShenandoahGenerationType::NON_GEN)), \"OO sanity\");\n+#endif\n+  return _free_set->get_bytes_allocated_since_gc_start();\n+}\n+\n+size_t ShenandoahGlobalGeneration::get_affiliated_region_count() const {\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_generational = heap->mode()->is_generational();\n+  assert((is_generational && (type() == ShenandoahGenerationType::GLOBAL)) ||\n+         (!is_generational && (type() == ShenandoahGenerationType::NON_GEN)), \"OO sanity\");\n+#endif\n+  return _free_set->global_affiliated_regions();\n+}\n+\n+size_t ShenandoahGlobalGeneration::get_humongous_waste() const {\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_generational = heap->mode()->is_generational();\n+  assert((is_generational && (type() == ShenandoahGenerationType::GLOBAL)) ||\n+         (!is_generational && (type() == ShenandoahGenerationType::NON_GEN)), \"OO sanity\");\n+#endif\n+  return _free_set->total_humongous_waste();\n+}\n+\n+\n@@ -44,3 +85,7 @@\n-  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n-  assert(heap->mode()->is_generational(), \"Region usage accounting is only for generational mode\");\n-  return heap->old_generation()->used_regions() + heap->young_generation()->used_regions();\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_generational = heap->mode()->is_generational();\n+  assert((is_generational && (type() == ShenandoahGenerationType::GLOBAL)) ||\n+         (!is_generational && (type() == ShenandoahGenerationType::NON_GEN)), \"OO sanity\");\n+#endif\n+  return _free_set->global_affiliated_regions();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":48,"deletions":3,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -41,0 +41,4 @@\n+  size_t used() const override;\n+  size_t bytes_allocated_since_gc_start() const override;\n+  size_t get_affiliated_region_count() const override;\n+  size_t get_humongous_waste() const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -407,1 +407,0 @@\n-    _free_set = new ShenandoahFreeSet(this, _num_regions);\n@@ -422,0 +421,1 @@\n+    _free_set = new ShenandoahFreeSet(this, _num_regions);\n@@ -423,1 +423,0 @@\n-    size_t young_cset_regions, old_cset_regions;\n@@ -425,0 +424,1 @@\n+    post_initialize_heuristics();\n@@ -426,1 +426,1 @@\n-    size_t first_old, last_old, num_old;\n+    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n@@ -428,0 +428,8 @@\n+    if (mode()->is_generational()) {\n+      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+      \/\/ We cannot call\n+      \/\/  gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions)\n+      \/\/ until after the heap is fully initialized.  So we make up a safe value here.\n+      size_t allocation_runway = InitialHeapSize \/ 2;\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+    }\n@@ -525,0 +533,4 @@\n+void ShenandoahHeap::post_initialize_heuristics() {\n+  _global_generation->post_initialize(this);\n+}\n+\n@@ -740,1 +752,0 @@\n-    increase_used(generation, actual_bytes + wasted_bytes);\n@@ -743,37 +754,0 @@\n-    \/\/ padding and actual size both count towards allocation counter\n-    generation->increase_allocated(actual_bytes + wasted_bytes);\n-\n-    \/\/ only actual size counts toward usage for mutator allocations\n-    increase_used(generation, actual_bytes);\n-\n-    if (wasted_bytes > 0 && ShenandoahHeapRegion::requires_humongous(req.actual_size())) {\n-      increase_humongous_waste(generation,wasted_bytes);\n-    }\n-  }\n-}\n-\n-void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->increase_humongous_waste(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->increase_humongous_waste(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->decrease_humongous_waste(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->decrease_humongous_waste(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->increase_used(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->increase_used(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->decrease_used(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->decrease_used(bytes);\n@@ -2352,0 +2326,2 @@\n+  ShenandoahFreeSet* _free_set = free_set();\n+  size_t bytes_allocated = _free_set->get_bytes_allocated_since_gc_start();\n@@ -2353,1 +2329,0 @@\n-    size_t bytes_allocated = young_generation()->bytes_allocated_since_gc_start();\n@@ -2355,3 +2330,0 @@\n-    young_generation()->reset_bytes_allocated_since_gc_start(unaccounted_bytes);\n-    unaccounted_bytes = 0;\n-    old_generation()->reset_bytes_allocated_since_gc_start(unaccounted_bytes);\n@@ -2359,1 +2331,0 @@\n-    size_t bytes_allocated = global_generation()->bytes_allocated_since_gc_start();\n@@ -2363,1 +2334,2 @@\n-  global_generation()->reset_bytes_allocated_since_gc_start(unaccounted_bytes);\n+  ShenandoahHeapLocker locker(lock());\n+  _free_set->reset_bytes_allocated_since_gc_start(unaccounted_bytes);\n@@ -2744,0 +2716,4 @@\n+  assert(_initial_size <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n+  assert(used() <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n+  assert(committed() <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n+  assert(max_capacity() <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":23,"deletions":47,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -205,0 +205,1 @@\n+  virtual void post_initialize_heuristics();\n@@ -714,2 +715,0 @@\n-  void notify_mutator_alloc_words(size_t words, size_t waste);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -581,0 +581,2 @@\n+\/\/ Upon return, this region has been recycled.  We try to recycle it.\n+\/\/ We may fail if some other thread recycled it before we do.\n@@ -585,6 +587,4 @@\n-      ShenandoahHeap* heap = ShenandoahHeap::heap();\n-      ShenandoahGeneration* generation = heap->generation_for(affiliation());\n-\n-      heap->decrease_used(generation, used());\n-      generation->decrement_affiliated_region_count();\n-\n+      \/\/ At freeset rebuild time, which precedes recycling of collection set, we treat all cset regions as\n+      \/\/ part of capacity, as empty, as fully available, and as unaffiliated.  This provides short-lived optimism\n+      \/\/ for triggering heuristics.  It greatly simplifies and reduces the locking overhead required\n+      \/\/ by more time-precise accounting of these details.\n@@ -611,5 +611,4 @@\n-      ShenandoahHeap* heap = ShenandoahHeap::heap();\n-      ShenandoahGeneration* generation = heap->generation_for(affiliation());\n-      heap->decrease_used(generation, used());\n-      generation->decrement_affiliated_region_count_without_lock();\n-\n+      \/\/ At freeset rebuild time, which precedes recycling of collection set, we treat all cset regions as\n+      \/\/ part of capacity, as empty, as fully available, and as unaffiliated.  This provides short-lived optimism\n+      \/\/ for triggering and pacing heuristics.  It greatly simplifies and reduces the locking overhead required\n+      \/\/ by more time-precise accounting of these details.\n@@ -903,1 +902,1 @@\n-void ShenandoahHeapRegion::decrement_humongous_waste() const {\n+void ShenandoahHeapRegion::decrement_humongous_waste() {\n@@ -908,2 +907,1 @@\n-    ShenandoahGeneration* generation = heap->generation_for(affiliation());\n-    heap->decrease_humongous_waste(generation, waste_bytes);\n+    heap->free_set()->decrease_humongous_waste_for_regular_bypass(this, waste_bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":12,"deletions":14,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -369,0 +369,3 @@\n+  \/\/ Allocate fill after top\n+  inline HeapWord* allocate_fill(size_t word_size);\n+\n@@ -495,1 +498,1 @@\n-  void decrement_humongous_waste() const;\n+  void decrement_humongous_waste();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -90,0 +90,17 @@\n+HeapWord* ShenandoahHeapRegion::allocate_fill(size_t size) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(is_object_aligned(size), \"alloc size breaks alignment: %zu\", size);\n+  assert(size >= ShenandoahHeap::min_fill_size(), \"Cannot fill unless min fill size\");\n+\n+  HeapWord* obj = top();\n+  HeapWord* new_top = obj + size;\n+  ShenandoahHeap::fill_with_object(obj, size);\n+  set_top(new_top);\n+\n+  assert(is_object_aligned(new_top), \"new top breaks alignment: \" PTR_FORMAT, p2i(new_top));\n+  assert(is_object_aligned(obj),     \"obj is not aligned: \"       PTR_FORMAT, p2i(obj));\n+\n+  return obj;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -65,2 +65,4 @@\n-  assert(used <= committed, \"used: %zu, committed: %zu\", used,      committed);\n-\n+  assert(used <= committed, \"used: %zu, committed: %zu\", used, committed);\n+  assert(initial <= _heap->max_capacity(), \"sanity\");\n+  assert(committed <= _heap->max_capacity(), \"sanity\");\n+  assert(max <= _heap->max_capacity(), \"sanity\");\n@@ -89,0 +91,4 @@\n+  assert(initial <= _heap->max_capacity(), \"sanity\");\n+  assert(used <= _heap->max_capacity(), \"sanity\");\n+  assert(committed <= _heap->max_capacity(), \"sanity\");\n+  assert(max <= _heap->max_capacity(), \"sanity\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMemoryPool.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -147,2 +147,0 @@\n-  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed. However, we may\n-  \/\/ need to transfer memory to OLD in order to efficiently support the mixed evacuations that might immediately follow.\n@@ -151,12 +149,0 @@\n-\n-  ShenandoahGenerationalHeap::TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-    result = heap->balance_generations();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Old Mark\", &ls);\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -513,1 +513,1 @@\n-    size_t cset_young_regions, cset_old_regions;\n+    size_t young_trash_regions, old_trash_regions;\n@@ -515,4 +515,12 @@\n-    heap->free_set()->prepare_to_rebuild(cset_young_regions, cset_old_regions, first_old, last_old, num_old);\n-    \/\/ This is just old-gen completion.  No future budgeting required here.  The only reason to rebuild the freeset here\n-    \/\/ is in case there was any immediate old garbage identified.\n-    heap->free_set()->finish_rebuild(cset_young_regions, cset_old_regions, num_old);\n+    heap->free_set()->prepare_to_rebuild(young_trash_regions, old_trash_regions, first_old, last_old, num_old);\n+    \/\/ At the end of old-gen, we may find that we have reclaimed immediate garbage, allowing a longer allocation runway.\n+    \/\/ We may also find that we have accumulated canddiate regions for mixed evacuation.  If so, we will want to expand\n+    \/\/ the OldCollector reserve in order to make room for these mixed evacuations.\n+    assert(ShenandoahHeap::heap()->mode()->is_generational(), \"sanity\");\n+    assert(young_trash_regions == 0, \"sanity\");\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway =\n+      gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_trash_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions);\n+\n+    heap->free_set()->finish_rebuild(young_trash_regions, old_trash_regions, num_old);\n@@ -719,5 +727,0 @@\n-\n-  if (promotion) {\n-    \/\/ This evacuation was a promotion, track this as allocation against old gen\n-    increase_allocated(words * HeapWordSize);\n-  }\n@@ -829,0 +832,25 @@\n+\n+size_t ShenandoahOldGeneration::used() const {\n+  assert(type() == ShenandoahGenerationType::OLD, \"OO sanity\");\n+  return _free_set->old_used();\n+}\n+\n+size_t ShenandoahOldGeneration::bytes_allocated_since_gc_start() const {\n+  assert(type() == ShenandoahGenerationType::OLD, \"OO sanity\");\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"NON_GEN implies not generational\");\n+  return 0;\n+}\n+\n+size_t ShenandoahOldGeneration::get_affiliated_region_count() const {\n+  assert(type() == ShenandoahGenerationType::OLD, \"OO sanity\");\n+  return _free_set->old_affiliated_regions();\n+}\n+\n+size_t ShenandoahOldGeneration::get_humongous_waste() const {\n+  return _free_set->humongous_waste_in_old();\n+}\n+\n+size_t ShenandoahOldGeneration::used_regions() const {\n+  assert(type() == ShenandoahGenerationType::OLD, \"OO sanity\");\n+  return _free_set->old_affiliated_regions();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":38,"deletions":10,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -103,0 +103,5 @@\n+  \/\/ Use this only for unit testing.  Do not use for production.\n+  inline void set_capacity(size_t bytes) {\n+    ShenandoahHeap::heap()->free_set()->resize_old_collector_capacity(bytes \/ ShenandoahHeapRegion::region_size_bytes());\n+  }\n+\n@@ -138,1 +143,3 @@\n-  void set_region_balance(ssize_t balance) { _region_balance = balance; }\n+  void set_region_balance(ssize_t balance) {\n+    _region_balance = balance;\n+  }\n@@ -322,0 +329,5 @@\n+  size_t used() const override;\n+  size_t bytes_allocated_since_gc_start() const override;\n+  size_t get_affiliated_region_count() const override;\n+  size_t get_humongous_waste() const override;\n+  size_t used_regions() const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-ShenandoahSimpleBitMap::ShenandoahSimpleBitMap(size_t num_bits) :\n+ShenandoahSimpleBitMap::ShenandoahSimpleBitMap(index_type num_bits) :\n@@ -42,1 +42,1 @@\n-size_t ShenandoahSimpleBitMap::count_leading_ones(idx_t start_idx) const {\n+size_t ShenandoahSimpleBitMap::count_leading_ones(index_type start_idx) const {\n@@ -69,1 +69,1 @@\n-size_t ShenandoahSimpleBitMap::count_trailing_ones(idx_t last_idx) const {\n+size_t ShenandoahSimpleBitMap::count_trailing_ones(index_type last_idx) const {\n@@ -96,1 +96,1 @@\n-bool ShenandoahSimpleBitMap::is_forward_consecutive_ones(idx_t start_idx, idx_t count) const {\n+bool ShenandoahSimpleBitMap::is_forward_consecutive_ones(index_type start_idx, index_type count) const {\n@@ -100,1 +100,1 @@\n-    assert(start_idx + count <= (idx_t) _num_bits, \"precondition\");\n+    assert(start_idx + count <= (index_type) _num_bits, \"precondition\");\n@@ -126,1 +126,1 @@\n-bool ShenandoahSimpleBitMap::is_backward_consecutive_ones(idx_t last_idx, idx_t count) const {\n+bool ShenandoahSimpleBitMap::is_backward_consecutive_ones(index_type last_idx, index_type count) const {\n@@ -155,1 +155,1 @@\n-idx_t ShenandoahSimpleBitMap::find_first_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const {\n+index_type ShenandoahSimpleBitMap::find_first_consecutive_set_bits(index_type beg, index_type end, size_t num_bits) const {\n@@ -159,1 +159,1 @@\n-  idx_t start_boundary = end - num_bits;\n+  index_type start_boundary = end - num_bits;\n@@ -234,1 +234,1 @@\n-idx_t ShenandoahSimpleBitMap::find_last_consecutive_set_bits(const idx_t beg, idx_t end, const size_t num_bits) const {\n+index_type ShenandoahSimpleBitMap::find_last_consecutive_set_bits(const index_type beg, index_type end, const size_t num_bits) const {\n@@ -239,1 +239,1 @@\n-  idx_t last_boundary = beg + num_bits;\n+  index_type last_boundary = beg + num_bits;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-\/\/ idx_t is defined here as ssize_t.  In src\/hotspot\/share\/utiliities\/bitMap.hpp, idx is defined as size_t.\n+\/\/ index_type is defined here as ssize_t.  In src\/hotspot\/share\/utiliities\/bitMap.hpp, idx is defined as size_t.\n@@ -41,1 +41,1 @@\n-\/\/ The API and internal implementation of ShenandoahSimpleBitMap and ShenandoahRegionPartitions use idx_t to\n+\/\/ The API and internal implementation of ShenandoahSimpleBitMap and ShenandoahRegionPartitions use index_type to\n@@ -45,1 +45,1 @@\n-\/\/  2. Certain loops are written most naturally if the iterator, which may hold the sentinel -1 value, can be\n+\/\/  2. Certain loops are written most naturally if the induction variable, which may hold the sentinel -1 value, can be\n@@ -48,1 +48,1 @@\n-typedef ssize_t idx_t;\n+typedef ssize_t index_type;\n@@ -55,1 +55,1 @@\n-  const idx_t _num_bits;\n+  const index_type _num_bits;\n@@ -60,1 +60,1 @@\n-  ShenandoahSimpleBitMap(size_t num_bits);\n+  ShenandoahSimpleBitMap(index_type num_bits);\n@@ -74,1 +74,1 @@\n-  size_t count_leading_ones(idx_t start_idx) const;\n+  size_t count_leading_ones(index_type start_idx) const;\n@@ -78,1 +78,1 @@\n-  size_t count_trailing_ones(idx_t last_idx) const;\n+  size_t count_trailing_ones(index_type last_idx) const;\n@@ -80,2 +80,2 @@\n-  bool is_forward_consecutive_ones(idx_t start_idx, idx_t count) const;\n-  bool is_backward_consecutive_ones(idx_t last_idx, idx_t count) const;\n+  bool is_forward_consecutive_ones(index_type start_idx, index_type count) const;\n+  bool is_backward_consecutive_ones(index_type last_idx, index_type count) const;\n@@ -87,1 +87,1 @@\n-  inline idx_t aligned_index(idx_t idx) const {\n+  inline index_type aligned_index(index_type idx) const {\n@@ -89,1 +89,1 @@\n-    idx_t array_idx = idx & ~(BitsPerWord - 1);\n+    index_type array_idx = idx & ~(BitsPerWord - 1);\n@@ -93,1 +93,1 @@\n-  inline constexpr idx_t alignment() const {\n+  inline constexpr index_type alignment() const {\n@@ -98,1 +98,1 @@\n-  inline idx_t size() const {\n+  inline index_type size() const {\n@@ -103,1 +103,1 @@\n-  inline uintx bits_at(idx_t idx) const {\n+  inline uintx bits_at(index_type idx) const {\n@@ -105,1 +105,1 @@\n-    idx_t array_idx = idx >> LogBitsPerWord;\n+    index_type array_idx = idx >> LogBitsPerWord;\n@@ -109,1 +109,1 @@\n-  inline void set_bit(idx_t idx) {\n+  inline void set_bit(index_type idx) {\n@@ -117,1 +117,1 @@\n-  inline void clear_bit(idx_t idx) {\n+  inline void clear_bit(index_type idx) {\n@@ -119,1 +119,0 @@\n-    assert(idx >= 0, \"precondition\");\n@@ -126,1 +125,1 @@\n-  inline bool is_set(idx_t idx) const {\n+  inline bool is_set(index_type idx) const {\n@@ -128,1 +127,0 @@\n-    assert(idx >= 0, \"precondition\");\n@@ -137,1 +135,1 @@\n-  inline idx_t find_first_set_bit(idx_t beg) const;\n+  inline index_type find_first_set_bit(index_type beg) const;\n@@ -141,1 +139,1 @@\n-  inline idx_t find_first_set_bit(idx_t beg, idx_t end) const;\n+  inline index_type find_first_set_bit(index_type beg, index_type end) const;\n@@ -145,1 +143,1 @@\n-  inline idx_t find_last_set_bit(idx_t end) const;\n+  inline index_type find_last_set_bit(index_type end) const;\n@@ -149,1 +147,1 @@\n-  inline idx_t find_last_set_bit(idx_t beg, idx_t end) const;\n+  inline index_type find_last_set_bit(index_type beg, index_type end) const;\n@@ -154,1 +152,1 @@\n-  inline idx_t find_first_consecutive_set_bits(idx_t beg, size_t num_bits) const;\n+  inline index_type find_first_consecutive_set_bits(index_type beg, size_t num_bits) const;\n@@ -159,1 +157,1 @@\n-  idx_t find_first_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const;\n+  index_type find_first_consecutive_set_bits(index_type beg, index_type end, size_t num_bits) const;\n@@ -164,1 +162,1 @@\n-  inline idx_t find_last_consecutive_set_bits(idx_t end, size_t num_bits) const;\n+  inline index_type find_last_consecutive_set_bits(index_type end, size_t num_bits) const;\n@@ -169,1 +167,1 @@\n-  idx_t find_last_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const;\n+  index_type find_last_consecutive_set_bits(index_type beg, index_type end, size_t num_bits) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.hpp","additions":27,"deletions":29,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-inline idx_t ShenandoahSimpleBitMap::find_first_set_bit(idx_t beg, idx_t end) const {\n+inline index_type ShenandoahSimpleBitMap::find_first_set_bit(index_type beg, index_type end) const {\n@@ -52,1 +52,1 @@\n-      idx_t candidate_result = (array_idx * BitsPerWord) + bit_number + first_set_bit;\n+      index_type candidate_result = (array_idx * BitsPerWord) + bit_number + first_set_bit;\n@@ -62,1 +62,1 @@\n-inline idx_t ShenandoahSimpleBitMap::find_first_set_bit(idx_t beg) const {\n+inline index_type ShenandoahSimpleBitMap::find_first_set_bit(index_type beg) const {\n@@ -67,1 +67,1 @@\n-inline idx_t ShenandoahSimpleBitMap::find_last_set_bit(idx_t beg, idx_t end) const {\n+inline index_type ShenandoahSimpleBitMap::find_last_set_bit(index_type beg, index_type end) const {\n@@ -71,1 +71,1 @@\n-    idx_t array_idx = end >> LogBitsPerWord;\n+    index_type array_idx = end >> LogBitsPerWord;\n@@ -82,1 +82,1 @@\n-      idx_t candidate_result = array_idx * BitsPerWord + (bit_number - first_set_bit);\n+      index_type candidate_result = array_idx * BitsPerWord + (bit_number - first_set_bit);\n@@ -92,1 +92,1 @@\n-inline idx_t ShenandoahSimpleBitMap::find_last_set_bit(idx_t end) const {\n+inline index_type ShenandoahSimpleBitMap::find_last_set_bit(index_type end) const {\n@@ -97,1 +97,1 @@\n-inline idx_t ShenandoahSimpleBitMap::find_first_consecutive_set_bits(idx_t beg, size_t num_bits) const {\n+inline index_type ShenandoahSimpleBitMap::find_first_consecutive_set_bits(index_type beg, size_t num_bits) const {\n@@ -102,1 +102,1 @@\n-inline idx_t ShenandoahSimpleBitMap::find_last_consecutive_set_bits(idx_t end, size_t num_bits) const {\n+inline index_type ShenandoahSimpleBitMap::find_last_consecutive_set_bits(index_type end, size_t num_bits) const {\n@@ -104,1 +104,1 @@\n-  return find_last_consecutive_set_bits((idx_t) -1, end, num_bits);\n+  return find_last_consecutive_set_bits((index_type) -1, end, num_bits);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -378,1 +378,2 @@\n-  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions;\n+  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions, _trashed_used;\n+  size_t _region_size_bytes, _min_free_size;\n@@ -381,1 +382,6 @@\n-      _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0) {};\n+     _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0), _trashed_used(0)\n+  {\n+    _region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    \/\/ Retired regions are not necessarily filled, thouugh their remnant memory is considered used.\n+    _min_free_size = PLAB::min_size() * HeapWordSize;\n+  };\n@@ -384,8 +390,26 @@\n-    _used += r->used();\n-    _garbage += r->garbage();\n-    _committed += r->is_committed() ? ShenandoahHeapRegion::region_size_bytes() : 0;\n-    if (r->is_humongous()) {\n-      _humongous_waste += r->free();\n-    }\n-    if (r->is_trash()) {\n-      _trashed_regions++;\n+    if (r->is_cset() || r->is_trash()) {\n+      \/\/ Count the entire cset or trashed (formerly cset) region as used\n+      \/\/ Note: Immediate garbage trash regions were never in the cset.\n+      _used += _region_size_bytes;\n+      _garbage += _region_size_bytes - r->get_live_data_bytes();\n+      if (r->is_trash()) {\n+        _trashed_regions++;\n+        _trashed_used += _region_size_bytes;\n+      }\n+    } else {\n+      if (r->is_humongous()) {\n+        _used += _region_size_bytes;\n+        _garbage += _region_size_bytes - r->get_live_data_bytes();\n+        _humongous_waste += r->free();\n+      } else {\n+        size_t alloc_capacity = r->free();\n+        if (alloc_capacity < _min_free_size) {\n+          \/\/ this region has been retired already, count it as entirely consumed\n+          alloc_capacity = 0;\n+        }\n+        size_t bytes_used_in_region = _region_size_bytes - alloc_capacity;\n+        size_t bytes_garbage_in_region = bytes_used_in_region - r->get_live_data_bytes();\n+        size_t waste_bytes = r->free();\n+        _used += bytes_used_in_region;\n+        _garbage += bytes_garbage_in_region;\n+      }\n@@ -393,0 +417,1 @@\n+    _committed += r->is_committed() ? _region_size_bytes : 0;\n@@ -399,0 +424,1 @@\n+  size_t used_after_recycle() const { return _used - _trashed_used; }\n@@ -402,0 +428,1 @@\n+  size_t trashed_regions() const { return _trashed_regions; }\n@@ -411,3 +438,3 @@\n-  ShenandoahCalculateRegionStatsClosure old;\n-  ShenandoahCalculateRegionStatsClosure young;\n-  ShenandoahCalculateRegionStatsClosure global;\n+  ShenandoahCalculateRegionStatsClosure _old;\n+  ShenandoahCalculateRegionStatsClosure _young;\n+  ShenandoahCalculateRegionStatsClosure _global;\n@@ -420,2 +447,2 @@\n-        young.heap_region_do(r);\n-        global.heap_region_do(r);\n+        _young.heap_region_do(r);\n+        _global.heap_region_do(r);\n@@ -424,2 +451,2 @@\n-        old.heap_region_do(r);\n-        global.heap_region_do(r);\n+        _old.heap_region_do(r);\n+        _global.heap_region_do(r);\n@@ -439,1 +466,1 @@\n-  static void validate_usage(const bool adjust_for_padding,\n+  static void validate_usage(const bool adjust_for_padding, const bool adjust_for_trash,\n@@ -444,4 +471,0 @@\n-    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n-      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n-      generation_used += pad;\n-    }\n@@ -449,1 +472,2 @@\n-    guarantee(stats.used() == generation_used,\n+    size_t stats_used = adjust_for_trash? stats.used_after_recycle(): stats.used();\n+    guarantee(stats_used == generation_used,\n@@ -451,1 +475,1 @@\n-              label, generation->name(), PROPERFMTARGS(generation_used), PROPERFMTARGS(stats.used()));\n+              label, generation->name(), PROPERFMTARGS(generation_used), PROPERFMTARGS(stats_used));\n@@ -453,3 +477,5 @@\n-    guarantee(stats.regions() == generation_used_regions,\n-              \"%s: generation (%s) used regions (%zu) must equal regions that are in use (%zu)\",\n-              label, generation->name(), generation->used_regions(), stats.regions());\n+    size_t stats_regions = adjust_for_trash? stats.regions() - stats.trashed_regions(): stats.regions();\n+    guarantee(stats_regions == generation_used_regions,\n+              \"%s: generation (%s) used regions (%zu) must equal regions that are in use (%zu)%s\",\n+              label, generation->name(), generation->used_regions(), stats_regions,\n+              adjust_for_trash? \" (after adjusting for trash)\": \"\");\n@@ -472,1 +498,1 @@\n-private:\n+  private:\n@@ -476,1 +502,1 @@\n-public:\n+  public:\n@@ -478,3 +504,3 @@\n-    _heap(ShenandoahHeap::heap()),\n-    _phase(phase),\n-    _regions(regions) {};\n+      _heap(ShenandoahHeap::heap()),\n+      _phase(phase),\n+      _regions(regions) {};\n@@ -567,1 +593,1 @@\n-private:\n+  private:\n@@ -575,1 +601,1 @@\n-public:\n+  public:\n@@ -580,7 +606,7 @@\n-    WorkerTask(\"Shenandoah Verifier Reachable Objects\"),\n-    _label(label),\n-    _options(options),\n-    _heap(ShenandoahHeap::heap()),\n-    _ld(ld),\n-    _bitmap(bitmap),\n-    _processed(0) {};\n+      WorkerTask(\"Shenandoah Verifier Reachable Objects\"),\n+      _label(label),\n+      _options(options),\n+      _heap(ShenandoahHeap::heap()),\n+      _ld(ld),\n+      _bitmap(bitmap),\n+      _processed(0) {};\n@@ -602,8 +628,8 @@\n-        ShenandoahVerifyOopClosure cl(&stack, _bitmap, _ld,\n-                                      ShenandoahMessageBuffer(\"%s, Roots\", _label),\n-                                      _options);\n-        if (_heap->unload_classes()) {\n-          ShenandoahRootVerifier::strong_roots_do(&cl);\n-        } else {\n-          ShenandoahRootVerifier::roots_do(&cl);\n-        }\n+      ShenandoahVerifyOopClosure cl(&stack, _bitmap, _ld,\n+                                    ShenandoahMessageBuffer(\"%s, Roots\", _label),\n+                                    _options);\n+      if (_heap->unload_classes()) {\n+        ShenandoahRootVerifier::strong_roots_do(&cl);\n+      } else {\n+        ShenandoahRootVerifier::roots_do(&cl);\n+      }\n@@ -630,1 +656,1 @@\n-public:\n+  public:\n@@ -640,1 +666,1 @@\n-private:\n+  private:\n@@ -650,1 +676,1 @@\n-public:\n+  public:\n@@ -655,9 +681,9 @@\n-          WorkerTask(\"Shenandoah Verifier Marked Objects\"),\n-          _label(label),\n-          _options(options),\n-          _heap(ShenandoahHeap::heap()),\n-          _bitmap(bitmap),\n-          _ld(ld),\n-          _claimed(0),\n-          _processed(0),\n-          _generation(nullptr) {\n+      WorkerTask(\"Shenandoah Verifier Marked Objects\"),\n+      _label(label),\n+      _options(options),\n+      _heap(ShenandoahHeap::heap()),\n+      _bitmap(bitmap),\n+      _ld(ld),\n+      _claimed(0),\n+      _processed(0),\n+      _generation(nullptr) {\n@@ -773,1 +799,1 @@\n-private:\n+  private:\n@@ -775,1 +801,1 @@\n-         char const _expected;\n+  char const _expected;\n@@ -777,1 +803,1 @@\n-public:\n+  public:\n@@ -878,1 +904,2 @@\n-      heap_used = _heap->used() + _heap->old_generation()->get_pad_for_promote_in_place();\n+      \/\/ but this padding is already represented in _heap->used()\n+      heap_used = _heap->used();\n@@ -883,1 +910,2 @@\n-      guarantee(cl.used() == heap_used,\n+      size_t cl_size = (sizeness == _verify_size_exact_including_trash)? cl.used(): cl.used_after_recycle();\n+      guarantee(cl_size == heap_used,\n@@ -887,1 +915,1 @@\n-                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+                byte_size_in_proper_unit(cl_size), proper_unit_for_byte_size(cl_size));\n@@ -934,3 +962,3 @@\n-      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(),    cl.old);\n-      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(),  cl.young);\n-      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(),    cl._old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(),  cl._young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl._global);\n@@ -939,7 +967,8 @@\n-      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n-      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->young_generation(), cl.young);\n-      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->global_generation(), cl.global);\n-    } else if (sizeness == _verify_size_exact) {\n-      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n-      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->young_generation(), cl.young);\n-      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->global_generation(), cl.global);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, true, label, _heap->old_generation(), cl._old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, true, label, _heap->young_generation(), cl._young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, true, label, _heap->global_generation(), cl._global);\n+    } else if (sizeness == _verify_size_exact || sizeness == _verify_size_exact_including_trash) {\n+      bool adjust_trash = (sizeness == _verify_size_exact);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, adjust_trash, label, _heap->old_generation(), cl._old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, adjust_trash, label, _heap->young_generation(), cl._young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, adjust_trash, label, _heap->global_generation(), cl._global);\n@@ -1161,1 +1190,2 @@\n-          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+                                       \/\/ expect generation and heap sizes to match exactly, including trash\n+          _verify_size_exact_including_trash,\n@@ -1425,3 +1455,3 @@\n-  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->old_generation(), cl.old);\n-  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->young_generation(), cl.young);\n-  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->global_generation(), cl.global);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, true, \"Before free set rebuild\", _heap->old_generation(), cl._old);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, true, \"Before free set rebuild\", _heap->young_generation(), cl._young);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, true, \"Before free set rebuild\", _heap->global_generation(), cl._global);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":111,"deletions":81,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -158,1 +158,4 @@\n-    _verify_size_adjusted_for_padding\n+    _verify_size_adjusted_for_padding,\n+\n+    \/\/ Expected heap size should not include\n+    _verify_size_exact_including_trash\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -98,0 +98,26 @@\n+size_t ShenandoahYoungGeneration::used() const {\n+  assert(type() == ShenandoahGenerationType::YOUNG, \"OO sanity\");\n+  return _free_set->young_used();\n+}\n+\n+size_t ShenandoahYoungGeneration::bytes_allocated_since_gc_start() const {\n+  assert(type() == ShenandoahGenerationType::YOUNG, \"OO sanity\");\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Young implies generational\");\n+  return _free_set->get_bytes_allocated_since_gc_start();\n+}\n+\n+size_t ShenandoahYoungGeneration::get_affiliated_region_count() const {\n+  assert(type() == ShenandoahGenerationType::YOUNG, \"OO sanity\");\n+  return _free_set->young_affiliated_regions();\n+}\n+\n+size_t ShenandoahYoungGeneration::get_humongous_waste() const {\n+  assert(type() == ShenandoahGenerationType::YOUNG, \"OO sanity\");\n+  return _free_set->humongous_waste_in_mutator();\n+}\n+\n+size_t ShenandoahYoungGeneration::used_regions() const {\n+  assert(type() == ShenandoahGenerationType::YOUNG, \"OO sanity\");\n+  return _free_set->young_affiliated_regions();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -76,0 +76,6 @@\n+  size_t used() const override;\n+  size_t bytes_allocated_since_gc_start() const override;\n+  size_t get_affiliated_region_count() const override;\n+  size_t get_humongous_waste() const override;\n+  size_t used_regions() const override;\n+\n@@ -78,1 +84,0 @@\n-  \/\/ Do not override available_with_reserve() because that needs to see memory reserved for Collector\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n- * @run main\/othervm\/timeout=240 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ * @run main\/othervm\/timeout=480 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n@@ -153,0 +153,13 @@\n+\/*\n+ * @test id=no-tlab-genshen\n+ * @summary Acceptance tests: collector can deal with retained objects\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm\/timeout=480 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      TestSieveObjects\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestSieveObjects.java","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -115,2 +115,4 @@\n-    static final long HEAP_MB = 128;                           \/\/ adjust for test configuration above\n-    static final long TARGET_MB = Long.getLong(\"target\", 2_000); \/\/ 2 Gb allocation\n+    static final long HEAP_MB = 128;                                      \/\/ adjust for test configuration above\n+    static final long TARGET_MB = Long.getLong(\"target\", 2_000);          \/\/ 2 Gb allocation\n+    static final long ANTICIPATED_HUMONGOUS_WASTE_PER_ARRAY = 124_272;\n+\n@@ -162,1 +164,1 @@\n-        long mem = count * (16 + 4 * size);\n+        long mem = count * (16 + 4 * size + ANTICIPATED_HUMONGOUS_WASTE_PER_ARRAY);\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestChurnNotifications.java","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"}]}