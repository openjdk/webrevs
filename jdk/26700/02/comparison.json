{"files":[{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/gcArguments.hpp\"\n@@ -40,1 +41,1 @@\n-MutableNUMASpace::MutableNUMASpace(size_t alignment) : MutableSpace(alignment), _must_use_large_pages(false) {\n+MutableNUMASpace::MutableNUMASpace(size_t page_size) : MutableSpace(page_size) {\n@@ -42,1 +43,0 @@\n-  _page_size = os::vm_page_size();\n@@ -46,9 +46,0 @@\n-#ifdef LINUX\n-  \/\/ Changing the page size can lead to freeing of memory. When using large pages\n-  \/\/ and the memory has been both reserved and committed, Linux does not support\n-  \/\/ freeing parts of it.\n-    if (UseLargePages && !os::can_commit_large_page_memory()) {\n-      _must_use_large_pages = true;\n-    }\n-#endif \/\/ LINUX\n-\n@@ -63,1 +54,1 @@\n-    lgrp_spaces()->append(new LGRPSpace(lgrp_ids[i], alignment));\n+    lgrp_spaces()->append(new LGRPSpace(lgrp_ids[i], page_size));\n@@ -192,15 +183,5 @@\n-  HeapWord *start = align_up(mr.start(), page_size());\n-  HeapWord *end = align_down(mr.end(), page_size());\n-  if (end > start) {\n-    MemRegion aligned_region(start, end);\n-    assert((intptr_t)aligned_region.start()     % page_size() == 0 &&\n-           (intptr_t)aligned_region.byte_size() % page_size() == 0, \"Bad alignment\");\n-    assert(region().contains(aligned_region), \"Sanity\");\n-    \/\/ First we tell the OS which page size we want in the given range. The underlying\n-    \/\/ large page can be broken down if we require small pages.\n-    const size_t os_align = UseLargePages ? page_size() : os::vm_page_size();\n-    os::realign_memory((char*)aligned_region.start(), aligned_region.byte_size(), os_align);\n-    \/\/ Then we uncommit the pages in the range.\n-    os::disclaim_memory((char*)aligned_region.start(), aligned_region.byte_size());\n-    \/\/ And make them local\/first-touch biased.\n-    os::numa_make_local((char*)aligned_region.start(), aligned_region.byte_size(), checked_cast<int>(lgrp_id));\n+  assert(is_aligned(mr.start(), page_size()), \"precondition\");\n+  assert(is_aligned(mr.end(), page_size()), \"precondition\");\n+\n+  if (mr.is_empty()) {\n+    return;\n@@ -208,0 +189,7 @@\n+  \/\/ First we tell the OS which page size we want in the given range. The underlying\n+  \/\/ large page can be broken down if we require small pages.\n+  os::realign_memory((char*) mr.start(), mr.byte_size(), SpaceAlignment);\n+  \/\/ Then we uncommit the pages in the range.\n+  os::disclaim_memory((char*) mr.start(), mr.byte_size());\n+  \/\/ And make them local\/first-touch biased.\n+  os::numa_make_local((char*)mr.start(), mr.byte_size(), checked_cast<int>(lgrp_id));\n@@ -256,1 +244,2 @@\n-  return base_space_size() \/ lgrp_spaces()->length() * page_size();\n+  \/\/ The number of pages may not be evenly divided.\n+  return align_down(capacity_in_bytes() \/ lgrp_spaces()->length(), page_size());\n@@ -263,1 +252,1 @@\n-  size_t pages_available = base_space_size();\n+  size_t pages_available = capacity_in_bytes() \/ page_size();\n@@ -309,0 +298,5 @@\n+  assert(is_aligned(new_region.start(), page_size()), \"precondition\");\n+  assert(is_aligned(new_region.end(), page_size()), \"precondition\");\n+  assert(is_aligned(intersection.start(), page_size()), \"precondition\");\n+  assert(is_aligned(intersection.end(), page_size()), \"precondition\");\n+\n@@ -311,12 +305,0 @@\n-    \/\/ Try to coalesce small pages into a large one.\n-    if (UseLargePages && page_size() >= alignment()) {\n-      HeapWord* p = align_up(intersection.start(), alignment());\n-      if (new_region.contains(p)\n-          && pointer_delta(p, new_region.start(), sizeof(char)) >= alignment()) {\n-        if (intersection.contains(p)) {\n-          intersection = MemRegion(p, intersection.end());\n-        } else {\n-          intersection = MemRegion(p, p);\n-        }\n-      }\n-    }\n@@ -330,12 +312,0 @@\n-    \/\/ Try to coalesce small pages into a large one.\n-    if (UseLargePages && page_size() >= alignment()) {\n-      HeapWord* p = align_down(intersection.end(), alignment());\n-      if (new_region.contains(p)\n-          && pointer_delta(new_region.end(), p, sizeof(char)) >= alignment()) {\n-        if (intersection.contains(p)) {\n-          intersection = MemRegion(intersection.start(), p);\n-        } else {\n-          intersection = MemRegion(p, p);\n-        }\n-      }\n-    }\n@@ -355,0 +325,2 @@\n+  assert(is_aligned(mr.start(), page_size()), \"precondition\");\n+  assert(is_aligned(mr.end(), page_size()), \"precondition\");\n@@ -362,18 +334,5 @@\n-  \/\/ Compute chunk sizes\n-  size_t prev_page_size = page_size();\n-  set_page_size(alignment());\n-  HeapWord* rounded_bottom = align_up(bottom(), page_size());\n-  HeapWord* rounded_end = align_down(end(), page_size());\n-  size_t base_space_size_pages = pointer_delta(rounded_end, rounded_bottom, sizeof(char)) \/ page_size();\n-\n-  \/\/ Try small pages if the chunk size is too small\n-  if (base_space_size_pages \/ lgrp_spaces()->length() == 0\n-      && page_size() > os::vm_page_size()) {\n-    \/\/ Changing the page size below can lead to freeing of memory. So we fail initialization.\n-    if (_must_use_large_pages) {\n-      vm_exit_during_initialization(\"Failed initializing NUMA with large pages. Too small heap size\");\n-    }\n-    set_page_size(os::vm_page_size());\n-    rounded_bottom = align_up(bottom(), page_size());\n-    rounded_end = align_down(end(), page_size());\n-    base_space_size_pages = pointer_delta(rounded_end, rounded_bottom, sizeof(char)) \/ page_size();\n+  size_t num_pages = mr.byte_size() \/ page_size();\n+\n+  if (num_pages < (size_t)lgrp_spaces()->length()) {\n+    vm_exit_during_initialization(err_msg(\"Failed initializing NUMA, #pages-per-CPU is less than one: space-size: %zu, page-size: %zu, #CPU: %d\",\n+      mr.byte_size(), page_size(), lgrp_spaces()->length()));\n@@ -381,2 +340,0 @@\n-  guarantee(base_space_size_pages \/ lgrp_spaces()->length() > 0, \"Space too small\");\n-  set_base_space_size(base_space_size_pages);\n@@ -387,1 +344,1 @@\n-    new_region = MemRegion(rounded_bottom, rounded_end);\n+    new_region = mr;\n@@ -389,4 +346,1 @@\n-    if (intersection.start() == nullptr ||\n-        intersection.end() == nullptr   ||\n-        prev_page_size > page_size()) { \/\/ If the page size got smaller we have to change\n-                                        \/\/ the page size preference for the whole space.\n+    if (intersection.is_empty()) {\n@@ -439,1 +393,1 @@\n-        new_region = MemRegion(bottom(), rounded_bottom + (chunk_byte_size >> LogHeapWordSize));\n+        new_region = MemRegion(bottom(), chunk_byte_size >> LogHeapWordSize);\n@@ -443,1 +397,1 @@\n-    } else\n+    } else {\n@@ -447,1 +401,1 @@\n-                               ps->end() + (chunk_byte_size >> LogHeapWordSize));\n+                               chunk_byte_size >> LogHeapWordSize);\n@@ -452,0 +406,1 @@\n+    }\n@@ -478,2 +433,0 @@\n-\n-    set_adaptation_cycles(samples_count());\n@@ -481,0 +434,1 @@\n+  set_adaptation_cycles(samples_count());\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":37,"deletions":83,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -85,2 +85,2 @@\n-    LGRPSpace(uint l, size_t alignment) : _lgrp_id(l), _allocation_failed(false) {\n-      _space = new MutableSpace(alignment);\n+    LGRPSpace(uint l, size_t page_size) : _lgrp_id(l), _allocation_failed(false) {\n+      _space = new MutableSpace(page_size);\n@@ -122,1 +122,0 @@\n-  size_t _page_size;\n@@ -125,5 +124,0 @@\n-  bool _must_use_large_pages;\n-\n-  void set_page_size(size_t psz)                     { _page_size = psz;          }\n-  size_t page_size() const                           { return _page_size;         }\n-\n@@ -138,1 +132,0 @@\n-  size_t base_space_size() const                     { return _base_space_size;   }\n@@ -159,1 +152,1 @@\n-  MutableNUMASpace(size_t alignment);\n+  MutableNUMASpace(size_t page_size);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.hpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-MutableSpace::MutableSpace(size_t alignment) :\n+MutableSpace::MutableSpace(size_t page_size) :\n@@ -39,1 +39,1 @@\n-  _alignment(alignment),\n+  _page_size(page_size),\n@@ -42,5 +42,1 @@\n-  _end(nullptr)\n-{\n-  assert(MutableSpace::alignment() % os::vm_page_size() == 0,\n-         \"Space should be aligned\");\n-}\n+  _end(nullptr) {}\n@@ -48,12 +44,6 @@\n-void MutableSpace::numa_setup_pages(MemRegion mr, size_t page_size, bool clear_space) {\n-  if (!mr.is_empty()) {\n-    HeapWord *start = align_up(mr.start(), page_size);\n-    HeapWord *end =   align_down(mr.end(), page_size);\n-    if (end > start) {\n-      size_t size = pointer_delta(end, start, sizeof(char));\n-      if (clear_space) {\n-        \/\/ Prefer page reallocation to migration.\n-        os::disclaim_memory((char*)start, size);\n-      }\n-      os::numa_make_global((char*)start, size);\n-    }\n+void MutableSpace::numa_setup_pages(MemRegion mr, bool clear_space) {\n+  assert(is_aligned(mr.start(), page_size()), \"precondition\");\n+  assert(is_aligned(mr.end(), page_size()), \"precondition\");\n+\n+  if (mr.is_empty()) {\n+    return;\n@@ -61,0 +51,6 @@\n+\n+  if (clear_space) {\n+    \/\/ Prefer page reallocation to migration.\n+    os::disclaim_memory((char*) mr.start(), mr.byte_size());\n+  }\n+  os::numa_make_global((char*) mr.start(), mr.byte_size());\n@@ -108,2 +104,0 @@\n-    size_t page_size = alignment();\n-\n@@ -111,2 +105,2 @@\n-      numa_setup_pages(head, page_size, clear_space);\n-      numa_setup_pages(tail, page_size, clear_space);\n+      numa_setup_pages(head, clear_space);\n+      numa_setup_pages(tail, clear_space);\n@@ -116,1 +110,0 @@\n-      size_t pretouch_page_size = UseLargePages ? page_size : os::vm_page_size();\n@@ -118,1 +111,1 @@\n-                             pretouch_page_size, pretouch_workers);\n+                             page_size(), pretouch_workers);\n@@ -121,1 +114,1 @@\n-                             pretouch_page_size, pretouch_workers);\n+                             page_size(), pretouch_workers);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":19,"deletions":26,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-  size_t _alignment;\n+  size_t _page_size;\n@@ -59,1 +59,1 @@\n-  void numa_setup_pages(MemRegion mr, size_t page_size, bool clear_space);\n+  void numa_setup_pages(MemRegion mr, bool clear_space);\n@@ -64,1 +64,4 @@\n- public:\n+protected:\n+  size_t page_size() const                           { return _page_size;         }\n+\n+public:\n@@ -80,2 +83,0 @@\n-  size_t alignment()                       { return _alignment; }\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -50,1 +50,4 @@\n-  ReservedSpace backing_store = MemoryReserver::reserve(bytes_to_reserve, mtGC);\n+  ReservedSpace backing_store = MemoryReserver::reserve(bytes_to_reserve,\n+                                                        os::vm_allocation_granularity(),\n+                                                        os::vm_page_size(),\n+                                                        mtGC);\n","filename":"src\/hotspot\/share\/gc\/parallel\/objectStartArray.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -106,5 +106,0 @@\n-\/\/ The alignment used for spaces in young gen and old gen\n-static size_t default_space_alignment() {\n-  return 64 * K * HeapWordSize;\n-}\n-\n@@ -114,1 +109,1 @@\n-  SpaceAlignment = default_space_alignment();\n+  SpaceAlignment = ParallelScavengeHeap::default_space_alignment();\n@@ -126,0 +121,5 @@\n+  if (!UseLargePages) {\n+    return;\n+  }\n+\n+  \/\/ If using large-page, need to update SpaceAlignment so that spaces are page-size aligned.\n@@ -129,0 +129,5 @@\n+  if (page_sz == os::vm_page_size()) {\n+    log_warning(gc, heap)(\"MinHeapSize (%zu) must be large enough for 4 * page-size; Disabling UseLargePages for heap\", MinHeapSize);\n+    return;\n+  }\n+  \/\/ Using largepage\n@@ -131,1 +136,2 @@\n-  size_t new_alignment = align_up(page_sz, SpaceAlignment);\n+  \/\/ Space is largepage-aligned.\n+  size_t new_alignment = page_sz;\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -68,1 +68,25 @@\n-  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_heap_size, HeapAlignment);\n+  \/\/ If using largepage, SpaceAlignment is the desired largepage size.\n+  size_t desired_page_size = (SpaceAlignment == default_space_alignment())\n+                           ? os::vm_page_size()\n+                           : SpaceAlignment;\n+  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_heap_size, HeapAlignment, desired_page_size);\n+  \/\/ Check if SpaceAlignment needs adjustment\n+  if (UseLargePages) {\n+    if (SpaceAlignment == default_space_alignment()) {\n+      \/\/ Opted out of using largepage because MinHeapSize is too small.\n+      assert(!is_aligned(SpaceAlignment, os::large_page_size()), \"inv\");\n+      assert(heap_rs.page_size() == os::vm_page_size(), \"inv\");\n+    } else {\n+      \/\/ Opted in to using largepage\n+      if (os::can_commit_large_page_memory()) {\n+        \/\/ Keep SpaceAlignment as is so that largepage can be formed\n+      } else {\n+        \/\/ Explicit largepage; use actual pagesize or the default\n+        SpaceAlignment = MAX2(heap_rs.page_size(), default_space_alignment());\n+      }\n+    }\n+  } else {\n+    assert(heap_rs.page_size() == os::vm_page_size(), \"inv\");\n+    assert(SpaceAlignment == default_space_alignment(), \"inv\");\n+  }\n+  assert(is_aligned(SpaceAlignment, heap_rs.page_size()), \"inv\");\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":25,"deletions":1,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -134,0 +134,5 @@\n+  \/\/ The alignment used for spaces in young gen and old gen\n+  static size_t default_space_alignment() {\n+    return 64 * K * HeapWordSize;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-  _object_space = new MutableSpace(virtual_space()->alignment());\n+  _object_space = new MutableSpace(virtual_space()->page_size());\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,2 +32,2 @@\n-  _alignment(alignment)\n-{\n+  _alignment(alignment),\n+  _page_size(rs.page_size()) {\n@@ -91,1 +91,2 @@\n-  assert(is_aligned(_alignment, os::vm_page_size()), \"bad alignment\");\n+  assert(is_aligned(_page_size, os::vm_page_size()), \"bad alignment\");\n+  assert(is_aligned(_alignment, _page_size), \"inv\");\n","filename":"src\/hotspot\/share\/gc\/parallel\/psVirtualspace.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -44,0 +44,3 @@\n+  \/\/ OS page size used. If using transparent large pages, it's the ordinary page-size.\n+  const size_t _page_size;\n+\n@@ -71,0 +74,1 @@\n+  size_t page_size()          const { return _page_size; }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psVirtualspace.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-    _eden_space = new MutableNUMASpace(virtual_space()->alignment());\n+    _eden_space = new MutableNUMASpace(virtual_space()->page_size());\n@@ -88,1 +88,1 @@\n-    _eden_space = new MutableSpace(virtual_space()->alignment());\n+    _eden_space = new MutableSpace(virtual_space()->page_size());\n@@ -90,2 +90,2 @@\n-  _from_space = new MutableSpace(virtual_space()->alignment());\n-  _to_space   = new MutableSpace(virtual_space()->alignment());\n+  _from_space = new MutableSpace(virtual_space()->page_size());\n+  _to_space   = new MutableSpace(virtual_space()->page_size());\n","filename":"src\/hotspot\/share\/gc\/parallel\/psYoungGen.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  nonstatic_field(PSVirtualSpace,              _page_size,                                    const size_t)                          \\\n","filename":"src\/hotspot\/share\/gc\/parallel\/vmStructs_parallelgc.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -952,1 +952,1 @@\n-ReservedHeapSpace Universe::reserve_heap(size_t heap_size, size_t alignment) {\n+ReservedHeapSpace Universe::reserve_heap(size_t heap_size, size_t alignment, size_t desired_page_size) {\n@@ -963,3 +963,7 @@\n-  size_t page_size = os::vm_page_size();\n-  if (UseLargePages && is_aligned(alignment, os::large_page_size())) {\n-    page_size = os::large_page_size();\n+  size_t page_size;\n+  if (desired_page_size == 0) {\n+    if (UseLargePages) {\n+      page_size = os::large_page_size();\n+    } else {\n+      page_size = os::vm_page_size();\n+    }\n@@ -969,1 +973,3 @@\n-    assert(!UseLargePages || UseParallelGC , \"Wrong alignment to use large pages\");\n+    assert(UseParallelGC , \"only Parallel\");\n+    \/\/ Use caller provided value.\n+    page_size = desired_page_size;\n@@ -971,1 +977,1 @@\n-\n+  assert(is_aligned(heap_size, page_size), \"inv\");\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -315,1 +315,1 @@\n-  static ReservedHeapSpace reserve_heap(size_t heap_size, size_t alignment);\n+  static ReservedHeapSpace reserve_heap(size_t heap_size, size_t alignment, size_t desired_page_size = 0);\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}