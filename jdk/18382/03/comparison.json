{"files":[{"patch":"@@ -1422,0 +1422,4 @@\n+  \/\/ Vector Widening Integer Reduction Instructions\n+  INSN(vwredsum_vs,    0b1010111, 0b000, 0b110001);\n+  INSN(vwredsumu_vs,   0b1010111, 0b000, 0b110000);\n+\n@@ -1460,0 +1464,4 @@\n+  \/\/ Vector Widening Integer Multiply Instructions\n+  INSN(vwmul_vv,    0b1010111, 0b010, 0b111011);\n+  INSN(vwmulu_vv,   0b1010111, 0b010, 0b111000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5045,0 +5045,252 @@\n+  void generate_updateBytesAdler32_accum(Register buff, Register temp0,\n+          VectorRegister vtemp1, VectorRegister vtemp2, VectorRegister vtemp3, VectorRegister vzero,\n+          VectorRegister vbytes, VectorRegister vs1acc, VectorRegister vs2acc, VectorRegister vtable) {\n+    \/\/ Below is a vectorized implementation of updating s1 and s2 for 16 bytes.\n+    \/\/ We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.\n+    \/\/ In non-vectorized code, we update s1 and s2 as:\n+    \/\/   s1 <- s1 + b1\n+    \/\/   s2 <- s2 + s1\n+    \/\/   s1 <- s1 + b2\n+    \/\/   s2 <- s2 + b1\n+    \/\/   ...\n+    \/\/   s1 <- s1 + b16\n+    \/\/   s2 <- s2 + s1\n+    \/\/ Putting above assignments together, we have:\n+    \/\/   s1_new = s1 + b1 + b2 + ... + b16\n+    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n+    \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n+    \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n+    \/\/\n+    \/\/ To add more acceleration, it is possible to postpone final summation\n+    \/\/ until all unrolled calculations are done.\n+\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+\n+    \/\/ Load data\n+    __ vle8_v(vbytes, buff);\n+    __ addi(buff, buff, 16);\n+\n+    \/\/ vs1acc = b1 + b2 + b3 + ... + b16\n+    __ vwredsumu_vs(vs1acc, vbytes, vzero);\n+\n+    \/\/ vs2acc = (b1 * 16) + (b2 * 15) + (b3 * 14) + ... + (b16 * 1)\n+    __ vwmulu_vv(vtemp1, vtable, vbytes); \/\/ vtemp2 now contains the second part of multiplication\n+    __ vsetivli(temp0, 8, Assembler::e16, Assembler::m1);\n+    __ vadd_vv(vtemp3, vtemp1, vtemp2); \/\/ 0x14 * 0xFF * 2 = 0x27D8 -- max value per element, \n+                                        \/\/ so no need to do vector-widening operation\n+    __ vwredsumu_vs(vs2acc, vtemp3, vzero);\n+  }\n+\n+  \/***\n+   *  int java.util.zip.Adler32.updateBytes(int adler, byte[] b, int off, int len)\n+   *\n+   *  Arguments:\n+   *\n+   *  Inputs:\n+   *   c_rarg0   - int   adler\n+   *   c_rarg1   - byte* buff (b + off)\n+   *   c_rarg2   - int   len\n+   *\n+   * Output:\n+   *   c_rarg0   - int adler result\n+   *\/\n+  address generate_updateBytesAdler32() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+    address start = __ pc();\n+\n+    Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_nmax_loop_entry, L_by16, L_by16_loop, L_by1_loop, L_do_mod, L_combine, L_by1;\n+\n+    \/\/ Aliases\n+    Register adler  = c_rarg0;\n+    Register s1     = c_rarg0;\n+    Register s2     = c_rarg3;\n+    Register buff   = c_rarg1;\n+    Register len    = c_rarg2;\n+    Register nmax  = x29; \/\/ t4\n+    Register base  = x30; \/\/ t5\n+    Register count = x31; \/\/ t6\n+    Register temp0 = c_rarg4;\n+    Register temp1 = c_rarg5;\n+    Register buf_end = c_rarg6;\n+    VectorRegister vbytes = v1;\n+    VectorRegister vtable = v3;\n+    VectorRegister vtemp1 = v4;\n+    VectorRegister vtemp2 = v5;\n+    VectorRegister vtemp3 = v30;\n+    VectorRegister vzero = v12;\n+\n+    VectorRegister unroll_regs[] = {\n+      v13, v14, v15, v16, v17, v18, v19, v20,\n+      v21, v22, v23, v24, v25, v26, v27, v28,\n+    };\n+\n+    \/\/ Max number of bytes we can process before having to take the mod\n+    \/\/ 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)\/2 + (n+1)(BASE-1) <= 2^32-1\n+    const uint64_t BASE = 0xfff1;\n+    const uint64_t NMAX = 0x15B0;\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+\n+    \/\/ Generating accumulation coefficients for further calculations\n+    __ vid_v(vtemp1);\n+    __ li(temp0, 16);\n+    __ vmv_v_x(vtable, temp0);\n+    __ vsub_vv(vtable, vtable, vtemp1);\n+    \/\/ vtable now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n+\n+    __ vmv_v_i(vzero, 0);\n+\n+    __ mv(base, BASE);\n+    __ mv(nmax, NMAX);\n+\n+    \/\/ Zeroing all unroll registers\n+    for (int i = 0; i < 16; i++) {\n+      __ vmv_v_i(unroll_regs[i], 0);\n+    }\n+\n+    __ srli(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n+    \/\/ s1 is initialized to the lower 16 bits of adler\n+    \/\/ s2 is initialized to the upper 16 bits of adler\n+    if (!UseZbb) {\n+      const uint64_t right_16_bits = right_n_bits(16);\n+      __ mv(temp0, right_16_bits);\n+      __ andr(s2, s2, temp0);\n+      __ andr(s1, adler, temp0); \/\/ s1 = (adler & 0xffff)\n+    } else {\n+      __ zext_h(s2, s2);\n+      __ zext_h(s1, adler);\n+    }\n+\n+    \/\/ The pipelined loop needs at least 16 elements for 1 iteration\n+    \/\/ It does check this, but it is more effective to skip to the cleanup loop\n+    __ mv(temp0, (u1)16);\n+    __ bgeu(len, temp0, L_nmax);\n+    __ beqz(len, L_combine);\n+\n+  __ bind(L_simple_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, 1);\n+    __ add(s1, s1, temp0);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, 1);\n+    __ bgtz(len, L_simple_by1_loop);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ j(L_combine);\n+\n+  __ bind(L_nmax);\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bltz(len, L_by16);\n+\n+  __ bind(L_nmax_loop_entry);\n+    \/\/ buf_end will be used as endpoint for loop below\n+    __ add(buf_end, buff, count); \/\/ buf_end will be used as endpoint for loop below\n+    __ sub(buf_end, buf_end, 32); \/\/ After partial unrolling 3 additional iterations are required,\n+                                  \/\/ and bytes for one of them are already subtracted\n+\n+  __ bind(L_nmax_loop);\n+    for (int i = 0; i < 8; i++)\n+      generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[i], unroll_regs[i + 8], vtable);\n+\n+    \/\/ Summing up\n+    __ vsetivli(temp0, 2, Assembler::e64, Assembler::m1); \/\/ Set SEW to 64 to avoid sign-extension\n+                                                          \/\/ in the next instructions\n+    for (int i = 0; i < 8; i++) {\n+      \/\/ s2 = s2 + s1 * 16\n+      __ slli(temp1, s1, 4);\n+      __ add(s2, s2, temp1);\n+\n+      __ vmv_x_s(temp0, unroll_regs[i]);\n+      __ vmv_x_s(temp1, unroll_regs[i + 8]);\n+      __ add(s1, s1, temp0);\n+      __ add(s2, s2, temp1);\n+    }\n+    __ blt(buff, buf_end, L_nmax_loop);\n+\n+    \/\/ Do the calculations for remaining 48 bytes\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[0], unroll_regs[8], vtable);\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[1], unroll_regs[9], vtable);\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[2], unroll_regs[10], vtable);\n+    \/\/ Summing up\n+    __ vsetivli(temp0, 2, Assembler::e64, Assembler::m1); \/\/ Set SEW to 64 to avoid sign-extension\n+                                                          \/\/ in the next instructions\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ s2 = s2 + s1 * 16\n+      __ slli(temp1, s1, 4);\n+      __ add(s2, s2, temp1);\n+\n+      __ vmv_x_s(temp0, unroll_regs[i]);\n+      __ vmv_x_s(temp1, unroll_regs[i + 8]);\n+      __ add(s1, s1, temp0);\n+      __ add(s2, s2, temp1);\n+    }\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bgez(len, L_nmax_loop_entry);\n+\n+  __ bind(L_by16);\n+    __ add(len, len, count);\n+    __ bltz(len, L_by1);\n+\n+  __ bind(L_by16_loop);\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[0], unroll_regs[8], vtable);\n+    \/\/ s1 = s1 + unroll_regs[0], s2 = s2 + unroll_regs[8]\n+    __ vsetivli(temp0, 2, Assembler::e64, Assembler::m1); \/\/ Set SEW to 64 to avoid sign-extension\n+                                                          \/\/ in the next instructions\n+    \/\/ s2 = s2 + s1 * 16\n+    __ slli(temp1, s1, 4);\n+    __ add(s2, s2, temp1);\n+    __ vmv_x_s(temp0, unroll_regs[0]);\n+    __ vmv_x_s(temp1, unroll_regs[8]);\n+    __ add(s1, s1, temp0);\n+    __ add(s2, s2, temp1);\n+\n+    __ sub(len, len, 16);\n+    __ bgez(len, L_by16_loop);\n+\n+  __ bind(L_by1);\n+    __ add(len, len, 15);\n+    __ bltz(len, L_do_mod);\n+\n+  __ bind(L_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, 1);\n+    __ add(s1, temp0, s1);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, 1);\n+    __ bgez(len, L_by1_loop);\n+\n+  __ bind(L_do_mod);\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    \/\/ Combine lower bits and higher bits\n+    \/\/ adler = s1 | (s2 << 16)\n+  __ bind(L_combine);\n+    __ slli(s2, s2, 16);\n+    __ orr(s1, s1, s2);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -5624,0 +5876,4 @@\n+    if (UseAdler32Intrinsics) {\n+      StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":256,"deletions":0,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -236,0 +236,12 @@\n+  \/\/ Adler32\n+  if (UseRVV) {\n+    if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      FLAG_SET_DEFAULT(UseAdler32Intrinsics, true);\n+    }\n+  } else if (UseAdler32Intrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      warning(\"Adler32 intrinsic requires RVV instructions (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}