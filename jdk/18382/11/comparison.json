{"files":[{"patch":"@@ -1422,0 +1422,4 @@\n+  \/\/ Vector Widening Integer Reduction Instructions\n+  INSN(vwredsum_vs,    0b1010111, 0b000, 0b110001);\n+  INSN(vwredsumu_vs,   0b1010111, 0b000, 0b110000);\n+\n@@ -1460,0 +1464,4 @@\n+  \/\/ Vector Widening Integer Multiply Instructions\n+  INSN(vwmul_vv,    0b1010111, 0b010, 0b111011);\n+  INSN(vwmulu_vv,   0b1010111, 0b010, 0b111000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5045,0 +5045,266 @@\n+  void adler32_process_bytes(Register buff, Register s1, Register s2, VectorRegister vtable,\n+    VectorRegister vzero, VectorRegister vbytes, VectorRegister vs1acc, VectorRegister vs2acc,\n+    Register temp0, Register temp1, Register temp2,  Register temp3,\n+    VectorRegister vtemp1, VectorRegister vtemp2, int step, Assembler::LMUL LMUL) {\n+\n+    assert((LMUL == Assembler::m4 && step == 64) ||\n+           (LMUL == Assembler::m2 && step == 32) ||\n+           (LMUL == Assembler::m1 && step == 16),\n+           \"LMUL should be aligned with step: m4 and 64, m2 and 32 or m1 and 16\");\n+    \/\/ Below is function for calculating Adler32 checksum with 64-, 32- or 16-byte step. LMUL=m4, m2 or m1 is used.\n+    \/\/ The results are in v12, v13, ..., v22, v23. Example below is for 64-byte step case.\n+    \/\/ We use b1, b2, ..., b64 to denote the 64 bytes loaded in each iteration.\n+    \/\/ In non-vectorized code, we update s1 and s2 as:\n+    \/\/   s1 <- s1 + b1\n+    \/\/   s2 <- s2 + s1\n+    \/\/   s1 <- s1 + b2\n+    \/\/   s2 <- s2 + b1\n+    \/\/   ...\n+    \/\/   s1 <- s1 + b64\n+    \/\/   s2 <- s2 + s1\n+    \/\/ Putting above assignments together, we have:\n+    \/\/   s1_new = s1 + b1 + b2 + ... + b64\n+    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b64) =\n+    \/\/          = s2 + s1 * 64 + (b1 * 64 + b2 * 63 + ... + b64 * 1) =\n+    \/\/          = s2 + s1 * 64 + (b1, b2, ... b64) dot (64, 63, ... 1)\n+\n+    __ mv(temp3, step);\n+    \/\/ Load data\n+    __ vsetvli(temp0, temp3, Assembler::e8, LMUL);\n+    __ vle8_v(vbytes, buff);\n+    __ addi(buff, buff, step);\n+\n+    \/\/ Upper bound reduction sum for s1_new:\n+    \/\/ 0xFF * 64 = 0x3FC0, so:\n+    \/\/ 1. Need to do vector-widening reduction sum\n+    \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 16-bits elements\n+    __ vwredsumu_vs(vs1acc, vbytes, vzero);\n+    \/\/ Multiplication for s2_new\n+    __ vwmulu_vv(vs2acc, vtable, vbytes);\n+\n+    \/\/ s2 = s2 + s1 * log2(step)\n+    __ slli(temp1, s1, exact_log2(step));\n+    __ add(s2, s2, temp1);\n+\n+    \/\/ Summing up calculated results for s2_new\n+    \/\/\n+    \/\/ Half of vector-widening multiplication result is in successor of vs2acc\n+    \/\/ group for vlen == 16, in which case we need to double vector register\n+    \/\/ group width in order to reduction sum all of them\n+    Assembler::LMUL LMULx2 = (LMUL == Assembler::m1) ? Assembler::m2 :\n+                             (LMUL == Assembler::m2) ? Assembler::m4 : Assembler::m8;\n+    __ vsetvli(temp0, temp3, Assembler::e16, LMULx2);\n+    \/\/ Upper bound for reduction sum:\n+    \/\/ 0xFF * (64 + 63 + ... + 2 + 1) = 0x817E0 max for whole register group, so:\n+    \/\/ 1. Need to do vector-widening reduction sum\n+    \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 32-bits elements\n+    __ vwredsumu_vs(vtemp1, vs2acc, vzero);\n+\n+    \/\/ Extracting results for:\n+    \/\/ s1_new\n+    __ vmv_x_s(temp0, vs1acc);\n+    __ add(s1, s1, temp0);\n+    \/\/ s2_new\n+    __ vsetvli(temp0, temp3, Assembler::e32, Assembler::m1);\n+    __ vmv_x_s(temp1, vtemp1);\n+    __ add(s2, s2, temp1);\n+  }\n+\n+  \/***\n+   *  int java.util.zip.Adler32.updateBytes(int adler, byte[] b, int off, int len)\n+   *\n+   *  Arguments:\n+   *\n+   *  Inputs:\n+   *   c_rarg0   - int   adler\n+   *   c_rarg1   - byte* buff (b + off)\n+   *   c_rarg2   - int   len\n+   *\n+   *  Output:\n+   *   c_rarg0   - int adler result\n+   *\/\n+  address generate_updateBytesAdler32() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+    address start = __ pc();\n+\n+    Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_nmax_loop_entry,\n+      L_by16, L_by16_loop, L_by16_loop_unroll, L_by1_loop, L_do_mod, L_combine, L_by1;\n+\n+    \/\/ Aliases\n+    Register adler  = c_rarg0;\n+    Register s1     = c_rarg0;\n+    Register s2     = c_rarg3;\n+    Register buff   = c_rarg1;\n+    Register len    = c_rarg2;\n+    Register nmax  = x29; \/\/ t4\n+    Register base  = x30; \/\/ t5\n+    Register count = x31; \/\/ t6\n+    Register temp0 = c_rarg4;\n+    Register temp1 = c_rarg5;\n+    Register temp2 = c_rarg6;\n+    Register temp3 = x28; \/\/ t3\n+\n+    VectorRegister vzero = v31;\n+    VectorRegister vbytes = v8; \/\/ group: v8, v9, v10, v11\n+    VectorRegister vs1acc = v12; \/\/ group: v12, v13, v14, v15\n+    VectorRegister vs2acc = v16; \/\/ group: v16, v17, v18, v19, v20, v21, v22, v23\n+    VectorRegister vtable_64 = v24; \/\/ group: v24, v25, v26, v27\n+    VectorRegister vtable_32 = v4; \/\/ group: v4, v5\n+    VectorRegister vtable_16 = v30;\n+    VectorRegister vtemp1 = v28;\n+    VectorRegister vtemp2 = v29;\n+\n+    \/\/ Max number of bytes we can process before having to take the mod\n+    \/\/ 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)\/2 + (n+1)(BASE-1) <= 2^32-1\n+    const uint64_t BASE = 0xfff1;\n+    const uint64_t NMAX = 0x15B0;\n+\n+    \/\/ Loops steps\n+    int step_64 = 64;\n+    int step_32 = 32;\n+    int step_16 = 16;\n+    int step_1  = 1;\n+\n+    __ enter(); \/\/ Required for proper stackwalking of RuntimeStub frame\n+    __ mv(temp1, 64);\n+    __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m4);\n+\n+    \/\/ Generating accumulation coefficients for further calculations\n+    \/\/ vtable_64:\n+    __ vid_v(vtemp1);\n+    __ vrsub_vx(vtable_64, vtemp1, temp1);\n+    \/\/ vtable_64 group now contains { 0x40, 0x3f, 0x3e, ..., 0x3, 0x2, 0x1 }\n+\n+    \/\/ vtable_32:\n+    __ mv(temp1, 32);\n+    __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m2);\n+    __ vid_v(vtemp1);\n+    __ vrsub_vx(vtable_32, vtemp1, temp1);\n+    \/\/ vtable_32 group now contains { 0x20, 0x1f, 0x1e, ..., 0x3, 0x2, 0x1 }\n+\n+    \/\/ vtable_16:\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+    __ mv(temp1, 16);\n+    __ vid_v(vtemp1);\n+    __ vrsub_vx(vtable_16, vtemp1, temp1);\n+    \/\/ vtable_16 group now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n+\n+    __ vmv_v_i(vzero, 0);\n+\n+    __ mv(base, BASE);\n+    __ mv(nmax, NMAX);\n+\n+    \/\/ s1 is initialized to the lower 16 bits of adler\n+    \/\/ s2 is initialized to the upper 16 bits of adler\n+    __ zero_extend(s1, s1, 16); \/\/ s1 = (adler & 0xffff)\n+    __ srliw(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n+\n+    \/\/ The pipelined loop needs at least 16 elements for 1 iteration\n+    \/\/ It does check this, but it is more effective to skip to the cleanup loop\n+    __ mv(temp0, step_16);\n+    __ bgeu(len, temp0, L_nmax);\n+    __ beqz(len, L_combine);\n+\n+  __ bind(L_simple_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, step_1);\n+    __ add(s1, s1, temp0);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, step_1);\n+    __ bgtz(len, L_simple_by1_loop);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ j(L_combine);\n+\n+  __ bind(L_nmax);\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bltz(len, L_by16);\n+\n+  \/\/ Align L_nmax loop by 64\n+  __ bind(L_nmax_loop_entry);\n+    __ sub(count, count, 32);\n+\n+  __ bind(L_nmax_loop);\n+    adler32_process_bytes(buff, s1, s2, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, temp3,\n+      vtemp1, vtemp2, step_64, Assembler::m4);\n+    __ sub(count, count, step_64);\n+    __ bgtz(count, L_nmax_loop);\n+\n+    \/\/ There are three iterations left to do\n+    adler32_process_bytes(buff, s1, s2, vtable_32, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, temp3,\n+      vtemp1, vtemp2, step_32, Assembler::m2);\n+    adler32_process_bytes(buff, s1, s2, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, temp3, \n+      vtemp1, vtemp2, step_16, Assembler::m1);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bgez(len, L_nmax_loop_entry);\n+\n+  __ bind(L_by16);\n+    __ add(len, len, count);\n+    __ bltz(len, L_by1);\n+    \/\/ Trying to unroll\n+    __ mv(count, step_64);\n+    __ blt(len, count, L_by16_loop);\n+\n+  __ bind(L_by16_loop_unroll);\n+    adler32_process_bytes(buff, s1, s2, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, count,\n+      vtemp1, vtemp2, step_64, Assembler::m4);\n+    __ sub(len, len, step_64);\n+    \/\/ By now the count should still be 64\n+    __ bge(len, count, L_by16_loop_unroll);\n+    __ mv(count, step_16);\n+    __ blt(len, count, L_by1);\n+\n+  __ bind(L_by16_loop);\n+    adler32_process_bytes(buff, s1, s2, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, count,\n+      vtemp1, vtemp2, step_16, Assembler::m1);\n+    __ sub(len, len, step_16);\n+    __ bgez(len, L_by16_loop);\n+\n+  __ bind(L_by1);\n+    __ add(len, len, 15);\n+    __ bltz(len, L_do_mod);\n+\n+  __ bind(L_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, step_1);\n+    __ add(s1, temp0, s1);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, step_1);\n+    __ bgez(len, L_by1_loop);\n+\n+  __ bind(L_do_mod);\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    \/\/ Combine lower bits and higher bits\n+    \/\/ adler = s1 | (s2 << 16)\n+  __ bind(L_combine);\n+    __ slli(s2, s2, 16);\n+    __ orr(s1, s1, s2);\n+\n+    __ leave(); \/\/ Required for proper stackwalking of RuntimeStub frame\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -5624,0 +5890,4 @@\n+    if (UseAdler32Intrinsics) {\n+      StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":270,"deletions":0,"binary":false,"changes":270,"status":"modified"},{"patch":"@@ -236,0 +236,12 @@\n+  \/\/ Adler32\n+  if (UseRVV) {\n+    if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      FLAG_SET_DEFAULT(UseAdler32Intrinsics, true);\n+    }\n+  } else if (UseAdler32Intrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      warning(\"Adler32 intrinsic requires RVV instructions (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}