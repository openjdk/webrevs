{"files":[{"patch":"@@ -1422,0 +1422,4 @@\n+  \/\/ Vector Widening Integer Reduction Instructions\n+  INSN(vwredsum_vs,    0b1010111, 0b000, 0b110001);\n+  INSN(vwredsumu_vs,   0b1010111, 0b000, 0b110000);\n+\n@@ -1460,0 +1464,4 @@\n+  \/\/ Vector Widening Integer Multiply Instructions\n+  INSN(vwmul_vv,    0b1010111, 0b010, 0b111011);\n+  INSN(vwmulu_vv,   0b1010111, 0b010, 0b111000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5045,40 +5045,30 @@\n-  void generate_updateBytesAdler32_accum(const Register acc1, const Register acc2, const Register data, const Register temp) {\n-\n-    __ andi(temp, data, right_8_bits);\n-    __ add(acc1, acc1, temp);\n-\n-    __ srli(temp, data, 8);\n-    __ andi(temp, temp, right_8_bits);\n-    __ add(acc2, acc2, acc1);\n-    __ add(acc1, acc1, temp);\n-\n-    __ srli(temp, data, 16);\n-    __ andi(temp, temp, right_8_bits);\n-    __ add(acc2, acc2, acc1);\n-    __ add(acc1, acc1, temp);\n-\n-    __ srli(temp, data, 24);\n-    __ andi(temp, temp, right_8_bits);\n-    __ add(acc2, acc2, acc1);\n-    __ add(acc1, acc1, temp);\n-\n-    __ srli(temp, data, 32);\n-    __ andi(temp, temp, right_8_bits);\n-    __ add(acc2, acc2, acc1);\n-    __ add(acc1, acc1, temp);\n-\n-    __ srli(temp, data, 40);\n-    __ andi(temp, temp, right_8_bits);\n-    __ add(acc2, acc2, acc1);\n-    __ add(acc1, acc1, temp);\n-\n-    __ srli(temp, data, 48);\n-    __ andi(temp, temp, right_8_bits);\n-    __ add(acc2, acc2, acc1);\n-    __ add(acc1, acc1, temp);\n-    __ add(acc2, acc2, acc1);\n-\n-    __ srli(temp, data, 56);\n-    __ add(acc1, acc1, temp);\n-    __ add(acc2, acc2, acc1);\n-  }\n+  void generate_updateBytesAdler32_accum(Register buff, Register temp0,\n+          VectorRegister vtemp1, VectorRegister vtemp2, VectorRegister vtemp3, VectorRegister vzero,\n+          VectorRegister vbytes, VectorRegister vs1acc, VectorRegister vs2acc, VectorRegister vtable) {\n+    \/\/ Below is a vectorized implementation of updating s1 and s2 for 16 bytes.\n+    \/\/ We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.\n+    \/\/ In non-vectorized code, we update s1 and s2 as:\n+    \/\/   s1 <- s1 + b1\n+    \/\/   s2 <- s2 + s1\n+    \/\/   s1 <- s1 + b2\n+    \/\/   s2 <- s2 + b1\n+    \/\/   ...\n+    \/\/   s1 <- s1 + b16\n+    \/\/   s2 <- s2 + s1\n+    \/\/ Putting above assignments together, we have:\n+    \/\/   s1_new = s1 + b1 + b2 + ... + b16\n+    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n+    \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n+    \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n+    \/\/\n+    \/\/ To add more acceleration, it is possible to postpone final summation\n+    \/\/ until all unrolled calculations are done.\n+\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+\n+    \/\/ Load data\n+    __ vle8_v(vbytes, buff);\n+    __ addi(buff, buff, 16);\n+\n+    \/\/ vs1acc = b1 + b2 + b3 + ... + b16\n+    __ vwredsumu_vs(vs1acc, vbytes, vzero);\n@@ -5086,2 +5076,7 @@\n-const static uint64_t right_16_bits = right_n_bits(16);\n-const static uint64_t right_8_bits = right_n_bits(8);\n+    \/\/ vs2acc = (b1 * 16) + (b2 * 15) + (b3 * 14) + ... + (b16 * 1)\n+    __ vwmulu_vv(vtemp1, vtable, vbytes); \/\/ vtemp2 now contains the second part of multiplication\n+    __ vsetivli(temp0, 8, Assembler::e16, Assembler::m1);\n+    __ vadd_vv(vtemp3, vtemp1, vtemp2);   \/\/ 0x14 * 0xFF * 2 = 0x27D8 -- max value per element, \n+                                          \/\/ so no need to do vector-widening operation\n+    __ vwredsumu_vs(vs2acc, vtemp3, vzero);\n+  }\n@@ -5120,2 +5115,0 @@\n-    Register temp2 = t2;\n-    Register temp3 = x28; \/\/ t3\n@@ -5123,0 +5116,11 @@\n+    VectorRegister vbytes = v1;\n+    VectorRegister vtable = v3;\n+    VectorRegister vtemp1 = v4;\n+    VectorRegister vtemp2 = v5;\n+    VectorRegister vtemp3 = v30;\n+    VectorRegister vzero = v12;\n+\n+    VectorRegister unroll_regs[] = {\n+      v13, v14, v15, v16, v17, v18, v19, v20,\n+      v21, v22, v23, v24, v25, v26, v27, v28,\n+    };\n@@ -5130,0 +5134,8 @@\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+\n+    \/\/ Generating accumulation coefficients for further calculations\n+    __ vid_v(vtemp1);\n+    __ li(temp0, 16);\n+    __ vmv_v_x(vtable, temp0);\n+    __ vsub_vv(vtable, vtable, vtemp1);\n+    \/\/ vtable now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n@@ -5131,1 +5143,1 @@\n-    __ mv(temp3, right_16_bits);\n+    __ vmv_v_i(vzero, 0);\n@@ -5136,0 +5148,6 @@\n+    \/\/ Zeroing all unroll registers\n+    for (int i = 0; i < 16; i++) {\n+      __ vmv_v_i(unroll_regs[i], 0);\n+    }\n+\n+    __ srli(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n@@ -5138,3 +5156,9 @@\n-    __ srli(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n-    __ andr(s2, s2, temp3);\n-    __ andr(s1, adler, temp3); \/\/ s1 = (adler & 0xffff)\n+    if (!UseZbb) {\n+      const uint64_t right_16_bits = right_n_bits(16);\n+      __ mv(temp0, right_16_bits);\n+      __ andr(s2, s2, temp0);\n+      __ andr(s1, adler, temp0); \/\/ s1 = (adler & 0xffff)\n+    } else {\n+      __ zext_h(s2, s2);\n+      __ zext_h(s1, adler);\n+    }\n@@ -5172,0 +5196,2 @@\n+    __ sub(buf_end, buf_end, 32); \/\/ After partial unrolling 3 additional iterations are required,\n+                                  \/\/ and bytes for one of them are already subtracted\n@@ -5174,8 +5200,35 @@\n-\n-    __ ld(temp0, Address(buff, 0));\n-    generate_updateBytesAdler32_accum(s1, s2, temp0, temp1);\n-    __ ld(temp0, Address(buff, sizeof(jlong)));\n-    generate_updateBytesAdler32_accum(s1, s2, temp0, temp1);\n-\n-    __ addi(buff, buff, 16);\n-    __ ble(buff, buf_end, L_nmax_loop);\n+    for (int i = 0; i < 8; i++)\n+      generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[i], unroll_regs[i + 8], vtable);\n+\n+    \/\/ Summing up\n+    __ vsetivli(temp0, 2, Assembler::e64, Assembler::m1); \/\/ Set SEW to 64 to avoid sign-extension\n+                                                          \/\/ in the next instructions\n+    for (int i = 0; i < 8; i++) {\n+      \/\/ s2 = s2 + s1 * 16\n+      __ slli(temp1, s1, 4);\n+      __ add(s2, s2, temp1);\n+\n+      __ vmv_x_s(temp0, unroll_regs[i]);\n+      __ vmv_x_s(temp1, unroll_regs[i + 8]);\n+      __ add(s1, s1, temp0);\n+      __ add(s2, s2, temp1);\n+    }\n+    __ blt(buff, buf_end, L_nmax_loop);\n+\n+    \/\/ Do the calculations for remaining 48 bytes\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[0], unroll_regs[8], vtable);\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[1], unroll_regs[9], vtable);\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[2], unroll_regs[10], vtable);\n+    \/\/ Summing up\n+    __ vsetivli(temp0, 2, Assembler::e64, Assembler::m1); \/\/ Set SEW to 64 to avoid sign-extension\n+                                                          \/\/ in the next instructions\n+    for (int i = 0; i < 3; i++) {\n+      \/\/ s2 = s2 + s1 * 16\n+      __ slli(temp1, s1, 4);\n+      __ add(s2, s2, temp1);\n+\n+      __ vmv_x_s(temp0, unroll_regs[i]);\n+      __ vmv_x_s(temp1, unroll_regs[i + 8]);\n+      __ add(s1, s1, temp0);\n+      __ add(s2, s2, temp1);\n+    }\n@@ -5198,0 +5251,11 @@\n+    generate_updateBytesAdler32_accum(buff, temp0, vtemp1, vtemp2, vtemp3, vzero, vbytes, unroll_regs[0], unroll_regs[8], vtable);\n+    \/\/ s1 = s1 + unroll_regs[0], s2 = s2 + unroll_regs[8]\n+    __ vsetivli(temp0, 2, Assembler::e64, Assembler::m1); \/\/ Set SEW to 64 to avoid sign-extension\n+                                                          \/\/ in the next instructions\n+    \/\/ s2 = s2 + s1 * 16\n+    __ slli(temp1, s1, 4);\n+    __ add(s2, s2, temp1);\n+    __ vmv_x_s(temp0, unroll_regs[0]);\n+    __ vmv_x_s(temp1, unroll_regs[8]);\n+    __ add(s1, s1, temp0);\n+    __ add(s2, s2, temp1);\n@@ -5199,6 +5263,0 @@\n-    __ ld(temp0, Address(buff, 0));\n-    generate_updateBytesAdler32_accum(s1, s2, temp0, temp1);\n-    __ ld(temp0, Address(buff, sizeof(jlong)));\n-    generate_updateBytesAdler32_accum(s1, s2, temp0, temp1);\n-\n-    __ addi(buff, buff, 16);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":120,"deletions":62,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -162,4 +162,0 @@\n-  if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n-    FLAG_SET_DEFAULT(UseAdler32Intrinsics, true);\n-  }\n-\n@@ -258,0 +254,12 @@\n+  \/\/ Adler32\n+  if (UseRVV) {\n+    if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      FLAG_SET_DEFAULT(UseAdler32Intrinsics, true);\n+    }\n+  } else if (UseAdler32Intrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      warning(\"Adler32 intrinsic requires RVV instructions (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"}]}