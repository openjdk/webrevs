{"files":[{"patch":"@@ -1422,0 +1422,4 @@\n+  \/\/ Vector Widening Integer Reduction Instructions\n+  INSN(vwredsum_vs,    0b1010111, 0b000, 0b110001);\n+  INSN(vwredsumu_vs,   0b1010111, 0b000, 0b110000);\n+\n@@ -1460,0 +1464,4 @@\n+  \/\/ Vector Widening Integer Multiply Instructions\n+  INSN(vwmul_vv,    0b1010111, 0b010, 0b111011);\n+  INSN(vwmulu_vv,   0b1010111, 0b010, 0b111000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5045,0 +5045,320 @@\n+static const uint64_t right_16_bits = right_n_bits(16);\n+\n+  void adler32_process_bytes_by64(Register buff, Register s1, Register s2, Register count,\n+    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n+    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2) {\n+\n+    \/\/ Below is function for calculating Adler32 checksum with 64-byte step, LMUL=m4 is used.\n+    \/\/ The results are in v12, v13, ..., v22, v23.\n+    \/\/ We use b1, b2, ..., b64 to denote the 64 bytes loaded in each iteration.\n+    \/\/ In non-vectorized code, we update s1 and s2 as:\n+    \/\/   s1 <- s1 + b1\n+    \/\/   s2 <- s2 + s1\n+    \/\/   s1 <- s1 + b2\n+    \/\/   s2 <- s2 + b1\n+    \/\/   ...\n+    \/\/   s1 <- s1 + b64\n+    \/\/   s2 <- s2 + s1\n+    \/\/ Putting above assignments together, we have:\n+    \/\/   s1_new = s1 + b1 + b2 + ... + b64\n+    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b64) =\n+    \/\/          = s2 + s1 * 64 + (b1 * 64 + b2 * 63 + ... + b64 * 1) =\n+    \/\/          = s2 + s1 * 64 + (b1, b2, ... b64) dot (64, 63, ... 1)\n+\n+    \/\/ Load data\n+    __ vsetvli(temp0, count, Assembler::e8, Assembler::m4);\n+    __ vle8_v(vbytes[0], buff);\n+    __ add(buff, buff, temp0);\n+\n+    \/\/ Reduction sum for s1_new, multiplication for s2_new\n+    __ vwredsumu_vs(vs1acc[0], vbytes[0], vzero);\n+    __ vwmulu_vv(vs2acc[0], vtable, vbytes[0]);\n+\n+    \/\/ s2 = s2 + s1 * 64\n+    __ slli(temp1, s1, 6);\n+    __ add(s2, s2, temp1);\n+\n+    \/\/ Summing up calculated results for s2_new\n+    __ vsetvli(temp0, count, Assembler::e16, Assembler::m4);\n+    \/\/ 0xFF   * 0x10 = 0xFF0   max per single vector element,\n+    \/\/ 0xFF0  * 0x10 = 0xFF00  max per vector,\n+    \/\/ 0xFF00 * 0x4  = 0x3FC00 max for the whole group, so:\n+    \/\/ 1. Need to do vector-widening reduction sum\n+    \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 32-bits elements\n+    __ vwredsumu_vs(vtemp1, vs2acc[0], vzero);\n+    if (MaxVectorSize == 16)\n+      \/\/ For small vector length, the rest of multiplied data\n+      \/\/ is in successor of vs2acc[i], so summing it up, too\n+      __ vwredsumu_vs(vtemp2, vs2acc[2], vzero);\n+\n+    \/\/ Extracting results for:\n+    \/\/ s1_new\n+    __ vmv_x_s(temp0, vs1acc[0]);\n+    __ add(s1, s1, temp0);\n+    \/\/ s2_new\n+    __ vsetvli(temp0, count, Assembler::e32, Assembler::m1);\n+    __ vmv_x_s(temp1, vtemp1);\n+    __ add(s2, s2, temp1);\n+    if (MaxVectorSize == 16) {\n+      __ vmv_x_s(temp2, vtemp2);\n+      __ add(s2, s2, temp2);\n+    }\n+  }\n+\n+  void adler32_process_bytes_by16(Register buff, Register s1, Register s2, Register right_16_bits,\n+    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n+    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2, int LMUL) {\n+\n+    assert(LMUL <= 4, \"Not enough vector registers\");\n+    \/\/ Below is partial loop unrolling for updateBytesAdler32:\n+    \/\/ First, the LMUL*16 bytes are processed (with 16 bytes steps), the results are in\n+    \/\/ v8, v9, and up to v23, depending on the LMUL value.\n+    \/\/ Second, the final summation for unrolled part of the loop should be performed.\n+\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+    for (int i = 0; i < LMUL; i++) {\n+      __ vle8_v(vbytes[i], buff);\n+      __ addi(buff, buff, 16);\n+\n+      \/\/ Same algorithm as for 64 bytes, but shrinked to 16 bytes per unroll step:\n+      \/\/   s1_new = s1 + b1 + b2 + ... + b16\n+      \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n+      \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n+      \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n+      \/\/\n+      \/\/ vs1acc[i] = { (b1 + b2 + b3 + ... + b16), 0, 0, ..., 0 }\n+      __ vwredsumu_vs(vs1acc[i], vbytes[i], vzero);\n+\n+      \/\/ vs2acc[i] = { (b1 * 16), (b2 * 15), (b3 * 14), ..., (b8 * 9) }\n+      \/\/ vs2acc[i]->successor() = { (b9 * 8), (b10 * 7), (b11 * 6), ..., (b16 * 1) }\n+      __ vwmulu_vv(vs2acc[i], vtable, vbytes[i]);\n+    }\n+    \/\/ The only thing that remains is to sum up the remembered result\n+\n+    __ vsetivli(temp0, (MaxVectorSize == 16) ? 8 : 16, Assembler::e16, Assembler::m1);\n+    for (int i = 0; i < LMUL; i++) {\n+      \/\/ s2 = s2 + s1 * 16\n+      __ slli(temp1, s1, 4);\n+      __ add(s2, s2, temp1);\n+\n+      \/\/ Summing up calculated results for s2_new\n+      \/\/ 0xFF  * 0x10 = 0xFF0  max per single vector element,\n+      \/\/ 0xFF0 + 0xFEF + ... + FE1 = 0xFE88 max sum for 16-byte step\n+      \/\/ No need to do vector-widening reduction sum\n+      __ vredsum_vs(vtemp1, vs2acc[i], vzero);\n+      if (MaxVectorSize == 16) {\n+        \/\/ For small vector length, the rest of multiplied data\n+        \/\/ is in successor of vs2acc[i], so summing it up, too\n+        __ vredsum_vs(vtemp2, vs2acc[i]->successor(), vzero);\n+      }\n+\n+      \/\/ Extracting results\n+      __ vmv_x_s(temp0, vs1acc[i]);\n+      __ vmv_x_s(temp1, vtemp1);\n+      __ add(s1, s1, temp0);\n+      if (MaxVectorSize == 16) {\n+        __ vmv_x_s(temp2, vtemp2);\n+        __ add(s2, s2, temp2);\n+      } else {\n+        \/\/ For MaxVectorSize > 16 multiplied data is in single register, so it is\n+        \/\/ not safe to perform sign-extension during vmv.x.s with 16-bits elements\n+        __ andr(temp1, temp1, right_16_bits);\n+      }\n+      __ add(s2, s2, temp1);\n+    }\n+  }\n+\n+  \/***\n+   *  int java.util.zip.Adler32.updateBytes(int adler, byte[] b, int off, int len)\n+   *\n+   *  Arguments:\n+   *\n+   *  Inputs:\n+   *   c_rarg0   - int   adler\n+   *   c_rarg1   - byte* buff (b + off)\n+   *   c_rarg2   - int   len\n+   *\n+   * Output:\n+   *   c_rarg0   - int adler result\n+   *\/\n+  address generate_updateBytesAdler32() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+    address start = __ pc();\n+\n+    Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_nmax_loop_entry,\n+      L_by16, L_by16_loop, L_by16_loop_unroll, L_by1_loop, L_do_mod, L_combine, L_by1;\n+\n+    \/\/ Aliases\n+    Register adler  = c_rarg0;\n+    Register s1     = c_rarg0;\n+    Register s2     = c_rarg3;\n+    Register buff   = c_rarg1;\n+    Register len    = c_rarg2;\n+    Register nmax  = x29; \/\/ t4\n+    Register base  = x30; \/\/ t5\n+    Register count = x31; \/\/ t6\n+    Register temp0 = c_rarg4;\n+    Register temp1 = c_rarg5;\n+    Register temp2 = c_rarg6;\n+    Register right_16_bits = c_rarg7;\n+    Register step = x28; \/\/ t3\n+\n+    VectorRegister vzero = v4; \/\/ group: v5, v6, v7\n+    VectorRegister vbytes[] = {\n+      v8, v9, v10, v11\n+    };\n+    VectorRegister vs1acc[] = {\n+      v12, v13, v14, v15\n+    };\n+    VectorRegister vs2acc[] = {\n+      v16, v18, v20, v22\n+    };\n+    VectorRegister vtable_64 = v24; \/\/ group: v25, v26, v27\n+    VectorRegister vtable_16 = (MaxVectorSize == 16) ? v27 : v30;\n+    VectorRegister vtemp1 = v28; \/\/ group: v29, v30, v31\n+    VectorRegister vtemp2 = v29;\n+\n+    \/\/ Max number of bytes we can process before having to take the mod\n+    \/\/ 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)\/2 + (n+1)(BASE-1) <= 2^32-1\n+    const uint64_t BASE = 0xfff1;\n+    const uint64_t NMAX = 0x15B0;\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ mv(temp1, 64);\n+    __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m4);\n+\n+    \/\/ Generating accumulation coefficients for further calculations\n+    __ vid_v(vtemp1);\n+    __ vmv_v_x(vtable_64, temp1);\n+    __ vsub_vv(vtable_64, vtable_64, vtemp1);\n+    \/\/ vtable_64 group now contains { 0x40, 0x3f, 0x3e, ..., 0x3, 0x2, 0x1 }\n+    __ vmv_v_i(vzero, 0);\n+    if (MaxVectorSize > 16) {\n+      \/\/ Need to generate vtable_16 explicitly\n+      __ mv(temp1, 16);\n+      __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m1);\n+\n+      \/\/ Generating accumulation coefficients for further calculations\n+      __ vid_v(vtemp1);\n+      __ vmv_v_x(vtable_16, temp1);\n+      __ vsub_vv(vtable_16, vtable_16, vtemp1);\n+    }\n+\n+    __ mv(base, BASE);\n+    __ mv(nmax, NMAX);\n+    __ mv(right_16_bits, right_n_bits(16));\n+\n+    __ srli(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n+    \/\/ s1 is initialized to the lower 16 bits of adler\n+    \/\/ s2 is initialized to the upper 16 bits of adler\n+    if (!UseZbb) {\n+      __ andr(s2, s2, right_16_bits);\n+      __ andr(s1, adler, right_16_bits); \/\/ s1 = (adler & 0xffff)\n+    } else {\n+      __ zext_h(s2, s2);\n+      __ zext_h(s1, adler);\n+    }\n+\n+    \/\/ The pipelined loop needs at least 16 elements for 1 iteration\n+    \/\/ It does check this, but it is more effective to skip to the cleanup loop\n+    __ mv(temp0, (u1)16);\n+    __ bgeu(len, temp0, L_nmax);\n+    __ beqz(len, L_combine);\n+\n+  __ bind(L_simple_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, 1);\n+    __ add(s1, s1, temp0);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, 1);\n+    __ bgtz(len, L_simple_by1_loop);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ j(L_combine);\n+\n+  __ bind(L_nmax);\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bltz(len, L_by16);\n+    __ mv(step, 64);\n+\n+  \/\/ Align L_nmax loop by 64\n+  __ bind(L_nmax_loop_entry);\n+    __ sub(count, count, 32);\n+\n+  __ bind(L_nmax_loop);\n+    adler32_process_bytes_by64(buff, s1, s2, step, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2);\n+    __ sub(count, count, 64);\n+    __ bgtz(count, L_nmax_loop);\n+\n+    \/\/ There are three iterations left to do\n+    const int remainder = 3;\n+    adler32_process_bytes_by16(buff, s1, s2, right_16_bits, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2, remainder);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bgez(len, L_nmax_loop_entry);\n+\n+  __ bind(L_by16);\n+    __ add(len, len, count);\n+    __ bltz(len, L_by1);\n+    \/\/ Trying to unroll\n+    __ mv(count, 64);\n+    __ blt(len, count, L_by16_loop);\n+\n+  __ bind(L_by16_loop_unroll);\n+    adler32_process_bytes_by64(buff, s1, s2, count, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2);\n+    __ sub(len, len, 64);\n+    __ bge(len, count, L_by16_loop_unroll);\n+    __ mv(count, 16);\n+    __ blt(len, count, L_by1);\n+\n+  __ bind(L_by16_loop);\n+    adler32_process_bytes_by16(buff, s1, s2, right_16_bits, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2, 1);\n+    __ sub(len, len, 16);\n+    __ bgez(len, L_by16_loop);\n+\n+  __ bind(L_by1);\n+    __ add(len, len, 15);\n+    __ bltz(len, L_do_mod);\n+\n+  __ bind(L_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, 1);\n+    __ add(s1, temp0, s1);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, 1);\n+    __ bgez(len, L_by1_loop);\n+\n+  __ bind(L_do_mod);\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    \/\/ Combine lower bits and higher bits\n+    \/\/ adler = s1 | (s2 << 16)\n+  __ bind(L_combine);\n+    __ slli(s2, s2, 16);\n+    __ orr(s1, s1, s2);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -5624,0 +5944,4 @@\n+    if (UseAdler32Intrinsics) {\n+      StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":324,"deletions":0,"binary":false,"changes":324,"status":"modified"},{"patch":"@@ -236,0 +236,12 @@\n+  \/\/ Adler32\n+  if (UseRVV) {\n+    if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      FLAG_SET_DEFAULT(UseAdler32Intrinsics, true);\n+    }\n+  } else if (UseAdler32Intrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      warning(\"Adler32 intrinsic requires RVV instructions (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}