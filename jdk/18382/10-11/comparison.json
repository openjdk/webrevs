{"files":[{"patch":"@@ -5045,7 +5045,9 @@\n-  void adler32_process_bytes(Register buff, Register s1, Register s2, Register count,\n-    VectorRegister vtable, VectorRegister vzero, VectorRegister vbytes, VectorRegister vs1acc, VectorRegister *vs2acc,\n-    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2,\n-    int step, Assembler::LMUL LMUL) {\n-\n-    assert((LMUL == Assembler::m4 && step == 64) || (LMUL == Assembler::m2 && step == 32) ||\n-      (LMUL == Assembler::m1 && step == 16), \"LMUL should be aligned with step: m4 and 64, m2 and 32 or m1 and 16\");\n+  void adler32_process_bytes(Register buff, Register s1, Register s2, VectorRegister vtable,\n+    VectorRegister vzero, VectorRegister vbytes, VectorRegister vs1acc, VectorRegister vs2acc,\n+    Register temp0, Register temp1, Register temp2,  Register temp3,\n+    VectorRegister vtemp1, VectorRegister vtemp2, int step, Assembler::LMUL LMUL) {\n+\n+    assert((LMUL == Assembler::m4 && step == 64) ||\n+           (LMUL == Assembler::m2 && step == 32) ||\n+           (LMUL == Assembler::m1 && step == 16),\n+           \"LMUL should be aligned with step: m4 and 64, m2 and 32 or m1 and 16\");\n@@ -5069,1 +5071,1 @@\n-    __ mv(count, step);\n+    __ mv(temp3, step);\n@@ -5071,1 +5073,1 @@\n-    __ vsetvli(temp0, count, Assembler::e8, LMUL);\n+    __ vsetvli(temp0, temp3, Assembler::e8, LMUL);\n@@ -5081,1 +5083,1 @@\n-    __ vwmulu_vv(vs2acc[0], vtable, vbytes);\n+    __ vwmulu_vv(vs2acc, vtable, vbytes);\n@@ -5087,7 +5089,0 @@\n-    \/\/ MaxVectorSize == 16: Remainder for vwmulu.vv is in successor of vs2acc[0] group\n-    VectorRegister vs2acc_successor = vs2acc[2];\n-    if (step == 32)\n-      vs2acc_successor = vs2acc[1];\n-    else if (step == 16)\n-      vs2acc_successor = vs2acc[0]->successor();\n-\n@@ -5095,1 +5090,7 @@\n-    __ vsetvli(temp0, count, Assembler::e16, LMUL);\n+    \/\/\n+    \/\/ Half of vector-widening multiplication result is in successor of vs2acc\n+    \/\/ group for vlen == 16, in which case we need to double vector register\n+    \/\/ group width in order to reduction sum all of them\n+    Assembler::LMUL LMULx2 = (LMUL == Assembler::m1) ? Assembler::m2 :\n+                             (LMUL == Assembler::m2) ? Assembler::m4 : Assembler::m8;\n+    __ vsetvli(temp0, temp3, Assembler::e16, LMULx2);\n@@ -5100,5 +5101,1 @@\n-    __ vwredsumu_vs(vtemp1, vs2acc[0], vzero);\n-    if (MaxVectorSize == 16)\n-      \/\/ For small vector length, the rest of multiplied data\n-      \/\/ is in successor of vs2acc[0] group, so summing it up, too\n-      __ vwredsumu_vs(vtemp2, vs2acc_successor, vzero);\n+    __ vwredsumu_vs(vtemp1, vs2acc, vzero);\n@@ -5111,1 +5108,1 @@\n-    __ vsetvli(temp0, count, Assembler::e32, Assembler::m1);\n+    __ vsetvli(temp0, temp3, Assembler::e32, Assembler::m1);\n@@ -5114,4 +5111,0 @@\n-    if (MaxVectorSize == 16) {\n-      __ vmv_x_s(temp2, vtemp2);\n-      __ add(s2, s2, temp2);\n-    }\n@@ -5153,1 +5146,1 @@\n-    Register step = x28; \/\/ t3\n+    Register temp3 = x28; \/\/ t3\n@@ -5158,3 +5151,1 @@\n-    VectorRegister vs2acc[] = {\n-      v16, v18, v20, v22\n-    };\n+    VectorRegister vs2acc = v16; \/\/ group: v16, v17, v18, v19, v20, v21, v22, v23\n@@ -5162,2 +5153,2 @@\n-    VectorRegister vtable_32 = (MaxVectorSize == 16) ? v26 : v4;\n-    VectorRegister vtable_16 = (MaxVectorSize == 16) ? v27 : v30;\n+    VectorRegister vtable_32 = v4; \/\/ group: v4, v5\n+    VectorRegister vtable_16 = v30;\n@@ -5183,0 +5174,1 @@\n+    \/\/ vtable_64:\n@@ -5186,8 +5178,9 @@\n-    if (MaxVectorSize > 16) {\n-      \/\/ Need to generate vtable_32 explicitly\n-      __ mv(temp1, 32);\n-      __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m2);\n-      __ vid_v(vtemp1);\n-      __ vrsub_vx(vtable_32, vtemp1, temp1);\n-      \/\/ vtable_32 group now contains { 0x20, 0x1f, 0x1e, ..., 0x3, 0x2, 0x1 }\n-    }\n+\n+    \/\/ vtable_32:\n+    __ mv(temp1, 32);\n+    __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m2);\n+    __ vid_v(vtemp1);\n+    __ vrsub_vx(vtable_32, vtemp1, temp1);\n+    \/\/ vtable_32 group now contains { 0x20, 0x1f, 0x1e, ..., 0x3, 0x2, 0x1 }\n+\n+    \/\/ vtable_16:\n@@ -5195,7 +5188,5 @@\n-    if (MaxVectorSize > 16) {\n-      \/\/ Need to generate vtable_16 explicitly\n-      __ mv(temp1, 16);\n-      __ vid_v(vtemp1);\n-      __ vrsub_vx(vtable_16, vtemp1, temp1);\n-      \/\/ vtable_32 group now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n-    }\n+    __ mv(temp1, 16);\n+    __ vid_v(vtemp1);\n+    __ vrsub_vx(vtable_16, vtemp1, temp1);\n+    \/\/ vtable_16 group now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n+\n@@ -5207,1 +5198,0 @@\n-    __ srli(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n@@ -5210,8 +5200,2 @@\n-    if (!UseZbb) {\n-      __ mv(temp0, right_n_bits(16));\n-      __ andr(s2, s2, temp0);\n-      __ andr(s1, adler, temp0); \/\/ s1 = (adler & 0xffff)\n-    } else {\n-      __ zext_h(s2, s2);\n-      __ zext_h(s1, adler);\n-    }\n+    __ zero_extend(s1, s1, 16); \/\/ s1 = (adler & 0xffff)\n+    __ srliw(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n@@ -5250,3 +5234,3 @@\n-    adler32_process_bytes(buff, s1, s2, step, vtable_64, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n-      step_64, Assembler::m4);\n+    adler32_process_bytes(buff, s1, s2, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, temp3,\n+      vtemp1, vtemp2, step_64, Assembler::m4);\n@@ -5257,6 +5241,6 @@\n-    adler32_process_bytes(buff, s1, s2, step, vtable_32, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n-      step_32, Assembler::m2);\n-    adler32_process_bytes(buff, s1, s2, step, vtable_16, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n-      step_16, Assembler::m1);\n+    adler32_process_bytes(buff, s1, s2, vtable_32, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, temp3,\n+      vtemp1, vtemp2, step_32, Assembler::m2);\n+    adler32_process_bytes(buff, s1, s2, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, temp3, \n+      vtemp1, vtemp2, step_16, Assembler::m1);\n@@ -5281,3 +5265,3 @@\n-    adler32_process_bytes(buff, s1, s2, count, vtable_64, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n-      step_64, Assembler::m4);\n+    adler32_process_bytes(buff, s1, s2, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, count,\n+      vtemp1, vtemp2, step_64, Assembler::m4);\n@@ -5291,3 +5275,3 @@\n-    adler32_process_bytes(buff, s1, s2, count, vtable_16, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n-      step_16, Assembler::m1);\n+    adler32_process_bytes(buff, s1, s2, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, count,\n+      vtemp1, vtemp2, step_16, Assembler::m1);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":57,"deletions":73,"binary":false,"changes":130,"status":"modified"}]}