{"files":[{"patch":"@@ -5045,4 +5045,7 @@\n-  void generate_updateBytesAdler32_accum(Register buff, VectorRegister vzero, VectorRegister vbytes,\n-    VectorRegister vs1acc, VectorRegister vs2acc, VectorRegister vtable) {\n-    \/\/ Below is a vectorized implementation of updating s1 and s2 for 16 bytes.\n-    \/\/ We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.\n+  void adler32_process_bytes_by64(Register buff, Register s1, Register s2, Register count,\n+    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n+    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2) {\n+\n+    \/\/ Below is function for calculating Adler32 checksum with 64-byte step, LMUL=m4 is used.\n+    \/\/ The results are in v12, v13, ..., v22, v23.\n+    \/\/ We use b1, b2, ..., b64 to denote the 64 bytes loaded in each iteration.\n@@ -5055,1 +5058,1 @@\n-    \/\/   s1 <- s1 + b16\n+    \/\/   s1 <- s1 + b64\n@@ -5058,7 +5061,4 @@\n-    \/\/   s1_new = s1 + b1 + b2 + ... + b16\n-    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n-    \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n-    \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n-    \/\/\n-    \/\/ To add more acceleration, it is possible to postpone final summation\n-    \/\/ until all unrolled calculations are done.\n+    \/\/   s1_new = s1 + b1 + b2 + ... + b64\n+    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b64) =\n+    \/\/          = s2 + s1 * 64 + (b1 * 64 + b2 * 63 + ... + b64 * 1) =\n+    \/\/          = s2 + s1 * 64 + (b1, b2, ... b64) dot (64, 63, ... 1)\n@@ -5067,10 +5067,30 @@\n-    __ vle8_v(vbytes, buff);\n-    __ addi(buff, buff, 16);\n-\n-    \/\/ vs1acc = b1 + b2 + b3 + ... + b16\n-    __ vwredsumu_vs(vs1acc, vbytes, vzero);\n-\n-    \/\/ vs2acc = { (b1 * 16), (b2 * 15), (b3 * 14), ..., (b8 * 9) }\n-    \/\/ vs2acc->successor() = { (b9 * 8), (b10 * 7), (b11 * 6), ..., (b16 * 1) }\n-    __ vwmulu_vv(vs2acc, vtable, vbytes); \/\/ vs2acc->successor() now contains the second part of multiplication\n-    \/\/ The only thing that remains is to sum up the remembered result\n+    __ vsetvli(temp0, count, Assembler::e8, Assembler::m4);\n+    __ vle8_v(vbytes[0], buff);\n+    __ add(buff, buff, temp0);\n+\n+    \/\/ Reduction sum for s1_new, multiplication for s2_new\n+    __ vwredsumu_vs(vs1acc[0], vbytes[0], vzero);\n+    __ vwmulu_vv(vs2acc[0], vtable, vbytes[0]);\n+\n+    \/\/ s2 = s2 + s1 * 64\n+    __ slli(temp1, s1, 6);\n+    __ add(s2, s2, temp1);\n+\n+    \/\/ Summing up calculated results for s2_new\n+    __ vsetvli(temp0, count, Assembler::e16, Assembler::m4);\n+    \/\/ 0xFF * 0x10 = 0xFF0, 0xFF0 * 8 = 7F80, 7F80 * 4 = 1FE00 max, so:\n+    \/\/ 1. Need to do vector-widening reduction sum\n+    \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 32-bits elements\n+    __ vwredsumu_vs(vtemp1, vs2acc[0], vzero);\n+    __ vwredsumu_vs(vtemp2, vs2acc[2], vzero);\n+\n+    \/\/ Extracting results for:\n+    \/\/ s1_new\n+    __ vmv_x_s(temp0, vs1acc[0]);\n+    __ add(s1, s1, temp0);\n+    \/\/ s2_new\n+    __ vsetvli(temp0, count, Assembler::e32, Assembler::m1);\n+    __ vmv_x_s(temp1, vtemp1);\n+    __ add(s2, s2, temp1);\n+    __ vmv_x_s(temp2, vtemp2);\n+    __ add(s2, s2, temp2);\n@@ -5079,3 +5099,3 @@\n-  void generate_updateBytesAdler32_unroll(Register buff, Register s1, Register s2, int unroll_factor,\n-    VectorRegister vtable, VectorRegister vbytes, VectorRegister vzero, VectorRegister *unroll_regs,\n-    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2) {\n+  void adler32_process_bytes_by16(Register buff, Register s1, Register s2, Register count,\n+    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n+    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2, int LMUL) {\n@@ -5083,1 +5103,1 @@\n-    assert(unroll_factor <= 8, \"Not enough vector registers in unroll_regs\");\n+    assert(LMUL <= 4, \"Not enough vector registers\");\n@@ -5085,3 +5105,8 @@\n-    \/\/ First, the unroll*16 bytes are processed, the results are in\n-    \/\/ v4, v5, v6, ..., v25, v26, v27\n-    \/\/ Second, the final summation for unrolled part of the loop should be performed\n+    \/\/ First, the LMUL*16 bytes are processed (with 16 bytes steps), the results are in\n+    \/\/ v8, v9, and up to v23, depending on the LMUL value.\n+    \/\/ Second, the final summation for unrolled part of the loop should be performed.\n+\n+    \/\/ Load data for unrolling steps all at once\n+    __ vsetvli(temp0, count, Assembler::e8, Assembler::m4);\n+    __ vle8_v(vbytes[0], buff);\n+    __ addi(buff, buff, LMUL*16);\n@@ -5090,3 +5115,16 @@\n-    for (int i = 0; i < unroll_factor; i++)\n-      generate_updateBytesAdler32_accum(buff, vzero, vbytes, unroll_regs[i], unroll_regs[i + 8], vtable);\n-    \/\/ Summing up\n+    for (int i = 0; i < LMUL; i++) {\n+      \/\/ Same algorithm as for 64 bytes, but shrinked to 16 bytes per unroll step:\n+      \/\/   s1_new = s1 + b1 + b2 + ... + b16\n+      \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n+      \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n+      \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n+      \/\/\n+      \/\/ vs1acc[i] = { (b1 + b2 + b3 + ... + b16), 0, 0, ..., 0 }\n+      __ vwredsumu_vs(vs1acc[i], vbytes[i], vzero);\n+\n+      \/\/ vs2acc[i] = { (b1 * 16), (b2 * 15), (b3 * 14), ..., (b8 * 9) }\n+      \/\/ vs2acc[i]->successor() = { (b9 * 8), (b10 * 7), (b11 * 6), ..., (b16 * 1) }\n+      __ vwmulu_vv(vs2acc[i], vtable, vbytes[i]);\n+    }\n+    \/\/ The only thing that remains is to sum up the remembered result\n+\n@@ -5094,1 +5132,1 @@\n-    for (int i = 0; i < unroll_factor; i++) {\n+    for (int i = 0; i < LMUL; i++) {\n@@ -5099,0 +5137,1 @@\n+      \/\/ Summing up calculated results for s2_new\n@@ -5101,3 +5140,3 @@\n-      \/\/ 2. It is safe to perform sign-extension during vmv.x.s\n-      __ vredsum_vs(vtemp1, unroll_regs[i + 8], vzero);\n-      __ vredsum_vs(vtemp2, unroll_regs[i + 8]->successor(), vzero);\n+      \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 16-bits elements\n+      __ vredsum_vs(vtemp1, vs2acc[i], vzero);\n+      __ vredsum_vs(vtemp2, vs2acc[i]->successor(), vzero);\n@@ -5105,1 +5144,2 @@\n-      __ vmv_x_s(temp0, unroll_regs[i]);\n+      \/\/ Extracting results\n+      __ vmv_x_s(temp0, vs1acc[i]);\n@@ -5146,8 +5186,4 @@\n-    Register buf_end = c_rarg7;\n-\n-    VectorRegister vbytes = v1;\n-    VectorRegister vtable = v2;\n-    VectorRegister vzero = v3;\n-    VectorRegister unroll_regs[] = {\n-      v4, v5, v6, v7, v8, v9, v10, v11,\n-      v12, v14, v16, v18, v20, v22, v24, v26\n+\n+    VectorRegister vzero = v4; \/\/ group: v5, v6, v7\n+    VectorRegister vbytes[] = {\n+      v8, v9, v10, v11\n@@ -5155,1 +5191,9 @@\n-    VectorRegister vtemp1 = v28;\n+    VectorRegister vs1acc[] = {\n+      v12, v13, v14, v15\n+    };\n+    VectorRegister vs2acc[] = {\n+      v16, v18, v20, v22\n+    };\n+    VectorRegister vtable_64 = v24; \/\/ group: v25, v26, v27\n+    VectorRegister vtable_16 = v27;\n+    VectorRegister vtemp1 = v28; \/\/ group: v29, v30, v31\n@@ -5163,3 +5207,0 @@\n-    \/\/ Unroll factor for L_nmax loop\n-    const int unroll = 8;\n-\n@@ -5167,1 +5208,2 @@\n-    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+    __ mv(temp1, 64);\n+    __ vsetvli(temp0, temp1, Assembler::e8, Assembler::m4);\n@@ -5171,4 +5213,3 @@\n-    __ mv(temp0, 16);\n-    __ vmv_v_x(vtable, temp0);\n-    __ vsub_vv(vtable, vtable, vtemp1);\n-    \/\/ vtable now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n+    __ vmv_v_x(vtable_64, temp1);\n+    __ vsub_vv(vtable_64, vtable_64, vtemp1);\n+    \/\/ vtable_64 group now contains { 0x40, 0x3f, 0x3e, ..., 0x3, 0x2, 0x1 }\n@@ -5220,0 +5261,1 @@\n+  \/\/ Align L_nmax loop by 64\n@@ -5221,4 +5263,1 @@\n-    \/\/ buf_end will be used as endpoint for loop below\n-    __ add(buf_end, buff, count); \/\/ buf_end will be used as endpoint for loop below\n-    __ sub(buf_end, buf_end, 32); \/\/ After partial unrolling 3 additional iterations are required,\n-                                  \/\/ and bytes for one of them are already subtracted\n+    __ sub(count, count, 32);\n@@ -5227,3 +5266,4 @@\n-    generate_updateBytesAdler32_unroll(buff, s1, s2, unroll, vtable, vbytes,\n-      vzero, unroll_regs, temp0, temp1, temp2, vtemp1, vtemp2);\n-    __ blt(buff, buf_end, L_nmax_loop);\n+    adler32_process_bytes_by64(buff, s1, s2, count, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2);\n+    __ sub(count, count, 64);\n+    __ bgtz(count, L_nmax_loop);\n@@ -5231,4 +5271,5 @@\n-    const int remainder = ((NMAX \/ 16) % unroll);\n-    \/\/ Do the calculations for remaining 16*remainder bytes\n-    generate_updateBytesAdler32_unroll(buff, s1, s2, remainder, vtable, vbytes,\n-      vzero, unroll_regs, temp0, temp1, temp2, vtemp1, vtemp2);\n+    \/\/ There are three iterations left to do\n+    const int remainder = 3;\n+    __ mv(count, 16*remainder);\n+    adler32_process_bytes_by16(buff, s1, s2, count, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2, remainder);\n@@ -5248,0 +5289,1 @@\n+    __ mv(count, 16);\n@@ -5250,3 +5292,2 @@\n-    generate_updateBytesAdler32_unroll(buff, s1, s2, 1, vtable, vbytes,\n-      vzero, unroll_regs, temp0, temp1, temp2, vtemp1, vtemp2);\n-\n+    adler32_process_bytes_by16(buff, s1, s2, count, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2, 1);\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":109,"deletions":68,"binary":false,"changes":177,"status":"modified"}]}