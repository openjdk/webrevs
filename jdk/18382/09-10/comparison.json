{"files":[{"patch":"@@ -5045,8 +5045,9 @@\n-static const uint64_t right_16_bits = right_n_bits(16);\n-\n-  void adler32_process_bytes_by64(Register buff, Register s1, Register s2, Register count,\n-    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n-    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2) {\n-\n-    \/\/ Below is function for calculating Adler32 checksum with 64-byte step, LMUL=m4 is used.\n-    \/\/ The results are in v12, v13, ..., v22, v23.\n+  void adler32_process_bytes(Register buff, Register s1, Register s2, Register count,\n+    VectorRegister vtable, VectorRegister vzero, VectorRegister vbytes, VectorRegister vs1acc, VectorRegister *vs2acc,\n+    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2,\n+    int step, Assembler::LMUL LMUL) {\n+\n+    assert((LMUL == Assembler::m4 && step == 64) || (LMUL == Assembler::m2 && step == 32) ||\n+      (LMUL == Assembler::m1 && step == 16), \"LMUL should be aligned with step: m4 and 64, m2 and 32 or m1 and 16\");\n+    \/\/ Below is function for calculating Adler32 checksum with 64-, 32- or 16-byte step. LMUL=m4, m2 or m1 is used.\n+    \/\/ The results are in v12, v13, ..., v22, v23. Example below is for 64-byte step case.\n@@ -5068,1 +5069,1 @@\n-    __ mv(count, 64);\n+    __ mv(count, step);\n@@ -5070,3 +5071,3 @@\n-    __ vsetvli(temp0, count, Assembler::e8, Assembler::m4);\n-    __ vle8_v(vbytes[0], buff);\n-    __ addi(buff, buff, 64);\n+    __ vsetvli(temp0, count, Assembler::e8, LMUL);\n+    __ vle8_v(vbytes, buff);\n+    __ addi(buff, buff, step);\n@@ -5074,1 +5075,1 @@\n-    \/\/ Reduction sum for s1_new\n+    \/\/ Upper bound reduction sum for s1_new:\n@@ -5078,1 +5079,1 @@\n-    __ vwredsumu_vs(vs1acc[0], vbytes[0], vzero);\n+    __ vwredsumu_vs(vs1acc, vbytes, vzero);\n@@ -5080,17 +5081,1 @@\n-    __ vwmulu_vv(vs2acc[0], vtable, vbytes[0]);\n-\n-    \/\/ s2 = s2 + s1 * 64\n-    __ slli(temp1, s1, 6);\n-    __ add(s2, s2, temp1);\n-\n-    \/\/ Summing up calculated results for s2_new\n-    __ vsetvli(temp0, count, Assembler::e16, Assembler::m4);\n-    \/\/ Upper bound for reduction sum:\n-    \/\/ 0xFF * (64 + 63 + ... + 2 + 1) = 0x817E0 max for whole register group, so:\n-    \/\/ 1. Need to do vector-widening reduction sum\n-    \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 32-bits elements\n-    __ vwredsumu_vs(vtemp1, vs2acc[0], vzero);\n-    if (MaxVectorSize == 16)\n-      \/\/ For small vector length, the rest of multiplied data\n-      \/\/ is in successor of vs2acc[i], so summing it up, too\n-      __ vwredsumu_vs(vtemp2, vs2acc[2], vzero);\n+    __ vwmulu_vv(vs2acc[0], vtable, vbytes);\n@@ -5098,7 +5083,2 @@\n-    \/\/ Extracting results for:\n-    \/\/ s1_new\n-    __ vmv_x_s(temp0, vs1acc[0]);\n-    __ add(s1, s1, temp0);\n-    \/\/ s2_new\n-    __ vsetvli(temp0, count, Assembler::e32, Assembler::m1);\n-    __ vmv_x_s(temp1, vtemp1);\n+    \/\/ s2 = s2 + s1 * log2(step)\n+    __ slli(temp1, s1, exact_log2(step));\n@@ -5106,15 +5086,0 @@\n-    if (MaxVectorSize == 16) {\n-      __ vmv_x_s(temp2, vtemp2);\n-      __ add(s2, s2, temp2);\n-    }\n-  }\n-\n-  void adler32_process_bytes_by32(Register buff, Register s1, Register s2, Register count,\n-    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n-    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2) {\n-\n-    \/\/ Same algorithm as for 64 bytes, but shrinked to 32 bytes step:\n-    \/\/   s1_new = s1 + b1 + b2 + ... + b32\n-    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b32)\n-    \/\/          = s2 + s1 * 32 + (b1 * 32 + b2 * 31 + ... + b32 * 1)\n-    \/\/          = s2 + s1 * 32 + (b1, b2, ... b16) dot (32, 31, ... 1)\n@@ -5122,17 +5087,6 @@\n-    __ mv(count, 32);\n-    \/\/ Load data\n-    __ vsetvli(temp0, count, Assembler::e8, Assembler::m2);\n-    __ vle8_v(vbytes[0], buff);\n-    __ addi(buff, buff, 32);\n-\n-    \/\/ Reduction sum for s1_new\n-    \/\/ 0xFF * 32 = 0x1FE0, so:\n-    \/\/ 1. Need to do vector-widening reduction sum\n-    \/\/ 2. It is safe to perform sign-extension during vmv.x.s with 16-bits elements\n-    __ vwredsumu_vs(vs1acc[0], vbytes[0], vzero);\n-    \/\/ Multiplication for s2_new\n-    __ vwmulu_vv(vs2acc[0], vtable, vbytes[0]);\n-\n-    \/\/ s2 = s2 + s1 * 32\n-    __ slli(temp1, s1, 5);\n-    __ add(s2, s2, temp1);\n+    \/\/ MaxVectorSize == 16: Remainder for vwmulu.vv is in successor of vs2acc[0] group\n+    VectorRegister vs2acc_successor = vs2acc[2];\n+    if (step == 32)\n+      vs2acc_successor = vs2acc[1];\n+    else if (step == 16)\n+      vs2acc_successor = vs2acc[0]->successor();\n@@ -5141,1 +5095,1 @@\n-    __ vsetvli(temp0, count, Assembler::e16, Assembler::m2);\n+    __ vsetvli(temp0, count, Assembler::e16, LMUL);\n@@ -5143,1 +5097,1 @@\n-    \/\/ 0xFF * (32 + 31 + ... + 2 + 1) = 0x20DF0 max for whole register group, so:\n+    \/\/ 0xFF * (64 + 63 + ... + 2 + 1) = 0x817E0 max for whole register group, so:\n@@ -5149,2 +5103,2 @@\n-      \/\/ is in successor of vs2acc[i], so summing it up, too\n-      __ vwredsumu_vs(vtemp2, vs2acc[1], vzero);\n+      \/\/ is in successor of vs2acc[0] group, so summing it up, too\n+      __ vwredsumu_vs(vtemp2, vs2acc_successor, vzero);\n@@ -5154,1 +5108,1 @@\n-    __ vmv_x_s(temp0, vs1acc[0]);\n+    __ vmv_x_s(temp0, vs1acc);\n@@ -5166,63 +5120,0 @@\n-  void adler32_process_bytes_by16(Register buff, Register s1, Register s2, Register right_16_bits,\n-    VectorRegister vtable, VectorRegister vzero, VectorRegister *vbytes, VectorRegister *vs1acc, VectorRegister *vs2acc, \n-    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2, int LMUL) {\n-\n-    assert(LMUL <= 4, \"Not enough vector registers\");\n-    \/\/ Below is partial loop unrolling for updateBytesAdler32:\n-    \/\/ First, the LMUL*16 bytes are processed (with 16 bytes steps), the results are in\n-    \/\/ v8, v9, and up to v23, depending on the LMUL value.\n-    \/\/ Second, the final summation for unrolled part of the loop should be performed.\n-\n-    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n-    for (int i = 0; i < LMUL; i++) {\n-      __ vle8_v(vbytes[i], buff);\n-      __ addi(buff, buff, 16);\n-\n-      \/\/ Same algorithm as for 64 bytes, but shrinked to 16 bytes per unroll step:\n-      \/\/   s1_new = s1 + b1 + b2 + ... + b16\n-      \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n-      \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n-      \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n-      \/\/\n-      \/\/ vs1acc[i] = { (b1 + b2 + b3 + ... + b16), 0, 0, ..., 0 }\n-      __ vwredsumu_vs(vs1acc[i], vbytes[i], vzero);\n-\n-      \/\/ vs2acc[i] = { (b1 * 16), (b2 * 15), (b3 * 14), ..., (b8 * 9) }\n-      \/\/ vs2acc[i]->successor() = { (b9 * 8), (b10 * 7), (b11 * 6), ..., (b16 * 1) }\n-      __ vwmulu_vv(vs2acc[i], vtable, vbytes[i]);\n-    }\n-    \/\/ The only thing that remains is to sum up the remembered result\n-\n-    __ vsetivli(temp0, (MaxVectorSize == 16) ? 8 : 16, Assembler::e16, Assembler::m1);\n-    for (int i = 0; i < LMUL; i++) {\n-      \/\/ s2 = s2 + s1 * 16\n-      __ slli(temp1, s1, 4);\n-      __ add(s2, s2, temp1);\n-\n-      \/\/ Summing up calculated results for s2_new\n-      \/\/ 0xFF  * 0x10 = 0xFF0  max per single vector element,\n-      \/\/ 0xFF0 + 0xFEF + ... + FE1 = 0xFE88 max sum for 16-byte step\n-      \/\/ No need to do vector-widening reduction sum\n-      __ vredsum_vs(vtemp1, vs2acc[i], vzero);\n-      if (MaxVectorSize == 16) {\n-        \/\/ For small vector length, the rest of multiplied data\n-        \/\/ is in successor of vs2acc[i], so summing it up, too\n-        __ vredsum_vs(vtemp2, vs2acc[i]->successor(), vzero);\n-      }\n-\n-      \/\/ Extracting results\n-      __ vmv_x_s(temp0, vs1acc[i]);\n-      __ vmv_x_s(temp1, vtemp1);\n-      __ add(s1, s1, temp0);\n-      if (MaxVectorSize == 16) {\n-        __ vmv_x_s(temp2, vtemp2);\n-        __ add(s2, s2, temp2);\n-      } else {\n-        \/\/ For MaxVectorSize > 16 multiplied data is in single register, so it is\n-        \/\/ not safe to perform sign-extension during vmv.x.s with 16-bits elements\n-        __ andr(temp1, temp1, right_16_bits);\n-      }\n-      __ add(s2, s2, temp1);\n-    }\n-  }\n-\n@@ -5239,1 +5130,1 @@\n-   * Output:\n+   *  Output:\n@@ -5262,1 +5153,0 @@\n-    Register right_16_bits = c_rarg7;\n@@ -5266,6 +5156,2 @@\n-    VectorRegister vbytes[] = {\n-      v8, v9, v10, v11\n-    };\n-    VectorRegister vs1acc[] = {\n-      v12, v13, v14, v15\n-    };\n+    VectorRegister vbytes = v8; \/\/ group: v8, v9, v10, v11\n+    VectorRegister vs1acc = v12; \/\/ group: v12, v13, v14, v15\n@@ -5286,1 +5172,7 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    \/\/ Loops steps\n+    int step_64 = 64;\n+    int step_32 = 32;\n+    int step_16 = 16;\n+    int step_1  = 1;\n+\n+    __ enter(); \/\/ Required for proper stackwalking of RuntimeStub frame\n@@ -5314,1 +5206,0 @@\n-    __ mv(right_16_bits, right_n_bits(16));\n@@ -5320,2 +5211,3 @@\n-      __ andr(s2, s2, right_16_bits);\n-      __ andr(s1, adler, right_16_bits); \/\/ s1 = (adler & 0xffff)\n+      __ mv(temp0, right_n_bits(16));\n+      __ andr(s2, s2, temp0);\n+      __ andr(s1, adler, temp0); \/\/ s1 = (adler & 0xffff)\n@@ -5329,1 +5221,1 @@\n-    __ mv(temp0, (u1)16);\n+    __ mv(temp0, step_16);\n@@ -5335,1 +5227,1 @@\n-    __ addi(buff, buff, 1);\n+    __ addi(buff, buff, step_1);\n@@ -5338,1 +5230,1 @@\n-    __ sub(len, len, 1);\n+    __ sub(len, len, step_1);\n@@ -5358,3 +5250,4 @@\n-    adler32_process_bytes_by64(buff, s1, s2, step, vtable_64, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2);\n-    __ sub(count, count, 64);\n+    adler32_process_bytes(buff, s1, s2, step, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n+      step_64, Assembler::m4);\n+    __ sub(count, count, step_64);\n@@ -5364,5 +5257,6 @@\n-    adler32_process_bytes_by32(buff, s1, s2, step, vtable_32, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2);\n-    const int remainder = 1;\n-    adler32_process_bytes_by16(buff, s1, s2, right_16_bits, vtable_16, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2, remainder);\n+    adler32_process_bytes(buff, s1, s2, step, vtable_32, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n+      step_32, Assembler::m2);\n+    adler32_process_bytes(buff, s1, s2, step, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n+      step_16, Assembler::m1);\n@@ -5383,1 +5277,1 @@\n-    __ mv(count, 64);\n+    __ mv(count, step_64);\n@@ -5387,3 +5281,5 @@\n-    adler32_process_bytes_by64(buff, s1, s2, count, vtable_64, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2);\n-    __ sub(len, len, 64);\n+    adler32_process_bytes(buff, s1, s2, count, vtable_64, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n+      step_64, Assembler::m4);\n+    __ sub(len, len, step_64);\n+    \/\/ By now the count should still be 64\n@@ -5391,1 +5287,1 @@\n-    __ mv(count, 16);\n+    __ mv(count, step_16);\n@@ -5395,3 +5291,4 @@\n-    adler32_process_bytes_by16(buff, s1, s2, right_16_bits, vtable_16, vzero,\n-      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2, 1);\n-    __ sub(len, len, 16);\n+    adler32_process_bytes(buff, s1, s2, count, vtable_16, vzero,\n+      vbytes, vs1acc, vs2acc, temp0, temp1, temp2, vtemp1, vtemp2,\n+      step_16, Assembler::m1);\n+    __ sub(len, len, step_16);\n@@ -5406,1 +5303,1 @@\n-    __ addi(buff, buff, 1);\n+    __ addi(buff, buff, step_1);\n@@ -5409,1 +5306,1 @@\n-    __ sub(len, len, 1);\n+    __ sub(len, len, step_1);\n@@ -5424,1 +5321,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ leave(); \/\/ Required for proper stackwalking of RuntimeStub frame\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":69,"deletions":172,"binary":false,"changes":241,"status":"modified"}]}