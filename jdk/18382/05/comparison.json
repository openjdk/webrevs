{"files":[{"patch":"@@ -1422,0 +1422,4 @@\n+  \/\/ Vector Widening Integer Reduction Instructions\n+  INSN(vwredsum_vs,    0b1010111, 0b000, 0b110001);\n+  INSN(vwredsumu_vs,   0b1010111, 0b000, 0b110000);\n+\n@@ -1460,0 +1464,4 @@\n+  \/\/ Vector Widening Integer Multiply Instructions\n+  INSN(vwmul_vv,    0b1010111, 0b010, 0b111011);\n+  INSN(vwmulu_vv,   0b1010111, 0b010, 0b111000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5045,0 +5045,241 @@\n+  void generate_updateBytesAdler32_accum(Register buff, VectorRegister vzero, VectorRegister vbytes,\n+    VectorRegister vs1acc, VectorRegister vs2acc, VectorRegister vtable) {\n+    \/\/ Below is a vectorized implementation of updating s1 and s2 for 16 bytes.\n+    \/\/ We use b1, b2, ..., b16 to denote the 16 bytes loaded in each iteration.\n+    \/\/ In non-vectorized code, we update s1 and s2 as:\n+    \/\/   s1 <- s1 + b1\n+    \/\/   s2 <- s2 + s1\n+    \/\/   s1 <- s1 + b2\n+    \/\/   s2 <- s2 + b1\n+    \/\/   ...\n+    \/\/   s1 <- s1 + b16\n+    \/\/   s2 <- s2 + s1\n+    \/\/ Putting above assignments together, we have:\n+    \/\/   s1_new = s1 + b1 + b2 + ... + b16\n+    \/\/   s2_new = s2 + (s1 + b1) + (s1 + b1 + b2) + ... + (s1 + b1 + b2 + ... + b16)\n+    \/\/          = s2 + s1 * 16 + (b1 * 16 + b2 * 15 + ... + b16 * 1)\n+    \/\/          = s2 + s1 * 16 + (b1, b2, ... b16) dot (16, 15, ... 1)\n+    \/\/\n+    \/\/ To add more acceleration, it is possible to postpone final summation\n+    \/\/ until all unrolled calculations are done.\n+\n+    \/\/ Load data\n+    __ vle8_v(vbytes, buff);\n+    __ addi(buff, buff, 16);\n+\n+    \/\/ vs1acc = b1 + b2 + b3 + ... + b16\n+    __ vwredsumu_vs(vs1acc, vbytes, vzero);\n+\n+    \/\/ vs2acc = { (b1 * 16), (b2 * 15), (b3 * 14), ..., (b8 * 9) }\n+    \/\/ vs2acc->successor() = { (b9 * 8), (b10 * 7), (b11 * 6), ..., (b16 * 1) }\n+    __ vwmulu_vv(vs2acc, vtable, vbytes); \/\/ vs2acc->successor() now contains the second part of multiplication\n+    \/\/ The only thing that remains is to sum up the remembered result\n+  }\n+\n+  void generate_updateBytesAdler32_unroll(Register buff, Register s1, Register s2, int unroll_factor,\n+    VectorRegister vtable, VectorRegister vbytes, VectorRegister vzero, VectorRegister *unroll_regs,\n+    Register temp0, Register temp1, Register temp2, VectorRegister vtemp1, VectorRegister vtemp2) {\n+\n+    assert(unroll_factor <= 8, \"Not enough vector registers in unroll_regs\");\n+    \/\/ Below is partial loop unrolling for updateBytesAdler32:\n+    \/\/ First, the unroll*16 bytes are processed, the results are in\n+    \/\/ v4, v5, v6, ..., v25, v26, v27\n+    \/\/ Second, the final summation for unrolled part of the loop should be performed\n+\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+    for (int i = 0; i < unroll_factor; i++)\n+      generate_updateBytesAdler32_accum(buff, vzero, vbytes, unroll_regs[i], unroll_regs[i + 8], vtable);\n+    \/\/ Summing up\n+    __ vsetivli(temp0, 8, Assembler::e16, Assembler::m1);\n+    for (int i = 0; i < unroll_factor; i++) {\n+      \/\/ s2 = s2 + s1 * 16\n+      __ slli(temp1, s1, 4);\n+      __ add(s2, s2, temp1);\n+\n+      \/\/ 0xFF * 0x10 = 0xFF0, 0xFF0 * 8 = 7F80, so:\n+      \/\/ 1. No need to do vector-widening reduction sum\n+      \/\/ 2. It is safe to perform sign-extension during vmv.x.s\n+      __ vredsum_vs(vtemp1, unroll_regs[i + 8], vzero);\n+      __ vredsum_vs(vtemp2, unroll_regs[i + 8]->successor(), vzero);\n+\n+      __ vmv_x_s(temp0, unroll_regs[i]);\n+      __ vmv_x_s(temp1, vtemp1);\n+      __ vmv_x_s(temp2, vtemp2);\n+      __ add(s1, s1, temp0);\n+      __ add(s2, s2, temp1);\n+      __ add(s2, s2, temp2);\n+    }\n+  }\n+\n+  \/***\n+   *  int java.util.zip.Adler32.updateBytes(int adler, byte[] b, int off, int len)\n+   *\n+   *  Arguments:\n+   *\n+   *  Inputs:\n+   *   c_rarg0   - int   adler\n+   *   c_rarg1   - byte* buff (b + off)\n+   *   c_rarg2   - int   len\n+   *\n+   * Output:\n+   *   c_rarg0   - int adler result\n+   *\/\n+  address generate_updateBytesAdler32() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n+    address start = __ pc();\n+\n+    Label L_simple_by1_loop, L_nmax, L_nmax_loop, L_nmax_loop_entry, L_by16, L_by16_loop, L_by1_loop, L_do_mod, L_combine, L_by1;\n+\n+    \/\/ Aliases\n+    Register adler  = c_rarg0;\n+    Register s1     = c_rarg0;\n+    Register s2     = c_rarg3;\n+    Register buff   = c_rarg1;\n+    Register len    = c_rarg2;\n+    Register nmax  = x29; \/\/ t4\n+    Register base  = x30; \/\/ t5\n+    Register count = x31; \/\/ t6\n+    Register temp0 = c_rarg4;\n+    Register temp1 = c_rarg5;\n+    Register temp2 = c_rarg6;\n+    Register buf_end = c_rarg7;\n+\n+    VectorRegister vbytes = v1;\n+    VectorRegister vtable = v2;\n+    VectorRegister vzero = v3;\n+    VectorRegister unroll_regs[] = {\n+      v4, v5, v6, v7, v8, v9, v10, v11,\n+      v12, v14, v16, v18, v20, v22, v24, v26\n+    };\n+    VectorRegister vtemp1 = v28;\n+    VectorRegister vtemp2 = v29;\n+\n+    \/\/ Max number of bytes we can process before having to take the mod\n+    \/\/ 0x15B0 is 5552 in decimal, the largest n such that 255n(n+1)\/2 + (n+1)(BASE-1) <= 2^32-1\n+    const uint64_t BASE = 0xfff1;\n+    const uint64_t NMAX = 0x15B0;\n+\n+    \/\/ Unroll factor for L_nmax loop\n+    const int unroll = 8;\n+\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ vsetivli(temp0, 16, Assembler::e8, Assembler::m1);\n+\n+    \/\/ Generating accumulation coefficients for further calculations\n+    __ vid_v(vtemp1);\n+    __ mv(temp0, 16);\n+    __ vmv_v_x(vtable, temp0);\n+    __ vsub_vv(vtable, vtable, vtemp1);\n+    \/\/ vtable now contains { 0x10, 0xf, 0xe, ..., 0x3, 0x2, 0x1 }\n+\n+    __ vmv_v_i(vzero, 0);\n+\n+    __ mv(base, BASE);\n+    __ mv(nmax, NMAX);\n+\n+    __ srli(s2, adler, 16); \/\/ s2 = ((adler >> 16) & 0xffff)\n+    \/\/ s1 is initialized to the lower 16 bits of adler\n+    \/\/ s2 is initialized to the upper 16 bits of adler\n+    if (!UseZbb) {\n+      const uint64_t right_16_bits = right_n_bits(16);\n+      __ mv(temp0, right_16_bits);\n+      __ andr(s2, s2, temp0);\n+      __ andr(s1, adler, temp0); \/\/ s1 = (adler & 0xffff)\n+    } else {\n+      __ zext_h(s2, s2);\n+      __ zext_h(s1, adler);\n+    }\n+\n+    \/\/ The pipelined loop needs at least 16 elements for 1 iteration\n+    \/\/ It does check this, but it is more effective to skip to the cleanup loop\n+    __ mv(temp0, (u1)16);\n+    __ bgeu(len, temp0, L_nmax);\n+    __ beqz(len, L_combine);\n+\n+  __ bind(L_simple_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, 1);\n+    __ add(s1, s1, temp0);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, 1);\n+    __ bgtz(len, L_simple_by1_loop);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ j(L_combine);\n+\n+  __ bind(L_nmax);\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bltz(len, L_by16);\n+\n+  __ bind(L_nmax_loop_entry);\n+    \/\/ buf_end will be used as endpoint for loop below\n+    __ add(buf_end, buff, count); \/\/ buf_end will be used as endpoint for loop below\n+    __ sub(buf_end, buf_end, 32); \/\/ After partial unrolling 3 additional iterations are required,\n+                                  \/\/ and bytes for one of them are already subtracted\n+\n+  __ bind(L_nmax_loop);\n+    generate_updateBytesAdler32_unroll(buff, s1, s2, unroll, vtable, vbytes,\n+      vzero, unroll_regs, temp0, temp1, temp2, vtemp1, vtemp2);\n+    __ blt(buff, buf_end, L_nmax_loop);\n+\n+    const int remainder = ((NMAX \/ 16) % unroll);\n+    \/\/ Do the calculations for remaining 16*remainder bytes\n+    generate_updateBytesAdler32_unroll(buff, s1, s2, remainder, vtable, vbytes,\n+      vzero, unroll_regs, temp0, temp1, temp2, vtemp1, vtemp2);\n+\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    __ sub(len, len, nmax);\n+    __ sub(count, nmax, 16);\n+    __ bgez(len, L_nmax_loop_entry);\n+\n+  __ bind(L_by16);\n+    __ add(len, len, count);\n+    __ bltz(len, L_by1);\n+\n+  __ bind(L_by16_loop);\n+    generate_updateBytesAdler32_unroll(buff, s1, s2, 1, vtable, vbytes,\n+      vzero, unroll_regs, temp0, temp1, temp2, vtemp1, vtemp2);\n+\n+    __ sub(len, len, 16);\n+    __ bgez(len, L_by16_loop);\n+\n+  __ bind(L_by1);\n+    __ add(len, len, 15);\n+    __ bltz(len, L_do_mod);\n+\n+  __ bind(L_by1_loop);\n+    __ lbu(temp0, Address(buff, 0));\n+    __ addi(buff, buff, 1);\n+    __ add(s1, temp0, s1);\n+    __ add(s2, s2, s1);\n+    __ sub(len, len, 1);\n+    __ bgez(len, L_by1_loop);\n+\n+  __ bind(L_do_mod);\n+    \/\/ s1 = s1 % BASE\n+    __ remuw(s1, s1, base);\n+    \/\/ s2 = s2 % BASE\n+    __ remuw(s2, s2, base);\n+\n+    \/\/ Combine lower bits and higher bits\n+    \/\/ adler = s1 | (s2 << 16)\n+  __ bind(L_combine);\n+    __ slli(s2, s2, 16);\n+    __ orr(s1, s1, s2);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -5624,0 +5865,4 @@\n+    if (UseAdler32Intrinsics) {\n+      StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":245,"deletions":0,"binary":false,"changes":245,"status":"modified"},{"patch":"@@ -236,0 +236,12 @@\n+  \/\/ Adler32\n+  if (UseRVV) {\n+    if (FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      FLAG_SET_DEFAULT(UseAdler32Intrinsics, true);\n+    }\n+  } else if (UseAdler32Intrinsics) {\n+    if (!FLAG_IS_DEFAULT(UseAdler32Intrinsics)) {\n+      warning(\"Adler32 intrinsic requires RVV instructions (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}