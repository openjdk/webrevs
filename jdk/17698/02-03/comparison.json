{"files":[{"patch":"@@ -1812,2 +1812,2 @@\n-  \/\/ in riscv, NaN needs a special process as fcvt does not work in that case.\n-  \/\/ in riscv, Inf does not need a special process as fcvt can handle it correctly.\n+  \/\/ On riscv, NaN needs a special process as fcvt does not work in that case.\n+  \/\/ On riscv, Inf does not need a special process as fcvt can handle it correctly.\n@@ -1860,1 +1860,1 @@\n-  \/\/ in riscv, NaN needs a special process as fcvt does not work in that case.\n+  \/\/ On riscv, NaN needs a special process as fcvt does not work in that case.\n@@ -1875,90 +1875,0 @@\n-void C2_MacroAssembler::signum_fp_v(VectorRegister dst, VectorRegister one, BasicType bt, int vlen) {\n-  vsetvli_helper(bt, vlen);\n-\n-  \/\/ check if input is -0, +0, signaling NaN or quiet NaN\n-  vfclass_v(v0, dst);\n-  mv(t0, fclass_mask::zero | fclass_mask::nan);\n-  vand_vx(v0, v0, t0);\n-  vmseq_vi(v0, v0, 0);\n-\n-  \/\/ use floating-point 1.0 with a sign of input\n-  vfsgnj_vv(dst, one, dst, v0_t);\n-}\n-\n-void C2_MacroAssembler::compress_bits_v(Register dst, Register src, Register mask, bool is_long) {\n-  Assembler::SEW sew = is_long ? Assembler::e64 : Assembler::e32;\n-  \/\/ intrinsic is enabled when MaxVectorSize >= 16\n-  Assembler::LMUL lmul = is_long ? Assembler::m4 : Assembler::m2;\n-  long len = is_long ? 64 : 32;\n-\n-  \/\/ load the src data(in bits) to be compressed.\n-  vsetivli(x0, 1, sew, Assembler::m1);\n-  vmv_s_x(v0, src);\n-  \/\/ reset the src data(in bytes) to zero.\n-  mv(t0, len);\n-  vsetvli(x0, t0, Assembler::e8, lmul);\n-  vmv_v_i(v4, 0);\n-  \/\/ convert the src data from bits to bytes.\n-  vmerge_vim(v4, v4, 1); \/\/ v0 as the implicit mask register\n-  \/\/ reset the dst data(in bytes) to zero.\n-  vmv_v_i(v8, 0);\n-  \/\/ load the mask data(in bits).\n-  vsetivli(x0, 1, sew, Assembler::m1);\n-  vmv_s_x(v0, mask);\n-  \/\/ compress the src data(in bytes) to dst(in bytes).\n-  vsetvli(x0, t0, Assembler::e8, lmul);\n-  vcompress_vm(v8, v4, v0);\n-  \/\/ convert the dst data from bytes to bits.\n-  vmseq_vi(v0, v8, 1);\n-  \/\/ store result back.\n-  vsetivli(x0, 1, sew, Assembler::m1);\n-  vmv_x_s(dst, v0);\n-}\n-\n-void C2_MacroAssembler::compress_bits_i_v(Register dst, Register src, Register mask) {\n-  compress_bits_v(dst, src, mask, \/* is_long *\/ false);\n-}\n-\n-void C2_MacroAssembler::compress_bits_l_v(Register dst, Register src, Register mask) {\n-  compress_bits_v(dst, src, mask, \/* is_long *\/ true);\n-}\n-\n-void C2_MacroAssembler::expand_bits_v(Register dst, Register src, Register mask, bool is_long) {\n-  Assembler::SEW sew = is_long ? Assembler::e64 : Assembler::e32;\n-  \/\/ intrinsic is enabled when MaxVectorSize >= 16\n-  Assembler::LMUL lmul = is_long ? Assembler::m4 : Assembler::m2;\n-  long len = is_long ? 64 : 32;\n-\n-  \/\/ load the src data(in bits) to be expanded.\n-  vsetivli(x0, 1, sew, Assembler::m1);\n-  vmv_s_x(v0, src);\n-  \/\/ reset the src data(in bytes) to zero.\n-  mv(t0, len);\n-  vsetvli(x0, t0, Assembler::e8, lmul);\n-  vmv_v_i(v4, 0);\n-  \/\/ convert the src data from bits to bytes.\n-  vmerge_vim(v4, v4, 1); \/\/ v0 as implicit mask register\n-  \/\/ reset the dst data(in bytes) to zero.\n-  vmv_v_i(v12, 0);\n-  \/\/ load the mask data(in bits).\n-  vsetivli(x0, 1, sew, Assembler::m1);\n-  vmv_s_x(v0, mask);\n-  \/\/ expand the src data(in bytes) to dst(in bytes).\n-  vsetvli(x0, t0, Assembler::e8, lmul);\n-  viota_m(v8, v0);\n-  vrgather_vv(v12, v4, v8, VectorMask::v0_t); \/\/ v0 as implicit mask register\n-  \/\/ convert the dst data from bytes to bits.\n-  vmseq_vi(v0, v12, 1);\n-  \/\/ store result back.\n-  vsetivli(x0, 1, sew, Assembler::m1);\n-  vmv_x_s(dst, v0);\n-}\n-\n-void C2_MacroAssembler::expand_bits_i_v(Register dst, Register src, Register mask) {\n-  expand_bits_v(dst, src, mask, \/* is_long *\/ false);\n-}\n-\n-void C2_MacroAssembler::expand_bits_l_v(Register dst, Register src, Register mask) {\n-  expand_bits_v(dst, src, mask, \/* is_long *\/ true);\n-}\n-\n@@ -1978,2 +1888,0 @@\n-  \/\/ widen and sign-extend src data.\n-  __ vwadd_vx(dst, src, zr, Assembler::v0_t);\n@@ -1982,0 +1890,2 @@\n+  \/\/ widen and sign-extend src data.\n+  __ vsext_vf2(dst, src, Assembler::v0_t);\n@@ -1996,2 +1906,2 @@\n-  \/\/ in riscv, NaN needs a special process as vfwcvt_f_f_v does not work in that case.\n-  \/\/ in riscv, Inf does not need a special process as vfwcvt_f_f_v can handle it correctly.\n+  \/\/ On riscv, NaN needs a special process as vfwcvt_f_f_v does not work in that case.\n+  \/\/ On riscv, Inf does not need a special process as vfwcvt_f_f_v can handle it correctly.\n@@ -2030,1 +1940,1 @@\n-  __ vsetvli_helper(BasicType::T_SHORT, length, Assembler::mf2);\n+  \/\/ mul is already set to mf2 in float_to_float16_v.\n@@ -2054,1 +1964,1 @@\n-  \/\/ in riscv, NaN needs a special process as vfncvt_f_f_w does not work in that case.\n+  \/\/ On riscv, NaN needs a special process as vfncvt_f_f_w does not work in that case.\n@@ -2061,0 +1971,4 @@\n+\n+  \/\/ move vsetvli_helper forward here, as t0 is used as default temp register in vsetvli_helper in some situations.\n+  \/\/ and also moving vsetvli_helper(..., mf2) here does not impact vcpop_m.\n+  vsetvli_helper(BasicType::T_SHORT, length, Assembler::mf2);\n@@ -2063,1 +1977,0 @@\n-  vsetvli_helper(BasicType::T_SHORT, length, Assembler::mf2, zr);\n@@ -2073,0 +1986,90 @@\n+void C2_MacroAssembler::signum_fp_v(VectorRegister dst, VectorRegister one, BasicType bt, int vlen) {\n+  vsetvli_helper(bt, vlen);\n+\n+  \/\/ check if input is -0, +0, signaling NaN or quiet NaN\n+  vfclass_v(v0, dst);\n+  mv(t0, fclass_mask::zero | fclass_mask::nan);\n+  vand_vx(v0, v0, t0);\n+  vmseq_vi(v0, v0, 0);\n+\n+  \/\/ use floating-point 1.0 with a sign of input\n+  vfsgnj_vv(dst, one, dst, v0_t);\n+}\n+\n+void C2_MacroAssembler::compress_bits_v(Register dst, Register src, Register mask, bool is_long) {\n+  Assembler::SEW sew = is_long ? Assembler::e64 : Assembler::e32;\n+  \/\/ intrinsic is enabled when MaxVectorSize >= 16\n+  Assembler::LMUL lmul = is_long ? Assembler::m4 : Assembler::m2;\n+  long len = is_long ? 64 : 32;\n+\n+  \/\/ load the src data(in bits) to be compressed.\n+  vsetivli(x0, 1, sew, Assembler::m1);\n+  vmv_s_x(v0, src);\n+  \/\/ reset the src data(in bytes) to zero.\n+  mv(t0, len);\n+  vsetvli(x0, t0, Assembler::e8, lmul);\n+  vmv_v_i(v4, 0);\n+  \/\/ convert the src data from bits to bytes.\n+  vmerge_vim(v4, v4, 1); \/\/ v0 as the implicit mask register\n+  \/\/ reset the dst data(in bytes) to zero.\n+  vmv_v_i(v8, 0);\n+  \/\/ load the mask data(in bits).\n+  vsetivli(x0, 1, sew, Assembler::m1);\n+  vmv_s_x(v0, mask);\n+  \/\/ compress the src data(in bytes) to dst(in bytes).\n+  vsetvli(x0, t0, Assembler::e8, lmul);\n+  vcompress_vm(v8, v4, v0);\n+  \/\/ convert the dst data from bytes to bits.\n+  vmseq_vi(v0, v8, 1);\n+  \/\/ store result back.\n+  vsetivli(x0, 1, sew, Assembler::m1);\n+  vmv_x_s(dst, v0);\n+}\n+\n+void C2_MacroAssembler::compress_bits_i_v(Register dst, Register src, Register mask) {\n+  compress_bits_v(dst, src, mask, \/* is_long *\/ false);\n+}\n+\n+void C2_MacroAssembler::compress_bits_l_v(Register dst, Register src, Register mask) {\n+  compress_bits_v(dst, src, mask, \/* is_long *\/ true);\n+}\n+\n+void C2_MacroAssembler::expand_bits_v(Register dst, Register src, Register mask, bool is_long) {\n+  Assembler::SEW sew = is_long ? Assembler::e64 : Assembler::e32;\n+  \/\/ intrinsic is enabled when MaxVectorSize >= 16\n+  Assembler::LMUL lmul = is_long ? Assembler::m4 : Assembler::m2;\n+  long len = is_long ? 64 : 32;\n+\n+  \/\/ load the src data(in bits) to be expanded.\n+  vsetivli(x0, 1, sew, Assembler::m1);\n+  vmv_s_x(v0, src);\n+  \/\/ reset the src data(in bytes) to zero.\n+  mv(t0, len);\n+  vsetvli(x0, t0, Assembler::e8, lmul);\n+  vmv_v_i(v4, 0);\n+  \/\/ convert the src data from bits to bytes.\n+  vmerge_vim(v4, v4, 1); \/\/ v0 as implicit mask register\n+  \/\/ reset the dst data(in bytes) to zero.\n+  vmv_v_i(v12, 0);\n+  \/\/ load the mask data(in bits).\n+  vsetivli(x0, 1, sew, Assembler::m1);\n+  vmv_s_x(v0, mask);\n+  \/\/ expand the src data(in bytes) to dst(in bytes).\n+  vsetvli(x0, t0, Assembler::e8, lmul);\n+  viota_m(v8, v0);\n+  vrgather_vv(v12, v4, v8, VectorMask::v0_t); \/\/ v0 as implicit mask register\n+  \/\/ convert the dst data from bytes to bits.\n+  vmseq_vi(v0, v12, 1);\n+  \/\/ store result back.\n+  vsetivli(x0, 1, sew, Assembler::m1);\n+  vmv_x_s(dst, v0);\n+}\n+\n+void C2_MacroAssembler::expand_bits_i_v(Register dst, Register src, Register mask) {\n+  expand_bits_v(dst, src, mask, \/* is_long *\/ false);\n+}\n+\n+void C2_MacroAssembler::expand_bits_l_v(Register dst, Register src, Register mask) {\n+  expand_bits_v(dst, src, mask, \/* is_long *\/ true);\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":103,"deletions":100,"binary":false,"changes":203,"status":"modified"}]}