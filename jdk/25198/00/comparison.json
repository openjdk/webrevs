{"files":[{"patch":"@@ -306,1 +306,1 @@\n-  _min = MIN2(_size, _min);\n+  _min_last_uncommit_cycle = MIN2(_size, _min_last_uncommit_cycle);\n@@ -455,1 +455,2 @@\n-    _min(_size) {}\n+    _min_last_uncommit_cycle(_size),\n+    _removed_last_uncommit_cycle(0) {}\n@@ -554,2 +555,2 @@\n-size_t ZMappedCache::reset_min() {\n-  const size_t old_min = _min;\n+size_t ZMappedCache::reset_uncommit_cycle() {\n+  const size_t old_min = _min_last_uncommit_cycle;\n@@ -557,1 +558,2 @@\n-  _min = _size;\n+  _removed_last_uncommit_cycle = 0;\n+  _min_last_uncommit_cycle = _size;\n@@ -562,2 +564,6 @@\n-size_t ZMappedCache::remove_from_min(size_t max_size, ZArray<ZVirtualMemory>* out) {\n-  const size_t size = MIN2(_min, max_size);\n+size_t ZMappedCache::remove_for_uncommit(size_t max_size, ZArray<ZVirtualMemory>* out) {\n+  const size_t remove_allowed =\n+      _min_last_uncommit_cycle < _removed_last_uncommit_cycle\n+          ? 0\n+          : _min_last_uncommit_cycle - _removed_last_uncommit_cycle;\n+  const size_t size = MIN2(remove_allowed, max_size);\n@@ -569,1 +575,4 @@\n-  return remove_discontiguous_with_strategy<RemovalStrategy::HighestAddress>(size, out);\n+  const size_t removed = remove_discontiguous_with_strategy<RemovalStrategy::HighestAddress>(size, out);\n+  _removed_last_uncommit_cycle += removed;\n+\n+  return removed;\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.cpp","additions":17,"deletions":8,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -66,1 +66,2 @@\n-  size_t        _min;\n+  size_t        _min_last_uncommit_cycle;\n+  size_t        _removed_last_uncommit_cycle;\n@@ -105,2 +106,2 @@\n-  size_t reset_min();\n-  size_t remove_from_min(size_t max_size, ZArray<ZVirtualMemory>* out);\n+  size_t reset_uncommit_cycle();\n+  size_t remove_for_uncommit(size_t max_size, ZArray<ZVirtualMemory>* out);\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -612,3 +612,0 @@\n-    _last_commit(0.0),\n-    _last_uncommit(0.0),\n-    _to_uncommit(0),\n@@ -632,3 +629,1 @@\n-    _last_commit = os::elapsedTime();\n-    _last_uncommit = 0;\n-    _cache.reset_min();\n+    _uncommitter.cancel_uncommit_cycle(&_cache);\n@@ -743,1 +738,1 @@\n-size_t ZPartition::uncommit(uint64_t* timeout) {\n+size_t ZPartition::uncommit() {\n@@ -753,5 +748,1 @@\n-    const double now = os::elapsedTime();\n-    const double time_since_last_commit = std::floor(now - _last_commit);\n-    const double time_since_last_uncommit = std::floor(now - _last_uncommit);\n-\n-    if (time_since_last_commit < double(ZUncommitDelay)) {\n+    if (_uncommitter.uncommit_cycle_is_canceled()) {\n@@ -759,1 +750,0 @@\n-      *timeout = uint64_t(double(ZUncommitDelay) - time_since_last_commit);\n@@ -772,2 +762,1 @@\n-      \/\/ Set timeout to ZUncommitDelay\n-      *timeout = ZUncommitDelay;\n+      _uncommitter.cancel_uncommit_cycle(&_cache);\n@@ -777,17 +766,4 @@\n-    if (time_since_last_uncommit < double(ZUncommitDelay)) {\n-      \/\/ We are in the uncommit phase\n-      const size_t num_uncommits_left = _to_uncommit \/ limit;\n-      const double time_left = double(ZUncommitDelay) - time_since_last_uncommit;\n-      if (time_left < *timeout * num_uncommits_left) {\n-        \/\/ Running out of time, speed up.\n-        uint64_t new_timeout = uint64_t(std::floor(time_left \/ double(num_uncommits_left + 1)));\n-        *timeout = new_timeout;\n-      }\n-    } else {\n-      \/\/ We are about to start uncommitting\n-      _to_uncommit = _cache.reset_min();\n-      _last_uncommit = now;\n-\n-      const size_t split = _to_uncommit \/ limit + 1;\n-      uint64_t new_timeout = ZUncommitDelay \/ split;\n-      *timeout = new_timeout;\n+    if (!_uncommitter.uncommit_cycle_is_active()) {\n+      \/\/ We are activating a new cycle\n+      const size_t uncommit_limit = _capacity - _min_capacity;\n+      _uncommitter.activate_uncommit_cycle(&_cache, uncommit_limit);\n@@ -796,0 +772,2 @@\n+    const size_t to_uncommit = _uncommitter.to_uncommit();\n+\n@@ -799,1 +777,1 @@\n-    const size_t flush = MIN3(release, limit, _to_uncommit);\n+    const size_t flush = MIN3(release, limit, to_uncommit);\n@@ -803,0 +781,1 @@\n+      _uncommitter.cancel_uncommit_cycle(&_cache);\n@@ -807,1 +786,1 @@\n-    flushed = _cache.remove_from_min(flush, &flushed_vmems);\n+    flushed = _cache.remove_for_uncommit(flush, &flushed_vmems);\n@@ -810,0 +789,1 @@\n+      _uncommitter.cancel_uncommit_cycle(&_cache);\n@@ -815,1 +795,0 @@\n-    _to_uncommit -= flushed;\n@@ -833,0 +812,1 @@\n+    _uncommitter.register_uncommit(flushed);\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":15,"deletions":35,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -71,3 +71,0 @@\n-  double                _last_commit;\n-  double                _last_uncommit;\n-  size_t                _to_uncommit;\n@@ -105,1 +102,3 @@\n-  size_t uncommit(uint64_t* timeout);\n+  template <typename Fn>\n+  void evaluate_under_lock(Fn function) const;\n+  size_t uncommit();\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,0 +29,6 @@\n+template <typename Fn>\n+void ZPartition::evaluate_under_lock(Fn function) const {\n+  ZLocker<ZLock> locker(&_page_allocator->_lock);\n+  function();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.inline.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -116,0 +116,5 @@\n+  const size_t max_delay_without_overflow = std::numeric_limits<uint64_t>::max() \/ MILLIUNITS;\n+  if (ZUncommitDelay > max_delay_without_overflow) {\n+    FLAG_SET_ERGO(ZUncommitDelay, max_delay_without_overflow);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemoryManager.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/z\/zGlobals.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"gc\/z\/zMappedCache.hpp\"\n@@ -31,0 +33,6 @@\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+#include <cmath>\n@@ -38,1 +46,7 @@\n-    _stop(false) {\n+    _stop(false),\n+    _cancel_time(0.0),\n+    _next_cycle_timeout(0),\n+    _next_uncommit_timeout(0),\n+    _cycle_start(0.0),\n+    _to_uncommit(0),\n+    _uncommitted(0) {\n@@ -50,2 +64,21 @@\n-    log_debug(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"s\", _id, timeout);\n-    _lock.wait(timeout * MILLIUNITS);\n+    if (!uncommit_cycle_is_finished()) {\n+      log_trace(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"ms left to uncommit: \"\n+                          EXACTFMT, _id, timeout, EXACTFMTARGS(_to_uncommit));\n+    } else {\n+      log_debug(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"ms\", _id, timeout);\n+    }\n+\n+    double now = os::elapsedTime();\n+    const double wait_until = now + double(timeout) \/ MILLIUNITS;\n+    do {\n+      const uint64_t remaining_timeout_ms = to_millis(wait_until - now);\n+      if (remaining_timeout_ms == 0) {\n+        \/\/ Less than a millisecond left to wait, just return early\n+        break;\n+      }\n+\n+      \/\/ Wait\n+      _lock.wait(remaining_timeout_ms);\n+\n+      now = os::elapsedTime();\n+    } while (!_stop && now < wait_until);\n@@ -63,1 +96,2 @@\n-  uint64_t timeout = 0;\n+  \/\/ Initialize first cycle timeout\n+  _next_cycle_timeout = to_millis(ZUncommitDelay);\n@@ -65,3 +99,4 @@\n-  while (wait(timeout)) {\n-    EventZUncommit event;\n-    size_t total_uncommitted = 0;\n+  while (wait(_next_cycle_timeout)) {\n+    \/\/ Counters for event and statistics\n+    Ticks start = Ticks::now();\n+    size_t uncommitted_since_last_timeout = 0;\n@@ -71,2 +106,6 @@\n-      const size_t uncommitted = _partition->uncommit(&timeout);\n-      if (uncommitted == 0) {\n+      const size_t uncommitted = _partition->uncommit();\n+\n+      \/\/ Update uncommitted counter\n+      uncommitted_since_last_timeout += uncommitted;\n+\n+      if (uncommitted == 0 || uncommit_cycle_is_finished()) {\n@@ -77,1 +116,14 @@\n-      total_uncommitted += uncommitted;\n+      if (_next_uncommit_timeout != 0) {\n+        \/\/ Update statistics\n+        ZStatInc(ZCounterUncommit, uncommitted_since_last_timeout);\n+\n+        \/\/ Send event\n+        EventZUncommit::commit(start, Ticks::now(), uncommitted_since_last_timeout);\n+\n+        \/\/ Wait until next uncommit\n+        wait(_next_uncommit_timeout);\n+\n+        \/\/ Reset event and statistics counters\n+        start = Ticks::now();\n+        uncommitted_since_last_timeout = 0;\n+      }\n@@ -80,3 +132,1 @@\n-    if (total_uncommitted > 0) {\n-      \/\/ Update statistics\n-      ZStatInc(ZCounterUncommit, total_uncommitted);\n+    if (_uncommitted > 0) {\n@@ -84,1 +134,5 @@\n-                         _id, total_uncommitted \/ M, percent_of(total_uncommitted, ZHeap::heap()->max_capacity()));\n+                         _id, _uncommitted \/ M, percent_of(_uncommitted, ZHeap::heap()->max_capacity()));\n+\n+      if (uncommitted_since_last_timeout > 0) {\n+        \/\/ Update statistics\n+        ZStatInc(ZCounterUncommit, uncommitted_since_last_timeout);\n@@ -86,2 +140,3 @@\n-      \/\/ Send event\n-      event.commit(total_uncommitted);\n+        \/\/ Send event\n+        EventZUncommit::commit(start, Ticks::now(), uncommitted_since_last_timeout);\n+      }\n@@ -89,0 +144,2 @@\n+\n+    deactivate_uncommit_cycle();\n@@ -97,0 +154,153 @@\n+\n+void ZUncommitter::deactivate_uncommit_cycle() {\n+  if (!should_continue()) {\n+    \/\/ We are stopping\n+    return;\n+  }\n+\n+  _partition->evaluate_under_lock([&]() {\n+    precond(uncommit_cycle_is_active() || uncommit_cycle_is_canceled());\n+    precond(uncommit_cycle_is_finished() || uncommit_cycle_is_canceled());\n+\n+    \/\/ Update the next timeout\n+    if (uncommit_cycle_is_canceled()) {\n+      update_next_cycle_timeout_on_cancel();\n+    } else {\n+      update_next_cycle_timeout_on_finish();\n+    }\n+\n+    \/\/ Reset the cycle\n+    _to_uncommit = 0;\n+    _uncommitted = 0;\n+    _cycle_start = 0.0;\n+    _cancel_time = 0.0;\n+\n+    postcond(uncommit_cycle_is_finished());\n+    postcond(!uncommit_cycle_is_canceled());\n+    postcond(!uncommit_cycle_is_active());\n+  });\n+}\n+\n+void ZUncommitter::activate_uncommit_cycle(ZMappedCache* cache, size_t uncommit_limit) {\n+  precond(uncommit_cycle_is_finished());\n+  precond(!uncommit_cycle_is_active());\n+  precond(!uncommit_cycle_is_canceled());\n+  precond(is_aligned(uncommit_limit, ZGranuleSize));\n+\n+  \/\/ Claim and reset the cache cycle tracking and register the cycle start time.\n+  _cycle_start = os::elapsedTime();\n+  _to_uncommit = MIN2(uncommit_limit, cache->reset_uncommit_cycle());\n+  _uncommitted = 0;\n+\n+  postcond(is_aligned(_to_uncommit, ZGranuleSize));\n+}\n+\n+size_t ZUncommitter::to_uncommit() const {\n+  return _to_uncommit;\n+}\n+\n+uint64_t ZUncommitter::to_millis(double seconds) const {\n+  return uint64_t(std::floor(seconds * double(MILLIUNITS)));\n+}\n+\n+void ZUncommitter::update_next_cycle_timeout(double from_time) {\n+  const double now = os::elapsedTime();\n+\n+  if (now < from_time + double(ZUncommitDelay)) {\n+    _next_cycle_timeout = to_millis(ZUncommitDelay) - to_millis(now - from_time);\n+  } else {\n+    \/\/ ZUncommitDelay has already expired\n+    _next_cycle_timeout = 0;\n+  }\n+}\n+\n+void ZUncommitter::update_next_cycle_timeout_on_cancel() {\n+  precond(uncommit_cycle_is_canceled());\n+\n+  update_next_cycle_timeout(_cancel_time);\n+\n+  log_debug(gc, heap)(\"Uncommitter (%u) Cancel Next Cycle Timeout: \" UINT64_FORMAT \"ms\",\n+                      _id, _next_cycle_timeout);\n+}\n+\n+void ZUncommitter::update_next_cycle_timeout_on_finish() {\n+  precond(uncommit_cycle_is_active());\n+  precond(uncommit_cycle_is_finished());\n+\n+  update_next_cycle_timeout(_cycle_start);\n+\n+  log_debug(gc, heap)(\"Uncommitter (%u) Finish Next Cycle Timeout: \" UINT64_FORMAT \"ms\",\n+                      _id, _next_cycle_timeout);\n+}\n+\n+void ZUncommitter::cancel_uncommit_cycle(ZMappedCache* cache) {\n+  \/\/ Reset the cache cycle tracking and register the cancel time.\n+  cache->reset_uncommit_cycle();\n+  _cancel_time = os::elapsedTime();\n+}\n+\n+void ZUncommitter::register_uncommit(size_t size) {\n+  precond(uncommit_cycle_is_active());\n+  precond(size > 0);\n+  precond(size <= _to_uncommit);\n+  precond(is_aligned(size, ZGranuleSize));\n+\n+  _to_uncommit -= size;\n+  _uncommitted += size;\n+\n+  if (uncommit_cycle_is_canceled()) {\n+    \/\/ Uncommit cycle got canceled while uncommitting.\n+    return;\n+  }\n+\n+  if (uncommit_cycle_is_finished()) {\n+    \/\/ Everything has been uncommitted.\n+    return;\n+  }\n+\n+  const double now = os::elapsedTime();\n+  const double time_since_start = now - _cycle_start;\n+\n+  if (time_since_start == 0.0) {\n+    \/\/ Handle degenerate case where no time has elapsed.\n+    _next_uncommit_timeout = 0;\n+    return;\n+  }\n+\n+  const double uncommit_rate = double(_uncommitted) \/ time_since_start;\n+  const double time_to_compleat = double(_to_uncommit) \/ uncommit_rate;\n+  const double time_left = double(ZUncommitDelay) - time_since_start;\n+\n+  if (time_left < time_to_compleat) {\n+    \/\/ To slow, work as fast as we can.\n+    _next_uncommit_timeout = 0;\n+    return;\n+  }\n+\n+  const size_t uncommits_remaining_estimate = _to_uncommit \/ size + 1;\n+  const uint64_t millis_left_rounded_down = to_millis(time_left);\n+\n+  if (uncommits_remaining_estimate < millis_left_rounded_down) {\n+    \/\/ We have at least one millisecond per uncommit, spread them out.\n+    _next_uncommit_timeout = millis_left_rounded_down \/ uncommits_remaining_estimate;\n+    return;\n+  }\n+\n+  \/\/ Randomly distribute the extra time, one millisecond at a time.\n+  const double extra_time = time_left - time_to_compleat;\n+  const double random = double(uint32_t(os::random())) \/ double(std::numeric_limits<uint32_t>::max());\n+\n+  _next_uncommit_timeout = random < (extra_time \/ time_left) ? 1 : 0;\n+}\n+\n+bool ZUncommitter::uncommit_cycle_is_finished() const {\n+  return _to_uncommit == 0;\n+}\n+\n+bool ZUncommitter::uncommit_cycle_is_active() const {\n+  return _cycle_start != 0.0;\n+}\n+\n+bool ZUncommitter::uncommit_cycle_is_canceled() const {\n+  return _cancel_time != 0.0;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.cpp","additions":226,"deletions":16,"binary":false,"changes":242,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+class ZMappedCache;\n@@ -38,0 +39,6 @@\n+  double                 _cancel_time;\n+  uint64_t               _next_cycle_timeout;\n+  uint64_t               _next_uncommit_timeout;\n+  double                 _cycle_start;\n+  size_t                 _to_uncommit;\n+  size_t                 _uncommitted;\n@@ -42,0 +49,8 @@\n+  uint64_t to_millis(double seconds) const;\n+\n+  void update_next_cycle_timeout(double from_time);\n+  void update_next_cycle_timeout_on_cancel();\n+  void update_next_cycle_timeout_on_finish();\n+\n+  void deactivate_uncommit_cycle();\n+\n@@ -48,0 +63,9 @@\n+\n+  void activate_uncommit_cycle(ZMappedCache* cache, size_t uncommit_limit);\n+  size_t to_uncommit() const;\n+  void cancel_uncommit_cycle(ZMappedCache* cache);\n+  void register_uncommit(size_t size);\n+\n+  bool uncommit_cycle_is_finished() const;\n+  bool uncommit_cycle_is_active() const;\n+  bool uncommit_cycle_is_canceled() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.hpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n- * @library \/test\/lib\n@@ -35,1 +34,0 @@\n-import jdk.test.lib.Utils;\n@@ -113,1 +111,1 @@\n-        if (actualDelay > delay * 2 * Utils.TIMEOUT_FACTOR) {\n+        if (actualDelay > delay * 3) {\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestUncommit.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"}]}