{"files":[{"patch":"@@ -306,1 +306,1 @@\n-  _min = MIN2(_size, _min);\n+  _min_size_watermark = MIN2(_size, _min_size_watermark);\n@@ -455,1 +455,1 @@\n-    _min(_size) {}\n+    _min_size_watermark(_size) {}\n@@ -554,6 +554,2 @@\n-size_t ZMappedCache::reset_min() {\n-  const size_t old_min = _min;\n-\n-  _min = _size;\n-\n-  return old_min;\n+void ZMappedCache::reset_uncommit_watermark() {\n+  _min_size_watermark = _size;\n@@ -562,2 +558,3 @@\n-size_t ZMappedCache::remove_from_min(size_t max_size, ZArray<ZVirtualMemory>* out) {\n-  const size_t size = MIN2(_min, max_size);\n+size_t ZMappedCache::uncommit_watermark() {\n+  return _min_size_watermark;\n+}\n@@ -565,0 +562,1 @@\n+size_t ZMappedCache::remove_for_uncommit(size_t size, ZArray<ZVirtualMemory>* out) {\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.cpp","additions":8,"deletions":10,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-  size_t        _min;\n+  size_t        _min_size_watermark;\n@@ -105,2 +105,3 @@\n-  size_t reset_min();\n-  size_t remove_from_min(size_t max_size, ZArray<ZVirtualMemory>* out);\n+  void reset_uncommit_watermark();\n+  size_t uncommit_watermark();\n+  size_t remove_for_uncommit(size_t size, ZArray<ZVirtualMemory>* out);\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -612,3 +612,0 @@\n-    _last_commit(0.0),\n-    _last_uncommit(0.0),\n-    _to_uncommit(0),\n@@ -632,3 +629,1 @@\n-    _last_commit = os::elapsedTime();\n-    _last_uncommit = 0;\n-    _cache.reset_min();\n+    _uncommitter.cancel_uncommit_cycle();\n@@ -743,95 +738,0 @@\n-size_t ZPartition::uncommit(uint64_t* timeout) {\n-  ZArray<ZVirtualMemory> flushed_vmems;\n-  size_t flushed = 0;\n-\n-  {\n-    \/\/ We need to join the suspendible thread set while manipulating capacity\n-    \/\/ and used, to make sure GC safepoints will have a consistent view.\n-    SuspendibleThreadSetJoiner sts_joiner;\n-    ZLocker<ZLock> locker(&_page_allocator->_lock);\n-\n-    const double now = os::elapsedTime();\n-    const double time_since_last_commit = std::floor(now - _last_commit);\n-    const double time_since_last_uncommit = std::floor(now - _last_uncommit);\n-\n-    if (time_since_last_commit < double(ZUncommitDelay)) {\n-      \/\/ We have committed within the delay, stop uncommitting.\n-      *timeout = uint64_t(double(ZUncommitDelay) - time_since_last_commit);\n-      return 0;\n-    }\n-\n-    \/\/ We flush out and uncommit chunks at a time (~0.8% of the max capacity,\n-    \/\/ but at least one granule and at most 256M), in case demand for memory\n-    \/\/ increases while we are uncommitting.\n-    const size_t limit_upper_bound = MAX2(ZGranuleSize, align_down(256 * M \/ ZNUMA::count(), ZGranuleSize));\n-    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, ZGranuleSize), limit_upper_bound);\n-\n-    if (limit == 0) {\n-      \/\/ This may occur if the current max capacity for this partition is 0\n-\n-      \/\/ Set timeout to ZUncommitDelay\n-      *timeout = ZUncommitDelay;\n-      return 0;\n-    }\n-\n-    if (time_since_last_uncommit < double(ZUncommitDelay)) {\n-      \/\/ We are in the uncommit phase\n-      const size_t num_uncommits_left = _to_uncommit \/ limit;\n-      const double time_left = double(ZUncommitDelay) - time_since_last_uncommit;\n-      if (time_left < *timeout * num_uncommits_left) {\n-        \/\/ Running out of time, speed up.\n-        uint64_t new_timeout = uint64_t(std::floor(time_left \/ double(num_uncommits_left + 1)));\n-        *timeout = new_timeout;\n-      }\n-    } else {\n-      \/\/ We are about to start uncommitting\n-      _to_uncommit = _cache.reset_min();\n-      _last_uncommit = now;\n-\n-      const size_t split = _to_uncommit \/ limit + 1;\n-      uint64_t new_timeout = ZUncommitDelay \/ split;\n-      *timeout = new_timeout;\n-    }\n-\n-    \/\/ Never uncommit below min capacity.\n-    const size_t retain = MAX2(_used, _min_capacity);\n-    const size_t release = _capacity - retain;\n-    const size_t flush = MIN3(release, limit, _to_uncommit);\n-\n-    if (flush == 0) {\n-      \/\/ Nothing to flush\n-      return 0;\n-    }\n-\n-    \/\/ Flush memory from the mapped cache to uncommit\n-    flushed = _cache.remove_from_min(flush, &flushed_vmems);\n-    if (flushed == 0) {\n-      \/\/ Nothing flushed\n-      return 0;\n-    }\n-\n-    \/\/ Record flushed memory as claimed and how much we've flushed for this partition\n-    Atomic::add(&_claimed, flushed);\n-    _to_uncommit -= flushed;\n-  }\n-\n-  \/\/ Unmap and uncommit flushed memory\n-  for (const ZVirtualMemory vmem : flushed_vmems) {\n-    unmap_virtual(vmem);\n-    uncommit_physical(vmem);\n-    free_physical(vmem);\n-    free_virtual(vmem);\n-  }\n-\n-  {\n-    SuspendibleThreadSetJoiner sts_joiner;\n-    ZLocker<ZLock> locker(&_page_allocator->_lock);\n-\n-    \/\/ Adjust claimed and capacity to reflect the uncommit\n-    Atomic::sub(&_claimed, flushed);\n-    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n-  }\n-\n-  return flushed;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":1,"deletions":101,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+  friend class ZUncommitter;\n@@ -71,3 +72,0 @@\n-  double                _last_commit;\n-  double                _last_uncommit;\n-  size_t                _to_uncommit;\n@@ -105,2 +103,0 @@\n-  size_t uncommit(uint64_t* timeout);\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -116,0 +116,5 @@\n+  const size_t max_delay_without_overflow = std::numeric_limits<uint64_t>::max() \/ MILLIUNITS;\n+  if (ZUncommitDelay > max_delay_without_overflow) {\n+    FLAG_SET_ERGO(ZUncommitDelay, max_delay_without_overflow);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemoryManager.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/z\/zGlobals.hpp\"\n@@ -27,0 +28,2 @@\n+#include \"gc\/z\/zMappedCache.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n@@ -31,0 +34,6 @@\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+#include <cmath>\n@@ -38,1 +47,7 @@\n-    _stop(false) {\n+    _stop(false),\n+    _cancel_time(0.0),\n+    _next_cycle_timeout(0),\n+    _next_uncommit_timeout(0),\n+    _cycle_start(0.0),\n+    _to_uncommit(0),\n+    _uncommitted(0) {\n@@ -50,2 +65,21 @@\n-    log_debug(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"s\", _id, timeout);\n-    _lock.wait(timeout * MILLIUNITS);\n+    if (!uncommit_cycle_is_finished()) {\n+      log_trace(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"ms left to uncommit: \"\n+                          EXACTFMT, _id, timeout, EXACTFMTARGS(_to_uncommit));\n+    } else {\n+      log_debug(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"ms\", _id, timeout);\n+    }\n+\n+    double now = os::elapsedTime();\n+    const double wait_until = now + double(timeout) \/ MILLIUNITS;\n+    do {\n+      const uint64_t remaining_timeout_ms = to_millis(wait_until - now);\n+      if (remaining_timeout_ms == 0) {\n+        \/\/ Less than a millisecond left to wait, just return early\n+        break;\n+      }\n+\n+      \/\/ Wait\n+      _lock.wait(remaining_timeout_ms);\n+\n+      now = os::elapsedTime();\n+    } while (!_stop && now < wait_until);\n@@ -63,1 +97,2 @@\n-  uint64_t timeout = 0;\n+  \/\/ Initialize first cycle timeout\n+  _next_cycle_timeout = to_millis(ZUncommitDelay);\n@@ -65,3 +100,10 @@\n-  while (wait(timeout)) {\n-    EventZUncommit event;\n-    size_t total_uncommitted = 0;\n+  while (wait(_next_cycle_timeout)) {\n+    \/\/ Counters for event and statistics\n+    Ticks start = Ticks::now();\n+    size_t uncommitted_since_last_timeout = 0;\n+    Tickspan accumulated_time;\n+\n+    if (!activate_uncommit_cycle()) {\n+      \/\/ We failed activating a new cycle, continue until next cycle\n+      continue;\n+    }\n@@ -71,2 +113,8 @@\n-      const size_t uncommitted = _partition->uncommit(&timeout);\n-      if (uncommitted == 0) {\n+      const size_t uncommitted = uncommit();\n+\n+      \/\/ Update uncommitted counter\n+      uncommitted_since_last_timeout += uncommitted;\n+\n+      \/\/ 'uncommitted == 0' is a proxy for uncommit_cycle_is_canceled() without\n+      \/\/ having to take the page allocator lock\n+      if (uncommitted == 0 || uncommit_cycle_is_finished()) {\n@@ -77,1 +125,19 @@\n-      total_uncommitted += uncommitted;\n+      if (_next_uncommit_timeout != 0) {\n+        \/\/ Update statistics\n+        ZStatInc(ZCounterUncommit, uncommitted_since_last_timeout);\n+\n+        Ticks end = Ticks::now();\n+\n+        \/\/ Send event\n+        EventZUncommit::commit(start, end, uncommitted_since_last_timeout);\n+\n+        \/\/ Track accumulated time\n+        accumulated_time += end - start;\n+\n+        \/\/ Wait until next uncommit\n+        wait(_next_uncommit_timeout);\n+\n+        \/\/ Reset event and statistics counters\n+        start = Ticks::now();\n+        uncommitted_since_last_timeout = 0;\n+      }\n@@ -80,5 +146,13 @@\n-    if (total_uncommitted > 0) {\n-      \/\/ Update statistics\n-      ZStatInc(ZCounterUncommit, total_uncommitted);\n-      log_info(gc, heap)(\"Uncommitter (%u) Uncommitted: %zuM(%.0f%%)\",\n-                         _id, total_uncommitted \/ M, percent_of(total_uncommitted, ZHeap::heap()->max_capacity()));\n+    if (_uncommitted > 0) {\n+      if (uncommitted_since_last_timeout > 0) {\n+        \/\/ Update statistics\n+        ZStatInc(ZCounterUncommit, uncommitted_since_last_timeout);\n+\n+        Ticks end = Ticks::now();\n+\n+        \/\/ Send event\n+        EventZUncommit::commit(start, end, uncommitted_since_last_timeout);\n+\n+        \/\/ Track accumulated time\n+        accumulated_time += end - start;\n+      }\n@@ -86,2 +160,3 @@\n-      \/\/ Send event\n-      event.commit(total_uncommitted);\n+      log_info(gc, heap)(\"Uncommitter (%u) Uncommitted: %zuM(%.0f%%) in %.3fms\",\n+                         _id, _uncommitted \/ M, percent_of(_uncommitted, ZHeap::heap()->max_capacity()),\n+                         accumulated_time.seconds() * MILLIUNITS);\n@@ -89,0 +164,2 @@\n+\n+    deactivate_uncommit_cycle();\n@@ -97,0 +174,250 @@\n+\n+void ZUncommitter::reset_uncommit_cycle() {\n+  _to_uncommit = 0;\n+  _uncommitted = 0;\n+  _cycle_start = 0.0;\n+  _cancel_time = 0.0;\n+\n+  postcond(uncommit_cycle_is_finished());\n+  postcond(!uncommit_cycle_is_canceled());\n+  postcond(!uncommit_cycle_is_active());\n+}\n+\n+void ZUncommitter::deactivate_uncommit_cycle() {\n+  ZLocker<ZLock> locker(&_partition->_page_allocator->_lock);\n+\n+  precond(uncommit_cycle_is_active());\n+  precond(uncommit_cycle_is_finished() || uncommit_cycle_is_canceled());\n+\n+  \/\/ Update the next timeout\n+  if (uncommit_cycle_is_canceled()) {\n+    update_next_cycle_timeout_on_cancel();\n+  } else {\n+    update_next_cycle_timeout_on_finish();\n+  }\n+\n+  \/\/ Reset the cycle\n+  reset_uncommit_cycle();\n+}\n+\n+bool ZUncommitter::activate_uncommit_cycle() {\n+  ZLocker<ZLock> locker(&_partition->_page_allocator->_lock);\n+\n+  precond(uncommit_cycle_is_finished());\n+  precond(!uncommit_cycle_is_active());\n+\n+  if (uncommit_cycle_is_canceled()) {\n+    \/\/ We were canceled before we managed to activate, update the timeout\n+    update_next_cycle_timeout_on_cancel();\n+\n+    \/\/ Reset the cycle\n+    reset_uncommit_cycle();\n+\n+    return false;\n+  }\n+\n+  ZMappedCache* const cache = &_partition->_cache;\n+\n+  \/\/ Claim and reset the cache cycle tracking and register the cycle start time.\n+  _cycle_start = os::elapsedTime();\n+\n+  \/\/ Read watermark from cache\n+  const size_t uncommit_watermark = cache->uncommit_watermark();\n+\n+  \/\/ Keep 10% as a headroom\n+  const size_t to_uncommit = align_up(size_t(double(uncommit_watermark) * 0.9), ZGranuleSize);\n+\n+  \/\/ Never uncommit below min capacity\n+  const size_t uncommit_limit = _partition->_capacity - _partition->_min_capacity;\n+\n+  _to_uncommit = MIN2(uncommit_limit, to_uncommit);\n+  _uncommitted = 0;\n+\n+  \/\/ Reset watermark for next uncommit cycle\n+  cache->reset_uncommit_watermark();\n+\n+  postcond(is_aligned(_to_uncommit, ZGranuleSize));\n+\n+  return true;\n+}\n+\n+uint64_t ZUncommitter::to_millis(double seconds) const {\n+  return uint64_t(std::floor(seconds * double(MILLIUNITS)));\n+}\n+\n+void ZUncommitter::update_next_cycle_timeout(double from_time) {\n+  const double now = os::elapsedTime();\n+\n+  if (now < from_time + double(ZUncommitDelay)) {\n+    _next_cycle_timeout = to_millis(ZUncommitDelay) - to_millis(now - from_time);\n+  } else {\n+    \/\/ ZUncommitDelay has already expired\n+    _next_cycle_timeout = 0;\n+  }\n+}\n+\n+void ZUncommitter::update_next_cycle_timeout_on_cancel() {\n+  precond(uncommit_cycle_is_canceled());\n+\n+  update_next_cycle_timeout(_cancel_time);\n+\n+  \/\/ Skip logging if there is no delay\n+  if (ZUncommitDelay > 0) {\n+    log_debug(gc, heap)(\"Uncommitter (%u) Cancel Next Cycle Timeout: \" UINT64_FORMAT \"ms\",\n+                        _id, _next_cycle_timeout);\n+  }\n+}\n+\n+void ZUncommitter::update_next_cycle_timeout_on_finish() {\n+  precond(uncommit_cycle_is_active());\n+  precond(uncommit_cycle_is_finished());\n+\n+  update_next_cycle_timeout(_cycle_start);\n+\n+  \/\/ Skip logging if there is no delay\n+  if (ZUncommitDelay > 0) {\n+    log_debug(gc, heap)(\"Uncommitter (%u) Finish Next Cycle Timeout: \" UINT64_FORMAT \"ms\",\n+                        _id, _next_cycle_timeout);\n+  }\n+}\n+\n+void ZUncommitter::cancel_uncommit_cycle() {\n+  \/\/ Reset the cache cycle tracking and register the cancel time.\n+  _partition->_cache.reset_uncommit_watermark();\n+  _cancel_time = os::elapsedTime();\n+}\n+\n+void ZUncommitter::register_uncommit(size_t size) {\n+  precond(uncommit_cycle_is_active());\n+  precond(size > 0);\n+  precond(size <= _to_uncommit);\n+  precond(is_aligned(size, ZGranuleSize));\n+\n+  _to_uncommit -= size;\n+  _uncommitted += size;\n+\n+  if (uncommit_cycle_is_canceled()) {\n+    \/\/ Uncommit cycle got canceled while uncommitting.\n+    return;\n+  }\n+\n+  if (uncommit_cycle_is_finished()) {\n+    \/\/ Everything has been uncommitted.\n+    return;\n+  }\n+\n+  const double now = os::elapsedTime();\n+  const double time_since_start = now - _cycle_start;\n+\n+  if (time_since_start == 0.0) {\n+    \/\/ Handle degenerate case where no time has elapsed.\n+    _next_uncommit_timeout = 0;\n+    return;\n+  }\n+\n+  const double uncommit_rate = double(_uncommitted) \/ time_since_start;\n+  const double time_to_complete = double(_to_uncommit) \/ uncommit_rate;\n+  const double time_left = double(ZUncommitDelay) - time_since_start;\n+\n+  if (time_left < time_to_complete) {\n+    \/\/ Too slow, work as fast as we can.\n+    _next_uncommit_timeout = 0;\n+    return;\n+  }\n+\n+  const size_t uncommits_remaining_estimate = _to_uncommit \/ size + 1;\n+  const uint64_t millis_left_rounded_down = to_millis(time_left);\n+\n+  if (uncommits_remaining_estimate < millis_left_rounded_down) {\n+    \/\/ We have at least one millisecond per uncommit, spread them out.\n+    _next_uncommit_timeout = millis_left_rounded_down \/ uncommits_remaining_estimate;\n+    return;\n+  }\n+\n+  \/\/ Randomly distribute the extra time, one millisecond at a time.\n+  const double extra_time = time_left - time_to_complete;\n+  const double random = double(uint32_t(os::random())) \/ double(std::numeric_limits<uint32_t>::max());\n+\n+  _next_uncommit_timeout = random < (extra_time \/ time_left) ? 1 : 0;\n+}\n+\n+bool ZUncommitter::uncommit_cycle_is_finished() const {\n+  return _to_uncommit == 0;\n+}\n+\n+bool ZUncommitter::uncommit_cycle_is_active() const {\n+  return _cycle_start != 0.0;\n+}\n+\n+bool ZUncommitter::uncommit_cycle_is_canceled() const {\n+  return _cancel_time != 0.0;\n+}\n+\n+size_t ZUncommitter::uncommit() {\n+  precond(uncommit_cycle_is_active());\n+\n+  ZArray<ZVirtualMemory> flushed_vmems;\n+  size_t flushed = 0;\n+\n+  {\n+    \/\/ We need to join the suspendible thread set while manipulating capacity\n+    \/\/ and used, to make sure GC safepoints will have a consistent view.\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZLocker<ZLock> locker(&_partition->_page_allocator->_lock);\n+\n+    if (uncommit_cycle_is_canceled()) {\n+      \/\/ We have committed within the delay, stop uncommitting.\n+      return 0;\n+    }\n+\n+    \/\/ We flush out and uncommit chunks at a time (~0.8% of the max capacity,\n+    \/\/ but at least one granule and at most 256M), in case demand for memory\n+    \/\/ increases while we are uncommitting.\n+    const size_t current_max_capacity = _partition->_current_max_capacity;\n+    const size_t limit_upper_bound = MAX2(ZGranuleSize, align_down(256 * M \/ ZNUMA::count(), ZGranuleSize));\n+    const size_t limit = MIN2(align_up(current_max_capacity >> 7, ZGranuleSize), limit_upper_bound);\n+\n+    ZMappedCache& cache = _partition->_cache;\n+\n+    \/\/ Never uncommit more than the current uncommit watermark,\n+    \/\/ (adjusted by what has already been uncommitted).\n+    const size_t allowed_to_uncommit = MAX2(cache.uncommit_watermark(), _uncommitted) - _uncommitted;\n+    const size_t to_uncommit = MIN2(_to_uncommit, allowed_to_uncommit);\n+\n+    \/\/ Never uncommit below min capacity.\n+    const size_t retain = MAX2(_partition->_used, _partition->_min_capacity);\n+    const size_t release = _partition->_capacity - retain;\n+    const size_t flush = MIN3(release, limit, to_uncommit);\n+\n+    \/\/ Flush memory from the mapped cache for uncommit\n+    flushed = cache.remove_for_uncommit(flush, &flushed_vmems);\n+    if (flushed == 0) {\n+      \/\/ Nothing flushed\n+      cancel_uncommit_cycle();\n+      return 0;\n+    }\n+\n+    \/\/ Record flushed memory as claimed and how much we've flushed for this partition\n+    Atomic::add(&_partition->_claimed, flushed);\n+  }\n+\n+  \/\/ Unmap and uncommit flushed memory\n+  for (const ZVirtualMemory vmem : flushed_vmems) {\n+    _partition->unmap_virtual(vmem);\n+    _partition->uncommit_physical(vmem);\n+    _partition->free_physical(vmem);\n+    _partition->free_virtual(vmem);\n+  }\n+\n+  {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZLocker<ZLock> locker(&_partition->_page_allocator->_lock);\n+\n+    \/\/ Adjust claimed and capacity to reflect the uncommit\n+    Atomic::sub(&_partition->_claimed, flushed);\n+    _partition->decrease_capacity(flushed, false \/* set_max_capacity *\/);\n+    register_uncommit(flushed);\n+  }\n+\n+  return flushed;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.cpp","additions":344,"deletions":17,"binary":false,"changes":361,"status":"modified"},{"patch":"@@ -38,0 +38,6 @@\n+  double                 _cancel_time;\n+  uint64_t               _next_cycle_timeout;\n+  uint64_t               _next_uncommit_timeout;\n+  double                 _cycle_start;\n+  size_t                 _to_uncommit;\n+  size_t                 _uncommitted;\n@@ -42,0 +48,17 @@\n+  uint64_t to_millis(double seconds) const;\n+\n+  void update_next_cycle_timeout(double from_time);\n+  void update_next_cycle_timeout_on_cancel();\n+  void update_next_cycle_timeout_on_finish();\n+\n+  void reset_uncommit_cycle();\n+  void deactivate_uncommit_cycle();\n+  bool activate_uncommit_cycle();\n+  void register_uncommit(size_t size);\n+\n+  bool uncommit_cycle_is_finished() const;\n+  bool uncommit_cycle_is_active() const;\n+  bool uncommit_cycle_is_canceled() const;\n+\n+  size_t uncommit();\n+\n@@ -48,0 +71,2 @@\n+\n+  void cancel_uncommit_cycle();\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n- * @library \/test\/lib\n@@ -35,1 +34,0 @@\n-import jdk.test.lib.Utils;\n@@ -113,1 +111,1 @@\n-        if (actualDelay > delay * 2 * Utils.TIMEOUT_FACTOR) {\n+        if (actualDelay > delay * 3) {\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestUncommit.java","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"}]}