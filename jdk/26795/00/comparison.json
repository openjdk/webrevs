{"files":[{"patch":"@@ -2429,7 +2429,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    if (op->info() != nullptr) {\n-      add_debug_info_for_null_check_here(op->info());\n-      __ null_check(obj);\n-    }\n-    __ b(*op->stub()->entry());\n-  } else if (op->code() == lir_lock) {\n+  if (op->code() == lir_lock) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -180,1 +180,0 @@\n-  Label done, fast_lock, fast_lock_done;\n@@ -187,2 +186,1 @@\n-  const ByteSize obj_offset = BasicObjectLock::obj_offset();\n-  const int mark_offset = BasicLock::displaced_header_offset_in_bytes();\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"Required by atomic instructions\");\n@@ -191,1 +189,1 @@\n-  str(obj, Address(disp_hdr, obj_offset));\n+  str(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n@@ -202,47 +200,3 @@\n-  assert(oopDesc::mark_offset_in_bytes() == 0, \"Required by atomic instructions\");\n-\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-\n-    Register t1 = disp_hdr; \/\/ Needs saving, probably\n-    Register t2 = hdr;      \/\/ blow\n-    Register t3 = Rtemp;    \/\/ blow\n-\n-    lightweight_lock(obj \/* obj *\/, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n-    \/\/ Success: fall through\n-\n-  } else if (LockingMode == LM_LEGACY) {\n-\n-    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n-\n-    \/\/ Must be the first instruction here, because implicit null check relies on it\n-    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n-\n-    tst(hdr, markWord::unlocked_value);\n-    b(fast_lock, ne);\n-\n-    \/\/ Check for recursive locking\n-    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-    \/\/ explanations on the fast recursive locking check.\n-    \/\/ -1- test low 2 bits\n-    movs(tmp2, AsmOperand(hdr, lsl, 30));\n-    \/\/ -2- test (hdr - SP) if the low two bits are 0\n-    sub(tmp2, hdr, SP, eq);\n-    movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK\n-    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-    str(tmp2, Address(disp_hdr, mark_offset));\n-    b(fast_lock_done, eq);\n-    \/\/ else need slow case\n-    b(slow_case);\n-\n-\n-    bind(fast_lock);\n-    \/\/ Save previous object header in BasicLock structure and update the header\n-    str(hdr, Address(disp_hdr, mark_offset));\n-\n-    cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n-\n-    bind(fast_lock_done);\n-  }\n-  bind(done);\n+  Register t1 = disp_hdr; \/\/ Needs saving, probably\n+  Register t2 = hdr;      \/\/ blow\n+  Register t3 = Rtemp;    \/\/ blow\n@@ -250,0 +204,2 @@\n+  lightweight_lock(obj, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n+  \/\/ Success: fall through\n@@ -255,1 +211,0 @@\n-  Register tmp2 = Rtemp;\n@@ -258,5 +213,0 @@\n-  const ByteSize obj_offset = BasicObjectLock::obj_offset();\n-  const int mark_offset = BasicLock::displaced_header_offset_in_bytes();\n-\n-  Label done;\n-\n@@ -265,1 +215,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n+  ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n@@ -267,1 +217,3 @@\n-    ldr(obj, Address(disp_hdr, obj_offset));\n+  Register t1 = disp_hdr; \/\/ Needs saving, probably\n+  Register t2 = hdr;      \/\/ blow\n+  Register t3 = Rtemp;    \/\/ blow\n@@ -269,22 +221,2 @@\n-    Register t1 = disp_hdr; \/\/ Needs saving, probably\n-    Register t2 = hdr;      \/\/ blow\n-    Register t3 = Rtemp;    \/\/ blow\n-\n-    lightweight_unlock(obj \/* object *\/, t1, t2, t3, 1 \/* savemask (save t1) *\/,\n-                       slow_case);\n-    \/\/ Success: Fall through\n-\n-  } else if (LockingMode == LM_LEGACY) {\n-\n-    \/\/ Load displaced header and object from the lock\n-    ldr(hdr, Address(disp_hdr, mark_offset));\n-    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-    cbz(hdr, done);\n-\n-    \/\/ load object\n-    ldr(obj, Address(disp_hdr, obj_offset));\n-\n-    \/\/ Restore the object header\n-    cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n-  }\n-  bind(done);\n+  lightweight_unlock(obj, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n+  \/\/ Success: fall through\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":13,"deletions":81,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-  Label fast_lock, done;\n+  Label done;\n@@ -93,36 +93,2 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-\n-    lightweight_lock(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n-                     1 \/* savemask (save t1) *\/, done);\n-\n-    \/\/ Success: set Z\n-    cmp(Roop, Roop);\n-\n-  } else if (LockingMode == LM_LEGACY) {\n-\n-    Register Rmark      = Rscratch2;\n-\n-    ldr(Rmark, Address(Roop, oopDesc::mark_offset_in_bytes()));\n-    tst(Rmark, markWord::unlocked_value);\n-    b(fast_lock, ne);\n-\n-    \/\/ Check for recursive lock\n-    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-    \/\/ explanations on the fast recursive locking check.\n-    \/\/ -1- test low 2 bits\n-    movs(Rscratch, AsmOperand(Rmark, lsl, 30));\n-    \/\/ -2- test (hdr - SP) if the low two bits are 0\n-    sub(Rscratch, Rmark, SP, eq);\n-    movs(Rscratch, AsmOperand(Rscratch, lsr, exact_log2(os::vm_page_size())), eq);\n-    \/\/ If still 'eq' then recursive locking OK\n-    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8153107)\n-    str(Rscratch, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-    b(done);\n-\n-    bind(fast_lock);\n-    str(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-\n-    bool allow_fallthrough_on_failure = true;\n-    bool one_shot = true;\n-    cas_for_lock_acquire(Rmark, Rbox, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n-  }\n+  lightweight_lock(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                   1 \/* savemask (save t1) *\/, done);\n@@ -130,0 +96,1 @@\n+  cmp(Roop, Roop); \/\/ Success: set Z\n@@ -143,11 +110,2 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-\n-    lightweight_unlock(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n-                       1 \/* savemask (save t1) *\/, done);\n-\n-    cmp(Roop, Roop); \/\/ Success: Set Z\n-    \/\/ Fall through\n-\n-  } else if (LockingMode == LM_LEGACY) {\n-\n-    Register Rmark      = Rscratch2;\n+  lightweight_unlock(Roop \/* obj *\/, Rbox \/* t1 *\/, Rscratch \/* t2 *\/, Rscratch2 \/* t3 *\/,\n+                     1 \/* savemask (save t1) *\/, done);\n@@ -155,5 +113,2 @@\n-    \/\/ Find the lock address and load the displaced header from the stack.\n-    ldr(Rmark, Address(Rbox, BasicLock::displaced_header_offset_in_bytes()));\n-    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-    cmp(Rmark, 0);\n-    b(done, eq);\n+  cmp(Roop, Roop); \/\/ Success: Set Z\n+  \/\/ Fall through\n@@ -161,5 +116,0 @@\n-    \/\/ Restore the object header\n-    bool allow_fallthrough_on_failure = true;\n-    bool one_shot = true;\n-    cas_for_lock_release(Rbox, Rmark, Roop, Rscratch, done, allow_fallthrough_on_failure, one_shot);\n-  }\n","filename":"src\/hotspot\/cpu\/arm\/c2_MacroAssembler_arm.cpp","additions":8,"deletions":58,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -891,12 +891,3 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-  } else {\n-    Label done;\n-\n-    const Register Robj = R2;\n-    const Register Rmark = R3;\n-    assert_different_registers(Robj, Rmark, Rlock, R0, Rtemp);\n-\n-    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n-    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n-    const int mark_offset = lock_offset + BasicLock::displaced_header_offset_in_bytes();\n+  const Register Robj = R2;\n+  const Register Rmark = R3;\n+  assert_different_registers(Robj, Rmark, Rlock, R0, Rtemp);\n@@ -904,1 +895,1 @@\n-    Label already_locked, slow_case;\n+  Label done, slow_case;\n@@ -906,2 +897,2 @@\n-    \/\/ Load object pointer\n-    ldr(Robj, Address(Rlock, obj_offset));\n+  \/\/ Load object pointer\n+  ldr(Robj, Address(Rlock, BasicObjectLock::obj_offset()));\n@@ -909,6 +900,6 @@\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      load_klass(R0, Robj);\n-      ldrb(R0, Address(R0, Klass::misc_flags_offset()));\n-      tst(R0, KlassFlags::_misc_is_value_based_class);\n-      b(slow_case, ne);\n-    }\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(R0, Robj);\n+    ldrb(R0, Address(R0, Klass::misc_flags_offset()));\n+    tst(R0, KlassFlags::_misc_is_value_based_class);\n+    b(slow_case, ne);\n+  }\n@@ -916,67 +907,2 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_lock(Robj, R0 \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/, 0 \/* savemask *\/, slow_case);\n-      b(done);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-      \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n-      \/\/ Exception to that is if the object is locked by the calling thread, then the recursive test will pass (guaranteed as\n-      \/\/ loads are satisfied from a store queue if performed on the same processor).\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"must be\");\n-      ldr(Rmark, Address(Robj, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Test if object is already locked\n-      tst(Rmark, markWord::unlocked_value);\n-      b(already_locked, eq);\n-\n-      \/\/ Save old object->mark() into BasicLock's displaced header\n-      str(Rmark, Address(Rlock, mark_offset));\n-\n-      cas_for_lock_acquire(Rmark, Rlock, Robj, Rtemp, slow_case);\n-\n-      b(done);\n-\n-      \/\/ If we got here that means the object is locked by ether calling thread or another thread.\n-      bind(already_locked);\n-      \/\/ Handling of locked objects: recursive locks and slow case.\n-\n-      \/\/ Fast check for recursive lock.\n-      \/\/\n-      \/\/ Can apply the optimization only if this is a stack lock\n-      \/\/ allocated in this thread. For efficiency, we can focus on\n-      \/\/ recently allocated stack locks (instead of reading the stack\n-      \/\/ base and checking whether 'mark' points inside the current\n-      \/\/ thread stack):\n-      \/\/  1) (mark & 3) == 0\n-      \/\/  2) SP <= mark < SP + os::pagesize()\n-      \/\/\n-      \/\/ Warning: SP + os::pagesize can overflow the stack base. We must\n-      \/\/ neither apply the optimization for an inflated lock allocated\n-      \/\/ just above the thread stack (this is why condition 1 matters)\n-      \/\/ nor apply the optimization if the stack lock is inside the stack\n-      \/\/ of another thread. The latter is avoided even in case of overflow\n-      \/\/ because we have guard pages at the end of all stacks. Hence, if\n-      \/\/ we go over the stack base and hit the stack of another thread,\n-      \/\/ this should not be in a writeable area that could contain a\n-      \/\/ stack lock allocated by that thread. As a consequence, a stack\n-      \/\/ lock less than page size away from SP is guaranteed to be\n-      \/\/ owned by the current thread.\n-      \/\/\n-      \/\/ Note: assuming SP is aligned, we can check the low bits of\n-      \/\/ (mark-SP) instead of the low bits of mark. In that case,\n-      \/\/ assuming page size is a power of 2, we can merge the two\n-      \/\/ conditions into a single test:\n-      \/\/ => ((mark - SP) & (3 - os::pagesize())) == 0\n-\n-      \/\/ (3 - os::pagesize()) cannot be encoded as an ARM immediate operand.\n-      \/\/ Check independently the low bits and the distance to SP.\n-      \/\/ -1- test low 2 bits\n-      movs(R0, AsmOperand(Rmark, lsl, 30));\n-      \/\/ -2- test (mark - SP) if the low two bits are 0\n-      sub(R0, Rmark, SP, eq);\n-      movs(R0, AsmOperand(R0, lsr, exact_log2(os::vm_page_size())), eq);\n-      \/\/ If still 'eq' then recursive locking OK: store 0 into lock record\n-      str(R0, Address(Rlock, mark_offset), eq);\n-\n-      b(done, eq);\n-    }\n+  lightweight_lock(Robj, R0 \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/, 0 \/* savemask *\/, slow_case);\n+  b(done);\n@@ -984,1 +910,1 @@\n-    bind(slow_case);\n+  bind(slow_case);\n@@ -986,4 +912,3 @@\n-    \/\/ Call the runtime routine for slow case\n-    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-    bind(done);\n-  }\n+  \/\/ Call the runtime routine for slow case\n+  call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n+  bind(done);\n@@ -1000,12 +925,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), Rlock);\n-  } else {\n-    Label done, slow_case;\n-\n-    const Register Robj = R2;\n-    const Register Rmark = R3;\n-    assert_different_registers(Robj, Rmark, Rlock, Rtemp);\n-\n-    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n-    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n-    const int mark_offset = lock_offset + BasicLock::displaced_header_offset_in_bytes();\n+  Label done, slow_case;\n@@ -1013,27 +927,3 @@\n-    const Register Rzero = zero_register(Rtemp);\n-\n-    \/\/ Load oop into Robj\n-    ldr(Robj, Address(Rlock, obj_offset));\n-\n-    \/\/ Free entry\n-    str(Rzero, Address(Rlock, obj_offset));\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-\n-      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n-      \/\/ must handle it.\n-      ldr(Rtemp, Address(Rthread, JavaThread::lock_stack_top_offset()));\n-      sub(Rtemp, Rtemp, oopSize);\n-      ldr(Rtemp, Address(Rthread, Rtemp));\n-      cmpoop(Rtemp, Robj);\n-      b(slow_case, ne);\n-\n-      lightweight_unlock(Robj \/* obj *\/, Rlock \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/,\n-                         1 \/* savemask (save t1) *\/, slow_case);\n-\n-      b(done);\n-\n-    } else if (LockingMode == LM_LEGACY) {\n-\n-      \/\/ Load the old header from BasicLock structure\n-      ldr(Rmark, Address(Rlock, mark_offset));\n+  const Register Robj = R2;\n+  const Register Rmark = R3;\n+  assert_different_registers(Robj, Rmark, Rlock, Rtemp);\n@@ -1041,2 +931,2 @@\n-      \/\/ Test for recursion (zero mark in BasicLock)\n-      cbz(Rmark, done);\n+  const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n+  const Register Rzero = zero_register(Rtemp);\n@@ -1044,1 +934,2 @@\n-      bool allow_fallthrough_on_failure = true;\n+  \/\/ Load oop into Robj\n+  ldr(Robj, Address(Rlock, obj_offset));\n@@ -1046,1 +937,2 @@\n-      cas_for_lock_release(Rlock, Rmark, Robj, Rtemp, slow_case, allow_fallthrough_on_failure);\n+  \/\/ Free entry\n+  str(Rzero, Address(Rlock, obj_offset));\n@@ -1048,1 +940,7 @@\n-      b(done, eq);\n+  \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+  \/\/ must handle it.\n+  ldr(Rtemp, Address(Rthread, JavaThread::lock_stack_top_offset()));\n+  sub(Rtemp, Rtemp, oopSize);\n+  ldr(Rtemp, Address(Rthread, Rtemp));\n+  cmpoop(Rtemp, Robj);\n+  b(slow_case, ne);\n@@ -1050,2 +948,3 @@\n-    }\n-    bind(slow_case);\n+  lightweight_unlock(Robj \/* obj *\/, Rlock \/* t1 *\/, Rmark \/* t2 *\/, Rtemp \/* t3 *\/,\n+                     1 \/* savemask (save t1) *\/, slow_case);\n+  b(done);\n@@ -1053,3 +952,4 @@\n-    \/\/ Call the runtime routine for slow case.\n-    str(Robj, Address(Rlock, obj_offset)); \/\/ restore obj\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), Rlock);\n+  bind(slow_case);\n+  \/\/ Call the runtime routine for slow case.\n+  str(Robj, Address(Rlock, obj_offset)); \/\/ restore obj\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), Rlock);\n@@ -1057,2 +957,1 @@\n-    bind(done);\n-  }\n+  bind(done);\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":43,"deletions":144,"binary":false,"changes":187,"status":"modified"},{"patch":"@@ -1761,1 +1761,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -1819,1 +1818,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n","filename":"src\/hotspot\/cpu\/arm\/macroAssembler_arm.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1142,4 +1142,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      log_trace(fastlock)(\"SharedRuntime lock fast\");\n-      __ lightweight_lock(sync_obj \/* object *\/, disp_hdr \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n-                          0x7 \/* savemask *\/, slow_lock);\n+    log_trace(fastlock)(\"SharedRuntime lock fast\");\n+    __ lightweight_lock(sync_obj \/* object *\/, disp_hdr \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n+                        0x7 \/* savemask *\/, slow_lock);\n@@ -1147,30 +1146,0 @@\n-    } else if (LockingMode == LM_LEGACY) {\n-      const Register mark = tmp;\n-      \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-      \/\/ That would be acceptable as either CAS or slow case path is taken in that case\n-\n-      __ ldr(mark, Address(sync_obj, oopDesc::mark_offset_in_bytes()));\n-      __ sub(disp_hdr, FP, lock_slot_fp_offset);\n-      __ tst(mark, markWord::unlocked_value);\n-      __ b(fast_lock, ne);\n-\n-      \/\/ Check for recursive lock\n-      \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-      \/\/ explanations on the fast recursive locking check.\n-      \/\/ Check independently the low bits and the distance to SP\n-      \/\/ -1- test low 2 bits\n-      __ movs(Rtemp, AsmOperand(mark, lsl, 30));\n-      \/\/ -2- test (hdr - SP) if the low two bits are 0\n-      __ sub(Rtemp, mark, SP, eq);\n-      __ movs(Rtemp, AsmOperand(Rtemp, lsr, exact_log2(os::vm_page_size())), eq);\n-      \/\/ If still 'eq' then recursive locking OK\n-      \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-      __ str(Rtemp, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-      __ b(lock_done, eq);\n-      __ b(slow_lock);\n-\n-      __ bind(fast_lock);\n-      __ str(mark, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      __ cas_for_lock_acquire(mark, disp_hdr, sync_obj, Rtemp, slow_lock);\n-    }\n@@ -1229,15 +1198,5 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      log_trace(fastlock)(\"SharedRuntime unlock fast\");\n-      __ lightweight_unlock(sync_obj, R2 \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n-                            7 \/* savemask *\/, slow_unlock);\n-      \/\/ Fall through\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n-      __ ldr(sync_obj, Address(sync_handle));\n-\n-      \/\/ See C1_MacroAssembler::unlock_object() for more comments\n-      __ ldr(R2, Address(disp_hdr, BasicLock::displaced_header_offset_in_bytes()));\n-      __ cbz(R2, unlock_done);\n-\n-      __ cas_for_lock_release(disp_hdr, R2, sync_obj, Rtemp, slow_unlock);\n-    }\n+    log_trace(fastlock)(\"SharedRuntime unlock fast\");\n+    __ lightweight_unlock(sync_obj, R2 \/* t1 *\/, tmp \/* t2 *\/, Rtemp \/* t3 *\/,\n+                          7 \/* savemask *\/, slow_unlock);\n+    \/\/ Fall through\n+\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":8,"deletions":49,"binary":false,"changes":57,"status":"modified"}]}