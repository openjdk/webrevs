{"files":[{"patch":"@@ -131,1 +131,2 @@\n-      systemDictionaryShared.cpp\n+      systemDictionaryShared.cpp \\\n+      trainingData.cpp\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -167,0 +168,2 @@\n+\n+  TrainingData::cleanup_training_data();\n","filename":"src\/hotspot\/share\/cds\/aotArtifactFinder.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"compiler\/compilationPolicy.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -51,0 +53,11 @@\n+bool AOTLinkedClassBulkLoader::class_preloading_finished() {\n+  if (!CDSConfig::is_using_aot_linked_classes()) {\n+    return true;\n+  } else {\n+    \/\/ The ConstantPools of preloaded classes have references to other preloaded classes. We don't\n+    \/\/ want any Java code (including JVMCI compiler) to use these classes until all of them\n+    \/\/ are loaded.\n+    return Atomic::load_acquire(&_all_completed);\n+  }\n+}\n+\n@@ -73,0 +86,6 @@\n+\n+  if (AOTPrintTrainingInfo) {\n+    tty->print_cr(\"==================== archived_training_data ** after all classes preloaded ====================\");\n+    TrainingData::print_archived_training_data_on(tty);\n+  }\n+\n@@ -74,1 +93,1 @@\n-  _all_completed = true;\n+  Atomic::release_store(&_all_completed, true);\n@@ -397,0 +416,22 @@\n+\n+void AOTLinkedClassBulkLoader::replay_training_at_init(Array<InstanceKlass*>* classes, TRAPS) {\n+  if (classes != nullptr) {\n+    for (int i = 0; i < classes->length(); i++) {\n+      InstanceKlass* ik = classes->at(i);\n+      if (ik->has_aot_initialized_mirror() && ik->is_initialized() && !ik->has_init_deps_processed()) {\n+        CompilationPolicy::replay_training_at_init(ik, CHECK);\n+      }\n+    }\n+  }\n+}\n+\n+void AOTLinkedClassBulkLoader::replay_training_at_init_for_preloaded_classes(TRAPS) {\n+  if (CDSConfig::is_using_aot_linked_classes() && TrainingData::have_data()) {\n+    \/\/ Only static archive can have training data.\n+    AOTLinkedClassTable* table = AOTLinkedClassTable::for_static_archive();\n+    replay_training_at_init(table->boot(),     CHECK);\n+    replay_training_at_init(table->boot2(),    CHECK);\n+    replay_training_at_init(table->platform(), CHECK);\n+    replay_training_at_init(table->app(),      CHECK);\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/cds\/aotLinkedClassBulkLoader.cpp","additions":42,"deletions":1,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,0 +58,1 @@\n+  static void replay_training_at_init(Array<InstanceKlass*>* classes, TRAPS) NOT_CDS_RETURN;\n@@ -66,0 +67,2 @@\n+  static void replay_training_at_init_for_preloaded_classes(TRAPS) NOT_CDS_RETURN;\n+  static bool class_preloading_finished();\n","filename":"src\/hotspot\/share\/cds\/aotLinkedClassBulkLoader.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+#include \"oops\/methodCounters.hpp\"\n+#include \"oops\/methodData.hpp\"\n@@ -59,0 +61,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -133,1 +136,5 @@\n-    address old_p = *ptr_loc;\n+    address old_p_with_tags = *ptr_loc;\n+    assert(old_p_with_tags != nullptr, \"null ptrs shouldn't have been marked\");\n+\n+    address old_p = MetaspaceClosure::strip_tags(old_p_with_tags);\n+    uintx tags = MetaspaceClosure::decode_tags(old_p_with_tags);\n@@ -136,2 +143,11 @@\n-    log_trace(cds)(\"Ref: [\" PTR_FORMAT \"] -> \" PTR_FORMAT \" => \" PTR_FORMAT,\n-                   p2i(ptr_loc), p2i(old_p), p2i(new_p));\n+    bool nulled;\n+    if (new_p == nullptr) {\n+      \/\/ old_p had a FollowMode of set_to_null\n+      nulled = true;\n+    } else {\n+      new_p = MetaspaceClosure::add_tags(new_p, tags);\n+      nulled = false;\n+    }\n+\n+    log_trace(cds)(\"Ref: [\" PTR_FORMAT \"] -> \" PTR_FORMAT \" => \" PTR_FORMAT \" %zu\",\n+                   p2i(ptr_loc), p2i(old_p) + tags, p2i(new_p), tags);\n@@ -140,0 +156,1 @@\n+    ArchiveBuilder::current()->count_relocated_pointer(tags != 0, nulled);\n@@ -180,0 +197,3 @@\n+  _relocated_ptr_info._num_ptrs = 0;\n+  _relocated_ptr_info._num_tagged_ptrs = 0;\n+  _relocated_ptr_info._num_nulled_ptrs = 0;\n@@ -439,0 +459,5 @@\n+  if (ref->msotype() == MetaspaceObj::MethodDataType) {\n+    MethodData* md = (MethodData*)ref->obj();\n+    md->clean_method_data(false \/* always_clean *\/);\n+  }\n+\n@@ -536,2 +561,5 @@\n-             ref->msotype() == MetaspaceObj::MethodCountersType) {\n-    return set_to_null;\n+             ref->msotype() == MetaspaceObj::MethodCountersType ||\n+             ref->msotype() == MetaspaceObj::KlassTrainingDataType ||\n+             ref->msotype() == MetaspaceObj::MethodTrainingDataType ||\n+             ref->msotype() == MetaspaceObj::CompileTrainingDataType) {\n+    return (TrainingData::need_data() || TrainingData::assembling_data()) ? make_a_copy : set_to_null;\n@@ -758,0 +786,4 @@\n+  log_info(cds)(\"Relocating %zu pointers, %zu tagged, %zu nulled\",\n+                _relocated_ptr_info._num_ptrs,\n+                _relocated_ptr_info._num_tagged_ptrs,\n+                _relocated_ptr_info._num_nulled_ptrs);\n@@ -959,0 +991,22 @@\n+void ArchiveBuilder::make_training_data_shareable() {\n+  auto clean_td = [&] (address& src_obj,  SourceObjInfo& info) {\n+    if (!is_in_buffer_space(info.buffered_addr())) {\n+      return;\n+    }\n+\n+    if (info.msotype() == MetaspaceObj::KlassTrainingDataType ||\n+        info.msotype() == MetaspaceObj::MethodTrainingDataType ||\n+        info.msotype() == MetaspaceObj::CompileTrainingDataType) {\n+      TrainingData* buffered_td = (TrainingData*)info.buffered_addr();\n+      buffered_td->remove_unshareable_info();\n+    } else if (info.msotype() == MetaspaceObj::MethodDataType) {\n+      MethodData* buffered_mdo = (MethodData*)info.buffered_addr();\n+      buffered_mdo->remove_unshareable_info();\n+    } else if (info.msotype() == MetaspaceObj::MethodCountersType) {\n+      MethodCounters* buffered_mc = (MethodCounters*)info.buffered_addr();\n+      buffered_mc->remove_unshareable_info();\n+    }\n+  };\n+  _src_obj_table.iterate_all(clean_td);\n+}\n+\n@@ -1594,0 +1648,6 @@\n+void ArchiveBuilder::count_relocated_pointer(bool tagged, bool nulled) {\n+  _relocated_ptr_info._num_ptrs ++;\n+  _relocated_ptr_info._num_tagged_ptrs += tagged ? 1 : 0;\n+  _relocated_ptr_info._num_nulled_ptrs += nulled ? 1 : 0;\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":65,"deletions":5,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -241,0 +241,5 @@\n+  struct {\n+    size_t _num_ptrs;\n+    size_t _num_tagged_ptrs;\n+    size_t _num_nulled_ptrs;\n+  } _relocated_ptr_info;\n@@ -261,0 +266,2 @@\n+  void count_relocated_pointer(bool tagged, bool nulled);\n+\n@@ -422,0 +429,1 @@\n+  void make_training_data_shareable();\n@@ -446,1 +454,2 @@\n-    return (T)get_buffered_addr((address)src_addr);\n+    CDS_ONLY(return (T)get_buffered_addr((address)src_addr);)\n+    NOT_CDS(return nullptr;)\n@@ -459,1 +468,2 @@\n-    return (_current != nullptr);\n+    CDS_ONLY(return (_current != nullptr));\n+    NOT_CDS(return false;)\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -515,0 +515,2 @@\n+  setup_compiler_args();\n+\n@@ -587,0 +589,22 @@\n+void CDSConfig::setup_compiler_args() {\n+  \/\/ AOT profiles are supported only in the JEP 483 workflow.\n+  bool can_dump_profiles = AOTClassLinking && new_aot_flags_used();\n+\n+  if (is_dumping_preimage_static_archive() && can_dump_profiles) {\n+    \/\/ JEP 483 workflow -- training\n+    FLAG_SET_ERGO_IF_DEFAULT(AOTRecordTraining, true);\n+    FLAG_SET_ERGO(AOTReplayTraining, false);\n+  } else if (is_dumping_final_static_archive() && can_dump_profiles) {\n+    \/\/ JEP 483 workflow -- assembly\n+    FLAG_SET_ERGO(AOTRecordTraining, false);\n+    FLAG_SET_ERGO_IF_DEFAULT(AOTReplayTraining, true);\n+  } else if (is_using_archive() && new_aot_flags_used()) {\n+    \/\/ JEP 483 workflow -- production\n+    FLAG_SET_ERGO(AOTRecordTraining, false);\n+    FLAG_SET_ERGO_IF_DEFAULT(AOTReplayTraining, true);\n+  } else {\n+    FLAG_SET_ERGO(AOTReplayTraining, false);\n+    FLAG_SET_ERGO(AOTRecordTraining, false);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+  static void setup_compiler_args();\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -132,0 +132,17 @@\n+  \/* flags to control training and deployment modes  *\/                     \\\n+                                                                            \\\n+  product(bool, AOTRecordTraining, false, DIAGNOSTIC,                       \\\n+          \"Request output of training data for improved deployment.\")       \\\n+                                                                            \\\n+  product(bool, AOTReplayTraining, false, DIAGNOSTIC,                       \\\n+          \"Read training data, if available, for use in this execution\")    \\\n+                                                                            \\\n+  product(bool, AOTPrintTrainingInfo, false, DIAGNOSTIC,                    \\\n+          \"Print additional information about training\")                    \\\n+                                                                            \\\n+  product(bool, AOTVerifyTrainingData, trueInDebug, DIAGNOSTIC,             \\\n+          \"Verify archived training data\")                                  \\\n+                                                                            \\\n+  product(bool, AOTCompileEagerly, false, DIAGNOSTIC,                       \\\n+          \"Compile methods as soon as possible\")                            \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/cds\/cds_globals.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/methodCounters.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -63,0 +65,2 @@\n+  f(MethodData) \\\n+  f(MethodCounters) \\\n@@ -64,1 +68,4 @@\n-  f(TypeArrayKlass)\n+  f(TypeArrayKlass) \\\n+  f(KlassTrainingData) \\\n+  f(MethodTrainingData) \\\n+  f(CompileTrainingData)\n@@ -282,1 +289,0 @@\n-  case MetaspaceObj::MethodCountersType:\n@@ -288,4 +294,0 @@\n-  case MetaspaceObj::MethodDataType:\n-    \/\/ We don't archive MethodData <-- should have been removed in removed_unsharable_info\n-    ShouldNotReachHere();\n-    break;\n","filename":"src\/hotspot\/share\/cds\/cppVtables.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -121,2 +122,9 @@\n-  msg.info(\"Platform loader initiated classes = %5d\", AOTClassLinker::num_platform_initiated_classes());\n-  msg.info(\"App      loader initiated classes = %5d\", AOTClassLinker::num_app_initiated_classes());\n+  msg.info(\"Platform loader initiated classes = %6d\", AOTClassLinker::num_platform_initiated_classes());\n+  msg.info(\"App      loader initiated classes = %6d\", AOTClassLinker::num_app_initiated_classes());\n+  msg.info(\"MethodCounters                    = %6d (%8d bytes)\", _counts[RW][MethodCountersType],\n+                                                                  _bytes [RW][MethodCountersType]);\n+  msg.info(\"KlassTrainingData                 = %6d (%8d bytes)\", _counts[RW][KlassTrainingDataType],\n+                                                                  _bytes [RW][KlassTrainingDataType]);\n+  msg.info(\"MethodTrainingData                = %6d (%8d bytes)\", _counts[RW][MethodTrainingDataType],\n+                                                                  _bytes [RW][MethodTrainingDataType]);\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -233,0 +234,5 @@\n+  _type_profile_level = TypeProfileLevel;\n+  _type_profile_width = TypeProfileWidth;\n+  _bci_profile_width = BciProfileWidth;\n+  _profile_traps = ProfileTraps;\n+  _spec_trap_limit_extra_entries = SpecTrapLimitExtraEntries;\n@@ -1916,0 +1922,36 @@\n+  if (TrainingData::have_data()) {\n+    if (_type_profile_level != TypeProfileLevel) {\n+      log_info(cds)(\"The %s's TypeProfileLevel setting (%d)\"\n+                    \" does not equal the current TypeProfileLevel setting (%d).\", file_type,\n+                    _type_profile_level, TypeProfileLevel);\n+      return false;\n+    }\n+    if (_type_profile_width != TypeProfileWidth) {\n+      log_info(cds)(\"The %s's TypeProfileWidth setting (%d)\"\n+                    \" does not equal the current TypeProfileWidth setting (%d).\", file_type,\n+                    (int)_type_profile_width, (int)TypeProfileWidth);\n+      return false;\n+\n+    }\n+    if (_bci_profile_width != BciProfileWidth) {\n+      log_info(cds)(\"The %s's BciProfileWidth setting (%d)\"\n+                    \" does not equal the current BciProfileWidth setting (%d).\", file_type,\n+                    (int)_bci_profile_width, (int)BciProfileWidth);\n+      return false;\n+    }\n+    if (_profile_traps != ProfileTraps) {\n+      log_info(cds)(\"The %s's ProfileTraps setting (%s)\"\n+                    \" does not equal the current ProfileTraps setting (%s).\", file_type,\n+                    _profile_traps ? \"enabled\" : \"disabled\",\n+                    ProfileTraps   ? \"enabled\" : \"disabled\");\n+\n+      return false;\n+    }\n+    if (_spec_trap_limit_extra_entries != SpecTrapLimitExtraEntries) {\n+      log_info(cds)(\"The %s's SpecTrapLimitExtraEntries setting (%d)\"\n+                    \" does not equal the current SpecTrapLimitExtraEntries setting (%d).\", file_type,\n+                    _spec_trap_limit_extra_entries, SpecTrapLimitExtraEntries);\n+      return false;\n+\n+    }\n+  }\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-  int     _narrow_klass_shift;                    \/\/ save shift width used to pre-compute narrowKlass IDs in archived heap objects\n+  int     _narrow_klass_shift;                    \/\/ save shift width used to pre-compute narrowKlass IDs in archived heap objectsa\n@@ -148,0 +148,8 @@\n+\n+  \/\/ The following are parameters that affect MethodData layout.\n+  uint    _type_profile_level;\n+  intx    _type_profile_width;\n+  intx    _bci_profile_width;\n+  bool    _profile_traps;\n+  int     _spec_trap_limit_extra_entries;\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -83,0 +83,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -485,0 +486,1 @@\n+  TrainingData::serialize(soc);\n@@ -571,0 +573,1 @@\n+    TrainingData::iterate_roots(it);\n@@ -610,0 +613,3 @@\n+\n+  TrainingData::dump_training_data();\n+\n@@ -675,0 +681,3 @@\n+  log_info(cds)(\"Make training data shareable\");\n+  _builder.make_training_data_shareable();\n+\n@@ -793,0 +802,7 @@\n+ HandleMark hm(THREAD);\n+\n+ if (CDSConfig::is_dumping_final_static_archive() && AOTPrintTrainingInfo) {\n+   tty->print_cr(\"==================== archived_training_data ** before dumping ====================\");\n+   TrainingData::print_archived_training_data_on(tty);\n+ }\n+\n@@ -956,0 +972,1 @@\n+  TrainingData::init_dumptime_table(CHECK); \/\/ captures TrainingDataSetLocker\n@@ -1857,0 +1874,2 @@\n+    TrainingData::print_archived_training_data_on(tty);\n+\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -267,0 +267,1 @@\n+#if INCLUDE_CDS\n@@ -268,0 +269,3 @@\n+#else\n+    return false;\n+#endif\n","filename":"src\/hotspot\/share\/cds\/runTimeClassInfo.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1162,0 +1162,7 @@\n+  CompileTrainingData* ctd = task()->training_data();\n+  if (ctd != nullptr) {\n+    GUARDED_VM_ENTRY({\n+      methodHandle mh(Thread::current(), method->get_Method());\n+      ctd->notice_inlined_method(task(), mh);\n+    });\n+  }\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+  friend class CompileTrainingData;\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"compiler\/compileTask.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -1151,0 +1153,22 @@\n+  if (_inline_instructions_size == -1) {\n+    if (TrainingData::have_data()) {\n+      GUARDED_VM_ENTRY(\n+        CompLevel level = static_cast<CompLevel>(CURRENT_ENV->comp_level());\n+        methodHandle top_level_mh(Thread::current(), CURRENT_ENV->task()->method());\n+        MethodTrainingData* mtd = MethodTrainingData::find(top_level_mh);\n+        if (mtd != nullptr) {\n+          CompileTrainingData* ctd = mtd->last_toplevel_compile(level);\n+          if (ctd != nullptr) {\n+            methodHandle mh(Thread::current(), get_Method());\n+            MethodTrainingData* this_mtd = MethodTrainingData::find(mh);\n+            if (this_mtd != nullptr) {\n+              auto r = ctd->ci_records().ciMethod__inline_instructions_size.find(this_mtd);\n+              if (r.is_valid()) {\n+                _inline_instructions_size = r.result();\n+              }\n+            }\n+          }\n+        }\n+      );\n+    }\n+  }\n@@ -1160,0 +1184,8 @@\n+      if (TrainingData::need_data()) {\n+        CompileTrainingData* ctd = CURRENT_ENV->task()->training_data();\n+        if (ctd != nullptr) {\n+          methodHandle mh(Thread::current(), get_Method());\n+          MethodTrainingData* this_mtd = MethodTrainingData::make(mh);\n+          ctd->ci_records().ciMethod__inline_instructions_size.append_if_missing(_inline_instructions_size, this_mtd);\n+        }\n+      }\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -57,0 +58,9 @@\n+\n+static bool is_klass_loaded(Klass* k) {\n+  if (TrainingData::have_data()) {\n+    \/\/ If we're running in AOT mode some classes may not be loaded yet\n+    return !k->is_instance_klass() || InstanceKlass::cast(k)->is_loaded();\n+  }\n+  return true;\n+}\n+\n@@ -71,1 +81,2 @@\n-    if (!m->method_holder()->is_loader_alive()) {\n+    Klass* holder = m->method_holder();\n+    if (holder == nullptr || !holder->is_loader_present_and_alive() || !is_klass_loaded(holder)) {\n@@ -306,1 +317,1 @@\n-    if (k != nullptr) {\n+    if (k != nullptr && k->class_loader_data() != nullptr && is_klass_loaded(k)) {\n@@ -324,1 +335,1 @@\n-    if (klass != nullptr && !klass->is_loader_alive()) {\n+    if (klass == nullptr || !klass->is_loader_present_and_alive() || !is_klass_loaded(klass)) {\n@@ -336,1 +347,1 @@\n-  if (klass != nullptr && !klass->is_loader_alive()) {\n+  if (klass == nullptr || !klass->is_loader_present_and_alive() || !is_klass_loaded(klass)) {\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"compiler\/compileTask.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -111,1 +113,1 @@\n-  Arena* arena = new (mtCompiler) Arena(mtCompiler, Arena::Tag::tag_cienv);\n+  Arena* arena = new (mtCompiler) Arena(mtCompiler);\n@@ -235,1 +237,2 @@\n-  assert(Universe::heap()->is_in(key), \"must be\");\n+  Handle keyHandle(Thread::current(), key);\n+  assert(Universe::heap()->is_in(keyHandle()), \"must be\");\n@@ -237,1 +240,1 @@\n-  NonPermObject* &bucket = find_non_perm(key);\n+  NonPermObject* &bucket = find_non_perm(keyHandle);\n@@ -244,1 +247,0 @@\n-  Handle keyHandle(Thread::current(), key);\n@@ -251,1 +253,2 @@\n-  insert_non_perm(bucket, keyHandle(), new_object);\n+  insert_non_perm(bucket, keyHandle, new_object);\n+  notice_new_object(new_object);\n@@ -255,0 +258,13 @@\n+void ciObjectFactory::notice_new_object(ciBaseObject* new_object) {\n+  if (TrainingData::need_data()) {\n+    ciEnv* env = ciEnv::current();\n+    if (env->task() != nullptr) {\n+      \/\/ Note: task will be null during init_compiler_runtime.\n+      CompileTrainingData* td = env->task()->training_data();\n+      if (td != nullptr) {\n+        td->notice_jit_observation(env, new_object);\n+      }\n+    }\n+  }\n+}\n+\n@@ -334,0 +350,1 @@\n+    notice_new_object(new_object);\n@@ -639,3 +656,3 @@\n-ciObjectFactory::NonPermObject* &ciObjectFactory::find_non_perm(oop key) {\n-  assert(Universe::heap()->is_in(key), \"must be\");\n-  ciMetadata* klass = get_metadata(key->klass());\n+ciObjectFactory::NonPermObject* &ciObjectFactory::find_non_perm(Handle keyHandle) {\n+  assert(Universe::heap()->is_in(keyHandle()), \"must be\");\n+  ciMetadata* klass = get_metadata(keyHandle->klass()); \/\/ This may safepoint!\n@@ -644,1 +661,1 @@\n-    if (is_equal(p, key))  break;\n+    if (is_equal(p, keyHandle()))  break;\n@@ -667,2 +684,2 @@\n-void ciObjectFactory::insert_non_perm(ciObjectFactory::NonPermObject* &where, oop key, ciObject* obj) {\n-  assert(Universe::heap()->is_in_or_null(key), \"must be\");\n+void ciObjectFactory::insert_non_perm(ciObjectFactory::NonPermObject* &where, Handle keyHandle, ciObject* obj) {\n+  assert(Universe::heap()->is_in_or_null(keyHandle()), \"must be\");\n@@ -670,3 +687,3 @@\n-  NonPermObject* p = new (arena()) NonPermObject(where, key, obj);\n-  assert(where == p && is_equal(p, key) && p->object() == obj, \"entry must match\");\n-  assert(find_non_perm(key) == p, \"must find the same spot\");\n+  NonPermObject* p = new (arena()) NonPermObject(where, keyHandle(), obj);\n+  assert(where == p && is_equal(p, keyHandle()) && p->object() == obj, \"entry must match\");\n+  assert(find_non_perm(keyHandle) == p, \"must find the same spot\");\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":31,"deletions":14,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+  friend class VMStructs;\n@@ -80,2 +81,2 @@\n-  NonPermObject* &find_non_perm(oop key);\n-  void insert_non_perm(NonPermObject* &where, oop key, ciObject* obj);\n+  NonPermObject* &find_non_perm(Handle keyHandle);\n+  void insert_non_perm(NonPermObject* &where, Handle keyHandle, ciObject* obj);\n@@ -109,0 +110,3 @@\n+  \/\/ Called on every new object made.\n+  void notice_new_object(ciBaseObject* new_object);\n+\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -244,0 +244,1 @@\n+  friend class VMStructs;\n","filename":"src\/hotspot\/share\/classfile\/compactHashtable.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -652,0 +652,5 @@\n+    if (CDSConfig::is_dumping_dynamic_archive() && ik->is_shared()) {\n+      \/\/ ik is already part of the static archive, so it will never be considered as excluded.\n+      return false;\n+    }\n+\n@@ -991,1 +996,1 @@\n-  if (ArchiveBuilder::is_active()) {\n+  if (ArchiveBuilder::is_active() && ArchiveBuilder::current()->is_in_buffer_space(ptr)) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotLinkedClassBulkLoader.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -53,1 +55,1 @@\n-jlong CompilationPolicy::_start_time = 0;\n+int64_t CompilationPolicy::_start_time = 0;\n@@ -58,0 +60,2 @@\n+CompilationPolicy::TrainingReplayQueue CompilationPolicy::_training_replay_queue;\n+\n@@ -81,1 +85,1 @@\n-  return !UseInterpreter ||                                              \/\/ must compile all methods\n+  return !UseInterpreter ||                                                                        \/\/ must compile all methods\n@@ -85,0 +89,26 @@\n+void CompilationPolicy::maybe_compile_early(const methodHandle& m, TRAPS) {\n+  if (m->method_holder()->is_not_initialized()) {\n+    \/\/ 'is_not_initialized' means not only '!is_initialized', but also that\n+    \/\/ initialization has not been started yet ('!being_initialized')\n+    \/\/ Do not force compilation of methods in uninitialized classes.\n+    return;\n+  }\n+  if (!m->is_native() && MethodTrainingData::have_data()) {\n+    MethodTrainingData* mtd = MethodTrainingData::find_fast(m);\n+    if (mtd == nullptr) {\n+      return;              \/\/ there is no training data recorded for m\n+    }\n+    CompLevel cur_level = static_cast<CompLevel>(m->highest_comp_level());\n+    CompLevel next_level = trained_transition(m, cur_level, mtd, THREAD);\n+    if (next_level != cur_level && can_be_compiled(m, next_level) && !CompileBroker::compilation_is_in_queue(m)) {\n+      if (PrintTieredEvents) {\n+        print_event(FORCE_COMPILE, m(), m(), InvocationEntryBci, next_level);\n+      }\n+      CompileBroker::compile_method(m, InvocationEntryBci, next_level, methodHandle(), 0, CompileTask::Reason_MustBeCompiled, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+    }\n+  }\n+}\n+\n@@ -86,0 +116,15 @@\n+  if (!THREAD->can_call_java() || THREAD->is_Compiler_thread()) {\n+    \/\/ don't force compilation, resolve was on behalf of compiler\n+    return;\n+  }\n+  if (m->method_holder()->is_not_initialized()) {\n+    \/\/ 'is_not_initialized' means not only '!is_initialized', but also that\n+    \/\/ initialization has not been started yet ('!being_initialized')\n+    \/\/ Do not force compilation of methods in uninitialized classes.\n+    \/\/ Note that doing this would throw an assert later,\n+    \/\/ in CompileBroker::compile_method.\n+    \/\/ We sometimes use the link resolver to do reflective lookups\n+    \/\/ even before classes are initialized.\n+    return;\n+  }\n+\n@@ -88,15 +133,0 @@\n-\n-    if (!THREAD->can_call_java() || THREAD->is_Compiler_thread()) {\n-      \/\/ don't force compilation, resolve was on behalf of compiler\n-      return;\n-    }\n-    if (m->method_holder()->is_not_initialized()) {\n-      \/\/ 'is_not_initialized' means not only '!is_initialized', but also that\n-      \/\/ initialization has not been started yet ('!being_initialized')\n-      \/\/ Do not force compilation of methods in uninitialized classes.\n-      \/\/ Note that doing this would throw an assert later,\n-      \/\/ in CompileBroker::compile_method.\n-      \/\/ We sometimes use the link resolver to do reflective lookups\n-      \/\/ even before classes are initialized.\n-      return;\n-    }\n@@ -105,1 +135,1 @@\n-      print_event(COMPILE, m(), m(), InvocationEntryBci, level);\n+      print_event(FORCE_COMPILE, m(), m(), InvocationEntryBci, level);\n@@ -111,0 +141,59 @@\n+void CompilationPolicy::replay_training_at_init_impl(InstanceKlass* klass, TRAPS) {\n+  if (!klass->has_init_deps_processed()) {\n+    ResourceMark rm;\n+    log_debug(training)(\"Replay training: %s\", klass->external_name());\n+\n+    KlassTrainingData* ktd = KlassTrainingData::find(klass);\n+    if (ktd != nullptr) {\n+      guarantee(ktd->has_holder(), \"\");\n+      ktd->notice_fully_initialized(); \/\/ sets klass->has_init_deps_processed bit\n+      assert(klass->has_init_deps_processed(), \"\");\n+      if (AOTCompileEagerly) {\n+        ktd->iterate_comp_deps([&](CompileTrainingData* ctd) {\n+          if (ctd->init_deps_left() == 0) {\n+            MethodTrainingData* mtd = ctd->method();\n+            if (mtd->has_holder()) {\n+              const methodHandle mh(THREAD, const_cast<Method*>(mtd->holder()));\n+              CompilationPolicy::maybe_compile_early(mh, THREAD);\n+            }\n+          }\n+        });\n+      }\n+    }\n+  }\n+}\n+\n+void CompilationPolicy::flush_replay_training_at_init(TRAPS) {\n+   MonitorLocker locker(THREAD, TrainingReplayQueue_lock);\n+   while (!_training_replay_queue.is_empty_unlocked()) {\n+     locker.wait(); \/\/ let the replay training thread drain the queue\n+   }\n+}\n+\n+void CompilationPolicy::replay_training_at_init(InstanceKlass* klass, TRAPS) {\n+  assert(klass->is_initialized(), \"\");\n+  if (TrainingData::have_data() && klass->is_shared()) {\n+    _training_replay_queue.push(klass, TrainingReplayQueue_lock, THREAD);\n+  }\n+}\n+\n+\/\/ For TrainingReplayQueue\n+template<>\n+void CompilationPolicyUtils::Queue<InstanceKlass>::print_on(outputStream* st) {\n+  int pos = 0;\n+  for (QueueNode* cur = _head; cur != nullptr; cur = cur->next()) {\n+    ResourceMark rm;\n+    InstanceKlass* ik = cur->value();\n+    st->print_cr(\"%3d: \" INTPTR_FORMAT \" %s\", ++pos, p2i(ik), ik->external_name());\n+  }\n+}\n+\n+void CompilationPolicy::replay_training_at_init_loop(TRAPS) {\n+  while (!CompileBroker::is_compilation_disabled_forever() || AOTVerifyTrainingData) {\n+    InstanceKlass* ik = _training_replay_queue.pop(TrainingReplayQueue_lock, THREAD);\n+    if (ik != nullptr) {\n+      replay_training_at_init_impl(ik, THREAD);\n+    }\n+  }\n+}\n+\n@@ -125,1 +214,1 @@\n-  assert(WhiteBoxAPI || comp_level == CompLevel_any || is_compile(comp_level), \"illegal compilation level\");\n+  assert(WhiteBoxAPI || comp_level == CompLevel_any || is_compile(comp_level), \"illegal compilation level %d\", comp_level);\n@@ -325,1 +414,1 @@\n-void CompilationPolicy::print_counters(const char* prefix, const Method* m) {\n+void CompilationPolicy::print_counters(const char* prefix, Method* m) {\n@@ -345,0 +434,28 @@\n+void CompilationPolicy::print_training_data(const char* prefix, Method* method) {\n+  methodHandle m(Thread::current(), method);\n+  tty->print(\" %smtd: \", prefix);\n+  MethodTrainingData* mtd = MethodTrainingData::find(m);\n+  if (mtd == nullptr) {\n+    tty->print(\"null\");\n+  } else {\n+    MethodData* md = mtd->final_profile();\n+    tty->print(\"mdo=\");\n+    if (md == nullptr) {\n+      tty->print(\"null\");\n+    } else {\n+      int mdo_invocations = md->invocation_count();\n+      int mdo_backedges = md->backedge_count();\n+      int mdo_invocations_start = md->invocation_count_start();\n+      int mdo_backedges_start = md->backedge_count_start();\n+      tty->print(\"%d(%d), %d(%d)\", mdo_invocations, mdo_invocations_start, mdo_backedges, mdo_backedges_start);\n+    }\n+    CompileTrainingData* ctd = mtd->last_toplevel_compile(CompLevel_full_optimization);\n+    tty->print(\", deps=\");\n+    if (ctd == nullptr) {\n+      tty->print(\"null\");\n+    } else {\n+      tty->print(\"%d\", ctd->init_deps_left());\n+    }\n+  }\n+}\n+\n@@ -346,1 +463,1 @@\n-void CompilationPolicy::print_event(EventType type, const Method* m, const Method* im, int bci, CompLevel level) {\n+void CompilationPolicy::print_event(EventType type, Method* m, Method* im, int bci, CompLevel level) {\n@@ -362,0 +479,3 @@\n+  case FORCE_COMPILE:\n+    tty->print(\"force-compile\");\n+    break;\n@@ -427,0 +547,4 @@\n+    print_training_data(\"\", m);\n+    if (inlinee_event) {\n+      print_training_data(\"inlinee \", im);\n+    }\n@@ -620,1 +744,1 @@\n-CompileTask* CompilationPolicy::select_task(CompileQueue* compile_queue) {\n+CompileTask* CompilationPolicy::select_task(CompileQueue* compile_queue, JavaThread* THREAD) {\n@@ -625,1 +749,1 @@\n-  jlong t = nanos_to_millis(os::javaTimeNanos());\n+  int64_t t = nanos_to_millis(os::javaTimeNanos());\n@@ -642,1 +766,1 @@\n-    methodHandle mh(Thread::current(), method);\n+    methodHandle mh(THREAD, method);\n@@ -678,1 +802,1 @@\n-  methodHandle max_method_h(Thread::current(), max_method);\n+  methodHandle max_method_h(THREAD, max_method);\n@@ -697,1 +821,0 @@\n-\n@@ -720,0 +843,7 @@\n+#if INCLUDE_JVMCI\n+  if (EnableJVMCI && UseJVMCICompiler &&\n+      comp_level == CompLevel_full_optimization CDS_ONLY(&& !AOTLinkedClassBulkLoader::class_preloading_finished())) {\n+    return nullptr;\n+  }\n+#endif\n+\n@@ -820,1 +950,1 @@\n-void CompilationPolicy::update_rate(jlong t, const methodHandle& method) {\n+void CompilationPolicy::update_rate(int64_t t, const methodHandle& method) {\n@@ -834,2 +964,2 @@\n-  jlong delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n-  jlong delta_t = t - (method->prev_time() != 0 ? method->prev_time() : start_time()); \/\/ milliseconds since the last measurement\n+  int64_t delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n+  int64_t delta_t = t - (method->prev_time() != 0 ? method->prev_time() : start_time()); \/\/ milliseconds since the last measurement\n@@ -858,3 +988,3 @@\n-bool CompilationPolicy::is_stale(jlong t, jlong timeout, const methodHandle& method) {\n-  jlong delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n-  jlong delta_t = t - method->prev_time();\n+bool CompilationPolicy::is_stale(int64_t t, int64_t timeout, const methodHandle& method) {\n+  int64_t delta_s = t - SafepointTracing::end_of_last_safepoint_ms();\n+  int64_t delta_t = t - method->prev_time();\n@@ -911,1 +1041,1 @@\n-bool CompilationPolicy::is_mature(Method* method) {\n+bool CompilationPolicy::is_mature(MethodData* mdo) {\n@@ -916,2 +1046,1 @@\n-  methodHandle mh(Thread::current(), method);\n-  MethodData* mdo = method->method_data();\n+  methodHandle mh(Thread::current(), mdo->method());\n@@ -934,0 +1063,8 @@\n+\n+  if (TrainingData::have_data()) {\n+    MethodTrainingData* mtd = MethodTrainingData::find_fast(method);\n+    if (mtd != nullptr && mtd->saw_level(CompLevel_full_optimization)) {\n+      return true;\n+    }\n+  }\n+\n@@ -937,0 +1074,1 @@\n+\n@@ -970,1 +1108,1 @@\n-  if (ProfileInterpreter) {\n+  if (ProfileInterpreter && THREAD->has_last_Java_frame()) {\n@@ -983,0 +1121,85 @@\n+CompLevel CompilationPolicy::trained_transition_from_none(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD) {\n+  precond(mtd != nullptr);\n+  precond(cur_level == CompLevel_none);\n+\n+  if (mtd->only_inlined() && !mtd->saw_level(CompLevel_full_optimization)) {\n+    return CompLevel_none;\n+  }\n+\n+  bool training_has_profile = (mtd->final_profile() != nullptr);\n+  if (mtd->saw_level(CompLevel_full_optimization) && !training_has_profile) {\n+    return CompLevel_full_profile;\n+  }\n+\n+  CompLevel highest_training_level = static_cast<CompLevel>(mtd->highest_top_level());\n+  switch (highest_training_level) {\n+    case CompLevel_limited_profile:\n+    case CompLevel_full_profile:\n+      return CompLevel_limited_profile;\n+    case CompLevel_simple:\n+      return CompLevel_simple;\n+    case CompLevel_none:\n+      return CompLevel_none;\n+    default:\n+      break;\n+  }\n+\n+  \/\/ Now handle the case of level 4.\n+  assert(highest_training_level == CompLevel_full_optimization, \"Unexpected compilation level: %d\", highest_training_level);\n+  if (!training_has_profile) {\n+    \/\/ The method was a part of a level 4 compile, but don't have a stored profile,\n+    \/\/ we need to profile it.\n+    return CompLevel_full_profile;\n+  }\n+  const bool deopt = (static_cast<CompLevel>(method->highest_comp_level()) == CompLevel_full_optimization);\n+  \/\/ If we deopted, then we reprofile\n+  if (deopt && !is_method_profiled(method)) {\n+    return CompLevel_full_profile;\n+  }\n+\n+  CompileTrainingData* ctd = mtd->last_toplevel_compile(CompLevel_full_optimization);\n+  assert(ctd != nullptr, \"Should have CTD for CompLevel_full_optimization\");\n+  \/\/ With SkipTier2IfPossible and all deps satisfied, go to level 4 immediately\n+  if (SkipTier2IfPossible && ctd->init_deps_left() == 0) {\n+    if (method->method_data() == nullptr) {\n+      create_mdo(method, THREAD);\n+    }\n+    return CompLevel_full_optimization;\n+  }\n+\n+  \/\/ Otherwise go to level 2\n+  return CompLevel_limited_profile;\n+}\n+\n+\n+CompLevel CompilationPolicy::trained_transition_from_limited_profile(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD) {\n+  precond(mtd != nullptr);\n+  precond(cur_level == CompLevel_limited_profile);\n+\n+  \/\/ One of the main reasons that we can get here is that we're waiting for the stored C2 code to become ready.\n+\n+  \/\/ But first, check if we have a saved profile\n+  bool training_has_profile = (mtd->final_profile() != nullptr);\n+  if (!training_has_profile) {\n+    return CompLevel_full_profile;\n+  }\n+\n+\n+  assert(training_has_profile, \"Have to have a profile to be here\");\n+  \/\/ Check if the method is ready\n+  CompileTrainingData* ctd = mtd->last_toplevel_compile(CompLevel_full_optimization);\n+  if (ctd != nullptr && ctd->init_deps_left() == 0) {\n+    if (method->method_data() == nullptr) {\n+      create_mdo(method, THREAD);\n+    }\n+    return CompLevel_full_optimization;\n+  }\n+\n+  \/\/ Otherwise stay at the current level\n+  return CompLevel_limited_profile;\n+}\n+\n+\n+CompLevel CompilationPolicy::trained_transition_from_full_profile(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD) {\n+  precond(mtd != nullptr);\n+  precond(cur_level == CompLevel_full_profile);\n@@ -984,0 +1207,44 @@\n+  CompLevel highest_training_level = static_cast<CompLevel>(mtd->highest_top_level());\n+  \/\/ We have method at the full profile level and we also know that it's possibly an important method.\n+  if (highest_training_level == CompLevel_full_optimization && !mtd->only_inlined()) {\n+    \/\/ Check if it is adequately profiled\n+    if (is_method_profiled(method)) {\n+      return CompLevel_full_optimization;\n+    }\n+  }\n+\n+  \/\/ Otherwise stay at the current level\n+  return CompLevel_full_profile;\n+}\n+\n+CompLevel CompilationPolicy::trained_transition(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD) {\n+  precond(MethodTrainingData::have_data());\n+\n+  \/\/ If there is no training data recorded for this method, bail out.\n+  if (mtd == nullptr) {\n+    return cur_level;\n+  }\n+\n+  CompLevel next_level = cur_level;\n+  switch(cur_level) {\n+    default: break;\n+    case CompLevel_none:\n+      next_level = trained_transition_from_none(method, cur_level, mtd, THREAD);\n+      break;\n+    case CompLevel_limited_profile:\n+      next_level = trained_transition_from_limited_profile(method, cur_level, mtd, THREAD);\n+      break;\n+    case CompLevel_full_profile:\n+      next_level = trained_transition_from_full_profile(method, cur_level, mtd, THREAD);\n+      break;\n+  }\n+\n+  \/\/ We don't have any special strategies for the C2-only compilation modes, so just fix up the levels for now.\n+  if (CompilationModeFlag::high_only_quick_internal() && CompLevel_simple < next_level && next_level < CompLevel_full_optimization) {\n+    return CompLevel_none;\n+  }\n+  if (CompilationModeFlag::high_only() && next_level < CompLevel_full_optimization) {\n+    return CompLevel_none;\n+  }\n+  return (cur_level != next_level) ? limit_level(next_level) : cur_level;\n+}\n@@ -1025,1 +1292,1 @@\n-CompLevel CompilationPolicy::common(const methodHandle& method, CompLevel cur_level, bool disable_feedback) {\n+CompLevel CompilationPolicy::common(const methodHandle& method, CompLevel cur_level, JavaThread* THREAD, bool disable_feedback) {\n@@ -1027,2 +1294,0 @@\n-  int i = method->invocation_count();\n-  int b = method->backedge_count();\n@@ -1032,3 +1297,9 @@\n-  } else {\n-    if (is_trivial(method) || method->is_native()) {\n-      next_level = CompilationModeFlag::disable_intermediate() ? CompLevel_full_optimization : CompLevel_simple;\n+  } else if (is_trivial(method) || method->is_native()) {\n+    \/\/ We do not care if there is profiling data for these methods, throw them to compiler.\n+    next_level = CompilationModeFlag::disable_intermediate() ? CompLevel_full_optimization : CompLevel_simple;\n+  } else if (MethodTrainingData::have_data()) {\n+    MethodTrainingData* mtd = MethodTrainingData::find_fast(method);\n+    if (mtd == nullptr) {\n+      \/\/ We haven't see compilations of this method in training. It's either very cold or the behavior changed.\n+      \/\/ Feed it to the standard TF with no profiling delay.\n+      next_level = standard_transition<Predicate>(method, cur_level, false \/*delay_profiling*\/, disable_feedback);\n@@ -1036,64 +1307,6 @@\n-      switch(cur_level) {\n-      default: break;\n-      case CompLevel_none:\n-        \/\/ If we were at full profile level, would we switch to full opt?\n-        if (common<Predicate>(method, CompLevel_full_profile, disable_feedback) == CompLevel_full_optimization) {\n-          next_level = CompLevel_full_optimization;\n-        } else if (!CompilationModeFlag::disable_intermediate() && Predicate::apply(method, cur_level, i, b)) {\n-          \/\/ C1-generated fully profiled code is about 30% slower than the limited profile\n-          \/\/ code that has only invocation and backedge counters. The observation is that\n-          \/\/ if C2 queue is large enough we can spend too much time in the fully profiled code\n-          \/\/ while waiting for C2 to pick the method from the queue. To alleviate this problem\n-          \/\/ we introduce a feedback on the C2 queue size. If the C2 queue is sufficiently long\n-          \/\/ we choose to compile a limited profiled version and then recompile with full profiling\n-          \/\/ when the load on C2 goes down.\n-          if (!disable_feedback && CompileBroker::queue_size(CompLevel_full_optimization) >\n-              Tier3DelayOn * compiler_count(CompLevel_full_optimization)) {\n-            next_level = CompLevel_limited_profile;\n-          } else {\n-            next_level = CompLevel_full_profile;\n-          }\n-        }\n-        break;\n-      case CompLevel_limited_profile:\n-        if (is_method_profiled(method)) {\n-          \/\/ Special case: we got here because this method was fully profiled in the interpreter.\n-          next_level = CompLevel_full_optimization;\n-        } else {\n-          MethodData* mdo = method->method_data();\n-          if (mdo != nullptr) {\n-            if (mdo->would_profile()) {\n-              if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n-                                       Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n-                                       Predicate::apply(method, cur_level, i, b))) {\n-                next_level = CompLevel_full_profile;\n-              }\n-            } else {\n-              next_level = CompLevel_full_optimization;\n-            }\n-          } else {\n-            \/\/ If there is no MDO we need to profile\n-            if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n-                                     Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n-                                     Predicate::apply(method, cur_level, i, b))) {\n-              next_level = CompLevel_full_profile;\n-            }\n-          }\n-        }\n-        break;\n-      case CompLevel_full_profile:\n-        {\n-          MethodData* mdo = method->method_data();\n-          if (mdo != nullptr) {\n-            if (mdo->would_profile() || CompilationModeFlag::disable_intermediate()) {\n-              int mdo_i = mdo->invocation_count_delta();\n-              int mdo_b = mdo->backedge_count_delta();\n-              if (Predicate::apply(method, cur_level, mdo_i, mdo_b)) {\n-                next_level = CompLevel_full_optimization;\n-              }\n-            } else {\n-              next_level = CompLevel_full_optimization;\n-            }\n-          }\n-        }\n-        break;\n+      next_level = trained_transition(method, cur_level, mtd, THREAD);\n+      if (cur_level == next_level) {\n+        \/\/ trained_transtion() is going to return the same level if no startup\/warmup optimizations apply.\n+        \/\/ In order to catch possible pathologies due to behavior change we feed the event to the regular\n+        \/\/ TF but with profiling delay.\n+        next_level = standard_transition<Predicate>(method, cur_level, true \/*delay_profiling*\/, disable_feedback);\n@@ -1102,0 +1315,2 @@\n+  } else {\n+    next_level = standard_transition<Predicate>(method, cur_level, false \/*delay_profiling*\/, disable_feedback);\n@@ -1107,0 +1322,96 @@\n+template<typename Predicate>\n+CompLevel CompilationPolicy::standard_transition(const methodHandle& method, CompLevel cur_level, bool delay_profiling, bool disable_feedback) {\n+  CompLevel next_level = cur_level;\n+  switch(cur_level) {\n+  default: break;\n+  case CompLevel_none:\n+    next_level = transition_from_none<Predicate>(method, cur_level, delay_profiling, disable_feedback);\n+    break;\n+  case CompLevel_limited_profile:\n+    next_level = transition_from_limited_profile<Predicate>(method, cur_level, delay_profiling, disable_feedback);\n+    break;\n+  case CompLevel_full_profile:\n+    next_level = transition_from_full_profile<Predicate>(method, cur_level);\n+    break;\n+  }\n+  return next_level;\n+}\n+\n+template<typename Predicate>\n+CompLevel CompilationPolicy::transition_from_none(const methodHandle& method, CompLevel cur_level, bool delay_profiling, bool disable_feedback) {\n+  precond(cur_level == CompLevel_none);\n+  CompLevel next_level = cur_level;\n+  int i = method->invocation_count();\n+  int b = method->backedge_count();\n+  double scale = delay_profiling ? Tier0ProfileDelayFactor : 1.0;\n+  \/\/ If we were at full profile level, would we switch to full opt?\n+  if (transition_from_full_profile<Predicate>(method, CompLevel_full_profile) == CompLevel_full_optimization) {\n+    next_level = CompLevel_full_optimization;\n+  } else if (!CompilationModeFlag::disable_intermediate() && Predicate::apply_scaled(method, cur_level, i, b, scale)) {\n+    \/\/ C1-generated fully profiled code is about 30% slower than the limited profile\n+    \/\/ code that has only invocation and backedge counters. The observation is that\n+    \/\/ if C2 queue is large enough we can spend too much time in the fully profiled code\n+    \/\/ while waiting for C2 to pick the method from the queue. To alleviate this problem\n+    \/\/ we introduce a feedback on the C2 queue size. If the C2 queue is sufficiently long\n+    \/\/ we choose to compile a limited profiled version and then recompile with full profiling\n+    \/\/ when the load on C2 goes down.\n+    if (delay_profiling || (!disable_feedback && CompileBroker::queue_size(CompLevel_full_optimization) > Tier3DelayOn * compiler_count(CompLevel_full_optimization))) {\n+      next_level = CompLevel_limited_profile;\n+    } else {\n+      next_level = CompLevel_full_profile;\n+    }\n+  }\n+  return next_level;\n+}\n+\n+template<typename Predicate>\n+CompLevel CompilationPolicy::transition_from_full_profile(const methodHandle& method, CompLevel cur_level) {\n+  precond(cur_level == CompLevel_full_profile);\n+  CompLevel next_level = cur_level;\n+  MethodData* mdo = method->method_data();\n+  if (mdo != nullptr) {\n+    if (mdo->would_profile() || CompilationModeFlag::disable_intermediate()) {\n+      int mdo_i = mdo->invocation_count_delta();\n+      int mdo_b = mdo->backedge_count_delta();\n+      if (Predicate::apply(method, cur_level, mdo_i, mdo_b)) {\n+        next_level = CompLevel_full_optimization;\n+      }\n+    } else {\n+      next_level = CompLevel_full_optimization;\n+    }\n+  }\n+  return next_level;\n+}\n+\n+template<typename Predicate>\n+CompLevel CompilationPolicy::transition_from_limited_profile(const methodHandle& method, CompLevel cur_level, bool delay_profiling, bool disable_feedback) {\n+  precond(cur_level == CompLevel_limited_profile);\n+  CompLevel next_level = cur_level;\n+  int i = method->invocation_count();\n+  int b = method->backedge_count();\n+  double scale = delay_profiling ? Tier2ProfileDelayFactor : 1.0;\n+  MethodData* mdo = method->method_data();\n+  if (mdo != nullptr) {\n+    if (mdo->would_profile()) {\n+      if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n+                              Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n+                              Predicate::apply_scaled(method, cur_level, i, b, scale))) {\n+        next_level = CompLevel_full_profile;\n+      }\n+    } else {\n+      next_level = CompLevel_full_optimization;\n+    }\n+  } else {\n+    \/\/ If there is no MDO we need to profile\n+    if (disable_feedback || (CompileBroker::queue_size(CompLevel_full_optimization) <=\n+                            Tier3DelayOff * compiler_count(CompLevel_full_optimization) &&\n+                            Predicate::apply_scaled(method, cur_level, i, b, scale))) {\n+      next_level = CompLevel_full_profile;\n+    }\n+  }\n+  if (next_level == CompLevel_full_profile && is_method_profiled(method)) {\n+    next_level = CompLevel_full_optimization;\n+  }\n+  return next_level;\n+}\n+\n@@ -1109,3 +1420,3 @@\n-CompLevel CompilationPolicy::call_event(const methodHandle& method, CompLevel cur_level, Thread* thread) {\n-  CompLevel osr_level = MIN2((CompLevel) method->highest_osr_comp_level(), common<LoopPredicate>(method, cur_level, true));\n-  CompLevel next_level = common<CallPredicate>(method, cur_level, is_old(method));\n+CompLevel CompilationPolicy::call_event(const methodHandle& method, CompLevel cur_level, JavaThread* THREAD) {\n+  CompLevel osr_level = MIN2((CompLevel) method->highest_osr_comp_level(), common<LoopPredicate>(method, cur_level, THREAD, true));\n+  CompLevel next_level = common<CallPredicate>(method, cur_level, THREAD, !TrainingData::have_data() && is_old(method));\n@@ -1125,0 +1436,6 @@\n+#if INCLUDE_JVMCI\n+  if (EnableJVMCI && UseJVMCICompiler &&\n+      next_level == CompLevel_full_optimization CDS_ONLY(&& !AOTLinkedClassBulkLoader::class_preloading_finished())) {\n+    next_level = cur_level;\n+  }\n+#endif\n@@ -1129,2 +1446,2 @@\n-CompLevel CompilationPolicy::loop_event(const methodHandle& method, CompLevel cur_level, Thread* thread) {\n-  CompLevel next_level = common<LoopPredicate>(method, cur_level, true);\n+CompLevel CompilationPolicy::loop_event(const methodHandle& method, CompLevel cur_level, JavaThread* THREAD) {\n+  CompLevel next_level = common<LoopPredicate>(method, cur_level, THREAD, true);\n","filename":"src\/hotspot\/share\/compiler\/compilationPolicy.cpp","additions":428,"deletions":111,"binary":false,"changes":539,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -33,0 +34,73 @@\n+namespace CompilationPolicyUtils {\n+template<typename T>\n+class Queue {\n+  class QueueNode : public CHeapObj<mtCompiler> {\n+    T* _value;\n+    QueueNode* _next;\n+  public:\n+    QueueNode(T* value, QueueNode* next) : _value(value), _next(next) { }\n+    T* value() const { return _value; }\n+    void set_next(QueueNode* next) { _next = next; }\n+    QueueNode* next() const { return _next; }\n+  };\n+\n+  QueueNode* _head;\n+  QueueNode* _tail;\n+\n+  void push_unlocked(T* value) {\n+    QueueNode* n = new QueueNode(value, nullptr);\n+    if (_tail != nullptr) {\n+      _tail->set_next(n);\n+    }\n+    _tail = n;\n+    if (_head == nullptr) {\n+      _head = _tail;\n+    }\n+  }\n+  T* pop_unlocked() {\n+    QueueNode* n = _head;\n+    if (_head != nullptr) {\n+      _head = _head->next();\n+    }\n+    if (_head == nullptr) {\n+      _tail = _head;\n+    }\n+    T* value = nullptr;\n+    if (n != nullptr) {\n+      value = n->value();\n+      delete n;\n+    }\n+    return value;\n+  }\n+public:\n+  Queue() : _head(nullptr), _tail(nullptr) { }\n+  void push(T* value, Monitor* lock, TRAPS) {\n+    MonitorLocker locker(THREAD, lock);\n+    push_unlocked(value);\n+    locker.notify_all();\n+  }\n+\n+  bool is_empty_unlocked() const { return _head == nullptr; }\n+\n+  T* pop(Monitor* lock, TRAPS) {\n+    MonitorLocker locker(THREAD, lock);\n+    while(is_empty_unlocked() && !CompileBroker::is_compilation_disabled_forever()) {\n+      locker.wait();\n+    }\n+    T* value = pop_unlocked();\n+    return value;\n+  }\n+\n+  T* try_pop(Monitor* lock, TRAPS) {\n+    MonitorLocker locker(THREAD, lock);\n+    T* value = nullptr;\n+    if (!is_empty_unlocked()) {\n+      value = pop_unlocked();\n+    }\n+    return value;\n+  }\n+\n+  void print_on(outputStream* st);\n+};\n+} \/\/ namespace CompilationPolicyUtils\n+\n@@ -176,1 +250,3 @@\n-  static jlong _start_time;\n+  typedef CompilationPolicyUtils::Queue<InstanceKlass> TrainingReplayQueue;\n+\n+  static int64_t _start_time;\n@@ -179,0 +255,1 @@\n+  static TrainingReplayQueue _training_replay_queue;\n@@ -190,1 +267,16 @@\n-  static CompLevel common(const methodHandle& method, CompLevel cur_level, bool disable_feedback = false);\n+  static CompLevel common(const methodHandle& method, CompLevel cur_level, JavaThread* THREAD, bool disable_feedback = false);\n+\n+  template<typename Predicate>\n+  static CompLevel transition_from_none(const methodHandle& method, CompLevel cur_level, bool delay_profiling, bool disable_feedback);\n+  template<typename Predicate>\n+  static CompLevel transition_from_limited_profile(const methodHandle& method, CompLevel cur_level, bool delay_profiling, bool disable_feedback);\n+  template<typename Predicate>\n+  static CompLevel transition_from_full_profile(const methodHandle& method, CompLevel cur_level);\n+  template<typename Predicate>\n+  static CompLevel standard_transition(const methodHandle& method, CompLevel cur_level, bool delayprof, bool disable_feedback);\n+\n+  static CompLevel trained_transition_from_none(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD);\n+  static CompLevel trained_transition_from_limited_profile(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD);\n+  static CompLevel trained_transition_from_full_profile(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD);\n+  static CompLevel trained_transition(const methodHandle& method, CompLevel cur_level, MethodTrainingData* mtd, JavaThread* THREAD);\n+\n@@ -194,1 +286,1 @@\n-  static CompLevel call_event(const methodHandle& method, CompLevel cur_level, Thread* thread);\n+  static CompLevel call_event(const methodHandle& method, CompLevel cur_level, JavaThread* THREAD);\n@@ -197,2 +289,3 @@\n-  static CompLevel loop_event(const methodHandle& method, CompLevel cur_level, Thread* thread);\n-  static void print_counters(const char* prefix, const Method* m);\n+  static CompLevel loop_event(const methodHandle& method, CompLevel cur_level, JavaThread* THREAD);\n+  static void print_counters(const char* prefix, Method* m);\n+  static void print_training_data(const char* prefix, Method* method);\n@@ -205,1 +298,1 @@\n-  inline static bool is_stale(jlong t, jlong timeout, const methodHandle& method);\n+  inline static bool is_stale(int64_t t, int64_t timeout, const methodHandle& method);\n@@ -212,1 +305,1 @@\n-  inline static void update_rate(jlong t, const methodHandle& method);\n+  inline static void update_rate(int64_t t, const methodHandle& method);\n@@ -227,2 +320,2 @@\n-  enum EventType { CALL, LOOP, COMPILE, REMOVE_FROM_QUEUE, UPDATE_IN_QUEUE, REPROFILE, MAKE_NOT_ENTRANT };\n-  static void print_event(EventType type, const Method* m, const Method* im, int bci, CompLevel level);\n+  enum EventType { CALL, LOOP, COMPILE, FORCE_COMPILE, FORCE_RECOMPILE, REMOVE_FROM_QUEUE, UPDATE_IN_QUEUE, REPROFILE, MAKE_NOT_ENTRANT };\n+  static void print_event(EventType type, Method* m, Method* im, int bci, CompLevel level);\n@@ -245,2 +338,2 @@\n-  static void set_start_time(jlong t) { _start_time = t;    }\n-  static jlong start_time()           { return _start_time; }\n+  static void set_start_time(int64_t t) { _start_time = t;    }\n+  static int64_t start_time()           { return _start_time; }\n@@ -250,1 +343,3 @@\n-public:\n+  static void maybe_compile_early(const methodHandle& m, TRAPS);\n+  static void replay_training_at_init_impl(InstanceKlass* klass, TRAPS);\n+ public:\n@@ -255,1 +350,0 @@\n-\n@@ -260,0 +354,4 @@\n+  static void flush_replay_training_at_init(TRAPS);\n+  static void replay_training_at_init(InstanceKlass* klass, TRAPS);\n+  static void replay_training_at_init_loop(TRAPS);\n+\n@@ -272,1 +370,1 @@\n-  static CompileTask* select_task(CompileQueue* compile_queue);\n+  static CompileTask* select_task(CompileQueue* compile_queue, JavaThread* THREAD);\n@@ -274,1 +372,1 @@\n-  static bool is_mature(Method* method);\n+  static bool is_mature(MethodData* mdo);\n@@ -283,0 +381,1 @@\n+  static void dump();\n","filename":"src\/hotspot\/share\/compiler\/compilationPolicy.hpp","additions":115,"deletions":16,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -350,0 +351,7 @@\n+  if (TrainingData::need_data() && !CDSConfig::is_dumping_final_static_archive()) {\n+    CompileTrainingData* ctd = CompileTrainingData::make(task);\n+    if (ctd != nullptr) {\n+      task->set_training_data(ctd);\n+    }\n+  }\n+\n@@ -445,1 +453,1 @@\n-    task = CompilationPolicy::select_task(this);\n+    task = CompilationPolicy::select_task(this, thread);\n@@ -786,0 +794,4 @@\n+void TrainingReplayThread::training_replay_thread_entry(JavaThread* thread, TRAPS) {\n+  CompilationPolicy::replay_training_at_init_loop(thread);\n+}\n+\n@@ -863,0 +875,3 @@\n+    case training_replay_t:\n+      new_thread = new TrainingReplayThread();\n+      break;\n@@ -1020,0 +1035,10 @@\n+void CompileBroker::init_training_replay() {\n+  \/\/ Ensure any exceptions lead to vm_exit_during_initialization.\n+  EXCEPTION_MARK;\n+  if (TrainingData::have_data()) {\n+    Handle thread_oop = JavaThread::create_system_thread_object(\"Training replay thread\", CHECK);\n+    jobject thread_handle = JNIHandles::make_local(THREAD, thread_oop());\n+    make_thread(training_replay_t, thread_handle, nullptr, nullptr, THREAD);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":26,"deletions":1,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -257,1 +257,2 @@\n-    deoptimizer_t\n+    deoptimizer_t,\n+    training_replay_t\n@@ -262,0 +263,1 @@\n+  static void init_training_replay();\n@@ -454,0 +456,12 @@\n+\/\/ In order to achiveve a maximally fast warmup we attempt to compile important methods as soon as all\n+\/\/ the classes that they depend on are initialized. TrainingReplayThread processes a queue of InstanceKlass*\n+\/\/ that have just finished running their static initializers. We find all the methods that depend on the given class\n+\/\/ and for which the number of remaining dependencies is now zero, and eagerly compile them.\n+class TrainingReplayThread : public JavaThread {\n+  static void training_replay_thread_entry(JavaThread* thread, TRAPS);\n+public:\n+  TrainingReplayThread() : JavaThread(&training_replay_thread_entry) { }\n+\n+  bool is_hidden_from_external_view() const      { return true; }\n+};\n+\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -128,0 +128,1 @@\n+  _training_data = nullptr;\n","filename":"src\/hotspot\/share\/compiler\/compileTask.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+class CompileTrainingData;\n@@ -116,0 +117,1 @@\n+  CompileTrainingData* _training_data;\n@@ -218,0 +220,3 @@\n+  CompileTrainingData* training_data() const      { return _training_data; }\n+  void set_training_data(CompileTrainingData* td) { _training_data = td;   }\n+\n","filename":"src\/hotspot\/share\/compiler\/compileTask.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -62,1 +62,2 @@\n-  CompLevel_full_optimization = 4          \/\/ C2 or JVMCI\n+  CompLevel_full_optimization = 4,         \/\/ C2 or JVMCI\n+  CompLevel_count             = 5\n","filename":"src\/hotspot\/share\/compiler\/compilerDefinitions.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -272,0 +272,11 @@\n+  product(double, Tier0ProfileDelayFactor, 100.0, DIAGNOSTIC,               \\\n+          \"Delay profiling\/compiling of methods that were \"                 \\\n+          \"observed to be lukewarm\")                                        \\\n+                                                                            \\\n+  product(double, Tier2ProfileDelayFactor, 250.0, DIAGNOSTIC,               \\\n+          \"Delay profiling of methods that were observed to be lukewarm\")   \\\n+                                                                            \\\n+  product(bool, SkipTier2IfPossible, false, DIAGNOSTIC,                     \\\n+          \"Compile at tier 4 instead of tier 2 in training replay \"         \\\n+          \"mode if posssible\")                                              \\\n+                                                                            \\\n@@ -385,1 +396,0 @@\n-\n","filename":"src\/hotspot\/share\/compiler\/compiler_globals.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -208,0 +208,1 @@\n+  LOG_TAG(training) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -89,0 +89,7 @@\n+\/\/ This is used for allocating training data. We are allocating training data in many cases where a GC cannot be triggered.\n+void* MetaspaceObj::operator new(size_t size, MemTag flags) throw() {\n+  void* p = AllocateHeap(size, flags, CALLER_PC);\n+  memset(p, 0, size);\n+  return p;\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/allocation.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -318,0 +318,3 @@\n+  f(KlassTrainingData) \\\n+  f(MethodTrainingData) \\\n+  f(CompileTrainingData) \\\n@@ -357,0 +360,2 @@\n+  \/\/ This is used for allocating training data. We are allocating training data in many cases where a GC cannot be triggered.\n+  void* operator new(size_t size, MemTag flags) throw();\n","filename":"src\/hotspot\/share\/memory\/allocation.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -51,0 +51,6 @@\n+  \/\/ This API should be used for TrainingData only.\n+  template <typename T>\n+  static Array<T>* new_array_from_c_heap(int length, MemTag flags) {\n+    return new (length, flags) Array<T>(length);\n+  }\n+\n","filename":"src\/hotspot\/share\/memory\/metadataFactory.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -107,0 +108,12 @@\n+  \/\/\n+  \/\/ Pointer Tagging\n+  \/\/\n+  \/\/ All metaspace pointers are at least 4 byte aligned. Therefore, it's possible for\n+  \/\/ certain pointers to contain \"tags\" in their lowest 2 bits.\n+  \/\/\n+  \/\/ Ref::obj() clears the tag bits in the return values. As a result, most\n+  \/\/ callers who just want walk a closure of metaspace objects do not need to worry\n+  \/\/ about the tag bits.\n+  \/\/\n+  \/\/ If you need to use the tags, you can access the tagged pointer with Ref::addr()\n+  \/\/ and manipulate its parts with strip_tags(), decode_tags() and add_tags()\n@@ -126,1 +139,1 @@\n-      return *addr();\n+      return strip_tags(*addr());\n@@ -146,0 +159,23 @@\n+  \/\/ Pointer tagging support\n+  constexpr static uintx TAG_MASK = 0x03;\n+\n+  template <typename T>\n+  static T strip_tags(T ptr_with_tags) {\n+    uintx n = (uintx)ptr_with_tags;\n+    return (T)(n & ~TAG_MASK);\n+  }\n+\n+  template <typename T>\n+  static uintx decode_tags(T ptr_with_tags) {\n+    uintx n = (uintx)ptr_with_tags;\n+    return (n & TAG_MASK);\n+  }\n+\n+  template <typename T>\n+  static T add_tags(T ptr, uintx tags) {\n+    uintx n = (uintx)ptr;\n+    assert((n & TAG_MASK) == 0, \"sanity\");\n+    assert(tags <= TAG_MASK, \"sanity\");\n+    return (T)(n | tags);\n+  }\n+\n@@ -151,1 +187,1 @@\n-      return *_mpp;\n+      return strip_tags(*_mpp);\n@@ -179,1 +215,1 @@\n-      return *_mpp;\n+      return strip_tags(*_mpp);\n","filename":"src\/hotspot\/share\/memory\/metaspaceClosure.hpp","additions":40,"deletions":4,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -57,0 +57,5 @@\n+  inline void* operator new(size_t size, ClassLoaderData* loader_data, int length) throw();\n+\n+  \/\/ Work-around -- see JDK-8331086\n+  inline void* operator new(size_t size, int length, MemTag flags) throw();\n+\n","filename":"src\/hotspot\/share\/oops\/array.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -40,0 +40,15 @@\n+template <typename T>\n+inline void* Array<T>::operator new(size_t size, ClassLoaderData* loader_data, int length) throw() {\n+  size_t word_size = Array::size(length);\n+  return (void*) Metaspace::allocate(loader_data, word_size,\n+                                     MetaspaceObj::array_type(sizeof(T)), false);\n+}\n+\n+template <typename T>\n+inline void* Array<T>::operator new(size_t size, int length, MemTag flags) throw() {\n+  size = Array::size(length) * BytesPerWord;\n+  void* p = AllocateHeap(size * BytesPerWord, flags);\n+  memset(p, 0, size);\n+  return p;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/array.inline.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1330,0 +1330,1 @@\n+    CompilationPolicy::replay_training_at_init(this, THREAD);\n@@ -2650,0 +2651,2 @@\n+  _misc_flags.set_has_init_deps_processed(false);\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1127,0 +1127,6 @@\n+  bool     has_init_deps_processed() const { return _misc_flags.has_init_deps_processed(); }\n+  void set_has_init_deps_processed() {\n+    assert(is_initialized(), \"\");\n+    assert(!has_init_deps_processed(), \"already set\"); \/\/ one-off action\n+    _misc_flags.set_has_init_deps_processed(true);\n+  }\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+    status(has_init_deps_processed           , 1 << 5) \/* all init dependencies are processed *\/ \\\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -733,0 +733,1 @@\n+  inline bool is_loader_present_and_alive() const;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,0 +62,5 @@\n+inline bool Klass::is_loader_present_and_alive() const {\n+  ClassLoaderData* cld = class_loader_data();\n+  return (cld != nullptr) ? cld->is_alive() : false;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -410,0 +411,6 @@\n+  if (method_data() != nullptr) {\n+    method_data()->remove_unshareable_info();\n+  }\n+  if (method_counters() != nullptr) {\n+    method_counters()->remove_unshareable_info();\n+  }\n@@ -418,0 +425,6 @@\n+  if (method_data() != nullptr) {\n+    method_data()->restore_unshareable_info(CHECK);\n+  }\n+  if (method_counters() != nullptr) {\n+    method_counters()->restore_unshareable_info(CHECK);\n+  }\n@@ -589,0 +602,31 @@\n+MethodTrainingData* Method::training_data_or_null() const {\n+  MethodCounters* mcs = method_counters();\n+  if (mcs == nullptr) {\n+    return nullptr;\n+  } else {\n+    MethodTrainingData* mtd = mcs->method_training_data();\n+    if (mtd == mcs->method_training_data_sentinel()) {\n+      return nullptr;\n+    }\n+    return mtd;\n+  }\n+}\n+\n+bool Method::init_training_data(MethodTrainingData* td) {\n+  MethodCounters* mcs = method_counters();\n+  if (mcs == nullptr) {\n+    return false;\n+  } else {\n+    return mcs->init_method_training_data(td);\n+  }\n+}\n+\n+bool Method::install_training_method_data(const methodHandle& method) {\n+  MethodTrainingData* mtd = MethodTrainingData::find(method);\n+  if (mtd != nullptr && mtd->final_profile() != nullptr) {\n+    Atomic::replace_if_null(&method->_method_data, mtd->final_profile());\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -592,0 +636,3 @@\n+  if (install_training_method_data(method)) {\n+    return;\n+  }\n@@ -1164,0 +1211,6 @@\n+  clear_is_not_c1_compilable();\n+  clear_is_not_c1_osr_compilable();\n+  clear_is_not_c2_compilable();\n+  clear_is_not_c2_osr_compilable();\n+  clear_queued_for_compilation();\n+\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+class MethodTrainingData;\n@@ -313,1 +314,1 @@\n-  MethodData* method_data() const              {\n+  MethodData* method_data() const {\n@@ -316,0 +317,4 @@\n+  void set_method_data(MethodData* data);\n+\n+  MethodTrainingData* training_data_or_null() const;\n+  bool init_training_data(MethodTrainingData* td);\n@@ -344,1 +349,1 @@\n-\n+  static bool install_training_method_data(const methodHandle& method);\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"memory\/metaspaceClosure.hpp\"\n@@ -29,0 +31,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"memory\/resourceArea.hpp\"\n@@ -32,0 +36,2 @@\n+  _method(mh()),\n+  _method_training_data(method_training_data_sentinel()),\n@@ -50,0 +56,7 @@\n+#if INCLUDE_CDS\n+MethodCounters::MethodCounters() {\n+  \/\/ Used by cppVtables.cpp only\n+  assert(CDSConfig::is_dumping_static_archive() || UseSharedSpaces, \"only for CDS\");\n+}\n+#endif\n+\n@@ -52,1 +65,1 @@\n-  return new(loader_data, size(), MetaspaceObj::MethodCountersType) MethodCounters(mh);\n+  return new(loader_data, method_counters_size(), MetaspaceObj::MethodCountersType) MethodCounters(mh);\n@@ -57,1 +70,1 @@\n-  return new(loader_data, size(), MetaspaceObj::MethodCountersType, THREAD) MethodCounters(mh);\n+  return new(loader_data, method_counters_size(), MetaspaceObj::MethodCountersType, THREAD) MethodCounters(mh);\n@@ -71,0 +84,39 @@\n+void MethodCounters::metaspace_pointers_do(MetaspaceClosure* it) {\n+  log_trace(aot, training)(\"Iter(MethodCounters): %p\", this);\n+  it->push(&_method);\n+  it->push(&_method_training_data);\n+}\n+\n+#if INCLUDE_CDS\n+void MethodCounters::remove_unshareable_info() {\n+}\n+void MethodCounters::restore_unshareable_info(TRAPS) {\n+  _method_training_data = method_training_data_sentinel();\n+}\n+#endif \/\/ INCLUDE_CDS\n+\n+void MethodCounters::print_on(outputStream* st) const {\n+  assert(is_methodCounters(), \"should be method counters\");\n+  st->print(\"method counters\");\n+  print_data_on(st);\n+}\n+\n+void MethodCounters::print_data_on(outputStream* st) const {\n+  ResourceMark rm;\n+  st->print_cr(\"  - invocation_counter: %d carry=%d\", _invocation_counter.count(), _invocation_counter.carry());\n+  st->print_cr(\"  - backedge_counter: %d carry=%d\",   _backedge_counter.count(), _backedge_counter.carry());\n+  st->print_cr(\"  - prev_time: \" JLONG_FORMAT,        _prev_time);\n+  st->print_cr(\"  - rate: %.3f\",             _rate);\n+  st->print_cr(\"  - invoke_mask: %d\",        _invoke_mask);\n+  st->print_cr(\"  - backedge_mask: %d\",      _backedge_mask);\n+  st->print_cr(\"  - prev_event_count: %d\",   _prev_event_count);\n+#if COMPILER2_OR_JVMCI\n+  st->print_cr(\"  - interpreter_throwout_count: %u\", _interpreter_throwout_count);\n+#endif\n+#if INCLUDE_JVMTI\n+  st->print_cr(\"  - number_of_breakpoints: %u\", _number_of_breakpoints);\n+#endif\n+  st->print_cr(\"  - highest_comp_level: %u\", _highest_comp_level);\n+  st->print_cr(\"  - highest_osr_comp_level: %u\", _highest_osr_comp_level);\n+}\n+\n@@ -72,0 +124,1 @@\n+  assert(is_methodCounters(), \"must be methodCounters\");\n","filename":"src\/hotspot\/share\/oops\/methodCounters.cpp","additions":55,"deletions":2,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -33,1 +33,3 @@\n-class MethodCounters : public MetaspaceObj {\n+class MethodTrainingData;\n+\n+class MethodCounters : public Metadata {\n@@ -36,0 +38,6 @@\n+\n+ \/\/ Used by CDS. These classes need to access the private default constructor.\n+ template <class T> friend class CppVtableTesterA;\n+ template <class T> friend class CppVtableTesterB;\n+ template <class T> friend class CppVtableCloner;\n+\n@@ -39,0 +47,5 @@\n+\n+  \/\/ Back pointer to the Method*\n+  Method* _method;\n+\n+  Metadata*         _method_training_data;\n@@ -54,0 +67,2 @@\n+  MethodCounters();\n+\n@@ -55,0 +70,2 @@\n+  virtual bool is_methodCounters() const { return true; }\n+  Method* method() const { return _method; }\n@@ -58,1 +75,0 @@\n-  DEBUG_ONLY(bool on_stack() { return false; })\n@@ -61,3 +77,1 @@\n-  void metaspace_pointers_do(MetaspaceClosure* it) { return; }\n-\n-  static int size() {\n+  static int method_counters_size() {\n@@ -66,0 +80,3 @@\n+  virtual int size() const {\n+    return method_counters_size();\n+  }\n@@ -68,0 +85,2 @@\n+  void metaspace_pointers_do(MetaspaceClosure* iter);\n+\n@@ -130,1 +149,26 @@\n-  const char* internal_name() const { return \"{method counters}\"; }\n+  virtual const char* internal_name() const { return \"{method counters}\"; }\n+\n+  Metadata* method_training_data_sentinel() {\n+    return this;\n+  }\n+  MethodTrainingData* method_training_data() const {\n+    return reinterpret_cast<MethodTrainingData*>(_method_training_data);\n+  }\n+  bool init_method_training_data(MethodTrainingData* td) {\n+    MethodTrainingData* cur = method_training_data();\n+    if (cur == td) {\n+      return true;\n+    }\n+    if (cur == nullptr || cur == reinterpret_cast<MethodTrainingData*>(method_training_data_sentinel())) {\n+      return Atomic::cmpxchg(reinterpret_cast<MethodTrainingData**>(&_method_training_data), cur, td) == cur;\n+    }\n+    return false;\n+  }\n+\n+#if INCLUDE_CDS\n+  void remove_unshareable_info();\n+  void restore_unshareable_info(TRAPS);\n+#endif\n+\n+  \/\/ Printing\n+  void print_on      (outputStream* st) const;\n@@ -132,0 +176,1 @@\n+  void print_data_on(outputStream* st) const;\n","filename":"src\/hotspot\/share\/oops\/methodCounters.hpp","additions":51,"deletions":6,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"classfile\/systemDictionaryShared.hpp\"\n@@ -322,0 +324,20 @@\n+static bool is_excluded(Klass* k) {\n+#if INCLUDE_CDS\n+  if (SafepointSynchronize::is_at_safepoint() &&\n+      CDSConfig::is_dumping_archive() &&\n+      CDSConfig::current_thread_is_vm_or_dumper()) {\n+    if (k->is_instance_klass() && !InstanceKlass::cast(k)->is_loaded()) {\n+      log_debug(aot, training)(\"Purged %s from MDO: unloaded class\", k->name()->as_C_string());\n+      return true;\n+    } else {\n+      bool excluded = SystemDictionaryShared::should_be_excluded(k);\n+      if (excluded) {\n+        log_debug(aot, training)(\"Purged %s from MDO: excluded class\", k->name()->as_C_string());\n+      }\n+      return excluded;\n+    }\n+  }\n+#endif\n+  return false;\n+}\n+\n@@ -326,2 +348,7 @@\n-    if (k != nullptr && (always_clean || !k->is_loader_alive())) {\n-      set_type(i, with_status((Klass*)nullptr, p));\n+    if (k != nullptr) {\n+      if (!always_clean && k->is_instance_klass() && InstanceKlass::cast(k)->is_not_initialized()) {\n+        continue; \/\/ skip not-yet-initialized classes \/\/ TODO: maybe clear the slot instead?\n+      }\n+      if (always_clean || !k->is_loader_present_and_alive() || is_excluded(k)) {\n+        set_type(i, with_status((Klass*)nullptr, p));\n+      }\n@@ -332,0 +359,7 @@\n+void TypeStackSlotEntries::metaspace_pointers_do(MetaspaceClosure* it) {\n+  for (int i = 0; i < _number_of_entries; i++) {\n+    Klass** k = (Klass**)type_adr(i); \/\/ tagged\n+    it->push(k);\n+  }\n+}\n+\n@@ -335,2 +369,7 @@\n-  if (k != nullptr && (always_clean || !k->is_loader_alive())) {\n-    set_type(with_status((Klass*)nullptr, p));\n+  if (k != nullptr) {\n+    if (!always_clean && k->is_instance_klass() && InstanceKlass::cast(k)->is_not_initialized()) {\n+      return; \/\/ skip not-yet-initialized classes \/\/ TODO: maybe clear the slot instead?\n+    }\n+    if (always_clean || !k->is_loader_present_and_alive() || is_excluded(k)) {\n+      set_type(with_status((Klass*)nullptr, p));\n+    }\n@@ -340,0 +379,5 @@\n+void ReturnTypeEntry::metaspace_pointers_do(MetaspaceClosure* it) {\n+  Klass** k = (Klass**)type_adr(); \/\/ tagged\n+  it->push(k);\n+}\n+\n@@ -415,2 +459,7 @@\n-    if (p != nullptr && (always_clean || !p->is_loader_alive())) {\n-      clear_row(row);\n+    if (p != nullptr) {\n+      if (!always_clean && p->is_instance_klass() && InstanceKlass::cast(p)->is_not_initialized()) {\n+        continue; \/\/ skip not-yet-initialized classes \/\/ TODO: maybe clear the slot instead?\n+      }\n+      if (always_clean || !p->is_loader_present_and_alive() || is_excluded(p)) {\n+        clear_row(row);\n+      }\n@@ -421,0 +470,7 @@\n+void ReceiverTypeData::metaspace_pointers_do(MetaspaceClosure *it) {\n+  for (uint row = 0; row < row_limit(); row++) {\n+    Klass** recv = (Klass**)intptr_at_adr(receiver_cell_index(row));\n+    it->push(recv);\n+  }\n+}\n+\n@@ -649,0 +705,5 @@\n+void SpeculativeTrapData::metaspace_pointers_do(MetaspaceClosure* it) {\n+  Method** m = (Method**)intptr_at_adr(speculative_trap_method);\n+  it->push(m);\n+}\n+\n@@ -1226,1 +1287,0 @@\n-    _extra_data_lock(Mutex::nosafepoint, \"MDOExtraData_lock\"),\n@@ -1229,1 +1289,2 @@\n-  initialize();\n+    _extra_data_lock = nullptr;\n+    initialize();\n@@ -1232,0 +1293,7 @@\n+#if INCLUDE_CDS\n+MethodData::MethodData() {\n+  \/\/ Used by cppVtables.cpp only\n+  assert(CDSConfig::is_dumping_static_archive() || UseSharedSpaces, \"only for CDS\");\n+}\n+#endif\n+\n@@ -1367,1 +1435,1 @@\n-  return CompilationPolicy::is_mature(_method);\n+  return CompilationPolicy::is_mature(const_cast<MethodData*>(this));\n@@ -1555,1 +1623,2 @@\n-  ConditionalMutexLocker ml(extra_data_lock(), !extra_data_lock()->owned_by_self(),\n+  Mutex* lock = const_cast<MethodData*>(this)->extra_data_lock();\n+  ConditionalMutexLocker ml(lock, !lock->owned_by_self(),\n@@ -1728,1 +1797,1 @@\n-  log_trace(cds)(\"Iter(MethodData): %p\", this);\n+  log_trace(aot, training)(\"Iter(MethodData): %p for %p %s\", this, _method, _method->name_and_sig_as_C_string());\n@@ -1730,0 +1799,18 @@\n+  if (_parameters_type_data_di != no_parameters) {\n+    parameters_type_data()->metaspace_pointers_do(it);\n+  }\n+  for (ProfileData* data = first_data(); is_valid(data); data = next_data(data)) {\n+    data->metaspace_pointers_do(it);\n+  }\n+  for (DataLayout* dp = extra_data_base();\n+                   dp < extra_data_limit();\n+                   dp = MethodData::next_extra(dp)) {\n+    if (dp->tag() == DataLayout::speculative_trap_data_tag) {\n+      ResourceMark rm;\n+      SpeculativeTrapData* data = new SpeculativeTrapData(dp);\n+      data->metaspace_pointers_do(it);\n+    } else if (dp->tag() == DataLayout::no_tag ||\n+               dp->tag() == DataLayout::arg_info_data_tag) {\n+      break;\n+    }\n+  }\n@@ -1761,0 +1848,3 @@\n+    if (!_always_clean && m->method_holder()->is_instance_klass() && InstanceKlass::cast(m->method_holder())->is_not_initialized()) {\n+      return true; \/\/ TODO: treat as unloaded instead?\n+    }\n@@ -1772,0 +1862,14 @@\n+Mutex* MethodData::extra_data_lock() {\n+  Mutex* lock = Atomic::load(&_extra_data_lock);\n+  if (lock == nullptr) {\n+    \/\/ This lock could be acquired while we are holding DumpTimeTable_lock\/nosafepoint\n+    lock = new Mutex(Mutex::nosafepoint-1, \"MDOExtraData_lock\");\n+    Mutex* old = Atomic::cmpxchg(&_extra_data_lock, (Mutex*)nullptr, lock);\n+    if (old != nullptr) {\n+      \/\/ Another thread created the lock before us. Use that lock instead.\n+      delete lock;\n+      return old;\n+    }\n+  }\n+  return lock;\n+}\n@@ -1788,1 +1892,1 @@\n-      if (!cl->is_live(m)) {\n+      if (is_excluded(m->method_holder()) || !cl->is_live(m)) {\n@@ -1892,0 +1996,10 @@\n+#if INCLUDE_CDS\n+void MethodData::remove_unshareable_info() {\n+  _extra_data_lock = nullptr;\n+}\n+\n+void MethodData::restore_unshareable_info(TRAPS) {\n+  \/\/_extra_data_lock = new Mutex(Mutex::nosafepoint, \"MDOExtraData_lock\");\n+}\n+#endif \/\/ INCLUDE_CDS\n+\n@@ -1898,1 +2012,1 @@\n-    assert(self->extra_data_lock()->owned_by_self(), \"must have lock\");\n+    assert(self->extra_data_lock()->owned_by_self() || CDSConfig::is_dumping_archive(), \"must have lock\");\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":127,"deletions":13,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"oops\/oop.hpp\"\n@@ -205,0 +204,3 @@\n+  intptr_t* cell_at_adr(int index) const {\n+    return const_cast<intptr_t*>(&_cells[index]);\n+  }\n@@ -348,0 +350,4 @@\n+  intptr_t* intptr_at_adr(int index) const {\n+    assert(0 <= index && index < cell_count(), \"oob\");\n+    return data()->cell_at_adr(index);\n+  }\n@@ -365,6 +371,0 @@\n-  void set_oop_at(int index, oop value) {\n-    set_intptr_at(index, cast_from_oop<intptr_t>(value));\n-  }\n-  oop oop_at(int index) const {\n-    return cast_to_oop(intptr_at(index));\n-  }\n@@ -491,1 +491,4 @@\n-  \/\/ CI translation: ProfileData can represent both MethodDataOop data\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {}\n+\n+    \/\/ CI translation: ProfileData can represent both MethodDataOop data\n@@ -856,0 +859,5 @@\n+  intptr_t* type_adr(int i) const {\n+    assert(i >= 0 && i < _number_of_entries, \"oob\");\n+    return _pd->intptr_at_adr(type_offset_in_cells(i));\n+  }\n+\n@@ -877,0 +885,3 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it);\n+\n@@ -901,0 +912,4 @@\n+  intptr_t* type_adr() const {\n+    return _pd->intptr_at_adr(_base_off);\n+  }\n+\n@@ -920,0 +935,3 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it);\n+\n@@ -1111,0 +1129,10 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {\n+    if (has_arguments()) {\n+      _args.metaspace_pointers_do(it);\n+    }\n+    if (has_return()) {\n+      _ret.metaspace_pointers_do(it);\n+    }\n+  }\n+\n@@ -1221,0 +1249,3 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it);\n+\n@@ -1386,0 +1417,11 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {\n+    ReceiverTypeData::metaspace_pointers_do(it);\n+    if (has_arguments()) {\n+      _args.metaspace_pointers_do(it);\n+    }\n+    if (has_return()) {\n+      _ret.metaspace_pointers_do(it);\n+    }\n+  }\n+\n@@ -1569,4 +1611,0 @@\n-  oop array_oop_at(int index) const {\n-    int aindex = index + array_start_off_set;\n-    return oop_at(aindex);\n-  }\n@@ -1785,0 +1823,5 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it) {\n+    _parameters.metaspace_pointers_do(it);\n+  }\n+\n@@ -1855,0 +1898,3 @@\n+  \/\/ CDS support\n+  virtual void metaspace_pointers_do(MetaspaceClosure* it);\n+\n@@ -1965,1 +2011,1 @@\n-  Mutex _extra_data_lock;\n+  Mutex* volatile _extra_data_lock;\n@@ -1972,0 +2018,2 @@\n+  MethodData();\n+\n@@ -2269,0 +2317,5 @@\n+#if INCLUDE_CDS\n+  void remove_unshareable_info();\n+  void restore_unshareable_info(TRAPS);\n+#endif\n+\n@@ -2507,1 +2560,1 @@\n-  Mutex* extra_data_lock() const { return const_cast<Mutex*>(&_extra_data_lock); }\n+  Mutex* extra_data_lock();\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":67,"deletions":14,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -0,0 +1,794 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"ci\/ciEnv.hpp\"\n+#include \"ci\/ciMetadata.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/metaspaceShared.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/compactHashtable.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"compiler\/compileTask.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/methodCounters.hpp\"\n+#include \"oops\/trainingData.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/javaThread.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+TrainingData::TrainingDataSet TrainingData::_training_data_set(1024, 0x3fffffff);\n+TrainingData::TrainingDataDictionary TrainingData::_archived_training_data_dictionary;\n+TrainingData::TrainingDataDictionary TrainingData::_archived_training_data_dictionary_for_dumping;\n+TrainingData::DumptimeTrainingDataDictionary* TrainingData::_dumptime_training_data_dictionary = nullptr;\n+int TrainingData::TrainingDataLocker::_lock_mode;\n+volatile bool TrainingData::TrainingDataLocker::_snapshot = false;\n+\n+MethodTrainingData::MethodTrainingData() {\n+  \/\/ Used by cppVtables.cpp only\n+  assert(CDSConfig::is_dumping_static_archive() || UseSharedSpaces, \"only for CDS\");\n+}\n+\n+KlassTrainingData::KlassTrainingData() {\n+  \/\/ Used by cppVtables.cpp only\n+  assert(CDSConfig::is_dumping_static_archive() || UseSharedSpaces, \"only for CDS\");\n+}\n+\n+CompileTrainingData::CompileTrainingData() : _level(-1), _compile_id(-1) {\n+  \/\/ Used by cppVtables.cpp only\n+  assert(CDSConfig::is_dumping_static_archive() || UseSharedSpaces, \"only for CDS\");\n+}\n+\n+void TrainingData::initialize() {\n+  \/\/ this is a nop if training modes are not enabled\n+  if (have_data() || need_data()) {\n+    \/\/ Data structures that we have do not currently support iterative training. So you cannot replay\n+    \/\/ and train at the same time. Going forward we may want to adjust iteration\/search to enable that.\n+    guarantee(have_data() != need_data(), \"Iterative training is not supported\");\n+    TrainingDataLocker::initialize();\n+  }\n+}\n+\n+static void verify_archived_entry(TrainingData* td, const TrainingData::Key* k) {\n+  guarantee(TrainingData::Key::can_compute_cds_hash(k), \"\");\n+  TrainingData* td1 = TrainingData::lookup_archived_training_data(k);\n+  guarantee(td == td1, \"\");\n+}\n+\n+void TrainingData::verify() {\n+  if (TrainingData::have_data()) {\n+    archived_training_data_dictionary()->iterate([&](TrainingData* td) {\n+      if (td->is_KlassTrainingData()) {\n+        KlassTrainingData* ktd = td->as_KlassTrainingData();\n+        if (ktd->has_holder() && ktd->holder()->is_loaded()) {\n+          Key k(ktd->holder());\n+          verify_archived_entry(td, &k);\n+        }\n+        ktd->verify();\n+      } else if (td->is_MethodTrainingData()) {\n+        MethodTrainingData* mtd = td->as_MethodTrainingData();\n+        if (mtd->has_holder() && mtd->holder()->method_holder()->is_loaded()) {\n+          Key k(mtd->holder());\n+          verify_archived_entry(td, &k);\n+        }\n+        mtd->verify();\n+      } else if (td->is_CompileTrainingData()) {\n+        td->as_CompileTrainingData()->verify();\n+      }\n+    });\n+  }\n+}\n+\n+MethodTrainingData* MethodTrainingData::make(const methodHandle& method, bool null_if_not_found, bool use_cache) {\n+  MethodTrainingData* mtd = nullptr;\n+  if (!have_data() && !need_data()) {\n+    return mtd;\n+  }\n+  \/\/ Try grabbing the cached value first.\n+  \/\/ Cache value is stored in MethodCounters and the following are the\n+  \/\/ possible states:\n+  \/\/ 1. Cached value is method_training_data_sentinel().\n+  \/\/    This is an initial state and needs a full lookup.\n+  \/\/ 2. Cached value is null.\n+  \/\/    Lookup failed the last time, if we don't plan to create a new TD object,\n+  \/\/    i.e. null_if_no_found == true, then just return a null.\n+  \/\/ 3. Cache value is not null.\n+  \/\/    Return it, the value of training_data_lookup_failed doesn't matter.\n+  MethodCounters* mcs = method->method_counters();\n+  if (mcs != nullptr) {\n+    mtd = mcs->method_training_data();\n+    if (mtd != nullptr && mtd != mcs->method_training_data_sentinel()) {\n+      return mtd;\n+    }\n+    if (null_if_not_found && mtd == nullptr) {\n+      assert(mtd == nullptr, \"No training data found\");\n+      return nullptr;\n+    }\n+  } else if (use_cache) {\n+    mcs = Method::build_method_counters(Thread::current(), method());\n+  }\n+\n+  TrainingData* td = nullptr;\n+\n+  Key key(method());\n+  if (have_data()) {\n+    td = lookup_archived_training_data(&key);\n+    if (td != nullptr) {\n+      mtd = td->as_MethodTrainingData();\n+    } else {\n+      mtd = nullptr;\n+    }\n+    \/\/ Cache the pointer to MTD in MethodCounters for faster lookup (could be null if not found)\n+    method->init_training_data(mtd);\n+  }\n+\n+  if (need_data()) {\n+    TrainingDataLocker l;\n+    td = training_data_set()->find(&key);\n+    if (td == nullptr) {\n+      if (!null_if_not_found) {\n+        KlassTrainingData* ktd = KlassTrainingData::make(method->method_holder());\n+        if (ktd == nullptr) {\n+          return nullptr; \/\/ allocation failure\n+        }\n+        mtd = MethodTrainingData::allocate(method(), ktd);\n+        if (mtd == nullptr) {\n+          return nullptr; \/\/ allocation failure\n+        }\n+        td = training_data_set()->install(mtd);\n+        assert(td == mtd, \"\");\n+      } else {\n+        mtd = nullptr;\n+      }\n+    } else {\n+      mtd = td->as_MethodTrainingData();\n+    }\n+    \/\/ Cache the pointer to MTD in MethodCounters for faster lookup (could be null if not found)\n+    method->init_training_data(mtd);\n+  }\n+\n+  return mtd;\n+}\n+\n+void MethodTrainingData::print_on(outputStream* st, bool name_only) const {\n+  if (has_holder()) {\n+    _klass->print_on(st, true);\n+    st->print(\".\");\n+    name()->print_symbol_on(st);\n+    signature()->print_symbol_on(st);\n+  }\n+  if (name_only) {\n+    return;\n+  }\n+  if (!has_holder()) {\n+    st->print(\"[SYM]\");\n+  }\n+  if (_level_mask) {\n+    st->print(\" LM%d\", _level_mask);\n+  }\n+  st->print(\" mc=%p mdo=%p\", _final_counters, _final_profile);\n+}\n+\n+CompileTrainingData* CompileTrainingData::make(CompileTask* task) {\n+  int level = task->comp_level();\n+  int compile_id = task->compile_id();\n+  Thread* thread = Thread::current();\n+  methodHandle m(thread, task->method());\n+  if (m->method_holder() == nullptr) {\n+    return nullptr; \/\/ do not record (dynamically generated method)\n+  }\n+  MethodTrainingData* mtd = MethodTrainingData::make(m);\n+  if (mtd == nullptr) {\n+    return nullptr; \/\/ allocation failure\n+  }\n+  mtd->notice_compilation(level);\n+\n+  TrainingDataLocker l;\n+  CompileTrainingData* ctd = CompileTrainingData::allocate(mtd, level, compile_id);\n+  if (ctd != nullptr) {\n+    CompileTrainingData*& last_ctd = mtd->_last_toplevel_compiles[level - 1];\n+    if (last_ctd != nullptr) {\n+      assert(mtd->highest_top_level() >= level, \"consistency\");\n+      if (last_ctd->compile_id() < compile_id) {\n+        last_ctd->clear_init_deps();\n+        last_ctd = ctd;\n+      }\n+    } else {\n+       last_ctd = ctd;\n+       mtd->notice_toplevel_compilation(level);\n+    }\n+  }\n+  return ctd;\n+}\n+\n+\n+void CompileTrainingData::dec_init_deps_left(KlassTrainingData* ktd) {\n+  LogStreamHandle(Trace, training) log;\n+  if (log.is_enabled()) {\n+    log.print(\"CTD \"); print_on(&log); log.cr();\n+    log.print(\"KTD \"); ktd->print_on(&log); log.cr();\n+  }\n+  assert(ktd!= nullptr && ktd->has_holder(), \"\");\n+  assert(_init_deps.contains(ktd), \"\");\n+  assert(_init_deps_left > 0, \"\");\n+\n+  uint init_deps_left1 = Atomic::sub(&_init_deps_left, 1);\n+\n+  if (log.is_enabled()) {\n+    uint init_deps_left2 = compute_init_deps_left();\n+    log.print(\"init_deps_left: %d (%d)\", init_deps_left1, init_deps_left2);\n+    ktd->print_on(&log, true);\n+  }\n+}\n+\n+uint CompileTrainingData::compute_init_deps_left(bool count_initialized) {\n+  int left = 0;\n+  for (int i = 0; i < _init_deps.length(); i++) {\n+    KlassTrainingData* ktd = _init_deps.at(i);\n+    \/\/ Ignore symbolic refs and already initialized classes (unless explicitly requested).\n+    if (ktd->has_holder()) {\n+      InstanceKlass* holder = ktd->holder();\n+      if (!ktd->holder()->is_initialized() || count_initialized) {\n+        ++left;\n+      } else if (holder->is_shared_unregistered_class()) {\n+        Key k(holder);\n+        if (CDS_ONLY(!Key::can_compute_cds_hash(&k)) NOT_CDS(true)) {\n+          ++left;\n+        }\n+      }\n+    }\n+  }\n+  return left;\n+}\n+\n+void CompileTrainingData::print_on(outputStream* st, bool name_only) const {\n+  _method->print_on(st, true);\n+  st->print(\"#%dL%d\", _compile_id, _level);\n+  if (name_only) {\n+    return;\n+  }\n+  if (_init_deps.length() > 0) {\n+    if (_init_deps_left > 0) {\n+      st->print(\" udeps=%d\", _init_deps_left);\n+    }\n+    for (int i = 0, len = _init_deps.length(); i < len; i++) {\n+      st->print(\" dep:\");\n+      _init_deps.at(i)->print_on(st, true);\n+    }\n+  }\n+}\n+\n+void CompileTrainingData::notice_inlined_method(CompileTask* task,\n+                                                const methodHandle& method) {\n+  MethodTrainingData* mtd = MethodTrainingData::make(method);\n+  if (mtd != nullptr) {\n+    mtd->notice_compilation(task->comp_level(), true);\n+  }\n+}\n+\n+void CompileTrainingData::notice_jit_observation(ciEnv* env, ciBaseObject* what) {\n+  \/\/ A JIT is starting to look at class k.\n+  \/\/ We could follow the queries that it is making, but it is\n+  \/\/ simpler to assume, conservatively, that the JIT will\n+  \/\/ eventually depend on the initialization state of k.\n+  CompileTask* task = env->task();\n+  assert(task != nullptr, \"\");\n+  Method* method = task->method();\n+  InstanceKlass* compiling_klass = method->method_holder();\n+  if (what->is_metadata()) {\n+    ciMetadata* md = what->as_metadata();\n+    if (md->is_loaded() && md->is_instance_klass()) {\n+      ciInstanceKlass* cik = md->as_instance_klass();\n+\n+      if (cik->is_initialized()) {\n+        InstanceKlass* ik = md->as_instance_klass()->get_instanceKlass();\n+        KlassTrainingData* ktd = KlassTrainingData::make(ik);\n+        if (ktd == nullptr) {\n+          \/\/ Allocation failure or snapshot in progress\n+          return;\n+        }\n+        \/\/ This JIT task is (probably) requesting that ik be initialized,\n+        \/\/ so add him to my _init_deps list.\n+        TrainingDataLocker l;\n+        add_init_dep(ktd);\n+      }\n+    }\n+  }\n+}\n+\n+void KlassTrainingData::prepare(Visitor& visitor) {\n+  if (visitor.is_visited(this)) {\n+    return;\n+  }\n+  visitor.visit(this);\n+  ClassLoaderData* loader_data = nullptr;\n+  if (_holder != nullptr) {\n+    loader_data = _holder->class_loader_data();\n+  } else {\n+    loader_data = java_lang_ClassLoader::loader_data(SystemDictionary::java_system_loader()); \/\/ default CLD\n+  }\n+  _comp_deps.prepare(loader_data);\n+}\n+\n+void MethodTrainingData::prepare(Visitor& visitor) {\n+  if (visitor.is_visited(this)) {\n+    return;\n+  }\n+  visitor.visit(this);\n+  klass()->prepare(visitor);\n+  if (has_holder()) {\n+    _final_counters = holder()->method_counters();\n+    _final_profile  = holder()->method_data();\n+    assert(_final_profile == nullptr || _final_profile->method() == holder(), \"\");\n+  }\n+  for (int i = 0; i < CompLevel_count - 1; i++) {\n+    CompileTrainingData* ctd = _last_toplevel_compiles[i];\n+    if (ctd != nullptr) {\n+      ctd->prepare(visitor);\n+    }\n+  }\n+}\n+\n+void CompileTrainingData::prepare(Visitor& visitor) {\n+  if (visitor.is_visited(this)) {\n+    return;\n+  }\n+  visitor.visit(this);\n+  method()->prepare(visitor);\n+  ClassLoaderData* loader_data = _method->klass()->class_loader_data();\n+  _init_deps.prepare(loader_data);\n+  _ci_records.prepare(loader_data);\n+}\n+\n+KlassTrainingData* KlassTrainingData::make(InstanceKlass* holder, bool null_if_not_found) {\n+  Key key(holder);\n+  TrainingData* td = CDS_ONLY(have_data() ? lookup_archived_training_data(&key) :) nullptr;\n+  KlassTrainingData* ktd = nullptr;\n+  if (td != nullptr) {\n+    ktd = td->as_KlassTrainingData();\n+    guarantee(!ktd->has_holder() || ktd->holder() == holder, \"\");\n+    if (ktd->has_holder()) {\n+      return ktd;\n+    } else {\n+      ktd = nullptr;\n+    }\n+  }\n+  if (need_data()) {\n+    TrainingDataLocker l;\n+    td = training_data_set()->find(&key);\n+    if (td == nullptr) {\n+      if (null_if_not_found) {\n+        return nullptr;\n+      }\n+      ktd = KlassTrainingData::allocate(holder);\n+      if (ktd == nullptr) {\n+        return nullptr; \/\/ allocation failure\n+      }\n+      td = training_data_set()->install(ktd);\n+      assert(ktd == td, \"\");\n+    } else {\n+      ktd = td->as_KlassTrainingData();\n+      guarantee(ktd->holder() != nullptr, \"null holder\");\n+    }\n+    assert(ktd != nullptr, \"\");\n+    guarantee(ktd->holder() == holder, \"\");\n+  }\n+  return ktd;\n+}\n+\n+void KlassTrainingData::print_on(outputStream* st, bool name_only) const {\n+  if (has_holder()) {\n+    name()->print_symbol_on(st);\n+    switch (holder()->init_state()) {\n+      case InstanceKlass::allocated:            st->print(\"[A]\"); break;\n+      case InstanceKlass::loaded:               st->print(\"[D]\"); break;\n+      case InstanceKlass::linked:               st->print(\"[L]\"); break;\n+      case InstanceKlass::being_initialized:    st->print(\"[i]\"); break;\n+      case InstanceKlass::fully_initialized:                      break;\n+      case InstanceKlass::initialization_error: st->print(\"[E]\"); break;\n+      default: fatal(\"unknown state: %d\", holder()->init_state());\n+    }\n+    if (holder()->is_interface()) {\n+      st->print(\"I\");\n+    }\n+  } else {\n+    st->print(\"[SYM]\");\n+  }\n+  if (name_only) {\n+    return;\n+  }\n+  if (_comp_deps.length() > 0) {\n+    for (int i = 0, len = _comp_deps.length(); i < len; i++) {\n+      st->print(\" dep:\");\n+      _comp_deps.at(i)->print_on(st, true);\n+    }\n+  }\n+}\n+\n+KlassTrainingData::KlassTrainingData(InstanceKlass* klass) : TrainingData(klass) {\n+  if (holder() == klass) {\n+    return;   \/\/ no change to make\n+  }\n+\n+  jobject hmj = _holder_mirror;\n+  if (hmj != nullptr) {   \/\/ clear out previous handle, if any\n+    _holder_mirror = nullptr;\n+    assert(JNIHandles::is_global_handle(hmj), \"\");\n+    JNIHandles::destroy_global(hmj);\n+  }\n+\n+  if (klass != nullptr) {\n+    Handle hm(JavaThread::current(), klass->java_mirror());\n+    hmj = JNIHandles::make_global(hm);\n+    Atomic::release_store(&_holder_mirror, hmj);\n+  }\n+\n+  Atomic::release_store(&_holder, const_cast<InstanceKlass*>(klass));\n+  assert(holder() == klass, \"\");\n+}\n+\n+void KlassTrainingData::notice_fully_initialized() {\n+  ResourceMark rm;\n+  assert(has_holder(), \"\");\n+  assert(holder()->is_initialized(), \"wrong state: %s %s\",\n+         holder()->name()->as_C_string(), holder()->init_state_name());\n+\n+  TrainingDataLocker l; \/\/ Not a real lock if we don't collect the data,\n+                        \/\/ that's why we need the atomic decrement below.\n+  for (int i = 0; i < comp_dep_count(); i++) {\n+    comp_dep(i)->dec_init_deps_left(this);\n+  }\n+  holder()->set_has_init_deps_processed();\n+}\n+\n+void TrainingData::init_dumptime_table(TRAPS) {\n+  precond((!assembling_data() && !need_data()) || need_data() != assembling_data());\n+  if (assembling_data()) {\n+    _dumptime_training_data_dictionary = new DumptimeTrainingDataDictionary();\n+    _archived_training_data_dictionary.iterate([&](TrainingData* record) {\n+      _dumptime_training_data_dictionary->append(record);\n+    });\n+  }\n+  if (need_data()) {\n+    _dumptime_training_data_dictionary = new DumptimeTrainingDataDictionary();\n+    TrainingDataLocker l;\n+    TrainingDataLocker::snapshot();\n+\n+    ResourceMark rm;\n+    Visitor visitor(training_data_set()->size());\n+    training_data_set()->iterate([&](TrainingData* td) {\n+      td->prepare(visitor);\n+      if (!td->is_CompileTrainingData()) {\n+        _dumptime_training_data_dictionary->append(td);\n+      }\n+    });\n+\n+    if (AOTVerifyTrainingData) {\n+      training_data_set()->verify();\n+    }\n+  }\n+}\n+\n+void TrainingData::iterate_roots(MetaspaceClosure* it) {\n+  if (_dumptime_training_data_dictionary != nullptr) {\n+    for (int i = 0; i < _dumptime_training_data_dictionary->length(); i++) {\n+      _dumptime_training_data_dictionary->at(i).metaspace_pointers_do(it);\n+    }\n+  }\n+}\n+\n+void TrainingData::dump_training_data() {\n+  if (_dumptime_training_data_dictionary != nullptr) {\n+    CompactHashtableStats stats;\n+    _archived_training_data_dictionary_for_dumping.reset();\n+    CompactHashtableWriter writer(_dumptime_training_data_dictionary->length(), &stats);\n+    for (int i = 0; i < _dumptime_training_data_dictionary->length(); i++) {\n+      TrainingData* td = _dumptime_training_data_dictionary->at(i).training_data();\n+#ifdef ASSERT\n+      for (int j = i+1; j < _dumptime_training_data_dictionary->length(); j++) {\n+        TrainingData* td1 = _dumptime_training_data_dictionary->at(j).training_data();\n+        assert(!TrainingData::Key::equals(td1, td->key(), -1), \"conflict\");\n+      }\n+#endif \/\/ ASSERT\n+      td = ArchiveBuilder::current()->get_buffered_addr(td);\n+      uint hash = TrainingData::Key::cds_hash(td->key());\n+      u4 delta = ArchiveBuilder::current()->buffer_to_offset_u4((address)td);\n+      writer.add(hash, delta);\n+    }\n+    writer.dump(&_archived_training_data_dictionary_for_dumping, \"training data dictionary\");\n+  }\n+}\n+\n+void TrainingData::cleanup_training_data() {\n+  if (_dumptime_training_data_dictionary != nullptr) {\n+    ResourceMark rm;\n+    Visitor visitor(_dumptime_training_data_dictionary->length());\n+    for (int i = 0; i < _dumptime_training_data_dictionary->length(); i++) {\n+      TrainingData* td = _dumptime_training_data_dictionary->at(i).training_data();\n+      td->cleanup(visitor);\n+    }\n+    \/\/ Throw away all elements with empty keys\n+    int j = 0;\n+    for (int i = 0; i < _dumptime_training_data_dictionary->length(); i++) {\n+      TrainingData* td = _dumptime_training_data_dictionary->at(i).training_data();\n+      if (td->key()->is_empty()) {\n+        continue;\n+      }\n+      if (i != j) { \/\/ no need to copy if it's the same\n+        _dumptime_training_data_dictionary->at_put(j, td);\n+      }\n+      j++;\n+    }\n+    _dumptime_training_data_dictionary->trunc_to(j);\n+  }\n+}\n+\n+void KlassTrainingData::cleanup(Visitor& visitor) {\n+  if (visitor.is_visited(this)) {\n+    return;\n+  }\n+  visitor.visit(this);\n+  if (has_holder()) {\n+    bool is_excluded = !holder()->is_loaded() || SystemDictionaryShared::check_for_exclusion(holder(), nullptr);\n+    if (is_excluded) {\n+      ResourceMark rm;\n+      log_debug(aot, training)(\"Cleanup KTD %s\", name()->as_klass_external_name());\n+      _holder = nullptr;\n+      key()->make_empty();\n+    }\n+  }\n+  for (int i = 0; i < _comp_deps.length(); i++) {\n+    _comp_deps.at(i)->cleanup(visitor);\n+  }\n+}\n+\n+void MethodTrainingData::cleanup(Visitor& visitor) {\n+  if (visitor.is_visited(this)) {\n+    return;\n+  }\n+  visitor.visit(this);\n+  if (has_holder()) {\n+    if (SystemDictionaryShared::check_for_exclusion(holder()->method_holder(), nullptr)) {\n+      log_debug(aot, training)(\"Cleanup MTD %s::%s\", name()->as_klass_external_name(), signature()->as_utf8());\n+      if (_final_profile != nullptr && _final_profile->method() != _holder) {\n+        log_warning(aot, training)(\"Stale MDO for  %s::%s\", name()->as_klass_external_name(), signature()->as_utf8());\n+      }\n+      _final_profile = nullptr;\n+      _final_counters = nullptr;\n+      _holder = nullptr;\n+      key()->make_empty();\n+    }\n+  }\n+  for (int i = 0; i < CompLevel_count - 1; i++) {\n+    CompileTrainingData* ctd = _last_toplevel_compiles[i];\n+    if (ctd != nullptr) {\n+      ctd->cleanup(visitor);\n+    }\n+  }\n+}\n+\n+void KlassTrainingData::verify() {\n+  for (int i = 0; i < comp_dep_count(); i++) {\n+    CompileTrainingData* ctd = comp_dep(i);\n+    if (!ctd->_init_deps.contains(this)) {\n+      print_on(tty); tty->cr();\n+      ctd->print_on(tty); tty->cr();\n+    }\n+    guarantee(ctd->_init_deps.contains(this), \"\");\n+  }\n+}\n+\n+void MethodTrainingData::verify() {\n+  iterate_compiles([](CompileTrainingData* ctd) {\n+    ctd->verify();\n+\n+    int init_deps_left1 = ctd->init_deps_left();\n+    int init_deps_left2 = ctd->compute_init_deps_left();\n+\n+    if (init_deps_left1 != init_deps_left2) {\n+      ctd->print_on(tty); tty->cr();\n+    }\n+    guarantee(init_deps_left1 == init_deps_left2, \"mismatch: %d %d %d\",\n+              init_deps_left1, init_deps_left2, ctd->init_deps_left());\n+  });\n+}\n+\n+void CompileTrainingData::verify() {\n+  for (int i = 0; i < init_dep_count(); i++) {\n+    KlassTrainingData* ktd = init_dep(i);\n+    if (ktd->has_holder() && ktd->holder()->is_shared_unregistered_class()) {\n+      LogStreamHandle(Warning, training) log;\n+      if (log.is_enabled()) {\n+        ResourceMark rm;\n+        log.print(\"CTD \"); print_value_on(&log);\n+        log.print(\" depends on unregistered class %s\", ktd->holder()->name()->as_C_string());\n+      }\n+    }\n+    if (!ktd->_comp_deps.contains(this)) {\n+      print_on(tty); tty->cr();\n+      ktd->print_on(tty); tty->cr();\n+    }\n+    guarantee(ktd->_comp_deps.contains(this), \"\");\n+  }\n+}\n+\n+void CompileTrainingData::cleanup(Visitor& visitor) {\n+  if (visitor.is_visited(this)) {\n+    return;\n+  }\n+  visitor.visit(this);\n+  method()->cleanup(visitor);\n+}\n+\n+void TrainingData::serialize(SerializeClosure* soc) {\n+  if (soc->writing()) {\n+    _archived_training_data_dictionary_for_dumping.serialize_header(soc);\n+  } else {\n+    _archived_training_data_dictionary.serialize_header(soc);\n+  }\n+}\n+\n+class TrainingDataPrinter : StackObj {\n+  outputStream* _st;\n+  int _index;\n+public:\n+  TrainingDataPrinter(outputStream* st) : _st(st), _index(0) {}\n+  void do_value(TrainingData* td) {\n+    const char* type = (td->is_KlassTrainingData()   ? \"K\" :\n+                        td->is_MethodTrainingData()  ? \"M\" :\n+                        td->is_CompileTrainingData() ? \"C\" : \"?\");\n+    _st->print(\"%4d: %p %s \", _index++, td, type);\n+    td->print_on(_st);\n+    _st->cr();\n+    if (td->is_KlassTrainingData()) {\n+      td->as_KlassTrainingData()->iterate_comp_deps([&](CompileTrainingData* ctd) {\n+        ResourceMark rm;\n+        _st->print_raw(\"  C \");\n+        ctd->print_on(_st);\n+        _st->cr();\n+      });\n+    } else if (td->is_MethodTrainingData()) {\n+      td->as_MethodTrainingData()->iterate_compiles([&](CompileTrainingData* ctd) {\n+        ResourceMark rm;\n+        _st->print_raw(\"  C \");\n+        ctd->print_on(_st);\n+        _st->cr();\n+      });\n+    } else if (td->is_CompileTrainingData()) {\n+      \/\/ ?\n+    }\n+  }\n+};\n+\n+void TrainingData::print_archived_training_data_on(outputStream* st) {\n+  st->print_cr(\"Archived TrainingData Dictionary\");\n+  TrainingDataPrinter tdp(st);\n+  TrainingDataLocker::initialize();\n+  _archived_training_data_dictionary.iterate(&tdp);\n+}\n+\n+void TrainingData::Key::metaspace_pointers_do(MetaspaceClosure *iter) {\n+  iter->push(const_cast<Metadata**>(&_meta));\n+}\n+\n+void TrainingData::metaspace_pointers_do(MetaspaceClosure* iter) {\n+  _key.metaspace_pointers_do(iter);\n+}\n+\n+bool TrainingData::Key::can_compute_cds_hash(const Key* const& k) {\n+  return k->meta() == nullptr || MetaspaceObj::is_shared(k->meta());\n+}\n+\n+uint TrainingData::Key::cds_hash(const Key* const& k) {\n+  return SystemDictionaryShared::hash_for_shared_dictionary((address)k->meta());\n+}\n+\n+TrainingData* TrainingData::lookup_archived_training_data(const Key* k) {\n+  \/\/ For this to work, all components of the key must be in shared metaspace.\n+  if (!TrainingData::Key::can_compute_cds_hash(k) || _archived_training_data_dictionary.empty()) {\n+    return nullptr;\n+  }\n+  uint hash = TrainingData::Key::cds_hash(k);\n+  TrainingData* td = _archived_training_data_dictionary.lookup(k, hash, -1 \/*unused*\/);\n+  if (td != nullptr) {\n+    if ((td->is_KlassTrainingData()  && td->as_KlassTrainingData()->has_holder()) ||\n+        (td->is_MethodTrainingData() && td->as_MethodTrainingData()->has_holder())) {\n+      return td;\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+template <typename T>\n+void TrainingData::DepList<T>::metaspace_pointers_do(MetaspaceClosure* iter) {\n+  iter->push(&_deps);\n+}\n+\n+void KlassTrainingData::metaspace_pointers_do(MetaspaceClosure* iter) {\n+  log_trace(aot, training)(\"Iter(KlassTrainingData): %p\", this);\n+  TrainingData::metaspace_pointers_do(iter);\n+  _comp_deps.metaspace_pointers_do(iter);\n+  iter->push(&_holder);\n+}\n+\n+void MethodTrainingData::metaspace_pointers_do(MetaspaceClosure* iter) {\n+  log_trace(aot, training)(\"Iter(MethodTrainingData): %p\", this);\n+  TrainingData::metaspace_pointers_do(iter);\n+  iter->push(&_klass);\n+  iter->push((Method**)&_holder);\n+  for (int i = 0; i < CompLevel_count - 1; i++) {\n+    iter->push(&_last_toplevel_compiles[i]);\n+  }\n+  iter->push(&_final_profile);\n+  iter->push(&_final_counters);\n+}\n+\n+void CompileTrainingData::metaspace_pointers_do(MetaspaceClosure* iter) {\n+  log_trace(aot, training)(\"Iter(CompileTrainingData): %p\", this);\n+  TrainingData::metaspace_pointers_do(iter);\n+  _init_deps.metaspace_pointers_do(iter);\n+  _ci_records.metaspace_pointers_do(iter);\n+  iter->push(&_method);\n+}\n+\n+template <typename T>\n+void TrainingData::DepList<T>::prepare(ClassLoaderData* loader_data) {\n+  if (_deps == nullptr && _deps_dyn != nullptr) {\n+    int len = _deps_dyn->length();\n+    _deps = MetadataFactory::new_array_from_c_heap<T>(len, mtClassShared);\n+    for (int i = 0; i < len; i++) {\n+      _deps->at_put(i, _deps_dyn->at(i)); \/\/ copy\n+    }\n+  }\n+}\n+\n+void KlassTrainingData::remove_unshareable_info() {\n+  TrainingData::remove_unshareable_info();\n+  _holder_mirror = nullptr;\n+  _comp_deps.remove_unshareable_info();\n+}\n+\n+void MethodTrainingData::remove_unshareable_info() {\n+  TrainingData::remove_unshareable_info();\n+  if (_final_counters != nullptr) {\n+    _final_counters->remove_unshareable_info();\n+  }\n+  if (_final_profile != nullptr) {\n+    _final_profile->remove_unshareable_info();\n+  }\n+}\n+\n+void CompileTrainingData::remove_unshareable_info() {\n+  TrainingData::remove_unshareable_info();\n+  _init_deps.remove_unshareable_info();\n+  _ci_records.remove_unshareable_info();\n+  _init_deps_left = compute_init_deps_left(true);\n+}\n","filename":"src\/hotspot\/share\/oops\/trainingData.cpp","additions":794,"deletions":0,"binary":false,"changes":794,"status":"added"},{"patch":"@@ -0,0 +1,825 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_TRAININGDATA_HPP\n+#define SHARE_OOPS_TRAININGDATA_HPP\n+\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/compactHashtable.hpp\"\n+#include \"compiler\/compilerDefinitions.hpp\"\n+#include \"compiler\/compiler_globals.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/resizeableResourceHash.hpp\"\n+\n+class ciEnv;\n+class ciBaseObject;\n+class CompileTask;\n+class CompileTrainingData;\n+class KlassTrainingData;\n+class MethodTrainingData;\n+\n+\/\/ Base class for all the training data varieties\n+class TrainingData : public Metadata {\n+  friend KlassTrainingData;\n+  friend MethodTrainingData;\n+  friend CompileTrainingData;\n+public:\n+  \/\/ Key is used to insert any TrainingData (TD) object into a hash tables. The key is currently a\n+  \/\/ pointer to a metaspace object the TD is associated with. For example,\n+  \/\/ for KlassTrainingData it's an InstanceKlass, for MethodTrainingData it's a Method.\n+  \/\/ The utility of the these hash tables is to be able to find a TD object for a given metaspace\n+  \/\/ metaspace object.\n+  class Key {\n+    mutable Metadata* _meta;\n+    \/\/ These guys can get to my constructors:\n+    friend TrainingData;\n+    friend KlassTrainingData;\n+    friend MethodTrainingData;\n+    friend CompileTrainingData;\n+\n+    \/\/ The empty key\n+    Key() : _meta(nullptr) { }\n+    bool is_empty() const { return _meta == nullptr; }\n+  public:\n+    Key(Metadata* meta) : _meta(meta) { }\n+\n+    static bool can_compute_cds_hash(const Key* const& k);\n+    static uint cds_hash(const Key* const& k);\n+    static unsigned hash(const Key* const& k) {\n+      return primitive_hash(k->meta());\n+    }\n+    static bool equals(const Key* const& k1, const Key* const& k2) {\n+      return k1->meta() == k2->meta();\n+    }\n+    static inline bool equals(TrainingData* value, const TrainingData::Key* key, int unused) {\n+      return equals(value->key(), key);\n+    }\n+    int cmp(const Key* that) const {\n+      auto m1 = this->meta();\n+      auto m2 = that->meta();\n+      if (m1 < m2) return -1;\n+      if (m1 > m2) return +1;\n+      return 0;\n+    }\n+    Metadata* meta() const { return _meta; }\n+    void metaspace_pointers_do(MetaspaceClosure *iter);\n+    void make_empty() const { _meta = nullptr; }\n+  };\n+\n+  \/\/ TrainingDataLocker is used to guard read\/write operations on non-MT-safe data structures.\n+  \/\/ It supports recursive locking and a read-only mode (in which case no locks are taken).\n+  \/\/ It is also a part of the TD collection termination protocol (see the \"spanshot\" field).\n+  class TrainingDataLocker {\n+    static volatile bool _snapshot; \/\/ If true we're not allocating new training data\n+    static int _lock_mode;\n+    const bool _recursive;\n+    static void lock() {\n+#if INCLUDE_CDS\n+      assert(_lock_mode != 0, \"Forgot to call TrainingDataLocker::initialize()\");\n+      if (_lock_mode > 0) {\n+        TrainingData_lock->lock();\n+      }\n+#endif\n+    }\n+    static void unlock() {\n+#if INCLUDE_CDS\n+      if (_lock_mode > 0) {\n+        TrainingData_lock->unlock();\n+      }\n+#endif\n+    }\n+    static bool safely_locked() {\n+#if INCLUDE_CDS\n+      assert(_lock_mode != 0, \"Forgot to call TrainingDataLocker::initialize()\");\n+      if (_lock_mode > 0) {\n+        return is_self_locked();\n+      } else {\n+        return true;\n+      }\n+#else\n+      return true;\n+#endif\n+    }\n+    static bool is_self_locked() {\n+      return CDS_ONLY(TrainingData_lock->owned_by_self()) NOT_CDS(false);\n+    }\n+\n+  public:\n+    static void snapshot() {\n+#if INCLUDE_CDS\n+      assert_locked();\n+      _snapshot = true;\n+#endif\n+    }\n+    static bool can_add() {\n+#if INCLUDE_CDS\n+      assert_locked();\n+      return !_snapshot;\n+#else\n+      return false;\n+#endif\n+    }\n+    static void initialize() {\n+#if INCLUDE_CDS\n+      _lock_mode = need_data() ? +1 : -1;   \/\/ if -1, we go lock-free\n+#endif\n+    }\n+    static void assert_locked() {\n+      assert(safely_locked(), \"use under TrainingDataLocker\");\n+    }\n+    static void assert_can_add() {\n+      assert(can_add(), \"Cannot add TrainingData objects\");\n+    }\n+    TrainingDataLocker() : _recursive(is_self_locked()) {\n+      if (!_recursive) {\n+        lock();\n+      }\n+    }\n+    ~TrainingDataLocker() {\n+      if (!_recursive) {\n+        unlock();\n+      }\n+    }\n+  };\n+\n+  \/\/ A set of TD objects that we collect during the training run.\n+  class TrainingDataSet {\n+    friend TrainingData;\n+    ResizeableResourceHashtable<const Key*, TrainingData*,\n+                                AnyObj::C_HEAP, MemTag::mtCompiler,\n+                                &TrainingData::Key::hash,\n+                                &TrainingData::Key::equals>\n+      _table;\n+\n+  public:\n+    template<typename... Arg>\n+    TrainingDataSet(Arg... arg)\n+      : _table(arg...) {\n+    }\n+    TrainingData* find(const Key* key) const {\n+      TrainingDataLocker::assert_locked();\n+      if (TrainingDataLocker::can_add()) {\n+        auto res = _table.get(key);\n+        return res == nullptr ? nullptr : *res;\n+      }\n+      return nullptr;\n+    }\n+    bool remove(const Key* key) {\n+      return _table.remove(key);\n+    }\n+    TrainingData* install(TrainingData* td) {\n+      TrainingDataLocker::assert_locked();\n+      TrainingDataLocker::assert_can_add();\n+      auto key = td->key();\n+      if (key->is_empty()) {\n+        return td;  \/\/ unkeyed TD not installed\n+      }\n+      bool created = false;\n+      auto prior = _table.put_if_absent(key, td, &created);\n+      if (prior == nullptr || *prior == td) {\n+        return td;\n+      }\n+      assert(false, \"no pre-existing elements allowed\");\n+      return *prior;\n+    }\n+    template<typename Function>\n+    void iterate(const Function& fn) const { \/\/ lambda enabled API\n+      iterate(const_cast<Function&>(fn));\n+    }\n+    template<typename Function>\n+    void iterate(Function& fn) const { \/\/ lambda enabled API\n+      return _table.iterate_all([&](const TrainingData::Key* k, TrainingData* td) { fn(td); });\n+    }\n+    int size() const { return _table.number_of_entries(); }\n+\n+    void verify() const {\n+      TrainingDataLocker::assert_locked();\n+      iterate([&](TrainingData* td) { td->verify(); });\n+    }\n+  };\n+\n+  \/\/ A widget to ensure that we visit TD object only once (TD objects can have pointer to\n+  \/\/ other TD object that are sometimes circular).\n+  class Visitor {\n+    ResizeableResourceHashtable<TrainingData*, bool> _visited;\n+  public:\n+    Visitor(unsigned size) : _visited(size, 0x3fffffff) { }\n+    bool is_visited(TrainingData* td) {\n+      return _visited.contains(td);\n+    }\n+    void visit(TrainingData* td) {\n+      bool created;\n+      _visited.put_if_absent(td, &created);\n+    }\n+  };\n+\n+  typedef OffsetCompactHashtable<const TrainingData::Key*, TrainingData*, TrainingData::Key::equals> TrainingDataDictionary;\n+private:\n+  Key _key;\n+\n+  \/\/ just forward all constructor arguments to the embedded key\n+  template<typename... Arg>\n+  TrainingData(Arg... arg)\n+    : _key(arg...) { }\n+\n+  \/\/ Container for recording TD during training run\n+  static TrainingDataSet _training_data_set;\n+  \/\/ Containter for replaying the training data (read-only, populated from the AOT image)\n+  static TrainingDataDictionary _archived_training_data_dictionary;\n+  \/\/ Container used for writing the AOT image\n+  static TrainingDataDictionary _archived_training_data_dictionary_for_dumping;\n+  class DumpTimeTrainingDataInfo {\n+    TrainingData* _training_data;\n+  public:\n+    DumpTimeTrainingDataInfo() : DumpTimeTrainingDataInfo(nullptr) {}\n+    DumpTimeTrainingDataInfo(TrainingData* training_data) : _training_data(training_data) {}\n+    void metaspace_pointers_do(MetaspaceClosure* it) {\n+      it->push(&_training_data);\n+    }\n+    TrainingData* training_data() {\n+      return _training_data;\n+    }\n+  };\n+  typedef GrowableArrayCHeap<DumpTimeTrainingDataInfo, mtClassShared> DumptimeTrainingDataDictionary;\n+  \/\/ A temporary container that is used to accumulate and filter TD during dumping\n+  static DumptimeTrainingDataDictionary* _dumptime_training_data_dictionary;\n+\n+  static TrainingDataSet* training_data_set() { return &_training_data_set; }\n+  static TrainingDataDictionary* archived_training_data_dictionary() { return &_archived_training_data_dictionary; }\n+\n+ public:\n+  \/\/ Returns the key under which this TD is installed, or else\n+  \/\/ Key::EMPTY if it is not installed.\n+  const Key* key() const { return &_key; }\n+\n+  static bool have_data() { return AOTReplayTraining;  } \/\/ Going to read\n+  static bool need_data() { return AOTRecordTraining;  } \/\/ Going to write\n+  static bool assembling_data() { return have_data() && CDSConfig::is_dumping_final_static_archive() && CDSConfig::is_dumping_aot_linked_classes(); }\n+\n+  template<typename Function>\n+  static void iterate(const Function& fn) { iterate(const_cast<Function&>(fn)); }\n+\n+  template<typename Function>\n+  static void iterate(Function& fn) { \/\/ lambda enabled API\n+    TrainingDataLocker l;\n+    if (have_data()) {\n+      archived_training_data_dictionary()->iterate(fn);\n+    }\n+    if (need_data()) {\n+      training_data_set()->iterate(fn);\n+    }\n+  }\n+\n+  virtual MethodTrainingData*   as_MethodTrainingData()  const { return nullptr; }\n+  virtual KlassTrainingData*    as_KlassTrainingData()   const { return nullptr; }\n+  virtual CompileTrainingData*  as_CompileTrainingData() const { return nullptr; }\n+  bool is_MethodTrainingData()  const { return as_MethodTrainingData()  != nullptr; }\n+  bool is_KlassTrainingData()   const { return as_KlassTrainingData()   != nullptr; }\n+  bool is_CompileTrainingData() const { return as_CompileTrainingData() != nullptr; }\n+\n+  virtual void prepare(Visitor& visitor) = 0;\n+  virtual void cleanup(Visitor& visitor) = 0;\n+\n+  static void initialize() NOT_CDS_RETURN;\n+\n+  static void verify();\n+\n+  \/\/ Widget for recording dependencies, as an N-to-M graph relation,\n+  \/\/ possibly cyclic.\n+  template<typename E>\n+  class DepList : public StackObj {\n+    GrowableArrayCHeap<E, mtCompiler>* _deps_dyn;\n+    Array<E>*                          _deps;\n+  public:\n+    DepList() {\n+      _deps_dyn = nullptr;\n+      _deps = nullptr;\n+    }\n+\n+    int length() const {\n+      return (_deps_dyn != nullptr ? _deps_dyn->length()\n+              : _deps   != nullptr ? _deps->length()\n+              : 0);\n+    }\n+    E* adr_at(int i) const {\n+      return (_deps_dyn != nullptr ? _deps_dyn->adr_at(i)\n+              : _deps   != nullptr ? _deps->adr_at(i)\n+              : nullptr);\n+    }\n+    E at(int i) const {\n+      assert(i >= 0 && i < length(), \"oob\");\n+      return *adr_at(i);\n+    }\n+    bool append_if_missing(E dep) {\n+      if (_deps_dyn == nullptr) {\n+        _deps_dyn = new GrowableArrayCHeap<E, mtCompiler>(10);\n+        _deps_dyn->append(dep);\n+        return true;\n+      } else {\n+        return _deps_dyn->append_if_missing(dep);\n+      }\n+    }\n+    bool remove_if_existing(E dep) {\n+      if (_deps_dyn != nullptr) {\n+        return _deps_dyn->remove_if_existing(dep);\n+      }\n+      return false;\n+    }\n+    void clear() {\n+      if (_deps_dyn != nullptr)  {\n+        _deps_dyn->clear();\n+      }\n+    }\n+    void append(E dep) {\n+      if (_deps_dyn == nullptr) {\n+        _deps_dyn = new GrowableArrayCHeap<E, mtCompiler>(10);\n+      }\n+      _deps_dyn->append(dep);\n+    }\n+    bool contains(E dep) {\n+      for (int i = 0; i < length(); i++) {\n+        if (dep == at(i)) {\n+          return true; \/\/ found\n+        }\n+      }\n+      return false; \/\/ not found\n+    }\n+\n+#if INCLUDE_CDS\n+    void remove_unshareable_info() {\n+      _deps_dyn = nullptr;\n+    }\n+#endif\n+    void prepare(ClassLoaderData* loader_data);\n+    void metaspace_pointers_do(MetaspaceClosure *iter);\n+  };\n+\n+  virtual void metaspace_pointers_do(MetaspaceClosure *iter);\n+\n+  static void init_dumptime_table(TRAPS);\n+\n+#if INCLUDE_CDS\n+  virtual void remove_unshareable_info() {}\n+  static void iterate_roots(MetaspaceClosure* it);\n+  static void dump_training_data();\n+  static void cleanup_training_data();\n+  static void serialize(SerializeClosure* soc);\n+  static void print_archived_training_data_on(outputStream* st);\n+  static TrainingData* lookup_archived_training_data(const Key* k);\n+#endif\n+\n+  template<typename TrainingDataType, typename... ArgTypes>\n+  static TrainingDataType* allocate(ArgTypes... args) {\n+    assert(need_data() || have_data(), \"\");\n+    if (TrainingDataLocker::can_add()) {\n+      return new (mtClassShared) TrainingDataType(args...);\n+    }\n+    return nullptr;\n+  }\n+};\n+\n+\/\/ Training data that is associated with an InstanceKlass\n+class KlassTrainingData : public TrainingData {\n+  friend TrainingData;\n+  friend CompileTrainingData;\n+\n+  \/\/ Used by CDS. These classes need to access the private default constructor.\n+  template <class T> friend class CppVtableTesterA;\n+  template <class T> friend class CppVtableTesterB;\n+  template <class T> friend class CppVtableCloner;\n+\n+  \/\/ cross-link to live klass, or null if not loaded or encountered yet\n+  InstanceKlass* _holder;\n+  jobject _holder_mirror;   \/\/ extra link to prevent unloading by GC\n+\n+  DepList<CompileTrainingData*> _comp_deps; \/\/ compiles that depend on me\n+\n+  KlassTrainingData();\n+  KlassTrainingData(InstanceKlass* klass);\n+\n+  int comp_dep_count() const {\n+    TrainingDataLocker::assert_locked();\n+    return _comp_deps.length();\n+  }\n+  CompileTrainingData* comp_dep(int i) const {\n+    TrainingDataLocker::assert_locked();\n+    return _comp_deps.at(i);\n+  }\n+  void add_comp_dep(CompileTrainingData* ctd) {\n+    TrainingDataLocker::assert_locked();\n+     _comp_deps.append_if_missing(ctd);\n+  }\n+  void remove_comp_dep(CompileTrainingData* ctd) {\n+    TrainingDataLocker::assert_locked();\n+     _comp_deps.remove_if_existing(ctd);\n+  }\n+\n+ public:\n+  Symbol* name() const {\n+    precond(has_holder());\n+    return holder()->name();\n+  }\n+  bool has_holder()       const { return _holder != nullptr; }\n+  InstanceKlass* holder() const { return _holder; }\n+\n+  static KlassTrainingData* make(InstanceKlass* holder,\n+                                 bool null_if_not_found = false) NOT_CDS_RETURN_(nullptr);\n+  static KlassTrainingData* find(InstanceKlass* holder) {\n+    return make(holder, true);\n+  }\n+  virtual KlassTrainingData* as_KlassTrainingData() const { return const_cast<KlassTrainingData*>(this); };\n+\n+  ClassLoaderData* class_loader_data() {\n+    assert(has_holder(), \"\");\n+    return holder()->class_loader_data();\n+  }\n+  void notice_fully_initialized() NOT_CDS_RETURN;\n+\n+  void print_on(outputStream* st, bool name_only) const;\n+  virtual void print_on(outputStream* st) const { print_on(st, false); }\n+  virtual void print_value_on(outputStream* st) const { print_on(st, true); }\n+\n+  virtual void prepare(Visitor& visitor);\n+  virtual void cleanup(Visitor& visitor) NOT_CDS_RETURN;\n+\n+  MetaspaceObj::Type type() const {\n+    return KlassTrainingDataType;\n+  }\n+\n+#if INCLUDE_CDS\n+  virtual void remove_unshareable_info();\n+#endif\n+\n+  void metaspace_pointers_do(MetaspaceClosure *iter);\n+\n+  int size() const {\n+    return (int)align_metadata_size(align_up(sizeof(KlassTrainingData), BytesPerWord)\/BytesPerWord);\n+  }\n+\n+  const char* internal_name() const {\n+    return \"{ klass training data }\";\n+  };\n+\n+  void verify();\n+\n+  static KlassTrainingData* allocate(InstanceKlass* holder) {\n+    return TrainingData::allocate<KlassTrainingData>(holder);\n+  }\n+\n+  template<typename Function>\n+  void iterate_comp_deps(Function fn) const { \/\/ lambda enabled API\n+    TrainingDataLocker l;\n+    for (int i = 0; i < comp_dep_count(); i++) {\n+      fn(comp_dep(i));\n+    }\n+  }\n+};\n+\n+\/\/ Information about particular JIT tasks.\n+class CompileTrainingData : public TrainingData {\n+  friend TrainingData;\n+  friend KlassTrainingData;\n+\n+  \/\/ Used by CDS. These classes need to access the private default constructor.\n+  template <class T> friend class CppVtableTesterA;\n+  template <class T> friend class CppVtableTesterB;\n+  template <class T> friend class CppVtableCloner;\n+\n+  MethodTrainingData* _method;\n+  const short _level;\n+  const int _compile_id;\n+\n+  \/\/ classes that should be initialized before this JIT task runs\n+  DepList<KlassTrainingData*> _init_deps;\n+  \/\/ Number of uninitialized classes left, when it's 0, all deps are satisfied\n+  volatile int _init_deps_left;\n+\n+public:\n+  \/\/ ciRecords is a generic meachanism to memoize CI responses to arbitary queries. For each function we're interested in we record\n+  \/\/ (return_value, argument_values) tuples in a list. Arguments are allowed to have Metaspace pointers in them.\n+  class ciRecords {\n+    template <typename... Ts> class Arguments {\n+    public:\n+      bool operator==(const Arguments<>&) const { return true; }\n+      void metaspace_pointers_do(MetaspaceClosure *iter) { }\n+    };\n+    template <typename T, typename... Ts> class Arguments<T, Ts...> {\n+    private:\n+      T _first;\n+      Arguments<Ts...> _remaining;\n+\n+    public:\n+      constexpr Arguments(const T& first, const Ts&... remaining) noexcept\n+        : _first(first), _remaining(remaining...) {}\n+      constexpr Arguments() noexcept : _first(), _remaining() {}\n+      bool operator==(const Arguments<T, Ts...>& that) const {\n+        return _first == that._first && _remaining == that._remaining;\n+      }\n+      template<typename U = T, ENABLE_IF(std::is_pointer<U>::value && std::is_base_of<MetaspaceObj, typename std::remove_pointer<U>::type>::value)>\n+      void metaspace_pointers_do(MetaspaceClosure *iter) {\n+        iter->push(&_first);\n+        _remaining.metaspace_pointers_do(iter);\n+      }\n+      template<typename U = T, ENABLE_IF(!(std::is_pointer<U>::value && std::is_base_of<MetaspaceObj, typename std::remove_pointer<U>::type>::value))>\n+      void metaspace_pointers_do(MetaspaceClosure *iter) {\n+        _remaining.metaspace_pointers_do(iter);\n+      }\n+    };\n+\n+    template <typename ReturnType, typename... Args> class ciMemoizedFunction : public StackObj {\n+    public:\n+      class OptionalReturnType {\n+        bool _valid;\n+        ReturnType _result;\n+      public:\n+        OptionalReturnType(bool valid, const ReturnType& result) : _valid(valid), _result(result) {}\n+        bool is_valid() const { return _valid; }\n+        ReturnType result() const { return _result; }\n+      };\n+    private:\n+      typedef Arguments<Args...> ArgumentsType;\n+      class Record : public MetaspaceObj {\n+        ReturnType    _result;\n+        ArgumentsType _arguments;\n+      public:\n+        Record(const ReturnType& result, const ArgumentsType& arguments) : _result(result), _arguments(arguments) {}\n+        Record() { }\n+        ReturnType result() const { return _result; }\n+        ArgumentsType arguments() const { return _arguments; }\n+        bool operator==(const Record& that) { return _arguments == that._arguments; }\n+        void metaspace_pointers_do(MetaspaceClosure *iter) { _arguments.metaspace_pointers_do(iter); }\n+      };\n+      DepList<Record> _data;\n+    public:\n+      OptionalReturnType find(const Args&... args) {\n+        ArgumentsType a(args...);\n+        for (int i = 0; i < _data.length(); i++) {\n+          if (_data.at(i).arguments() == a) {\n+            return OptionalReturnType(true, _data.at(i).result());\n+          }\n+        }\n+        return OptionalReturnType(false, ReturnType());\n+      }\n+      bool append_if_missing(const ReturnType& result, const Args&... args) {\n+        return _data.append_if_missing(Record(result, ArgumentsType(args...)));\n+      }\n+#if INCLUDE_CDS\n+      void remove_unshareable_info() { _data.remove_unshareable_info(); }\n+#endif\n+      void prepare(ClassLoaderData* loader_data) {\n+        _data.prepare(loader_data);\n+      }\n+      void metaspace_pointers_do(MetaspaceClosure *iter) {\n+        _data.metaspace_pointers_do(iter);\n+      }\n+    };\n+\n+\n+public:\n+    \/\/ Record CI answers for the InlineSmallCode heuristic. It is importance since the heuristic is non-commutative and we may want to\n+    \/\/ compile methods in a different order than in the training run.\n+    typedef ciMemoizedFunction<int, MethodTrainingData*> ciMethod__inline_instructions_size_type;\n+    ciMethod__inline_instructions_size_type ciMethod__inline_instructions_size;\n+#if INCLUDE_CDS\n+    void remove_unshareable_info() {\n+      ciMethod__inline_instructions_size.remove_unshareable_info();\n+    }\n+#endif\n+    void prepare(ClassLoaderData* loader_data) {\n+      ciMethod__inline_instructions_size.prepare(loader_data);\n+    }\n+    void metaspace_pointers_do(MetaspaceClosure *iter) {\n+      ciMethod__inline_instructions_size.metaspace_pointers_do(iter);\n+    }\n+  };\n+\n+private:\n+  ciRecords _ci_records;\n+\n+  CompileTrainingData();\n+  CompileTrainingData(MethodTrainingData* mtd,\n+                      int level,\n+                      int compile_id)\n+      : TrainingData(),  \/\/ empty key\n+        _method(mtd), _level(level), _compile_id(compile_id), _init_deps_left(0) { }\n+public:\n+  ciRecords& ci_records() { return _ci_records; }\n+  static CompileTrainingData* make(CompileTask* task) NOT_CDS_RETURN_(nullptr);\n+\n+  virtual CompileTrainingData* as_CompileTrainingData() const { return const_cast<CompileTrainingData*>(this); };\n+\n+  MethodTrainingData* method() const { return _method; }\n+\n+  int level() const { return _level; }\n+\n+  int compile_id() const { return _compile_id; }\n+\n+  int init_dep_count() const {\n+    TrainingDataLocker::assert_locked();\n+    return _init_deps.length();\n+  }\n+  KlassTrainingData* init_dep(int i) const {\n+    TrainingDataLocker::assert_locked();\n+    return _init_deps.at(i);\n+  }\n+  void add_init_dep(KlassTrainingData* ktd) {\n+    TrainingDataLocker::assert_locked();\n+    ktd->add_comp_dep(this);\n+    _init_deps.append_if_missing(ktd);\n+  }\n+  void clear_init_deps() {\n+    TrainingDataLocker::assert_locked();\n+    for (int i = 0; i < _init_deps.length(); i++) {\n+      _init_deps.at(i)->remove_comp_dep(this);\n+    }\n+    _init_deps.clear();\n+  }\n+  void dec_init_deps_left(KlassTrainingData* ktd);\n+  int init_deps_left() const {\n+    return Atomic::load(&_init_deps_left);\n+  }\n+  uint compute_init_deps_left(bool count_initialized = false);\n+\n+  void notice_inlined_method(CompileTask* task, const methodHandle& method) NOT_CDS_RETURN;\n+\n+  \/\/ The JIT looks at classes and objects too and can depend on their state.\n+  \/\/ These simple calls just report the *possibility* of an observation.\n+  void notice_jit_observation(ciEnv* env, ciBaseObject* what) NOT_CDS_RETURN;\n+\n+  virtual void prepare(Visitor& visitor);\n+  virtual void cleanup(Visitor& visitor) NOT_CDS_RETURN;\n+\n+  void print_on(outputStream* st, bool name_only) const;\n+  virtual void print_on(outputStream* st) const { print_on(st, false); }\n+  virtual void print_value_on(outputStream* st) const { print_on(st, true); }\n+\n+#if INCLUDE_CDS\n+  virtual void remove_unshareable_info();\n+#endif\n+\n+  virtual void metaspace_pointers_do(MetaspaceClosure* iter);\n+  virtual MetaspaceObj::Type type() const { return CompileTrainingDataType; }\n+\n+  virtual const char* internal_name() const {\n+    return \"{ compile training data }\";\n+  };\n+\n+  virtual int size() const {\n+    return (int)align_metadata_size(align_up(sizeof(CompileTrainingData), BytesPerWord)\/BytesPerWord);\n+  }\n+\n+  void verify();\n+\n+  static CompileTrainingData* allocate(MethodTrainingData* mtd, int level, int compile_id) {\n+    return TrainingData::allocate<CompileTrainingData>(mtd, level, compile_id);\n+  }\n+};\n+\n+\/\/ Record information about a method at the time compilation is requested.\n+class MethodTrainingData : public TrainingData {\n+  friend TrainingData;\n+  friend CompileTrainingData;\n+\n+  \/\/ Used by CDS. These classes need to access the private default constructor.\n+  template <class T> friend class CppVtableTesterA;\n+  template <class T> friend class CppVtableTesterB;\n+  template <class T> friend class CppVtableCloner;\n+\n+  KlassTrainingData* _klass;\n+  Method* _holder;\n+  CompileTrainingData* _last_toplevel_compiles[CompLevel_count - 1];\n+  int _highest_top_level;\n+  int _level_mask;  \/\/ bit-set of all possible levels\n+  bool _was_toplevel;\n+  \/\/ metadata snapshots of final state:\n+  MethodCounters* _final_counters;\n+  MethodData*     _final_profile;\n+\n+  MethodTrainingData();\n+  MethodTrainingData(Method* method, KlassTrainingData* ktd) : TrainingData(method) {\n+    _klass = ktd;\n+    _holder = method;\n+    for (int i = 0; i < CompLevel_count - 1; i++) {\n+      _last_toplevel_compiles[i] = nullptr;\n+    }\n+    _highest_top_level = CompLevel_none;\n+    _level_mask = 0;\n+    _was_toplevel = false;\n+  }\n+\n+  static int level_mask(int level) {\n+    return ((level & 0xF) != level ? 0 : 1 << level);\n+  }\n+\n+ public:\n+  KlassTrainingData* klass()  const { return _klass; }\n+  bool has_holder()           const { return _holder != nullptr; }\n+  Method* holder()            const { return _holder; }\n+  bool only_inlined()         const { return !_was_toplevel; }\n+  bool saw_level(CompLevel l) const { return (_level_mask & level_mask(l)) != 0; }\n+  int highest_top_level()     const { return _highest_top_level; }\n+  MethodData* final_profile() const { return _final_profile; }\n+\n+  Symbol* name() const {\n+    precond(has_holder());\n+    return holder()->name();\n+  }\n+  Symbol* signature() const {\n+    precond(has_holder());\n+    return holder()->signature();\n+  }\n+\n+  CompileTrainingData* last_toplevel_compile(int level) const {\n+    if (level > CompLevel_none) {\n+      return _last_toplevel_compiles[level - 1];\n+    }\n+    return nullptr;\n+  }\n+\n+  void notice_compilation(int level, bool inlined = false) {\n+    if (!inlined) {\n+      _was_toplevel = true;\n+    }\n+    _level_mask |= level_mask(level);\n+  }\n+\n+  void notice_toplevel_compilation(int level) {\n+    _highest_top_level = MAX2(_highest_top_level, level);\n+  }\n+\n+  static MethodTrainingData* make(const methodHandle& method,\n+                                  bool null_if_not_found = false,\n+                                  bool use_cache = true) NOT_CDS_RETURN_(nullptr);\n+  static MethodTrainingData* find_fast(const methodHandle& method) { return make(method, true, true); }\n+  static MethodTrainingData* find(const methodHandle& method) { return make(method, true, false); }\n+\n+  virtual MethodTrainingData* as_MethodTrainingData() const {\n+    return const_cast<MethodTrainingData*>(this);\n+  };\n+\n+  void print_on(outputStream* st, bool name_only) const;\n+  virtual void print_on(outputStream* st) const { print_on(st, false); }\n+  virtual void print_value_on(outputStream* st) const { print_on(st, true); }\n+\n+  virtual void prepare(Visitor& visitor);\n+  virtual void cleanup(Visitor& visitor) NOT_CDS_RETURN;\n+\n+  template<typename Function>\n+  void iterate_compiles(Function fn) const { \/\/ lambda enabled API\n+    for (int i = 0; i < CompLevel_count - 1; i++) {\n+      CompileTrainingData* ctd = _last_toplevel_compiles[i];\n+      if (ctd != nullptr) {\n+        fn(ctd);\n+      }\n+    }\n+  }\n+\n+  virtual void metaspace_pointers_do(MetaspaceClosure* iter);\n+  virtual MetaspaceObj::Type type() const { return MethodTrainingDataType; }\n+\n+#if INCLUDE_CDS\n+  virtual void remove_unshareable_info();\n+#endif\n+\n+  virtual int size() const {\n+    return (int)align_metadata_size(align_up(sizeof(MethodTrainingData), BytesPerWord)\/BytesPerWord);\n+  }\n+\n+  virtual const char* internal_name() const {\n+    return \"{ method training data }\";\n+  };\n+\n+  void verify();\n+\n+  static MethodTrainingData* allocate(Method* m, KlassTrainingData* ktd) {\n+    return TrainingData::allocate<MethodTrainingData>(m, ktd);\n+  }\n+};\n+#endif \/\/ SHARE_OOPS_TRAININGDATA_HPP\n","filename":"src\/hotspot\/share\/oops\/trainingData.hpp","additions":825,"deletions":0,"binary":false,"changes":825,"status":"added"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/trainingData.hpp\"\n@@ -191,0 +192,5 @@\n+  \/\/ Initialize TrainingData only we're recording\/replaying\n+  if (TrainingData::have_data() || TrainingData::need_data()) {\n+   TrainingData::initialize();\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -93,0 +93,2 @@\n+Mutex*   TrainingData_lock            = nullptr;\n+Monitor* TrainingReplayQueue_lock     = nullptr;\n@@ -260,0 +262,2 @@\n+  MUTEX_DEFL(TrainingData_lock               , PaddedMutex  , MethodCompileQueue_lock);\n+  MUTEX_DEFN(TrainingReplayQueue_lock        , PaddedMonitor, safepoint);\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -86,0 +86,2 @@\n+extern Mutex*   TrainingData_lock;               \/\/ a lock used when accessing training records\n+extern Monitor* TrainingReplayQueue_lock;        \/\/ a lock held when class are added\/removed to the training replay queue\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -812,0 +812,5 @@\n+  \/\/ Initiate replay training processing once preloading is over.\n+  CompileBroker::init_training_replay();\n+\n+  AOTLinkedClassBulkLoader::replay_training_at_init_for_preloaded_classes(CHECK_JNI_ERR);\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1027,0 +1027,1 @@\n+        declare_type(TrainingReplayThread, JavaThread)                    \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -119,1 +119,2 @@\n-    metadataTypeArray = new Type[9];\n+    metadataTypeArray = new Type[11];\n+    \/\/ The order needs to match up with CPP_VTABLE_TYPES_DO in src\/hotspot\/share\/cds\/cppVtables.cpp\n@@ -128,2 +129,4 @@\n-    metadataTypeArray[7] = db.lookupType(\"ObjArrayKlass\");\n-    metadataTypeArray[8] = db.lookupType(\"TypeArrayKlass\");\n+    metadataTypeArray[9] = db.lookupType(\"MethodData\");\n+    metadataTypeArray[8] = db.lookupType(\"MethodCounters\");\n+    metadataTypeArray[9] = db.lookupType(\"ObjArrayKlass\");\n+    metadataTypeArray[10] = db.lookupType(\"TypeArrayKlass\");\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/FileMapInfo.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -154,0 +154,1 @@\n+            virtualConstructor.addMapping(\"TrainingReplayThread\", HiddenJavaThread.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,63 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/**\n+ * @test\n+ * @summary Sanity test of combinations of the diagnostic flags [+-]AOTRecordTraining and [+-]AOTReplayTraining\n+ * @requires vm.cds\n+ * @comment work around JDK-8345635\n+ * @requires !vm.jvmci.enabled\n+ * @requires vm.cds.supports.aot.class.linking\n+ * @requires vm.flagless\n+ * @library \/test\/lib \/test\/setup_aot\n+ * @build AOTProfileFlags JavacBenchApp\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller -jar app.jar\n+ *                 JavacBenchApp\n+ *                 JavacBenchApp$ClassFile\n+ *                 JavacBenchApp$FileManager\n+ *                 JavacBenchApp$SourceFile\n+ * @run driver AOTProfileFlags\n+ *\/\n+\n+import jdk.test.lib.cds.SimpleCDSAppTester;\n+\n+public class AOTProfileFlags {\n+    public static void main(String... args) throws Exception {\n+        for (int i = 0; i < 2; i++) {\n+            for (int j = 0; j < 2; j ++) {\n+                if (i == 1 && j == 1) {\n+                    \/\/ They are both on by default. No need to test this combination.\n+                    break;\n+                }\n+                SimpleCDSAppTester.of(\"AOTProfileFlags\" + i + \"\" + j)\n+                    .addVmArgs(\"-XX:+UnlockDiagnosticVMOptions\",\n+                               \"-XX:\" + (i == 0 ? \"-\" : \"+\") + \"AOTRecordTraining\",\n+                               \"-XX:\" + (j == 0 ? \"-\" : \"+\") + \"AOTReplayTraining\")\n+                    .classpath(\"app.jar\")\n+                    .appCommandLine(\"JavacBenchApp\", \"10\")\n+                    .runAOTWorkflow();\n+            }\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/aotProfile\/AOTProfileFlags.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"}]}