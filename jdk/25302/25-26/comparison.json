{"files":[{"patch":"@@ -173,1 +173,1 @@\n-JVM_ENTRY_NO_ENV(jboolean, jfr_set_cpu_throttle(JNIEnv* env, jclass jvm, jdouble rate, jboolean autoadapt))\n+JVM_ENTRY_NO_ENV(void, jfr_set_cpu_throttle(JNIEnv* env, jclass jvm, jdouble rate, jboolean autoadapt))\n@@ -176,1 +176,0 @@\n-  return JNI_TRUE;\n","filename":"src\/hotspot\/share\/jfr\/jni\/jfrJniMethod.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -132,1 +132,1 @@\n-jboolean JNICALL jfr_set_cpu_throttle(JNIEnv* env, jclass jvm, jdouble rate, jboolean autoadapt);\n+void JNICALL jfr_set_cpu_throttle(JNIEnv* env, jclass jvm, jdouble rate, jboolean autoadapt);\n","filename":"src\/hotspot\/share\/jfr\/jni\/jfrJniMethod.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-      (char*)\"setCPUThrottle\", (char*)\"(DZ)Z\", (void*)jfr_set_cpu_throttle,\n+      (char*)\"setCPUThrottle\", (char*)\"(DZ)V\", (void*)jfr_set_cpu_throttle,\n","filename":"src\/hotspot\/share\/jfr\/jni\/jfrJniMethodRegistration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-#include \"runtime\/safepointMechanism.hpp\"\n@@ -58,0 +57,2 @@\n+  assert(raw_thread != nullptr, \"invariant\");\n+  assert(raw_thread->is_Java_thread(), \"invariant\");\n@@ -59,3 +60,0 @@\n-  if (raw_thread == nullptr || !raw_thread->is_Java_thread()) { \/\/ this can happen due to the high level of parralelism\n-    return nullptr;\n-  }\n@@ -102,1 +100,1 @@\n-  return Atomic::load(&_head);\n+  return Atomic::load_acquire(&_head);\n@@ -121,4 +119,0 @@\n-bool JfrCPUTimeTraceQueue::is_full() const {\n-  return Atomic::load_acquire(&_head) >= _capacity;\n-}\n-\n@@ -126,1 +120,1 @@\n-  return Atomic::load_acquire(&_head) == 0;\n+  return Atomic::load(&_head) == 0;\n@@ -130,1 +124,1 @@\n-  return Atomic::load_acquire(&_lost_samples);\n+  return Atomic::load(&_lost_samples);\n@@ -139,1 +133,1 @@\n-  s4 lost_samples = Atomic::load_acquire(&_lost_samples);\n+  s4 lost_samples = Atomic::load(&_lost_samples);\n@@ -147,1 +141,1 @@\n-void JfrCPUTimeTraceQueue::ensure_capacity(u4 capacity) {\n+void JfrCPUTimeTraceQueue::resize(u4 capacity) {\n@@ -153,1 +147,1 @@\n-void JfrCPUTimeTraceQueue::ensure_capacity_for_period(u4 period_millis) {\n+void JfrCPUTimeTraceQueue::resize_for_period(u4 period_millis) {\n@@ -158,1 +152,1 @@\n-  ensure_capacity(capacity);\n+  resize(capacity);\n@@ -200,1 +194,1 @@\n-  \/\/ sample all threads that are in native state (and requested to be sampled)\n+  \/\/ process the queues for all threads that are in native state (and requested to be sampled)\n@@ -237,1 +231,1 @@\n-  Atomic::release_store(&_is_async_processing_of_cpu_time_jfr_requests_triggered, true);\n+  Atomic::store(&_is_async_processing_of_cpu_time_jfr_requests_triggered, true);\n@@ -246,1 +240,1 @@\n-  tl->cpu_time_jfr_queue().ensure_capacity_for_period(_current_sampling_period_ns \/ 1000000);\n+  tl->cpu_time_jfr_queue().resize_for_period(_current_sampling_period_ns \/ 1000000);\n@@ -317,2 +311,2 @@\n-    if (Atomic::load_acquire(&_is_async_processing_of_cpu_time_jfr_requests_triggered)) {\n-      Atomic::release_store(&_is_async_processing_of_cpu_time_jfr_requests_triggered, false);\n+    if (Atomic::load(&_is_async_processing_of_cpu_time_jfr_requests_triggered)) {\n+      Atomic::store(&_is_async_processing_of_cpu_time_jfr_requests_triggered, false);\n@@ -332,2 +326,3 @@\n-    if (tl != nullptr && tl->wants_async_processing_of_cpu_time_jfr_requests()) {\n-      if (!tl->acquire_cpu_time_jfr_native_lock()) {\n+    if (tl->wants_async_processing_of_cpu_time_jfr_requests()) {\n+      if (!tl->try_acquire_cpu_time_jfr_dequeue_lock()) {\n+        \/\/ the thread is already processing requests at its safepoint\n@@ -343,7 +338,0 @@\n-\/\/ equals operator for JfrSampleRequest\n-inline bool operator==(const JfrSampleRequest& lhs, const JfrSampleRequest& rhs) {\n-  return lhs._sample_sp == rhs._sample_sp &&\n-         lhs._sample_pc == rhs._sample_pc &&\n-         lhs._sample_bcp == rhs._sample_bcp;\n-}\n-\n@@ -359,1 +347,0 @@\n-  assert(!queue.is_empty(), \"invariant\");\n@@ -364,0 +351,1 @@\n+  bool in_continuation = is_in_continuation(top_frame, thread);\n@@ -367,2 +355,2 @@\n-    traceid tid = JfrThreadLocal::thread_id(thread);\n-    if (!stacktrace.record_inner(thread, top_frame, is_in_continuation(top_frame, thread), 0)) {\n+    const traceid tid = in_continuation ? tl->vthread_id_with_epoch_update(thread) : JfrThreadLocal::jvm_thread_id(thread);\n+    if (!stacktrace.record_inner(thread, top_frame, in_continuation, 0)) {\n@@ -377,0 +365,1 @@\n+    const traceid tid = in_continuation ? tl->vthread_id_with_epoch_update(thread) : JfrThreadLocal::jvm_thread_id(thread);\n@@ -378,1 +367,1 @@\n-    JfrCPUTimeThreadSampling::send_lost_event(now, JfrThreadLocal::thread_id(thread), queue.get_and_reset_lost_samples());\n+    JfrCPUTimeThreadSampling::send_lost_event(now, tid, queue.get_and_reset_lost_samples());\n@@ -413,1 +402,1 @@\n-    log_info(jfr)(\"CPU thread sampler sent %zu events, lost %d, biased %zu\\n\", Atomic::load(&count), Atomic::load(&_lost_samples_sum), Atomic::load(&biased_count));\n+    log_debug(jfr)(\"CPU thread sampler sent %zu events, lost %d, biased %zu\\n\", Atomic::load(&count), Atomic::load(&_lost_samples_sum), Atomic::load(&biased_count));\n@@ -536,2 +525,0 @@\n-volatile size_t count__ = 0;\n-\n@@ -555,2 +542,1 @@\n-  if (!check_state(jt) ||\n-      jt->is_JfrRecorder_thread()) {\n+  if (!check_state(jt)) {\n@@ -558,1 +544,0 @@\n-      tl->set_do_async_processing_of_cpu_time_jfr_requests(false);\n@@ -574,2 +559,4 @@\n-    tl->set_has_cpu_time_jfr_requests(true);\n-    SafepointMechanism::arm_local_poll_release(jt);\n+    if (queue.size() == 1) {\n+      tl->set_has_cpu_time_jfr_requests(true);\n+      SafepointMechanism::arm_local_poll_release(jt);\n+    }\n@@ -580,2 +567,1 @@\n-  if (jt->thread_state() == _thread_in_native &&\n-      queue.size() > queue.capacity() * 2 \/ 3) {\n+  if (jt->thread_state() == _thread_in_native) {\n@@ -583,2 +569,4 @@\n-    tl->set_do_async_processing_of_cpu_time_jfr_requests(true);\n-    JfrCPUTimeThreadSampling::trigger_async_processing_of_cpu_time_jfr_requests();\n+    if (queue.size() == queue.capacity() * 2 \/ 3) {\n+      tl->set_do_async_processing_of_cpu_time_jfr_requests(true);\n+      JfrCPUTimeThreadSampling::trigger_async_processing_of_cpu_time_jfr_requests();\n+    }\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrCPUTimeThreadSampler.cpp","additions":33,"deletions":45,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -79,2 +79,0 @@\n-  bool is_full() const;\n-\n@@ -90,1 +88,1 @@\n-  void ensure_capacity(u4 capacity);\n+  void resize(u4 capacity);\n@@ -92,1 +90,1 @@\n-  void ensure_capacity_for_period(u4 period_millis);\n+  void resize_for_period(u4 period_millis);\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrCPUTimeThreadSampler.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -233,0 +233,2 @@\n+  biased = true;\n+\n@@ -246,1 +248,0 @@\n-      biased = true;\n@@ -248,0 +249,2 @@\n+    } else {\n+      biased = false;\n@@ -250,1 +253,0 @@\n-    biased = true;\n@@ -363,0 +365,1 @@\n+    tl->set_do_async_processing_of_cpu_time_jfr_requests(false);\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrThreadSampling.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -587,2 +587,12 @@\n-bool JfrThreadLocal::acquire_cpu_time_jfr_native_lock() {\n-  return Atomic::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, NATIVE) == UNLOCKED;\n+bool JfrThreadLocal::try_acquire_cpu_time_jfr_dequeue_lock() {\n+  CPUTimeLockState got;\n+  while (true)  {\n+    CPUTimeLockState got = Atomic::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, DEQUEUE);\n+    if (got == UNLOCKED) {\n+      return true; \/\/ successfully locked for dequeue\n+    }\n+    if (got == DEQUEUE) {\n+      return false; \/\/ already locked for dequeue\n+    }\n+    \/\/ else wait for the lock to be released from a signal handler\n+  }\n@@ -604,1 +614,1 @@\n-  return Atomic::load(&_has_cpu_time_jfr_requests);\n+  return Atomic::load_acquire(&_has_cpu_time_jfr_requests);\n@@ -612,1 +622,1 @@\n-  cpu_time_jfr_queue().ensure_capacity(0);\n+  cpu_time_jfr_queue().resize(0);\n@@ -620,1 +630,1 @@\n-  return Atomic::load(&_do_async_processing_of_cpu_time_jfr_requests);\n+  return Atomic::load_acquire(&_do_async_processing_of_cpu_time_jfr_requests);\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrThreadLocal.cpp","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -94,3 +94,1 @@\n-    DEQUEUE,\n-    \/\/ locked for sampling a thread in native state\n-    NATIVE\n+    DEQUEUE\n@@ -376,1 +374,0 @@\n-  \/\/ - NATIVE: lock for writing events for threads in native state\n@@ -385,1 +382,1 @@\n-  bool acquire_cpu_time_jfr_native_lock();\n+  bool try_acquire_cpu_time_jfr_dequeue_lock();\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrThreadLocal.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n-    public static native boolean setCPUThrottle(double rate, boolean autoadapt);\n+    public static native void setCPUThrottle(double rate, boolean autoadapt);\n","filename":"src\/jdk.jfr\/share\/classes\/jdk\/jfr\/internal\/JVM.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}