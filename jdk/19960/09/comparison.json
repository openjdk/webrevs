{"files":[{"patch":"@@ -1965,0 +1965,7 @@\n+  \/\/ Vector AES instructions (Zvkned extension)\n+  INSN(vaesem_vv,   0b1110111, 0b010, 0b00010, 0b101000);\n+  INSN(vaesef_vv,   0b1110111, 0b010, 0b00011, 0b101000);\n+\n+  INSN(vaesdm_vv,   0b1110111, 0b010, 0b00000, 0b101000);\n+  INSN(vaesdf_vv,   0b1110111, 0b010, 0b00001, 0b101000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2279,0 +2279,165 @@\n+  void generate_aes_loadkeys(const Register &key, VectorRegister *working_vregs, int rounds) {\n+    const int step = 16;\n+    for (int i = 0; i < rounds; i++) {\n+      __ vle32_v(working_vregs[i], key);\n+      __ vrev8_v(working_vregs[i], working_vregs[i]);\n+      __ addi(key, key, step);\n+    }\n+  }\n+\n+  void generate_aes_encrypt(const VectorRegister &res, VectorRegister *working_vregs, int rounds) {\n+    assert(rounds <= 15, \"rounds should be less than or equal to working_vregs size\");\n+\n+    __ vxor_vv(res, res, working_vregs[0]);\n+    for (int i = 1; i < rounds - 1; i++) {\n+      __ vaesem_vv(res, working_vregs[i]);\n+    }\n+    __ vaesef_vv(res, working_vregs[rounds - 1]);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/\n+  address generate_aescrypt_encryptBlock() {\n+    assert(UseAESIntrinsics, \"need AES instructions (Zvkned extension) support\");\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n+\n+    Label L_aes128, L_aes192;\n+\n+    const Register from        = c_rarg0;  \/\/ source array address\n+    const Register to          = c_rarg1;  \/\/ destination array address\n+    const Register key         = c_rarg2;  \/\/ key array address\n+    const Register keylen      = c_rarg3;\n+\n+    VectorRegister working_vregs[] = {\n+      v4, v5, v6, v7, v8, v9, v10, v11,\n+      v12, v13, v14, v15, v16, v17, v18\n+    };\n+    const VectorRegister res   = v19;\n+\n+    address start = __ pc();\n+    __ enter();\n+\n+    __ lwu(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1);\n+    __ vle32_v(res, from);\n+\n+    __ mv(t2, 52);\n+    __ blt(keylen, t2, L_aes128);\n+    __ beq(keylen, t2, L_aes192);\n+    \/\/ Else we fallthrough to the biggest case (256-bit key size)\n+\n+    \/\/ Note: the following function performs key += 15*16\n+    generate_aes_loadkeys(key, working_vregs, 15);\n+    generate_aes_encrypt(res, working_vregs, 15);\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+  __ bind(L_aes192);\n+    \/\/ Note: the following function performs key += 13*16\n+    generate_aes_loadkeys(key, working_vregs, 13);\n+    generate_aes_encrypt(res, working_vregs, 13);\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+  __ bind(L_aes128);\n+    \/\/ Note: the following function performs key += 11*16\n+    generate_aes_loadkeys(key, working_vregs, 11);\n+    generate_aes_encrypt(res, working_vregs, 11);\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n+  void generate_aes_decrypt(const VectorRegister &res, VectorRegister *working_vregs, int rounds) {\n+    assert(rounds <= 15, \"rounds should be less than or equal to working_vregs size\");\n+\n+    __ vxor_vv(res, res, working_vregs[rounds - 1]);\n+    for (int i = rounds - 2; i > 0; i--) {\n+      __ vaesdm_vv(res, working_vregs[i]);\n+    }\n+    __ vaesdf_vv(res, working_vregs[0]);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/\n+  address generate_aescrypt_decryptBlock() {\n+    assert(UseAESIntrinsics, \"need AES instructions (Zvkned extension) support\");\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n+\n+    Label L_aes128, L_aes192;\n+\n+    const Register from        = c_rarg0;  \/\/ source array address\n+    const Register to          = c_rarg1;  \/\/ destination array address\n+    const Register key         = c_rarg2;  \/\/ key array address\n+    const Register keylen      = c_rarg3;\n+\n+    VectorRegister working_vregs[] = {\n+      v4, v5, v6, v7, v8, v9, v10, v11,\n+      v12, v13, v14, v15, v16, v17, v18\n+    };\n+    const VectorRegister res   = v19;\n+\n+    address start = __ pc();\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+    __ lwu(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1);\n+    __ vle32_v(res, from);\n+\n+    __ mv(t2, 52);\n+    __ blt(keylen, t2, L_aes128);\n+    __ beq(keylen, t2, L_aes192);\n+    \/\/ Else we fallthrough to the biggest case (256-bit key size)\n+\n+    \/\/ Note: the following function performs key += 15*16\n+    generate_aes_loadkeys(key, working_vregs, 15);\n+    generate_aes_decrypt(res, working_vregs, 15);\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+  __ bind(L_aes192);\n+    \/\/ Note: the following function performs key += 13*16\n+    generate_aes_loadkeys(key, working_vregs, 13);\n+    generate_aes_decrypt(res, working_vregs, 13);\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+  __ bind(L_aes128);\n+    \/\/ Note: the following function performs key += 11*16\n+    generate_aes_loadkeys(key, working_vregs, 11);\n+    generate_aes_decrypt(res, working_vregs, 11);\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -6297,0 +6462,5 @@\n+    if (UseAESIntrinsics) {\n+      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n+      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":170,"deletions":0,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -125,11 +125,0 @@\n-  if (UseAES || UseAESIntrinsics) {\n-    if (UseAES && !FLAG_IS_DEFAULT(UseAES)) {\n-      warning(\"AES instructions are not available on this CPU\");\n-      FLAG_SET_DEFAULT(UseAES, false);\n-    }\n-    if (UseAESIntrinsics && !FLAG_IS_DEFAULT(UseAESIntrinsics)) {\n-      warning(\"AES intrinsics are not available on this CPU\");\n-      FLAG_SET_DEFAULT(UseAESIntrinsics, false);\n-    }\n-  }\n-\n@@ -432,0 +421,13 @@\n+\n+  \/\/ AES\n+  if (UseZvkn) {\n+    if (FLAG_IS_DEFAULT(UseAESIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseAESIntrinsics, true);\n+    }\n+  } else if (UseAESIntrinsics || UseAES) {\n+    if (!FLAG_IS_DEFAULT(UseAESIntrinsics) || !FLAG_IS_DEFAULT(UseAES)) {\n+      warning(\"AES intrinsics require Zvkn extension (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAES, false);\n+    FLAG_SET_DEFAULT(UseAESIntrinsics, false);\n+  }\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -7401,1 +7401,1 @@\n-#if defined(PPC64) || defined(S390)\n+#if defined(PPC64) || defined(S390) || defined(RISCV64)\n@@ -7405,1 +7405,1 @@\n-  \/\/ The ppc64 stubs of encryption and decryption use the same round keys (sessionK[0]).\n+  \/\/ The ppc64 and riscv64 stubs of encryption and decryption use the same round keys (sessionK[0]).\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}