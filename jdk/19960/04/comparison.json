{"files":[{"patch":"@@ -1925,0 +1925,7 @@\n+  \/\/ Vector AES instructions (Zvkned extension)\n+  INSN(vaesem_vv,   0b1110111, 0b010, 0b00010, 0b101000);\n+  INSN(vaesef_vv,   0b1110111, 0b010, 0b00011, 0b101000);\n+\n+  INSN(vaesdm_vv,   0b1110111, 0b010, 0b00000, 0b101000);\n+  INSN(vaesdf_vv,   0b1110111, 0b010, 0b00001, 0b101000);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2279,0 +2279,231 @@\n+  void generate_rev8_pack2(const VectorRegister vtmp1, const VectorRegister vtmp2) {\n+    __ vrev8_v(vtmp1, vtmp1);\n+    __ vrev8_v(vtmp2, vtmp2);\n+  }\n+\n+  void generate_rev8_pack4(const VectorRegister vtmp1, const VectorRegister vtmp2,\n+                           const VectorRegister vtmp3, const VectorRegister vtmp4) {\n+    generate_rev8_pack2(vtmp1, vtmp2);\n+    generate_rev8_pack2(vtmp3, vtmp4);\n+  }\n+\n+  void generate_vle32_pack2(const Register key, const VectorRegister vtmp1,\n+                            const VectorRegister vtmp2) {\n+    const int step = 16;\n+    __ vle32_v(vtmp1, key);\n+    __ addi(key, key, step);\n+    __ vle32_v(vtmp2, key);\n+    __ addi(key, key, step);\n+  }\n+\n+  void generate_vle32_pack4(const Register key, const VectorRegister vtmp1,\n+                            const VectorRegister vtmp2, const VectorRegister vtmp3,\n+                            const VectorRegister vtmp4) {\n+    generate_vle32_pack2(key, vtmp1, vtmp2);\n+    generate_vle32_pack2(key, vtmp3, vtmp4);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/\n+  address generate_aescrypt_encryptBlock() {\n+    assert(UseAESIntrinsics, \"need AES instructions (Zvkned extension) support\");\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n+\n+    Label L_do44, L_do52;\n+\n+    const Register from        = c_rarg0;  \/\/ source array address\n+    const Register to          = c_rarg1;  \/\/ destination array address\n+    const Register key         = c_rarg2;  \/\/ key array address\n+    const Register keylen      = c_rarg3;\n+\n+    const VectorRegister res   = v19;\n+    VectorRegister working_vregs[] = {\n+      v4, v5, v6, v7, v8, v9, v10, v11,\n+      v12, v13, v14, v15, v16, v17, v18\n+    };\n+\n+    const VectorRegister vzero = v17;\n+\n+    address start = __ pc();\n+    __ enter();\n+\n+    __ lwu(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1);\n+    __ vle32_v(res, from);\n+\n+    __ mv(t2, 52);\n+    __ blt(keylen, t2, L_do44);\n+    __ beq(keylen, t2, L_do52);\n+    \/\/ Else we fallthrough to the biggest case (256-bit key size)\n+\n+    for (int i = 0; i < 15; i++) {\n+      __ vle32_v(working_vregs[i], key);\n+      __ vrev8_v(working_vregs[i], working_vregs[i]);\n+      __ addi(key, key, 16);\n+    }\n+\n+    __ vxor_vv(res, res, working_vregs[0]);\n+    for (int i = 1; i < 14; i++) {\n+      __ vaesem_vv(res, working_vregs[i]);\n+    }\n+    __ vaesef_vv(res, working_vregs[14]);\n+\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+  __ bind(L_do52);\n+    for (int i = 0; i < 13; i++) {\n+      __ vle32_v(working_vregs[i], key);\n+      __ vrev8_v(working_vregs[i], working_vregs[i]);\n+      __ addi(key, key, 16);\n+    }\n+\n+    __ vxor_vv(res, res, working_vregs[0]);\n+    for (int i = 1; i < 12; i++) {\n+      __ vaesem_vv(res, working_vregs[i]);\n+    }\n+    __ vaesef_vv(res, working_vregs[12]);\n+\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+  __ bind(L_do44);\n+    for (int i = 0; i < 11; i++) {\n+      __ vle32_v(working_vregs[i], key);\n+      __ vrev8_v(working_vregs[i], working_vregs[i]);\n+      __ addi(key, key, 16);\n+    }\n+\n+    __ vxor_vv(res, res, working_vregs[0]);\n+    for (int i = 1; i < 10; i++) {\n+      __ vaesem_vv(res, working_vregs[i]);\n+    }\n+    __ vaesef_vv(res, working_vregs[10]);\n+\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n+  void generate_aesdecrypt_round(const VectorRegister res, const VectorRegister vzero,\n+                                 const VectorRegister vtmp1, const VectorRegister vtmp2,\n+                                 const VectorRegister vtmp3, const VectorRegister vtmp4) {\n+    __ vxor_vv(res, res, vtmp1);\n+    __ vaesdm_vv(res, vzero);\n+    __ vxor_vv(res, res, vtmp2);\n+    __ vaesdm_vv(res, vzero);\n+    __ vxor_vv(res, res, vtmp3);\n+    __ vaesdm_vv(res, vzero);\n+    __ vxor_vv(res, res, vtmp4);\n+    __ vaesdm_vv(res, vzero);\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/\n+  address generate_aescrypt_decryptBlock() {\n+    assert(UseAESIntrinsics, \"need AES instructions (Zvkned extension) support\");\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n+    Label L_doLast;\n+\n+    const Register from        = c_rarg0;  \/\/ source array address\n+    const Register to          = c_rarg1;  \/\/ destination array address\n+    const Register key         = c_rarg2;  \/\/ key array address\n+    const Register keylen      = c_rarg3;\n+\n+    const VectorRegister res   = v16;\n+    const VectorRegister vtmp1 = v4;\n+    const VectorRegister vtmp2 = v5;\n+    const VectorRegister vtmp3 = v6;\n+    const VectorRegister vtmp4 = v7;\n+\n+    const VectorRegister vzero = v17;\n+    const VectorRegister vtemp = v18;\n+\n+    address start = __ pc();\n+    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+    __ lwu(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ vsetivli(x0, 4, Assembler::e32, Assembler::m1);\n+    __ vle32_v(res, from);\n+    __ vmv_v_x(vzero, zr);\n+    __ vle32_v(vtemp, key);\n+    __ addi(key, key, 16);\n+    \/\/ Note: the following function performs key += 4*16\n+    generate_vle32_pack4(key, vtmp1, vtmp2, vtmp3, vtmp4);\n+\n+    __ vrev8_v(vtemp, vtemp);\n+    generate_rev8_pack4(vtmp1, vtmp2, vtmp3, vtmp4);\n+    generate_aesdecrypt_round(res, vzero, vtmp1, vtmp2, vtmp3, vtmp4);\n+\n+    \/\/ Note: the following function performs key += 4*16\n+    generate_vle32_pack4(key, vtmp1, vtmp2, vtmp3, vtmp4);\n+    generate_rev8_pack4(vtmp1, vtmp2, vtmp3, vtmp4);\n+    generate_aesdecrypt_round(res, vzero, vtmp1, vtmp2, vtmp3, vtmp4);\n+\n+    \/\/ Note: the following function performs key += 2*16\n+    generate_vle32_pack2(key, vtmp1, vtmp2);\n+    generate_rev8_pack2(vtmp1, vtmp2);\n+\n+    __ mv(t2, 44);\n+    __ beq(keylen, t2, L_doLast);\n+\n+    __ vxor_vv(res, res, vtmp1);\n+    __ vaesdm_vv(res, vzero);\n+    __ vxor_vv(res, res, vtmp2);\n+    __ vaesdm_vv(res, vzero);\n+\n+    \/\/ Note: the following function performs key += 2*16\n+    generate_vle32_pack2(key, vtmp1, vtmp2);\n+    generate_rev8_pack2(vtmp1, vtmp2);\n+\n+    __ mv(t2, 52);\n+    __ beq(keylen, t2, L_doLast);\n+\n+    __ vxor_vv(res, res, vtmp1);\n+    __ vaesdm_vv(res, vzero);\n+    __ vxor_vv(res, res, vtmp2);\n+    __ vaesdm_vv(res, vzero);\n+\n+    \/\/ Note: the following function performs key += 2*16\n+    generate_vle32_pack2(key, vtmp1, vtmp2);\n+    generate_rev8_pack2(vtmp1, vtmp2);\n+\n+    __ bind(L_doLast);\n+\n+    __ vxor_vv(res, res, vtmp1);\n+    __ vaesdm_vv(res, vzero);\n+    __ vxor_vv(res, res, vtmp2);\n+    __ vaesdf_vv(res, vtemp);\n+\n+    __ vse32_v(res, to);\n+    __ mv(c_rarg0, 0);\n+\n+    __ leave();\n+    __ ret();\n+\n+    return start;\n+  }\n+\n@@ -5657,0 +5888,5 @@\n+    if (UseAESIntrinsics) {\n+      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n+      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":236,"deletions":0,"binary":false,"changes":236,"status":"modified"},{"patch":"@@ -118,11 +118,0 @@\n-  if (UseAES || UseAESIntrinsics) {\n-    if (UseAES && !FLAG_IS_DEFAULT(UseAES)) {\n-      warning(\"AES instructions are not available on this CPU\");\n-      FLAG_SET_DEFAULT(UseAES, false);\n-    }\n-    if (UseAESIntrinsics && !FLAG_IS_DEFAULT(UseAESIntrinsics)) {\n-      warning(\"AES intrinsics are not available on this CPU\");\n-      FLAG_SET_DEFAULT(UseAESIntrinsics, false);\n-    }\n-  }\n-\n@@ -314,0 +303,13 @@\n+\n+  \/\/ AES\n+  if (UseZvkn) {\n+    if (FLAG_IS_DEFAULT(UseAESIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseAESIntrinsics, true);\n+    }\n+  } else if (UseAESIntrinsics || UseAES) {\n+    if (!FLAG_IS_DEFAULT(UseAESIntrinsics) || !FLAG_IS_DEFAULT(UseAES)) {\n+      warning(\"AES intrinsics require Zvkn extension (not available on this CPU).\");\n+    }\n+    FLAG_SET_DEFAULT(UseAES, false);\n+    FLAG_SET_DEFAULT(UseAESIntrinsics, false);\n+  }\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"}]}