{"files":[{"patch":"@@ -850,5 +850,1 @@\n-    ifneq ($$(findstring -XX:-ZGenerational, $$(JTREG_ALL_OPTIONS)), )\n-      JTREG_AUTO_PROBLEM_LISTS += ProblemList-zgc.txt\n-    else\n-      JTREG_AUTO_PROBLEM_LISTS += ProblemList-generational-zgc.txt\n-    endif\n+    JTREG_AUTO_PROBLEM_LISTS += ProblemList-zgc.txt\n","filename":"make\/RunTests.gmk","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -196,2 +196,0 @@\n-        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/x\/x_$(HOTSPOT_TARGET_CPU).ad \\\n-        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/gc\/x\/x_$(HOTSPOT_TARGET_CPU_ARCH).ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -153,1 +153,0 @@\n-  JVM_EXCLUDE_PATTERNS += gc\/x\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -993,4 +993,1 @@\n-    if (!(UseZGC && !ZGenerational)) {\n-      \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n-      __ verify_oop(dest->as_register());\n-    }\n+    __ verify_oop(dest->as_register());\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,462 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/codeBlob.hpp\"\n-#include \"code\/vmreg.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#ifdef COMPILER1\n-#include \"c1\/c1_LIRAssembler.hpp\"\n-#include \"c1\/c1_MacroAssembler.hpp\"\n-#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n-#endif \/\/ COMPILER1\n-#ifdef COMPILER2\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/* nothing *\/\n-#else\n-#define BLOCK_COMMENT(str) __ block_comment(str)\n-#endif\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::load_at(MacroAssembler* masm,\n-                                   DecoratorSet decorators,\n-                                   BasicType type,\n-                                   Register dst,\n-                                   Address src,\n-                                   Register tmp1,\n-                                   Register tmp2) {\n-  if (!XBarrierSet::barrier_needed(decorators, type)) {\n-    \/\/ Barrier not needed\n-    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp2);\n-    return;\n-  }\n-\n-  assert_different_registers(rscratch1, rscratch2, src.base());\n-  assert_different_registers(rscratch1, rscratch2, dst);\n-\n-  Label done;\n-\n-  \/\/ Load bad mask into scratch register.\n-  __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n-  __ lea(rscratch2, src);\n-  __ ldr(dst, src);\n-\n-  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n-  __ tst(dst, rscratch1);\n-  __ br(Assembler::EQ, done);\n-\n-  __ enter(\/*strip_ret_addr*\/true);\n-\n-  __ push_call_clobbered_registers_except(RegSet::of(dst));\n-\n-  if (c_rarg0 != dst) {\n-    __ mov(c_rarg0, dst);\n-  }\n-  __ mov(c_rarg1, rscratch2);\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n-\n-  \/\/ Make sure dst has the return value.\n-  if (dst != r0) {\n-    __ mov(dst, r0);\n-  }\n-\n-  __ pop_call_clobbered_registers_except(RegSet::of(dst));\n-  __ leave();\n-\n-  __ bind(done);\n-}\n-\n-#ifdef ASSERT\n-\n-void XBarrierSetAssembler::store_at(MacroAssembler* masm,\n-                                        DecoratorSet decorators,\n-                                        BasicType type,\n-                                        Address dst,\n-                                        Register val,\n-                                        Register tmp1,\n-                                        Register tmp2,\n-                                        Register tmp3) {\n-  \/\/ Verify value\n-  if (is_reference_type(type)) {\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (val != noreg) {\n-      Label done;\n-\n-      \/\/ tmp1, tmp2 and tmp3 are often set to noreg.\n-      RegSet savedRegs = RegSet::of(rscratch1);\n-      __ push(savedRegs, sp);\n-\n-      __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n-      __ tst(val, rscratch1);\n-      __ br(Assembler::EQ, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n-      __ bind(done);\n-      __ pop(savedRegs, sp);\n-    }\n-  }\n-\n-  \/\/ Store value\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, noreg);\n-}\n-\n-#endif \/\/ ASSERT\n-\n-void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,\n-                                              DecoratorSet decorators,\n-                                              bool is_oop,\n-                                              Register src,\n-                                              Register dst,\n-                                              Register count,\n-                                              RegSet saved_regs) {\n-  if (!is_oop) {\n-    \/\/ Barrier not needed\n-    return;\n-  }\n-\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::arraycopy_prologue {\");\n-\n-  assert_different_registers(src, count, rscratch1);\n-\n-  __ push(saved_regs, sp);\n-\n-  if (count == c_rarg0) {\n-    if (src == c_rarg1) {\n-      \/\/ exactly backwards!!\n-      __ mov(rscratch1, c_rarg0);\n-      __ mov(c_rarg0, c_rarg1);\n-      __ mov(c_rarg1, rscratch1);\n-    } else {\n-      __ mov(c_rarg1, count);\n-      __ mov(c_rarg0, src);\n-    }\n-  } else {\n-    __ mov(c_rarg0, src);\n-    __ mov(c_rarg1, count);\n-  }\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_array_addr(), 2);\n-\n-  __ pop(saved_regs, sp);\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::arraycopy_prologue\");\n-}\n-\n-void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,\n-                                                         Register jni_env,\n-                                                         Register robj,\n-                                                         Register tmp,\n-                                                         Label& slowpath) {\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::try_resolve_jobject_in_native {\");\n-\n-  assert_different_registers(jni_env, robj, tmp);\n-\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, robj, tmp, slowpath);\n-\n-  \/\/ The Address offset is too large to direct load - -784. Our range is +127, -128.\n-  __ mov(tmp, (int64_t)(in_bytes(XThreadLocalData::address_bad_mask_offset()) -\n-              in_bytes(JavaThread::jni_environment_offset())));\n-\n-  \/\/ Load address bad mask\n-  __ add(tmp, jni_env, tmp);\n-  __ ldr(tmp, Address(tmp));\n-\n-  \/\/ Check address bad mask\n-  __ tst(robj, tmp);\n-  __ br(Assembler::NE, slowpath);\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::try_resolve_jobject_in_native\");\n-}\n-\n-#ifdef COMPILER1\n-\n-#undef __\n-#define __ ce->masm()->\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  assert_different_registers(rscratch1, rthread, ref->as_register());\n-\n-  __ ldr(rscratch1, address_bad_mask_from_thread(rthread));\n-  __ tst(ref->as_register(), rscratch1);\n-}\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                                         XLoadBarrierStubC1* stub) const {\n-  \/\/ Stub entry\n-  __ bind(*stub->entry());\n-\n-  Register ref = stub->ref()->as_register();\n-  Register ref_addr = noreg;\n-  Register tmp = noreg;\n-\n-  if (stub->tmp()->is_valid()) {\n-    \/\/ Load address into tmp register\n-    ce->leal(stub->ref_addr(), stub->tmp());\n-    ref_addr = tmp = stub->tmp()->as_pointer_register();\n-  } else {\n-    \/\/ Address already in register\n-    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n-  }\n-\n-  assert_different_registers(ref, ref_addr, noreg);\n-\n-  \/\/ Save r0 unless it is the result or tmp register\n-  \/\/ Set up SP to accommodate parameters and maybe r0..\n-  if (ref != r0 && tmp != r0) {\n-    __ sub(sp, sp, 32);\n-    __ str(r0, Address(sp, 16));\n-  } else {\n-    __ sub(sp, sp, 16);\n-  }\n-\n-  \/\/ Setup arguments and call runtime stub\n-  ce->store_parameter(ref_addr, 1);\n-  ce->store_parameter(ref, 0);\n-\n-  __ far_call(stub->runtime_stub());\n-\n-  \/\/ Verify result\n-  __ verify_oop(r0);\n-\n-  \/\/ Move result into place\n-  if (ref != r0) {\n-    __ mov(ref, r0);\n-  }\n-\n-  \/\/ Restore r0 unless it is the result or tmp register\n-  if (ref != r0 && tmp != r0) {\n-    __ ldr(r0, Address(sp, 16));\n-    __ add(sp, sp, 32);\n-  } else {\n-    __ add(sp, sp, 16);\n-  }\n-\n-  \/\/ Stub exit\n-  __ b(*stub->continuation());\n-}\n-\n-#undef __\n-#define __ sasm->\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                                                 DecoratorSet decorators) const {\n-  __ prologue(\"zgc_load_barrier stub\", false);\n-\n-  __ push_call_clobbered_registers_except(RegSet::of(r0));\n-\n-  \/\/ Setup arguments\n-  __ load_parameter(0, c_rarg0);\n-  __ load_parameter(1, c_rarg1);\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n-\n-  __ pop_call_clobbered_registers_except(RegSet::of(r0));\n-\n-  __ epilogue();\n-}\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-\n-OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  if (!OptoReg::is_reg(opto_reg)) {\n-    return OptoReg::Bad;\n-  }\n-\n-  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-  if (vm_reg->is_FloatRegister()) {\n-    return opto_reg & ~1;\n-  }\n-\n-  return opto_reg;\n-}\n-\n-#undef __\n-#define __ _masm->\n-\n-class XSaveLiveRegisters {\n-private:\n-  MacroAssembler* const _masm;\n-  RegSet                _gp_regs;\n-  FloatRegSet           _fp_regs;\n-  PRegSet               _p_regs;\n-\n-public:\n-  void initialize(XLoadBarrierStubC2* stub) {\n-    \/\/ Record registers that needs to be saved\/restored\n-    RegMaskIterator rmi(stub->live());\n-    while (rmi.has_next()) {\n-      const OptoReg::Name opto_reg = rmi.next();\n-      if (OptoReg::is_reg(opto_reg)) {\n-        const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-        if (vm_reg->is_Register()) {\n-          _gp_regs += RegSet::of(vm_reg->as_Register());\n-        } else if (vm_reg->is_FloatRegister()) {\n-          _fp_regs += FloatRegSet::of(vm_reg->as_FloatRegister());\n-        } else if (vm_reg->is_PRegister()) {\n-          _p_regs += PRegSet::of(vm_reg->as_PRegister());\n-        } else {\n-          fatal(\"Unknown register type\");\n-        }\n-      }\n-    }\n-\n-    \/\/ Remove C-ABI SOE registers, scratch regs and _ref register that will be updated\n-    _gp_regs -= RegSet::range(r19, r30) + RegSet::of(r8, r9, stub->ref());\n-  }\n-\n-  XSaveLiveRegisters(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _gp_regs(),\n-      _fp_regs(),\n-      _p_regs() {\n-\n-    \/\/ Figure out what registers to save\/restore\n-    initialize(stub);\n-\n-    \/\/ Save registers\n-    __ push(_gp_regs, sp);\n-    __ push_fp(_fp_regs, sp);\n-    __ push_p(_p_regs, sp);\n-  }\n-\n-  ~XSaveLiveRegisters() {\n-    \/\/ Restore registers\n-    __ pop_p(_p_regs, sp);\n-    __ pop_fp(_fp_regs, sp);\n-\n-    \/\/ External runtime call may clobber ptrue reg\n-    __ reinitialize_ptrue();\n-\n-    __ pop(_gp_regs, sp);\n-  }\n-};\n-\n-#undef __\n-#define __ _masm->\n-\n-class XSetupArguments {\n-private:\n-  MacroAssembler* const _masm;\n-  const Register        _ref;\n-  const Address         _ref_addr;\n-\n-public:\n-  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _ref(stub->ref()),\n-      _ref_addr(stub->ref_addr()) {\n-\n-    \/\/ Setup arguments\n-    if (_ref_addr.base() == noreg) {\n-      \/\/ No self healing\n-      if (_ref != c_rarg0) {\n-        __ mov(c_rarg0, _ref);\n-      }\n-      __ mov(c_rarg1, 0);\n-    } else {\n-      \/\/ Self healing\n-      if (_ref == c_rarg0) {\n-        \/\/ _ref is already at correct place\n-        __ lea(c_rarg1, _ref_addr);\n-      } else if (_ref != c_rarg1) {\n-        \/\/ _ref is in wrong place, but not in c_rarg1, so fix it first\n-        __ lea(c_rarg1, _ref_addr);\n-        __ mov(c_rarg0, _ref);\n-      } else if (_ref_addr.base() != c_rarg0 && _ref_addr.index() != c_rarg0) {\n-        assert(_ref == c_rarg1, \"Mov ref first, vacating c_rarg0\");\n-        __ mov(c_rarg0, _ref);\n-        __ lea(c_rarg1, _ref_addr);\n-      } else {\n-        assert(_ref == c_rarg1, \"Need to vacate c_rarg1 and _ref_addr is using c_rarg0\");\n-        if (_ref_addr.base() == c_rarg0 || _ref_addr.index() == c_rarg0) {\n-          __ mov(rscratch2, c_rarg1);\n-          __ lea(c_rarg1, _ref_addr);\n-          __ mov(c_rarg0, rscratch2);\n-        } else {\n-          ShouldNotReachHere();\n-        }\n-      }\n-    }\n-  }\n-\n-  ~XSetupArguments() {\n-    \/\/ Transfer result\n-    if (_ref != r0) {\n-      __ mov(_ref, r0);\n-    }\n-  }\n-};\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n-  BLOCK_COMMENT(\"XLoadBarrierStubC2\");\n-\n-  \/\/ Stub entry\n-  __ bind(*stub->entry());\n-\n-  {\n-    XSaveLiveRegisters save_live_registers(masm, stub);\n-    XSetupArguments setup_arguments(masm, stub);\n-    __ mov(rscratch1, stub->slow_path());\n-    __ blr(rscratch1);\n-  }\n-  \/\/ Stub exit\n-  __ b(*stub->continuation());\n-}\n-\n-#endif \/\/ COMPILER2\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error) {\n-  \/\/ Check if mask is good.\n-  \/\/ verifies that XAddressBadMask & r0 == 0\n-  __ ldr(tmp2, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp1, obj, tmp2);\n-  __ cbnz(tmp1, error);\n-\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n-}\n-\n-#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xBarrierSetAssembler_aarch64.cpp","additions":0,"deletions":462,"binary":false,"changes":462,"status":"deleted"},{"patch":"@@ -1,110 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef CPU_AARCH64_GC_X_XBARRIERSETASSEMBLER_AARCH64_HPP\n-#define CPU_AARCH64_GC_X_XBARRIERSETASSEMBLER_AARCH64_HPP\n-\n-#include \"code\/vmreg.hpp\"\n-#include \"oops\/accessDecorators.hpp\"\n-#ifdef COMPILER2\n-#include \"opto\/optoreg.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class LIR_Assembler;\n-class LIR_Opr;\n-class StubAssembler;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class Node;\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class XLoadBarrierStubC1;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class XLoadBarrierStubC2;\n-#endif \/\/ COMPILER2\n-\n-class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n-public:\n-  virtual void load_at(MacroAssembler* masm,\n-                       DecoratorSet decorators,\n-                       BasicType type,\n-                       Register dst,\n-                       Address src,\n-                       Register tmp1,\n-                       Register tmp2);\n-\n-#ifdef ASSERT\n-  virtual void store_at(MacroAssembler* masm,\n-                        DecoratorSet decorators,\n-                        BasicType type,\n-                        Address dst,\n-                        Register val,\n-                        Register tmp1,\n-                        Register tmp2,\n-                        Register tmp3);\n-#endif \/\/ ASSERT\n-\n-  virtual void arraycopy_prologue(MacroAssembler* masm,\n-                                  DecoratorSet decorators,\n-                                  bool is_oop,\n-                                  Register src,\n-                                  Register dst,\n-                                  Register count,\n-                                  RegSet saved_regs);\n-\n-  virtual void try_resolve_jobject_in_native(MacroAssembler* masm,\n-                                             Register jni_env,\n-                                             Register robj,\n-                                             Register tmp,\n-                                             Label& slowpath);\n-\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n-\n-#ifdef COMPILER1\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n-\n-  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                     XLoadBarrierStubC1* stub) const;\n-\n-  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                             DecoratorSet decorators) const;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-  OptoReg::Name refine_register(const Node* node,\n-                                OptoReg::Name opto_reg);\n-\n-  void generate_c2_load_barrier_stub(MacroAssembler* masm,\n-                                     XLoadBarrierStubC2* stub) const;\n-#endif \/\/ COMPILER2\n-\n-  void check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error);\n-};\n-\n-#endif \/\/ CPU_AARCH64_GC_X_XBARRIERSETASSEMBLER_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xBarrierSetAssembler_aarch64.hpp","additions":0,"deletions":110,"binary":false,"changes":110,"status":"deleted"},{"patch":"@@ -1,212 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-#ifdef LINUX\n-#include <sys\/mman.h>\n-#endif \/\/ LINUX\n-\n-\/\/\n-\/\/ The heap can have three different layouts, depending on the max heap size.\n-\/\/\n-\/\/ Address Space & Pointer Layout 1\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000014000000000 (20TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000010000000000 (16TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x00000c0000000000 (12TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000040000000000 (4TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                  4 4  4 4\n-\/\/   3                  6 5  2 1                                             0\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |00000000 00000000 00|1111|11 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |                    |    |\n-\/\/  |                    |    * 41-0 Object Offset (42-bits, 4TB address space)\n-\/\/  |                    |\n-\/\/  |                    * 45-42 Metadata Bits (4-bits)  0001 = Marked0      (Address view 4-8TB)\n-\/\/  |                                                    0010 = Marked1      (Address view 8-12TB)\n-\/\/  |                                                    0100 = Remapped     (Address view 16-20TB)\n-\/\/  |                                                    1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-46 Fixed (18-bits, always zero)\n-\/\/\n-\/\/\n-\/\/ Address Space & Pointer Layout 2\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000280000000000 (40TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000200000000000 (32TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000180000000000 (24TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                 4 4  4 4\n-\/\/   3                 7 6  3 2                                              0\n-\/\/  +------------------+-----+------------------------------------------------+\n-\/\/  |00000000 00000000 0|1111|111 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +-------------------+----+------------------------------------------------+\n-\/\/  |                   |    |\n-\/\/  |                   |    * 42-0 Object Offset (43-bits, 8TB address space)\n-\/\/  |                   |\n-\/\/  |                   * 46-43 Metadata Bits (4-bits)  0001 = Marked0      (Address view 8-16TB)\n-\/\/  |                                                   0010 = Marked1      (Address view 16-24TB)\n-\/\/  |                                                   0100 = Remapped     (Address view 32-40TB)\n-\/\/  |                                                   1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-47 Fixed (17-bits, always zero)\n-\/\/\n-\/\/\n-\/\/ Address Space & Pointer Layout 3\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000500000000000 (80TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000400000000000 (64TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000300000000000 (48TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000200000000000 (32TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6               4  4  4 4\n-\/\/   3               8  7  4 3                                               0\n-\/\/  +------------------+----+-------------------------------------------------+\n-\/\/  |00000000 00000000 |1111|1111 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +------------------+----+-------------------------------------------------+\n-\/\/  |                  |    |\n-\/\/  |                  |    * 43-0 Object Offset (44-bits, 16TB address space)\n-\/\/  |                  |\n-\/\/  |                  * 47-44 Metadata Bits (4-bits)  0001 = Marked0      (Address view 16-32TB)\n-\/\/  |                                                  0010 = Marked1      (Address view 32-48TB)\n-\/\/  |                                                  0100 = Remapped     (Address view 64-80TB)\n-\/\/  |                                                  1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-48 Fixed (16-bits, always zero)\n-\/\/\n-\n-\/\/ Default value if probing is not implemented for a certain platform\n-\/\/ Max address bit is restricted by implicit assumptions in the code, for instance\n-\/\/ the bit layout of XForwardingEntry or Partial array entry (see XMarkStackEntry) in mark stack\n-static const size_t DEFAULT_MAX_ADDRESS_BIT = 46;\n-\/\/ Minimum value returned, if probing fails\n-static const size_t MINIMUM_MAX_ADDRESS_BIT = 36;\n-\n-static size_t probe_valid_max_address_bit() {\n-#ifdef LINUX\n-  size_t max_address_bit = 0;\n-  const size_t page_size = os::vm_page_size();\n-  for (size_t i = DEFAULT_MAX_ADDRESS_BIT; i > MINIMUM_MAX_ADDRESS_BIT; --i) {\n-    const uintptr_t base_addr = ((uintptr_t) 1U) << i;\n-    if (msync((void*)base_addr, page_size, MS_ASYNC) == 0) {\n-      \/\/ msync succeeded, the address is valid, and maybe even already mapped.\n-      max_address_bit = i;\n-      break;\n-    }\n-    if (errno != ENOMEM) {\n-      \/\/ Some error occurred. This should never happen, but msync\n-      \/\/ has some undefined behavior, hence ignore this bit.\n-#ifdef ASSERT\n-      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n-#else \/\/ ASSERT\n-      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n-#endif \/\/ ASSERT\n-      continue;\n-    }\n-    \/\/ Since msync failed with ENOMEM, the page might not be mapped.\n-    \/\/ Try to map it, to see if the address is valid.\n-    void* const result_addr = mmap((void*) base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n-    if (result_addr != MAP_FAILED) {\n-      munmap(result_addr, page_size);\n-    }\n-    if ((uintptr_t) result_addr == base_addr) {\n-      \/\/ address is valid\n-      max_address_bit = i;\n-      break;\n-    }\n-  }\n-  if (max_address_bit == 0) {\n-    \/\/ probing failed, allocate a very high page and take that bit as the maximum\n-    const uintptr_t high_addr = ((uintptr_t) 1U) << DEFAULT_MAX_ADDRESS_BIT;\n-    void* const result_addr = mmap((void*) high_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n-    if (result_addr != MAP_FAILED) {\n-      max_address_bit = BitsPerSize_t - count_leading_zeros((size_t) result_addr) - 1;\n-      munmap(result_addr, page_size);\n-    }\n-  }\n-  log_info_p(gc, init)(\"Probing address space for the highest valid bit: \" SIZE_FORMAT, max_address_bit);\n-  return MAX2(max_address_bit, MINIMUM_MAX_ADDRESS_BIT);\n-#else \/\/ LINUX\n-  return DEFAULT_MAX_ADDRESS_BIT;\n-#endif \/\/ LINUX\n-}\n-\n-size_t XPlatformAddressOffsetBits() {\n-  const static size_t valid_max_address_offset_bits = probe_valid_max_address_bit() + 1;\n-  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n-  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n-  const size_t address_offset_bits = log2i_exact(address_offset);\n-  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n-}\n-\n-size_t XPlatformAddressMetadataShift() {\n-  return XPlatformAddressOffsetBits();\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xGlobals_aarch64.cpp","additions":0,"deletions":212,"binary":false,"changes":212,"status":"deleted"},{"patch":"@@ -1,33 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef CPU_AARCH64_GC_X_XGLOBALS_AARCH64_HPP\n-#define CPU_AARCH64_GC_X_XGLOBALS_AARCH64_HPP\n-\n-const size_t XPlatformHeapViews        = 3;\n-const size_t XPlatformCacheLineSize    = 64;\n-\n-size_t XPlatformAddressOffsetBits();\n-size_t XPlatformAddressMetadataShift();\n-\n-#endif \/\/ CPU_AARCH64_GC_X_XGLOBALS_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/xGlobals_aarch64.hpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"deleted"},{"patch":"@@ -1,249 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\n-source_hpp %{\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-\n-%}\n-\n-source %{\n-\n-static void x_load_barrier(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  if (barrier_data == XLoadBarrierElided) {\n-    return;\n-  }\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  __ ldr(tmp, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp, tmp, ref);\n-  __ cbnz(tmp, *stub->entry());\n-  __ bind(*stub->continuation());\n-}\n-\n-static void x_load_barrier_slow_path(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n-  __ b(*stub->entry());\n-  __ bind(*stub->continuation());\n-}\n-\n-%}\n-\n-\/\/ Load Pointer\n-instruct xLoadP(iRegPNoSp dst, memory8 mem, rFlagsReg cr)\n-%{\n-  match(Set dst (LoadP mem));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load(n) && (n->as_Load()->barrier_data() != 0));\n-  effect(TEMP dst, KILL cr);\n-\n-  ins_cost(4 * INSN_COST);\n-\n-  format %{ \"ldr  $dst, $mem\" %}\n-\n-  ins_encode %{\n-    Address ref_addr = mem2address($mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n-    if (ref_addr.getMode() == Address::base_plus_offset) {\n-      \/\/ Fix up any out-of-range offsets.\n-      assert_different_registers(rscratch1, as_Register($mem$$base));\n-      assert_different_registers(rscratch1, $dst$$Register);\n-      ref_addr = __ legitimize_address(ref_addr, 8, rscratch1);\n-    }\n-    __ ldr($dst$$Register, ref_addr);\n-    x_load_barrier(masm, this, ref_addr, $dst$$Register, rscratch2 \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(iload_reg_mem);\n-%}\n-\n-\/\/ Load Pointer Volatile\n-instruct xLoadPVolatile(iRegPNoSp dst, indirect mem \/* sync_memory *\/, rFlagsReg cr)\n-%{\n-  match(Set dst (LoadP mem));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n-  effect(TEMP dst, KILL cr);\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n-  format %{ \"ldar  $dst, $mem\\t\" %}\n-\n-  ins_encode %{\n-    __ ldar($dst$$Register, $mem$$Register);\n-    x_load_barrier(masm, this, Address($mem$$Register), $dst$$Register, rscratch2 \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct xCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(KILL cr, TEMP_DEF res);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n-            \"cset    $res, EQ\" %}\n-\n-  ins_encode %{\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n-    __ cset($res$$Register, Assembler::EQ);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, rscratch2);\n-      __ cbz(rscratch1, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), rscratch2 \/* ref *\/, rscratch1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n-      __ cset($res$$Register, Assembler::EQ);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load_exclusive(n) && (n->as_LoadStore()->barrier_data() == XLoadBarrierStrong));\n-  effect(KILL cr, TEMP_DEF res);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n- format %{ \"cmpxchg $mem, $oldval, $newval\\n\\t\"\n-           \"cset    $res, EQ\" %}\n-\n-  ins_encode %{\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n-    __ cset($res$$Register, Assembler::EQ);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, rscratch2);\n-      __ cbz(rscratch1, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), rscratch2 \/* ref *\/, rscratch1 \/* tmp *\/ );\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, rscratch2);\n-      __ cset($res$$Register, Assembler::EQ);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n-  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(TEMP_DEF res, KILL cr);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $res = $mem, $oldval, $newval\" %}\n-\n-  ins_encode %{\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-               false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, $res$$Register);\n-      __ cbz(rscratch1, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, rscratch1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 false \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, rFlagsReg cr) %{\n-  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(TEMP_DEF res, KILL cr);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $res = $mem, $oldval, $newval\" %}\n-\n-  ins_encode %{\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-               true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ldr(rscratch1, Address(rthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(rscratch1, rscratch1, $res$$Register);\n-      __ cbz(rscratch1, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, rscratch1 \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::xword,\n-                 true \/* acquire *\/, true \/* release *\/, false \/* weak *\/, $res$$Register);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xGetAndSetP(indirect mem, iRegP newv, iRegPNoSp prev, rFlagsReg cr) %{\n-  match(Set prev (GetAndSetP mem newv));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n-  effect(TEMP_DEF prev, KILL cr);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"atomic_xchg  $prev, $newv, [$mem]\" %}\n-\n-  ins_encode %{\n-    __ atomic_xchg($prev$$Register, $newv$$Register, $mem$$Register);\n-    x_load_barrier(masm, this, Address(noreg, 0), $prev$$Register, rscratch2 \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct xGetAndSetPAcq(indirect mem, iRegP newv, iRegPNoSp prev, rFlagsReg cr) %{\n-  match(Set prev (GetAndSetP mem newv));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load_exclusive(n) && (n->as_LoadStore()->barrier_data() != 0));\n-  effect(TEMP_DEF prev, KILL cr);\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n-  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem]\" %}\n-\n-  ins_encode %{\n-    __ atomic_xchgal($prev$$Register, $newv$$Register, $mem$$Register);\n-    x_load_barrier(masm, this, Address(noreg, 0), $prev$$Register, rscratch2 \/* tmp *\/, barrier_data());\n-  %}\n-  ins_pipe(pipe_serial);\n-%}\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/x\/x_aarch64.ad","additions":0,"deletions":249,"binary":false,"changes":249,"status":"deleted"},{"patch":"@@ -106,1 +106,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n@@ -132,1 +132,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load(n) && n->as_Load()->barrier_data() != 0);\n@@ -151,1 +151,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n@@ -168,1 +168,1 @@\n-  predicate(UseZGC && ZGenerational && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && needs_releasing_store(n) && n->as_Store()->barrier_data() != 0);\n@@ -185,1 +185,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -209,1 +209,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -233,1 +233,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -256,1 +256,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -279,1 +279,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -297,1 +297,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load_exclusive(n) && n->as_LoadStore()->barrier_data() != 0);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/z_aarch64.ad","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1,585 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, 2024 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/register.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/codeBlob.hpp\"\n-#include \"code\/vmreg.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"register_ppc.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#ifdef COMPILER1\n-#include \"c1\/c1_LIRAssembler.hpp\"\n-#include \"c1\/c1_MacroAssembler.hpp\"\n-#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n-#endif \/\/ COMPILER1\n-#ifdef COMPILER2\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n-                                   Register base, RegisterOrConstant ind_or_offs, Register dst,\n-                                   Register tmp1, Register tmp2,\n-                                   MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null) {\n-  __ block_comment(\"load_at (zgc) {\");\n-\n-  \/\/ Check whether a special gc barrier is required for this particular load\n-  \/\/ (e.g. whether it's a reference load or not)\n-  if (!XBarrierSet::barrier_needed(decorators, type)) {\n-    BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n-                                 tmp1, tmp2, preservation_level, L_handle_null);\n-    return;\n-  }\n-\n-  if (ind_or_offs.is_register()) {\n-    assert_different_registers(base, ind_or_offs.as_register(), tmp1, tmp2, R0, noreg);\n-    assert_different_registers(dst, ind_or_offs.as_register(), tmp1, tmp2, R0, noreg);\n-  } else {\n-    assert_different_registers(base, tmp1, tmp2, R0, noreg);\n-    assert_different_registers(dst, tmp1, tmp2, R0, noreg);\n-  }\n-\n-  \/* ==== Load the pointer using the standard implementation for the actual heap access\n-          and the decompression of compressed pointers ==== *\/\n-  \/\/ Result of 'load_at' (standard implementation) will be written back to 'dst'.\n-  \/\/ As 'base' is required for the C-call, it must be reserved in case of a register clash.\n-  Register saved_base = base;\n-  if (base == dst) {\n-    __ mr(tmp2, base);\n-    saved_base = tmp2;\n-  }\n-\n-  BarrierSetAssembler::load_at(masm, decorators, type, base, ind_or_offs, dst,\n-                               tmp1, noreg, preservation_level, L_handle_null);\n-\n-  \/* ==== Check whether pointer is dirty ==== *\/\n-  Label skip_barrier;\n-\n-  \/\/ Load bad mask into scratch register.\n-  __ ld(tmp1, (intptr_t) XThreadLocalData::address_bad_mask_offset(), R16_thread);\n-\n-  \/\/ The color bits of the to-be-tested pointer do not have to be equivalent to the 'bad_mask' testing bits.\n-  \/\/ A pointer is classified as dirty if any of the color bits that also match the bad mask is set.\n-  \/\/ Conversely, it follows that the logical AND of the bad mask and the pointer must be zero\n-  \/\/ if the pointer is not dirty.\n-  \/\/ Only dirty pointers must be processed by this barrier, so we can skip it in case the latter condition holds true.\n-  __ and_(tmp1, tmp1, dst);\n-  __ beq(CCR0, skip_barrier);\n-\n-  \/* ==== Invoke barrier ==== *\/\n-  int nbytes_save = 0;\n-\n-  const bool needs_frame = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR;\n-  const bool preserve_gp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_REGS;\n-  const bool preserve_fp_registers = preservation_level >= MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS;\n-\n-  const bool preserve_R3 = dst != R3_ARG1;\n-\n-  if (needs_frame) {\n-    if (preserve_gp_registers) {\n-      nbytes_save = (preserve_fp_registers\n-                     ? MacroAssembler::num_volatile_gp_regs + MacroAssembler::num_volatile_fp_regs\n-                     : MacroAssembler::num_volatile_gp_regs) * BytesPerWord;\n-      nbytes_save -= preserve_R3 ? 0 : BytesPerWord;\n-      __ save_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n-    }\n-\n-    __ save_LR(tmp1);\n-    __ push_frame_reg_args(nbytes_save, tmp1);\n-  }\n-\n-  \/\/ Setup arguments\n-  if (saved_base != R3_ARG1) {\n-    __ mr_if_needed(R3_ARG1, dst);\n-    __ add(R4_ARG2, ind_or_offs, saved_base);\n-  } else if (dst != R4_ARG2) {\n-    __ add(R4_ARG2, ind_or_offs, saved_base);\n-    __ mr(R3_ARG1, dst);\n-  } else {\n-    __ add(R0, ind_or_offs, saved_base);\n-    __ mr(R3_ARG1, dst);\n-    __ mr(R4_ARG2, R0);\n-  }\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n-\n-  Register result = R3_RET;\n-  if (needs_frame) {\n-    __ pop_frame();\n-    __ restore_LR(tmp1);\n-\n-    if (preserve_R3) {\n-      __ mr(R0, R3_RET);\n-      result = R0;\n-    }\n-\n-    if (preserve_gp_registers) {\n-      __ restore_volatile_gprs(R1_SP, -nbytes_save, preserve_fp_registers, preserve_R3);\n-    }\n-  }\n-  __ mr_if_needed(dst, result);\n-\n-  __ bind(skip_barrier);\n-  __ block_comment(\"} load_at (zgc)\");\n-}\n-\n-#ifdef ASSERT\n-\/\/ The Z store barrier only verifies the pointers it is operating on and is thus a sole debugging measure.\n-void XBarrierSetAssembler::store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n-                                    Register base, RegisterOrConstant ind_or_offs, Register val,\n-                                    Register tmp1, Register tmp2, Register tmp3,\n-                                    MacroAssembler::PreservationLevel preservation_level) {\n-  __ block_comment(\"store_at (zgc) {\");\n-\n-  \/\/ If the 'val' register is 'noreg', the to-be-stored value is a null pointer.\n-  if (is_reference_type(type) && val != noreg) {\n-    __ ld(tmp1, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n-    __ and_(tmp1, tmp1, val);\n-    __ asm_assert_eq(\"Detected dirty pointer on the heap in Z store barrier\");\n-  }\n-\n-  \/\/ Store value\n-  BarrierSetAssembler::store_at(masm, decorators, type, base, ind_or_offs, val, tmp1, tmp2, tmp3, preservation_level);\n-\n-  __ block_comment(\"} store_at (zgc)\");\n-}\n-#endif \/\/ ASSERT\n-\n-void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler *masm, DecoratorSet decorators, BasicType component_type,\n-                                              Register src, Register dst, Register count,\n-                                              Register preserve1, Register preserve2) {\n-  __ block_comment(\"arraycopy_prologue (zgc) {\");\n-\n-  \/* ==== Check whether a special gc barrier is required for this particular load ==== *\/\n-  if (!is_reference_type(component_type)) {\n-    return;\n-  }\n-\n-  Label skip_barrier;\n-\n-  \/\/ Fast path: Array is of length zero\n-  __ cmpdi(CCR0, count, 0);\n-  __ beq(CCR0, skip_barrier);\n-\n-  \/* ==== Ensure register sanity ==== *\/\n-  Register tmp_R11 = R11_scratch1;\n-\n-  assert_different_registers(src, dst, count, tmp_R11, noreg);\n-  if (preserve1 != noreg) {\n-    \/\/ Not technically required, but unlikely being intended.\n-    assert_different_registers(preserve1, preserve2);\n-  }\n-\n-  \/* ==== Invoke barrier (slowpath) ==== *\/\n-  int nbytes_save = 0;\n-\n-  {\n-    assert(!noreg->is_volatile(), \"sanity\");\n-\n-    if (preserve1->is_volatile()) {\n-      __ std(preserve1, -BytesPerWord * ++nbytes_save, R1_SP);\n-    }\n-\n-    if (preserve2->is_volatile() && preserve1 != preserve2) {\n-      __ std(preserve2, -BytesPerWord * ++nbytes_save, R1_SP);\n-    }\n-\n-    __ std(src, -BytesPerWord * ++nbytes_save, R1_SP);\n-    __ std(dst, -BytesPerWord * ++nbytes_save, R1_SP);\n-    __ std(count, -BytesPerWord * ++nbytes_save, R1_SP);\n-\n-    __ save_LR(tmp_R11);\n-    __ push_frame_reg_args(nbytes_save, tmp_R11);\n-  }\n-\n-  \/\/ XBarrierSetRuntime::load_barrier_on_oop_array_addr(src, count)\n-  if (count == R3_ARG1) {\n-    if (src == R4_ARG2) {\n-      \/\/ Arguments are provided in reverse order\n-      __ mr(tmp_R11, count);\n-      __ mr(R3_ARG1, src);\n-      __ mr(R4_ARG2, tmp_R11);\n-    } else {\n-      __ mr(R4_ARG2, count);\n-      __ mr(R3_ARG1, src);\n-    }\n-  } else {\n-    __ mr_if_needed(R3_ARG1, src);\n-    __ mr_if_needed(R4_ARG2, count);\n-  }\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_array_addr());\n-\n-  __ pop_frame();\n-  __ restore_LR(tmp_R11);\n-\n-  {\n-    __ ld(count, -BytesPerWord * nbytes_save--, R1_SP);\n-    __ ld(dst, -BytesPerWord * nbytes_save--, R1_SP);\n-    __ ld(src, -BytesPerWord * nbytes_save--, R1_SP);\n-\n-    if (preserve2->is_volatile() && preserve1 != preserve2) {\n-      __ ld(preserve2, -BytesPerWord * nbytes_save--, R1_SP);\n-    }\n-\n-    if (preserve1->is_volatile()) {\n-      __ ld(preserve1, -BytesPerWord * nbytes_save--, R1_SP);\n-    }\n-  }\n-\n-  __ bind(skip_barrier);\n-\n-  __ block_comment(\"} arraycopy_prologue (zgc)\");\n-}\n-\n-void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n-                                                         Register obj, Register tmp, Label& slowpath) {\n-  __ block_comment(\"try_resolve_jobject_in_native (zgc) {\");\n-\n-  assert_different_registers(jni_env, obj, tmp);\n-\n-  \/\/ Resolve the pointer using the standard implementation for weak tag handling and pointer verification.\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, dst, jni_env, obj, tmp, slowpath);\n-\n-  \/\/ Check whether pointer is dirty.\n-  __ ld(tmp,\n-        in_bytes(XThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset()),\n-        jni_env);\n-\n-  __ and_(tmp, obj, tmp);\n-  __ bne(CCR0, slowpath);\n-\n-  __ block_comment(\"} try_resolve_jobject_in_native (zgc)\");\n-}\n-\n-#undef __\n-\n-#ifdef COMPILER1\n-#define __ ce->masm()->\n-\n-\/\/ Code emitted by LIR node \"LIR_OpXLoadBarrierTest\" which in turn is emitted by XBarrierSetC1::load_barrier.\n-\/\/ The actual compare and branch instructions are represented as stand-alone LIR nodes.\n-void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  __ block_comment(\"load_barrier_test (zgc) {\");\n-\n-  __ ld(R0, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n-  __ andr(R0, R0, ref->as_pointer_register());\n-  __ cmpdi(CCR5 \/* as mandated by LIR node *\/, R0, 0);\n-\n-  __ block_comment(\"} load_barrier_test (zgc)\");\n-}\n-\n-\/\/ Code emitted by code stub \"XLoadBarrierStubC1\" which in turn is emitted by XBarrierSetC1::load_barrier.\n-\/\/ Invokes the runtime stub which is defined just below.\n-void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                                         XLoadBarrierStubC1* stub) const {\n-  __ block_comment(\"c1_load_barrier_stub (zgc) {\");\n-\n-  __ bind(*stub->entry());\n-\n-  \/* ==== Determine relevant data registers and ensure register sanity ==== *\/\n-  Register ref = stub->ref()->as_register();\n-  Register ref_addr = noreg;\n-\n-  \/\/ Determine reference address\n-  if (stub->tmp()->is_valid()) {\n-    \/\/ 'tmp' register is given, so address might have an index or a displacement.\n-    ce->leal(stub->ref_addr(), stub->tmp());\n-    ref_addr = stub->tmp()->as_pointer_register();\n-  } else {\n-    \/\/ 'tmp' register is not given, so address must have neither an index nor a displacement.\n-    \/\/ The address' base register is thus usable as-is.\n-    assert(stub->ref_addr()->as_address_ptr()->disp() == 0, \"illegal displacement\");\n-    assert(!stub->ref_addr()->as_address_ptr()->index()->is_valid(), \"illegal index\");\n-\n-    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n-  }\n-\n-  assert_different_registers(ref, ref_addr, R0, noreg);\n-\n-  \/* ==== Invoke stub ==== *\/\n-  \/\/ Pass arguments via stack. The stack pointer will be bumped by the stub.\n-  __ std(ref, (intptr_t) -1 * BytesPerWord, R1_SP);\n-  __ std(ref_addr, (intptr_t) -2 * BytesPerWord, R1_SP);\n-\n-  __ load_const_optimized(R0, stub->runtime_stub());\n-  __ call_stub(R0);\n-\n-  \/\/ The runtime stub passes the result via the R0 register, overriding the previously-loaded stub address.\n-  __ mr_if_needed(ref, R0);\n-  __ b(*stub->continuation());\n-\n-  __ block_comment(\"} c1_load_barrier_stub (zgc)\");\n-}\n-\n-#undef __\n-#define __ sasm->\n-\n-\/\/ Code emitted by runtime code stub which in turn is emitted by XBarrierSetC1::generate_c1_runtime_stubs.\n-void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                                                 DecoratorSet decorators) const {\n-  __ block_comment(\"c1_load_barrier_runtime_stub (zgc) {\");\n-\n-  const int stack_parameters = 2;\n-  const int nbytes_save = (MacroAssembler::num_volatile_regs + stack_parameters) * BytesPerWord;\n-\n-  __ save_volatile_gprs(R1_SP, -nbytes_save);\n-  __ save_LR(R0);\n-\n-  \/\/ Load arguments back again from the stack.\n-  __ ld(R3_ARG1, (intptr_t) -1 * BytesPerWord, R1_SP); \/\/ ref\n-  __ ld(R4_ARG2, (intptr_t) -2 * BytesPerWord, R1_SP); \/\/ ref_addr\n-\n-  __ push_frame_reg_args(nbytes_save, R0);\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators));\n-\n-  __ verify_oop(R3_RET, \"Bad pointer after barrier invocation\");\n-  __ mr(R0, R3_RET);\n-\n-  __ pop_frame();\n-  __ restore_LR(R3_RET);\n-  __ restore_volatile_gprs(R1_SP, -nbytes_save);\n-\n-  __ blr();\n-\n-  __ block_comment(\"} c1_load_barrier_runtime_stub (zgc)\");\n-}\n-\n-#undef __\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-\n-OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) const {\n-  if (!OptoReg::is_reg(opto_reg)) {\n-    return OptoReg::Bad;\n-  }\n-\n-  VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-  if ((vm_reg->is_Register() || vm_reg ->is_FloatRegister()) && (opto_reg & 1) != 0) {\n-    return OptoReg::Bad;\n-  }\n-\n-  return opto_reg;\n-}\n-\n-#define __ _masm->\n-\n-class XSaveLiveRegisters {\n-  MacroAssembler* _masm;\n-  RegMask _reg_mask;\n-  Register _result_reg;\n-  int _frame_size;\n-\n- public:\n-  XSaveLiveRegisters(MacroAssembler *masm, XLoadBarrierStubC2 *stub)\n-      : _masm(masm), _reg_mask(stub->live()), _result_reg(stub->ref()) {\n-\n-    const int register_save_size = iterate_over_register_mask(ACTION_COUNT_ONLY) * BytesPerWord;\n-    _frame_size = align_up(register_save_size, frame::alignment_in_bytes)\n-                  + frame::native_abi_reg_args_size;\n-\n-    __ save_LR_CR(R0);\n-    __ push_frame(_frame_size, R0);\n-\n-    iterate_over_register_mask(ACTION_SAVE, _frame_size);\n-  }\n-\n-  ~XSaveLiveRegisters() {\n-    iterate_over_register_mask(ACTION_RESTORE, _frame_size);\n-\n-    __ addi(R1_SP, R1_SP, _frame_size);\n-    __ restore_LR_CR(R0);\n-  }\n-\n- private:\n-  enum IterationAction : int {\n-    ACTION_SAVE,\n-    ACTION_RESTORE,\n-    ACTION_COUNT_ONLY\n-  };\n-\n-  int iterate_over_register_mask(IterationAction action, int offset = 0) {\n-    int reg_save_index = 0;\n-    RegMaskIterator live_regs_iterator(_reg_mask);\n-\n-    while(live_regs_iterator.has_next()) {\n-      const OptoReg::Name opto_reg = live_regs_iterator.next();\n-\n-      \/\/ Filter out stack slots (spilled registers, i.e., stack-allocated registers).\n-      if (!OptoReg::is_reg(opto_reg)) {\n-        continue;\n-      }\n-\n-      const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-      if (vm_reg->is_Register()) {\n-        Register std_reg = vm_reg->as_Register();\n-\n-        \/\/ '_result_reg' will hold the end result of the operation. Its content must thus not be preserved.\n-        if (std_reg == _result_reg) {\n-          continue;\n-        }\n-\n-        if (std_reg->encoding() >= R2->encoding() && std_reg->encoding() <= R12->encoding()) {\n-          reg_save_index++;\n-\n-          if (action == ACTION_SAVE) {\n-            _masm->std(std_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n-          } else if (action == ACTION_RESTORE) {\n-            _masm->ld(std_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n-          } else {\n-            assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n-          }\n-        }\n-      } else if (vm_reg->is_FloatRegister()) {\n-        FloatRegister fp_reg = vm_reg->as_FloatRegister();\n-        if (fp_reg->encoding() >= F0->encoding() && fp_reg->encoding() <= F13->encoding()) {\n-          reg_save_index++;\n-\n-          if (action == ACTION_SAVE) {\n-            _masm->stfd(fp_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n-          } else if (action == ACTION_RESTORE) {\n-            _masm->lfd(fp_reg, offset - reg_save_index * BytesPerWord, R1_SP);\n-          } else {\n-            assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n-          }\n-        }\n-      } else if (vm_reg->is_ConditionRegister()) {\n-        \/\/ NOP. Conditions registers are covered by save_LR_CR\n-      } else if (vm_reg->is_VectorSRegister()) {\n-        assert(SuperwordUseVSX, \"or should not reach here\");\n-        VectorSRegister vs_reg = vm_reg->as_VectorSRegister();\n-        if (vs_reg->encoding() >= VSR32->encoding() && vs_reg->encoding() <= VSR51->encoding()) {\n-          reg_save_index += 2;\n-\n-          Register spill_addr = R0;\n-          if (action == ACTION_SAVE) {\n-            _masm->addi(spill_addr, R1_SP, offset - reg_save_index * BytesPerWord);\n-            _masm->stxvd2x(vs_reg, spill_addr);\n-          } else if (action == ACTION_RESTORE) {\n-            _masm->addi(spill_addr, R1_SP, offset - reg_save_index * BytesPerWord);\n-            _masm->lxvd2x(vs_reg, spill_addr);\n-          } else {\n-            assert(action == ACTION_COUNT_ONLY, \"Sanity\");\n-          }\n-        }\n-      } else {\n-        if (vm_reg->is_SpecialRegister()) {\n-          fatal(\"Special registers are unsupported. Found register %s\", vm_reg->name());\n-        } else {\n-          fatal(\"Register type is not known\");\n-        }\n-      }\n-    }\n-\n-    return reg_save_index;\n-  }\n-};\n-\n-#undef __\n-#define __ _masm->\n-\n-class XSetupArguments {\n-  MacroAssembler* const _masm;\n-  const Register        _ref;\n-  const Address         _ref_addr;\n-\n- public:\n-  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _ref(stub->ref()),\n-      _ref_addr(stub->ref_addr()) {\n-\n-    \/\/ Desired register\/argument configuration:\n-    \/\/ _ref: R3_ARG1\n-    \/\/ _ref_addr: R4_ARG2\n-\n-    \/\/ '_ref_addr' can be unspecified. In that case, the barrier will not heal the reference.\n-    if (_ref_addr.base() == noreg) {\n-      assert_different_registers(_ref, R0, noreg);\n-\n-      __ mr_if_needed(R3_ARG1, _ref);\n-      __ li(R4_ARG2, 0);\n-    } else {\n-      assert_different_registers(_ref, _ref_addr.base(), R0, noreg);\n-      assert(!_ref_addr.index()->is_valid(), \"reference addresses must not contain an index component\");\n-\n-      if (_ref != R4_ARG2) {\n-        \/\/ Calculate address first as the address' base register might clash with R4_ARG2\n-        __ addi(R4_ARG2, _ref_addr.base(), _ref_addr.disp());\n-        __ mr_if_needed(R3_ARG1, _ref);\n-      } else if (_ref_addr.base() != R3_ARG1) {\n-        __ mr(R3_ARG1, _ref);\n-        __ addi(R4_ARG2, _ref_addr.base(), _ref_addr.disp()); \/\/ Clobbering _ref\n-      } else {\n-        \/\/ Arguments are provided in inverse order (i.e. _ref == R4_ARG2, _ref_addr == R3_ARG1)\n-        __ mr(R0, _ref);\n-        __ addi(R4_ARG2, _ref_addr.base(), _ref_addr.disp());\n-        __ mr(R3_ARG1, R0);\n-      }\n-    }\n-  }\n-};\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n-  __ block_comment(\"generate_c2_load_barrier_stub (zgc) {\");\n-\n-  __ bind(*stub->entry());\n-\n-  Register ref = stub->ref();\n-  Address ref_addr = stub->ref_addr();\n-\n-  assert_different_registers(ref, ref_addr.base());\n-\n-  {\n-    XSaveLiveRegisters save_live_registers(masm, stub);\n-    XSetupArguments setup_arguments(masm, stub);\n-\n-    __ call_VM_leaf(stub->slow_path());\n-    __ mr_if_needed(ref, R3_RET);\n-  }\n-\n-  __ b(*stub->continuation());\n-\n-  __ block_comment(\"} generate_c2_load_barrier_stub (zgc)\");\n-}\n-\n-#undef __\n-#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xBarrierSetAssembler_ppc.cpp","additions":0,"deletions":585,"binary":false,"changes":585,"status":"deleted"},{"patch":"@@ -1,93 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, 2022 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef CPU_PPC_GC_X_XBARRIERSETASSEMBLER_PPC_HPP\n-#define CPU_PPC_GC_X_XBARRIERSETASSEMBLER_PPC_HPP\n-\n-#include \"code\/vmreg.hpp\"\n-#include \"oops\/accessDecorators.hpp\"\n-#ifdef COMPILER2\n-#include \"opto\/optoreg.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class LIR_Assembler;\n-class LIR_Opr;\n-class StubAssembler;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class Node;\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class XLoadBarrierStubC1;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class XLoadBarrierStubC2;\n-#endif \/\/ COMPILER2\n-\n-class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n-public:\n-  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n-                       Register base, RegisterOrConstant ind_or_offs, Register dst,\n-                       Register tmp1, Register tmp2,\n-                       MacroAssembler::PreservationLevel preservation_level, Label *L_handle_null = nullptr);\n-\n-#ifdef ASSERT\n-  virtual void store_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n-                        Register base, RegisterOrConstant ind_or_offs, Register val,\n-                        Register tmp1, Register tmp2, Register tmp3,\n-                        MacroAssembler::PreservationLevel preservation_level);\n-#endif \/\/ ASSERT\n-\n-  virtual void arraycopy_prologue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n-                                  Register src, Register dst, Register count,\n-                                  Register preserve1, Register preserve2);\n-\n-  virtual void try_resolve_jobject_in_native(MacroAssembler* masm, Register dst, Register jni_env,\n-                                             Register obj, Register tmp, Label& slowpath);\n-\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n-\n-#ifdef COMPILER1\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n-\n-  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                     XLoadBarrierStubC1* stub) const;\n-\n-  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                             DecoratorSet decorators) const;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-  OptoReg::Name refine_register(const Node* node, OptoReg::Name opto_reg) const;\n-\n-  void generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const;\n-#endif \/\/ COMPILER2\n-};\n-\n-#endif \/\/ CPU_PPC_GC_X_XBARRIERSETASSEMBLER_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xBarrierSetAssembler_ppc.hpp","additions":0,"deletions":93,"binary":false,"changes":93,"status":"deleted"},{"patch":"@@ -1,203 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-#include <cstddef>\n-\n-#ifdef LINUX\n-#include <sys\/mman.h>\n-#endif \/\/ LINUX\n-\n-\/\/\n-\/\/ The overall memory layouts across different power platforms are similar and only differ with regards to\n-\/\/ the position of the highest addressable bit; the position of the metadata bits and the size of the actual\n-\/\/ addressable heap address space are adjusted accordingly.\n-\/\/\n-\/\/ The following memory schema shows an exemplary layout in which bit '45' is the highest addressable bit.\n-\/\/ It is assumed that this virtual memory address space layout is predominant on the power platform.\n-\/\/\n-\/\/ Standard Address Space & Pointer Layout\n-\/\/ ---------------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127 TiB - 1)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000140000000000 (20 TiB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16 TiB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x00000c0000000000 (12 TiB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8  TiB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000040000000000 (4  TiB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                  4 4  4 4\n-\/\/   3                  6 5  2 1                                             0\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |00000000 00000000 00|1111|11 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |                    |    |\n-\/\/  |                    |    * 41-0 Object Offset (42-bits, 4TB address space)\n-\/\/  |                    |\n-\/\/  |                    * 45-42 Metadata Bits (4-bits)  0001 = Marked0      (Address view 4-8TB)\n-\/\/  |                                                    0010 = Marked1      (Address view 8-12TB)\n-\/\/  |                                                    0100 = Remapped     (Address view 16-20TB)\n-\/\/  |                                                    1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-46 Fixed (18-bits, always zero)\n-\/\/\n-\n-\/\/ Maximum value as per spec (Power ISA v2.07): 2 ^ 60 bytes, i.e. 1 EiB (exbibyte)\n-static const unsigned int MAXIMUM_MAX_ADDRESS_BIT = 60;\n-\n-\/\/ Most modern power processors provide an address space with not more than 45 bit addressable bit,\n-\/\/ that is an address space of 32 TiB in size.\n-static const unsigned int DEFAULT_MAX_ADDRESS_BIT = 45;\n-\n-\/\/ Minimum value returned, if probing fails: 64 GiB\n-static const unsigned int MINIMUM_MAX_ADDRESS_BIT = 36;\n-\n-\/\/ Determines the highest addressable bit of the virtual address space (depends on platform)\n-\/\/ by trying to interact with memory in that address range,\n-\/\/ i.e. by syncing existing mappings (msync) or by temporarily mapping the memory area (mmap).\n-\/\/ If one of those operations succeeds, it is proven that the targeted memory area is within the virtual address space.\n-\/\/\n-\/\/ To reduce the number of required system calls to a bare minimum, the DEFAULT_MAX_ADDRESS_BIT is intentionally set\n-\/\/ lower than what the ABI would theoretically permit.\n-\/\/ Such an avoidance strategy, however, might impose unnecessary limits on processors that exceed this limit.\n-\/\/ If DEFAULT_MAX_ADDRESS_BIT is addressable, the next higher bit will be tested as well to ensure that\n-\/\/ the made assumption does not artificially restrict the memory availability.\n-static unsigned int probe_valid_max_address_bit(size_t init_bit, size_t min_bit) {\n-  assert(init_bit >= min_bit, \"Sanity\");\n-  assert(init_bit <= MAXIMUM_MAX_ADDRESS_BIT, \"Test bit is outside the assumed address space range\");\n-\n-#ifdef LINUX\n-  unsigned int max_valid_address_bit = 0;\n-  void* last_allocatable_address = nullptr;\n-\n-  const size_t page_size = os::vm_page_size();\n-\n-  for (size_t i = init_bit; i >= min_bit; --i) {\n-    void* base_addr = (void*) (((unsigned long) 1U) << i);\n-\n-    \/* ==== Try msync-ing already mapped memory page ==== *\/\n-    if (msync(base_addr, page_size, MS_ASYNC) == 0) {\n-      \/\/ The page of the given address was synced by the linux kernel and must thus be both, mapped and valid.\n-      max_valid_address_bit = i;\n-      break;\n-    }\n-    if (errno != ENOMEM) {\n-      \/\/ An unexpected error occurred, i.e. an error not indicating that the targeted memory page is unmapped,\n-      \/\/ but pointing out another type of issue.\n-      \/\/ Even though this should never happen, those issues may come up due to undefined behavior.\n-#ifdef ASSERT\n-      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n-#else \/\/ ASSERT\n-      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n-#endif \/\/ ASSERT\n-      continue;\n-    }\n-\n-    \/* ==== Try mapping memory page on our own ==== *\/\n-    last_allocatable_address = mmap(base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n-    if (last_allocatable_address != MAP_FAILED) {\n-      munmap(last_allocatable_address, page_size);\n-    }\n-\n-    if (last_allocatable_address == base_addr) {\n-      \/\/ As the linux kernel mapped exactly the page we have requested, the address must be valid.\n-      max_valid_address_bit = i;\n-      break;\n-    }\n-\n-    log_info_p(gc, init)(\"Probe failed for bit '%zu'\", i);\n-  }\n-\n-  if (max_valid_address_bit == 0) {\n-    \/\/ Probing did not bring up any usable address bit.\n-    \/\/ As an alternative, the VM evaluates the address returned by mmap as it is expected that the reserved page\n-    \/\/ will be close to the probed address that was out-of-range.\n-    \/\/ As per mmap(2), \"the kernel [will take] [the address] as a hint about where to\n-    \/\/ place the mapping; on Linux, the mapping will be created at a nearby page boundary\".\n-    \/\/ It should thus be a \"close enough\" approximation to the real virtual memory address space limit.\n-    \/\/\n-    \/\/ This recovery strategy is only applied in production builds.\n-    \/\/ In debug builds, an assertion in 'XPlatformAddressOffsetBits' will bail out the VM to indicate that\n-    \/\/ the assumed address space is no longer up-to-date.\n-    if (last_allocatable_address != MAP_FAILED) {\n-      const unsigned int bitpos = BitsPerSize_t - count_leading_zeros((size_t) last_allocatable_address) - 1;\n-      log_info_p(gc, init)(\"Did not find any valid addresses within the range, using address '%u' instead\", bitpos);\n-      return bitpos;\n-    }\n-\n-#ifdef ASSERT\n-    fatal(\"Available address space can not be determined\");\n-#else \/\/ ASSERT\n-    log_warning_p(gc)(\"Cannot determine available address space. Falling back to default value.\");\n-    return DEFAULT_MAX_ADDRESS_BIT;\n-#endif \/\/ ASSERT\n-  } else {\n-    if (max_valid_address_bit == init_bit) {\n-      \/\/ An usable address bit has been found immediately.\n-      \/\/ To ensure that the entire virtual address space is exploited, the next highest bit will be tested as well.\n-      log_info_p(gc, init)(\"Hit valid address '%u' on first try, retrying with next higher bit\", max_valid_address_bit);\n-      return MAX2(max_valid_address_bit, probe_valid_max_address_bit(init_bit + 1, init_bit + 1));\n-    }\n-  }\n-\n-  log_info_p(gc, init)(\"Found valid address '%u'\", max_valid_address_bit);\n-  return max_valid_address_bit;\n-#else \/\/ LINUX\n-  return DEFAULT_MAX_ADDRESS_BIT;\n-#endif \/\/ LINUX\n-}\n-\n-size_t XPlatformAddressOffsetBits() {\n-  const static unsigned int valid_max_address_offset_bits =\n-      probe_valid_max_address_bit(DEFAULT_MAX_ADDRESS_BIT, MINIMUM_MAX_ADDRESS_BIT) + 1;\n-  assert(valid_max_address_offset_bits >= MINIMUM_MAX_ADDRESS_BIT,\n-         \"Highest addressable bit is outside the assumed address space range\");\n-\n-  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n-  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n-  const size_t address_offset_bits = log2i_exact(address_offset);\n-\n-  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n-}\n-\n-size_t XPlatformAddressMetadataShift() {\n-  return XPlatformAddressOffsetBits();\n-}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xGlobals_ppc.cpp","additions":0,"deletions":203,"binary":false,"changes":203,"status":"deleted"},{"patch":"@@ -1,36 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef CPU_PPC_GC_X_XGLOBALS_PPC_HPP\n-#define CPU_PPC_GC_X_XGLOBALS_PPC_HPP\n-\n-#include \"globalDefinitions_ppc.hpp\"\n-\n-const size_t XPlatformHeapViews        = 3;\n-const size_t XPlatformCacheLineSize    = DEFAULT_CACHE_LINE_SIZE;\n-\n-size_t XPlatformAddressOffsetBits();\n-size_t XPlatformAddressMetadataShift();\n-\n-#endif \/\/ CPU_PPC_GC_X_XGLOBALS_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/xGlobals_ppc.hpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"deleted"},{"patch":"@@ -1,298 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ Copyright (c) 2021 SAP SE. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\n-source_hpp %{\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-\n-%}\n-\n-source %{\n-\n-static void x_load_barrier(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref,\n-                           Register tmp, uint8_t barrier_data) {\n-  if (barrier_data == XLoadBarrierElided) {\n-    return;\n-  }\n-\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  __ ld(tmp, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n-  __ and_(tmp, tmp, ref);\n-  __ bne_far(CCR0, *stub->entry(), MacroAssembler::bc_far_optimize_on_relocate);\n-  __ bind(*stub->continuation());\n-}\n-\n-static void x_load_barrier_slow_path(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref,\n-                                     Register tmp) {\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n-  __ b(*stub->entry());\n-  __ bind(*stub->continuation());\n-}\n-\n-static void x_compare_and_swap(MacroAssembler* masm, const MachNode* node,\n-                              Register res, Register mem, Register oldval, Register newval,\n-                              Register tmp_xchg, Register tmp_mask,\n-                              bool weak, bool acquire) {\n-  \/\/ z-specific load barrier requires strong CAS operations.\n-  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n-  __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n-              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, nullptr, true,\n-              weak && node->barrier_data() == XLoadBarrierElided);\n-\n-  if (node->barrier_data() != XLoadBarrierElided) {\n-    Label skip_barrier;\n-\n-    __ ld(tmp_mask, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n-    __ and_(tmp_mask, tmp_mask, tmp_xchg);\n-    __ beq(CCR0, skip_barrier);\n-\n-    \/\/ CAS must have failed because pointer in memory is bad.\n-    x_load_barrier_slow_path(masm, node, Address(mem), tmp_xchg, res \/* used as tmp *\/);\n-\n-    __ cmpxchgd(CCR0, tmp_xchg, oldval, newval, mem,\n-                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), res, nullptr, true, weak);\n-\n-    __ bind(skip_barrier);\n-  }\n-\n-  if (acquire) {\n-    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-      \/\/ Uses the isync instruction as an acquire barrier.\n-      \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n-      __ isync();\n-    } else {\n-      __ sync();\n-    }\n-  }\n-}\n-\n-static void x_compare_and_exchange(MacroAssembler* masm, const MachNode* node,\n-                                   Register res, Register mem, Register oldval, Register newval, Register tmp,\n-                                   bool weak, bool acquire) {\n-  \/\/ z-specific load barrier requires strong CAS operations.\n-  \/\/ Weak CAS operations are thus only emitted if the barrier is elided.\n-  __ cmpxchgd(CCR0, res, oldval, newval, mem,\n-              MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, nullptr, true,\n-              weak && node->barrier_data() == XLoadBarrierElided);\n-\n-  if (node->barrier_data() != XLoadBarrierElided) {\n-    Label skip_barrier;\n-    __ ld(tmp, in_bytes(XThreadLocalData::address_bad_mask_offset()), R16_thread);\n-    __ and_(tmp, tmp, res);\n-    __ beq(CCR0, skip_barrier);\n-\n-    x_load_barrier_slow_path(masm, node, Address(mem), res, tmp);\n-\n-    __ cmpxchgd(CCR0, res, oldval, newval, mem,\n-                MacroAssembler::MemBarNone, MacroAssembler::cmpxchgx_hint_atomic_update(), noreg, nullptr, true, weak);\n-\n-    __ bind(skip_barrier);\n-  }\n-\n-  if (acquire) {\n-    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-      \/\/ Uses the isync instruction as an acquire barrier.\n-      \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n-      __ isync();\n-    } else {\n-      __ sync();\n-    }\n-  }\n-}\n-\n-%}\n-\n-instruct xLoadP(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n-%{\n-  match(Set dst (LoadP mem));\n-  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n-  ins_cost(MEMORY_REF_COST);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_Load()->barrier_data() != 0)\n-            && (n->as_Load()->is_unordered() || followed_by_acquire(n)));\n-\n-  format %{ \"LD $dst, $mem\" %}\n-  ins_encode %{\n-    assert($mem$$index == 0, \"sanity\");\n-    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n-    x_load_barrier(masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-\/\/ Load Pointer Volatile\n-instruct xLoadP_acq(iRegPdst dst, memoryAlg4 mem, iRegPdst tmp, flagsRegCR0 cr0)\n-%{\n-  match(Set dst (LoadP mem));\n-  effect(TEMP_DEF dst, TEMP tmp, KILL cr0);\n-  ins_cost(3 * MEMORY_REF_COST);\n-\n-  \/\/ Predicate on instruction order is implicitly present due to the predicate of the cheaper zLoadP operation\n-  predicate(UseZGC && !ZGenerational && n->as_Load()->barrier_data() != 0);\n-\n-  format %{ \"LD acq $dst, $mem\" %}\n-  ins_encode %{\n-    __ ld($dst$$Register, $mem$$disp, $mem$$base$$Register);\n-    x_load_barrier(masm, this, Address($mem$$base$$Register, $mem$$disp), $dst$$Register, $tmp$$Register, barrier_data());\n-\n-    \/\/ Uses the isync instruction as an acquire barrier.\n-    \/\/ This exploits the compare and the branch in the z load barrier (load, compare and branch, isync).\n-    __ isync();\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xCompareAndSwapP(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                                iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n-            && (((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst));\n-\n-  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n-  ins_encode %{\n-    x_compare_and_swap(masm, this,\n-                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n-                                $tmp_xchg$$Register, $tmp_mask$$Register,\n-                                false \/* weak *\/, false \/* acquire *\/);\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xCompareAndSwapP_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                                    iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n-            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n-\n-  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n-  ins_encode %{\n-    x_compare_and_swap(masm, this,\n-                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n-                                $tmp_xchg$$Register, $tmp_mask$$Register,\n-                                false \/* weak *\/, true \/* acquire *\/);\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xCompareAndSwapPWeak(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                                    iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n-            && ((CompareAndSwapNode*)n)->order() != MemNode::acquire && ((CompareAndSwapNode*) n)->order() != MemNode::seqcst);\n-\n-  format %{ \"weak CMPXCHG $res, $mem, $oldval, $newval; as bool; ptr\" %}\n-  ins_encode %{\n-    x_compare_and_swap(masm, this,\n-                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n-                                $tmp_xchg$$Register, $tmp_mask$$Register,\n-                                true \/* weak *\/, false \/* acquire *\/);\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xCompareAndSwapPWeak_acq(iRegIdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                                        iRegPdst tmp_xchg, iRegPdst tmp_mask, flagsRegCR0 cr0) %{\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp_xchg, TEMP tmp_mask, KILL cr0);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n-            && (((CompareAndSwapNode*)n)->order() == MemNode::acquire || ((CompareAndSwapNode*) n)->order() == MemNode::seqcst));\n-\n-  format %{ \"weak CMPXCHG acq $res, $mem, $oldval, $newval; as bool; ptr\" %}\n-  ins_encode %{\n-    x_compare_and_swap(masm, this,\n-                                $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register,\n-                                $tmp_xchg$$Register, $tmp_mask$$Register,\n-                                true \/* weak *\/, true \/* acquire *\/);\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xCompareAndExchangeP(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                              iRegPdst tmp, flagsRegCR0 cr0) %{\n-  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n-            && (\n-              ((CompareAndSwapNode*)n)->order() != MemNode::acquire\n-              && ((CompareAndSwapNode*)n)->order() != MemNode::seqcst\n-            ));\n-\n-  format %{ \"CMPXCHG $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n-  ins_encode %{\n-    x_compare_and_exchange(masm, this,\n-                                    $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,\n-                                    false \/* weak *\/, false \/* acquire *\/);\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xCompareAndExchangeP_acq(iRegPdst res, iRegPdst mem, iRegPsrc oldval, iRegPsrc newval,\n-                                        iRegPdst tmp, flagsRegCR0 cr0) %{\n-  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n-  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n-\n-  predicate((UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong)\n-            && (\n-              ((CompareAndSwapNode*)n)->order() == MemNode::acquire\n-              || ((CompareAndSwapNode*)n)->order() == MemNode::seqcst\n-            ));\n-\n-  format %{ \"CMPXCHG acq $res, $mem, $oldval, $newval; as ptr; ptr\" %}\n-  ins_encode %{\n-    x_compare_and_exchange(masm, this,\n-                                    $res$$Register, $mem$$Register, $oldval$$Register, $newval$$Register, $tmp$$Register,\n-                                    false \/* weak *\/, true \/* acquire *\/);\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct xGetAndSetP(iRegPdst res, iRegPdst mem, iRegPsrc newval, iRegPdst tmp, flagsRegCR0 cr0) %{\n-  match(Set res (GetAndSetP mem newval));\n-  effect(TEMP_DEF res, TEMP tmp, KILL cr0);\n-\n-  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n-\n-  format %{ \"GetAndSetP $res, $mem, $newval\" %}\n-  ins_encode %{\n-    __ getandsetd($res$$Register, $newval$$Register, $mem$$Register, MacroAssembler::cmpxchgx_hint_atomic_update());\n-    x_load_barrier(masm, this, Address(noreg, (intptr_t) 0), $res$$Register, $tmp$$Register, barrier_data());\n-\n-    if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n-      __ isync();\n-    } else {\n-      __ sync();\n-    }\n-  %}\n-  ins_pipe(pipe_class_default);\n-%}\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/x\/x_ppc.ad","additions":0,"deletions":298,"binary":false,"changes":298,"status":"deleted"},{"patch":"@@ -145,1 +145,1 @@\n-  predicate((UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0)\n+  predicate((UseZGC && n->as_Load()->barrier_data() != 0)\n@@ -165,1 +165,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Load()->barrier_data() != 0);\n@@ -183,1 +183,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Store()->barrier_data() != 0);\n@@ -197,1 +197,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Store()->barrier_data() != 0);\n@@ -215,1 +215,1 @@\n-  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() != 0)\n@@ -234,1 +234,1 @@\n-  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() != 0)\n@@ -252,1 +252,1 @@\n-  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() != 0)\n@@ -272,1 +272,1 @@\n-  predicate((UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0)\n+  predicate((UseZGC && n->as_LoadStore()->barrier_data() != 0)\n@@ -291,1 +291,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/z\/z_ppc.ad","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n@@ -1979,1 +1978,1 @@\n-      if (UseZGC && ZGenerational) {\n+      if (UseZGC) {\n@@ -2022,1 +2021,1 @@\n-      if (UseZGC && ZGenerational) {\n+      if (UseZGC) {\n@@ -2140,1 +2139,1 @@\n-      if (UseZGC && ZGenerational) {\n+      if (UseZGC) {\n@@ -2156,1 +2155,1 @@\n-    if (UseZGC && ZGenerational) {\n+    if (UseZGC) {\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -841,4 +841,1 @@\n-    if (!(UseZGC && !ZGenerational)) {\n-      \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n-      __ verify_oop(dest->as_register());\n-    }\n+    __ verify_oop(dest->as_register());\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,454 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/codeBlob.hpp\"\n-#include \"code\/vmreg.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#ifdef COMPILER1\n-#include \"c1\/c1_LIRAssembler.hpp\"\n-#include \"c1\/c1_MacroAssembler.hpp\"\n-#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n-#endif \/\/ COMPILER1\n-#ifdef COMPILER2\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/* nothing *\/\n-#else\n-#define BLOCK_COMMENT(str) __ block_comment(str)\n-#endif\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::load_at(MacroAssembler* masm,\n-                                   DecoratorSet decorators,\n-                                   BasicType type,\n-                                   Register dst,\n-                                   Address src,\n-                                   Register tmp1,\n-                                   Register tmp2) {\n-  if (!XBarrierSet::barrier_needed(decorators, type)) {\n-    \/\/ Barrier not needed\n-    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp2);\n-    return;\n-  }\n-\n-  assert_different_registers(t1, src.base());\n-  assert_different_registers(t0, t1, dst);\n-\n-  Label done;\n-\n-  \/\/ Load bad mask into temp register.\n-  __ la(t0, src);\n-  __ ld(t1, address_bad_mask_from_thread(xthread));\n-  __ ld(dst, Address(t0));\n-\n-  \/\/ Test reference against bad mask. If mask bad, then we need to fix it up.\n-  __ andr(t1, dst, t1);\n-  __ beqz(t1, done);\n-\n-  __ enter();\n-\n-  __ push_call_clobbered_registers_except(RegSet::of(dst));\n-\n-  if (c_rarg0 != dst) {\n-    __ mv(c_rarg0, dst);\n-  }\n-\n-  __ mv(c_rarg1, t0);\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n-\n-  \/\/ Make sure dst has the return value.\n-  if (dst != x10) {\n-    __ mv(dst, x10);\n-  }\n-\n-  __ pop_call_clobbered_registers_except(RegSet::of(dst));\n-  __ leave();\n-\n-  __ bind(done);\n-}\n-\n-#ifdef ASSERT\n-\n-void XBarrierSetAssembler::store_at(MacroAssembler* masm,\n-                                    DecoratorSet decorators,\n-                                    BasicType type,\n-                                    Address dst,\n-                                    Register val,\n-                                    Register tmp1,\n-                                    Register tmp2,\n-                                    Register tmp3) {\n-  \/\/ Verify value\n-  if (is_reference_type(type)) {\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (val != noreg) {\n-      Label done;\n-\n-      \/\/ tmp1, tmp2 and tmp3 are often set to noreg.\n-      RegSet savedRegs = RegSet::of(t0);\n-      __ push_reg(savedRegs, sp);\n-\n-      __ ld(t0, address_bad_mask_from_thread(xthread));\n-      __ andr(t0, val, t0);\n-      __ beqz(t0, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n-      __ bind(done);\n-      __ pop_reg(savedRegs, sp);\n-    }\n-  }\n-\n-  \/\/ Store value\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, val, tmp1, tmp2, noreg);\n-}\n-\n-#endif \/\/ ASSERT\n-\n-void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,\n-                                              DecoratorSet decorators,\n-                                              bool is_oop,\n-                                              Register src,\n-                                              Register dst,\n-                                              Register count,\n-                                              RegSet saved_regs) {\n-  if (!is_oop) {\n-    \/\/ Barrier not needed\n-    return;\n-  }\n-\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::arraycopy_prologue {\");\n-\n-  assert_different_registers(src, count, t0);\n-\n-  __ push_reg(saved_regs, sp);\n-\n-  if (count == c_rarg0 && src == c_rarg1) {\n-    \/\/ exactly backwards!!\n-    __ xorr(c_rarg0, c_rarg0, c_rarg1);\n-    __ xorr(c_rarg1, c_rarg0, c_rarg1);\n-    __ xorr(c_rarg0, c_rarg0, c_rarg1);\n-  } else {\n-    __ mv(c_rarg0, src);\n-    __ mv(c_rarg1, count);\n-  }\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_array_addr(), 2);\n-\n-  __ pop_reg(saved_regs, sp);\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::arraycopy_prologue\");\n-}\n-\n-void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,\n-                                                         Register jni_env,\n-                                                         Register robj,\n-                                                         Register tmp,\n-                                                         Label& slowpath) {\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::try_resolve_jobject_in_native {\");\n-\n-  assert_different_registers(jni_env, robj, tmp);\n-\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, robj, tmp, slowpath);\n-\n-  \/\/ Compute the offset of address bad mask from the field of jni_environment\n-  long int bad_mask_relative_offset = (long int) (in_bytes(XThreadLocalData::address_bad_mask_offset()) -\n-                                                  in_bytes(JavaThread::jni_environment_offset()));\n-\n-  \/\/ Load the address bad mask\n-  __ ld(tmp, Address(jni_env, bad_mask_relative_offset));\n-\n-  \/\/ Check address bad mask\n-  __ andr(tmp, robj, tmp);\n-  __ bnez(tmp, slowpath);\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::try_resolve_jobject_in_native\");\n-}\n-\n-#ifdef COMPILER2\n-\n-OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  if (!OptoReg::is_reg(opto_reg)) {\n-    return OptoReg::Bad;\n-  }\n-\n-  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-  if (vm_reg->is_FloatRegister()) {\n-    return opto_reg & ~1;\n-  }\n-\n-  return opto_reg;\n-}\n-\n-#undef __\n-#define __ _masm->\n-\n-class XSaveLiveRegisters {\n-private:\n-  MacroAssembler* const _masm;\n-  RegSet                _gp_regs;\n-  FloatRegSet           _fp_regs;\n-  VectorRegSet          _vp_regs;\n-\n-public:\n-  void initialize(XLoadBarrierStubC2* stub) {\n-    \/\/ Record registers that needs to be saved\/restored\n-    RegMaskIterator rmi(stub->live());\n-    while (rmi.has_next()) {\n-      const OptoReg::Name opto_reg = rmi.next();\n-      if (OptoReg::is_reg(opto_reg)) {\n-        const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-        if (vm_reg->is_Register()) {\n-          _gp_regs += RegSet::of(vm_reg->as_Register());\n-        } else if (vm_reg->is_FloatRegister()) {\n-          _fp_regs += FloatRegSet::of(vm_reg->as_FloatRegister());\n-        } else if (vm_reg->is_VectorRegister()) {\n-          const VMReg vm_reg_base = OptoReg::as_VMReg(opto_reg & ~(VectorRegister::max_slots_per_register - 1));\n-          _vp_regs += VectorRegSet::of(vm_reg_base->as_VectorRegister());\n-        } else {\n-          fatal(\"Unknown register type\");\n-        }\n-      }\n-    }\n-\n-    \/\/ Remove C-ABI SOE registers, tmp regs and _ref register that will be updated\n-    _gp_regs -= RegSet::range(x18, x27) + RegSet::of(x2) + RegSet::of(x8, x9) + RegSet::of(x5, stub->ref());\n-  }\n-\n-  XSaveLiveRegisters(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _gp_regs(),\n-      _fp_regs(),\n-      _vp_regs() {\n-    \/\/ Figure out what registers to save\/restore\n-    initialize(stub);\n-\n-    \/\/ Save registers\n-    __ push_reg(_gp_regs, sp);\n-    __ push_fp(_fp_regs, sp);\n-    __ push_v(_vp_regs, sp);\n-  }\n-\n-  ~XSaveLiveRegisters() {\n-    \/\/ Restore registers\n-    __ pop_v(_vp_regs, sp);\n-    __ pop_fp(_fp_regs, sp);\n-    __ pop_reg(_gp_regs, sp);\n-  }\n-};\n-\n-class XSetupArguments {\n-private:\n-  MacroAssembler* const _masm;\n-  const Register        _ref;\n-  const Address         _ref_addr;\n-\n-public:\n-  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _ref(stub->ref()),\n-      _ref_addr(stub->ref_addr()) {\n-\n-    \/\/ Setup arguments\n-    if (_ref_addr.base() == noreg) {\n-      \/\/ No self healing\n-      if (_ref != c_rarg0) {\n-        __ mv(c_rarg0, _ref);\n-      }\n-      __ mv(c_rarg1, zr);\n-    } else {\n-      \/\/ Self healing\n-      if (_ref == c_rarg0) {\n-        \/\/ _ref is already at correct place\n-        __ la(c_rarg1, _ref_addr);\n-      } else if (_ref != c_rarg1) {\n-        \/\/ _ref is in wrong place, but not in c_rarg1, so fix it first\n-        __ la(c_rarg1, _ref_addr);\n-        __ mv(c_rarg0, _ref);\n-      } else if (_ref_addr.base() != c_rarg0) {\n-        assert(_ref == c_rarg1, \"Mov ref first, vacating c_rarg0\");\n-        __ mv(c_rarg0, _ref);\n-        __ la(c_rarg1, _ref_addr);\n-      } else {\n-        assert(_ref == c_rarg1, \"Need to vacate c_rarg1 and _ref_addr is using c_rarg0\");\n-        if (_ref_addr.base() == c_rarg0) {\n-          __ mv(t1, c_rarg1);\n-          __ la(c_rarg1, _ref_addr);\n-          __ mv(c_rarg0, t1);\n-        } else {\n-          ShouldNotReachHere();\n-        }\n-      }\n-    }\n-  }\n-\n-  ~XSetupArguments() {\n-    \/\/ Transfer result\n-    if (_ref != x10) {\n-      __ mv(_ref, x10);\n-    }\n-  }\n-};\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n-  BLOCK_COMMENT(\"XLoadBarrierStubC2\");\n-\n-  \/\/ Stub entry\n-  __ bind(*stub->entry());\n-\n-  {\n-    XSaveLiveRegisters save_live_registers(masm, stub);\n-    XSetupArguments setup_arguments(masm, stub);\n-\n-    __ mv(t0, stub->slow_path());\n-    __ jalr(t0);\n-  }\n-\n-  \/\/ Stub exit\n-  __ j(*stub->continuation());\n-}\n-\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-#undef __\n-#define __ ce->masm()->\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  assert_different_registers(xthread, ref->as_register(), t1);\n-  __ ld(t1, address_bad_mask_from_thread(xthread));\n-  __ andr(t1, t1, ref->as_register());\n-}\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                                         XLoadBarrierStubC1* stub) const {\n-  \/\/ Stub entry\n-  __ bind(*stub->entry());\n-\n-  Register ref = stub->ref()->as_register();\n-  Register ref_addr = noreg;\n-  Register tmp = noreg;\n-\n-  if (stub->tmp()->is_valid()) {\n-    \/\/ Load address into tmp register\n-    ce->leal(stub->ref_addr(), stub->tmp());\n-    ref_addr = tmp = stub->tmp()->as_pointer_register();\n-  } else {\n-    \/\/ Address already in register\n-    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n-  }\n-\n-  assert_different_registers(ref, ref_addr, noreg);\n-\n-  \/\/ Save x10 unless it is the result or tmp register\n-  \/\/ Set up SP to accommodate parameters and maybe x10.\n-  if (ref != x10 && tmp != x10) {\n-    __ sub(sp, sp, 32);\n-    __ sd(x10, Address(sp, 16));\n-  } else {\n-    __ sub(sp, sp, 16);\n-  }\n-\n-  \/\/ Setup arguments and call runtime stub\n-  ce->store_parameter(ref_addr, 1);\n-  ce->store_parameter(ref, 0);\n-\n-  __ far_call(stub->runtime_stub());\n-\n-  \/\/ Verify result\n-  __ verify_oop(x10);\n-\n-\n-  \/\/ Move result into place\n-  if (ref != x10) {\n-    __ mv(ref, x10);\n-  }\n-\n-  \/\/ Restore x10 unless it is the result or tmp register\n-  if (ref != x10 && tmp != x10) {\n-    __ ld(x10, Address(sp, 16));\n-    __ add(sp, sp, 32);\n-  } else {\n-    __ add(sp, sp, 16);\n-  }\n-\n-  \/\/ Stub exit\n-  __ j(*stub->continuation());\n-}\n-\n-#undef __\n-#define __ sasm->\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                                                 DecoratorSet decorators) const {\n-  __ prologue(\"zgc_load_barrier stub\", false);\n-\n-  __ push_call_clobbered_registers_except(RegSet::of(x10));\n-\n-  \/\/ Setup arguments\n-  __ load_parameter(0, c_rarg0);\n-  __ load_parameter(1, c_rarg1);\n-\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), 2);\n-\n-  __ pop_call_clobbered_registers_except(RegSet::of(x10));\n-\n-  __ epilogue();\n-}\n-\n-#endif \/\/ COMPILER1\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error) {\n-  \/\/ Check if mask is good.\n-  \/\/ verifies that XAddressBadMask & obj == 0\n-  __ ld(tmp2, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp1, obj, tmp2);\n-  __ bnez(tmp1, error);\n-\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n-}\n-\n-#undef __\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xBarrierSetAssembler_riscv.cpp","additions":0,"deletions":454,"binary":false,"changes":454,"status":"deleted"},{"patch":"@@ -1,112 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_RISCV_GC_X_XBARRIERSETASSEMBLER_RISCV_HPP\n-#define CPU_RISCV_GC_X_XBARRIERSETASSEMBLER_RISCV_HPP\n-\n-#include \"code\/vmreg.hpp\"\n-#include \"oops\/accessDecorators.hpp\"\n-#ifdef COMPILER2\n-#include \"opto\/optoreg.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class LIR_Assembler;\n-class LIR_Opr;\n-class StubAssembler;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class Node;\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class XLoadBarrierStubC1;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class XLoadBarrierStubC2;\n-#endif \/\/ COMPILER2\n-\n-class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n-public:\n-  virtual void load_at(MacroAssembler* masm,\n-                       DecoratorSet decorators,\n-                       BasicType type,\n-                       Register dst,\n-                       Address src,\n-                       Register tmp1,\n-                       Register tmp2);\n-\n-#ifdef ASSERT\n-  virtual void store_at(MacroAssembler* masm,\n-                        DecoratorSet decorators,\n-                        BasicType type,\n-                        Address dst,\n-                        Register val,\n-                        Register tmp1,\n-                        Register tmp2,\n-                        Register tmp3);\n-#endif \/\/ ASSERT\n-\n-  virtual void arraycopy_prologue(MacroAssembler* masm,\n-                                  DecoratorSet decorators,\n-                                  bool is_oop,\n-                                  Register src,\n-                                  Register dst,\n-                                  Register count,\n-                                  RegSet saved_regs);\n-\n-  virtual void try_resolve_jobject_in_native(MacroAssembler* masm,\n-                                             Register jni_env,\n-                                             Register robj,\n-                                             Register tmp,\n-                                             Label& slowpath);\n-\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n-\n-#ifdef COMPILER1\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n-\n-  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                     XLoadBarrierStubC1* stub) const;\n-\n-  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                             DecoratorSet decorators) const;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-  OptoReg::Name refine_register(const Node* node,\n-                                OptoReg::Name opto_reg);\n-\n-  void generate_c2_load_barrier_stub(MacroAssembler* masm,\n-                                     XLoadBarrierStubC2* stub) const;\n-#endif \/\/ COMPILER2\n-\n-  void check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error);\n-};\n-\n-#endif \/\/ CPU_RISCV_GC_X_XBARRIERSETASSEMBLER_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xBarrierSetAssembler_riscv.hpp","additions":0,"deletions":112,"binary":false,"changes":112,"status":"deleted"},{"patch":"@@ -1,212 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-#ifdef LINUX\n-#include <sys\/mman.h>\n-#endif \/\/ LINUX\n-\n-\/\/\n-\/\/ The heap can have three different layouts, depending on the max heap size.\n-\/\/\n-\/\/ Address Space & Pointer Layout 1\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000014000000000 (20TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000010000000000 (16TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x00000c0000000000 (12TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000040000000000 (4TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                  4 4  4 4\n-\/\/   3                  6 5  2 1                                             0\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |00000000 00000000 00|1111|11 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |                    |    |\n-\/\/  |                    |    * 41-0 Object Offset (42-bits, 4TB address space)\n-\/\/  |                    |\n-\/\/  |                    * 45-42 Metadata Bits (4-bits)  0001 = Marked0      (Address view 4-8TB)\n-\/\/  |                                                    0010 = Marked1      (Address view 8-12TB)\n-\/\/  |                                                    0100 = Remapped     (Address view 16-20TB)\n-\/\/  |                                                    1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-46 Fixed (18-bits, always zero)\n-\/\/\n-\/\/\n-\/\/ Address Space & Pointer Layout 2\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000280000000000 (40TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000200000000000 (32TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000180000000000 (24TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                 4 4  4 4\n-\/\/   3                 7 6  3 2                                              0\n-\/\/  +------------------+-----+------------------------------------------------+\n-\/\/  |00000000 00000000 0|1111|111 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +-------------------+----+------------------------------------------------+\n-\/\/  |                   |    |\n-\/\/  |                   |    * 42-0 Object Offset (43-bits, 8TB address space)\n-\/\/  |                   |\n-\/\/  |                   * 46-43 Metadata Bits (4-bits)  0001 = Marked0      (Address view 8-16TB)\n-\/\/  |                                                   0010 = Marked1      (Address view 16-24TB)\n-\/\/  |                                                   0100 = Remapped     (Address view 32-40TB)\n-\/\/  |                                                   1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-47 Fixed (17-bits, always zero)\n-\/\/\n-\/\/\n-\/\/ Address Space & Pointer Layout 3\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000500000000000 (80TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000400000000000 (64TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000300000000000 (48TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000200000000000 (32TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6               4  4  4 4\n-\/\/   3               8  7  4 3                                               0\n-\/\/  +------------------+----+-------------------------------------------------+\n-\/\/  |00000000 00000000 |1111|1111 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +------------------+----+-------------------------------------------------+\n-\/\/  |                  |    |\n-\/\/  |                  |    * 43-0 Object Offset (44-bits, 16TB address space)\n-\/\/  |                  |\n-\/\/  |                  * 47-44 Metadata Bits (4-bits)  0001 = Marked0      (Address view 16-32TB)\n-\/\/  |                                                  0010 = Marked1      (Address view 32-48TB)\n-\/\/  |                                                  0100 = Remapped     (Address view 64-80TB)\n-\/\/  |                                                  1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-48 Fixed (16-bits, always zero)\n-\/\/\n-\n-\/\/ Default value if probing is not implemented for a certain platform: 128TB\n-static const size_t DEFAULT_MAX_ADDRESS_BIT = 47;\n-\/\/ Minimum value returned, if probing fails: 64GB\n-static const size_t MINIMUM_MAX_ADDRESS_BIT = 36;\n-\n-static size_t probe_valid_max_address_bit() {\n-#ifdef LINUX\n-  size_t max_address_bit = 0;\n-  const size_t page_size = os::vm_page_size();\n-  for (size_t i = DEFAULT_MAX_ADDRESS_BIT; i > MINIMUM_MAX_ADDRESS_BIT; --i) {\n-    const uintptr_t base_addr = ((uintptr_t) 1U) << i;\n-    if (msync((void*)base_addr, page_size, MS_ASYNC) == 0) {\n-      \/\/ msync succeeded, the address is valid, and maybe even already mapped.\n-      max_address_bit = i;\n-      break;\n-    }\n-    if (errno != ENOMEM) {\n-      \/\/ Some error occurred. This should never happen, but msync\n-      \/\/ has some undefined behavior, hence ignore this bit.\n-#ifdef ASSERT\n-      fatal(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n-#else \/\/ ASSERT\n-      log_warning_p(gc)(\"Received '%s' while probing the address space for the highest valid bit\", os::errno_name(errno));\n-#endif \/\/ ASSERT\n-      continue;\n-    }\n-    \/\/ Since msync failed with ENOMEM, the page might not be mapped.\n-    \/\/ Try to map it, to see if the address is valid.\n-    void* const result_addr = mmap((void*) base_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n-    if (result_addr != MAP_FAILED) {\n-      munmap(result_addr, page_size);\n-    }\n-    if ((uintptr_t) result_addr == base_addr) {\n-      \/\/ address is valid\n-      max_address_bit = i;\n-      break;\n-    }\n-  }\n-  if (max_address_bit == 0) {\n-    \/\/ probing failed, allocate a very high page and take that bit as the maximum\n-    const uintptr_t high_addr = ((uintptr_t) 1U) << DEFAULT_MAX_ADDRESS_BIT;\n-    void* const result_addr = mmap((void*) high_addr, page_size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE, -1, 0);\n-    if (result_addr != MAP_FAILED) {\n-      max_address_bit = BitsPerSize_t - count_leading_zeros((size_t) result_addr) - 1;\n-      munmap(result_addr, page_size);\n-    }\n-  }\n-  log_info_p(gc, init)(\"Probing address space for the highest valid bit: \" SIZE_FORMAT, max_address_bit);\n-  return MAX2(max_address_bit, MINIMUM_MAX_ADDRESS_BIT);\n-#else \/\/ LINUX\n-  return DEFAULT_MAX_ADDRESS_BIT;\n-#endif \/\/ LINUX\n-}\n-\n-size_t XPlatformAddressOffsetBits() {\n-  const static size_t valid_max_address_offset_bits = probe_valid_max_address_bit() + 1;\n-  const size_t max_address_offset_bits = valid_max_address_offset_bits - 3;\n-  const size_t min_address_offset_bits = max_address_offset_bits - 2;\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n-  const size_t address_offset_bits = log2i_exact(address_offset);\n-  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n-}\n-\n-size_t XPlatformAddressMetadataShift() {\n-  return XPlatformAddressOffsetBits();\n-}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xGlobals_riscv.cpp","additions":0,"deletions":212,"binary":false,"changes":212,"status":"deleted"},{"patch":"@@ -1,35 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_RISCV_GC_X_XGLOBALS_RISCV_HPP\n-#define CPU_RISCV_GC_X_XGLOBALS_RISCV_HPP\n-\n-const size_t XPlatformHeapViews        = 3;\n-const size_t XPlatformCacheLineSize    = 64;\n-\n-size_t XPlatformAddressOffsetBits();\n-size_t XPlatformAddressMetadataShift();\n-\n-#endif \/\/ CPU_RISCV_GC_X_XGLOBALS_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xGlobals_riscv.hpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"deleted"},{"patch":"@@ -1,229 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\n-source_hpp %{\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-\n-%}\n-\n-source %{\n-\n-static void x_load_barrier(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, int barrier_data) {\n-  if (barrier_data == XLoadBarrierElided) {\n-    return;\n-  }\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  __ ld(tmp, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n-  __ andr(tmp, tmp, ref);\n-  __ bnez(tmp, *stub->entry(), true \/* far *\/);\n-  __ bind(*stub->continuation());\n-}\n-\n-static void x_load_barrier_slow_path(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref, Register tmp) {\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n-  __ j(*stub->entry());\n-  __ bind(*stub->continuation());\n-}\n-\n-%}\n-\n-\/\/ Load Pointer\n-instruct xLoadP(iRegPNoSp dst, memory mem, iRegPNoSp tmp)\n-%{\n-  match(Set dst (LoadP mem));\n-  predicate(UseZGC && !ZGenerational && (n->as_Load()->barrier_data() != 0));\n-  effect(TEMP dst, TEMP tmp);\n-\n-  ins_cost(4 * DEFAULT_COST);\n-\n-  format %{ \"ld  $dst, $mem, #@zLoadP\" %}\n-\n-  ins_encode %{\n-    const Address ref_addr (as_Register($mem$$base), $mem$$disp);\n-    __ ld($dst$$Register, ref_addr);\n-    x_load_barrier(masm, this, ref_addr, $dst$$Register, $tmp$$Register \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(iload_reg_mem);\n-%}\n-\n-instruct xCompareAndSwapP(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(TEMP_DEF res, TEMP tmp);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $mem, $oldval, $newval, #@zCompareAndSwapP\\n\\t\"\n-            \"mv $res, $res == $oldval\" %}\n-\n-  ins_encode %{\n-    Label failed;\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $tmp$$Register);\n-    __ sub(t0, $tmp$$Register, $oldval$$Register);\n-    __ seqz($res$$Register, t0);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ld(t0, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(t0, t0, $tmp$$Register);\n-      __ beqz(t0, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), $tmp$$Register \/* ref *\/, $res$$Register \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n-                 true \/* result_as_bool *\/);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xCompareAndSwapPAcq(iRegINoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load_reserved(n) && (n->as_LoadStore()->barrier_data() == XLoadBarrierStrong));\n-  effect(TEMP_DEF res, TEMP tmp);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $mem, $oldval, $newval, #@zCompareAndSwapPAcq\\n\\t\"\n-            \"mv $res, $res == $oldval\" %}\n-\n-  ins_encode %{\n-    Label failed;\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $tmp$$Register);\n-    __ sub(t0, $tmp$$Register, $oldval$$Register);\n-    __ seqz($res$$Register, t0);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ld(t0, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(t0, t0, $tmp$$Register);\n-      __ beqz(t0, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), $tmp$$Register \/* ref *\/, $res$$Register \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register,\n-                 true \/* result_as_bool *\/);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xCompareAndExchangeP(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp) %{\n-  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(TEMP_DEF res, TEMP tmp);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $res = $mem, $oldval, $newval, #@zCompareAndExchangeP\" %}\n-\n-  ins_encode %{\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ld(t0, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(t0, t0, $res$$Register);\n-      __ beqz(t0, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, $tmp$$Register \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::relaxed \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xCompareAndExchangePAcq(iRegPNoSp res, indirect mem, iRegP oldval, iRegP newval, iRegPNoSp tmp) %{\n-  match(Set res (CompareAndExchangeP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(TEMP_DEF res, TEMP tmp);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"cmpxchg $res = $mem, $oldval, $newval, #@zCompareAndExchangePAcq\" %}\n-\n-  ins_encode %{\n-    guarantee($mem$$index == -1 && $mem$$disp == 0, \"impossible encoding\");\n-    __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-               Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      Label good;\n-      __ ld(t0, Address(xthread, XThreadLocalData::address_bad_mask_offset()));\n-      __ andr(t0, t0, $res$$Register);\n-      __ beqz(t0, good);\n-      x_load_barrier_slow_path(masm, this, Address($mem$$Register), $res$$Register \/* ref *\/, $tmp$$Register \/* tmp *\/);\n-      __ cmpxchg($mem$$Register, $oldval$$Register, $newval$$Register, Assembler::int64,\n-                 Assembler::aq \/* acquire *\/, Assembler::rl \/* release *\/, $res$$Register);\n-      __ bind(good);\n-    }\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct xGetAndSetP(indirect mem, iRegP newv, iRegPNoSp prev, iRegPNoSp tmp) %{\n-  match(Set prev (GetAndSetP mem newv));\n-  predicate(UseZGC && !ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n-  effect(TEMP_DEF prev, TEMP tmp);\n-\n-  ins_cost(2 * VOLATILE_REF_COST);\n-\n-  format %{ \"atomic_xchg  $prev, $newv, [$mem], #@zGetAndSetP\" %}\n-\n-  ins_encode %{\n-    __ atomic_xchg($prev$$Register, $newv$$Register, as_Register($mem$$base));\n-    x_load_barrier(masm, this, Address(noreg, 0), $prev$$Register, $tmp$$Register \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct xGetAndSetPAcq(indirect mem, iRegP newv, iRegPNoSp prev, iRegPNoSp tmp) %{\n-  match(Set prev (GetAndSetP mem newv));\n-  predicate(UseZGC && !ZGenerational && needs_acquiring_load_reserved(n) && (n->as_LoadStore()->barrier_data() != 0));\n-  effect(TEMP_DEF prev, TEMP tmp);\n-\n-  ins_cost(VOLATILE_REF_COST);\n-\n-  format %{ \"atomic_xchg_acq  $prev, $newv, [$mem], #@zGetAndSetPAcq\" %}\n-\n-  ins_encode %{\n-    __ atomic_xchgal($prev$$Register, $newv$$Register, as_Register($mem$$base));\n-    x_load_barrier(masm, this, Address(noreg, 0), $prev$$Register, $tmp$$Register \/* tmp *\/, barrier_data());\n-  %}\n-  ins_pipe(pipe_serial);\n-%}\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/x_riscv.ad","additions":0,"deletions":229,"binary":false,"changes":229,"status":"deleted"},{"patch":"@@ -96,1 +96,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Load()->barrier_data() != 0);\n@@ -115,1 +115,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Store()->barrier_data() != 0);\n@@ -133,1 +133,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -156,1 +156,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -178,1 +178,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -200,1 +200,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -221,1 +221,1 @@\n-  predicate(UseZGC && ZGenerational && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && !needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n@@ -239,1 +239,1 @@\n-  predicate(UseZGC && ZGenerational && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && needs_acquiring_load_reserved(n) && n->as_LoadStore()->barrier_data() != 0);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/z_riscv.ad","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -949,1 +949,1 @@\n-    bool is_far = UseZGC && ZGenerational;\n+    bool is_far = UseZGC;\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1336,4 +1336,1 @@\n-    if (!(UseZGC && !ZGenerational)) {\n-      \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n-      __ verify_oop(dest->as_register());\n-    }\n+    __ verify_oop(dest->as_register());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,734 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/codeBlob.hpp\"\n-#include \"code\/vmreg.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#ifdef COMPILER1\n-#include \"c1\/c1_LIRAssembler.hpp\"\n-#include \"c1\/c1_MacroAssembler.hpp\"\n-#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n-#endif \/\/ COMPILER1\n-#ifdef COMPILER2\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#endif \/\/ COMPILER2\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/* nothing *\/\n-#else\n-#define BLOCK_COMMENT(str) __ block_comment(str)\n-#endif\n-\n-#undef __\n-#define __ masm->\n-\n-static void call_vm(MacroAssembler* masm,\n-                    address entry_point,\n-                    Register arg0,\n-                    Register arg1) {\n-  \/\/ Setup arguments\n-  if (arg1 == c_rarg0) {\n-    if (arg0 == c_rarg1) {\n-      __ xchgptr(c_rarg1, c_rarg0);\n-    } else {\n-      __ movptr(c_rarg1, arg1);\n-      __ movptr(c_rarg0, arg0);\n-    }\n-  } else {\n-    if (arg0 != c_rarg0) {\n-      __ movptr(c_rarg0, arg0);\n-    }\n-    if (arg1 != c_rarg1) {\n-      __ movptr(c_rarg1, arg1);\n-    }\n-  }\n-\n-  \/\/ Call VM\n-  __ MacroAssembler::call_VM_leaf_base(entry_point, 2);\n-}\n-\n-void XBarrierSetAssembler::load_at(MacroAssembler* masm,\n-                                   DecoratorSet decorators,\n-                                   BasicType type,\n-                                   Register dst,\n-                                   Address src,\n-                                   Register tmp1,\n-                                   Register tmp_thread) {\n-  if (!XBarrierSet::barrier_needed(decorators, type)) {\n-    \/\/ Barrier not needed\n-    BarrierSetAssembler::load_at(masm, decorators, type, dst, src, tmp1, tmp_thread);\n-    return;\n-  }\n-\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::load_at {\");\n-\n-  \/\/ Allocate scratch register\n-  Register scratch = tmp1;\n-  if (tmp1 == noreg) {\n-    scratch = r12;\n-    __ push(scratch);\n-  }\n-\n-  assert_different_registers(dst, scratch);\n-\n-  Label done;\n-\n-  \/\/\n-  \/\/ Fast Path\n-  \/\/\n-\n-  \/\/ Load address\n-  __ lea(scratch, src);\n-\n-  \/\/ Load oop at address\n-  __ movptr(dst, Address(scratch, 0));\n-\n-  \/\/ Test address bad mask\n-  __ testptr(dst, address_bad_mask_from_thread(r15_thread));\n-  __ jcc(Assembler::zero, done);\n-\n-  \/\/\n-  \/\/ Slow path\n-  \/\/\n-\n-  \/\/ Save registers\n-  __ push(rax);\n-  __ push(rcx);\n-  __ push(rdx);\n-  __ push(rdi);\n-  __ push(rsi);\n-  __ push(r8);\n-  __ push(r9);\n-  __ push(r10);\n-  __ push(r11);\n-\n-  \/\/ We may end up here from generate_native_wrapper, then the method may have\n-  \/\/ floats as arguments, and we must spill them before calling the VM runtime\n-  \/\/ leaf. From the interpreter all floats are passed on the stack.\n-  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-  const int xmm_size = wordSize * 2;\n-  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-  __ subptr(rsp, xmm_spill_size);\n-  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n-\n-  \/\/ Call VM\n-  call_vm(masm, XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);\n-\n-  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-  __ addptr(rsp, xmm_spill_size);\n-\n-  __ pop(r11);\n-  __ pop(r10);\n-  __ pop(r9);\n-  __ pop(r8);\n-  __ pop(rsi);\n-  __ pop(rdi);\n-  __ pop(rdx);\n-  __ pop(rcx);\n-\n-  if (dst == rax) {\n-    __ addptr(rsp, wordSize);\n-  } else {\n-    __ movptr(dst, rax);\n-    __ pop(rax);\n-  }\n-\n-  __ bind(done);\n-\n-  \/\/ Restore scratch register\n-  if (tmp1 == noreg) {\n-    __ pop(scratch);\n-  }\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::load_at\");\n-}\n-\n-#ifdef ASSERT\n-\n-void XBarrierSetAssembler::store_at(MacroAssembler* masm,\n-                                    DecoratorSet decorators,\n-                                    BasicType type,\n-                                    Address dst,\n-                                    Register src,\n-                                    Register tmp1,\n-                                    Register tmp2,\n-                                    Register tmp3) {\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::store_at {\");\n-\n-  \/\/ Verify oop store\n-  if (is_reference_type(type)) {\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (src != noreg) {\n-      Label done;\n-      __ testptr(src, address_bad_mask_from_thread(r15_thread));\n-      __ jcc(Assembler::zero, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n-      __ bind(done);\n-    }\n-  }\n-\n-  \/\/ Store value\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2, tmp3);\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::store_at\");\n-}\n-\n-#endif \/\/ ASSERT\n-\n-void XBarrierSetAssembler::arraycopy_prologue(MacroAssembler* masm,\n-                                              DecoratorSet decorators,\n-                                              BasicType type,\n-                                              Register src,\n-                                              Register dst,\n-                                              Register count) {\n-  if (!XBarrierSet::barrier_needed(decorators, type)) {\n-    \/\/ Barrier not needed\n-    return;\n-  }\n-\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::arraycopy_prologue {\");\n-\n-  \/\/ Save registers\n-  __ pusha();\n-\n-  \/\/ Call VM\n-  call_vm(masm, XBarrierSetRuntime::load_barrier_on_oop_array_addr(), src, count);\n-\n-  \/\/ Restore registers\n-  __ popa();\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::arraycopy_prologue\");\n-}\n-\n-void XBarrierSetAssembler::try_resolve_jobject_in_native(MacroAssembler* masm,\n-                                                         Register jni_env,\n-                                                         Register obj,\n-                                                         Register tmp,\n-                                                         Label& slowpath) {\n-  BLOCK_COMMENT(\"XBarrierSetAssembler::try_resolve_jobject_in_native {\");\n-\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, obj, tmp, slowpath);\n-\n-  \/\/ Test address bad mask\n-  __ testptr(obj, address_bad_mask_from_jni_env(jni_env));\n-  __ jcc(Assembler::notZero, slowpath);\n-\n-  BLOCK_COMMENT(\"} XBarrierSetAssembler::try_resolve_jobject_in_native\");\n-}\n-\n-#ifdef COMPILER1\n-\n-#undef __\n-#define __ ce->masm()->\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  __ testptr(ref->as_register(), address_bad_mask_from_thread(r15_thread));\n-}\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                                         XLoadBarrierStubC1* stub) const {\n-  \/\/ Stub entry\n-  __ bind(*stub->entry());\n-\n-  Register ref = stub->ref()->as_register();\n-  Register ref_addr = noreg;\n-  Register tmp = noreg;\n-\n-  if (stub->tmp()->is_valid()) {\n-    \/\/ Load address into tmp register\n-    ce->leal(stub->ref_addr(), stub->tmp());\n-    ref_addr = tmp = stub->tmp()->as_pointer_register();\n-  } else {\n-    \/\/ Address already in register\n-    ref_addr = stub->ref_addr()->as_address_ptr()->base()->as_pointer_register();\n-  }\n-\n-  assert_different_registers(ref, ref_addr, noreg);\n-\n-  \/\/ Save rax unless it is the result or tmp register\n-  if (ref != rax && tmp != rax) {\n-    __ push(rax);\n-  }\n-\n-  \/\/ Setup arguments and call runtime stub\n-  __ subptr(rsp, 2 * BytesPerWord);\n-  ce->store_parameter(ref_addr, 1);\n-  ce->store_parameter(ref, 0);\n-  __ call(RuntimeAddress(stub->runtime_stub()));\n-  __ addptr(rsp, 2 * BytesPerWord);\n-\n-  \/\/ Verify result\n-  __ verify_oop(rax);\n-\n-  \/\/ Move result into place\n-  if (ref != rax) {\n-    __ movptr(ref, rax);\n-  }\n-\n-  \/\/ Restore rax unless it is the result or tmp register\n-  if (ref != rax && tmp != rax) {\n-    __ pop(rax);\n-  }\n-\n-  \/\/ Stub exit\n-  __ jmp(*stub->continuation());\n-}\n-\n-#undef __\n-#define __ sasm->\n-\n-void XBarrierSetAssembler::generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                                                 DecoratorSet decorators) const {\n-  \/\/ Enter and save registers\n-  __ enter();\n-  __ save_live_registers_no_oop_map(true \/* save_fpu_registers *\/);\n-\n-  \/\/ Setup arguments\n-  __ load_parameter(1, c_rarg1);\n-  __ load_parameter(0, c_rarg0);\n-\n-  \/\/ Call VM\n-  __ call_VM_leaf(XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), c_rarg0, c_rarg1);\n-\n-  \/\/ Restore registers and return\n-  __ restore_live_registers_except_rax(true \/* restore_fpu_registers *\/);\n-  __ leave();\n-  __ ret(0);\n-}\n-\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-\n-OptoReg::Name XBarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  if (!OptoReg::is_reg(opto_reg)) {\n-    return OptoReg::Bad;\n-  }\n-\n-  const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-  if (vm_reg->is_XMMRegister()) {\n-    opto_reg &= ~15;\n-    switch (node->ideal_reg()) {\n-      case Op_VecX:\n-        opto_reg |= 2;\n-        break;\n-      case Op_VecY:\n-        opto_reg |= 4;\n-        break;\n-      case Op_VecZ:\n-        opto_reg |= 8;\n-        break;\n-      default:\n-        opto_reg |= 1;\n-        break;\n-    }\n-  }\n-\n-  return opto_reg;\n-}\n-\n-\/\/ We use the vec_spill_helper from the x86.ad file to avoid reinventing this wheel\n-extern void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n-                            int stack_offset, int reg, uint ireg, outputStream* st);\n-\n-#undef __\n-#define __ _masm->\n-\n-class XSaveLiveRegisters {\n-private:\n-  struct XMMRegisterData {\n-    XMMRegister _reg;\n-    int         _size;\n-\n-    \/\/ Used by GrowableArray::find()\n-    bool operator == (const XMMRegisterData& other) {\n-      return _reg == other._reg;\n-    }\n-  };\n-\n-  MacroAssembler* const          _masm;\n-  GrowableArray<Register>        _gp_registers;\n-  GrowableArray<KRegister>       _opmask_registers;\n-  GrowableArray<XMMRegisterData> _xmm_registers;\n-  int                            _spill_size;\n-  int                            _spill_offset;\n-\n-  static int xmm_compare_register_size(XMMRegisterData* left, XMMRegisterData* right) {\n-    if (left->_size == right->_size) {\n-      return 0;\n-    }\n-\n-    return (left->_size < right->_size) ? -1 : 1;\n-  }\n-\n-  static int xmm_slot_size(OptoReg::Name opto_reg) {\n-    \/\/ The low order 4 bytes denote what size of the XMM register is live\n-    return (opto_reg & 15) << 3;\n-  }\n-\n-  static uint xmm_ideal_reg_for_size(int reg_size) {\n-    switch (reg_size) {\n-    case 8:\n-      return Op_VecD;\n-    case 16:\n-      return Op_VecX;\n-    case 32:\n-      return Op_VecY;\n-    case 64:\n-      return Op_VecZ;\n-    default:\n-      fatal(\"Invalid register size %d\", reg_size);\n-      return 0;\n-    }\n-  }\n-\n-  bool xmm_needs_vzeroupper() const {\n-    return _xmm_registers.is_nonempty() && _xmm_registers.at(0)._size > 16;\n-  }\n-\n-  void xmm_register_save(const XMMRegisterData& reg_data) {\n-    const OptoReg::Name opto_reg = OptoReg::as_OptoReg(reg_data._reg->as_VMReg());\n-    const uint ideal_reg = xmm_ideal_reg_for_size(reg_data._size);\n-    _spill_offset -= reg_data._size;\n-    C2_MacroAssembler c2_masm(__ code());\n-    vec_spill_helper(&c2_masm, false \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n-  }\n-\n-  void xmm_register_restore(const XMMRegisterData& reg_data) {\n-    const OptoReg::Name opto_reg = OptoReg::as_OptoReg(reg_data._reg->as_VMReg());\n-    const uint ideal_reg = xmm_ideal_reg_for_size(reg_data._size);\n-    C2_MacroAssembler c2_masm(__ code());\n-    vec_spill_helper(&c2_masm, true \/* is_load *\/, _spill_offset, opto_reg, ideal_reg, tty);\n-    _spill_offset += reg_data._size;\n-  }\n-\n-  void gp_register_save(Register reg) {\n-    _spill_offset -= 8;\n-    __ movq(Address(rsp, _spill_offset), reg);\n-  }\n-\n-  void opmask_register_save(KRegister reg) {\n-    _spill_offset -= 8;\n-    __ kmov(Address(rsp, _spill_offset), reg);\n-  }\n-\n-  void gp_register_restore(Register reg) {\n-    __ movq(reg, Address(rsp, _spill_offset));\n-    _spill_offset += 8;\n-  }\n-\n-  void opmask_register_restore(KRegister reg) {\n-    __ kmov(reg, Address(rsp, _spill_offset));\n-    _spill_offset += 8;\n-  }\n-\n-  void initialize(XLoadBarrierStubC2* stub) {\n-    \/\/ Create mask of caller saved registers that need to\n-    \/\/ be saved\/restored if live\n-    RegMask caller_saved;\n-    caller_saved.Insert(OptoReg::as_OptoReg(rax->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(rcx->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(rdx->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(rsi->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(rdi->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(r8->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(r9->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(r10->as_VMReg()));\n-    caller_saved.Insert(OptoReg::as_OptoReg(r11->as_VMReg()));\n-    caller_saved.Remove(OptoReg::as_OptoReg(stub->ref()->as_VMReg()));\n-\n-    if (UseAPX) {\n-      caller_saved.Insert(OptoReg::as_OptoReg(r16->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r17->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r18->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r19->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r20->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r21->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r22->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r23->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r24->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r25->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r26->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r27->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r28->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r29->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r30->as_VMReg()));\n-      caller_saved.Insert(OptoReg::as_OptoReg(r31->as_VMReg()));\n-    }\n-\n-    \/\/ Create mask of live registers\n-    RegMask live = stub->live();\n-    if (stub->tmp() != noreg) {\n-      live.Insert(OptoReg::as_OptoReg(stub->tmp()->as_VMReg()));\n-    }\n-\n-    int gp_spill_size = 0;\n-    int opmask_spill_size = 0;\n-    int xmm_spill_size = 0;\n-\n-    \/\/ Record registers that needs to be saved\/restored\n-    RegMaskIterator rmi(live);\n-    while (rmi.has_next()) {\n-      const OptoReg::Name opto_reg = rmi.next();\n-      const VMReg vm_reg = OptoReg::as_VMReg(opto_reg);\n-\n-      if (vm_reg->is_Register()) {\n-        if (caller_saved.Member(opto_reg)) {\n-          _gp_registers.append(vm_reg->as_Register());\n-          gp_spill_size += 8;\n-        }\n-      } else if (vm_reg->is_KRegister()) {\n-        \/\/ All opmask registers are caller saved, thus spill the ones\n-        \/\/ which are live.\n-        if (_opmask_registers.find(vm_reg->as_KRegister()) == -1) {\n-          _opmask_registers.append(vm_reg->as_KRegister());\n-          opmask_spill_size += 8;\n-        }\n-      } else if (vm_reg->is_XMMRegister()) {\n-        \/\/ We encode in the low order 4 bits of the opto_reg, how large part of the register is live\n-        const VMReg vm_reg_base = OptoReg::as_VMReg(opto_reg & ~15);\n-        const int reg_size = xmm_slot_size(opto_reg);\n-        const XMMRegisterData reg_data = { vm_reg_base->as_XMMRegister(), reg_size };\n-        const int reg_index = _xmm_registers.find(reg_data);\n-        if (reg_index == -1) {\n-          \/\/ Not previously appended\n-          _xmm_registers.append(reg_data);\n-          xmm_spill_size += reg_size;\n-        } else {\n-          \/\/ Previously appended, update size\n-          const int reg_size_prev = _xmm_registers.at(reg_index)._size;\n-          if (reg_size > reg_size_prev) {\n-            _xmm_registers.at_put(reg_index, reg_data);\n-            xmm_spill_size += reg_size - reg_size_prev;\n-          }\n-        }\n-      } else {\n-        fatal(\"Unexpected register type\");\n-      }\n-    }\n-\n-    \/\/ Sort by size, largest first\n-    _xmm_registers.sort(xmm_compare_register_size);\n-\n-    \/\/ On Windows, the caller reserves stack space for spilling register arguments\n-    const int arg_spill_size = frame::arg_reg_save_area_bytes;\n-\n-    \/\/ Stack pointer must be 16 bytes aligned for the call\n-    _spill_offset = _spill_size = align_up(xmm_spill_size + gp_spill_size + opmask_spill_size + arg_spill_size, 16);\n-  }\n-\n-public:\n-  XSaveLiveRegisters(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _gp_registers(),\n-      _opmask_registers(),\n-      _xmm_registers(),\n-      _spill_size(0),\n-      _spill_offset(0) {\n-\n-    \/\/\n-    \/\/ Stack layout after registers have been spilled:\n-    \/\/\n-    \/\/ | ...            | original rsp, 16 bytes aligned\n-    \/\/ ------------------\n-    \/\/ | zmm0 high      |\n-    \/\/ | ...            |\n-    \/\/ | zmm0 low       | 16 bytes aligned\n-    \/\/ | ...            |\n-    \/\/ | ymm1 high      |\n-    \/\/ | ...            |\n-    \/\/ | ymm1 low       | 16 bytes aligned\n-    \/\/ | ...            |\n-    \/\/ | xmmN high      |\n-    \/\/ | ...            |\n-    \/\/ | xmmN low       | 8 bytes aligned\n-    \/\/ | reg0           | 8 bytes aligned\n-    \/\/ | reg1           |\n-    \/\/ | ...            |\n-    \/\/ | regN           | new rsp, if 16 bytes aligned\n-    \/\/ | <padding>      | else new rsp, 16 bytes aligned\n-    \/\/ ------------------\n-    \/\/\n-\n-    \/\/ Figure out what registers to save\/restore\n-    initialize(stub);\n-\n-    \/\/ Allocate stack space\n-    if (_spill_size > 0) {\n-      __ subptr(rsp, _spill_size);\n-    }\n-\n-    \/\/ Save XMM\/YMM\/ZMM registers\n-    for (int i = 0; i < _xmm_registers.length(); i++) {\n-      xmm_register_save(_xmm_registers.at(i));\n-    }\n-\n-    if (xmm_needs_vzeroupper()) {\n-      __ vzeroupper();\n-    }\n-\n-    \/\/ Save general purpose registers\n-    for (int i = 0; i < _gp_registers.length(); i++) {\n-      gp_register_save(_gp_registers.at(i));\n-    }\n-\n-    \/\/ Save opmask registers\n-    for (int i = 0; i < _opmask_registers.length(); i++) {\n-      opmask_register_save(_opmask_registers.at(i));\n-    }\n-  }\n-\n-  ~XSaveLiveRegisters() {\n-    \/\/ Restore opmask registers\n-    for (int i = _opmask_registers.length() - 1; i >= 0; i--) {\n-      opmask_register_restore(_opmask_registers.at(i));\n-    }\n-\n-    \/\/ Restore general purpose registers\n-    for (int i = _gp_registers.length() - 1; i >= 0; i--) {\n-      gp_register_restore(_gp_registers.at(i));\n-    }\n-\n-    __ vzeroupper();\n-\n-    \/\/ Restore XMM\/YMM\/ZMM registers\n-    for (int i = _xmm_registers.length() - 1; i >= 0; i--) {\n-      xmm_register_restore(_xmm_registers.at(i));\n-    }\n-\n-    \/\/ Free stack space\n-    if (_spill_size > 0) {\n-      __ addptr(rsp, _spill_size);\n-    }\n-  }\n-};\n-\n-class XSetupArguments {\n-private:\n-  MacroAssembler* const _masm;\n-  const Register        _ref;\n-  const Address         _ref_addr;\n-\n-public:\n-  XSetupArguments(MacroAssembler* masm, XLoadBarrierStubC2* stub) :\n-      _masm(masm),\n-      _ref(stub->ref()),\n-      _ref_addr(stub->ref_addr()) {\n-\n-    \/\/ Setup arguments\n-    if (_ref_addr.base() == noreg) {\n-      \/\/ No self healing\n-      if (_ref != c_rarg0) {\n-        __ movq(c_rarg0, _ref);\n-      }\n-      __ xorq(c_rarg1, c_rarg1);\n-    } else {\n-      \/\/ Self healing\n-      if (_ref == c_rarg0) {\n-        __ lea(c_rarg1, _ref_addr);\n-      } else if (_ref != c_rarg1) {\n-        __ lea(c_rarg1, _ref_addr);\n-        __ movq(c_rarg0, _ref);\n-      } else if (_ref_addr.base() != c_rarg0 && _ref_addr.index() != c_rarg0) {\n-        __ movq(c_rarg0, _ref);\n-        __ lea(c_rarg1, _ref_addr);\n-      } else {\n-        __ xchgq(c_rarg0, c_rarg1);\n-        if (_ref_addr.base() == c_rarg0) {\n-          __ lea(c_rarg1, Address(c_rarg1, _ref_addr.index(), _ref_addr.scale(), _ref_addr.disp()));\n-        } else if (_ref_addr.index() == c_rarg0) {\n-          __ lea(c_rarg1, Address(_ref_addr.base(), c_rarg1, _ref_addr.scale(), _ref_addr.disp()));\n-        } else {\n-          ShouldNotReachHere();\n-        }\n-      }\n-    }\n-  }\n-\n-  ~XSetupArguments() {\n-    \/\/ Transfer result\n-    if (_ref != rax) {\n-      __ movq(_ref, rax);\n-    }\n-  }\n-};\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::generate_c2_load_barrier_stub(MacroAssembler* masm, XLoadBarrierStubC2* stub) const {\n-  BLOCK_COMMENT(\"XLoadBarrierStubC2\");\n-\n-  \/\/ Stub entry\n-  __ bind(*stub->entry());\n-\n-  {\n-    XSaveLiveRegisters save_live_registers(masm, stub);\n-    XSetupArguments setup_arguments(masm, stub);\n-    __ call(RuntimeAddress(stub->slow_path()));\n-  }\n-\n-  \/\/ Stub exit\n-  __ jmp(*stub->continuation());\n-}\n-\n-#endif \/\/ COMPILER2\n-\n-#undef __\n-#define __ masm->\n-\n-void XBarrierSetAssembler::check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error) {\n-  \/\/ Check if metadata bits indicate a bad oop\n-  __ testptr(obj, Address(r15_thread, XThreadLocalData::address_bad_mask_offset()));\n-  __ jcc(Assembler::notZero, error);\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n-}\n-\n-#undef __\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xBarrierSetAssembler_x86.cpp","additions":0,"deletions":734,"binary":false,"changes":734,"status":"deleted"},{"patch":"@@ -1,109 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef CPU_X86_GC_X_XBARRIERSETASSEMBLER_X86_HPP\n-#define CPU_X86_GC_X_XBARRIERSETASSEMBLER_X86_HPP\n-\n-#include \"code\/vmreg.hpp\"\n-#include \"oops\/accessDecorators.hpp\"\n-#ifdef COMPILER2\n-#include \"opto\/optoreg.hpp\"\n-#endif \/\/ COMPILER2\n-\n-class MacroAssembler;\n-\n-#ifdef COMPILER1\n-class LIR_Assembler;\n-class LIR_Opr;\n-class StubAssembler;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class Node;\n-#endif \/\/ COMPILER2\n-\n-#ifdef COMPILER1\n-class XLoadBarrierStubC1;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-class XLoadBarrierStubC2;\n-#endif \/\/ COMPILER2\n-\n-class XBarrierSetAssembler : public XBarrierSetAssemblerBase {\n-public:\n-  virtual void load_at(MacroAssembler* masm,\n-                       DecoratorSet decorators,\n-                       BasicType type,\n-                       Register dst,\n-                       Address src,\n-                       Register tmp1,\n-                       Register tmp_thread);\n-\n-#ifdef ASSERT\n-  virtual void store_at(MacroAssembler* masm,\n-                        DecoratorSet decorators,\n-                        BasicType type,\n-                        Address dst,\n-                        Register src,\n-                        Register tmp1,\n-                        Register tmp2,\n-                        Register tmp3);\n-#endif \/\/ ASSERT\n-\n-  virtual void arraycopy_prologue(MacroAssembler* masm,\n-                                  DecoratorSet decorators,\n-                                  BasicType type,\n-                                  Register src,\n-                                  Register dst,\n-                                  Register count);\n-\n-  virtual void try_resolve_jobject_in_native(MacroAssembler* masm,\n-                                             Register jni_env,\n-                                             Register obj,\n-                                             Register tmp,\n-                                             Label& slowpath);\n-\n-#ifdef COMPILER1\n-  void generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                     LIR_Opr ref) const;\n-\n-  void generate_c1_load_barrier_stub(LIR_Assembler* ce,\n-                                     XLoadBarrierStubC1* stub) const;\n-\n-  void generate_c1_load_barrier_runtime_stub(StubAssembler* sasm,\n-                                             DecoratorSet decorators) const;\n-#endif \/\/ COMPILER1\n-\n-#ifdef COMPILER2\n-  OptoReg::Name refine_register(const Node* node,\n-                                OptoReg::Name opto_reg);\n-\n-  void generate_c2_load_barrier_stub(MacroAssembler* masm,\n-                                     XLoadBarrierStubC2* stub) const;\n-#endif \/\/ COMPILER2\n-\n-  void check_oop(MacroAssembler* masm, Register obj, Register tmp1, Register tmp2, Label& error);\n-};\n-\n-#endif \/\/ CPU_X86_GC_X_XBARRIERSETASSEMBLER_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xBarrierSetAssembler_x86.hpp","additions":0,"deletions":109,"binary":false,"changes":109,"status":"deleted"},{"patch":"@@ -1,149 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-\/\/\n-\/\/ The heap can have three different layouts, depending on the max heap size.\n-\/\/\n-\/\/ Address Space & Pointer Layout 1\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000014000000000 (20TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000010000000000 (16TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x00000c0000000000 (12TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000040000000000 (4TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                  4 4  4 4\n-\/\/   3                  6 5  2 1                                             0\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |00000000 00000000 00|1111|11 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +--------------------+----+-----------------------------------------------+\n-\/\/  |                    |    |\n-\/\/  |                    |    * 41-0 Object Offset (42-bits, 4TB address space)\n-\/\/  |                    |\n-\/\/  |                    * 45-42 Metadata Bits (4-bits)  0001 = Marked0      (Address view 4-8TB)\n-\/\/  |                                                    0010 = Marked1      (Address view 8-12TB)\n-\/\/  |                                                    0100 = Remapped     (Address view 16-20TB)\n-\/\/  |                                                    1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-46 Fixed (18-bits, always zero)\n-\/\/\n-\/\/\n-\/\/ Address Space & Pointer Layout 2\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000280000000000 (40TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000200000000000 (32TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000180000000000 (24TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000080000000000 (8TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6                 4 4  4 4\n-\/\/   3                 7 6  3 2                                              0\n-\/\/  +------------------+-----+------------------------------------------------+\n-\/\/  |00000000 00000000 0|1111|111 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +-------------------+----+------------------------------------------------+\n-\/\/  |                   |    |\n-\/\/  |                   |    * 42-0 Object Offset (43-bits, 8TB address space)\n-\/\/  |                   |\n-\/\/  |                   * 46-43 Metadata Bits (4-bits)  0001 = Marked0      (Address view 8-16TB)\n-\/\/  |                                                   0010 = Marked1      (Address view 16-24TB)\n-\/\/  |                                                   0100 = Remapped     (Address view 32-40TB)\n-\/\/  |                                                   1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-47 Fixed (17-bits, always zero)\n-\/\/\n-\/\/\n-\/\/ Address Space & Pointer Layout 3\n-\/\/ --------------------------------\n-\/\/\n-\/\/  +--------------------------------+ 0x00007FFFFFFFFFFF (127TB)\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000500000000000 (80TB)\n-\/\/  |         Remapped View          |\n-\/\/  +--------------------------------+ 0x0000400000000000 (64TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000300000000000 (48TB)\n-\/\/  |         Marked1 View           |\n-\/\/  +--------------------------------+ 0x0000200000000000 (32TB)\n-\/\/  |         Marked0 View           |\n-\/\/  +--------------------------------+ 0x0000100000000000 (16TB)\n-\/\/  .                                .\n-\/\/  +--------------------------------+ 0x0000000000000000\n-\/\/\n-\/\/   6               4  4  4 4\n-\/\/   3               8  7  4 3                                               0\n-\/\/  +------------------+----+-------------------------------------------------+\n-\/\/  |00000000 00000000 |1111|1111 11111111 11111111 11111111 11111111 11111111|\n-\/\/  +------------------+----+-------------------------------------------------+\n-\/\/  |                  |    |\n-\/\/  |                  |    * 43-0 Object Offset (44-bits, 16TB address space)\n-\/\/  |                  |\n-\/\/  |                  * 47-44 Metadata Bits (4-bits)  0001 = Marked0      (Address view 16-32TB)\n-\/\/  |                                                  0010 = Marked1      (Address view 32-48TB)\n-\/\/  |                                                  0100 = Remapped     (Address view 64-80TB)\n-\/\/  |                                                  1000 = Finalizable  (Address view N\/A)\n-\/\/  |\n-\/\/  * 63-48 Fixed (16-bits, always zero)\n-\/\/\n-\n-size_t XPlatformAddressOffsetBits() {\n-  const size_t min_address_offset_bits = 42; \/\/ 4TB\n-  const size_t max_address_offset_bits = 44; \/\/ 16TB\n-  const size_t address_offset = round_up_power_of_2(MaxHeapSize * XVirtualToPhysicalRatio);\n-  const size_t address_offset_bits = log2i_exact(address_offset);\n-  return clamp(address_offset_bits, min_address_offset_bits, max_address_offset_bits);\n-}\n-\n-size_t XPlatformAddressMetadataShift() {\n-  return XPlatformAddressOffsetBits();\n-}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xGlobals_x86.cpp","additions":0,"deletions":149,"binary":false,"changes":149,"status":"deleted"},{"patch":"@@ -1,33 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef CPU_X86_GC_X_XGLOBALS_X86_HPP\n-#define CPU_X86_GC_X_XGLOBALS_X86_HPP\n-\n-const size_t XPlatformHeapViews        = 3;\n-const size_t XPlatformCacheLineSize    = 64;\n-\n-size_t XPlatformAddressOffsetBits();\n-size_t XPlatformAddressMetadataShift();\n-\n-#endif \/\/ CPU_X86_GC_X_XGLOBALS_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/xGlobals_x86.hpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"deleted"},{"patch":"@@ -1,156 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\n-source_hpp %{\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-\n-%}\n-\n-source %{\n-\n-#include \"c2_intelJccErratum_x86.hpp\"\n-\n-static void x_load_barrier(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  if (barrier_data == XLoadBarrierElided) {\n-    return;\n-  }\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, barrier_data);\n-  {\n-    IntelJccErratumAlignment intel_alignment(masm, 10 \/* jcc_size *\/);\n-    __ testptr(ref, Address(r15_thread, XThreadLocalData::address_bad_mask_offset()));\n-    __ jcc(Assembler::notZero, *stub->entry());\n-  }\n-  __ bind(*stub->continuation());\n-}\n-\n-static void x_load_barrier_cmpxchg(MacroAssembler* masm, const MachNode* node, Address ref_addr, Register ref, Register tmp, Label& good) {\n-  XLoadBarrierStubC2* const stub = XLoadBarrierStubC2::create(node, ref_addr, ref, tmp, XLoadBarrierStrong);\n-  {\n-    IntelJccErratumAlignment intel_alignment(masm, 10 \/* jcc_size *\/);\n-    __ testptr(ref, Address(r15_thread, XThreadLocalData::address_bad_mask_offset()));\n-    __ jcc(Assembler::zero, good);\n-  }\n-  {\n-    IntelJccErratumAlignment intel_alignment(masm, 5 \/* jcc_size *\/);\n-    __ jmp(*stub->entry());\n-  }\n-  __ bind(*stub->continuation());\n-}\n-\n-static void x_cmpxchg_common(MacroAssembler* masm, const MachNode* node, Register mem_reg, Register newval, Register tmp) {\n-  \/\/ Compare value (oldval) is in rax\n-   const Address mem = Address(mem_reg, 0);\n-\n-  if (node->barrier_data() != XLoadBarrierElided) {\n-    __ movptr(tmp, rax);\n-  }\n-\n-  __ lock();\n-  __ cmpxchgptr(newval, mem);\n-\n-  if (node->barrier_data() != XLoadBarrierElided) {\n-    Label good;\n-    x_load_barrier_cmpxchg(masm, node, mem, rax, tmp, good);\n-    __ movptr(rax, tmp);\n-    __ lock();\n-    __ cmpxchgptr(newval, mem);\n-    __ bind(good);\n-  }\n-}\n-\n-%}\n-\n-\/\/ Load Pointer\n-instruct xLoadP(rRegP dst, memory mem, rFlagsReg cr)\n-%{\n-  predicate(UseZGC && !ZGenerational && n->as_Load()->barrier_data() != 0);\n-  match(Set dst (LoadP mem));\n-  effect(KILL cr, TEMP dst);\n-\n-  ins_cost(125);\n-\n-  format %{ \"movq     $dst, $mem\" %}\n-\n-  ins_encode %{\n-    __ movptr($dst$$Register, $mem$$Address);\n-    x_load_barrier(masm, this, $mem$$Address, $dst$$Register, noreg \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct xCompareAndExchangeP(indirect mem, rax_RegP oldval, rRegP newval, rRegP tmp, rFlagsReg cr) %{\n-  match(Set oldval (CompareAndExchangeP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(KILL cr, TEMP tmp);\n-\n-  format %{ \"lock\\n\\t\"\n-            \"cmpxchgq $newval, $mem\" %}\n-\n-  ins_encode %{\n-    precond($oldval$$Register == rax);\n-    x_cmpxchg_common(masm, this, $mem$$Register, $newval$$Register, $tmp$$Register);\n-  %}\n-\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xCompareAndSwapP(rRegI res, indirect mem, rRegP newval, rRegP tmp, rFlagsReg cr, rax_RegP oldval) %{\n-  match(Set res (CompareAndSwapP mem (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem (Binary oldval newval)));\n-  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() == XLoadBarrierStrong);\n-  effect(KILL cr, KILL oldval, TEMP tmp);\n-\n-  format %{ \"lock\\n\\t\"\n-            \"cmpxchgq $newval, $mem\\n\\t\"\n-            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n-\n-  ins_encode %{\n-    precond($oldval$$Register == rax);\n-    x_cmpxchg_common(masm, this, $mem$$Register, $newval$$Register, $tmp$$Register);\n-    if (barrier_data() != XLoadBarrierElided) {\n-      __ cmpptr($tmp$$Register, rax);\n-    }\n-    __ setcc(Assembler::equal, $res$$Register);\n-  %}\n-\n-  ins_pipe(pipe_cmpxchg);\n-%}\n-\n-instruct xXChgP(indirect mem, rRegP newval, rFlagsReg cr) %{\n-  match(Set newval (GetAndSetP mem newval));\n-  predicate(UseZGC && !ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n-  effect(KILL cr);\n-\n-  format %{ \"xchgq    $newval, $mem\" %}\n-\n-  ins_encode %{\n-    __ xchgptr($newval$$Register, Address($mem$$Register, 0));\n-    x_load_barrier(masm, this, Address(noreg, 0), $newval$$Register, noreg \/* tmp *\/, barrier_data());\n-  %}\n-\n-  ins_pipe(pipe_cmpxchg);\n-%}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/x\/x_x86_64.ad","additions":0,"deletions":156,"binary":false,"changes":156,"status":"deleted"},{"patch":"@@ -117,1 +117,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Load()->barrier_data() != 0);\n@@ -136,1 +136,1 @@\n-  predicate(UseZGC && ZGenerational && n->in(1)->as_Load()->barrier_data() != 0);\n+  predicate(UseZGC && n->in(1)->as_Load()->barrier_data() != 0);\n@@ -152,1 +152,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Store()->barrier_data() != 0);\n@@ -168,1 +168,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_Store()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_Store()->barrier_data() != 0);\n@@ -187,1 +187,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n@@ -210,1 +210,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n@@ -232,1 +232,1 @@\n-  predicate(UseZGC && ZGenerational && n->as_LoadStore()->barrier_data() != 0);\n+  predicate(UseZGC && n->as_LoadStore()->barrier_data() != 0);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/z_x86_64.ad","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1,34 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xLargePages.hpp\"\n-#include \"runtime\/globals.hpp\"\n-\n-void XLargePages::pd_initialize() {\n-  if (UseLargePages) {\n-    _state = Explicit;\n-  } else {\n-    _state = Disabled;\n-  }\n-}\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xLargePages_bsd.cpp","additions":0,"deletions":34,"binary":false,"changes":34,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-void XNUMA::pd_initialize() {\n-  _enabled = false;\n-}\n-\n-uint32_t XNUMA::count() {\n-  return 1;\n-}\n-\n-uint32_t XNUMA::id() {\n-  return 0;\n-}\n-\n-uint32_t XNUMA::memory_id(uintptr_t addr) {\n-  \/\/ NUMA support not enabled, assume everything belongs to node zero\n-  return 0;\n-}\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xNUMA_bsd.cpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,181 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xErrno.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLargePages.inline.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n-#include \"gc\/x\/xPhysicalMemoryBacking_bsd.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-#include <mach\/mach.h>\n-#include <mach\/mach_vm.h>\n-#include <sys\/mman.h>\n-#include <sys\/types.h>\n-\n-\/\/ The backing is represented by a reserved virtual address space, in which\n-\/\/ we commit and uncommit physical memory. Multi-mapping the different heap\n-\/\/ views is done by simply remapping the backing memory using mach_vm_remap().\n-\n-static int vm_flags_superpage() {\n-  if (!XLargePages::is_explicit()) {\n-    return 0;\n-  }\n-\n-  const int page_size_in_megabytes = XGranuleSize >> 20;\n-  return page_size_in_megabytes << VM_FLAGS_SUPERPAGE_SHIFT;\n-}\n-\n-static XErrno mremap(uintptr_t from_addr, uintptr_t to_addr, size_t size) {\n-  mach_vm_address_t remap_addr = to_addr;\n-  vm_prot_t remap_cur_prot;\n-  vm_prot_t remap_max_prot;\n-\n-  \/\/ Remap memory to an additional location\n-  const kern_return_t res = mach_vm_remap(mach_task_self(),\n-                                          &remap_addr,\n-                                          size,\n-                                          0 \/* mask *\/,\n-                                          VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | vm_flags_superpage(),\n-                                          mach_task_self(),\n-                                          from_addr,\n-                                          FALSE \/* copy *\/,\n-                                          &remap_cur_prot,\n-                                          &remap_max_prot,\n-                                          VM_INHERIT_COPY);\n-\n-  return (res == KERN_SUCCESS) ? XErrno(0) : XErrno(EINVAL);\n-}\n-\n-XPhysicalMemoryBacking::XPhysicalMemoryBacking(size_t max_capacity) :\n-    _base(0),\n-    _initialized(false) {\n-\n-  \/\/ Reserve address space for backing memory\n-  _base = (uintptr_t)os::reserve_memory(max_capacity);\n-  if (_base == 0) {\n-    \/\/ Failed\n-    log_error_pd(gc)(\"Failed to reserve address space for backing memory\");\n-    return;\n-  }\n-\n-  \/\/ Successfully initialized\n-  _initialized = true;\n-}\n-\n-bool XPhysicalMemoryBacking::is_initialized() const {\n-  return _initialized;\n-}\n-\n-void XPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n-  \/\/ Does nothing\n-}\n-\n-bool XPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {\n-  assert(is_aligned(offset, os::vm_page_size()), \"Invalid offset\");\n-  assert(is_aligned(length, os::vm_page_size()), \"Invalid length\");\n-\n-  log_trace(gc, heap)(\"Committing memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n-\n-  const uintptr_t addr = _base + offset;\n-  const void* const res = mmap((void*)addr, length, PROT_READ | PROT_WRITE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE, -1, 0);\n-  if (res == MAP_FAILED) {\n-    XErrno err;\n-    log_error(gc)(\"Failed to commit memory (%s)\", err.to_string());\n-    return false;\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-size_t XPhysicalMemoryBacking::commit(size_t offset, size_t length) const {\n-  \/\/ Try to commit the whole region\n-  if (commit_inner(offset, length)) {\n-    \/\/ Success\n-    return length;\n-  }\n-\n-  \/\/ Failed, try to commit as much as possible\n-  size_t start = offset;\n-  size_t end = offset + length;\n-\n-  for (;;) {\n-    length = align_down((end - start) \/ 2, XGranuleSize);\n-    if (length == 0) {\n-      \/\/ Done, don't commit more\n-      return start - offset;\n-    }\n-\n-    if (commit_inner(start, length)) {\n-      \/\/ Success, try commit more\n-      start += length;\n-    } else {\n-      \/\/ Failed, try commit less\n-      end -= length;\n-    }\n-  }\n-}\n-\n-size_t XPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {\n-  assert(is_aligned(offset, os::vm_page_size()), \"Invalid offset\");\n-  assert(is_aligned(length, os::vm_page_size()), \"Invalid length\");\n-\n-  log_trace(gc, heap)(\"Uncommitting memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n-\n-  const uintptr_t start = _base + offset;\n-  const void* const res = mmap((void*)start, length, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n-  if (res == MAP_FAILED) {\n-    XErrno err;\n-    log_error(gc)(\"Failed to uncommit memory (%s)\", err.to_string());\n-    return 0;\n-  }\n-\n-  return length;\n-}\n-\n-void XPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {\n-  const XErrno err = mremap(_base + offset, addr, size);\n-  if (err) {\n-    fatal(\"Failed to remap memory (%s)\", err.to_string());\n-  }\n-}\n-\n-void XPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n-  \/\/ Note that we must keep the address space reservation intact and just detach\n-  \/\/ the backing memory. For this reason we map a new anonymous, non-accessible\n-  \/\/ and non-reserved page over the mapping instead of actually unmapping.\n-  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n-  if (res == MAP_FAILED) {\n-    XErrno err;\n-    fatal(\"Failed to map memory (%s)\", err.to_string());\n-  }\n-}\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xPhysicalMemoryBacking_bsd.cpp","additions":0,"deletions":181,"binary":false,"changes":181,"status":"deleted"},{"patch":"@@ -1,48 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_BSD_GC_X_XPHYSICALMEMORYBACKING_BSD_HPP\n-#define OS_BSD_GC_X_XPHYSICALMEMORYBACKING_BSD_HPP\n-\n-class XPhysicalMemoryBacking {\n-private:\n-  uintptr_t _base;\n-  bool      _initialized;\n-\n-  bool commit_inner(size_t offset, size_t length) const;\n-\n-public:\n-  XPhysicalMemoryBacking(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  void warn_commit_limits(size_t max_capacity) const;\n-\n-  size_t commit(size_t offset, size_t length) const;\n-  size_t uncommit(size_t offset, size_t length) const;\n-\n-  void map(uintptr_t addr, size_t size, uintptr_t offset) const;\n-  void unmap(uintptr_t addr, size_t size) const;\n-};\n-\n-#endif \/\/ OS_BSD_GC_X_XPHYSICALMEMORYBACKING_BSD_HPP\n","filename":"src\/hotspot\/os\/bsd\/gc\/x\/xPhysicalMemoryBacking_bsd.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"deleted"},{"patch":"@@ -1,38 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xLargePages.hpp\"\n-#include \"runtime\/globals.hpp\"\n-\n-void XLargePages::pd_initialize() {\n-  if (UseLargePages) {\n-    if (UseTransparentHugePages) {\n-      _state = Transparent;\n-    } else {\n-      _state = Explicit;\n-    }\n-  } else {\n-    _state = Disabled;\n-  }\n-}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xLargePages_linux.cpp","additions":0,"deletions":38,"binary":false,"changes":38,"status":"deleted"},{"patch":"@@ -1,154 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xErrno.hpp\"\n-#include \"gc\/x\/xMountPoint_linux.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-#include <stdio.h>\n-#include <unistd.h>\n-\n-\/\/ Mount information, see proc(5) for more details.\n-#define PROC_SELF_MOUNTINFO        \"\/proc\/self\/mountinfo\"\n-\n-XMountPoint::XMountPoint(const char* filesystem, const char** preferred_mountpoints) {\n-  if (AllocateHeapAt != nullptr) {\n-    \/\/ Use specified path\n-    _path = os::strdup(AllocateHeapAt, mtGC);\n-  } else {\n-    \/\/ Find suitable path\n-    _path = find_mountpoint(filesystem, preferred_mountpoints);\n-  }\n-}\n-\n-XMountPoint::~XMountPoint() {\n-  os::free(_path);\n-  _path = nullptr;\n-}\n-\n-char* XMountPoint::get_mountpoint(const char* line, const char* filesystem) const {\n-  char* line_mountpoint = nullptr;\n-  char* line_filesystem = nullptr;\n-\n-  \/\/ Parse line and return a newly allocated string containing the mount point if\n-  \/\/ the line contains a matching filesystem and the mount point is accessible by\n-  \/\/ the current user.\n-  \/\/ sscanf, using %m, will return malloced memory. Need raw ::free, not os::free.\n-  if (sscanf(line, \"%*u %*u %*u:%*u %*s %ms %*[^-]- %ms\", &line_mountpoint, &line_filesystem) != 2 ||\n-      strcmp(line_filesystem, filesystem) != 0 ||\n-      access(line_mountpoint, R_OK|W_OK|X_OK) != 0) {\n-    \/\/ Not a matching or accessible filesystem\n-    ALLOW_C_FUNCTION(::free, ::free(line_mountpoint);)\n-    line_mountpoint = nullptr;\n-  }\n-\n-  ALLOW_C_FUNCTION(::free, ::free(line_filesystem);)\n-\n-  return line_mountpoint;\n-}\n-\n-void XMountPoint::get_mountpoints(const char* filesystem, XArray<char*>* mountpoints) const {\n-  FILE* fd = os::fopen(PROC_SELF_MOUNTINFO, \"r\");\n-  if (fd == nullptr) {\n-    XErrno err;\n-    log_error_p(gc)(\"Failed to open %s: %s\", PROC_SELF_MOUNTINFO, err.to_string());\n-    return;\n-  }\n-\n-  char* line = nullptr;\n-  size_t length = 0;\n-\n-  while (getline(&line, &length, fd) != -1) {\n-    char* const mountpoint = get_mountpoint(line, filesystem);\n-    if (mountpoint != nullptr) {\n-      mountpoints->append(mountpoint);\n-    }\n-  }\n-\n-  \/\/ readline will return malloced memory. Need raw ::free, not os::free.\n-  ALLOW_C_FUNCTION(::free, ::free(line);)\n-  fclose(fd);\n-}\n-\n-void XMountPoint::free_mountpoints(XArray<char*>* mountpoints) const {\n-  XArrayIterator<char*> iter(mountpoints);\n-  for (char* mountpoint; iter.next(&mountpoint);) {\n-    ALLOW_C_FUNCTION(::free, ::free(mountpoint);) \/\/ *not* os::free\n-  }\n-  mountpoints->clear();\n-}\n-\n-char* XMountPoint::find_preferred_mountpoint(const char* filesystem,\n-                                              XArray<char*>* mountpoints,\n-                                              const char** preferred_mountpoints) const {\n-  \/\/ Find preferred mount point\n-  XArrayIterator<char*> iter1(mountpoints);\n-  for (char* mountpoint; iter1.next(&mountpoint);) {\n-    for (const char** preferred = preferred_mountpoints; *preferred != nullptr; preferred++) {\n-      if (!strcmp(mountpoint, *preferred)) {\n-        \/\/ Preferred mount point found\n-        return os::strdup(mountpoint, mtGC);\n-      }\n-    }\n-  }\n-\n-  \/\/ Preferred mount point not found\n-  log_error_p(gc)(\"More than one %s filesystem found:\", filesystem);\n-  XArrayIterator<char*> iter2(mountpoints);\n-  for (char* mountpoint; iter2.next(&mountpoint);) {\n-    log_error_p(gc)(\"  %s\", mountpoint);\n-  }\n-\n-  return nullptr;\n-}\n-\n-char* XMountPoint::find_mountpoint(const char* filesystem, const char** preferred_mountpoints) const {\n-  char* path = nullptr;\n-  XArray<char*> mountpoints;\n-\n-  get_mountpoints(filesystem, &mountpoints);\n-\n-  if (mountpoints.length() == 0) {\n-    \/\/ No mount point found\n-    log_error_p(gc)(\"Failed to find an accessible %s filesystem\", filesystem);\n-  } else if (mountpoints.length() == 1) {\n-    \/\/ One mount point found\n-    path = os::strdup(mountpoints.at(0), mtGC);\n-  } else {\n-    \/\/ More than one mount point found\n-    path = find_preferred_mountpoint(filesystem, &mountpoints, preferred_mountpoints);\n-  }\n-\n-  free_mountpoints(&mountpoints);\n-\n-  return path;\n-}\n-\n-const char* XMountPoint::get() const {\n-  return _path;\n-}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xMountPoint_linux.cpp","additions":0,"deletions":154,"binary":false,"changes":154,"status":"deleted"},{"patch":"@@ -1,52 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_LINUX_GC_X_XMOUNTPOINT_LINUX_HPP\n-#define OS_LINUX_GC_X_XMOUNTPOINT_LINUX_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class XMountPoint : public StackObj {\n-private:\n-  char* _path;\n-\n-  char* get_mountpoint(const char* line,\n-                       const char* filesystem) const;\n-  void get_mountpoints(const char* filesystem,\n-                       XArray<char*>* mountpoints) const;\n-  void free_mountpoints(XArray<char*>* mountpoints) const;\n-  char* find_preferred_mountpoint(const char* filesystem,\n-                                  XArray<char*>* mountpoints,\n-                                  const char** preferred_mountpoints) const;\n-  char* find_mountpoint(const char* filesystem,\n-                        const char** preferred_mountpoints) const;\n-\n-public:\n-  XMountPoint(const char* filesystem, const char** preferred_mountpoints);\n-  ~XMountPoint();\n-\n-  const char* get() const;\n-};\n-\n-#endif \/\/ OS_LINUX_GC_X_XMOUNTPOINT_LINUX_HPP\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xMountPoint_linux.hpp","additions":0,"deletions":52,"binary":false,"changes":52,"status":"deleted"},{"patch":"@@ -1,71 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"gc\/x\/xCPU.inline.hpp\"\n-#include \"gc\/x\/xErrno.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-#include \"gc\/x\/xSyscall_linux.hpp\"\n-#include \"os_linux.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-void XNUMA::pd_initialize() {\n-  _enabled = UseNUMA;\n-}\n-\n-uint32_t XNUMA::count() {\n-  if (!_enabled) {\n-    \/\/ NUMA support not enabled\n-    return 1;\n-  }\n-\n-  return os::Linux::numa_max_node() + 1;\n-}\n-\n-uint32_t XNUMA::id() {\n-  if (!_enabled) {\n-    \/\/ NUMA support not enabled\n-    return 0;\n-  }\n-\n-  return os::Linux::get_node_by_cpu(XCPU::id());\n-}\n-\n-uint32_t XNUMA::memory_id(uintptr_t addr) {\n-  if (!_enabled) {\n-    \/\/ NUMA support not enabled, assume everything belongs to node zero\n-    return 0;\n-  }\n-\n-  uint32_t id = (uint32_t)-1;\n-\n-  if (XSyscall::get_mempolicy((int*)&id, nullptr, 0, (void*)addr, MPOL_F_NODE | MPOL_F_ADDR) == -1) {\n-    XErrno err;\n-    fatal(\"Failed to get NUMA id for memory at \" PTR_FORMAT \" (%s)\", addr, err.to_string());\n-  }\n-\n-  assert(id < count(), \"Invalid NUMA id\");\n-\n-  return id;\n-}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xNUMA_linux.cpp","additions":0,"deletions":71,"binary":false,"changes":71,"status":"deleted"},{"patch":"@@ -1,724 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xErrno.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLargePages.inline.hpp\"\n-#include \"gc\/x\/xMountPoint_linux.hpp\"\n-#include \"gc\/x\/xNUMA.inline.hpp\"\n-#include \"gc\/x\/xPhysicalMemoryBacking_linux.hpp\"\n-#include \"gc\/x\/xSyscall_linux.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"os_linux.hpp\"\n-#include \"runtime\/init.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/safefetch.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-\n-#include <fcntl.h>\n-#include <stdio.h>\n-#include <sys\/mman.h>\n-#include <sys\/stat.h>\n-#include <sys\/statfs.h>\n-#include <sys\/types.h>\n-#include <unistd.h>\n-\n-\/\/\n-\/\/ Support for building on older Linux systems\n-\/\/\n-\n-\/\/ memfd_create(2) flags\n-#ifndef MFD_CLOEXEC\n-#define MFD_CLOEXEC                      0x0001U\n-#endif\n-#ifndef MFD_HUGETLB\n-#define MFD_HUGETLB                      0x0004U\n-#endif\n-#ifndef MFD_HUGE_2MB\n-#define MFD_HUGE_2MB                     0x54000000U\n-#endif\n-\n-\/\/ open(2) flags\n-#ifndef O_CLOEXEC\n-#define O_CLOEXEC                        02000000\n-#endif\n-#ifndef O_TMPFILE\n-#define O_TMPFILE                        (020000000 | O_DIRECTORY)\n-#endif\n-\n-\/\/ fallocate(2) flags\n-#ifndef FALLOC_FL_KEEP_SIZE\n-#define FALLOC_FL_KEEP_SIZE              0x01\n-#endif\n-#ifndef FALLOC_FL_PUNCH_HOLE\n-#define FALLOC_FL_PUNCH_HOLE             0x02\n-#endif\n-\n-\/\/ Filesystem types, see statfs(2)\n-#ifndef TMPFS_MAGIC\n-#define TMPFS_MAGIC                      0x01021994\n-#endif\n-#ifndef HUGETLBFS_MAGIC\n-#define HUGETLBFS_MAGIC                  0x958458f6\n-#endif\n-\n-\/\/ Filesystem names\n-#define XFILESYSTEM_TMPFS                \"tmpfs\"\n-#define XFILESYSTEM_HUGETLBFS            \"hugetlbfs\"\n-\n-\/\/ Proc file entry for max map mount\n-#define XFILENAME_PROC_MAX_MAP_COUNT     \"\/proc\/sys\/vm\/max_map_count\"\n-\n-\/\/ Sysfs file for transparent huge page on tmpfs\n-#define XFILENAME_SHMEM_ENABLED          \"\/sys\/kernel\/mm\/transparent_hugepage\/shmem_enabled\"\n-\n-\/\/ Java heap filename\n-#define XFILENAME_HEAP                   \"java_heap\"\n-\n-\/\/ Preferred tmpfs mount points, ordered by priority\n-static const char* z_preferred_tmpfs_mountpoints[] = {\n-  \"\/dev\/shm\",\n-  \"\/run\/shm\",\n-  nullptr\n-};\n-\n-\/\/ Preferred hugetlbfs mount points, ordered by priority\n-static const char* z_preferred_hugetlbfs_mountpoints[] = {\n-  \"\/dev\/hugepages\",\n-  \"\/hugepages\",\n-  nullptr\n-};\n-\n-static int z_fallocate_hugetlbfs_attempts = 3;\n-static bool z_fallocate_supported = true;\n-\n-XPhysicalMemoryBacking::XPhysicalMemoryBacking(size_t max_capacity) :\n-    _fd(-1),\n-    _filesystem(0),\n-    _block_size(0),\n-    _available(0),\n-    _initialized(false) {\n-\n-  \/\/ Create backing file\n-  _fd = create_fd(XFILENAME_HEAP);\n-  if (_fd == -1) {\n-    return;\n-  }\n-\n-  \/\/ Truncate backing file\n-  while (ftruncate(_fd, max_capacity) == -1) {\n-    if (errno != EINTR) {\n-      XErrno err;\n-      log_error_p(gc)(\"Failed to truncate backing file (%s)\", err.to_string());\n-      return;\n-    }\n-  }\n-\n-  \/\/ Get filesystem statistics\n-  struct statfs buf;\n-  if (fstatfs(_fd, &buf) == -1) {\n-    XErrno err;\n-    log_error_p(gc)(\"Failed to determine filesystem type for backing file (%s)\", err.to_string());\n-    return;\n-  }\n-\n-  _filesystem = buf.f_type;\n-  _block_size = buf.f_bsize;\n-  _available = buf.f_bavail * _block_size;\n-\n-  log_info_p(gc, init)(\"Heap Backing Filesystem: %s (\" UINT64_FORMAT_X \")\",\n-                       is_tmpfs() ? XFILESYSTEM_TMPFS : is_hugetlbfs() ? XFILESYSTEM_HUGETLBFS : \"other\", _filesystem);\n-\n-  \/\/ Make sure the filesystem type matches requested large page type\n-  if (XLargePages::is_transparent() && !is_tmpfs()) {\n-    log_error_p(gc)(\"-XX:+UseTransparentHugePages can only be enabled when using a %s filesystem\",\n-                    XFILESYSTEM_TMPFS);\n-    return;\n-  }\n-\n-  if (XLargePages::is_transparent() && !tmpfs_supports_transparent_huge_pages()) {\n-    log_error_p(gc)(\"-XX:+UseTransparentHugePages on a %s filesystem not supported by kernel\",\n-                    XFILESYSTEM_TMPFS);\n-    return;\n-  }\n-\n-  if (XLargePages::is_explicit() && !is_hugetlbfs()) {\n-    log_error_p(gc)(\"-XX:+UseLargePages (without -XX:+UseTransparentHugePages) can only be enabled \"\n-                    \"when using a %s filesystem\", XFILESYSTEM_HUGETLBFS);\n-    return;\n-  }\n-\n-  if (!XLargePages::is_explicit() && is_hugetlbfs()) {\n-    log_error_p(gc)(\"-XX:+UseLargePages must be enabled when using a %s filesystem\",\n-                    XFILESYSTEM_HUGETLBFS);\n-    return;\n-  }\n-\n-  \/\/ Make sure the filesystem block size is compatible\n-  if (XGranuleSize % _block_size != 0) {\n-    log_error_p(gc)(\"Filesystem backing the heap has incompatible block size (\" SIZE_FORMAT \")\",\n-                    _block_size);\n-    return;\n-  }\n-\n-  if (is_hugetlbfs() && _block_size != XGranuleSize) {\n-    log_error_p(gc)(\"%s filesystem has unexpected block size \" SIZE_FORMAT \" (expected \" SIZE_FORMAT \")\",\n-                    XFILESYSTEM_HUGETLBFS, _block_size, XGranuleSize);\n-    return;\n-  }\n-\n-  \/\/ Successfully initialized\n-  _initialized = true;\n-}\n-\n-int XPhysicalMemoryBacking::create_mem_fd(const char* name) const {\n-  assert(XGranuleSize == 2 * M, \"Granule size must match MFD_HUGE_2MB\");\n-\n-  \/\/ Create file name\n-  char filename[PATH_MAX];\n-  snprintf(filename, sizeof(filename), \"%s%s\", name, XLargePages::is_explicit() ? \".hugetlb\" : \"\");\n-\n-  \/\/ Create file\n-  const int extra_flags = XLargePages::is_explicit() ? (MFD_HUGETLB | MFD_HUGE_2MB) : 0;\n-  const int fd = XSyscall::memfd_create(filename, MFD_CLOEXEC | extra_flags);\n-  if (fd == -1) {\n-    XErrno err;\n-    log_debug_p(gc, init)(\"Failed to create memfd file (%s)\",\n-                          (XLargePages::is_explicit() && (err == EINVAL || err == ENODEV)) ?\n-                          \"Hugepages (2M) not available\" : err.to_string());\n-    return -1;\n-  }\n-\n-  log_info_p(gc, init)(\"Heap Backing File: \/memfd:%s\", filename);\n-\n-  return fd;\n-}\n-\n-int XPhysicalMemoryBacking::create_file_fd(const char* name) const {\n-  const char* const filesystem = XLargePages::is_explicit()\n-                                 ? XFILESYSTEM_HUGETLBFS\n-                                 : XFILESYSTEM_TMPFS;\n-  const char** const preferred_mountpoints = XLargePages::is_explicit()\n-                                             ? z_preferred_hugetlbfs_mountpoints\n-                                             : z_preferred_tmpfs_mountpoints;\n-\n-  \/\/ Find mountpoint\n-  XMountPoint mountpoint(filesystem, preferred_mountpoints);\n-  if (mountpoint.get() == nullptr) {\n-    log_error_p(gc)(\"Use -XX:AllocateHeapAt to specify the path to a %s filesystem\", filesystem);\n-    return -1;\n-  }\n-\n-  \/\/ Try to create an anonymous file using the O_TMPFILE flag. Note that this\n-  \/\/ flag requires kernel >= 3.11. If this fails we fall back to open\/unlink.\n-  const int fd_anon = os::open(mountpoint.get(), O_TMPFILE|O_EXCL|O_RDWR|O_CLOEXEC, S_IRUSR|S_IWUSR);\n-  if (fd_anon == -1) {\n-    XErrno err;\n-    log_debug_p(gc, init)(\"Failed to create anonymous file in %s (%s)\", mountpoint.get(),\n-                          (err == EINVAL ? \"Not supported\" : err.to_string()));\n-  } else {\n-    \/\/ Get inode number for anonymous file\n-    struct stat stat_buf;\n-    if (fstat(fd_anon, &stat_buf) == -1) {\n-      XErrno err;\n-      log_error_pd(gc)(\"Failed to determine inode number for anonymous file (%s)\", err.to_string());\n-      return -1;\n-    }\n-\n-    log_info_p(gc, init)(\"Heap Backing File: %s\/#\" UINT64_FORMAT, mountpoint.get(), (uint64_t)stat_buf.st_ino);\n-\n-    return fd_anon;\n-  }\n-\n-  log_debug_p(gc, init)(\"Falling back to open\/unlink\");\n-\n-  \/\/ Create file name\n-  char filename[PATH_MAX];\n-  snprintf(filename, sizeof(filename), \"%s\/%s.%d\", mountpoint.get(), name, os::current_process_id());\n-\n-  \/\/ Create file\n-  const int fd = os::open(filename, O_CREAT|O_EXCL|O_RDWR|O_CLOEXEC, S_IRUSR|S_IWUSR);\n-  if (fd == -1) {\n-    XErrno err;\n-    log_error_p(gc)(\"Failed to create file %s (%s)\", filename, err.to_string());\n-    return -1;\n-  }\n-\n-  \/\/ Unlink file\n-  if (unlink(filename) == -1) {\n-    XErrno err;\n-    log_error_p(gc)(\"Failed to unlink file %s (%s)\", filename, err.to_string());\n-    return -1;\n-  }\n-\n-  log_info_p(gc, init)(\"Heap Backing File: %s\", filename);\n-\n-  return fd;\n-}\n-\n-int XPhysicalMemoryBacking::create_fd(const char* name) const {\n-  if (AllocateHeapAt == nullptr) {\n-    \/\/ If the path is not explicitly specified, then we first try to create a memfd file\n-    \/\/ instead of looking for a tmpfd\/hugetlbfs mount point. Note that memfd_create() might\n-    \/\/ not be supported at all (requires kernel >= 3.17), or it might not support large\n-    \/\/ pages (requires kernel >= 4.14). If memfd_create() fails, then we try to create a\n-    \/\/ file on an accessible tmpfs or hugetlbfs mount point.\n-    const int fd = create_mem_fd(name);\n-    if (fd != -1) {\n-      return fd;\n-    }\n-\n-    log_debug_p(gc)(\"Falling back to searching for an accessible mount point\");\n-  }\n-\n-  return create_file_fd(name);\n-}\n-\n-bool XPhysicalMemoryBacking::is_initialized() const {\n-  return _initialized;\n-}\n-\n-void XPhysicalMemoryBacking::warn_available_space(size_t max_capacity) const {\n-  \/\/ Note that the available space on a tmpfs or a hugetlbfs filesystem\n-  \/\/ will be zero if no size limit was specified when it was mounted.\n-  if (_available == 0) {\n-    \/\/ No size limit set, skip check\n-    log_info_p(gc, init)(\"Available space on backing filesystem: N\/A\");\n-    return;\n-  }\n-\n-  log_info_p(gc, init)(\"Available space on backing filesystem: \" SIZE_FORMAT \"M\", _available \/ M);\n-\n-  \/\/ Warn if the filesystem doesn't currently have enough space available to hold\n-  \/\/ the max heap size. The max heap size will be capped if we later hit this limit\n-  \/\/ when trying to expand the heap.\n-  if (_available < max_capacity) {\n-    log_warning_p(gc)(\"***** WARNING! INCORRECT SYSTEM CONFIGURATION DETECTED! *****\");\n-    log_warning_p(gc)(\"Not enough space available on the backing filesystem to hold the current max Java heap\");\n-    log_warning_p(gc)(\"size (\" SIZE_FORMAT \"M). Please adjust the size of the backing filesystem accordingly \"\n-                      \"(available\", max_capacity \/ M);\n-    log_warning_p(gc)(\"space is currently \" SIZE_FORMAT \"M). Continuing execution with the current filesystem \"\n-                      \"size could\", _available \/ M);\n-    log_warning_p(gc)(\"lead to a premature OutOfMemoryError being thrown, due to failure to commit memory.\");\n-  }\n-}\n-\n-void XPhysicalMemoryBacking::warn_max_map_count(size_t max_capacity) const {\n-  const char* const filename = XFILENAME_PROC_MAX_MAP_COUNT;\n-  FILE* const file = os::fopen(filename, \"r\");\n-  if (file == nullptr) {\n-    \/\/ Failed to open file, skip check\n-    log_debug_p(gc, init)(\"Failed to open %s\", filename);\n-    return;\n-  }\n-\n-  size_t actual_max_map_count = 0;\n-  const int result = fscanf(file, SIZE_FORMAT, &actual_max_map_count);\n-  fclose(file);\n-  if (result != 1) {\n-    \/\/ Failed to read file, skip check\n-    log_debug_p(gc, init)(\"Failed to read %s\", filename);\n-    return;\n-  }\n-\n-  \/\/ The required max map count is impossible to calculate exactly since subsystems\n-  \/\/ other than ZGC are also creating memory mappings, and we have no control over that.\n-  \/\/ However, ZGC tends to create the most mappings and dominate the total count.\n-  \/\/ In the worst cases, ZGC will map each granule three times, i.e. once per heap view.\n-  \/\/ We speculate that we need another 20% to allow for non-ZGC subsystems to map memory.\n-  const size_t required_max_map_count = (max_capacity \/ XGranuleSize) * 3 * 1.2;\n-  if (actual_max_map_count < required_max_map_count) {\n-    log_warning_p(gc)(\"***** WARNING! INCORRECT SYSTEM CONFIGURATION DETECTED! *****\");\n-    log_warning_p(gc)(\"The system limit on number of memory mappings per process might be too low for the given\");\n-    log_warning_p(gc)(\"max Java heap size (\" SIZE_FORMAT \"M). Please adjust %s to allow for at\",\n-                      max_capacity \/ M, filename);\n-    log_warning_p(gc)(\"least \" SIZE_FORMAT \" mappings (current limit is \" SIZE_FORMAT \"). Continuing execution \"\n-                      \"with the current\", required_max_map_count, actual_max_map_count);\n-    log_warning_p(gc)(\"limit could lead to a premature OutOfMemoryError being thrown, due to failure to map memory.\");\n-  }\n-}\n-\n-void XPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n-  \/\/ Warn if available space is too low\n-  warn_available_space(max_capacity);\n-\n-  \/\/ Warn if max map count is too low\n-  warn_max_map_count(max_capacity);\n-}\n-\n-bool XPhysicalMemoryBacking::is_tmpfs() const {\n-  return _filesystem == TMPFS_MAGIC;\n-}\n-\n-bool XPhysicalMemoryBacking::is_hugetlbfs() const {\n-  return _filesystem == HUGETLBFS_MAGIC;\n-}\n-\n-bool XPhysicalMemoryBacking::tmpfs_supports_transparent_huge_pages() const {\n-  \/\/ If the shmem_enabled file exists and is readable then we\n-  \/\/ know the kernel supports transparent huge pages for tmpfs.\n-  return access(XFILENAME_SHMEM_ENABLED, R_OK) == 0;\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const {\n-  \/\/ On hugetlbfs, mapping a file segment will fail immediately, without\n-  \/\/ the need to touch the mapped pages first, if there aren't enough huge\n-  \/\/ pages available to back the mapping.\n-  void* const addr = mmap(nullptr, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);\n-  if (addr == MAP_FAILED) {\n-    \/\/ Failed\n-    return errno;\n-  }\n-\n-  \/\/ Once mapped, the huge pages are only reserved. We need to touch them\n-  \/\/ to associate them with the file segment. Note that we can not punch\n-  \/\/ hole in file segments which only have reserved pages.\n-  if (touch) {\n-    char* const start = (char*)addr;\n-    char* const end = start + length;\n-    os::pretouch_memory(start, end, _block_size);\n-  }\n-\n-  \/\/ Unmap again. From now on, the huge pages that were mapped are allocated\n-  \/\/ to this file. There's no risk of getting a SIGBUS when mapping and\n-  \/\/ touching these pages again.\n-  if (munmap(addr, length) == -1) {\n-    \/\/ Failed\n-    return errno;\n-  }\n-\n-  \/\/ Success\n-  return 0;\n-}\n-\n-static bool safe_touch_mapping(void* addr, size_t length, size_t page_size) {\n-  char* const start = (char*)addr;\n-  char* const end = start + length;\n-\n-  \/\/ Touching a mapping that can't be backed by memory will generate a\n-  \/\/ SIGBUS. By using SafeFetch32 any SIGBUS will be safely caught and\n-  \/\/ handled. On tmpfs, doing a fetch (rather than a store) is enough\n-  \/\/ to cause backing pages to be allocated (there's no zero-page to\n-  \/\/ worry about).\n-  for (char *p = start; p < end; p += page_size) {\n-    if (SafeFetch32((int*)p, -1) == -1) {\n-      \/\/ Failed\n-      return false;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const {\n-  \/\/ On tmpfs, we need to touch the mapped pages to figure out\n-  \/\/ if there are enough pages available to back the mapping.\n-  void* const addr = mmap(nullptr, length, PROT_READ|PROT_WRITE, MAP_SHARED, _fd, offset);\n-  if (addr == MAP_FAILED) {\n-    \/\/ Failed\n-    return errno;\n-  }\n-\n-  \/\/ Advise mapping to use transparent huge pages\n-  os::realign_memory((char*)addr, length, XGranuleSize);\n-\n-  \/\/ Touch the mapping (safely) to make sure it's backed by memory\n-  const bool backed = safe_touch_mapping(addr, length, _block_size);\n-\n-  \/\/ Unmap again. If successfully touched, the backing memory will\n-  \/\/ be allocated to this file. There's no risk of getting a SIGBUS\n-  \/\/ when mapping and touching these pages again.\n-  if (munmap(addr, length) == -1) {\n-    \/\/ Failed\n-    return errno;\n-  }\n-\n-  \/\/ Success\n-  return backed ? 0 : ENOMEM;\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_compat_pwrite(size_t offset, size_t length) const {\n-  uint8_t data = 0;\n-\n-  \/\/ Allocate backing memory by writing to each block\n-  for (size_t pos = offset; pos < offset + length; pos += _block_size) {\n-    if (pwrite(_fd, &data, sizeof(data), pos) == -1) {\n-      \/\/ Failed\n-      return errno;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return 0;\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_fill_hole_compat(size_t offset, size_t length) const {\n-  \/\/ fallocate(2) is only supported by tmpfs since Linux 3.5, and by hugetlbfs\n-  \/\/ since Linux 4.3. When fallocate(2) is not supported we emulate it using\n-  \/\/ mmap\/munmap (for hugetlbfs and tmpfs with transparent huge pages) or pwrite\n-  \/\/ (for tmpfs without transparent huge pages and other filesystem types).\n-  if (XLargePages::is_explicit()) {\n-    return fallocate_compat_mmap_hugetlbfs(offset, length, false \/* touch *\/);\n-  } else if (XLargePages::is_transparent()) {\n-    return fallocate_compat_mmap_tmpfs(offset, length);\n-  } else {\n-    return fallocate_compat_pwrite(offset, length);\n-  }\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_fill_hole_syscall(size_t offset, size_t length) const {\n-  const int mode = 0; \/\/ Allocate\n-  const int res = XSyscall::fallocate(_fd, mode, offset, length);\n-  if (res == -1) {\n-    \/\/ Failed\n-    return errno;\n-  }\n-\n-  \/\/ Success\n-  return 0;\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_fill_hole(size_t offset, size_t length) const {\n-  \/\/ Using compat mode is more efficient when allocating space on hugetlbfs.\n-  \/\/ Note that allocating huge pages this way will only reserve them, and not\n-  \/\/ associate them with segments of the file. We must guarantee that we at\n-  \/\/ some point touch these segments, otherwise we can not punch hole in them.\n-  \/\/ Also note that we need to use compat mode when using transparent huge pages,\n-  \/\/ since we need to use madvise(2) on the mapping before the page is allocated.\n-  if (z_fallocate_supported && !XLargePages::is_enabled()) {\n-     const XErrno err = fallocate_fill_hole_syscall(offset, length);\n-     if (!err) {\n-       \/\/ Success\n-       return 0;\n-     }\n-\n-     if (err != ENOSYS && err != EOPNOTSUPP) {\n-       \/\/ Failed\n-       return err;\n-     }\n-\n-     \/\/ Not supported\n-     log_debug_p(gc)(\"Falling back to fallocate() compatibility mode\");\n-     z_fallocate_supported = false;\n-  }\n-\n-  return fallocate_fill_hole_compat(offset, length);\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate_punch_hole(size_t offset, size_t length) const {\n-  if (XLargePages::is_explicit()) {\n-    \/\/ We can only punch hole in pages that have been touched. Non-touched\n-    \/\/ pages are only reserved, and not associated with any specific file\n-    \/\/ segment. We don't know which pages have been previously touched, so\n-    \/\/ we always touch them here to guarantee that we can punch hole.\n-    const XErrno err = fallocate_compat_mmap_hugetlbfs(offset, length, true \/* touch *\/);\n-    if (err) {\n-      \/\/ Failed\n-      return err;\n-    }\n-  }\n-\n-  const int mode = FALLOC_FL_PUNCH_HOLE|FALLOC_FL_KEEP_SIZE;\n-  if (XSyscall::fallocate(_fd, mode, offset, length) == -1) {\n-    \/\/ Failed\n-    return errno;\n-  }\n-\n-  \/\/ Success\n-  return 0;\n-}\n-\n-XErrno XPhysicalMemoryBacking::split_and_fallocate(bool punch_hole, size_t offset, size_t length) const {\n-  \/\/ Try first half\n-  const size_t offset0 = offset;\n-  const size_t length0 = align_up(length \/ 2, _block_size);\n-  const XErrno err0 = fallocate(punch_hole, offset0, length0);\n-  if (err0) {\n-    return err0;\n-  }\n-\n-  \/\/ Try second half\n-  const size_t offset1 = offset0 + length0;\n-  const size_t length1 = length - length0;\n-  const XErrno err1 = fallocate(punch_hole, offset1, length1);\n-  if (err1) {\n-    return err1;\n-  }\n-\n-  \/\/ Success\n-  return 0;\n-}\n-\n-XErrno XPhysicalMemoryBacking::fallocate(bool punch_hole, size_t offset, size_t length) const {\n-  assert(is_aligned(offset, _block_size), \"Invalid offset\");\n-  assert(is_aligned(length, _block_size), \"Invalid length\");\n-\n-  const XErrno err = punch_hole ? fallocate_punch_hole(offset, length) : fallocate_fill_hole(offset, length);\n-  if (err == EINTR && length > _block_size) {\n-    \/\/ Calling fallocate(2) with a large length can take a long time to\n-    \/\/ complete. When running profilers, such as VTune, this syscall will\n-    \/\/ be constantly interrupted by signals. Expanding the file in smaller\n-    \/\/ steps avoids this problem.\n-    return split_and_fallocate(punch_hole, offset, length);\n-  }\n-\n-  return err;\n-}\n-\n-bool XPhysicalMemoryBacking::commit_inner(size_t offset, size_t length) const {\n-  log_trace(gc, heap)(\"Committing memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n-\n-retry:\n-  const XErrno err = fallocate(false \/* punch_hole *\/, offset, length);\n-  if (err) {\n-    if (err == ENOSPC && !is_init_completed() && XLargePages::is_explicit() && z_fallocate_hugetlbfs_attempts-- > 0) {\n-      \/\/ If we fail to allocate during initialization, due to lack of space on\n-      \/\/ the hugetlbfs filesystem, then we wait and retry a few times before\n-      \/\/ giving up. Otherwise there is a risk that running JVMs back-to-back\n-      \/\/ will fail, since there is a delay between process termination and the\n-      \/\/ huge pages owned by that process being returned to the huge page pool\n-      \/\/ and made available for new allocations.\n-      log_debug_p(gc, init)(\"Failed to commit memory (%s), retrying\", err.to_string());\n-\n-      \/\/ Wait and retry in one second, in the hope that huge pages will be\n-      \/\/ available by then.\n-      sleep(1);\n-      goto retry;\n-    }\n-\n-    \/\/ Failed\n-    log_error_p(gc)(\"Failed to commit memory (%s)\", err.to_string());\n-    return false;\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-static int offset_to_node(size_t offset) {\n-  const GrowableArray<int>* mapping = os::Linux::numa_nindex_to_node();\n-  const size_t nindex = (offset >> XGranuleSizeShift) % mapping->length();\n-  return mapping->at((int)nindex);\n-}\n-\n-size_t XPhysicalMemoryBacking::commit_numa_interleaved(size_t offset, size_t length) const {\n-  size_t committed = 0;\n-\n-  \/\/ Commit one granule at a time, so that each granule\n-  \/\/ can be allocated from a different preferred node.\n-  while (committed < length) {\n-    const size_t granule_offset = offset + committed;\n-\n-    \/\/ Setup NUMA policy to allocate memory from a preferred node\n-    os::Linux::numa_set_preferred(offset_to_node(granule_offset));\n-\n-    if (!commit_inner(granule_offset, XGranuleSize)) {\n-      \/\/ Failed\n-      break;\n-    }\n-\n-    committed += XGranuleSize;\n-  }\n-\n-  \/\/ Restore NUMA policy\n-  os::Linux::numa_set_preferred(-1);\n-\n-  return committed;\n-}\n-\n-size_t XPhysicalMemoryBacking::commit_default(size_t offset, size_t length) const {\n-  \/\/ Try to commit the whole region\n-  if (commit_inner(offset, length)) {\n-    \/\/ Success\n-    return length;\n-  }\n-\n-  \/\/ Failed, try to commit as much as possible\n-  size_t start = offset;\n-  size_t end = offset + length;\n-\n-  for (;;) {\n-    length = align_down((end - start) \/ 2, XGranuleSize);\n-    if (length < XGranuleSize) {\n-      \/\/ Done, don't commit more\n-      return start - offset;\n-    }\n-\n-    if (commit_inner(start, length)) {\n-      \/\/ Success, try commit more\n-      start += length;\n-    } else {\n-      \/\/ Failed, try commit less\n-      end -= length;\n-    }\n-  }\n-}\n-\n-size_t XPhysicalMemoryBacking::commit(size_t offset, size_t length) const {\n-  if (XNUMA::is_enabled() && !XLargePages::is_explicit()) {\n-    \/\/ To get granule-level NUMA interleaving when using non-large pages,\n-    \/\/ we must explicitly interleave the memory at commit\/fallocate time.\n-    return commit_numa_interleaved(offset, length);\n-  }\n-\n-  return commit_default(offset, length);\n-}\n-\n-size_t XPhysicalMemoryBacking::uncommit(size_t offset, size_t length) const {\n-  log_trace(gc, heap)(\"Uncommitting memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n-\n-  const XErrno err = fallocate(true \/* punch_hole *\/, offset, length);\n-  if (err) {\n-    log_error(gc)(\"Failed to uncommit memory (%s)\", err.to_string());\n-    return 0;\n-  }\n-\n-  return length;\n-}\n-\n-void XPhysicalMemoryBacking::map(uintptr_t addr, size_t size, uintptr_t offset) const {\n-  const void* const res = mmap((void*)addr, size, PROT_READ|PROT_WRITE, MAP_FIXED|MAP_SHARED, _fd, offset);\n-  if (res == MAP_FAILED) {\n-    XErrno err;\n-    fatal(\"Failed to map memory (%s)\", err.to_string());\n-  }\n-}\n-\n-void XPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n-  \/\/ Note that we must keep the address space reservation intact and just detach\n-  \/\/ the backing memory. For this reason we map a new anonymous, non-accessible\n-  \/\/ and non-reserved page over the mapping instead of actually unmapping.\n-  const void* const res = mmap((void*)addr, size, PROT_NONE, MAP_FIXED | MAP_ANONYMOUS | MAP_PRIVATE | MAP_NORESERVE, -1, 0);\n-  if (res == MAP_FAILED) {\n-    XErrno err;\n-    fatal(\"Failed to map memory (%s)\", err.to_string());\n-  }\n-}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xPhysicalMemoryBacking_linux.cpp","additions":0,"deletions":724,"binary":false,"changes":724,"status":"deleted"},{"patch":"@@ -1,77 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_LINUX_GC_X_XPHYSICALMEMORYBACKING_LINUX_HPP\n-#define OS_LINUX_GC_X_XPHYSICALMEMORYBACKING_LINUX_HPP\n-\n-class XErrno;\n-\n-class XPhysicalMemoryBacking {\n-private:\n-  int      _fd;\n-  size_t   _size;\n-  uint64_t _filesystem;\n-  size_t   _block_size;\n-  size_t   _available;\n-  bool     _initialized;\n-\n-  void warn_available_space(size_t max_capacity) const;\n-  void warn_max_map_count(size_t max_capacity) const;\n-\n-  int create_mem_fd(const char* name) const;\n-  int create_file_fd(const char* name) const;\n-  int create_fd(const char* name) const;\n-\n-  bool is_tmpfs() const;\n-  bool is_hugetlbfs() const;\n-  bool tmpfs_supports_transparent_huge_pages() const;\n-\n-  XErrno fallocate_compat_mmap_hugetlbfs(size_t offset, size_t length, bool touch) const;\n-  XErrno fallocate_compat_mmap_tmpfs(size_t offset, size_t length) const;\n-  XErrno fallocate_compat_pwrite(size_t offset, size_t length) const;\n-  XErrno fallocate_fill_hole_compat(size_t offset, size_t length) const;\n-  XErrno fallocate_fill_hole_syscall(size_t offset, size_t length) const;\n-  XErrno fallocate_fill_hole(size_t offset, size_t length) const;\n-  XErrno fallocate_punch_hole(size_t offset, size_t length) const;\n-  XErrno split_and_fallocate(bool punch_hole, size_t offset, size_t length) const;\n-  XErrno fallocate(bool punch_hole, size_t offset, size_t length) const;\n-\n-  bool commit_inner(size_t offset, size_t length) const;\n-  size_t commit_numa_interleaved(size_t offset, size_t length) const;\n-  size_t commit_default(size_t offset, size_t length) const;\n-\n-public:\n-  XPhysicalMemoryBacking(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  void warn_commit_limits(size_t max_capacity) const;\n-\n-  size_t commit(size_t offset, size_t length) const;\n-  size_t uncommit(size_t offset, size_t length) const;\n-\n-  void map(uintptr_t addr, size_t size, uintptr_t offset) const;\n-  void unmap(uintptr_t addr, size_t size) const;\n-};\n-\n-#endif \/\/ OS_LINUX_GC_X_XPHYSICALMEMORYBACKING_LINUX_HPP\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xPhysicalMemoryBacking_linux.hpp","additions":0,"deletions":77,"binary":false,"changes":77,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xSyscall_linux.hpp\"\n-#include OS_CPU_HEADER(gc\/x\/xSyscall)\n-\n-#include <unistd.h>\n-\n-int XSyscall::memfd_create(const char *name, unsigned int flags) {\n-  return syscall(SYS_memfd_create, name, flags);\n-}\n-\n-int XSyscall::fallocate(int fd, int mode, size_t offset, size_t length) {\n-  return syscall(SYS_fallocate, fd, mode, offset, length);\n-}\n-\n-long XSyscall::get_mempolicy(int* mode, unsigned long* nodemask, unsigned long maxnode, void* addr, unsigned long flags) {\n-  return syscall(SYS_get_mempolicy, mode, nodemask, maxnode, addr, flags);\n-}\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xSyscall_linux.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_LINUX_GC_X_XSYSCALL_LINUX_HPP\n-#define OS_LINUX_GC_X_XSYSCALL_LINUX_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/\/ Flags for get_mempolicy()\n-#ifndef MPOL_F_NODE\n-#define MPOL_F_NODE        (1<<0)\n-#endif\n-#ifndef MPOL_F_ADDR\n-#define MPOL_F_ADDR        (1<<1)\n-#endif\n-\n-class XSyscall : public AllStatic {\n-public:\n-  static int memfd_create(const char* name, unsigned int flags);\n-  static int fallocate(int fd, int mode, size_t offset, size_t length);\n-  static long get_mempolicy(int* mode, unsigned long* nodemask, unsigned long maxnode, void* addr, unsigned long flags);\n-};\n-\n-#endif \/\/ OS_LINUX_GC_X_XSYSCALL_LINUX_HPP\n","filename":"src\/hotspot\/os\/linux\/gc\/x\/xSyscall_linux.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,29 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xArguments.hpp\"\n-\n-bool XArguments::is_os_supported() {\n-  return true;\n-}\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xArguments_posix.cpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"deleted"},{"patch":"@@ -1,29 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xInitialize.hpp\"\n-\n-void XInitialize::pd_initialize() {\n-  \/\/ Does nothing\n-}\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xInitialize_posix.cpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xUtils.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-#include <stdlib.h>\n-\n-uintptr_t XUtils::alloc_aligned(size_t alignment, size_t size) {\n-  void* res = nullptr;\n-\n-  \/\/ Use raw posix_memalign as long as we have no wrapper for it\n-  ALLOW_C_FUNCTION(::posix_memalign, int rc = posix_memalign(&res, alignment, size);)\n-  if (rc != 0) {\n-    fatal(\"posix_memalign() failed\");\n-  }\n-\n-  memset(res, 0, size);\n-\n-  return (uintptr_t)res;\n-}\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xUtils_posix.cpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xVirtualMemory.hpp\"\n-#include \"logging\/log.hpp\"\n-\n-#include <sys\/mman.h>\n-#include <sys\/types.h>\n-\n-void XVirtualMemoryManager::pd_initialize_before_reserve() {\n-  \/\/ Does nothing\n-}\n-\n-void XVirtualMemoryManager::pd_initialize_after_reserve() {\n-  \/\/ Does nothing\n-}\n-\n-bool XVirtualMemoryManager::pd_reserve(uintptr_t addr, size_t size) {\n-  const uintptr_t res = (uintptr_t)mmap((void*)addr, size, PROT_NONE, MAP_ANONYMOUS|MAP_PRIVATE|MAP_NORESERVE, -1, 0);\n-  if (res == (uintptr_t)MAP_FAILED) {\n-    \/\/ Failed to reserve memory\n-    return false;\n-  }\n-\n-  if (res != addr) {\n-    \/\/ Failed to reserve memory at the requested address\n-    munmap((void*)res, size);\n-    return false;\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-void XVirtualMemoryManager::pd_unreserve(uintptr_t addr, size_t size) {\n-  const int res = munmap((void*)addr, size);\n-  assert(res == 0, \"Failed to unmap memory\");\n-}\n","filename":"src\/hotspot\/os\/posix\/gc\/x\/xVirtualMemory_posix.cpp","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -1,30 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xArguments.hpp\"\n-#include \"gc\/x\/xSyscall_windows.hpp\"\n-\n-bool XArguments::is_os_supported() {\n-  return XSyscall::is_supported();\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xArguments_windows.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"deleted"},{"patch":"@@ -1,30 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xInitialize.hpp\"\n-#include \"gc\/x\/xSyscall_windows.hpp\"\n-\n-void XInitialize::pd_initialize() {\n-  XSyscall::initialize();\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xInitialize_windows.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xLargePages.hpp\"\n-#include \"gc\/x\/xSyscall_windows.hpp\"\n-#include \"runtime\/globals.hpp\"\n-\n-void XLargePages::pd_initialize() {\n-  if (UseLargePages) {\n-    if (XSyscall::is_large_pages_supported()) {\n-      _state = Explicit;\n-      return;\n-    }\n-    log_info_p(gc, init)(\"Shared large pages not supported on this OS version\");\n-  }\n-\n-  _state = Disabled;\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xLargePages_windows.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -1,310 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xMapper_windows.hpp\"\n-#include \"gc\/x\/xSyscall_windows.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-#include <Windows.h>\n-\n-\/\/ Memory reservation, commit, views, and placeholders.\n-\/\/\n-\/\/ To be able to up-front reserve address space for the heap views, and later\n-\/\/ multi-map the heap views to the same physical memory, without ever losing the\n-\/\/ reservation of the reserved address space, we use \"placeholders\".\n-\/\/\n-\/\/ These placeholders block out the address space from being used by other parts\n-\/\/ of the process. To commit memory in this address space, the placeholder must\n-\/\/ be replaced by anonymous memory, or replaced by mapping a view against a\n-\/\/ paging file mapping. We use the later to support multi-mapping.\n-\/\/\n-\/\/ We want to be able to dynamically commit and uncommit the physical memory of\n-\/\/ the heap (and also unmap ZPages), in granules of ZGranuleSize bytes. There is\n-\/\/ no way to grow and shrink the committed memory of a paging file mapping.\n-\/\/ Therefore, we create multiple granule-sized page file mappings. The memory is\n-\/\/ committed by creating a page file mapping, map a view against it, commit the\n-\/\/ memory, unmap the view. The memory will stay committed until all views are\n-\/\/ unmapped, and the paging file mapping handle is closed.\n-\/\/\n-\/\/ When replacing a placeholder address space reservation with a mapped view\n-\/\/ against a paging file mapping, the virtual address space must exactly match\n-\/\/ an existing placeholder's address and size. Therefore we only deal with\n-\/\/ granule-sized placeholders at this layer. Higher layers that keep track of\n-\/\/ reserved available address space can (and will) coalesce placeholders, but\n-\/\/ they will be split before being used.\n-\n-#define fatal_error(msg, addr, size)                  \\\n-  fatal(msg \": \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\", \\\n-        (addr), (size) \/ M, GetLastError())\n-\n-uintptr_t XMapper::reserve(uintptr_t addr, size_t size) {\n-  void* const res = XSyscall::VirtualAlloc2(\n-    GetCurrentProcess(),                   \/\/ Process\n-    (void*)addr,                           \/\/ BaseAddress\n-    size,                                  \/\/ Size\n-    MEM_RESERVE | MEM_RESERVE_PLACEHOLDER, \/\/ AllocationType\n-    PAGE_NOACCESS,                         \/\/ PageProtection\n-    nullptr,                               \/\/ ExtendedParameters\n-    0                                      \/\/ ParameterCount\n-    );\n-\n-  \/\/ Caller responsible for error handling\n-  return (uintptr_t)res;\n-}\n-\n-void XMapper::unreserve(uintptr_t addr, size_t size) {\n-  const bool res = XSyscall::VirtualFreeEx(\n-    GetCurrentProcess(), \/\/ hProcess\n-    (void*)addr,         \/\/ lpAddress\n-    size,                \/\/ dwSize\n-    MEM_RELEASE          \/\/ dwFreeType\n-    );\n-\n-  if (!res) {\n-    fatal_error(\"Failed to unreserve memory\", addr, size);\n-  }\n-}\n-\n-HANDLE XMapper::create_paging_file_mapping(size_t size) {\n-  \/\/ Create mapping with SEC_RESERVE instead of SEC_COMMIT.\n-  \/\/\n-  \/\/ We use MapViewOfFile3 for two different reasons:\n-  \/\/  1) When committing memory for the created paging file\n-  \/\/  2) When mapping a view of the memory created in (2)\n-  \/\/\n-  \/\/ The non-platform code is only setup to deal with out-of-memory\n-  \/\/ errors in (1). By using SEC_RESERVE, we prevent MapViewOfFile3\n-  \/\/ from failing because of \"commit limit\" checks. To actually commit\n-  \/\/ memory in (1), a call to VirtualAlloc2 is done.\n-\n-  HANDLE const res = XSyscall::CreateFileMappingW(\n-    INVALID_HANDLE_VALUE,         \/\/ hFile\n-    nullptr,                      \/\/ lpFileMappingAttribute\n-    PAGE_READWRITE | SEC_RESERVE, \/\/ flProtect\n-    size >> 32,                   \/\/ dwMaximumSizeHigh\n-    size & 0xFFFFFFFF,            \/\/ dwMaximumSizeLow\n-    nullptr                       \/\/ lpName\n-    );\n-\n-  \/\/ Caller responsible for error handling\n-  return res;\n-}\n-\n-bool XMapper::commit_paging_file_mapping(HANDLE file_handle, uintptr_t file_offset, size_t size) {\n-  const uintptr_t addr = map_view_no_placeholder(file_handle, file_offset, size);\n-  if (addr == 0) {\n-    log_error(gc)(\"Failed to map view of paging file mapping (%d)\", GetLastError());\n-    return false;\n-  }\n-\n-  const uintptr_t res = commit(addr, size);\n-  if (res != addr) {\n-    log_error(gc)(\"Failed to commit memory (%d)\", GetLastError());\n-  }\n-\n-  unmap_view_no_placeholder(addr, size);\n-\n-  return res == addr;\n-}\n-\n-uintptr_t XMapper::map_view_no_placeholder(HANDLE file_handle, uintptr_t file_offset, size_t size) {\n-  void* const res = XSyscall::MapViewOfFile3(\n-    file_handle,         \/\/ FileMapping\n-    GetCurrentProcess(), \/\/ ProcessHandle\n-    nullptr,             \/\/ BaseAddress\n-    file_offset,         \/\/ Offset\n-    size,                \/\/ ViewSize\n-    0,                   \/\/ AllocationType\n-    PAGE_NOACCESS,       \/\/ PageProtection\n-    nullptr,             \/\/ ExtendedParameters\n-    0                    \/\/ ParameterCount\n-    );\n-\n-  \/\/ Caller responsible for error handling\n-  return (uintptr_t)res;\n-}\n-\n-void XMapper::unmap_view_no_placeholder(uintptr_t addr, size_t size) {\n-  const bool res = XSyscall::UnmapViewOfFile2(\n-    GetCurrentProcess(), \/\/ ProcessHandle\n-    (void*)addr,         \/\/ BaseAddress\n-    0                    \/\/ UnmapFlags\n-    );\n-\n-  if (!res) {\n-    fatal_error(\"Failed to unmap memory\", addr, size);\n-  }\n-}\n-\n-uintptr_t XMapper::commit(uintptr_t addr, size_t size) {\n-  void* const res = XSyscall::VirtualAlloc2(\n-    GetCurrentProcess(), \/\/ Process\n-    (void*)addr,         \/\/ BaseAddress\n-    size,                \/\/ Size\n-    MEM_COMMIT,          \/\/ AllocationType\n-    PAGE_NOACCESS,       \/\/ PageProtection\n-    nullptr,             \/\/ ExtendedParameters\n-    0                    \/\/ ParameterCount\n-    );\n-\n-  \/\/ Caller responsible for error handling\n-  return (uintptr_t)res;\n-}\n-\n-HANDLE XMapper::create_and_commit_paging_file_mapping(size_t size) {\n-  HANDLE const file_handle = create_paging_file_mapping(size);\n-  if (file_handle == 0) {\n-    log_error(gc)(\"Failed to create paging file mapping (%d)\", GetLastError());\n-    return 0;\n-  }\n-\n-  const bool res = commit_paging_file_mapping(file_handle, 0 \/* file_offset *\/, size);\n-  if (!res) {\n-    close_paging_file_mapping(file_handle);\n-    return 0;\n-  }\n-\n-  return file_handle;\n-}\n-\n-void XMapper::close_paging_file_mapping(HANDLE file_handle) {\n-  const bool res = CloseHandle(\n-    file_handle \/\/ hObject\n-    );\n-\n-  if (!res) {\n-    fatal(\"Failed to close paging file handle (%d)\", GetLastError());\n-  }\n-}\n-\n-HANDLE XMapper::create_shared_awe_section() {\n-  MEM_EXTENDED_PARAMETER parameter = { 0 };\n-  parameter.Type = MemSectionExtendedParameterUserPhysicalFlags;\n-  parameter.ULong64 = 0;\n-\n-  HANDLE section = XSyscall::CreateFileMapping2(\n-    INVALID_HANDLE_VALUE,                 \/\/ File\n-    nullptr,                              \/\/ SecurityAttributes\n-    SECTION_MAP_READ | SECTION_MAP_WRITE, \/\/ DesiredAccess\n-    PAGE_READWRITE,                       \/\/ PageProtection\n-    SEC_RESERVE | SEC_LARGE_PAGES,        \/\/ AllocationAttributes\n-    0,                                    \/\/ MaximumSize\n-    nullptr,                              \/\/ Name\n-    &parameter,                           \/\/ ExtendedParameters\n-    1                                     \/\/ ParameterCount\n-    );\n-\n-  if (section == nullptr) {\n-    fatal(\"Could not create shared AWE section (%d)\", GetLastError());\n-  }\n-\n-  return section;\n-}\n-\n-uintptr_t XMapper::reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size) {\n-  MEM_EXTENDED_PARAMETER parameter = { 0 };\n-  parameter.Type = MemExtendedParameterUserPhysicalHandle;\n-  parameter.Handle = awe_section;\n-\n-  void* const res = XSyscall::VirtualAlloc2(\n-    GetCurrentProcess(),        \/\/ Process\n-    (void*)addr,                \/\/ BaseAddress\n-    size,                       \/\/ Size\n-    MEM_RESERVE | MEM_PHYSICAL, \/\/ AllocationType\n-    PAGE_READWRITE,             \/\/ PageProtection\n-    &parameter,                 \/\/ ExtendedParameters\n-    1                           \/\/ ParameterCount\n-    );\n-\n-  \/\/ Caller responsible for error handling\n-  return (uintptr_t)res;\n-}\n-\n-void XMapper::unreserve_for_shared_awe(uintptr_t addr, size_t size) {\n-  bool res = VirtualFree(\n-    (void*)addr, \/\/ lpAddress\n-    0,           \/\/ dwSize\n-    MEM_RELEASE  \/\/ dwFreeType\n-    );\n-\n-  if (!res) {\n-    fatal(\"Failed to unreserve memory: \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\",\n-          addr, size \/ M, GetLastError());\n-  }\n-}\n-\n-void XMapper::split_placeholder(uintptr_t addr, size_t size) {\n-  const bool res = VirtualFree(\n-    (void*)addr,                           \/\/ lpAddress\n-    size,                                  \/\/ dwSize\n-    MEM_RELEASE | MEM_PRESERVE_PLACEHOLDER \/\/ dwFreeType\n-    );\n-\n-  if (!res) {\n-    fatal_error(\"Failed to split placeholder\", addr, size);\n-  }\n-}\n-\n-void XMapper::coalesce_placeholders(uintptr_t addr, size_t size) {\n-  const bool res = VirtualFree(\n-    (void*)addr,                            \/\/ lpAddress\n-    size,                                   \/\/ dwSize\n-    MEM_RELEASE | MEM_COALESCE_PLACEHOLDERS \/\/ dwFreeType\n-    );\n-\n-  if (!res) {\n-    fatal_error(\"Failed to coalesce placeholders\", addr, size);\n-  }\n-}\n-\n-void XMapper::map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, uintptr_t addr, size_t size) {\n-  void* const res = XSyscall::MapViewOfFile3(\n-    file_handle,             \/\/ FileMapping\n-    GetCurrentProcess(),     \/\/ ProcessHandle\n-    (void*)addr,             \/\/ BaseAddress\n-    file_offset,             \/\/ Offset\n-    size,                    \/\/ ViewSize\n-    MEM_REPLACE_PLACEHOLDER, \/\/ AllocationType\n-    PAGE_READWRITE,          \/\/ PageProtection\n-    nullptr,                 \/\/ ExtendedParameters\n-    0                        \/\/ ParameterCount\n-    );\n-\n-  if (res == nullptr) {\n-    fatal_error(\"Failed to map memory\", addr, size);\n-  }\n-}\n-\n-void XMapper::unmap_view_preserve_placeholder(uintptr_t addr, size_t size) {\n-  const bool res = XSyscall::UnmapViewOfFile2(\n-    GetCurrentProcess(),     \/\/ ProcessHandle\n-    (void*)addr,             \/\/ BaseAddress\n-    MEM_PRESERVE_PLACEHOLDER \/\/ UnmapFlags\n-    );\n-\n-  if (!res) {\n-    fatal_error(\"Failed to unmap memory\", addr, size);\n-  }\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xMapper_windows.cpp","additions":0,"deletions":310,"binary":false,"changes":310,"status":"deleted"},{"patch":"@@ -1,94 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_WINDOWS_GC_X_XMAPPER_WINDOWS_HPP\n-#define OS_WINDOWS_GC_X_XMAPPER_WINDOWS_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-#include <Windows.h>\n-\n-class XMapper : public AllStatic {\n-private:\n-  \/\/ Create paging file mapping\n-  static HANDLE create_paging_file_mapping(size_t size);\n-\n-  \/\/ Commit paging file mapping\n-  static bool commit_paging_file_mapping(HANDLE file_handle, uintptr_t file_offset, size_t size);\n-\n-  \/\/ Map a view anywhere without a placeholder\n-  static uintptr_t map_view_no_placeholder(HANDLE file_handle, uintptr_t file_offset, size_t size);\n-\n-  \/\/ Unmap a view without preserving a placeholder\n-  static void unmap_view_no_placeholder(uintptr_t addr, size_t size);\n-\n-  \/\/ Commit memory covering the given virtual address range\n-  static uintptr_t commit(uintptr_t addr, size_t size);\n-\n-public:\n-  \/\/ Reserve memory with a placeholder\n-  static uintptr_t reserve(uintptr_t addr, size_t size);\n-\n-  \/\/ Unreserve memory\n-  static void unreserve(uintptr_t addr, size_t size);\n-\n-  \/\/ Create and commit paging file mapping\n-  static HANDLE create_and_commit_paging_file_mapping(size_t size);\n-\n-  \/\/ Close paging file mapping\n-  static void close_paging_file_mapping(HANDLE file_handle);\n-\n-  \/\/ Create a shared AWE section\n-  static HANDLE create_shared_awe_section();\n-\n-  \/\/ Reserve memory attached to the shared AWE section\n-  static uintptr_t reserve_for_shared_awe(HANDLE awe_section, uintptr_t addr, size_t size);\n-\n-  \/\/ Unreserve memory attached to a shared AWE section\n-  static void unreserve_for_shared_awe(uintptr_t addr, size_t size);\n-\n-  \/\/ Split a placeholder\n-  \/\/\n-  \/\/ A view can only replace an entire placeholder, so placeholders need to be\n-  \/\/ split and coalesced to be the exact size of the new views.\n-  \/\/ [addr, addr + size) needs to be a proper sub-placeholder of an existing\n-  \/\/ placeholder.\n-  static void split_placeholder(uintptr_t addr, size_t size);\n-\n-  \/\/ Coalesce a placeholder\n-  \/\/\n-  \/\/ [addr, addr + size) is the new placeholder. A sub-placeholder needs to\n-  \/\/ exist within that range.\n-  static void coalesce_placeholders(uintptr_t addr, size_t size);\n-\n-  \/\/ Map a view of the file handle and replace the placeholder covering the\n-  \/\/ given virtual address range\n-  static void map_view_replace_placeholder(HANDLE file_handle, uintptr_t file_offset, uintptr_t addr, size_t size);\n-\n-  \/\/ Unmap the view and reinstate a placeholder covering the given virtual\n-  \/\/ address range\n-  static void unmap_view_preserve_placeholder(uintptr_t addr, size_t size);\n-};\n-\n-#endif \/\/ OS_WINDOWS_GC_X_XMAPPER_WINDOWS_HPP\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xMapper_windows.hpp","additions":0,"deletions":94,"binary":false,"changes":94,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-\n-void XNUMA::pd_initialize() {\n-  _enabled = false;\n-}\n-\n-uint32_t XNUMA::count() {\n-  return 1;\n-}\n-\n-uint32_t XNUMA::id() {\n-  return 0;\n-}\n-\n-uint32_t XNUMA::memory_id(uintptr_t addr) {\n-  \/\/ NUMA support not enabled, assume everything belongs to node zero\n-  return 0;\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xNUMA_windows.cpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,252 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xGranuleMap.inline.hpp\"\n-#include \"gc\/x\/xLargePages.inline.hpp\"\n-#include \"gc\/x\/xMapper_windows.hpp\"\n-#include \"gc\/x\/xPhysicalMemoryBacking_windows.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-class XPhysicalMemoryBackingImpl : public CHeapObj<mtGC> {\n-public:\n-  virtual size_t commit(size_t offset, size_t size) = 0;\n-  virtual size_t uncommit(size_t offset, size_t size) = 0;\n-  virtual void map(uintptr_t addr, size_t size, size_t offset) const = 0;\n-  virtual void unmap(uintptr_t addr, size_t size) const = 0;\n-};\n-\n-\/\/ Implements small pages (paged) support using placeholder reservation.\n-\/\/\n-\/\/ The backing commits and uncommits physical memory, that can be\n-\/\/ multi-mapped into the virtual address space. To support fine-graned\n-\/\/ committing and uncommitting, each XGranuleSize'd chunk is mapped to\n-\/\/ a separate paging file mapping.\n-\n-class XPhysicalMemoryBackingSmallPages : public XPhysicalMemoryBackingImpl {\n-private:\n-  XGranuleMap<HANDLE> _handles;\n-\n-  HANDLE get_handle(uintptr_t offset) const {\n-    HANDLE const handle = _handles.get(offset);\n-    assert(handle != 0, \"Should be set\");\n-    return handle;\n-  }\n-\n-  void put_handle(uintptr_t offset, HANDLE handle) {\n-    assert(handle != INVALID_HANDLE_VALUE, \"Invalid handle\");\n-    assert(_handles.get(offset) == 0, \"Should be cleared\");\n-    _handles.put(offset, handle);\n-  }\n-\n-  void clear_handle(uintptr_t offset) {\n-    assert(_handles.get(offset) != 0, \"Should be set\");\n-    _handles.put(offset, 0);\n-  }\n-\n-public:\n-  XPhysicalMemoryBackingSmallPages(size_t max_capacity) :\n-      XPhysicalMemoryBackingImpl(),\n-      _handles(max_capacity) {}\n-\n-  size_t commit(size_t offset, size_t size) {\n-    for (size_t i = 0; i < size; i += XGranuleSize) {\n-      HANDLE const handle = XMapper::create_and_commit_paging_file_mapping(XGranuleSize);\n-      if (handle == 0) {\n-        return i;\n-      }\n-\n-      put_handle(offset + i, handle);\n-    }\n-\n-    return size;\n-  }\n-\n-  size_t uncommit(size_t offset, size_t size) {\n-    for (size_t i = 0; i < size; i += XGranuleSize) {\n-      HANDLE const handle = get_handle(offset + i);\n-      clear_handle(offset + i);\n-      XMapper::close_paging_file_mapping(handle);\n-    }\n-\n-    return size;\n-  }\n-\n-  void map(uintptr_t addr, size_t size, size_t offset) const {\n-    assert(is_aligned(offset, XGranuleSize), \"Misaligned\");\n-    assert(is_aligned(addr, XGranuleSize), \"Misaligned\");\n-    assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n-\n-    for (size_t i = 0; i < size; i += XGranuleSize) {\n-      HANDLE const handle = get_handle(offset + i);\n-      XMapper::map_view_replace_placeholder(handle, 0 \/* offset *\/, addr + i, XGranuleSize);\n-    }\n-  }\n-\n-  void unmap(uintptr_t addr, size_t size) const {\n-    assert(is_aligned(addr, XGranuleSize), \"Misaligned\");\n-    assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n-\n-    for (size_t i = 0; i < size; i += XGranuleSize) {\n-      XMapper::unmap_view_preserve_placeholder(addr + i, XGranuleSize);\n-    }\n-  }\n-};\n-\n-\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n-\/\/\n-\/\/ Shared AWE physical memory also works with small pages, but it has\n-\/\/ a few drawbacks that makes it a no-go to use it at this point:\n-\/\/\n-\/\/ 1) It seems to use 8 bytes of committed memory per *reserved* memory.\n-\/\/ Given our scheme to use a large address space range this turns out to\n-\/\/ use too much memory.\n-\/\/\n-\/\/ 2) It requires memory locking privileges, even for small pages. This\n-\/\/ has always been a requirement for large pages, and would be an extra\n-\/\/ restriction for usage with small pages.\n-\/\/\n-\/\/ Note: The large pages size is tied to our XGranuleSize.\n-\n-extern HANDLE XAWESection;\n-\n-class XPhysicalMemoryBackingLargePages : public XPhysicalMemoryBackingImpl {\n-private:\n-  ULONG_PTR* const _page_array;\n-\n-  static ULONG_PTR* alloc_page_array(size_t max_capacity) {\n-    const size_t npages = max_capacity \/ XGranuleSize;\n-    const size_t array_size = npages * sizeof(ULONG_PTR);\n-\n-    return (ULONG_PTR*)os::malloc(array_size, mtGC);\n-  }\n-\n-public:\n-  XPhysicalMemoryBackingLargePages(size_t max_capacity) :\n-      XPhysicalMemoryBackingImpl(),\n-      _page_array(alloc_page_array(max_capacity)) {}\n-\n-  size_t commit(size_t offset, size_t size) {\n-    const size_t index = offset >> XGranuleSizeShift;\n-    const size_t npages = size >> XGranuleSizeShift;\n-\n-    size_t npages_res = npages;\n-    const bool res = AllocateUserPhysicalPages(XAWESection, &npages_res, &_page_array[index]);\n-    if (!res) {\n-      fatal(\"Failed to allocate physical memory \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n-            size \/ M, offset, GetLastError());\n-    } else {\n-      log_debug(gc)(\"Allocated physical memory: \" SIZE_FORMAT \"M @ \" PTR_FORMAT, size \/ M, offset);\n-    }\n-\n-    \/\/ AllocateUserPhysicalPages might not be able to allocate the requested amount of memory.\n-    \/\/ The allocated number of pages are written in npages_res.\n-    return npages_res << XGranuleSizeShift;\n-  }\n-\n-  size_t uncommit(size_t offset, size_t size) {\n-    const size_t index = offset >> XGranuleSizeShift;\n-    const size_t npages = size >> XGranuleSizeShift;\n-\n-    size_t npages_res = npages;\n-    const bool res = FreeUserPhysicalPages(XAWESection, &npages_res, &_page_array[index]);\n-    if (!res) {\n-      fatal(\"Failed to uncommit physical memory \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n-            size, offset, GetLastError());\n-    }\n-\n-    return npages_res << XGranuleSizeShift;\n-  }\n-\n-  void map(uintptr_t addr, size_t size, size_t offset) const {\n-    const size_t npages = size >> XGranuleSizeShift;\n-    const size_t index = offset >> XGranuleSizeShift;\n-\n-    const bool res = MapUserPhysicalPages((char*)addr, npages, &_page_array[index]);\n-    if (!res) {\n-      fatal(\"Failed to map view \" PTR_FORMAT \" \" SIZE_FORMAT \"M @ \" PTR_FORMAT \" (%d)\",\n-            addr, size \/ M, offset, GetLastError());\n-    }\n-  }\n-\n-  void unmap(uintptr_t addr, size_t size) const {\n-    const size_t npages = size >> XGranuleSizeShift;\n-\n-    const bool res = MapUserPhysicalPages((char*)addr, npages, nullptr);\n-    if (!res) {\n-      fatal(\"Failed to unmap view \" PTR_FORMAT \" \" SIZE_FORMAT \"M (%d)\",\n-            addr, size \/ M, GetLastError());\n-    }\n-  }\n-};\n-\n-static XPhysicalMemoryBackingImpl* select_impl(size_t max_capacity) {\n-  if (XLargePages::is_enabled()) {\n-    return new XPhysicalMemoryBackingLargePages(max_capacity);\n-  }\n-\n-  return new XPhysicalMemoryBackingSmallPages(max_capacity);\n-}\n-\n-XPhysicalMemoryBacking::XPhysicalMemoryBacking(size_t max_capacity) :\n-    _impl(select_impl(max_capacity)) {}\n-\n-bool XPhysicalMemoryBacking::is_initialized() const {\n-  return true;\n-}\n-\n-void XPhysicalMemoryBacking::warn_commit_limits(size_t max_capacity) const {\n-  \/\/ Does nothing\n-}\n-\n-size_t XPhysicalMemoryBacking::commit(size_t offset, size_t length) {\n-  log_trace(gc, heap)(\"Committing memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n-\n-  return _impl->commit(offset, length);\n-}\n-\n-size_t XPhysicalMemoryBacking::uncommit(size_t offset, size_t length) {\n-  log_trace(gc, heap)(\"Uncommitting memory: \" SIZE_FORMAT \"M-\" SIZE_FORMAT \"M (\" SIZE_FORMAT \"M)\",\n-                      offset \/ M, (offset + length) \/ M, length \/ M);\n-\n-  return _impl->uncommit(offset, length);\n-}\n-\n-void XPhysicalMemoryBacking::map(uintptr_t addr, size_t size, size_t offset) const {\n-  assert(is_aligned(offset, XGranuleSize), \"Misaligned: \" PTR_FORMAT, offset);\n-  assert(is_aligned(addr, XGranuleSize), \"Misaligned: \" PTR_FORMAT, addr);\n-  assert(is_aligned(size, XGranuleSize), \"Misaligned: \" PTR_FORMAT, size);\n-\n-  _impl->map(addr, size, offset);\n-}\n-\n-void XPhysicalMemoryBacking::unmap(uintptr_t addr, size_t size) const {\n-  assert(is_aligned(addr, XGranuleSize), \"Misaligned\");\n-  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n-\n-  _impl->unmap(addr, size);\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xPhysicalMemoryBacking_windows.cpp","additions":0,"deletions":252,"binary":false,"changes":252,"status":"deleted"},{"patch":"@@ -1,51 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_WINDOWS_GC_X_XPHYSICALMEMORYBACKING_WINDOWS_HPP\n-#define OS_WINDOWS_GC_X_XPHYSICALMEMORYBACKING_WINDOWS_HPP\n-\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-#include <Windows.h>\n-\n-class XPhysicalMemoryBackingImpl;\n-\n-class XPhysicalMemoryBacking {\n-private:\n-  XPhysicalMemoryBackingImpl* _impl;\n-\n-public:\n-  XPhysicalMemoryBacking(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  void warn_commit_limits(size_t max_capacity) const;\n-\n-  size_t commit(size_t offset, size_t length);\n-  size_t uncommit(size_t offset, size_t length);\n-\n-  void map(uintptr_t addr, size_t size, size_t offset) const;\n-  void unmap(uintptr_t addr, size_t size) const;\n-};\n-\n-#endif \/\/ OS_WINDOWS_GC_X_XPHYSICALMEMORYBACKING_WINDOWS_HPP\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xPhysicalMemoryBacking_windows.hpp","additions":0,"deletions":51,"binary":false,"changes":51,"status":"deleted"},{"patch":"@@ -1,93 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xSyscall_windows.hpp\"\n-#include \"runtime\/java.hpp\"\n-#include \"runtime\/os.hpp\"\n-\n-XSyscall::CreateFileMappingWFn XSyscall::CreateFileMappingW;\n-XSyscall::CreateFileMapping2Fn XSyscall::CreateFileMapping2;\n-XSyscall::VirtualAlloc2Fn XSyscall::VirtualAlloc2;\n-XSyscall::VirtualFreeExFn XSyscall::VirtualFreeEx;\n-XSyscall::MapViewOfFile3Fn XSyscall::MapViewOfFile3;\n-XSyscall::UnmapViewOfFile2Fn XSyscall::UnmapViewOfFile2;\n-\n-static void* lookup_kernelbase_library() {\n-  const char* const name = \"KernelBase\";\n-  char ebuf[1024];\n-  void* const handle = os::dll_load(name, ebuf, sizeof(ebuf));\n-  if (handle == nullptr) {\n-    log_error_p(gc)(\"Failed to load library: %s\", name);\n-  }\n-  return handle;\n-}\n-\n-static void* lookup_kernelbase_symbol(const char* name) {\n-  static void* const handle = lookup_kernelbase_library();\n-  if (handle == nullptr) {\n-    return nullptr;\n-  }\n-  return os::dll_lookup(handle, name);\n-}\n-\n-static bool has_kernelbase_symbol(const char* name) {\n-  return lookup_kernelbase_symbol(name) != nullptr;\n-}\n-\n-template <typename Fn>\n-static void install_kernelbase_symbol(Fn*& fn, const char* name) {\n-  fn = reinterpret_cast<Fn*>(lookup_kernelbase_symbol(name));\n-}\n-\n-template <typename Fn>\n-static void install_kernelbase_1803_symbol_or_exit(Fn*& fn, const char* name) {\n-  install_kernelbase_symbol(fn, name);\n-  if (fn == nullptr) {\n-    log_error_p(gc)(\"Failed to lookup symbol: %s\", name);\n-    vm_exit_during_initialization(\"ZGC requires Windows version 1803 or later\");\n-  }\n-}\n-\n-void XSyscall::initialize() {\n-  \/\/ Required\n-  install_kernelbase_1803_symbol_or_exit(CreateFileMappingW, \"CreateFileMappingW\");\n-  install_kernelbase_1803_symbol_or_exit(VirtualAlloc2,      \"VirtualAlloc2\");\n-  install_kernelbase_1803_symbol_or_exit(VirtualFreeEx,      \"VirtualFreeEx\");\n-  install_kernelbase_1803_symbol_or_exit(MapViewOfFile3,     \"MapViewOfFile3\");\n-  install_kernelbase_1803_symbol_or_exit(UnmapViewOfFile2,   \"UnmapViewOfFile2\");\n-\n-  \/\/ Optional - for large pages support\n-  install_kernelbase_symbol(CreateFileMapping2, \"CreateFileMapping2\");\n-}\n-\n-bool XSyscall::is_supported() {\n-  \/\/ Available in Windows version 1803 and later\n-  return has_kernelbase_symbol(\"VirtualAlloc2\");\n-}\n-\n-bool XSyscall::is_large_pages_supported() {\n-  \/\/ Available in Windows version 1809 and later\n-  return has_kernelbase_symbol(\"CreateFileMapping2\");\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xSyscall_windows.cpp","additions":0,"deletions":93,"binary":false,"changes":93,"status":"deleted"},{"patch":"@@ -1,55 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_WINDOWS_GC_X_XSYSCALL_WINDOWS_HPP\n-#define OS_WINDOWS_GC_X_XSYSCALL_WINDOWS_HPP\n-\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-#include <Windows.h>\n-#include <Memoryapi.h>\n-\n-class XSyscall {\n-private:\n-  typedef HANDLE (*CreateFileMappingWFn)(HANDLE, LPSECURITY_ATTRIBUTES, DWORD, DWORD, DWORD, LPCWSTR);\n-  typedef HANDLE (*CreateFileMapping2Fn)(HANDLE, LPSECURITY_ATTRIBUTES, ULONG, ULONG, ULONG, ULONG64, PCWSTR, PMEM_EXTENDED_PARAMETER, ULONG);\n-  typedef PVOID (*VirtualAlloc2Fn)(HANDLE, PVOID, SIZE_T, ULONG, ULONG, MEM_EXTENDED_PARAMETER*, ULONG);\n-  typedef BOOL (*VirtualFreeExFn)(HANDLE, LPVOID, SIZE_T, DWORD);\n-  typedef PVOID (*MapViewOfFile3Fn)(HANDLE, HANDLE, PVOID, ULONG64, SIZE_T, ULONG, ULONG, MEM_EXTENDED_PARAMETER*, ULONG);\n-  typedef BOOL (*UnmapViewOfFile2Fn)(HANDLE, PVOID, ULONG);\n-\n-public:\n-  static CreateFileMappingWFn CreateFileMappingW;\n-  static CreateFileMapping2Fn CreateFileMapping2;\n-  static VirtualAlloc2Fn      VirtualAlloc2;\n-  static VirtualFreeExFn      VirtualFreeEx;\n-  static MapViewOfFile3Fn     MapViewOfFile3;\n-  static UnmapViewOfFile2Fn   UnmapViewOfFile2;\n-\n-  static void initialize();\n-\n-  static bool is_supported();\n-  static bool is_large_pages_supported();\n-};\n-\n-#endif \/\/ OS_WINDOWS_GC_X_XSYSCALL_WINDOWS_HPP\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xSyscall_windows.hpp","additions":0,"deletions":55,"binary":false,"changes":55,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xUtils.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-#include <malloc.h>\n-\n-uintptr_t XUtils::alloc_aligned(size_t alignment, size_t size) {\n-  void* const res = _aligned_malloc(size, alignment);\n-\n-  if (res == nullptr) {\n-    fatal(\"_aligned_malloc failed\");\n-  }\n-\n-  memset(res, 0, size);\n-\n-  return (uintptr_t)res;\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xUtils_windows.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -1,195 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLargePages.inline.hpp\"\n-#include \"gc\/x\/xMapper_windows.hpp\"\n-#include \"gc\/x\/xSyscall_windows.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-class XVirtualMemoryManagerImpl : public CHeapObj<mtGC> {\n-public:\n-  virtual void initialize_before_reserve() {}\n-  virtual void initialize_after_reserve(XMemoryManager* manager) {}\n-  virtual bool reserve(uintptr_t addr, size_t size) = 0;\n-  virtual void unreserve(uintptr_t addr, size_t size) = 0;\n-};\n-\n-\/\/ Implements small pages (paged) support using placeholder reservation.\n-class XVirtualMemoryManagerSmallPages : public XVirtualMemoryManagerImpl {\n-private:\n-  class PlaceholderCallbacks : public AllStatic {\n-  public:\n-    static void split_placeholder(uintptr_t start, size_t size) {\n-      XMapper::split_placeholder(XAddress::marked0(start), size);\n-      XMapper::split_placeholder(XAddress::marked1(start), size);\n-      XMapper::split_placeholder(XAddress::remapped(start), size);\n-    }\n-\n-    static void coalesce_placeholders(uintptr_t start, size_t size) {\n-      XMapper::coalesce_placeholders(XAddress::marked0(start), size);\n-      XMapper::coalesce_placeholders(XAddress::marked1(start), size);\n-      XMapper::coalesce_placeholders(XAddress::remapped(start), size);\n-    }\n-\n-    static void split_into_placeholder_granules(uintptr_t start, size_t size) {\n-      for (uintptr_t addr = start; addr < start + size; addr += XGranuleSize) {\n-        split_placeholder(addr, XGranuleSize);\n-      }\n-    }\n-\n-    static void coalesce_into_one_placeholder(uintptr_t start, size_t size) {\n-      assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n-\n-      if (size > XGranuleSize) {\n-        coalesce_placeholders(start, size);\n-      }\n-    }\n-\n-    static void create_callback(const XMemory* area) {\n-      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n-      coalesce_into_one_placeholder(area->start(), area->size());\n-    }\n-\n-    static void destroy_callback(const XMemory* area) {\n-      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n-      \/\/ Don't try split the last granule - VirtualFree will fail\n-      split_into_placeholder_granules(area->start(), area->size() - XGranuleSize);\n-    }\n-\n-    static void shrink_from_front_callback(const XMemory* area, size_t size) {\n-      assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n-      split_into_placeholder_granules(area->start(), size);\n-    }\n-\n-    static void shrink_from_back_callback(const XMemory* area, size_t size) {\n-      assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n-      \/\/ Don't try split the last granule - VirtualFree will fail\n-      split_into_placeholder_granules(area->end() - size, size - XGranuleSize);\n-    }\n-\n-    static void grow_from_front_callback(const XMemory* area, size_t size) {\n-      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n-      coalesce_into_one_placeholder(area->start() - size, area->size() + size);\n-    }\n-\n-    static void grow_from_back_callback(const XMemory* area, size_t size) {\n-      assert(is_aligned(area->size(), XGranuleSize), \"Must be granule aligned\");\n-      coalesce_into_one_placeholder(area->start(), area->size() + size);\n-    }\n-\n-    static void register_with(XMemoryManager* manager) {\n-      \/\/ Each reserved virtual memory address area registered in _manager is\n-      \/\/ exactly covered by a single placeholder. Callbacks are installed so\n-      \/\/ that whenever a memory area changes, the corresponding placeholder\n-      \/\/ is adjusted.\n-      \/\/\n-      \/\/ The create and grow callbacks are called when virtual memory is\n-      \/\/ returned to the memory manager. The new memory area is then covered\n-      \/\/ by a new single placeholder.\n-      \/\/\n-      \/\/ The destroy and shrink callbacks are called when virtual memory is\n-      \/\/ allocated from the memory manager. The memory area is then is split\n-      \/\/ into granule-sized placeholders.\n-      \/\/\n-      \/\/ See comment in zMapper_windows.cpp explaining why placeholders are\n-      \/\/ split into XGranuleSize sized placeholders.\n-\n-      XMemoryManager::Callbacks callbacks;\n-\n-      callbacks._create = &create_callback;\n-      callbacks._destroy = &destroy_callback;\n-      callbacks._shrink_from_front = &shrink_from_front_callback;\n-      callbacks._shrink_from_back = &shrink_from_back_callback;\n-      callbacks._grow_from_front = &grow_from_front_callback;\n-      callbacks._grow_from_back = &grow_from_back_callback;\n-\n-      manager->register_callbacks(callbacks);\n-    }\n-  };\n-\n-  virtual void initialize_after_reserve(XMemoryManager* manager) {\n-    PlaceholderCallbacks::register_with(manager);\n-  }\n-\n-  virtual bool reserve(uintptr_t addr, size_t size) {\n-    const uintptr_t res = XMapper::reserve(addr, size);\n-\n-    assert(res == addr || res == 0, \"Should not reserve other memory than requested\");\n-    return res == addr;\n-  }\n-\n-  virtual void unreserve(uintptr_t addr, size_t size) {\n-    XMapper::unreserve(addr, size);\n-  }\n-};\n-\n-\/\/ Implements Large Pages (locked) support using shared AWE physical memory.\n-\n-\/\/ XPhysicalMemory layer needs access to the section\n-HANDLE XAWESection;\n-\n-class XVirtualMemoryManagerLargePages : public XVirtualMemoryManagerImpl {\n-private:\n-  virtual void initialize_before_reserve() {\n-    XAWESection = XMapper::create_shared_awe_section();\n-  }\n-\n-  virtual bool reserve(uintptr_t addr, size_t size) {\n-    const uintptr_t res = XMapper::reserve_for_shared_awe(XAWESection, addr, size);\n-\n-    assert(res == addr || res == 0, \"Should not reserve other memory than requested\");\n-    return res == addr;\n-  }\n-\n-  virtual void unreserve(uintptr_t addr, size_t size) {\n-    XMapper::unreserve_for_shared_awe(addr, size);\n-  }\n-};\n-\n-static XVirtualMemoryManagerImpl* _impl = nullptr;\n-\n-void XVirtualMemoryManager::pd_initialize_before_reserve() {\n-  if (XLargePages::is_enabled()) {\n-    _impl = new XVirtualMemoryManagerLargePages();\n-  } else {\n-    _impl = new XVirtualMemoryManagerSmallPages();\n-  }\n-  _impl->initialize_before_reserve();\n-}\n-\n-void XVirtualMemoryManager::pd_initialize_after_reserve() {\n-  _impl->initialize_after_reserve(&_manager);\n-}\n-\n-bool XVirtualMemoryManager::pd_reserve(uintptr_t addr, size_t size) {\n-  return _impl->reserve(addr, size);\n-}\n-\n-void XVirtualMemoryManager::pd_unreserve(uintptr_t addr, size_t size) {\n-  _impl->unreserve(addr, size);\n-}\n","filename":"src\/hotspot\/os\/windows\/gc\/x\/xVirtualMemory_windows.cpp","additions":0,"deletions":195,"binary":false,"changes":195,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_CPU_LINUX_AARCH64_GC_X_XSYSCALL_LINUX_AARCH64_HPP\n-#define OS_CPU_LINUX_AARCH64_GC_X_XSYSCALL_LINUX_AARCH64_HPP\n-\n-#include <sys\/syscall.h>\n-\n-\/\/\n-\/\/ Support for building on older Linux systems\n-\/\/\n-\n-#ifndef SYS_memfd_create\n-#define SYS_memfd_create     279\n-#endif\n-#ifndef SYS_fallocate\n-#define SYS_fallocate        47\n-#endif\n-\n-#endif \/\/ OS_CPU_LINUX_AARCH64_GC_X_XSYSCALL_LINUX_AARCH64_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/gc\/x\/xSyscall_linux_aarch64.hpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_CPU_LINUX_PPC_GC_X_XSYSCALL_LINUX_PPC_HPP\n-#define OS_CPU_LINUX_PPC_GC_X_XSYSCALL_LINUX_PPC_HPP\n-\n-#include <sys\/syscall.h>\n-\n-\/\/\n-\/\/ Support for building on older Linux systems\n-\/\/\n-\n-\n-#ifndef SYS_memfd_create\n-#define SYS_memfd_create     360\n-#endif\n-#ifndef SYS_fallocate\n-#define SYS_fallocate        309\n-#endif\n-\n-#endif \/\/ OS_CPU_LINUX_PPC_GC_X_XSYSCALL_LINUX_PPC_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/gc\/x\/xSyscall_linux_ppc.hpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef OS_CPU_LINUX_RISCV_GC_X_XSYSCALL_LINUX_RISCV_HPP\n-#define OS_CPU_LINUX_RISCV_GC_X_XSYSCALL_LINUX_RISCV_HPP\n-\n-#include <sys\/syscall.h>\n-\n-\/\/\n-\/\/ Support for building on older Linux systems\n-\/\/\n-\n-#ifndef SYS_memfd_create\n-#define SYS_memfd_create     279\n-#endif\n-#ifndef SYS_fallocate\n-#define SYS_fallocate        47\n-#endif\n-\n-#endif \/\/ OS_CPU_LINUX_RISCV_GC_X_XSYSCALL_LINUX_RISCV_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/gc\/x\/xSyscall_linux_riscv.hpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef OS_CPU_LINUX_X86_GC_X_XSYSCALL_LINUX_X86_HPP\n-#define OS_CPU_LINUX_X86_GC_X_XSYSCALL_LINUX_X86_HPP\n-\n-#include <sys\/syscall.h>\n-\n-\/\/\n-\/\/ Support for building on older Linux systems\n-\/\/\n-\n-#ifndef SYS_memfd_create\n-#define SYS_memfd_create     319\n-#endif\n-#ifndef SYS_fallocate\n-#define SYS_fallocate        285\n-#endif\n-\n-#endif \/\/ OS_CPU_LINUX_X86_GC_X_XSYSCALL_LINUX_X86_HPP\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/gc\/x\/xSyscall_linux_x86.hpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -36,1 +36,0 @@\n-  ZGC_ONLY(f(XBarrierSet))                           \\\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetConfig.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-#include \"gc\/x\/xBarrierSet.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetConfig.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-#include \"gc\/z\/shared\/zSharedArguments.hpp\"\n+#include \"gc\/z\/zArguments.hpp\"\n@@ -65,1 +65,1 @@\n-         ZGC_ONLY(static ZSharedArguments    zArguments;)\n+         ZGC_ONLY(static ZArguments          zArguments;)\n","filename":"src\/hotspot\/share\/gc\/shared\/gcConfig.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -46,5 +46,1 @@\n-    if (ZGenerational) {\n-      return ZMinor;\n-    } else {\n-      return NA;\n-    }\n+    return ZMinor;\n@@ -69,6 +65,2 @@\n-  if (UseZGC) {\n-    if (ZGenerational) {\n-      return ZMajor;\n-    } else {\n-      return Z;\n-    }\n+if (UseZGC) {\n+    return ZMajor;\n","filename":"src\/hotspot\/share\/gc\/shared\/gcConfiguration.cpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-#include \"gc\/z\/shared\/z_shared_globals.hpp\"\n+#include \"gc\/z\/z_globals.hpp\"\n@@ -96,1 +96,1 @@\n-  ZGC_ONLY(GC_Z_SHARED_FLAGS(                                               \\\n+  ZGC_ONLY(GC_Z_FLAGS(                                                      \\\n@@ -121,3 +121,0 @@\n-  product(bool, ZGenerational, true,                                        \\\n-          \"Use the generational version of ZGC\")                            \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"gc\/z\/shared\/vmStructs_z_shared.hpp\"\n+#include \"gc\/z\/vmStructs_z.hpp\"\n@@ -72,1 +72,1 @@\n-  ZGC_ONLY(VM_STRUCTS_Z_SHARED(nonstatic_field,                                                                                      \\\n+  ZGC_ONLY(VM_STRUCTS_Z(nonstatic_field,                                                                                             \\\n@@ -123,1 +123,1 @@\n-  ZGC_ONLY(VM_TYPES_Z_SHARED(declare_type,                                \\\n+  ZGC_ONLY(VM_TYPES_Z(declare_type,                                       \\\n@@ -177,1 +177,1 @@\n-  ZGC_ONLY(VM_INT_CONSTANTS_Z_SHARED(declare_constant,                      \\\n+  ZGC_ONLY(VM_INT_CONSTANTS_Z(declare_constant,                             \\\n@@ -201,1 +201,1 @@\n-  ZGC_ONLY(VM_LONG_CONSTANTS_Z_SHARED(declare_constant))\n+  ZGC_ONLY(VM_LONG_CONSTANTS_Z(declare_constant))\n","filename":"src\/hotspot\/share\/gc\/shared\/vmStructs_gc.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1,237 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_LIR.hpp\"\n-#include \"c1\/c1_LIRGenerator.hpp\"\n-#include \"c1\/c1_CodeStubs.hpp\"\n-#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-XLoadBarrierStubC1::XLoadBarrierStubC1(LIRAccess& access, LIR_Opr ref, address runtime_stub) :\n-    _decorators(access.decorators()),\n-    _ref_addr(access.resolved_addr()),\n-    _ref(ref),\n-    _tmp(LIR_OprFact::illegalOpr),\n-    _runtime_stub(runtime_stub) {\n-\n-  assert(_ref_addr->is_address(), \"Must be an address\");\n-  assert(_ref->is_register(), \"Must be a register\");\n-\n-  \/\/ Allocate tmp register if needed\n-  if (_ref_addr->as_address_ptr()->index()->is_valid() ||\n-      _ref_addr->as_address_ptr()->disp() != 0) {\n-    \/\/ Has index or displacement, need tmp register to load address into\n-    _tmp = access.gen()->new_pointer_register();\n-  }\n-\n-  FrameMap* f = Compilation::current()->frame_map();\n-  f->update_reserved_argument_area_size(2 * BytesPerWord);\n-}\n-\n-DecoratorSet XLoadBarrierStubC1::decorators() const {\n-  return _decorators;\n-}\n-\n-LIR_Opr XLoadBarrierStubC1::ref() const {\n-  return _ref;\n-}\n-\n-LIR_Opr XLoadBarrierStubC1::ref_addr() const {\n-  return _ref_addr;\n-}\n-\n-LIR_Opr XLoadBarrierStubC1::tmp() const {\n-  return _tmp;\n-}\n-\n-address XLoadBarrierStubC1::runtime_stub() const {\n-  return _runtime_stub;\n-}\n-\n-void XLoadBarrierStubC1::visit(LIR_OpVisitState* visitor) {\n-  visitor->do_slow_case();\n-  visitor->do_input(_ref_addr);\n-  visitor->do_output(_ref);\n-  if (_tmp->is_valid()) {\n-    visitor->do_temp(_tmp);\n-  }\n-}\n-\n-void XLoadBarrierStubC1::emit_code(LIR_Assembler* ce) {\n-  XBarrierSet::assembler()->generate_c1_load_barrier_stub(ce, this);\n-}\n-\n-#ifndef PRODUCT\n-void XLoadBarrierStubC1::print_name(outputStream* out) const {\n-  out->print(\"XLoadBarrierStubC1\");\n-}\n-#endif \/\/ PRODUCT\n-\n-class LIR_OpXLoadBarrierTest : public LIR_Op {\n-private:\n-  LIR_Opr _opr;\n-\n-public:\n-  LIR_OpXLoadBarrierTest(LIR_Opr opr) :\n-      LIR_Op(lir_xloadbarrier_test, LIR_OprFact::illegalOpr, nullptr),\n-      _opr(opr) {}\n-\n-  virtual void visit(LIR_OpVisitState* state) {\n-    state->do_input(_opr);\n-  }\n-\n-  virtual void emit_code(LIR_Assembler* ce) {\n-    XBarrierSet::assembler()->generate_c1_load_barrier_test(ce, _opr);\n-  }\n-\n-  virtual void print_instr(outputStream* out) const {\n-    _opr->print(out);\n-    out->print(\" \");\n-  }\n-\n-#ifndef PRODUCT\n-  virtual const char* name() const {\n-    return \"lir_z_load_barrier_test\";\n-  }\n-#endif \/\/ PRODUCT\n-};\n-\n-static bool barrier_needed(LIRAccess& access) {\n-  return XBarrierSet::barrier_needed(access.decorators(), access.type());\n-}\n-\n-XBarrierSetC1::XBarrierSetC1() :\n-    _load_barrier_on_oop_field_preloaded_runtime_stub(nullptr),\n-    _load_barrier_on_weak_oop_field_preloaded_runtime_stub(nullptr) {}\n-\n-address XBarrierSetC1::load_barrier_on_oop_field_preloaded_runtime_stub(DecoratorSet decorators) const {\n-  assert((decorators & ON_PHANTOM_OOP_REF) == 0, \"Unsupported decorator\");\n-  \/\/assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"Unsupported decorator\");\n-\n-  if ((decorators & ON_WEAK_OOP_REF) != 0) {\n-    return _load_barrier_on_weak_oop_field_preloaded_runtime_stub;\n-  } else {\n-    return _load_barrier_on_oop_field_preloaded_runtime_stub;\n-  }\n-}\n-\n-#ifdef ASSERT\n-#define __ access.gen()->lir(__FILE__, __LINE__)->\n-#else\n-#define __ access.gen()->lir()->\n-#endif\n-\n-void XBarrierSetC1::load_barrier(LIRAccess& access, LIR_Opr result) const {\n-  \/\/ Fast path\n-  __ append(new LIR_OpXLoadBarrierTest(result));\n-\n-  \/\/ Slow path\n-  const address runtime_stub = load_barrier_on_oop_field_preloaded_runtime_stub(access.decorators());\n-  CodeStub* const stub = new XLoadBarrierStubC1(access, result, runtime_stub);\n-  __ branch(lir_cond_notEqual, stub);\n-  __ branch_destination(stub->continuation());\n-}\n-\n-LIR_Opr XBarrierSetC1::resolve_address(LIRAccess& access, bool resolve_in_register) {\n-  \/\/ We must resolve in register when patching. This is to avoid\n-  \/\/ having a patch area in the load barrier stub, since the call\n-  \/\/ into the runtime to patch will not have the proper oop map.\n-  const bool patch_before_barrier = barrier_needed(access) && (access.decorators() & C1_NEEDS_PATCHING) != 0;\n-  return BarrierSetC1::resolve_address(access, resolve_in_register || patch_before_barrier);\n-}\n-\n-#undef __\n-\n-void XBarrierSetC1::load_at_resolved(LIRAccess& access, LIR_Opr result) {\n-  BarrierSetC1::load_at_resolved(access, result);\n-\n-  if (barrier_needed(access)) {\n-    load_barrier(access, result);\n-  }\n-}\n-\n-static void pre_load_barrier(LIRAccess& access) {\n-  DecoratorSet decorators = access.decorators();\n-\n-  \/\/ Downgrade access to MO_UNORDERED\n-  decorators = (decorators & ~MO_DECORATOR_MASK) | MO_UNORDERED;\n-\n-  \/\/ Remove ACCESS_WRITE\n-  decorators = (decorators & ~ACCESS_WRITE);\n-\n-  \/\/ Generate synthetic load at\n-  access.gen()->access_load_at(decorators,\n-                               access.type(),\n-                               access.base().item(),\n-                               access.offset().opr(),\n-                               access.gen()->new_register(access.type()),\n-                               nullptr \/* patch_emit_info *\/,\n-                               nullptr \/* load_emit_info *\/);\n-}\n-\n-LIR_Opr XBarrierSetC1::atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value) {\n-  if (barrier_needed(access)) {\n-    pre_load_barrier(access);\n-  }\n-\n-  return BarrierSetC1::atomic_xchg_at_resolved(access, value);\n-}\n-\n-LIR_Opr XBarrierSetC1::atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value) {\n-  if (barrier_needed(access)) {\n-    pre_load_barrier(access);\n-  }\n-\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n-}\n-\n-class XLoadBarrierRuntimeStubCodeGenClosure : public StubAssemblerCodeGenClosure {\n-private:\n-  const DecoratorSet _decorators;\n-\n-public:\n-  XLoadBarrierRuntimeStubCodeGenClosure(DecoratorSet decorators) :\n-      _decorators(decorators) {}\n-\n-  virtual OopMapSet* generate_code(StubAssembler* sasm) {\n-    XBarrierSet::assembler()->generate_c1_load_barrier_runtime_stub(sasm, _decorators);\n-    return nullptr;\n-  }\n-};\n-\n-static address generate_c1_runtime_stub(BufferBlob* blob, DecoratorSet decorators, const char* name) {\n-  XLoadBarrierRuntimeStubCodeGenClosure cl(decorators);\n-  CodeBlob* const code_blob = Runtime1::generate_blob(blob, C1StubId::NO_STUBID \/* stub_id *\/, name, false \/* expect_oop_map*\/, &cl);\n-  return code_blob->code_begin();\n-}\n-\n-void XBarrierSetC1::generate_c1_runtime_stubs(BufferBlob* blob) {\n-  _load_barrier_on_oop_field_preloaded_runtime_stub =\n-    generate_c1_runtime_stub(blob, ON_STRONG_OOP_REF, \"load_barrier_on_oop_field_preloaded_runtime_stub\");\n-  _load_barrier_on_weak_oop_field_preloaded_runtime_stub =\n-    generate_c1_runtime_stub(blob, ON_WEAK_OOP_REF, \"load_barrier_on_weak_oop_field_preloaded_runtime_stub\");\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/c1\/xBarrierSetC1.cpp","additions":0,"deletions":237,"binary":false,"changes":237,"status":"deleted"},{"patch":"@@ -1,78 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_C1_XBARRIERSETC1_HPP\n-#define SHARE_GC_X_C1_XBARRIERSETC1_HPP\n-\n-#include \"c1\/c1_CodeStubs.hpp\"\n-#include \"c1\/c1_IR.hpp\"\n-#include \"c1\/c1_LIR.hpp\"\n-#include \"gc\/shared\/c1\/barrierSetC1.hpp\"\n-#include \"oops\/accessDecorators.hpp\"\n-\n-class XLoadBarrierStubC1 : public CodeStub {\n-private:\n-  DecoratorSet _decorators;\n-  LIR_Opr      _ref_addr;\n-  LIR_Opr      _ref;\n-  LIR_Opr      _tmp;\n-  address      _runtime_stub;\n-\n-public:\n-  XLoadBarrierStubC1(LIRAccess& access, LIR_Opr ref, address runtime_stub);\n-\n-  DecoratorSet decorators() const;\n-  LIR_Opr ref() const;\n-  LIR_Opr ref_addr() const;\n-  LIR_Opr tmp() const;\n-  address runtime_stub() const;\n-\n-  virtual void emit_code(LIR_Assembler* ce);\n-  virtual void visit(LIR_OpVisitState* visitor);\n-\n-#ifndef PRODUCT\n-  virtual void print_name(outputStream* out) const;\n-#endif \/\/ PRODUCT\n-};\n-\n-class XBarrierSetC1 : public BarrierSetC1 {\n-private:\n-  address _load_barrier_on_oop_field_preloaded_runtime_stub;\n-  address _load_barrier_on_weak_oop_field_preloaded_runtime_stub;\n-\n-  address load_barrier_on_oop_field_preloaded_runtime_stub(DecoratorSet decorators) const;\n-  void load_barrier(LIRAccess& access, LIR_Opr result) const;\n-\n-protected:\n-  virtual LIR_Opr resolve_address(LIRAccess& access, bool resolve_in_register);\n-  virtual void load_at_resolved(LIRAccess& access, LIR_Opr result);\n-  virtual LIR_Opr atomic_xchg_at_resolved(LIRAccess& access, LIRItem& value);\n-  virtual LIR_Opr atomic_cmpxchg_at_resolved(LIRAccess& access, LIRItem& cmp_value, LIRItem& new_value);\n-\n-public:\n-  XBarrierSetC1();\n-\n-  virtual void generate_c1_runtime_stubs(BufferBlob* blob);\n-};\n-\n-#endif \/\/ SHARE_GC_X_C1_XBARRIERSETC1_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/c1\/xBarrierSetC1.hpp","additions":0,"deletions":78,"binary":false,"changes":78,"status":"deleted"},{"patch":"@@ -1,583 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/javaClasses.hpp\"\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"opto\/arraycopynode.hpp\"\n-#include \"opto\/addnode.hpp\"\n-#include \"opto\/block.hpp\"\n-#include \"opto\/compile.hpp\"\n-#include \"opto\/graphKit.hpp\"\n-#include \"opto\/machnode.hpp\"\n-#include \"opto\/macro.hpp\"\n-#include \"opto\/memnode.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"opto\/output.hpp\"\n-#include \"opto\/regalloc.hpp\"\n-#include \"opto\/rootnode.hpp\"\n-#include \"opto\/runtime.hpp\"\n-#include \"opto\/type.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class XBarrierSetC2State : public ArenaObj {\n-private:\n-  GrowableArray<XLoadBarrierStubC2*>* _stubs;\n-  Node_Array                          _live;\n-\n-public:\n-  XBarrierSetC2State(Arena* arena) :\n-    _stubs(new (arena) GrowableArray<XLoadBarrierStubC2*>(arena, 8,  0, nullptr)),\n-    _live(arena) {}\n-\n-  GrowableArray<XLoadBarrierStubC2*>* stubs() {\n-    return _stubs;\n-  }\n-\n-  RegMask* live(const Node* node) {\n-    if (!node->is_Mach()) {\n-      \/\/ Don't need liveness for non-MachNodes\n-      return nullptr;\n-    }\n-\n-    const MachNode* const mach = node->as_Mach();\n-    if (mach->barrier_data() == XLoadBarrierElided) {\n-      \/\/ Don't need liveness data for nodes without barriers\n-      return nullptr;\n-    }\n-\n-    RegMask* live = (RegMask*)_live[node->_idx];\n-    if (live == nullptr) {\n-      live = new (Compile::current()->comp_arena()->AmallocWords(sizeof(RegMask))) RegMask();\n-      _live.map(node->_idx, (Node*)live);\n-    }\n-\n-    return live;\n-  }\n-};\n-\n-static XBarrierSetC2State* barrier_set_state() {\n-  return reinterpret_cast<XBarrierSetC2State*>(Compile::current()->barrier_set_state());\n-}\n-\n-XLoadBarrierStubC2* XLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  XLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) XLoadBarrierStubC2(node, ref_addr, ref, tmp, barrier_data);\n-  if (!Compile::current()->output()->in_scratch_emit_size()) {\n-    barrier_set_state()->stubs()->append(stub);\n-  }\n-\n-  return stub;\n-}\n-\n-XLoadBarrierStubC2::XLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) :\n-    _node(node),\n-    _ref_addr(ref_addr),\n-    _ref(ref),\n-    _tmp(tmp),\n-    _barrier_data(barrier_data),\n-    _entry(),\n-    _continuation() {\n-  assert_different_registers(ref, ref_addr.base());\n-  assert_different_registers(ref, ref_addr.index());\n-}\n-\n-Address XLoadBarrierStubC2::ref_addr() const {\n-  return _ref_addr;\n-}\n-\n-Register XLoadBarrierStubC2::ref() const {\n-  return _ref;\n-}\n-\n-Register XLoadBarrierStubC2::tmp() const {\n-  return _tmp;\n-}\n-\n-address XLoadBarrierStubC2::slow_path() const {\n-  DecoratorSet decorators = DECORATORS_NONE;\n-  if (_barrier_data & XLoadBarrierStrong) {\n-    decorators |= ON_STRONG_OOP_REF;\n-  }\n-  if (_barrier_data & XLoadBarrierWeak) {\n-    decorators |= ON_WEAK_OOP_REF;\n-  }\n-  if (_barrier_data & XLoadBarrierPhantom) {\n-    decorators |= ON_PHANTOM_OOP_REF;\n-  }\n-  if (_barrier_data & XLoadBarrierNoKeepalive) {\n-    decorators |= AS_NO_KEEPALIVE;\n-  }\n-  return XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators);\n-}\n-\n-RegMask& XLoadBarrierStubC2::live() const {\n-  RegMask* mask = barrier_set_state()->live(_node);\n-  assert(mask != nullptr, \"must be mach-node with barrier\");\n-  return *mask;\n-}\n-\n-Label* XLoadBarrierStubC2::entry() {\n-  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n-  \/\/ However, we still need to return a label that is not bound now, but\n-  \/\/ will eventually be bound. Any label will do, as it will only act as\n-  \/\/ a placeholder, so we return the _continuation label.\n-  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n-}\n-\n-Label* XLoadBarrierStubC2::continuation() {\n-  return &_continuation;\n-}\n-\n-void* XBarrierSetC2::create_barrier_state(Arena* comp_arena) const {\n-  return new (comp_arena) XBarrierSetC2State(comp_arena);\n-}\n-\n-void XBarrierSetC2::late_barrier_analysis() const {\n-  analyze_dominating_barriers();\n-  compute_liveness_at_stubs();\n-}\n-\n-void XBarrierSetC2::emit_stubs(CodeBuffer& cb) const {\n-  MacroAssembler masm(&cb);\n-  GrowableArray<XLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n-\n-  for (int i = 0; i < stubs->length(); i++) {\n-    \/\/ Make sure there is enough space in the code buffer\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    }\n-\n-    XBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n-  }\n-\n-  masm.flush();\n-}\n-\n-int XBarrierSetC2::estimate_stub_size() const {\n-  Compile* const C = Compile::current();\n-  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n-  GrowableArray<XLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n-  int size = 0;\n-\n-  for (int i = 0; i < stubs->length(); i++) {\n-    CodeBuffer cb(blob->content_begin(), (address)C->output()->scratch_locs_memory() - blob->content_begin());\n-    MacroAssembler masm(&cb);\n-    XBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n-    size += cb.insts_size();\n-  }\n-\n-  return size;\n-}\n-\n-static void set_barrier_data(C2Access& access) {\n-  if (XBarrierSet::barrier_needed(access.decorators(), access.type())) {\n-    uint8_t barrier_data = 0;\n-\n-    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n-      barrier_data |= XLoadBarrierPhantom;\n-    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n-      barrier_data |= XLoadBarrierWeak;\n-    } else {\n-      barrier_data |= XLoadBarrierStrong;\n-    }\n-\n-    if (access.decorators() & AS_NO_KEEPALIVE) {\n-      barrier_data |= XLoadBarrierNoKeepalive;\n-    }\n-\n-    access.set_barrier_data(barrier_data);\n-  }\n-}\n-\n-Node* XBarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n-  set_barrier_data(access);\n-  return BarrierSetC2::load_at_resolved(access, val_type);\n-}\n-\n-Node* XBarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n-                                                    Node* new_val, const Type* val_type) const {\n-  set_barrier_data(access);\n-  return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, val_type);\n-}\n-\n-Node* XBarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n-                                                     Node* new_val, const Type* value_type) const {\n-  set_barrier_data(access);\n-  return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n-}\n-\n-Node* XBarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* val_type) const {\n-  set_barrier_data(access);\n-  return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, val_type);\n-}\n-\n-bool XBarrierSetC2::array_copy_requires_gc_barriers(bool tightly_coupled_alloc, BasicType type,\n-                                                    bool is_clone, bool is_clone_instance,\n-                                                    ArrayCopyPhase phase) const {\n-  if (phase == ArrayCopyPhase::Parsing) {\n-    return false;\n-  }\n-  if (phase == ArrayCopyPhase::Optimization) {\n-    return is_clone_instance;\n-  }\n-  \/\/ else ArrayCopyPhase::Expansion\n-  return type == T_OBJECT || type == T_ARRAY;\n-}\n-\n-\/\/ This TypeFunc assumes a 64bit system\n-static const TypeFunc* clone_type() {\n-  \/\/ Create input type (domain)\n-  const Type** domain_fields = TypeTuple::fields(4);\n-  domain_fields[TypeFunc::Parms + 0] = TypeInstPtr::NOTNULL;  \/\/ src\n-  domain_fields[TypeFunc::Parms + 1] = TypeInstPtr::NOTNULL;  \/\/ dst\n-  domain_fields[TypeFunc::Parms + 2] = TypeLong::LONG;        \/\/ size lower\n-  domain_fields[TypeFunc::Parms + 3] = Type::HALF;            \/\/ size upper\n-  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n-\n-  \/\/ Create result type (range)\n-  const Type** range_fields = TypeTuple::fields(0);\n-  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n-\n-  return TypeFunc::make(domain, range);\n-}\n-\n-#define XTOP LP64_ONLY(COMMA phase->top())\n-\n-void XBarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {\n-  Node* const src = ac->in(ArrayCopyNode::Src);\n-  const TypeAryPtr* ary_ptr = src->get_ptr_type()->isa_aryptr();\n-\n-  if (ac->is_clone_array() && ary_ptr != nullptr) {\n-    BasicType bt = ary_ptr->elem()->array_element_basic_type();\n-    if (is_reference_type(bt)) {\n-      \/\/ Clone object array\n-      bt = T_OBJECT;\n-    } else {\n-      \/\/ Clone primitive array\n-      bt = T_LONG;\n-    }\n-\n-    Node* ctrl = ac->in(TypeFunc::Control);\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    Node* src = ac->in(ArrayCopyNode::Src);\n-    Node* src_offset = ac->in(ArrayCopyNode::SrcPos);\n-    Node* dest = ac->in(ArrayCopyNode::Dest);\n-    Node* dest_offset = ac->in(ArrayCopyNode::DestPos);\n-    Node* length = ac->in(ArrayCopyNode::Length);\n-\n-    if (bt == T_OBJECT) {\n-      \/\/ BarrierSetC2::clone sets the offsets via BarrierSetC2::arraycopy_payload_base_offset\n-      \/\/ which 8-byte aligns them to allow for word size copies. Make sure the offsets point\n-      \/\/ to the first element in the array when cloning object arrays. Otherwise, load\n-      \/\/ barriers are applied to parts of the header. Also adjust the length accordingly.\n-      assert(src_offset == dest_offset, \"should be equal\");\n-      jlong offset = src_offset->get_long();\n-      if (offset != arrayOopDesc::base_offset_in_bytes(T_OBJECT)) {\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n-        assert((arrayOopDesc::base_offset_in_bytes(T_OBJECT) - offset) == BytesPerLong, \"unexpected offset\");\n-        length = phase->transform_later(new SubLNode(length, phase->longcon(1))); \/\/ Size is in longs\n-        src_offset = phase->longcon(arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n-        dest_offset = src_offset;\n-      }\n-    }\n-    Node* payload_src = phase->basic_plus_adr(src, src_offset);\n-    Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);\n-\n-    const char* copyfunc_name = \"arraycopy\";\n-    address     copyfunc_addr = phase->basictype2arraycopy(bt, nullptr, nullptr, true, copyfunc_name, true);\n-\n-    const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n-\n-    Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n-    phase->transform_later(call);\n-\n-    phase->igvn().replace_node(ac, call);\n-    return;\n-  }\n-\n-  \/\/ Clone instance\n-  Node* const ctrl       = ac->in(TypeFunc::Control);\n-  Node* const mem        = ac->in(TypeFunc::Memory);\n-  Node* const dst        = ac->in(ArrayCopyNode::Dest);\n-  Node* const size       = ac->in(ArrayCopyNode::Length);\n-\n-  assert(size->bottom_type()->is_long(), \"Should be long\");\n-\n-  \/\/ The native clone we are calling here expects the instance size in words\n-  \/\/ Add header\/offset size to payload size to get instance size.\n-  Node* const base_offset = phase->longcon(arraycopy_payload_base_offset(ac->is_clone_array()) >> LogBytesPerLong);\n-  Node* const full_size = phase->transform_later(new AddLNode(size, base_offset));\n-\n-  Node* const call = phase->make_leaf_call(ctrl,\n-                                           mem,\n-                                           clone_type(),\n-                                           XBarrierSetRuntime::clone_addr(),\n-                                           \"XBarrierSetRuntime::clone\",\n-                                           TypeRawPtr::BOTTOM,\n-                                           src,\n-                                           dst,\n-                                           full_size,\n-                                           phase->top());\n-  phase->transform_later(call);\n-  phase->igvn().replace_node(ac, call);\n-}\n-\n-#undef XTOP\n-\n-\/\/ == Dominating barrier elision ==\n-\n-static bool block_has_safepoint(const Block* block, uint from, uint to) {\n-  for (uint i = from; i < to; i++) {\n-    if (block->get_node(i)->is_MachSafePoint()) {\n-      \/\/ Safepoint found\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Safepoint not found\n-  return false;\n-}\n-\n-static bool block_has_safepoint(const Block* block) {\n-  return block_has_safepoint(block, 0, block->number_of_nodes());\n-}\n-\n-static uint block_index(const Block* block, const Node* node) {\n-  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-    if (block->get_node(j) == node) {\n-      return j;\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return 0;\n-}\n-\n-void XBarrierSetC2::analyze_dominating_barriers() const {\n-  ResourceMark rm;\n-  Compile* const C = Compile::current();\n-  PhaseCFG* const cfg = C->cfg();\n-  Block_List worklist;\n-  Node_List mem_ops;\n-  Node_List barrier_loads;\n-\n-  \/\/ Step 1 - Find accesses, and track them in lists\n-  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n-    const Block* const block = cfg->get_block(i);\n-    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-      const Node* const node = block->get_node(j);\n-      if (!node->is_Mach()) {\n-        continue;\n-      }\n-\n-      MachNode* const mach = node->as_Mach();\n-      switch (mach->ideal_Opcode()) {\n-      case Op_LoadP:\n-        if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-        if ((mach->barrier_data() & (XLoadBarrierStrong | XLoadBarrierNoKeepalive)) ==\n-            XLoadBarrierStrong) {\n-          mem_ops.push(mach);\n-        }\n-        break;\n-      case Op_CompareAndExchangeP:\n-      case Op_CompareAndSwapP:\n-      case Op_GetAndSetP:\n-        if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-      case Op_StoreP:\n-        mem_ops.push(mach);\n-        break;\n-\n-      default:\n-        break;\n-      }\n-    }\n-  }\n-\n-  \/\/ Step 2 - Find dominating accesses for each load\n-  for (uint i = 0; i < barrier_loads.size(); i++) {\n-    MachNode* const load = barrier_loads.at(i)->as_Mach();\n-    const TypePtr* load_adr_type = nullptr;\n-    intptr_t load_offset = 0;\n-    const Node* const load_obj = load->get_base_and_disp(load_offset, load_adr_type);\n-    Block* const load_block = cfg->get_block_for_node(load);\n-    const uint load_index = block_index(load_block, load);\n-\n-    for (uint j = 0; j < mem_ops.size(); j++) {\n-      MachNode* mem = mem_ops.at(j)->as_Mach();\n-      const TypePtr* mem_adr_type = nullptr;\n-      intptr_t mem_offset = 0;\n-      const Node* mem_obj = mem->get_base_and_disp(mem_offset, mem_adr_type);\n-      Block* mem_block = cfg->get_block_for_node(mem);\n-      uint mem_index = block_index(mem_block, mem);\n-\n-      if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||\n-          load_obj == nullptr || mem_obj == nullptr ||\n-          load_offset < 0 || mem_offset < 0) {\n-        continue;\n-      }\n-\n-      if (mem_obj != load_obj || mem_offset != load_offset) {\n-        \/\/ Not the same addresses, not a candidate\n-        continue;\n-      }\n-\n-      if (load_block == mem_block) {\n-        \/\/ Earlier accesses in the same block\n-        if (mem_index < load_index && !block_has_safepoint(mem_block, mem_index + 1, load_index)) {\n-          load->set_barrier_data(XLoadBarrierElided);\n-        }\n-      } else if (mem_block->dominates(load_block)) {\n-        \/\/ Dominating block? Look around for safepoints\n-        ResourceMark rm;\n-        Block_List stack;\n-        VectorSet visited;\n-        stack.push(load_block);\n-        bool safepoint_found = block_has_safepoint(load_block);\n-        while (!safepoint_found && stack.size() > 0) {\n-          Block* block = stack.pop();\n-          if (visited.test_set(block->_pre_order)) {\n-            continue;\n-          }\n-          if (block_has_safepoint(block)) {\n-            safepoint_found = true;\n-            break;\n-          }\n-          if (block == mem_block) {\n-            continue;\n-          }\n-\n-          \/\/ Push predecessor blocks\n-          for (uint p = 1; p < block->num_preds(); ++p) {\n-            Block* pred = cfg->get_block_for_node(block->pred(p));\n-            stack.push(pred);\n-          }\n-        }\n-\n-        if (!safepoint_found) {\n-          load->set_barrier_data(XLoadBarrierElided);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ == Reduced spilling optimization ==\n-\n-void XBarrierSetC2::compute_liveness_at_stubs() const {\n-  ResourceMark rm;\n-  Compile* const C = Compile::current();\n-  Arena* const A = Thread::current()->resource_area();\n-  PhaseCFG* const cfg = C->cfg();\n-  PhaseRegAlloc* const regalloc = C->regalloc();\n-  RegMask* const live = NEW_ARENA_ARRAY(A, RegMask, cfg->number_of_blocks() * sizeof(RegMask));\n-  XBarrierSetAssembler* const bs = XBarrierSet::assembler();\n-  Block_List worklist;\n-\n-  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n-    new ((void*)(live + i)) RegMask();\n-    worklist.push(cfg->get_block(i));\n-  }\n-\n-  while (worklist.size() > 0) {\n-    const Block* const block = worklist.pop();\n-    RegMask& old_live = live[block->_pre_order];\n-    RegMask new_live;\n-\n-    \/\/ Initialize to union of successors\n-    for (uint i = 0; i < block->_num_succs; i++) {\n-      const uint succ_id = block->_succs[i]->_pre_order;\n-      new_live.OR(live[succ_id]);\n-    }\n-\n-    \/\/ Walk block backwards, computing liveness\n-    for (int i = block->number_of_nodes() - 1; i >= 0; --i) {\n-      const Node* const node = block->get_node(i);\n-\n-      \/\/ Remove def bits\n-      const OptoReg::Name first = bs->refine_register(node, regalloc->get_reg_first(node));\n-      const OptoReg::Name second = bs->refine_register(node, regalloc->get_reg_second(node));\n-      if (first != OptoReg::Bad) {\n-        new_live.Remove(first);\n-      }\n-      if (second != OptoReg::Bad) {\n-        new_live.Remove(second);\n-      }\n-\n-      \/\/ Add use bits\n-      for (uint j = 1; j < node->req(); ++j) {\n-        const Node* const use = node->in(j);\n-        const OptoReg::Name first = bs->refine_register(use, regalloc->get_reg_first(use));\n-        const OptoReg::Name second = bs->refine_register(use, regalloc->get_reg_second(use));\n-        if (first != OptoReg::Bad) {\n-          new_live.Insert(first);\n-        }\n-        if (second != OptoReg::Bad) {\n-          new_live.Insert(second);\n-        }\n-      }\n-\n-      \/\/ If this node tracks liveness, update it\n-      RegMask* const regs = barrier_set_state()->live(node);\n-      if (regs != nullptr) {\n-        regs->OR(new_live);\n-      }\n-    }\n-\n-    \/\/ Now at block top, see if we have any changes\n-    new_live.SUBTRACT(old_live);\n-    if (new_live.is_NotEmpty()) {\n-      \/\/ Liveness has refined, update and propagate to prior blocks\n-      old_live.OR(new_live);\n-      for (uint i = 1; i < block->num_preds(); ++i) {\n-        Block* const pred = cfg->get_block_for_node(block->pred(i));\n-        worklist.push(pred);\n-      }\n-    }\n-  }\n-}\n-\n-#ifndef PRODUCT\n-void XBarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n-  if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n-    st->print(\"strong \");\n-  }\n-  if ((mach->barrier_data() & XLoadBarrierWeak) != 0) {\n-    st->print(\"weak \");\n-  }\n-  if ((mach->barrier_data() & XLoadBarrierPhantom) != 0) {\n-    st->print(\"phantom \");\n-  }\n-  if ((mach->barrier_data() & XLoadBarrierNoKeepalive) != 0) {\n-    st->print(\"nokeepalive \");\n-  }\n-}\n-#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":0,"deletions":583,"binary":false,"changes":583,"status":"deleted"},{"patch":"@@ -1,100 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_C2_XBARRIERSETC2_HPP\n-#define SHARE_GC_X_C2_XBARRIERSETC2_HPP\n-\n-#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"opto\/node.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-\n-const uint8_t XLoadBarrierElided      = 0;\n-const uint8_t XLoadBarrierStrong      = 1;\n-const uint8_t XLoadBarrierWeak        = 2;\n-const uint8_t XLoadBarrierPhantom     = 4;\n-const uint8_t XLoadBarrierNoKeepalive = 8;\n-\n-class XLoadBarrierStubC2 : public ArenaObj {\n-private:\n-  const MachNode* _node;\n-  const Address   _ref_addr;\n-  const Register  _ref;\n-  const Register  _tmp;\n-  const uint8_t   _barrier_data;\n-  Label           _entry;\n-  Label           _continuation;\n-\n-  XLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n-\n-public:\n-  static XLoadBarrierStubC2* create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n-\n-  Address ref_addr() const;\n-  Register ref() const;\n-  Register tmp() const;\n-  address slow_path() const;\n-  RegMask& live() const;\n-  Label* entry();\n-  Label* continuation();\n-};\n-\n-class XBarrierSetC2 : public BarrierSetC2 {\n-private:\n-  void compute_liveness_at_stubs() const;\n-  void analyze_dominating_barriers() const;\n-\n-protected:\n-  virtual Node* load_at_resolved(C2Access& access, const Type* val_type) const;\n-  virtual Node* atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access,\n-                                               Node* expected_val,\n-                                               Node* new_val,\n-                                               const Type* val_type) const;\n-  virtual Node* atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access,\n-                                                Node* expected_val,\n-                                                Node* new_val,\n-                                                const Type* value_type) const;\n-  virtual Node* atomic_xchg_at_resolved(C2AtomicParseAccess& access,\n-                                        Node* new_val,\n-                                        const Type* val_type) const;\n-\n-public:\n-  virtual void* create_barrier_state(Arena* comp_arena) const;\n-  virtual bool array_copy_requires_gc_barriers(bool tightly_coupled_alloc,\n-                                               BasicType type,\n-                                               bool is_clone,\n-                                               bool is_clone_instance,\n-                                               ArrayCopyPhase phase) const;\n-  virtual void clone_at_expansion(PhaseMacroExpand* phase,\n-                                  ArrayCopyNode* ac) const;\n-\n-  virtual void late_barrier_analysis() const;\n-  virtual int estimate_stub_size() const;\n-  virtual void emit_stubs(CodeBuffer& cb) const;\n-\n-#ifndef PRODUCT\n-  virtual void dump_barrier_data(const MachNode* mach, outputStream* st) const;\n-#endif\n-};\n-\n-#endif \/\/ SHARE_GC_X_C2_XBARRIERSETC2_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.hpp","additions":0,"deletions":100,"binary":false,"changes":100,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/vmStructs_x.hpp\"\n-\n-XGlobalsForVMStructs::XGlobalsForVMStructs() :\n-    _XGlobalPhase(&XGlobalPhase),\n-    _XGlobalSeqNum(&XGlobalSeqNum),\n-    _XAddressOffsetMask(&XAddressOffsetMask),\n-    _XAddressMetadataMask(&XAddressMetadataMask),\n-    _XAddressMetadataFinalizable(&XAddressMetadataFinalizable),\n-    _XAddressGoodMask(&XAddressGoodMask),\n-    _XAddressBadMask(&XAddressBadMask),\n-    _XAddressWeakBadMask(&XAddressWeakBadMask),\n-    _XObjectAlignmentSmallShift(&XObjectAlignmentSmallShift),\n-    _XObjectAlignmentSmall(&XObjectAlignmentSmall) {\n-}\n-\n-XGlobalsForVMStructs XGlobalsForVMStructs::_instance;\n-XGlobalsForVMStructs* XGlobalsForVMStructs::_instance_p = &XGlobalsForVMStructs::_instance;\n","filename":"src\/hotspot\/share\/gc\/x\/vmStructs_x.cpp","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,143 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_VMSTRUCTS_X_HPP\n-#define SHARE_GC_X_VMSTRUCTS_X_HPP\n-\n-#include \"gc\/x\/xAttachedArray.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xForwarding.hpp\"\n-#include \"gc\/x\/xGranuleMap.hpp\"\n-#include \"gc\/x\/xHeap.hpp\"\n-#include \"gc\/x\/xPageAllocator.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-\/\/ Expose some ZGC globals to the SA agent.\n-class XGlobalsForVMStructs {\n-  static XGlobalsForVMStructs _instance;\n-\n-public:\n-  static XGlobalsForVMStructs* _instance_p;\n-\n-  XGlobalsForVMStructs();\n-\n-  uint32_t* _XGlobalPhase;\n-\n-  uint32_t* _XGlobalSeqNum;\n-\n-  uintptr_t* _XAddressOffsetMask;\n-  uintptr_t* _XAddressMetadataMask;\n-  uintptr_t* _XAddressMetadataFinalizable;\n-  uintptr_t* _XAddressGoodMask;\n-  uintptr_t* _XAddressBadMask;\n-  uintptr_t* _XAddressWeakBadMask;\n-\n-  const int* _XObjectAlignmentSmallShift;\n-  const int* _XObjectAlignmentSmall;\n-};\n-\n-typedef XGranuleMap<XPage*> XGranuleMapForPageTable;\n-typedef XGranuleMap<XForwarding*> XGranuleMapForForwarding;\n-typedef XAttachedArray<XForwarding, XForwardingEntry> XAttachedArrayForForwarding;\n-\n-#define VM_STRUCTS_X(nonstatic_field, volatile_nonstatic_field, static_field)                            \\\n-  static_field(XGlobalsForVMStructs,            _instance_p,          XGlobalsForVMStructs*)             \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XGlobalPhase,        uint32_t*)                         \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XGlobalSeqNum,       uint32_t*)                         \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XAddressOffsetMask,  uintptr_t*)                        \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XAddressMetadataMask, uintptr_t*)                       \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XAddressMetadataFinalizable, uintptr_t*)                \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XAddressGoodMask,    uintptr_t*)                        \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XAddressBadMask,     uintptr_t*)                        \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XAddressWeakBadMask, uintptr_t*)                        \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XObjectAlignmentSmallShift, const int*)                 \\\n-  nonstatic_field(XGlobalsForVMStructs,         _XObjectAlignmentSmall, const int*)                      \\\n-                                                                                                         \\\n-  nonstatic_field(XCollectedHeap,               _heap,                XHeap)                             \\\n-                                                                                                         \\\n-  nonstatic_field(XHeap,                        _page_allocator,      XPageAllocator)                    \\\n-  nonstatic_field(XHeap,                        _page_table,          XPageTable)                        \\\n-  nonstatic_field(XHeap,                        _forwarding_table,    XForwardingTable)                  \\\n-  nonstatic_field(XHeap,                        _relocate,            XRelocate)                         \\\n-                                                                                                         \\\n-  nonstatic_field(XPage,                        _type,                const uint8_t)                     \\\n-  nonstatic_field(XPage,                        _seqnum,              uint32_t)                          \\\n-  nonstatic_field(XPage,                        _virtual,             const XVirtualMemory)              \\\n-  volatile_nonstatic_field(XPage,               _top,                 uintptr_t)                         \\\n-                                                                                                         \\\n-  nonstatic_field(XPageAllocator,               _max_capacity,        const size_t)                      \\\n-  volatile_nonstatic_field(XPageAllocator,      _capacity,            size_t)                            \\\n-  volatile_nonstatic_field(XPageAllocator,      _used,                size_t)                            \\\n-                                                                                                         \\\n-  nonstatic_field(XPageTable,                   _map,                 XGranuleMapForPageTable)           \\\n-                                                                                                         \\\n-  nonstatic_field(XGranuleMapForPageTable,      _map,                 XPage** const)                     \\\n-  nonstatic_field(XGranuleMapForForwarding,     _map,                 XForwarding** const)               \\\n-                                                                                                         \\\n-  nonstatic_field(XForwardingTable,             _map,                 XGranuleMapForForwarding)          \\\n-                                                                                                         \\\n-  nonstatic_field(XVirtualMemory,               _start,               const uintptr_t)                   \\\n-  nonstatic_field(XVirtualMemory,               _end,                 const uintptr_t)                   \\\n-                                                                                                         \\\n-  nonstatic_field(XForwarding,                  _virtual,             const XVirtualMemory)              \\\n-  nonstatic_field(XForwarding,                  _object_alignment_shift, const size_t)                   \\\n-  volatile_nonstatic_field(XForwarding,         _ref_count,           int)                               \\\n-  nonstatic_field(XForwarding,                  _entries,             const XAttachedArrayForForwarding) \\\n-  nonstatic_field(XForwardingEntry,             _entry,               uint64_t)                          \\\n-  nonstatic_field(XAttachedArrayForForwarding,  _length,              const size_t)\n-\n-#define VM_INT_CONSTANTS_X(declare_constant, declare_constant_with_value)                                \\\n-  declare_constant(XPhaseRelocate)                                                                       \\\n-  declare_constant(XPageTypeSmall)                                                                       \\\n-  declare_constant(XPageTypeMedium)                                                                      \\\n-  declare_constant(XPageTypeLarge)                                                                       \\\n-  declare_constant(XObjectAlignmentMediumShift)                                                          \\\n-  declare_constant(XObjectAlignmentLargeShift)\n-\n-#define VM_LONG_CONSTANTS_X(declare_constant)                                                            \\\n-  declare_constant(XGranuleSizeShift)                                                                    \\\n-  declare_constant(XPageSizeSmallShift)                                                                  \\\n-  declare_constant(XPageSizeMediumShift)                                                                 \\\n-  declare_constant(XAddressOffsetShift)                                                                  \\\n-  declare_constant(XAddressOffsetBits)                                                                   \\\n-  declare_constant(XAddressOffsetMask)                                                                   \\\n-  declare_constant(XAddressOffsetMax)\n-\n-#define VM_TYPES_X(declare_type, declare_toplevel_type, declare_integer_type)                            \\\n-  declare_toplevel_type(XGlobalsForVMStructs)                                                            \\\n-  declare_type(XCollectedHeap, CollectedHeap)                                                            \\\n-  declare_toplevel_type(XHeap)                                                                           \\\n-  declare_toplevel_type(XRelocate)                                                                       \\\n-  declare_toplevel_type(XPage)                                                                           \\\n-  declare_toplevel_type(XPageAllocator)                                                                  \\\n-  declare_toplevel_type(XPageTable)                                                                      \\\n-  declare_toplevel_type(XAttachedArrayForForwarding)                                                     \\\n-  declare_toplevel_type(XGranuleMapForPageTable)                                                         \\\n-  declare_toplevel_type(XGranuleMapForForwarding)                                                        \\\n-  declare_toplevel_type(XVirtualMemory)                                                                  \\\n-  declare_toplevel_type(XForwardingTable)                                                                \\\n-  declare_toplevel_type(XForwarding)                                                                     \\\n-  declare_toplevel_type(XForwardingEntry)                                                                \\\n-  declare_toplevel_type(XPhysicalMemoryManager)\n-\n-#endif \/\/ SHARE_GC_X_VMSTRUCTS_X_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/vmStructs_x.hpp","additions":0,"deletions":143,"binary":false,"changes":143,"status":"deleted"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAbort.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-\n-volatile bool XAbort::_should_abort = false;\n-\n-void XAbort::abort() {\n-  Atomic::release_store_fence(&_should_abort, true);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xAbort.cpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -1,38 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XABORT_HPP\n-#define SHARE_GC_X_XABORT_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class XAbort : public AllStatic {\n-private:\n-  static volatile bool _should_abort;\n-\n-public:\n-  static bool should_abort();\n-  static void abort();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XABORT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAbort.hpp","additions":0,"deletions":38,"binary":false,"changes":38,"status":"deleted"},{"patch":"@@ -1,35 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XABORT_INLINE_HPP\n-#define SHARE_GC_X_XABORT_INLINE_HPP\n-\n-#include \"gc\/x\/xAbort.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-\n-inline bool XAbort::should_abort() {\n-  return Atomic::load_acquire(&_should_abort);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XABORT_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAbort.inline.hpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"deleted"},{"patch":"@@ -1,58 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-\n-void XAddress::set_good_mask(uintptr_t mask) {\n-  XAddressGoodMask = mask;\n-  XAddressBadMask = XAddressGoodMask ^ XAddressMetadataMask;\n-  XAddressWeakBadMask = (XAddressGoodMask | XAddressMetadataRemapped | XAddressMetadataFinalizable) ^ XAddressMetadataMask;\n-}\n-\n-void XAddress::initialize() {\n-  XAddressOffsetBits = XPlatformAddressOffsetBits();\n-  XAddressOffsetMask = (((uintptr_t)1 << XAddressOffsetBits) - 1) << XAddressOffsetShift;\n-  XAddressOffsetMax = (uintptr_t)1 << XAddressOffsetBits;\n-\n-  XAddressMetadataShift = XPlatformAddressMetadataShift();\n-  XAddressMetadataMask = (((uintptr_t)1 << XAddressMetadataBits) - 1) << XAddressMetadataShift;\n-\n-  XAddressMetadataMarked0 = (uintptr_t)1 << (XAddressMetadataShift + 0);\n-  XAddressMetadataMarked1 = (uintptr_t)1 << (XAddressMetadataShift + 1);\n-  XAddressMetadataRemapped = (uintptr_t)1 << (XAddressMetadataShift + 2);\n-  XAddressMetadataFinalizable = (uintptr_t)1 << (XAddressMetadataShift + 3);\n-\n-  XAddressMetadataMarked = XAddressMetadataMarked0;\n-  set_good_mask(XAddressMetadataRemapped);\n-}\n-\n-void XAddress::flip_to_marked() {\n-  XAddressMetadataMarked ^= (XAddressMetadataMarked0 | XAddressMetadataMarked1);\n-  set_good_mask(XAddressMetadataMarked);\n-}\n-\n-void XAddress::flip_to_remapped() {\n-  set_good_mask(XAddressMetadataRemapped);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xAddress.cpp","additions":0,"deletions":58,"binary":false,"changes":58,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XADDRESS_HPP\n-#define SHARE_GC_X_XADDRESS_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XAddress : public AllStatic {\n-  friend class XAddressTest;\n-\n-private:\n-  static void set_good_mask(uintptr_t mask);\n-\n-public:\n-  static void initialize();\n-\n-  static void flip_to_marked();\n-  static void flip_to_remapped();\n-\n-  static bool is_null(uintptr_t value);\n-  static bool is_bad(uintptr_t value);\n-  static bool is_good(uintptr_t value);\n-  static bool is_good_or_null(uintptr_t value);\n-  static bool is_weak_bad(uintptr_t value);\n-  static bool is_weak_good(uintptr_t value);\n-  static bool is_weak_good_or_null(uintptr_t value);\n-  static bool is_marked(uintptr_t value);\n-  static bool is_marked_or_null(uintptr_t value);\n-  static bool is_finalizable(uintptr_t value);\n-  static bool is_finalizable_good(uintptr_t value);\n-  static bool is_remapped(uintptr_t value);\n-  static bool is_in(uintptr_t value);\n-\n-  static uintptr_t offset(uintptr_t value);\n-  static uintptr_t good(uintptr_t value);\n-  static uintptr_t good_or_null(uintptr_t value);\n-  static uintptr_t finalizable_good(uintptr_t value);\n-  static uintptr_t marked(uintptr_t value);\n-  static uintptr_t marked0(uintptr_t value);\n-  static uintptr_t marked1(uintptr_t value);\n-  static uintptr_t remapped(uintptr_t value);\n-  static uintptr_t remapped_or_null(uintptr_t value);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XADDRESS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAddress.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,137 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XADDRESS_INLINE_HPP\n-#define SHARE_GC_X_XADDRESS_INLINE_HPP\n-\n-#include \"gc\/x\/xAddress.hpp\"\n-\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-inline bool XAddress::is_null(uintptr_t value) {\n-  return value == 0;\n-}\n-\n-inline bool XAddress::is_bad(uintptr_t value) {\n-  return value & XAddressBadMask;\n-}\n-\n-inline bool XAddress::is_good(uintptr_t value) {\n-  return !is_bad(value) && !is_null(value);\n-}\n-\n-inline bool XAddress::is_good_or_null(uintptr_t value) {\n-  \/\/ Checking if an address is \"not bad\" is an optimized version of\n-  \/\/ checking if it's \"good or null\", which eliminates an explicit\n-  \/\/ null check. However, the implicit null check only checks that\n-  \/\/ the mask bits are zero, not that the entire address is zero.\n-  \/\/ This means that an address without mask bits would pass through\n-  \/\/ the barrier as if it was null. This should be harmless as such\n-  \/\/ addresses should ever be passed through the barrier.\n-  const bool result = !is_bad(value);\n-  assert((is_good(value) || is_null(value)) == result, \"Bad address\");\n-  return result;\n-}\n-\n-inline bool XAddress::is_weak_bad(uintptr_t value) {\n-  return value & XAddressWeakBadMask;\n-}\n-\n-inline bool XAddress::is_weak_good(uintptr_t value) {\n-  return !is_weak_bad(value) && !is_null(value);\n-}\n-\n-inline bool XAddress::is_weak_good_or_null(uintptr_t value) {\n-  return !is_weak_bad(value);\n-}\n-\n-inline bool XAddress::is_marked(uintptr_t value) {\n-  return value & XAddressMetadataMarked;\n-}\n-\n-inline bool XAddress::is_marked_or_null(uintptr_t value) {\n-  return is_marked(value) || is_null(value);\n-}\n-\n-inline bool XAddress::is_finalizable(uintptr_t value) {\n-  return value & XAddressMetadataFinalizable;\n-}\n-\n-inline bool XAddress::is_finalizable_good(uintptr_t value) {\n-  return is_finalizable(value) && is_good(value ^ XAddressMetadataFinalizable);\n-}\n-\n-inline bool XAddress::is_remapped(uintptr_t value) {\n-  return value & XAddressMetadataRemapped;\n-}\n-\n-inline bool XAddress::is_in(uintptr_t value) {\n-  \/\/ Check that exactly one non-offset bit is set\n-  if (!is_power_of_2(value & ~XAddressOffsetMask)) {\n-    return false;\n-  }\n-\n-  \/\/ Check that one of the non-finalizable metadata is set\n-  return value & (XAddressMetadataMask & ~XAddressMetadataFinalizable);\n-}\n-\n-inline uintptr_t XAddress::offset(uintptr_t value) {\n-  return value & XAddressOffsetMask;\n-}\n-\n-inline uintptr_t XAddress::good(uintptr_t value) {\n-  return offset(value) | XAddressGoodMask;\n-}\n-\n-inline uintptr_t XAddress::good_or_null(uintptr_t value) {\n-  return is_null(value) ? 0 : good(value);\n-}\n-\n-inline uintptr_t XAddress::finalizable_good(uintptr_t value) {\n-  return offset(value) | XAddressMetadataFinalizable | XAddressGoodMask;\n-}\n-\n-inline uintptr_t XAddress::marked(uintptr_t value) {\n-  return offset(value) | XAddressMetadataMarked;\n-}\n-\n-inline uintptr_t XAddress::marked0(uintptr_t value) {\n-  return offset(value) | XAddressMetadataMarked0;\n-}\n-\n-inline uintptr_t XAddress::marked1(uintptr_t value) {\n-  return offset(value) | XAddressMetadataMarked1;\n-}\n-\n-inline uintptr_t XAddress::remapped(uintptr_t value) {\n-  return offset(value) | XAddressMetadataRemapped;\n-}\n-\n-inline uintptr_t XAddress::remapped_or_null(uintptr_t value) {\n-  return is_null(value) ? 0 : remapped(value);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XADDRESS_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAddress.inline.hpp","additions":0,"deletions":137,"binary":false,"changes":137,"status":"deleted"},{"patch":"@@ -1,53 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-static size_t address_space_limit() {\n-  size_t limit = 0;\n-\n-  if (os::has_allocatable_memory_limit(&limit)) {\n-    return limit;\n-  }\n-\n-  \/\/ No limit\n-  return SIZE_MAX;\n-}\n-\n-size_t XAddressSpaceLimit::mark_stack() {\n-  \/\/ Allow mark stacks to occupy 10% of the address space\n-  const size_t limit = address_space_limit() \/ 10;\n-  return align_up(limit, XMarkStackSpaceExpandSize);\n-}\n-\n-size_t XAddressSpaceLimit::heap_view() {\n-  \/\/ Allow all heap views to occupy 50% of the address space\n-  const size_t limit = address_space_limit() \/ MaxVirtMemFraction \/ XHeapViews;\n-  return align_up(limit, XGranuleSize);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xAddressSpaceLimit.cpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"deleted"},{"patch":"@@ -1,36 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XADDRESSSPACELIMIT_HPP\n-#define SHARE_GC_X_XADDRESSSPACELIMIT_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XAddressSpaceLimit : public AllStatic {\n-public:\n-  static size_t mark_stack();\n-  static size_t heap_view();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XADDRESSSPACELIMIT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAddressSpaceLimit.hpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"deleted"},{"patch":"@@ -1,85 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XALLOCATIONFLAGS_HPP\n-#define SHARE_GC_X_XALLOCATIONFLAGS_HPP\n-\n-#include \"gc\/x\/xBitField.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-\/\/\n-\/\/ Allocation flags layout\n-\/\/ -----------------------\n-\/\/\n-\/\/   7     2 1 0\n-\/\/  +-----+-+-+-+\n-\/\/  |00000|1|1|1|\n-\/\/  +-----+-+-+-+\n-\/\/  |     | | |\n-\/\/  |     | | * 0-0 Non-Blocking Flag (1-bit)\n-\/\/  |     | |\n-\/\/  |     | * 1-1 Worker Relocation Flag (1-bit)\n-\/\/  |     |\n-\/\/  |     * 2-2 Low Address Flag (1-bit)\n-\/\/  |\n-\/\/  * 7-3 Unused (5-bits)\n-\/\/\n-\n-class XAllocationFlags {\n-private:\n-  typedef XBitField<uint8_t, bool, 0, 1> field_non_blocking;\n-  typedef XBitField<uint8_t, bool, 1, 1> field_worker_relocation;\n-  typedef XBitField<uint8_t, bool, 2, 1> field_low_address;\n-\n-  uint8_t _flags;\n-\n-public:\n-  XAllocationFlags() :\n-      _flags(0) {}\n-\n-  void set_non_blocking() {\n-    _flags |= field_non_blocking::encode(true);\n-  }\n-\n-  void set_worker_relocation() {\n-    _flags |= field_worker_relocation::encode(true);\n-  }\n-\n-  void set_low_address() {\n-    _flags |= field_low_address::encode(true);\n-  }\n-\n-  bool non_blocking() const {\n-    return field_non_blocking::decode(_flags);\n-  }\n-\n-  bool worker_relocation() const {\n-    return field_worker_relocation::decode(_flags);\n-  }\n-\n-  bool low_address() const {\n-    return field_low_address::decode(_flags);\n-  }\n-};\n-\n-#endif \/\/ SHARE_GC_X_XALLOCATIONFLAGS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAllocationFlags.hpp","additions":0,"deletions":85,"binary":false,"changes":85,"status":"deleted"},{"patch":"@@ -1,129 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n-#include \"gc\/x\/xArguments.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeuristics.hpp\"\n-#include \"gc\/shared\/gcArguments.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/java.hpp\"\n-\n-void XArguments::initialize_alignments() {\n-  SpaceAlignment = XGranuleSize;\n-  HeapAlignment = SpaceAlignment;\n-}\n-\n-void XArguments::initialize_heap_flags_and_sizes() {\n-  \/\/ Nothing extra to do\n-}\n-\n-void XArguments::initialize() {\n-  warning(\"Non-generational ZGC is deprecated.\");\n-\n-  \/\/ Check mark stack size\n-  const size_t mark_stack_space_limit = XAddressSpaceLimit::mark_stack();\n-  if (ZMarkStackSpaceLimit > mark_stack_space_limit) {\n-    if (!FLAG_IS_DEFAULT(ZMarkStackSpaceLimit)) {\n-      vm_exit_during_initialization(\"ZMarkStackSpaceLimit too large for limited address space\");\n-    }\n-    FLAG_SET_DEFAULT(ZMarkStackSpaceLimit, mark_stack_space_limit);\n-  }\n-\n-  \/\/ Enable NUMA by default\n-  if (FLAG_IS_DEFAULT(UseNUMA)) {\n-    FLAG_SET_DEFAULT(UseNUMA, true);\n-  }\n-\n-  if (FLAG_IS_DEFAULT(ZFragmentationLimit)) {\n-    FLAG_SET_DEFAULT(ZFragmentationLimit, 25.0);\n-  }\n-\n-  \/\/ Select number of parallel threads\n-  if (FLAG_IS_DEFAULT(ParallelGCThreads)) {\n-    FLAG_SET_DEFAULT(ParallelGCThreads, XHeuristics::nparallel_workers());\n-  }\n-\n-  if (ParallelGCThreads == 0) {\n-    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ParallelGCThreads=0\");\n-  }\n-\n-  \/\/ Select number of concurrent threads\n-  if (FLAG_IS_DEFAULT(ConcGCThreads)) {\n-    FLAG_SET_DEFAULT(ConcGCThreads, XHeuristics::nconcurrent_workers());\n-  }\n-\n-  if (ConcGCThreads == 0) {\n-    vm_exit_during_initialization(\"The flag -XX:+UseZGC can not be combined with -XX:ConcGCThreads=0\");\n-  }\n-\n-  \/\/ Large page size must match granule size\n-  if (!FLAG_IS_DEFAULT(LargePageSizeInBytes) && LargePageSizeInBytes != XGranuleSize) {\n-    vm_exit_during_initialization(err_msg(\"Incompatible -XX:LargePageSizeInBytes, only \"\n-                                          SIZE_FORMAT \"M large pages are supported by ZGC\",\n-                                          XGranuleSize \/ M));\n-  }\n-\n-  \/\/ The heuristics used when UseDynamicNumberOfGCThreads is\n-  \/\/ enabled defaults to using a ZAllocationSpikeTolerance of 1.\n-  if (UseDynamicNumberOfGCThreads && FLAG_IS_DEFAULT(ZAllocationSpikeTolerance)) {\n-    FLAG_SET_DEFAULT(ZAllocationSpikeTolerance, 1);\n-  }\n-\n-#ifdef COMPILER2\n-  \/\/ Enable loop strip mining by default\n-  if (FLAG_IS_DEFAULT(UseCountedLoopSafepoints)) {\n-    FLAG_SET_DEFAULT(UseCountedLoopSafepoints, true);\n-    if (FLAG_IS_DEFAULT(LoopStripMiningIter)) {\n-      FLAG_SET_DEFAULT(LoopStripMiningIter, 1000);\n-    }\n-  }\n-#endif\n-\n-  \/\/ CompressedOops not supported\n-  FLAG_SET_DEFAULT(UseCompressedOops, false);\n-\n-  \/\/ Verification before startup and after exit not (yet) supported\n-  FLAG_SET_DEFAULT(VerifyDuringStartup, false);\n-  FLAG_SET_DEFAULT(VerifyBeforeExit, false);\n-\n-  if (VerifyBeforeGC || VerifyDuringGC || VerifyAfterGC) {\n-    FLAG_SET_DEFAULT(ZVerifyRoots, true);\n-    FLAG_SET_DEFAULT(ZVerifyObjects, true);\n-  }\n-}\n-\n-size_t XArguments::heap_virtual_to_physical_ratio() {\n-  return XHeapViews * XVirtualToPhysicalRatio;\n-}\n-\n-CollectedHeap* XArguments::create_heap() {\n-  return new XCollectedHeap();\n-}\n-\n-bool XArguments::is_supported() {\n-  return is_os_supported();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xArguments.cpp","additions":0,"deletions":129,"binary":false,"changes":129,"status":"deleted"},{"patch":"@@ -1,44 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XARGUMENTS_HPP\n-#define SHARE_GC_X_XARGUMENTS_HPP\n-\n-#include \"gc\/shared\/gcArguments.hpp\"\n-\n-class CollectedHeap;\n-\n-class XArguments : AllStatic {\n-public:\n-  static void initialize_alignments();\n-  static void initialize_heap_flags_and_sizes();\n-  static void initialize();\n-  static size_t heap_virtual_to_physical_ratio();\n-  static CollectedHeap* create_heap();\n-\n-  static bool is_supported();\n-\n-  static bool is_os_supported();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XARGUMENTS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xArguments.hpp","additions":0,"deletions":44,"binary":false,"changes":44,"status":"deleted"},{"patch":"@@ -1,51 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XARRAY_HPP\n-#define SHARE_GC_X_XARRAY_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-\n-template <typename T> using XArray = GrowableArrayCHeap<T, mtGC>;\n-\n-template <typename T, bool Parallel>\n-class XArrayIteratorImpl : public StackObj {\n-private:\n-  const T*       _next;\n-  const T* const _end;\n-\n-  bool next_serial(T* elem);\n-  bool next_parallel(T* elem);\n-\n-public:\n-  XArrayIteratorImpl(const T* array, size_t length);\n-  XArrayIteratorImpl(const XArray<T>* array);\n-\n-  bool next(T* elem);\n-};\n-\n-template <typename T> using XArrayIterator = XArrayIteratorImpl<T, false \/* Parallel *\/>;\n-template <typename T> using XArrayParallelIterator = XArrayIteratorImpl<T, true \/* Parallel *\/>;\n-\n-#endif \/\/ SHARE_GC_X_XARRAY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xArray.hpp","additions":0,"deletions":51,"binary":false,"changes":51,"status":"deleted"},{"patch":"@@ -1,81 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XARRAY_INLINE_HPP\n-#define SHARE_GC_X_XARRAY_INLINE_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-\n-template <typename T, bool Parallel>\n-inline bool XArrayIteratorImpl<T, Parallel>::next_serial(T* elem) {\n-  if (_next == _end) {\n-    return false;\n-  }\n-\n-  *elem = *_next;\n-  _next++;\n-\n-  return true;\n-}\n-\n-template <typename T, bool Parallel>\n-inline bool XArrayIteratorImpl<T, Parallel>::next_parallel(T* elem) {\n-  const T* old_next = Atomic::load(&_next);\n-\n-  for (;;) {\n-    if (old_next == _end) {\n-      return false;\n-    }\n-\n-    const T* const new_next = old_next + 1;\n-    const T* const prev_next = Atomic::cmpxchg(&_next, old_next, new_next);\n-    if (prev_next == old_next) {\n-      *elem = *old_next;\n-      return true;\n-    }\n-\n-    old_next = prev_next;\n-  }\n-}\n-\n-template <typename T, bool Parallel>\n-inline XArrayIteratorImpl<T, Parallel>::XArrayIteratorImpl(const T* array, size_t length) :\n-    _next(array),\n-    _end(array + length) {}\n-\n-template <typename T, bool Parallel>\n-inline XArrayIteratorImpl<T, Parallel>::XArrayIteratorImpl(const XArray<T>* array) :\n-    XArrayIteratorImpl<T, Parallel>(array->is_empty() ? nullptr : array->adr_at(0), array->length()) {}\n-\n-template <typename T, bool Parallel>\n-inline bool XArrayIteratorImpl<T, Parallel>::next(T* elem) {\n-  if (Parallel) {\n-    return next_parallel(elem);\n-  } else {\n-    return next_serial(elem);\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XARRAY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xArray.inline.hpp","additions":0,"deletions":81,"binary":false,"changes":81,"status":"deleted"},{"patch":"@@ -1,54 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XATTACHEDARRAY_HPP\n-#define SHARE_GC_X_XATTACHEDARRAY_HPP\n-\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class VMStructs;\n-\n-template <typename ObjectT, typename ArrayT>\n-class XAttachedArray {\n-  friend class ::VMStructs;\n-\n-private:\n-  const size_t _length;\n-\n-  static size_t object_size();\n-  static size_t array_size(size_t length);\n-\n-public:\n-  template <typename Allocator>\n-  static void* alloc(Allocator* allocator, size_t length);\n-\n-  static void* alloc(size_t length);\n-  static void free(ObjectT* obj);\n-\n-  XAttachedArray(size_t length);\n-\n-  size_t length() const;\n-  ArrayT* operator()(const ObjectT* obj) const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XATTACHEDARRAY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAttachedArray.hpp","additions":0,"deletions":54,"binary":false,"changes":54,"status":"deleted"},{"patch":"@@ -1,86 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XATTACHEDARRAY_INLINE_HPP\n-#define SHARE_GC_X_XATTACHEDARRAY_INLINE_HPP\n-\n-#include \"gc\/x\/xAttachedArray.hpp\"\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-template <typename ObjectT, typename ArrayT>\n-inline size_t XAttachedArray<ObjectT, ArrayT>::object_size() {\n-  return align_up(sizeof(ObjectT), sizeof(ArrayT));\n-}\n-\n-template <typename ObjectT, typename ArrayT>\n-inline size_t XAttachedArray<ObjectT, ArrayT>::array_size(size_t length) {\n-  return sizeof(ArrayT) * length;\n-}\n-\n-template <typename ObjectT, typename ArrayT>\n-template <typename Allocator>\n-inline void* XAttachedArray<ObjectT, ArrayT>::alloc(Allocator* allocator, size_t length) {\n-  \/\/ Allocate memory for object and array\n-  const size_t size = object_size() + array_size(length);\n-  void* const addr = allocator->alloc(size);\n-\n-  \/\/ Placement new array\n-  void* const array_addr = reinterpret_cast<char*>(addr) + object_size();\n-  ::new (array_addr) ArrayT[length];\n-\n-  \/\/ Return pointer to object\n-  return addr;\n-}\n-\n-template <typename ObjectT, typename ArrayT>\n-inline void* XAttachedArray<ObjectT, ArrayT>::alloc(size_t length) {\n-  struct Allocator {\n-    void* alloc(size_t size) const {\n-      return AllocateHeap(size, mtGC);\n-    }\n-  } allocator;\n-  return alloc(&allocator, length);\n-}\n-\n-template <typename ObjectT, typename ArrayT>\n-inline void XAttachedArray<ObjectT, ArrayT>::free(ObjectT* obj) {\n-  FreeHeap(obj);\n-}\n-\n-template <typename ObjectT, typename ArrayT>\n-inline XAttachedArray<ObjectT, ArrayT>::XAttachedArray(size_t length) :\n-    _length(length) {}\n-\n-template <typename ObjectT, typename ArrayT>\n-inline size_t XAttachedArray<ObjectT, ArrayT>::length() const {\n-  return _length;\n-}\n-\n-template <typename ObjectT, typename ArrayT>\n-inline ArrayT* XAttachedArray<ObjectT, ArrayT>::operator()(const ObjectT* obj) const {\n-  return reinterpret_cast<ArrayT*>(reinterpret_cast<uintptr_t>(obj) + object_size());\n-}\n-\n-#endif \/\/ SHARE_GC_X_XATTACHEDARRAY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xAttachedArray.inline.hpp","additions":0,"deletions":86,"binary":false,"changes":86,"status":"deleted"},{"patch":"@@ -1,275 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/javaClasses.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-template <bool finalizable>\n-bool XBarrier::should_mark_through(uintptr_t addr) {\n-  \/\/ Finalizable marked oops can still exists on the heap after marking\n-  \/\/ has completed, in which case we just want to convert this into a\n-  \/\/ good oop and not push it on the mark stack.\n-  if (!during_mark()) {\n-    assert(XAddress::is_marked(addr), \"Should be marked\");\n-    assert(XAddress::is_finalizable(addr), \"Should be finalizable\");\n-    return false;\n-  }\n-\n-  \/\/ During marking, we mark through already marked oops to avoid having\n-  \/\/ some large part of the object graph hidden behind a pushed, but not\n-  \/\/ yet flushed, entry on a mutator mark stack. Always marking through\n-  \/\/ allows the GC workers to proceed through the object graph even if a\n-  \/\/ mutator touched an oop first, which in turn will reduce the risk of\n-  \/\/ having to flush mark stacks multiple times to terminate marking.\n-  \/\/\n-  \/\/ However, when doing finalizable marking we don't always want to mark\n-  \/\/ through. First, marking through an already strongly marked oop would\n-  \/\/ be wasteful, since we will then proceed to do finalizable marking on\n-  \/\/ an object which is, or will be, marked strongly. Second, marking\n-  \/\/ through an already finalizable marked oop would also be wasteful,\n-  \/\/ since such oops can never end up on a mutator mark stack and can\n-  \/\/ therefore not hide some part of the object graph from GC workers.\n-  if (finalizable) {\n-    return !XAddress::is_marked(addr);\n-  }\n-\n-  \/\/ Mark through\n-  return true;\n-}\n-\n-template <bool gc_thread, bool follow, bool finalizable, bool publish>\n-uintptr_t XBarrier::mark(uintptr_t addr) {\n-  uintptr_t good_addr;\n-\n-  if (XAddress::is_marked(addr)) {\n-    \/\/ Already marked, but try to mark though anyway\n-    good_addr = XAddress::good(addr);\n-  } else if (XAddress::is_remapped(addr)) {\n-    \/\/ Already remapped, but also needs to be marked\n-    good_addr = XAddress::good(addr);\n-  } else {\n-    \/\/ Needs to be both remapped and marked\n-    good_addr = remap(addr);\n-  }\n-\n-  \/\/ Mark\n-  if (should_mark_through<finalizable>(addr)) {\n-    XHeap::heap()->mark_object<gc_thread, follow, finalizable, publish>(good_addr);\n-  }\n-\n-  if (finalizable) {\n-    \/\/ Make the oop finalizable marked\/good, instead of normal marked\/good.\n-    \/\/ This is needed because an object might first becomes finalizable\n-    \/\/ marked by the GC, and then loaded by a mutator thread. In this case,\n-    \/\/ the mutator thread must be able to tell that the object needs to be\n-    \/\/ strongly marked. The finalizable bit in the oop exists to make sure\n-    \/\/ that a load of a finalizable marked oop will fall into the barrier\n-    \/\/ slow path so that we can mark the object as strongly reachable.\n-    return XAddress::finalizable_good(good_addr);\n-  }\n-\n-  return good_addr;\n-}\n-\n-uintptr_t XBarrier::remap(uintptr_t addr) {\n-  assert(!XAddress::is_good(addr), \"Should not be good\");\n-  assert(!XAddress::is_weak_good(addr), \"Should not be weak good\");\n-  return XHeap::heap()->remap_object(addr);\n-}\n-\n-uintptr_t XBarrier::relocate(uintptr_t addr) {\n-  assert(!XAddress::is_good(addr), \"Should not be good\");\n-  assert(!XAddress::is_weak_good(addr), \"Should not be weak good\");\n-  return XHeap::heap()->relocate_object(addr);\n-}\n-\n-uintptr_t XBarrier::relocate_or_mark(uintptr_t addr) {\n-  return during_relocate() ? relocate(addr) : mark<AnyThread, Follow, Strong, Publish>(addr);\n-}\n-\n-uintptr_t XBarrier::relocate_or_mark_no_follow(uintptr_t addr) {\n-  return during_relocate() ? relocate(addr) : mark<AnyThread, DontFollow, Strong, Publish>(addr);\n-}\n-\n-uintptr_t XBarrier::relocate_or_remap(uintptr_t addr) {\n-  return during_relocate() ? relocate(addr) : remap(addr);\n-}\n-\n-\/\/\n-\/\/ Load barrier\n-\/\/\n-uintptr_t XBarrier::load_barrier_on_oop_slow_path(uintptr_t addr) {\n-  return relocate_or_mark(addr);\n-}\n-\n-uintptr_t XBarrier::load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr) {\n-  return relocate_or_mark_no_follow(addr);\n-}\n-\n-void XBarrier::load_barrier_on_oop_fields(oop o) {\n-  assert(XAddress::is_good(XOop::to_address(o)), \"Should be good\");\n-  XLoadBarrierOopClosure cl;\n-  o->oop_iterate(&cl);\n-}\n-\n-\/\/\n-\/\/ Weak load barrier\n-\/\/\n-uintptr_t XBarrier::weak_load_barrier_on_oop_slow_path(uintptr_t addr) {\n-  return XAddress::is_weak_good(addr) ? XAddress::good(addr) : relocate_or_remap(addr);\n-}\n-\n-uintptr_t XBarrier::weak_load_barrier_on_weak_oop_slow_path(uintptr_t addr) {\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  if (XHeap::heap()->is_object_strongly_live(good_addr)) {\n-    return good_addr;\n-  }\n-\n-  \/\/ Not strongly live\n-  return 0;\n-}\n-\n-uintptr_t XBarrier::weak_load_barrier_on_phantom_oop_slow_path(uintptr_t addr) {\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  if (XHeap::heap()->is_object_live(good_addr)) {\n-    return good_addr;\n-  }\n-\n-  \/\/ Not live\n-  return 0;\n-}\n-\n-\/\/\n-\/\/ Keep alive barrier\n-\/\/\n-uintptr_t XBarrier::keep_alive_barrier_on_oop_slow_path(uintptr_t addr) {\n-  assert(during_mark(), \"Invalid phase\");\n-\n-  \/\/ Mark\n-  return mark<AnyThread, Follow, Strong, Overflow>(addr);\n-}\n-\n-uintptr_t XBarrier::keep_alive_barrier_on_weak_oop_slow_path(uintptr_t addr) {\n-  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  assert(XHeap::heap()->is_object_strongly_live(good_addr), \"Should be live\");\n-  return good_addr;\n-}\n-\n-uintptr_t XBarrier::keep_alive_barrier_on_phantom_oop_slow_path(uintptr_t addr) {\n-  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const uintptr_t good_addr = weak_load_barrier_on_oop_slow_path(addr);\n-  assert(XHeap::heap()->is_object_live(good_addr), \"Should be live\");\n-  return good_addr;\n-}\n-\n-\/\/\n-\/\/ Mark barrier\n-\/\/\n-uintptr_t XBarrier::mark_barrier_on_oop_slow_path(uintptr_t addr) {\n-  assert(during_mark(), \"Invalid phase\");\n-  assert(XThread::is_worker(), \"Invalid thread\");\n-\n-  \/\/ Mark\n-  return mark<GCThread, Follow, Strong, Overflow>(addr);\n-}\n-\n-uintptr_t XBarrier::mark_barrier_on_finalizable_oop_slow_path(uintptr_t addr) {\n-  assert(during_mark(), \"Invalid phase\");\n-  assert(XThread::is_worker(), \"Invalid thread\");\n-\n-  \/\/ Mark\n-  return mark<GCThread, Follow, Finalizable, Overflow>(addr);\n-}\n-\n-\/\/\n-\/\/ Narrow oop variants, never used.\n-\/\/\n-oop XBarrier::load_barrier_on_oop_field(volatile narrowOop* p) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-oop XBarrier::load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-void XBarrier::load_barrier_on_oop_array(volatile narrowOop* p, size_t length) {\n-  ShouldNotReachHere();\n-}\n-\n-oop XBarrier::load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-oop XBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-oop XBarrier::weak_load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-oop XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-oop XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-#ifdef ASSERT\n-\n-\/\/ ON_WEAK barriers should only ever be applied to j.l.r.Reference.referents.\n-void XBarrier::verify_on_weak(volatile oop* referent_addr) {\n-  if (referent_addr != nullptr) {\n-    uintptr_t base = (uintptr_t)referent_addr - java_lang_ref_Reference::referent_offset();\n-    oop obj = cast_to_oop(base);\n-    assert(oopDesc::is_oop(obj), \"Verification failed for: ref \" PTR_FORMAT \" obj: \" PTR_FORMAT, (uintptr_t)referent_addr, base);\n-    assert(java_lang_ref_Reference::is_referent_field(obj, java_lang_ref_Reference::referent_offset()), \"Sanity\");\n-  }\n-}\n-\n-#endif\n-\n-void XLoadBarrierOopClosure::do_oop(oop* p) {\n-  XBarrier::load_barrier_on_oop_field(p);\n-}\n-\n-void XLoadBarrierOopClosure::do_oop(narrowOop* p) {\n-  ShouldNotReachHere();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrier.cpp","additions":0,"deletions":275,"binary":false,"changes":275,"status":"deleted"},{"patch":"@@ -1,135 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIER_HPP\n-#define SHARE_GC_X_XBARRIER_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"oops\/oop.hpp\"\n-\n-typedef bool (*XBarrierFastPath)(uintptr_t);\n-typedef uintptr_t (*XBarrierSlowPath)(uintptr_t);\n-\n-class XBarrier : public AllStatic {\n-private:\n-  static const bool GCThread    = true;\n-  static const bool AnyThread   = false;\n-\n-  static const bool Follow      = true;\n-  static const bool DontFollow  = false;\n-\n-  static const bool Strong      = false;\n-  static const bool Finalizable = true;\n-\n-  static const bool Publish     = true;\n-  static const bool Overflow    = false;\n-\n-  template <XBarrierFastPath fast_path> static void self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr);\n-\n-  template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path> static oop barrier(volatile oop* p, oop o);\n-  template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path> static oop weak_barrier(volatile oop* p, oop o);\n-  template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path> static void root_barrier(oop* p, oop o);\n-\n-  static bool is_good_or_null_fast_path(uintptr_t addr);\n-  static bool is_weak_good_or_null_fast_path(uintptr_t addr);\n-  static bool is_marked_or_null_fast_path(uintptr_t addr);\n-\n-  static bool during_mark();\n-  static bool during_relocate();\n-  template <bool finalizable> static bool should_mark_through(uintptr_t addr);\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> static uintptr_t mark(uintptr_t addr);\n-  static uintptr_t remap(uintptr_t addr);\n-  static uintptr_t relocate(uintptr_t addr);\n-  static uintptr_t relocate_or_mark(uintptr_t addr);\n-  static uintptr_t relocate_or_mark_no_follow(uintptr_t addr);\n-  static uintptr_t relocate_or_remap(uintptr_t addr);\n-\n-  static uintptr_t load_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr);\n-\n-  static uintptr_t weak_load_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t weak_load_barrier_on_weak_oop_slow_path(uintptr_t addr);\n-  static uintptr_t weak_load_barrier_on_phantom_oop_slow_path(uintptr_t addr);\n-\n-  static uintptr_t keep_alive_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t keep_alive_barrier_on_weak_oop_slow_path(uintptr_t addr);\n-  static uintptr_t keep_alive_barrier_on_phantom_oop_slow_path(uintptr_t addr);\n-\n-  static uintptr_t mark_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t mark_barrier_on_finalizable_oop_slow_path(uintptr_t addr);\n-\n-  static void verify_on_weak(volatile oop* referent_addr) NOT_DEBUG_RETURN;\n-\n-public:\n-  \/\/ Load barrier\n-  static  oop load_barrier_on_oop(oop o);\n-  static  oop load_barrier_on_oop_field(volatile oop* p);\n-  static  oop load_barrier_on_oop_field_preloaded(volatile oop* p, oop o);\n-  static void load_barrier_on_oop_array(volatile oop* p, size_t length);\n-  static void load_barrier_on_oop_fields(oop o);\n-  static  oop load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o);\n-  static  oop load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o);\n-  static void load_barrier_on_root_oop_field(oop* p);\n-  static void load_barrier_on_invisible_root_oop_field(oop* p);\n-\n-  \/\/ Weak load barrier\n-  static oop weak_load_barrier_on_oop_field(volatile oop* p);\n-  static oop weak_load_barrier_on_oop_field_preloaded(volatile oop* p, oop o);\n-  static oop weak_load_barrier_on_weak_oop(oop o);\n-  static oop weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o);\n-  static oop weak_load_barrier_on_phantom_oop(oop o);\n-  static oop weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o);\n-\n-  \/\/ Is alive barrier\n-  static bool is_alive_barrier_on_weak_oop(oop o);\n-  static bool is_alive_barrier_on_phantom_oop(oop o);\n-\n-  \/\/ Keep alive barrier\n-  static void keep_alive_barrier_on_oop(oop o);\n-  static void keep_alive_barrier_on_weak_oop_field(volatile oop* p);\n-  static void keep_alive_barrier_on_phantom_oop_field(volatile oop* p);\n-  static void keep_alive_barrier_on_phantom_root_oop_field(oop* p);\n-\n-  \/\/ Mark barrier\n-  static void mark_barrier_on_oop_field(volatile oop* p, bool finalizable);\n-  static void mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable);\n-\n-  \/\/ Narrow oop variants, never used.\n-  static oop  load_barrier_on_oop_field(volatile narrowOop* p);\n-  static oop  load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static void load_barrier_on_oop_array(volatile narrowOop* p, size_t length);\n-  static oop  load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o);\n-};\n-\n-class XLoadBarrierOopClosure : public BasicOopIterateClosure {\n-public:\n-  virtual void do_oop(oop* p);\n-  virtual void do_oop(narrowOop* p);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBARRIER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrier.hpp","additions":0,"deletions":135,"binary":false,"changes":135,"status":"deleted"},{"patch":"@@ -1,394 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIER_INLINE_HPP\n-#define SHARE_GC_X_XBARRIER_INLINE_HPP\n-\n-#include \"gc\/x\/xBarrier.hpp\"\n-\n-#include \"code\/codeCache.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"gc\/x\/xResurrection.inline.hpp\"\n-#include \"oops\/oop.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/continuation.hpp\"\n-\n-\/\/ A self heal must always \"upgrade\" the address metadata bits in\n-\/\/ accordance with the metadata bits state machine, which has the\n-\/\/ valid state transitions as described below (where N is the GC\n-\/\/ cycle).\n-\/\/\n-\/\/ Note the subtleness of overlapping GC cycles. Specifically that\n-\/\/ oops are colored Remapped(N) starting at relocation N and ending\n-\/\/ at marking N + 1.\n-\/\/\n-\/\/              +--- Mark Start\n-\/\/              | +--- Mark End\n-\/\/              | | +--- Relocate Start\n-\/\/              | | | +--- Relocate End\n-\/\/              | | | |\n-\/\/ Marked       |---N---|--N+1--|--N+2--|----\n-\/\/ Finalizable  |---N---|--N+1--|--N+2--|----\n-\/\/ Remapped     ----|---N---|--N+1--|--N+2--|\n-\/\/\n-\/\/ VALID STATE TRANSITIONS\n-\/\/\n-\/\/   Marked(N)           -> Remapped(N)\n-\/\/                       -> Marked(N + 1)\n-\/\/                       -> Finalizable(N + 1)\n-\/\/\n-\/\/   Finalizable(N)      -> Marked(N)\n-\/\/                       -> Remapped(N)\n-\/\/                       -> Marked(N + 1)\n-\/\/                       -> Finalizable(N + 1)\n-\/\/\n-\/\/   Remapped(N)         -> Marked(N + 1)\n-\/\/                       -> Finalizable(N + 1)\n-\/\/\n-\/\/ PHASE VIEW\n-\/\/\n-\/\/ XPhaseMark\n-\/\/   Load & Mark\n-\/\/     Marked(N)         <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/                       <- Remapped(N - 1)\n-\/\/                       <- Finalizable(N)\n-\/\/\n-\/\/   Mark(Finalizable)\n-\/\/     Finalizable(N)    <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/                       <- Remapped(N - 1)\n-\/\/\n-\/\/   Load(AS_NO_KEEPALIVE)\n-\/\/     Remapped(N - 1)   <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/\n-\/\/ XPhaseMarkCompleted (Resurrection blocked)\n-\/\/   Load & Load(ON_WEAK\/PHANTOM_OOP_REF | AS_NO_KEEPALIVE) & KeepAlive\n-\/\/     Marked(N)         <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/                       <- Remapped(N - 1)\n-\/\/                       <- Finalizable(N)\n-\/\/\n-\/\/   Load(ON_STRONG_OOP_REF | AS_NO_KEEPALIVE)\n-\/\/     Remapped(N - 1)   <- Marked(N - 1)\n-\/\/                       <- Finalizable(N - 1)\n-\/\/\n-\/\/ XPhaseMarkCompleted (Resurrection unblocked)\n-\/\/   Load\n-\/\/     Marked(N)         <- Finalizable(N)\n-\/\/\n-\/\/ XPhaseRelocate\n-\/\/   Load & Load(AS_NO_KEEPALIVE)\n-\/\/     Remapped(N)       <- Marked(N)\n-\/\/                       <- Finalizable(N)\n-\n-template <XBarrierFastPath fast_path>\n-inline void XBarrier::self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr) {\n-  if (heal_addr == 0) {\n-    \/\/ Never heal with null since it interacts badly with reference processing.\n-    \/\/ A mutator clearing an oop would be similar to calling Reference.clear(),\n-    \/\/ which would make the reference non-discoverable or silently dropped\n-    \/\/ by the reference processor.\n-    return;\n-  }\n-\n-  assert(!fast_path(addr), \"Invalid self heal\");\n-  assert(fast_path(heal_addr), \"Invalid self heal\");\n-\n-  for (;;) {\n-    \/\/ Heal\n-    const uintptr_t prev_addr = Atomic::cmpxchg((volatile uintptr_t*)p, addr, heal_addr, memory_order_relaxed);\n-    if (prev_addr == addr) {\n-      \/\/ Success\n-      return;\n-    }\n-\n-    if (fast_path(prev_addr)) {\n-      \/\/ Must not self heal\n-      return;\n-    }\n-\n-    \/\/ The oop location was healed by another barrier, but still needs upgrading.\n-    \/\/ Re-apply healing to make sure the oop is not left with weaker (remapped or\n-    \/\/ finalizable) metadata bits than what this barrier tried to apply.\n-    assert(XAddress::offset(prev_addr) == XAddress::offset(heal_addr), \"Invalid offset\");\n-    addr = prev_addr;\n-  }\n-}\n-\n-template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path>\n-inline oop XBarrier::barrier(volatile oop* p, oop o) {\n-  const uintptr_t addr = XOop::to_address(o);\n-\n-  \/\/ Fast path\n-  if (fast_path(addr)) {\n-    return XOop::from_address(addr);\n-  }\n-\n-  \/\/ Slow path\n-  const uintptr_t good_addr = slow_path(addr);\n-\n-  if (p != nullptr) {\n-    self_heal<fast_path>(p, addr, good_addr);\n-  }\n-\n-  return XOop::from_address(good_addr);\n-}\n-\n-template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path>\n-inline oop XBarrier::weak_barrier(volatile oop* p, oop o) {\n-  const uintptr_t addr = XOop::to_address(o);\n-\n-  \/\/ Fast path\n-  if (fast_path(addr)) {\n-    \/\/ Return the good address instead of the weak good address\n-    \/\/ to ensure that the currently active heap view is used.\n-    return XOop::from_address(XAddress::good_or_null(addr));\n-  }\n-\n-  \/\/ Slow path\n-  const uintptr_t good_addr = slow_path(addr);\n-\n-  if (p != nullptr) {\n-    \/\/ The slow path returns a good\/marked address or null, but we never mark\n-    \/\/ oops in a weak load barrier so we always heal with the remapped address.\n-    self_heal<fast_path>(p, addr, XAddress::remapped_or_null(good_addr));\n-  }\n-\n-  return XOop::from_address(good_addr);\n-}\n-\n-template <XBarrierFastPath fast_path, XBarrierSlowPath slow_path>\n-inline void XBarrier::root_barrier(oop* p, oop o) {\n-  const uintptr_t addr = XOop::to_address(o);\n-\n-  \/\/ Fast path\n-  if (fast_path(addr)) {\n-    return;\n-  }\n-\n-  \/\/ Slow path\n-  const uintptr_t good_addr = slow_path(addr);\n-\n-  \/\/ Non-atomic healing helps speed up root scanning. This is safe to do\n-  \/\/ since we are always healing roots in a safepoint, or under a lock,\n-  \/\/ which ensures we are never racing with mutators modifying roots while\n-  \/\/ we are healing them. It's also safe in case multiple GC threads try\n-  \/\/ to heal the same root if it is aligned, since they would always heal\n-  \/\/ the root in the same way and it does not matter in which order it\n-  \/\/ happens. For misaligned oops, there needs to be mutual exclusion.\n-  *p = XOop::from_address(good_addr);\n-}\n-\n-inline bool XBarrier::is_good_or_null_fast_path(uintptr_t addr) {\n-  return XAddress::is_good_or_null(addr);\n-}\n-\n-inline bool XBarrier::is_weak_good_or_null_fast_path(uintptr_t addr) {\n-  return XAddress::is_weak_good_or_null(addr);\n-}\n-\n-inline bool XBarrier::is_marked_or_null_fast_path(uintptr_t addr) {\n-  return XAddress::is_marked_or_null(addr);\n-}\n-\n-inline bool XBarrier::during_mark() {\n-  return XGlobalPhase == XPhaseMark;\n-}\n-\n-inline bool XBarrier::during_relocate() {\n-  return XGlobalPhase == XPhaseRelocate;\n-}\n-\n-\/\/\n-\/\/ Load barrier\n-\/\/\n-inline oop XBarrier::load_barrier_on_oop(oop o) {\n-  return load_barrier_on_oop_field_preloaded((oop*)nullptr, o);\n-}\n-\n-inline oop XBarrier::load_barrier_on_oop_field(volatile oop* p) {\n-  const oop o = Atomic::load(p);\n-  return load_barrier_on_oop_field_preloaded(p, o);\n-}\n-\n-inline oop XBarrier::load_barrier_on_oop_field_preloaded(volatile oop* p, oop o) {\n-  return barrier<is_good_or_null_fast_path, load_barrier_on_oop_slow_path>(p, o);\n-}\n-\n-inline void XBarrier::load_barrier_on_oop_array(volatile oop* p, size_t length) {\n-  for (volatile const oop* const end = p + length; p < end; p++) {\n-    load_barrier_on_oop_field(p);\n-  }\n-}\n-\n-inline oop XBarrier::load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {\n-  verify_on_weak(p);\n-\n-  if (XResurrection::is_blocked()) {\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);\n-  }\n-\n-  return load_barrier_on_oop_field_preloaded(p, o);\n-}\n-\n-inline oop XBarrier::load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {\n-  if (XResurrection::is_blocked()) {\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);\n-  }\n-\n-  return load_barrier_on_oop_field_preloaded(p, o);\n-}\n-\n-inline void XBarrier::load_barrier_on_root_oop_field(oop* p) {\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, load_barrier_on_oop_slow_path>(p, o);\n-}\n-\n-inline void XBarrier::load_barrier_on_invisible_root_oop_field(oop* p) {\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, load_barrier_on_invisible_root_oop_slow_path>(p, o);\n-}\n-\n-\/\/\n-\/\/ Weak load barrier\n-\/\/\n-inline oop XBarrier::weak_load_barrier_on_oop_field(volatile oop* p) {\n-  assert(!XResurrection::is_blocked(), \"Should not be called during resurrection blocked phase\");\n-  const oop o = Atomic::load(p);\n-  return weak_load_barrier_on_oop_field_preloaded(p, o);\n-}\n-\n-inline oop XBarrier::weak_load_barrier_on_oop_field_preloaded(volatile oop* p, oop o) {\n-  return weak_barrier<is_weak_good_or_null_fast_path, weak_load_barrier_on_oop_slow_path>(p, o);\n-}\n-\n-inline oop XBarrier::weak_load_barrier_on_weak_oop(oop o) {\n-  return weak_load_barrier_on_weak_oop_field_preloaded((oop*)nullptr, o);\n-}\n-\n-inline oop XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o) {\n-  verify_on_weak(p);\n-\n-  if (XResurrection::is_blocked()) {\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_weak_oop_slow_path>(p, o);\n-  }\n-\n-  return weak_load_barrier_on_oop_field_preloaded(p, o);\n-}\n-\n-inline oop XBarrier::weak_load_barrier_on_phantom_oop(oop o) {\n-  return weak_load_barrier_on_phantom_oop_field_preloaded((oop*)nullptr, o);\n-}\n-\n-inline oop XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o) {\n-  if (XResurrection::is_blocked()) {\n-    return barrier<is_good_or_null_fast_path, weak_load_barrier_on_phantom_oop_slow_path>(p, o);\n-  }\n-\n-  return weak_load_barrier_on_oop_field_preloaded(p, o);\n-}\n-\n-\/\/\n-\/\/ Is alive barrier\n-\/\/\n-inline bool XBarrier::is_alive_barrier_on_weak_oop(oop o) {\n-  \/\/ Check if oop is logically non-null. This operation\n-  \/\/ is only valid when resurrection is blocked.\n-  assert(XResurrection::is_blocked(), \"Invalid phase\");\n-  return weak_load_barrier_on_weak_oop(o) != nullptr;\n-}\n-\n-inline bool XBarrier::is_alive_barrier_on_phantom_oop(oop o) {\n-  \/\/ Check if oop is logically non-null. This operation\n-  \/\/ is only valid when resurrection is blocked.\n-  assert(XResurrection::is_blocked(), \"Invalid phase\");\n-  return weak_load_barrier_on_phantom_oop(o) != nullptr;\n-}\n-\n-\/\/\n-\/\/ Keep alive barrier\n-\/\/\n-inline void XBarrier::keep_alive_barrier_on_weak_oop_field(volatile oop* p) {\n-  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const oop o = Atomic::load(p);\n-  barrier<is_good_or_null_fast_path, keep_alive_barrier_on_weak_oop_slow_path>(p, o);\n-}\n-\n-inline void XBarrier::keep_alive_barrier_on_phantom_oop_field(volatile oop* p) {\n-  assert(XResurrection::is_blocked(), \"This operation is only valid when resurrection is blocked\");\n-  const oop o = Atomic::load(p);\n-  barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);\n-}\n-\n-inline void XBarrier::keep_alive_barrier_on_phantom_root_oop_field(oop* p) {\n-  \/\/ The keep alive operation is only valid when resurrection is blocked.\n-  \/\/\n-  \/\/ Except with Loom, where we intentionally trigger arms nmethods after\n-  \/\/ unlinking, to get a sense of what nmethods are alive. This will trigger\n-  \/\/ the keep alive barriers, but the oops are healed and the slow-paths\n-  \/\/ will not trigger. We have stronger checks in the slow-paths.\n-  assert(XResurrection::is_blocked() || (CodeCache::contains((void*)p)),\n-         \"This operation is only valid when resurrection is blocked\");\n-  const oop o = *p;\n-  root_barrier<is_good_or_null_fast_path, keep_alive_barrier_on_phantom_oop_slow_path>(p, o);\n-}\n-\n-inline void XBarrier::keep_alive_barrier_on_oop(oop o) {\n-  const uintptr_t addr = XOop::to_address(o);\n-  assert(XAddress::is_good(addr), \"Invalid address\");\n-\n-  if (during_mark()) {\n-    keep_alive_barrier_on_oop_slow_path(addr);\n-  }\n-}\n-\n-\/\/\n-\/\/ Mark barrier\n-\/\/\n-inline void XBarrier::mark_barrier_on_oop_field(volatile oop* p, bool finalizable) {\n-  const oop o = Atomic::load(p);\n-\n-  if (finalizable) {\n-    barrier<is_marked_or_null_fast_path, mark_barrier_on_finalizable_oop_slow_path>(p, o);\n-  } else {\n-    const uintptr_t addr = XOop::to_address(o);\n-    if (XAddress::is_good(addr)) {\n-      \/\/ Mark through good oop\n-      mark_barrier_on_oop_slow_path(addr);\n-    } else {\n-      \/\/ Mark through bad oop\n-      barrier<is_good_or_null_fast_path, mark_barrier_on_oop_slow_path>(p, o);\n-    }\n-  }\n-}\n-\n-inline void XBarrier::mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable) {\n-  for (volatile const oop* const end = p + length; p < end; p++) {\n-    mark_barrier_on_oop_field(p, finalizable);\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XBARRIER_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrier.inline.hpp","additions":0,"deletions":394,"binary":false,"changes":394,"status":"deleted"},{"patch":"@@ -1,99 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xBarrierSetNMethod.hpp\"\n-#include \"gc\/x\/xBarrierSetStackChunk.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xStackWatermark.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#ifdef COMPILER1\n-#include \"gc\/x\/c1\/xBarrierSetC1.hpp\"\n-#endif\n-#ifdef COMPILER2\n-#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n-#endif\n-\n-class XBarrierSetC1;\n-class XBarrierSetC2;\n-\n-XBarrierSet::XBarrierSet() :\n-    BarrierSet(make_barrier_set_assembler<XBarrierSetAssembler>(),\n-               make_barrier_set_c1<XBarrierSetC1>(),\n-               make_barrier_set_c2<XBarrierSetC2>(),\n-               new XBarrierSetNMethod(),\n-               new XBarrierSetStackChunk(),\n-               BarrierSet::FakeRtti(BarrierSet::XBarrierSet)) {}\n-\n-XBarrierSetAssembler* XBarrierSet::assembler() {\n-  BarrierSetAssembler* const bsa = BarrierSet::barrier_set()->barrier_set_assembler();\n-  return reinterpret_cast<XBarrierSetAssembler*>(bsa);\n-}\n-\n-bool XBarrierSet::barrier_needed(DecoratorSet decorators, BasicType type) {\n-  assert((decorators & AS_RAW) == 0, \"Unexpected decorator\");\n-  \/\/assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"Unexpected decorator\");\n-\n-  if (is_reference_type(type)) {\n-    assert((decorators & (IN_HEAP | IN_NATIVE)) != 0, \"Where is reference?\");\n-    \/\/ Barrier needed even when IN_NATIVE, to allow concurrent scanning.\n-    return true;\n-  }\n-\n-  \/\/ Barrier not needed\n-  return false;\n-}\n-\n-void XBarrierSet::on_thread_create(Thread* thread) {\n-  \/\/ Create thread local data\n-  XThreadLocalData::create(thread);\n-}\n-\n-void XBarrierSet::on_thread_destroy(Thread* thread) {\n-  \/\/ Destroy thread local data\n-  XThreadLocalData::destroy(thread);\n-}\n-\n-void XBarrierSet::on_thread_attach(Thread* thread) {\n-  \/\/ Set thread local address bad mask\n-  XThreadLocalData::set_address_bad_mask(thread, XAddressBadMask);\n-  if (thread->is_Java_thread()) {\n-    JavaThread* const jt = JavaThread::cast(thread);\n-    StackWatermark* const watermark = new XStackWatermark(jt);\n-    StackWatermarkSet::add_watermark(jt, watermark);\n-  }\n-}\n-\n-void XBarrierSet::on_thread_detach(Thread* thread) {\n-  \/\/ Flush and free any remaining mark stacks\n-  XHeap::heap()->mark_flush_and_free(thread);\n-}\n-\n-void XBarrierSet::print_on(outputStream* st) const {\n-  st->print_cr(\"XBarrierSet\");\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.cpp","additions":0,"deletions":99,"binary":false,"changes":99,"status":"deleted"},{"patch":"@@ -1,109 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIERSET_HPP\n-#define SHARE_GC_X_XBARRIERSET_HPP\n-\n-#include \"gc\/shared\/barrierSet.hpp\"\n-\n-class XBarrierSetAssembler;\n-\n-class XBarrierSet : public BarrierSet {\n-public:\n-  XBarrierSet();\n-\n-  static XBarrierSetAssembler* assembler();\n-  static bool barrier_needed(DecoratorSet decorators, BasicType type);\n-\n-  virtual void on_thread_create(Thread* thread);\n-  virtual void on_thread_destroy(Thread* thread);\n-  virtual void on_thread_attach(Thread* thread);\n-  virtual void on_thread_detach(Thread* thread);\n-\n-  virtual void print_on(outputStream* st) const;\n-\n-  template <DecoratorSet decorators, typename BarrierSetT = XBarrierSet>\n-  class AccessBarrier : public BarrierSet::AccessBarrier<decorators, BarrierSetT> {\n-  private:\n-    typedef BarrierSet::AccessBarrier<decorators, BarrierSetT> Raw;\n-\n-    template <DecoratorSet expected>\n-    static void verify_decorators_present();\n-\n-    template <DecoratorSet expected>\n-    static void verify_decorators_absent();\n-\n-    static oop* field_addr(oop base, ptrdiff_t offset);\n-\n-    template <typename T>\n-    static oop load_barrier_on_oop_field_preloaded(T* addr, oop o);\n-\n-    template <typename T>\n-    static oop load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o);\n-\n-  public:\n-    \/\/\n-    \/\/ In heap\n-    \/\/\n-    template <typename T>\n-    static oop oop_load_in_heap(T* addr);\n-    static oop oop_load_in_heap_at(oop base, ptrdiff_t offset);\n-\n-    template <typename T>\n-    static oop oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value);\n-    static oop oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value);\n-\n-    template <typename T>\n-    static oop oop_atomic_xchg_in_heap(T* addr, oop new_value);\n-    static oop oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value);\n-\n-    template <typename T>\n-    static bool oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n-                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n-                                      size_t length);\n-\n-    static void clone_in_heap(oop src, oop dst, size_t size);\n-\n-    \/\/\n-    \/\/ Not in heap\n-    \/\/\n-    template <typename T>\n-    static oop oop_load_not_in_heap(T* addr);\n-\n-    template <typename T>\n-    static oop oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value);\n-\n-    template <typename T>\n-    static oop oop_atomic_xchg_not_in_heap(T* addr, oop new_value);\n-  };\n-};\n-\n-template<> struct BarrierSet::GetName<XBarrierSet> {\n-  static const BarrierSet::Name value = BarrierSet::XBarrierSet;\n-};\n-\n-template<> struct BarrierSet::GetType<BarrierSet::XBarrierSet> {\n-  typedef ::XBarrierSet type;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBARRIERSET_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.hpp","additions":0,"deletions":109,"binary":false,"changes":109,"status":"deleted"},{"patch":"@@ -1,242 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIERSET_INLINE_HPP\n-#define SHARE_GC_X_XBARRIERSET_INLINE_HPP\n-\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-\n-#include \"gc\/shared\/accessBarrierSupport.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <DecoratorSet expected>\n-inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::verify_decorators_present() {\n-  if ((decorators & expected) == 0) {\n-    fatal(\"Using unsupported access decorators\");\n-  }\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <DecoratorSet expected>\n-inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::verify_decorators_absent() {\n-  if ((decorators & expected) != 0) {\n-    fatal(\"Using unsupported access decorators\");\n-  }\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-inline oop* XBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n-  assert(base != nullptr, \"Invalid base\");\n-  return reinterpret_cast<oop*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_oop_field_preloaded(T* addr, oop o) {\n-  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n-\n-  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n-    if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n-      return XBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n-    } else if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n-      return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n-    } else {\n-      assert((HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value), \"Must be\");\n-      return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n-    }\n-  } else {\n-    if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n-      return XBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n-    } else if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n-      return XBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n-    } else {\n-      assert((HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value), \"Must be\");\n-      return XBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n-    }\n-  }\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o) {\n-  verify_decorators_present<ON_UNKNOWN_OOP_REF>();\n-\n-  const DecoratorSet decorators_known_strength =\n-    AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n-\n-  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n-    if (decorators_known_strength & ON_STRONG_OOP_REF) {\n-      return XBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n-    } else if (decorators_known_strength & ON_WEAK_OOP_REF) {\n-      return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n-    } else {\n-      assert(decorators_known_strength & ON_PHANTOM_OOP_REF, \"Must be\");\n-      return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n-    }\n-  } else {\n-    if (decorators_known_strength & ON_STRONG_OOP_REF) {\n-      return XBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n-    } else if (decorators_known_strength & ON_WEAK_OOP_REF) {\n-      return XBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n-    } else {\n-      assert(decorators_known_strength & ON_PHANTOM_OOP_REF, \"Must be\");\n-      return XBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n-    }\n-  }\n-}\n-\n-\/\/\n-\/\/ In heap\n-\/\/\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(T* addr) {\n-  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n-\n-  const oop o = Raw::oop_load_in_heap(addr);\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap_at(oop base, ptrdiff_t offset) {\n-  oop* const addr = field_addr(base, offset);\n-  const oop o = Raw::oop_load_in_heap(addr);\n-\n-  if (HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) {\n-    return load_barrier_on_unknown_oop_field_preloaded(base, offset, addr, o);\n-  }\n-\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value) {\n-  verify_decorators_present<ON_STRONG_OOP_REF>();\n-  verify_decorators_absent<AS_NO_KEEPALIVE>();\n-\n-  XBarrier::load_barrier_on_oop_field(addr);\n-  return Raw::oop_atomic_cmpxchg_in_heap(addr, compare_value, new_value);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value) {\n-  verify_decorators_present<ON_STRONG_OOP_REF | ON_UNKNOWN_OOP_REF>();\n-  verify_decorators_absent<AS_NO_KEEPALIVE>();\n-\n-  \/\/ Through Unsafe.CompareAndExchangeObject()\/CompareAndSetObject() we can receive\n-  \/\/ calls with ON_UNKNOWN_OOP_REF set. However, we treat these as ON_STRONG_OOP_REF,\n-  \/\/ with the motivation that if you're doing Unsafe operations on a Reference.referent\n-  \/\/ field, then you're on your own anyway.\n-  XBarrier::load_barrier_on_oop_field(field_addr(base, offset));\n-  return Raw::oop_atomic_cmpxchg_in_heap_at(base, offset, compare_value, new_value);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(T* addr, oop new_value) {\n-  verify_decorators_present<ON_STRONG_OOP_REF>();\n-  verify_decorators_absent<AS_NO_KEEPALIVE>();\n-\n-  const oop o = Raw::oop_atomic_xchg_in_heap(addr, new_value);\n-  return XBarrier::load_barrier_on_oop(o);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value) {\n-  verify_decorators_present<ON_STRONG_OOP_REF>();\n-  verify_decorators_absent<AS_NO_KEEPALIVE>();\n-\n-  const oop o = Raw::oop_atomic_xchg_in_heap_at(base, offset, new_value);\n-  return XBarrier::load_barrier_on_oop(o);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline bool XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n-                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n-                                                                                       size_t length) {\n-  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n-  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n-\n-  if (!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n-    \/\/ No check cast, bulk barrier and bulk copy\n-    XBarrier::load_barrier_on_oop_array(src, length);\n-    return Raw::oop_arraycopy_in_heap(nullptr, 0, src, nullptr, 0, dst, length);\n-  }\n-\n-  \/\/ Check cast and copy each elements\n-  Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n-  for (const T* const end = src + length; src < end; src++, dst++) {\n-    const oop elem = XBarrier::load_barrier_on_oop_field(src);\n-    if (!oopDesc::is_instanceof_or_null(elem, dst_klass)) {\n-      \/\/ Check cast failed\n-      return false;\n-    }\n-\n-    \/\/ Cast is safe, since we know it's never a narrowOop\n-    *(oop*)dst = elem;\n-  }\n-\n-  return true;\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::clone_in_heap(oop src, oop dst, size_t size) {\n-  XBarrier::load_barrier_on_oop_fields(src);\n-  Raw::clone_in_heap(src, dst, size);\n-}\n-\n-\/\/\n-\/\/ Not in heap\n-\/\/\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(T* addr) {\n-  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n-\n-  const oop o = Raw::oop_load_not_in_heap(addr);\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value) {\n-  verify_decorators_present<ON_STRONG_OOP_REF>();\n-  verify_decorators_absent<AS_NO_KEEPALIVE>();\n-\n-  return Raw::oop_atomic_cmpxchg_not_in_heap(addr, compare_value, new_value);\n-}\n-\n-template <DecoratorSet decorators, typename BarrierSetT>\n-template <typename T>\n-inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(T* addr, oop new_value) {\n-  verify_decorators_present<ON_STRONG_OOP_REF>();\n-  verify_decorators_absent<AS_NO_KEEPALIVE>();\n-\n-  return Raw::oop_atomic_xchg_not_in_heap(addr, new_value);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XBARRIERSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.inline.hpp","additions":0,"deletions":242,"binary":false,"changes":242,"status":"deleted"},{"patch":"@@ -1,35 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-\n-Address XBarrierSetAssemblerBase::address_bad_mask_from_thread(Register thread) {\n-  return Address(thread, XThreadLocalData::address_bad_mask_offset());\n-}\n-\n-Address XBarrierSetAssemblerBase::address_bad_mask_from_jni_env(Register env) {\n-  return Address(env, XThreadLocalData::address_bad_mask_offset() - JavaThread::jni_environment_offset());\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetAssembler.cpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIERSETASSEMBLER_HPP\n-#define SHARE_GC_X_XBARRIERSETASSEMBLER_HPP\n-\n-#include \"gc\/shared\/barrierSetAssembler.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class XBarrierSetAssemblerBase : public BarrierSetAssembler {\n-public:\n-  static Address address_bad_mask_from_thread(Register thread);\n-  static Address address_bad_mask_from_jni_env(Register env);\n-};\n-\n-\/\/ Needs to be included after definition of XBarrierSetAssemblerBase\n-#include CPU_HEADER(gc\/x\/xBarrierSetAssembler)\n-\n-#endif \/\/ SHARE_GC_X_XBARRIERSETASSEMBLER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetAssembler.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,83 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"gc\/x\/xBarrierSetNMethod.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/threadWXSetters.inline.hpp\"\n-\n-bool XBarrierSetNMethod::nmethod_entry_barrier(nmethod* nm) {\n-  if (!is_armed(nm)) {\n-    \/\/ Some other thread got here first and healed the oops\n-    \/\/ and disarmed the nmethod. No need to continue.\n-    return true;\n-  }\n-\n-  XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n-  log_trace(nmethod, barrier)(\"Entered critical zone for %p\", nm);\n-\n-  if (!is_armed(nm)) {\n-    \/\/ Some other thread managed to complete while we were\n-    \/\/ waiting for lock. No need to continue.\n-    return true;\n-  }\n-\n-  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, Thread::current()));\n-\n-  if (nm->is_unloading()) {\n-    \/\/ We don't need to take the lock when unlinking nmethods from\n-    \/\/ the Method, because it is only concurrently unlinked by\n-    \/\/ the entry barrier, which acquires the per nmethod lock.\n-    nm->unlink_from_method();\n-\n-    \/\/ We can end up calling nmethods that are unloading\n-    \/\/ since we clear compiled ICs lazily. Returning false\n-    \/\/ will re-resovle the call and update the compiled IC.\n-    return false;\n-  }\n-\n-  \/\/ Heal oops\n-  XNMethod::nmethod_oops_barrier(nm);\n-\n-\n-  \/\/ CodeCache unloading support\n-  nm->mark_as_maybe_on_stack();\n-\n-  \/\/ Disarm\n-  disarm(nm);\n-\n-  return true;\n-}\n-\n-int* XBarrierSetNMethod::disarmed_guard_value_address() const {\n-  return (int*)XAddressBadMaskHighOrderBitsAddr;\n-}\n-\n-ByteSize XBarrierSetNMethod::thread_disarmed_guard_value_offset() const {\n-  return XThreadLocalData::nmethod_disarmed_offset();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetNMethod.cpp","additions":0,"deletions":83,"binary":false,"changes":83,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIERSETNMETHOD_HPP\n-#define SHARE_GC_X_XBARRIERSETNMETHOD_HPP\n-\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class nmethod;\n-\n-class XBarrierSetNMethod : public BarrierSetNMethod {\n-protected:\n-  virtual bool nmethod_entry_barrier(nmethod* nm);\n-\n-public:\n-  virtual ByteSize thread_disarmed_guard_value_offset() const;\n-  virtual int* disarmed_guard_value_address() const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBARRIERSETNMETHOD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetNMethod.hpp","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,114 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"oops\/access.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n-\n-JRT_LEAF(oopDesc*, XBarrierSetRuntime::load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p))\n-  return XBarrier::load_barrier_on_oop_field_preloaded(p, o);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, XBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p))\n-  return XBarrier::weak_load_barrier_on_oop_field_preloaded(p, o);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, XBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n-  return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(p, o);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, XBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n-  return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(p, o);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, XBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p))\n-  return XBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, XBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p))\n-  return XBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n-JRT_END\n-\n-JRT_LEAF(void, XBarrierSetRuntime::load_barrier_on_oop_array(oop* p, size_t length))\n-  XBarrier::load_barrier_on_oop_array(p, length);\n-JRT_END\n-\n-JRT_LEAF(void, XBarrierSetRuntime::clone(oopDesc* src, oopDesc* dst, size_t size))\n-  HeapAccess<>::clone(src, dst, size);\n-JRT_END\n-\n-address XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(DecoratorSet decorators) {\n-  if (decorators & ON_PHANTOM_OOP_REF) {\n-    if (decorators & AS_NO_KEEPALIVE) {\n-      return weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n-    } else {\n-      return load_barrier_on_phantom_oop_field_preloaded_addr();\n-    }\n-  } else if (decorators & ON_WEAK_OOP_REF) {\n-    if (decorators & AS_NO_KEEPALIVE) {\n-      return weak_load_barrier_on_weak_oop_field_preloaded_addr();\n-    } else {\n-      return load_barrier_on_weak_oop_field_preloaded_addr();\n-    }\n-  } else {\n-    if (decorators & AS_NO_KEEPALIVE) {\n-      return weak_load_barrier_on_oop_field_preloaded_addr();\n-    } else {\n-      return load_barrier_on_oop_field_preloaded_addr();\n-    }\n-  }\n-}\n-\n-address XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(load_barrier_on_oop_field_preloaded);\n-}\n-\n-address XBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(load_barrier_on_weak_oop_field_preloaded);\n-}\n-\n-address XBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(load_barrier_on_phantom_oop_field_preloaded);\n-}\n-\n-address XBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(weak_load_barrier_on_oop_field_preloaded);\n-}\n-\n-address XBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(weak_load_barrier_on_weak_oop_field_preloaded);\n-}\n-\n-address XBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded_addr() {\n-  return reinterpret_cast<address>(weak_load_barrier_on_phantom_oop_field_preloaded);\n-}\n-\n-address XBarrierSetRuntime::load_barrier_on_oop_array_addr() {\n-  return reinterpret_cast<address>(load_barrier_on_oop_array);\n-}\n-\n-address XBarrierSetRuntime::clone_addr() {\n-  return reinterpret_cast<address>(clone);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetRuntime.cpp","additions":0,"deletions":114,"binary":false,"changes":114,"status":"deleted"},{"patch":"@@ -1,56 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIERSETRUNTIME_HPP\n-#define SHARE_GC_X_XBARRIERSETRUNTIME_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"oops\/accessDecorators.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class oopDesc;\n-\n-class XBarrierSetRuntime : public AllStatic {\n-private:\n-  static oopDesc* load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* weak_load_barrier_on_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* weak_load_barrier_on_weak_oop_field_preloaded(oopDesc* o, oop* p);\n-  static oopDesc* weak_load_barrier_on_phantom_oop_field_preloaded(oopDesc* o, oop* p);\n-  static void load_barrier_on_oop_array(oop* p, size_t length);\n-  static void clone(oopDesc* src, oopDesc* dst, size_t size);\n-\n-public:\n-  static address load_barrier_on_oop_field_preloaded_addr(DecoratorSet decorators);\n-  static address load_barrier_on_oop_field_preloaded_addr();\n-  static address load_barrier_on_weak_oop_field_preloaded_addr();\n-  static address load_barrier_on_phantom_oop_field_preloaded_addr();\n-  static address weak_load_barrier_on_oop_field_preloaded_addr();\n-  static address weak_load_barrier_on_weak_oop_field_preloaded_addr();\n-  static address weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n-  static address load_barrier_on_oop_array_addr();\n-  static address clone_addr();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBARRIERSETRUNTIME_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetRuntime.hpp","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -1,47 +0,0 @@\n-\/*\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xBarrierSetStackChunk.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-void XBarrierSetStackChunk::encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n-  \/\/ Do nothing\n-}\n-\n-void XBarrierSetStackChunk::decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) {\n-  \/\/ Do nothing\n-}\n-\n-oop XBarrierSetStackChunk::load_oop(stackChunkOop chunk, oop* addr) {\n-  oop obj = Atomic::load(addr);\n-  return XBarrier::load_barrier_on_oop_field_preloaded((volatile oop*)nullptr, obj);\n-}\n-\n-oop XBarrierSetStackChunk::load_oop(stackChunkOop chunk, narrowOop* addr) {\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetStackChunk.cpp","additions":0,"deletions":47,"binary":false,"changes":47,"status":"deleted"},{"patch":"@@ -1,44 +0,0 @@\n-\/*\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_X_XBARRIERSETSTACKCHUNK_HPP\n-#define SHARE_GC_X_XBARRIERSETSTACKCHUNK_HPP\n-\n-#include \"gc\/shared\/barrierSetStackChunk.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class OopClosure;\n-\n-class XBarrierSetStackChunk : public BarrierSetStackChunk {\n-public:\n-  virtual void encode_gc_mode(stackChunkOop chunk, OopIterator* iterator) override;\n-  virtual void decode_gc_mode(stackChunkOop chunk, OopIterator* iterator) override;\n-\n-  virtual oop load_oop(stackChunkOop chunk, oop* addr) override;\n-  virtual oop load_oop(stackChunkOop chunk, narrowOop* addr) override;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBARRIERSETSTACKCHUNK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSetStackChunk.hpp","additions":0,"deletions":44,"binary":false,"changes":44,"status":"deleted"},{"patch":"@@ -1,81 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBITFIELD_HPP\n-#define SHARE_GC_X_XBITFIELD_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/\/\n-\/\/  Example\n-\/\/  -------\n-\/\/\n-\/\/  typedef XBitField<uint64_t, uint8_t,  0,  2, 3> field_word_aligned_size;\n-\/\/  typedef XBitField<uint64_t, uint32_t, 2, 30>    field_length;\n-\/\/\n-\/\/\n-\/\/   6                                 3 3\n-\/\/   3                                 2 1                               2 10\n-\/\/  +-----------------------------------+---------------------------------+--+\n-\/\/  |11111111 11111111 11111111 11111111|11111111 11111111 11111111 111111|11|\n-\/\/  +-----------------------------------+---------------------------------+--+\n-\/\/  |                                   |                                 |\n-\/\/  |       31-2 field_length (30-bits) *                                 |\n-\/\/  |                                                                     |\n-\/\/  |                                1-0 field_word_aligned_size (2-bits) *\n-\/\/  |\n-\/\/  * 63-32 Unused (32-bits)\n-\/\/\n-\/\/\n-\/\/  field_word_aligned_size::encode(16) = 2\n-\/\/  field_length::encode(2342) = 9368\n-\/\/\n-\/\/  field_word_aligned_size::decode(9368 | 2) = 16\n-\/\/  field_length::decode(9368 | 2) = 2342\n-\/\/\n-\n-template <typename ContainerType, typename ValueType, int FieldShift, int FieldBits, int ValueShift = 0>\n-class XBitField : public AllStatic {\n-private:\n-  static const int ContainerBits = sizeof(ContainerType) * BitsPerByte;\n-\n-  static_assert(FieldBits < ContainerBits, \"Field too large\");\n-  static_assert(FieldShift + FieldBits <= ContainerBits, \"Field too large\");\n-  static_assert(ValueShift + FieldBits <= ContainerBits, \"Field too large\");\n-\n-  static const ContainerType FieldMask = (((ContainerType)1 << FieldBits) - 1);\n-\n-public:\n-  static ValueType decode(ContainerType container) {\n-    return (ValueType)(((container >> FieldShift) & FieldMask) << ValueShift);\n-  }\n-\n-  static ContainerType encode(ValueType value) {\n-    assert(((ContainerType)value & (FieldMask << ValueShift)) == (ContainerType)value, \"Invalid value\");\n-    return ((ContainerType)value >> ValueShift) << FieldShift;\n-  }\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBITFIELD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBitField.hpp","additions":0,"deletions":81,"binary":false,"changes":81,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBITMAP_HPP\n-#define SHARE_GC_X_XBITMAP_HPP\n-\n-#include \"utilities\/bitMap.hpp\"\n-\n-class XBitMap : public CHeapBitMap {\n-private:\n-  static bm_word_t bit_mask_pair(idx_t bit);\n-\n-  bool par_set_bit_pair_finalizable(idx_t bit, bool& inc_live);\n-  bool par_set_bit_pair_strong(idx_t bit, bool& inc_live);\n-\n-public:\n-  XBitMap(idx_t size_in_bits);\n-\n-  bool par_set_bit_pair(idx_t bit, bool finalizable, bool& inc_live);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBitMap.hpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,80 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBITMAP_INLINE_HPP\n-#define SHARE_GC_X_XBITMAP_INLINE_HPP\n-\n-#include \"gc\/x\/xBitMap.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline XBitMap::XBitMap(idx_t size_in_bits) :\n-    CHeapBitMap(size_in_bits, mtGC, false \/* clear *\/) {}\n-\n-inline BitMap::bm_word_t XBitMap::bit_mask_pair(idx_t bit) {\n-  assert(bit_in_word(bit) < BitsPerWord - 1, \"Invalid bit index\");\n-  return (bm_word_t)3 << bit_in_word(bit);\n-}\n-\n-inline bool XBitMap::par_set_bit_pair_finalizable(idx_t bit, bool& inc_live) {\n-  inc_live = par_set_bit(bit);\n-  return inc_live;\n-}\n-\n-inline bool XBitMap::par_set_bit_pair_strong(idx_t bit, bool& inc_live) {\n-  verify_index(bit);\n-  volatile bm_word_t* const addr = word_addr(bit);\n-  const bm_word_t pair_mask = bit_mask_pair(bit);\n-  bm_word_t old_val = *addr;\n-\n-  do {\n-    const bm_word_t new_val = old_val | pair_mask;\n-    if (new_val == old_val) {\n-      \/\/ Someone else beat us to it\n-      inc_live = false;\n-      return false;\n-    }\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val);\n-    if (cur_val == old_val) {\n-      \/\/ Success\n-      const bm_word_t marked_mask = bit_mask(bit);\n-      inc_live = !(old_val & marked_mask);\n-      return true;\n-    }\n-\n-    \/\/ The value changed, retry\n-    old_val = cur_val;\n-  } while (true);\n-}\n-\n-inline bool XBitMap::par_set_bit_pair(idx_t bit, bool finalizable, bool& inc_live) {\n-  if (finalizable) {\n-    return par_set_bit_pair_finalizable(bit, inc_live);\n-  } else {\n-    return par_set_bit_pair_strong(bit, inc_live);\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBitMap.inline.hpp","additions":0,"deletions":80,"binary":false,"changes":80,"status":"deleted"},{"patch":"@@ -1,63 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/concurrentGCBreakpoints.hpp\"\n-#include \"gc\/x\/xBreakpoint.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-bool XBreakpoint::_start_gc = false;\n-\n-void XBreakpoint::start_gc() {\n-  MonitorLocker ml(ConcurrentGCBreakpoints::monitor());\n-  assert(ConcurrentGCBreakpoints::is_controlled(), \"Invalid state\");\n-  assert(!_start_gc, \"Invalid state\");\n-  _start_gc = true;\n-  ml.notify_all();\n-}\n-\n-void XBreakpoint::at_before_gc() {\n-  MonitorLocker ml(ConcurrentGCBreakpoints::monitor(), Mutex::_no_safepoint_check_flag);\n-  while (ConcurrentGCBreakpoints::is_controlled() && !_start_gc) {\n-    ml.wait();\n-  }\n-  _start_gc = false;\n-  ConcurrentGCBreakpoints::notify_idle_to_active();\n-}\n-\n-void XBreakpoint::at_after_gc() {\n-  ConcurrentGCBreakpoints::notify_active_to_idle();\n-}\n-\n-void XBreakpoint::at_after_marking_started() {\n-  ConcurrentGCBreakpoints::at(\"AFTER MARKING STARTED\");\n-}\n-\n-void XBreakpoint::at_before_marking_completed() {\n-  ConcurrentGCBreakpoints::at(\"BEFORE MARKING COMPLETED\");\n-}\n-\n-void XBreakpoint::at_after_reference_processing_started() {\n-  ConcurrentGCBreakpoints::at(\"AFTER CONCURRENT REFERENCE PROCESSING STARTED\");\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xBreakpoint.cpp","additions":0,"deletions":63,"binary":false,"changes":63,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XBREAKPOINT_HPP\n-#define SHARE_GC_X_XBREAKPOINT_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class XBreakpoint : public AllStatic {\n-private:\n-  static bool _start_gc;\n-\n-public:\n-  static void start_gc();\n-\n-  static void at_before_gc();\n-  static void at_after_gc();\n-  static void at_after_marking_started();\n-  static void at_before_marking_completed();\n-  static void at_after_reference_processing_started();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XBREAKPOINT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBreakpoint.hpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xCPU.inline.hpp\"\n-#include \"memory\/padded.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-#define XCPU_UNKNOWN_AFFINITY ((Thread*)-1)\n-#define XCPU_UNKNOWN_SELF     ((Thread*)-2)\n-\n-PaddedEnd<XCPU::XCPUAffinity>* XCPU::_affinity = nullptr;\n-THREAD_LOCAL Thread*           XCPU::_self     = XCPU_UNKNOWN_SELF;\n-THREAD_LOCAL uint32_t          XCPU::_cpu      = 0;\n-\n-void XCPU::initialize() {\n-  assert(_affinity == nullptr, \"Already initialized\");\n-  const uint32_t ncpus = count();\n-\n-  _affinity = PaddedArray<XCPUAffinity, mtGC>::create_unfreeable(ncpus);\n-\n-  for (uint32_t i = 0; i < ncpus; i++) {\n-    _affinity[i]._thread = XCPU_UNKNOWN_AFFINITY;\n-  }\n-\n-  log_info_p(gc, init)(\"CPUs: %u total, %u available\",\n-                       os::processor_count(),\n-                       os::initial_active_processor_count());\n-}\n-\n-uint32_t XCPU::id_slow() {\n-  \/\/ Set current thread\n-  if (_self == XCPU_UNKNOWN_SELF) {\n-    _self = Thread::current();\n-  }\n-\n-  \/\/ Set current CPU\n-  _cpu = os::processor_id();\n-\n-  \/\/ Update affinity table\n-  _affinity[_cpu]._thread = _self;\n-\n-  return _cpu;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xCPU.cpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,52 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XCPU_HPP\n-#define SHARE_GC_X_XCPU_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"memory\/padded.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class Thread;\n-\n-class XCPU : public AllStatic {\n-private:\n-  struct XCPUAffinity {\n-    Thread* _thread;\n-  };\n-\n-  static PaddedEnd<XCPUAffinity>* _affinity;\n-  static THREAD_LOCAL Thread*     _self;\n-  static THREAD_LOCAL uint32_t    _cpu;\n-\n-  static uint32_t id_slow();\n-\n-public:\n-  static void initialize();\n-\n-  static uint32_t count();\n-  static uint32_t id();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XCPU_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xCPU.hpp","additions":0,"deletions":52,"binary":false,"changes":52,"status":"deleted"},{"patch":"@@ -1,48 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XCPU_INLINE_HPP\n-#define SHARE_GC_X_XCPU_INLINE_HPP\n-\n-#include \"gc\/x\/xCPU.hpp\"\n-\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline uint32_t XCPU::count() {\n-  return os::processor_count();\n-}\n-\n-inline uint32_t XCPU::id() {\n-  assert(_affinity != nullptr, \"Not initialized\");\n-\n-  \/\/ Fast path\n-  if (_affinity[_cpu]._thread == _self) {\n-    return _cpu;\n-  }\n-\n-  \/\/ Slow path\n-  return id_slow();\n-}\n-\n-#endif \/\/ SHARE_GC_X_XCPU_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xCPU.inline.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"deleted"},{"patch":"@@ -1,344 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderData.hpp\"\n-#include \"gc\/shared\/gcHeapSummary.hpp\"\n-#include \"gc\/shared\/gcLocker.inline.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xDirector.hpp\"\n-#include \"gc\/x\/xDriver.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xObjArrayAllocator.hpp\"\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"gc\/x\/xServiceability.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xUtils.inline.hpp\"\n-#include \"memory\/classLoaderMetaspace.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/metaspaceCriticalAllocation.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/stackChunkOop.hpp\"\n-#include \"runtime\/continuationJavaClasses.hpp\"\n-#include \"runtime\/stackWatermarkSet.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-XCollectedHeap* XCollectedHeap::heap() {\n-  return named_heap<XCollectedHeap>(CollectedHeap::Z);\n-}\n-\n-XCollectedHeap::XCollectedHeap() :\n-    _barrier_set(),\n-    _initialize(&_barrier_set),\n-    _heap(),\n-    _driver(new XDriver()),\n-    _director(new XDirector(_driver)),\n-    _stat(new XStat()),\n-    _runtime_workers() {}\n-\n-CollectedHeap::Name XCollectedHeap::kind() const {\n-  return CollectedHeap::Z;\n-}\n-\n-const char* XCollectedHeap::name() const {\n-  return XName;\n-}\n-\n-jint XCollectedHeap::initialize() {\n-  if (!_heap.is_initialized()) {\n-    return JNI_ENOMEM;\n-  }\n-\n-  Universe::calculate_verify_data((HeapWord*)0, (HeapWord*)UINTPTR_MAX);\n-\n-  return JNI_OK;\n-}\n-\n-void XCollectedHeap::initialize_serviceability() {\n-  _heap.serviceability_initialize();\n-}\n-\n-class XStopConcurrentGCThreadClosure : public ThreadClosure {\n-public:\n-  virtual void do_thread(Thread* thread) {\n-    if (thread->is_ConcurrentGC_thread()) {\n-      ConcurrentGCThread::cast(thread)->stop();\n-    }\n-  }\n-};\n-\n-void XCollectedHeap::stop() {\n-  XStopConcurrentGCThreadClosure cl;\n-  gc_threads_do(&cl);\n-}\n-\n-size_t XCollectedHeap::max_capacity() const {\n-  return _heap.max_capacity();\n-}\n-\n-size_t XCollectedHeap::capacity() const {\n-  return _heap.capacity();\n-}\n-\n-size_t XCollectedHeap::used() const {\n-  return _heap.used();\n-}\n-\n-size_t XCollectedHeap::unused() const {\n-  return _heap.unused();\n-}\n-\n-bool XCollectedHeap::is_maximal_no_gc() const {\n-  \/\/ Not supported\n-  ShouldNotReachHere();\n-  return false;\n-}\n-\n-bool XCollectedHeap::is_in(const void* p) const {\n-  return _heap.is_in((uintptr_t)p);\n-}\n-\n-bool XCollectedHeap::requires_barriers(stackChunkOop obj) const {\n-  uintptr_t* cont_addr = obj->field_addr<uintptr_t>(jdk_internal_vm_StackChunk::cont_offset());\n-\n-  if (!_heap.is_allocating(cast_from_oop<uintptr_t>(obj))) {\n-    \/\/ An object that isn't allocating, is visible from GC tracing. Such\n-    \/\/ stack chunks require barriers.\n-    return true;\n-  }\n-\n-  if (!XAddress::is_good_or_null(*cont_addr)) {\n-    \/\/ If a chunk is allocated after a GC started, but before relocate start\n-    \/\/ we can have an allocating chunk that isn't deeply good. That means that\n-    \/\/ the contained oops might be bad and require GC barriers.\n-    return true;\n-  }\n-\n-  \/\/ The chunk is allocating and its pointers are good. This chunk needs no\n-  \/\/ GC barriers\n-  return false;\n-}\n-\n-HeapWord* XCollectedHeap::allocate_new_tlab(size_t min_size, size_t requested_size, size_t* actual_size) {\n-  const size_t size_in_bytes = XUtils::words_to_bytes(align_object_size(requested_size));\n-  const uintptr_t addr = _heap.alloc_tlab(size_in_bytes);\n-\n-  if (addr != 0) {\n-    *actual_size = requested_size;\n-  }\n-\n-  return (HeapWord*)addr;\n-}\n-\n-oop XCollectedHeap::array_allocate(Klass* klass, size_t size, int length, bool do_zero, TRAPS) {\n-  XObjArrayAllocator allocator(klass, size, length, do_zero, THREAD);\n-  return allocator.allocate();\n-}\n-\n-HeapWord* XCollectedHeap::mem_allocate(size_t size, bool* gc_overhead_limit_was_exceeded) {\n-  const size_t size_in_bytes = XUtils::words_to_bytes(align_object_size(size));\n-  return (HeapWord*)_heap.alloc_object(size_in_bytes);\n-}\n-\n-MetaWord* XCollectedHeap::satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,\n-                                                             size_t size,\n-                                                             Metaspace::MetadataType mdtype) {\n-  \/\/ Start asynchronous GC\n-  collect(GCCause::_metadata_GC_threshold);\n-\n-  \/\/ Expand and retry allocation\n-  MetaWord* const result = loader_data->metaspace_non_null()->expand_and_allocate(size, mdtype);\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ As a last resort, try a critical allocation, riding on a synchronous full GC\n-  return MetaspaceCriticalAllocation::allocate(loader_data, size, mdtype);\n-}\n-\n-void XCollectedHeap::collect(GCCause::Cause cause) {\n-  _driver->collect(cause);\n-}\n-\n-void XCollectedHeap::collect_as_vm_thread(GCCause::Cause cause) {\n-  \/\/ These collection requests are ignored since ZGC can't run a synchronous\n-  \/\/ GC cycle from within the VM thread. This is considered benign, since the\n-  \/\/ only GC causes coming in here should be heap dumper and heap inspector.\n-  \/\/ If the heap dumper or heap inspector explicitly requests a gc and the\n-  \/\/ caller is not the VM thread a synchronous GC cycle is performed from the\n-  \/\/ caller thread in the prologue.\n-  assert(Thread::current()->is_VM_thread(), \"Should be the VM thread\");\n-  guarantee(cause == GCCause::_heap_dump ||\n-            cause == GCCause::_heap_inspection, \"Invalid cause\");\n-}\n-\n-void XCollectedHeap::do_full_collection(bool clear_all_soft_refs) {\n-  \/\/ Not supported\n-  ShouldNotReachHere();\n-}\n-\n-size_t XCollectedHeap::tlab_capacity(Thread* ignored) const {\n-  return _heap.tlab_capacity();\n-}\n-\n-size_t XCollectedHeap::tlab_used(Thread* ignored) const {\n-  return _heap.tlab_used();\n-}\n-\n-size_t XCollectedHeap::max_tlab_size() const {\n-  return _heap.max_tlab_size();\n-}\n-\n-size_t XCollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {\n-  return _heap.unsafe_max_tlab_alloc();\n-}\n-\n-MemoryUsage XCollectedHeap::memory_usage() {\n-  return _heap.serviceability_memory_pool()->get_memory_usage();\n-}\n-\n-GrowableArray<GCMemoryManager*> XCollectedHeap::memory_managers() {\n-  GrowableArray<GCMemoryManager*> memory_managers(2);\n-  memory_managers.append(_heap.serviceability_cycle_memory_manager());\n-  memory_managers.append(_heap.serviceability_pause_memory_manager());\n-  return memory_managers;\n-}\n-\n-GrowableArray<MemoryPool*> XCollectedHeap::memory_pools() {\n-  GrowableArray<MemoryPool*> memory_pools(1);\n-  memory_pools.append(_heap.serviceability_memory_pool());\n-  return memory_pools;\n-}\n-\n-void XCollectedHeap::object_iterate(ObjectClosure* cl) {\n-  _heap.object_iterate(cl, true \/* visit_weaks *\/);\n-}\n-\n-ParallelObjectIteratorImpl* XCollectedHeap::parallel_object_iterator(uint nworkers) {\n-  return _heap.parallel_object_iterator(nworkers, true \/* visit_weaks *\/);\n-}\n-\n-void XCollectedHeap::keep_alive(oop obj) {\n-  _heap.keep_alive(obj);\n-}\n-\n-void XCollectedHeap::register_nmethod(nmethod* nm) {\n-  XNMethod::register_nmethod(nm);\n-}\n-\n-void XCollectedHeap::unregister_nmethod(nmethod* nm) {\n-  XNMethod::unregister_nmethod(nm);\n-}\n-\n-void XCollectedHeap::verify_nmethod(nmethod* nm) {\n-  \/\/ Does nothing\n-}\n-\n-WorkerThreads* XCollectedHeap::safepoint_workers() {\n-  return _runtime_workers.workers();\n-}\n-\n-void XCollectedHeap::gc_threads_do(ThreadClosure* tc) const {\n-  tc->do_thread(_director);\n-  tc->do_thread(_driver);\n-  tc->do_thread(_stat);\n-  _heap.threads_do(tc);\n-  _runtime_workers.threads_do(tc);\n-}\n-\n-VirtualSpaceSummary XCollectedHeap::create_heap_space_summary() {\n-  return VirtualSpaceSummary((HeapWord*)0, (HeapWord*)capacity(), (HeapWord*)max_capacity());\n-}\n-\n-void XCollectedHeap::safepoint_synchronize_begin() {\n-  StackWatermarkSet::safepoint_synchronize_begin();\n-  SuspendibleThreadSet::synchronize();\n-}\n-\n-void XCollectedHeap::safepoint_synchronize_end() {\n-  SuspendibleThreadSet::desynchronize();\n-}\n-\n-void XCollectedHeap::pin_object(JavaThread* thread, oop obj) {\n-  GCLocker::lock_critical(thread);\n-}\n-\n-void XCollectedHeap::unpin_object(JavaThread* thread, oop obj) {\n-  GCLocker::unlock_critical(thread);\n-}\n-\n-void XCollectedHeap::prepare_for_verify() {\n-  \/\/ Does nothing\n-}\n-\n-void XCollectedHeap::print_on(outputStream* st) const {\n-  _heap.print_on(st);\n-}\n-\n-void XCollectedHeap::print_on_error(outputStream* st) const {\n-  st->print_cr(\"ZGC Globals:\");\n-  st->print_cr(\" GlobalPhase:       %u (%s)\", XGlobalPhase, XGlobalPhaseToString());\n-  st->print_cr(\" GlobalSeqNum:      %u\", XGlobalSeqNum);\n-  st->print_cr(\" Offset Max:        \" SIZE_FORMAT \"%s (\" PTR_FORMAT \")\",\n-               byte_size_in_exact_unit(XAddressOffsetMax),\n-               exact_unit_for_byte_size(XAddressOffsetMax),\n-               XAddressOffsetMax);\n-  st->print_cr(\" Page Size Small:   \" SIZE_FORMAT \"M\", XPageSizeSmall \/ M);\n-  st->print_cr(\" Page Size Medium:  \" SIZE_FORMAT \"M\", XPageSizeMedium \/ M);\n-  st->cr();\n-  st->print_cr(\"ZGC Metadata Bits:\");\n-  st->print_cr(\" Good:              \" PTR_FORMAT, XAddressGoodMask);\n-  st->print_cr(\" Bad:               \" PTR_FORMAT, XAddressBadMask);\n-  st->print_cr(\" WeakBad:           \" PTR_FORMAT, XAddressWeakBadMask);\n-  st->print_cr(\" Marked:            \" PTR_FORMAT, XAddressMetadataMarked);\n-  st->print_cr(\" Remapped:          \" PTR_FORMAT, XAddressMetadataRemapped);\n-  st->cr();\n-  CollectedHeap::print_on_error(st);\n-}\n-\n-void XCollectedHeap::print_extended_on(outputStream* st) const {\n-  _heap.print_extended_on(st);\n-}\n-\n-void XCollectedHeap::print_tracing_info() const {\n-  \/\/ Does nothing\n-}\n-\n-bool XCollectedHeap::print_location(outputStream* st, void* addr) const {\n-  return _heap.print_location(st, (uintptr_t)addr);\n-}\n-\n-void XCollectedHeap::verify(VerifyOption option \/* ignored *\/) {\n-  _heap.verify();\n-}\n-\n-bool XCollectedHeap::is_oop(oop object) const {\n-  return _heap.is_oop(XOop::to_address(object));\n-}\n-\n-bool XCollectedHeap::supports_concurrent_gc_breakpoints() const {\n-  return true;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xCollectedHeap.cpp","additions":0,"deletions":344,"binary":false,"changes":344,"status":"deleted"},{"patch":"@@ -1,127 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XCOLLECTEDHEAP_HPP\n-#define SHARE_GC_X_XCOLLECTEDHEAP_HPP\n-\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"gc\/shared\/softRefPolicy.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xHeap.hpp\"\n-#include \"gc\/x\/xInitialize.hpp\"\n-#include \"gc\/x\/xRuntimeWorkers.hpp\"\n-#include \"memory\/metaspace.hpp\"\n-#include \"services\/memoryUsage.hpp\"\n-\n-class VMStructs;\n-class XDirector;\n-class XDriver;\n-class XStat;\n-\n-class XCollectedHeap : public CollectedHeap {\n-  friend class ::VMStructs;\n-\n-private:\n-  XBarrierSet       _barrier_set;\n-  XInitialize       _initialize;\n-  XHeap             _heap;\n-  XDriver*          _driver;\n-  XDirector*        _director;\n-  XStat*            _stat;\n-  XRuntimeWorkers   _runtime_workers;\n-\n-  HeapWord* allocate_new_tlab(size_t min_size,\n-                              size_t requested_size,\n-                              size_t* actual_size) override;\n-\n-public:\n-  static XCollectedHeap* heap();\n-\n-  XCollectedHeap();\n-  Name kind() const override;\n-  const char* name() const override;\n-  jint initialize() override;\n-  void initialize_serviceability() override;\n-  void stop() override;\n-\n-  size_t max_capacity() const override;\n-  size_t capacity() const override;\n-  size_t used() const override;\n-  size_t unused() const override;\n-\n-  bool is_maximal_no_gc() const override;\n-  bool is_in(const void* p) const override;\n-  bool requires_barriers(stackChunkOop obj) const override;\n-\n-  oop array_allocate(Klass* klass, size_t size, int length, bool do_zero, TRAPS) override;\n-  HeapWord* mem_allocate(size_t size, bool* gc_overhead_limit_was_exceeded) override;\n-  MetaWord* satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,\n-                                               size_t size,\n-                                               Metaspace::MetadataType mdtype) override;\n-  void collect(GCCause::Cause cause) override;\n-  void collect_as_vm_thread(GCCause::Cause cause) override;\n-  void do_full_collection(bool clear_all_soft_refs) override;\n-\n-  size_t tlab_capacity(Thread* thr) const override;\n-  size_t tlab_used(Thread* thr) const override;\n-  size_t max_tlab_size() const override;\n-  size_t unsafe_max_tlab_alloc(Thread* thr) const override;\n-\n-  MemoryUsage memory_usage() override;\n-  GrowableArray<GCMemoryManager*> memory_managers() override;\n-  GrowableArray<MemoryPool*> memory_pools() override;\n-\n-  void object_iterate(ObjectClosure* cl) override;\n-  ParallelObjectIteratorImpl* parallel_object_iterator(uint nworkers) override;\n-\n-  void keep_alive(oop obj) override;\n-\n-  void register_nmethod(nmethod* nm) override;\n-  void unregister_nmethod(nmethod* nm) override;\n-  void verify_nmethod(nmethod* nmethod) override;\n-\n-  WorkerThreads* safepoint_workers() override;\n-\n-  void gc_threads_do(ThreadClosure* tc) const override;\n-\n-  VirtualSpaceSummary create_heap_space_summary() override;\n-\n-  void safepoint_synchronize_begin() override;\n-  void safepoint_synchronize_end() override;\n-\n-  void pin_object(JavaThread* thread, oop obj) override;\n-  void unpin_object(JavaThread* thread, oop obj) override;\n-\n-  void print_on(outputStream* st) const override;\n-  void print_on_error(outputStream* st) const override;\n-  void print_extended_on(outputStream* st) const override;\n-  void print_tracing_info() const override;\n-  bool print_location(outputStream* st, void* addr) const override;\n-\n-  void prepare_for_verify() override;\n-  void verify(VerifyOption option \/* ignored *\/) override;\n-  bool is_oop(oop object) const override;\n-  bool supports_concurrent_gc_breakpoints() const override;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XCOLLECTEDHEAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xCollectedHeap.hpp","additions":0,"deletions":127,"binary":false,"changes":127,"status":"deleted"},{"patch":"@@ -1,148 +0,0 @@\n-#\n-# GDB functions for debugging the Z Garbage Collector\n-#\n-\n-printf \"Loading zDebug.gdb\\n\"\n-\n-# Print Klass*\n-define zpk\n-    printf \"Klass: %s\\n\", (char*)((Klass*)($arg0))->_name->_body\n-end\n-\n-# Print oop\n-define zpo\n-    set $obj = (oopDesc*)($arg0)\n-\n-    printf \"Oop:   0x%016llx\\tState: \", (uintptr_t)$obj\n-    if ((uintptr_t)$obj & (uintptr_t)XAddressGoodMask)\n-        printf \"Good \"\n-        if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataRemapped)\n-            printf \"(Remapped)\"\n-        else\n-            if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataMarked)\n-                printf \"(Marked)\"\n-            else\n-                printf \"(Unknown)\"\n-            end\n-        end\n-    else\n-        printf \"Bad \"\n-        if ((uintptr_t)XAddressGoodMask & (uintptr_t)XAddressMetadataMarked)\n-            # Should be marked\n-            if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataRemapped)\n-                printf \"(Not Marked, Remapped)\"\n-            else\n-                printf \"(Not Marked, Not Remapped)\"\n-            end\n-        else\n-            if ((uintptr_t)XAddressGoodMask & (uintptr_t)XAddressMetadataRemapped)\n-                # Should be remapped\n-                if ((uintptr_t)$obj & (uintptr_t)XAddressMetadataMarked)\n-                    printf \"(Marked, Not Remapped)\"\n-                else\n-                    printf \"(Not Marked, Not Remapped)\"\n-                end\n-            else\n-                # Unknown\n-                printf \"(Unknown)\"\n-            end\n-        end\n-    end\n-    printf \"\\t Page: %llu\\n\", ((uintptr_t)$obj & XAddressOffsetMask) >> XGranuleSizeShift\n-    x\/16gx $obj\n-    if (UseCompressedClassPointers)\n-        set $klass = (Klass*)(void*)((uintptr_t)CompressedKlassPointers::_base +((uintptr_t)$obj->_metadata->_compressed_klass << CompressedKlassPointers::_shift))\n-    else\n-        set $klass = $obj->_metadata->_klass\n-    end\n-    printf \"Mark:  0x%016llx\\tKlass: %s\\n\", (uintptr_t)$obj->_mark, (char*)$klass->_name->_body\n-end\n-\n-# Print heap page by page table index\n-define zpp\n-    set $page = (XPage*)((uintptr_t)XHeap::_heap._page_table._map._map[($arg0)] & ~1)\n-    printf \"Page %p\\n\", $page\n-    print *$page\n-end\n-\n-# Print page_table\n-define zpt\n-    printf \"Pagetable (first 128 slots)\\n\"\n-    x\/128gx XHeap::_heap._page_table._map._map\n-end\n-\n-# Print live map\n-define __zmarked\n-    set $livemap   = $arg0\n-    set $bit        = $arg1\n-    set $size       = $livemap._bitmap._size\n-    set $segment    = $size \/ XLiveMap::nsegments\n-    set $segment_bit = 1 << $segment\n-\n-    printf \"Segment is \"\n-    if !($livemap._segment_live_bits & $segment_bit)\n-        printf \"NOT \"\n-    end\n-    printf \"live (segment %d)\\n\", $segment\n-\n-    if $bit >= $size\n-        print \"Error: Bit %z out of bounds (bitmap size %z)\\n\", $bit, $size\n-    else\n-        set $word_index = $bit \/ 64\n-        set $bit_index  = $bit % 64\n-        set $word       = $livemap._bitmap._map[$word_index]\n-        set $live_bit   = $word & (1 << $bit_index)\n-\n-        printf \"Object is \"\n-        if $live_bit == 0\n-            printf \"NOT \"\n-        end\n-        printf \"live (word index %d, bit index %d)\\n\", $word_index, $bit_index\n-    end\n-end\n-\n-define zmarked\n-    set $addr          = $arg0\n-    set $obj           = ((uintptr_t)$addr & XAddressOffsetMask)\n-    set $page_index    = $obj >> XGranuleSizeShift\n-    set $page_entry    = (uintptr_t)XHeap::_heap._page_table._map._map[$page_index]\n-    set $page          = (XPage*)($page_entry & ~1)\n-    set $page_start    = (uintptr_t)$page._virtual._start\n-    set $page_end      = (uintptr_t)$page._virtual._end\n-    set $page_seqnum   = $page._livemap._seqnum\n-    set $global_seqnum = XGlobalSeqNum\n-\n-    if $obj < $page_start || $obj >= $page_end\n-        printf \"Error: %p not in page %p (start %p, end %p)\\n\", $obj, $page, $page_start, $page_end\n-    else\n-        printf \"Page is \"\n-        if $page_seqnum != $global_seqnum\n-            printf \"NOT \"\n-        end\n-        printf \"live (page %p, page seqnum %d, global seqnum %d)\\n\", $page, $page_seqnum, $global_seqnum\n-\n-        #if $page_seqnum == $global_seqnum\n-            set $offset = $obj - $page_start\n-            set $bit = $offset \/ 8\n-            __zmarked $page._livemap $bit\n-        #end\n-    end\n-end\n-\n-# Print heap information\n-define zph\n-    printf \"Heap\\n\"\n-    printf \"     GlobalPhase:       %u\\n\", XGlobalPhase\n-    printf \"     GlobalSeqNum:      %u\\n\", XGlobalSeqNum\n-    printf \"     Offset Max:        %-15llu (0x%llx)\\n\", XAddressOffsetMax, XAddressOffsetMax\n-    printf \"     Page Size Small:   %-15llu (0x%llx)\\n\", XPageSizeSmall, XPageSizeSmall\n-    printf \"     Page Size Medium:  %-15llu (0x%llx)\\n\", XPageSizeMedium, XPageSizeMedium\n-    printf \"Metadata Bits\\n\"\n-    printf \"     Good:              0x%016llx\\n\", XAddressGoodMask\n-    printf \"     Bad:               0x%016llx\\n\", XAddressBadMask\n-    printf \"     WeakBad:           0x%016llx\\n\", XAddressWeakBadMask\n-    printf \"     Marked:            0x%016llx\\n\", XAddressMetadataMarked\n-    printf \"     Remapped:          0x%016llx\\n\", XAddressMetadataRemapped\n-end\n-\n-# End of file\n","filename":"src\/hotspot\/share\/gc\/x\/xDebug.gdb","additions":0,"deletions":148,"binary":false,"changes":148,"status":"deleted"},{"patch":"@@ -1,406 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xDirector.hpp\"\n-#include \"gc\/x\/xDriver.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xHeuristics.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"logging\/log.hpp\"\n-\n-constexpr double one_in_1000 = 3.290527;\n-constexpr double sample_interval = 1.0 \/ XStatAllocRate::sample_hz;\n-\n-XDirector::XDirector(XDriver* driver) :\n-    _driver(driver),\n-    _metronome(XStatAllocRate::sample_hz) {\n-  set_name(\"XDirector\");\n-  create_and_start();\n-}\n-\n-static void sample_allocation_rate() {\n-  \/\/ Sample allocation rate. This is needed by rule_allocation_rate()\n-  \/\/ below to estimate the time we have until we run out of memory.\n-  const double bytes_per_second = XStatAllocRate::sample_and_reset();\n-\n-  log_debug(gc, alloc)(\"Allocation Rate: %.1fMB\/s, Predicted: %.1fMB\/s, Avg: %.1f(+\/-%.1f)MB\/s\",\n-                       bytes_per_second \/ M,\n-                       XStatAllocRate::predict() \/ M,\n-                       XStatAllocRate::avg() \/ M,\n-                       XStatAllocRate::sd() \/ M);\n-}\n-\n-static XDriverRequest rule_allocation_stall() {\n-  \/\/ Perform GC if we've observed at least one allocation stall since\n-  \/\/ the last GC started.\n-  if (!XHeap::heap()->has_alloc_stalled()) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  log_debug(gc, director)(\"Rule: Allocation Stall Observed\");\n-\n-  return GCCause::_z_allocation_stall;\n-}\n-\n-static XDriverRequest rule_warmup() {\n-  if (XStatCycle::is_warm()) {\n-    \/\/ Rule disabled\n-    return GCCause::_no_gc;\n-  }\n-\n-  \/\/ Perform GC if heap usage passes 10\/20\/30% and no other GC has been\n-  \/\/ performed yet. This allows us to get some early samples of the GC\n-  \/\/ duration, which is needed by the other rules.\n-  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n-  const size_t used = XHeap::heap()->used();\n-  const double used_threshold_percent = (XStatCycle::nwarmup_cycles() + 1) * 0.1;\n-  const size_t used_threshold = soft_max_capacity * used_threshold_percent;\n-\n-  log_debug(gc, director)(\"Rule: Warmup %.0f%%, Used: \" SIZE_FORMAT \"MB, UsedThreshold: \" SIZE_FORMAT \"MB\",\n-                          used_threshold_percent * 100, used \/ M, used_threshold \/ M);\n-\n-  if (used < used_threshold) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  return GCCause::_z_warmup;\n-}\n-\n-static XDriverRequest rule_timer() {\n-  if (ZCollectionInterval <= 0) {\n-    \/\/ Rule disabled\n-    return GCCause::_no_gc;\n-  }\n-\n-  \/\/ Perform GC if timer has expired.\n-  const double time_since_last_gc = XStatCycle::time_since_last();\n-  const double time_until_gc = ZCollectionInterval - time_since_last_gc;\n-\n-  log_debug(gc, director)(\"Rule: Timer, Interval: %.3fs, TimeUntilGC: %.3fs\",\n-                          ZCollectionInterval, time_until_gc);\n-\n-  if (time_until_gc > 0) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  return GCCause::_z_timer;\n-}\n-\n-static double estimated_gc_workers(double serial_gc_time, double parallelizable_gc_time, double time_until_deadline) {\n-  const double parallelizable_time_until_deadline = MAX2(time_until_deadline - serial_gc_time, 0.001);\n-  return parallelizable_gc_time \/ parallelizable_time_until_deadline;\n-}\n-\n-static uint discrete_gc_workers(double gc_workers) {\n-  return clamp<uint>(ceil(gc_workers), 1, ConcGCThreads);\n-}\n-\n-static double select_gc_workers(double serial_gc_time, double parallelizable_gc_time, double alloc_rate_sd_percent, double time_until_oom) {\n-  \/\/ Use all workers until we're warm\n-  if (!XStatCycle::is_warm()) {\n-    const double not_warm_gc_workers = ConcGCThreads;\n-    log_debug(gc, director)(\"Select GC Workers (Not Warm), GCWorkers: %.3f\", not_warm_gc_workers);\n-    return not_warm_gc_workers;\n-  }\n-\n-  \/\/ Calculate number of GC workers needed to avoid a long GC cycle and to avoid OOM.\n-  const double avoid_long_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, 10 \/* seconds *\/);\n-  const double avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, time_until_oom);\n-\n-  const double gc_workers = MAX2(avoid_long_gc_workers, avoid_oom_gc_workers);\n-  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n-  const uint last_gc_workers = XStatCycle::last_active_workers();\n-\n-  \/\/ More than 15% division from the average is considered unsteady\n-  if (alloc_rate_sd_percent >= 0.15) {\n-    const double half_gc_workers = ConcGCThreads \/ 2.0;\n-    const double unsteady_gc_workers = MAX3<double>(gc_workers, last_gc_workers, half_gc_workers);\n-    log_debug(gc, director)(\"Select GC Workers (Unsteady), \"\n-                            \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, HalfGCWorkers: %.3f, GCWorkers: %.3f\",\n-                            avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, half_gc_workers, unsteady_gc_workers);\n-    return unsteady_gc_workers;\n-  }\n-\n-  if (actual_gc_workers < last_gc_workers) {\n-    \/\/ Before decreasing number of GC workers compared to the previous GC cycle, check if the\n-    \/\/ next GC cycle will need to increase it again. If so, use the same number of GC workers\n-    \/\/ that will be needed in the next cycle.\n-    const double gc_duration_delta = (parallelizable_gc_time \/ actual_gc_workers) - (parallelizable_gc_time \/ last_gc_workers);\n-    const double additional_time_for_allocations = XStatCycle::time_since_last() - gc_duration_delta - sample_interval;\n-    const double next_time_until_oom = time_until_oom + additional_time_for_allocations;\n-    const double next_avoid_oom_gc_workers = estimated_gc_workers(serial_gc_time, parallelizable_gc_time, next_time_until_oom);\n-\n-    \/\/ Add 0.5 to increase friction and avoid lowering too eagerly\n-    const double next_gc_workers = next_avoid_oom_gc_workers + 0.5;\n-    const double try_lowering_gc_workers = clamp<double>(next_gc_workers, actual_gc_workers, last_gc_workers);\n-\n-    log_debug(gc, director)(\"Select GC Workers (Try Lowering), \"\n-                           \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, NextAvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n-                            avoid_long_gc_workers, avoid_oom_gc_workers, next_avoid_oom_gc_workers, (double)last_gc_workers, try_lowering_gc_workers);\n-    return try_lowering_gc_workers;\n-  }\n-\n-  log_debug(gc, director)(\"Select GC Workers (Normal), \"\n-                         \"AvoidLongGCWorkers: %.3f, AvoidOOMGCWorkers: %.3f, LastGCWorkers: %.3f, GCWorkers: %.3f\",\n-                         avoid_long_gc_workers, avoid_oom_gc_workers, (double)last_gc_workers, gc_workers);\n-  return gc_workers;\n-}\n-\n-static XDriverRequest rule_allocation_rate_dynamic() {\n-  if (!XStatCycle::is_time_trustable()) {\n-    \/\/ Rule disabled\n-    return GCCause::_no_gc;\n-  }\n-\n-  \/\/ Calculate amount of free memory available. Note that we take the\n-  \/\/ relocation headroom into account to avoid in-place relocation.\n-  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n-  const size_t used = XHeap::heap()->used();\n-  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n-  const size_t free = free_including_headroom - MIN2(free_including_headroom, XHeuristics::relocation_headroom());\n-\n-  \/\/ Calculate time until OOM given the max allocation rate and the amount\n-  \/\/ of free memory. The allocation rate is a moving average and we multiply\n-  \/\/ that with an allocation spike tolerance factor to guard against unforeseen\n-  \/\/ phase changes in the allocate rate. We then add ~3.3 sigma to account for\n-  \/\/ the allocation rate variance, which means the probability is 1 in 1000\n-  \/\/ that a sample is outside of the confidence interval.\n-  const double alloc_rate_predict = XStatAllocRate::predict();\n-  const double alloc_rate_avg = XStatAllocRate::avg();\n-  const double alloc_rate_sd = XStatAllocRate::sd();\n-  const double alloc_rate_sd_percent = alloc_rate_sd \/ (alloc_rate_avg + 1.0);\n-  const double alloc_rate = (MAX2(alloc_rate_predict, alloc_rate_avg) * ZAllocationSpikeTolerance) + (alloc_rate_sd * one_in_1000) + 1.0;\n-  const double time_until_oom = (free \/ alloc_rate) \/ (1.0 + alloc_rate_sd_percent);\n-\n-  \/\/ Calculate max serial\/parallel times of a GC cycle. The times are\n-  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n-  const double serial_gc_time = XStatCycle::serial_time().davg() + (XStatCycle::serial_time().dsd() * one_in_1000);\n-  const double parallelizable_gc_time = XStatCycle::parallelizable_time().davg() + (XStatCycle::parallelizable_time().dsd() * one_in_1000);\n-\n-  \/\/ Calculate number of GC workers needed to avoid OOM.\n-  const double gc_workers = select_gc_workers(serial_gc_time, parallelizable_gc_time, alloc_rate_sd_percent, time_until_oom);\n-\n-  \/\/ Convert to a discrete number of GC workers within limits.\n-  const uint actual_gc_workers = discrete_gc_workers(gc_workers);\n-\n-  \/\/ Calculate GC duration given number of GC workers needed.\n-  const double actual_gc_duration = serial_gc_time + (parallelizable_gc_time \/ actual_gc_workers);\n-  const uint last_gc_workers = XStatCycle::last_active_workers();\n-\n-  \/\/ Calculate time until GC given the time until OOM and GC duration.\n-  \/\/ We also subtract the sample interval, so that we don't overshoot the\n-  \/\/ target time and end up starting the GC too late in the next interval.\n-  const double time_until_gc = time_until_oom - actual_gc_duration - sample_interval;\n-\n-  log_debug(gc, director)(\"Rule: Allocation Rate (Dynamic GC Workers), \"\n-                          \"MaxAllocRate: %.1fMB\/s (+\/-%.1f%%), Free: \" SIZE_FORMAT \"MB, GCCPUTime: %.3f, \"\n-                          \"GCDuration: %.3fs, TimeUntilOOM: %.3fs, TimeUntilGC: %.3fs, GCWorkers: %u -> %u\",\n-                          alloc_rate \/ M,\n-                          alloc_rate_sd_percent * 100,\n-                          free \/ M,\n-                          serial_gc_time + parallelizable_gc_time,\n-                          serial_gc_time + (parallelizable_gc_time \/ actual_gc_workers),\n-                          time_until_oom,\n-                          time_until_gc,\n-                          last_gc_workers,\n-                          actual_gc_workers);\n-\n-  if (actual_gc_workers <= last_gc_workers && time_until_gc > 0) {\n-    return XDriverRequest(GCCause::_no_gc, actual_gc_workers);\n-  }\n-\n-  return XDriverRequest(GCCause::_z_allocation_rate, actual_gc_workers);\n-}\n-\n-static XDriverRequest rule_allocation_rate_static() {\n-  if (!XStatCycle::is_time_trustable()) {\n-    \/\/ Rule disabled\n-    return GCCause::_no_gc;\n-  }\n-\n-  \/\/ Perform GC if the estimated max allocation rate indicates that we\n-  \/\/ will run out of memory. The estimated max allocation rate is based\n-  \/\/ on the moving average of the sampled allocation rate plus a safety\n-  \/\/ margin based on variations in the allocation rate and unforeseen\n-  \/\/ allocation spikes.\n-\n-  \/\/ Calculate amount of free memory available. Note that we take the\n-  \/\/ relocation headroom into account to avoid in-place relocation.\n-  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n-  const size_t used = XHeap::heap()->used();\n-  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n-  const size_t free = free_including_headroom - MIN2(free_including_headroom, XHeuristics::relocation_headroom());\n-\n-  \/\/ Calculate time until OOM given the max allocation rate and the amount\n-  \/\/ of free memory. The allocation rate is a moving average and we multiply\n-  \/\/ that with an allocation spike tolerance factor to guard against unforeseen\n-  \/\/ phase changes in the allocate rate. We then add ~3.3 sigma to account for\n-  \/\/ the allocation rate variance, which means the probability is 1 in 1000\n-  \/\/ that a sample is outside of the confidence interval.\n-  const double max_alloc_rate = (XStatAllocRate::avg() * ZAllocationSpikeTolerance) + (XStatAllocRate::sd() * one_in_1000);\n-  const double time_until_oom = free \/ (max_alloc_rate + 1.0); \/\/ Plus 1.0B\/s to avoid division by zero\n-\n-  \/\/ Calculate max serial\/parallel times of a GC cycle. The times are\n-  \/\/ moving averages, we add ~3.3 sigma to account for the variance.\n-  const double serial_gc_time = XStatCycle::serial_time().davg() + (XStatCycle::serial_time().dsd() * one_in_1000);\n-  const double parallelizable_gc_time = XStatCycle::parallelizable_time().davg() + (XStatCycle::parallelizable_time().dsd() * one_in_1000);\n-\n-  \/\/ Calculate GC duration given number of GC workers needed.\n-  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n-\n-  \/\/ Calculate time until GC given the time until OOM and max duration of GC.\n-  \/\/ We also deduct the sample interval, so that we don't overshoot the target\n-  \/\/ time and end up starting the GC too late in the next interval.\n-  const double time_until_gc = time_until_oom - gc_duration - sample_interval;\n-\n-  log_debug(gc, director)(\"Rule: Allocation Rate (Static GC Workers), MaxAllocRate: %.1fMB\/s, Free: \" SIZE_FORMAT \"MB, GCDuration: %.3fs, TimeUntilGC: %.3fs\",\n-                          max_alloc_rate \/ M, free \/ M, gc_duration, time_until_gc);\n-\n-  if (time_until_gc > 0) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  return GCCause::_z_allocation_rate;\n-}\n-\n-static XDriverRequest rule_allocation_rate() {\n-  if (UseDynamicNumberOfGCThreads) {\n-    return rule_allocation_rate_dynamic();\n-  } else {\n-    return rule_allocation_rate_static();\n-  }\n-}\n-\n-static XDriverRequest rule_high_usage() {\n-  \/\/ Perform GC if the amount of free memory is 5% or less. This is a preventive\n-  \/\/ meassure in the case where the application has a very low allocation rate,\n-  \/\/ such that the allocation rate rule doesn't trigger, but the amount of free\n-  \/\/ memory is still slowly but surely heading towards zero. In this situation,\n-  \/\/ we start a GC cycle to avoid a potential allocation stall later.\n-\n-  \/\/ Calculate amount of free memory available. Note that we take the\n-  \/\/ relocation headroom into account to avoid in-place relocation.\n-  const size_t soft_max_capacity = XHeap::heap()->soft_max_capacity();\n-  const size_t used = XHeap::heap()->used();\n-  const size_t free_including_headroom = soft_max_capacity - MIN2(soft_max_capacity, used);\n-  const size_t free = free_including_headroom - MIN2(free_including_headroom, XHeuristics::relocation_headroom());\n-  const double free_percent = percent_of(free, soft_max_capacity);\n-\n-  log_debug(gc, director)(\"Rule: High Usage, Free: \" SIZE_FORMAT \"MB(%.1f%%)\",\n-                          free \/ M, free_percent);\n-\n-  if (free_percent > 5.0) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  return GCCause::_z_high_usage;\n-}\n-\n-static XDriverRequest rule_proactive() {\n-  if (!ZProactive || !XStatCycle::is_warm()) {\n-    \/\/ Rule disabled\n-    return GCCause::_no_gc;\n-  }\n-\n-  \/\/ Perform GC if the impact of doing so, in terms of application throughput\n-  \/\/ reduction, is considered acceptable. This rule allows us to keep the heap\n-  \/\/ size down and allow reference processing to happen even when we have a lot\n-  \/\/ of free space on the heap.\n-\n-  \/\/ Only consider doing a proactive GC if the heap usage has grown by at least\n-  \/\/ 10% of the max capacity since the previous GC, or more than 5 minutes has\n-  \/\/ passed since the previous GC. This helps avoid superfluous GCs when running\n-  \/\/ applications with very low allocation rate.\n-  const size_t used_after_last_gc = XStatHeap::used_at_relocate_end();\n-  const size_t used_increase_threshold = XHeap::heap()->soft_max_capacity() * 0.10; \/\/ 10%\n-  const size_t used_threshold = used_after_last_gc + used_increase_threshold;\n-  const size_t used = XHeap::heap()->used();\n-  const double time_since_last_gc = XStatCycle::time_since_last();\n-  const double time_since_last_gc_threshold = 5 * 60; \/\/ 5 minutes\n-  if (used < used_threshold && time_since_last_gc < time_since_last_gc_threshold) {\n-    \/\/ Don't even consider doing a proactive GC\n-    log_debug(gc, director)(\"Rule: Proactive, UsedUntilEnabled: \" SIZE_FORMAT \"MB, TimeUntilEnabled: %.3fs\",\n-                            (used_threshold - used) \/ M,\n-                            time_since_last_gc_threshold - time_since_last_gc);\n-    return GCCause::_no_gc;\n-  }\n-\n-  const double assumed_throughput_drop_during_gc = 0.50; \/\/ 50%\n-  const double acceptable_throughput_drop = 0.01;        \/\/ 1%\n-  const double serial_gc_time = XStatCycle::serial_time().davg() + (XStatCycle::serial_time().dsd() * one_in_1000);\n-  const double parallelizable_gc_time = XStatCycle::parallelizable_time().davg() + (XStatCycle::parallelizable_time().dsd() * one_in_1000);\n-  const double gc_duration = serial_gc_time + (parallelizable_gc_time \/ ConcGCThreads);\n-  const double acceptable_gc_interval = gc_duration * ((assumed_throughput_drop_during_gc \/ acceptable_throughput_drop) - 1.0);\n-  const double time_until_gc = acceptable_gc_interval - time_since_last_gc;\n-\n-  log_debug(gc, director)(\"Rule: Proactive, AcceptableGCInterval: %.3fs, TimeSinceLastGC: %.3fs, TimeUntilGC: %.3fs\",\n-                          acceptable_gc_interval, time_since_last_gc, time_until_gc);\n-\n-  if (time_until_gc > 0) {\n-    return GCCause::_no_gc;\n-  }\n-\n-  return GCCause::_z_proactive;\n-}\n-\n-static XDriverRequest make_gc_decision() {\n-  \/\/ List of rules\n-  using XDirectorRule = XDriverRequest (*)();\n-  const XDirectorRule rules[] = {\n-    rule_allocation_stall,\n-    rule_warmup,\n-    rule_timer,\n-    rule_allocation_rate,\n-    rule_high_usage,\n-    rule_proactive,\n-  };\n-\n-  \/\/ Execute rules\n-  for (size_t i = 0; i < ARRAY_SIZE(rules); i++) {\n-    const XDriverRequest request = rules[i]();\n-    if (request.cause() != GCCause::_no_gc) {\n-      return request;\n-    }\n-  }\n-\n-  return GCCause::_no_gc;\n-}\n-\n-void XDirector::run_service() {\n-  \/\/ Main loop\n-  while (_metronome.wait_for_tick()) {\n-    sample_allocation_rate();\n-    if (!_driver->is_busy()) {\n-      const XDriverRequest request = make_gc_decision();\n-      if (request.cause() != GCCause::_no_gc) {\n-        _driver->collect(request);\n-      }\n-    }\n-  }\n-}\n-\n-void XDirector::stop_service() {\n-  _metronome.stop();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xDirector.cpp","additions":0,"deletions":406,"binary":false,"changes":406,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XDIRECTOR_HPP\n-#define SHARE_GC_X_XDIRECTOR_HPP\n-\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/x\/xMetronome.hpp\"\n-\n-class XDriver;\n-\n-class XDirector : public ConcurrentGCThread {\n-private:\n-  XDriver* const _driver;\n-  XMetronome     _metronome;\n-\n-protected:\n-  virtual void run_service();\n-  virtual void stop_service();\n-\n-public:\n-  XDirector(XDriver* driver);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XDIRECTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xDirector.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,518 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcId.hpp\"\n-#include \"gc\/shared\/gcLocker.hpp\"\n-#include \"gc\/shared\/gcVMOperations.hpp\"\n-#include \"gc\/shared\/isGCActiveMark.hpp\"\n-#include \"gc\/x\/xAbort.inline.hpp\"\n-#include \"gc\/x\/xBreakpoint.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xDriver.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xMessagePort.inline.hpp\"\n-#include \"gc\/x\/xServiceability.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xVerify.hpp\"\n-#include \"interpreter\/oopMapCache.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"runtime\/threads.hpp\"\n-#include \"runtime\/vmOperations.hpp\"\n-#include \"runtime\/vmThread.hpp\"\n-\n-static const XStatPhaseCycle      XPhaseCycle(\"Garbage Collection Cycle\");\n-static const XStatPhasePause      XPhasePauseMarkStart(\"Pause Mark Start\");\n-static const XStatPhaseConcurrent XPhaseConcurrentMark(\"Concurrent Mark\");\n-static const XStatPhaseConcurrent XPhaseConcurrentMarkContinue(\"Concurrent Mark Continue\");\n-static const XStatPhaseConcurrent XPhaseConcurrentMarkFree(\"Concurrent Mark Free\");\n-static const XStatPhasePause      XPhasePauseMarkEnd(\"Pause Mark End\");\n-static const XStatPhaseConcurrent XPhaseConcurrentProcessNonStrongReferences(\"Concurrent Process Non-Strong References\");\n-static const XStatPhaseConcurrent XPhaseConcurrentResetRelocationSet(\"Concurrent Reset Relocation Set\");\n-static const XStatPhaseConcurrent XPhaseConcurrentSelectRelocationSet(\"Concurrent Select Relocation Set\");\n-static const XStatPhasePause      XPhasePauseRelocateStart(\"Pause Relocate Start\");\n-static const XStatPhaseConcurrent XPhaseConcurrentRelocated(\"Concurrent Relocate\");\n-static const XStatCriticalPhase   XCriticalPhaseGCLockerStall(\"GC Locker Stall\", false \/* verbose *\/);\n-static const XStatSampler         XSamplerJavaThreads(\"System\", \"Java Threads\", XStatUnitThreads);\n-\n-XDriverRequest::XDriverRequest() :\n-    XDriverRequest(GCCause::_no_gc) {}\n-\n-XDriverRequest::XDriverRequest(GCCause::Cause cause) :\n-    XDriverRequest(cause, ConcGCThreads) {}\n-\n-XDriverRequest::XDriverRequest(GCCause::Cause cause, uint nworkers) :\n-    _cause(cause),\n-    _nworkers(nworkers) {}\n-\n-bool XDriverRequest::operator==(const XDriverRequest& other) const {\n-  return _cause == other._cause;\n-}\n-\n-GCCause::Cause XDriverRequest::cause() const {\n-  return _cause;\n-}\n-\n-uint XDriverRequest::nworkers() const {\n-  return _nworkers;\n-}\n-\n-class VM_XOperation : public VM_Operation {\n-private:\n-  const uint _gc_id;\n-  bool       _gc_locked;\n-  bool       _success;\n-\n-public:\n-  VM_XOperation() :\n-      _gc_id(GCId::current()),\n-      _gc_locked(false),\n-      _success(false) {}\n-\n-  virtual bool needs_inactive_gc_locker() const {\n-    \/\/ An inactive GC locker is needed in operations where we change the bad\n-    \/\/ mask or move objects. Changing the bad mask will invalidate all oops,\n-    \/\/ which makes it conceptually the same thing as moving all objects.\n-    return false;\n-  }\n-\n-  virtual bool skip_thread_oop_barriers() const {\n-    return true;\n-  }\n-\n-  virtual bool do_operation() = 0;\n-\n-  virtual bool doit_prologue() {\n-    Heap_lock->lock();\n-    return true;\n-  }\n-\n-  virtual void doit() {\n-    \/\/ Abort if GC locker state is incompatible\n-    if (needs_inactive_gc_locker() && GCLocker::check_active_before_gc()) {\n-      _gc_locked = true;\n-      return;\n-    }\n-\n-    \/\/ Setup GC id and active marker\n-    GCIdMark gc_id_mark(_gc_id);\n-    IsSTWGCActiveMark gc_active_mark;\n-\n-    \/\/ Verify before operation\n-    XVerify::before_zoperation();\n-\n-    \/\/ Execute operation\n-    _success = do_operation();\n-\n-    \/\/ Update statistics\n-    XStatSample(XSamplerJavaThreads, Threads::number_of_threads());\n-  }\n-\n-  virtual void doit_epilogue() {\n-    Heap_lock->unlock();\n-\n-    \/\/ GC thread root traversal likely used OopMapCache a lot, which\n-    \/\/ might have created lots of old entries. Trigger the cleanup now.\n-    OopMapCache::try_trigger_cleanup();\n-  }\n-\n-  bool gc_locked() const {\n-    return _gc_locked;\n-  }\n-\n-  bool success() const {\n-    return _success;\n-  }\n-};\n-\n-class VM_XMarkStart : public VM_XOperation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_XMarkStart;\n-  }\n-\n-  virtual bool needs_inactive_gc_locker() const {\n-    return true;\n-  }\n-\n-  virtual bool do_operation() {\n-    XStatTimer timer(XPhasePauseMarkStart);\n-    XServiceabilityPauseTracer tracer;\n-\n-    XCollectedHeap::heap()->increment_total_collections(true \/* full *\/);\n-\n-    XHeap::heap()->mark_start();\n-    return true;\n-  }\n-};\n-\n-class VM_XMarkEnd : public VM_XOperation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_XMarkEnd;\n-  }\n-\n-  virtual bool do_operation() {\n-    XStatTimer timer(XPhasePauseMarkEnd);\n-    XServiceabilityPauseTracer tracer;\n-    return XHeap::heap()->mark_end();\n-  }\n-};\n-\n-class VM_XRelocateStart : public VM_XOperation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_XRelocateStart;\n-  }\n-\n-  virtual bool needs_inactive_gc_locker() const {\n-    return true;\n-  }\n-\n-  virtual bool do_operation() {\n-    XStatTimer timer(XPhasePauseRelocateStart);\n-    XServiceabilityPauseTracer tracer;\n-    XHeap::heap()->relocate_start();\n-    return true;\n-  }\n-};\n-\n-class VM_XVerify : public VM_Operation {\n-public:\n-  virtual VMOp_Type type() const {\n-    return VMOp_XVerify;\n-  }\n-\n-  virtual bool skip_thread_oop_barriers() const {\n-    return true;\n-  }\n-\n-  virtual void doit() {\n-    XVerify::after_weak_processing();\n-  }\n-};\n-\n-XDriver::XDriver() :\n-    _gc_cycle_port(),\n-    _gc_locker_port() {\n-  set_name(\"XDriver\");\n-  create_and_start();\n-}\n-\n-bool XDriver::is_busy() const {\n-  return _gc_cycle_port.is_busy();\n-}\n-\n-void XDriver::collect(const XDriverRequest& request) {\n-  switch (request.cause()) {\n-  case GCCause::_heap_dump:\n-  case GCCause::_heap_inspection:\n-  case GCCause::_wb_young_gc:\n-  case GCCause::_wb_full_gc:\n-  case GCCause::_dcmd_gc_run:\n-  case GCCause::_java_lang_system_gc:\n-  case GCCause::_full_gc_alot:\n-  case GCCause::_scavenge_alot:\n-  case GCCause::_jvmti_force_gc:\n-  case GCCause::_metadata_GC_clear_soft_refs:\n-  case GCCause::_codecache_GC_aggressive:\n-    \/\/ Start synchronous GC\n-    _gc_cycle_port.send_sync(request);\n-    break;\n-\n-  case GCCause::_z_timer:\n-  case GCCause::_z_warmup:\n-  case GCCause::_z_allocation_rate:\n-  case GCCause::_z_allocation_stall:\n-  case GCCause::_z_proactive:\n-  case GCCause::_z_high_usage:\n-  case GCCause::_codecache_GC_threshold:\n-  case GCCause::_metadata_GC_threshold:\n-    \/\/ Start asynchronous GC\n-    _gc_cycle_port.send_async(request);\n-    break;\n-\n-  case GCCause::_gc_locker:\n-    \/\/ Restart VM operation previously blocked by the GC locker\n-    _gc_locker_port.signal();\n-    break;\n-\n-  case GCCause::_wb_breakpoint:\n-    XBreakpoint::start_gc();\n-    _gc_cycle_port.send_async(request);\n-    break;\n-\n-  default:\n-    \/\/ Other causes not supported\n-    fatal(\"Unsupported GC cause (%s)\", GCCause::to_string(request.cause()));\n-    break;\n-  }\n-}\n-\n-template <typename T>\n-bool XDriver::pause() {\n-  for (;;) {\n-    T op;\n-    VMThread::execute(&op);\n-    if (op.gc_locked()) {\n-      \/\/ Wait for GC to become unlocked and restart the VM operation\n-      XStatTimer timer(XCriticalPhaseGCLockerStall);\n-      _gc_locker_port.wait();\n-      continue;\n-    }\n-\n-    \/\/ Notify VM operation completed\n-    _gc_locker_port.ack();\n-\n-    return op.success();\n-  }\n-}\n-\n-void XDriver::pause_mark_start() {\n-  pause<VM_XMarkStart>();\n-}\n-\n-void XDriver::concurrent_mark() {\n-  XStatTimer timer(XPhaseConcurrentMark);\n-  XBreakpoint::at_after_marking_started();\n-  XHeap::heap()->mark(true \/* initial *\/);\n-  XBreakpoint::at_before_marking_completed();\n-}\n-\n-bool XDriver::pause_mark_end() {\n-  return pause<VM_XMarkEnd>();\n-}\n-\n-void XDriver::concurrent_mark_continue() {\n-  XStatTimer timer(XPhaseConcurrentMarkContinue);\n-  XHeap::heap()->mark(false \/* initial *\/);\n-}\n-\n-void XDriver::concurrent_mark_free() {\n-  XStatTimer timer(XPhaseConcurrentMarkFree);\n-  XHeap::heap()->mark_free();\n-}\n-\n-void XDriver::concurrent_process_non_strong_references() {\n-  XStatTimer timer(XPhaseConcurrentProcessNonStrongReferences);\n-  XBreakpoint::at_after_reference_processing_started();\n-  XHeap::heap()->process_non_strong_references();\n-}\n-\n-void XDriver::concurrent_reset_relocation_set() {\n-  XStatTimer timer(XPhaseConcurrentResetRelocationSet);\n-  XHeap::heap()->reset_relocation_set();\n-}\n-\n-void XDriver::pause_verify() {\n-  if (ZVerifyRoots || ZVerifyObjects) {\n-    VM_XVerify op;\n-    VMThread::execute(&op);\n-  }\n-}\n-\n-void XDriver::concurrent_select_relocation_set() {\n-  XStatTimer timer(XPhaseConcurrentSelectRelocationSet);\n-  XHeap::heap()->select_relocation_set();\n-}\n-\n-void XDriver::pause_relocate_start() {\n-  pause<VM_XRelocateStart>();\n-}\n-\n-void XDriver::concurrent_relocate() {\n-  XStatTimer timer(XPhaseConcurrentRelocated);\n-  XHeap::heap()->relocate();\n-}\n-\n-void XDriver::check_out_of_memory() {\n-  XHeap::heap()->check_out_of_memory();\n-}\n-\n-static bool should_clear_soft_references(const XDriverRequest& request) {\n-  \/\/ Clear soft references if implied by the GC cause\n-  if (request.cause() == GCCause::_wb_full_gc ||\n-      request.cause() == GCCause::_metadata_GC_clear_soft_refs ||\n-      request.cause() == GCCause::_z_allocation_stall) {\n-    \/\/ Clear\n-    return true;\n-  }\n-\n-  \/\/ Don't clear\n-  return false;\n-}\n-\n-static uint select_active_worker_threads_dynamic(const XDriverRequest& request) {\n-  \/\/ Use requested number of worker threads\n-  return request.nworkers();\n-}\n-\n-static uint select_active_worker_threads_static(const XDriverRequest& request) {\n-  const GCCause::Cause cause = request.cause();\n-  const uint nworkers = request.nworkers();\n-\n-  \/\/ Boost number of worker threads if implied by the GC cause\n-  if (cause == GCCause::_wb_full_gc ||\n-      cause == GCCause::_java_lang_system_gc ||\n-      cause == GCCause::_metadata_GC_clear_soft_refs ||\n-      cause == GCCause::_z_allocation_stall) {\n-    \/\/ Boost\n-    const uint boosted_nworkers = MAX2(nworkers, ParallelGCThreads);\n-    return boosted_nworkers;\n-  }\n-\n-  \/\/ Use requested number of worker threads\n-  return nworkers;\n-}\n-\n-static uint select_active_worker_threads(const XDriverRequest& request) {\n-  if (UseDynamicNumberOfGCThreads) {\n-    return select_active_worker_threads_dynamic(request);\n-  } else {\n-    return select_active_worker_threads_static(request);\n-  }\n-}\n-\n-class XDriverGCScope : public StackObj {\n-private:\n-  GCIdMark                   _gc_id;\n-  GCCause::Cause             _gc_cause;\n-  GCCauseSetter              _gc_cause_setter;\n-  XStatTimer                 _timer;\n-  XServiceabilityCycleTracer _tracer;\n-\n-public:\n-  XDriverGCScope(const XDriverRequest& request) :\n-      _gc_id(),\n-      _gc_cause(request.cause()),\n-      _gc_cause_setter(XCollectedHeap::heap(), _gc_cause),\n-      _timer(XPhaseCycle),\n-      _tracer() {\n-    \/\/ Update statistics\n-    XStatCycle::at_start();\n-\n-    \/\/ Set up soft reference policy\n-    const bool clear = should_clear_soft_references(request);\n-    XHeap::heap()->set_soft_reference_policy(clear);\n-\n-    \/\/ Select number of worker threads to use\n-    const uint nworkers = select_active_worker_threads(request);\n-    XHeap::heap()->set_active_workers(nworkers);\n-  }\n-\n-  ~XDriverGCScope() {\n-    \/\/ Update statistics\n-    XStatCycle::at_end(_gc_cause, XHeap::heap()->active_workers());\n-\n-    \/\/ Update data used by soft reference policy\n-    Universe::heap()->update_capacity_and_used_at_gc();\n-\n-    \/\/ Signal that we have completed a visit to all live objects\n-    Universe::heap()->record_whole_heap_examined_timestamp();\n-  }\n-};\n-\n-\/\/ Macro to execute a termination check after a concurrent phase. Note\n-\/\/ that it's important that the termination check comes after the call\n-\/\/ to the function f, since we can't abort between pause_relocate_start()\n-\/\/ and concurrent_relocate(). We need to let concurrent_relocate() call\n-\/\/ abort_page() on the remaining entries in the relocation set.\n-#define concurrent(f)                 \\\n-  do {                                \\\n-    concurrent_##f();                 \\\n-    if (should_terminate()) {         \\\n-      return;                         \\\n-    }                                 \\\n-  } while (false)\n-\n-void XDriver::gc(const XDriverRequest& request) {\n-  XDriverGCScope scope(request);\n-\n-  \/\/ Phase 1: Pause Mark Start\n-  pause_mark_start();\n-\n-  \/\/ Phase 2: Concurrent Mark\n-  concurrent(mark);\n-\n-  \/\/ Phase 3: Pause Mark End\n-  while (!pause_mark_end()) {\n-    \/\/ Phase 3.5: Concurrent Mark Continue\n-    concurrent(mark_continue);\n-  }\n-\n-  \/\/ Phase 4: Concurrent Mark Free\n-  concurrent(mark_free);\n-\n-  \/\/ Phase 5: Concurrent Process Non-Strong References\n-  concurrent(process_non_strong_references);\n-\n-  \/\/ Phase 6: Concurrent Reset Relocation Set\n-  concurrent(reset_relocation_set);\n-\n-  \/\/ Phase 7: Pause Verify\n-  pause_verify();\n-\n-  \/\/ Phase 8: Concurrent Select Relocation Set\n-  concurrent(select_relocation_set);\n-\n-  \/\/ Phase 9: Pause Relocate Start\n-  pause_relocate_start();\n-\n-  \/\/ Phase 10: Concurrent Relocate\n-  concurrent(relocate);\n-}\n-\n-void XDriver::run_service() {\n-  \/\/ Main loop\n-  while (!should_terminate()) {\n-    \/\/ Wait for GC request\n-    const XDriverRequest request = _gc_cycle_port.receive();\n-    if (request.cause() == GCCause::_no_gc) {\n-      continue;\n-    }\n-\n-    XBreakpoint::at_before_gc();\n-\n-    \/\/ Run GC\n-    gc(request);\n-\n-    if (should_terminate()) {\n-      \/\/ Abort\n-      break;\n-    }\n-\n-    \/\/ Notify GC completed\n-    _gc_cycle_port.ack();\n-\n-    \/\/ Check for out of memory condition\n-    check_out_of_memory();\n-\n-    XBreakpoint::at_after_gc();\n-  }\n-}\n-\n-void XDriver::stop_service() {\n-  XAbort::abort();\n-  _gc_cycle_port.send_async(GCCause::_no_gc);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xDriver.cpp","additions":0,"deletions":518,"binary":false,"changes":518,"status":"deleted"},{"patch":"@@ -1,84 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XDRIVER_HPP\n-#define SHARE_GC_X_XDRIVER_HPP\n-\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/shared\/gcCause.hpp\"\n-#include \"gc\/x\/xMessagePort.hpp\"\n-\n-class VM_XOperation;\n-\n-class XDriverRequest {\n-private:\n-  GCCause::Cause _cause;\n-  uint           _nworkers;\n-\n-public:\n-  XDriverRequest();\n-  XDriverRequest(GCCause::Cause cause);\n-  XDriverRequest(GCCause::Cause cause, uint nworkers);\n-\n-  bool operator==(const XDriverRequest& other) const;\n-\n-  GCCause::Cause cause() const;\n-  uint nworkers() const;\n-};\n-\n-class XDriver : public ConcurrentGCThread {\n-private:\n-  XMessagePort<XDriverRequest> _gc_cycle_port;\n-  XRendezvousPort              _gc_locker_port;\n-\n-  template <typename T> bool pause();\n-\n-  void pause_mark_start();\n-  void concurrent_mark();\n-  bool pause_mark_end();\n-  void concurrent_mark_continue();\n-  void concurrent_mark_free();\n-  void concurrent_process_non_strong_references();\n-  void concurrent_reset_relocation_set();\n-  void pause_verify();\n-  void concurrent_select_relocation_set();\n-  void pause_relocate_start();\n-  void concurrent_relocate();\n-\n-  void check_out_of_memory();\n-\n-  void gc(const XDriverRequest& request);\n-\n-protected:\n-  virtual void run_service();\n-  virtual void stop_service();\n-\n-public:\n-  XDriver();\n-\n-  bool is_busy() const;\n-\n-  void collect(const XDriverRequest& request);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XDRIVER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xDriver.hpp","additions":0,"deletions":84,"binary":false,"changes":84,"status":"deleted"},{"patch":"@@ -1,51 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xErrno.hpp\"\n-#include \"runtime\/os.hpp\"\n-\n-#include <errno.h>\n-#include <string.h>\n-\n-XErrno::XErrno() :\n-    _error(errno) {}\n-\n-XErrno::XErrno(int error) :\n-    _error(error) {}\n-\n-XErrno::operator bool() const {\n-  return _error != 0;\n-}\n-\n-bool XErrno::operator==(int error) const {\n-  return _error == error;\n-}\n-\n-bool XErrno::operator!=(int error) const {\n-  return _error != error;\n-}\n-\n-const char* XErrno::to_string() const {\n-  return os::strerror(_error);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xErrno.cpp","additions":0,"deletions":51,"binary":false,"changes":51,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XERRNO_HPP\n-#define SHARE_GC_X_XERRNO_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-\n-class XErrno : public StackObj {\n-private:\n-  const int _error;\n-\n-public:\n-  XErrno();\n-  XErrno(int error);\n-\n-  operator bool() const;\n-  bool operator==(int error) const;\n-  bool operator!=(int error) const;\n-  const char* to_string() const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XERRNO_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xErrno.hpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,210 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xForwarding.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xUtils.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-\/\/\n-\/\/ Reference count states:\n-\/\/\n-\/\/ * If the reference count is zero, it will never change again.\n-\/\/\n-\/\/ * If the reference count is positive, it can be both retained\n-\/\/   (increased) and released (decreased).\n-\/\/\n-\/\/ * If the reference count is negative, is can only be released\n-\/\/   (increased). A negative reference count means that one or more\n-\/\/   threads are waiting for one or more other threads to release\n-\/\/   their references.\n-\/\/\n-\/\/ The reference lock is used for waiting until the reference\n-\/\/ count has become zero (released) or negative one (claimed).\n-\/\/\n-\n-static const XStatCriticalPhase XCriticalPhaseRelocationStall(\"Relocation Stall\");\n-\n-bool XForwarding::retain_page() {\n-  for (;;) {\n-    const int32_t ref_count = Atomic::load_acquire(&_ref_count);\n-\n-    if (ref_count == 0) {\n-      \/\/ Released\n-      return false;\n-    }\n-\n-    if (ref_count < 0) {\n-      \/\/ Claimed\n-      const bool success = wait_page_released();\n-      assert(success, \"Should always succeed\");\n-      return false;\n-    }\n-\n-    if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) == ref_count) {\n-      \/\/ Retained\n-      return true;\n-    }\n-  }\n-}\n-\n-XPage* XForwarding::claim_page() {\n-  for (;;) {\n-    const int32_t ref_count = Atomic::load(&_ref_count);\n-    assert(ref_count > 0, \"Invalid state\");\n-\n-    \/\/ Invert reference count\n-    if (Atomic::cmpxchg(&_ref_count, ref_count, -ref_count) != ref_count) {\n-      continue;\n-    }\n-\n-    \/\/ If the previous reference count was 1, then we just changed it to -1,\n-    \/\/ and we have now claimed the page. Otherwise we wait until it is claimed.\n-    if (ref_count != 1) {\n-      XLocker<XConditionLock> locker(&_ref_lock);\n-      while (Atomic::load_acquire(&_ref_count) != -1) {\n-        _ref_lock.wait();\n-      }\n-    }\n-\n-    return _page;\n-  }\n-}\n-\n-void XForwarding::release_page() {\n-  for (;;) {\n-    const int32_t ref_count = Atomic::load(&_ref_count);\n-    assert(ref_count != 0, \"Invalid state\");\n-\n-    if (ref_count > 0) {\n-      \/\/ Decrement reference count\n-      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count - 1) != ref_count) {\n-        continue;\n-      }\n-\n-      \/\/ If the previous reference count was 1, then we just decremented\n-      \/\/ it to 0 and we should signal that the page is now released.\n-      if (ref_count == 1) {\n-        \/\/ Notify released\n-        XLocker<XConditionLock> locker(&_ref_lock);\n-        _ref_lock.notify_all();\n-      }\n-    } else {\n-      \/\/ Increment reference count\n-      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) != ref_count) {\n-        continue;\n-      }\n-\n-      \/\/ If the previous reference count was -2 or -1, then we just incremented it\n-      \/\/ to -1 or 0, and we should signal the that page is now claimed or released.\n-      if (ref_count == -2 || ref_count == -1) {\n-        \/\/ Notify claimed or released\n-        XLocker<XConditionLock> locker(&_ref_lock);\n-        _ref_lock.notify_all();\n-      }\n-    }\n-\n-    return;\n-  }\n-}\n-\n-bool XForwarding::wait_page_released() const {\n-  if (Atomic::load_acquire(&_ref_count) != 0) {\n-    XStatTimer timer(XCriticalPhaseRelocationStall);\n-    XLocker<XConditionLock> locker(&_ref_lock);\n-    while (Atomic::load_acquire(&_ref_count) != 0) {\n-      if (_ref_abort) {\n-        return false;\n-      }\n-\n-      _ref_lock.wait();\n-    }\n-  }\n-\n-  return true;\n-}\n-\n-XPage* XForwarding::detach_page() {\n-  \/\/ Wait until released\n-  if (Atomic::load_acquire(&_ref_count) != 0) {\n-    XLocker<XConditionLock> locker(&_ref_lock);\n-    while (Atomic::load_acquire(&_ref_count) != 0) {\n-      _ref_lock.wait();\n-    }\n-  }\n-\n-  \/\/ Detach and return page\n-  XPage* const page = _page;\n-  _page = nullptr;\n-  return page;\n-}\n-\n-void XForwarding::abort_page() {\n-  XLocker<XConditionLock> locker(&_ref_lock);\n-  assert(Atomic::load(&_ref_count) > 0, \"Invalid state\");\n-  assert(!_ref_abort, \"Invalid state\");\n-  _ref_abort = true;\n-  _ref_lock.notify_all();\n-}\n-\n-void XForwarding::verify() const {\n-  guarantee(_ref_count != 0, \"Invalid reference count\");\n-  guarantee(_page != nullptr, \"Invalid page\");\n-\n-  uint32_t live_objects = 0;\n-  size_t live_bytes = 0;\n-\n-  for (XForwardingCursor i = 0; i < _entries.length(); i++) {\n-    const XForwardingEntry entry = at(&i);\n-    if (!entry.populated()) {\n-      \/\/ Skip empty entries\n-      continue;\n-    }\n-\n-    \/\/ Check from index\n-    guarantee(entry.from_index() < _page->object_max_count(), \"Invalid from index\");\n-\n-    \/\/ Check for duplicates\n-    for (XForwardingCursor j = i + 1; j < _entries.length(); j++) {\n-      const XForwardingEntry other = at(&j);\n-      if (!other.populated()) {\n-        \/\/ Skip empty entries\n-        continue;\n-      }\n-\n-      guarantee(entry.from_index() != other.from_index(), \"Duplicate from\");\n-      guarantee(entry.to_offset() != other.to_offset(), \"Duplicate to\");\n-    }\n-\n-    const uintptr_t to_addr = XAddress::good(entry.to_offset());\n-    const size_t size = XUtils::object_size(to_addr);\n-    const size_t aligned_size = align_up(size, _page->object_alignment());\n-    live_bytes += aligned_size;\n-    live_objects++;\n-  }\n-\n-  \/\/ Verify number of live objects and bytes\n-  _page->verify_live(live_objects, live_bytes);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xForwarding.cpp","additions":0,"deletions":210,"binary":false,"changes":210,"status":"deleted"},{"patch":"@@ -1,88 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDING_HPP\n-#define SHARE_GC_X_XFORWARDING_HPP\n-\n-#include \"gc\/x\/xAttachedArray.hpp\"\n-#include \"gc\/x\/xForwardingEntry.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-#include \"gc\/x\/xVirtualMemory.hpp\"\n-\n-class ObjectClosure;\n-class VMStructs;\n-class XForwardingAllocator;\n-class XPage;\n-\n-typedef size_t XForwardingCursor;\n-\n-class XForwarding {\n-  friend class ::VMStructs;\n-  friend class XForwardingTest;\n-\n-private:\n-  typedef XAttachedArray<XForwarding, XForwardingEntry> AttachedArray;\n-\n-  const XVirtualMemory   _virtual;\n-  const size_t           _object_alignment_shift;\n-  const AttachedArray    _entries;\n-  XPage*                 _page;\n-  mutable XConditionLock _ref_lock;\n-  volatile int32_t       _ref_count;\n-  bool                   _ref_abort;\n-  bool                   _in_place;\n-\n-  XForwardingEntry* entries() const;\n-  XForwardingEntry at(XForwardingCursor* cursor) const;\n-  XForwardingEntry first(uintptr_t from_index, XForwardingCursor* cursor) const;\n-  XForwardingEntry next(XForwardingCursor* cursor) const;\n-\n-  XForwarding(XPage* page, size_t nentries);\n-\n-public:\n-  static uint32_t nentries(const XPage* page);\n-  static XForwarding* alloc(XForwardingAllocator* allocator, XPage* page);\n-\n-  uint8_t type() const;\n-  uintptr_t start() const;\n-  size_t size() const;\n-  size_t object_alignment_shift() const;\n-  void object_iterate(ObjectClosure *cl);\n-\n-  bool retain_page();\n-  XPage* claim_page();\n-  void release_page();\n-  bool wait_page_released() const;\n-  XPage* detach_page();\n-  void abort_page();\n-\n-  void set_in_place();\n-  bool in_place() const;\n-\n-  XForwardingEntry find(uintptr_t from_index, XForwardingCursor* cursor) const;\n-  uintptr_t insert(uintptr_t from_index, uintptr_t to_offset, XForwardingCursor* cursor);\n-\n-  void verify() const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwarding.hpp","additions":0,"deletions":88,"binary":false,"changes":88,"status":"deleted"},{"patch":"@@ -1,163 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDING_INLINE_HPP\n-#define SHARE_GC_X_XFORWARDING_INLINE_HPP\n-\n-#include \"gc\/x\/xForwarding.hpp\"\n-\n-#include \"gc\/x\/xAttachedArray.inline.hpp\"\n-#include \"gc\/x\/xForwardingAllocator.inline.hpp\"\n-#include \"gc\/x\/xHash.inline.hpp\"\n-#include \"gc\/x\/xHeap.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-inline uint32_t XForwarding::nentries(const XPage* page) {\n-  \/\/ The number returned by the function is used to size the hash table of\n-  \/\/ forwarding entries for this page. This hash table uses linear probing.\n-  \/\/ The size of the table must be a power of two to allow for quick and\n-  \/\/ inexpensive indexing\/masking. The table is also sized to have a load\n-  \/\/ factor of 50%, i.e. sized to have double the number of entries actually\n-  \/\/ inserted, to allow for good lookup\/insert performance.\n-  return round_up_power_of_2(page->live_objects() * 2);\n-}\n-\n-inline XForwarding* XForwarding::alloc(XForwardingAllocator* allocator, XPage* page) {\n-  const size_t nentries = XForwarding::nentries(page);\n-  void* const addr = AttachedArray::alloc(allocator, nentries);\n-  return ::new (addr) XForwarding(page, nentries);\n-}\n-\n-inline XForwarding::XForwarding(XPage* page, size_t nentries) :\n-    _virtual(page->virtual_memory()),\n-    _object_alignment_shift(page->object_alignment_shift()),\n-    _entries(nentries),\n-    _page(page),\n-    _ref_lock(),\n-    _ref_count(1),\n-    _ref_abort(false),\n-    _in_place(false) {}\n-\n-inline uint8_t XForwarding::type() const {\n-  return _page->type();\n-}\n-\n-inline uintptr_t XForwarding::start() const {\n-  return _virtual.start();\n-}\n-\n-inline size_t XForwarding::size() const {\n-  return _virtual.size();\n-}\n-\n-inline size_t XForwarding::object_alignment_shift() const {\n-  return _object_alignment_shift;\n-}\n-\n-inline void XForwarding::object_iterate(ObjectClosure *cl) {\n-  return _page->object_iterate(cl);\n-}\n-\n-inline void XForwarding::set_in_place() {\n-  _in_place = true;\n-}\n-\n-inline bool XForwarding::in_place() const {\n-  return _in_place;\n-}\n-\n-inline XForwardingEntry* XForwarding::entries() const {\n-  return _entries(this);\n-}\n-\n-inline XForwardingEntry XForwarding::at(XForwardingCursor* cursor) const {\n-  \/\/ Load acquire for correctness with regards to\n-  \/\/ accesses to the contents of the forwarded object.\n-  return Atomic::load_acquire(entries() + *cursor);\n-}\n-\n-inline XForwardingEntry XForwarding::first(uintptr_t from_index, XForwardingCursor* cursor) const {\n-  const size_t mask = _entries.length() - 1;\n-  const size_t hash = XHash::uint32_to_uint32((uint32_t)from_index);\n-  *cursor = hash & mask;\n-  return at(cursor);\n-}\n-\n-inline XForwardingEntry XForwarding::next(XForwardingCursor* cursor) const {\n-  const size_t mask = _entries.length() - 1;\n-  *cursor = (*cursor + 1) & mask;\n-  return at(cursor);\n-}\n-\n-inline XForwardingEntry XForwarding::find(uintptr_t from_index, XForwardingCursor* cursor) const {\n-  \/\/ Reading entries in the table races with the atomic CAS done for\n-  \/\/ insertion into the table. This is safe because each entry is at\n-  \/\/ most updated once (from zero to something else).\n-  XForwardingEntry entry = first(from_index, cursor);\n-  while (entry.populated()) {\n-    if (entry.from_index() == from_index) {\n-      \/\/ Match found, return matching entry\n-      return entry;\n-    }\n-\n-    entry = next(cursor);\n-  }\n-\n-  \/\/ Match not found, return empty entry\n-  return entry;\n-}\n-\n-inline uintptr_t XForwarding::insert(uintptr_t from_index, uintptr_t to_offset, XForwardingCursor* cursor) {\n-  const XForwardingEntry new_entry(from_index, to_offset);\n-  const XForwardingEntry old_entry; \/\/ Empty\n-\n-  \/\/ Make sure that object copy is finished\n-  \/\/ before forwarding table installation\n-  OrderAccess::release();\n-\n-  for (;;) {\n-    const XForwardingEntry prev_entry = Atomic::cmpxchg(entries() + *cursor, old_entry, new_entry, memory_order_relaxed);\n-    if (!prev_entry.populated()) {\n-      \/\/ Success\n-      return to_offset;\n-    }\n-\n-    \/\/ Find next empty or matching entry\n-    XForwardingEntry entry = at(cursor);\n-    while (entry.populated()) {\n-      if (entry.from_index() == from_index) {\n-        \/\/ Match found, return already inserted address\n-        return entry.to_offset();\n-      }\n-\n-      entry = next(cursor);\n-    }\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwarding.inline.hpp","additions":0,"deletions":163,"binary":false,"changes":163,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xForwardingAllocator.hpp\"\n-#include \"memory\/allocation.inline.hpp\"\n-\n-XForwardingAllocator::XForwardingAllocator() :\n-    _start(nullptr),\n-    _end(nullptr),\n-    _top(nullptr) {}\n-\n-XForwardingAllocator::~XForwardingAllocator() {\n-  FREE_C_HEAP_ARRAY(char, _start);\n-}\n-\n-void XForwardingAllocator::reset(size_t size) {\n-  _start = _top = REALLOC_C_HEAP_ARRAY(char, _start, size, mtGC);\n-  _end = _start + size;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingAllocator.cpp","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDINGALLOCATOR_HPP\n-#define SHARE_GC_X_XFORWARDINGALLOCATOR_HPP\n-\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XForwardingAllocator {\n-private:\n-  char* _start;\n-  char* _end;\n-  char* _top;\n-\n-public:\n-  XForwardingAllocator();\n-  ~XForwardingAllocator();\n-\n-  void reset(size_t size);\n-  size_t size() const;\n-  bool is_full() const;\n-\n-  void* alloc(size_t size);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDINGALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingAllocator.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDINGALLOCATOR_INLINE_HPP\n-#define SHARE_GC_X_XFORWARDINGALLOCATOR_INLINE_HPP\n-\n-#include \"gc\/x\/xForwardingAllocator.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline size_t XForwardingAllocator::size() const {\n-  return _end - _start;\n-}\n-\n-inline bool XForwardingAllocator::is_full() const {\n-  return _top == _end;\n-}\n-\n-inline void* XForwardingAllocator::alloc(size_t size) {\n-  char* const addr = Atomic::fetch_then_add(&_top, size);\n-  assert(addr + size <= _end, \"Allocation should never fail\");\n-  return addr;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDINGALLOCATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingAllocator.inline.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -1,102 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDINGENTRY_HPP\n-#define SHARE_GC_X_XFORWARDINGENTRY_HPP\n-\n-#include \"gc\/x\/xBitField.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"metaprogramming\/primitiveConversions.hpp\"\n-\n-#include <type_traits>\n-\n-class VMStructs;\n-\n-\/\/\n-\/\/ Forwarding entry layout\n-\/\/ -----------------------\n-\/\/\n-\/\/   6                  4 4\n-\/\/   3                  6 5                                                1 0\n-\/\/  +--------------------+--------------------------------------------------+-+\n-\/\/  |11111111 11111111 11|111111 11111111 11111111 11111111 11111111 1111111|1|\n-\/\/  +--------------------+--------------------------------------------------+-+\n-\/\/  |                    |                                                  |\n-\/\/  |                    |                      0-0 Populated Flag (1-bits) *\n-\/\/  |                    |\n-\/\/  |                    * 45-1 To Object Offset (45-bits)\n-\/\/  |\n-\/\/  * 63-46 From Object Index (18-bits)\n-\/\/\n-\n-class XForwardingEntry {\n-  friend struct PrimitiveConversions::Translate<XForwardingEntry>;\n-  friend class ::VMStructs;\n-\n-private:\n-  typedef XBitField<uint64_t, bool,   0,   1> field_populated;\n-  typedef XBitField<uint64_t, size_t, 1,  45> field_to_offset;\n-  typedef XBitField<uint64_t, size_t, 46, 18> field_from_index;\n-\n-  uint64_t _entry;\n-\n-public:\n-  XForwardingEntry() :\n-      _entry(0) {}\n-\n-  XForwardingEntry(size_t from_index, size_t to_offset) :\n-      _entry(field_populated::encode(true) |\n-             field_to_offset::encode(to_offset) |\n-             field_from_index::encode(from_index)) {}\n-\n-  bool populated() const {\n-    return field_populated::decode(_entry);\n-  }\n-\n-  size_t to_offset() const {\n-    return field_to_offset::decode(_entry);\n-  }\n-\n-  size_t from_index() const {\n-    return field_from_index::decode(_entry);\n-  }\n-};\n-\n-\/\/ Needed to allow atomic operations on XForwardingEntry\n-template <>\n-struct PrimitiveConversions::Translate<XForwardingEntry> : public std::true_type {\n-  typedef XForwardingEntry Value;\n-  typedef uint64_t                  Decayed;\n-\n-  static Decayed decay(Value v) {\n-    return v._entry;\n-  }\n-\n-  static Value recover(Decayed d) {\n-    XForwardingEntry entry;\n-    entry._entry = d;\n-    return entry;\n-  }\n-};\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDINGENTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingEntry.hpp","additions":0,"deletions":102,"binary":false,"changes":102,"status":"deleted"},{"patch":"@@ -1,47 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDINGTABLE_HPP\n-#define SHARE_GC_X_XFORWARDINGTABLE_HPP\n-\n-#include \"gc\/x\/xGranuleMap.hpp\"\n-\n-class VMStructs;\n-class XForwarding;\n-\n-class XForwardingTable {\n-  friend class ::VMStructs;\n-\n-private:\n-  XGranuleMap<XForwarding*> _map;\n-\n-public:\n-  XForwardingTable();\n-\n-  XForwarding* get(uintptr_t addr) const;\n-\n-  void insert(XForwarding* forwarding);\n-  void remove(XForwarding* forwarding);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDINGTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingTable.hpp","additions":0,"deletions":47,"binary":false,"changes":47,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFORWARDINGTABLE_INLINE_HPP\n-#define SHARE_GC_X_XFORWARDINGTABLE_INLINE_HPP\n-\n-#include \"gc\/x\/xForwardingTable.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xForwarding.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xGranuleMap.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline XForwardingTable::XForwardingTable() :\n-    _map(XAddressOffsetMax) {}\n-\n-inline XForwarding* XForwardingTable::get(uintptr_t addr) const {\n-  assert(!XAddress::is_null(addr), \"Invalid address\");\n-  return _map.get(XAddress::offset(addr));\n-}\n-\n-inline void XForwardingTable::insert(XForwarding* forwarding) {\n-  const uintptr_t offset = forwarding->start();\n-  const size_t size = forwarding->size();\n-\n-  assert(_map.get(offset) == nullptr, \"Invalid entry\");\n-  _map.put(offset, size, forwarding);\n-}\n-\n-inline void XForwardingTable::remove(XForwarding* forwarding) {\n-  const uintptr_t offset = forwarding->start();\n-  const size_t size = forwarding->size();\n-\n-  assert(_map.get(offset) == forwarding, \"Invalid entry\");\n-  _map.put(offset, size, nullptr);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XFORWARDINGTABLE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xForwardingTable.inline.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFUTURE_HPP\n-#define SHARE_GC_X_XFUTURE_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/semaphore.hpp\"\n-\n-template <typename T>\n-class XFuture {\n-private:\n-  Semaphore _sema;\n-  T         _value;\n-\n-public:\n-  XFuture();\n-\n-  void set(T value);\n-  T get();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XFUTURE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xFuture.hpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XFUTURE_INLINE_HPP\n-#define SHARE_GC_X_XFUTURE_INLINE_HPP\n-\n-#include \"gc\/x\/xFuture.hpp\"\n-\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/semaphore.inline.hpp\"\n-\n-template <typename T>\n-inline XFuture<T>::XFuture() :\n-    _value() {}\n-\n-template <typename T>\n-inline void XFuture<T>::set(T value) {\n-  \/\/ Set value\n-  _value = value;\n-\n-  \/\/ Notify waiter\n-  _sema.signal();\n-}\n-\n-template <typename T>\n-inline T XFuture<T>::get() {\n-  \/\/ Wait for notification\n-  Thread* const thread = Thread::current();\n-  if (thread->is_Java_thread()) {\n-    _sema.wait_with_safepoint_check(JavaThread::cast(thread));\n-  } else {\n-    _sema.wait();\n-  }\n-\n-  \/\/ Return value\n-  return _value;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XFUTURE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xFuture.inline.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-\n-uint32_t   XGlobalPhase                = XPhaseRelocate;\n-uint32_t   XGlobalSeqNum               = 1;\n-\n-size_t     XPageSizeMediumShift;\n-size_t     XPageSizeMedium;\n-\n-size_t     XObjectSizeLimitMedium;\n-\n-const int& XObjectAlignmentSmallShift  = LogMinObjAlignmentInBytes;\n-int        XObjectAlignmentMediumShift;\n-\n-const int& XObjectAlignmentSmall       = MinObjAlignmentInBytes;\n-int        XObjectAlignmentMedium;\n-\n-uintptr_t  XAddressGoodMask;\n-uintptr_t  XAddressBadMask;\n-uintptr_t  XAddressWeakBadMask;\n-\n-static uint32_t* XAddressCalculateBadMaskHighOrderBitsAddr() {\n-  const uintptr_t addr = reinterpret_cast<uintptr_t>(&XAddressBadMask);\n-  return reinterpret_cast<uint32_t*>(addr + XAddressBadMaskHighOrderBitsOffset);\n-}\n-\n-uint32_t*  XAddressBadMaskHighOrderBitsAddr = XAddressCalculateBadMaskHighOrderBitsAddr();\n-\n-size_t     XAddressOffsetBits;\n-uintptr_t  XAddressOffsetMask;\n-size_t     XAddressOffsetMax;\n-\n-size_t     XAddressMetadataShift;\n-uintptr_t  XAddressMetadataMask;\n-\n-uintptr_t  XAddressMetadataMarked;\n-uintptr_t  XAddressMetadataMarked0;\n-uintptr_t  XAddressMetadataMarked1;\n-uintptr_t  XAddressMetadataRemapped;\n-uintptr_t  XAddressMetadataFinalizable;\n-\n-const char* XGlobalPhaseToString() {\n-  switch (XGlobalPhase) {\n-  case XPhaseMark:\n-    return \"Mark\";\n-\n-  case XPhaseMarkCompleted:\n-    return \"MarkCompleted\";\n-\n-  case XPhaseRelocate:\n-    return \"Relocate\";\n-\n-  default:\n-    return \"Unknown\";\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xGlobals.cpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,158 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XGLOBALS_HPP\n-#define SHARE_GC_X_XGLOBALS_HPP\n-\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#include CPU_HEADER(gc\/x\/xGlobals)\n-\n-\/\/ Collector name\n-const char* const XName                         = \"The Z Garbage Collector\";\n-\n-\/\/ Global phase state\n-extern uint32_t   XGlobalPhase;\n-const uint32_t    XPhaseMark                    = 0;\n-const uint32_t    XPhaseMarkCompleted           = 1;\n-const uint32_t    XPhaseRelocate                = 2;\n-const char*       XGlobalPhaseToString();\n-\n-\/\/ Global sequence number\n-extern uint32_t   XGlobalSeqNum;\n-\n-\/\/ Granule shift\/size\n-const size_t      XGranuleSizeShift             = 21; \/\/ 2MB\n-const size_t      XGranuleSize                  = (size_t)1 << XGranuleSizeShift;\n-\n-\/\/ Number of heap views\n-const size_t      XHeapViews                    = XPlatformHeapViews;\n-\n-\/\/ Virtual memory to physical memory ratio\n-const size_t      XVirtualToPhysicalRatio       = 16; \/\/ 16:1\n-\n-\/\/ Page types\n-const uint8_t     XPageTypeSmall                = 0;\n-const uint8_t     XPageTypeMedium               = 1;\n-const uint8_t     XPageTypeLarge                = 2;\n-\n-\/\/ Page size shifts\n-const size_t      XPageSizeSmallShift           = XGranuleSizeShift;\n-extern size_t     XPageSizeMediumShift;\n-\n-\/\/ Page sizes\n-const size_t      XPageSizeSmall                = (size_t)1 << XPageSizeSmallShift;\n-extern size_t     XPageSizeMedium;\n-\n-\/\/ Object size limits\n-const size_t      XObjectSizeLimitSmall         = XPageSizeSmall \/ 8; \/\/ 12.5% max waste\n-extern size_t     XObjectSizeLimitMedium;\n-\n-\/\/ Object alignment shifts\n-extern const int& XObjectAlignmentSmallShift;\n-extern int        XObjectAlignmentMediumShift;\n-const int         XObjectAlignmentLargeShift    = XGranuleSizeShift;\n-\n-\/\/ Object alignments\n-extern const int& XObjectAlignmentSmall;\n-extern int        XObjectAlignmentMedium;\n-const int         XObjectAlignmentLarge         = 1 << XObjectAlignmentLargeShift;\n-\n-\/\/\n-\/\/ Good\/Bad mask states\n-\/\/ --------------------\n-\/\/\n-\/\/                 GoodMask         BadMask          WeakGoodMask     WeakBadMask\n-\/\/                 --------------------------------------------------------------\n-\/\/  Marked0        001              110              101              010\n-\/\/  Marked1        010              101              110              001\n-\/\/  Remapped       100              011              100              011\n-\/\/\n-\n-\/\/ Good\/bad masks\n-extern uintptr_t  XAddressGoodMask;\n-extern uintptr_t  XAddressBadMask;\n-extern uintptr_t  XAddressWeakBadMask;\n-\n-\/\/ The bad mask is 64 bit. Its high order 32 bits contain all possible value combinations\n-\/\/ that this mask will have. Therefore, the memory where the 32 high order bits are stored,\n-\/\/ can be used as a 32 bit GC epoch counter, that has a different bit pattern every time\n-\/\/ the bad mask is flipped. This provides a pointer to said 32 bits.\n-extern uint32_t*  XAddressBadMaskHighOrderBitsAddr;\n-const int         XAddressBadMaskHighOrderBitsOffset = LITTLE_ENDIAN_ONLY(4) BIG_ENDIAN_ONLY(0);\n-\n-\/\/ Pointer part of address\n-extern size_t     XAddressOffsetBits;\n-const  size_t     XAddressOffsetShift           = 0;\n-extern uintptr_t  XAddressOffsetMask;\n-extern size_t     XAddressOffsetMax;\n-\n-\/\/ Metadata part of address\n-const size_t      XAddressMetadataBits          = 4;\n-extern size_t     XAddressMetadataShift;\n-extern uintptr_t  XAddressMetadataMask;\n-\n-\/\/ Metadata types\n-extern uintptr_t  XAddressMetadataMarked;\n-extern uintptr_t  XAddressMetadataMarked0;\n-extern uintptr_t  XAddressMetadataMarked1;\n-extern uintptr_t  XAddressMetadataRemapped;\n-extern uintptr_t  XAddressMetadataFinalizable;\n-\n-\/\/ Cache line size\n-const size_t      XCacheLineSize                = XPlatformCacheLineSize;\n-#define           XCACHE_ALIGNED                ATTRIBUTE_ALIGNED(XCacheLineSize)\n-\n-\/\/ Mark stack space\n-extern uintptr_t  XMarkStackSpaceStart;\n-const size_t      XMarkStackSpaceExpandSize     = (size_t)1 << 25; \/\/ 32M\n-\n-\/\/ Mark stack and magazine sizes\n-const size_t      XMarkStackSizeShift           = 11; \/\/ 2K\n-const size_t      XMarkStackSize                = (size_t)1 << XMarkStackSizeShift;\n-const size_t      XMarkStackHeaderSize          = (size_t)1 << 4; \/\/ 16B\n-const size_t      XMarkStackSlots               = (XMarkStackSize - XMarkStackHeaderSize) \/ sizeof(uintptr_t);\n-const size_t      XMarkStackMagazineSize        = (size_t)1 << 15; \/\/ 32K\n-const size_t      XMarkStackMagazineSlots       = (XMarkStackMagazineSize \/ XMarkStackSize) - 1;\n-\n-\/\/ Mark stripe size\n-const size_t      XMarkStripeShift              = XGranuleSizeShift;\n-\n-\/\/ Max number of mark stripes\n-const size_t      XMarkStripesMax               = 16; \/\/ Must be a power of two\n-\n-\/\/ Mark cache size\n-const size_t      XMarkCacheSize                = 1024; \/\/ Must be a power of two\n-\n-\/\/ Partial array minimum size\n-const size_t      XMarkPartialArrayMinSizeShift = 12; \/\/ 4K\n-const size_t      XMarkPartialArrayMinSize      = (size_t)1 << XMarkPartialArrayMinSizeShift;\n-\n-\/\/ Max number of proactive\/terminate flush attempts\n-const size_t      XMarkProactiveFlushMax        = 10;\n-const size_t      XMarkTerminateFlushMax        = 3;\n-\n-\/\/ Try complete mark timeout\n-const uint64_t    XMarkCompleteTimeout          = 200; \/\/ us\n-\n-#endif \/\/ SHARE_GC_X_XGLOBALS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xGlobals.hpp","additions":0,"deletions":158,"binary":false,"changes":158,"status":"deleted"},{"patch":"@@ -1,61 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XGRANULEMAP_HPP\n-#define SHARE_GC_X_XGRANULEMAP_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class VMStructs;\n-\n-template <typename T>\n-class XGranuleMap {\n-  friend class ::VMStructs;\n-  template <typename> friend class XGranuleMapIterator;\n-\n-private:\n-  const size_t _size;\n-  T* const     _map;\n-\n-  size_t index_for_offset(uintptr_t offset) const;\n-\n-public:\n-  XGranuleMap(size_t max_offset);\n-  ~XGranuleMap();\n-\n-  T get(uintptr_t offset) const;\n-  void put(uintptr_t offset, T value);\n-  void put(uintptr_t offset, size_t size, T value);\n-\n-  T get_acquire(uintptr_t offset) const;\n-  void release_put(uintptr_t offset, T value);\n-};\n-\n-template <typename T>\n-class XGranuleMapIterator : public XArrayIteratorImpl<T, false \/* Parallel *\/> {\n-public:\n-  XGranuleMapIterator(const XGranuleMap<T>* granule_map);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XGRANULEMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xGranuleMap.hpp","additions":0,"deletions":61,"binary":false,"changes":61,"status":"deleted"},{"patch":"@@ -1,94 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XGRANULEMAP_INLINE_HPP\n-#define SHARE_GC_X_XGRANULEMAP_INLINE_HPP\n-\n-#include \"gc\/x\/xGranuleMap.hpp\"\n-\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"memory\/allocation.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-template <typename T>\n-inline XGranuleMap<T>::XGranuleMap(size_t max_offset) :\n-    _size(max_offset >> XGranuleSizeShift),\n-    _map(MmapArrayAllocator<T>::allocate(_size, mtGC)) {\n-  assert(is_aligned(max_offset, XGranuleSize), \"Misaligned\");\n-}\n-\n-template <typename T>\n-inline XGranuleMap<T>::~XGranuleMap() {\n-  MmapArrayAllocator<T>::free(_map, _size);\n-}\n-\n-template <typename T>\n-inline size_t XGranuleMap<T>::index_for_offset(uintptr_t offset) const {\n-  const size_t index = offset >> XGranuleSizeShift;\n-  assert(index < _size, \"Invalid index\");\n-  return index;\n-}\n-\n-template <typename T>\n-inline T XGranuleMap<T>::get(uintptr_t offset) const {\n-  const size_t index = index_for_offset(offset);\n-  return _map[index];\n-}\n-\n-template <typename T>\n-inline void XGranuleMap<T>::put(uintptr_t offset, T value) {\n-  const size_t index = index_for_offset(offset);\n-  _map[index] = value;\n-}\n-\n-template <typename T>\n-inline void XGranuleMap<T>::put(uintptr_t offset, size_t size, T value) {\n-  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n-\n-  const size_t start_index = index_for_offset(offset);\n-  const size_t end_index = start_index + (size >> XGranuleSizeShift);\n-  for (size_t index = start_index; index < end_index; index++) {\n-    _map[index] = value;\n-  }\n-}\n-\n-template <typename T>\n-inline T XGranuleMap<T>::get_acquire(uintptr_t offset) const {\n-  const size_t index = index_for_offset(offset);\n-  return Atomic::load_acquire(_map + index);\n-}\n-\n-template <typename T>\n-inline void XGranuleMap<T>::release_put(uintptr_t offset, T value) {\n-  const size_t index = index_for_offset(offset);\n-  Atomic::release_store(_map + index, value);\n-}\n-\n-template <typename T>\n-inline XGranuleMapIterator<T>::XGranuleMapIterator(const XGranuleMap<T>* granule_map) :\n-    XArrayIteratorImpl<T, false \/* Parallel *\/>(granule_map->_map, granule_map->_size) {}\n-\n-#endif \/\/ SHARE_GC_X_XGRANULEMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xGranuleMap.inline.hpp","additions":0,"deletions":94,"binary":false,"changes":94,"status":"deleted"},{"patch":"@@ -1,36 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XHASH_HPP\n-#define SHARE_GC_X_XHASH_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XHash : public AllStatic {\n-public:\n-  static uint32_t uint32_to_uint32(uint32_t key);\n-  static uint32_t address_to_uint32(uintptr_t key);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XHASH_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHash.hpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"deleted"},{"patch":"@@ -1,77 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/*\n- * This file is available under and governed by the GNU General Public\n- * License version 2 only, as published by the Free Software Foundation.\n- * However, the following notice accompanied the original version of this\n- * file:\n- *\n- *  (C) 2009 by Remo Dentato (rdentato@gmail.com)\n- *\n- *\n- * Redistribution and use in source and binary forms, with or without modification,\n- * are permitted provided that the following conditions are met:\n- *\n- *     * Redistributions of source code must retain the above copyright notice,\n- *       this list of conditions and the following disclaimer.\n- *     * Redistributions in binary form must reproduce the above copyright notice,\n- *       this list of conditions and the following disclaimer in the documentation\n- *       and\/or other materials provided with the distribution.\n- *\n- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n- * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n- * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n- *\n- * http:\/\/opensource.org\/licenses\/bsd-license.php\n- *\/\n-\n-#ifndef SHARE_GC_X_XHASH_INLINE_HPP\n-#define SHARE_GC_X_XHASH_INLINE_HPP\n-\n-#include \"gc\/x\/xHash.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-\n-inline uint32_t XHash::uint32_to_uint32(uint32_t key) {\n-  key = ~key + (key << 15);\n-  key = key ^ (key >> 12);\n-  key = key + (key << 2);\n-  key = key ^ (key >> 4);\n-  key = key * 2057;\n-  key = key ^ (key >> 16);\n-  return key;\n-}\n-\n-inline uint32_t XHash::address_to_uint32(uintptr_t key) {\n-  return uint32_to_uint32((uint32_t)(key >> 3));\n-}\n-\n-#endif \/\/ SHARE_GC_X_XHASH_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHash.inline.hpp","additions":0,"deletions":77,"binary":false,"changes":77,"status":"deleted"},{"patch":"@@ -1,541 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/classUnloadingContext.hpp\"\n-#include \"gc\/shared\/locationPrinter.hpp\"\n-#include \"gc\/shared\/tlab_globals.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xHeapIterator.hpp\"\n-#include \"gc\/x\/xHeuristics.hpp\"\n-#include \"gc\/x\/xMark.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageTable.inline.hpp\"\n-#include \"gc\/x\/xRelocationSet.inline.hpp\"\n-#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n-#include \"gc\/x\/xResurrection.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xVerify.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"prims\/jvmtiTagMap.hpp\"\n-#include \"runtime\/handshake.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-static const XStatCounter XCounterUndoPageAllocation(\"Memory\", \"Undo Page Allocation\", XStatUnitOpsPerSecond);\n-static const XStatCounter XCounterOutOfMemory(\"Memory\", \"Out Of Memory\", XStatUnitOpsPerSecond);\n-\n-XHeap* XHeap::_heap = nullptr;\n-\n-XHeap::XHeap() :\n-    _workers(),\n-    _object_allocator(),\n-    _page_allocator(&_workers, MinHeapSize, InitialHeapSize, MaxHeapSize),\n-    _page_table(),\n-    _forwarding_table(),\n-    _mark(&_workers, &_page_table),\n-    _reference_processor(&_workers),\n-    _weak_roots_processor(&_workers),\n-    _relocate(&_workers),\n-    _relocation_set(&_workers),\n-    _unload(&_workers),\n-    _serviceability(min_capacity(), max_capacity()) {\n-  \/\/ Install global heap instance\n-  assert(_heap == nullptr, \"Already initialized\");\n-  _heap = this;\n-\n-  \/\/ Update statistics\n-  XStatHeap::set_at_initialize(_page_allocator.stats());\n-}\n-\n-bool XHeap::is_initialized() const {\n-  return _page_allocator.is_initialized() && _mark.is_initialized();\n-}\n-\n-size_t XHeap::min_capacity() const {\n-  return _page_allocator.min_capacity();\n-}\n-\n-size_t XHeap::max_capacity() const {\n-  return _page_allocator.max_capacity();\n-}\n-\n-size_t XHeap::soft_max_capacity() const {\n-  return _page_allocator.soft_max_capacity();\n-}\n-\n-size_t XHeap::capacity() const {\n-  return _page_allocator.capacity();\n-}\n-\n-size_t XHeap::used() const {\n-  return _page_allocator.used();\n-}\n-\n-size_t XHeap::unused() const {\n-  return _page_allocator.unused();\n-}\n-\n-size_t XHeap::tlab_capacity() const {\n-  return capacity();\n-}\n-\n-size_t XHeap::tlab_used() const {\n-  return _object_allocator.used();\n-}\n-\n-size_t XHeap::max_tlab_size() const {\n-  return XObjectSizeLimitSmall;\n-}\n-\n-size_t XHeap::unsafe_max_tlab_alloc() const {\n-  size_t size = _object_allocator.remaining();\n-\n-  if (size < MinTLABSize) {\n-    \/\/ The remaining space in the allocator is not enough to\n-    \/\/ fit the smallest possible TLAB. This means that the next\n-    \/\/ TLAB allocation will force the allocator to get a new\n-    \/\/ backing page anyway, which in turn means that we can then\n-    \/\/ fit the largest possible TLAB.\n-    size = max_tlab_size();\n-  }\n-\n-  return MIN2(size, max_tlab_size());\n-}\n-\n-bool XHeap::is_in(uintptr_t addr) const {\n-  \/\/ An address is considered to be \"in the heap\" if it points into\n-  \/\/ the allocated part of a page, regardless of which heap view is\n-  \/\/ used. Note that an address with the finalizable metadata bit set\n-  \/\/ is not pointing into a heap view, and therefore not considered\n-  \/\/ to be \"in the heap\".\n-\n-  if (XAddress::is_in(addr)) {\n-    const XPage* const page = _page_table.get(addr);\n-    if (page != nullptr) {\n-      return page->is_in(addr);\n-    }\n-  }\n-\n-  return false;\n-}\n-\n-uint XHeap::active_workers() const {\n-  return _workers.active_workers();\n-}\n-\n-void XHeap::set_active_workers(uint nworkers) {\n-  _workers.set_active_workers(nworkers);\n-}\n-\n-void XHeap::threads_do(ThreadClosure* tc) const {\n-  _page_allocator.threads_do(tc);\n-  _workers.threads_do(tc);\n-}\n-\n-void XHeap::out_of_memory() {\n-  ResourceMark rm;\n-\n-  XStatInc(XCounterOutOfMemory);\n-  log_info(gc)(\"Out Of Memory (%s)\", Thread::current()->name());\n-}\n-\n-XPage* XHeap::alloc_page(uint8_t type, size_t size, XAllocationFlags flags) {\n-  XPage* const page = _page_allocator.alloc_page(type, size, flags);\n-  if (page != nullptr) {\n-    \/\/ Insert page table entry\n-    _page_table.insert(page);\n-  }\n-\n-  return page;\n-}\n-\n-void XHeap::undo_alloc_page(XPage* page) {\n-  assert(page->is_allocating(), \"Invalid page state\");\n-\n-  XStatInc(XCounterUndoPageAllocation);\n-  log_trace(gc)(\"Undo page allocation, thread: \" PTR_FORMAT \" (%s), page: \" PTR_FORMAT \", size: \" SIZE_FORMAT,\n-                XThread::id(), XThread::name(), p2i(page), page->size());\n-\n-  free_page(page, false \/* reclaimed *\/);\n-}\n-\n-void XHeap::free_page(XPage* page, bool reclaimed) {\n-  \/\/ Remove page table entry\n-  _page_table.remove(page);\n-\n-  \/\/ Free page\n-  _page_allocator.free_page(page, reclaimed);\n-}\n-\n-void XHeap::free_pages(const XArray<XPage*>* pages, bool reclaimed) {\n-  \/\/ Remove page table entries\n-  XArrayIterator<XPage*> iter(pages);\n-  for (XPage* page; iter.next(&page);) {\n-    _page_table.remove(page);\n-  }\n-\n-  \/\/ Free pages\n-  _page_allocator.free_pages(pages, reclaimed);\n-}\n-\n-void XHeap::flip_to_marked() {\n-  XVerifyViewsFlip flip(&_page_allocator);\n-  XAddress::flip_to_marked();\n-}\n-\n-void XHeap::flip_to_remapped() {\n-  XVerifyViewsFlip flip(&_page_allocator);\n-  XAddress::flip_to_remapped();\n-}\n-\n-void XHeap::mark_start() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  \/\/ Verification\n-  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_strong);\n-\n-  if (XHeap::heap()->has_alloc_stalled()) {\n-    \/\/ If there are stalled allocations, ensure that regardless of the\n-    \/\/ cause of the GC, we have to clear soft references, as we are just\n-    \/\/ about to increment the sequence number, and all previous allocations\n-    \/\/ will throw if not presented with enough memory.\n-    XHeap::heap()->set_soft_reference_policy(true);\n-  }\n-\n-  \/\/ Flip address view\n-  flip_to_marked();\n-\n-  \/\/ Retire allocating pages\n-  _object_allocator.retire_pages();\n-\n-  \/\/ Reset allocated\/reclaimed\/used statistics\n-  _page_allocator.reset_statistics();\n-\n-  \/\/ Reset encountered\/dropped\/enqueued statistics\n-  _reference_processor.reset_statistics();\n-\n-  \/\/ Enter mark phase\n-  XGlobalPhase = XPhaseMark;\n-\n-  \/\/ Reset marking information\n-  _mark.start();\n-\n-  \/\/ Update statistics\n-  XStatHeap::set_at_mark_start(_page_allocator.stats());\n-}\n-\n-void XHeap::mark(bool initial) {\n-  _mark.mark(initial);\n-}\n-\n-void XHeap::mark_flush_and_free(Thread* thread) {\n-  _mark.flush_and_free(thread);\n-}\n-\n-bool XHeap::mark_end() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  \/\/ Try end marking\n-  if (!_mark.end()) {\n-    \/\/ Marking not completed, continue concurrent mark\n-    return false;\n-  }\n-\n-  \/\/ Enter mark completed phase\n-  XGlobalPhase = XPhaseMarkCompleted;\n-\n-  \/\/ Verify after mark\n-  XVerify::after_mark();\n-\n-  \/\/ Update statistics\n-  XStatHeap::set_at_mark_end(_page_allocator.stats());\n-\n-  \/\/ Block resurrection of weak\/phantom references\n-  XResurrection::block();\n-\n-  \/\/ Prepare to unload stale metadata and nmethods\n-  _unload.prepare();\n-\n-  \/\/ Notify JVMTI that some tagmap entry objects may have died.\n-  JvmtiTagMap::set_needs_cleaning();\n-\n-  return true;\n-}\n-\n-void XHeap::mark_free() {\n-  _mark.free();\n-}\n-\n-void XHeap::keep_alive(oop obj) {\n-  XBarrier::keep_alive_barrier_on_oop(obj);\n-}\n-\n-void XHeap::set_soft_reference_policy(bool clear) {\n-  _reference_processor.set_soft_reference_policy(clear);\n-}\n-\n-class XRendezvousClosure : public HandshakeClosure {\n-public:\n-  XRendezvousClosure() :\n-      HandshakeClosure(\"XRendezvous\") {}\n-\n-  void do_thread(Thread* thread) {}\n-};\n-\n-void XHeap::process_non_strong_references() {\n-  \/\/ Process Soft\/Weak\/Final\/PhantomReferences\n-  _reference_processor.process_references();\n-\n-  \/\/ Process weak roots\n-  _weak_roots_processor.process_weak_roots();\n-\n-  ClassUnloadingContext ctx(_workers.active_workers(),\n-                            true \/* unregister_nmethods_during_purge *\/,\n-                            true \/* lock_nmethod_free_separately *\/);\n-\n-  \/\/ Unlink stale metadata and nmethods\n-  _unload.unlink();\n-\n-  \/\/ Perform a handshake. This is needed 1) to make sure that stale\n-  \/\/ metadata and nmethods are no longer observable. And 2), to\n-  \/\/ prevent the race where a mutator first loads an oop, which is\n-  \/\/ logically null but not yet cleared. Then this oop gets cleared\n-  \/\/ by the reference processor and resurrection is unblocked. At\n-  \/\/ this point the mutator could see the unblocked state and pass\n-  \/\/ this invalid oop through the normal barrier path, which would\n-  \/\/ incorrectly try to mark the oop.\n-  XRendezvousClosure cl;\n-  Handshake::execute(&cl);\n-\n-  \/\/ Unblock resurrection of weak\/phantom references\n-  XResurrection::unblock();\n-\n-  \/\/ Purge stale metadata and nmethods that were unlinked\n-  _unload.purge();\n-\n-  \/\/ Enqueue Soft\/Weak\/Final\/PhantomReferences. Note that this\n-  \/\/ must be done after unblocking resurrection. Otherwise the\n-  \/\/ Finalizer thread could call Reference.get() on the Finalizers\n-  \/\/ that were just enqueued, which would incorrectly return null\n-  \/\/ during the resurrection block window, since such referents\n-  \/\/ are only Finalizable marked.\n-  _reference_processor.enqueue_references();\n-\n-  \/\/ Clear old markings claim bits.\n-  \/\/ Note: Clearing _claim_strong also clears _claim_finalizable.\n-  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_strong);\n-}\n-\n-void XHeap::free_empty_pages(XRelocationSetSelector* selector, int bulk) {\n-  \/\/ Freeing empty pages in bulk is an optimization to avoid grabbing\n-  \/\/ the page allocator lock, and trying to satisfy stalled allocations\n-  \/\/ too frequently.\n-  if (selector->should_free_empty_pages(bulk)) {\n-    free_pages(selector->empty_pages(), true \/* reclaimed *\/);\n-    selector->clear_empty_pages();\n-  }\n-}\n-\n-void XHeap::select_relocation_set() {\n-  \/\/ Do not allow pages to be deleted\n-  _page_allocator.enable_deferred_delete();\n-\n-  \/\/ Register relocatable pages with selector\n-  XRelocationSetSelector selector;\n-  XPageTableIterator pt_iter(&_page_table);\n-  for (XPage* page; pt_iter.next(&page);) {\n-    if (!page->is_relocatable()) {\n-      \/\/ Not relocatable, don't register\n-      continue;\n-    }\n-\n-    if (page->is_marked()) {\n-      \/\/ Register live page\n-      selector.register_live_page(page);\n-    } else {\n-      \/\/ Register empty page\n-      selector.register_empty_page(page);\n-\n-      \/\/ Reclaim empty pages in bulk\n-      free_empty_pages(&selector, 64 \/* bulk *\/);\n-    }\n-  }\n-\n-  \/\/ Reclaim remaining empty pages\n-  free_empty_pages(&selector, 0 \/* bulk *\/);\n-\n-  \/\/ Allow pages to be deleted\n-  _page_allocator.disable_deferred_delete();\n-\n-  \/\/ Select relocation set\n-  selector.select();\n-\n-  \/\/ Install relocation set\n-  _relocation_set.install(&selector);\n-\n-  \/\/ Setup forwarding table\n-  XRelocationSetIterator rs_iter(&_relocation_set);\n-  for (XForwarding* forwarding; rs_iter.next(&forwarding);) {\n-    _forwarding_table.insert(forwarding);\n-  }\n-\n-  \/\/ Update statistics\n-  XStatRelocation::set_at_select_relocation_set(selector.stats());\n-  XStatHeap::set_at_select_relocation_set(selector.stats());\n-}\n-\n-void XHeap::reset_relocation_set() {\n-  \/\/ Reset forwarding table\n-  XRelocationSetIterator iter(&_relocation_set);\n-  for (XForwarding* forwarding; iter.next(&forwarding);) {\n-    _forwarding_table.remove(forwarding);\n-  }\n-\n-  \/\/ Reset relocation set\n-  _relocation_set.reset();\n-}\n-\n-void XHeap::relocate_start() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  \/\/ Finish unloading stale metadata and nmethods\n-  _unload.finish();\n-\n-  \/\/ Flip address view\n-  flip_to_remapped();\n-\n-  \/\/ Enter relocate phase\n-  XGlobalPhase = XPhaseRelocate;\n-\n-  \/\/ Update statistics\n-  XStatHeap::set_at_relocate_start(_page_allocator.stats());\n-}\n-\n-void XHeap::relocate() {\n-  \/\/ Relocate relocation set\n-  _relocate.relocate(&_relocation_set);\n-\n-  \/\/ Update statistics\n-  XStatHeap::set_at_relocate_end(_page_allocator.stats(), _object_allocator.relocated());\n-}\n-\n-bool XHeap::is_allocating(uintptr_t addr) const {\n-  const XPage* const page = _page_table.get(addr);\n-  return page->is_allocating();\n-}\n-\n-void XHeap::object_iterate(ObjectClosure* cl, bool visit_weaks) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  XHeapIterator iter(1 \/* nworkers *\/, visit_weaks);\n-  iter.object_iterate(cl, 0 \/* worker_id *\/);\n-}\n-\n-ParallelObjectIteratorImpl* XHeap::parallel_object_iterator(uint nworkers, bool visit_weaks) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  return new XHeapIterator(nworkers, visit_weaks);\n-}\n-\n-void XHeap::pages_do(XPageClosure* cl) {\n-  XPageTableIterator iter(&_page_table);\n-  for (XPage* page; iter.next(&page);) {\n-    cl->do_page(page);\n-  }\n-  _page_allocator.pages_do(cl);\n-}\n-\n-void XHeap::serviceability_initialize() {\n-  _serviceability.initialize();\n-}\n-\n-GCMemoryManager* XHeap::serviceability_cycle_memory_manager() {\n-  return _serviceability.cycle_memory_manager();\n-}\n-\n-GCMemoryManager* XHeap::serviceability_pause_memory_manager() {\n-  return _serviceability.pause_memory_manager();\n-}\n-\n-MemoryPool* XHeap::serviceability_memory_pool() {\n-  return _serviceability.memory_pool();\n-}\n-\n-XServiceabilityCounters* XHeap::serviceability_counters() {\n-  return _serviceability.counters();\n-}\n-\n-void XHeap::print_on(outputStream* st) const {\n-  st->print_cr(\" ZHeap           used \" SIZE_FORMAT \"M, capacity \" SIZE_FORMAT \"M, max capacity \" SIZE_FORMAT \"M\",\n-               used() \/ M,\n-               capacity() \/ M,\n-               max_capacity() \/ M);\n-  MetaspaceUtils::print_on(st);\n-}\n-\n-void XHeap::print_extended_on(outputStream* st) const {\n-  print_on(st);\n-  st->cr();\n-\n-  \/\/ Do not allow pages to be deleted\n-  _page_allocator.enable_deferred_delete();\n-\n-  \/\/ Print all pages\n-  st->print_cr(\"ZGC Page Table:\");\n-  XPageTableIterator iter(&_page_table);\n-  for (XPage* page; iter.next(&page);) {\n-    page->print_on(st);\n-  }\n-\n-  \/\/ Allow pages to be deleted\n-  _page_allocator.disable_deferred_delete();\n-}\n-\n-bool XHeap::print_location(outputStream* st, uintptr_t addr) const {\n-  if (LocationPrinter::is_valid_obj((void*)addr)) {\n-    st->print(PTR_FORMAT \" is a %s oop: \", addr, XAddress::is_good(addr) ? \"good\" : \"bad\");\n-    XOop::from_address(addr)->print_on(st);\n-    return true;\n-  }\n-\n-  return false;\n-}\n-\n-void XHeap::verify() {\n-  \/\/ Heap verification can only be done between mark end and\n-  \/\/ relocate start. This is the only window where all oop are\n-  \/\/ good and the whole heap is in a consistent state.\n-  guarantee(XGlobalPhase == XPhaseMarkCompleted, \"Invalid phase\");\n-\n-  XVerify::after_weak_processing();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xHeap.cpp","additions":0,"deletions":541,"binary":false,"changes":541,"status":"deleted"},{"patch":"@@ -1,167 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XHEAP_HPP\n-#define SHARE_GC_X_XHEAP_HPP\n-\n-#include \"gc\/x\/xAllocationFlags.hpp\"\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"gc\/x\/xForwardingTable.hpp\"\n-#include \"gc\/x\/xMark.hpp\"\n-#include \"gc\/x\/xObjectAllocator.hpp\"\n-#include \"gc\/x\/xPageAllocator.hpp\"\n-#include \"gc\/x\/xPageTable.hpp\"\n-#include \"gc\/x\/xReferenceProcessor.hpp\"\n-#include \"gc\/x\/xRelocate.hpp\"\n-#include \"gc\/x\/xRelocationSet.hpp\"\n-#include \"gc\/x\/xWeakRootsProcessor.hpp\"\n-#include \"gc\/x\/xServiceability.hpp\"\n-#include \"gc\/x\/xUnload.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-\n-class ThreadClosure;\n-class VMStructs;\n-class XPage;\n-class XRelocationSetSelector;\n-\n-class XHeap {\n-  friend class ::VMStructs;\n-\n-private:\n-  static XHeap*       _heap;\n-\n-  XWorkers            _workers;\n-  XObjectAllocator    _object_allocator;\n-  XPageAllocator      _page_allocator;\n-  XPageTable          _page_table;\n-  XForwardingTable    _forwarding_table;\n-  XMark               _mark;\n-  XReferenceProcessor _reference_processor;\n-  XWeakRootsProcessor _weak_roots_processor;\n-  XRelocate           _relocate;\n-  XRelocationSet      _relocation_set;\n-  XUnload             _unload;\n-  XServiceability     _serviceability;\n-\n-  void flip_to_marked();\n-  void flip_to_remapped();\n-\n-  void free_empty_pages(XRelocationSetSelector* selector, int bulk);\n-\n-  void out_of_memory();\n-\n-public:\n-  static XHeap* heap();\n-\n-  XHeap();\n-\n-  bool is_initialized() const;\n-\n-  \/\/ Heap metrics\n-  size_t min_capacity() const;\n-  size_t max_capacity() const;\n-  size_t soft_max_capacity() const;\n-  size_t capacity() const;\n-  size_t used() const;\n-  size_t unused() const;\n-\n-  size_t tlab_capacity() const;\n-  size_t tlab_used() const;\n-  size_t max_tlab_size() const;\n-  size_t unsafe_max_tlab_alloc() const;\n-\n-  bool is_in(uintptr_t addr) const;\n-\n-  \/\/ Threads\n-  uint active_workers() const;\n-  void set_active_workers(uint nworkers);\n-  void threads_do(ThreadClosure* tc) const;\n-\n-  \/\/ Reference processing\n-  ReferenceDiscoverer* reference_discoverer();\n-  void set_soft_reference_policy(bool clear);\n-\n-  \/\/ Non-strong reference processing\n-  void process_non_strong_references();\n-\n-  \/\/ Page allocation\n-  XPage* alloc_page(uint8_t type, size_t size, XAllocationFlags flags);\n-  void undo_alloc_page(XPage* page);\n-  void free_page(XPage* page, bool reclaimed);\n-  void free_pages(const XArray<XPage*>* pages, bool reclaimed);\n-\n-  \/\/ Object allocation\n-  uintptr_t alloc_tlab(size_t size);\n-  uintptr_t alloc_object(size_t size);\n-  uintptr_t alloc_object_for_relocation(size_t size);\n-  void undo_alloc_object_for_relocation(uintptr_t addr, size_t size);\n-  bool has_alloc_stalled() const;\n-  void check_out_of_memory();\n-\n-  \/\/ Marking\n-  bool is_object_live(uintptr_t addr) const;\n-  bool is_object_strongly_live(uintptr_t addr) const;\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> void mark_object(uintptr_t addr);\n-  void mark_start();\n-  void mark(bool initial);\n-  void mark_flush_and_free(Thread* thread);\n-  bool mark_end();\n-  void mark_free();\n-  void keep_alive(oop obj);\n-\n-  \/\/ Relocation set\n-  void select_relocation_set();\n-  void reset_relocation_set();\n-\n-  \/\/ Relocation\n-  void relocate_start();\n-  uintptr_t relocate_object(uintptr_t addr);\n-  uintptr_t remap_object(uintptr_t addr);\n-  void relocate();\n-\n-  \/\/ Continuations\n-  bool is_allocating(uintptr_t addr) const;\n-\n-  \/\/ Iteration\n-  void object_iterate(ObjectClosure* cl, bool visit_weaks);\n-  ParallelObjectIteratorImpl* parallel_object_iterator(uint nworkers, bool visit_weaks);\n-  void pages_do(XPageClosure* cl);\n-\n-  \/\/ Serviceability\n-  void serviceability_initialize();\n-  GCMemoryManager* serviceability_cycle_memory_manager();\n-  GCMemoryManager* serviceability_pause_memory_manager();\n-  MemoryPool* serviceability_memory_pool();\n-  XServiceabilityCounters* serviceability_counters();\n-\n-  \/\/ Printing\n-  void print_on(outputStream* st) const;\n-  void print_extended_on(outputStream* st) const;\n-  bool print_location(outputStream* st, uintptr_t addr) const;\n-\n-  \/\/ Verification\n-  bool is_oop(uintptr_t addr) const;\n-  void verify();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XHEAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeap.hpp","additions":0,"deletions":167,"binary":false,"changes":167,"status":"deleted"},{"patch":"@@ -1,127 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XHEAP_INLINE_HPP\n-#define SHARE_GC_X_XHEAP_INLINE_HPP\n-\n-#include \"gc\/x\/xHeap.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xForwardingTable.inline.hpp\"\n-#include \"gc\/x\/xMark.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageTable.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline XHeap* XHeap::heap() {\n-  assert(_heap != nullptr, \"Not initialized\");\n-  return _heap;\n-}\n-\n-inline ReferenceDiscoverer* XHeap::reference_discoverer() {\n-  return &_reference_processor;\n-}\n-\n-inline bool XHeap::is_object_live(uintptr_t addr) const {\n-  XPage* page = _page_table.get(addr);\n-  return page->is_object_live(addr);\n-}\n-\n-inline bool XHeap::is_object_strongly_live(uintptr_t addr) const {\n-  XPage* page = _page_table.get(addr);\n-  return page->is_object_strongly_live(addr);\n-}\n-\n-template <bool gc_thread, bool follow, bool finalizable, bool publish>\n-inline void XHeap::mark_object(uintptr_t addr) {\n-  assert(XGlobalPhase == XPhaseMark, \"Mark not allowed\");\n-  _mark.mark_object<gc_thread, follow, finalizable, publish>(addr);\n-}\n-\n-inline uintptr_t XHeap::alloc_tlab(size_t size) {\n-  guarantee(size <= max_tlab_size(), \"TLAB too large\");\n-  return _object_allocator.alloc_object(size);\n-}\n-\n-inline uintptr_t XHeap::alloc_object(size_t size) {\n-  uintptr_t addr = _object_allocator.alloc_object(size);\n-  assert(XAddress::is_good_or_null(addr), \"Bad address\");\n-\n-  if (addr == 0) {\n-    out_of_memory();\n-  }\n-\n-  return addr;\n-}\n-\n-inline uintptr_t XHeap::alloc_object_for_relocation(size_t size) {\n-  const uintptr_t addr = _object_allocator.alloc_object_for_relocation(&_page_table, size);\n-  assert(XAddress::is_good_or_null(addr), \"Bad address\");\n-  return addr;\n-}\n-\n-inline void XHeap::undo_alloc_object_for_relocation(uintptr_t addr, size_t size) {\n-  XPage* const page = _page_table.get(addr);\n-  _object_allocator.undo_alloc_object_for_relocation(page, addr, size);\n-}\n-\n-inline uintptr_t XHeap::relocate_object(uintptr_t addr) {\n-  assert(XGlobalPhase == XPhaseRelocate, \"Relocate not allowed\");\n-\n-  XForwarding* const forwarding = _forwarding_table.get(addr);\n-  if (forwarding == nullptr) {\n-    \/\/ Not forwarding\n-    return XAddress::good(addr);\n-  }\n-\n-  \/\/ Relocate object\n-  return _relocate.relocate_object(forwarding, XAddress::good(addr));\n-}\n-\n-inline uintptr_t XHeap::remap_object(uintptr_t addr) {\n-  assert(XGlobalPhase == XPhaseMark ||\n-         XGlobalPhase == XPhaseMarkCompleted, \"Forward not allowed\");\n-\n-  XForwarding* const forwarding = _forwarding_table.get(addr);\n-  if (forwarding == nullptr) {\n-    \/\/ Not forwarding\n-    return XAddress::good(addr);\n-  }\n-\n-  \/\/ Forward object\n-  return _relocate.forward_object(forwarding, XAddress::good(addr));\n-}\n-\n-inline bool XHeap::has_alloc_stalled() const {\n-  return _page_allocator.has_alloc_stalled();\n-}\n-\n-inline void XHeap::check_out_of_memory() {\n-  _page_allocator.check_out_of_memory();\n-}\n-\n-inline bool XHeap::is_oop(uintptr_t addr) const {\n-  return XAddress::is_good(addr) && is_object_aligned(addr) && is_in(addr);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XHEAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeap.inline.hpp","additions":0,"deletions":127,"binary":false,"changes":127,"status":"deleted"},{"patch":"@@ -1,439 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/taskqueue.inline.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xGranuleMap.inline.hpp\"\n-#include \"gc\/x\/xHeapIterator.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-\n-class XHeapIteratorBitMap : public CHeapObj<mtGC> {\n-private:\n-  CHeapBitMap _bitmap;\n-\n-public:\n-  XHeapIteratorBitMap(size_t size_in_bits) :\n-      _bitmap(size_in_bits, mtGC) {}\n-\n-  bool try_set_bit(size_t index) {\n-    return _bitmap.par_set_bit(index);\n-  }\n-};\n-\n-class XHeapIteratorContext {\n-private:\n-  XHeapIterator* const           _iter;\n-  XHeapIteratorQueue* const      _queue;\n-  XHeapIteratorArrayQueue* const _array_queue;\n-  const uint                     _worker_id;\n-  XStatTimerDisable              _timer_disable;\n-\n-public:\n-  XHeapIteratorContext(XHeapIterator* iter, uint worker_id) :\n-      _iter(iter),\n-      _queue(_iter->_queues.queue(worker_id)),\n-      _array_queue(_iter->_array_queues.queue(worker_id)),\n-      _worker_id(worker_id) {}\n-\n-  void mark_and_push(oop obj) const {\n-    if (_iter->mark_object(obj)) {\n-      _queue->push(obj);\n-    }\n-  }\n-\n-  void push_array(const ObjArrayTask& array) const {\n-    _array_queue->push(array);\n-  }\n-\n-  bool pop(oop& obj) const {\n-    return _queue->pop_overflow(obj) || _queue->pop_local(obj);\n-  }\n-\n-  bool pop_array(ObjArrayTask& array) const {\n-    return _array_queue->pop_overflow(array) || _array_queue->pop_local(array);\n-  }\n-\n-  bool steal(oop& obj) const {\n-    return _iter->_queues.steal(_worker_id, obj);\n-  }\n-\n-  bool steal_array(ObjArrayTask& array) const {\n-    return _iter->_array_queues.steal(_worker_id, array);\n-  }\n-\n-  bool is_drained() const {\n-    return _queue->is_empty() && _array_queue->is_empty();\n-  }\n-};\n-\n-template <bool Weak>\n-class XHeapIteratorRootOopClosure : public OopClosure {\n-private:\n-  const XHeapIteratorContext& _context;\n-\n-  oop load_oop(oop* p) {\n-    if (Weak) {\n-      return NativeAccess<AS_NO_KEEPALIVE | ON_PHANTOM_OOP_REF>::oop_load(p);\n-    }\n-\n-    return NativeAccess<AS_NO_KEEPALIVE>::oop_load(p);\n-  }\n-\n-public:\n-  XHeapIteratorRootOopClosure(const XHeapIteratorContext& context) :\n-      _context(context) {}\n-\n-  virtual void do_oop(oop* p) {\n-    const oop obj = load_oop(p);\n-    _context.mark_and_push(obj);\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-template <bool VisitReferents>\n-class XHeapIteratorOopClosure : public OopIterateClosure {\n-private:\n-  const XHeapIteratorContext& _context;\n-  const oop                   _base;\n-\n-  oop load_oop(oop* p) {\n-    assert(XCollectedHeap::heap()->is_in(p), \"Should be in heap\");\n-\n-    if (VisitReferents) {\n-      return HeapAccess<AS_NO_KEEPALIVE | ON_UNKNOWN_OOP_REF>::oop_load_at(_base, _base->field_offset(p));\n-    }\n-\n-    return HeapAccess<AS_NO_KEEPALIVE>::oop_load(p);\n-  }\n-\n-public:\n-  XHeapIteratorOopClosure(const XHeapIteratorContext& context, oop base) :\n-      OopIterateClosure(),\n-      _context(context),\n-      _base(base) {}\n-\n-  virtual ReferenceIterationMode reference_iteration_mode() {\n-    return VisitReferents ? DO_FIELDS : DO_FIELDS_EXCEPT_REFERENT;\n-  }\n-\n-  virtual void do_oop(oop* p) {\n-    const oop obj = load_oop(p);\n-    _context.mark_and_push(obj);\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-\n-  virtual bool do_metadata() {\n-    return true;\n-  }\n-\n-  virtual void do_klass(Klass* k) {\n-    ClassLoaderData* const cld = k->class_loader_data();\n-    XHeapIteratorOopClosure::do_cld(cld);\n-  }\n-\n-  virtual void do_cld(ClassLoaderData* cld) {\n-    class NativeAccessClosure : public OopClosure {\n-    private:\n-      const XHeapIteratorContext& _context;\n-\n-    public:\n-      explicit NativeAccessClosure(const XHeapIteratorContext& context) :\n-          _context(context) {}\n-\n-      virtual void do_oop(oop* p) {\n-        assert(!XCollectedHeap::heap()->is_in(p), \"Should not be in heap\");\n-        const oop obj = NativeAccess<AS_NO_KEEPALIVE>::oop_load(p);\n-        _context.mark_and_push(obj);\n-      }\n-\n-      virtual void do_oop(narrowOop* p) {\n-        ShouldNotReachHere();\n-      }\n-    };\n-\n-    NativeAccessClosure cl(_context);\n-    cld->oops_do(&cl, ClassLoaderData::_claim_other);\n-  }\n-\n-  \/\/ Don't follow loom stack metadata; it's already followed in other ways through CLDs\n-  virtual void do_nmethod(nmethod* nm) {}\n-  virtual void do_method(Method* m) {}\n-};\n-\n-XHeapIterator::XHeapIterator(uint nworkers, bool visit_weaks) :\n-    _visit_weaks(visit_weaks),\n-    _timer_disable(),\n-    _bitmaps(XAddressOffsetMax),\n-    _bitmaps_lock(),\n-    _queues(nworkers),\n-    _array_queues(nworkers),\n-    _roots(ClassLoaderData::_claim_other),\n-    _weak_roots(),\n-    _terminator(nworkers, &_queues) {\n-\n-  \/\/ Create queues\n-  for (uint i = 0; i < _queues.size(); i++) {\n-    XHeapIteratorQueue* const queue = new XHeapIteratorQueue();\n-    _queues.register_queue(i, queue);\n-  }\n-\n-  \/\/ Create array queues\n-  for (uint i = 0; i < _array_queues.size(); i++) {\n-    XHeapIteratorArrayQueue* const array_queue = new XHeapIteratorArrayQueue();\n-    _array_queues.register_queue(i, array_queue);\n-  }\n-}\n-\n-XHeapIterator::~XHeapIterator() {\n-  \/\/ Destroy bitmaps\n-  XHeapIteratorBitMapsIterator iter(&_bitmaps);\n-  for (XHeapIteratorBitMap* bitmap; iter.next(&bitmap);) {\n-    delete bitmap;\n-  }\n-\n-  \/\/ Destroy array queues\n-  for (uint i = 0; i < _array_queues.size(); i++) {\n-    delete _array_queues.queue(i);\n-  }\n-\n-  \/\/ Destroy queues\n-  for (uint i = 0; i < _queues.size(); i++) {\n-    delete _queues.queue(i);\n-  }\n-\n-  \/\/ Clear claimed CLD bits\n-  ClassLoaderDataGraph::clear_claimed_marks(ClassLoaderData::_claim_other);\n-}\n-\n-static size_t object_index_max() {\n-  return XGranuleSize >> XObjectAlignmentSmallShift;\n-}\n-\n-static size_t object_index(oop obj) {\n-  const uintptr_t addr = XOop::to_address(obj);\n-  const uintptr_t offset = XAddress::offset(addr);\n-  const uintptr_t mask = XGranuleSize - 1;\n-  return (offset & mask) >> XObjectAlignmentSmallShift;\n-}\n-\n-XHeapIteratorBitMap* XHeapIterator::object_bitmap(oop obj) {\n-  const uintptr_t offset = XAddress::offset(XOop::to_address(obj));\n-  XHeapIteratorBitMap* bitmap = _bitmaps.get_acquire(offset);\n-  if (bitmap == nullptr) {\n-    XLocker<XLock> locker(&_bitmaps_lock);\n-    bitmap = _bitmaps.get(offset);\n-    if (bitmap == nullptr) {\n-      \/\/ Install new bitmap\n-      bitmap = new XHeapIteratorBitMap(object_index_max());\n-      _bitmaps.release_put(offset, bitmap);\n-    }\n-  }\n-\n-  return bitmap;\n-}\n-\n-bool XHeapIterator::mark_object(oop obj) {\n-  if (obj == nullptr) {\n-    return false;\n-  }\n-\n-  XHeapIteratorBitMap* const bitmap = object_bitmap(obj);\n-  const size_t index = object_index(obj);\n-  return bitmap->try_set_bit(index);\n-}\n-\n-typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_other> XHeapIteratorCLDCLosure;\n-\n-class XHeapIteratorNMethodClosure : public NMethodClosure {\n-private:\n-  OopClosure* const        _cl;\n-  BarrierSetNMethod* const _bs_nm;\n-\n-public:\n-  XHeapIteratorNMethodClosure(OopClosure* cl) :\n-      _cl(cl),\n-      _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()) {}\n-\n-  virtual void do_nmethod(nmethod* nm) {\n-    \/\/ If ClassUnloading is turned off, all nmethods are considered strong,\n-    \/\/ not only those on the call stacks. The heap iteration might happen\n-    \/\/ before the concurrent processign of the code cache, make sure that\n-    \/\/ all nmethods have been processed before visiting the oops.\n-    _bs_nm->nmethod_entry_barrier(nm);\n-\n-    XNMethod::nmethod_oops_do(nm, _cl);\n-  }\n-};\n-\n-class XHeapIteratorThreadClosure : public ThreadClosure {\n-private:\n-  OopClosure* const     _cl;\n-  NMethodClosure* const _nm_cl;\n-\n-public:\n-  XHeapIteratorThreadClosure(OopClosure* cl, NMethodClosure* nm_cl) :\n-      _cl(cl),\n-      _nm_cl(nm_cl) {}\n-\n-  void do_thread(Thread* thread) {\n-    thread->oops_do(_cl, _nm_cl);\n-  }\n-};\n-\n-void XHeapIterator::push_strong_roots(const XHeapIteratorContext& context) {\n-  XHeapIteratorRootOopClosure<false \/* Weak *\/> cl(context);\n-  XHeapIteratorCLDCLosure cld_cl(&cl);\n-  XHeapIteratorNMethodClosure nm_cl(&cl);\n-  XHeapIteratorThreadClosure thread_cl(&cl, &nm_cl);\n-\n-  _roots.apply(&cl,\n-               &cld_cl,\n-               &thread_cl,\n-               &nm_cl);\n-}\n-\n-void XHeapIterator::push_weak_roots(const XHeapIteratorContext& context) {\n-  XHeapIteratorRootOopClosure<true  \/* Weak *\/> cl(context);\n-  _weak_roots.apply(&cl);\n-}\n-\n-template <bool VisitWeaks>\n-void XHeapIterator::push_roots(const XHeapIteratorContext& context) {\n-  push_strong_roots(context);\n-  if (VisitWeaks) {\n-    push_weak_roots(context);\n-  }\n-}\n-\n-template <bool VisitReferents>\n-void XHeapIterator::follow_object(const XHeapIteratorContext& context, oop obj) {\n-  XHeapIteratorOopClosure<VisitReferents> cl(context, obj);\n-  obj->oop_iterate(&cl);\n-}\n-\n-void XHeapIterator::follow_array(const XHeapIteratorContext& context, oop obj) {\n-  \/\/ Follow klass\n-  XHeapIteratorOopClosure<false \/* VisitReferents *\/> cl(context, obj);\n-  cl.do_klass(obj->klass());\n-\n-  \/\/ Push array chunk\n-  context.push_array(ObjArrayTask(obj, 0 \/* index *\/));\n-}\n-\n-void XHeapIterator::follow_array_chunk(const XHeapIteratorContext& context, const ObjArrayTask& array) {\n-  const objArrayOop obj = objArrayOop(array.obj());\n-  const int length = obj->length();\n-  const int start = array.index();\n-  const int stride = MIN2<int>(length - start, ObjArrayMarkingStride);\n-  const int end = start + stride;\n-\n-  \/\/ Push remaining array chunk first\n-  if (end < length) {\n-    context.push_array(ObjArrayTask(obj, end));\n-  }\n-\n-  \/\/ Follow array chunk\n-  XHeapIteratorOopClosure<false \/* VisitReferents *\/> cl(context, obj);\n-  obj->oop_iterate_range(&cl, start, end);\n-}\n-\n-template <bool VisitWeaks>\n-void XHeapIterator::visit_and_follow(const XHeapIteratorContext& context, ObjectClosure* cl, oop obj) {\n-  \/\/ Visit\n-  cl->do_object(obj);\n-\n-  \/\/ Follow\n-  if (obj->is_objArray()) {\n-    follow_array(context, obj);\n-  } else {\n-    follow_object<VisitWeaks>(context, obj);\n-  }\n-}\n-\n-template <bool VisitWeaks>\n-void XHeapIterator::drain(const XHeapIteratorContext& context, ObjectClosure* cl) {\n-  ObjArrayTask array;\n-  oop obj;\n-\n-  do {\n-    while (context.pop(obj)) {\n-      visit_and_follow<VisitWeaks>(context, cl, obj);\n-    }\n-\n-    if (context.pop_array(array)) {\n-      follow_array_chunk(context, array);\n-    }\n-  } while (!context.is_drained());\n-}\n-\n-template <bool VisitWeaks>\n-void XHeapIterator::steal(const XHeapIteratorContext& context, ObjectClosure* cl) {\n-  ObjArrayTask array;\n-  oop obj;\n-\n-  if (context.steal_array(array)) {\n-    follow_array_chunk(context, array);\n-  } else if (context.steal(obj)) {\n-    visit_and_follow<VisitWeaks>(context, cl, obj);\n-  }\n-}\n-\n-template <bool VisitWeaks>\n-void XHeapIterator::drain_and_steal(const XHeapIteratorContext& context, ObjectClosure* cl) {\n-  do {\n-    drain<VisitWeaks>(context, cl);\n-    steal<VisitWeaks>(context, cl);\n-  } while (!context.is_drained() || !_terminator.offer_termination());\n-}\n-\n-template <bool VisitWeaks>\n-void XHeapIterator::object_iterate_inner(const XHeapIteratorContext& context, ObjectClosure* object_cl) {\n-  push_roots<VisitWeaks>(context);\n-  drain_and_steal<VisitWeaks>(context, object_cl);\n-}\n-\n-void XHeapIterator::object_iterate(ObjectClosure* cl, uint worker_id) {\n-  XHeapIteratorContext context(this, worker_id);\n-\n-  if (_visit_weaks) {\n-    object_iterate_inner<true \/* VisitWeaks *\/>(context, cl);\n-  } else {\n-    object_iterate_inner<false \/* VisitWeaks *\/>(context, cl);\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xHeapIterator.cpp","additions":0,"deletions":439,"binary":false,"changes":439,"status":"deleted"},{"patch":"@@ -1,97 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XHEAPITERATOR_HPP\n-#define SHARE_GC_X_XHEAPITERATOR_HPP\n-\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"gc\/shared\/taskTerminator.hpp\"\n-#include \"gc\/shared\/taskqueue.hpp\"\n-#include \"gc\/x\/xGranuleMap.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-#include \"gc\/x\/xRootsIterator.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-\n-class XHeapIteratorBitMap;\n-class XHeapIteratorContext;\n-\n-using XHeapIteratorBitMaps = XGranuleMap<XHeapIteratorBitMap*>;\n-using XHeapIteratorBitMapsIterator = XGranuleMapIterator<XHeapIteratorBitMap*>;\n-using XHeapIteratorQueue = OverflowTaskQueue<oop, mtGC>;\n-using XHeapIteratorQueues = GenericTaskQueueSet<XHeapIteratorQueue, mtGC>;\n-using XHeapIteratorArrayQueue = OverflowTaskQueue<ObjArrayTask, mtGC>;\n-using XHeapIteratorArrayQueues = GenericTaskQueueSet<XHeapIteratorArrayQueue, mtGC>;\n-\n-class XHeapIterator : public ParallelObjectIteratorImpl {\n-  friend class XHeapIteratorContext;\n-\n-private:\n-  const bool               _visit_weaks;\n-  XStatTimerDisable        _timer_disable;\n-  XHeapIteratorBitMaps     _bitmaps;\n-  XLock                    _bitmaps_lock;\n-  XHeapIteratorQueues      _queues;\n-  XHeapIteratorArrayQueues _array_queues;\n-  XRootsIterator           _roots;\n-  XWeakRootsIterator       _weak_roots;\n-  TaskTerminator           _terminator;\n-\n-  XHeapIteratorBitMap* object_bitmap(oop obj);\n-\n-  bool mark_object(oop obj);\n-\n-  void push_strong_roots(const XHeapIteratorContext& context);\n-  void push_weak_roots(const XHeapIteratorContext& context);\n-\n-  template <bool VisitWeaks>\n-  void push_roots(const XHeapIteratorContext& context);\n-\n-  template <bool VisitReferents>\n-  void follow_object(const XHeapIteratorContext& context, oop obj);\n-\n-  void follow_array(const XHeapIteratorContext& context, oop obj);\n-  void follow_array_chunk(const XHeapIteratorContext& context, const ObjArrayTask& array);\n-\n-  template <bool VisitWeaks>\n-  void visit_and_follow(const XHeapIteratorContext& context, ObjectClosure* cl, oop obj);\n-\n-  template <bool VisitWeaks>\n-  void drain(const XHeapIteratorContext& context, ObjectClosure* cl);\n-\n-  template <bool VisitWeaks>\n-  void steal(const XHeapIteratorContext& context, ObjectClosure* cl);\n-\n-  template <bool VisitWeaks>\n-  void drain_and_steal(const XHeapIteratorContext& context, ObjectClosure* cl);\n-\n-  template <bool VisitWeaks>\n-  void object_iterate_inner(const XHeapIteratorContext& context, ObjectClosure* cl);\n-\n-public:\n-  XHeapIterator(uint nworkers, bool visit_weaks);\n-  virtual ~XHeapIterator();\n-\n-  virtual void object_iterate(ObjectClosure* cl, uint worker_id);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XHEAPITERATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeapIterator.hpp","additions":0,"deletions":97,"binary":false,"changes":97,"status":"deleted"},{"patch":"@@ -1,104 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xCPU.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeuristics.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-void XHeuristics::set_medium_page_size() {\n-  \/\/ Set XPageSizeMedium so that a medium page occupies at most 3.125% of the\n-  \/\/ max heap size. XPageSizeMedium is initially set to 0, which means medium\n-  \/\/ pages are effectively disabled. It is adjusted only if XPageSizeMedium\n-  \/\/ becomes larger than XPageSizeSmall.\n-  const size_t min = XGranuleSize;\n-  const size_t max = XGranuleSize * 16;\n-  const size_t unclamped = MaxHeapSize * 0.03125;\n-  const size_t clamped = clamp(unclamped, min, max);\n-  const size_t size = round_down_power_of_2(clamped);\n-\n-  if (size > XPageSizeSmall) {\n-    \/\/ Enable medium pages\n-    XPageSizeMedium             = size;\n-    XPageSizeMediumShift        = log2i_exact(XPageSizeMedium);\n-    XObjectSizeLimitMedium      = XPageSizeMedium \/ 8;\n-    XObjectAlignmentMediumShift = (int)XPageSizeMediumShift - 13;\n-    XObjectAlignmentMedium      = 1 << XObjectAlignmentMediumShift;\n-  }\n-}\n-\n-size_t XHeuristics::relocation_headroom() {\n-  \/\/ Calculate headroom needed to avoid in-place relocation. Each worker will try\n-  \/\/ to allocate a small page, and all workers will share a single medium page.\n-  const uint nworkers = UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n-  return (nworkers * XPageSizeSmall) + XPageSizeMedium;\n-}\n-\n-bool XHeuristics::use_per_cpu_shared_small_pages() {\n-  \/\/ Use per-CPU shared small pages only if these pages occupy at most 3.125%\n-  \/\/ of the max heap size. Otherwise fall back to using a single shared small\n-  \/\/ page. This is useful when using small heaps on large machines.\n-  const size_t per_cpu_share = (MaxHeapSize * 0.03125) \/ XCPU::count();\n-  return per_cpu_share >= XPageSizeSmall;\n-}\n-\n-static uint nworkers_based_on_ncpus(double cpu_share_in_percent) {\n-  return ceil(os::initial_active_processor_count() * cpu_share_in_percent \/ 100.0);\n-}\n-\n-static uint nworkers_based_on_heap_size(double heap_share_in_percent) {\n-  const int nworkers = (MaxHeapSize * (heap_share_in_percent \/ 100.0)) \/ XPageSizeSmall;\n-  return MAX2(nworkers, 1);\n-}\n-\n-static uint nworkers(double cpu_share_in_percent) {\n-  \/\/ Cap number of workers so that they don't use more than 2% of the max heap\n-  \/\/ during relocation. This is useful when using small heaps on large machines.\n-  return MIN2(nworkers_based_on_ncpus(cpu_share_in_percent),\n-              nworkers_based_on_heap_size(2.0));\n-}\n-\n-uint XHeuristics::nparallel_workers() {\n-  \/\/ Use 60% of the CPUs, rounded up. We would like to use as many threads as\n-  \/\/ possible to increase parallelism. However, using a thread count that is\n-  \/\/ close to the number of processors tends to lead to over-provisioning and\n-  \/\/ scheduling latency issues. Using 60% of the active processors appears to\n-  \/\/ be a fairly good balance.\n-  return nworkers(60.0);\n-}\n-\n-uint XHeuristics::nconcurrent_workers() {\n-  \/\/ The number of concurrent threads we would like to use heavily depends\n-  \/\/ on the type of workload we are running. Using too many threads will have\n-  \/\/ a negative impact on the application throughput, while using too few\n-  \/\/ threads will prolong the GC-cycle and we then risk being out-run by the\n-  \/\/ application. When in dynamic mode, use up to 25% of the active processors.\n-  \/\/  When in non-dynamic mode, use 12.5% of the active processors.\n-  return nworkers(UseDynamicNumberOfGCThreads ? 25.0 : 12.5);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xHeuristics.cpp","additions":0,"deletions":104,"binary":false,"changes":104,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XHEURISTICS_HPP\n-#define SHARE_GC_X_XHEURISTICS_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class XHeuristics : public AllStatic {\n-public:\n-  static void set_medium_page_size();\n-\n-  static size_t relocation_headroom();\n-\n-  static bool use_per_cpu_shared_small_pages();\n-\n-  static uint nparallel_workers();\n-  static uint nconcurrent_workers();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xHeuristics.hpp","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,58 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.hpp\"\n-#include \"gc\/x\/xBarrierSet.hpp\"\n-#include \"gc\/x\/xCPU.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeuristics.hpp\"\n-#include \"gc\/x\/xInitialize.hpp\"\n-#include \"gc\/x\/xLargePages.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n-#include \"gc\/x\/xTracer.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/vm_version.hpp\"\n-\n-XInitialize::XInitialize(XBarrierSet* barrier_set) {\n-  log_info(gc, init)(\"Initializing %s\", XName);\n-  log_info(gc, init)(\"Version: %s (%s)\",\n-                     VM_Version::vm_release(),\n-                     VM_Version::jdk_debug_level());\n-  log_info(gc, init)(\"Using deprecated non-generational mode\");\n-\n-  \/\/ Early initialization\n-  XAddress::initialize();\n-  XNUMA::initialize();\n-  XCPU::initialize();\n-  XStatValue::initialize();\n-  XThreadLocalAllocBuffer::initialize();\n-  XTracer::initialize();\n-  XLargePages::initialize();\n-  XHeuristics::set_medium_page_size();\n-  XBarrierSet::set_barrier_set(barrier_set);\n-\n-  pd_initialize();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xInitialize.cpp","additions":0,"deletions":58,"binary":false,"changes":58,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XINITIALIZE_HPP\n-#define SHARE_GC_X_XINITIALIZE_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-\n-class XBarrierSet;\n-\n-class XInitialize {\n-private:\n-  void pd_initialize();\n-\n-public:\n-  XInitialize(XBarrierSet* barrier_set);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XINITIALIZE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xInitialize.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,49 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xLargePages.hpp\"\n-#include \"runtime\/os.hpp\"\n-\n-XLargePages::State XLargePages::_state;\n-\n-void XLargePages::initialize() {\n-  pd_initialize();\n-\n-  log_info_p(gc, init)(\"Memory: \" JULONG_FORMAT \"M\", os::physical_memory() \/ M);\n-  log_info_p(gc, init)(\"Large Page Support: %s\", to_string());\n-}\n-\n-const char* XLargePages::to_string() {\n-  switch (_state) {\n-  case Explicit:\n-    return \"Enabled (Explicit)\";\n-\n-  case Transparent:\n-    return \"Enabled (Transparent)\";\n-\n-  default:\n-    return \"Disabled\";\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xLargePages.cpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"deleted"},{"patch":"@@ -1,51 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLARGEPAGES_HPP\n-#define SHARE_GC_X_XLARGEPAGES_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class XLargePages : public AllStatic {\n-private:\n-  enum State {\n-    Disabled,\n-    Explicit,\n-    Transparent\n-  };\n-\n-  static State _state;\n-\n-  static void pd_initialize();\n-\n-public:\n-  static void initialize();\n-\n-  static bool is_enabled();\n-  static bool is_explicit();\n-  static bool is_transparent();\n-\n-  static const char* to_string();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XLARGEPAGES_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLargePages.hpp","additions":0,"deletions":51,"binary":false,"changes":51,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLARGEPAGES_INLINE_HPP\n-#define SHARE_GC_X_XLARGEPAGES_INLINE_HPP\n-\n-#include \"gc\/x\/xLargePages.hpp\"\n-\n-inline bool XLargePages::is_enabled() {\n-  return _state != Disabled;\n-}\n-\n-inline bool XLargePages::is_explicit() {\n-  return _state == Explicit;\n-}\n-\n-inline bool XLargePages::is_transparent() {\n-  return _state == Transparent;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XLARGEPAGES_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLargePages.inline.hpp","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,116 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLIST_HPP\n-#define SHARE_GC_X_XLIST_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-template <typename T> class XList;\n-\n-\/\/ Element in a doubly linked list\n-template <typename T>\n-class XListNode {\n-  friend class XList<T>;\n-\n-private:\n-  XListNode<T>* _next;\n-  XListNode<T>* _prev;\n-\n-  NONCOPYABLE(XListNode);\n-\n-  void verify_links() const;\n-  void verify_links_linked() const;\n-  void verify_links_unlinked() const;\n-\n-public:\n-  XListNode();\n-  ~XListNode();\n-};\n-\n-\/\/ Doubly linked list\n-template <typename T>\n-class XList {\n-private:\n-  XListNode<T> _head;\n-  size_t       _size;\n-\n-  NONCOPYABLE(XList);\n-\n-  void verify_head() const;\n-\n-  void insert(XListNode<T>* before, XListNode<T>* node);\n-\n-  XListNode<T>* cast_to_inner(T* elem) const;\n-  T* cast_to_outer(XListNode<T>* node) const;\n-\n-public:\n-  XList();\n-\n-  size_t size() const;\n-  bool is_empty() const;\n-\n-  T* first() const;\n-  T* last() const;\n-  T* next(T* elem) const;\n-  T* prev(T* elem) const;\n-\n-  void insert_first(T* elem);\n-  void insert_last(T* elem);\n-  void insert_before(T* before, T* elem);\n-  void insert_after(T* after, T* elem);\n-\n-  void remove(T* elem);\n-  T* remove_first();\n-  T* remove_last();\n-};\n-\n-template <typename T, bool Forward>\n-class XListIteratorImpl : public StackObj {\n-private:\n-  const XList<T>* const _list;\n-  T*                    _next;\n-\n-public:\n-  XListIteratorImpl(const XList<T>* list);\n-\n-  bool next(T** elem);\n-};\n-\n-template <typename T, bool Forward>\n-class XListRemoveIteratorImpl : public StackObj {\n-private:\n-  XList<T>* const _list;\n-\n-public:\n-  XListRemoveIteratorImpl(XList<T>* list);\n-\n-  bool next(T** elem);\n-};\n-\n-template <typename T> using XListIterator = XListIteratorImpl<T, true \/* Forward *\/>;\n-template <typename T> using XListReverseIterator = XListIteratorImpl<T, false \/* Forward *\/>;\n-template <typename T> using XListRemoveIterator = XListRemoveIteratorImpl<T, true \/* Forward *\/>;\n-\n-#endif \/\/ SHARE_GC_X_XLIST_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xList.hpp","additions":0,"deletions":116,"binary":false,"changes":116,"status":"deleted"},{"patch":"@@ -1,238 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLIST_INLINE_HPP\n-#define SHARE_GC_X_XLIST_INLINE_HPP\n-\n-#include \"gc\/x\/xList.hpp\"\n-\n-#include \"utilities\/debug.hpp\"\n-\n-template <typename T>\n-inline XListNode<T>::XListNode() :\n-    _next(this),\n-    _prev(this) {}\n-\n-template <typename T>\n-inline XListNode<T>::~XListNode() {\n-  verify_links_unlinked();\n-}\n-\n-template <typename T>\n-inline void XListNode<T>::verify_links() const {\n-  assert(_next->_prev == this, \"Corrupt list node\");\n-  assert(_prev->_next == this, \"Corrupt list node\");\n-}\n-\n-template <typename T>\n-inline void XListNode<T>::verify_links_linked() const {\n-  assert(_next != this, \"Should be in a list\");\n-  assert(_prev != this, \"Should be in a list\");\n-  verify_links();\n-}\n-\n-template <typename T>\n-inline void XListNode<T>::verify_links_unlinked() const {\n-  assert(_next == this, \"Should not be in a list\");\n-  assert(_prev == this, \"Should not be in a list\");\n-}\n-\n-template <typename T>\n-inline void XList<T>::verify_head() const {\n-  _head.verify_links();\n-}\n-\n-template <typename T>\n-inline void XList<T>::insert(XListNode<T>* before, XListNode<T>* node) {\n-  verify_head();\n-\n-  before->verify_links();\n-  node->verify_links_unlinked();\n-\n-  node->_prev = before;\n-  node->_next = before->_next;\n-  before->_next = node;\n-  node->_next->_prev = node;\n-\n-  before->verify_links_linked();\n-  node->verify_links_linked();\n-\n-  _size++;\n-}\n-\n-template <typename T>\n-inline XListNode<T>* XList<T>::cast_to_inner(T* elem) const {\n-  return &elem->_node;\n-}\n-\n-template <typename T>\n-inline T* XList<T>::cast_to_outer(XListNode<T>* node) const {\n-  return (T*)((uintptr_t)node - offset_of(T, _node));\n-}\n-\n-template <typename T>\n-inline XList<T>::XList() :\n-    _head(),\n-    _size(0) {\n-  verify_head();\n-}\n-\n-template <typename T>\n-inline size_t XList<T>::size() const {\n-  verify_head();\n-  return _size;\n-}\n-\n-template <typename T>\n-inline bool XList<T>::is_empty() const {\n-  return size() == 0;\n-}\n-\n-template <typename T>\n-inline T* XList<T>::first() const {\n-  return is_empty() ? nullptr : cast_to_outer(_head._next);\n-}\n-\n-template <typename T>\n-inline T* XList<T>::last() const {\n-  return is_empty() ? nullptr : cast_to_outer(_head._prev);\n-}\n-\n-template <typename T>\n-inline T* XList<T>::next(T* elem) const {\n-  verify_head();\n-\n-  XListNode<T>* const node = cast_to_inner(elem);\n-  node->verify_links_linked();\n-\n-  XListNode<T>* const next = node->_next;\n-  next->verify_links_linked();\n-\n-  return (next == &_head) ? nullptr : cast_to_outer(next);\n-}\n-\n-template <typename T>\n-inline T* XList<T>::prev(T* elem) const {\n-  verify_head();\n-\n-  XListNode<T>* const node = cast_to_inner(elem);\n-  node->verify_links_linked();\n-\n-  XListNode<T>* const prev = node->_prev;\n-  prev->verify_links_linked();\n-\n-  return (prev == &_head) ? nullptr : cast_to_outer(prev);\n-}\n-\n-template <typename T>\n-inline void XList<T>::insert_first(T* elem) {\n-  insert(&_head, cast_to_inner(elem));\n-}\n-\n-template <typename T>\n-inline void XList<T>::insert_last(T* elem) {\n-  insert(_head._prev, cast_to_inner(elem));\n-}\n-\n-template <typename T>\n-inline void XList<T>::insert_before(T* before, T* elem) {\n-  insert(cast_to_inner(before)->_prev, cast_to_inner(elem));\n-}\n-\n-template <typename T>\n-inline void XList<T>::insert_after(T* after, T* elem) {\n-  insert(cast_to_inner(after), cast_to_inner(elem));\n-}\n-\n-template <typename T>\n-inline void XList<T>::remove(T* elem) {\n-  verify_head();\n-\n-  XListNode<T>* const node = cast_to_inner(elem);\n-  node->verify_links_linked();\n-\n-  XListNode<T>* const next = node->_next;\n-  XListNode<T>* const prev = node->_prev;\n-  next->verify_links_linked();\n-  prev->verify_links_linked();\n-\n-  node->_next = prev->_next;\n-  node->_prev = next->_prev;\n-  node->verify_links_unlinked();\n-\n-  next->_prev = prev;\n-  prev->_next = next;\n-  next->verify_links();\n-  prev->verify_links();\n-\n-  _size--;\n-}\n-\n-template <typename T>\n-inline T* XList<T>::remove_first() {\n-  T* elem = first();\n-  if (elem != nullptr) {\n-    remove(elem);\n-  }\n-\n-  return elem;\n-}\n-\n-template <typename T>\n-inline T* XList<T>::remove_last() {\n-  T* elem = last();\n-  if (elem != nullptr) {\n-    remove(elem);\n-  }\n-\n-  return elem;\n-}\n-\n-template <typename T, bool Forward>\n-inline XListIteratorImpl<T, Forward>::XListIteratorImpl(const XList<T>* list) :\n-    _list(list),\n-    _next(Forward ? list->first() : list->last()) {}\n-\n-template <typename T, bool Forward>\n-inline bool XListIteratorImpl<T, Forward>::next(T** elem) {\n-  if (_next != nullptr) {\n-    *elem = _next;\n-    _next = Forward ? _list->next(_next) : _list->prev(_next);\n-    return true;\n-  }\n-\n-  \/\/ No more elements\n-  return false;\n-}\n-\n-template <typename T, bool Forward>\n-inline XListRemoveIteratorImpl<T, Forward>::XListRemoveIteratorImpl(XList<T>* list) :\n-    _list(list) {}\n-\n-template <typename T, bool Forward>\n-inline bool XListRemoveIteratorImpl<T, Forward>::next(T** elem) {\n-  *elem = Forward ? _list->remove_first() : _list->remove_last();\n-  return *elem != nullptr;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XLIST_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xList.inline.hpp","additions":0,"deletions":238,"binary":false,"changes":238,"status":"deleted"},{"patch":"@@ -1,133 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xLiveMap.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-static const XStatCounter XCounterMarkSeqNumResetContention(\"Contention\", \"Mark SeqNum Reset Contention\", XStatUnitOpsPerSecond);\n-static const XStatCounter XCounterMarkSegmentResetContention(\"Contention\", \"Mark Segment Reset Contention\", XStatUnitOpsPerSecond);\n-\n-static size_t bitmap_size(uint32_t size, size_t nsegments) {\n-  \/\/ We need at least one bit per segment\n-  return MAX2<size_t>(size, nsegments) * 2;\n-}\n-\n-XLiveMap::XLiveMap(uint32_t size) :\n-    _seqnum(0),\n-    _live_objects(0),\n-    _live_bytes(0),\n-    _segment_live_bits(0),\n-    _segment_claim_bits(0),\n-    _bitmap(bitmap_size(size, nsegments)),\n-    _segment_shift(exact_log2(segment_size())) {}\n-\n-void XLiveMap::reset(size_t index) {\n-  const uint32_t seqnum_initializing = (uint32_t)-1;\n-  bool contention = false;\n-\n-  \/\/ Multiple threads can enter here, make sure only one of them\n-  \/\/ resets the marking information while the others busy wait.\n-  for (uint32_t seqnum = Atomic::load_acquire(&_seqnum);\n-       seqnum != XGlobalSeqNum;\n-       seqnum = Atomic::load_acquire(&_seqnum)) {\n-    if ((seqnum != seqnum_initializing) &&\n-        (Atomic::cmpxchg(&_seqnum, seqnum, seqnum_initializing) == seqnum)) {\n-      \/\/ Reset marking information\n-      _live_bytes = 0;\n-      _live_objects = 0;\n-\n-      \/\/ Clear segment claimed\/live bits\n-      segment_live_bits().clear();\n-      segment_claim_bits().clear();\n-\n-      assert(_seqnum == seqnum_initializing, \"Invalid\");\n-\n-      \/\/ Make sure the newly reset marking information is ordered\n-      \/\/ before the update of the page seqnum, such that when the\n-      \/\/ up-to-date seqnum is load acquired, the bit maps will not\n-      \/\/ contain stale information.\n-      Atomic::release_store(&_seqnum, XGlobalSeqNum);\n-      break;\n-    }\n-\n-    \/\/ Mark reset contention\n-    if (!contention) {\n-      \/\/ Count contention once\n-      XStatInc(XCounterMarkSeqNumResetContention);\n-      contention = true;\n-\n-      log_trace(gc)(\"Mark seqnum reset contention, thread: \" PTR_FORMAT \" (%s), map: \" PTR_FORMAT \", bit: \" SIZE_FORMAT,\n-                    XThread::id(), XThread::name(), p2i(this), index);\n-    }\n-  }\n-}\n-\n-void XLiveMap::reset_segment(BitMap::idx_t segment) {\n-  bool contention = false;\n-\n-  if (!claim_segment(segment)) {\n-    \/\/ Already claimed, wait for live bit to be set\n-    while (!is_segment_live(segment)) {\n-      \/\/ Mark reset contention\n-      if (!contention) {\n-        \/\/ Count contention once\n-        XStatInc(XCounterMarkSegmentResetContention);\n-        contention = true;\n-\n-        log_trace(gc)(\"Mark segment reset contention, thread: \" PTR_FORMAT \" (%s), map: \" PTR_FORMAT \", segment: \" SIZE_FORMAT,\n-                      XThread::id(), XThread::name(), p2i(this), segment);\n-      }\n-    }\n-\n-    \/\/ Segment is live\n-    return;\n-  }\n-\n-  \/\/ Segment claimed, clear it\n-  const BitMap::idx_t start_index = segment_start(segment);\n-  const BitMap::idx_t end_index   = segment_end(segment);\n-  if (segment_size() \/ BitsPerWord >= 32) {\n-    _bitmap.clear_large_range(start_index, end_index);\n-  } else {\n-    _bitmap.clear_range(start_index, end_index);\n-  }\n-\n-  \/\/ Set live bit\n-  const bool success = set_segment_live(segment);\n-  assert(success, \"Should never fail\");\n-}\n-\n-void XLiveMap::resize(uint32_t size) {\n-  const size_t new_bitmap_size = bitmap_size(size, nsegments);\n-  if (_bitmap.size() != new_bitmap_size) {\n-    _bitmap.reinitialize(new_bitmap_size, false \/* clear *\/);\n-    _segment_shift = exact_log2(segment_size());\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xLiveMap.cpp","additions":0,"deletions":133,"binary":false,"changes":133,"status":"deleted"},{"patch":"@@ -1,90 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLIVEMAP_HPP\n-#define SHARE_GC_X_XLIVEMAP_HPP\n-\n-#include \"gc\/x\/xBitMap.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class ObjectClosure;\n-\n-class XLiveMap {\n-  friend class XLiveMapTest;\n-\n-private:\n-  static const size_t nsegments = 64;\n-\n-  volatile uint32_t _seqnum;\n-  volatile uint32_t _live_objects;\n-  volatile size_t   _live_bytes;\n-  BitMap::bm_word_t _segment_live_bits;\n-  BitMap::bm_word_t _segment_claim_bits;\n-  XBitMap           _bitmap;\n-  size_t            _segment_shift;\n-\n-  const BitMapView segment_live_bits() const;\n-  const BitMapView segment_claim_bits() const;\n-\n-  BitMapView segment_live_bits();\n-  BitMapView segment_claim_bits();\n-\n-  BitMap::idx_t segment_size() const;\n-\n-  BitMap::idx_t segment_start(BitMap::idx_t segment) const;\n-  BitMap::idx_t segment_end(BitMap::idx_t segment) const;\n-\n-  bool is_segment_live(BitMap::idx_t segment) const;\n-  bool set_segment_live(BitMap::idx_t segment);\n-\n-  BitMap::idx_t first_live_segment() const;\n-  BitMap::idx_t next_live_segment(BitMap::idx_t segment) const;\n-  BitMap::idx_t index_to_segment(BitMap::idx_t index) const;\n-\n-  bool claim_segment(BitMap::idx_t segment);\n-\n-  void reset(size_t index);\n-  void reset_segment(BitMap::idx_t segment);\n-\n-  void iterate_segment(ObjectClosure* cl, BitMap::idx_t segment, uintptr_t page_start, size_t page_object_alignment_shift);\n-\n-public:\n-  XLiveMap(uint32_t size);\n-\n-  void reset();\n-  void resize(uint32_t size);\n-\n-  bool is_marked() const;\n-\n-  uint32_t live_objects() const;\n-  size_t live_bytes() const;\n-\n-  bool get(size_t index) const;\n-  bool set(size_t index, bool finalizable, bool& inc_live);\n-\n-  void inc_live(uint32_t objects, size_t bytes);\n-\n-  void iterate(ObjectClosure* cl, uintptr_t page_start, size_t page_object_alignment_shift);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XLIVEMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLiveMap.hpp","additions":0,"deletions":90,"binary":false,"changes":90,"status":"deleted"},{"patch":"@@ -1,175 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLIVEMAP_INLINE_HPP\n-#define SHARE_GC_X_XLIVEMAP_INLINE_HPP\n-\n-#include \"gc\/x\/xLiveMap.hpp\"\n-\n-#include \"gc\/x\/xBitMap.inline.hpp\"\n-#include \"gc\/x\/xMark.hpp\"\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"gc\/x\/xUtils.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline void XLiveMap::reset() {\n-  _seqnum = 0;\n-}\n-\n-inline bool XLiveMap::is_marked() const {\n-  return Atomic::load_acquire(&_seqnum) == XGlobalSeqNum;\n-}\n-\n-inline uint32_t XLiveMap::live_objects() const {\n-  assert(XGlobalPhase != XPhaseMark, \"Invalid phase\");\n-  return _live_objects;\n-}\n-\n-inline size_t XLiveMap::live_bytes() const {\n-  assert(XGlobalPhase != XPhaseMark, \"Invalid phase\");\n-  return _live_bytes;\n-}\n-\n-inline const BitMapView XLiveMap::segment_live_bits() const {\n-  return BitMapView(const_cast<BitMap::bm_word_t*>(&_segment_live_bits), nsegments);\n-}\n-\n-inline const BitMapView XLiveMap::segment_claim_bits() const {\n-  return BitMapView(const_cast<BitMap::bm_word_t*>(&_segment_claim_bits), nsegments);\n-}\n-\n-inline BitMapView XLiveMap::segment_live_bits() {\n-  return BitMapView(&_segment_live_bits, nsegments);\n-}\n-\n-inline BitMapView XLiveMap::segment_claim_bits() {\n-  return BitMapView(&_segment_claim_bits, nsegments);\n-}\n-\n-inline bool XLiveMap::is_segment_live(BitMap::idx_t segment) const {\n-  return segment_live_bits().par_at(segment);\n-}\n-\n-inline bool XLiveMap::set_segment_live(BitMap::idx_t segment) {\n-  return segment_live_bits().par_set_bit(segment, memory_order_release);\n-}\n-\n-inline bool XLiveMap::claim_segment(BitMap::idx_t segment) {\n-  return segment_claim_bits().par_set_bit(segment, memory_order_acq_rel);\n-}\n-\n-inline BitMap::idx_t XLiveMap::first_live_segment() const {\n-  return segment_live_bits().find_first_set_bit(0, nsegments);\n-}\n-\n-inline BitMap::idx_t XLiveMap::next_live_segment(BitMap::idx_t segment) const {\n-  return segment_live_bits().find_first_set_bit(segment + 1, nsegments);\n-}\n-\n-inline BitMap::idx_t XLiveMap::segment_size() const {\n-  return _bitmap.size() \/ nsegments;\n-}\n-\n-inline BitMap::idx_t XLiveMap::index_to_segment(BitMap::idx_t index) const {\n-  return index >> _segment_shift;\n-}\n-\n-inline bool XLiveMap::get(size_t index) const {\n-  BitMap::idx_t segment = index_to_segment(index);\n-  return is_marked() &&                               \/\/ Page is marked\n-         is_segment_live(segment) &&                  \/\/ Segment is marked\n-         _bitmap.par_at(index, memory_order_relaxed); \/\/ Object is marked\n-}\n-\n-inline bool XLiveMap::set(size_t index, bool finalizable, bool& inc_live) {\n-  if (!is_marked()) {\n-    \/\/ First object to be marked during this\n-    \/\/ cycle, reset marking information.\n-    reset(index);\n-  }\n-\n-  const BitMap::idx_t segment = index_to_segment(index);\n-  if (!is_segment_live(segment)) {\n-    \/\/ First object to be marked in this segment during\n-    \/\/ this cycle, reset segment bitmap.\n-    reset_segment(segment);\n-  }\n-\n-  return _bitmap.par_set_bit_pair(index, finalizable, inc_live);\n-}\n-\n-inline void XLiveMap::inc_live(uint32_t objects, size_t bytes) {\n-  Atomic::add(&_live_objects, objects);\n-  Atomic::add(&_live_bytes, bytes);\n-}\n-\n-inline BitMap::idx_t XLiveMap::segment_start(BitMap::idx_t segment) const {\n-  return segment_size() * segment;\n-}\n-\n-inline BitMap::idx_t XLiveMap::segment_end(BitMap::idx_t segment) const {\n-  return segment_start(segment) + segment_size();\n-}\n-\n-inline void XLiveMap::iterate_segment(ObjectClosure* cl, BitMap::idx_t segment, uintptr_t page_start, size_t page_object_alignment_shift) {\n-  assert(is_segment_live(segment), \"Must be\");\n-\n-  const BitMap::idx_t start_index = segment_start(segment);\n-  const BitMap::idx_t end_index   = segment_end(segment);\n-  BitMap::idx_t index = _bitmap.find_first_set_bit(start_index, end_index);\n-\n-  while (index < end_index) {\n-    \/\/ Calculate object address\n-    const uintptr_t addr = page_start + ((index \/ 2) << page_object_alignment_shift);\n-\n-    \/\/ Get the size of the object before calling the closure, which\n-    \/\/ might overwrite the object in case we are relocating in-place.\n-    const size_t size = XUtils::object_size(addr);\n-\n-    \/\/ Apply closure\n-    cl->do_object(XOop::from_address(addr));\n-\n-    \/\/ Find next bit after this object\n-    const uintptr_t next_addr = align_up(addr + size, 1 << page_object_alignment_shift);\n-    const BitMap::idx_t next_index = ((next_addr - page_start) >> page_object_alignment_shift) * 2;\n-    if (next_index >= end_index) {\n-      \/\/ End of live map\n-      break;\n-    }\n-\n-    index = _bitmap.find_first_set_bit(next_index, end_index);\n-  }\n-}\n-\n-inline void XLiveMap::iterate(ObjectClosure* cl, uintptr_t page_start, size_t page_object_alignment_shift) {\n-  if (is_marked()) {\n-    for (BitMap::idx_t segment = first_live_segment(); segment < nsegments; segment = next_live_segment(segment)) {\n-      \/\/ For each live segment\n-      iterate_segment(cl, segment, page_start, page_object_alignment_shift);\n-    }\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XLIVEMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLiveMap.inline.hpp","additions":0,"deletions":175,"binary":false,"changes":175,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLOCK_HPP\n-#define SHARE_GC_X_XLOCK_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/mutex.hpp\"\n-\n-class XLock {\n-private:\n-  PlatformMutex _lock;\n-\n-public:\n-  void lock();\n-  bool try_lock();\n-  void unlock();\n-};\n-\n-class XReentrantLock {\n-private:\n-  XLock            _lock;\n-  Thread* volatile _owner;\n-  uint64_t         _count;\n-\n-public:\n-  XReentrantLock();\n-\n-  void lock();\n-  void unlock();\n-\n-  bool is_owned() const;\n-};\n-\n-class XConditionLock {\n-private:\n-  PlatformMonitor _lock;\n-\n-public:\n-  void lock();\n-  bool try_lock();\n-  void unlock();\n-\n-  bool wait(uint64_t millis = 0);\n-  void notify();\n-  void notify_all();\n-};\n-\n-template <typename T>\n-class XLocker : public StackObj {\n-private:\n-  T* const _lock;\n-\n-public:\n-  XLocker(T* lock);\n-  ~XLocker();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XLOCK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLock.hpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,120 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XLOCK_INLINE_HPP\n-#define SHARE_GC_X_XLOCK_INLINE_HPP\n-\n-#include \"gc\/x\/xLock.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/os.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline void XLock::lock() {\n-  _lock.lock();\n-}\n-\n-inline bool XLock::try_lock() {\n-  return _lock.try_lock();\n-}\n-\n-inline void XLock::unlock() {\n-  _lock.unlock();\n-}\n-\n-inline XReentrantLock::XReentrantLock() :\n-    _lock(),\n-    _owner(nullptr),\n-    _count(0) {}\n-\n-inline void XReentrantLock::lock() {\n-  Thread* const thread = Thread::current();\n-  Thread* const owner = Atomic::load(&_owner);\n-\n-  if (owner != thread) {\n-    _lock.lock();\n-    Atomic::store(&_owner, thread);\n-  }\n-\n-  _count++;\n-}\n-\n-inline void XReentrantLock::unlock() {\n-  assert(is_owned(), \"Invalid owner\");\n-  assert(_count > 0, \"Invalid count\");\n-\n-  _count--;\n-\n-  if (_count == 0) {\n-    Atomic::store(&_owner, (Thread*)nullptr);\n-    _lock.unlock();\n-  }\n-}\n-\n-inline bool XReentrantLock::is_owned() const {\n-  Thread* const thread = Thread::current();\n-  Thread* const owner = Atomic::load(&_owner);\n-  return owner == thread;\n-}\n-\n-inline void XConditionLock::lock() {\n-  _lock.lock();\n-}\n-\n-inline bool XConditionLock::try_lock() {\n-  return _lock.try_lock();\n-}\n-\n-inline void XConditionLock::unlock() {\n-  _lock.unlock();\n-}\n-\n-inline bool XConditionLock::wait(uint64_t millis) {\n-  return _lock.wait(millis) == OS_OK;\n-}\n-\n-inline void XConditionLock::notify() {\n-  _lock.notify();\n-}\n-\n-inline void XConditionLock::notify_all() {\n-  _lock.notify_all();\n-}\n-\n-template <typename T>\n-inline XLocker<T>::XLocker(T* lock) :\n-    _lock(lock) {\n-  if (_lock != nullptr) {\n-    _lock->lock();\n-  }\n-}\n-\n-template <typename T>\n-inline XLocker<T>::~XLocker() {\n-  if (_lock != nullptr) {\n-    _lock->unlock();\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XLOCK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xLock.inline.hpp","additions":0,"deletions":120,"binary":false,"changes":120,"status":"deleted"},{"patch":"@@ -1,877 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderData.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"classfile\/javaClasses.inline.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"gc\/x\/xAbort.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xMark.inline.hpp\"\n-#include \"gc\/x\/xMarkCache.inline.hpp\"\n-#include \"gc\/x\/xMarkContext.inline.hpp\"\n-#include \"gc\/x\/xMarkStack.inline.hpp\"\n-#include \"gc\/x\/xMarkTerminate.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"gc\/x\/xPage.hpp\"\n-#include \"gc\/x\/xPageTable.inline.hpp\"\n-#include \"gc\/x\/xRootsIterator.hpp\"\n-#include \"gc\/x\/xStackWatermark.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n-#include \"gc\/x\/xUtils.inline.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"oops\/objArrayOop.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/continuation.hpp\"\n-#include \"runtime\/handshake.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/prefetch.inline.hpp\"\n-#include \"runtime\/safepointMechanism.hpp\"\n-#include \"runtime\/stackWatermark.hpp\"\n-#include \"runtime\/stackWatermarkSet.inline.hpp\"\n-#include \"runtime\/threads.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-#include \"utilities\/ticks.hpp\"\n-\n-static const XStatSubPhase XSubPhaseConcurrentMark(\"Concurrent Mark\");\n-static const XStatSubPhase XSubPhaseConcurrentMarkTryFlush(\"Concurrent Mark Try Flush\");\n-static const XStatSubPhase XSubPhaseConcurrentMarkTryTerminate(\"Concurrent Mark Try Terminate\");\n-static const XStatSubPhase XSubPhaseMarkTryComplete(\"Pause Mark Try Complete\");\n-\n-XMark::XMark(XWorkers* workers, XPageTable* page_table) :\n-    _workers(workers),\n-    _page_table(page_table),\n-    _allocator(),\n-    _stripes(),\n-    _terminate(),\n-    _work_terminateflush(true),\n-    _work_nproactiveflush(0),\n-    _work_nterminateflush(0),\n-    _nproactiveflush(0),\n-    _nterminateflush(0),\n-    _ntrycomplete(0),\n-    _ncontinue(0),\n-    _nworkers(0) {}\n-\n-bool XMark::is_initialized() const {\n-  return _allocator.is_initialized();\n-}\n-\n-size_t XMark::calculate_nstripes(uint nworkers) const {\n-  \/\/ Calculate the number of stripes from the number of workers we use,\n-  \/\/ where the number of stripes must be a power of two and we want to\n-  \/\/ have at least one worker per stripe.\n-  const size_t nstripes = round_down_power_of_2(nworkers);\n-  return MIN2(nstripes, XMarkStripesMax);\n-}\n-\n-void XMark::start() {\n-  \/\/ Verification\n-  if (ZVerifyMarking) {\n-    verify_all_stacks_empty();\n-  }\n-\n-  \/\/ Increment global sequence number to invalidate\n-  \/\/ marking information for all pages.\n-  XGlobalSeqNum++;\n-\n-  \/\/ Note that we start a marking cycle.\n-  \/\/ Unlike other GCs, the color switch implicitly changes the nmethods\n-  \/\/ to be armed, and the thread-local disarm values are lazily updated\n-  \/\/ when JavaThreads wake up from safepoints.\n-  CodeCache::on_gc_marking_cycle_start();\n-\n-  \/\/ Reset flush\/continue counters\n-  _nproactiveflush = 0;\n-  _nterminateflush = 0;\n-  _ntrycomplete = 0;\n-  _ncontinue = 0;\n-\n-  \/\/ Set number of workers to use\n-  _nworkers = _workers->active_workers();\n-\n-  \/\/ Set number of mark stripes to use, based on number\n-  \/\/ of workers we will use in the concurrent mark phase.\n-  const size_t nstripes = calculate_nstripes(_nworkers);\n-  _stripes.set_nstripes(nstripes);\n-\n-  \/\/ Update statistics\n-  XStatMark::set_at_mark_start(nstripes);\n-\n-  \/\/ Print worker\/stripe distribution\n-  LogTarget(Debug, gc, marking) log;\n-  if (log.is_enabled()) {\n-    log.print(\"Mark Worker\/Stripe Distribution\");\n-    for (uint worker_id = 0; worker_id < _nworkers; worker_id++) {\n-      const XMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, worker_id);\n-      const size_t stripe_id = _stripes.stripe_id(stripe);\n-      log.print(\"  Worker %u(%u) -> Stripe \" SIZE_FORMAT \"(\" SIZE_FORMAT \")\",\n-                worker_id, _nworkers, stripe_id, nstripes);\n-    }\n-  }\n-}\n-\n-void XMark::prepare_work() {\n-  assert(_nworkers == _workers->active_workers(), \"Invalid number of workers\");\n-\n-  \/\/ Set number of active workers\n-  _terminate.reset(_nworkers);\n-\n-  \/\/ Reset flush counters\n-  _work_nproactiveflush = _work_nterminateflush = 0;\n-  _work_terminateflush = true;\n-}\n-\n-void XMark::finish_work() {\n-  \/\/ Accumulate proactive\/terminate flush counters\n-  _nproactiveflush += _work_nproactiveflush;\n-  _nterminateflush += _work_nterminateflush;\n-}\n-\n-bool XMark::is_array(uintptr_t addr) const {\n-  return XOop::from_address(addr)->is_objArray();\n-}\n-\n-void XMark::push_partial_array(uintptr_t addr, size_t size, bool finalizable) {\n-  assert(is_aligned(addr, XMarkPartialArrayMinSize), \"Address misaligned\");\n-  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(Thread::current());\n-  XMarkStripe* const stripe = _stripes.stripe_for_addr(addr);\n-  const uintptr_t offset = XAddress::offset(addr) >> XMarkPartialArrayMinSizeShift;\n-  const uintptr_t length = size \/ oopSize;\n-  const XMarkStackEntry entry(offset, length, finalizable);\n-\n-  log_develop_trace(gc, marking)(\"Array push partial: \" PTR_FORMAT \" (\" SIZE_FORMAT \"), stripe: \" SIZE_FORMAT,\n-                                 addr, size, _stripes.stripe_id(stripe));\n-\n-  stacks->push(&_allocator, &_stripes, stripe, entry, false \/* publish *\/);\n-}\n-\n-void XMark::follow_small_array(uintptr_t addr, size_t size, bool finalizable) {\n-  assert(size <= XMarkPartialArrayMinSize, \"Too large, should be split\");\n-  const size_t length = size \/ oopSize;\n-\n-  log_develop_trace(gc, marking)(\"Array follow small: \" PTR_FORMAT \" (\" SIZE_FORMAT \")\", addr, size);\n-\n-  XBarrier::mark_barrier_on_oop_array((oop*)addr, length, finalizable);\n-}\n-\n-void XMark::follow_large_array(uintptr_t addr, size_t size, bool finalizable) {\n-  assert(size <= (size_t)arrayOopDesc::max_array_length(T_OBJECT) * oopSize, \"Too large\");\n-  assert(size > XMarkPartialArrayMinSize, \"Too small, should not be split\");\n-  const uintptr_t start = addr;\n-  const uintptr_t end = start + size;\n-\n-  \/\/ Calculate the aligned middle start\/end\/size, where the middle start\n-  \/\/ should always be greater than the start (hence the +1 below) to make\n-  \/\/ sure we always do some follow work, not just split the array into pieces.\n-  const uintptr_t middle_start = align_up(start + 1, XMarkPartialArrayMinSize);\n-  const size_t    middle_size = align_down(end - middle_start, XMarkPartialArrayMinSize);\n-  const uintptr_t middle_end = middle_start + middle_size;\n-\n-  log_develop_trace(gc, marking)(\"Array follow large: \" PTR_FORMAT \"-\" PTR_FORMAT\" (\" SIZE_FORMAT \"), \"\n-                                 \"middle: \" PTR_FORMAT \"-\" PTR_FORMAT \" (\" SIZE_FORMAT \")\",\n-                                 start, end, size, middle_start, middle_end, middle_size);\n-\n-  \/\/ Push unaligned trailing part\n-  if (end > middle_end) {\n-    const uintptr_t trailing_addr = middle_end;\n-    const size_t trailing_size = end - middle_end;\n-    push_partial_array(trailing_addr, trailing_size, finalizable);\n-  }\n-\n-  \/\/ Push aligned middle part(s)\n-  uintptr_t partial_addr = middle_end;\n-  while (partial_addr > middle_start) {\n-    const size_t parts = 2;\n-    const size_t partial_size = align_up((partial_addr - middle_start) \/ parts, XMarkPartialArrayMinSize);\n-    partial_addr -= partial_size;\n-    push_partial_array(partial_addr, partial_size, finalizable);\n-  }\n-\n-  \/\/ Follow leading part\n-  assert(start < middle_start, \"Miscalculated middle start\");\n-  const uintptr_t leading_addr = start;\n-  const size_t leading_size = middle_start - start;\n-  follow_small_array(leading_addr, leading_size, finalizable);\n-}\n-\n-void XMark::follow_array(uintptr_t addr, size_t size, bool finalizable) {\n-  if (size <= XMarkPartialArrayMinSize) {\n-    follow_small_array(addr, size, finalizable);\n-  } else {\n-    follow_large_array(addr, size, finalizable);\n-  }\n-}\n-\n-void XMark::follow_partial_array(XMarkStackEntry entry, bool finalizable) {\n-  const uintptr_t addr = XAddress::good(entry.partial_array_offset() << XMarkPartialArrayMinSizeShift);\n-  const size_t size = entry.partial_array_length() * oopSize;\n-\n-  follow_array(addr, size, finalizable);\n-}\n-\n-template <bool finalizable>\n-class XMarkBarrierOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n-public:\n-  XMarkBarrierOopClosure() :\n-      ClaimMetadataVisitingOopIterateClosure(finalizable\n-                                                 ? ClassLoaderData::_claim_finalizable\n-                                                 : ClassLoaderData::_claim_strong,\n-                                             finalizable\n-                                                 ? nullptr\n-                                                 : XHeap::heap()->reference_discoverer()) {}\n-\n-  virtual void do_oop(oop* p) {\n-    XBarrier::mark_barrier_on_oop_field(p, finalizable);\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-\n-  virtual void do_nmethod(nmethod* nm) {\n-    assert(!finalizable, \"Can't handle finalizable marking of nmethods\");\n-    nm->run_nmethod_entry_barrier();\n-  }\n-};\n-\n-void XMark::follow_array_object(objArrayOop obj, bool finalizable) {\n-  if (finalizable) {\n-    XMarkBarrierOopClosure<true \/* finalizable *\/> cl;\n-    cl.do_klass(obj->klass());\n-  } else {\n-    XMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n-    cl.do_klass(obj->klass());\n-  }\n-\n-  const uintptr_t addr = (uintptr_t)obj->base();\n-  const size_t size = (size_t)obj->length() * oopSize;\n-\n-  follow_array(addr, size, finalizable);\n-}\n-\n-void XMark::follow_object(oop obj, bool finalizable) {\n-  if (ContinuationGCSupport::relativize_stack_chunk(obj)) {\n-    \/\/ Loom doesn't support mixing of finalizable marking and strong marking of\n-    \/\/ stack chunks. See: RelativizeDerivedOopClosure.\n-    XMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n-    obj->oop_iterate(&cl);\n-    return;\n-  }\n-\n-  if (finalizable) {\n-    XMarkBarrierOopClosure<true \/* finalizable *\/> cl;\n-    obj->oop_iterate(&cl);\n-  } else {\n-    XMarkBarrierOopClosure<false \/* finalizable *\/> cl;\n-    obj->oop_iterate(&cl);\n-  }\n-}\n-\n-static void try_deduplicate(XMarkContext* context, oop obj) {\n-  if (!StringDedup::is_enabled()) {\n-    \/\/ Not enabled\n-    return;\n-  }\n-\n-  if (!java_lang_String::is_instance(obj)) {\n-    \/\/ Not a String object\n-    return;\n-  }\n-\n-  if (java_lang_String::test_and_set_deduplication_requested(obj)) {\n-    \/\/ Already requested deduplication\n-    return;\n-  }\n-\n-  \/\/ Request deduplication\n-  context->string_dedup_requests()->add(obj);\n-}\n-\n-void XMark::mark_and_follow(XMarkContext* context, XMarkStackEntry entry) {\n-  \/\/ Decode flags\n-  const bool finalizable = entry.finalizable();\n-  const bool partial_array = entry.partial_array();\n-\n-  if (partial_array) {\n-    follow_partial_array(entry, finalizable);\n-    return;\n-  }\n-\n-  \/\/ Decode object address and additional flags\n-  const uintptr_t addr = entry.object_address();\n-  const bool mark = entry.mark();\n-  bool inc_live = entry.inc_live();\n-  const bool follow = entry.follow();\n-\n-  XPage* const page = _page_table->get(addr);\n-  assert(page->is_relocatable(), \"Invalid page state\");\n-\n-  \/\/ Mark\n-  if (mark && !page->mark_object(addr, finalizable, inc_live)) {\n-    \/\/ Already marked\n-    return;\n-  }\n-\n-  \/\/ Increment live\n-  if (inc_live) {\n-    \/\/ Update live objects\/bytes for page. We use the aligned object\n-    \/\/ size since that is the actual number of bytes used on the page\n-    \/\/ and alignment paddings can never be reclaimed.\n-    const size_t size = XUtils::object_size(addr);\n-    const size_t aligned_size = align_up(size, page->object_alignment());\n-    context->cache()->inc_live(page, aligned_size);\n-  }\n-\n-  \/\/ Follow\n-  if (follow) {\n-    if (is_array(addr)) {\n-      follow_array_object(objArrayOop(XOop::from_address(addr)), finalizable);\n-    } else {\n-      const oop obj = XOop::from_address(addr);\n-      follow_object(obj, finalizable);\n-\n-      if (!finalizable) {\n-        \/\/ Try deduplicate\n-        try_deduplicate(context, obj);\n-      }\n-    }\n-  }\n-}\n-\n-template <typename T>\n-bool XMark::drain(XMarkContext* context, T* timeout) {\n-  XMarkStripe* const stripe = context->stripe();\n-  XMarkThreadLocalStacks* const stacks = context->stacks();\n-  XMarkStackEntry entry;\n-\n-  \/\/ Drain stripe stacks\n-  while (stacks->pop(&_allocator, &_stripes, stripe, entry)) {\n-    mark_and_follow(context, entry);\n-\n-    \/\/ Check timeout\n-    if (timeout->has_expired()) {\n-      \/\/ Timeout\n-      return false;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return !timeout->has_expired();\n-}\n-\n-bool XMark::try_steal_local(XMarkContext* context) {\n-  XMarkStripe* const stripe = context->stripe();\n-  XMarkThreadLocalStacks* const stacks = context->stacks();\n-\n-  \/\/ Try to steal a local stack from another stripe\n-  for (XMarkStripe* victim_stripe = _stripes.stripe_next(stripe);\n-       victim_stripe != stripe;\n-       victim_stripe = _stripes.stripe_next(victim_stripe)) {\n-    XMarkStack* const stack = stacks->steal(&_stripes, victim_stripe);\n-    if (stack != nullptr) {\n-      \/\/ Success, install the stolen stack\n-      stacks->install(&_stripes, stripe, stack);\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Nothing to steal\n-  return false;\n-}\n-\n-bool XMark::try_steal_global(XMarkContext* context) {\n-  XMarkStripe* const stripe = context->stripe();\n-  XMarkThreadLocalStacks* const stacks = context->stacks();\n-\n-  \/\/ Try to steal a stack from another stripe\n-  for (XMarkStripe* victim_stripe = _stripes.stripe_next(stripe);\n-       victim_stripe != stripe;\n-       victim_stripe = _stripes.stripe_next(victim_stripe)) {\n-    XMarkStack* const stack = victim_stripe->steal_stack();\n-    if (stack != nullptr) {\n-      \/\/ Success, install the stolen stack\n-      stacks->install(&_stripes, stripe, stack);\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Nothing to steal\n-  return false;\n-}\n-\n-bool XMark::try_steal(XMarkContext* context) {\n-  return try_steal_local(context) || try_steal_global(context);\n-}\n-\n-void XMark::idle() const {\n-  os::naked_short_sleep(1);\n-}\n-\n-class XMarkFlushAndFreeStacksClosure : public HandshakeClosure {\n-private:\n-  XMark* const _mark;\n-  bool         _flushed;\n-\n-public:\n-  XMarkFlushAndFreeStacksClosure(XMark* mark) :\n-      HandshakeClosure(\"XMarkFlushAndFreeStacks\"),\n-      _mark(mark),\n-      _flushed(false) {}\n-\n-  void do_thread(Thread* thread) {\n-    if (_mark->flush_and_free(thread)) {\n-      _flushed = true;\n-    }\n-  }\n-\n-  bool flushed() const {\n-    return _flushed;\n-  }\n-};\n-\n-bool XMark::flush(bool at_safepoint) {\n-  XMarkFlushAndFreeStacksClosure cl(this);\n-  if (at_safepoint) {\n-    Threads::threads_do(&cl);\n-  } else {\n-    Handshake::execute(&cl);\n-  }\n-\n-  \/\/ Returns true if more work is available\n-  return cl.flushed() || !_stripes.is_empty();\n-}\n-\n-bool XMark::try_flush(volatile size_t* nflush) {\n-  Atomic::inc(nflush);\n-\n-  XStatTimer timer(XSubPhaseConcurrentMarkTryFlush);\n-  return flush(false \/* at_safepoint *\/);\n-}\n-\n-bool XMark::try_proactive_flush() {\n-  \/\/ Only do proactive flushes from worker 0\n-  if (XThread::worker_id() != 0) {\n-    return false;\n-  }\n-\n-  if (Atomic::load(&_work_nproactiveflush) == XMarkProactiveFlushMax ||\n-      Atomic::load(&_work_nterminateflush) != 0) {\n-    \/\/ Limit reached or we're trying to terminate\n-    return false;\n-  }\n-\n-  return try_flush(&_work_nproactiveflush);\n-}\n-\n-bool XMark::try_terminate() {\n-  XStatTimer timer(XSubPhaseConcurrentMarkTryTerminate);\n-\n-  if (_terminate.enter_stage0()) {\n-    \/\/ Last thread entered stage 0, flush\n-    if (Atomic::load(&_work_terminateflush) &&\n-        Atomic::load(&_work_nterminateflush) != XMarkTerminateFlushMax) {\n-      \/\/ Exit stage 0 to allow other threads to continue marking\n-      _terminate.exit_stage0();\n-\n-      \/\/ Flush before termination\n-      if (!try_flush(&_work_nterminateflush)) {\n-        \/\/ No more work available, skip further flush attempts\n-        Atomic::store(&_work_terminateflush, false);\n-      }\n-\n-      \/\/ Don't terminate, regardless of whether we successfully\n-      \/\/ flushed out more work or not. We've already exited\n-      \/\/ termination stage 0, to allow other threads to continue\n-      \/\/ marking, so this thread has to return false and also\n-      \/\/ make another round of attempted marking.\n-      return false;\n-    }\n-  }\n-\n-  for (;;) {\n-    if (_terminate.enter_stage1()) {\n-      \/\/ Last thread entered stage 1, terminate\n-      return true;\n-    }\n-\n-    \/\/ Idle to give the other threads\n-    \/\/ a chance to enter termination.\n-    idle();\n-\n-    if (!_terminate.try_exit_stage1()) {\n-      \/\/ All workers in stage 1, terminate\n-      return true;\n-    }\n-\n-    if (_terminate.try_exit_stage0()) {\n-      \/\/ More work available, don't terminate\n-      return false;\n-    }\n-  }\n-}\n-\n-class XMarkNoTimeout : public StackObj {\n-public:\n-  bool has_expired() {\n-    \/\/ No timeout, but check for signal to abort\n-    return XAbort::should_abort();\n-  }\n-};\n-\n-void XMark::work_without_timeout(XMarkContext* context) {\n-  XStatTimer timer(XSubPhaseConcurrentMark);\n-  XMarkNoTimeout no_timeout;\n-\n-  for (;;) {\n-    if (!drain(context, &no_timeout)) {\n-      \/\/ Abort\n-      break;\n-    }\n-\n-    if (try_steal(context)) {\n-      \/\/ Stole work\n-      continue;\n-    }\n-\n-    if (try_proactive_flush()) {\n-      \/\/ Work available\n-      continue;\n-    }\n-\n-    if (try_terminate()) {\n-      \/\/ Terminate\n-      break;\n-    }\n-  }\n-}\n-\n-class XMarkTimeout : public StackObj {\n-private:\n-  const Ticks    _start;\n-  const uint64_t _timeout;\n-  const uint64_t _check_interval;\n-  uint64_t       _check_at;\n-  uint64_t       _check_count;\n-  bool           _expired;\n-\n-public:\n-  XMarkTimeout(uint64_t timeout_in_micros) :\n-      _start(Ticks::now()),\n-      _timeout(_start.value() + TimeHelper::micros_to_counter(timeout_in_micros)),\n-      _check_interval(200),\n-      _check_at(_check_interval),\n-      _check_count(0),\n-      _expired(false) {}\n-\n-  ~XMarkTimeout() {\n-    const Tickspan duration = Ticks::now() - _start;\n-    log_debug(gc, marking)(\"Mark With Timeout (%s): %s, \" UINT64_FORMAT \" oops, %.3fms\",\n-                           XThread::name(), _expired ? \"Expired\" : \"Completed\",\n-                           _check_count, TimeHelper::counter_to_millis(duration.value()));\n-  }\n-\n-  bool has_expired() {\n-    if (++_check_count == _check_at) {\n-      _check_at += _check_interval;\n-      if ((uint64_t)Ticks::now().value() >= _timeout) {\n-        \/\/ Timeout\n-        _expired = true;\n-      }\n-    }\n-\n-    return _expired;\n-  }\n-};\n-\n-void XMark::work_with_timeout(XMarkContext* context, uint64_t timeout_in_micros) {\n-  XStatTimer timer(XSubPhaseMarkTryComplete);\n-  XMarkTimeout timeout(timeout_in_micros);\n-\n-  for (;;) {\n-    if (!drain(context, &timeout)) {\n-      \/\/ Timed out\n-      break;\n-    }\n-\n-    if (try_steal(context)) {\n-      \/\/ Stole work\n-      continue;\n-    }\n-\n-    \/\/ Terminate\n-    break;\n-  }\n-}\n-\n-void XMark::work(uint64_t timeout_in_micros) {\n-  XMarkStripe* const stripe = _stripes.stripe_for_worker(_nworkers, XThread::worker_id());\n-  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(Thread::current());\n-  XMarkContext context(_stripes.nstripes(), stripe, stacks);\n-\n-  if (timeout_in_micros == 0) {\n-    work_without_timeout(&context);\n-  } else {\n-    work_with_timeout(&context, timeout_in_micros);\n-  }\n-\n-  \/\/ Flush and publish stacks\n-  stacks->flush(&_allocator, &_stripes);\n-\n-  \/\/ Free remaining stacks\n-  stacks->free(&_allocator);\n-}\n-\n-class XMarkOopClosure : public OopClosure {\n-  virtual void do_oop(oop* p) {\n-    XBarrier::mark_barrier_on_oop_field(p, false \/* finalizable *\/);\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-class XMarkThreadClosure : public ThreadClosure {\n-private:\n-  OopClosure* const _cl;\n-\n-public:\n-  XMarkThreadClosure(OopClosure* cl) :\n-      _cl(cl) {\n-    XThreadLocalAllocBuffer::reset_statistics();\n-  }\n-  ~XMarkThreadClosure() {\n-    XThreadLocalAllocBuffer::publish_statistics();\n-  }\n-  virtual void do_thread(Thread* thread) {\n-    JavaThread* const jt = JavaThread::cast(thread);\n-    StackWatermarkSet::finish_processing(jt, _cl, StackWatermarkKind::gc);\n-    XThreadLocalAllocBuffer::update_stats(jt);\n-  }\n-};\n-\n-class XMarkNMethodClosure : public NMethodClosure {\n-private:\n-  OopClosure* const _cl;\n-\n-public:\n-  XMarkNMethodClosure(OopClosure* cl) :\n-      _cl(cl) {}\n-\n-  virtual void do_nmethod(nmethod* nm) {\n-    XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n-    if (XNMethod::is_armed(nm)) {\n-      XNMethod::nmethod_oops_do_inner(nm, _cl);\n-\n-      \/\/ CodeCache unloading support\n-      nm->mark_as_maybe_on_stack();\n-\n-      XNMethod::disarm(nm);\n-    }\n-  }\n-};\n-\n-typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_strong> XMarkCLDClosure;\n-\n-class XMarkRootsTask : public XTask {\n-private:\n-  XMark* const               _mark;\n-  SuspendibleThreadSetJoiner _sts_joiner;\n-  XRootsIterator             _roots;\n-\n-  XMarkOopClosure            _cl;\n-  XMarkCLDClosure            _cld_cl;\n-  XMarkThreadClosure         _thread_cl;\n-  XMarkNMethodClosure        _nm_cl;\n-\n-public:\n-  XMarkRootsTask(XMark* mark) :\n-      XTask(\"XMarkRootsTask\"),\n-      _mark(mark),\n-      _sts_joiner(),\n-      _roots(ClassLoaderData::_claim_strong),\n-      _cl(),\n-      _cld_cl(&_cl),\n-      _thread_cl(&_cl),\n-      _nm_cl(&_cl) {\n-    ClassLoaderDataGraph_lock->lock();\n-  }\n-\n-  ~XMarkRootsTask() {\n-    ClassLoaderDataGraph_lock->unlock();\n-  }\n-\n-  virtual void work() {\n-    _roots.apply(&_cl,\n-                 &_cld_cl,\n-                 &_thread_cl,\n-                 &_nm_cl);\n-\n-    \/\/ Flush and free worker stacks. Needed here since\n-    \/\/ the set of workers executing during root scanning\n-    \/\/ can be different from the set of workers executing\n-    \/\/ during mark.\n-    _mark->flush_and_free();\n-  }\n-};\n-\n-class XMarkTask : public XTask {\n-private:\n-  XMark* const   _mark;\n-  const uint64_t _timeout_in_micros;\n-\n-public:\n-  XMarkTask(XMark* mark, uint64_t timeout_in_micros = 0) :\n-      XTask(\"XMarkTask\"),\n-      _mark(mark),\n-      _timeout_in_micros(timeout_in_micros) {\n-    _mark->prepare_work();\n-  }\n-\n-  ~XMarkTask() {\n-    _mark->finish_work();\n-  }\n-\n-  virtual void work() {\n-    _mark->work(_timeout_in_micros);\n-  }\n-};\n-\n-void XMark::mark(bool initial) {\n-  if (initial) {\n-    XMarkRootsTask task(this);\n-    _workers->run(&task);\n-  }\n-\n-  XMarkTask task(this);\n-  _workers->run(&task);\n-}\n-\n-bool XMark::try_complete() {\n-  _ntrycomplete++;\n-\n-  \/\/ Use nconcurrent number of worker threads to maintain the\n-  \/\/ worker\/stripe distribution used during concurrent mark.\n-  XMarkTask task(this, XMarkCompleteTimeout);\n-  _workers->run(&task);\n-\n-  \/\/ Successful if all stripes are empty\n-  return _stripes.is_empty();\n-}\n-\n-bool XMark::try_end() {\n-  \/\/ Flush all mark stacks\n-  if (!flush(true \/* at_safepoint *\/)) {\n-    \/\/ Mark completed\n-    return true;\n-  }\n-\n-  \/\/ Try complete marking by doing a limited\n-  \/\/ amount of mark work in this phase.\n-  return try_complete();\n-}\n-\n-bool XMark::end() {\n-  \/\/ Try end marking\n-  if (!try_end()) {\n-    \/\/ Mark not completed\n-    _ncontinue++;\n-    return false;\n-  }\n-\n-  \/\/ Verification\n-  if (ZVerifyMarking) {\n-    verify_all_stacks_empty();\n-  }\n-\n-  \/\/ Update statistics\n-  XStatMark::set_at_mark_end(_nproactiveflush, _nterminateflush, _ntrycomplete, _ncontinue);\n-\n-  \/\/ Note that we finished a marking cycle.\n-  \/\/ Unlike other GCs, we do not arm the nmethods\n-  \/\/ when marking terminates.\n-  CodeCache::on_gc_marking_cycle_finish();\n-\n-  \/\/ Mark completed\n-  return true;\n-}\n-\n-void XMark::free() {\n-  \/\/ Free any unused mark stack space\n-  _allocator.free();\n-\n-  \/\/ Update statistics\n-  XStatMark::set_at_mark_free(_allocator.size());\n-}\n-\n-void XMark::flush_and_free() {\n-  Thread* const thread = Thread::current();\n-  flush_and_free(thread);\n-}\n-\n-bool XMark::flush_and_free(Thread* thread) {\n-  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(thread);\n-  const bool flushed = stacks->flush(&_allocator, &_stripes);\n-  stacks->free(&_allocator);\n-  return flushed;\n-}\n-\n-class XVerifyMarkStacksEmptyClosure : public ThreadClosure {\n-private:\n-  const XMarkStripeSet* const _stripes;\n-\n-public:\n-  XVerifyMarkStacksEmptyClosure(const XMarkStripeSet* stripes) :\n-      _stripes(stripes) {}\n-\n-  void do_thread(Thread* thread) {\n-    XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(thread);\n-    guarantee(stacks->is_empty(_stripes), \"Should be empty\");\n-  }\n-};\n-\n-void XMark::verify_all_stacks_empty() const {\n-  \/\/ Verify thread stacks\n-  XVerifyMarkStacksEmptyClosure cl(&_stripes);\n-  Threads::threads_do(&cl);\n-\n-  \/\/ Verify stripe stacks\n-  guarantee(_stripes.is_empty(), \"Should be empty\");\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xMark.cpp","additions":0,"deletions":877,"binary":false,"changes":877,"status":"deleted"},{"patch":"@@ -1,106 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARK_HPP\n-#define SHARE_GC_X_XMARK_HPP\n-\n-#include \"gc\/x\/xMarkStack.hpp\"\n-#include \"gc\/x\/xMarkStackAllocator.hpp\"\n-#include \"gc\/x\/xMarkStackEntry.hpp\"\n-#include \"gc\/x\/xMarkTerminate.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class Thread;\n-class XMarkContext;\n-class XPageTable;\n-class XWorkers;\n-\n-class XMark {\n-  friend class XMarkTask;\n-\n-private:\n-  XWorkers* const     _workers;\n-  XPageTable* const   _page_table;\n-  XMarkStackAllocator _allocator;\n-  XMarkStripeSet      _stripes;\n-  XMarkTerminate      _terminate;\n-  volatile bool       _work_terminateflush;\n-  volatile size_t     _work_nproactiveflush;\n-  volatile size_t     _work_nterminateflush;\n-  size_t              _nproactiveflush;\n-  size_t              _nterminateflush;\n-  size_t              _ntrycomplete;\n-  size_t              _ncontinue;\n-  uint                _nworkers;\n-\n-  size_t calculate_nstripes(uint nworkers) const;\n-\n-  bool is_array(uintptr_t addr) const;\n-  void push_partial_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_small_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_large_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_array(uintptr_t addr, size_t size, bool finalizable);\n-  void follow_partial_array(XMarkStackEntry entry, bool finalizable);\n-  void follow_array_object(objArrayOop obj, bool finalizable);\n-  void follow_object(oop obj, bool finalizable);\n-  void mark_and_follow(XMarkContext* context, XMarkStackEntry entry);\n-\n-  template <typename T> bool drain(XMarkContext* context, T* timeout);\n-  bool try_steal_local(XMarkContext* context);\n-  bool try_steal_global(XMarkContext* context);\n-  bool try_steal(XMarkContext* context);\n-  void idle() const;\n-  bool flush(bool at_safepoint);\n-  bool try_proactive_flush();\n-  bool try_flush(volatile size_t* nflush);\n-  bool try_terminate();\n-  bool try_complete();\n-  bool try_end();\n-\n-  void prepare_work();\n-  void finish_work();\n-\n-  void work_without_timeout(XMarkContext* context);\n-  void work_with_timeout(XMarkContext* context, uint64_t timeout_in_micros);\n-  void work(uint64_t timeout_in_micros);\n-\n-  void verify_all_stacks_empty() const;\n-\n-public:\n-  XMark(XWorkers* workers, XPageTable* page_table);\n-\n-  bool is_initialized() const;\n-\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> void mark_object(uintptr_t addr);\n-\n-  void start();\n-  void mark(bool initial);\n-  bool end();\n-  void free();\n-\n-  void flush_and_free();\n-  bool flush_and_free(Thread* thread);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMark.hpp","additions":0,"deletions":106,"binary":false,"changes":106,"status":"deleted"},{"patch":"@@ -1,80 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARK_INLINE_HPP\n-#define SHARE_GC_X_XMARK_INLINE_HPP\n-\n-#include \"gc\/x\/xMark.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xMarkStack.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageTable.inline.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-\/\/ Marking before pushing helps reduce mark stack memory usage. However,\n-\/\/ we only mark before pushing in GC threads to avoid burdening Java threads\n-\/\/ with writing to, and potentially first having to clear, mark bitmaps.\n-\/\/\n-\/\/ It's also worth noting that while marking an object can be done at any\n-\/\/ time in the marking phase, following an object can only be done after\n-\/\/ root processing has called ClassLoaderDataGraph::clear_claimed_marks(),\n-\/\/ since it otherwise would interact badly with claiming of CLDs.\n-\n-template <bool gc_thread, bool follow, bool finalizable, bool publish>\n-inline void XMark::mark_object(uintptr_t addr) {\n-  assert(XAddress::is_marked(addr), \"Should be marked\");\n-\n-  XPage* const page = _page_table->get(addr);\n-  if (page->is_allocating()) {\n-    \/\/ Already implicitly marked\n-    return;\n-  }\n-\n-  const bool mark_before_push = gc_thread;\n-  bool inc_live = false;\n-\n-  if (mark_before_push) {\n-    \/\/ Try mark object\n-    if (!page->mark_object(addr, finalizable, inc_live)) {\n-      \/\/ Already marked\n-      return;\n-    }\n-  } else {\n-    \/\/ Don't push if already marked\n-    if (page->is_object_marked<finalizable>(addr)) {\n-      \/\/ Already marked\n-      return;\n-    }\n-  }\n-\n-  \/\/ Push\n-  XMarkThreadLocalStacks* const stacks = XThreadLocalData::stacks(Thread::current());\n-  XMarkStripe* const stripe = _stripes.stripe_for_addr(addr);\n-  XMarkStackEntry entry(addr, !mark_before_push, inc_live, follow, finalizable);\n-  stacks->push(&_allocator, &_stripes, stripe, entry, publish);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMARK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMark.inline.hpp","additions":0,"deletions":80,"binary":false,"changes":80,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xMarkCache.inline.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-XMarkCacheEntry::XMarkCacheEntry() :\n-    _page(nullptr),\n-    _objects(0),\n-    _bytes(0) {}\n-\n-XMarkCache::XMarkCache(size_t nstripes) :\n-    _shift(XMarkStripeShift + exact_log2(nstripes)) {}\n-\n-XMarkCache::~XMarkCache() {\n-  \/\/ Evict all entries\n-  for (size_t i = 0; i < XMarkCacheSize; i++) {\n-    _cache[i].evict();\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkCache.cpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,57 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKCACHE_HPP\n-#define SHARE_GC_X_XMARKCACHE_HPP\n-\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class XPage;\n-\n-class XMarkCacheEntry {\n-private:\n-  XPage*   _page;\n-  uint32_t _objects;\n-  size_t   _bytes;\n-\n-public:\n-  XMarkCacheEntry();\n-\n-  void inc_live(XPage* page, size_t bytes);\n-  void evict();\n-};\n-\n-class XMarkCache : public StackObj {\n-private:\n-  const size_t    _shift;\n-  XMarkCacheEntry _cache[XMarkCacheSize];\n-\n-public:\n-  XMarkCache(size_t nstripes);\n-  ~XMarkCache();\n-\n-  void inc_live(XPage* page, size_t bytes);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARKCACHE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkCache.hpp","additions":0,"deletions":57,"binary":false,"changes":57,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKCACHE_INLINE_HPP\n-#define SHARE_GC_X_XMARKCACHE_INLINE_HPP\n-\n-#include \"gc\/x\/xMarkCache.hpp\"\n-\n-#include \"gc\/x\/xPage.inline.hpp\"\n-\n-inline void XMarkCacheEntry::inc_live(XPage* page, size_t bytes) {\n-  if (_page == page) {\n-    \/\/ Cache hit\n-    _objects++;\n-    _bytes += bytes;\n-  } else {\n-    \/\/ Cache miss\n-    evict();\n-    _page = page;\n-    _objects = 1;\n-    _bytes = bytes;\n-  }\n-}\n-\n-inline void XMarkCacheEntry::evict() {\n-  if (_page != nullptr) {\n-    \/\/ Write cached data out to page\n-    _page->inc_live(_objects, _bytes);\n-    _page = nullptr;\n-  }\n-}\n-\n-inline void XMarkCache::inc_live(XPage* page, size_t bytes) {\n-  const size_t mask = XMarkCacheSize - 1;\n-  const size_t index = (page->start() >> _shift) & mask;\n-  _cache[index].inc_live(page, bytes);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMARKCACHE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkCache.inline.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -1,52 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKCONTEXT_HPP\n-#define SHARE_GC_X_XMARKCONTEXT_HPP\n-\n-#include \"gc\/x\/xMarkCache.hpp\"\n-#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class XMarkStripe;\n-class XMarkThreadLocalStacks;\n-\n-class XMarkContext : public StackObj {\n-private:\n-  XMarkCache                    _cache;\n-  XMarkStripe* const            _stripe;\n-  XMarkThreadLocalStacks* const _stacks;\n-  StringDedup::Requests         _string_dedup_requests;\n-\n-public:\n-  XMarkContext(size_t nstripes,\n-               XMarkStripe* stripe,\n-               XMarkThreadLocalStacks* stacks);\n-\n-  XMarkCache* cache();\n-  XMarkStripe* stripe();\n-  XMarkThreadLocalStacks* stacks();\n-  StringDedup::Requests* string_dedup_requests();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARKCONTEXT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkContext.hpp","additions":0,"deletions":52,"binary":false,"changes":52,"status":"deleted"},{"patch":"@@ -1,53 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKCONTEXT_INLINE_HPP\n-#define SHARE_GC_X_XMARKCONTEXT_INLINE_HPP\n-\n-#include \"gc\/x\/xMarkContext.hpp\"\n-\n-inline XMarkContext::XMarkContext(size_t nstripes,\n-                                  XMarkStripe* stripe,\n-                                  XMarkThreadLocalStacks* stacks) :\n-    _cache(nstripes),\n-    _stripe(stripe),\n-    _stacks(stacks),\n-    _string_dedup_requests() {}\n-\n-inline XMarkCache* XMarkContext::cache() {\n-  return &_cache;\n-}\n-\n-inline XMarkStripe* XMarkContext::stripe() {\n-  return _stripe;\n-}\n-\n-inline XMarkThreadLocalStacks* XMarkContext::stacks() {\n-  return _stacks;\n-}\n-\n-inline StringDedup::Requests* XMarkContext::string_dedup_requests() {\n-  return &_string_dedup_requests;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMARKCACHE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkContext.inline.hpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"deleted"},{"patch":"@@ -1,226 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xMarkStack.inline.hpp\"\n-#include \"gc\/x\/xMarkStackAllocator.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-XMarkStripe::XMarkStripe() :\n-    _published(),\n-    _overflowed() {}\n-\n-XMarkStripeSet::XMarkStripeSet() :\n-    _nstripes(0),\n-    _nstripes_mask(0),\n-    _stripes() {}\n-\n-void XMarkStripeSet::set_nstripes(size_t nstripes) {\n-  assert(is_power_of_2(nstripes), \"Must be a power of two\");\n-  assert(is_power_of_2(XMarkStripesMax), \"Must be a power of two\");\n-  assert(nstripes >= 1, \"Invalid number of stripes\");\n-  assert(nstripes <= XMarkStripesMax, \"Invalid number of stripes\");\n-\n-  _nstripes = nstripes;\n-  _nstripes_mask = nstripes - 1;\n-\n-  log_debug(gc, marking)(\"Using \" SIZE_FORMAT \" mark stripes\", _nstripes);\n-}\n-\n-bool XMarkStripeSet::is_empty() const {\n-  for (size_t i = 0; i < _nstripes; i++) {\n-    if (!_stripes[i].is_empty()) {\n-      return false;\n-    }\n-  }\n-\n-  return true;\n-}\n-\n-XMarkStripe* XMarkStripeSet::stripe_for_worker(uint nworkers, uint worker_id) {\n-  const size_t spillover_limit = (nworkers \/ _nstripes) * _nstripes;\n-  size_t index;\n-\n-  if (worker_id < spillover_limit) {\n-    \/\/ Not a spillover worker, use natural stripe\n-    index = worker_id & _nstripes_mask;\n-  } else {\n-    \/\/ Distribute spillover workers evenly across stripes\n-    const size_t spillover_nworkers = nworkers - spillover_limit;\n-    const size_t spillover_worker_id = worker_id - spillover_limit;\n-    const double spillover_chunk = (double)_nstripes \/ (double)spillover_nworkers;\n-    index = spillover_worker_id * spillover_chunk;\n-  }\n-\n-  assert(index < _nstripes, \"Invalid index\");\n-  return &_stripes[index];\n-}\n-\n-XMarkThreadLocalStacks::XMarkThreadLocalStacks() :\n-    _magazine(nullptr) {\n-  for (size_t i = 0; i < XMarkStripesMax; i++) {\n-    _stacks[i] = nullptr;\n-  }\n-}\n-\n-bool XMarkThreadLocalStacks::is_empty(const XMarkStripeSet* stripes) const {\n-  for (size_t i = 0; i < stripes->nstripes(); i++) {\n-    XMarkStack* const stack = _stacks[i];\n-    if (stack != nullptr) {\n-      return false;\n-    }\n-  }\n-\n-  return true;\n-}\n-\n-XMarkStack* XMarkThreadLocalStacks::allocate_stack(XMarkStackAllocator* allocator) {\n-  if (_magazine == nullptr) {\n-    \/\/ Allocate new magazine\n-    _magazine = allocator->alloc_magazine();\n-    if (_magazine == nullptr) {\n-      return nullptr;\n-    }\n-  }\n-\n-  XMarkStack* stack = nullptr;\n-\n-  if (!_magazine->pop(stack)) {\n-    \/\/ Magazine is empty, convert magazine into a new stack\n-    _magazine->~XMarkStackMagazine();\n-    stack = new ((void*)_magazine) XMarkStack();\n-    _magazine = nullptr;\n-  }\n-\n-  return stack;\n-}\n-\n-void XMarkThreadLocalStacks::free_stack(XMarkStackAllocator* allocator, XMarkStack* stack) {\n-  for (;;) {\n-    if (_magazine == nullptr) {\n-      \/\/ Convert stack into a new magazine\n-      stack->~XMarkStack();\n-      _magazine = new ((void*)stack) XMarkStackMagazine();\n-      return;\n-    }\n-\n-    if (_magazine->push(stack)) {\n-      \/\/ Success\n-      return;\n-    }\n-\n-    \/\/ Free and uninstall full magazine\n-    allocator->free_magazine(_magazine);\n-    _magazine = nullptr;\n-  }\n-}\n-\n-bool XMarkThreadLocalStacks::push_slow(XMarkStackAllocator* allocator,\n-                                       XMarkStripe* stripe,\n-                                       XMarkStack** stackp,\n-                                       XMarkStackEntry entry,\n-                                       bool publish) {\n-  XMarkStack* stack = *stackp;\n-\n-  for (;;) {\n-    if (stack == nullptr) {\n-      \/\/ Allocate and install new stack\n-      *stackp = stack = allocate_stack(allocator);\n-      if (stack == nullptr) {\n-        \/\/ Out of mark stack memory\n-        return false;\n-      }\n-    }\n-\n-    if (stack->push(entry)) {\n-      \/\/ Success\n-      return true;\n-    }\n-\n-    \/\/ Publish\/Overflow and uninstall stack\n-    stripe->publish_stack(stack, publish);\n-    *stackp = stack = nullptr;\n-  }\n-}\n-\n-bool XMarkThreadLocalStacks::pop_slow(XMarkStackAllocator* allocator,\n-                                      XMarkStripe* stripe,\n-                                      XMarkStack** stackp,\n-                                      XMarkStackEntry& entry) {\n-  XMarkStack* stack = *stackp;\n-\n-  for (;;) {\n-    if (stack == nullptr) {\n-      \/\/ Try steal and install stack\n-      *stackp = stack = stripe->steal_stack();\n-      if (stack == nullptr) {\n-        \/\/ Nothing to steal\n-        return false;\n-      }\n-    }\n-\n-    if (stack->pop(entry)) {\n-      \/\/ Success\n-      return true;\n-    }\n-\n-    \/\/ Free and uninstall stack\n-    free_stack(allocator, stack);\n-    *stackp = stack = nullptr;\n-  }\n-}\n-\n-bool XMarkThreadLocalStacks::flush(XMarkStackAllocator* allocator, XMarkStripeSet* stripes) {\n-  bool flushed = false;\n-\n-  \/\/ Flush all stacks\n-  for (size_t i = 0; i < stripes->nstripes(); i++) {\n-    XMarkStripe* const stripe = stripes->stripe_at(i);\n-    XMarkStack** const stackp = &_stacks[i];\n-    XMarkStack* const stack = *stackp;\n-    if (stack == nullptr) {\n-      continue;\n-    }\n-\n-    \/\/ Free\/Publish and uninstall stack\n-    if (stack->is_empty()) {\n-      free_stack(allocator, stack);\n-    } else {\n-      stripe->publish_stack(stack);\n-      flushed = true;\n-    }\n-    *stackp = nullptr;\n-  }\n-\n-  return flushed;\n-}\n-\n-void XMarkThreadLocalStacks::free(XMarkStackAllocator* allocator) {\n-  \/\/ Free and uninstall magazine\n-  if (_magazine != nullptr) {\n-    allocator->free_magazine(_magazine);\n-    _magazine = nullptr;\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStack.cpp","additions":0,"deletions":226,"binary":false,"changes":226,"status":"deleted"},{"patch":"@@ -1,164 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKSTACK_HPP\n-#define SHARE_GC_X_XMARKSTACK_HPP\n-\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xMarkStackEntry.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-template <typename T, size_t S>\n-class XStack {\n-private:\n-  size_t        _top;\n-  XStack<T, S>* _next;\n-  T             _slots[S];\n-\n-  bool is_full() const;\n-\n-public:\n-  XStack();\n-\n-  bool is_empty() const;\n-\n-  bool push(T value);\n-  bool pop(T& value);\n-\n-  XStack<T, S>* next() const;\n-  XStack<T, S>** next_addr();\n-};\n-\n-template <typename T>\n-class XStackList {\n-private:\n-  T* volatile _head;\n-\n-  T* encode_versioned_pointer(const T* stack, uint32_t version) const;\n-  void decode_versioned_pointer(const T* vstack, T** stack, uint32_t* version) const;\n-\n-public:\n-  XStackList();\n-\n-  bool is_empty() const;\n-\n-  void push(T* stack);\n-  T* pop();\n-\n-  void clear();\n-};\n-\n-using XMarkStack = XStack<XMarkStackEntry, XMarkStackSlots>;\n-using XMarkStackList = XStackList<XMarkStack>;\n-using XMarkStackMagazine = XStack<XMarkStack*, XMarkStackMagazineSlots>;\n-using XMarkStackMagazineList = XStackList<XMarkStackMagazine>;\n-\n-static_assert(sizeof(XMarkStack) == XMarkStackSize, \"XMarkStack size mismatch\");\n-static_assert(sizeof(XMarkStackMagazine) <= XMarkStackSize, \"XMarkStackMagazine size too large\");\n-\n-class XMarkStripe {\n-private:\n-  XCACHE_ALIGNED XMarkStackList _published;\n-  XCACHE_ALIGNED XMarkStackList _overflowed;\n-\n-public:\n-  XMarkStripe();\n-\n-  bool is_empty() const;\n-\n-  void publish_stack(XMarkStack* stack, bool publish = true);\n-  XMarkStack* steal_stack();\n-};\n-\n-class XMarkStripeSet {\n-private:\n-  size_t      _nstripes;\n-  size_t      _nstripes_mask;\n-  XMarkStripe _stripes[XMarkStripesMax];\n-\n-public:\n-  XMarkStripeSet();\n-\n-  size_t nstripes() const;\n-  void set_nstripes(size_t nstripes);\n-\n-  bool is_empty() const;\n-\n-  size_t stripe_id(const XMarkStripe* stripe) const;\n-  XMarkStripe* stripe_at(size_t index);\n-  XMarkStripe* stripe_next(XMarkStripe* stripe);\n-  XMarkStripe* stripe_for_worker(uint nworkers, uint worker_id);\n-  XMarkStripe* stripe_for_addr(uintptr_t addr);\n-};\n-\n-class XMarkStackAllocator;\n-\n-class XMarkThreadLocalStacks {\n-private:\n-  XMarkStackMagazine* _magazine;\n-  XMarkStack*         _stacks[XMarkStripesMax];\n-\n-  XMarkStack* allocate_stack(XMarkStackAllocator* allocator);\n-  void free_stack(XMarkStackAllocator* allocator, XMarkStack* stack);\n-\n-  bool push_slow(XMarkStackAllocator* allocator,\n-                 XMarkStripe* stripe,\n-                 XMarkStack** stackp,\n-                 XMarkStackEntry entry,\n-                 bool publish);\n-\n-  bool pop_slow(XMarkStackAllocator* allocator,\n-                XMarkStripe* stripe,\n-                XMarkStack** stackp,\n-                XMarkStackEntry& entry);\n-\n-public:\n-  XMarkThreadLocalStacks();\n-\n-  bool is_empty(const XMarkStripeSet* stripes) const;\n-\n-  void install(XMarkStripeSet* stripes,\n-               XMarkStripe* stripe,\n-               XMarkStack* stack);\n-\n-  XMarkStack* steal(XMarkStripeSet* stripes,\n-                    XMarkStripe* stripe);\n-\n-  bool push(XMarkStackAllocator* allocator,\n-            XMarkStripeSet* stripes,\n-            XMarkStripe* stripe,\n-            XMarkStackEntry entry,\n-            bool publish);\n-\n-  bool pop(XMarkStackAllocator* allocator,\n-           XMarkStripeSet* stripes,\n-           XMarkStripe* stripe,\n-           XMarkStackEntry& entry);\n-\n-  bool flush(XMarkStackAllocator* allocator,\n-             XMarkStripeSet* stripes);\n-\n-  void free(XMarkStackAllocator* allocator);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARKSTACK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStack.hpp","additions":0,"deletions":164,"binary":false,"changes":164,"status":"deleted"},{"patch":"@@ -1,266 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKSTACK_INLINE_HPP\n-#define SHARE_GC_X_XMARKSTACK_INLINE_HPP\n-\n-#include \"gc\/x\/xMarkStack.hpp\"\n-\n-#include \"utilities\/debug.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-\n-template <typename T, size_t S>\n-inline XStack<T, S>::XStack() :\n-    _top(0),\n-    _next(nullptr) {}\n-\n-template <typename T, size_t S>\n-inline bool XStack<T, S>::is_empty() const {\n-  return _top == 0;\n-}\n-\n-template <typename T, size_t S>\n-inline bool XStack<T, S>::is_full() const {\n-  return _top == S;\n-}\n-\n-template <typename T, size_t S>\n-inline bool XStack<T, S>::push(T value) {\n-  if (is_full()) {\n-    return false;\n-  }\n-\n-  _slots[_top++] = value;\n-  return true;\n-}\n-\n-template <typename T, size_t S>\n-inline bool XStack<T, S>::pop(T& value) {\n-  if (is_empty()) {\n-    return false;\n-  }\n-\n-  value = _slots[--_top];\n-  return true;\n-}\n-\n-template <typename T, size_t S>\n-inline XStack<T, S>* XStack<T, S>::next() const {\n-  return _next;\n-}\n-\n-template <typename T, size_t S>\n-inline XStack<T, S>** XStack<T, S>::next_addr() {\n-  return &_next;\n-}\n-\n-template <typename T>\n-inline XStackList<T>::XStackList() :\n-    _head(encode_versioned_pointer(nullptr, 0)) {}\n-\n-template <typename T>\n-inline T* XStackList<T>::encode_versioned_pointer(const T* stack, uint32_t version) const {\n-  uint64_t addr;\n-\n-  if (stack == nullptr) {\n-    addr = (uint32_t)-1;\n-  } else {\n-    addr = ((uint64_t)stack - XMarkStackSpaceStart) >> XMarkStackSizeShift;\n-  }\n-\n-  return (T*)((addr << 32) | (uint64_t)version);\n-}\n-\n-template <typename T>\n-inline void XStackList<T>::decode_versioned_pointer(const T* vstack, T** stack, uint32_t* version) const {\n-  const uint64_t addr = (uint64_t)vstack >> 32;\n-\n-  if (addr == (uint32_t)-1) {\n-    *stack = nullptr;\n-  } else {\n-    *stack = (T*)((addr << XMarkStackSizeShift) + XMarkStackSpaceStart);\n-  }\n-\n-  *version = (uint32_t)(uint64_t)vstack;\n-}\n-\n-template <typename T>\n-inline bool XStackList<T>::is_empty() const {\n-  const T* vstack = _head;\n-  T* stack = nullptr;\n-  uint32_t version = 0;\n-\n-  decode_versioned_pointer(vstack, &stack, &version);\n-  return stack == nullptr;\n-}\n-\n-template <typename T>\n-inline void XStackList<T>::push(T* stack) {\n-  T* vstack = _head;\n-  uint32_t version = 0;\n-\n-  for (;;) {\n-    decode_versioned_pointer(vstack, stack->next_addr(), &version);\n-    T* const new_vstack = encode_versioned_pointer(stack, version + 1);\n-    T* const prev_vstack = Atomic::cmpxchg(&_head, vstack, new_vstack);\n-    if (prev_vstack == vstack) {\n-      \/\/ Success\n-      break;\n-    }\n-\n-    \/\/ Retry\n-    vstack = prev_vstack;\n-  }\n-}\n-\n-template <typename T>\n-inline T* XStackList<T>::pop() {\n-  T* vstack = _head;\n-  T* stack = nullptr;\n-  uint32_t version = 0;\n-\n-  for (;;) {\n-    decode_versioned_pointer(vstack, &stack, &version);\n-    if (stack == nullptr) {\n-      return nullptr;\n-    }\n-\n-    T* const new_vstack = encode_versioned_pointer(stack->next(), version + 1);\n-    T* const prev_vstack = Atomic::cmpxchg(&_head, vstack, new_vstack);\n-    if (prev_vstack == vstack) {\n-      \/\/ Success\n-      return stack;\n-    }\n-\n-    \/\/ Retry\n-    vstack = prev_vstack;\n-  }\n-}\n-\n-template <typename T>\n-inline void XStackList<T>::clear() {\n-  _head = encode_versioned_pointer(nullptr, 0);\n-}\n-\n-inline bool XMarkStripe::is_empty() const {\n-  return _published.is_empty() && _overflowed.is_empty();\n-}\n-\n-inline void XMarkStripe::publish_stack(XMarkStack* stack, bool publish) {\n-  \/\/ A stack is published either on the published list or the overflowed\n-  \/\/ list. The published list is used by mutators publishing stacks for GC\n-  \/\/ workers to work on, while the overflowed list is used by GC workers\n-  \/\/ to publish stacks that overflowed. The intention here is to avoid\n-  \/\/ contention between mutators and GC workers as much as possible, while\n-  \/\/ still allowing GC workers to help out and steal work from each other.\n-  if (publish) {\n-    _published.push(stack);\n-  } else {\n-    _overflowed.push(stack);\n-  }\n-}\n-\n-inline XMarkStack* XMarkStripe::steal_stack() {\n-  \/\/ Steal overflowed stacks first, then published stacks\n-  XMarkStack* const stack = _overflowed.pop();\n-  if (stack != nullptr) {\n-    return stack;\n-  }\n-\n-  return _published.pop();\n-}\n-\n-inline size_t XMarkStripeSet::nstripes() const {\n-  return _nstripes;\n-}\n-\n-inline size_t XMarkStripeSet::stripe_id(const XMarkStripe* stripe) const {\n-  const size_t index = ((uintptr_t)stripe - (uintptr_t)_stripes) \/ sizeof(XMarkStripe);\n-  assert(index < _nstripes, \"Invalid index\");\n-  return index;\n-}\n-\n-inline XMarkStripe* XMarkStripeSet::stripe_at(size_t index) {\n-  assert(index < _nstripes, \"Invalid index\");\n-  return &_stripes[index];\n-}\n-\n-inline XMarkStripe* XMarkStripeSet::stripe_next(XMarkStripe* stripe) {\n-  const size_t index = (stripe_id(stripe) + 1) & _nstripes_mask;\n-  assert(index < _nstripes, \"Invalid index\");\n-  return &_stripes[index];\n-}\n-\n-inline XMarkStripe* XMarkStripeSet::stripe_for_addr(uintptr_t addr) {\n-  const size_t index = (addr >> XMarkStripeShift) & _nstripes_mask;\n-  assert(index < _nstripes, \"Invalid index\");\n-  return &_stripes[index];\n-}\n-\n-inline void XMarkThreadLocalStacks::install(XMarkStripeSet* stripes,\n-                                            XMarkStripe* stripe,\n-                                            XMarkStack* stack) {\n-  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n-  assert(*stackp == nullptr, \"Should be empty\");\n-  *stackp = stack;\n-}\n-\n-inline XMarkStack* XMarkThreadLocalStacks::steal(XMarkStripeSet* stripes,\n-                                                 XMarkStripe* stripe) {\n-  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n-  XMarkStack* const stack = *stackp;\n-  if (stack != nullptr) {\n-    *stackp = nullptr;\n-  }\n-\n-  return stack;\n-}\n-\n-inline bool XMarkThreadLocalStacks::push(XMarkStackAllocator* allocator,\n-                                         XMarkStripeSet* stripes,\n-                                         XMarkStripe* stripe,\n-                                         XMarkStackEntry entry,\n-                                         bool publish) {\n-  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n-  XMarkStack* const stack = *stackp;\n-  if (stack != nullptr && stack->push(entry)) {\n-    return true;\n-  }\n-\n-  return push_slow(allocator, stripe, stackp, entry, publish);\n-}\n-\n-inline bool XMarkThreadLocalStacks::pop(XMarkStackAllocator* allocator,\n-                                        XMarkStripeSet* stripes,\n-                                        XMarkStripe* stripe,\n-                                        XMarkStackEntry& entry) {\n-  XMarkStack** const stackp = &_stacks[stripes->stripe_id(stripe)];\n-  XMarkStack* const stack = *stackp;\n-  if (stack != nullptr && stack->pop(entry)) {\n-    return true;\n-  }\n-\n-  return pop_slow(allocator, stripe, stackp, entry);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMARKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStack.inline.hpp","additions":0,"deletions":266,"binary":false,"changes":266,"status":"deleted"},{"patch":"@@ -1,221 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xMarkStack.inline.hpp\"\n-#include \"gc\/x\/xMarkStackAllocator.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-uintptr_t XMarkStackSpaceStart;\n-\n-XMarkStackSpace::XMarkStackSpace() :\n-    _expand_lock(),\n-    _start(0),\n-    _top(0),\n-    _end(0) {\n-  assert(ZMarkStackSpaceLimit >= XMarkStackSpaceExpandSize, \"ZMarkStackSpaceLimit too small\");\n-\n-  \/\/ Reserve address space\n-  const size_t size = ZMarkStackSpaceLimit;\n-  const uintptr_t addr = (uintptr_t)os::reserve_memory(size, !ExecMem, mtGC);\n-  if (addr == 0) {\n-    log_error_pd(gc, marking)(\"Failed to reserve address space for mark stacks\");\n-    return;\n-  }\n-\n-  \/\/ Successfully initialized\n-  _start = _top = _end = addr;\n-\n-  \/\/ Register mark stack space start\n-  XMarkStackSpaceStart = _start;\n-\n-  \/\/ Prime space\n-  _end += expand_space();\n-}\n-\n-bool XMarkStackSpace::is_initialized() const {\n-  return _start != 0;\n-}\n-\n-size_t XMarkStackSpace::size() const {\n-  return _end - _start;\n-}\n-\n-size_t XMarkStackSpace::used() const {\n-  return _top - _start;\n-}\n-\n-size_t XMarkStackSpace::expand_space() {\n-  const size_t expand_size = XMarkStackSpaceExpandSize;\n-  const size_t old_size = size();\n-  const size_t new_size = old_size + expand_size;\n-\n-  if (new_size > ZMarkStackSpaceLimit) {\n-    \/\/ Expansion limit reached. This is a fatal error since we\n-    \/\/ currently can't recover from running out of mark stack space.\n-    fatal(\"Mark stack space exhausted. Use -XX:ZMarkStackSpaceLimit=<size> to increase the \"\n-          \"maximum number of bytes allocated for mark stacks. Current limit is \" SIZE_FORMAT \"M.\",\n-          ZMarkStackSpaceLimit \/ M);\n-  }\n-\n-  log_debug(gc, marking)(\"Expanding mark stack space: \" SIZE_FORMAT \"M->\" SIZE_FORMAT \"M\",\n-                         old_size \/ M, new_size \/ M);\n-\n-  \/\/ Expand\n-  os::commit_memory_or_exit((char*)_end, expand_size, false \/* executable *\/, \"Mark stack space\");\n-\n-  return expand_size;\n-}\n-\n-size_t XMarkStackSpace::shrink_space() {\n-  \/\/ Shrink to what is currently used\n-  const size_t old_size = size();\n-  const size_t new_size = align_up(used(), XMarkStackSpaceExpandSize);\n-  const size_t shrink_size = old_size - new_size;\n-\n-  if (shrink_size > 0) {\n-    \/\/ Shrink\n-    log_debug(gc, marking)(\"Shrinking mark stack space: \" SIZE_FORMAT \"M->\" SIZE_FORMAT \"M\",\n-                           old_size \/ M, new_size \/ M);\n-\n-    const uintptr_t shrink_start = _end - shrink_size;\n-    os::uncommit_memory((char*)shrink_start, shrink_size, false \/* executable *\/);\n-  }\n-\n-  return shrink_size;\n-}\n-\n-uintptr_t XMarkStackSpace::alloc_space(size_t size) {\n-  uintptr_t top = Atomic::load(&_top);\n-\n-  for (;;) {\n-    const uintptr_t end = Atomic::load(&_end);\n-    const uintptr_t new_top = top + size;\n-    if (new_top > end) {\n-      \/\/ Not enough space left\n-      return 0;\n-    }\n-\n-    const uintptr_t prev_top = Atomic::cmpxchg(&_top, top, new_top);\n-    if (prev_top == top) {\n-      \/\/ Success\n-      return top;\n-    }\n-\n-    \/\/ Retry\n-    top = prev_top;\n-  }\n-}\n-\n-uintptr_t XMarkStackSpace::expand_and_alloc_space(size_t size) {\n-  XLocker<XLock> locker(&_expand_lock);\n-\n-  \/\/ Retry allocation before expanding\n-  uintptr_t addr = alloc_space(size);\n-  if (addr != 0) {\n-    return addr;\n-  }\n-\n-  \/\/ Expand\n-  const size_t expand_size = expand_space();\n-\n-  \/\/ Increment top before end to make sure another\n-  \/\/ thread can't steal out newly expanded space.\n-  addr = Atomic::fetch_then_add(&_top, size);\n-  Atomic::add(&_end, expand_size);\n-\n-  return addr;\n-}\n-\n-uintptr_t XMarkStackSpace::alloc(size_t size) {\n-  assert(size <= XMarkStackSpaceExpandSize, \"Invalid size\");\n-\n-  const uintptr_t addr = alloc_space(size);\n-  if (addr != 0) {\n-    return addr;\n-  }\n-\n-  return expand_and_alloc_space(size);\n-}\n-\n-void XMarkStackSpace::free() {\n-  _end -= shrink_space();\n-  _top = _start;\n-}\n-\n-XMarkStackAllocator::XMarkStackAllocator() :\n-    _freelist(),\n-    _space() {}\n-\n-bool XMarkStackAllocator::is_initialized() const {\n-  return _space.is_initialized();\n-}\n-\n-size_t XMarkStackAllocator::size() const {\n-  return _space.size();\n-}\n-\n-XMarkStackMagazine* XMarkStackAllocator::create_magazine_from_space(uintptr_t addr, size_t size) {\n-  assert(is_aligned(size, XMarkStackSize), \"Invalid size\");\n-\n-  \/\/ Use first stack as magazine\n-  XMarkStackMagazine* const magazine = new ((void*)addr) XMarkStackMagazine();\n-  for (size_t i = XMarkStackSize; i < size; i += XMarkStackSize) {\n-    XMarkStack* const stack = new ((void*)(addr + i)) XMarkStack();\n-    const bool success = magazine->push(stack);\n-    assert(success, \"Magazine should never get full\");\n-  }\n-\n-  return magazine;\n-}\n-\n-XMarkStackMagazine* XMarkStackAllocator::alloc_magazine() {\n-  \/\/ Try allocating from the free list first\n-  XMarkStackMagazine* const magazine = _freelist.pop();\n-  if (magazine != nullptr) {\n-    return magazine;\n-  }\n-\n-  \/\/ Allocate new magazine\n-  const uintptr_t addr = _space.alloc(XMarkStackMagazineSize);\n-  if (addr == 0) {\n-    return nullptr;\n-  }\n-\n-  return create_magazine_from_space(addr, XMarkStackMagazineSize);\n-}\n-\n-void XMarkStackAllocator::free_magazine(XMarkStackMagazine* magazine) {\n-  _freelist.push(magazine);\n-}\n-\n-void XMarkStackAllocator::free() {\n-  _freelist.clear();\n-  _space.free();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStackAllocator.cpp","additions":0,"deletions":221,"binary":false,"changes":221,"status":"deleted"},{"patch":"@@ -1,77 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKSTACKALLOCATOR_HPP\n-#define SHARE_GC_X_XMARKSTACKALLOCATOR_HPP\n-\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XMarkStackSpace {\n-private:\n-  XLock              _expand_lock;\n-  uintptr_t          _start;\n-  volatile uintptr_t _top;\n-  volatile uintptr_t _end;\n-\n-  size_t used() const;\n-\n-  size_t expand_space();\n-  size_t shrink_space();\n-\n-  uintptr_t alloc_space(size_t size);\n-  uintptr_t expand_and_alloc_space(size_t size);\n-\n-public:\n-  XMarkStackSpace();\n-\n-  bool is_initialized() const;\n-\n-  size_t size() const;\n-\n-  uintptr_t alloc(size_t size);\n-  void free();\n-};\n-\n-class XMarkStackAllocator {\n-private:\n-  XCACHE_ALIGNED XMarkStackMagazineList _freelist;\n-  XCACHE_ALIGNED XMarkStackSpace        _space;\n-\n-  XMarkStackMagazine* create_magazine_from_space(uintptr_t addr, size_t size);\n-\n-public:\n-  XMarkStackAllocator();\n-\n-  bool is_initialized() const;\n-\n-  size_t size() const;\n-\n-  XMarkStackMagazine* alloc_magazine();\n-  void free_magazine(XMarkStackMagazine* magazine);\n-\n-  void free();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARKSTACKALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStackAllocator.hpp","additions":0,"deletions":77,"binary":false,"changes":77,"status":"deleted"},{"patch":"@@ -1,142 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKSTACKENTRY_HPP\n-#define SHARE_GC_X_XMARKSTACKENTRY_HPP\n-\n-#include \"gc\/x\/xBitField.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-\/\/\n-\/\/ Mark stack entry layout\n-\/\/ -----------------------\n-\/\/\n-\/\/  Object entry\n-\/\/  ------------\n-\/\/\n-\/\/   6\n-\/\/   3                                                                5 4 3 2 1 0\n-\/\/  +------------------------------------------------------------------+-+-+-+-+-+\n-\/\/  |11111111 11111111 11111111 11111111 11111111 11111111 11111111 111|1|1|1|1|1|\n-\/\/  +------------------------------------------------------------------+-+-+-+-+-+\n-\/\/  |                                                                  | | | | |\n-\/\/  |                                            4-4 Mark Flag (1-bit) * | | | |\n-\/\/  |                                                                    | | | |\n-\/\/  |                                    3-3 Increment Live Flag (1-bit) * | | |\n-\/\/  |                                                                      | | |\n-\/\/  |                                              2-2 Follow Flag (1-bit) * | |\n-\/\/  |                                                                        | |\n-\/\/  |                                         1-1 Partial Array Flag (1-bit) * |\n-\/\/  |                                                                          |\n-\/\/  |                                                   0-0 Final Flag (1-bit) *\n-\/\/  |\n-\/\/  * 63-5 Object Address (59-bits)\n-\/\/\n-\/\/\n-\/\/  Partial array entry\n-\/\/  -------------------\n-\/\/\n-\/\/   6                                 3  3\n-\/\/   3                                 2  1                               2 1 0\n-\/\/  +------------------------------------+---------------------------------+-+-+\n-\/\/  |11111111 11111111 11111111 11111111 |11111111 11111111 11111111 111111|1|1|\n-\/\/  +------------------------------------+---------------------------------+-+-+\n-\/\/  |                                    |                                 | |\n-\/\/  |                                    |  1-1 Partial Array Flag (1-bit) * |\n-\/\/  |                                    |                                   |\n-\/\/  |                                    |            0-0 Final Flag (1-bit) *\n-\/\/  |                                    |\n-\/\/  |                                    * 31-2 Partial Array Length (30-bits)\n-\/\/  |\n-\/\/  * 63-32 Partial Array Address Offset (32-bits)\n-\/\/\n-\n-class XMarkStackEntry  {\n-private:\n-  typedef XBitField<uint64_t, bool,      0,  1>  field_finalizable;\n-  typedef XBitField<uint64_t, bool,      1,  1>  field_partial_array;\n-  typedef XBitField<uint64_t, bool,      2,  1>  field_follow;\n-  typedef XBitField<uint64_t, bool,      3,  1>  field_inc_live;\n-  typedef XBitField<uint64_t, bool,      4,  1>  field_mark;\n-  typedef XBitField<uint64_t, uintptr_t, 5,  59> field_object_address;\n-  typedef XBitField<uint64_t, size_t,    2,  30> field_partial_array_length;\n-  typedef XBitField<uint64_t, size_t,    32, 32> field_partial_array_offset;\n-\n-  uint64_t _entry;\n-\n-public:\n-  XMarkStackEntry() {\n-    \/\/ This constructor is intentionally left empty and does not initialize\n-    \/\/ _entry to allow it to be optimized out when instantiating XMarkStack,\n-    \/\/ which has a long array of XMarkStackEntry elements, but doesn't care\n-    \/\/ what _entry is initialized to.\n-  }\n-\n-  XMarkStackEntry(uintptr_t object_address, bool mark, bool inc_live, bool follow, bool finalizable) :\n-      _entry(field_object_address::encode(object_address) |\n-             field_mark::encode(mark) |\n-             field_inc_live::encode(inc_live) |\n-             field_follow::encode(follow) |\n-             field_partial_array::encode(false) |\n-             field_finalizable::encode(finalizable)) {}\n-\n-  XMarkStackEntry(size_t partial_array_offset, size_t partial_array_length, bool finalizable) :\n-      _entry(field_partial_array_offset::encode(partial_array_offset) |\n-             field_partial_array_length::encode(partial_array_length) |\n-             field_partial_array::encode(true) |\n-             field_finalizable::encode(finalizable)) {}\n-\n-  bool finalizable() const {\n-    return field_finalizable::decode(_entry);\n-  }\n-\n-  bool partial_array() const {\n-    return field_partial_array::decode(_entry);\n-  }\n-\n-  size_t partial_array_offset() const {\n-    return field_partial_array_offset::decode(_entry);\n-  }\n-\n-  size_t partial_array_length() const {\n-    return field_partial_array_length::decode(_entry);\n-  }\n-\n-  bool follow() const {\n-    return field_follow::decode(_entry);\n-  }\n-\n-  bool inc_live() const {\n-    return field_inc_live::decode(_entry);\n-  }\n-\n-  bool mark() const {\n-    return field_mark::decode(_entry);\n-  }\n-\n-  uintptr_t object_address() const {\n-    return field_object_address::decode(_entry);\n-  }\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARKSTACKENTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkStackEntry.hpp","additions":0,"deletions":142,"binary":false,"changes":142,"status":"deleted"},{"patch":"@@ -1,54 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKTERMINATE_HPP\n-#define SHARE_GC_X_XMARKTERMINATE_HPP\n-\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XMarkTerminate {\n-private:\n-  uint                         _nworkers;\n-  XCACHE_ALIGNED volatile uint _nworking_stage0;\n-  volatile uint                _nworking_stage1;\n-\n-  bool enter_stage(volatile uint* nworking_stage);\n-  void exit_stage(volatile uint* nworking_stage);\n-  bool try_exit_stage(volatile uint* nworking_stage);\n-\n-public:\n-  XMarkTerminate();\n-\n-  void reset(uint nworkers);\n-\n-  bool enter_stage0();\n-  void exit_stage0();\n-  bool try_exit_stage0();\n-\n-  bool enter_stage1();\n-  bool try_exit_stage1();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMARKTERMINATE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkTerminate.hpp","additions":0,"deletions":54,"binary":false,"changes":54,"status":"deleted"},{"patch":"@@ -1,88 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMARKTERMINATE_INLINE_HPP\n-#define SHARE_GC_X_XMARKTERMINATE_INLINE_HPP\n-\n-#include \"gc\/x\/xMarkTerminate.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-\n-inline XMarkTerminate::XMarkTerminate() :\n-    _nworkers(0),\n-    _nworking_stage0(0),\n-    _nworking_stage1(0) {}\n-\n-inline bool XMarkTerminate::enter_stage(volatile uint* nworking_stage) {\n-  return Atomic::sub(nworking_stage, 1u) == 0;\n-}\n-\n-inline void XMarkTerminate::exit_stage(volatile uint* nworking_stage) {\n-  Atomic::add(nworking_stage, 1u);\n-}\n-\n-inline bool XMarkTerminate::try_exit_stage(volatile uint* nworking_stage) {\n-  uint nworking = Atomic::load(nworking_stage);\n-\n-  for (;;) {\n-    if (nworking == 0) {\n-      return false;\n-    }\n-\n-    const uint new_nworking = nworking + 1;\n-    const uint prev_nworking = Atomic::cmpxchg(nworking_stage, nworking, new_nworking);\n-    if (prev_nworking == nworking) {\n-      \/\/ Success\n-      return true;\n-    }\n-\n-    \/\/ Retry\n-    nworking = prev_nworking;\n-  }\n-}\n-\n-inline void XMarkTerminate::reset(uint nworkers) {\n-  _nworkers = _nworking_stage0 = _nworking_stage1 = nworkers;\n-}\n-\n-inline bool XMarkTerminate::enter_stage0() {\n-  return enter_stage(&_nworking_stage0);\n-}\n-\n-inline void XMarkTerminate::exit_stage0() {\n-  exit_stage(&_nworking_stage0);\n-}\n-\n-inline bool XMarkTerminate::try_exit_stage0() {\n-  return try_exit_stage(&_nworking_stage0);\n-}\n-\n-inline bool XMarkTerminate::enter_stage1() {\n-  return enter_stage(&_nworking_stage1);\n-}\n-\n-inline bool XMarkTerminate::try_exit_stage1() {\n-  return try_exit_stage(&_nworking_stage1);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMARKTERMINATE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMarkTerminate.inline.hpp","additions":0,"deletions":88,"binary":false,"changes":88,"status":"deleted"},{"patch":"@@ -1,220 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xMemory.inline.hpp\"\n-\n-XMemory* XMemoryManager::create(uintptr_t start, size_t size) {\n-  XMemory* const area = new XMemory(start, size);\n-  if (_callbacks._create != nullptr) {\n-    _callbacks._create(area);\n-  }\n-  return area;\n-}\n-\n-void XMemoryManager::destroy(XMemory* area) {\n-  if (_callbacks._destroy != nullptr) {\n-    _callbacks._destroy(area);\n-  }\n-  delete area;\n-}\n-\n-void XMemoryManager::shrink_from_front(XMemory* area, size_t size) {\n-  if (_callbacks._shrink_from_front != nullptr) {\n-    _callbacks._shrink_from_front(area, size);\n-  }\n-  area->shrink_from_front(size);\n-}\n-\n-void XMemoryManager::shrink_from_back(XMemory* area, size_t size) {\n-  if (_callbacks._shrink_from_back != nullptr) {\n-    _callbacks._shrink_from_back(area, size);\n-  }\n-  area->shrink_from_back(size);\n-}\n-\n-void XMemoryManager::grow_from_front(XMemory* area, size_t size) {\n-  if (_callbacks._grow_from_front != nullptr) {\n-    _callbacks._grow_from_front(area, size);\n-  }\n-  area->grow_from_front(size);\n-}\n-\n-void XMemoryManager::grow_from_back(XMemory* area, size_t size) {\n-  if (_callbacks._grow_from_back != nullptr) {\n-    _callbacks._grow_from_back(area, size);\n-  }\n-  area->grow_from_back(size);\n-}\n-\n-XMemoryManager::Callbacks::Callbacks() :\n-    _create(nullptr),\n-    _destroy(nullptr),\n-    _shrink_from_front(nullptr),\n-    _shrink_from_back(nullptr),\n-    _grow_from_front(nullptr),\n-    _grow_from_back(nullptr) {}\n-\n-XMemoryManager::XMemoryManager() :\n-    _freelist(),\n-    _callbacks() {}\n-\n-void XMemoryManager::register_callbacks(const Callbacks& callbacks) {\n-  _callbacks = callbacks;\n-}\n-\n-uintptr_t XMemoryManager::peek_low_address() const {\n-  XLocker<XLock> locker(&_lock);\n-\n-  const XMemory* const area = _freelist.first();\n-  if (area != nullptr) {\n-    return area->start();\n-  }\n-\n-  \/\/ Out of memory\n-  return UINTPTR_MAX;\n-}\n-\n-uintptr_t XMemoryManager::alloc_low_address(size_t size) {\n-  XLocker<XLock> locker(&_lock);\n-\n-  XListIterator<XMemory> iter(&_freelist);\n-  for (XMemory* area; iter.next(&area);) {\n-    if (area->size() >= size) {\n-      if (area->size() == size) {\n-        \/\/ Exact match, remove area\n-        const uintptr_t start = area->start();\n-        _freelist.remove(area);\n-        destroy(area);\n-        return start;\n-      } else {\n-        \/\/ Larger than requested, shrink area\n-        const uintptr_t start = area->start();\n-        shrink_from_front(area, size);\n-        return start;\n-      }\n-    }\n-  }\n-\n-  \/\/ Out of memory\n-  return UINTPTR_MAX;\n-}\n-\n-uintptr_t XMemoryManager::alloc_low_address_at_most(size_t size, size_t* allocated) {\n-  XLocker<XLock> locker(&_lock);\n-\n-  XMemory* area = _freelist.first();\n-  if (area != nullptr) {\n-    if (area->size() <= size) {\n-      \/\/ Smaller than or equal to requested, remove area\n-      const uintptr_t start = area->start();\n-      *allocated = area->size();\n-      _freelist.remove(area);\n-      destroy(area);\n-      return start;\n-    } else {\n-      \/\/ Larger than requested, shrink area\n-      const uintptr_t start = area->start();\n-      shrink_from_front(area, size);\n-      *allocated = size;\n-      return start;\n-    }\n-  }\n-\n-  \/\/ Out of memory\n-  *allocated = 0;\n-  return UINTPTR_MAX;\n-}\n-\n-uintptr_t XMemoryManager::alloc_high_address(size_t size) {\n-  XLocker<XLock> locker(&_lock);\n-\n-  XListReverseIterator<XMemory> iter(&_freelist);\n-  for (XMemory* area; iter.next(&area);) {\n-    if (area->size() >= size) {\n-      if (area->size() == size) {\n-        \/\/ Exact match, remove area\n-        const uintptr_t start = area->start();\n-        _freelist.remove(area);\n-        destroy(area);\n-        return start;\n-      } else {\n-        \/\/ Larger than requested, shrink area\n-        shrink_from_back(area, size);\n-        return area->end();\n-      }\n-    }\n-  }\n-\n-  \/\/ Out of memory\n-  return UINTPTR_MAX;\n-}\n-\n-void XMemoryManager::free(uintptr_t start, size_t size) {\n-  assert(start != UINTPTR_MAX, \"Invalid address\");\n-  const uintptr_t end = start + size;\n-\n-  XLocker<XLock> locker(&_lock);\n-\n-  XListIterator<XMemory> iter(&_freelist);\n-  for (XMemory* area; iter.next(&area);) {\n-    if (start < area->start()) {\n-      XMemory* const prev = _freelist.prev(area);\n-      if (prev != nullptr && start == prev->end()) {\n-        if (end == area->start()) {\n-          \/\/ Merge with prev and current area\n-          grow_from_back(prev, size + area->size());\n-          _freelist.remove(area);\n-          delete area;\n-        } else {\n-          \/\/ Merge with prev area\n-          grow_from_back(prev, size);\n-        }\n-      } else if (end == area->start()) {\n-        \/\/ Merge with current area\n-        grow_from_front(area, size);\n-      } else {\n-        \/\/ Insert new area before current area\n-        assert(end < area->start(), \"Areas must not overlap\");\n-        XMemory* const new_area = create(start, size);\n-        _freelist.insert_before(area, new_area);\n-      }\n-\n-      \/\/ Done\n-      return;\n-    }\n-  }\n-\n-  \/\/ Insert last\n-  XMemory* const last = _freelist.last();\n-  if (last != nullptr && start == last->end()) {\n-    \/\/ Merge with last area\n-    grow_from_back(last, size);\n-  } else {\n-    \/\/ Insert new area last\n-    XMemory* const new_area = create(start, size);\n-    _freelist.insert_last(new_area);\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xMemory.cpp","additions":0,"deletions":220,"binary":false,"changes":220,"status":"deleted"},{"patch":"@@ -1,93 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMEMORY_HPP\n-#define SHARE_GC_X_XMEMORY_HPP\n-\n-#include \"gc\/x\/xList.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class XMemory : public CHeapObj<mtGC> {\n-  friend class XList<XMemory>;\n-\n-private:\n-  uintptr_t          _start;\n-  uintptr_t          _end;\n-  XListNode<XMemory> _node;\n-\n-public:\n-  XMemory(uintptr_t start, size_t size);\n-\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n-  size_t size() const;\n-\n-  void shrink_from_front(size_t size);\n-  void shrink_from_back(size_t size);\n-  void grow_from_front(size_t size);\n-  void grow_from_back(size_t size);\n-};\n-\n-class XMemoryManager {\n-public:\n-  typedef void (*CreateDestroyCallback)(const XMemory* area);\n-  typedef void (*ResizeCallback)(const XMemory* area, size_t size);\n-\n-  struct Callbacks {\n-    CreateDestroyCallback _create;\n-    CreateDestroyCallback _destroy;\n-    ResizeCallback        _shrink_from_front;\n-    ResizeCallback        _shrink_from_back;\n-    ResizeCallback        _grow_from_front;\n-    ResizeCallback        _grow_from_back;\n-\n-    Callbacks();\n-  };\n-\n-private:\n-  mutable XLock  _lock;\n-  XList<XMemory> _freelist;\n-  Callbacks      _callbacks;\n-\n-  XMemory* create(uintptr_t start, size_t size);\n-  void destroy(XMemory* area);\n-  void shrink_from_front(XMemory* area, size_t size);\n-  void shrink_from_back(XMemory* area, size_t size);\n-  void grow_from_front(XMemory* area, size_t size);\n-  void grow_from_back(XMemory* area, size_t size);\n-\n-public:\n-  XMemoryManager();\n-\n-  void register_callbacks(const Callbacks& callbacks);\n-\n-  uintptr_t peek_low_address() const;\n-  uintptr_t alloc_low_address(size_t size);\n-  uintptr_t alloc_low_address_at_most(size_t size, size_t* allocated);\n-  uintptr_t alloc_high_address(size_t size);\n-\n-  void free(uintptr_t start, size_t size);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMemory.hpp","additions":0,"deletions":93,"binary":false,"changes":93,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMEMORY_INLINE_HPP\n-#define SHARE_GC_X_XMEMORY_INLINE_HPP\n-\n-#include \"gc\/x\/xMemory.hpp\"\n-\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline XMemory::XMemory(uintptr_t start, size_t size) :\n-    _start(start),\n-    _end(start + size) {}\n-\n-inline uintptr_t XMemory::start() const {\n-  return _start;\n-}\n-\n-inline uintptr_t XMemory::end() const {\n-  return _end;\n-}\n-\n-inline size_t XMemory::size() const {\n-  return end() - start();\n-}\n-\n-inline void XMemory::shrink_from_front(size_t size) {\n-  assert(this->size() > size, \"Too small\");\n-  _start += size;\n-}\n-\n-inline void XMemory::shrink_from_back(size_t size) {\n-  assert(this->size() > size, \"Too small\");\n-  _end -= size;\n-}\n-\n-inline void XMemory::grow_from_front(size_t size) {\n-  assert(start() >= size, \"Too big\");\n-  _start -= size;\n-}\n-\n-inline void XMemory::grow_from_back(size_t size) {\n-  _end += size;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMemory.inline.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,66 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMESSAGEPORT_HPP\n-#define SHARE_GC_X_XMESSAGEPORT_HPP\n-\n-#include \"gc\/x\/xFuture.hpp\"\n-#include \"gc\/x\/xList.hpp\"\n-#include \"runtime\/mutex.hpp\"\n-\n-template <typename T> class XMessageRequest;\n-\n-template <typename T>\n-class XMessagePort {\n-private:\n-  typedef XMessageRequest<T> Request;\n-\n-  mutable Monitor _monitor;\n-  bool            _has_message;\n-  T               _message;\n-  uint64_t        _seqnum;\n-  XList<Request>  _queue;\n-\n-public:\n-  XMessagePort();\n-\n-  bool is_busy() const;\n-\n-  void send_sync(const T& message);\n-  void send_async(const T& message);\n-\n-  T receive();\n-  void ack();\n-};\n-\n-class XRendezvousPort {\n-private:\n-  XMessagePort<bool> _port;\n-\n-public:\n-  void signal();\n-  void wait();\n-  void ack();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMESSAGEPORT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMessagePort.hpp","additions":0,"deletions":66,"binary":false,"changes":66,"status":"deleted"},{"patch":"@@ -1,181 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMESSAGEPORT_INLINE_HPP\n-#define SHARE_GC_X_XMESSAGEPORT_INLINE_HPP\n-\n-#include \"gc\/x\/xMessagePort.hpp\"\n-\n-#include \"gc\/x\/xFuture.inline.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-\n-template <typename T>\n-class XMessageRequest : public StackObj {\n-  friend class XList<XMessageRequest>;\n-\n-private:\n-  T                          _message;\n-  uint64_t                   _seqnum;\n-  XFuture<T>                 _result;\n-  XListNode<XMessageRequest> _node;\n-\n-public:\n-  void initialize(T message, uint64_t seqnum) {\n-    _message = message;\n-    _seqnum = seqnum;\n-  }\n-\n-  T message() const {\n-    return _message;\n-  }\n-\n-  uint64_t seqnum() const {\n-    return _seqnum;\n-  }\n-\n-  void wait() {\n-    const T message = _result.get();\n-    assert(message == _message, \"Message mismatch\");\n-  }\n-\n-  void satisfy(T message) {\n-    _result.set(message);\n-  }\n-};\n-\n-template <typename T>\n-inline XMessagePort<T>::XMessagePort() :\n-    _monitor(Monitor::nosafepoint, \"XMessagePort_lock\"),\n-    _has_message(false),\n-    _seqnum(0),\n-    _queue() {}\n-\n-template <typename T>\n-inline bool XMessagePort<T>::is_busy() const {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  return _has_message;\n-}\n-\n-template <typename T>\n-inline void XMessagePort<T>::send_sync(const T& message) {\n-  Request request;\n-\n-  {\n-    \/\/ Enqueue message\n-    MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-    request.initialize(message, _seqnum);\n-    _queue.insert_last(&request);\n-    ml.notify();\n-  }\n-\n-  \/\/ Wait for completion\n-  request.wait();\n-\n-  {\n-    \/\/ Guard deletion of underlying semaphore. This is a workaround for a\n-    \/\/ bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n-    \/\/ the semaphore immediately after returning from sem_wait(). The\n-    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n-    \/\/ thread have returned from sem_wait(). To avoid this race we are\n-    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n-    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n-    MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  }\n-}\n-\n-template <typename T>\n-inline void XMessagePort<T>::send_async(const T& message) {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  if (!_has_message) {\n-    \/\/ Post message\n-    _message = message;\n-    _has_message = true;\n-    ml.notify();\n-  }\n-}\n-\n-template <typename T>\n-inline T XMessagePort<T>::receive() {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-\n-  \/\/ Wait for message\n-  while (!_has_message && _queue.is_empty()) {\n-    ml.wait();\n-  }\n-\n-  \/\/ Increment request sequence number\n-  _seqnum++;\n-\n-  if (!_has_message) {\n-    \/\/ Message available in the queue\n-    _message = _queue.first()->message();\n-    _has_message = true;\n-  }\n-\n-  return _message;\n-}\n-\n-template <typename T>\n-inline void XMessagePort<T>::ack() {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-\n-  if (!_has_message) {\n-    \/\/ Nothing to ack\n-    return;\n-  }\n-\n-  \/\/ Satisfy requests (and duplicates) in queue\n-  XListIterator<Request> iter(&_queue);\n-  for (Request* request; iter.next(&request);) {\n-    if (request->message() == _message && request->seqnum() < _seqnum) {\n-      \/\/ Dequeue and satisfy request. Note that the dequeue operation must\n-      \/\/ happen first, since the request will immediately be deallocated\n-      \/\/ once it has been satisfied.\n-      _queue.remove(request);\n-      request->satisfy(_message);\n-    }\n-  }\n-\n-  if (_queue.is_empty()) {\n-    \/\/ Queue is empty\n-    _has_message = false;\n-  } else {\n-    \/\/ Post first message in queue\n-    _message = _queue.first()->message();\n-  }\n-}\n-\n-inline void XRendezvousPort::signal() {\n-  _port.send_sync(true \/* ignored *\/);\n-}\n-\n-inline void XRendezvousPort::wait() {\n-  _port.receive();\n-}\n-\n-inline void XRendezvousPort::ack() {\n-  _port.ack();\n-}\n-\n-#endif \/\/ SHARE_GC_X_XMESSAGEPORT_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMessagePort.inline.hpp","additions":0,"deletions":181,"binary":false,"changes":181,"status":"deleted"},{"patch":"@@ -1,81 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xMetronome.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/timer.hpp\"\n-#include \"utilities\/ticks.hpp\"\n-\n-XMetronome::XMetronome(uint64_t hz) :\n-    _monitor(Monitor::nosafepoint, \"XMetronome_lock\"),\n-    _interval_ms(MILLIUNITS \/ hz),\n-    _start_ms(0),\n-    _nticks(0),\n-    _stopped(false) {}\n-\n-bool XMetronome::wait_for_tick() {\n-  if (_nticks++ == 0) {\n-    \/\/ First tick, set start time\n-    const Ticks now = Ticks::now();\n-    _start_ms = TimeHelper::counter_to_millis(now.value());\n-  }\n-\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-\n-  while (!_stopped) {\n-    \/\/ We might wake up spuriously from wait, so always recalculate\n-    \/\/ the timeout after a wakeup to see if we need to wait again.\n-    const Ticks now = Ticks::now();\n-    const uint64_t now_ms = TimeHelper::counter_to_millis(now.value());\n-    const uint64_t next_ms = _start_ms + (_interval_ms * _nticks);\n-    const int64_t timeout_ms = next_ms - now_ms;\n-\n-    if (timeout_ms > 0) {\n-      \/\/ Wait\n-      ml.wait(timeout_ms);\n-    } else {\n-      \/\/ Tick\n-      if (timeout_ms < 0) {\n-        const uint64_t overslept = -timeout_ms;\n-        if (overslept > _interval_ms) {\n-          \/\/ Missed one or more ticks. Bump _nticks accordingly to\n-          \/\/ avoid firing a string of immediate ticks to make up\n-          \/\/ for the ones we missed.\n-          _nticks += overslept \/ _interval_ms;\n-        }\n-      }\n-\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Stopped\n-  return false;\n-}\n-\n-void XMetronome::stop() {\n-  MonitorLocker ml(&_monitor, Monitor::_no_safepoint_check_flag);\n-  _stopped = true;\n-  ml.notify();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xMetronome.cpp","additions":0,"deletions":81,"binary":false,"changes":81,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XMETRONOME_HPP\n-#define SHARE_GC_X_XMETRONOME_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/mutex.hpp\"\n-\n-class XMetronome : public StackObj {\n-private:\n-  Monitor        _monitor;\n-  const uint64_t _interval_ms;\n-  uint64_t       _start_ms;\n-  uint64_t       _nticks;\n-  bool           _stopped;\n-\n-public:\n-  XMetronome(uint64_t hz);\n-\n-  bool wait_for_tick();\n-  void stop();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XMETRONOME_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xMetronome.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,349 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/relocInfo.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"gc\/shared\/classUnloadingContext.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xNMethodData.hpp\"\n-#include \"gc\/x\/xNMethodTable.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/allocation.inline.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/continuation.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-static XNMethodData* gc_data(const nmethod* nm) {\n-  return nm->gc_data<XNMethodData>();\n-}\n-\n-static void set_gc_data(nmethod* nm, XNMethodData* data) {\n-  return nm->set_gc_data<XNMethodData>(data);\n-}\n-\n-void XNMethod::attach_gc_data(nmethod* nm) {\n-  GrowableArray<oop*> immediate_oops;\n-  bool non_immediate_oops = false;\n-\n-  \/\/ Find all oop relocations\n-  RelocIterator iter(nm);\n-  while (iter.next()) {\n-    if (iter.type() != relocInfo::oop_type) {\n-      \/\/ Not an oop\n-      continue;\n-    }\n-\n-    oop_Relocation* r = iter.oop_reloc();\n-\n-    if (!r->oop_is_immediate()) {\n-      \/\/ Non-immediate oop found\n-      non_immediate_oops = true;\n-      continue;\n-    }\n-\n-    if (r->oop_value() != nullptr) {\n-      \/\/ Non-null immediate oop found. Null oops can safely be\n-      \/\/ ignored since the method will be re-registered if they\n-      \/\/ are later patched to be non-null.\n-      immediate_oops.push(r->oop_addr());\n-    }\n-  }\n-\n-  \/\/ Attach GC data to nmethod\n-  XNMethodData* data = gc_data(nm);\n-  if (data == nullptr) {\n-    data = new XNMethodData();\n-    set_gc_data(nm, data);\n-  }\n-\n-  \/\/ Attach oops in GC data\n-  XNMethodDataOops* const new_oops = XNMethodDataOops::create(immediate_oops, non_immediate_oops);\n-  XNMethodDataOops* const old_oops = data->swap_oops(new_oops);\n-  XNMethodDataOops::destroy(old_oops);\n-}\n-\n-XReentrantLock* XNMethod::lock_for_nmethod(nmethod* nm) {\n-  return gc_data(nm)->lock();\n-}\n-\n-XReentrantLock* XNMethod::ic_lock_for_nmethod(nmethod* nm) {\n-  return gc_data(nm)->ic_lock();\n-}\n-\n-void XNMethod::log_register(const nmethod* nm) {\n-  LogTarget(Trace, gc, nmethod) log;\n-  if (!log.is_enabled()) {\n-    return;\n-  }\n-\n-  const XNMethodDataOops* const oops = gc_data(nm)->oops();\n-\n-  log.print(\"Register NMethod: %s.%s (\" PTR_FORMAT \"), \"\n-            \"Compiler: %s, Oops: %d, ImmediateOops: \" SIZE_FORMAT \", NonImmediateOops: %s\",\n-            nm->method()->method_holder()->external_name(),\n-            nm->method()->name()->as_C_string(),\n-            p2i(nm),\n-            nm->compiler_name(),\n-            nm->oops_count() - 1,\n-            oops->immediates_count(),\n-            oops->has_non_immediates() ? \"Yes\" : \"No\");\n-\n-  LogTarget(Trace, gc, nmethod, oops) log_oops;\n-  if (!log_oops.is_enabled()) {\n-    return;\n-  }\n-\n-  \/\/ Print nmethod oops table\n-  {\n-    oop* const begin = nm->oops_begin();\n-    oop* const end = nm->oops_end();\n-    for (oop* p = begin; p < end; p++) {\n-      const oop o = Atomic::load(p); \/\/ C1 PatchingStub may replace it concurrently.\n-      const char* external_name = (o == nullptr) ? \"N\/A\" : o->klass()->external_name();\n-      log_oops.print(\"           Oop[\" SIZE_FORMAT \"] \" PTR_FORMAT \" (%s)\",\n-                     (p - begin), p2i(o), external_name);\n-    }\n-  }\n-\n-  \/\/ Print nmethod immediate oops\n-  {\n-    oop** const begin = oops->immediates_begin();\n-    oop** const end = oops->immediates_end();\n-    for (oop** p = begin; p < end; p++) {\n-      log_oops.print(\"  ImmediateOop[\" SIZE_FORMAT \"] \" PTR_FORMAT \" @ \" PTR_FORMAT \" (%s)\",\n-                     (p - begin), p2i(**p), p2i(*p), (**p)->klass()->external_name());\n-    }\n-  }\n-}\n-\n-void XNMethod::log_unregister(const nmethod* nm) {\n-  LogTarget(Debug, gc, nmethod) log;\n-  if (!log.is_enabled()) {\n-    return;\n-  }\n-\n-  log.print(\"Unregister NMethod: %s.%s (\" PTR_FORMAT \")\",\n-            nm->method()->method_holder()->external_name(),\n-            nm->method()->name()->as_C_string(),\n-            p2i(nm));\n-}\n-\n-void XNMethod::register_nmethod(nmethod* nm) {\n-  ResourceMark rm;\n-\n-  \/\/ Create and attach gc data\n-  attach_gc_data(nm);\n-\n-  log_register(nm);\n-\n-  XNMethodTable::register_nmethod(nm);\n-\n-  \/\/ Disarm nmethod entry barrier\n-  disarm(nm);\n-}\n-\n-void XNMethod::unregister_nmethod(nmethod* nm) {\n-  ResourceMark rm;\n-\n-  log_unregister(nm);\n-\n-  XNMethodTable::unregister_nmethod(nm);\n-\n-  \/\/ Destroy GC data\n-  delete gc_data(nm);\n-}\n-\n-bool XNMethod::supports_entry_barrier(nmethod* nm) {\n-  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  return bs->supports_entry_barrier(nm);\n-}\n-\n-bool XNMethod::is_armed(nmethod* nm) {\n-  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  return bs->is_armed(nm);\n-}\n-\n-void XNMethod::disarm(nmethod* nm) {\n-  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  bs->disarm(nm);\n-}\n-\n-void XNMethod::set_guard_value(nmethod* nm, int value) {\n-  BarrierSetNMethod* const bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  bs->set_guard_value(nm, value);\n-}\n-\n-void XNMethod::nmethod_oops_do(nmethod* nm, OopClosure* cl) {\n-  XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n-  XNMethod::nmethod_oops_do_inner(nm, cl);\n-}\n-\n-void XNMethod::nmethod_oops_do_inner(nmethod* nm, OopClosure* cl) {\n-  \/\/ Process oops table\n-  {\n-    oop* const begin = nm->oops_begin();\n-    oop* const end = nm->oops_end();\n-    for (oop* p = begin; p < end; p++) {\n-      if (!Universe::contains_non_oop_word(p)) {\n-        cl->do_oop(p);\n-      }\n-    }\n-  }\n-\n-  XNMethodDataOops* const oops = gc_data(nm)->oops();\n-\n-  \/\/ Process immediate oops\n-  {\n-    oop** const begin = oops->immediates_begin();\n-    oop** const end = oops->immediates_end();\n-    for (oop** p = begin; p < end; p++) {\n-      if (*p != Universe::non_oop_word()) {\n-        cl->do_oop(*p);\n-      }\n-    }\n-  }\n-\n-  \/\/ Process non-immediate oops\n-  if (oops->has_non_immediates()) {\n-    nm->fix_oop_relocations();\n-  }\n-}\n-\n-class XNMethodOopClosure : public OopClosure {\n-public:\n-  virtual void do_oop(oop* p) {\n-    if (XResurrection::is_blocked()) {\n-      XBarrier::keep_alive_barrier_on_phantom_root_oop_field(p);\n-    } else {\n-      XBarrier::load_barrier_on_root_oop_field(p);\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-void XNMethod::nmethod_oops_barrier(nmethod* nm) {\n-  XNMethodOopClosure cl;\n-  nmethod_oops_do_inner(nm, &cl);\n-}\n-\n-void XNMethod::nmethods_do_begin() {\n-  XNMethodTable::nmethods_do_begin();\n-}\n-\n-void XNMethod::nmethods_do_end() {\n-  XNMethodTable::nmethods_do_end();\n-}\n-\n-void XNMethod::nmethods_do(NMethodClosure* cl) {\n-  XNMethodTable::nmethods_do(cl);\n-}\n-\n-class XNMethodUnlinkClosure : public NMethodClosure {\n-private:\n-  bool          _unloading_occurred;\n-  volatile bool _failed;\n-\n-  void set_failed() {\n-    Atomic::store(&_failed, true);\n-  }\n-\n-public:\n-  XNMethodUnlinkClosure(bool unloading_occurred) :\n-      _unloading_occurred(unloading_occurred),\n-      _failed(false) {}\n-\n-  virtual void do_nmethod(nmethod* nm) {\n-    if (failed()) {\n-      return;\n-    }\n-\n-    if (nm->is_unloading()) {\n-      XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n-      nm->unlink();\n-      return;\n-    }\n-\n-    {\n-      XLocker<XReentrantLock> locker(XNMethod::lock_for_nmethod(nm));\n-\n-      if (XNMethod::is_armed(nm)) {\n-        \/\/ Heal oops and arm phase invariantly\n-        XNMethod::nmethod_oops_barrier(nm);\n-        XNMethod::set_guard_value(nm, 0);\n-      }\n-    }\n-\n-    \/\/ Clear compiled ICs and exception caches\n-    XLocker<XReentrantLock> locker(XNMethod::ic_lock_for_nmethod(nm));\n-    nm->unload_nmethod_caches(_unloading_occurred);\n-  }\n-\n-  bool failed() const {\n-    return Atomic::load(&_failed);\n-  }\n-};\n-\n-class XNMethodUnlinkTask : public XTask {\n-private:\n-  XNMethodUnlinkClosure _cl;\n-\n-public:\n-  XNMethodUnlinkTask(bool unloading_occurred) :\n-      XTask(\"XNMethodUnlinkTask\"),\n-      _cl(unloading_occurred) {\n-    XNMethodTable::nmethods_do_begin();\n-  }\n-\n-  ~XNMethodUnlinkTask() {\n-    XNMethodTable::nmethods_do_end();\n-  }\n-\n-  virtual void work() {\n-    XNMethodTable::nmethods_do(&_cl);\n-  }\n-};\n-\n-void XNMethod::unlink(XWorkers* workers, bool unloading_occurred) {\n-  XNMethodUnlinkTask task(unloading_occurred);\n-  workers->run(&task);\n-}\n-\n-void XNMethod::purge() {\n-  ClassUnloadingContext::context()->purge_and_free_nmethods();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethod.cpp","additions":0,"deletions":349,"binary":false,"changes":349,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNMETHOD_HPP\n-#define SHARE_GC_X_XNMETHOD_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class nmethod;\n-class NMethodClosure;\n-class XReentrantLock;\n-class XWorkers;\n-\n-class XNMethod : public AllStatic {\n-private:\n-  static void attach_gc_data(nmethod* nm);\n-\n-  static void log_register(const nmethod* nm);\n-  static void log_unregister(const nmethod* nm);\n-\n-public:\n-  static void register_nmethod(nmethod* nm);\n-  static void unregister_nmethod(nmethod* nm);\n-\n-  static bool supports_entry_barrier(nmethod* nm);\n-\n-  static bool is_armed(nmethod* nm);\n-  static void disarm(nmethod* nm);\n-  static void set_guard_value(nmethod* nm, int value);\n-\n-  static void nmethod_oops_do(nmethod* nm, OopClosure* cl);\n-  static void nmethod_oops_do_inner(nmethod* nm, OopClosure* cl);\n-\n-  static void nmethod_oops_barrier(nmethod* nm);\n-\n-  static void nmethods_do_begin();\n-  static void nmethods_do_end();\n-  static void nmethods_do(NMethodClosure* cl);\n-\n-  static XReentrantLock* lock_for_nmethod(nmethod* nm);\n-  static XReentrantLock* ic_lock_for_nmethod(nmethod* nm);\n-\n-  static void unlink(XWorkers* workers, bool unloading_occurred);\n-  static void purge();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XNMETHOD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethod.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,93 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAttachedArray.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xNMethodData.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-\n-XNMethodDataOops* XNMethodDataOops::create(const GrowableArray<oop*>& immediates, bool has_non_immediates) {\n-  return ::new (AttachedArray::alloc(immediates.length())) XNMethodDataOops(immediates, has_non_immediates);\n-}\n-\n-void XNMethodDataOops::destroy(XNMethodDataOops* oops) {\n-  AttachedArray::free(oops);\n-}\n-\n-XNMethodDataOops::XNMethodDataOops(const GrowableArray<oop*>& immediates, bool has_non_immediates) :\n-    _immediates(immediates.length()),\n-    _has_non_immediates(has_non_immediates) {\n-  \/\/ Save all immediate oops\n-  for (size_t i = 0; i < immediates_count(); i++) {\n-    immediates_begin()[i] = immediates.at(int(i));\n-  }\n-}\n-\n-size_t XNMethodDataOops::immediates_count() const {\n-  return _immediates.length();\n-}\n-\n-oop** XNMethodDataOops::immediates_begin() const {\n-  return _immediates(this);\n-}\n-\n-oop** XNMethodDataOops::immediates_end() const {\n-  return immediates_begin() + immediates_count();\n-}\n-\n-bool XNMethodDataOops::has_non_immediates() const {\n-  return _has_non_immediates;\n-}\n-\n-XNMethodData::XNMethodData() :\n-    _lock(),\n-    _ic_lock(),\n-    _oops(nullptr) {}\n-\n-XNMethodData::~XNMethodData() {\n-  XNMethodDataOops::destroy(_oops);\n-}\n-\n-XReentrantLock* XNMethodData::lock() {\n-  return &_lock;\n-}\n-\n-XReentrantLock* XNMethodData::ic_lock() {\n-  return &_ic_lock;\n-}\n-\n-XNMethodDataOops* XNMethodData::oops() const {\n-  return Atomic::load_acquire(&_oops);\n-}\n-\n-XNMethodDataOops* XNMethodData::swap_oops(XNMethodDataOops* new_oops) {\n-  XLocker<XReentrantLock> locker(&_lock);\n-  XNMethodDataOops* const old_oops = _oops;\n-  _oops = new_oops;\n-  return old_oops;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodData.cpp","additions":0,"deletions":93,"binary":false,"changes":93,"status":"deleted"},{"patch":"@@ -1,73 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNMETHODDATA_HPP\n-#define SHARE_GC_X_XNMETHODDATA_HPP\n-\n-#include \"gc\/x\/xAttachedArray.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class nmethod;\n-template <typename T> class GrowableArray;\n-\n-class XNMethodDataOops {\n-private:\n-  typedef XAttachedArray<XNMethodDataOops, oop*> AttachedArray;\n-\n-  const AttachedArray _immediates;\n-  const bool          _has_non_immediates;\n-\n-  XNMethodDataOops(const GrowableArray<oop*>& immediates, bool has_non_immediates);\n-\n-public:\n-  static XNMethodDataOops* create(const GrowableArray<oop*>& immediates, bool has_non_immediates);\n-  static void destroy(XNMethodDataOops* oops);\n-\n-  size_t immediates_count() const;\n-  oop** immediates_begin() const;\n-  oop** immediates_end() const;\n-\n-  bool has_non_immediates() const;\n-};\n-\n-class XNMethodData : public CHeapObj<mtGC> {\n-private:\n-  XReentrantLock             _lock;\n-  XReentrantLock             _ic_lock;\n-  XNMethodDataOops* volatile _oops;\n-\n-public:\n-  XNMethodData();\n-  ~XNMethodData();\n-\n-  XReentrantLock* lock();\n-  XReentrantLock* ic_lock();\n-\n-  XNMethodDataOops* oops() const;\n-  XNMethodDataOops* swap_oops(XNMethodDataOops* oops);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XNMETHODDATA_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodData.hpp","additions":0,"deletions":73,"binary":false,"changes":73,"status":"deleted"},{"patch":"@@ -1,234 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/relocInfo.hpp\"\n-#include \"code\/nmethod.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHash.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xNMethodData.hpp\"\n-#include \"gc\/x\/xNMethodTable.hpp\"\n-#include \"gc\/x\/xNMethodTableEntry.hpp\"\n-#include \"gc\/x\/xNMethodTableIteration.hpp\"\n-#include \"gc\/x\/xSafeDelete.inline.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-XNMethodTableEntry* XNMethodTable::_table = nullptr;\n-size_t XNMethodTable::_size = 0;\n-size_t XNMethodTable::_nregistered = 0;\n-size_t XNMethodTable::_nunregistered = 0;\n-XNMethodTableIteration XNMethodTable::_iteration;\n-XSafeDeleteNoLock<XNMethodTableEntry[]> XNMethodTable::_safe_delete;\n-\n-size_t XNMethodTable::first_index(const nmethod* nm, size_t size) {\n-  assert(is_power_of_2(size), \"Invalid size\");\n-  const size_t mask = size - 1;\n-  const size_t hash = XHash::address_to_uint32((uintptr_t)nm);\n-  return hash & mask;\n-}\n-\n-size_t XNMethodTable::next_index(size_t prev_index, size_t size) {\n-  assert(is_power_of_2(size), \"Invalid size\");\n-  const size_t mask = size - 1;\n-  return (prev_index + 1) & mask;\n-}\n-\n-bool XNMethodTable::register_entry(XNMethodTableEntry* table, size_t size, nmethod* nm) {\n-  const XNMethodTableEntry entry(nm);\n-  size_t index = first_index(nm, size);\n-\n-  for (;;) {\n-    const XNMethodTableEntry table_entry = table[index];\n-\n-    if (!table_entry.registered() && !table_entry.unregistered()) {\n-      \/\/ Insert new entry\n-      table[index] = entry;\n-      return true;\n-    }\n-\n-    if (table_entry.registered() && table_entry.method() == nm) {\n-      \/\/ Replace existing entry\n-      table[index] = entry;\n-      return false;\n-    }\n-\n-    index = next_index(index, size);\n-  }\n-}\n-\n-void XNMethodTable::unregister_entry(XNMethodTableEntry* table, size_t size, nmethod* nm) {\n-  size_t index = first_index(nm, size);\n-\n-  for (;;) {\n-    const XNMethodTableEntry table_entry = table[index];\n-    assert(table_entry.registered() || table_entry.unregistered(), \"Entry not found\");\n-\n-    if (table_entry.registered() && table_entry.method() == nm) {\n-      \/\/ Remove entry\n-      table[index] = XNMethodTableEntry(true \/* unregistered *\/);\n-      return;\n-    }\n-\n-    index = next_index(index, size);\n-  }\n-}\n-\n-void XNMethodTable::rebuild(size_t new_size) {\n-  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n-\n-  assert(is_power_of_2(new_size), \"Invalid size\");\n-\n-  log_debug(gc, nmethod)(\"Rebuilding NMethod Table: \"\n-                         SIZE_FORMAT \"->\" SIZE_FORMAT \" entries, \"\n-                         SIZE_FORMAT \"(%.0f%%->%.0f%%) registered, \"\n-                         SIZE_FORMAT \"(%.0f%%->%.0f%%) unregistered\",\n-                         _size, new_size,\n-                         _nregistered, percent_of(_nregistered, _size), percent_of(_nregistered, new_size),\n-                         _nunregistered, percent_of(_nunregistered, _size), 0.0);\n-\n-  \/\/ Allocate new table\n-  XNMethodTableEntry* const new_table = new XNMethodTableEntry[new_size];\n-\n-  \/\/ Transfer all registered entries\n-  for (size_t i = 0; i < _size; i++) {\n-    const XNMethodTableEntry entry = _table[i];\n-    if (entry.registered()) {\n-      register_entry(new_table, new_size, entry.method());\n-    }\n-  }\n-\n-  \/\/ Free old table\n-  _safe_delete(_table);\n-\n-  \/\/ Install new table\n-  _table = new_table;\n-  _size = new_size;\n-  _nunregistered = 0;\n-}\n-\n-void XNMethodTable::rebuild_if_needed() {\n-  \/\/ The hash table uses linear probing. To avoid wasting memory while\n-  \/\/ at the same time maintaining good hash collision behavior we want\n-  \/\/ to keep the table occupancy between 30% and 70%. The table always\n-  \/\/ grows\/shrinks by doubling\/halving its size. Pruning of unregistered\n-  \/\/ entries is done by rebuilding the table with or without resizing it.\n-  const size_t min_size = 1024;\n-  const size_t shrink_threshold = _size * 0.30;\n-  const size_t prune_threshold = _size * 0.65;\n-  const size_t grow_threshold = _size * 0.70;\n-\n-  if (_size == 0) {\n-    \/\/ Initialize table\n-    rebuild(min_size);\n-  } else if (_nregistered < shrink_threshold && _size > min_size) {\n-    \/\/ Shrink table\n-    rebuild(_size \/ 2);\n-  } else if (_nregistered + _nunregistered > grow_threshold) {\n-    \/\/ Prune or grow table\n-    if (_nregistered < prune_threshold) {\n-      \/\/ Prune table\n-      rebuild(_size);\n-    } else {\n-      \/\/ Grow table\n-      rebuild(_size * 2);\n-    }\n-  }\n-}\n-\n-size_t XNMethodTable::registered_nmethods() {\n-  return _nregistered;\n-}\n-\n-size_t XNMethodTable::unregistered_nmethods() {\n-  return _nunregistered;\n-}\n-\n-void XNMethodTable::register_nmethod(nmethod* nm) {\n-  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n-\n-  \/\/ Grow\/Shrink\/Prune table if needed\n-  rebuild_if_needed();\n-\n-  \/\/ Insert new entry\n-  if (register_entry(_table, _size, nm)) {\n-    \/\/ New entry registered. When register_entry() instead returns\n-    \/\/ false the nmethod was already in the table so we do not want\n-    \/\/ to increase number of registered entries in that case.\n-    _nregistered++;\n-  }\n-}\n-\n-void XNMethodTable::wait_until_iteration_done() {\n-  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n-\n-  while (_iteration.in_progress()) {\n-    CodeCache_lock->wait_without_safepoint_check();\n-  }\n-}\n-\n-void XNMethodTable::unregister_nmethod(nmethod* nm) {\n-  assert(CodeCache_lock->owned_by_self(), \"Lock must be held\");\n-\n-  \/\/ Remove entry\n-  unregister_entry(_table, _size, nm);\n-  _nunregistered++;\n-  _nregistered--;\n-}\n-\n-void XNMethodTable::nmethods_do_begin() {\n-  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-\n-  \/\/ Do not allow the table to be deleted while iterating\n-  _safe_delete.enable_deferred_delete();\n-\n-  \/\/ Prepare iteration\n-  _iteration.nmethods_do_begin(_table, _size);\n-}\n-\n-void XNMethodTable::nmethods_do_end() {\n-  MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-\n-  \/\/ Finish iteration\n-  _iteration.nmethods_do_end();\n-\n-  \/\/ Allow the table to be deleted\n-  _safe_delete.disable_deferred_delete();\n-\n-  \/\/ Notify iteration done\n-  CodeCache_lock->notify_all();\n-}\n-\n-void XNMethodTable::nmethods_do(NMethodClosure* cl) {\n-  _iteration.nmethods_do(cl);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTable.cpp","additions":0,"deletions":234,"binary":false,"changes":234,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNMETHODTABLE_HPP\n-#define SHARE_GC_X_XNMETHODTABLE_HPP\n-\n-#include \"gc\/x\/xNMethodTableIteration.hpp\"\n-#include \"gc\/x\/xSafeDelete.hpp\"\n-#include \"memory\/allStatic.hpp\"\n-\n-class nmethod;\n-class NMethodClosure;\n-class XNMethodTableEntry;\n-class XWorkers;\n-\n-class XNMethodTable : public AllStatic {\n-private:\n-  static XNMethodTableEntry*                     _table;\n-  static size_t                                  _size;\n-  static size_t                                  _nregistered;\n-  static size_t                                  _nunregistered;\n-  static XNMethodTableIteration                  _iteration;\n-  static XSafeDeleteNoLock<XNMethodTableEntry[]> _safe_delete;\n-\n-  static XNMethodTableEntry* create(size_t size);\n-  static void destroy(XNMethodTableEntry* table);\n-\n-  static size_t first_index(const nmethod* nm, size_t size);\n-  static size_t next_index(size_t prev_index, size_t size);\n-\n-  static bool register_entry(XNMethodTableEntry* table, size_t size, nmethod* nm);\n-  static void unregister_entry(XNMethodTableEntry* table, size_t size, nmethod* nm);\n-\n-  static void rebuild(size_t new_size);\n-  static void rebuild_if_needed();\n-\n-public:\n-  static size_t registered_nmethods();\n-  static size_t unregistered_nmethods();\n-\n-  static void register_nmethod(nmethod* nm);\n-  static void unregister_nmethod(nmethod* nm);\n-\n-  static void wait_until_iteration_done();\n-\n-  static void nmethods_do_begin();\n-  static void nmethods_do_end();\n-  static void nmethods_do(NMethodClosure* cl);\n-\n-  static void unlink(XWorkers* workers, bool unloading_occurred);\n-  static void purge(XWorkers* workers);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XNMETHODTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTable.hpp","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -1,81 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNMETHODTABLEENTRY_HPP\n-#define SHARE_GC_X_XNMETHODTABLEENTRY_HPP\n-\n-#include \"gc\/x\/xBitField.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class nmethod;\n-\n-\/\/\n-\/\/ NMethod table entry layout\n-\/\/ --------------------------\n-\/\/\n-\/\/   6\n-\/\/   3                                                                   2 1 0\n-\/\/  +---------------------------------------------------------------------+-+-+\n-\/\/  |11111111 11111111 11111111 11111111 11111111 11111111 11111111 111111|1|1|\n-\/\/  +---------------------------------------------------------------------+-+-+\n-\/\/  |                                                                     | |\n-\/\/  |                                      1-1 Unregistered Flag (1-bits) * |\n-\/\/  |                                                                       |\n-\/\/  |                                          0-0 Registered Flag (1-bits) *\n-\/\/  |\n-\/\/  * 63-2 NMethod Address (62-bits)\n-\/\/\n-\n-class XNMethodTableEntry : public CHeapObj<mtGC> {\n-private:\n-  typedef XBitField<uint64_t, bool,     0,  1>    field_registered;\n-  typedef XBitField<uint64_t, bool,     1,  1>    field_unregistered;\n-  typedef XBitField<uint64_t, nmethod*, 2, 62, 2> field_method;\n-\n-  uint64_t _entry;\n-\n-public:\n-  explicit XNMethodTableEntry(bool unregistered = false) :\n-      _entry(field_registered::encode(false) |\n-             field_unregistered::encode(unregistered) |\n-             field_method::encode(nullptr)) {}\n-\n-  explicit XNMethodTableEntry(nmethod* method) :\n-      _entry(field_registered::encode(true) |\n-             field_unregistered::encode(false) |\n-             field_method::encode(method)) {}\n-\n-  bool registered() const {\n-    return field_registered::decode(_entry);\n-  }\n-\n-  bool unregistered() const {\n-    return field_unregistered::decode(_entry);\n-  }\n-\n-  nmethod* method() const {\n-    return field_method::decode(_entry);\n-  }\n-};\n-\n-#endif \/\/ SHARE_GC_X_XNMETHODTABLEENTRY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTableEntry.hpp","additions":0,"deletions":81,"binary":false,"changes":81,"status":"deleted"},{"patch":"@@ -1,76 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xNMethodTableEntry.hpp\"\n-#include \"gc\/x\/xNMethodTableIteration.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-XNMethodTableIteration::XNMethodTableIteration() :\n-    _table(nullptr),\n-    _size(0),\n-    _claimed(0) {}\n-\n-bool XNMethodTableIteration::in_progress() const {\n-  return _table != nullptr;\n-}\n-\n-void XNMethodTableIteration::nmethods_do_begin(XNMethodTableEntry* table, size_t size) {\n-  assert(!in_progress(), \"precondition\");\n-\n-  _table = table;\n-  _size = size;\n-  _claimed = 0;\n-}\n-\n-void XNMethodTableIteration::nmethods_do_end() {\n-  assert(_claimed >= _size, \"Failed to claim all table entries\");\n-\n-  \/\/ Finish iteration\n-  _table = nullptr;\n-}\n-\n-void XNMethodTableIteration::nmethods_do(NMethodClosure* cl) {\n-  for (;;) {\n-    \/\/ Claim table partition. Each partition is currently sized to span\n-    \/\/ two cache lines. This number is just a guess, but seems to work well.\n-    const size_t partition_size = (XCacheLineSize * 2) \/ sizeof(XNMethodTableEntry);\n-    const size_t partition_start = MIN2(Atomic::fetch_then_add(&_claimed, partition_size), _size);\n-    const size_t partition_end = MIN2(partition_start + partition_size, _size);\n-    if (partition_start == partition_end) {\n-      \/\/ End of table\n-      break;\n-    }\n-\n-    \/\/ Process table partition\n-    for (size_t i = partition_start; i < partition_end; i++) {\n-      const XNMethodTableEntry entry = _table[i];\n-      if (entry.registered()) {\n-        cl->do_nmethod(entry.method());\n-      }\n-    }\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTableIteration.cpp","additions":0,"deletions":76,"binary":false,"changes":76,"status":"deleted"},{"patch":"@@ -1,48 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNMETHODTABLEITERATION_HPP\n-#define SHARE_GC_X_XNMETHODTABLEITERATION_HPP\n-\n-#include \"gc\/x\/xGlobals.hpp\"\n-\n-class NMethodClosure;\n-class XNMethodTableEntry;\n-\n-class XNMethodTableIteration {\n-private:\n-  XNMethodTableEntry*            _table;\n-  size_t                         _size;\n-  XCACHE_ALIGNED volatile size_t _claimed;\n-\n-public:\n-  XNMethodTableIteration();\n-\n-  bool in_progress() const;\n-\n-  void nmethods_do_begin(XNMethodTableEntry* table, size_t size);\n-  void nmethods_do_end();\n-  void nmethods_do(NMethodClosure* cl);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XNMETHODTABLEITERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNMethodTableIteration.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-\n-bool XNUMA::_enabled;\n-\n-void XNUMA::initialize() {\n-  pd_initialize();\n-\n-  log_info_p(gc, init)(\"NUMA Support: %s\", to_string());\n-  if (_enabled) {\n-    log_info_p(gc, init)(\"NUMA Nodes: %u\", count());\n-  }\n-}\n-\n-const char* XNUMA::to_string() {\n-  return _enabled ? \"Enabled\" : \"Disabled\";\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xNUMA.cpp","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,48 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNUMA_HPP\n-#define SHARE_GC_X_XNUMA_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XNUMA : public AllStatic {\n-private:\n-  static bool _enabled;\n-\n-  static void pd_initialize();\n-\n-public:\n-  static void initialize();\n-  static bool is_enabled();\n-\n-  static uint32_t count();\n-  static uint32_t id();\n-\n-  static uint32_t memory_id(uintptr_t addr);\n-\n-  static const char* to_string();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XNUMA_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNUMA.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"deleted"},{"patch":"@@ -1,33 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XNUMA_INLINE_HPP\n-#define SHARE_GC_X_XNUMA_INLINE_HPP\n-\n-#include \"gc\/x\/xNUMA.hpp\"\n-\n-inline bool XNUMA::is_enabled() {\n-  return _enabled;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XNUMA_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xNUMA.inline.hpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"deleted"},{"patch":"@@ -1,103 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"gc\/x\/xObjArrayAllocator.hpp\"\n-#include \"gc\/x\/xUtils.inline.hpp\"\n-#include \"oops\/arrayKlass.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-XObjArrayAllocator::XObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread) :\n-    ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n-\n-void XObjArrayAllocator::yield_for_safepoint() const {\n-  ThreadBlockInVM tbivm(JavaThread::cast(_thread));\n-}\n-\n-oop XObjArrayAllocator::initialize(HeapWord* mem) const {\n-  \/\/ ZGC specializes the initialization by performing segmented clearing\n-  \/\/ to allow shorter time-to-safepoints.\n-\n-  if (!_do_zero) {\n-    \/\/ No need for ZGC specialization\n-    return ObjArrayAllocator::initialize(mem);\n-  }\n-\n-  \/\/ A max segment size of 64K was chosen because microbenchmarking\n-  \/\/ suggested that it offered a good trade-off between allocation\n-  \/\/ time and time-to-safepoint\n-  const size_t segment_max = XUtils::bytes_to_words(64 * K);\n-  const BasicType element_type = ArrayKlass::cast(_klass)->element_type();\n-\n-  \/\/ Clear leading 32 bits, if necessary.\n-  int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n-  if (!is_aligned(base_offset, HeapWordSize)) {\n-    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n-    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n-    base_offset += BytesPerInt;\n-  }\n-  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n-\n-  const size_t header = heap_word_size(base_offset);\n-  const size_t payload_size = _word_size - header;\n-\n-  if (payload_size <= segment_max) {\n-    \/\/ To small to use segmented clearing\n-    return ObjArrayAllocator::initialize(mem);\n-  }\n-\n-  \/\/ Segmented clearing\n-\n-  \/\/ The array is going to be exposed before it has been completely\n-  \/\/ cleared, therefore we can't expose the header at the end of this\n-  \/\/ function. Instead explicitly initialize it according to our needs.\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n-  assert(_length >= 0, \"length should be non-negative\");\n-  arrayOopDesc::set_length(mem, _length);\n-\n-  \/\/ Keep the array alive across safepoints through an invisible\n-  \/\/ root. Invisible roots are not visited by the heap itarator\n-  \/\/ and the marking logic will not attempt to follow its elements.\n-  \/\/ Relocation knows how to dodge iterating over such objects.\n-  XThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n-\n-  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n-    \/\/ Calculate segment\n-    HeapWord* const start = (HeapWord*)(mem + header + processed);\n-    const size_t remaining = payload_size - processed;\n-    const size_t segment_size = MIN2(remaining, segment_max);\n-\n-    \/\/ Clear segment\n-    Copy::zero_to_words(start, segment_size);\n-\n-    \/\/ Safepoint\n-    yield_for_safepoint();\n-  }\n-\n-  XThreadLocalData::clear_invisible_root(_thread);\n-\n-  return cast_to_oop(mem);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":0,"deletions":103,"binary":false,"changes":103,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XOBJARRAYALLOCATOR_HPP\n-#define SHARE_GC_X_XOBJARRAYALLOCATOR_HPP\n-\n-#include \"gc\/shared\/memAllocator.hpp\"\n-\n-class XObjArrayAllocator : public ObjArrayAllocator {\n-private:\n-  virtual oop initialize(HeapWord* mem) const override;\n-\n-  void yield_for_safepoint() const;\n-\n-public:\n-  XObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XOBJARRAYALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,267 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xHeuristics.hpp\"\n-#include \"gc\/x\/xObjectAllocator.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageTable.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xValue.inline.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-static const XStatCounter XCounterUndoObjectAllocationSucceeded(\"Memory\", \"Undo Object Allocation Succeeded\", XStatUnitOpsPerSecond);\n-static const XStatCounter XCounterUndoObjectAllocationFailed(\"Memory\", \"Undo Object Allocation Failed\", XStatUnitOpsPerSecond);\n-\n-XObjectAllocator::XObjectAllocator() :\n-    _use_per_cpu_shared_small_pages(XHeuristics::use_per_cpu_shared_small_pages()),\n-    _used(0),\n-    _undone(0),\n-    _alloc_for_relocation(0),\n-    _undo_alloc_for_relocation(0),\n-    _shared_medium_page(nullptr),\n-    _shared_small_page(nullptr) {}\n-\n-XPage** XObjectAllocator::shared_small_page_addr() {\n-  return _use_per_cpu_shared_small_pages ? _shared_small_page.addr() : _shared_small_page.addr(0);\n-}\n-\n-XPage* const* XObjectAllocator::shared_small_page_addr() const {\n-  return _use_per_cpu_shared_small_pages ? _shared_small_page.addr() : _shared_small_page.addr(0);\n-}\n-\n-void XObjectAllocator::register_alloc_for_relocation(const XPageTable* page_table, uintptr_t addr, size_t size) {\n-  const XPage* const page = page_table->get(addr);\n-  const size_t aligned_size = align_up(size, page->object_alignment());\n-  Atomic::add(_alloc_for_relocation.addr(), aligned_size);\n-}\n-\n-void XObjectAllocator::register_undo_alloc_for_relocation(const XPage* page, size_t size) {\n-  const size_t aligned_size = align_up(size, page->object_alignment());\n-  Atomic::add(_undo_alloc_for_relocation.addr(), aligned_size);\n-}\n-\n-XPage* XObjectAllocator::alloc_page(uint8_t type, size_t size, XAllocationFlags flags) {\n-  XPage* const page = XHeap::heap()->alloc_page(type, size, flags);\n-  if (page != nullptr) {\n-    \/\/ Increment used bytes\n-    Atomic::add(_used.addr(), size);\n-  }\n-\n-  return page;\n-}\n-\n-void XObjectAllocator::undo_alloc_page(XPage* page) {\n-  \/\/ Increment undone bytes\n-  Atomic::add(_undone.addr(), page->size());\n-\n-  XHeap::heap()->undo_alloc_page(page);\n-}\n-\n-uintptr_t XObjectAllocator::alloc_object_in_shared_page(XPage** shared_page,\n-                                                        uint8_t page_type,\n-                                                        size_t page_size,\n-                                                        size_t size,\n-                                                        XAllocationFlags flags) {\n-  uintptr_t addr = 0;\n-  XPage* page = Atomic::load_acquire(shared_page);\n-\n-  if (page != nullptr) {\n-    addr = page->alloc_object_atomic(size);\n-  }\n-\n-  if (addr == 0) {\n-    \/\/ Allocate new page\n-    XPage* const new_page = alloc_page(page_type, page_size, flags);\n-    if (new_page != nullptr) {\n-      \/\/ Allocate object before installing the new page\n-      addr = new_page->alloc_object(size);\n-\n-    retry:\n-      \/\/ Install new page\n-      XPage* const prev_page = Atomic::cmpxchg(shared_page, page, new_page);\n-      if (prev_page != page) {\n-        if (prev_page == nullptr) {\n-          \/\/ Previous page was retired, retry installing the new page\n-          page = prev_page;\n-          goto retry;\n-        }\n-\n-        \/\/ Another page already installed, try allocation there first\n-        const uintptr_t prev_addr = prev_page->alloc_object_atomic(size);\n-        if (prev_addr == 0) {\n-          \/\/ Allocation failed, retry installing the new page\n-          page = prev_page;\n-          goto retry;\n-        }\n-\n-        \/\/ Allocation succeeded in already installed page\n-        addr = prev_addr;\n-\n-        \/\/ Undo new page allocation\n-        undo_alloc_page(new_page);\n-      }\n-    }\n-  }\n-\n-  return addr;\n-}\n-\n-uintptr_t XObjectAllocator::alloc_large_object(size_t size, XAllocationFlags flags) {\n-  uintptr_t addr = 0;\n-\n-  \/\/ Allocate new large page\n-  const size_t page_size = align_up(size, XGranuleSize);\n-  XPage* const page = alloc_page(XPageTypeLarge, page_size, flags);\n-  if (page != nullptr) {\n-    \/\/ Allocate the object\n-    addr = page->alloc_object(size);\n-  }\n-\n-  return addr;\n-}\n-\n-uintptr_t XObjectAllocator::alloc_medium_object(size_t size, XAllocationFlags flags) {\n-  return alloc_object_in_shared_page(_shared_medium_page.addr(), XPageTypeMedium, XPageSizeMedium, size, flags);\n-}\n-\n-uintptr_t XObjectAllocator::alloc_small_object(size_t size, XAllocationFlags flags) {\n-  return alloc_object_in_shared_page(shared_small_page_addr(), XPageTypeSmall, XPageSizeSmall, size, flags);\n-}\n-\n-uintptr_t XObjectAllocator::alloc_object(size_t size, XAllocationFlags flags) {\n-  if (size <= XObjectSizeLimitSmall) {\n-    \/\/ Small\n-    return alloc_small_object(size, flags);\n-  } else if (size <= XObjectSizeLimitMedium) {\n-    \/\/ Medium\n-    return alloc_medium_object(size, flags);\n-  } else {\n-    \/\/ Large\n-    return alloc_large_object(size, flags);\n-  }\n-}\n-\n-uintptr_t XObjectAllocator::alloc_object(size_t size) {\n-  XAllocationFlags flags;\n-  return alloc_object(size, flags);\n-}\n-\n-uintptr_t XObjectAllocator::alloc_object_for_relocation(const XPageTable* page_table, size_t size) {\n-  XAllocationFlags flags;\n-  flags.set_non_blocking();\n-\n-  const uintptr_t addr = alloc_object(size, flags);\n-  if (addr != 0) {\n-    register_alloc_for_relocation(page_table, addr, size);\n-  }\n-\n-  return addr;\n-}\n-\n-void XObjectAllocator::undo_alloc_object_for_relocation(XPage* page, uintptr_t addr, size_t size) {\n-  const uint8_t type = page->type();\n-\n-  if (type == XPageTypeLarge) {\n-    register_undo_alloc_for_relocation(page, size);\n-    undo_alloc_page(page);\n-    XStatInc(XCounterUndoObjectAllocationSucceeded);\n-  } else {\n-    if (page->undo_alloc_object_atomic(addr, size)) {\n-      register_undo_alloc_for_relocation(page, size);\n-      XStatInc(XCounterUndoObjectAllocationSucceeded);\n-    } else {\n-      XStatInc(XCounterUndoObjectAllocationFailed);\n-    }\n-  }\n-}\n-\n-size_t XObjectAllocator::used() const {\n-  size_t total_used = 0;\n-  size_t total_undone = 0;\n-\n-  XPerCPUConstIterator<size_t> iter_used(&_used);\n-  for (const size_t* cpu_used; iter_used.next(&cpu_used);) {\n-    total_used += *cpu_used;\n-  }\n-\n-  XPerCPUConstIterator<size_t> iter_undone(&_undone);\n-  for (const size_t* cpu_undone; iter_undone.next(&cpu_undone);) {\n-    total_undone += *cpu_undone;\n-  }\n-\n-  return total_used - total_undone;\n-}\n-\n-size_t XObjectAllocator::remaining() const {\n-  assert(XThread::is_java(), \"Should be a Java thread\");\n-\n-  const XPage* const page = Atomic::load_acquire(shared_small_page_addr());\n-  if (page != nullptr) {\n-    return page->remaining();\n-  }\n-\n-  return 0;\n-}\n-\n-size_t XObjectAllocator::relocated() const {\n-  size_t total_alloc = 0;\n-  size_t total_undo_alloc = 0;\n-\n-  XPerCPUConstIterator<size_t> iter_alloc(&_alloc_for_relocation);\n-  for (const size_t* alloc; iter_alloc.next(&alloc);) {\n-    total_alloc += Atomic::load(alloc);\n-  }\n-\n-  XPerCPUConstIterator<size_t> iter_undo_alloc(&_undo_alloc_for_relocation);\n-  for (const size_t* undo_alloc; iter_undo_alloc.next(&undo_alloc);) {\n-    total_undo_alloc += Atomic::load(undo_alloc);\n-  }\n-\n-  assert(total_alloc >= total_undo_alloc, \"Mismatch\");\n-\n-  return total_alloc - total_undo_alloc;\n-}\n-\n-void XObjectAllocator::retire_pages() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  \/\/ Reset used and undone bytes\n-  _used.set_all(0);\n-  _undone.set_all(0);\n-\n-  \/\/ Reset relocated bytes\n-  _alloc_for_relocation.set_all(0);\n-  _undo_alloc_for_relocation.set_all(0);\n-\n-  \/\/ Reset allocation pages\n-  _shared_medium_page.set(nullptr);\n-  _shared_small_page.set_all(nullptr);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xObjectAllocator.cpp","additions":0,"deletions":267,"binary":false,"changes":267,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XOBJECTALLOCATOR_HPP\n-#define SHARE_GC_X_XOBJECTALLOCATOR_HPP\n-\n-#include \"gc\/x\/xAllocationFlags.hpp\"\n-#include \"gc\/x\/xValue.hpp\"\n-\n-class XPage;\n-class XPageTable;\n-\n-class XObjectAllocator {\n-private:\n-  const bool         _use_per_cpu_shared_small_pages;\n-  XPerCPU<size_t>    _used;\n-  XPerCPU<size_t>    _undone;\n-  XPerCPU<size_t>    _alloc_for_relocation;\n-  XPerCPU<size_t>    _undo_alloc_for_relocation;\n-  XContended<XPage*> _shared_medium_page;\n-  XPerCPU<XPage*>    _shared_small_page;\n-\n-  XPage** shared_small_page_addr();\n-  XPage* const* shared_small_page_addr() const;\n-\n-  void register_alloc_for_relocation(const XPageTable* page_table, uintptr_t addr, size_t size);\n-  void register_undo_alloc_for_relocation(const XPage* page, size_t size);\n-\n-  XPage* alloc_page(uint8_t type, size_t size, XAllocationFlags flags);\n-  void undo_alloc_page(XPage* page);\n-\n-  \/\/ Allocate an object in a shared page. Allocate and\n-  \/\/ atomically install a new page if necessary.\n-  uintptr_t alloc_object_in_shared_page(XPage** shared_page,\n-                                        uint8_t page_type,\n-                                        size_t page_size,\n-                                        size_t size,\n-                                        XAllocationFlags flags);\n-\n-  uintptr_t alloc_large_object(size_t size, XAllocationFlags flags);\n-  uintptr_t alloc_medium_object(size_t size, XAllocationFlags flags);\n-  uintptr_t alloc_small_object(size_t size, XAllocationFlags flags);\n-  uintptr_t alloc_object(size_t size, XAllocationFlags flags);\n-\n-public:\n-  XObjectAllocator();\n-\n-  uintptr_t alloc_object(size_t size);\n-  uintptr_t alloc_object_for_relocation(const XPageTable* page_table, size_t size);\n-  void undo_alloc_object_for_relocation(XPage* page, uintptr_t addr, size_t size);\n-\n-  size_t used() const;\n-  size_t remaining() const;\n-  size_t relocated() const;\n-\n-  void retire_pages();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XOBJECTALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xObjectAllocator.hpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,36 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XOOP_HPP\n-#define SHARE_GC_X_XOOP_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-\n-class XOop : public AllStatic {\n-public:\n-  static oop from_address(uintptr_t addr);\n-  static uintptr_t to_address(oop o);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XOOP_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xOop.hpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"deleted"},{"patch":"@@ -1,37 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XOOP_INLINE_HPP\n-#define SHARE_GC_X_XOOP_INLINE_HPP\n-\n-#include \"gc\/x\/xOop.hpp\"\n-\n-inline oop XOop::from_address(uintptr_t addr) {\n-  return cast_to_oop(addr);\n-}\n-\n-inline uintptr_t XOop::to_address(oop o) {\n-  return cast_from_oop<uintptr_t>(o);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XOOP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xOop.inline.hpp","additions":0,"deletions":37,"binary":false,"changes":37,"status":"deleted"},{"patch":"@@ -1,135 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-XPage::XPage(const XVirtualMemory& vmem, const XPhysicalMemory& pmem) :\n-    XPage(type_from_size(vmem.size()), vmem, pmem) {}\n-\n-XPage::XPage(uint8_t type, const XVirtualMemory& vmem, const XPhysicalMemory& pmem) :\n-    _type(type),\n-    _numa_id((uint8_t)-1),\n-    _seqnum(0),\n-    _virtual(vmem),\n-    _top(start()),\n-    _livemap(object_max_count()),\n-    _last_used(0),\n-    _physical(pmem),\n-    _node() {\n-  assert_initialized();\n-}\n-\n-XPage::~XPage() {}\n-\n-void XPage::assert_initialized() const {\n-  assert(!_virtual.is_null(), \"Should not be null\");\n-  assert(!_physical.is_null(), \"Should not be null\");\n-  assert(_virtual.size() == _physical.size(), \"Virtual\/Physical size mismatch\");\n-  assert((_type == XPageTypeSmall && size() == XPageSizeSmall) ||\n-         (_type == XPageTypeMedium && size() == XPageSizeMedium) ||\n-         (_type == XPageTypeLarge && is_aligned(size(), XGranuleSize)),\n-         \"Page type\/size mismatch\");\n-}\n-\n-void XPage::reset() {\n-  _seqnum = XGlobalSeqNum;\n-  _top = start();\n-  _livemap.reset();\n-  _last_used = 0;\n-}\n-\n-void XPage::reset_for_in_place_relocation() {\n-  _seqnum = XGlobalSeqNum;\n-  _top = start();\n-}\n-\n-XPage* XPage::retype(uint8_t type) {\n-  assert(_type != type, \"Invalid retype\");\n-  _type = type;\n-  _livemap.resize(object_max_count());\n-  return this;\n-}\n-\n-XPage* XPage::split(size_t size) {\n-  return split(type_from_size(size), size);\n-}\n-\n-XPage* XPage::split(uint8_t type, size_t size) {\n-  assert(_virtual.size() > size, \"Invalid split\");\n-\n-  \/\/ Resize this page, keep _numa_id, _seqnum, and _last_used\n-  const XVirtualMemory vmem = _virtual.split(size);\n-  const XPhysicalMemory pmem = _physical.split(size);\n-  _type = type_from_size(_virtual.size());\n-  _top = start();\n-  _livemap.resize(object_max_count());\n-\n-  \/\/ Create new page, inherit _seqnum and _last_used\n-  XPage* const page = new XPage(type, vmem, pmem);\n-  page->_seqnum = _seqnum;\n-  page->_last_used = _last_used;\n-  return page;\n-}\n-\n-XPage* XPage::split_committed() {\n-  \/\/ Split any committed part of this page into a separate page,\n-  \/\/ leaving this page with only uncommitted physical memory.\n-  const XPhysicalMemory pmem = _physical.split_committed();\n-  if (pmem.is_null()) {\n-    \/\/ Nothing committed\n-    return nullptr;\n-  }\n-\n-  assert(!_physical.is_null(), \"Should not be null\");\n-\n-  \/\/ Resize this page\n-  const XVirtualMemory vmem = _virtual.split(pmem.size());\n-  _type = type_from_size(_virtual.size());\n-  _top = start();\n-  _livemap.resize(object_max_count());\n-\n-  \/\/ Create new page\n-  return new XPage(vmem, pmem);\n-}\n-\n-void XPage::print_on(outputStream* out) const {\n-  out->print_cr(\" %-6s  \" PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %s%s\",\n-                type_to_string(), start(), top(), end(),\n-                is_allocating()  ? \" Allocating\"  : \"\",\n-                is_relocatable() ? \" Relocatable\" : \"\");\n-}\n-\n-void XPage::print() const {\n-  print_on(tty);\n-}\n-\n-void XPage::verify_live(uint32_t live_objects, size_t live_bytes) const {\n-  guarantee(live_objects == _livemap.live_objects(), \"Invalid number of live objects\");\n-  guarantee(live_bytes == _livemap.live_bytes(), \"Invalid number of live bytes\");\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xPage.cpp","additions":0,"deletions":135,"binary":false,"changes":135,"status":"deleted"},{"patch":"@@ -1,125 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGE_HPP\n-#define SHARE_GC_X_XPAGE_HPP\n-\n-#include \"gc\/x\/xList.hpp\"\n-#include \"gc\/x\/xLiveMap.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.hpp\"\n-#include \"gc\/x\/xVirtualMemory.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class VMStructs;\n-\n-class XPage : public CHeapObj<mtGC> {\n-  friend class ::VMStructs;\n-  friend class XList<XPage>;\n-\n-private:\n-  uint8_t            _type;\n-  uint8_t            _numa_id;\n-  uint32_t           _seqnum;\n-  XVirtualMemory     _virtual;\n-  volatile uintptr_t _top;\n-  XLiveMap           _livemap;\n-  uint64_t           _last_used;\n-  XPhysicalMemory    _physical;\n-  XListNode<XPage>   _node;\n-\n-  void assert_initialized() const;\n-\n-  uint8_t type_from_size(size_t size) const;\n-  const char* type_to_string() const;\n-\n-  bool is_object_marked(uintptr_t addr) const;\n-  bool is_object_strongly_marked(uintptr_t addr) const;\n-\n-public:\n-  XPage(const XVirtualMemory& vmem, const XPhysicalMemory& pmem);\n-  XPage(uint8_t type, const XVirtualMemory& vmem, const XPhysicalMemory& pmem);\n-  ~XPage();\n-\n-  uint32_t object_max_count() const;\n-  size_t object_alignment_shift() const;\n-  size_t object_alignment() const;\n-\n-  uint8_t type() const;\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n-  size_t size() const;\n-  uintptr_t top() const;\n-  size_t remaining() const;\n-\n-  const XVirtualMemory& virtual_memory() const;\n-  const XPhysicalMemory& physical_memory() const;\n-  XPhysicalMemory& physical_memory();\n-\n-  uint8_t numa_id();\n-\n-  bool is_allocating() const;\n-  bool is_relocatable() const;\n-\n-  uint64_t last_used() const;\n-  void set_last_used();\n-\n-  void reset();\n-  void reset_for_in_place_relocation();\n-\n-  XPage* retype(uint8_t type);\n-  XPage* split(size_t size);\n-  XPage* split(uint8_t type, size_t size);\n-  XPage* split_committed();\n-\n-  bool is_in(uintptr_t addr) const;\n-\n-  bool is_marked() const;\n-  template <bool finalizable> bool is_object_marked(uintptr_t addr) const;\n-  bool is_object_live(uintptr_t addr) const;\n-  bool is_object_strongly_live(uintptr_t addr) const;\n-  bool mark_object(uintptr_t addr, bool finalizable, bool& inc_live);\n-\n-  void inc_live(uint32_t objects, size_t bytes);\n-  uint32_t live_objects() const;\n-  size_t live_bytes() const;\n-\n-  void object_iterate(ObjectClosure* cl);\n-\n-  uintptr_t alloc_object(size_t size);\n-  uintptr_t alloc_object_atomic(size_t size);\n-\n-  bool undo_alloc_object(uintptr_t addr, size_t size);\n-  bool undo_alloc_object_atomic(uintptr_t addr, size_t size);\n-\n-  void print_on(outputStream* out) const;\n-  void print() const;\n-\n-  void verify_live(uint32_t live_objects, size_t live_bytes) const;\n-};\n-\n-class XPageClosure {\n-public:\n-  virtual void do_page(const XPage* page) = 0;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XPAGE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPage.hpp","additions":0,"deletions":125,"binary":false,"changes":125,"status":"deleted"},{"patch":"@@ -1,313 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGE_INLINE_HPP\n-#define SHARE_GC_X_XPAGE_INLINE_HPP\n-\n-#include \"gc\/x\/xPage.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLiveMap.inline.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/checkedCast.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline uint8_t XPage::type_from_size(size_t size) const {\n-  if (size == XPageSizeSmall) {\n-    return XPageTypeSmall;\n-  } else if (size == XPageSizeMedium) {\n-    return XPageTypeMedium;\n-  } else {\n-    return XPageTypeLarge;\n-  }\n-}\n-\n-inline const char* XPage::type_to_string() const {\n-  switch (type()) {\n-  case XPageTypeSmall:\n-    return \"Small\";\n-\n-  case XPageTypeMedium:\n-    return \"Medium\";\n-\n-  default:\n-    assert(type() == XPageTypeLarge, \"Invalid page type\");\n-    return \"Large\";\n-  }\n-}\n-\n-inline uint32_t XPage::object_max_count() const {\n-  switch (type()) {\n-  case XPageTypeLarge:\n-    \/\/ A large page can only contain a single\n-    \/\/ object aligned to the start of the page.\n-    return 1;\n-\n-  default:\n-    return (uint32_t)(size() >> object_alignment_shift());\n-  }\n-}\n-\n-inline size_t XPage::object_alignment_shift() const {\n-  switch (type()) {\n-  case XPageTypeSmall:\n-    return XObjectAlignmentSmallShift;\n-\n-  case XPageTypeMedium:\n-    return XObjectAlignmentMediumShift;\n-\n-  default:\n-    assert(type() == XPageTypeLarge, \"Invalid page type\");\n-    return XObjectAlignmentLargeShift;\n-  }\n-}\n-\n-inline size_t XPage::object_alignment() const {\n-  switch (type()) {\n-  case XPageTypeSmall:\n-    return XObjectAlignmentSmall;\n-\n-  case XPageTypeMedium:\n-    return XObjectAlignmentMedium;\n-\n-  default:\n-    assert(type() == XPageTypeLarge, \"Invalid page type\");\n-    return XObjectAlignmentLarge;\n-  }\n-}\n-\n-inline uint8_t XPage::type() const {\n-  return _type;\n-}\n-\n-inline uintptr_t XPage::start() const {\n-  return _virtual.start();\n-}\n-\n-inline uintptr_t XPage::end() const {\n-  return _virtual.end();\n-}\n-\n-inline size_t XPage::size() const {\n-  return _virtual.size();\n-}\n-\n-inline uintptr_t XPage::top() const {\n-  return _top;\n-}\n-\n-inline size_t XPage::remaining() const {\n-  return end() - top();\n-}\n-\n-inline const XVirtualMemory& XPage::virtual_memory() const {\n-  return _virtual;\n-}\n-\n-inline const XPhysicalMemory& XPage::physical_memory() const {\n-  return _physical;\n-}\n-\n-inline XPhysicalMemory& XPage::physical_memory() {\n-  return _physical;\n-}\n-\n-inline uint8_t XPage::numa_id() {\n-  if (_numa_id == (uint8_t)-1) {\n-    _numa_id = checked_cast<uint8_t>(XNUMA::memory_id(XAddress::good(start())));\n-  }\n-\n-  return _numa_id;\n-}\n-\n-inline bool XPage::is_allocating() const {\n-  return _seqnum == XGlobalSeqNum;\n-}\n-\n-inline bool XPage::is_relocatable() const {\n-  return _seqnum < XGlobalSeqNum;\n-}\n-\n-inline uint64_t XPage::last_used() const {\n-  return _last_used;\n-}\n-\n-inline void XPage::set_last_used() {\n-  _last_used = (uint64_t)ceil(os::elapsedTime());\n-}\n-\n-inline bool XPage::is_in(uintptr_t addr) const {\n-  const uintptr_t offset = XAddress::offset(addr);\n-  return offset >= start() && offset < top();\n-}\n-\n-inline bool XPage::is_marked() const {\n-  assert(is_relocatable(), \"Invalid page state\");\n-  return _livemap.is_marked();\n-}\n-\n-inline bool XPage::is_object_marked(uintptr_t addr) const {\n-  assert(is_relocatable(), \"Invalid page state\");\n-  const size_t index = ((XAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n-  return _livemap.get(index);\n-}\n-\n-inline bool XPage::is_object_strongly_marked(uintptr_t addr) const {\n-  assert(is_relocatable(), \"Invalid page state\");\n-  const size_t index = ((XAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n-  return _livemap.get(index + 1);\n-}\n-\n-template <bool finalizable>\n-inline bool XPage::is_object_marked(uintptr_t addr) const {\n-  return finalizable ? is_object_marked(addr) : is_object_strongly_marked(addr);\n-}\n-\n-inline bool XPage::is_object_live(uintptr_t addr) const {\n-  return is_allocating() || is_object_marked(addr);\n-}\n-\n-inline bool XPage::is_object_strongly_live(uintptr_t addr) const {\n-  return is_allocating() || is_object_strongly_marked(addr);\n-}\n-\n-inline bool XPage::mark_object(uintptr_t addr, bool finalizable, bool& inc_live) {\n-  assert(XAddress::is_marked(addr), \"Invalid address\");\n-  assert(is_relocatable(), \"Invalid page state\");\n-  assert(is_in(addr), \"Invalid address\");\n-\n-  \/\/ Set mark bit\n-  const size_t index = ((XAddress::offset(addr) - start()) >> object_alignment_shift()) * 2;\n-  return _livemap.set(index, finalizable, inc_live);\n-}\n-\n-inline void XPage::inc_live(uint32_t objects, size_t bytes) {\n-  _livemap.inc_live(objects, bytes);\n-}\n-\n-inline uint32_t XPage::live_objects() const {\n-  assert(is_marked(), \"Should be marked\");\n-  return _livemap.live_objects();\n-}\n-\n-inline size_t XPage::live_bytes() const {\n-  assert(is_marked(), \"Should be marked\");\n-  return _livemap.live_bytes();\n-}\n-\n-inline void XPage::object_iterate(ObjectClosure* cl) {\n-  _livemap.iterate(cl, XAddress::good(start()), object_alignment_shift());\n-}\n-\n-inline uintptr_t XPage::alloc_object(size_t size) {\n-  assert(is_allocating(), \"Invalid state\");\n-\n-  const size_t aligned_size = align_up(size, object_alignment());\n-  const uintptr_t addr = top();\n-  const uintptr_t new_top = addr + aligned_size;\n-\n-  if (new_top > end()) {\n-    \/\/ Not enough space left\n-    return 0;\n-  }\n-\n-  _top = new_top;\n-\n-  return XAddress::good(addr);\n-}\n-\n-inline uintptr_t XPage::alloc_object_atomic(size_t size) {\n-  assert(is_allocating(), \"Invalid state\");\n-\n-  const size_t aligned_size = align_up(size, object_alignment());\n-  uintptr_t addr = top();\n-\n-  for (;;) {\n-    const uintptr_t new_top = addr + aligned_size;\n-    if (new_top > end()) {\n-      \/\/ Not enough space left\n-      return 0;\n-    }\n-\n-    const uintptr_t prev_top = Atomic::cmpxchg(&_top, addr, new_top);\n-    if (prev_top == addr) {\n-      \/\/ Success\n-      return XAddress::good(addr);\n-    }\n-\n-    \/\/ Retry\n-    addr = prev_top;\n-  }\n-}\n-\n-inline bool XPage::undo_alloc_object(uintptr_t addr, size_t size) {\n-  assert(is_allocating(), \"Invalid state\");\n-\n-  const uintptr_t offset = XAddress::offset(addr);\n-  const size_t aligned_size = align_up(size, object_alignment());\n-  const uintptr_t old_top = top();\n-  const uintptr_t new_top = old_top - aligned_size;\n-\n-  if (new_top != offset) {\n-    \/\/ Failed to undo allocation, not the last allocated object\n-    return false;\n-  }\n-\n-  _top = new_top;\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-inline bool XPage::undo_alloc_object_atomic(uintptr_t addr, size_t size) {\n-  assert(is_allocating(), \"Invalid state\");\n-\n-  const uintptr_t offset = XAddress::offset(addr);\n-  const size_t aligned_size = align_up(size, object_alignment());\n-  uintptr_t old_top = top();\n-\n-  for (;;) {\n-    const uintptr_t new_top = old_top - aligned_size;\n-    if (new_top != offset) {\n-      \/\/ Failed to undo allocation, not the last allocated object\n-      return false;\n-    }\n-\n-    const uintptr_t prev_top = Atomic::cmpxchg(&_top, old_top, new_top);\n-    if (prev_top == old_top) {\n-      \/\/ Success\n-      return true;\n-    }\n-\n-    \/\/ Retry\n-    old_top = prev_top;\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XPAGE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPage.inline.hpp","additions":0,"deletions":313,"binary":false,"changes":313,"status":"deleted"},{"patch":"@@ -1,870 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xFuture.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageAllocator.inline.hpp\"\n-#include \"gc\/x\/xPageCache.hpp\"\n-#include \"gc\/x\/xSafeDelete.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xUncommitter.hpp\"\n-#include \"gc\/x\/xUnmapper.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/init.hpp\"\n-#include \"runtime\/java.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-static const XStatCounter       XCounterAllocationRate(\"Memory\", \"Allocation Rate\", XStatUnitBytesPerSecond);\n-static const XStatCounter       XCounterPageCacheFlush(\"Memory\", \"Page Cache Flush\", XStatUnitBytesPerSecond);\n-static const XStatCounter       XCounterDefragment(\"Memory\", \"Defragment\", XStatUnitOpsPerSecond);\n-static const XStatCriticalPhase XCriticalPhaseAllocationStall(\"Allocation Stall\");\n-\n-enum XPageAllocationStall {\n-  XPageAllocationStallSuccess,\n-  XPageAllocationStallFailed,\n-  XPageAllocationStallStartGC\n-};\n-\n-class XPageAllocation : public StackObj {\n-  friend class XList<XPageAllocation>;\n-\n-private:\n-  const uint8_t                 _type;\n-  const size_t                  _size;\n-  const XAllocationFlags        _flags;\n-  const uint32_t                _seqnum;\n-  size_t                        _flushed;\n-  size_t                        _committed;\n-  XList<XPage>                  _pages;\n-  XListNode<XPageAllocation>    _node;\n-  XFuture<XPageAllocationStall> _stall_result;\n-\n-public:\n-  XPageAllocation(uint8_t type, size_t size, XAllocationFlags flags) :\n-      _type(type),\n-      _size(size),\n-      _flags(flags),\n-      _seqnum(XGlobalSeqNum),\n-      _flushed(0),\n-      _committed(0),\n-      _pages(),\n-      _node(),\n-      _stall_result() {}\n-\n-  uint8_t type() const {\n-    return _type;\n-  }\n-\n-  size_t size() const {\n-    return _size;\n-  }\n-\n-  XAllocationFlags flags() const {\n-    return _flags;\n-  }\n-\n-  uint32_t seqnum() const {\n-    return _seqnum;\n-  }\n-\n-  size_t flushed() const {\n-    return _flushed;\n-  }\n-\n-  void set_flushed(size_t flushed) {\n-    _flushed = flushed;\n-  }\n-\n-  size_t committed() const {\n-    return _committed;\n-  }\n-\n-  void set_committed(size_t committed) {\n-    _committed = committed;\n-  }\n-\n-  XPageAllocationStall wait() {\n-    return _stall_result.get();\n-  }\n-\n-  XList<XPage>* pages() {\n-    return &_pages;\n-  }\n-\n-  void satisfy(XPageAllocationStall result) {\n-    _stall_result.set(result);\n-  }\n-};\n-\n-XPageAllocator::XPageAllocator(XWorkers* workers,\n-                               size_t min_capacity,\n-                               size_t initial_capacity,\n-                               size_t max_capacity) :\n-    _lock(),\n-    _cache(),\n-    _virtual(max_capacity),\n-    _physical(max_capacity),\n-    _min_capacity(min_capacity),\n-    _max_capacity(max_capacity),\n-    _current_max_capacity(max_capacity),\n-    _capacity(0),\n-    _claimed(0),\n-    _used(0),\n-    _used_high(0),\n-    _used_low(0),\n-    _reclaimed(0),\n-    _stalled(),\n-    _nstalled(0),\n-    _satisfied(),\n-    _unmapper(new XUnmapper(this)),\n-    _uncommitter(new XUncommitter(this)),\n-    _safe_delete(),\n-    _initialized(false) {\n-\n-  if (!_virtual.is_initialized() || !_physical.is_initialized()) {\n-    return;\n-  }\n-\n-  log_info_p(gc, init)(\"Min Capacity: \" SIZE_FORMAT \"M\", min_capacity \/ M);\n-  log_info_p(gc, init)(\"Initial Capacity: \" SIZE_FORMAT \"M\", initial_capacity \/ M);\n-  log_info_p(gc, init)(\"Max Capacity: \" SIZE_FORMAT \"M\", max_capacity \/ M);\n-  if (XPageSizeMedium > 0) {\n-    log_info_p(gc, init)(\"Medium Page Size: \" SIZE_FORMAT \"M\", XPageSizeMedium \/ M);\n-  } else {\n-    log_info_p(gc, init)(\"Medium Page Size: N\/A\");\n-  }\n-  log_info_p(gc, init)(\"Pre-touch: %s\", AlwaysPreTouch ? \"Enabled\" : \"Disabled\");\n-\n-  \/\/ Warn if system limits could stop us from reaching max capacity\n-  _physical.warn_commit_limits(max_capacity);\n-\n-  \/\/ Check if uncommit should and can be enabled\n-  _physical.try_enable_uncommit(min_capacity, max_capacity);\n-\n-  \/\/ Pre-map initial capacity\n-  if (!prime_cache(workers, initial_capacity)) {\n-    log_error_p(gc)(\"Failed to allocate initial Java heap (\" SIZE_FORMAT \"M)\", initial_capacity \/ M);\n-    return;\n-  }\n-\n-  \/\/ Successfully initialized\n-  _initialized = true;\n-}\n-\n-class XPreTouchTask : public XTask {\n-private:\n-  const XPhysicalMemoryManager* const _physical;\n-  volatile uintptr_t                  _start;\n-  const uintptr_t                     _end;\n-\n-public:\n-  XPreTouchTask(const XPhysicalMemoryManager* physical, uintptr_t start, uintptr_t end) :\n-      XTask(\"XPreTouchTask\"),\n-      _physical(physical),\n-      _start(start),\n-      _end(end) {}\n-\n-  virtual void work() {\n-    for (;;) {\n-      \/\/ Get granule offset\n-      const size_t size = XGranuleSize;\n-      const uintptr_t offset = Atomic::fetch_then_add(&_start, size);\n-      if (offset >= _end) {\n-        \/\/ Done\n-        break;\n-      }\n-\n-      \/\/ Pre-touch granule\n-      _physical->pretouch(offset, size);\n-    }\n-  }\n-};\n-\n-bool XPageAllocator::prime_cache(XWorkers* workers, size_t size) {\n-  XAllocationFlags flags;\n-\n-  flags.set_non_blocking();\n-  flags.set_low_address();\n-\n-  XPage* const page = alloc_page(XPageTypeLarge, size, flags);\n-  if (page == nullptr) {\n-    return false;\n-  }\n-\n-  if (AlwaysPreTouch) {\n-    \/\/ Pre-touch page\n-    XPreTouchTask task(&_physical, page->start(), page->end());\n-    workers->run_all(&task);\n-  }\n-\n-  free_page(page, false \/* reclaimed *\/);\n-\n-  return true;\n-}\n-\n-bool XPageAllocator::is_initialized() const {\n-  return _initialized;\n-}\n-\n-size_t XPageAllocator::min_capacity() const {\n-  return _min_capacity;\n-}\n-\n-size_t XPageAllocator::max_capacity() const {\n-  return _max_capacity;\n-}\n-\n-size_t XPageAllocator::soft_max_capacity() const {\n-  \/\/ Note that SoftMaxHeapSize is a manageable flag\n-  const size_t soft_max_capacity = Atomic::load(&SoftMaxHeapSize);\n-  const size_t current_max_capacity = Atomic::load(&_current_max_capacity);\n-  return MIN2(soft_max_capacity, current_max_capacity);\n-}\n-\n-size_t XPageAllocator::capacity() const {\n-  return Atomic::load(&_capacity);\n-}\n-\n-size_t XPageAllocator::used() const {\n-  return Atomic::load(&_used);\n-}\n-\n-size_t XPageAllocator::unused() const {\n-  const ssize_t capacity = (ssize_t)Atomic::load(&_capacity);\n-  const ssize_t used = (ssize_t)Atomic::load(&_used);\n-  const ssize_t claimed = (ssize_t)Atomic::load(&_claimed);\n-  const ssize_t unused = capacity - used - claimed;\n-  return unused > 0 ? (size_t)unused : 0;\n-}\n-\n-XPageAllocatorStats XPageAllocator::stats() const {\n-  XLocker<XLock> locker(&_lock);\n-  return XPageAllocatorStats(_min_capacity,\n-                             _max_capacity,\n-                             soft_max_capacity(),\n-                             _capacity,\n-                             _used,\n-                             _used_high,\n-                             _used_low,\n-                             _reclaimed);\n-}\n-\n-void XPageAllocator::reset_statistics() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _reclaimed = 0;\n-  _used_high = _used_low = _used;\n-  _nstalled = 0;\n-}\n-\n-size_t XPageAllocator::increase_capacity(size_t size) {\n-  const size_t increased = MIN2(size, _current_max_capacity - _capacity);\n-\n-  if (increased > 0) {\n-    \/\/ Update atomically since we have concurrent readers\n-    Atomic::add(&_capacity, increased);\n-\n-    \/\/ Record time of last commit. When allocation, we prefer increasing\n-    \/\/ the capacity over flushing the cache. That means there could be\n-    \/\/ expired pages in the cache at this time. However, since we are\n-    \/\/ increasing the capacity we are obviously in need of committed\n-    \/\/ memory and should therefore not be uncommitting memory.\n-    _cache.set_last_commit();\n-  }\n-\n-  return increased;\n-}\n-\n-void XPageAllocator::decrease_capacity(size_t size, bool set_max_capacity) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::sub(&_capacity, size);\n-\n-  if (set_max_capacity) {\n-    \/\/ Adjust current max capacity to avoid further attempts to increase capacity\n-    log_error_p(gc)(\"Forced to lower max Java heap size from \"\n-                    SIZE_FORMAT \"M(%.0f%%) to \" SIZE_FORMAT \"M(%.0f%%)\",\n-                    _current_max_capacity \/ M, percent_of(_current_max_capacity, _max_capacity),\n-                    _capacity \/ M, percent_of(_capacity, _max_capacity));\n-\n-    \/\/ Update atomically since we have concurrent readers\n-    Atomic::store(&_current_max_capacity, _capacity);\n-  }\n-}\n-\n-void XPageAllocator::increase_used(size_t size, bool worker_relocation) {\n-  if (worker_relocation) {\n-    \/\/ Allocating a page for the purpose of worker relocation has\n-    \/\/ a negative contribution to the number of reclaimed bytes.\n-    _reclaimed -= size;\n-  }\n-\n-  \/\/ Update atomically since we have concurrent readers\n-  const size_t used = Atomic::add(&_used, size);\n-  if (used > _used_high) {\n-    _used_high = used;\n-  }\n-}\n-\n-void XPageAllocator::decrease_used(size_t size, bool reclaimed) {\n-  \/\/ Only pages explicitly released with the reclaimed flag set\n-  \/\/ counts as reclaimed bytes. This flag is true when we release\n-  \/\/ a page after relocation, and is false when we release a page\n-  \/\/ to undo an allocation.\n-  if (reclaimed) {\n-    _reclaimed += size;\n-  }\n-\n-  \/\/ Update atomically since we have concurrent readers\n-  const size_t used = Atomic::sub(&_used, size);\n-  if (used < _used_low) {\n-    _used_low = used;\n-  }\n-}\n-\n-bool XPageAllocator::commit_page(XPage* page) {\n-  \/\/ Commit physical memory\n-  return _physical.commit(page->physical_memory());\n-}\n-\n-void XPageAllocator::uncommit_page(XPage* page) {\n-  if (!ZUncommit) {\n-    return;\n-  }\n-\n-  \/\/ Uncommit physical memory\n-  _physical.uncommit(page->physical_memory());\n-}\n-\n-void XPageAllocator::map_page(const XPage* page) const {\n-  \/\/ Map physical memory\n-  _physical.map(page->start(), page->physical_memory());\n-}\n-\n-void XPageAllocator::unmap_page(const XPage* page) const {\n-  \/\/ Unmap physical memory\n-  _physical.unmap(page->start(), page->size());\n-}\n-\n-void XPageAllocator::destroy_page(XPage* page) {\n-  \/\/ Free virtual memory\n-  _virtual.free(page->virtual_memory());\n-\n-  \/\/ Free physical memory\n-  _physical.free(page->physical_memory());\n-\n-  \/\/ Delete page safely\n-  _safe_delete(page);\n-}\n-\n-bool XPageAllocator::is_alloc_allowed(size_t size) const {\n-  const size_t available = _current_max_capacity - _used - _claimed;\n-  return available >= size;\n-}\n-\n-bool XPageAllocator::alloc_page_common_inner(uint8_t type, size_t size, XList<XPage>* pages) {\n-  if (!is_alloc_allowed(size)) {\n-    \/\/ Out of memory\n-    return false;\n-  }\n-\n-  \/\/ Try allocate from the page cache\n-  XPage* const page = _cache.alloc_page(type, size);\n-  if (page != nullptr) {\n-    \/\/ Success\n-    pages->insert_last(page);\n-    return true;\n-  }\n-\n-  \/\/ Try increase capacity\n-  const size_t increased = increase_capacity(size);\n-  if (increased < size) {\n-    \/\/ Could not increase capacity enough to satisfy the allocation\n-    \/\/ completely. Flush the page cache to satisfy the remainder.\n-    const size_t remaining = size - increased;\n-    _cache.flush_for_allocation(remaining, pages);\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-bool XPageAllocator::alloc_page_common(XPageAllocation* allocation) {\n-  const uint8_t type = allocation->type();\n-  const size_t size = allocation->size();\n-  const XAllocationFlags flags = allocation->flags();\n-  XList<XPage>* const pages = allocation->pages();\n-\n-  if (!alloc_page_common_inner(type, size, pages)) {\n-    \/\/ Out of memory\n-    return false;\n-  }\n-\n-  \/\/ Updated used statistics\n-  increase_used(size, flags.worker_relocation());\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-static void check_out_of_memory_during_initialization() {\n-  if (!is_init_completed()) {\n-    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\", \"Java heap too small\");\n-  }\n-}\n-\n-bool XPageAllocator::alloc_page_stall(XPageAllocation* allocation) {\n-  XStatTimer timer(XCriticalPhaseAllocationStall);\n-  EventZAllocationStall event;\n-  XPageAllocationStall result;\n-\n-  \/\/ We can only block if the VM is fully initialized\n-  check_out_of_memory_during_initialization();\n-\n-  \/\/ Increment stalled counter\n-  Atomic::inc(&_nstalled);\n-\n-  do {\n-    \/\/ Start asynchronous GC\n-    XCollectedHeap::heap()->collect(GCCause::_z_allocation_stall);\n-\n-    \/\/ Wait for allocation to complete, fail or request a GC\n-    result = allocation->wait();\n-  } while (result == XPageAllocationStallStartGC);\n-\n-  {\n-    \/\/\n-    \/\/ We grab the lock here for two different reasons:\n-    \/\/\n-    \/\/ 1) Guard deletion of underlying semaphore. This is a workaround for\n-    \/\/ a bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n-    \/\/ the semaphore immediately after returning from sem_wait(). The\n-    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n-    \/\/ thread have returned from sem_wait(). To avoid this race we are\n-    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n-    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n-    \/\/\n-    \/\/ 2) Guard the list of satisfied pages.\n-    \/\/\n-    XLocker<XLock> locker(&_lock);\n-    _satisfied.remove(allocation);\n-  }\n-\n-  \/\/ Send event\n-  event.commit(allocation->type(), allocation->size());\n-\n-  return (result == XPageAllocationStallSuccess);\n-}\n-\n-bool XPageAllocator::alloc_page_or_stall(XPageAllocation* allocation) {\n-  {\n-    XLocker<XLock> locker(&_lock);\n-\n-    if (alloc_page_common(allocation)) {\n-      \/\/ Success\n-      return true;\n-    }\n-\n-    \/\/ Failed\n-    if (allocation->flags().non_blocking()) {\n-      \/\/ Don't stall\n-      return false;\n-    }\n-\n-    \/\/ Enqueue allocation request\n-    _stalled.insert_last(allocation);\n-  }\n-\n-  \/\/ Stall\n-  return alloc_page_stall(allocation);\n-}\n-\n-XPage* XPageAllocator::alloc_page_create(XPageAllocation* allocation) {\n-  const size_t size = allocation->size();\n-\n-  \/\/ Allocate virtual memory. To make error handling a lot more straight\n-  \/\/ forward, we allocate virtual memory before destroying flushed pages.\n-  \/\/ Flushed pages are also unmapped and destroyed asynchronously, so we\n-  \/\/ can't immediately reuse that part of the address space anyway.\n-  const XVirtualMemory vmem = _virtual.alloc(size, allocation->flags().low_address());\n-  if (vmem.is_null()) {\n-    log_error(gc)(\"Out of address space\");\n-    return nullptr;\n-  }\n-\n-  XPhysicalMemory pmem;\n-  size_t flushed = 0;\n-\n-  \/\/ Harvest physical memory from flushed pages\n-  XListRemoveIterator<XPage> iter(allocation->pages());\n-  for (XPage* page; iter.next(&page);) {\n-    flushed += page->size();\n-\n-    \/\/ Harvest flushed physical memory\n-    XPhysicalMemory& fmem = page->physical_memory();\n-    pmem.add_segments(fmem);\n-    fmem.remove_segments();\n-\n-    \/\/ Unmap and destroy page\n-    _unmapper->unmap_and_destroy_page(page);\n-  }\n-\n-  if (flushed > 0) {\n-    allocation->set_flushed(flushed);\n-\n-    \/\/ Update statistics\n-    XStatInc(XCounterPageCacheFlush, flushed);\n-    log_debug(gc, heap)(\"Page Cache Flushed: \" SIZE_FORMAT \"M\", flushed \/ M);\n-  }\n-\n-  \/\/ Allocate any remaining physical memory. Capacity and used has\n-  \/\/ already been adjusted, we just need to fetch the memory, which\n-  \/\/ is guaranteed to succeed.\n-  if (flushed < size) {\n-    const size_t remaining = size - flushed;\n-    allocation->set_committed(remaining);\n-    _physical.alloc(pmem, remaining);\n-  }\n-\n-  \/\/ Create new page\n-  return new XPage(allocation->type(), vmem, pmem);\n-}\n-\n-bool XPageAllocator::should_defragment(const XPage* page) const {\n-  \/\/ A small page can end up at a high address (second half of the address space)\n-  \/\/ if we've split a larger page or we have a constrained address space. To help\n-  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n-  \/\/ a lower address is available.\n-  return page->type() == XPageTypeSmall &&\n-         page->start() >= _virtual.reserved() \/ 2 &&\n-         page->start() > _virtual.lowest_available_address();\n-}\n-\n-bool XPageAllocator::is_alloc_satisfied(XPageAllocation* allocation) const {\n-  \/\/ The allocation is immediately satisfied if the list of pages contains\n-  \/\/ exactly one page, with the type and size that was requested. However,\n-  \/\/ even if the allocation is immediately satisfied we might still want to\n-  \/\/ return false here to force the page to be remapped to fight address\n-  \/\/ space fragmentation.\n-\n-  if (allocation->pages()->size() != 1) {\n-    \/\/ Not a single page\n-    return false;\n-  }\n-\n-  const XPage* const page = allocation->pages()->first();\n-  if (page->type() != allocation->type() ||\n-      page->size() != allocation->size()) {\n-    \/\/ Wrong type or size\n-    return false;\n-  }\n-\n-  if (should_defragment(page)) {\n-    \/\/ Defragment address space\n-    XStatInc(XCounterDefragment);\n-    return false;\n-  }\n-\n-  \/\/ Allocation immediately satisfied\n-  return true;\n-}\n-\n-XPage* XPageAllocator::alloc_page_finalize(XPageAllocation* allocation) {\n-  \/\/ Fast path\n-  if (is_alloc_satisfied(allocation)) {\n-    return allocation->pages()->remove_first();\n-  }\n-\n-  \/\/ Slow path\n-  XPage* const page = alloc_page_create(allocation);\n-  if (page == nullptr) {\n-    \/\/ Out of address space\n-    return nullptr;\n-  }\n-\n-  \/\/ Commit page\n-  if (commit_page(page)) {\n-    \/\/ Success\n-    map_page(page);\n-    return page;\n-  }\n-\n-  \/\/ Failed or partially failed. Split of any successfully committed\n-  \/\/ part of the page into a new page and insert it into list of pages,\n-  \/\/ so that it will be re-inserted into the page cache.\n-  XPage* const committed_page = page->split_committed();\n-  destroy_page(page);\n-\n-  if (committed_page != nullptr) {\n-    map_page(committed_page);\n-    allocation->pages()->insert_last(committed_page);\n-  }\n-\n-  return nullptr;\n-}\n-\n-void XPageAllocator::alloc_page_failed(XPageAllocation* allocation) {\n-  XLocker<XLock> locker(&_lock);\n-\n-  size_t freed = 0;\n-\n-  \/\/ Free any allocated\/flushed pages\n-  XListRemoveIterator<XPage> iter(allocation->pages());\n-  for (XPage* page; iter.next(&page);) {\n-    freed += page->size();\n-    free_page_inner(page, false \/* reclaimed *\/);\n-  }\n-\n-  \/\/ Adjust capacity and used to reflect the failed capacity increase\n-  const size_t remaining = allocation->size() - freed;\n-  decrease_used(remaining, false \/* reclaimed *\/);\n-  decrease_capacity(remaining, true \/* set_max_capacity *\/);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n-}\n-\n-XPage* XPageAllocator::alloc_page(uint8_t type, size_t size, XAllocationFlags flags) {\n-  EventZPageAllocation event;\n-\n-retry:\n-  XPageAllocation allocation(type, size, flags);\n-\n-  \/\/ Allocate one or more pages from the page cache. If the allocation\n-  \/\/ succeeds but the returned pages don't cover the complete allocation,\n-  \/\/ then finalize phase is allowed to allocate the remaining memory\n-  \/\/ directly from the physical memory manager. Note that this call might\n-  \/\/ block in a safepoint if the non-blocking flag is not set.\n-  if (!alloc_page_or_stall(&allocation)) {\n-    \/\/ Out of memory\n-    return nullptr;\n-  }\n-\n-  XPage* const page = alloc_page_finalize(&allocation);\n-  if (page == nullptr) {\n-    \/\/ Failed to commit or map. Clean up and retry, in the hope that\n-    \/\/ we can still allocate by flushing the page cache (more aggressively).\n-    alloc_page_failed(&allocation);\n-    goto retry;\n-  }\n-\n-  \/\/ Reset page. This updates the page's sequence number and must\n-  \/\/ be done after we potentially blocked in a safepoint (stalled)\n-  \/\/ where the global sequence number was updated.\n-  page->reset();\n-\n-  \/\/ Update allocation statistics. Exclude worker relocations to avoid\n-  \/\/ artificial inflation of the allocation rate during relocation.\n-  if (!flags.worker_relocation() && is_init_completed()) {\n-    \/\/ Note that there are two allocation rate counters, which have\n-    \/\/ different purposes and are sampled at different frequencies.\n-    const size_t bytes = page->size();\n-    XStatInc(XCounterAllocationRate, bytes);\n-    XStatInc(XStatAllocRate::counter(), bytes);\n-  }\n-\n-  \/\/ Send event\n-  event.commit(type, size, allocation.flushed(), allocation.committed(),\n-               page->physical_memory().nsegments(), flags.non_blocking());\n-\n-  return page;\n-}\n-\n-void XPageAllocator::satisfy_stalled() {\n-  for (;;) {\n-    XPageAllocation* const allocation = _stalled.first();\n-    if (allocation == nullptr) {\n-      \/\/ Allocation queue is empty\n-      return;\n-    }\n-\n-    if (!alloc_page_common(allocation)) {\n-      \/\/ Allocation could not be satisfied, give up\n-      return;\n-    }\n-\n-    \/\/ Allocation succeeded, dequeue and satisfy allocation request.\n-    \/\/ Note that we must dequeue the allocation request first, since\n-    \/\/ it will immediately be deallocated once it has been satisfied.\n-    _stalled.remove(allocation);\n-    _satisfied.insert_last(allocation);\n-    allocation->satisfy(XPageAllocationStallSuccess);\n-  }\n-}\n-\n-void XPageAllocator::free_page_inner(XPage* page, bool reclaimed) {\n-  \/\/ Update used statistics\n-  decrease_used(page->size(), reclaimed);\n-\n-  \/\/ Set time when last used\n-  page->set_last_used();\n-\n-  \/\/ Cache page\n-  _cache.free_page(page);\n-}\n-\n-void XPageAllocator::free_page(XPage* page, bool reclaimed) {\n-  XLocker<XLock> locker(&_lock);\n-\n-  \/\/ Free page\n-  free_page_inner(page, reclaimed);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n-}\n-\n-void XPageAllocator::free_pages(const XArray<XPage*>* pages, bool reclaimed) {\n-  XLocker<XLock> locker(&_lock);\n-\n-  \/\/ Free pages\n-  XArrayIterator<XPage*> iter(pages);\n-  for (XPage* page; iter.next(&page);) {\n-    free_page_inner(page, reclaimed);\n-  }\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n-}\n-\n-size_t XPageAllocator::uncommit(uint64_t* timeout) {\n-  \/\/ We need to join the suspendible thread set while manipulating capacity and\n-  \/\/ used, to make sure GC safepoints will have a consistent view. However, when\n-  \/\/ ZVerifyViews is enabled we need to join at a broader scope to also make sure\n-  \/\/ we don't change the address good mask after pages have been flushed, and\n-  \/\/ thereby made invisible to pages_do(), but before they have been unmapped.\n-  SuspendibleThreadSetJoiner joiner(ZVerifyViews);\n-  XList<XPage> pages;\n-  size_t flushed;\n-\n-  {\n-    SuspendibleThreadSetJoiner joiner(!ZVerifyViews);\n-    XLocker<XLock> locker(&_lock);\n-\n-    \/\/ Never uncommit below min capacity. We flush out and uncommit chunks at\n-    \/\/ a time (~0.8% of the max capacity, but at least one granule and at most\n-    \/\/ 256M), in case demand for memory increases while we are uncommitting.\n-    const size_t retain = MAX2(_used, _min_capacity);\n-    const size_t release = _capacity - retain;\n-    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, XGranuleSize), 256 * M);\n-    const size_t flush = MIN2(release, limit);\n-\n-    \/\/ Flush pages to uncommit\n-    flushed = _cache.flush_for_uncommit(flush, &pages, timeout);\n-    if (flushed == 0) {\n-      \/\/ Nothing flushed\n-      return 0;\n-    }\n-\n-    \/\/ Record flushed pages as claimed\n-    Atomic::add(&_claimed, flushed);\n-  }\n-\n-  \/\/ Unmap, uncommit, and destroy flushed pages\n-  XListRemoveIterator<XPage> iter(&pages);\n-  for (XPage* page; iter.next(&page);) {\n-    unmap_page(page);\n-    uncommit_page(page);\n-    destroy_page(page);\n-  }\n-\n-  {\n-    SuspendibleThreadSetJoiner joiner(!ZVerifyViews);\n-    XLocker<XLock> locker(&_lock);\n-\n-    \/\/ Adjust claimed and capacity to reflect the uncommit\n-    Atomic::sub(&_claimed, flushed);\n-    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n-  }\n-\n-  return flushed;\n-}\n-\n-void XPageAllocator::enable_deferred_delete() const {\n-  _safe_delete.enable_deferred_delete();\n-}\n-\n-void XPageAllocator::disable_deferred_delete() const {\n-  _safe_delete.disable_deferred_delete();\n-}\n-\n-void XPageAllocator::debug_map_page(const XPage* page) const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _physical.debug_map(page->start(), page->physical_memory());\n-}\n-\n-void XPageAllocator::debug_unmap_page(const XPage* page) const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _physical.debug_unmap(page->start(), page->size());\n-}\n-\n-void XPageAllocator::pages_do(XPageClosure* cl) const {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-\n-  XListIterator<XPageAllocation> iter_satisfied(&_satisfied);\n-  for (XPageAllocation* allocation; iter_satisfied.next(&allocation);) {\n-    XListIterator<XPage> iter_pages(allocation->pages());\n-    for (XPage* page; iter_pages.next(&page);) {\n-      cl->do_page(page);\n-    }\n-  }\n-\n-  _cache.pages_do(cl);\n-}\n-\n-bool XPageAllocator::has_alloc_stalled() const {\n-  return Atomic::load(&_nstalled) != 0;\n-}\n-\n-void XPageAllocator::check_out_of_memory() {\n-  XLocker<XLock> locker(&_lock);\n-\n-  \/\/ Fail allocation requests that were enqueued before the\n-  \/\/ last GC cycle started, otherwise start a new GC cycle.\n-  for (XPageAllocation* allocation = _stalled.first(); allocation != nullptr; allocation = _stalled.first()) {\n-    if (allocation->seqnum() == XGlobalSeqNum) {\n-      \/\/ Start a new GC cycle, keep allocation requests enqueued\n-      allocation->satisfy(XPageAllocationStallStartGC);\n-      return;\n-    }\n-\n-    \/\/ Out of memory, fail allocation request\n-    _stalled.remove(allocation);\n-    _satisfied.insert_last(allocation);\n-    allocation->satisfy(XPageAllocationStallFailed);\n-  }\n-}\n-\n-void XPageAllocator::threads_do(ThreadClosure* tc) const {\n-  tc->do_thread(_unmapper);\n-  tc->do_thread(_uncommitter);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xPageAllocator.cpp","additions":0,"deletions":870,"binary":false,"changes":870,"status":"deleted"},{"patch":"@@ -1,174 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGEALLOCATOR_HPP\n-#define SHARE_GC_X_XPAGEALLOCATOR_HPP\n-\n-#include \"gc\/x\/xAllocationFlags.hpp\"\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"gc\/x\/xList.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-#include \"gc\/x\/xPageCache.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.hpp\"\n-#include \"gc\/x\/xSafeDelete.hpp\"\n-#include \"gc\/x\/xVirtualMemory.hpp\"\n-\n-class ThreadClosure;\n-class VMStructs;\n-class XPageAllocation;\n-class XPageAllocatorStats;\n-class XWorkers;\n-class XUncommitter;\n-class XUnmapper;\n-\n-class XPageAllocator {\n-  friend class ::VMStructs;\n-  friend class XUnmapper;\n-  friend class XUncommitter;\n-\n-private:\n-  mutable XLock              _lock;\n-  XPageCache                 _cache;\n-  XVirtualMemoryManager      _virtual;\n-  XPhysicalMemoryManager     _physical;\n-  const size_t               _min_capacity;\n-  const size_t               _max_capacity;\n-  volatile size_t            _current_max_capacity;\n-  volatile size_t            _capacity;\n-  volatile size_t            _claimed;\n-  volatile size_t            _used;\n-  size_t                     _used_high;\n-  size_t                     _used_low;\n-  ssize_t                    _reclaimed;\n-  XList<XPageAllocation>     _stalled;\n-  volatile uint64_t          _nstalled;\n-  XList<XPageAllocation>     _satisfied;\n-  XUnmapper*                 _unmapper;\n-  XUncommitter*              _uncommitter;\n-  mutable XSafeDelete<XPage> _safe_delete;\n-  bool                       _initialized;\n-\n-  bool prime_cache(XWorkers* workers, size_t size);\n-\n-  size_t increase_capacity(size_t size);\n-  void decrease_capacity(size_t size, bool set_max_capacity);\n-\n-  void increase_used(size_t size, bool relocation);\n-  void decrease_used(size_t size, bool reclaimed);\n-\n-  bool commit_page(XPage* page);\n-  void uncommit_page(XPage* page);\n-\n-  void map_page(const XPage* page) const;\n-  void unmap_page(const XPage* page) const;\n-\n-  void destroy_page(XPage* page);\n-\n-  bool is_alloc_allowed(size_t size) const;\n-\n-  bool alloc_page_common_inner(uint8_t type, size_t size, XList<XPage>* pages);\n-  bool alloc_page_common(XPageAllocation* allocation);\n-  bool alloc_page_stall(XPageAllocation* allocation);\n-  bool alloc_page_or_stall(XPageAllocation* allocation);\n-  bool should_defragment(const XPage* page) const;\n-  bool is_alloc_satisfied(XPageAllocation* allocation) const;\n-  XPage* alloc_page_create(XPageAllocation* allocation);\n-  XPage* alloc_page_finalize(XPageAllocation* allocation);\n-  void alloc_page_failed(XPageAllocation* allocation);\n-\n-  void satisfy_stalled();\n-\n-  void free_page_inner(XPage* page, bool reclaimed);\n-\n-  size_t uncommit(uint64_t* timeout);\n-\n-public:\n-  XPageAllocator(XWorkers* workers,\n-                 size_t min_capacity,\n-                 size_t initial_capacity,\n-                 size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  size_t min_capacity() const;\n-  size_t max_capacity() const;\n-  size_t soft_max_capacity() const;\n-  size_t capacity() const;\n-  size_t used() const;\n-  size_t unused() const;\n-\n-  XPageAllocatorStats stats() const;\n-\n-  void reset_statistics();\n-\n-  XPage* alloc_page(uint8_t type, size_t size, XAllocationFlags flags);\n-  void free_page(XPage* page, bool reclaimed);\n-  void free_pages(const XArray<XPage*>* pages, bool reclaimed);\n-\n-  void enable_deferred_delete() const;\n-  void disable_deferred_delete() const;\n-\n-  void debug_map_page(const XPage* page) const;\n-  void debug_unmap_page(const XPage* page) const;\n-\n-  bool has_alloc_stalled() const;\n-  void check_out_of_memory();\n-\n-  void pages_do(XPageClosure* cl) const;\n-\n-  void threads_do(ThreadClosure* tc) const;\n-};\n-\n-class XPageAllocatorStats {\n-private:\n-  size_t _min_capacity;\n-  size_t _max_capacity;\n-  size_t _soft_max_capacity;\n-  size_t _current_max_capacity;\n-  size_t _capacity;\n-  size_t _used;\n-  size_t _used_high;\n-  size_t _used_low;\n-  size_t _reclaimed;\n-\n-public:\n-  XPageAllocatorStats(size_t min_capacity,\n-                      size_t max_capacity,\n-                      size_t soft_max_capacity,\n-                      size_t capacity,\n-                      size_t used,\n-                      size_t used_high,\n-                      size_t used_low,\n-                      size_t reclaimed);\n-\n-  size_t min_capacity() const;\n-  size_t max_capacity() const;\n-  size_t soft_max_capacity() const;\n-  size_t capacity() const;\n-  size_t used() const;\n-  size_t used_high() const;\n-  size_t used_low() const;\n-  size_t reclaimed() const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XPAGEALLOCATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageAllocator.hpp","additions":0,"deletions":174,"binary":false,"changes":174,"status":"deleted"},{"patch":"@@ -1,78 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGEALLOCATOR_INLINE_HPP\n-#define SHARE_GC_X_XPAGEALLOCATOR_INLINE_HPP\n-\n-#include \"gc\/x\/xPageAllocator.hpp\"\n-\n-inline XPageAllocatorStats::XPageAllocatorStats(size_t min_capacity,\n-                                                size_t max_capacity,\n-                                                size_t soft_max_capacity,\n-                                                size_t capacity,\n-                                                size_t used,\n-                                                size_t used_high,\n-                                                size_t used_low,\n-                                                size_t reclaimed) :\n-    _min_capacity(min_capacity),\n-    _max_capacity(max_capacity),\n-    _soft_max_capacity(soft_max_capacity),\n-    _capacity(capacity),\n-    _used(used),\n-    _used_high(used_high),\n-    _used_low(used_low),\n-    _reclaimed(reclaimed) {}\n-\n-inline size_t XPageAllocatorStats::min_capacity() const {\n-  return _min_capacity;\n-}\n-\n-inline size_t XPageAllocatorStats::max_capacity() const {\n-  return _max_capacity;\n-}\n-\n-inline size_t XPageAllocatorStats::soft_max_capacity() const {\n-  return _soft_max_capacity;\n-}\n-\n-inline size_t XPageAllocatorStats::capacity() const {\n-  return _capacity;\n-}\n-\n-inline size_t XPageAllocatorStats::used() const {\n-  return _used;\n-}\n-\n-inline size_t XPageAllocatorStats::used_high() const {\n-  return _used_high;\n-}\n-\n-inline size_t XPageAllocatorStats::used_low() const {\n-  return _used_low;\n-}\n-\n-inline size_t XPageAllocatorStats::reclaimed() const {\n-  return _reclaimed;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XPAGEALLOCATOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageAllocator.inline.hpp","additions":0,"deletions":78,"binary":false,"changes":78,"status":"deleted"},{"patch":"@@ -1,356 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageCache.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xValue.inline.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/os.hpp\"\n-\n-static const XStatCounter XCounterPageCacheHitL1(\"Memory\", \"Page Cache Hit L1\", XStatUnitOpsPerSecond);\n-static const XStatCounter XCounterPageCacheHitL2(\"Memory\", \"Page Cache Hit L2\", XStatUnitOpsPerSecond);\n-static const XStatCounter XCounterPageCacheHitL3(\"Memory\", \"Page Cache Hit L3\", XStatUnitOpsPerSecond);\n-static const XStatCounter XCounterPageCacheMiss(\"Memory\", \"Page Cache Miss\", XStatUnitOpsPerSecond);\n-\n-class XPageCacheFlushClosure : public StackObj {\n-  friend class XPageCache;\n-\n-protected:\n-  const size_t _requested;\n-  size_t       _flushed;\n-\n-public:\n-  XPageCacheFlushClosure(size_t requested);\n-  virtual bool do_page(const XPage* page) = 0;\n-};\n-\n-XPageCacheFlushClosure::XPageCacheFlushClosure(size_t requested) :\n-    _requested(requested),\n-    _flushed(0) {}\n-\n-XPageCache::XPageCache() :\n-    _small(),\n-    _medium(),\n-    _large(),\n-    _last_commit(0) {}\n-\n-XPage* XPageCache::alloc_small_page() {\n-  const uint32_t numa_id = XNUMA::id();\n-  const uint32_t numa_count = XNUMA::count();\n-\n-  \/\/ Try NUMA local page cache\n-  XPage* const l1_page = _small.get(numa_id).remove_first();\n-  if (l1_page != nullptr) {\n-    XStatInc(XCounterPageCacheHitL1);\n-    return l1_page;\n-  }\n-\n-  \/\/ Try NUMA remote page cache(s)\n-  uint32_t remote_numa_id = numa_id + 1;\n-  const uint32_t remote_numa_count = numa_count - 1;\n-  for (uint32_t i = 0; i < remote_numa_count; i++) {\n-    if (remote_numa_id == numa_count) {\n-      remote_numa_id = 0;\n-    }\n-\n-    XPage* const l2_page = _small.get(remote_numa_id).remove_first();\n-    if (l2_page != nullptr) {\n-      XStatInc(XCounterPageCacheHitL2);\n-      return l2_page;\n-    }\n-\n-    remote_numa_id++;\n-  }\n-\n-  return nullptr;\n-}\n-\n-XPage* XPageCache::alloc_medium_page() {\n-  XPage* const page = _medium.remove_first();\n-  if (page != nullptr) {\n-    XStatInc(XCounterPageCacheHitL1);\n-    return page;\n-  }\n-\n-  return nullptr;\n-}\n-\n-XPage* XPageCache::alloc_large_page(size_t size) {\n-  \/\/ Find a page with the right size\n-  XListIterator<XPage> iter(&_large);\n-  for (XPage* page; iter.next(&page);) {\n-    if (size == page->size()) {\n-      \/\/ Page found\n-      _large.remove(page);\n-      XStatInc(XCounterPageCacheHitL1);\n-      return page;\n-    }\n-  }\n-\n-  return nullptr;\n-}\n-\n-XPage* XPageCache::alloc_oversized_medium_page(size_t size) {\n-  if (size <= XPageSizeMedium) {\n-    return _medium.remove_first();\n-  }\n-\n-  return nullptr;\n-}\n-\n-XPage* XPageCache::alloc_oversized_large_page(size_t size) {\n-  \/\/ Find a page that is large enough\n-  XListIterator<XPage> iter(&_large);\n-  for (XPage* page; iter.next(&page);) {\n-    if (size <= page->size()) {\n-      \/\/ Page found\n-      _large.remove(page);\n-      return page;\n-    }\n-  }\n-\n-  return nullptr;\n-}\n-\n-XPage* XPageCache::alloc_oversized_page(size_t size) {\n-  XPage* page = alloc_oversized_large_page(size);\n-  if (page == nullptr) {\n-    page = alloc_oversized_medium_page(size);\n-  }\n-\n-  if (page != nullptr) {\n-    XStatInc(XCounterPageCacheHitL3);\n-  }\n-\n-  return page;\n-}\n-\n-XPage* XPageCache::alloc_page(uint8_t type, size_t size) {\n-  XPage* page;\n-\n-  \/\/ Try allocate exact page\n-  if (type == XPageTypeSmall) {\n-    page = alloc_small_page();\n-  } else if (type == XPageTypeMedium) {\n-    page = alloc_medium_page();\n-  } else {\n-    page = alloc_large_page(size);\n-  }\n-\n-  if (page == nullptr) {\n-    \/\/ Try allocate potentially oversized page\n-    XPage* const oversized = alloc_oversized_page(size);\n-    if (oversized != nullptr) {\n-      if (size < oversized->size()) {\n-        \/\/ Split oversized page\n-        page = oversized->split(type, size);\n-\n-        \/\/ Cache remainder\n-        free_page(oversized);\n-      } else {\n-        \/\/ Re-type correctly sized page\n-        page = oversized->retype(type);\n-      }\n-    }\n-  }\n-\n-  if (page == nullptr) {\n-    XStatInc(XCounterPageCacheMiss);\n-  }\n-\n-  return page;\n-}\n-\n-void XPageCache::free_page(XPage* page) {\n-  const uint8_t type = page->type();\n-  if (type == XPageTypeSmall) {\n-    _small.get(page->numa_id()).insert_first(page);\n-  } else if (type == XPageTypeMedium) {\n-    _medium.insert_first(page);\n-  } else {\n-    _large.insert_first(page);\n-  }\n-}\n-\n-bool XPageCache::flush_list_inner(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to) {\n-  XPage* const page = from->last();\n-  if (page == nullptr || !cl->do_page(page)) {\n-    \/\/ Don't flush page\n-    return false;\n-  }\n-\n-  \/\/ Flush page\n-  from->remove(page);\n-  to->insert_last(page);\n-  return true;\n-}\n-\n-void XPageCache::flush_list(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to) {\n-  while (flush_list_inner(cl, from, to));\n-}\n-\n-void XPageCache::flush_per_numa_lists(XPageCacheFlushClosure* cl, XPerNUMA<XList<XPage> >* from, XList<XPage>* to) {\n-  const uint32_t numa_count = XNUMA::count();\n-  uint32_t numa_done = 0;\n-  uint32_t numa_next = 0;\n-\n-  \/\/ Flush lists round-robin\n-  while (numa_done < numa_count) {\n-    XList<XPage>* numa_list = from->addr(numa_next);\n-    if (++numa_next == numa_count) {\n-      numa_next = 0;\n-    }\n-\n-    if (flush_list_inner(cl, numa_list, to)) {\n-      \/\/ Not done\n-      numa_done = 0;\n-    } else {\n-      \/\/ Done\n-      numa_done++;\n-    }\n-  }\n-}\n-\n-void XPageCache::flush(XPageCacheFlushClosure* cl, XList<XPage>* to) {\n-  \/\/ Prefer flushing large, then medium and last small pages\n-  flush_list(cl, &_large, to);\n-  flush_list(cl, &_medium, to);\n-  flush_per_numa_lists(cl, &_small, to);\n-\n-  if (cl->_flushed > cl->_requested) {\n-    \/\/ Overflushed, re-insert part of last page into the cache\n-    const size_t overflushed = cl->_flushed - cl->_requested;\n-    XPage* const reinsert = to->last()->split(overflushed);\n-    free_page(reinsert);\n-    cl->_flushed -= overflushed;\n-  }\n-}\n-\n-class XPageCacheFlushForAllocationClosure : public XPageCacheFlushClosure {\n-public:\n-  XPageCacheFlushForAllocationClosure(size_t requested) :\n-      XPageCacheFlushClosure(requested) {}\n-\n-  virtual bool do_page(const XPage* page) {\n-    if (_flushed < _requested) {\n-      \/\/ Flush page\n-      _flushed += page->size();\n-      return true;\n-    }\n-\n-    \/\/ Don't flush page\n-    return false;\n-  }\n-};\n-\n-void XPageCache::flush_for_allocation(size_t requested, XList<XPage>* to) {\n-  XPageCacheFlushForAllocationClosure cl(requested);\n-  flush(&cl, to);\n-}\n-\n-class XPageCacheFlushForUncommitClosure : public XPageCacheFlushClosure {\n-private:\n-  const uint64_t _now;\n-  uint64_t*      _timeout;\n-\n-public:\n-  XPageCacheFlushForUncommitClosure(size_t requested, uint64_t now, uint64_t* timeout) :\n-      XPageCacheFlushClosure(requested),\n-      _now(now),\n-      _timeout(timeout) {\n-    \/\/ Set initial timeout\n-    *_timeout = ZUncommitDelay;\n-  }\n-\n-  virtual bool do_page(const XPage* page) {\n-    const uint64_t expires = page->last_used() + ZUncommitDelay;\n-    if (expires > _now) {\n-      \/\/ Don't flush page, record shortest non-expired timeout\n-      *_timeout = MIN2(*_timeout, expires - _now);\n-      return false;\n-    }\n-\n-    if (_flushed >= _requested) {\n-      \/\/ Don't flush page, requested amount flushed\n-      return false;\n-    }\n-\n-    \/\/ Flush page\n-    _flushed += page->size();\n-    return true;\n-  }\n-};\n-\n-size_t XPageCache::flush_for_uncommit(size_t requested, XList<XPage>* to, uint64_t* timeout) {\n-  const uint64_t now = os::elapsedTime();\n-  const uint64_t expires = _last_commit + ZUncommitDelay;\n-  if (expires > now) {\n-    \/\/ Delay uncommit, set next timeout\n-    *timeout = expires - now;\n-    return 0;\n-  }\n-\n-  if (requested == 0) {\n-    \/\/ Nothing to flush, set next timeout\n-    *timeout = ZUncommitDelay;\n-    return 0;\n-  }\n-\n-  XPageCacheFlushForUncommitClosure cl(requested, now, timeout);\n-  flush(&cl, to);\n-\n-  return cl._flushed;\n-}\n-\n-void XPageCache::set_last_commit() {\n-  _last_commit = ceil(os::elapsedTime());\n-}\n-\n-void XPageCache::pages_do(XPageClosure* cl) const {\n-  \/\/ Small\n-  XPerNUMAConstIterator<XList<XPage> > iter_numa(&_small);\n-  for (const XList<XPage>* list; iter_numa.next(&list);) {\n-    XListIterator<XPage> iter_small(list);\n-    for (XPage* page; iter_small.next(&page);) {\n-      cl->do_page(page);\n-    }\n-  }\n-\n-  \/\/ Medium\n-  XListIterator<XPage> iter_medium(&_medium);\n-  for (XPage* page; iter_medium.next(&page);) {\n-    cl->do_page(page);\n-  }\n-\n-  \/\/ Large\n-  XListIterator<XPage> iter_large(&_large);\n-  for (XPage* page; iter_large.next(&page);) {\n-    cl->do_page(page);\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xPageCache.cpp","additions":0,"deletions":356,"binary":false,"changes":356,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGECACHE_HPP\n-#define SHARE_GC_X_XPAGECACHE_HPP\n-\n-#include \"gc\/x\/xList.hpp\"\n-#include \"gc\/x\/xPage.hpp\"\n-#include \"gc\/x\/xValue.hpp\"\n-\n-class XPageCacheFlushClosure;\n-\n-class XPageCache {\n-private:\n-  XPerNUMA<XList<XPage> > _small;\n-  XList<XPage>            _medium;\n-  XList<XPage>            _large;\n-  uint64_t                _last_commit;\n-\n-  XPage* alloc_small_page();\n-  XPage* alloc_medium_page();\n-  XPage* alloc_large_page(size_t size);\n-\n-  XPage* alloc_oversized_medium_page(size_t size);\n-  XPage* alloc_oversized_large_page(size_t size);\n-  XPage* alloc_oversized_page(size_t size);\n-\n-  bool flush_list_inner(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to);\n-  void flush_list(XPageCacheFlushClosure* cl, XList<XPage>* from, XList<XPage>* to);\n-  void flush_per_numa_lists(XPageCacheFlushClosure* cl, XPerNUMA<XList<XPage> >* from, XList<XPage>* to);\n-  void flush(XPageCacheFlushClosure* cl, XList<XPage>* to);\n-\n-public:\n-  XPageCache();\n-\n-  XPage* alloc_page(uint8_t type, size_t size);\n-  void free_page(XPage* page);\n-\n-  void flush_for_allocation(size_t requested, XList<XPage>* to);\n-  size_t flush_for_uncommit(size_t requested, XList<XPage>* to, uint64_t* timeout);\n-\n-  void set_last_commit();\n-\n-  void pages_do(XPageClosure* cl) const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XPAGECACHE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageCache.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,53 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xGranuleMap.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageTable.inline.hpp\"\n-#include \"runtime\/orderAccess.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-XPageTable::XPageTable() :\n-    _map(XAddressOffsetMax) {}\n-\n-void XPageTable::insert(XPage* page) {\n-  const uintptr_t offset = page->start();\n-  const size_t size = page->size();\n-\n-  \/\/ Make sure a newly created page is\n-  \/\/ visible before updating the page table.\n-  OrderAccess::storestore();\n-\n-  assert(_map.get(offset) == nullptr, \"Invalid entry\");\n-  _map.put(offset, size, page);\n-}\n-\n-void XPageTable::remove(XPage* page) {\n-  const uintptr_t offset = page->start();\n-  const size_t size = page->size();\n-\n-  assert(_map.get(offset) == page, \"Invalid entry\");\n-  _map.put(offset, size, nullptr);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xPageTable.cpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"deleted"},{"patch":"@@ -1,61 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGETABLE_HPP\n-#define SHARE_GC_X_XPAGETABLE_HPP\n-\n-#include \"gc\/x\/xGranuleMap.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class VMStructs;\n-class XPage;\n-class XPageTableIterator;\n-\n-class XPageTable {\n-  friend class ::VMStructs;\n-  friend class XPageTableIterator;\n-\n-private:\n-  XGranuleMap<XPage*> _map;\n-\n-public:\n-  XPageTable();\n-\n-  XPage* get(uintptr_t addr) const;\n-\n-  void insert(XPage* page);\n-  void remove(XPage* page);\n-};\n-\n-class XPageTableIterator : public StackObj {\n-private:\n-  XGranuleMapIterator<XPage*> _iter;\n-  XPage*                      _prev;\n-\n-public:\n-  XPageTableIterator(const XPageTable* page_table);\n-\n-  bool next(XPage** page);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XPAGETABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageTable.hpp","additions":0,"deletions":61,"binary":false,"changes":61,"status":"deleted"},{"patch":"@@ -1,54 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPAGETABLE_INLINE_HPP\n-#define SHARE_GC_X_XPAGETABLE_INLINE_HPP\n-\n-#include \"gc\/x\/xPageTable.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xGranuleMap.inline.hpp\"\n-\n-inline XPage* XPageTable::get(uintptr_t addr) const {\n-  assert(!XAddress::is_null(addr), \"Invalid address\");\n-  return _map.get(XAddress::offset(addr));\n-}\n-\n-inline XPageTableIterator::XPageTableIterator(const XPageTable* page_table) :\n-    _iter(&page_table->_map),\n-    _prev(nullptr) {}\n-\n-inline bool XPageTableIterator::next(XPage** page) {\n-  for (XPage* entry; _iter.next(&entry);) {\n-    if (entry != nullptr && entry != _prev) {\n-      \/\/ Next page found\n-      *page = _prev = entry;\n-      return true;\n-    }\n-  }\n-\n-  \/\/ No more pages\n-  return false;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XPAGETABLE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPageTable.inline.hpp","additions":0,"deletions":54,"binary":false,"changes":54,"status":"deleted"},{"patch":"@@ -1,434 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xLargePages.inline.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"gc\/x\/xNUMA.inline.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/init.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-XPhysicalMemory::XPhysicalMemory() :\n-    _segments() {}\n-\n-XPhysicalMemory::XPhysicalMemory(const XPhysicalMemorySegment& segment) :\n-    _segments() {\n-  add_segment(segment);\n-}\n-\n-XPhysicalMemory::XPhysicalMemory(const XPhysicalMemory& pmem) :\n-    _segments() {\n-  add_segments(pmem);\n-}\n-\n-const XPhysicalMemory& XPhysicalMemory::operator=(const XPhysicalMemory& pmem) {\n-  \/\/ Free segments\n-  _segments.clear_and_deallocate();\n-\n-  \/\/ Copy segments\n-  add_segments(pmem);\n-\n-  return *this;\n-}\n-\n-size_t XPhysicalMemory::size() const {\n-  size_t size = 0;\n-\n-  for (int i = 0; i < _segments.length(); i++) {\n-    size += _segments.at(i).size();\n-  }\n-\n-  return size;\n-}\n-\n-void XPhysicalMemory::insert_segment(int index, uintptr_t start, size_t size, bool committed) {\n-  _segments.insert_before(index, XPhysicalMemorySegment(start, size, committed));\n-}\n-\n-void XPhysicalMemory::replace_segment(int index, uintptr_t start, size_t size, bool committed) {\n-  _segments.at_put(index, XPhysicalMemorySegment(start, size, committed));\n-}\n-\n-void XPhysicalMemory::remove_segment(int index) {\n-  _segments.remove_at(index);\n-}\n-\n-void XPhysicalMemory::add_segments(const XPhysicalMemory& pmem) {\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    add_segment(pmem.segment(i));\n-  }\n-}\n-\n-void XPhysicalMemory::remove_segments() {\n-  _segments.clear_and_deallocate();\n-}\n-\n-static bool is_mergable(const XPhysicalMemorySegment& before, const XPhysicalMemorySegment& after) {\n-  return before.end() == after.start() && before.is_committed() == after.is_committed();\n-}\n-\n-void XPhysicalMemory::add_segment(const XPhysicalMemorySegment& segment) {\n-  \/\/ Insert segments in address order, merge segments when possible\n-  for (int i = _segments.length(); i > 0; i--) {\n-    const int current = i - 1;\n-\n-    if (_segments.at(current).end() <= segment.start()) {\n-      if (is_mergable(_segments.at(current), segment)) {\n-        if (current + 1 < _segments.length() && is_mergable(segment, _segments.at(current + 1))) {\n-          \/\/ Merge with end of current segment and start of next segment\n-          const size_t start = _segments.at(current).start();\n-          const size_t size = _segments.at(current).size() + segment.size() + _segments.at(current + 1).size();\n-          replace_segment(current, start, size, segment.is_committed());\n-          remove_segment(current + 1);\n-          return;\n-        }\n-\n-        \/\/ Merge with end of current segment\n-        const size_t start = _segments.at(current).start();\n-        const size_t size = _segments.at(current).size() + segment.size();\n-        replace_segment(current, start, size, segment.is_committed());\n-        return;\n-      } else if (current + 1 < _segments.length() && is_mergable(segment, _segments.at(current + 1))) {\n-        \/\/ Merge with start of next segment\n-        const size_t start = segment.start();\n-        const size_t size = segment.size() + _segments.at(current + 1).size();\n-        replace_segment(current + 1, start, size, segment.is_committed());\n-        return;\n-      }\n-\n-      \/\/ Insert after current segment\n-      insert_segment(current + 1, segment.start(), segment.size(), segment.is_committed());\n-      return;\n-    }\n-  }\n-\n-  if (_segments.length() > 0 && is_mergable(segment, _segments.at(0))) {\n-    \/\/ Merge with start of first segment\n-    const size_t start = segment.start();\n-    const size_t size = segment.size() + _segments.at(0).size();\n-    replace_segment(0, start, size, segment.is_committed());\n-    return;\n-  }\n-\n-  \/\/ Insert before first segment\n-  insert_segment(0, segment.start(), segment.size(), segment.is_committed());\n-}\n-\n-bool XPhysicalMemory::commit_segment(int index, size_t size) {\n-  assert(size <= _segments.at(index).size(), \"Invalid size\");\n-  assert(!_segments.at(index).is_committed(), \"Invalid state\");\n-\n-  if (size == _segments.at(index).size()) {\n-    \/\/ Completely committed\n-    _segments.at(index).set_committed(true);\n-    return true;\n-  }\n-\n-  if (size > 0) {\n-    \/\/ Partially committed, split segment\n-    insert_segment(index + 1, _segments.at(index).start() + size, _segments.at(index).size() - size, false \/* committed *\/);\n-    replace_segment(index, _segments.at(index).start(), size, true \/* committed *\/);\n-  }\n-\n-  return false;\n-}\n-\n-bool XPhysicalMemory::uncommit_segment(int index, size_t size) {\n-  assert(size <= _segments.at(index).size(), \"Invalid size\");\n-  assert(_segments.at(index).is_committed(), \"Invalid state\");\n-\n-  if (size == _segments.at(index).size()) {\n-    \/\/ Completely uncommitted\n-    _segments.at(index).set_committed(false);\n-    return true;\n-  }\n-\n-  if (size > 0) {\n-    \/\/ Partially uncommitted, split segment\n-    insert_segment(index + 1, _segments.at(index).start() + size, _segments.at(index).size() - size, true \/* committed *\/);\n-    replace_segment(index, _segments.at(index).start(), size, false \/* committed *\/);\n-  }\n-\n-  return false;\n-}\n-\n-XPhysicalMemory XPhysicalMemory::split(size_t size) {\n-  XPhysicalMemory pmem;\n-  int nsegments = 0;\n-\n-  for (int i = 0; i < _segments.length(); i++) {\n-    const XPhysicalMemorySegment& segment = _segments.at(i);\n-    if (pmem.size() < size) {\n-      if (pmem.size() + segment.size() <= size) {\n-        \/\/ Transfer segment\n-        pmem.add_segment(segment);\n-      } else {\n-        \/\/ Split segment\n-        const size_t split_size = size - pmem.size();\n-        pmem.add_segment(XPhysicalMemorySegment(segment.start(), split_size, segment.is_committed()));\n-        _segments.at_put(nsegments++, XPhysicalMemorySegment(segment.start() + split_size, segment.size() - split_size, segment.is_committed()));\n-      }\n-    } else {\n-      \/\/ Keep segment\n-      _segments.at_put(nsegments++, segment);\n-    }\n-  }\n-\n-  _segments.trunc_to(nsegments);\n-\n-  return pmem;\n-}\n-\n-XPhysicalMemory XPhysicalMemory::split_committed() {\n-  XPhysicalMemory pmem;\n-  int nsegments = 0;\n-\n-  for (int i = 0; i < _segments.length(); i++) {\n-    const XPhysicalMemorySegment& segment = _segments.at(i);\n-    if (segment.is_committed()) {\n-      \/\/ Transfer segment\n-      pmem.add_segment(segment);\n-    } else {\n-      \/\/ Keep segment\n-      _segments.at_put(nsegments++, segment);\n-    }\n-  }\n-\n-  _segments.trunc_to(nsegments);\n-\n-  return pmem;\n-}\n-\n-XPhysicalMemoryManager::XPhysicalMemoryManager(size_t max_capacity) :\n-    _backing(max_capacity) {\n-  \/\/ Make the whole range free\n-  _manager.free(0, max_capacity);\n-}\n-\n-bool XPhysicalMemoryManager::is_initialized() const {\n-  return _backing.is_initialized();\n-}\n-\n-void XPhysicalMemoryManager::warn_commit_limits(size_t max_capacity) const {\n-  _backing.warn_commit_limits(max_capacity);\n-}\n-\n-void XPhysicalMemoryManager::try_enable_uncommit(size_t min_capacity, size_t max_capacity) {\n-  assert(!is_init_completed(), \"Invalid state\");\n-\n-  \/\/ If uncommit is not explicitly disabled, max capacity is greater than\n-  \/\/ min capacity, and uncommit is supported by the platform, then uncommit\n-  \/\/ will be enabled.\n-  if (!ZUncommit) {\n-    log_info_p(gc, init)(\"Uncommit: Disabled\");\n-    return;\n-  }\n-\n-  if (max_capacity == min_capacity) {\n-    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (-Xms equals -Xmx)\");\n-    FLAG_SET_ERGO(ZUncommit, false);\n-    return;\n-  }\n-\n-  \/\/ Test if uncommit is supported by the operating system by committing\n-  \/\/ and then uncommitting a granule.\n-  XPhysicalMemory pmem(XPhysicalMemorySegment(0, XGranuleSize, false \/* committed *\/));\n-  if (!commit(pmem) || !uncommit(pmem)) {\n-    log_info_p(gc, init)(\"Uncommit: Implicitly Disabled (Not supported by operating system)\");\n-    FLAG_SET_ERGO(ZUncommit, false);\n-    return;\n-  }\n-\n-  log_info_p(gc, init)(\"Uncommit: Enabled\");\n-  log_info_p(gc, init)(\"Uncommit Delay: \" UINTX_FORMAT \"s\", ZUncommitDelay);\n-}\n-\n-void XPhysicalMemoryManager::nmt_commit(uintptr_t offset, size_t size) const {\n-  \/\/ From an NMT point of view we treat the first heap view (marked0) as committed\n-  const uintptr_t addr = XAddress::marked0(offset);\n-  MemTracker::record_virtual_memory_commit((void*)addr, size, CALLER_PC);\n-}\n-\n-void XPhysicalMemoryManager::nmt_uncommit(uintptr_t offset, size_t size) const {\n-  const uintptr_t addr = XAddress::marked0(offset);\n-  ThreadCritical tc;\n-  MemTracker::record_virtual_memory_uncommit((address)addr, size);\n-}\n-\n-void XPhysicalMemoryManager::alloc(XPhysicalMemory& pmem, size_t size) {\n-  assert(is_aligned(size, XGranuleSize), \"Invalid size\");\n-\n-  \/\/ Allocate segments\n-  while (size > 0) {\n-    size_t allocated = 0;\n-    const uintptr_t start = _manager.alloc_low_address_at_most(size, &allocated);\n-    assert(start != UINTPTR_MAX, \"Allocation should never fail\");\n-    pmem.add_segment(XPhysicalMemorySegment(start, allocated, false \/* committed *\/));\n-    size -= allocated;\n-  }\n-}\n-\n-void XPhysicalMemoryManager::free(const XPhysicalMemory& pmem) {\n-  \/\/ Free segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const XPhysicalMemorySegment& segment = pmem.segment(i);\n-    _manager.free(segment.start(), segment.size());\n-  }\n-}\n-\n-bool XPhysicalMemoryManager::commit(XPhysicalMemory& pmem) {\n-  \/\/ Commit segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const XPhysicalMemorySegment& segment = pmem.segment(i);\n-    if (segment.is_committed()) {\n-      \/\/ Segment already committed\n-      continue;\n-    }\n-\n-    \/\/ Commit segment\n-    const size_t committed = _backing.commit(segment.start(), segment.size());\n-    if (!pmem.commit_segment(i, committed)) {\n-      \/\/ Failed or partially failed\n-      return false;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-bool XPhysicalMemoryManager::uncommit(XPhysicalMemory& pmem) {\n-  \/\/ Commit segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const XPhysicalMemorySegment& segment = pmem.segment(i);\n-    if (!segment.is_committed()) {\n-      \/\/ Segment already uncommitted\n-      continue;\n-    }\n-\n-    \/\/ Uncommit segment\n-    const size_t uncommitted = _backing.uncommit(segment.start(), segment.size());\n-    if (!pmem.uncommit_segment(i, uncommitted)) {\n-      \/\/ Failed or partially failed\n-      return false;\n-    }\n-  }\n-\n-  \/\/ Success\n-  return true;\n-}\n-\n-void XPhysicalMemoryManager::pretouch_view(uintptr_t addr, size_t size) const {\n-  const size_t page_size = XLargePages::is_explicit() ? XGranuleSize : os::vm_page_size();\n-  os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n-}\n-\n-void XPhysicalMemoryManager::map_view(uintptr_t addr, const XPhysicalMemory& pmem) const {\n-  size_t size = 0;\n-\n-  \/\/ Map segments\n-  for (int i = 0; i < pmem.nsegments(); i++) {\n-    const XPhysicalMemorySegment& segment = pmem.segment(i);\n-    _backing.map(addr + size, segment.size(), segment.start());\n-    size += segment.size();\n-  }\n-\n-  \/\/ Setup NUMA interleaving for large pages\n-  if (XNUMA::is_enabled() && XLargePages::is_explicit()) {\n-    \/\/ To get granule-level NUMA interleaving when using large pages,\n-    \/\/ we simply let the kernel interleave the memory for us at page\n-    \/\/ fault time.\n-    os::numa_make_global((char*)addr, size);\n-  }\n-}\n-\n-void XPhysicalMemoryManager::unmap_view(uintptr_t addr, size_t size) const {\n-  _backing.unmap(addr, size);\n-}\n-\n-void XPhysicalMemoryManager::pretouch(uintptr_t offset, size_t size) const {\n-  if (ZVerifyViews) {\n-    \/\/ Pre-touch good view\n-    pretouch_view(XAddress::good(offset), size);\n-  } else {\n-    \/\/ Pre-touch all views\n-    pretouch_view(XAddress::marked0(offset), size);\n-    pretouch_view(XAddress::marked1(offset), size);\n-    pretouch_view(XAddress::remapped(offset), size);\n-  }\n-}\n-\n-void XPhysicalMemoryManager::map(uintptr_t offset, const XPhysicalMemory& pmem) const {\n-  const size_t size = pmem.size();\n-\n-  if (ZVerifyViews) {\n-    \/\/ Map good view\n-    map_view(XAddress::good(offset), pmem);\n-  } else {\n-    \/\/ Map all views\n-    map_view(XAddress::marked0(offset), pmem);\n-    map_view(XAddress::marked1(offset), pmem);\n-    map_view(XAddress::remapped(offset), pmem);\n-  }\n-\n-  nmt_commit(offset, size);\n-}\n-\n-void XPhysicalMemoryManager::unmap(uintptr_t offset, size_t size) const {\n-  nmt_uncommit(offset, size);\n-\n-  if (ZVerifyViews) {\n-    \/\/ Unmap good view\n-    unmap_view(XAddress::good(offset), size);\n-  } else {\n-    \/\/ Unmap all views\n-    unmap_view(XAddress::marked0(offset), size);\n-    unmap_view(XAddress::marked1(offset), size);\n-    unmap_view(XAddress::remapped(offset), size);\n-  }\n-}\n-\n-void XPhysicalMemoryManager::debug_map(uintptr_t offset, const XPhysicalMemory& pmem) const {\n-  \/\/ Map good view\n-  assert(ZVerifyViews, \"Should be enabled\");\n-  map_view(XAddress::good(offset), pmem);\n-}\n-\n-void XPhysicalMemoryManager::debug_unmap(uintptr_t offset, size_t size) const {\n-  \/\/ Unmap good view\n-  assert(ZVerifyViews, \"Should be enabled\");\n-  unmap_view(XAddress::good(offset), size);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xPhysicalMemory.cpp","additions":0,"deletions":434,"binary":false,"changes":434,"status":"deleted"},{"patch":"@@ -1,116 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPHYSICALMEMORY_HPP\n-#define SHARE_GC_X_XPHYSICALMEMORY_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"gc\/x\/xMemory.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include OS_HEADER(gc\/x\/xPhysicalMemoryBacking)\n-\n-class XPhysicalMemorySegment : public CHeapObj<mtGC> {\n-private:\n-  uintptr_t _start;\n-  uintptr_t _end;\n-  bool      _committed;\n-\n-public:\n-  XPhysicalMemorySegment();\n-  XPhysicalMemorySegment(uintptr_t start, size_t size, bool committed);\n-\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n-  size_t size() const;\n-\n-  bool is_committed() const;\n-  void set_committed(bool committed);\n-};\n-\n-class XPhysicalMemory {\n-private:\n-  XArray<XPhysicalMemorySegment> _segments;\n-\n-  void insert_segment(int index, uintptr_t start, size_t size, bool committed);\n-  void replace_segment(int index, uintptr_t start, size_t size, bool committed);\n-  void remove_segment(int index);\n-\n-public:\n-  XPhysicalMemory();\n-  XPhysicalMemory(const XPhysicalMemorySegment& segment);\n-  XPhysicalMemory(const XPhysicalMemory& pmem);\n-  const XPhysicalMemory& operator=(const XPhysicalMemory& pmem);\n-\n-  bool is_null() const;\n-  size_t size() const;\n-\n-  int nsegments() const;\n-  const XPhysicalMemorySegment& segment(int index) const;\n-\n-  void add_segments(const XPhysicalMemory& pmem);\n-  void remove_segments();\n-\n-  void add_segment(const XPhysicalMemorySegment& segment);\n-  bool commit_segment(int index, size_t size);\n-  bool uncommit_segment(int index, size_t size);\n-\n-  XPhysicalMemory split(size_t size);\n-  XPhysicalMemory split_committed();\n-};\n-\n-class XPhysicalMemoryManager {\n-private:\n-  XPhysicalMemoryBacking _backing;\n-  XMemoryManager         _manager;\n-\n-  void nmt_commit(uintptr_t offset, size_t size) const;\n-  void nmt_uncommit(uintptr_t offset, size_t size) const;\n-\n-  void pretouch_view(uintptr_t addr, size_t size) const;\n-  void map_view(uintptr_t addr, const XPhysicalMemory& pmem) const;\n-  void unmap_view(uintptr_t addr, size_t size) const;\n-\n-public:\n-  XPhysicalMemoryManager(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  void warn_commit_limits(size_t max_capacity) const;\n-  void try_enable_uncommit(size_t min_capacity, size_t max_capacity);\n-\n-  void alloc(XPhysicalMemory& pmem, size_t size);\n-  void free(const XPhysicalMemory& pmem);\n-\n-  bool commit(XPhysicalMemory& pmem);\n-  bool uncommit(XPhysicalMemory& pmem);\n-\n-  void pretouch(uintptr_t offset, size_t size) const;\n-\n-  void map(uintptr_t offset, const XPhysicalMemory& pmem) const;\n-  void unmap(uintptr_t offset, size_t size) const;\n-\n-  void debug_map(uintptr_t offset, const XPhysicalMemory& pmem) const;\n-  void debug_unmap(uintptr_t offset, size_t size) const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XPHYSICALMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPhysicalMemory.hpp","additions":0,"deletions":116,"binary":false,"changes":116,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XPHYSICALMEMORY_INLINE_HPP\n-#define SHARE_GC_X_XPHYSICALMEMORY_INLINE_HPP\n-\n-#include \"gc\/x\/xPhysicalMemory.hpp\"\n-\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-inline XPhysicalMemorySegment::XPhysicalMemorySegment() :\n-    _start(UINTPTR_MAX),\n-    _end(UINTPTR_MAX),\n-    _committed(false) {}\n-\n-inline XPhysicalMemorySegment::XPhysicalMemorySegment(uintptr_t start, size_t size, bool committed) :\n-    _start(start),\n-    _end(start + size),\n-    _committed(committed) {}\n-\n-inline uintptr_t XPhysicalMemorySegment::start() const {\n-  return _start;\n-}\n-\n-inline uintptr_t XPhysicalMemorySegment::end() const {\n-  return _end;\n-}\n-\n-inline size_t XPhysicalMemorySegment::size() const {\n-  return _end - _start;\n-}\n-\n-inline bool XPhysicalMemorySegment::is_committed() const {\n-  return _committed;\n-}\n-\n-inline void XPhysicalMemorySegment::set_committed(bool committed) {\n-  _committed = committed;\n-}\n-\n-inline bool XPhysicalMemory::is_null() const {\n-  return _segments.length() == 0;\n-}\n-\n-inline int XPhysicalMemory::nsegments() const {\n-  return _segments.length();\n-}\n-\n-inline const XPhysicalMemorySegment& XPhysicalMemory::segment(int index) const {\n-  return _segments.at(index);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XPHYSICALMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xPhysicalMemory.inline.hpp","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -1,459 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/javaClasses.inline.hpp\"\n-#include \"gc\/shared\/referencePolicy.hpp\"\n-#include \"gc\/shared\/referenceProcessorStats.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xReferenceProcessor.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xTracer.inline.hpp\"\n-#include \"gc\/x\/xValue.inline.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/os.hpp\"\n-\n-static const XStatSubPhase XSubPhaseConcurrentReferencesProcess(\"Concurrent References Process\");\n-static const XStatSubPhase XSubPhaseConcurrentReferencesEnqueue(\"Concurrent References Enqueue\");\n-\n-static ReferenceType reference_type(oop reference) {\n-  return InstanceKlass::cast(reference->klass())->reference_type();\n-}\n-\n-static const char* reference_type_name(ReferenceType type) {\n-  switch (type) {\n-  case REF_SOFT:\n-    return \"Soft\";\n-\n-  case REF_WEAK:\n-    return \"Weak\";\n-\n-  case REF_FINAL:\n-    return \"Final\";\n-\n-  case REF_PHANTOM:\n-    return \"Phantom\";\n-\n-  default:\n-    ShouldNotReachHere();\n-    return \"Unknown\";\n-  }\n-}\n-\n-static volatile oop* reference_referent_addr(oop reference) {\n-  return (volatile oop*)java_lang_ref_Reference::referent_addr_raw(reference);\n-}\n-\n-static oop reference_referent(oop reference) {\n-  return Atomic::load(reference_referent_addr(reference));\n-}\n-\n-static void reference_clear_referent(oop reference) {\n-  java_lang_ref_Reference::clear_referent_raw(reference);\n-}\n-\n-static oop* reference_discovered_addr(oop reference) {\n-  return (oop*)java_lang_ref_Reference::discovered_addr_raw(reference);\n-}\n-\n-static oop reference_discovered(oop reference) {\n-  return *reference_discovered_addr(reference);\n-}\n-\n-static void reference_set_discovered(oop reference, oop discovered) {\n-  java_lang_ref_Reference::set_discovered_raw(reference, discovered);\n-}\n-\n-static oop* reference_next_addr(oop reference) {\n-  return (oop*)java_lang_ref_Reference::next_addr_raw(reference);\n-}\n-\n-static oop reference_next(oop reference) {\n-  return *reference_next_addr(reference);\n-}\n-\n-static void reference_set_next(oop reference, oop next) {\n-  java_lang_ref_Reference::set_next_raw(reference, next);\n-}\n-\n-static void soft_reference_update_clock() {\n-  const jlong now = os::javaTimeNanos() \/ NANOSECS_PER_MILLISEC;\n-  java_lang_ref_SoftReference::set_clock(now);\n-}\n-\n-XReferenceProcessor::XReferenceProcessor(XWorkers* workers) :\n-    _workers(workers),\n-    _soft_reference_policy(nullptr),\n-    _encountered_count(),\n-    _discovered_count(),\n-    _enqueued_count(),\n-    _discovered_list(nullptr),\n-    _pending_list(nullptr),\n-    _pending_list_tail(_pending_list.addr()) {}\n-\n-void XReferenceProcessor::set_soft_reference_policy(bool clear) {\n-  static AlwaysClearPolicy always_clear_policy;\n-  static LRUMaxHeapPolicy lru_max_heap_policy;\n-\n-  if (clear) {\n-    log_info(gc, ref)(\"Clearing All SoftReferences\");\n-    _soft_reference_policy = &always_clear_policy;\n-  } else {\n-    _soft_reference_policy = &lru_max_heap_policy;\n-  }\n-\n-  _soft_reference_policy->setup();\n-}\n-\n-bool XReferenceProcessor::is_inactive(oop reference, oop referent, ReferenceType type) const {\n-  if (type == REF_FINAL) {\n-    \/\/ A FinalReference is inactive if its next field is non-null. An application can't\n-    \/\/ call enqueue() or clear() on a FinalReference.\n-    return reference_next(reference) != nullptr;\n-  } else {\n-    \/\/ A non-FinalReference is inactive if the referent is null. The referent can only\n-    \/\/ be null if the application called Reference.enqueue() or Reference.clear().\n-    return referent == nullptr;\n-  }\n-}\n-\n-bool XReferenceProcessor::is_strongly_live(oop referent) const {\n-  return XHeap::heap()->is_object_strongly_live(XOop::to_address(referent));\n-}\n-\n-bool XReferenceProcessor::is_softly_live(oop reference, ReferenceType type) const {\n-  if (type != REF_SOFT) {\n-    \/\/ Not a SoftReference\n-    return false;\n-  }\n-\n-  \/\/ Ask SoftReference policy\n-  const jlong clock = java_lang_ref_SoftReference::clock();\n-  assert(clock != 0, \"Clock not initialized\");\n-  assert(_soft_reference_policy != nullptr, \"Policy not initialized\");\n-  return !_soft_reference_policy->should_clear_reference(reference, clock);\n-}\n-\n-bool XReferenceProcessor::should_discover(oop reference, ReferenceType type) const {\n-  volatile oop* const referent_addr = reference_referent_addr(reference);\n-  const oop referent = XBarrier::weak_load_barrier_on_oop_field(referent_addr);\n-\n-  if (is_inactive(reference, referent, type)) {\n-    return false;\n-  }\n-\n-  if (is_strongly_live(referent)) {\n-    return false;\n-  }\n-\n-  if (is_softly_live(reference, type)) {\n-    return false;\n-  }\n-\n-  \/\/ PhantomReferences with finalizable marked referents should technically not have\n-  \/\/ to be discovered. However, InstanceRefKlass::oop_oop_iterate_ref_processing()\n-  \/\/ does not know about the finalizable mark concept, and will therefore mark\n-  \/\/ referents in non-discovered PhantomReferences as strongly live. To prevent\n-  \/\/ this, we always discover PhantomReferences with finalizable marked referents.\n-  \/\/ They will automatically be dropped during the reference processing phase.\n-  return true;\n-}\n-\n-bool XReferenceProcessor::should_drop(oop reference, ReferenceType type) const {\n-  const oop referent = reference_referent(reference);\n-  if (referent == nullptr) {\n-    \/\/ Reference has been cleared, by a call to Reference.enqueue()\n-    \/\/ or Reference.clear() from the application, which means we\n-    \/\/ should drop the reference.\n-    return true;\n-  }\n-\n-  \/\/ Check if the referent is still alive, in which case we should\n-  \/\/ drop the reference.\n-  if (type == REF_PHANTOM) {\n-    return XBarrier::is_alive_barrier_on_phantom_oop(referent);\n-  } else {\n-    return XBarrier::is_alive_barrier_on_weak_oop(referent);\n-  }\n-}\n-\n-void XReferenceProcessor::keep_alive(oop reference, ReferenceType type) const {\n-  volatile oop* const p = reference_referent_addr(reference);\n-  if (type == REF_PHANTOM) {\n-    XBarrier::keep_alive_barrier_on_phantom_oop_field(p);\n-  } else {\n-    XBarrier::keep_alive_barrier_on_weak_oop_field(p);\n-  }\n-}\n-\n-void XReferenceProcessor::make_inactive(oop reference, ReferenceType type) const {\n-  if (type == REF_FINAL) {\n-    \/\/ Don't clear referent. It is needed by the Finalizer thread to make the call\n-    \/\/ to finalize(). A FinalReference is instead made inactive by self-looping the\n-    \/\/ next field. An application can't call FinalReference.enqueue(), so there is\n-    \/\/ no race to worry about when setting the next field.\n-    assert(reference_next(reference) == nullptr, \"Already inactive\");\n-    reference_set_next(reference, reference);\n-  } else {\n-    \/\/ Clear referent\n-    reference_clear_referent(reference);\n-  }\n-}\n-\n-void XReferenceProcessor::discover(oop reference, ReferenceType type) {\n-  log_trace(gc, ref)(\"Discovered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n-\n-  \/\/ Update statistics\n-  _discovered_count.get()[type]++;\n-\n-  if (type == REF_FINAL) {\n-    \/\/ Mark referent (and its reachable subgraph) finalizable. This avoids\n-    \/\/ the problem of later having to mark those objects if the referent is\n-    \/\/ still final reachable during processing.\n-    volatile oop* const referent_addr = reference_referent_addr(reference);\n-    XBarrier::mark_barrier_on_oop_field(referent_addr, true \/* finalizable *\/);\n-  }\n-\n-  \/\/ Add reference to discovered list\n-  assert(reference_discovered(reference) == nullptr, \"Already discovered\");\n-  oop* const list = _discovered_list.addr();\n-  reference_set_discovered(reference, *list);\n-  *list = reference;\n-}\n-\n-bool XReferenceProcessor::discover_reference(oop reference, ReferenceType type) {\n-  if (!RegisterReferences) {\n-    \/\/ Reference processing disabled\n-    return false;\n-  }\n-\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n-\n-  \/\/ Update statistics\n-  _encountered_count.get()[type]++;\n-\n-  if (!should_discover(reference, type)) {\n-    \/\/ Not discovered\n-    return false;\n-  }\n-\n-  discover(reference, type);\n-\n-  \/\/ Discovered\n-  return true;\n-}\n-\n-oop XReferenceProcessor::drop(oop reference, ReferenceType type) {\n-  log_trace(gc, ref)(\"Dropped Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n-\n-  \/\/ Keep referent alive\n-  keep_alive(reference, type);\n-\n-  \/\/ Unlink and return next in list\n-  const oop next = reference_discovered(reference);\n-  reference_set_discovered(reference, nullptr);\n-  return next;\n-}\n-\n-oop* XReferenceProcessor::keep(oop reference, ReferenceType type) {\n-  log_trace(gc, ref)(\"Enqueued Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n-\n-  \/\/ Update statistics\n-  _enqueued_count.get()[type]++;\n-\n-  \/\/ Make reference inactive\n-  make_inactive(reference, type);\n-\n-  \/\/ Return next in list\n-  return reference_discovered_addr(reference);\n-}\n-\n-void XReferenceProcessor::work() {\n-  \/\/ Process discovered references\n-  oop* const list = _discovered_list.addr();\n-  oop* p = list;\n-\n-  while (*p != nullptr) {\n-    const oop reference = *p;\n-    const ReferenceType type = reference_type(reference);\n-\n-    if (should_drop(reference, type)) {\n-      *p = drop(reference, type);\n-    } else {\n-      p = keep(reference, type);\n-    }\n-  }\n-\n-  \/\/ Prepend discovered references to internal pending list\n-  if (*list != nullptr) {\n-    *p = Atomic::xchg(_pending_list.addr(), *list);\n-    if (*p == nullptr) {\n-      \/\/ First to prepend to list, record tail\n-      _pending_list_tail = p;\n-    }\n-\n-    \/\/ Clear discovered list\n-    *list = nullptr;\n-  }\n-}\n-\n-bool XReferenceProcessor::is_empty() const {\n-  XPerWorkerConstIterator<oop> iter(&_discovered_list);\n-  for (const oop* list; iter.next(&list);) {\n-    if (*list != nullptr) {\n-      return false;\n-    }\n-  }\n-\n-  if (_pending_list.get() != nullptr) {\n-    return false;\n-  }\n-\n-  return true;\n-}\n-\n-void XReferenceProcessor::reset_statistics() {\n-  assert(is_empty(), \"Should be empty\");\n-\n-  \/\/ Reset encountered\n-  XPerWorkerIterator<Counters> iter_encountered(&_encountered_count);\n-  for (Counters* counters; iter_encountered.next(&counters);) {\n-    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n-      (*counters)[i] = 0;\n-    }\n-  }\n-\n-  \/\/ Reset discovered\n-  XPerWorkerIterator<Counters> iter_discovered(&_discovered_count);\n-  for (Counters* counters; iter_discovered.next(&counters);) {\n-    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n-      (*counters)[i] = 0;\n-    }\n-  }\n-\n-  \/\/ Reset enqueued\n-  XPerWorkerIterator<Counters> iter_enqueued(&_enqueued_count);\n-  for (Counters* counters; iter_enqueued.next(&counters);) {\n-    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n-      (*counters)[i] = 0;\n-    }\n-  }\n-}\n-\n-void XReferenceProcessor::collect_statistics() {\n-  Counters encountered = {};\n-  Counters discovered = {};\n-  Counters enqueued = {};\n-\n-  \/\/ Sum encountered\n-  XPerWorkerConstIterator<Counters> iter_encountered(&_encountered_count);\n-  for (const Counters* counters; iter_encountered.next(&counters);) {\n-    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n-      encountered[i] += (*counters)[i];\n-    }\n-  }\n-\n-  \/\/ Sum discovered\n-  XPerWorkerConstIterator<Counters> iter_discovered(&_discovered_count);\n-  for (const Counters* counters; iter_discovered.next(&counters);) {\n-    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n-      discovered[i] += (*counters)[i];\n-    }\n-  }\n-\n-  \/\/ Sum enqueued\n-  XPerWorkerConstIterator<Counters> iter_enqueued(&_enqueued_count);\n-  for (const Counters* counters; iter_enqueued.next(&counters);) {\n-    for (int i = REF_SOFT; i <= REF_PHANTOM; i++) {\n-      enqueued[i] += (*counters)[i];\n-    }\n-  }\n-\n-  \/\/ Update statistics\n-  XStatReferences::set_soft(encountered[REF_SOFT], discovered[REF_SOFT], enqueued[REF_SOFT]);\n-  XStatReferences::set_weak(encountered[REF_WEAK], discovered[REF_WEAK], enqueued[REF_WEAK]);\n-  XStatReferences::set_final(encountered[REF_FINAL], discovered[REF_FINAL], enqueued[REF_FINAL]);\n-  XStatReferences::set_phantom(encountered[REF_PHANTOM], discovered[REF_PHANTOM], enqueued[REF_PHANTOM]);\n-\n-  \/\/ Trace statistics\n-  const ReferenceProcessorStats stats(discovered[REF_SOFT],\n-                                      discovered[REF_WEAK],\n-                                      discovered[REF_FINAL],\n-                                      discovered[REF_PHANTOM]);\n-  XTracer::tracer()->report_gc_reference_stats(stats);\n-}\n-\n-class XReferenceProcessorTask : public XTask {\n-private:\n-  XReferenceProcessor* const _reference_processor;\n-\n-public:\n-  XReferenceProcessorTask(XReferenceProcessor* reference_processor) :\n-      XTask(\"XReferenceProcessorTask\"),\n-      _reference_processor(reference_processor) {}\n-\n-  virtual void work() {\n-    _reference_processor->work();\n-  }\n-};\n-\n-void XReferenceProcessor::process_references() {\n-  XStatTimer timer(XSubPhaseConcurrentReferencesProcess);\n-\n-  \/\/ Process discovered lists\n-  XReferenceProcessorTask task(this);\n-  _workers->run(&task);\n-\n-  \/\/ Update SoftReference clock\n-  soft_reference_update_clock();\n-\n-  \/\/ Collect, log and trace statistics\n-  collect_statistics();\n-}\n-\n-void XReferenceProcessor::enqueue_references() {\n-  XStatTimer timer(XSubPhaseConcurrentReferencesEnqueue);\n-\n-  if (_pending_list.get() == nullptr) {\n-    \/\/ Nothing to enqueue\n-    return;\n-  }\n-\n-  {\n-    \/\/ Heap_lock protects external pending list\n-    MonitorLocker ml(Heap_lock);\n-\n-    \/\/ Prepend internal pending list to external pending list\n-    *_pending_list_tail = Universe::swap_reference_pending_list(_pending_list.get());\n-\n-    \/\/ Notify ReferenceHandler thread\n-    ml.notify_all();\n-  }\n-\n-  \/\/ Reset internal pending list\n-  _pending_list.set(nullptr);\n-  _pending_list_tail = _pending_list.addr();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xReferenceProcessor.cpp","additions":0,"deletions":459,"binary":false,"changes":459,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XREFERENCEPROCESSOR_HPP\n-#define SHARE_GC_X_XREFERENCEPROCESSOR_HPP\n-\n-#include \"gc\/shared\/referenceDiscoverer.hpp\"\n-#include \"gc\/x\/xValue.hpp\"\n-\n-class ReferencePolicy;\n-class XWorkers;\n-\n-class XReferenceProcessor : public ReferenceDiscoverer {\n-  friend class XReferenceProcessorTask;\n-\n-private:\n-  static const size_t reference_type_count = REF_PHANTOM + 1;\n-  typedef size_t Counters[reference_type_count];\n-\n-  XWorkers* const      _workers;\n-  ReferencePolicy*     _soft_reference_policy;\n-  XPerWorker<Counters> _encountered_count;\n-  XPerWorker<Counters> _discovered_count;\n-  XPerWorker<Counters> _enqueued_count;\n-  XPerWorker<oop>      _discovered_list;\n-  XContended<oop>      _pending_list;\n-  oop*                 _pending_list_tail;\n-\n-  bool is_inactive(oop reference, oop referent, ReferenceType type) const;\n-  bool is_strongly_live(oop referent) const;\n-  bool is_softly_live(oop reference, ReferenceType type) const;\n-\n-  bool should_discover(oop reference, ReferenceType type) const;\n-  bool should_drop(oop reference, ReferenceType type) const;\n-  void keep_alive(oop reference, ReferenceType type) const;\n-  void make_inactive(oop reference, ReferenceType type) const;\n-\n-  void discover(oop reference, ReferenceType type);\n-\n-  oop drop(oop reference, ReferenceType type);\n-  oop* keep(oop reference, ReferenceType type);\n-\n-  bool is_empty() const;\n-\n-  void work();\n-  void collect_statistics();\n-\n-public:\n-  XReferenceProcessor(XWorkers* workers);\n-\n-  void set_soft_reference_policy(bool clear);\n-  void reset_statistics();\n-\n-  virtual bool discover_reference(oop reference, ReferenceType type);\n-  void process_references();\n-  void enqueue_references();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XREFERENCEPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xReferenceProcessor.hpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,419 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xAbort.inline.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xForwarding.inline.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xRelocate.hpp\"\n-#include \"gc\/x\/xRelocationSet.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"prims\/jvmtiTagMap.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-XRelocate::XRelocate(XWorkers* workers) :\n-    _workers(workers) {}\n-\n-static uintptr_t forwarding_index(XForwarding* forwarding, uintptr_t from_addr) {\n-  const uintptr_t from_offset = XAddress::offset(from_addr);\n-  return (from_offset - forwarding->start()) >> forwarding->object_alignment_shift();\n-}\n-\n-static uintptr_t forwarding_find(XForwarding* forwarding, uintptr_t from_addr, XForwardingCursor* cursor) {\n-  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n-  const XForwardingEntry entry = forwarding->find(from_index, cursor);\n-  return entry.populated() ? XAddress::good(entry.to_offset()) : 0;\n-}\n-\n-static uintptr_t forwarding_insert(XForwarding* forwarding, uintptr_t from_addr, uintptr_t to_addr, XForwardingCursor* cursor) {\n-  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n-  const uintptr_t to_offset = XAddress::offset(to_addr);\n-  const uintptr_t to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n-  return XAddress::good(to_offset_final);\n-}\n-\n-static uintptr_t relocate_object_inner(XForwarding* forwarding, uintptr_t from_addr, XForwardingCursor* cursor) {\n-  assert(XHeap::heap()->is_object_live(from_addr), \"Should be live\");\n-\n-  \/\/ Allocate object\n-  const size_t size = XUtils::object_size(from_addr);\n-  const uintptr_t to_addr = XHeap::heap()->alloc_object_for_relocation(size);\n-  if (to_addr == 0) {\n-    \/\/ Allocation failed\n-    return 0;\n-  }\n-\n-  \/\/ Copy object\n-  XUtils::object_copy_disjoint(from_addr, to_addr, size);\n-\n-  \/\/ Insert forwarding\n-  const uintptr_t to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, cursor);\n-  if (to_addr_final != to_addr) {\n-    \/\/ Already relocated, try undo allocation\n-    XHeap::heap()->undo_alloc_object_for_relocation(to_addr, size);\n-  }\n-\n-  return to_addr_final;\n-}\n-\n-uintptr_t XRelocate::relocate_object(XForwarding* forwarding, uintptr_t from_addr) const {\n-  XForwardingCursor cursor;\n-\n-  \/\/ Lookup forwarding\n-  uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n-  if (to_addr != 0) {\n-    \/\/ Already relocated\n-    return to_addr;\n-  }\n-\n-  \/\/ Relocate object\n-  if (forwarding->retain_page()) {\n-    to_addr = relocate_object_inner(forwarding, from_addr, &cursor);\n-    forwarding->release_page();\n-\n-    if (to_addr != 0) {\n-      \/\/ Success\n-      return to_addr;\n-    }\n-\n-    \/\/ Failed to relocate object. Wait for a worker thread to complete\n-    \/\/ relocation of this page, and then forward the object. If the GC\n-    \/\/ aborts the relocation phase before the page has been relocated,\n-    \/\/ then wait return false and we just forward the object in-place.\n-    if (!forwarding->wait_page_released()) {\n-      \/\/ Forward object in-place\n-      return forwarding_insert(forwarding, from_addr, from_addr, &cursor);\n-    }\n-  }\n-\n-  \/\/ Forward object\n-  return forward_object(forwarding, from_addr);\n-}\n-\n-uintptr_t XRelocate::forward_object(XForwarding* forwarding, uintptr_t from_addr) const {\n-  XForwardingCursor cursor;\n-  const uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n-  assert(to_addr != 0, \"Should be forwarded\");\n-  return to_addr;\n-}\n-\n-static XPage* alloc_page(const XForwarding* forwarding) {\n-  if (ZStressRelocateInPlace) {\n-    \/\/ Simulate failure to allocate a new page. This will\n-    \/\/ cause the page being relocated to be relocated in-place.\n-    return nullptr;\n-  }\n-\n-  XAllocationFlags flags;\n-  flags.set_non_blocking();\n-  flags.set_worker_relocation();\n-  return XHeap::heap()->alloc_page(forwarding->type(), forwarding->size(), flags);\n-}\n-\n-static void free_page(XPage* page) {\n-  XHeap::heap()->free_page(page, true \/* reclaimed *\/);\n-}\n-\n-static bool should_free_target_page(XPage* page) {\n-  \/\/ Free target page if it is empty. We can end up with an empty target\n-  \/\/ page if we allocated a new target page, and then lost the race to\n-  \/\/ relocate the remaining objects, leaving the target page empty when\n-  \/\/ relocation completed.\n-  return page != nullptr && page->top() == page->start();\n-}\n-\n-class XRelocateSmallAllocator {\n-private:\n-  volatile size_t _in_place_count;\n-\n-public:\n-  XRelocateSmallAllocator() :\n-      _in_place_count(0) {}\n-\n-  XPage* alloc_target_page(XForwarding* forwarding, XPage* target) {\n-    XPage* const page = alloc_page(forwarding);\n-    if (page == nullptr) {\n-      Atomic::inc(&_in_place_count);\n-    }\n-\n-    return page;\n-  }\n-\n-  void share_target_page(XPage* page) {\n-    \/\/ Does nothing\n-  }\n-\n-  void free_target_page(XPage* page) {\n-    if (should_free_target_page(page)) {\n-      free_page(page);\n-    }\n-  }\n-\n-  void free_relocated_page(XPage* page) {\n-    free_page(page);\n-  }\n-\n-  uintptr_t alloc_object(XPage* page, size_t size) const {\n-    return (page != nullptr) ? page->alloc_object(size) : 0;\n-  }\n-\n-  void undo_alloc_object(XPage* page, uintptr_t addr, size_t size) const {\n-    page->undo_alloc_object(addr, size);\n-  }\n-\n-  size_t in_place_count() const {\n-    return _in_place_count;\n-  }\n-};\n-\n-class XRelocateMediumAllocator {\n-private:\n-  XConditionLock      _lock;\n-  XPage*              _shared;\n-  bool                _in_place;\n-  volatile size_t     _in_place_count;\n-\n-public:\n-  XRelocateMediumAllocator() :\n-      _lock(),\n-      _shared(nullptr),\n-      _in_place(false),\n-      _in_place_count(0) {}\n-\n-  ~XRelocateMediumAllocator() {\n-    if (should_free_target_page(_shared)) {\n-      free_page(_shared);\n-    }\n-  }\n-\n-  XPage* alloc_target_page(XForwarding* forwarding, XPage* target) {\n-    XLocker<XConditionLock> locker(&_lock);\n-\n-    \/\/ Wait for any ongoing in-place relocation to complete\n-    while (_in_place) {\n-      _lock.wait();\n-    }\n-\n-    \/\/ Allocate a new page only if the shared page is the same as the\n-    \/\/ current target page. The shared page will be different from the\n-    \/\/ current target page if another thread shared a page, or allocated\n-    \/\/ a new page.\n-    if (_shared == target) {\n-      _shared = alloc_page(forwarding);\n-      if (_shared == nullptr) {\n-        Atomic::inc(&_in_place_count);\n-        _in_place = true;\n-      }\n-    }\n-\n-    return _shared;\n-  }\n-\n-  void share_target_page(XPage* page) {\n-    XLocker<XConditionLock> locker(&_lock);\n-\n-    assert(_in_place, \"Invalid state\");\n-    assert(_shared == nullptr, \"Invalid state\");\n-    assert(page != nullptr, \"Invalid page\");\n-\n-    _shared = page;\n-    _in_place = false;\n-\n-    _lock.notify_all();\n-  }\n-\n-  void free_target_page(XPage* page) {\n-    \/\/ Does nothing\n-  }\n-\n-  void free_relocated_page(XPage* page) {\n-    free_page(page);\n-  }\n-\n-  uintptr_t alloc_object(XPage* page, size_t size) const {\n-    return (page != nullptr) ? page->alloc_object_atomic(size) : 0;\n-  }\n-\n-  void undo_alloc_object(XPage* page, uintptr_t addr, size_t size) const {\n-    page->undo_alloc_object_atomic(addr, size);\n-  }\n-\n-  size_t in_place_count() const {\n-    return _in_place_count;\n-  }\n-};\n-\n-template <typename Allocator>\n-class XRelocateClosure : public ObjectClosure {\n-private:\n-  Allocator* const _allocator;\n-  XForwarding*     _forwarding;\n-  XPage*           _target;\n-\n-  bool relocate_object(uintptr_t from_addr) const {\n-    XForwardingCursor cursor;\n-\n-    \/\/ Lookup forwarding\n-    if (forwarding_find(_forwarding, from_addr, &cursor) != 0) {\n-      \/\/ Already relocated\n-      return true;\n-    }\n-\n-    \/\/ Allocate object\n-    const size_t size = XUtils::object_size(from_addr);\n-    const uintptr_t to_addr = _allocator->alloc_object(_target, size);\n-    if (to_addr == 0) {\n-      \/\/ Allocation failed\n-      return false;\n-    }\n-\n-    \/\/ Copy object. Use conjoint copying if we are relocating\n-    \/\/ in-place and the new object overlapps with the old object.\n-    if (_forwarding->in_place() && to_addr + size > from_addr) {\n-      XUtils::object_copy_conjoint(from_addr, to_addr, size);\n-    } else {\n-      XUtils::object_copy_disjoint(from_addr, to_addr, size);\n-    }\n-\n-    \/\/ Insert forwarding\n-    if (forwarding_insert(_forwarding, from_addr, to_addr, &cursor) != to_addr) {\n-      \/\/ Already relocated, undo allocation\n-      _allocator->undo_alloc_object(_target, to_addr, size);\n-    }\n-\n-    return true;\n-  }\n-\n-  virtual void do_object(oop obj) {\n-    const uintptr_t addr = XOop::to_address(obj);\n-    assert(XHeap::heap()->is_object_live(addr), \"Should be live\");\n-\n-    while (!relocate_object(addr)) {\n-      \/\/ Allocate a new target page, or if that fails, use the page being\n-      \/\/ relocated as the new target, which will cause it to be relocated\n-      \/\/ in-place.\n-      _target = _allocator->alloc_target_page(_forwarding, _target);\n-      if (_target != nullptr) {\n-        continue;\n-      }\n-\n-      \/\/ Claim the page being relocated to block other threads from accessing\n-      \/\/ it, or its forwarding table, until it has been released (relocation\n-      \/\/ completed).\n-      _target = _forwarding->claim_page();\n-      _target->reset_for_in_place_relocation();\n-      _forwarding->set_in_place();\n-    }\n-  }\n-\n-public:\n-  XRelocateClosure(Allocator* allocator) :\n-      _allocator(allocator),\n-      _forwarding(nullptr),\n-      _target(nullptr) {}\n-\n-  ~XRelocateClosure() {\n-    _allocator->free_target_page(_target);\n-  }\n-\n-  void do_forwarding(XForwarding* forwarding) {\n-    _forwarding = forwarding;\n-\n-    \/\/ Check if we should abort\n-    if (XAbort::should_abort()) {\n-      _forwarding->abort_page();\n-      return;\n-    }\n-\n-    \/\/ Relocate objects\n-    _forwarding->object_iterate(this);\n-\n-    \/\/ Verify\n-    if (ZVerifyForwarding) {\n-      _forwarding->verify();\n-    }\n-\n-    \/\/ Release relocated page\n-    _forwarding->release_page();\n-\n-    if (_forwarding->in_place()) {\n-      \/\/ The relocated page has been relocated in-place and should not\n-      \/\/ be freed. Keep it as target page until it is full, and offer to\n-      \/\/ share it with other worker threads.\n-      _allocator->share_target_page(_target);\n-    } else {\n-      \/\/ Detach and free relocated page\n-      XPage* const page = _forwarding->detach_page();\n-      _allocator->free_relocated_page(page);\n-    }\n-  }\n-};\n-\n-class XRelocateTask : public XTask {\n-private:\n-  XRelocationSetParallelIterator _iter;\n-  XRelocateSmallAllocator        _small_allocator;\n-  XRelocateMediumAllocator       _medium_allocator;\n-\n-  static bool is_small(XForwarding* forwarding) {\n-    return forwarding->type() == XPageTypeSmall;\n-  }\n-\n-public:\n-  XRelocateTask(XRelocationSet* relocation_set) :\n-      XTask(\"XRelocateTask\"),\n-      _iter(relocation_set),\n-      _small_allocator(),\n-      _medium_allocator() {}\n-\n-  ~XRelocateTask() {\n-    XStatRelocation::set_at_relocate_end(_small_allocator.in_place_count(),\n-                                         _medium_allocator.in_place_count());\n-  }\n-\n-  virtual void work() {\n-    XRelocateClosure<XRelocateSmallAllocator> small(&_small_allocator);\n-    XRelocateClosure<XRelocateMediumAllocator> medium(&_medium_allocator);\n-\n-    for (XForwarding* forwarding; _iter.next(&forwarding);) {\n-      if (is_small(forwarding)) {\n-        small.do_forwarding(forwarding);\n-      } else {\n-        medium.do_forwarding(forwarding);\n-      }\n-    }\n-  }\n-};\n-\n-void XRelocate::relocate(XRelocationSet* relocation_set) {\n-  XRelocateTask task(relocation_set);\n-  _workers->run(&task);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocate.cpp","additions":0,"deletions":419,"binary":false,"changes":419,"status":"deleted"},{"patch":"@@ -1,49 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRELOCATE_HPP\n-#define SHARE_GC_X_XRELOCATE_HPP\n-\n-#include \"gc\/x\/xRelocationSet.hpp\"\n-\n-class XForwarding;\n-class XWorkers;\n-\n-class XRelocate {\n-  friend class XRelocateTask;\n-\n-private:\n-  XWorkers* const _workers;\n-\n-  void work(XRelocationSetParallelIterator* iter);\n-\n-public:\n-  XRelocate(XWorkers* workers);\n-\n-  uintptr_t relocate_object(XForwarding* forwarding, uintptr_t from_addr) const;\n-  uintptr_t forward_object(XForwarding* forwarding, uintptr_t from_addr) const;\n-\n-  void relocate(XRelocationSet* relocation_set);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XRELOCATE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocate.hpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"deleted"},{"patch":"@@ -1,135 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xForwarding.inline.hpp\"\n-#include \"gc\/x\/xForwardingAllocator.inline.hpp\"\n-#include \"gc\/x\/xRelocationSet.inline.hpp\"\n-#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-class XRelocationSetInstallTask : public XTask {\n-private:\n-  XForwardingAllocator* const    _allocator;\n-  XForwarding**                  _forwardings;\n-  const size_t                   _nforwardings;\n-  XArrayParallelIterator<XPage*> _small_iter;\n-  XArrayParallelIterator<XPage*> _medium_iter;\n-  volatile size_t                _small_next;\n-  volatile size_t                _medium_next;\n-\n-  void install(XForwarding* forwarding, volatile size_t* next) {\n-    const size_t index = Atomic::fetch_then_add(next, 1u);\n-    assert(index < _nforwardings, \"Invalid index\");\n-    _forwardings[index] = forwarding;\n-  }\n-\n-  void install_small(XForwarding* forwarding) {\n-    install(forwarding, &_small_next);\n-  }\n-\n-  void install_medium(XForwarding* forwarding) {\n-    install(forwarding, &_medium_next);\n-  }\n-\n-public:\n-  XRelocationSetInstallTask(XForwardingAllocator* allocator, const XRelocationSetSelector* selector) :\n-      XTask(\"XRelocationSetInstallTask\"),\n-      _allocator(allocator),\n-      _forwardings(nullptr),\n-      _nforwardings(selector->small()->length() + selector->medium()->length()),\n-      _small_iter(selector->small()),\n-      _medium_iter(selector->medium()),\n-      _small_next(selector->medium()->length()),\n-      _medium_next(0) {\n-\n-    \/\/ Reset the allocator to have room for the relocation\n-    \/\/ set, all forwardings, and all forwarding entries.\n-    const size_t relocation_set_size = _nforwardings * sizeof(XForwarding*);\n-    const size_t forwardings_size = _nforwardings * sizeof(XForwarding);\n-    const size_t forwarding_entries_size = selector->forwarding_entries() * sizeof(XForwardingEntry);\n-    _allocator->reset(relocation_set_size + forwardings_size + forwarding_entries_size);\n-\n-    \/\/ Allocate relocation set\n-    _forwardings = new (_allocator->alloc(relocation_set_size)) XForwarding*[_nforwardings];\n-  }\n-\n-  ~XRelocationSetInstallTask() {\n-    assert(_allocator->is_full(), \"Should be full\");\n-  }\n-\n-  virtual void work() {\n-    \/\/ Allocate and install forwardings for small pages\n-    for (XPage* page; _small_iter.next(&page);) {\n-      XForwarding* const forwarding = XForwarding::alloc(_allocator, page);\n-      install_small(forwarding);\n-    }\n-\n-    \/\/ Allocate and install forwardings for medium pages\n-    for (XPage* page; _medium_iter.next(&page);) {\n-      XForwarding* const forwarding = XForwarding::alloc(_allocator, page);\n-      install_medium(forwarding);\n-    }\n-  }\n-\n-  XForwarding** forwardings() const {\n-    return _forwardings;\n-  }\n-\n-  size_t nforwardings() const {\n-    return _nforwardings;\n-  }\n-};\n-\n-XRelocationSet::XRelocationSet(XWorkers* workers) :\n-    _workers(workers),\n-    _allocator(),\n-    _forwardings(nullptr),\n-    _nforwardings(0) {}\n-\n-void XRelocationSet::install(const XRelocationSetSelector* selector) {\n-  \/\/ Install relocation set\n-  XRelocationSetInstallTask task(&_allocator, selector);\n-  _workers->run(&task);\n-\n-  _forwardings = task.forwardings();\n-  _nforwardings = task.nforwardings();\n-\n-  \/\/ Update statistics\n-  XStatRelocation::set_at_install_relocation_set(_allocator.size());\n-}\n-\n-void XRelocationSet::reset() {\n-  \/\/ Destroy forwardings\n-  XRelocationSetIterator iter(this);\n-  for (XForwarding* forwarding; iter.next(&forwarding);) {\n-    forwarding->~XForwarding();\n-  }\n-\n-  _nforwardings = 0;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSet.cpp","additions":0,"deletions":135,"binary":false,"changes":135,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRELOCATIONSET_HPP\n-#define SHARE_GC_X_XRELOCATIONSET_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"gc\/x\/xForwardingAllocator.hpp\"\n-\n-class XForwarding;\n-class XRelocationSetSelector;\n-class XWorkers;\n-\n-class XRelocationSet {\n-  template <bool> friend class XRelocationSetIteratorImpl;\n-\n-private:\n-  XWorkers*            _workers;\n-  XForwardingAllocator _allocator;\n-  XForwarding**        _forwardings;\n-  size_t               _nforwardings;\n-\n-public:\n-  XRelocationSet(XWorkers* workers);\n-\n-  void install(const XRelocationSetSelector* selector);\n-  void reset();\n-};\n-\n-template <bool Parallel>\n-class XRelocationSetIteratorImpl : public XArrayIteratorImpl<XForwarding*, Parallel> {\n-public:\n-  XRelocationSetIteratorImpl(XRelocationSet* relocation_set);\n-};\n-\n-using XRelocationSetIterator = XRelocationSetIteratorImpl<false \/* Parallel *\/>;\n-using XRelocationSetParallelIterator = XRelocationSetIteratorImpl<true \/* Parallel *\/>;\n-\n-#endif \/\/ SHARE_GC_X_XRELOCATIONSET_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSet.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -1,35 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRELOCATIONSET_INLINE_HPP\n-#define SHARE_GC_X_XRELOCATIONSET_INLINE_HPP\n-\n-#include \"gc\/x\/xRelocationSet.hpp\"\n-\n-#include \"gc\/x\/xArray.inline.hpp\"\n-\n-template <bool Parallel>\n-inline XRelocationSetIteratorImpl<Parallel>::XRelocationSetIteratorImpl(XRelocationSet* relocation_set) :\n-    XArrayIteratorImpl<XForwarding*, Parallel>(relocation_set->_forwardings, relocation_set->_nforwardings) {}\n-\n-#endif \/\/ SHARE_GC_X_XRELOCATIONSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSet.inline.hpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"deleted"},{"patch":"@@ -1,213 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xForwarding.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-XRelocationSetSelectorGroupStats::XRelocationSetSelectorGroupStats() :\n-    _npages_candidates(0),\n-    _total(0),\n-    _live(0),\n-    _empty(0),\n-    _npages_selected(0),\n-    _relocate(0) {}\n-\n-XRelocationSetSelectorGroup::XRelocationSetSelectorGroup(const char* name,\n-                                                         uint8_t page_type,\n-                                                         size_t page_size,\n-                                                         size_t object_size_limit) :\n-    _name(name),\n-    _page_type(page_type),\n-    _page_size(page_size),\n-    _object_size_limit(object_size_limit),\n-    _fragmentation_limit(page_size * (ZFragmentationLimit \/ 100)),\n-    _live_pages(),\n-    _forwarding_entries(0),\n-    _stats() {}\n-\n-bool XRelocationSetSelectorGroup::is_disabled() {\n-  \/\/ Medium pages are disabled when their page size is zero\n-  return _page_type == XPageTypeMedium && _page_size == 0;\n-}\n-\n-bool XRelocationSetSelectorGroup::is_selectable() {\n-  \/\/ Large pages are not selectable\n-  return _page_type != XPageTypeLarge;\n-}\n-\n-void XRelocationSetSelectorGroup::semi_sort() {\n-  \/\/ Semi-sort live pages by number of live bytes in ascending order\n-  const size_t npartitions_shift = 11;\n-  const size_t npartitions = (size_t)1 << npartitions_shift;\n-  const size_t partition_size = _page_size >> npartitions_shift;\n-  const size_t partition_size_shift = exact_log2(partition_size);\n-\n-  \/\/ Partition slots\/fingers\n-  int partitions[npartitions] = { \/* zero initialize *\/ };\n-\n-  \/\/ Calculate partition slots\n-  XArrayIterator<XPage*> iter1(&_live_pages);\n-  for (XPage* page; iter1.next(&page);) {\n-    const size_t index = page->live_bytes() >> partition_size_shift;\n-    partitions[index]++;\n-  }\n-\n-  \/\/ Calculate partition fingers\n-  int finger = 0;\n-  for (size_t i = 0; i < npartitions; i++) {\n-    const int slots = partitions[i];\n-    partitions[i] = finger;\n-    finger += slots;\n-  }\n-\n-  \/\/ Allocate destination array\n-  const int npages = _live_pages.length();\n-  XArray<XPage*> sorted_live_pages(npages, npages, nullptr);\n-\n-  \/\/ Sort pages into partitions\n-  XArrayIterator<XPage*> iter2(&_live_pages);\n-  for (XPage* page; iter2.next(&page);) {\n-    const size_t index = page->live_bytes() >> partition_size_shift;\n-    const int finger = partitions[index]++;\n-    assert(sorted_live_pages.at(finger) == nullptr, \"Invalid finger\");\n-    sorted_live_pages.at_put(finger, page);\n-  }\n-\n-  _live_pages.swap(&sorted_live_pages);\n-}\n-\n-void XRelocationSetSelectorGroup::select_inner() {\n-  \/\/ Calculate the number of pages to relocate by successively including pages in\n-  \/\/ a candidate relocation set and calculate the maximum space requirement for\n-  \/\/ their live objects.\n-  const int npages = _live_pages.length();\n-  int selected_from = 0;\n-  int selected_to = 0;\n-  size_t npages_selected = 0;\n-  size_t selected_live_bytes = 0;\n-  size_t selected_forwarding_entries = 0;\n-  size_t from_live_bytes = 0;\n-  size_t from_forwarding_entries = 0;\n-\n-  semi_sort();\n-\n-  for (int from = 1; from <= npages; from++) {\n-    \/\/ Add page to the candidate relocation set\n-    XPage* const page = _live_pages.at(from - 1);\n-    from_live_bytes += page->live_bytes();\n-    from_forwarding_entries += XForwarding::nentries(page);\n-\n-    \/\/ Calculate the maximum number of pages needed by the candidate relocation set.\n-    \/\/ By subtracting the object size limit from the pages size we get the maximum\n-    \/\/ number of pages that the relocation set is guaranteed to fit in, regardless\n-    \/\/ of in which order the objects are relocated.\n-    const int to = ceil((double)(from_live_bytes) \/ (double)(_page_size - _object_size_limit));\n-\n-    \/\/ Calculate the relative difference in reclaimable space compared to our\n-    \/\/ currently selected final relocation set. If this number is larger than the\n-    \/\/ acceptable fragmentation limit, then the current candidate relocation set\n-    \/\/ becomes our new final relocation set.\n-    const int diff_from = from - selected_from;\n-    const int diff_to = to - selected_to;\n-    const double diff_reclaimable = 100 - percent_of(diff_to, diff_from);\n-    if (diff_reclaimable > ZFragmentationLimit) {\n-      selected_from = from;\n-      selected_to = to;\n-      selected_live_bytes = from_live_bytes;\n-      npages_selected += 1;\n-      selected_forwarding_entries = from_forwarding_entries;\n-    }\n-\n-    log_trace(gc, reloc)(\"Candidate Relocation Set (%s Pages): %d->%d, \"\n-                         \"%.1f%% relative defragmentation, \" SIZE_FORMAT \" forwarding entries, %s\",\n-                         _name, from, to, diff_reclaimable, from_forwarding_entries,\n-                         (selected_from == from) ? \"Selected\" : \"Rejected\");\n-  }\n-\n-  \/\/ Finalize selection\n-  _live_pages.trunc_to(selected_from);\n-  _forwarding_entries = selected_forwarding_entries;\n-\n-  \/\/ Update statistics\n-  _stats._relocate = selected_live_bytes;\n-  _stats._npages_selected = npages_selected;\n-\n-  log_trace(gc, reloc)(\"Relocation Set (%s Pages): %d->%d, %d skipped, \" SIZE_FORMAT \" forwarding entries\",\n-                       _name, selected_from, selected_to, npages - selected_from, selected_forwarding_entries);\n-}\n-\n-void XRelocationSetSelectorGroup::select() {\n-  if (is_disabled()) {\n-    return;\n-  }\n-\n-  EventZRelocationSetGroup event;\n-\n-  if (is_selectable()) {\n-    select_inner();\n-  }\n-\n-  \/\/ Send event\n-  event.commit(_page_type, _stats.npages_candidates(), _stats.total(), _stats.empty(), _stats.npages_selected(), _stats.relocate());\n-}\n-\n-XRelocationSetSelector::XRelocationSetSelector() :\n-    _small(\"Small\", XPageTypeSmall, XPageSizeSmall, XObjectSizeLimitSmall),\n-    _medium(\"Medium\", XPageTypeMedium, XPageSizeMedium, XObjectSizeLimitMedium),\n-    _large(\"Large\", XPageTypeLarge, 0 \/* page_size *\/, 0 \/* object_size_limit *\/),\n-    _empty_pages() {}\n-\n-void XRelocationSetSelector::select() {\n-  \/\/ Select pages to relocate. The resulting relocation set will be\n-  \/\/ sorted such that medium pages comes first, followed by small\n-  \/\/ pages. Pages within each page group will be semi-sorted by live\n-  \/\/ bytes in ascending order. Relocating pages in this order allows\n-  \/\/ us to start reclaiming memory more quickly.\n-\n-  EventZRelocationSet event;\n-\n-  \/\/ Select pages from each group\n-  _large.select();\n-  _medium.select();\n-  _small.select();\n-\n-  \/\/ Send event\n-  event.commit(total(), empty(), relocate());\n-}\n-\n-XRelocationSetSelectorStats XRelocationSetSelector::stats() const {\n-  XRelocationSetSelectorStats stats;\n-  stats._small = _small.stats();\n-  stats._medium = _medium.stats();\n-  stats._large = _large.stats();\n-  return stats;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSetSelector.cpp","additions":0,"deletions":213,"binary":false,"changes":213,"status":"deleted"},{"patch":"@@ -1,134 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRELOCATIONSETSELECTOR_HPP\n-#define SHARE_GC_X_XRELOCATIONSETSELECTOR_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class XPage;\n-\n-class XRelocationSetSelectorGroupStats {\n-  friend class XRelocationSetSelectorGroup;\n-\n-private:\n-  \/\/ Candidate set\n-  size_t _npages_candidates;\n-  size_t _total;\n-  size_t _live;\n-  size_t _empty;\n-\n-  \/\/ Selected set\n-  size_t _npages_selected;\n-  size_t _relocate;\n-\n-public:\n-  XRelocationSetSelectorGroupStats();\n-\n-  size_t npages_candidates() const;\n-  size_t total() const;\n-  size_t live() const;\n-  size_t empty() const;\n-\n-  size_t npages_selected() const;\n-  size_t relocate() const;\n-};\n-\n-class XRelocationSetSelectorStats {\n-  friend class XRelocationSetSelector;\n-\n-private:\n-  XRelocationSetSelectorGroupStats _small;\n-  XRelocationSetSelectorGroupStats _medium;\n-  XRelocationSetSelectorGroupStats _large;\n-\n-public:\n-  const XRelocationSetSelectorGroupStats& small() const;\n-  const XRelocationSetSelectorGroupStats& medium() const;\n-  const XRelocationSetSelectorGroupStats& large() const;\n-};\n-\n-class XRelocationSetSelectorGroup {\n-private:\n-  const char* const                _name;\n-  const uint8_t                    _page_type;\n-  const size_t                     _page_size;\n-  const size_t                     _object_size_limit;\n-  const size_t                     _fragmentation_limit;\n-  XArray<XPage*>                   _live_pages;\n-  size_t                           _forwarding_entries;\n-  XRelocationSetSelectorGroupStats _stats;\n-\n-  bool is_disabled();\n-  bool is_selectable();\n-  void semi_sort();\n-  void select_inner();\n-\n-public:\n-  XRelocationSetSelectorGroup(const char* name,\n-                              uint8_t page_type,\n-                              size_t page_size,\n-                              size_t object_size_limit);\n-\n-  void register_live_page(XPage* page);\n-  void register_empty_page(XPage* page);\n-  void select();\n-\n-  const XArray<XPage*>* selected() const;\n-  size_t forwarding_entries() const;\n-\n-  const XRelocationSetSelectorGroupStats& stats() const;\n-};\n-\n-class XRelocationSetSelector : public StackObj {\n-private:\n-  XRelocationSetSelectorGroup _small;\n-  XRelocationSetSelectorGroup _medium;\n-  XRelocationSetSelectorGroup _large;\n-  XArray<XPage*>              _empty_pages;\n-\n-  size_t total() const;\n-  size_t empty() const;\n-  size_t relocate() const;\n-\n-public:\n-  XRelocationSetSelector();\n-\n-  void register_live_page(XPage* page);\n-  void register_empty_page(XPage* page);\n-\n-  bool should_free_empty_pages(int bulk) const;\n-  const XArray<XPage*>* empty_pages() const;\n-  void clear_empty_pages();\n-\n-  void select();\n-\n-  const XArray<XPage*>* small() const;\n-  const XArray<XPage*>* medium() const;\n-  size_t forwarding_entries() const;\n-\n-  XRelocationSetSelectorStats stats() const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XRELOCATIONSETSELECTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSetSelector.hpp","additions":0,"deletions":134,"binary":false,"changes":134,"status":"deleted"},{"patch":"@@ -1,165 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRELOCATIONSETSELECTOR_INLINE_HPP\n-#define SHARE_GC_X_XRELOCATIONSETSELECTOR_INLINE_HPP\n-\n-#include \"gc\/x\/xRelocationSetSelector.hpp\"\n-\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-\n-inline size_t XRelocationSetSelectorGroupStats::npages_candidates() const {\n-  return _npages_candidates;\n-}\n-\n-inline size_t XRelocationSetSelectorGroupStats::total() const {\n-  return _total;\n-}\n-\n-inline size_t XRelocationSetSelectorGroupStats::live() const {\n-  return _live;\n-}\n-\n-inline size_t XRelocationSetSelectorGroupStats::empty() const {\n-  return _empty;\n-}\n-\n-inline size_t XRelocationSetSelectorGroupStats::npages_selected() const {\n-  return _npages_selected;\n-}\n-\n-inline size_t XRelocationSetSelectorGroupStats::relocate() const {\n-  return _relocate;\n-}\n-\n-inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorStats::small() const {\n-  return _small;\n-}\n-\n-inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorStats::medium() const {\n-  return _medium;\n-}\n-\n-inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorStats::large() const {\n-  return _large;\n-}\n-\n-inline void XRelocationSetSelectorGroup::register_live_page(XPage* page) {\n-  const uint8_t type = page->type();\n-  const size_t size = page->size();\n-  const size_t live = page->live_bytes();\n-  const size_t garbage = size - live;\n-\n-  if (garbage > _fragmentation_limit) {\n-    _live_pages.append(page);\n-  }\n-\n-  _stats._npages_candidates++;\n-  _stats._total += size;\n-  _stats._live += live;\n-}\n-\n-inline void XRelocationSetSelectorGroup::register_empty_page(XPage* page) {\n-  const size_t size = page->size();\n-\n-  _stats._npages_candidates++;\n-  _stats._total += size;\n-  _stats._empty += size;\n-}\n-\n-inline const XArray<XPage*>* XRelocationSetSelectorGroup::selected() const {\n-  return &_live_pages;\n-}\n-\n-inline size_t XRelocationSetSelectorGroup::forwarding_entries() const {\n-  return _forwarding_entries;\n-}\n-\n-inline const XRelocationSetSelectorGroupStats& XRelocationSetSelectorGroup::stats() const {\n-  return _stats;\n-}\n-\n-inline void XRelocationSetSelector::register_live_page(XPage* page) {\n-  const uint8_t type = page->type();\n-\n-  if (type == XPageTypeSmall) {\n-    _small.register_live_page(page);\n-  } else if (type == XPageTypeMedium) {\n-    _medium.register_live_page(page);\n-  } else {\n-    _large.register_live_page(page);\n-  }\n-}\n-\n-inline void XRelocationSetSelector::register_empty_page(XPage* page) {\n-  const uint8_t type = page->type();\n-\n-  if (type == XPageTypeSmall) {\n-    _small.register_empty_page(page);\n-  } else if (type == XPageTypeMedium) {\n-    _medium.register_empty_page(page);\n-  } else {\n-    _large.register_empty_page(page);\n-  }\n-\n-  _empty_pages.append(page);\n-}\n-\n-inline bool XRelocationSetSelector::should_free_empty_pages(int bulk) const {\n-  return _empty_pages.length() >= bulk && _empty_pages.is_nonempty();\n-}\n-\n-inline const XArray<XPage*>* XRelocationSetSelector::empty_pages() const {\n-  return &_empty_pages;\n-}\n-\n-inline void XRelocationSetSelector::clear_empty_pages() {\n-  return _empty_pages.clear();\n-}\n-\n-inline size_t XRelocationSetSelector::total() const {\n-  return _small.stats().total() + _medium.stats().total() + _large.stats().total();\n-}\n-\n-inline size_t XRelocationSetSelector::empty() const {\n-  return _small.stats().empty() + _medium.stats().empty() + _large.stats().empty();\n-}\n-\n-inline size_t XRelocationSetSelector::relocate() const {\n-  return _small.stats().relocate() + _medium.stats().relocate() + _large.stats().relocate();\n-}\n-\n-inline const XArray<XPage*>* XRelocationSetSelector::small() const {\n-  return _small.selected();\n-}\n-\n-inline const XArray<XPage*>* XRelocationSetSelector::medium() const {\n-  return _medium.selected();\n-}\n-\n-inline size_t XRelocationSetSelector::forwarding_entries() const {\n-  return _small.forwarding_entries() + _medium.forwarding_entries();\n-}\n-\n-#endif \/\/ SHARE_GC_X_XRELOCATIONSETSELECTOR_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRelocationSetSelector.inline.hpp","additions":0,"deletions":165,"binary":false,"changes":165,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xResurrection.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-volatile bool XResurrection::_blocked = false;\n-\n-void XResurrection::block() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _blocked = true;\n-}\n-\n-void XResurrection::unblock() {\n-  \/\/ No need for anything stronger than a relaxed store here.\n-  \/\/ The preceding handshake makes sure that all non-strong\n-  \/\/ oops have already been healed at this point.\n-  Atomic::store(&_blocked, false);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xResurrection.cpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRESURRECTION_HPP\n-#define SHARE_GC_X_XRESURRECTION_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class XResurrection : public AllStatic {\n-private:\n-  static volatile bool _blocked;\n-\n-public:\n-  static bool is_blocked();\n-  static void block();\n-  static void unblock();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XRESURRECTION_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xResurrection.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,35 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRESURRECTION_INLINE_HPP\n-#define SHARE_GC_X_XRESURRECTION_INLINE_HPP\n-\n-#include \"gc\/x\/xResurrection.hpp\"\n-\n-#include \"runtime\/atomic.hpp\"\n-\n-inline bool XResurrection::is_blocked() {\n-  return Atomic::load(&_blocked);\n-}\n-\n-#endif \/\/ SHARE_GC_X_XRESURRECTION_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xResurrection.inline.hpp","additions":0,"deletions":35,"binary":false,"changes":35,"status":"deleted"},{"patch":"@@ -1,142 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xNMethodTable.hpp\"\n-#include \"gc\/x\/xRootsIterator.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"prims\/jvmtiTagMap.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-static const XStatSubPhase XSubPhaseConcurrentRootsOopStorageSet(\"Concurrent Roots OopStorageSet\");\n-static const XStatSubPhase XSubPhaseConcurrentRootsClassLoaderDataGraph(\"Concurrent Roots ClassLoaderDataGraph\");\n-static const XStatSubPhase XSubPhaseConcurrentRootsJavaThreads(\"Concurrent Roots JavaThreads\");\n-static const XStatSubPhase XSubPhaseConcurrentRootsCodeCache(\"Concurrent Roots CodeCache\");\n-static const XStatSubPhase XSubPhaseConcurrentWeakRootsOopStorageSet(\"Concurrent Weak Roots OopStorageSet\");\n-\n-template <typename Iterator>\n-template <typename ClosureType>\n-void XParallelApply<Iterator>::apply(ClosureType* cl) {\n-  if (!Atomic::load(&_completed)) {\n-    _iter.apply(cl);\n-    if (!Atomic::load(&_completed)) {\n-      Atomic::store(&_completed, true);\n-    }\n-  }\n-}\n-\n-XStrongOopStorageSetIterator::XStrongOopStorageSetIterator() :\n-    _iter() {}\n-\n-void XStrongOopStorageSetIterator::apply(OopClosure* cl) {\n-  XStatTimer timer(XSubPhaseConcurrentRootsOopStorageSet);\n-  _iter.oops_do(cl);\n-}\n-\n-void XStrongCLDsIterator::apply(CLDClosure* cl) {\n-  XStatTimer timer(XSubPhaseConcurrentRootsClassLoaderDataGraph);\n-  ClassLoaderDataGraph::always_strong_cld_do(cl);\n-}\n-\n-XJavaThreadsIterator::XJavaThreadsIterator() :\n-    _threads(),\n-    _claimed(0) {}\n-\n-uint XJavaThreadsIterator::claim() {\n-  return Atomic::fetch_then_add(&_claimed, 1u);\n-}\n-\n-void XJavaThreadsIterator::apply(ThreadClosure* cl) {\n-  XStatTimer timer(XSubPhaseConcurrentRootsJavaThreads);\n-\n-  \/\/ The resource mark is needed because interpreter oop maps are\n-  \/\/ not reused in concurrent mode. Instead, they are temporary and\n-  \/\/ resource allocated.\n-  ResourceMark                 _rm;\n-\n-  for (uint i = claim(); i < _threads.length(); i = claim()) {\n-    cl->do_thread(_threads.thread_at(i));\n-  }\n-}\n-\n-XNMethodsIterator::XNMethodsIterator() {\n-  if (!ClassUnloading) {\n-    XNMethod::nmethods_do_begin();\n-  }\n-}\n-\n-XNMethodsIterator::~XNMethodsIterator() {\n-  if (!ClassUnloading) {\n-    XNMethod::nmethods_do_end();\n-  }\n-}\n-\n-void XNMethodsIterator::apply(NMethodClosure* cl) {\n-  XStatTimer timer(XSubPhaseConcurrentRootsCodeCache);\n-  XNMethod::nmethods_do(cl);\n-}\n-\n-XRootsIterator::XRootsIterator(int cld_claim) {\n-  if (cld_claim != ClassLoaderData::_claim_none) {\n-    ClassLoaderDataGraph::verify_claimed_marks_cleared(cld_claim);\n-  }\n-}\n-\n-void XRootsIterator::apply(OopClosure* cl,\n-                           CLDClosure* cld_cl,\n-                           ThreadClosure* thread_cl,\n-                           NMethodClosure* nm_cl) {\n-  _oop_storage_set.apply(cl);\n-  _class_loader_data_graph.apply(cld_cl);\n-  _java_threads.apply(thread_cl);\n-  if (!ClassUnloading) {\n-    _nmethods.apply(nm_cl);\n-  }\n-}\n-\n-XWeakOopStorageSetIterator::XWeakOopStorageSetIterator() :\n-    _iter() {}\n-\n-void XWeakOopStorageSetIterator::apply(OopClosure* cl) {\n-  XStatTimer timer(XSubPhaseConcurrentWeakRootsOopStorageSet);\n-  _iter.oops_do(cl);\n-}\n-\n-void XWeakOopStorageSetIterator::report_num_dead() {\n-  _iter.report_num_dead();\n-}\n-\n-void XWeakRootsIterator::report_num_dead() {\n-  _oop_storage_set.iter().report_num_dead();\n-}\n-\n-void XWeakRootsIterator::apply(OopClosure* cl) {\n-  _oop_storage_set.apply(cl);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xRootsIterator.cpp","additions":0,"deletions":142,"binary":false,"changes":142,"status":"deleted"},{"patch":"@@ -1,124 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XROOTSITERATOR_HPP\n-#define SHARE_GC_X_XROOTSITERATOR_HPP\n-\n-#include \"gc\/shared\/oopStorageSetParState.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"runtime\/threadSMR.hpp\"\n-\n-template <typename Iterator>\n-class XParallelApply {\n-private:\n-  Iterator      _iter;\n-  volatile bool _completed;\n-\n-public:\n-  XParallelApply() :\n-      _iter(),\n-      _completed(false) {}\n-\n-  template <typename ClosureType>\n-  void apply(ClosureType* cl);\n-\n-  Iterator& iter() {\n-    return _iter;\n-  }\n-};\n-\n-class XStrongOopStorageSetIterator {\n-  OopStorageSetStrongParState<true \/* concurrent *\/, false \/* is_const *\/> _iter;\n-\n-public:\n-  XStrongOopStorageSetIterator();\n-\n-  void apply(OopClosure* cl);\n-};\n-\n-class XStrongCLDsIterator {\n-public:\n-  void apply(CLDClosure* cl);\n-};\n-\n-class XJavaThreadsIterator {\n-private:\n-  ThreadsListHandle _threads;\n-  volatile uint     _claimed;\n-\n-  uint claim();\n-\n-public:\n-  XJavaThreadsIterator();\n-\n-  void apply(ThreadClosure* cl);\n-};\n-\n-class XNMethodsIterator {\n-public:\n-  XNMethodsIterator();\n-  ~XNMethodsIterator();\n-\n-  void apply(NMethodClosure* cl);\n-};\n-\n-class XRootsIterator {\n-private:\n-  XParallelApply<XStrongOopStorageSetIterator> _oop_storage_set;\n-  XParallelApply<XStrongCLDsIterator>          _class_loader_data_graph;\n-  XParallelApply<XJavaThreadsIterator>         _java_threads;\n-  XParallelApply<XNMethodsIterator>            _nmethods;\n-\n-public:\n-  XRootsIterator(int cld_claim);\n-\n-  void apply(OopClosure* cl,\n-             CLDClosure* cld_cl,\n-             ThreadClosure* thread_cl,\n-             NMethodClosure* nm_cl);\n-};\n-\n-class XWeakOopStorageSetIterator {\n-private:\n-  OopStorageSetWeakParState<true \/* concurrent *\/, false \/* is_const *\/> _iter;\n-\n-public:\n-  XWeakOopStorageSetIterator();\n-\n-  void apply(OopClosure* cl);\n-\n-  void report_num_dead();\n-};\n-\n-class XWeakRootsIterator {\n-private:\n-  XParallelApply<XWeakOopStorageSetIterator> _oop_storage_set;\n-\n-public:\n-  void apply(OopClosure* cl);\n-\n-  void report_num_dead();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XROOTSITERATOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRootsIterator.hpp","additions":0,"deletions":124,"binary":false,"changes":124,"status":"deleted"},{"patch":"@@ -1,85 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xRuntimeWorkers.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xThread.hpp\"\n-#include \"runtime\/java.hpp\"\n-\n-class XRuntimeWorkersInitializeTask : public WorkerTask {\n-private:\n-  const uint     _nworkers;\n-  uint           _started;\n-  XConditionLock _lock;\n-\n-public:\n-  XRuntimeWorkersInitializeTask(uint nworkers) :\n-      WorkerTask(\"XRuntimeWorkersInitializeTask\"),\n-      _nworkers(nworkers),\n-      _started(0),\n-      _lock() {}\n-\n-  virtual void work(uint worker_id) {\n-    \/\/ Wait for all threads to start\n-    XLocker<XConditionLock> locker(&_lock);\n-    if (++_started == _nworkers) {\n-      \/\/ All threads started\n-      _lock.notify_all();\n-    } else {\n-      while (_started != _nworkers) {\n-        _lock.wait();\n-      }\n-    }\n-  }\n-};\n-\n-XRuntimeWorkers::XRuntimeWorkers() :\n-    _workers(\"RuntimeWorker\",\n-             ParallelGCThreads) {\n-\n-  log_info_p(gc, init)(\"Runtime Workers: %u\", _workers.max_workers());\n-\n-  \/\/ Initialize worker threads\n-  _workers.initialize_workers();\n-  _workers.set_active_workers(_workers.max_workers());\n-  if (_workers.active_workers() != _workers.max_workers()) {\n-    vm_exit_during_initialization(\"Failed to create XRuntimeWorkers\");\n-  }\n-\n-  \/\/ Execute task to reduce latency in early safepoints,\n-  \/\/ which otherwise would have to take on any warmup costs.\n-  XRuntimeWorkersInitializeTask task(_workers.max_workers());\n-  _workers.run_task(&task);\n-}\n-\n-WorkerThreads* XRuntimeWorkers::workers() {\n-  return &_workers;\n-}\n-\n-void XRuntimeWorkers::threads_do(ThreadClosure* tc) const {\n-  _workers.threads_do(tc);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xRuntimeWorkers.cpp","additions":0,"deletions":85,"binary":false,"changes":85,"status":"deleted"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XRUNTIMEWORKERS_HPP\n-#define SHARE_GC_X_XRUNTIMEWORKERS_HPP\n-\n-#include \"gc\/shared\/workerThread.hpp\"\n-\n-class ThreadClosure;\n-\n-class XRuntimeWorkers {\n-private:\n-  WorkerThreads _workers;\n-\n-public:\n-  XRuntimeWorkers();\n-\n-  WorkerThreads* workers();\n-\n-  void threads_do(ThreadClosure* tc) const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XRUNTIMEWORKERS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xRuntimeWorkers.hpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,68 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XSAFEDELETE_HPP\n-#define SHARE_GC_X_XSAFEDELETE_HPP\n-\n-#include \"gc\/x\/xArray.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-\n-#include <type_traits>\n-\n-template <typename T>\n-class XSafeDeleteImpl {\n-private:\n-  using ItemT = std::remove_extent_t<T>;\n-\n-  XLock*         _lock;\n-  uint64_t       _enabled;\n-  XArray<ItemT*> _deferred;\n-\n-  bool deferred_delete(ItemT* item);\n-  void immediate_delete(ItemT* item);\n-\n-public:\n-  XSafeDeleteImpl(XLock* lock);\n-\n-  void enable_deferred_delete();\n-  void disable_deferred_delete();\n-\n-  void operator()(ItemT* item);\n-};\n-\n-template <typename T>\n-class XSafeDelete : public XSafeDeleteImpl<T> {\n-private:\n-  XLock _lock;\n-\n-public:\n-  XSafeDelete();\n-};\n-\n-template <typename T>\n-class XSafeDeleteNoLock : public XSafeDeleteImpl<T> {\n-public:\n-  XSafeDeleteNoLock();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XSAFEDELETE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xSafeDelete.hpp","additions":0,"deletions":68,"binary":false,"changes":68,"status":"deleted"},{"patch":"@@ -1,100 +0,0 @@\n-\/*\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XSAFEDELETE_INLINE_HPP\n-#define SHARE_GC_X_XSAFEDELETE_INLINE_HPP\n-\n-#include \"gc\/x\/xSafeDelete.hpp\"\n-\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-#include <type_traits>\n-\n-template <typename T>\n-XSafeDeleteImpl<T>::XSafeDeleteImpl(XLock* lock) :\n-    _lock(lock),\n-    _enabled(0),\n-    _deferred() {}\n-\n-template <typename T>\n-bool XSafeDeleteImpl<T>::deferred_delete(ItemT* item) {\n-  XLocker<XLock> locker(_lock);\n-  if (_enabled > 0) {\n-    _deferred.append(item);\n-    return true;\n-  }\n-\n-  return false;\n-}\n-\n-template <typename T>\n-void XSafeDeleteImpl<T>::immediate_delete(ItemT* item) {\n-  if (std::is_array<T>::value) {\n-    delete [] item;\n-  } else {\n-    delete item;\n-  }\n-}\n-\n-template <typename T>\n-void XSafeDeleteImpl<T>::enable_deferred_delete() {\n-  XLocker<XLock> locker(_lock);\n-  _enabled++;\n-}\n-\n-template <typename T>\n-void XSafeDeleteImpl<T>::disable_deferred_delete() {\n-  XArray<ItemT*> deferred;\n-\n-  {\n-    XLocker<XLock> locker(_lock);\n-    assert(_enabled > 0, \"Invalid state\");\n-    if (--_enabled == 0) {\n-      deferred.swap(&_deferred);\n-    }\n-  }\n-\n-  XArrayIterator<ItemT*> iter(&deferred);\n-  for (ItemT* item; iter.next(&item);) {\n-    immediate_delete(item);\n-  }\n-}\n-\n-template <typename T>\n-void XSafeDeleteImpl<T>::operator()(ItemT* item) {\n-  if (!deferred_delete(item)) {\n-    immediate_delete(item);\n-  }\n-}\n-\n-template <typename T>\n-XSafeDelete<T>::XSafeDelete() :\n-    XSafeDeleteImpl<T>(&_lock),\n-    _lock() {}\n-\n-template <typename T>\n-XSafeDeleteNoLock<T>::XSafeDeleteNoLock() :\n-    XSafeDeleteImpl<T>(nullptr) {}\n-\n-#endif \/\/ SHARE_GC_X_XSAFEDELETE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xSafeDelete.inline.hpp","additions":0,"deletions":100,"binary":false,"changes":100,"status":"deleted"},{"patch":"@@ -1,177 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/generationCounters.hpp\"\n-#include \"gc\/shared\/hSpaceCounters.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xServiceability.hpp\"\n-#include \"memory\/metaspaceCounters.hpp\"\n-#include \"runtime\/perfData.hpp\"\n-\n-class XGenerationCounters : public GenerationCounters {\n-public:\n-  XGenerationCounters(const char* name, int ordinal, int spaces,\n-                      size_t min_capacity, size_t max_capacity, size_t curr_capacity) :\n-      GenerationCounters(name, ordinal, spaces,\n-                         min_capacity, max_capacity, curr_capacity) {}\n-\n-  void update_capacity(size_t capacity) {\n-    _current_size->set_value(capacity);\n-  }\n-};\n-\n-\/\/ Class to expose perf counters used by jstat.\n-class XServiceabilityCounters : public CHeapObj<mtGC> {\n-private:\n-  XGenerationCounters _generation_counters;\n-  HSpaceCounters      _space_counters;\n-  CollectorCounters   _collector_counters;\n-\n-public:\n-  XServiceabilityCounters(size_t min_capacity, size_t max_capacity);\n-\n-  CollectorCounters* collector_counters();\n-\n-  void update_sizes();\n-};\n-\n-XServiceabilityCounters::XServiceabilityCounters(size_t min_capacity, size_t max_capacity) :\n-    \/\/ generation.1\n-    _generation_counters(\"old\"        \/* name *\/,\n-                         1            \/* ordinal *\/,\n-                         1            \/* spaces *\/,\n-                         min_capacity \/* min_capacity *\/,\n-                         max_capacity \/* max_capacity *\/,\n-                         min_capacity \/* curr_capacity *\/),\n-    \/\/ generation.1.space.0\n-    _space_counters(_generation_counters.name_space(),\n-                    \"space\"      \/* name *\/,\n-                    0            \/* ordinal *\/,\n-                    max_capacity \/* max_capacity *\/,\n-                    min_capacity \/* init_capacity *\/),\n-    \/\/ gc.collector.2\n-    _collector_counters(\"Z concurrent cycle pauses\" \/* name *\/,\n-                        2                           \/* ordinal *\/) {}\n-\n-CollectorCounters* XServiceabilityCounters::collector_counters() {\n-  return &_collector_counters;\n-}\n-\n-void XServiceabilityCounters::update_sizes() {\n-  if (UsePerfData) {\n-    const size_t capacity = XHeap::heap()->capacity();\n-    const size_t used = MIN2(XHeap::heap()->used(), capacity);\n-\n-    _generation_counters.update_capacity(capacity);\n-    _space_counters.update_capacity(capacity);\n-    _space_counters.update_used(used);\n-\n-    MetaspaceCounters::update_performance_counters();\n-  }\n-}\n-\n-XServiceabilityMemoryPool::XServiceabilityMemoryPool(size_t min_capacity, size_t max_capacity) :\n-    CollectedMemoryPool(\"ZHeap\",\n-                        min_capacity,\n-                        max_capacity,\n-                        true \/* support_usage_threshold *\/) {}\n-\n-size_t XServiceabilityMemoryPool::used_in_bytes() {\n-  return XHeap::heap()->used();\n-}\n-\n-MemoryUsage XServiceabilityMemoryPool::get_memory_usage() {\n-  const size_t committed = XHeap::heap()->capacity();\n-  const size_t used      = MIN2(XHeap::heap()->used(), committed);\n-\n-  return MemoryUsage(initial_size(), used, committed, max_size());\n-}\n-\n-XServiceabilityMemoryManager::XServiceabilityMemoryManager(const char* name,\n-                                                           XServiceabilityMemoryPool* pool) :\n-    GCMemoryManager(name) {\n-  add_pool(pool);\n-}\n-\n-XServiceability::XServiceability(size_t min_capacity, size_t max_capacity) :\n-    _min_capacity(min_capacity),\n-    _max_capacity(max_capacity),\n-    _memory_pool(_min_capacity, _max_capacity),\n-    _cycle_memory_manager(\"ZGC Cycles\", &_memory_pool),\n-    _pause_memory_manager(\"ZGC Pauses\", &_memory_pool),\n-    _counters(nullptr) {}\n-\n-void XServiceability::initialize() {\n-  _counters = new XServiceabilityCounters(_min_capacity, _max_capacity);\n-}\n-\n-MemoryPool* XServiceability::memory_pool() {\n-  return &_memory_pool;\n-}\n-\n-GCMemoryManager* XServiceability::cycle_memory_manager() {\n-  return &_cycle_memory_manager;\n-}\n-\n-GCMemoryManager* XServiceability::pause_memory_manager() {\n-  return &_pause_memory_manager;\n-}\n-\n-XServiceabilityCounters* XServiceability::counters() {\n-  return _counters;\n-}\n-\n-XServiceabilityCycleTracer::XServiceabilityCycleTracer() :\n-    _memory_manager_stats(XHeap::heap()->serviceability_cycle_memory_manager(),\n-                          XCollectedHeap::heap()->gc_cause(),\n-                          \"end of GC cycle\",\n-                          true  \/* allMemoryPoolsAffected *\/,\n-                          true  \/* recordGCBeginTime *\/,\n-                          true  \/* recordPreGCUsage *\/,\n-                          true  \/* recordPeakUsage *\/,\n-                          true  \/* recordPostGCUsage *\/,\n-                          true  \/* recordAccumulatedGCTime *\/,\n-                          true  \/* recordGCEndTime *\/,\n-                          true  \/* countCollection *\/) {}\n-\n-XServiceabilityPauseTracer::XServiceabilityPauseTracer() :\n-    _svc_gc_marker(SvcGCMarker::CONCURRENT),\n-    _counters_stats(XHeap::heap()->serviceability_counters()->collector_counters()),\n-    _memory_manager_stats(XHeap::heap()->serviceability_pause_memory_manager(),\n-                          XCollectedHeap::heap()->gc_cause(),\n-                          \"end of GC pause\",\n-                          true  \/* allMemoryPoolsAffected *\/,\n-                          true  \/* recordGCBeginTime *\/,\n-                          false \/* recordPreGCUsage *\/,\n-                          false \/* recordPeakUsage *\/,\n-                          false \/* recordPostGCUsage *\/,\n-                          true  \/* recordAccumulatedGCTime *\/,\n-                          true  \/* recordGCEndTime *\/,\n-                          true  \/* countCollection *\/) {}\n-\n-XServiceabilityPauseTracer::~XServiceabilityPauseTracer()  {\n-  XHeap::heap()->serviceability_counters()->update_sizes();\n-  MemoryService::track_memory_usage();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xServiceability.cpp","additions":0,"deletions":177,"binary":false,"changes":177,"status":"deleted"},{"patch":"@@ -1,89 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XSERVICEABILITY_HPP\n-#define SHARE_GC_X_XSERVICEABILITY_HPP\n-\n-#include \"gc\/shared\/collectorCounters.hpp\"\n-#include \"gc\/shared\/gcVMOperations.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"services\/memoryManager.hpp\"\n-#include \"services\/memoryPool.hpp\"\n-#include \"services\/memoryService.hpp\"\n-\n-class XServiceabilityCounters;\n-\n-class XServiceabilityMemoryPool : public CollectedMemoryPool {\n-public:\n-  XServiceabilityMemoryPool(size_t min_capacity, size_t max_capacity);\n-\n-  virtual size_t used_in_bytes();\n-  virtual MemoryUsage get_memory_usage();\n-};\n-\n-class XServiceabilityMemoryManager : public GCMemoryManager {\n-public:\n-  XServiceabilityMemoryManager(const char* name,\n-                               XServiceabilityMemoryPool* pool);\n-};\n-\n-class XServiceability {\n-private:\n-  const size_t                 _min_capacity;\n-  const size_t                 _max_capacity;\n-  XServiceabilityMemoryPool    _memory_pool;\n-  XServiceabilityMemoryManager _cycle_memory_manager;\n-  XServiceabilityMemoryManager _pause_memory_manager;\n-  XServiceabilityCounters*     _counters;\n-\n-public:\n-  XServiceability(size_t min_capacity, size_t max_capacity);\n-\n-  void initialize();\n-\n-  MemoryPool* memory_pool();\n-  GCMemoryManager* cycle_memory_manager();\n-  GCMemoryManager* pause_memory_manager();\n-  XServiceabilityCounters* counters();\n-};\n-\n-class XServiceabilityCycleTracer : public StackObj {\n-private:\n-  TraceMemoryManagerStats _memory_manager_stats;\n-\n-public:\n-  XServiceabilityCycleTracer();\n-};\n-\n-class XServiceabilityPauseTracer : public StackObj {\n-private:\n-  SvcGCMarker             _svc_gc_marker;\n-  TraceCollectorStats     _counters_stats;\n-  TraceMemoryManagerStats _memory_manager_stats;\n-\n-public:\n-  XServiceabilityPauseTracer();\n-  ~XServiceabilityPauseTracer();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XSERVICEABILITY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xServiceability.hpp","additions":0,"deletions":89,"binary":false,"changes":89,"status":"deleted"},{"patch":"@@ -1,96 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xStackWatermark.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n-#include \"gc\/x\/xVerify.hpp\"\n-#include \"memory\/resourceArea.inline.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"utilities\/preserveException.hpp\"\n-\n-XOnStackNMethodClosure::XOnStackNMethodClosure() :\n-    _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()) {}\n-\n-void XOnStackNMethodClosure::do_nmethod(nmethod* nm) {\n-  const bool result = _bs_nm->nmethod_entry_barrier(nm);\n-  assert(result, \"NMethod on-stack must be alive\");\n-}\n-\n-ThreadLocalAllocStats& XStackWatermark::stats() {\n-  return _stats;\n-}\n-\n-uint32_t XStackWatermark::epoch_id() const {\n-  return *XAddressBadMaskHighOrderBitsAddr;\n-}\n-\n-XStackWatermark::XStackWatermark(JavaThread* jt) :\n-    StackWatermark(jt, StackWatermarkKind::gc, *XAddressBadMaskHighOrderBitsAddr),\n-    _jt_cl(),\n-    _nm_cl(),\n-    _stats() {}\n-\n-OopClosure* XStackWatermark::closure_from_context(void* context) {\n-  if (context != nullptr) {\n-    assert(XThread::is_worker(), \"Unexpected thread passing in context: \" PTR_FORMAT, p2i(context));\n-    return reinterpret_cast<OopClosure*>(context);\n-  } else {\n-    return &_jt_cl;\n-  }\n-}\n-\n-void XStackWatermark::start_processing_impl(void* context) {\n-  \/\/ Verify the head (no_frames) of the thread is bad before fixing it.\n-  XVerify::verify_thread_head_bad(_jt);\n-\n-  \/\/ Process the non-frame part of the thread\n-  _jt->oops_do_no_frames(closure_from_context(context), &_nm_cl);\n-  XThreadLocalData::do_invisible_root(_jt, XBarrier::load_barrier_on_invisible_root_oop_field);\n-\n-  \/\/ Verification of frames is done after processing of the \"head\" (no_frames).\n-  \/\/ The reason is that the exception oop is fiddled with during frame processing.\n-  XVerify::verify_thread_frames_bad(_jt);\n-\n-  \/\/ Update thread local address bad mask\n-  XThreadLocalData::set_address_bad_mask(_jt, XAddressBadMask);\n-\n-  \/\/ Retire TLAB\n-  if (XGlobalPhase == XPhaseMark) {\n-    XThreadLocalAllocBuffer::retire(_jt, &_stats);\n-  } else {\n-    XThreadLocalAllocBuffer::remap(_jt);\n-  }\n-\n-  \/\/ Publishes the processing start to concurrent threads\n-  StackWatermark::start_processing_impl(context);\n-}\n-\n-void XStackWatermark::process(const frame& fr, RegisterMap& register_map, void* context) {\n-  XVerify::verify_frame_bad(fr, register_map);\n-  fr.oops_do(closure_from_context(context), &_nm_cl, &register_map, DerivedPointerIterationMode::_directly);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xStackWatermark.cpp","additions":0,"deletions":96,"binary":false,"changes":96,"status":"deleted"},{"patch":"@@ -1,68 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XSTACKWATERMARK_HPP\n-#define SHARE_GC_X_XSTACKWATERMARK_HPP\n-\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"gc\/shared\/threadLocalAllocBuffer.hpp\"\n-#include \"gc\/x\/xBarrier.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"oops\/oopsHierarchy.hpp\"\n-#include \"runtime\/stackWatermark.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class frame;\n-class JavaThread;\n-\n-class XOnStackNMethodClosure : public NMethodClosure {\n-private:\n-  BarrierSetNMethod* _bs_nm;\n-\n-  virtual void do_nmethod(nmethod* nm);\n-\n-public:\n-  XOnStackNMethodClosure();\n-};\n-\n-class XStackWatermark : public StackWatermark {\n-private:\n-  XLoadBarrierOopClosure _jt_cl;\n-  XOnStackNMethodClosure _nm_cl;\n-  ThreadLocalAllocStats  _stats;\n-\n-  OopClosure* closure_from_context(void* context);\n-\n-  virtual uint32_t epoch_id() const;\n-  virtual void start_processing_impl(void* context);\n-  virtual void process(const frame& fr, RegisterMap& register_map, void* context);\n-\n-public:\n-  XStackWatermark(JavaThread* jt);\n-\n-  ThreadLocalAllocStats& stats();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XSTACKWATERMARK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xStackWatermark.hpp","additions":0,"deletions":68,"binary":false,"changes":68,"status":"deleted"},{"patch":"@@ -1,1513 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xAbort.inline.hpp\"\n-#include \"gc\/x\/xCollectedHeap.hpp\"\n-#include \"gc\/x\/xCPU.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xNMethodTable.hpp\"\n-#include \"gc\/x\/xPageAllocator.inline.hpp\"\n-#include \"gc\/x\/xRelocationSetSelector.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xTracer.inline.hpp\"\n-#include \"gc\/x\/xUtils.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/timer.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/ticks.hpp\"\n-\n-#define XSIZE_FMT                       SIZE_FORMAT \"M(%.0f%%)\"\n-#define XSIZE_ARGS_WITH_MAX(size, max)  ((size) \/ M), (percent_of(size, max))\n-#define XSIZE_ARGS(size)                XSIZE_ARGS_WITH_MAX(size, XStatHeap::max_capacity())\n-\n-#define XTABLE_ARGS_NA                  \"%9s\", \"-\"\n-#define XTABLE_ARGS(size)               SIZE_FORMAT_W(8) \"M (%.0f%%)\", \\\n-                                        ((size) \/ M), (percent_of(size, XStatHeap::max_capacity()))\n-\n-\/\/\n-\/\/ Stat sampler\/counter data\n-\/\/\n-struct XStatSamplerData {\n-  uint64_t _nsamples;\n-  uint64_t _sum;\n-  uint64_t _max;\n-\n-  XStatSamplerData() :\n-    _nsamples(0),\n-    _sum(0),\n-    _max(0) {}\n-\n-  void add(const XStatSamplerData& new_sample) {\n-    _nsamples += new_sample._nsamples;\n-    _sum += new_sample._sum;\n-    _max = MAX2(_max, new_sample._max);\n-  }\n-};\n-\n-struct XStatCounterData {\n-  uint64_t _counter;\n-\n-  XStatCounterData() :\n-    _counter(0) {}\n-};\n-\n-\/\/\n-\/\/ Stat sampler history\n-\/\/\n-template <size_t size>\n-class XStatSamplerHistoryInterval {\n-private:\n-  size_t           _next;\n-  XStatSamplerData _samples[size];\n-  XStatSamplerData _accumulated;\n-  XStatSamplerData _total;\n-\n-public:\n-  XStatSamplerHistoryInterval() :\n-      _next(0),\n-      _samples(),\n-      _accumulated(),\n-      _total() {}\n-\n-  bool add(const XStatSamplerData& new_sample) {\n-    \/\/ Insert sample\n-    const XStatSamplerData old_sample = _samples[_next];\n-    _samples[_next] = new_sample;\n-\n-    \/\/ Adjust accumulated\n-    _accumulated._nsamples += new_sample._nsamples;\n-    _accumulated._sum += new_sample._sum;\n-    _accumulated._max = MAX2(_accumulated._max, new_sample._max);\n-\n-    \/\/ Adjust total\n-    _total._nsamples -= old_sample._nsamples;\n-    _total._sum -= old_sample._sum;\n-    _total._nsamples += new_sample._nsamples;\n-    _total._sum += new_sample._sum;\n-    if (_total._max < new_sample._max) {\n-      \/\/ Found new max\n-      _total._max = new_sample._max;\n-    } else if (_total._max == old_sample._max) {\n-      \/\/ Removed old max, reset and find new max\n-      _total._max = 0;\n-      for (size_t i = 0; i < size; i++) {\n-        if (_total._max < _samples[i]._max) {\n-          _total._max = _samples[i]._max;\n-        }\n-      }\n-    }\n-\n-    \/\/ Adjust next\n-    if (++_next == size) {\n-      _next = 0;\n-\n-      \/\/ Clear accumulated\n-      const XStatSamplerData zero;\n-      _accumulated = zero;\n-\n-      \/\/ Became full\n-      return true;\n-    }\n-\n-    \/\/ Not yet full\n-    return false;\n-  }\n-\n-  const XStatSamplerData& total() const {\n-    return _total;\n-  }\n-\n-  const XStatSamplerData& accumulated() const {\n-    return _accumulated;\n-  }\n-};\n-\n-class XStatSamplerHistory : public CHeapObj<mtGC> {\n-private:\n-  XStatSamplerHistoryInterval<10> _10seconds;\n-  XStatSamplerHistoryInterval<60> _10minutes;\n-  XStatSamplerHistoryInterval<60> _10hours;\n-  XStatSamplerData                _total;\n-\n-  uint64_t avg(uint64_t sum, uint64_t nsamples) const {\n-    return (nsamples > 0) ? sum \/ nsamples : 0;\n-  }\n-\n-public:\n-  XStatSamplerHistory() :\n-      _10seconds(),\n-      _10minutes(),\n-      _10hours(),\n-      _total() {}\n-\n-  void add(const XStatSamplerData& new_sample) {\n-    if (_10seconds.add(new_sample)) {\n-      if (_10minutes.add(_10seconds.total())) {\n-        if (_10hours.add(_10minutes.total())) {\n-          _total.add(_10hours.total());\n-        }\n-      }\n-    }\n-  }\n-\n-  uint64_t avg_10_seconds() const {\n-    const uint64_t sum      = _10seconds.total()._sum;\n-    const uint64_t nsamples = _10seconds.total()._nsamples;\n-    return avg(sum, nsamples);\n-  }\n-\n-  uint64_t avg_10_minutes() const {\n-    const uint64_t sum      = _10seconds.accumulated()._sum +\n-                              _10minutes.total()._sum;\n-    const uint64_t nsamples = _10seconds.accumulated()._nsamples +\n-                              _10minutes.total()._nsamples;\n-    return avg(sum, nsamples);\n-  }\n-\n-  uint64_t avg_10_hours() const {\n-    const uint64_t sum      = _10seconds.accumulated()._sum +\n-                              _10minutes.accumulated()._sum +\n-                              _10hours.total()._sum;\n-    const uint64_t nsamples = _10seconds.accumulated()._nsamples +\n-                              _10minutes.accumulated()._nsamples +\n-                              _10hours.total()._nsamples;\n-    return avg(sum, nsamples);\n-  }\n-\n-  uint64_t avg_total() const {\n-    const uint64_t sum      = _10seconds.accumulated()._sum +\n-                              _10minutes.accumulated()._sum +\n-                              _10hours.accumulated()._sum +\n-                              _total._sum;\n-    const uint64_t nsamples = _10seconds.accumulated()._nsamples +\n-                              _10minutes.accumulated()._nsamples +\n-                              _10hours.accumulated()._nsamples +\n-                              _total._nsamples;\n-    return avg(sum, nsamples);\n-  }\n-\n-  uint64_t max_10_seconds() const {\n-    return _10seconds.total()._max;\n-  }\n-\n-  uint64_t max_10_minutes() const {\n-    return MAX2(_10seconds.accumulated()._max,\n-                _10minutes.total()._max);\n-  }\n-\n-  uint64_t max_10_hours() const {\n-    return MAX3(_10seconds.accumulated()._max,\n-                _10minutes.accumulated()._max,\n-                _10hours.total()._max);\n-  }\n-\n-  uint64_t max_total() const {\n-    return MAX4(_10seconds.accumulated()._max,\n-                _10minutes.accumulated()._max,\n-                _10hours.accumulated()._max,\n-                _total._max);\n-  }\n-};\n-\n-\/\/\n-\/\/ Stat unit printers\n-\/\/\n-void XStatUnitTime(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n-  log.print(\" %10s: %-41s \"\n-            \"%9.3f \/ %-9.3f \"\n-            \"%9.3f \/ %-9.3f \"\n-            \"%9.3f \/ %-9.3f \"\n-            \"%9.3f \/ %-9.3f   ms\",\n-            sampler.group(),\n-            sampler.name(),\n-            TimeHelper::counter_to_millis(history.avg_10_seconds()),\n-            TimeHelper::counter_to_millis(history.max_10_seconds()),\n-            TimeHelper::counter_to_millis(history.avg_10_minutes()),\n-            TimeHelper::counter_to_millis(history.max_10_minutes()),\n-            TimeHelper::counter_to_millis(history.avg_10_hours()),\n-            TimeHelper::counter_to_millis(history.max_10_hours()),\n-            TimeHelper::counter_to_millis(history.avg_total()),\n-            TimeHelper::counter_to_millis(history.max_total()));\n-}\n-\n-void XStatUnitBytes(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n-  log.print(\" %10s: %-41s \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   MB\",\n-            sampler.group(),\n-            sampler.name(),\n-            history.avg_10_seconds() \/ M,\n-            history.max_10_seconds() \/ M,\n-            history.avg_10_minutes() \/ M,\n-            history.max_10_minutes() \/ M,\n-            history.avg_10_hours() \/ M,\n-            history.max_10_hours() \/ M,\n-            history.avg_total() \/ M,\n-            history.max_total() \/ M);\n-}\n-\n-void XStatUnitThreads(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n-  log.print(\" %10s: %-41s \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   threads\",\n-            sampler.group(),\n-            sampler.name(),\n-            history.avg_10_seconds(),\n-            history.max_10_seconds(),\n-            history.avg_10_minutes(),\n-            history.max_10_minutes(),\n-            history.avg_10_hours(),\n-            history.max_10_hours(),\n-            history.avg_total(),\n-            history.max_total());\n-}\n-\n-void XStatUnitBytesPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n-  log.print(\" %10s: %-41s \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   MB\/s\",\n-            sampler.group(),\n-            sampler.name(),\n-            history.avg_10_seconds() \/ M,\n-            history.max_10_seconds() \/ M,\n-            history.avg_10_minutes() \/ M,\n-            history.max_10_minutes() \/ M,\n-            history.avg_10_hours() \/ M,\n-            history.max_10_hours() \/ M,\n-            history.avg_total() \/ M,\n-            history.max_total() \/ M);\n-}\n-\n-void XStatUnitOpsPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history) {\n-  log.print(\" %10s: %-41s \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \" \"\n-            UINT64_FORMAT_W(9) \" \/ \" UINT64_FORMAT_W(-9) \"   ops\/s\",\n-            sampler.group(),\n-            sampler.name(),\n-            history.avg_10_seconds(),\n-            history.max_10_seconds(),\n-            history.avg_10_minutes(),\n-            history.max_10_minutes(),\n-            history.avg_10_hours(),\n-            history.max_10_hours(),\n-            history.avg_total(),\n-            history.max_total());\n-}\n-\n-\/\/\n-\/\/ Stat value\n-\/\/\n-uintptr_t XStatValue::_base = 0;\n-uint32_t  XStatValue::_cpu_offset = 0;\n-\n-XStatValue::XStatValue(const char* group,\n-                          const char* name,\n-                          uint32_t id,\n-                          uint32_t size) :\n-    _group(group),\n-    _name(name),\n-    _id(id),\n-    _offset(_cpu_offset) {\n-  assert(_base == 0, \"Already initialized\");\n-  _cpu_offset += size;\n-}\n-\n-template <typename T>\n-T* XStatValue::get_cpu_local(uint32_t cpu) const {\n-  assert(_base != 0, \"Not initialized\");\n-  const uintptr_t cpu_base = _base + (_cpu_offset * cpu);\n-  const uintptr_t value_addr = cpu_base + _offset;\n-  return (T*)value_addr;\n-}\n-\n-void XStatValue::initialize() {\n-  \/\/ Finalize and align CPU offset\n-  _cpu_offset = align_up(_cpu_offset, (uint32_t)XCacheLineSize);\n-\n-  \/\/ Allocation aligned memory\n-  const size_t size = _cpu_offset * XCPU::count();\n-  _base = XUtils::alloc_aligned(XCacheLineSize, size);\n-}\n-\n-const char* XStatValue::group() const {\n-  return _group;\n-}\n-\n-const char* XStatValue::name() const {\n-  return _name;\n-}\n-\n-uint32_t XStatValue::id() const {\n-  return _id;\n-}\n-\n-\/\/\n-\/\/ Stat iterable value\n-\/\/\n-\n-template <typename T>\n-XStatIterableValue<T>::XStatIterableValue(const char* group,\n-                                          const char* name,\n-                                          uint32_t size) :\n-    XStatValue(group, name, _count++, size),\n-    _next(insert()) {}\n-\n-template <typename T>\n-T* XStatIterableValue<T>::insert() const {\n-  T* const next = _first;\n-  _first = (T*)this;\n-  return next;\n-}\n-\n-template <typename T>\n-void XStatIterableValue<T>::sort() {\n-  T* first_unsorted = _first;\n-  _first = nullptr;\n-\n-  while (first_unsorted != nullptr) {\n-    T* const value = first_unsorted;\n-    first_unsorted = value->_next;\n-    value->_next = nullptr;\n-\n-    T** current = &_first;\n-\n-    while (*current != nullptr) {\n-      \/\/ First sort by group, then by name\n-      const int group_cmp = strcmp((*current)->group(), value->group());\n-      if ((group_cmp > 0) || (group_cmp == 0 && strcmp((*current)->name(), value->name()) > 0)) {\n-        break;\n-      }\n-\n-      current = &(*current)->_next;\n-    }\n-    value->_next = *current;\n-    *current = value;\n-  }\n-}\n-\n-\/\/\n-\/\/ Stat sampler\n-\/\/\n-XStatSampler::XStatSampler(const char* group, const char* name, XStatUnitPrinter printer) :\n-    XStatIterableValue<XStatSampler>(group, name, sizeof(XStatSamplerData)),\n-    _printer(printer) {}\n-\n-XStatSamplerData* XStatSampler::get() const {\n-  return get_cpu_local<XStatSamplerData>(XCPU::id());\n-}\n-\n-XStatSamplerData XStatSampler::collect_and_reset() const {\n-  XStatSamplerData all;\n-\n-  const uint32_t ncpus = XCPU::count();\n-  for (uint32_t i = 0; i < ncpus; i++) {\n-    XStatSamplerData* const cpu_data = get_cpu_local<XStatSamplerData>(i);\n-    if (cpu_data->_nsamples > 0) {\n-      const uint64_t nsamples = Atomic::xchg(&cpu_data->_nsamples, (uint64_t)0);\n-      const uint64_t sum = Atomic::xchg(&cpu_data->_sum, (uint64_t)0);\n-      const uint64_t max = Atomic::xchg(&cpu_data->_max, (uint64_t)0);\n-      all._nsamples += nsamples;\n-      all._sum += sum;\n-      if (all._max < max) {\n-        all._max = max;\n-      }\n-    }\n-  }\n-\n-  return all;\n-}\n-\n-XStatUnitPrinter XStatSampler::printer() const {\n-  return _printer;\n-}\n-\n-\/\/\n-\/\/ Stat counter\n-\/\/\n-XStatCounter::XStatCounter(const char* group, const char* name, XStatUnitPrinter printer) :\n-    XStatIterableValue<XStatCounter>(group, name, sizeof(XStatCounterData)),\n-    _sampler(group, name, printer) {}\n-\n-XStatCounterData* XStatCounter::get() const {\n-  return get_cpu_local<XStatCounterData>(XCPU::id());\n-}\n-\n-void XStatCounter::sample_and_reset() const {\n-  uint64_t counter = 0;\n-\n-  const uint32_t ncpus = XCPU::count();\n-  for (uint32_t i = 0; i < ncpus; i++) {\n-    XStatCounterData* const cpu_data = get_cpu_local<XStatCounterData>(i);\n-    counter += Atomic::xchg(&cpu_data->_counter, (uint64_t)0);\n-  }\n-\n-  XStatSample(_sampler, counter);\n-}\n-\n-\/\/\n-\/\/ Stat unsampled counter\n-\/\/\n-XStatUnsampledCounter::XStatUnsampledCounter(const char* name) :\n-    XStatIterableValue<XStatUnsampledCounter>(\"Unsampled\", name, sizeof(XStatCounterData)) {}\n-\n-XStatCounterData* XStatUnsampledCounter::get() const {\n-  return get_cpu_local<XStatCounterData>(XCPU::id());\n-}\n-\n-XStatCounterData XStatUnsampledCounter::collect_and_reset() const {\n-  XStatCounterData all;\n-\n-  const uint32_t ncpus = XCPU::count();\n-  for (uint32_t i = 0; i < ncpus; i++) {\n-    XStatCounterData* const cpu_data = get_cpu_local<XStatCounterData>(i);\n-    all._counter += Atomic::xchg(&cpu_data->_counter, (uint64_t)0);\n-  }\n-\n-  return all;\n-}\n-\n-\/\/\n-\/\/ Stat MMU (Minimum Mutator Utilization)\n-\/\/\n-XStatMMUPause::XStatMMUPause() :\n-    _start(0.0),\n-    _end(0.0) {}\n-\n-XStatMMUPause::XStatMMUPause(const Ticks& start, const Ticks& end) :\n-    _start(TimeHelper::counter_to_millis(start.value())),\n-    _end(TimeHelper::counter_to_millis(end.value())) {}\n-\n-double XStatMMUPause::end() const {\n-  return _end;\n-}\n-\n-double XStatMMUPause::overlap(double start, double end) const {\n-  const double start_max = MAX2(start, _start);\n-  const double end_min = MIN2(end, _end);\n-\n-  if (end_min > start_max) {\n-    \/\/ Overlap found\n-    return end_min - start_max;\n-  }\n-\n-  \/\/ No overlap\n-  return 0.0;\n-}\n-\n-size_t XStatMMU::_next = 0;\n-size_t XStatMMU::_npauses = 0;\n-XStatMMUPause XStatMMU::_pauses[200];\n-double XStatMMU::_mmu_2ms = 100.0;\n-double XStatMMU::_mmu_5ms = 100.0;\n-double XStatMMU::_mmu_10ms = 100.0;\n-double XStatMMU::_mmu_20ms = 100.0;\n-double XStatMMU::_mmu_50ms = 100.0;\n-double XStatMMU::_mmu_100ms = 100.0;\n-\n-const XStatMMUPause& XStatMMU::pause(size_t index) {\n-  return _pauses[(_next - index - 1) % ARRAY_SIZE(_pauses)];\n-}\n-\n-double XStatMMU::calculate_mmu(double time_slice) {\n-  const double end = pause(0).end();\n-  const double start = end - time_slice;\n-  double time_paused = 0.0;\n-\n-  \/\/ Find all overlapping pauses\n-  for (size_t i = 0; i < _npauses; i++) {\n-    const double overlap = pause(i).overlap(start, end);\n-    if (overlap == 0.0) {\n-      \/\/ No overlap\n-      break;\n-    }\n-\n-    time_paused += overlap;\n-  }\n-\n-  \/\/ Calculate MMU\n-  const double time_mutator = time_slice - time_paused;\n-  return percent_of(time_mutator, time_slice);\n-}\n-\n-void XStatMMU::register_pause(const Ticks& start, const Ticks& end) {\n-  \/\/ Add pause\n-  const size_t index = _next++ % ARRAY_SIZE(_pauses);\n-  _pauses[index] = XStatMMUPause(start, end);\n-  _npauses = MIN2(_npauses + 1, ARRAY_SIZE(_pauses));\n-\n-  \/\/ Recalculate MMUs\n-  _mmu_2ms    = MIN2(_mmu_2ms,   calculate_mmu(2));\n-  _mmu_5ms    = MIN2(_mmu_5ms,   calculate_mmu(5));\n-  _mmu_10ms   = MIN2(_mmu_10ms,  calculate_mmu(10));\n-  _mmu_20ms   = MIN2(_mmu_20ms,  calculate_mmu(20));\n-  _mmu_50ms   = MIN2(_mmu_50ms,  calculate_mmu(50));\n-  _mmu_100ms  = MIN2(_mmu_100ms, calculate_mmu(100));\n-}\n-\n-void XStatMMU::print() {\n-  log_info(gc, mmu)(\"MMU: 2ms\/%.1f%%, 5ms\/%.1f%%, 10ms\/%.1f%%, 20ms\/%.1f%%, 50ms\/%.1f%%, 100ms\/%.1f%%\",\n-                    _mmu_2ms, _mmu_5ms, _mmu_10ms, _mmu_20ms, _mmu_50ms, _mmu_100ms);\n-}\n-\n-\/\/\n-\/\/ Stat phases\n-\/\/\n-ConcurrentGCTimer XStatPhase::_timer;\n-\n-XStatPhase::XStatPhase(const char* group, const char* name) :\n-    _sampler(group, name, XStatUnitTime) {}\n-\n-void XStatPhase::log_start(LogTargetHandle log, bool thread) const {\n-  if (!log.is_enabled()) {\n-    return;\n-  }\n-\n-  if (thread) {\n-    ResourceMark rm;\n-    log.print(\"%s (%s)\", name(), Thread::current()->name());\n-  } else {\n-    log.print(\"%s\", name());\n-  }\n-}\n-\n-void XStatPhase::log_end(LogTargetHandle log, const Tickspan& duration, bool thread) const {\n-  if (!log.is_enabled()) {\n-    return;\n-  }\n-\n-  if (thread) {\n-    ResourceMark rm;\n-    log.print(\"%s (%s) %.3fms\", name(), Thread::current()->name(), TimeHelper::counter_to_millis(duration.value()));\n-  } else {\n-    log.print(\"%s %.3fms\", name(), TimeHelper::counter_to_millis(duration.value()));\n-  }\n-}\n-\n-ConcurrentGCTimer* XStatPhase::timer() {\n-  return &_timer;\n-}\n-\n-const char* XStatPhase::name() const {\n-  return _sampler.name();\n-}\n-\n-XStatPhaseCycle::XStatPhaseCycle(const char* name) :\n-    XStatPhase(\"Collector\", name) {}\n-\n-void XStatPhaseCycle::register_start(const Ticks& start) const {\n-  timer()->register_gc_start(start);\n-\n-  XTracer::tracer()->report_gc_start(XCollectedHeap::heap()->gc_cause(), start);\n-\n-  XCollectedHeap::heap()->print_heap_before_gc();\n-  XCollectedHeap::heap()->trace_heap_before_gc(XTracer::tracer());\n-\n-  log_info(gc, start)(\"Garbage Collection (%s)\",\n-                       GCCause::to_string(XCollectedHeap::heap()->gc_cause()));\n-}\n-\n-void XStatPhaseCycle::register_end(const Ticks& start, const Ticks& end) const {\n-  if (XAbort::should_abort()) {\n-    log_info(gc)(\"Garbage Collection (%s) Aborted\",\n-                 GCCause::to_string(XCollectedHeap::heap()->gc_cause()));\n-    return;\n-  }\n-\n-  timer()->register_gc_end(end);\n-\n-  XCollectedHeap::heap()->print_heap_after_gc();\n-  XCollectedHeap::heap()->trace_heap_after_gc(XTracer::tracer());\n-\n-  XTracer::tracer()->report_gc_end(end, timer()->time_partitions());\n-\n-  const Tickspan duration = end - start;\n-  XStatSample(_sampler, duration.value());\n-\n-  XStatLoad::print();\n-  XStatMMU::print();\n-  XStatMark::print();\n-  XStatNMethods::print();\n-  XStatMetaspace::print();\n-  XStatReferences::print();\n-  XStatRelocation::print();\n-  XStatHeap::print();\n-\n-  log_info(gc)(\"Garbage Collection (%s) \" XSIZE_FMT \"->\" XSIZE_FMT,\n-               GCCause::to_string(XCollectedHeap::heap()->gc_cause()),\n-               XSIZE_ARGS(XStatHeap::used_at_mark_start()),\n-               XSIZE_ARGS(XStatHeap::used_at_relocate_end()));\n-}\n-\n-Tickspan XStatPhasePause::_max;\n-\n-XStatPhasePause::XStatPhasePause(const char* name) :\n-    XStatPhase(\"Phase\", name) {}\n-\n-const Tickspan& XStatPhasePause::max() {\n-  return _max;\n-}\n-\n-void XStatPhasePause::register_start(const Ticks& start) const {\n-  timer()->register_gc_pause_start(name(), start);\n-\n-  LogTarget(Debug, gc, phases, start) log;\n-  log_start(log);\n-}\n-\n-void XStatPhasePause::register_end(const Ticks& start, const Ticks& end) const {\n-  timer()->register_gc_pause_end(end);\n-\n-  const Tickspan duration = end - start;\n-  XStatSample(_sampler, duration.value());\n-\n-  \/\/ Track max pause time\n-  if (_max < duration) {\n-    _max = duration;\n-  }\n-\n-  \/\/ Track minimum mutator utilization\n-  XStatMMU::register_pause(start, end);\n-\n-  LogTarget(Info, gc, phases) log;\n-  log_end(log, duration);\n-}\n-\n-XStatPhaseConcurrent::XStatPhaseConcurrent(const char* name) :\n-    XStatPhase(\"Phase\", name) {}\n-\n-void XStatPhaseConcurrent::register_start(const Ticks& start) const {\n-  timer()->register_gc_concurrent_start(name(), start);\n-\n-  LogTarget(Debug, gc, phases, start) log;\n-  log_start(log);\n-}\n-\n-void XStatPhaseConcurrent::register_end(const Ticks& start, const Ticks& end) const {\n-  if (XAbort::should_abort()) {\n-    return;\n-  }\n-\n-  timer()->register_gc_concurrent_end(end);\n-\n-  const Tickspan duration = end - start;\n-  XStatSample(_sampler, duration.value());\n-\n-  LogTarget(Info, gc, phases) log;\n-  log_end(log, duration);\n-}\n-\n-XStatSubPhase::XStatSubPhase(const char* name) :\n-    XStatPhase(\"Subphase\", name) {}\n-\n-void XStatSubPhase::register_start(const Ticks& start) const {\n-  if (XThread::is_worker()) {\n-    LogTarget(Trace, gc, phases, start) log;\n-    log_start(log, true \/* thread *\/);\n-  } else {\n-    LogTarget(Debug, gc, phases, start) log;\n-    log_start(log, false \/* thread *\/);\n-  }\n-}\n-\n-void XStatSubPhase::register_end(const Ticks& start, const Ticks& end) const {\n-  if (XAbort::should_abort()) {\n-    return;\n-  }\n-\n-  XTracer::tracer()->report_thread_phase(name(), start, end);\n-\n-  const Tickspan duration = end - start;\n-  XStatSample(_sampler, duration.value());\n-\n-  if (XThread::is_worker()) {\n-    LogTarget(Trace, gc, phases) log;\n-    log_end(log, duration, true \/* thread *\/);\n-  } else {\n-    LogTarget(Debug, gc, phases) log;\n-    log_end(log, duration, false \/* thread *\/);\n-  }\n-}\n-\n-XStatCriticalPhase::XStatCriticalPhase(const char* name, bool verbose) :\n-    XStatPhase(\"Critical\", name),\n-    _counter(\"Critical\", name, XStatUnitOpsPerSecond),\n-    _verbose(verbose) {}\n-\n-void XStatCriticalPhase::register_start(const Ticks& start) const {\n-  \/\/ This is called from sensitive contexts, for example before an allocation stall\n-  \/\/ has been resolved. This means we must not access any oops in here since that\n-  \/\/ could lead to infinite recursion. Without access to the thread name we can't\n-  \/\/ really log anything useful here.\n-}\n-\n-void XStatCriticalPhase::register_end(const Ticks& start, const Ticks& end) const {\n-  XTracer::tracer()->report_thread_phase(name(), start, end);\n-\n-  const Tickspan duration = end - start;\n-  XStatSample(_sampler, duration.value());\n-  XStatInc(_counter);\n-\n-  if (_verbose) {\n-    LogTarget(Info, gc) log;\n-    log_end(log, duration, true \/* thread *\/);\n-  } else {\n-    LogTarget(Debug, gc) log;\n-    log_end(log, duration, true \/* thread *\/);\n-  }\n-}\n-\n-\/\/\n-\/\/ Stat timer\n-\/\/\n-THREAD_LOCAL uint32_t XStatTimerDisable::_active = 0;\n-\n-\/\/\n-\/\/ Stat sample\/inc\n-\/\/\n-void XStatSample(const XStatSampler& sampler, uint64_t value) {\n-  XStatSamplerData* const cpu_data = sampler.get();\n-  Atomic::add(&cpu_data->_nsamples, 1u);\n-  Atomic::add(&cpu_data->_sum, value);\n-\n-  uint64_t max = cpu_data->_max;\n-  for (;;) {\n-    if (max >= value) {\n-      \/\/ Not max\n-      break;\n-    }\n-\n-    const uint64_t new_max = value;\n-    const uint64_t prev_max = Atomic::cmpxchg(&cpu_data->_max, max, new_max);\n-    if (prev_max == max) {\n-      \/\/ Success\n-      break;\n-    }\n-\n-    \/\/ Retry\n-    max = prev_max;\n-  }\n-\n-  XTracer::tracer()->report_stat_sampler(sampler, value);\n-}\n-\n-void XStatInc(const XStatCounter& counter, uint64_t increment) {\n-  XStatCounterData* const cpu_data = counter.get();\n-  const uint64_t value = Atomic::add(&cpu_data->_counter, increment);\n-\n-  XTracer::tracer()->report_stat_counter(counter, increment, value);\n-}\n-\n-void XStatInc(const XStatUnsampledCounter& counter, uint64_t increment) {\n-  XStatCounterData* const cpu_data = counter.get();\n-  Atomic::add(&cpu_data->_counter, increment);\n-}\n-\n-\/\/\n-\/\/ Stat allocation rate\n-\/\/\n-const XStatUnsampledCounter XStatAllocRate::_counter(\"Allocation Rate\");\n-TruncatedSeq                XStatAllocRate::_samples(XStatAllocRate::sample_hz);\n-TruncatedSeq                XStatAllocRate::_rate(XStatAllocRate::sample_hz);\n-\n-const XStatUnsampledCounter& XStatAllocRate::counter() {\n-  return _counter;\n-}\n-\n-uint64_t XStatAllocRate::sample_and_reset() {\n-  const XStatCounterData bytes_per_sample = _counter.collect_and_reset();\n-  _samples.add(bytes_per_sample._counter);\n-\n-  const uint64_t bytes_per_second = _samples.sum();\n-  _rate.add(bytes_per_second);\n-\n-  return bytes_per_second;\n-}\n-\n-double XStatAllocRate::predict() {\n-  return _rate.predict_next();\n-}\n-\n-double XStatAllocRate::avg() {\n-  return _rate.avg();\n-}\n-\n-double XStatAllocRate::sd() {\n-  return _rate.sd();\n-}\n-\n-\/\/\n-\/\/ Stat thread\n-\/\/\n-XStat::XStat() :\n-    _metronome(sample_hz) {\n-  set_name(\"XStat\");\n-  create_and_start();\n-}\n-\n-void XStat::sample_and_collect(XStatSamplerHistory* history) const {\n-  \/\/ Sample counters\n-  for (const XStatCounter* counter = XStatCounter::first(); counter != nullptr; counter = counter->next()) {\n-    counter->sample_and_reset();\n-  }\n-\n-  \/\/ Collect samples\n-  for (const XStatSampler* sampler = XStatSampler::first(); sampler != nullptr; sampler = sampler->next()) {\n-    XStatSamplerHistory& sampler_history = history[sampler->id()];\n-    sampler_history.add(sampler->collect_and_reset());\n-  }\n-}\n-\n-bool XStat::should_print(LogTargetHandle log) const {\n-  static uint64_t print_at = ZStatisticsInterval;\n-  const uint64_t now = os::elapsedTime();\n-\n-  if (now < print_at) {\n-    return false;\n-  }\n-\n-  print_at = ((now \/ ZStatisticsInterval) * ZStatisticsInterval) + ZStatisticsInterval;\n-\n-  return log.is_enabled();\n-}\n-\n-void XStat::print(LogTargetHandle log, const XStatSamplerHistory* history) const {\n-  \/\/ Print\n-  log.print(\"=== Garbage Collection Statistics =======================================================================================================================\");\n-  log.print(\"                                                             Last 10s              Last 10m              Last 10h                Total\");\n-  log.print(\"                                                             Avg \/ Max             Avg \/ Max             Avg \/ Max             Avg \/ Max\");\n-\n-  for (const XStatSampler* sampler = XStatSampler::first(); sampler != nullptr; sampler = sampler->next()) {\n-    const XStatSamplerHistory& sampler_history = history[sampler->id()];\n-    const XStatUnitPrinter printer = sampler->printer();\n-    printer(log, *sampler, sampler_history);\n-  }\n-\n-  log.print(\"=========================================================================================================================================================\");\n-}\n-\n-void XStat::run_service() {\n-  XStatSamplerHistory* const history = new XStatSamplerHistory[XStatSampler::count()];\n-  LogTarget(Info, gc, stats) log;\n-\n-  XStatSampler::sort();\n-\n-  \/\/ Main loop\n-  while (_metronome.wait_for_tick()) {\n-    sample_and_collect(history);\n-    if (should_print(log)) {\n-      print(log, history);\n-    }\n-  }\n-\n-  delete [] history;\n-}\n-\n-void XStat::stop_service() {\n-  _metronome.stop();\n-}\n-\n-\/\/\n-\/\/ Stat table\n-\/\/\n-class XStatTablePrinter {\n-private:\n-  static const size_t _buffer_size = 256;\n-\n-  const size_t _column0_width;\n-  const size_t _columnN_width;\n-  char         _buffer[_buffer_size];\n-\n-public:\n-  class XColumn {\n-  private:\n-    char* const  _buffer;\n-    const size_t _position;\n-    const size_t _width;\n-    const size_t _width_next;\n-\n-    XColumn next() const {\n-      \/\/ Insert space between columns\n-      _buffer[_position + _width] = ' ';\n-      return XColumn(_buffer, _position + _width + 1, _width_next, _width_next);\n-    }\n-\n-    size_t print(size_t position, const char* fmt, va_list va) {\n-      const int res = jio_vsnprintf(_buffer + position, _buffer_size - position, fmt, va);\n-      if (res < 0) {\n-        return 0;\n-      }\n-\n-      return (size_t)res;\n-    }\n-\n-  public:\n-    XColumn(char* buffer, size_t position, size_t width, size_t width_next) :\n-        _buffer(buffer),\n-        _position(position),\n-        _width(width),\n-        _width_next(width_next) {}\n-\n-    XColumn left(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3) {\n-      va_list va;\n-\n-      va_start(va, fmt);\n-      const size_t written = print(_position, fmt, va);\n-      va_end(va);\n-\n-      if (written < _width) {\n-        \/\/ Fill empty space\n-        memset(_buffer + _position + written, ' ', _width - written);\n-      }\n-\n-      return next();\n-    }\n-\n-    XColumn right(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3) {\n-      va_list va;\n-\n-      va_start(va, fmt);\n-      const size_t written = print(_position, fmt, va);\n-      va_end(va);\n-\n-      if (written > _width) {\n-        \/\/ Line too long\n-        return fill('?');\n-      }\n-\n-      if (written < _width) {\n-        \/\/ Short line, move all to right\n-        memmove(_buffer + _position + _width - written, _buffer + _position, written);\n-\n-        \/\/ Fill empty space\n-        memset(_buffer + _position, ' ', _width - written);\n-      }\n-\n-      return next();\n-    }\n-\n-    XColumn center(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3) {\n-      va_list va;\n-\n-      va_start(va, fmt);\n-      const size_t written = print(_position, fmt, va);\n-      va_end(va);\n-\n-      if (written > _width) {\n-        \/\/ Line too long\n-        return fill('?');\n-      }\n-\n-      if (written < _width) {\n-        \/\/ Short line, move all to center\n-        const size_t start_space = (_width - written) \/ 2;\n-        const size_t end_space = _width - written - start_space;\n-        memmove(_buffer + _position + start_space, _buffer + _position, written);\n-\n-        \/\/ Fill empty spaces\n-        memset(_buffer + _position, ' ', start_space);\n-        memset(_buffer + _position + start_space + written, ' ', end_space);\n-      }\n-\n-      return next();\n-    }\n-\n-    XColumn fill(char filler = ' ') {\n-      memset(_buffer + _position, filler, _width);\n-      return next();\n-    }\n-\n-    const char* end() {\n-      _buffer[_position] = '\\0';\n-      return _buffer;\n-    }\n-  };\n-\n-public:\n-  XStatTablePrinter(size_t column0_width, size_t columnN_width) :\n-      _column0_width(column0_width),\n-      _columnN_width(columnN_width) {}\n-\n-  XColumn operator()() {\n-    return XColumn(_buffer, 0, _column0_width, _columnN_width);\n-  }\n-};\n-\n-\/\/\n-\/\/ Stat cycle\n-\/\/\n-uint64_t  XStatCycle::_nwarmup_cycles = 0;\n-Ticks     XStatCycle::_start_of_last;\n-Ticks     XStatCycle::_end_of_last;\n-NumberSeq XStatCycle::_serial_time(0.7 \/* alpha *\/);\n-NumberSeq XStatCycle::_parallelizable_time(0.7 \/* alpha *\/);\n-uint      XStatCycle::_last_active_workers = 0;\n-\n-void XStatCycle::at_start() {\n-  _start_of_last = Ticks::now();\n-}\n-\n-void XStatCycle::at_end(GCCause::Cause cause, uint active_workers) {\n-  _end_of_last = Ticks::now();\n-\n-  if (cause == GCCause::_z_warmup) {\n-    _nwarmup_cycles++;\n-  }\n-\n-  _last_active_workers = active_workers;\n-\n-  \/\/ Calculate serial and parallelizable GC cycle times\n-  const double duration = (_end_of_last - _start_of_last).seconds();\n-  const double workers_duration = XStatWorkers::get_and_reset_duration();\n-  const double serial_time = duration - workers_duration;\n-  const double parallelizable_time = workers_duration * active_workers;\n-  _serial_time.add(serial_time);\n-  _parallelizable_time.add(parallelizable_time);\n-}\n-\n-bool XStatCycle::is_warm() {\n-  return _nwarmup_cycles >= 3;\n-}\n-\n-uint64_t XStatCycle::nwarmup_cycles() {\n-  return _nwarmup_cycles;\n-}\n-\n-bool XStatCycle::is_time_trustable() {\n-  \/\/ The times are considered trustable if we\n-  \/\/ have completed at least one warmup cycle.\n-  return _nwarmup_cycles > 0;\n-}\n-\n-const AbsSeq& XStatCycle::serial_time() {\n-  return _serial_time;\n-}\n-\n-const AbsSeq& XStatCycle::parallelizable_time() {\n-  return _parallelizable_time;\n-}\n-\n-uint XStatCycle::last_active_workers() {\n-  return _last_active_workers;\n-}\n-\n-double XStatCycle::time_since_last() {\n-  if (_end_of_last.value() == 0) {\n-    \/\/ No end recorded yet, return time since VM start\n-    return os::elapsedTime();\n-  }\n-\n-  const Ticks now = Ticks::now();\n-  const Tickspan time_since_last = now - _end_of_last;\n-  return time_since_last.seconds();\n-}\n-\n-\/\/\n-\/\/ Stat workers\n-\/\/\n-Ticks XStatWorkers::_start_of_last;\n-Tickspan XStatWorkers::_accumulated_duration;\n-\n-void XStatWorkers::at_start() {\n-  _start_of_last = Ticks::now();\n-}\n-\n-void XStatWorkers::at_end() {\n-  const Ticks now = Ticks::now();\n-  const Tickspan duration = now - _start_of_last;\n-  _accumulated_duration += duration;\n-}\n-\n-double XStatWorkers::get_and_reset_duration() {\n-  const double duration = _accumulated_duration.seconds();\n-  const Ticks now = Ticks::now();\n-  _accumulated_duration = now - now;\n-  return duration;\n-}\n-\n-\/\/\n-\/\/ Stat load\n-\/\/\n-void XStatLoad::print() {\n-  double loadavg[3] = {};\n-  os::loadavg(loadavg, ARRAY_SIZE(loadavg));\n-  log_info(gc, load)(\"Load: %.2f\/%.2f\/%.2f\", loadavg[0], loadavg[1], loadavg[2]);\n-}\n-\n-\/\/\n-\/\/ Stat mark\n-\/\/\n-size_t XStatMark::_nstripes;\n-size_t XStatMark::_nproactiveflush;\n-size_t XStatMark::_nterminateflush;\n-size_t XStatMark::_ntrycomplete;\n-size_t XStatMark::_ncontinue;\n-size_t XStatMark::_mark_stack_usage;\n-\n-void XStatMark::set_at_mark_start(size_t nstripes) {\n-  _nstripes = nstripes;\n-}\n-\n-void XStatMark::set_at_mark_end(size_t nproactiveflush,\n-                                size_t nterminateflush,\n-                                size_t ntrycomplete,\n-                                size_t ncontinue) {\n-  _nproactiveflush = nproactiveflush;\n-  _nterminateflush = nterminateflush;\n-  _ntrycomplete = ntrycomplete;\n-  _ncontinue = ncontinue;\n-}\n-\n-void XStatMark::set_at_mark_free(size_t mark_stack_usage) {\n-  _mark_stack_usage = mark_stack_usage;\n-}\n-\n-void XStatMark::print() {\n-  log_info(gc, marking)(\"Mark: \"\n-                        SIZE_FORMAT \" stripe(s), \"\n-                        SIZE_FORMAT \" proactive flush(es), \"\n-                        SIZE_FORMAT \" terminate flush(es), \"\n-                        SIZE_FORMAT \" completion(s), \"\n-                        SIZE_FORMAT \" continuation(s) \",\n-                        _nstripes,\n-                        _nproactiveflush,\n-                        _nterminateflush,\n-                        _ntrycomplete,\n-                        _ncontinue);\n-\n-  log_info(gc, marking)(\"Mark Stack Usage: \" SIZE_FORMAT \"M\", _mark_stack_usage \/ M);\n-}\n-\n-\/\/\n-\/\/ Stat relocation\n-\/\/\n-XRelocationSetSelectorStats XStatRelocation::_selector_stats;\n-size_t                      XStatRelocation::_forwarding_usage;\n-size_t                      XStatRelocation::_small_in_place_count;\n-size_t                      XStatRelocation::_medium_in_place_count;\n-\n-void XStatRelocation::set_at_select_relocation_set(const XRelocationSetSelectorStats& selector_stats) {\n-  _selector_stats = selector_stats;\n-}\n-\n-void XStatRelocation::set_at_install_relocation_set(size_t forwarding_usage) {\n-  _forwarding_usage = forwarding_usage;\n-}\n-\n-void XStatRelocation::set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count) {\n-  _small_in_place_count = small_in_place_count;\n-  _medium_in_place_count = medium_in_place_count;\n-}\n-\n-void XStatRelocation::print(const char* name,\n-                            const XRelocationSetSelectorGroupStats& selector_group,\n-                            size_t in_place_count) {\n-  log_info(gc, reloc)(\"%s Pages: \" SIZE_FORMAT \" \/ \" SIZE_FORMAT \"M, Empty: \" SIZE_FORMAT \"M, \"\n-                      \"Relocated: \" SIZE_FORMAT \"M, In-Place: \" SIZE_FORMAT,\n-                      name,\n-                      selector_group.npages_candidates(),\n-                      selector_group.total() \/ M,\n-                      selector_group.empty() \/ M,\n-                      selector_group.relocate() \/ M,\n-                      in_place_count);\n-}\n-\n-void XStatRelocation::print() {\n-  print(\"Small\", _selector_stats.small(), _small_in_place_count);\n-  if (XPageSizeMedium != 0) {\n-    print(\"Medium\", _selector_stats.medium(), _medium_in_place_count);\n-  }\n-  print(\"Large\", _selector_stats.large(), 0 \/* in_place_count *\/);\n-\n-  log_info(gc, reloc)(\"Forwarding Usage: \" SIZE_FORMAT \"M\", _forwarding_usage \/ M);\n-}\n-\n-\/\/\n-\/\/ Stat nmethods\n-\/\/\n-void XStatNMethods::print() {\n-  log_info(gc, nmethod)(\"NMethods: \" SIZE_FORMAT \" registered, \" SIZE_FORMAT \" unregistered\",\n-                        XNMethodTable::registered_nmethods(),\n-                        XNMethodTable::unregistered_nmethods());\n-}\n-\n-\/\/\n-\/\/ Stat metaspace\n-\/\/\n-void XStatMetaspace::print() {\n-  MetaspaceCombinedStats stats = MetaspaceUtils::get_combined_statistics();\n-  log_info(gc, metaspace)(\"Metaspace: \"\n-                          SIZE_FORMAT \"M used, \"\n-                          SIZE_FORMAT \"M committed, \" SIZE_FORMAT \"M reserved\",\n-                          stats.used() \/ M,\n-                          stats.committed() \/ M,\n-                          stats.reserved() \/ M);\n-}\n-\n-\/\/\n-\/\/ Stat references\n-\/\/\n-XStatReferences::XCount XStatReferences::_soft;\n-XStatReferences::XCount XStatReferences::_weak;\n-XStatReferences::XCount XStatReferences::_final;\n-XStatReferences::XCount XStatReferences::_phantom;\n-\n-void XStatReferences::set(XCount* count, size_t encountered, size_t discovered, size_t enqueued) {\n-  count->encountered = encountered;\n-  count->discovered = discovered;\n-  count->enqueued = enqueued;\n-}\n-\n-void XStatReferences::set_soft(size_t encountered, size_t discovered, size_t enqueued) {\n-  set(&_soft, encountered, discovered, enqueued);\n-}\n-\n-void XStatReferences::set_weak(size_t encountered, size_t discovered, size_t enqueued) {\n-  set(&_weak, encountered, discovered, enqueued);\n-}\n-\n-void XStatReferences::set_final(size_t encountered, size_t discovered, size_t enqueued) {\n-  set(&_final, encountered, discovered, enqueued);\n-}\n-\n-void XStatReferences::set_phantom(size_t encountered, size_t discovered, size_t enqueued) {\n-  set(&_phantom, encountered, discovered, enqueued);\n-}\n-\n-void XStatReferences::print(const char* name, const XStatReferences::XCount& ref) {\n-  log_info(gc, ref)(\"%s: \"\n-                    SIZE_FORMAT \" encountered, \"\n-                    SIZE_FORMAT \" discovered, \"\n-                    SIZE_FORMAT \" enqueued\",\n-                    name,\n-                    ref.encountered,\n-                    ref.discovered,\n-                    ref.enqueued);\n-}\n-\n-void XStatReferences::print() {\n-  print(\"Soft\", _soft);\n-  print(\"Weak\", _weak);\n-  print(\"Final\", _final);\n-  print(\"Phantom\", _phantom);\n-}\n-\n-\/\/\n-\/\/ Stat heap\n-\/\/\n-XStatHeap::XAtInitialize XStatHeap::_at_initialize;\n-XStatHeap::XAtMarkStart XStatHeap::_at_mark_start;\n-XStatHeap::XAtMarkEnd XStatHeap::_at_mark_end;\n-XStatHeap::XAtRelocateStart XStatHeap::_at_relocate_start;\n-XStatHeap::XAtRelocateEnd XStatHeap::_at_relocate_end;\n-\n-size_t XStatHeap::capacity_high() {\n-  return MAX4(_at_mark_start.capacity,\n-              _at_mark_end.capacity,\n-              _at_relocate_start.capacity,\n-              _at_relocate_end.capacity);\n-}\n-\n-size_t XStatHeap::capacity_low() {\n-  return MIN4(_at_mark_start.capacity,\n-              _at_mark_end.capacity,\n-              _at_relocate_start.capacity,\n-              _at_relocate_end.capacity);\n-}\n-\n-size_t XStatHeap::free(size_t used) {\n-  return _at_initialize.max_capacity - used;\n-}\n-\n-size_t XStatHeap::allocated(size_t used, size_t reclaimed) {\n-  \/\/ The amount of allocated memory between point A and B is used(B) - used(A).\n-  \/\/ However, we might also have reclaimed memory between point A and B. This\n-  \/\/ means the current amount of used memory must be incremented by the amount\n-  \/\/ reclaimed, so that used(B) represents the amount of used memory we would\n-  \/\/ have had if we had not reclaimed anything.\n-  return (used + reclaimed) - _at_mark_start.used;\n-}\n-\n-size_t XStatHeap::garbage(size_t reclaimed) {\n-  return _at_mark_end.garbage - reclaimed;\n-}\n-\n-void XStatHeap::set_at_initialize(const XPageAllocatorStats& stats) {\n-  _at_initialize.min_capacity = stats.min_capacity();\n-  _at_initialize.max_capacity = stats.max_capacity();\n-}\n-\n-void XStatHeap::set_at_mark_start(const XPageAllocatorStats& stats) {\n-  _at_mark_start.soft_max_capacity = stats.soft_max_capacity();\n-  _at_mark_start.capacity = stats.capacity();\n-  _at_mark_start.free = free(stats.used());\n-  _at_mark_start.used = stats.used();\n-}\n-\n-void XStatHeap::set_at_mark_end(const XPageAllocatorStats& stats) {\n-  _at_mark_end.capacity = stats.capacity();\n-  _at_mark_end.free = free(stats.used());\n-  _at_mark_end.used = stats.used();\n-  _at_mark_end.allocated = allocated(stats.used(), 0 \/* reclaimed *\/);\n-}\n-\n-void XStatHeap::set_at_select_relocation_set(const XRelocationSetSelectorStats& stats) {\n-  const size_t live = stats.small().live() + stats.medium().live() + stats.large().live();\n-  _at_mark_end.live = live;\n-  _at_mark_end.garbage = _at_mark_start.used - live;\n-}\n-\n-void XStatHeap::set_at_relocate_start(const XPageAllocatorStats& stats) {\n-  _at_relocate_start.capacity = stats.capacity();\n-  _at_relocate_start.free = free(stats.used());\n-  _at_relocate_start.used = stats.used();\n-  _at_relocate_start.allocated = allocated(stats.used(), stats.reclaimed());\n-  _at_relocate_start.garbage = garbage(stats.reclaimed());\n-  _at_relocate_start.reclaimed = stats.reclaimed();\n-}\n-\n-void XStatHeap::set_at_relocate_end(const XPageAllocatorStats& stats, size_t non_worker_relocated) {\n-  const size_t reclaimed = stats.reclaimed() - MIN2(non_worker_relocated, stats.reclaimed());\n-\n-  _at_relocate_end.capacity = stats.capacity();\n-  _at_relocate_end.capacity_high = capacity_high();\n-  _at_relocate_end.capacity_low = capacity_low();\n-  _at_relocate_end.free = free(stats.used());\n-  _at_relocate_end.free_high = free(stats.used_low());\n-  _at_relocate_end.free_low = free(stats.used_high());\n-  _at_relocate_end.used = stats.used();\n-  _at_relocate_end.used_high = stats.used_high();\n-  _at_relocate_end.used_low = stats.used_low();\n-  _at_relocate_end.allocated = allocated(stats.used(), reclaimed);\n-  _at_relocate_end.garbage = garbage(reclaimed);\n-  _at_relocate_end.reclaimed = reclaimed;\n-}\n-\n-size_t XStatHeap::max_capacity() {\n-  return _at_initialize.max_capacity;\n-}\n-\n-size_t XStatHeap::used_at_mark_start() {\n-  return _at_mark_start.used;\n-}\n-\n-size_t XStatHeap::used_at_relocate_end() {\n-  return _at_relocate_end.used;\n-}\n-\n-void XStatHeap::print() {\n-  log_info(gc, heap)(\"Min Capacity: \"\n-                     XSIZE_FMT, XSIZE_ARGS(_at_initialize.min_capacity));\n-  log_info(gc, heap)(\"Max Capacity: \"\n-                     XSIZE_FMT, XSIZE_ARGS(_at_initialize.max_capacity));\n-  log_info(gc, heap)(\"Soft Max Capacity: \"\n-                     XSIZE_FMT, XSIZE_ARGS(_at_mark_start.soft_max_capacity));\n-\n-  XStatTablePrinter table(10, 18);\n-  log_info(gc, heap)(\"%s\", table()\n-                     .fill()\n-                     .center(\"Mark Start\")\n-                     .center(\"Mark End\")\n-                     .center(\"Relocate Start\")\n-                     .center(\"Relocate End\")\n-                     .center(\"High\")\n-                     .center(\"Low\")\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Capacity:\")\n-                     .left(XTABLE_ARGS(_at_mark_start.capacity))\n-                     .left(XTABLE_ARGS(_at_mark_end.capacity))\n-                     .left(XTABLE_ARGS(_at_relocate_start.capacity))\n-                     .left(XTABLE_ARGS(_at_relocate_end.capacity))\n-                     .left(XTABLE_ARGS(_at_relocate_end.capacity_high))\n-                     .left(XTABLE_ARGS(_at_relocate_end.capacity_low))\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Free:\")\n-                     .left(XTABLE_ARGS(_at_mark_start.free))\n-                     .left(XTABLE_ARGS(_at_mark_end.free))\n-                     .left(XTABLE_ARGS(_at_relocate_start.free))\n-                     .left(XTABLE_ARGS(_at_relocate_end.free))\n-                     .left(XTABLE_ARGS(_at_relocate_end.free_high))\n-                     .left(XTABLE_ARGS(_at_relocate_end.free_low))\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Used:\")\n-                     .left(XTABLE_ARGS(_at_mark_start.used))\n-                     .left(XTABLE_ARGS(_at_mark_end.used))\n-                     .left(XTABLE_ARGS(_at_relocate_start.used))\n-                     .left(XTABLE_ARGS(_at_relocate_end.used))\n-                     .left(XTABLE_ARGS(_at_relocate_end.used_high))\n-                     .left(XTABLE_ARGS(_at_relocate_end.used_low))\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Live:\")\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS(_at_mark_end.live))\n-                     .left(XTABLE_ARGS(_at_mark_end.live \/* Same as at mark end *\/))\n-                     .left(XTABLE_ARGS(_at_mark_end.live \/* Same as at mark end *\/))\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS_NA)\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Allocated:\")\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS(_at_mark_end.allocated))\n-                     .left(XTABLE_ARGS(_at_relocate_start.allocated))\n-                     .left(XTABLE_ARGS(_at_relocate_end.allocated))\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS_NA)\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Garbage:\")\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS(_at_mark_end.garbage))\n-                     .left(XTABLE_ARGS(_at_relocate_start.garbage))\n-                     .left(XTABLE_ARGS(_at_relocate_end.garbage))\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS_NA)\n-                     .end());\n-  log_info(gc, heap)(\"%s\", table()\n-                     .right(\"Reclaimed:\")\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS(_at_relocate_start.reclaimed))\n-                     .left(XTABLE_ARGS(_at_relocate_end.reclaimed))\n-                     .left(XTABLE_ARGS_NA)\n-                     .left(XTABLE_ARGS_NA)\n-                     .end());\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xStat.cpp","additions":0,"deletions":1513,"binary":false,"changes":1513,"status":"deleted"},{"patch":"@@ -1,578 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XSTAT_HPP\n-#define SHARE_GC_X_XSTAT_HPP\n-\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/shared\/gcCause.hpp\"\n-#include \"gc\/shared\/gcTimer.hpp\"\n-#include \"gc\/x\/xMetronome.hpp\"\n-#include \"logging\/logHandle.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/numberSeq.hpp\"\n-#include \"utilities\/ticks.hpp\"\n-\n-class XPage;\n-class XPageAllocatorStats;\n-class XRelocationSetSelectorGroupStats;\n-class XRelocationSetSelectorStats;\n-class XStatSampler;\n-class XStatSamplerHistory;\n-struct XStatCounterData;\n-struct XStatSamplerData;\n-\n-\/\/\n-\/\/ Stat unit printers\n-\/\/\n-typedef void (*XStatUnitPrinter)(LogTargetHandle log, const XStatSampler&, const XStatSamplerHistory&);\n-\n-void XStatUnitTime(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n-void XStatUnitBytes(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n-void XStatUnitThreads(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n-void XStatUnitBytesPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n-void XStatUnitOpsPerSecond(LogTargetHandle log, const XStatSampler& sampler, const XStatSamplerHistory& history);\n-\n-\/\/\n-\/\/ Stat value\n-\/\/\n-class XStatValue {\n-private:\n-  static uintptr_t _base;\n-  static uint32_t  _cpu_offset;\n-\n-  const char* const _group;\n-  const char* const _name;\n-  const uint32_t    _id;\n-  const uint32_t    _offset;\n-\n-protected:\n-  XStatValue(const char* group,\n-             const char* name,\n-             uint32_t id,\n-             uint32_t size);\n-\n-  template <typename T> T* get_cpu_local(uint32_t cpu) const;\n-\n-public:\n-  static void initialize();\n-\n-  const char* group() const;\n-  const char* name() const;\n-  uint32_t id() const;\n-};\n-\n-\/\/\n-\/\/ Stat iterable value\n-\/\/\n-template <typename T>\n-class XStatIterableValue : public XStatValue {\n-private:\n-  static uint32_t _count;\n-  static T*       _first;\n-\n-  T* _next;\n-\n-  T* insert() const;\n-\n-protected:\n-  XStatIterableValue(const char* group,\n-                     const char* name,\n-                     uint32_t size);\n-\n-public:\n-  static void sort();\n-\n-  static uint32_t count() {\n-    return _count;\n-  }\n-\n-  static T* first() {\n-    return _first;\n-  }\n-\n-  T* next() const {\n-    return _next;\n-  }\n-};\n-\n-template <typename T> uint32_t XStatIterableValue<T>::_count = 0;\n-template <typename T> T*       XStatIterableValue<T>::_first = nullptr;\n-\n-\/\/\n-\/\/ Stat sampler\n-\/\/\n-class XStatSampler : public XStatIterableValue<XStatSampler> {\n-private:\n-  const XStatUnitPrinter _printer;\n-\n-public:\n-  XStatSampler(const char* group,\n-               const char* name,\n-               XStatUnitPrinter printer);\n-\n-  XStatSamplerData* get() const;\n-  XStatSamplerData collect_and_reset() const;\n-\n-  XStatUnitPrinter printer() const;\n-};\n-\n-\/\/\n-\/\/ Stat counter\n-\/\/\n-class XStatCounter : public XStatIterableValue<XStatCounter> {\n-private:\n-  const XStatSampler _sampler;\n-\n-public:\n-  XStatCounter(const char* group,\n-               const char* name,\n-               XStatUnitPrinter printer);\n-\n-  XStatCounterData* get() const;\n-  void sample_and_reset() const;\n-};\n-\n-\/\/\n-\/\/ Stat unsampled counter\n-\/\/\n-class XStatUnsampledCounter : public XStatIterableValue<XStatUnsampledCounter> {\n-public:\n-  XStatUnsampledCounter(const char* name);\n-\n-  XStatCounterData* get() const;\n-  XStatCounterData collect_and_reset() const;\n-};\n-\n-\/\/\n-\/\/ Stat MMU (Minimum Mutator Utilization)\n-\/\/\n-class XStatMMUPause {\n-private:\n-  double _start;\n-  double _end;\n-\n-public:\n-  XStatMMUPause();\n-  XStatMMUPause(const Ticks& start, const Ticks& end);\n-\n-  double end() const;\n-  double overlap(double start, double end) const;\n-};\n-\n-class XStatMMU {\n-private:\n-  static size_t        _next;\n-  static size_t        _npauses;\n-  static XStatMMUPause _pauses[200]; \/\/ Record the last 200 pauses\n-\n-  static double _mmu_2ms;\n-  static double _mmu_5ms;\n-  static double _mmu_10ms;\n-  static double _mmu_20ms;\n-  static double _mmu_50ms;\n-  static double _mmu_100ms;\n-\n-  static const XStatMMUPause& pause(size_t index);\n-  static double calculate_mmu(double time_slice);\n-\n-public:\n-  static void register_pause(const Ticks& start, const Ticks& end);\n-\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat phases\n-\/\/\n-class XStatPhase {\n-private:\n-  static ConcurrentGCTimer _timer;\n-\n-protected:\n-  const XStatSampler _sampler;\n-\n-  XStatPhase(const char* group, const char* name);\n-\n-  void log_start(LogTargetHandle log, bool thread = false) const;\n-  void log_end(LogTargetHandle log, const Tickspan& duration, bool thread = false) const;\n-\n-public:\n-  static ConcurrentGCTimer* timer();\n-\n-  const char* name() const;\n-\n-  virtual void register_start(const Ticks& start) const = 0;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const = 0;\n-};\n-\n-class XStatPhaseCycle : public XStatPhase {\n-public:\n-  XStatPhaseCycle(const char* name);\n-\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n-};\n-\n-class XStatPhasePause : public XStatPhase {\n-private:\n-  static Tickspan _max; \/\/ Max pause time\n-\n-public:\n-  XStatPhasePause(const char* name);\n-\n-  static const Tickspan& max();\n-\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n-};\n-\n-class XStatPhaseConcurrent : public XStatPhase {\n-public:\n-  XStatPhaseConcurrent(const char* name);\n-\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n-};\n-\n-class XStatSubPhase : public XStatPhase {\n-public:\n-  XStatSubPhase(const char* name);\n-\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n-};\n-\n-class XStatCriticalPhase : public XStatPhase {\n-private:\n-  const XStatCounter _counter;\n-  const bool         _verbose;\n-\n-public:\n-  XStatCriticalPhase(const char* name, bool verbose = true);\n-\n-  virtual void register_start(const Ticks& start) const;\n-  virtual void register_end(const Ticks& start, const Ticks& end) const;\n-};\n-\n-\/\/\n-\/\/ Stat timer\n-\/\/\n-class XStatTimerDisable : public StackObj {\n-private:\n-  static THREAD_LOCAL uint32_t _active;\n-\n-public:\n-  XStatTimerDisable() {\n-    _active++;\n-  }\n-\n-  ~XStatTimerDisable() {\n-    _active--;\n-  }\n-\n-  static bool is_active() {\n-    return _active > 0;\n-  }\n-};\n-\n-class XStatTimer : public StackObj {\n-private:\n-  const bool        _enabled;\n-  const XStatPhase& _phase;\n-  const Ticks       _start;\n-\n-public:\n-  XStatTimer(const XStatPhase& phase) :\n-      _enabled(!XStatTimerDisable::is_active()),\n-      _phase(phase),\n-      _start(Ticks::now()) {\n-    if (_enabled) {\n-      _phase.register_start(_start);\n-    }\n-  }\n-\n-  ~XStatTimer() {\n-    if (_enabled) {\n-      const Ticks end = Ticks::now();\n-      _phase.register_end(_start, end);\n-    }\n-  }\n-};\n-\n-\/\/\n-\/\/ Stat sample\/increment\n-\/\/\n-void XStatSample(const XStatSampler& sampler, uint64_t value);\n-void XStatInc(const XStatCounter& counter, uint64_t increment = 1);\n-void XStatInc(const XStatUnsampledCounter& counter, uint64_t increment = 1);\n-\n-\/\/\n-\/\/ Stat allocation rate\n-\/\/\n-class XStatAllocRate : public AllStatic {\n-private:\n-  static const XStatUnsampledCounter _counter;\n-  static TruncatedSeq                _samples;\n-  static TruncatedSeq                _rate;\n-\n-public:\n-  static const uint64_t sample_hz = 10;\n-\n-  static const XStatUnsampledCounter& counter();\n-  static uint64_t sample_and_reset();\n-\n-  static double predict();\n-  static double avg();\n-  static double sd();\n-};\n-\n-\/\/\n-\/\/ Stat thread\n-\/\/\n-class XStat : public ConcurrentGCThread {\n-private:\n-  static const uint64_t sample_hz = 1;\n-\n-  XMetronome _metronome;\n-\n-  void sample_and_collect(XStatSamplerHistory* history) const;\n-  bool should_print(LogTargetHandle log) const;\n-  void print(LogTargetHandle log, const XStatSamplerHistory* history) const;\n-\n-protected:\n-  virtual void run_service();\n-  virtual void stop_service();\n-\n-public:\n-  XStat();\n-};\n-\n-\/\/\n-\/\/ Stat cycle\n-\/\/\n-class XStatCycle : public AllStatic {\n-private:\n-  static uint64_t  _nwarmup_cycles;\n-  static Ticks     _start_of_last;\n-  static Ticks     _end_of_last;\n-  static NumberSeq _serial_time;\n-  static NumberSeq _parallelizable_time;\n-  static uint      _last_active_workers;\n-\n-public:\n-  static void at_start();\n-  static void at_end(GCCause::Cause cause, uint active_workers);\n-\n-  static bool is_warm();\n-  static uint64_t nwarmup_cycles();\n-\n-  static bool is_time_trustable();\n-  static const AbsSeq& serial_time();\n-  static const AbsSeq& parallelizable_time();\n-\n-  static uint last_active_workers();\n-\n-  static double time_since_last();\n-};\n-\n-\/\/\n-\/\/ Stat workers\n-\/\/\n-class XStatWorkers : public AllStatic {\n-private:\n-  static Ticks    _start_of_last;\n-  static Tickspan _accumulated_duration;\n-\n-public:\n-  static void at_start();\n-  static void at_end();\n-\n-  static double get_and_reset_duration();\n-};\n-\n-\/\/\n-\/\/ Stat load\n-\/\/\n-class XStatLoad : public AllStatic {\n-public:\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat mark\n-\/\/\n-class XStatMark : public AllStatic {\n-private:\n-  static size_t _nstripes;\n-  static size_t _nproactiveflush;\n-  static size_t _nterminateflush;\n-  static size_t _ntrycomplete;\n-  static size_t _ncontinue;\n-  static size_t _mark_stack_usage;\n-\n-public:\n-  static void set_at_mark_start(size_t nstripes);\n-  static void set_at_mark_end(size_t nproactiveflush,\n-                              size_t nterminateflush,\n-                              size_t ntrycomplete,\n-                              size_t ncontinue);\n-  static void set_at_mark_free(size_t mark_stack_usage);\n-\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat relocation\n-\/\/\n-class XStatRelocation : public AllStatic {\n-private:\n-  static XRelocationSetSelectorStats _selector_stats;\n-  static size_t                      _forwarding_usage;\n-  static size_t                      _small_in_place_count;\n-  static size_t                      _medium_in_place_count;\n-\n-  static void print(const char* name,\n-                    const XRelocationSetSelectorGroupStats& selector_group,\n-                    size_t in_place_count);\n-\n-public:\n-  static void set_at_select_relocation_set(const XRelocationSetSelectorStats& selector_stats);\n-  static void set_at_install_relocation_set(size_t forwarding_usage);\n-  static void set_at_relocate_end(size_t small_in_place_count, size_t medium_in_place_count);\n-\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat nmethods\n-\/\/\n-class XStatNMethods : public AllStatic {\n-public:\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat metaspace\n-\/\/\n-class XStatMetaspace : public AllStatic {\n-public:\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat references\n-\/\/\n-class XStatReferences : public AllStatic {\n-private:\n-  static struct XCount {\n-    size_t encountered;\n-    size_t discovered;\n-    size_t enqueued;\n-  } _soft, _weak, _final, _phantom;\n-\n-  static void set(XCount* count, size_t encountered, size_t discovered, size_t enqueued);\n-  static void print(const char* name, const XCount& ref);\n-\n-public:\n-  static void set_soft(size_t encountered, size_t discovered, size_t enqueued);\n-  static void set_weak(size_t encountered, size_t discovered, size_t enqueued);\n-  static void set_final(size_t encountered, size_t discovered, size_t enqueued);\n-  static void set_phantom(size_t encountered, size_t discovered, size_t enqueued);\n-\n-  static void print();\n-};\n-\n-\/\/\n-\/\/ Stat heap\n-\/\/\n-class XStatHeap : public AllStatic {\n-private:\n-  static struct XAtInitialize {\n-    size_t min_capacity;\n-    size_t max_capacity;\n-  } _at_initialize;\n-\n-  static struct XAtMarkStart {\n-    size_t soft_max_capacity;\n-    size_t capacity;\n-    size_t free;\n-    size_t used;\n-  } _at_mark_start;\n-\n-  static struct XAtMarkEnd {\n-    size_t capacity;\n-    size_t free;\n-    size_t used;\n-    size_t live;\n-    size_t allocated;\n-    size_t garbage;\n-  } _at_mark_end;\n-\n-  static struct XAtRelocateStart {\n-    size_t capacity;\n-    size_t free;\n-    size_t used;\n-    size_t allocated;\n-    size_t garbage;\n-    size_t reclaimed;\n-  } _at_relocate_start;\n-\n-  static struct XAtRelocateEnd {\n-    size_t capacity;\n-    size_t capacity_high;\n-    size_t capacity_low;\n-    size_t free;\n-    size_t free_high;\n-    size_t free_low;\n-    size_t used;\n-    size_t used_high;\n-    size_t used_low;\n-    size_t allocated;\n-    size_t garbage;\n-    size_t reclaimed;\n-  } _at_relocate_end;\n-\n-  static size_t capacity_high();\n-  static size_t capacity_low();\n-  static size_t free(size_t used);\n-  static size_t allocated(size_t used, size_t reclaimed);\n-  static size_t garbage(size_t reclaimed);\n-\n-public:\n-  static void set_at_initialize(const XPageAllocatorStats& stats);\n-  static void set_at_mark_start(const XPageAllocatorStats& stats);\n-  static void set_at_mark_end(const XPageAllocatorStats& stats);\n-  static void set_at_select_relocation_set(const XRelocationSetSelectorStats& stats);\n-  static void set_at_relocate_start(const XPageAllocatorStats& stats);\n-  static void set_at_relocate_end(const XPageAllocatorStats& stats, size_t non_worker_relocated);\n-\n-  static size_t max_capacity();\n-  static size_t used_at_mark_start();\n-  static size_t used_at_relocate_end();\n-\n-  static void print();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XSTAT_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xStat.hpp","additions":0,"deletions":578,"binary":false,"changes":578,"status":"deleted"},{"patch":"@@ -1,47 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xThread.hpp\"\n-\n-XTask::Task::Task(XTask* task, const char* name) :\n-    WorkerTask(name),\n-    _task(task) {}\n-\n-void XTask::Task::work(uint worker_id) {\n-  XThread::set_worker_id(worker_id);\n-  _task->work();\n-  XThread::clear_worker_id();\n-}\n-\n-XTask::XTask(const char* name) :\n-    _worker_task(this, name) {}\n-\n-const char* XTask::name() const {\n-  return _worker_task.name();\n-}\n-\n-WorkerTask* XTask::worker_task() {\n-  return &_worker_task;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xTask.cpp","additions":0,"deletions":47,"binary":false,"changes":47,"status":"deleted"},{"patch":"@@ -1,53 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTASK_HPP\n-#define SHARE_GC_X_XTASK_HPP\n-\n-#include \"gc\/shared\/workerThread.hpp\"\n-#include \"memory\/allocation.hpp\"\n-\n-class XTask : public StackObj {\n-private:\n-  class Task : public WorkerTask {\n-  private:\n-    XTask* const _task;\n-\n-  public:\n-    Task(XTask* task, const char* name);\n-\n-    virtual void work(uint worker_id);\n-  };\n-\n-  Task _worker_task;\n-\n-public:\n-  XTask(const char* name);\n-\n-  const char* name() const;\n-  WorkerTask* worker_task();\n-\n-  virtual void work() = 0;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XTASK_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xTask.hpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"deleted"},{"patch":"@@ -1,80 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/nonJavaThread.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-THREAD_LOCAL bool      XThread::_initialized;\n-THREAD_LOCAL uintptr_t XThread::_id;\n-THREAD_LOCAL bool      XThread::_is_vm;\n-THREAD_LOCAL bool      XThread::_is_java;\n-THREAD_LOCAL bool      XThread::_is_worker;\n-THREAD_LOCAL uint      XThread::_worker_id;\n-\n-void XThread::initialize() {\n-  assert(!_initialized, \"Already initialized\");\n-  const Thread* const thread = Thread::current();\n-  _initialized = true;\n-  _id = (uintptr_t)thread;\n-  _is_vm = thread->is_VM_thread();\n-  _is_java = thread->is_Java_thread();\n-  _is_worker = false;\n-  _worker_id = (uint)-1;\n-}\n-\n-const char* XThread::name() {\n-  const Thread* const thread = Thread::current();\n-  if (thread->is_Named_thread()) {\n-    const NamedThread* const named = (const NamedThread*)thread;\n-    return named->name();\n-  } else if (thread->is_Java_thread()) {\n-    return \"Java\";\n-  }\n-\n-  return \"Unknown\";\n-}\n-\n-void XThread::set_worker() {\n-  ensure_initialized();\n-  _is_worker = true;\n-}\n-\n-bool XThread::has_worker_id() {\n-  return _initialized &&\n-         _is_worker &&\n-         _worker_id != (uint)-1;\n-}\n-\n-void XThread::set_worker_id(uint worker_id) {\n-  ensure_initialized();\n-  assert(!has_worker_id(), \"Worker id already initialized\");\n-  _worker_id = worker_id;\n-}\n-\n-void XThread::clear_worker_id() {\n-  assert(has_worker_id(), \"Worker id not initialized\");\n-  _worker_id = (uint)-1;\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xThread.cpp","additions":0,"deletions":80,"binary":false,"changes":80,"status":"deleted"},{"patch":"@@ -1,61 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTHREAD_HPP\n-#define SHARE_GC_X_XTHREAD_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XThread : public AllStatic {\n-  friend class XTask;\n-  friend class XWorkersInitializeTask;\n-  friend class XRuntimeWorkersInitializeTask;\n-\n-private:\n-  static THREAD_LOCAL bool      _initialized;\n-  static THREAD_LOCAL uintptr_t _id;\n-  static THREAD_LOCAL bool      _is_vm;\n-  static THREAD_LOCAL bool      _is_java;\n-  static THREAD_LOCAL bool      _is_worker;\n-  static THREAD_LOCAL uint      _worker_id;\n-\n-  static void initialize();\n-  static void ensure_initialized();\n-\n-  static void set_worker();\n-\n-  static bool has_worker_id();\n-  static void set_worker_id(uint worker_id);\n-  static void clear_worker_id();\n-\n-public:\n-  static const char* name();\n-  static uintptr_t id();\n-  static bool is_vm();\n-  static bool is_java();\n-  static bool is_worker();\n-  static uint worker_id();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThread.hpp","additions":0,"deletions":61,"binary":false,"changes":61,"status":"deleted"},{"patch":"@@ -1,62 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTHREAD_INLINE_HPP\n-#define SHARE_GC_X_XTHREAD_INLINE_HPP\n-\n-#include \"gc\/x\/xThread.hpp\"\n-\n-#include \"utilities\/debug.hpp\"\n-\n-inline void XThread::ensure_initialized() {\n-  if (!_initialized) {\n-    initialize();\n-  }\n-}\n-\n-inline uintptr_t XThread::id() {\n-  ensure_initialized();\n-  return _id;\n-}\n-\n-inline bool XThread::is_vm() {\n-  ensure_initialized();\n-  return _is_vm;\n-}\n-\n-inline bool XThread::is_java() {\n-  ensure_initialized();\n-  return _is_java;\n-}\n-\n-inline bool XThread::is_worker() {\n-  ensure_initialized();\n-  return _is_worker;\n-}\n-\n-inline uint XThread::worker_id() {\n-  assert(has_worker_id(), \"Worker id not initialized\");\n-  return _worker_id;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XTHREAD_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThread.inline.hpp","additions":0,"deletions":62,"binary":false,"changes":62,"status":"deleted"},{"patch":"@@ -1,92 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/tlab_globals.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xStackWatermark.hpp\"\n-#include \"gc\/x\/xThreadLocalAllocBuffer.hpp\"\n-#include \"gc\/x\/xValue.inline.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/stackWatermarkSet.inline.hpp\"\n-\n-XPerWorker<ThreadLocalAllocStats>* XThreadLocalAllocBuffer::_stats = nullptr;\n-\n-void XThreadLocalAllocBuffer::initialize() {\n-  if (UseTLAB) {\n-    assert(_stats == nullptr, \"Already initialized\");\n-    _stats = new XPerWorker<ThreadLocalAllocStats>();\n-    reset_statistics();\n-  }\n-}\n-\n-void XThreadLocalAllocBuffer::reset_statistics() {\n-  if (UseTLAB) {\n-    XPerWorkerIterator<ThreadLocalAllocStats> iter(_stats);\n-    for (ThreadLocalAllocStats* stats; iter.next(&stats);) {\n-      stats->reset();\n-    }\n-  }\n-}\n-\n-void XThreadLocalAllocBuffer::publish_statistics() {\n-  if (UseTLAB) {\n-    ThreadLocalAllocStats total;\n-\n-    XPerWorkerIterator<ThreadLocalAllocStats> iter(_stats);\n-    for (ThreadLocalAllocStats* stats; iter.next(&stats);) {\n-      total.update(*stats);\n-    }\n-\n-    total.publish();\n-  }\n-}\n-\n-static void fixup_address(HeapWord** p) {\n-  *p = (HeapWord*)XAddress::good_or_null((uintptr_t)*p);\n-}\n-\n-void XThreadLocalAllocBuffer::retire(JavaThread* thread, ThreadLocalAllocStats* stats) {\n-  if (UseTLAB) {\n-    stats->reset();\n-    thread->tlab().addresses_do(fixup_address);\n-    thread->tlab().retire(stats);\n-    if (ResizeTLAB) {\n-      thread->tlab().resize();\n-    }\n-  }\n-}\n-\n-void XThreadLocalAllocBuffer::remap(JavaThread* thread) {\n-  if (UseTLAB) {\n-    thread->tlab().addresses_do(fixup_address);\n-  }\n-}\n-\n-void XThreadLocalAllocBuffer::update_stats(JavaThread* thread) {\n-  if (UseTLAB) {\n-    XStackWatermark* const watermark = StackWatermarkSet::get<XStackWatermark>(thread, StackWatermarkKind::gc);\n-    _stats->addr()->update(watermark->stats());\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xThreadLocalAllocBuffer.cpp","additions":0,"deletions":92,"binary":false,"changes":92,"status":"deleted"},{"patch":"@@ -1,48 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTHREADLOCALALLOCBUFFER_HPP\n-#define SHARE_GC_X_XTHREADLOCALALLOCBUFFER_HPP\n-\n-#include \"gc\/shared\/threadLocalAllocBuffer.hpp\"\n-#include \"gc\/x\/xValue.hpp\"\n-#include \"memory\/allStatic.hpp\"\n-\n-class JavaThread;\n-\n-class XThreadLocalAllocBuffer : public AllStatic {\n-private:\n-  static XPerWorker<ThreadLocalAllocStats>* _stats;\n-\n-public:\n-  static void initialize();\n-\n-  static void reset_statistics();\n-  static void publish_statistics();\n-\n-  static void retire(JavaThread* thread, ThreadLocalAllocStats* stats);\n-  static void remap(JavaThread* thread);\n-  static void update_stats(JavaThread* thread);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XTHREADLOCALALLOCBUFFER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThreadLocalAllocBuffer.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"deleted"},{"patch":"@@ -1,91 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTHREADLOCALDATA_HPP\n-#define SHARE_GC_X_XTHREADLOCALDATA_HPP\n-\n-#include \"gc\/x\/xMarkStack.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/sizes.hpp\"\n-\n-class XThreadLocalData {\n-private:\n-  uintptr_t              _address_bad_mask;\n-  XMarkThreadLocalStacks _stacks;\n-  oop*                   _invisible_root;\n-\n-  XThreadLocalData() :\n-      _address_bad_mask(0),\n-      _stacks(),\n-      _invisible_root(nullptr) {}\n-\n-  static XThreadLocalData* data(Thread* thread) {\n-    return thread->gc_data<XThreadLocalData>();\n-  }\n-\n-public:\n-  static void create(Thread* thread) {\n-    new (data(thread)) XThreadLocalData();\n-  }\n-\n-  static void destroy(Thread* thread) {\n-    data(thread)->~XThreadLocalData();\n-  }\n-\n-  static void set_address_bad_mask(Thread* thread, uintptr_t mask) {\n-    data(thread)->_address_bad_mask = mask;\n-  }\n-\n-  static XMarkThreadLocalStacks* stacks(Thread* thread) {\n-    return &data(thread)->_stacks;\n-  }\n-\n-  static void set_invisible_root(Thread* thread, oop* root) {\n-    assert(data(thread)->_invisible_root == nullptr, \"Already set\");\n-    data(thread)->_invisible_root = root;\n-  }\n-\n-  static void clear_invisible_root(Thread* thread) {\n-    assert(data(thread)->_invisible_root != nullptr, \"Should be set\");\n-    data(thread)->_invisible_root = nullptr;\n-  }\n-\n-  template <typename T>\n-  static void do_invisible_root(Thread* thread, T f) {\n-    if (data(thread)->_invisible_root != nullptr) {\n-      f(data(thread)->_invisible_root);\n-    }\n-  }\n-\n-  static ByteSize address_bad_mask_offset() {\n-    return Thread::gc_data_offset() + byte_offset_of(XThreadLocalData, _address_bad_mask);\n-  }\n-\n-  static ByteSize nmethod_disarmed_offset() {\n-    return address_bad_mask_offset() + in_ByteSize(XAddressBadMaskHighOrderBitsOffset);\n-  }\n-};\n-\n-#endif \/\/ SHARE_GC_X_XTHREADLOCALDATA_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xThreadLocalData.hpp","additions":0,"deletions":91,"binary":false,"changes":91,"status":"deleted"},{"patch":"@@ -1,146 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcId.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTracer.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"runtime\/safepointVerifiers.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#if INCLUDE_JFR\n-#include \"jfr\/metadata\/jfrSerializer.hpp\"\n-#endif\n-\n-#if INCLUDE_JFR\n-\n-class XPageTypeConstant : public JfrSerializer {\n-public:\n-  virtual void serialize(JfrCheckpointWriter& writer) {\n-    writer.write_count(3);\n-    writer.write_key(XPageTypeSmall);\n-    writer.write(\"Small\");\n-    writer.write_key(XPageTypeMedium);\n-    writer.write(\"Medium\");\n-    writer.write_key(XPageTypeLarge);\n-    writer.write(\"Large\");\n-  }\n-};\n-\n-class XStatisticsCounterTypeConstant : public JfrSerializer {\n-public:\n-  virtual void serialize(JfrCheckpointWriter& writer) {\n-    writer.write_count(XStatCounter::count());\n-    for (XStatCounter* counter = XStatCounter::first(); counter != nullptr; counter = counter->next()) {\n-      writer.write_key(counter->id());\n-      writer.write(counter->name());\n-    }\n-  }\n-};\n-\n-class XStatisticsSamplerTypeConstant : public JfrSerializer {\n-public:\n-  virtual void serialize(JfrCheckpointWriter& writer) {\n-    writer.write_count(XStatSampler::count());\n-    for (XStatSampler* sampler = XStatSampler::first(); sampler != nullptr; sampler = sampler->next()) {\n-      writer.write_key(sampler->id());\n-      writer.write(sampler->name());\n-    }\n-  }\n-};\n-\n-static void register_jfr_type_serializers() {\n-  JfrSerializer::register_serializer(TYPE_ZPAGETYPETYPE,\n-                                     true \/* permit_cache *\/,\n-                                     new XPageTypeConstant());\n-  JfrSerializer::register_serializer(TYPE_ZSTATISTICSCOUNTERTYPE,\n-                                     true \/* permit_cache *\/,\n-                                     new XStatisticsCounterTypeConstant());\n-  JfrSerializer::register_serializer(TYPE_ZSTATISTICSSAMPLERTYPE,\n-                                     true \/* permit_cache *\/,\n-                                     new XStatisticsSamplerTypeConstant());\n-}\n-\n-#endif \/\/ INCLUDE_JFR\n-\n-XTracer* XTracer::_tracer = nullptr;\n-\n-XTracer::XTracer() :\n-    GCTracer(Z) {}\n-\n-void XTracer::initialize() {\n-  assert(_tracer == nullptr, \"Already initialized\");\n-  _tracer = new XTracer();\n-  JFR_ONLY(register_jfr_type_serializers();)\n-}\n-\n-void XTracer::send_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value) {\n-  NoSafepointVerifier nsv;\n-\n-  EventZStatisticsCounter e;\n-  if (e.should_commit()) {\n-    e.set_id(counter.id());\n-    e.set_increment(increment);\n-    e.set_value(value);\n-    e.commit();\n-  }\n-}\n-\n-void XTracer::send_stat_sampler(const XStatSampler& sampler, uint64_t value) {\n-  NoSafepointVerifier nsv;\n-\n-  EventZStatisticsSampler e;\n-  if (e.should_commit()) {\n-    e.set_id(sampler.id());\n-    e.set_value(value);\n-    e.commit();\n-  }\n-}\n-\n-void XTracer::send_thread_phase(const char* name, const Ticks& start, const Ticks& end) {\n-  NoSafepointVerifier nsv;\n-\n-  EventZThreadPhase e(UNTIMED);\n-  if (e.should_commit()) {\n-    e.set_gcId(GCId::current_or_undefined());\n-    e.set_name(name);\n-    e.set_starttime(start);\n-    e.set_endtime(end);\n-    e.commit();\n-  }\n-}\n-\n-void XTracer::send_thread_debug(const char* name, const Ticks& start, const Ticks& end) {\n-  NoSafepointVerifier nsv;\n-\n-  EventZThreadDebug e(UNTIMED);\n-  if (e.should_commit()) {\n-    e.set_gcId(GCId::current_or_undefined());\n-    e.set_name(name);\n-    e.set_starttime(start);\n-    e.set_endtime(end);\n-    e.commit();\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xTracer.cpp","additions":0,"deletions":146,"binary":false,"changes":146,"status":"deleted"},{"patch":"@@ -1,65 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTRACER_HPP\n-#define SHARE_GC_X_XTRACER_HPP\n-\n-#include \"gc\/shared\/gcTrace.hpp\"\n-\n-class XStatCounter;\n-class XStatPhase;\n-class XStatSampler;\n-\n-class XTracer : public GCTracer, public CHeapObj<mtGC> {\n-private:\n-  static XTracer* _tracer;\n-\n-  XTracer();\n-\n-  void send_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value);\n-  void send_stat_sampler(const XStatSampler& sampler, uint64_t value);\n-  void send_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n-  void send_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n-\n-public:\n-  static XTracer* tracer();\n-  static void initialize();\n-\n-  void report_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value);\n-  void report_stat_sampler(const XStatSampler& sampler, uint64_t value);\n-  void report_thread_phase(const char* name, const Ticks& start, const Ticks& end);\n-  void report_thread_debug(const char* name, const Ticks& start, const Ticks& end);\n-};\n-\n-\/\/ For temporary latency measurements during development and debugging\n-class XTraceThreadDebug : public StackObj {\n-private:\n-  const Ticks       _start;\n-  const char* const _name;\n-\n-public:\n-  XTraceThreadDebug(const char* name);\n-  ~XTraceThreadDebug();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XTRACER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xTracer.hpp","additions":0,"deletions":65,"binary":false,"changes":65,"status":"deleted"},{"patch":"@@ -1,67 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XTRACER_INLINE_HPP\n-#define SHARE_GC_X_XTRACER_INLINE_HPP\n-\n-#include \"gc\/x\/xTracer.hpp\"\n-\n-#include \"jfr\/jfrEvents.hpp\"\n-\n-inline XTracer* XTracer::tracer() {\n-  return _tracer;\n-}\n-\n-inline void XTracer::report_stat_counter(const XStatCounter& counter, uint64_t increment, uint64_t value) {\n-  if (EventZStatisticsCounter::is_enabled()) {\n-    send_stat_counter(counter, increment, value);\n-  }\n-}\n-\n-inline void XTracer::report_stat_sampler(const XStatSampler& sampler, uint64_t value) {\n-  if (EventZStatisticsSampler::is_enabled()) {\n-    send_stat_sampler(sampler, value);\n-  }\n-}\n-\n-inline void XTracer::report_thread_phase(const char* name, const Ticks& start, const Ticks& end) {\n-  if (EventZThreadPhase::is_enabled()) {\n-    send_thread_phase(name, start, end);\n-  }\n-}\n-\n-inline void XTracer::report_thread_debug(const char* name, const Ticks& start, const Ticks& end) {\n-  if (EventZThreadDebug::is_enabled()) {\n-    send_thread_debug(name, start, end);\n-  }\n-}\n-\n-inline XTraceThreadDebug::XTraceThreadDebug(const char* name) :\n-    _start(Ticks::now()),\n-    _name(name) {}\n-\n-inline XTraceThreadDebug::~XTraceThreadDebug() {\n-  XTracer::tracer()->report_thread_debug(_name, _start, Ticks::now());\n-}\n-\n-#endif \/\/ SHARE_GC_X_XTRACER_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xTracer.inline.hpp","additions":0,"deletions":67,"binary":false,"changes":67,"status":"deleted"},{"patch":"@@ -1,96 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xUncommitter.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"logging\/log.hpp\"\n-\n-static const XStatCounter XCounterUncommit(\"Memory\", \"Uncommit\", XStatUnitBytesPerSecond);\n-\n-XUncommitter::XUncommitter(XPageAllocator* page_allocator) :\n-    _page_allocator(page_allocator),\n-    _lock(),\n-    _stop(false) {\n-  set_name(\"XUncommitter\");\n-  create_and_start();\n-}\n-\n-bool XUncommitter::wait(uint64_t timeout) const {\n-  XLocker<XConditionLock> locker(&_lock);\n-  while (!ZUncommit && !_stop) {\n-    _lock.wait();\n-  }\n-\n-  if (!_stop && timeout > 0) {\n-    log_debug(gc, heap)(\"Uncommit Timeout: \" UINT64_FORMAT \"s\", timeout);\n-    _lock.wait(timeout * MILLIUNITS);\n-  }\n-\n-  return !_stop;\n-}\n-\n-bool XUncommitter::should_continue() const {\n-  XLocker<XConditionLock> locker(&_lock);\n-  return !_stop;\n-}\n-\n-void XUncommitter::run_service() {\n-  uint64_t timeout = 0;\n-\n-  while (wait(timeout)) {\n-    EventZUncommit event;\n-    size_t uncommitted = 0;\n-\n-    while (should_continue()) {\n-      \/\/ Uncommit chunk\n-      const size_t flushed = _page_allocator->uncommit(&timeout);\n-      if (flushed == 0) {\n-        \/\/ Done\n-        break;\n-      }\n-\n-      uncommitted += flushed;\n-    }\n-\n-    if (uncommitted > 0) {\n-      \/\/ Update statistics\n-      XStatInc(XCounterUncommit, uncommitted);\n-      log_info(gc, heap)(\"Uncommitted: \" SIZE_FORMAT \"M(%.0f%%)\",\n-                         uncommitted \/ M, percent_of(uncommitted, XHeap::heap()->max_capacity()));\n-\n-      \/\/ Send event\n-      event.commit(uncommitted);\n-    }\n-  }\n-}\n-\n-void XUncommitter::stop_service() {\n-  XLocker<XConditionLock> locker(&_lock);\n-  _stop = true;\n-  _lock.notify_all();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xUncommitter.cpp","additions":0,"deletions":96,"binary":false,"changes":96,"status":"deleted"},{"patch":"@@ -1,49 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XUNCOMMITTER_HPP\n-#define SHARE_GC_X_XUNCOMMITTER_HPP\n-\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-\n-class XPageAllocation;\n-\n-class XUncommitter : public ConcurrentGCThread {\n-private:\n-  XPageAllocator* const  _page_allocator;\n-  mutable XConditionLock _lock;\n-  bool                   _stop;\n-\n-  bool wait(uint64_t timeout) const;\n-  bool should_continue() const;\n-\n-protected:\n-  virtual void run_service();\n-  virtual void stop_service();\n-\n-public:\n-  XUncommitter(XPageAllocator* page_allocator);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XUNCOMMITTER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUncommitter.hpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"deleted"},{"patch":"@@ -1,172 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"code\/codeBehaviours.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"code\/dependencyContext.hpp\"\n-#include \"gc\/shared\/gcBehaviours.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xUnload.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"oops\/access.inline.hpp\"\n-\n-static const XStatSubPhase XSubPhaseConcurrentClassesUnlink(\"Concurrent Classes Unlink\");\n-static const XStatSubPhase XSubPhaseConcurrentClassesPurge(\"Concurrent Classes Purge\");\n-\n-class XPhantomIsAliveObjectClosure : public BoolObjectClosure {\n-public:\n-  virtual bool do_object_b(oop o) {\n-    return XBarrier::is_alive_barrier_on_phantom_oop(o);\n-  }\n-};\n-\n-class XIsUnloadingOopClosure : public OopClosure {\n-private:\n-  XPhantomIsAliveObjectClosure _is_alive;\n-  bool                         _is_unloading;\n-\n-public:\n-  XIsUnloadingOopClosure() :\n-      _is_alive(),\n-      _is_unloading(false) {}\n-\n-  virtual void do_oop(oop* p) {\n-    const oop o = RawAccess<>::oop_load(p);\n-    if (o != nullptr && !_is_alive.do_object_b(o)) {\n-      _is_unloading = true;\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-\n-  bool is_unloading() const {\n-    return _is_unloading;\n-  }\n-};\n-\n-class XIsUnloadingBehaviour : public IsUnloadingBehaviour {\n-public:\n-  virtual bool has_dead_oop(nmethod* nm) const {\n-    XReentrantLock* const lock = XNMethod::lock_for_nmethod(nm);\n-    XLocker<XReentrantLock> locker(lock);\n-    XIsUnloadingOopClosure cl;\n-    XNMethod::nmethod_oops_do_inner(nm, &cl);\n-    return cl.is_unloading();\n-  }\n-};\n-\n-class XCompiledICProtectionBehaviour : public CompiledICProtectionBehaviour {\n-public:\n-  virtual bool lock(nmethod* nm) {\n-    XReentrantLock* const lock = XNMethod::ic_lock_for_nmethod(nm);\n-    lock->lock();\n-    return true;\n-  }\n-\n-  virtual void unlock(nmethod* nm) {\n-    XReentrantLock* const lock = XNMethod::ic_lock_for_nmethod(nm);\n-    lock->unlock();\n-  }\n-\n-  virtual bool is_safe(nmethod* nm) {\n-    if (SafepointSynchronize::is_at_safepoint() || nm->is_unloading()) {\n-      return true;\n-    }\n-\n-    XReentrantLock* const lock = XNMethod::ic_lock_for_nmethod(nm);\n-    return lock->is_owned();\n-  }\n-};\n-\n-XUnload::XUnload(XWorkers* workers) :\n-    _workers(workers) {\n-\n-  if (!ClassUnloading) {\n-    return;\n-  }\n-\n-  static XIsUnloadingBehaviour is_unloading_behaviour;\n-  IsUnloadingBehaviour::set_current(&is_unloading_behaviour);\n-\n-  static XCompiledICProtectionBehaviour ic_protection_behaviour;\n-  CompiledICProtectionBehaviour::set_current(&ic_protection_behaviour);\n-}\n-\n-void XUnload::prepare() {\n-  if (!ClassUnloading) {\n-    return;\n-  }\n-\n-  CodeCache::increment_unloading_cycle();\n-  DependencyContext::cleaning_start();\n-}\n-\n-void XUnload::unlink() {\n-  if (!ClassUnloading) {\n-    return;\n-  }\n-\n-  XStatTimer timer(XSubPhaseConcurrentClassesUnlink);\n-  SuspendibleThreadSetJoiner sts;\n-  bool unloading_occurred;\n-\n-  {\n-    MutexLocker ml(ClassLoaderDataGraph_lock);\n-    unloading_occurred = SystemDictionary::do_unloading(XStatPhase::timer());\n-  }\n-\n-  Klass::clean_weak_klass_links(unloading_occurred);\n-  XNMethod::unlink(_workers, unloading_occurred);\n-  DependencyContext::cleaning_end();\n-}\n-\n-void XUnload::purge() {\n-  if (!ClassUnloading) {\n-    return;\n-  }\n-\n-  XStatTimer timer(XSubPhaseConcurrentClassesPurge);\n-\n-  {\n-    SuspendibleThreadSetJoiner sts;\n-    XNMethod::purge();\n-  }\n-\n-  ClassLoaderDataGraph::purge(\/*at_safepoint*\/false);\n-  CodeCache::purge_exception_caches();\n-}\n-\n-void XUnload::finish() {\n-  \/\/ Resize and verify metaspace\n-  MetaspaceGC::compute_new_size();\n-  DEBUG_ONLY(MetaspaceUtils::verify();)\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xUnload.cpp","additions":0,"deletions":172,"binary":false,"changes":172,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XUNLOAD_HPP\n-#define SHARE_GC_X_XUNLOAD_HPP\n-\n-class XWorkers;\n-\n-class XUnload {\n-private:\n-  XWorkers* const _workers;\n-\n-public:\n-  XUnload(XWorkers* workers);\n-\n-  void prepare();\n-  void unlink();\n-  void purge();\n-  void finish();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XUNLOAD_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUnload.hpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -1,135 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"gc\/x\/xPageAllocator.hpp\"\n-#include \"gc\/x\/xUnmapper.hpp\"\n-#include \"jfr\/jfrEvents.hpp\"\n-#include \"runtime\/globals.hpp\"\n-\n-XUnmapper::XUnmapper(XPageAllocator* page_allocator) :\n-    _page_allocator(page_allocator),\n-    _lock(),\n-    _queue(),\n-    _enqueued_bytes(0),\n-    _warned_sync_unmapping(false),\n-    _stop(false) {\n-  set_name(\"XUnmapper\");\n-  create_and_start();\n-}\n-\n-XPage* XUnmapper::dequeue() {\n-  XLocker<XConditionLock> locker(&_lock);\n-\n-  for (;;) {\n-    if (_stop) {\n-      return nullptr;\n-    }\n-\n-    XPage* const page = _queue.remove_first();\n-    if (page != nullptr) {\n-      _enqueued_bytes -= page->size();\n-      return page;\n-    }\n-\n-    _lock.wait();\n-  }\n-}\n-\n-bool XUnmapper::try_enqueue(XPage* page) {\n-  if (ZVerifyViews) {\n-    \/\/ Asynchronous unmap and destroy is not supported with ZVerifyViews\n-    return false;\n-  }\n-\n-  \/\/ Enqueue for asynchronous unmap and destroy\n-  XLocker<XConditionLock> locker(&_lock);\n-  if (is_saturated()) {\n-    \/\/ The unmapper thread is lagging behind and is unable to unmap memory fast enough\n-    if (!_warned_sync_unmapping) {\n-      _warned_sync_unmapping = true;\n-      log_warning_p(gc)(\"WARNING: Encountered synchronous unmapping because asynchronous unmapping could not keep up\");\n-    }\n-    log_debug(gc, unmap)(\"Synchronous unmapping \" SIZE_FORMAT \"M page\", page->size() \/ M);\n-    return false;\n-  }\n-\n-  log_trace(gc, unmap)(\"Asynchronous unmapping \" SIZE_FORMAT \"M page (\" SIZE_FORMAT \"M \/ \" SIZE_FORMAT \"M enqueued)\",\n-                       page->size() \/ M, _enqueued_bytes \/ M, queue_capacity() \/ M);\n-\n-  _queue.insert_last(page);\n-  _enqueued_bytes += page->size();\n-  _lock.notify_all();\n-\n-  return true;\n-}\n-\n-size_t XUnmapper::queue_capacity() const {\n-  return align_up<size_t>(_page_allocator->max_capacity() * ZAsyncUnmappingLimit \/ 100.0, XGranuleSize);\n-}\n-\n-bool XUnmapper::is_saturated() const {\n-  return _enqueued_bytes >= queue_capacity();\n-}\n-\n-void XUnmapper::do_unmap_and_destroy_page(XPage* page) const {\n-  EventZUnmap event;\n-  const size_t unmapped = page->size();\n-\n-  \/\/ Unmap and destroy\n-  _page_allocator->unmap_page(page);\n-  _page_allocator->destroy_page(page);\n-\n-  \/\/ Send event\n-  event.commit(unmapped);\n-}\n-\n-void XUnmapper::unmap_and_destroy_page(XPage* page) {\n-  if (!try_enqueue(page)) {\n-    \/\/ Synchronously unmap and destroy\n-    do_unmap_and_destroy_page(page);\n-  }\n-}\n-\n-void XUnmapper::run_service() {\n-  for (;;) {\n-    XPage* const page = dequeue();\n-    if (page == nullptr) {\n-      \/\/ Stop\n-      return;\n-    }\n-\n-    do_unmap_and_destroy_page(page);\n-  }\n-}\n-\n-void XUnmapper::stop_service() {\n-  XLocker<XConditionLock> locker(&_lock);\n-  _stop = true;\n-  _lock.notify_all();\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xUnmapper.cpp","additions":0,"deletions":135,"binary":false,"changes":135,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XUNMAPPER_HPP\n-#define SHARE_GC_X_XUNMAPPER_HPP\n-\n-#include \"gc\/shared\/concurrentGCThread.hpp\"\n-#include \"gc\/x\/xList.hpp\"\n-#include \"gc\/x\/xLock.hpp\"\n-\n-class XPage;\n-class XPageAllocator;\n-\n-class XUnmapper : public ConcurrentGCThread {\n-private:\n-  XPageAllocator* const _page_allocator;\n-  XConditionLock        _lock;\n-  XList<XPage>          _queue;\n-  size_t                _enqueued_bytes;\n-  bool                  _warned_sync_unmapping;\n-  bool                  _stop;\n-\n-  XPage* dequeue();\n-  bool try_enqueue(XPage* page);\n-  size_t queue_capacity() const;\n-  bool is_saturated() const;\n-  void do_unmap_and_destroy_page(XPage* page) const;\n-\n-protected:\n-  virtual void run_service();\n-  virtual void stop_service();\n-\n-public:\n-  XUnmapper(XPageAllocator* page_allocator);\n-\n-  void unmap_and_destroy_page(XPage* page);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XUNMAPPER_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUnmapper.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XUTILS_HPP\n-#define SHARE_GC_X_XUTILS_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-class XUtils : public AllStatic {\n-public:\n-  \/\/ Allocation\n-  static uintptr_t alloc_aligned(size_t alignment, size_t size);\n-\n-  \/\/ Size conversion\n-  static size_t bytes_to_words(size_t size_in_words);\n-  static size_t words_to_bytes(size_t size_in_words);\n-\n-  \/\/ Object\n-  static size_t object_size(uintptr_t addr);\n-  static void object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size);\n-  static void object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XUTILS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUtils.hpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,59 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XUTILS_INLINE_HPP\n-#define SHARE_GC_X_XUTILS_INLINE_HPP\n-\n-#include \"gc\/x\/xUtils.hpp\"\n-\n-#include \"gc\/x\/xOop.inline.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/copy.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-inline size_t XUtils::bytes_to_words(size_t size_in_bytes) {\n-  assert(is_aligned(size_in_bytes, BytesPerWord), \"Size not word aligned\");\n-  return size_in_bytes >> LogBytesPerWord;\n-}\n-\n-inline size_t XUtils::words_to_bytes(size_t size_in_words) {\n-  return size_in_words << LogBytesPerWord;\n-}\n-\n-inline size_t XUtils::object_size(uintptr_t addr) {\n-  return words_to_bytes(XOop::from_address(addr)->size());\n-}\n-\n-inline void XUtils::object_copy_disjoint(uintptr_t from, uintptr_t to, size_t size) {\n-  Copy::aligned_disjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n-}\n-\n-inline void XUtils::object_copy_conjoint(uintptr_t from, uintptr_t to, size_t size) {\n-  if (from != to) {\n-    Copy::aligned_conjoint_words((HeapWord*)from, (HeapWord*)to, bytes_to_words(size));\n-  }\n-}\n-\n-#endif \/\/ SHARE_GC_X_XUTILS_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xUtils.inline.hpp","additions":0,"deletions":59,"binary":false,"changes":59,"status":"deleted"},{"patch":"@@ -1,140 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XVALUE_HPP\n-#define SHARE_GC_X_XVALUE_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/\/\n-\/\/ Storage\n-\/\/\n-\n-template <typename S>\n-class XValueStorage : public AllStatic {\n-private:\n-  static uintptr_t _top;\n-  static uintptr_t _end;\n-\n-public:\n-  static const size_t offset = 4 * K;\n-\n-  static uintptr_t alloc(size_t size);\n-};\n-\n-class XContendedStorage : public XValueStorage<XContendedStorage> {\n-public:\n-  static size_t alignment();\n-  static uint32_t count();\n-  static uint32_t id();\n-};\n-\n-class XPerCPUStorage : public XValueStorage<XPerCPUStorage> {\n-public:\n-  static size_t alignment();\n-  static uint32_t count();\n-  static uint32_t id();\n-};\n-\n-class XPerNUMAStorage : public XValueStorage<XPerNUMAStorage> {\n-public:\n-  static size_t alignment();\n-  static uint32_t count();\n-  static uint32_t id();\n-};\n-\n-class XPerWorkerStorage : public XValueStorage<XPerWorkerStorage> {\n-public:\n-  static size_t alignment();\n-  static uint32_t count();\n-  static uint32_t id();\n-};\n-\n-\/\/\n-\/\/ Value\n-\/\/\n-\n-template <typename S, typename T>\n-class XValue : public CHeapObj<mtGC> {\n-private:\n-  const uintptr_t _addr;\n-\n-  uintptr_t value_addr(uint32_t value_id) const;\n-\n-public:\n-  XValue();\n-  XValue(const T& value);\n-\n-  const T* addr(uint32_t value_id = S::id()) const;\n-  T* addr(uint32_t value_id = S::id());\n-\n-  const T& get(uint32_t value_id = S::id()) const;\n-  T& get(uint32_t value_id = S::id());\n-\n-  void set(const T& value, uint32_t value_id = S::id());\n-  void set_all(const T& value);\n-};\n-\n-template <typename T> using XContended = XValue<XContendedStorage, T>;\n-template <typename T> using XPerCPU = XValue<XPerCPUStorage, T>;\n-template <typename T> using XPerNUMA = XValue<XPerNUMAStorage, T>;\n-template <typename T> using XPerWorker = XValue<XPerWorkerStorage, T>;\n-\n-\/\/\n-\/\/ Iterator\n-\/\/\n-\n-template <typename S, typename T>\n-class XValueIterator {\n-private:\n-  XValue<S, T>* const _value;\n-  uint32_t            _value_id;\n-\n-public:\n-  XValueIterator(XValue<S, T>* value);\n-\n-  bool next(T** value);\n-};\n-\n-template <typename T> using XPerCPUIterator = XValueIterator<XPerCPUStorage, T>;\n-template <typename T> using XPerNUMAIterator = XValueIterator<XPerNUMAStorage, T>;\n-template <typename T> using XPerWorkerIterator = XValueIterator<XPerWorkerStorage, T>;\n-\n-template <typename S, typename T>\n-class XValueConstIterator {\n-private:\n-  const XValue<S, T>* const _value;\n-  uint32_t                  _value_id;\n-\n-public:\n-  XValueConstIterator(const XValue<S, T>* value);\n-\n-  bool next(const T** value);\n-};\n-\n-template <typename T> using XPerCPUConstIterator = XValueConstIterator<XPerCPUStorage, T>;\n-template <typename T> using XPerNUMAConstIterator = XValueConstIterator<XPerNUMAStorage, T>;\n-template <typename T> using XPerWorkerConstIterator = XValueConstIterator<XPerWorkerStorage, T>;\n-\n-#endif \/\/ SHARE_GC_X_XVALUE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xValue.hpp","additions":0,"deletions":140,"binary":false,"changes":140,"status":"deleted"},{"patch":"@@ -1,210 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XVALUE_INLINE_HPP\n-#define SHARE_GC_X_XVALUE_INLINE_HPP\n-\n-#include \"gc\/x\/xValue.hpp\"\n-\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xCPU.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xNUMA.hpp\"\n-#include \"gc\/x\/xThread.inline.hpp\"\n-#include \"gc\/x\/xUtils.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-\/\/\n-\/\/ Storage\n-\/\/\n-\n-template <typename T> uintptr_t XValueStorage<T>::_end = 0;\n-template <typename T> uintptr_t XValueStorage<T>::_top = 0;\n-\n-template <typename S>\n-uintptr_t XValueStorage<S>::alloc(size_t size) {\n-  assert(size <= offset, \"Allocation too large\");\n-\n-  \/\/ Allocate entry in existing memory block\n-  const uintptr_t addr = align_up(_top, S::alignment());\n-  _top = addr + size;\n-\n-  if (_top < _end) {\n-    \/\/ Success\n-    return addr;\n-  }\n-\n-  \/\/ Allocate new block of memory\n-  const size_t block_alignment = offset;\n-  const size_t block_size = offset * S::count();\n-  _top = XUtils::alloc_aligned(block_alignment, block_size);\n-  _end = _top + offset;\n-\n-  \/\/ Retry allocation\n-  return alloc(size);\n-}\n-\n-inline size_t XContendedStorage::alignment() {\n-  return XCacheLineSize;\n-}\n-\n-inline uint32_t XContendedStorage::count() {\n-  return 1;\n-}\n-\n-inline uint32_t XContendedStorage::id() {\n-  return 0;\n-}\n-\n-inline size_t XPerCPUStorage::alignment() {\n-  return sizeof(uintptr_t);\n-}\n-\n-inline uint32_t XPerCPUStorage::count() {\n-  return XCPU::count();\n-}\n-\n-inline uint32_t XPerCPUStorage::id() {\n-  return XCPU::id();\n-}\n-\n-inline size_t XPerNUMAStorage::alignment() {\n-  return sizeof(uintptr_t);\n-}\n-\n-inline uint32_t XPerNUMAStorage::count() {\n-  return XNUMA::count();\n-}\n-\n-inline uint32_t XPerNUMAStorage::id() {\n-  return XNUMA::id();\n-}\n-\n-inline size_t XPerWorkerStorage::alignment() {\n-  return sizeof(uintptr_t);\n-}\n-\n-inline uint32_t XPerWorkerStorage::count() {\n-  return UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads);\n-}\n-\n-inline uint32_t XPerWorkerStorage::id() {\n-  return XThread::worker_id();\n-}\n-\n-\/\/\n-\/\/ Value\n-\/\/\n-\n-template <typename S, typename T>\n-inline uintptr_t XValue<S, T>::value_addr(uint32_t value_id) const {\n-  return _addr + (value_id * S::offset);\n-}\n-\n-template <typename S, typename T>\n-inline XValue<S, T>::XValue() :\n-    _addr(S::alloc(sizeof(T))) {\n-  \/\/ Initialize all instances\n-  XValueIterator<S, T> iter(this);\n-  for (T* addr; iter.next(&addr);) {\n-    ::new (addr) T;\n-  }\n-}\n-\n-template <typename S, typename T>\n-inline XValue<S, T>::XValue(const T& value) :\n-    _addr(S::alloc(sizeof(T))) {\n-  \/\/ Initialize all instances\n-  XValueIterator<S, T> iter(this);\n-  for (T* addr; iter.next(&addr);) {\n-    ::new (addr) T(value);\n-  }\n-}\n-\n-template <typename S, typename T>\n-inline const T* XValue<S, T>::addr(uint32_t value_id) const {\n-  return reinterpret_cast<const T*>(value_addr(value_id));\n-}\n-\n-template <typename S, typename T>\n-inline T* XValue<S, T>::addr(uint32_t value_id) {\n-  return reinterpret_cast<T*>(value_addr(value_id));\n-}\n-\n-template <typename S, typename T>\n-inline const T& XValue<S, T>::get(uint32_t value_id) const {\n-  return *addr(value_id);\n-}\n-\n-template <typename S, typename T>\n-inline T& XValue<S, T>::get(uint32_t value_id) {\n-  return *addr(value_id);\n-}\n-\n-template <typename S, typename T>\n-inline void XValue<S, T>::set(const T& value, uint32_t value_id) {\n-  get(value_id) = value;\n-}\n-\n-template <typename S, typename T>\n-inline void XValue<S, T>::set_all(const T& value) {\n-  XValueIterator<S, T> iter(this);\n-  for (T* addr; iter.next(&addr);) {\n-    *addr = value;\n-  }\n-}\n-\n-\/\/\n-\/\/ Iterator\n-\/\/\n-\n-template <typename S, typename T>\n-inline XValueIterator<S, T>::XValueIterator(XValue<S, T>* value) :\n-    _value(value),\n-    _value_id(0) {}\n-\n-template <typename S, typename T>\n-inline bool XValueIterator<S, T>::next(T** value) {\n-  if (_value_id < S::count()) {\n-    *value = _value->addr(_value_id++);\n-    return true;\n-  }\n-  return false;\n-}\n-\n-template <typename S, typename T>\n-inline XValueConstIterator<S, T>::XValueConstIterator(const XValue<S, T>* value) :\n-    _value(value),\n-    _value_id(0) {}\n-\n-template <typename S, typename T>\n-inline bool XValueConstIterator<S, T>::next(const T** value) {\n-  if (_value_id < S::count()) {\n-    *value = _value->addr(_value_id++);\n-    return true;\n-  }\n-  return false;\n-}\n-\n-#endif \/\/ SHARE_GC_X_XVALUE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xValue.inline.hpp","additions":0,"deletions":210,"binary":false,"changes":210,"status":"deleted"},{"patch":"@@ -1,405 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderData.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xHeap.inline.hpp\"\n-#include \"gc\/x\/xNMethod.hpp\"\n-#include \"gc\/x\/xOop.hpp\"\n-#include \"gc\/x\/xPageAllocator.hpp\"\n-#include \"gc\/x\/xResurrection.hpp\"\n-#include \"gc\/x\/xRootsIterator.hpp\"\n-#include \"gc\/x\/xStackWatermark.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xVerify.hpp\"\n-#include \"memory\/iterator.inline.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"oops\/oop.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/handles.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"runtime\/stackFrameStream.inline.hpp\"\n-#include \"runtime\/stackWatermark.inline.hpp\"\n-#include \"runtime\/stackWatermarkSet.inline.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/preserveException.hpp\"\n-\n-#define BAD_OOP_ARG(o, p)   \"Bad oop \" PTR_FORMAT \" found at \" PTR_FORMAT, p2i(o), p2i(p)\n-\n-static void z_verify_oop(oop* p) {\n-  const oop o = RawAccess<>::oop_load(p);\n-  if (o != nullptr) {\n-    const uintptr_t addr = XOop::to_address(o);\n-    guarantee(XAddress::is_good(addr), BAD_OOP_ARG(o, p));\n-    guarantee(oopDesc::is_oop(XOop::from_address(addr)), BAD_OOP_ARG(o, p));\n-  }\n-}\n-\n-static void z_verify_possibly_weak_oop(oop* p) {\n-  const oop o = RawAccess<>::oop_load(p);\n-  if (o != nullptr) {\n-    const uintptr_t addr = XOop::to_address(o);\n-    guarantee(XAddress::is_good(addr) || XAddress::is_finalizable_good(addr), BAD_OOP_ARG(o, p));\n-    guarantee(oopDesc::is_oop(XOop::from_address(XAddress::good(addr))), BAD_OOP_ARG(o, p));\n-  }\n-}\n-\n-class XVerifyRootClosure : public OopClosure {\n-private:\n-  const bool _verify_fixed;\n-\n-public:\n-  XVerifyRootClosure(bool verify_fixed) :\n-      _verify_fixed(verify_fixed) {}\n-\n-  virtual void do_oop(oop* p) {\n-    if (_verify_fixed) {\n-      z_verify_oop(p);\n-    } else {\n-      \/\/ Don't know the state of the oop.\n-      oop obj = *p;\n-      obj = NativeAccess<AS_NO_KEEPALIVE>::oop_load(&obj);\n-      z_verify_oop(&obj);\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop*) {\n-    ShouldNotReachHere();\n-  }\n-\n-  bool verify_fixed() const {\n-    return _verify_fixed;\n-  }\n-};\n-\n-class XVerifyStack : public OopClosure {\n-private:\n-  XVerifyRootClosure* const _cl;\n-  JavaThread*         const _jt;\n-  uint64_t                  _last_good;\n-  bool                      _verifying_bad_frames;\n-\n-public:\n-  XVerifyStack(XVerifyRootClosure* cl, JavaThread* jt) :\n-      _cl(cl),\n-      _jt(jt),\n-      _last_good(0),\n-      _verifying_bad_frames(false) {\n-    XStackWatermark* const stack_watermark = StackWatermarkSet::get<XStackWatermark>(jt, StackWatermarkKind::gc);\n-\n-    if (_cl->verify_fixed()) {\n-      assert(stack_watermark->processing_started(), \"Should already have been fixed\");\n-      assert(stack_watermark->processing_completed(), \"Should already have been fixed\");\n-    } else {\n-      \/\/ We don't really know the state of the stack, verify watermark.\n-      if (!stack_watermark->processing_started()) {\n-        _verifying_bad_frames = true;\n-      } else {\n-        \/\/ Not time yet to verify bad frames\n-        _last_good = stack_watermark->last_processed();\n-      }\n-    }\n-  }\n-\n-  void do_oop(oop* p) {\n-    if (_verifying_bad_frames) {\n-      const oop obj = *p;\n-      guarantee(!XAddress::is_good(XOop::to_address(obj)), BAD_OOP_ARG(obj, p));\n-    }\n-    _cl->do_oop(p);\n-  }\n-\n-  void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-\n-  void prepare_next_frame(frame& frame) {\n-    if (_cl->verify_fixed()) {\n-      \/\/ All frames need to be good\n-      return;\n-    }\n-\n-    \/\/ The verification has two modes, depending on whether we have reached the\n-    \/\/ last processed frame or not. Before it is reached, we expect everything to\n-    \/\/ be good. After reaching it, we expect everything to be bad.\n-    const uintptr_t sp = reinterpret_cast<uintptr_t>(frame.sp());\n-\n-    if (!_verifying_bad_frames && sp == _last_good) {\n-      \/\/ Found the last good frame, now verify the bad ones\n-      _verifying_bad_frames = true;\n-    }\n-  }\n-\n-  void verify_frames() {\n-    NMethodToOopClosure nm_cl(_cl, false \/* fix_relocations *\/);\n-    for (StackFrameStream frames(_jt, true \/* update *\/, false \/* process_frames *\/);\n-         !frames.is_done();\n-         frames.next()) {\n-      frame& frame = *frames.current();\n-      frame.oops_do(this, &nm_cl, frames.register_map(), DerivedPointerIterationMode::_ignore);\n-      prepare_next_frame(frame);\n-    }\n-  }\n-};\n-\n-class XVerifyOopClosure : public ClaimMetadataVisitingOopIterateClosure {\n-private:\n-  const bool _verify_weaks;\n-\n-public:\n-  XVerifyOopClosure(bool verify_weaks) :\n-      ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_other),\n-      _verify_weaks(verify_weaks) {}\n-\n-  virtual void do_oop(oop* p) {\n-    if (_verify_weaks) {\n-      z_verify_possibly_weak_oop(p);\n-    } else {\n-      \/\/ We should never encounter finalizable oops through strong\n-      \/\/ paths. This assumes we have only visited strong roots.\n-      z_verify_oop(p);\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-\n-  virtual ReferenceIterationMode reference_iteration_mode() {\n-    return _verify_weaks ? DO_FIELDS : DO_FIELDS_EXCEPT_REFERENT;\n-  }\n-\n-  \/\/ Don't follow this metadata when verifying oops\n-  virtual void do_method(Method* m) {}\n-  virtual void do_nmethod(nmethod* nm) {}\n-};\n-\n-typedef ClaimingCLDToOopClosure<ClassLoaderData::_claim_none> XVerifyCLDClosure;\n-\n-class XVerifyThreadClosure : public ThreadClosure {\n-private:\n-  XVerifyRootClosure* const _cl;\n-\n-public:\n-  XVerifyThreadClosure(XVerifyRootClosure* cl) :\n-      _cl(cl) {}\n-\n-  virtual void do_thread(Thread* thread) {\n-    thread->oops_do_no_frames(_cl, nullptr);\n-\n-    JavaThread* const jt = JavaThread::cast(thread);\n-    if (!jt->has_last_Java_frame()) {\n-      return;\n-    }\n-\n-    XVerifyStack verify_stack(_cl, jt);\n-    verify_stack.verify_frames();\n-  }\n-};\n-\n-class XVerifyNMethodClosure : public NMethodClosure {\n-private:\n-  OopClosure* const        _cl;\n-  BarrierSetNMethod* const _bs_nm;\n-  const bool               _verify_fixed;\n-\n-  bool trust_nmethod_state() const {\n-    \/\/ The root iterator will visit non-processed\n-    \/\/ nmethods class unloading is turned off.\n-    return ClassUnloading || _verify_fixed;\n-  }\n-\n-public:\n-  XVerifyNMethodClosure(OopClosure* cl, bool verify_fixed) :\n-      _cl(cl),\n-      _bs_nm(BarrierSet::barrier_set()->barrier_set_nmethod()),\n-      _verify_fixed(verify_fixed) {}\n-\n-  virtual void do_nmethod(nmethod* nm) {\n-    assert(!trust_nmethod_state() || !_bs_nm->is_armed(nm), \"Should not encounter any armed nmethods\");\n-\n-    XNMethod::nmethod_oops_do(nm, _cl);\n-  }\n-};\n-\n-void XVerify::roots_strong(bool verify_fixed) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n-  assert(!XResurrection::is_blocked(), \"Invalid phase\");\n-\n-  XVerifyRootClosure cl(verify_fixed);\n-  XVerifyCLDClosure cld_cl(&cl);\n-  XVerifyThreadClosure thread_cl(&cl);\n-  XVerifyNMethodClosure nm_cl(&cl, verify_fixed);\n-\n-  XRootsIterator iter(ClassLoaderData::_claim_none);\n-  iter.apply(&cl,\n-             &cld_cl,\n-             &thread_cl,\n-             &nm_cl);\n-}\n-\n-void XVerify::roots_weak() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n-  assert(!XResurrection::is_blocked(), \"Invalid phase\");\n-\n-  XVerifyRootClosure cl(true \/* verify_fixed *\/);\n-  XWeakRootsIterator iter;\n-  iter.apply(&cl);\n-}\n-\n-void XVerify::objects(bool verify_weaks) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Must be at a safepoint\");\n-  assert(XGlobalPhase == XPhaseMarkCompleted, \"Invalid phase\");\n-  assert(!XResurrection::is_blocked(), \"Invalid phase\");\n-\n-  XVerifyOopClosure cl(verify_weaks);\n-  ObjectToOopClosure object_cl(&cl);\n-  XHeap::heap()->object_iterate(&object_cl, verify_weaks);\n-}\n-\n-void XVerify::before_zoperation() {\n-  \/\/ Verify strong roots\n-  XStatTimerDisable disable;\n-  if (ZVerifyRoots) {\n-    roots_strong(false \/* verify_fixed *\/);\n-  }\n-}\n-\n-void XVerify::after_mark() {\n-  \/\/ Verify all strong roots and strong references\n-  XStatTimerDisable disable;\n-  if (ZVerifyRoots) {\n-    roots_strong(true \/* verify_fixed *\/);\n-  }\n-  if (ZVerifyObjects) {\n-    objects(false \/* verify_weaks *\/);\n-  }\n-}\n-\n-void XVerify::after_weak_processing() {\n-  \/\/ Verify all roots and all references\n-  XStatTimerDisable disable;\n-  if (ZVerifyRoots) {\n-    roots_strong(true \/* verify_fixed *\/);\n-    roots_weak();\n-  }\n-  if (ZVerifyObjects) {\n-    objects(true \/* verify_weaks *\/);\n-  }\n-}\n-\n-template <bool Map>\n-class XPageDebugMapOrUnmapClosure : public XPageClosure {\n-private:\n-  const XPageAllocator* const _allocator;\n-\n-public:\n-  XPageDebugMapOrUnmapClosure(const XPageAllocator* allocator) :\n-      _allocator(allocator) {}\n-\n-  void do_page(const XPage* page) {\n-    if (Map) {\n-      _allocator->debug_map_page(page);\n-    } else {\n-      _allocator->debug_unmap_page(page);\n-    }\n-  }\n-};\n-\n-XVerifyViewsFlip::XVerifyViewsFlip(const XPageAllocator* allocator) :\n-    _allocator(allocator) {\n-  if (ZVerifyViews) {\n-    \/\/ Unmap all pages\n-    XPageDebugMapOrUnmapClosure<false \/* Map *\/> cl(_allocator);\n-    XHeap::heap()->pages_do(&cl);\n-  }\n-}\n-\n-XVerifyViewsFlip::~XVerifyViewsFlip() {\n-  if (ZVerifyViews) {\n-    \/\/ Map all pages\n-    XPageDebugMapOrUnmapClosure<true \/* Map *\/> cl(_allocator);\n-    XHeap::heap()->pages_do(&cl);\n-  }\n-}\n-\n-#ifdef ASSERT\n-\n-class XVerifyBadOopClosure : public OopClosure {\n-public:\n-  virtual void do_oop(oop* p) {\n-    const oop o = *p;\n-    assert(!XAddress::is_good(XOop::to_address(o)), \"Should not be good: \" PTR_FORMAT, p2i(o));\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-\/\/ This class encapsulates various marks we need to deal with calling the\n-\/\/ frame iteration code from arbitrary points in the runtime. It is mostly\n-\/\/ due to problems that we might want to eventually clean up inside of the\n-\/\/ frame iteration code, such as creating random handles even though there\n-\/\/ is no safepoint to protect against, and fiddling around with exceptions.\n-class StackWatermarkProcessingMark {\n-  ResetNoHandleMark     _rnhm;\n-  HandleMark            _hm;\n-  PreserveExceptionMark _pem;\n-  ResourceMark          _rm;\n-\n-public:\n-  StackWatermarkProcessingMark(Thread* thread) :\n-      _rnhm(),\n-      _hm(thread),\n-      _pem(thread),\n-      _rm(thread) {}\n-};\n-\n-void XVerify::verify_frame_bad(const frame& fr, RegisterMap& register_map) {\n-  XVerifyBadOopClosure verify_cl;\n-  fr.oops_do(&verify_cl, nullptr, &register_map, DerivedPointerIterationMode::_ignore);\n-}\n-\n-void XVerify::verify_thread_head_bad(JavaThread* jt) {\n-  XVerifyBadOopClosure verify_cl;\n-  jt->oops_do_no_frames(&verify_cl, nullptr);\n-}\n-\n-void XVerify::verify_thread_frames_bad(JavaThread* jt) {\n-  if (jt->has_last_Java_frame()) {\n-    XVerifyBadOopClosure verify_cl;\n-    StackWatermarkProcessingMark swpm(Thread::current());\n-    \/\/ Traverse the execution stack\n-    for (StackFrameStream fst(jt, true \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n-      fst.current()->oops_do(&verify_cl, nullptr \/* code_cl *\/, fst.register_map(), DerivedPointerIterationMode::_ignore);\n-    }\n-  }\n-}\n-\n-#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/gc\/x\/xVerify.cpp","additions":0,"deletions":405,"binary":false,"changes":405,"status":"deleted"},{"patch":"@@ -1,58 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XVERIFY_HPP\n-#define SHARE_GC_X_XVERIFY_HPP\n-\n-#include \"memory\/allStatic.hpp\"\n-\n-class frame;\n-class XPageAllocator;\n-\n-class XVerify : public AllStatic {\n-private:\n-  static void roots_strong(bool verify_fixed);\n-  static void roots_weak();\n-\n-  static void objects(bool verify_weaks);\n-\n-public:\n-  static void before_zoperation();\n-  static void after_mark();\n-  static void after_weak_processing();\n-\n-  static void verify_thread_head_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n-  static void verify_thread_frames_bad(JavaThread* thread) NOT_DEBUG_RETURN;\n-  static void verify_frame_bad(const frame& fr, RegisterMap& register_map) NOT_DEBUG_RETURN;\n-};\n-\n-class XVerifyViewsFlip {\n-private:\n-  const XPageAllocator* const _allocator;\n-\n-public:\n-  XVerifyViewsFlip(const XPageAllocator* allocator);\n-  ~XVerifyViewsFlip();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XVERIFY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xVerify.hpp","additions":0,"deletions":58,"binary":false,"changes":58,"status":"deleted"},{"patch":"@@ -1,208 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-XVirtualMemoryManager::XVirtualMemoryManager(size_t max_capacity) :\n-    _manager(),\n-    _reserved(0),\n-    _initialized(false) {\n-\n-  \/\/ Check max supported heap size\n-  if (max_capacity > XAddressOffsetMax) {\n-    log_error_p(gc)(\"Java heap too large (max supported heap size is \" SIZE_FORMAT \"G)\",\n-                    XAddressOffsetMax \/ G);\n-    return;\n-  }\n-\n-  \/\/ Initialize platform specific parts before reserving address space\n-  pd_initialize_before_reserve();\n-\n-  \/\/ Reserve address space\n-  if (!reserve(max_capacity)) {\n-    log_error_pd(gc)(\"Failed to reserve enough address space for Java heap\");\n-    return;\n-  }\n-\n-  \/\/ Initialize platform specific parts after reserving address space\n-  pd_initialize_after_reserve();\n-\n-  \/\/ Successfully initialized\n-  _initialized = true;\n-}\n-\n-size_t XVirtualMemoryManager::reserve_discontiguous(uintptr_t start, size_t size, size_t min_range) {\n-  if (size < min_range) {\n-    \/\/ Too small\n-    return 0;\n-  }\n-\n-  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n-\n-  if (reserve_contiguous(start, size)) {\n-    return size;\n-  }\n-\n-  const size_t half = size \/ 2;\n-  if (half < min_range) {\n-    \/\/ Too small\n-    return 0;\n-  }\n-\n-  \/\/ Divide and conquer\n-  const size_t first_part = align_down(half, XGranuleSize);\n-  const size_t second_part = size - first_part;\n-  return reserve_discontiguous(start, first_part, min_range) +\n-         reserve_discontiguous(start + first_part, second_part, min_range);\n-}\n-\n-size_t XVirtualMemoryManager::reserve_discontiguous(size_t size) {\n-  \/\/ Don't try to reserve address ranges smaller than 1% of the requested size.\n-  \/\/ This avoids an explosion of reservation attempts in case large parts of the\n-  \/\/ address space is already occupied.\n-  const size_t min_range = align_up(size \/ 100, XGranuleSize);\n-  size_t start = 0;\n-  size_t reserved = 0;\n-\n-  \/\/ Reserve size somewhere between [0, XAddressOffsetMax)\n-  while (reserved < size && start < XAddressOffsetMax) {\n-    const size_t remaining = MIN2(size - reserved, XAddressOffsetMax - start);\n-    reserved += reserve_discontiguous(start, remaining, min_range);\n-    start += remaining;\n-  }\n-\n-  return reserved;\n-}\n-\n-bool XVirtualMemoryManager::reserve_contiguous(uintptr_t start, size_t size) {\n-  assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n-\n-  \/\/ Reserve address views\n-  const uintptr_t marked0 = XAddress::marked0(start);\n-  const uintptr_t marked1 = XAddress::marked1(start);\n-  const uintptr_t remapped = XAddress::remapped(start);\n-\n-  \/\/ Reserve address space\n-  if (!pd_reserve(marked0, size)) {\n-    return false;\n-  }\n-\n-  if (!pd_reserve(marked1, size)) {\n-    pd_unreserve(marked0, size);\n-    return false;\n-  }\n-\n-  if (!pd_reserve(remapped, size)) {\n-    pd_unreserve(marked0, size);\n-    pd_unreserve(marked1, size);\n-    return false;\n-  }\n-\n-  \/\/ Register address views with native memory tracker\n-  nmt_reserve(marked0, size);\n-  nmt_reserve(marked1, size);\n-  nmt_reserve(remapped, size);\n-\n-  \/\/ Make the address range free\n-  _manager.free(start, size);\n-\n-  return true;\n-}\n-\n-bool XVirtualMemoryManager::reserve_contiguous(size_t size) {\n-  \/\/ Allow at most 8192 attempts spread evenly across [0, XAddressOffsetMax)\n-  const size_t unused = XAddressOffsetMax - size;\n-  const size_t increment = MAX2(align_up(unused \/ 8192, XGranuleSize), XGranuleSize);\n-\n-  for (size_t start = 0; start + size <= XAddressOffsetMax; start += increment) {\n-    if (reserve_contiguous(start, size)) {\n-      \/\/ Success\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Failed\n-  return false;\n-}\n-\n-bool XVirtualMemoryManager::reserve(size_t max_capacity) {\n-  const size_t limit = MIN2(XAddressOffsetMax, XAddressSpaceLimit::heap_view());\n-  const size_t size = MIN2(max_capacity * XVirtualToPhysicalRatio, limit);\n-\n-  size_t reserved = size;\n-  bool contiguous = true;\n-\n-  \/\/ Prefer a contiguous address space\n-  if (!reserve_contiguous(size)) {\n-    \/\/ Fall back to a discontiguous address space\n-    reserved = reserve_discontiguous(size);\n-    contiguous = false;\n-  }\n-\n-  log_info_p(gc, init)(\"Address Space Type: %s\/%s\/%s\",\n-                       (contiguous ? \"Contiguous\" : \"Discontiguous\"),\n-                       (limit == XAddressOffsetMax ? \"Unrestricted\" : \"Restricted\"),\n-                       (reserved == size ? \"Complete\" : \"Degraded\"));\n-  log_info_p(gc, init)(\"Address Space Size: \" SIZE_FORMAT \"M x \" SIZE_FORMAT \" = \" SIZE_FORMAT \"M\",\n-                       reserved \/ M, XHeapViews, (reserved * XHeapViews) \/ M);\n-\n-  \/\/ Record reserved\n-  _reserved = reserved;\n-\n-  return reserved >= max_capacity;\n-}\n-\n-void XVirtualMemoryManager::nmt_reserve(uintptr_t start, size_t size) {\n-  MemTracker::record_virtual_memory_reserve((void*)start, size, CALLER_PC);\n-  MemTracker::record_virtual_memory_tag((void*)start, mtJavaHeap);\n-}\n-\n-bool XVirtualMemoryManager::is_initialized() const {\n-  return _initialized;\n-}\n-\n-XVirtualMemory XVirtualMemoryManager::alloc(size_t size, bool force_low_address) {\n-  uintptr_t start;\n-\n-  \/\/ Small pages are allocated at low addresses, while medium\/large pages\n-  \/\/ are allocated at high addresses (unless forced to be at a low address).\n-  if (force_low_address || size <= XPageSizeSmall) {\n-    start = _manager.alloc_low_address(size);\n-  } else {\n-    start = _manager.alloc_high_address(size);\n-  }\n-\n-  return XVirtualMemory(start, size);\n-}\n-\n-void XVirtualMemoryManager::free(const XVirtualMemory& vmem) {\n-  _manager.free(vmem.start(), vmem.size());\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.cpp","additions":0,"deletions":208,"binary":false,"changes":208,"status":"deleted"},{"patch":"@@ -1,82 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XVIRTUALMEMORY_HPP\n-#define SHARE_GC_X_XVIRTUALMEMORY_HPP\n-\n-#include \"gc\/x\/xMemory.hpp\"\n-\n-class VMStructs;\n-\n-class XVirtualMemory {\n-  friend class ::VMStructs;\n-\n-private:\n-  uintptr_t _start;\n-  uintptr_t _end;\n-\n-public:\n-  XVirtualMemory();\n-  XVirtualMemory(uintptr_t start, size_t size);\n-\n-  bool is_null() const;\n-  uintptr_t start() const;\n-  uintptr_t end() const;\n-  size_t size() const;\n-\n-  XVirtualMemory split(size_t size);\n-};\n-\n-class XVirtualMemoryManager {\n-private:\n-  XMemoryManager _manager;\n-  uintptr_t      _reserved;\n-  bool           _initialized;\n-\n-  \/\/ Platform specific implementation\n-  void pd_initialize_before_reserve();\n-  void pd_initialize_after_reserve();\n-  bool pd_reserve(uintptr_t addr, size_t size);\n-  void pd_unreserve(uintptr_t addr, size_t size);\n-\n-  bool reserve_contiguous(uintptr_t start, size_t size);\n-  bool reserve_contiguous(size_t size);\n-  size_t reserve_discontiguous(uintptr_t start, size_t size, size_t min_range);\n-  size_t reserve_discontiguous(size_t size);\n-  bool reserve(size_t max_capacity);\n-\n-  void nmt_reserve(uintptr_t start, size_t size);\n-\n-public:\n-  XVirtualMemoryManager(size_t max_capacity);\n-\n-  bool is_initialized() const;\n-\n-  size_t reserved() const;\n-  uintptr_t lowest_available_address() const;\n-\n-  XVirtualMemory alloc(size_t size, bool force_low_address);\n-  void free(const XVirtualMemory& vmem);\n-};\n-\n-#endif \/\/ SHARE_GC_X_XVIRTUALMEMORY_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.hpp","additions":0,"deletions":82,"binary":false,"changes":82,"status":"deleted"},{"patch":"@@ -1,68 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XVIRTUALMEMORY_INLINE_HPP\n-#define SHARE_GC_X_XVIRTUALMEMORY_INLINE_HPP\n-\n-#include \"gc\/x\/xVirtualMemory.hpp\"\n-\n-#include \"gc\/x\/xMemory.inline.hpp\"\n-\n-inline XVirtualMemory::XVirtualMemory() :\n-    _start(UINTPTR_MAX),\n-    _end(UINTPTR_MAX) {}\n-\n-inline XVirtualMemory::XVirtualMemory(uintptr_t start, size_t size) :\n-    _start(start),\n-    _end(start + size) {}\n-\n-inline bool XVirtualMemory::is_null() const {\n-  return _start == UINTPTR_MAX;\n-}\n-\n-inline uintptr_t XVirtualMemory::start() const {\n-  return _start;\n-}\n-\n-inline uintptr_t XVirtualMemory::end() const {\n-  return _end;\n-}\n-\n-inline size_t XVirtualMemory::size() const {\n-  return _end - _start;\n-}\n-\n-inline XVirtualMemory XVirtualMemory::split(size_t size) {\n-  _start += size;\n-  return XVirtualMemory(_start - size, size);\n-}\n-\n-inline size_t XVirtualMemoryManager::reserved() const {\n-  return _reserved;\n-}\n-\n-inline uintptr_t XVirtualMemoryManager::lowest_available_address() const {\n-  return _manager.peek_low_address();\n-}\n-\n-#endif \/\/ SHARE_GC_X_XVIRTUALMEMORY_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.inline.hpp","additions":0,"deletions":68,"binary":false,"changes":68,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBarrier.inline.hpp\"\n-#include \"gc\/x\/xRootsIterator.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xWeakRootsProcessor.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-\n-class XPhantomCleanOopClosure : public OopClosure {\n-public:\n-  virtual void do_oop(oop* p) {\n-    \/\/ Read the oop once, to make sure the liveness check\n-    \/\/ and the later clearing uses the same value.\n-    const oop obj = Atomic::load(p);\n-    if (XBarrier::is_alive_barrier_on_phantom_oop(obj)) {\n-      XBarrier::keep_alive_barrier_on_phantom_oop_field(p);\n-    } else {\n-      \/\/ The destination could have been modified\/reused, in which case\n-      \/\/ we don't want to clear it. However, no one could write the same\n-      \/\/ oop here again (the object would be strongly live and we would\n-      \/\/ not consider clearing such oops), so therefore we don't have an\n-      \/\/ ABA problem here.\n-      Atomic::cmpxchg(p, obj, oop(nullptr));\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-XWeakRootsProcessor::XWeakRootsProcessor(XWorkers* workers) :\n-    _workers(workers) {}\n-\n-class XProcessWeakRootsTask : public XTask {\n-private:\n-  XWeakRootsIterator _weak_roots;\n-\n-public:\n-  XProcessWeakRootsTask() :\n-      XTask(\"XProcessWeakRootsTask\"),\n-      _weak_roots() {}\n-\n-  ~XProcessWeakRootsTask() {\n-    _weak_roots.report_num_dead();\n-  }\n-\n-  virtual void work() {\n-    XPhantomCleanOopClosure cl;\n-    _weak_roots.apply(&cl);\n-  }\n-};\n-\n-void XWeakRootsProcessor::process_weak_roots() {\n-  XProcessWeakRootsTask task;\n-  _workers->run(&task);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xWeakRootsProcessor.cpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XWEAKROOTSPROCESSOR_HPP\n-#define SHARE_GC_X_XWEAKROOTSPROCESSOR_HPP\n-\n-class XWorkers;\n-\n-class XWeakRootsProcessor {\n-private:\n-  XWorkers* const _workers;\n-\n-public:\n-  XWeakRootsProcessor(XWorkers* workers);\n-\n-  void process_weak_roots();\n-};\n-\n-#endif \/\/ SHARE_GC_X_XWEAKROOTSPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xWeakRootsProcessor.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,117 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gc_globals.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xLock.inline.hpp\"\n-#include \"gc\/x\/xStat.hpp\"\n-#include \"gc\/x\/xTask.hpp\"\n-#include \"gc\/x\/xThread.hpp\"\n-#include \"gc\/x\/xWorkers.hpp\"\n-#include \"runtime\/java.hpp\"\n-\n-class XWorkersInitializeTask : public WorkerTask {\n-private:\n-  const uint     _nworkers;\n-  uint           _started;\n-  XConditionLock _lock;\n-\n-public:\n-  XWorkersInitializeTask(uint nworkers) :\n-      WorkerTask(\"XWorkersInitializeTask\"),\n-      _nworkers(nworkers),\n-      _started(0),\n-      _lock() {}\n-\n-  virtual void work(uint worker_id) {\n-    \/\/ Register as worker\n-    XThread::set_worker();\n-\n-    \/\/ Wait for all threads to start\n-    XLocker<XConditionLock> locker(&_lock);\n-    if (++_started == _nworkers) {\n-      \/\/ All threads started\n-      _lock.notify_all();\n-    } else {\n-      while (_started != _nworkers) {\n-        _lock.wait();\n-      }\n-    }\n-  }\n-};\n-\n-XWorkers::XWorkers() :\n-    _workers(\"XWorker\",\n-             UseDynamicNumberOfGCThreads ? ConcGCThreads : MAX2(ConcGCThreads, ParallelGCThreads)) {\n-\n-  if (UseDynamicNumberOfGCThreads) {\n-    log_info_p(gc, init)(\"GC Workers: %u (dynamic)\", _workers.max_workers());\n-  } else {\n-    log_info_p(gc, init)(\"GC Workers: %u\/%u (static)\", ConcGCThreads, _workers.max_workers());\n-  }\n-\n-  \/\/ Initialize worker threads\n-  _workers.initialize_workers();\n-  _workers.set_active_workers(_workers.max_workers());\n-  if (_workers.active_workers() != _workers.max_workers()) {\n-    vm_exit_during_initialization(\"Failed to create XWorkers\");\n-  }\n-\n-  \/\/ Execute task to register threads as workers\n-  XWorkersInitializeTask task(_workers.max_workers());\n-  _workers.run_task(&task);\n-}\n-\n-uint XWorkers::active_workers() const {\n-  return _workers.active_workers();\n-}\n-\n-void XWorkers::set_active_workers(uint nworkers) {\n-  log_info(gc, task)(\"Using %u workers\", nworkers);\n-  _workers.set_active_workers(nworkers);\n-}\n-\n-void XWorkers::run(XTask* task) {\n-  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n-  XStatWorkers::at_start();\n-  _workers.run_task(task->worker_task());\n-  XStatWorkers::at_end();\n-}\n-\n-void XWorkers::run_all(XTask* task) {\n-  \/\/ Save number of active workers\n-  const uint prev_active_workers = _workers.active_workers();\n-\n-  \/\/ Execute task using all workers\n-  _workers.set_active_workers(_workers.max_workers());\n-  log_debug(gc, task)(\"Executing Task: %s, Active Workers: %u\", task->name(), active_workers());\n-  _workers.run_task(task->worker_task());\n-\n-  \/\/ Restore number of active workers\n-  _workers.set_active_workers(prev_active_workers);\n-}\n-\n-void XWorkers::threads_do(ThreadClosure* tc) const {\n-  _workers.threads_do(tc);\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xWorkers.cpp","additions":0,"deletions":117,"binary":false,"changes":117,"status":"deleted"},{"patch":"@@ -1,48 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_XWORKERS_HPP\n-#define SHARE_GC_X_XWORKERS_HPP\n-\n-#include \"gc\/shared\/workerThread.hpp\"\n-\n-class ThreadClosure;\n-class XTask;\n-\n-class XWorkers {\n-private:\n-  WorkerThreads _workers;\n-\n-public:\n-  XWorkers();\n-\n-  uint active_workers() const;\n-  void set_active_workers(uint nworkers);\n-\n-  void run(XTask* task);\n-  void run_all(XTask* task);\n-\n-  void threads_do(ThreadClosure* tc) const;\n-};\n-\n-#endif \/\/ SHARE_GC_X_XWORKERS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xWorkers.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"deleted"},{"patch":"@@ -1,39 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_X_X_GLOBALS_HPP\n-#define SHARE_GC_X_X_GLOBALS_HPP\n-\n-#define GC_X_FLAGS(develop,                                          \\\n-                   develop_pd,                                       \\\n-                   product,                                          \\\n-                   product_pd,                                       \\\n-                   range,                                            \\\n-                   constraint)                                       \\\n-                                                                     \\\n-  product(bool, ZVerifyViews, false, DIAGNOSTIC,                     \\\n-          \"Verify heap view accesses\")                               \\\n-                                                                     \\\n-\/\/ end of GC_X_FLAGS\n-\n-#endif \/\/ SHARE_GC_X_X_GLOBALS_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/x_globals.hpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"deleted"},{"patch":"@@ -1,68 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_SHARED_VMSTRUCTS_Z_SHARED_HPP\n-#define SHARE_GC_Z_SHARED_VMSTRUCTS_Z_SHARED_HPP\n-\n-#include \"gc\/x\/vmStructs_x.hpp\"\n-#include \"gc\/z\/vmStructs_z.hpp\"\n-\n-#define VM_STRUCTS_Z_SHARED(nonstatic_field, volatile_nonstatic_field, static_field)    \\\n-  VM_STRUCTS_X(                                                                         \\\n-    nonstatic_field,                                                                    \\\n-    volatile_nonstatic_field,                                                           \\\n-    static_field)                                                                       \\\n-                                                                                        \\\n-  VM_STRUCTS_Z(                                                                         \\\n-    nonstatic_field,                                                                    \\\n-    volatile_nonstatic_field,                                                           \\\n-    static_field)\n-\n-#define VM_INT_CONSTANTS_Z_SHARED(declare_constant, declare_constant_with_value)        \\\n-  VM_INT_CONSTANTS_X(                                                                   \\\n-    declare_constant,                                                                   \\\n-    declare_constant_with_value)                                                        \\\n-                                                                                        \\\n-  VM_INT_CONSTANTS_Z(                                                                   \\\n-    declare_constant,                                                                   \\\n-    declare_constant_with_value)\n-\n-#define VM_LONG_CONSTANTS_Z_SHARED(declare_constant)                                    \\\n-  VM_LONG_CONSTANTS_X(                                                                  \\\n-    declare_constant)                                                                   \\\n-                                                                                        \\\n-  VM_LONG_CONSTANTS_Z(                                                                  \\\n-    declare_constant)\n-\n-#define VM_TYPES_Z_SHARED(declare_type, declare_toplevel_type, declare_integer_type)    \\\n-  VM_TYPES_X(                                                                           \\\n-    declare_type,                                                                       \\\n-    declare_toplevel_type,                                                              \\\n-    declare_integer_type)                                                               \\\n-                                                                                        \\\n-  VM_TYPES_Z(                                                                           \\\n-    declare_type,                                                                       \\\n-    declare_toplevel_type,                                                              \\\n-    declare_integer_type)\n-\n-#endif \/\/ SHARE_GC_Z_SHARED_VMSTRUCTS_Z_SHARED_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/vmStructs_z_shared.hpp","additions":0,"deletions":68,"binary":false,"changes":68,"status":"deleted"},{"patch":"@@ -1,87 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcArguments.hpp\"\n-#include \"gc\/x\/xArguments.hpp\"\n-#include \"gc\/z\/shared\/zSharedArguments.hpp\"\n-#include \"gc\/z\/zArguments.hpp\"\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/java.hpp\"\n-\n-void ZSharedArguments::initialize_alignments() {\n-  if (ZGenerational) {\n-    ZArguments::initialize_alignments();\n-  } else {\n-    XArguments::initialize_alignments();\n-  }\n-}\n-\n-void ZSharedArguments::initialize_heap_flags_and_sizes() {\n-  GCArguments::initialize_heap_flags_and_sizes();\n-\n-  if (ZGenerational) {\n-    ZArguments::initialize_heap_flags_and_sizes();\n-  } else {\n-    XArguments::initialize_heap_flags_and_sizes();\n-  }\n-}\n-\n-void ZSharedArguments::initialize() {\n-  GCArguments::initialize();\n-\n-  if (ZGenerational) {\n-    ZArguments::initialize();\n-  } else {\n-    XArguments::initialize();\n-  }\n-}\n-\n-size_t ZSharedArguments::heap_virtual_to_physical_ratio() {\n-  if (ZGenerational) {\n-    return ZArguments::heap_virtual_to_physical_ratio();\n-  } else {\n-    return XArguments::heap_virtual_to_physical_ratio();\n-  }\n-}\n-\n-size_t ZSharedArguments::conservative_max_heap_alignment() {\n-  return 0;\n-}\n-\n-CollectedHeap* ZSharedArguments::create_heap() {\n-  if (ZGenerational) {\n-    return ZArguments::create_heap();\n-  } else {\n-    return XArguments::create_heap();\n-  }\n-}\n-\n-bool ZSharedArguments::is_supported() const {\n-  if (ZGenerational) {\n-    return ZArguments::is_os_supported();\n-  } else {\n-    return XArguments::is_os_supported();\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/zSharedArguments.cpp","additions":0,"deletions":87,"binary":false,"changes":87,"status":"deleted"},{"patch":"@@ -1,46 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_SHARED_ZSHAREDARGUMENTS_HPP\n-#define SHARE_GC_Z_SHARED_ZSHAREDARGUMENTS_HPP\n-\n-#include \"gc\/shared\/gcArguments.hpp\"\n-\n-class CollectedHeap;\n-\n-class ZSharedArguments : public GCArguments {\n-private:\n-  virtual void initialize_alignments();\n-  virtual void initialize_heap_flags_and_sizes();\n-\n-  virtual void initialize();\n-  virtual size_t conservative_max_heap_alignment();\n-  virtual size_t heap_virtual_to_physical_ratio();\n-  virtual CollectedHeap* create_heap();\n-\n-  virtual bool is_supported() const;\n-\n-  bool is_os_supported() const;\n-};\n-\n-#endif \/\/ SHARE_GC_Z_SHARED_ZSHAREDARGUMENTS_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/zSharedArguments.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"deleted"},{"patch":"@@ -1,107 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#ifndef SHARE_GC_Z_SHARED_Z_SHARED_GLOBALS_HPP\n-#define SHARE_GC_Z_SHARED_Z_SHARED_GLOBALS_HPP\n-\n-#include \"gc\/x\/x_globals.hpp\"\n-#include \"gc\/z\/z_globals.hpp\"\n-\n-#define GC_Z_SHARED_FLAGS(develop,                                          \\\n-                          develop_pd,                                       \\\n-                          product,                                          \\\n-                          product_pd,                                       \\\n-                          range,                                            \\\n-                          constraint)                                       \\\n-                                                                            \\\n-  product(double, ZAllocationSpikeTolerance, 2.0,                           \\\n-          \"Allocation spike tolerance factor\")                              \\\n-                                                                            \\\n-  \/* Updated in arguments parsing to ZGenerational ? 5.0 : 25.0 *\/          \\\n-  product(double, ZFragmentationLimit, 0 \/* ignored *\/,                     \\\n-          \"Maximum allowed heap fragmentation\")                             \\\n-          range(0, 100)                                                     \\\n-                                                                            \\\n-  product(size_t, ZMarkStackSpaceLimit, 8*G,                                \\\n-          \"Maximum number of bytes allocated for mark stacks\")              \\\n-          range(32*M, 1024*G)                                               \\\n-                                                                            \\\n-  product(double, ZCollectionInterval, 0,                                   \\\n-          \"Force GC at a fixed time interval (in seconds). \"                \\\n-          \"Backwards compatible alias for ZCollectionIntervalMajor\")        \\\n-                                                                            \\\n-  product(bool, ZProactive, true,                                           \\\n-          \"Enable proactive GC cycles\")                                     \\\n-                                                                            \\\n-  product(bool, ZUncommit, true,                                            \\\n-          \"Uncommit unused memory\")                                         \\\n-                                                                            \\\n-  product(uintx, ZUncommitDelay, 5 * 60,                                    \\\n-          \"Uncommit memory if it has been unused for the specified \"        \\\n-          \"amount of time (in seconds)\")                                    \\\n-                                                                            \\\n-  product(double, ZAsyncUnmappingLimit, 100.0, DIAGNOSTIC,                  \\\n-          \"Specify the max amount (percentage of max heap size) of async \"  \\\n-          \"unmapping that can be in-flight before unmapping requests are \"  \\\n-          \"temporarily forced to be synchronous instead. \"                  \\\n-          \"The default means after an amount of pages proportional to the \" \\\n-          \"max capacity is enqueued, we resort to synchronous unmapping.\")  \\\n-                                                                            \\\n-  product(uint, ZStatisticsInterval, 10, DIAGNOSTIC,                        \\\n-          \"Time between statistics print outs (in seconds)\")                \\\n-          range(1, (uint)-1)                                                \\\n-                                                                            \\\n-  product(bool, ZStressRelocateInPlace, false, DIAGNOSTIC,                  \\\n-          \"Always relocate pages in-place\")                                 \\\n-                                                                            \\\n-  product(bool, ZVerifyRoots, trueInDebug, DIAGNOSTIC,                      \\\n-          \"Verify roots\")                                                   \\\n-                                                                            \\\n-  product(bool, ZVerifyObjects, false, DIAGNOSTIC,                          \\\n-          \"Verify objects\")                                                 \\\n-                                                                            \\\n-  product(bool, ZVerifyMarking, trueInDebug, DIAGNOSTIC,                    \\\n-          \"Verify marking stacks\")                                          \\\n-                                                                            \\\n-  product(bool, ZVerifyForwarding, false, DIAGNOSTIC,                       \\\n-          \"Verify forwarding tables\")                                       \\\n-                                                                            \\\n-  GC_X_FLAGS(                                                               \\\n-    develop,                                                                \\\n-    develop_pd,                                                             \\\n-    product,                                                                \\\n-    product_pd,                                                             \\\n-    range,                                                                  \\\n-    constraint)                                                             \\\n-                                                                            \\\n-  GC_Z_FLAGS(                                                               \\\n-    develop,                                                                \\\n-    develop_pd,                                                             \\\n-    product,                                                                \\\n-    product_pd,                                                             \\\n-    range,                                                                  \\\n-    constraint)\n-\n-\/\/ end of GC_Z_SHARED_FLAGS\n-\n-#endif \/\/ SHARE_GC_Z_SHARED_Z_SHARED_GLOBALS_HPP\n","filename":"src\/hotspot\/share\/gc\/z\/shared\/z_shared_globals.hpp","additions":0,"deletions":107,"binary":false,"changes":107,"status":"deleted"},{"patch":"@@ -41,0 +41,2 @@\n+  GCArguments::initialize_heap_flags_and_sizes();\n+\n@@ -120,0 +122,2 @@\n+  GCArguments::initialize();\n+\n@@ -223,0 +227,4 @@\n+size_t ZArguments::conservative_max_heap_alignment() {\n+  return 0;\n+}\n+\n@@ -231,1 +239,1 @@\n-bool ZArguments::is_supported() {\n+bool ZArguments::is_supported() const {\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-class ZArguments : AllStatic {\n+class ZArguments : public GCArguments {\n@@ -35,9 +35,0 @@\n-public:\n-  static void initialize_alignments();\n-  static void initialize_heap_flags_and_sizes();\n-  static void initialize();\n-  static size_t heap_virtual_to_physical_ratio();\n-  static CollectedHeap* create_heap();\n-\n-  static bool is_supported();\n-\n@@ -45,0 +36,10 @@\n+\n+public:\n+  virtual void initialize_alignments();\n+  virtual void initialize_heap_flags_and_sizes();\n+  virtual void initialize();\n+  virtual size_t conservative_max_heap_alignment();\n+  virtual size_t heap_virtual_to_physical_ratio();\n+  virtual CollectedHeap* create_heap();\n+\n+  virtual bool is_supported() const;\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.hpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -37,0 +37,25 @@\n+  product(double, ZAllocationSpikeTolerance, 2.0,                           \\\n+          \"Allocation spike tolerance factor\")                              \\\n+                                                                            \\\n+  product(double, ZFragmentationLimit, 5.0,                                 \\\n+          \"Maximum allowed heap fragmentation\")                             \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(size_t, ZMarkStackSpaceLimit, 8*G,                                \\\n+          \"Maximum number of bytes allocated for mark stacks\")              \\\n+          range(32*M, 1024*G)                                               \\\n+                                                                            \\\n+  product(double, ZCollectionInterval, 0,                                   \\\n+          \"Force GC at a fixed time interval (in seconds). \"                \\\n+          \"Backwards compatible alias for ZCollectionIntervalMajor\")        \\\n+                                                                            \\\n+  product(bool, ZProactive, true,                                           \\\n+          \"Enable proactive GC cycles\")                                     \\\n+                                                                            \\\n+  product(bool, ZUncommit, true,                                            \\\n+          \"Uncommit unused memory\")                                         \\\n+                                                                            \\\n+  product(uintx, ZUncommitDelay, 5 * 60,                                    \\\n+          \"Uncommit memory if it has been unused for the specified \"        \\\n+          \"amount of time (in seconds)\")                                    \\\n+                                                                            \\\n@@ -50,0 +75,26 @@\n+  product(double, ZAsyncUnmappingLimit, 100.0, DIAGNOSTIC,                  \\\n+          \"Specify the max amount (percentage of max heap size) of async \"  \\\n+          \"unmapping that can be in-flight before unmapping requests are \"  \\\n+          \"temporarily forced to be synchronous instead. \"                  \\\n+          \"The default means after an amount of pages proportional to the \" \\\n+          \"max capacity is enqueued, we resort to synchronous unmapping.\")  \\\n+                                                                            \\\n+  product(uint, ZStatisticsInterval, 10, DIAGNOSTIC,                        \\\n+          \"Time between statistics print outs (in seconds)\")                \\\n+          range(1, (uint)-1)                                                \\\n+                                                                            \\\n+  product(bool, ZStressRelocateInPlace, false, DIAGNOSTIC,                  \\\n+          \"Always relocate pages in-place\")                                 \\\n+                                                                            \\\n+  product(bool, ZVerifyRoots, trueInDebug, DIAGNOSTIC,                      \\\n+          \"Verify roots\")                                                   \\\n+                                                                            \\\n+  product(bool, ZVerifyObjects, false, DIAGNOSTIC,                          \\\n+          \"Verify objects\")                                                 \\\n+                                                                            \\\n+  product(bool, ZVerifyMarking, trueInDebug, DIAGNOSTIC,                    \\\n+          \"Verify marking stacks\")                                          \\\n+                                                                            \\\n+  product(bool, ZVerifyForwarding, false, DIAGNOSTIC,                       \\\n+          \"Verify forwarding tables\")                                       \\\n+                                                                            \\\n@@ -67,3 +118,0 @@\n-  develop(bool, ZVerifyOops, false,                                         \\\n-          \"Verify accessed oops\")                                           \\\n-                                                                            \\\n@@ -74,0 +122,3 @@\n+  develop(bool, ZVerifyOops, false,                                         \\\n+          \"Verify accessed oops\")                                           \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/z\/z_globals.hpp","additions":54,"deletions":3,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -39,2 +39,0 @@\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n-#include \"gc\/x\/xThreadLocalData.hpp\"\n@@ -176,17 +174,3 @@\n-    if (ZGenerational) {\n-      ZPointerVectorLoadBadMask_address   = (address) &ZPointerVectorLoadBadMask;\n-      ZPointerVectorStoreBadMask_address  = (address) &ZPointerVectorStoreBadMask;\n-      ZPointerVectorStoreGoodMask_address = (address) &ZPointerVectorStoreGoodMask;\n-    } else {\n-      thread_address_bad_mask_offset = in_bytes(XThreadLocalData::address_bad_mask_offset());\n-      \/\/ Initialize the old names for compatibility.  The proper XBarrierSetRuntime names are\n-      \/\/ exported as addresses in vmStructs_jvmci.cpp as are the new ZBarrierSetRuntime names.\n-      ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded              = XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr();\n-      ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded         = XBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded_addr();\n-      ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded      = XBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded_addr();\n-      ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded         = XBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded_addr();\n-      ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded    = XBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded_addr();\n-      ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded = XBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded_addr();\n-      ZBarrierSetRuntime_load_barrier_on_oop_array                        = XBarrierSetRuntime::load_barrier_on_oop_array_addr();\n-      ZBarrierSetRuntime_clone                                            = XBarrierSetRuntime::clone_addr();\n-    }\n+    ZPointerVectorLoadBadMask_address   = (address) &ZPointerVectorLoadBadMask;\n+    ZPointerVectorStoreBadMask_address  = (address) &ZPointerVectorStoreBadMask;\n+    ZPointerVectorStoreGoodMask_address = (address) &ZPointerVectorStoreGoodMask;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":3,"deletions":19,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -53,1 +53,0 @@\n-#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n@@ -835,9 +834,0 @@\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::load_barrier_on_oop_field_preloaded))                      \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::load_barrier_on_weak_oop_field_preloaded))                 \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::load_barrier_on_phantom_oop_field_preloaded))              \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::weak_load_barrier_on_oop_field_preloaded))                 \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::weak_load_barrier_on_weak_oop_field_preloaded))            \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::weak_load_barrier_on_phantom_oop_field_preloaded))         \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::load_barrier_on_oop_array))                                \\\n-  ZGC_ONLY(DECLARE_FUNCTION_FROM_ADDR(declare_function_with_value, XBarrierSetRuntime::clone))                                                    \\\n-                                                                                                                      \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -181,1 +181,1 @@\n-void oopDesc::obj_field_put_raw(int offset, oop value)                { assert(!(UseZGC && ZGenerational), \"Generational ZGC must use store barriers\");\n+void oopDesc::obj_field_put_raw(int offset, oop value)                { assert(!UseZGC, \"ZGC must use store barriers\");\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -91,14 +91,1 @@\n-inline oop stackChunkOopDesc::cont() const                {\n-  if (UseZGC && !ZGenerational) {\n-    assert(!UseCompressedOops, \"Non-generational ZGC does not support compressed oops\");\n-    \/\/ The state of the cont oop is used by XCollectedHeap::requires_barriers,\n-    \/\/ to determine the age of the stackChunkOopDesc. For that to work, it is\n-    \/\/ only the GC that is allowed to perform a load barrier on the oop.\n-    \/\/ This function is used by non-GC code and therfore create a stack-local\n-    \/\/ copy on the oop and perform the load barrier on that copy instead.\n-    oop obj = jdk_internal_vm_StackChunk::cont_raw<oop>(as_oop());\n-    obj = (oop)NativeAccess<>::oop_load(&obj);\n-    return obj;\n-  }\n-  return jdk_internal_vm_StackChunk::cont(as_oop());\n-}\n+inline oop stackChunkOopDesc::cont() const                { return jdk_internal_vm_StackChunk::cont(as_oop()); }\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.inline.hpp","additions":1,"deletions":14,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -427,5 +427,1 @@\n-    if (ZGenerational) {\n-      return ZHeap::heap()->is_old(to_zaddress(p));\n-    } else {\n-      return Universe::heap()->is_in(p);\n-    }\n+    return ZHeap::heap()->is_old(to_zaddress(p));\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -508,1 +508,0 @@\n-  { \"ZGenerational\",                JDK_Version::jdk(23), JDK_Version::undefined(), JDK_Version::undefined() },\n@@ -524,1 +523,1 @@\n-\n+  { \"ZGenerational\",                JDK_Version::jdk(23), JDK_Version::jdk(24), JDK_Version::undefined() },\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1445,3 +1445,1 @@\n-    if (ZGenerational) {\n-      ZStackChunkGCData::initialize(chunk);\n-    }\n+    ZStackChunkGCData::initialize(chunk);\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-import sun.jvm.hotspot.gc.x.*;\n@@ -1127,4 +1126,0 @@\n-                        } else if (collHeap instanceof XCollectedHeap) {\n-                          XCollectedHeap heap = (XCollectedHeap) collHeap;\n-                          anno = \"ZHeap \";\n-                          bad = false;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/HSDB.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,77 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-\n-class XAddress {\n-    static long as_long(Address value) {\n-        if (value == null) {\n-            return 0;\n-        }\n-        return value.asLongValue();\n-    };\n-\n-    static boolean is_null(Address value) {\n-        return value == null;\n-    }\n-\n-    static boolean is_weak_bad(Address value) {\n-        return (as_long(value) & XGlobals.XAddressWeakBadMask()) != 0L;\n-    }\n-\n-    static boolean is_weak_good(Address value) {\n-        return !is_weak_bad(value) && !is_null(value);\n-    }\n-\n-    static boolean is_weak_good_or_null(Address value) {\n-        return !is_weak_bad(value);\n-    }\n-\n-    static long offset(Address address) {\n-        return as_long(address) & XGlobals.XAddressOffsetMask();\n-    }\n-\n-    static Address good(Address value) {\n-        return VM.getVM().getDebugger().newAddress(offset(value) | XGlobals.XAddressGoodMask());\n-    }\n-\n-    static Address good_or_null(Address value) {\n-        return is_null(value) ? value : good(value);\n-    }\n-\n-    private static boolean isPowerOf2(long value) {\n-        return (value != 0L) && ((value & (value - 1)) == 0L);\n-    }\n-\n-    static boolean isIn(Address addr) {\n-        long value = as_long(addr);\n-        if (!isPowerOf2(value & ~XGlobals.XAddressOffsetMask())) {\n-            return false;\n-        }\n-        return (value & (XGlobals.XAddressMetadataMask() & ~XGlobals.XAddressMetadataFinalizable())) != 0L;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XAddress.java","additions":0,"deletions":77,"binary":false,"changes":77,"status":"deleted"},{"patch":"@@ -1,71 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XAttachedArrayForForwarding extends VMObject {\n-    private static CIntegerField lengthField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XAttachedArrayForForwarding\");\n-\n-        lengthField = type.getCIntegerField(\"_length\");\n-    }\n-\n-    public XAttachedArrayForForwarding(Address addr) {\n-        super(addr);\n-    }\n-\n-    public long length() {\n-        return lengthField.getValue(addr);\n-    }\n-\n-    \/\/ ObjectT: XForwarding\n-    \/\/  ArrayT: XForwardingEntry\n-    \/\/\n-    \/\/ template <typename ObjectT, typename ArrayT>\n-    \/\/ inline size_t XAttachedArray<ObjectT, ArrayT>::object_size()\n-    private long objectSize() {\n-        return XUtils.alignUp(XForwarding.getSize(), XForwardingEntry.getSize());\n-    }\n-\n-    \/\/ ArrayT* operator()(const ObjectT* obj) const\n-    public XForwardingEntry get(XForwarding obj) {\n-        Address o = obj.getAddress().addOffsetTo(objectSize());\n-        return VMObjectFactory.newObject(XForwardingEntry.class, o);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XAttachedArrayForForwarding.java","additions":0,"deletions":71,"binary":false,"changes":71,"status":"deleted"},{"patch":"@@ -1,71 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-\n-class XBarrier {\n-    private static boolean is_weak_good_or_null_fast_path(Address addr) {\n-        return XAddress.is_weak_good_or_null(addr);\n-    }\n-\n-    private static Address weak_load_barrier_on_oop_slow_path(Address addr) {\n-        return XAddress.is_weak_good(addr) ? XAddress.good(addr) : relocate_or_remap(addr);\n-    }\n-\n-    private static boolean during_relocate() {\n-        return XGlobals.XGlobalPhase() == XGlobals.XPhaseRelocate;\n-    }\n-\n-    private static Address relocate(Address addr) {\n-        return zheap().relocate_object(addr);\n-    }\n-\n-    private static XHeap zheap() {\n-        XCollectedHeap zCollectedHeap = (XCollectedHeap)VM.getVM().getUniverse().heap();\n-        return zCollectedHeap.heap();\n-    }\n-\n-    private static Address remap(Address addr) {\n-        return zheap().remapObject(addr);\n-    }\n-\n-    private static Address relocate_or_remap(Address addr) {\n-        return during_relocate() ? relocate(addr) : remap(addr);\n-    }\n-\n-    static Address weak_barrier(Address o) {\n-        \/\/ Fast path\n-        if (is_weak_good_or_null_fast_path(o)) {\n-            \/\/ Return the good address instead of the weak good address\n-            \/\/ to ensure that the currently active heap view is used.\n-            return XAddress.good_or_null(o);\n-        }\n-\n-        \/\/ Slow path\n-        return weak_load_barrier_on_oop_slow_path(o);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XBarrier.java","additions":0,"deletions":71,"binary":false,"changes":71,"status":"deleted"},{"patch":"@@ -1,139 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import java.io.PrintStream;\n-import java.util.Iterator;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.debugger.OopHandle;\n-import sun.jvm.hotspot.gc.shared.CollectedHeap;\n-import sun.jvm.hotspot.gc.shared.CollectedHeapName;\n-import sun.jvm.hotspot.gc.shared.LiveRegionsClosure;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-import sun.jvm.hotspot.utilities.BitMapInterface;\n-\n-\/\/ Mirror class for XCollectedHeap.\n-\n-public class XCollectedHeap extends CollectedHeap {\n-    private static long zHeapFieldOffset;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XCollectedHeap\");\n-\n-        zHeapFieldOffset = type.getAddressField(\"_heap\").getOffset();\n-    }\n-\n-    public XHeap heap() {\n-        Address heapAddr = addr.addOffsetTo(zHeapFieldOffset);\n-        return VMObjectFactory.newObject(XHeap.class, heapAddr);\n-    }\n-\n-    @Override\n-    public CollectedHeapName kind() {\n-        return CollectedHeapName.Z;\n-    }\n-\n-    @Override\n-    public void printOn(PrintStream tty) {\n-        heap().printOn(tty);\n-    }\n-\n-    public XCollectedHeap(Address addr) {\n-        super(addr);\n-    }\n-\n-    @Override\n-    public long capacity() {\n-        return heap().capacity();\n-    }\n-\n-    @Override\n-    public long used() {\n-        return heap().used();\n-    }\n-\n-    @Override\n-    public boolean isInReserved(Address a) {\n-        return heap().isIn(a);\n-    }\n-\n-    private OopHandle oop_load_barrier(Address oopAddress) {\n-        oopAddress = XBarrier.weak_barrier(oopAddress);\n-        if (oopAddress == null) {\n-            return null;\n-        }\n-\n-        return oopAddress.addOffsetToAsOopHandle(0);\n-    }\n-\n-    @Override\n-    public OopHandle oop_load_at(OopHandle handle, long offset) {\n-        assert(!VM.getVM().isCompressedOopsEnabled());\n-\n-        Address oopAddress = handle.getAddressAt(offset);\n-\n-        return oop_load_barrier(oopAddress);\n-    }\n-\n-    \/\/ addr can be either in heap or in native\n-    @Override\n-    public OopHandle oop_load_in_native(Address addr) {\n-        Address oopAddress = addr.getAddressAt(0);\n-        return oop_load_barrier(oopAddress);\n-    }\n-\n-    public String oopAddressDescription(OopHandle handle) {\n-        Address origOop = XOop.to_address(handle);\n-        Address loadBarrieredOop = XBarrier.weak_barrier(origOop);\n-        if (!origOop.equals(loadBarrieredOop)) {\n-            return origOop + \" (\" + loadBarrieredOop.toString() + \")\";\n-        } else {\n-            return handle.toString();\n-        }\n-    }\n-\n-    @Override\n-    public void liveRegionsIterate(LiveRegionsClosure closure) {\n-        Iterator<XPage> iter = heap().pageTable().activePagesIterator();\n-        while (iter.hasNext()) {\n-            XPage page = iter.next();\n-            closure.doLiveRegions(page);\n-        }\n-    }\n-\n-    @Override\n-    public BitMapInterface createBitMap(long size) {\n-        \/\/ Ignores the size\n-        return new XExternalBitMap(this);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XCollectedHeap.java","additions":0,"deletions":139,"binary":false,"changes":139,"status":"deleted"},{"patch":"@@ -1,111 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import java.util.HashMap;\n-\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.utilities.BitMap;\n-import sun.jvm.hotspot.utilities.BitMapInterface;\n-\n-\/** Discontiguous bitmap for ZGC. *\/\n-public class XExternalBitMap implements BitMapInterface {\n-    private XPageTable pageTable;\n-    private final long oopSize;\n-\n-    private HashMap<XPage, BitMap> pageToBitMap = new HashMap<XPage, BitMap>();\n-\n-    public XExternalBitMap(XCollectedHeap collectedHeap) {\n-        pageTable = collectedHeap.heap().pageTable();\n-        oopSize = VM.getVM().getOopSize();\n-    }\n-\n-    private XPage getPage(long zOffset) {\n-        if (zOffset > XGlobals.XAddressOffsetMask()) {\n-            throw new RuntimeException(\"Not a Z offset: \" + zOffset);\n-        }\n-\n-        XPage page = pageTable.get(XUtils.longToAddress(zOffset));\n-        if (page == null) {\n-            throw new RuntimeException(\"Address not in pageTable: \" + zOffset);\n-        }\n-        return page;\n-    }\n-\n-    private BitMap getOrAddBitMap(XPage page) {\n-        BitMap bitMap = pageToBitMap.get(page);\n-        if (bitMap == null) {\n-            long size = page.size();\n-\n-            long maxNumObjects = size >>> page.object_alignment_shift();\n-            if (maxNumObjects > Integer.MAX_VALUE) {\n-                throw new RuntimeException(\"int overflow\");\n-            }\n-            int intMaxNumObjects = (int)maxNumObjects;\n-\n-            bitMap = new BitMap(intMaxNumObjects);\n-            pageToBitMap.put(page,  bitMap);\n-        }\n-\n-        return bitMap;\n-    }\n-\n-    private int pageLocalBitMapIndex(XPage page, long zOffset) {\n-        long pageLocalZOffset = zOffset - page.start();\n-        return (int)(pageLocalZOffset >>> page.object_alignment_shift());\n-    }\n-\n-    private long convertToZOffset(long offset) {\n-        long addr = oopSize * offset;\n-        return addr & XGlobals.XAddressOffsetMask();\n-    }\n-\n-    @Override\n-    public boolean at(long offset) {\n-        long zOffset = convertToZOffset(offset);\n-        XPage page = getPage(zOffset);\n-        BitMap bitMap = getOrAddBitMap(page);\n-        int index = pageLocalBitMapIndex(page, zOffset);\n-\n-        return bitMap.at(index);\n-    }\n-\n-    @Override\n-    public void atPut(long offset, boolean value) {\n-        long zOffset = convertToZOffset(offset);\n-        XPage page = getPage(zOffset);\n-        BitMap bitMap = getOrAddBitMap(page);\n-        int index = pageLocalBitMapIndex(page, zOffset);\n-\n-        bitMap.atPut(index, value);\n-    }\n-\n-    @Override\n-    public void clear() {\n-        for (BitMap bitMap : pageToBitMap.values()) {\n-            bitMap.clear();\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XExternalBitMap.java","additions":0,"deletions":111,"binary":false,"changes":111,"status":"deleted"},{"patch":"@@ -1,136 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import java.util.Iterator;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XForwarding extends VMObject {\n-    private static Type type;\n-    private static long virtualFieldOffset;\n-    private static long entriesFieldOffset;\n-    private static CIntegerField objectAlignmentShiftField;\n-    private static CIntegerField refCountField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        type = db.lookupType(\"XForwarding\");\n-\n-        virtualFieldOffset = type.getField(\"_virtual\").getOffset();\n-        entriesFieldOffset = type.getField(\"_entries\").getOffset();\n-        objectAlignmentShiftField = type.getCIntegerField(\"_object_alignment_shift\");\n-        refCountField = type.getCIntegerField(\"_ref_count\");\n-    }\n-\n-    public XForwarding(Address addr) {\n-        super(addr);\n-    }\n-\n-    public static long getSize() {\n-        return type.getSize();\n-    }\n-\n-    private XVirtualMemory virtual() {\n-        return VMObjectFactory.newObject(XVirtualMemory.class, addr.addOffsetTo(virtualFieldOffset));\n-    }\n-\n-    private XAttachedArrayForForwarding entries() {\n-        return VMObjectFactory.newObject(XAttachedArrayForForwarding.class, addr.addOffsetTo(entriesFieldOffset));\n-    }\n-\n-    public long start() {\n-        return virtual().start();\n-    }\n-\n-    public int objectAlignmentShift() {\n-        return (int)objectAlignmentShiftField.getValue(addr);\n-    }\n-\n-    public boolean retainPage() {\n-        return refCountField.getValue(addr) > 0;\n-    }\n-\n-    private XForwardingEntry at(long cursor) {\n-        long offset = XForwardingEntry.getSize() * cursor;\n-        Address entryAddress = entries().get(this).getAddress().addOffsetTo(offset);\n-        return VMObjectFactory.newObject(XForwardingEntry.class, entryAddress);\n-    }\n-\n-    private class XForwardEntryIterator implements Iterator<XForwardingEntry> {\n-\n-        private long cursor;\n-\n-        private XForwardingEntry nextEntry;\n-\n-        public XForwardEntryIterator(long fromIndex) {\n-            long mask = entries().length() - 1;\n-            long hash = XHash.uint32_to_uint32(fromIndex);\n-            cursor = hash & mask;\n-            nextEntry = at(cursor);\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return nextEntry.populated();\n-        }\n-\n-        @Override\n-        public XForwardingEntry next() {\n-            XForwardingEntry entry = nextEntry;\n-\n-            long mask = entries().length() - 1;\n-            cursor = (cursor + 1) & mask;\n-            nextEntry = at(cursor);\n-\n-            return entry;\n-        }\n-\n-        public XForwardingEntry peak() {\n-            return nextEntry;\n-        }\n-    }\n-\n-    public XForwardingEntry find(long fromIndex) {\n-        XForwardEntryIterator itr = new XForwardEntryIterator(fromIndex);\n-        while (itr.hasNext()) {\n-            XForwardingEntry entry = itr.next();\n-            if (entry.fromIndex() == fromIndex) {\n-                return entry;\n-            }\n-        }\n-        return itr.peak();\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwarding.java","additions":0,"deletions":136,"binary":false,"changes":136,"status":"deleted"},{"patch":"@@ -1,97 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XForwardingEntry extends VMObject {\n-    private static Type type;\n-    private static CIntegerField entryField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        type = db.lookupType(\"XForwardingEntry\");\n-\n-        entryField = type.getCIntegerField(\"_entry\");\n-    }\n-\n-    public static long getSize() {\n-        return type.getSize();\n-    }\n-\n-    public XForwardingEntry(Address addr) {\n-        super(addr);\n-    }\n-\n-    public long entry() {\n-        return entryField.getValue(addr);\n-    }\n-\n-    \/\/ typedef XBitField<uint64_t, bool,   0,   1> field_populated\n-    private boolean fieldPopulatedDecode(long value) {\n-        long FieldMask = (1L << 1) - 1;\n-        int FieldShift = 1;\n-        int ValueShift = 0;\n-        return (((value >>> FieldShift) & FieldMask) << ValueShift) != 0L;\n-    }\n-\n-    \/\/ typedef XBitField<uint64_t, size_t, 1,  45> field_to_offset;\n-    private long fieldToOffsetDecode(long value) {\n-        long FieldMask = (1L << 45) - 1;\n-        int FieldShift = 1;\n-        int ValueShift = 0;\n-        return ((value >>> FieldShift) & FieldMask) << ValueShift;\n-    }\n-\n-    \/\/ typedef XBitField<uint64_t, size_t, 46, 18> field_from_index;\n-    private long fieldFromIndexDecode(long value) {\n-        long FieldMask = (1L << 18) - 1;\n-        int FieldShift = 46;\n-        int ValueShift = 0;\n-        return ((value >>> FieldShift) & FieldMask) << ValueShift;\n-    }\n-\n-    public boolean populated() {\n-        return fieldPopulatedDecode(entry());\n-    }\n-\n-    public long toOffset() {\n-        return fieldToOffsetDecode(entry());\n-    }\n-\n-    public long fromIndex() {\n-        return fieldFromIndexDecode(entry());\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingEntry.java","additions":0,"deletions":97,"binary":false,"changes":97,"status":"deleted"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XForwardingTable extends VMObject {\n-    private static long mapFieldOffset;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XForwardingTable\");\n-\n-        mapFieldOffset = type.getAddressField(\"_map\").getOffset();\n-    }\n-\n-    public XForwardingTable(Address addr) {\n-        super(addr);\n-    }\n-\n-    private XGranuleMapForForwarding map() {\n-        return VMObjectFactory.newObject(XGranuleMapForForwarding.class, addr.addOffsetTo(mapFieldOffset));\n-    }\n-\n-    public XForwarding get(Address o) {\n-        return VMObjectFactory.newObject(XForwarding.class, map().get(XAddress.offset(o)));\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingTable.java","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -1,29 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-class XForwardingTableCursor {\n-    long _value;\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingTableCursor.java","additions":0,"deletions":29,"binary":false,"changes":29,"status":"deleted"},{"patch":"@@ -1,55 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-\n-class XForwardingTableEntry {\n-    private Address entry;\n-\n-    XForwardingTableEntry(Address addr) {\n-        entry = addr;\n-    }\n-\n-    private static long empty() {\n-        return ~0L;\n-    }\n-\n-    boolean is_empty() {\n-        return entry.asLongValue() == empty();\n-    }\n-\n-    Address to_offset() {\n-        return entry.andWithMask((1L << 42) - 1);\n-    }\n-\n-    long from_index() {\n-        return entry.asLongValue() >>> 42;\n-    }\n-\n-    public String toString() {\n-        return entry + \" - from_index: \" + from_index() + \" to_offset: \" + to_offset();\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XForwardingTableEntry.java","additions":0,"deletions":55,"binary":false,"changes":55,"status":"deleted"},{"patch":"@@ -1,132 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.types.Field;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XGlobals {\n-    private static Field instanceField;\n-\n-    \/\/ Global phase state\n-    public static int XPhaseRelocate;\n-\n-    public static byte XPageTypeSmall;\n-    public static byte XPageTypeMedium;\n-    public static byte XPageTypeLarge;\n-\n-    \/\/ Granule size shift\n-    public static long XGranuleSizeShift;\n-\n-    \/\/ Page size shifts\n-    public static long XPageSizeSmallShift;\n-    public static long XPageSizeMediumShift;\n-\n-    \/\/ Object alignment shifts\n-    public static int  XObjectAlignmentMediumShift;\n-    public static int  XObjectAlignmentLargeShift;\n-\n-    \/\/ Pointer part of address\n-    public static long XAddressOffsetShift;\n-\n-    \/\/ Pointer part of address\n-    public static long XAddressOffsetBits;\n-    public static long XAddressOffsetMax;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XGlobalsForVMStructs\");\n-\n-        instanceField = type.getField(\"_instance_p\");\n-\n-        XPhaseRelocate = db.lookupIntConstant(\"XPhaseRelocate\").intValue();\n-\n-        XPageTypeSmall = db.lookupIntConstant(\"XPageTypeSmall\").byteValue();\n-        XPageTypeMedium = db.lookupIntConstant(\"XPageTypeMedium\").byteValue();\n-        XPageTypeLarge = db.lookupIntConstant(\"XPageTypeLarge\").byteValue();\n-\n-        XGranuleSizeShift = db.lookupLongConstant(\"XGranuleSizeShift\").longValue();\n-\n-        XPageSizeSmallShift = db.lookupLongConstant(\"XPageSizeSmallShift\").longValue();\n-        XPageSizeMediumShift = db.lookupLongConstant(\"XPageSizeMediumShift\").longValue();\n-\n-        XObjectAlignmentMediumShift = db.lookupIntConstant(\"XObjectAlignmentMediumShift\").intValue();\n-        XObjectAlignmentLargeShift = db.lookupIntConstant(\"XObjectAlignmentLargeShift\").intValue();\n-\n-        XAddressOffsetShift = db.lookupLongConstant(\"XAddressOffsetShift\").longValue();\n-\n-        XAddressOffsetBits = db.lookupLongConstant(\"XAddressOffsetBits\").longValue();\n-        XAddressOffsetMax  = db.lookupLongConstant(\"XAddressOffsetMax\").longValue();\n-    }\n-\n-    private static XGlobalsForVMStructs instance() {\n-        return new XGlobalsForVMStructs(instanceField.getAddress());\n-    }\n-\n-    public static int XGlobalPhase() {\n-        return instance().XGlobalPhase();\n-    }\n-\n-    public static int XGlobalSeqNum() {\n-        return instance().XGlobalSeqNum();\n-    }\n-\n-    public static long XAddressOffsetMask() {\n-        return instance().XAddressOffsetMask();\n-    }\n-\n-    public static long XAddressMetadataMask() {\n-        return instance().XAddressMetadataMask();\n-    }\n-\n-    public static long XAddressMetadataFinalizable() {\n-        return instance().XAddressMetadataFinalizable();\n-    }\n-\n-    public static long XAddressGoodMask() {\n-        return instance().XAddressGoodMask();\n-    }\n-\n-    public static long XAddressBadMask() {\n-        return instance().XAddressBadMask();\n-    }\n-\n-    public static long XAddressWeakBadMask() {\n-        return instance().XAddressWeakBadMask();\n-    }\n-\n-    public static int XObjectAlignmentSmallShift() {\n-        return instance().XObjectAlignmentSmallShift();\n-    }\n-\n-    public static int XObjectAlignmentSmall() {\n-        return instance().XObjectAlignmentSmall();\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGlobals.java","additions":0,"deletions":132,"binary":false,"changes":132,"status":"deleted"},{"patch":"@@ -1,108 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-class XGlobalsForVMStructs extends VMObject {\n-    private static AddressField XGlobalPhaseField;\n-    private static AddressField XGlobalSeqNumField;\n-    private static AddressField XAddressOffsetMaskField;\n-    private static AddressField XAddressMetadataMaskField;\n-    private static AddressField XAddressMetadataFinalizableField;\n-    private static AddressField XAddressGoodMaskField;\n-    private static AddressField XAddressBadMaskField;\n-    private static AddressField XAddressWeakBadMaskField;\n-    private static AddressField XObjectAlignmentSmallShiftField;\n-    private static AddressField XObjectAlignmentSmallField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XGlobalsForVMStructs\");\n-\n-        XGlobalPhaseField = type.getAddressField(\"_XGlobalPhase\");\n-        XGlobalSeqNumField = type.getAddressField(\"_XGlobalSeqNum\");\n-        XAddressOffsetMaskField = type.getAddressField(\"_XAddressOffsetMask\");\n-        XAddressMetadataMaskField = type.getAddressField(\"_XAddressMetadataMask\");\n-        XAddressMetadataFinalizableField = type.getAddressField(\"_XAddressMetadataFinalizable\");\n-        XAddressGoodMaskField = type.getAddressField(\"_XAddressGoodMask\");\n-        XAddressBadMaskField = type.getAddressField(\"_XAddressBadMask\");\n-        XAddressWeakBadMaskField = type.getAddressField(\"_XAddressWeakBadMask\");\n-        XObjectAlignmentSmallShiftField = type.getAddressField(\"_XObjectAlignmentSmallShift\");\n-        XObjectAlignmentSmallField = type.getAddressField(\"_XObjectAlignmentSmall\");\n-    }\n-\n-    XGlobalsForVMStructs(Address addr) {\n-        super(addr);\n-    }\n-\n-    int XGlobalPhase() {\n-        return XGlobalPhaseField.getValue(addr).getJIntAt(0);\n-    }\n-\n-    int XGlobalSeqNum() {\n-        return XGlobalSeqNumField.getValue(addr).getJIntAt(0);\n-    }\n-\n-    long XAddressOffsetMask() {\n-        return XAddressOffsetMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long XAddressMetadataMask() {\n-        return XAddressMetadataMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long XAddressMetadataFinalizable() {\n-        return XAddressMetadataFinalizableField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long XAddressGoodMask() {\n-        return XAddressGoodMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long XAddressBadMask() {\n-        return XAddressBadMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    long XAddressWeakBadMask() {\n-        return XAddressWeakBadMaskField.getValue(addr).getJLongAt(0);\n-    }\n-\n-    int XObjectAlignmentSmallShift() {\n-        return XObjectAlignmentSmallShiftField.getValue(addr).getJIntAt(0);\n-    }\n-\n-    int XObjectAlignmentSmall() {\n-        return XObjectAlignmentSmallField.getValue(addr).getJIntAt(0);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGlobalsForVMStructs.java","additions":0,"deletions":108,"binary":false,"changes":108,"status":"deleted"},{"patch":"@@ -1,90 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XGranuleMapForForwarding  extends VMObject {\n-    private static AddressField mapField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XGranuleMapForForwarding\");\n-\n-        mapField = type.getAddressField(\"_map\");\n-    }\n-\n-    public XGranuleMapForForwarding(Address addr) {\n-        super(addr);\n-    }\n-\n-    private Address map() {\n-        return mapField.getValue(addr);\n-    }\n-\n-    public long size() {\n-        return XGlobals.XAddressOffsetMax >> XGlobals.XGranuleSizeShift;\n-    }\n-\n-    private long index_for_offset(long offset) {\n-        long index = offset >>> XGlobals.XGranuleSizeShift;\n-\n-        return index;\n-    }\n-\n-    Address at(long index) {\n-        return map().getAddressAt(index * VM.getVM().getAddressSize());\n-    }\n-\n-    Address get(long offset) {\n-        long index = index_for_offset(offset);\n-        return at(index);\n-    }\n-\n-    public class Iterator {\n-        private long next = 0;\n-\n-        boolean hasNext() {\n-            return next < size();\n-        }\n-\n-        Address next() {\n-            if (next >= size()) {\n-                throw new RuntimeException(\"OOIBE\");\n-            }\n-\n-            return at(next++);\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGranuleMapForForwarding.java","additions":0,"deletions":90,"binary":false,"changes":90,"status":"deleted"},{"patch":"@@ -1,89 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XGranuleMapForPageTable  extends VMObject {\n-    private static AddressField mapField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XGranuleMapForPageTable\");\n-\n-        mapField = type.getAddressField(\"_map\");\n-    }\n-\n-    public XGranuleMapForPageTable(Address addr) {\n-        super(addr);\n-    }\n-\n-    private Address map() {\n-        return mapField.getValue(addr);\n-    }\n-\n-    public long size() {\n-        return XGlobals.XAddressOffsetMax >> XGlobals.XGranuleSizeShift;\n-    }\n-\n-    private long index_for_addr(Address addr) {\n-        long index = XAddress.offset(addr) >> XGlobals.XGranuleSizeShift;\n-\n-        return index;\n-    }\n-\n-    Address at(long index) {\n-        return map().getAddressAt(index * VM.getVM().getBytesPerLong());\n-    }\n-\n-    Address get(Address addr) {\n-        long index = index_for_addr(addr);\n-        return at(index);\n-    }\n-\n-    public class Iterator {\n-        private long next = 0;\n-\n-        boolean hasNext() {\n-            return next < size();\n-        }\n-\n-        Address next() {\n-            if (next >= size()) {\n-                throw new RuntimeException(\"OOIBE\");\n-            }\n-\n-            return at(next++);\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XGranuleMapForPageTable.java","additions":0,"deletions":89,"binary":false,"changes":89,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-class XHash {\n-    private static long uint32(long value) {\n-        return value & 0xFFFFFFFFL;\n-    }\n-\n-    static long uint32_to_uint32(long key) {\n-        key = uint32(~key + (key << 15));\n-        key = uint32(key ^ (key >>> 12));\n-        key = uint32(key + (key << 2));\n-        key = uint32(key ^ (key >>> 4));\n-        key = uint32(key * 2057);\n-        key = uint32(key ^ (key >>> 16));\n-        return key;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XHash.java","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,127 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import java.io.PrintStream;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-\/\/ Mirror class for XHeap\n-\n-public class XHeap extends VMObject {\n-\n-    private static long pageAllocatorFieldOffset;\n-    private static long pageTableFieldOffset;\n-    private static long forwardingTableFieldOffset;\n-    private static long relocateFieldOffset;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XHeap\");\n-\n-        pageAllocatorFieldOffset = type.getAddressField(\"_page_allocator\").getOffset();\n-        pageTableFieldOffset = type.getAddressField(\"_page_table\").getOffset();\n-        forwardingTableFieldOffset = type.getAddressField(\"_forwarding_table\").getOffset();\n-        relocateFieldOffset = type.getAddressField(\"_relocate\").getOffset();\n-    }\n-\n-    public XHeap(Address addr) {\n-        super(addr);\n-    }\n-\n-    private XPageAllocator pageAllocator() {\n-        Address pageAllocatorAddr = addr.addOffsetTo(pageAllocatorFieldOffset);\n-        return VMObjectFactory.newObject(XPageAllocator.class, pageAllocatorAddr);\n-    }\n-\n-    XPageTable pageTable() {\n-        return VMObjectFactory.newObject(XPageTable.class, addr.addOffsetTo(pageTableFieldOffset));\n-    }\n-\n-    XForwardingTable forwardingTable() {\n-        return VMObjectFactory.newObject(XForwardingTable.class, addr.addOffsetTo(forwardingTableFieldOffset));\n-    }\n-\n-    XRelocate relocate() {\n-        return VMObjectFactory.newObject(XRelocate.class, addr.addOffsetTo(relocateFieldOffset));\n-    }\n-\n-    public long maxCapacity() {\n-        return pageAllocator().maxCapacity();\n-    }\n-\n-    public long capacity() {\n-        return pageAllocator().capacity();\n-    }\n-\n-    public long used() {\n-        return pageAllocator().used();\n-    }\n-\n-    boolean is_relocating(Address o) {\n-        return pageTable().is_relocating(o);\n-    }\n-\n-    Address relocate_object(Address addr) {\n-        XForwarding forwarding = forwardingTable().get(addr);\n-        if (forwarding == null) {\n-            return XAddress.good(addr);\n-        }\n-        return relocate().relocateObject(forwarding, XAddress.good(addr));\n-    }\n-\n-    public boolean isIn(Address addr) {\n-        if (XAddress.isIn(addr)) {\n-            XPage page = pageTable().get(addr);\n-            if (page != null) {\n-                return page.isIn(addr);\n-            }\n-        }\n-        return false;\n-    }\n-\n-    public Address remapObject(Address o) {\n-        XForwarding forwarding = forwardingTable().get(addr);\n-        if (forwarding == null) {\n-            return XAddress.good(o);\n-        }\n-        return relocate().forwardObject(forwarding, XAddress.good(o));\n-    }\n-\n-    public void printOn(PrintStream tty) {\n-        tty.print(\" ZHeap          \");\n-        tty.print(\"used \" + (used() \/ 1024 \/ 1024) + \"M, \");\n-        tty.print(\"capacity \" + (capacity() \/ 1024 \/ 1024) + \"M, \");\n-        tty.println(\"max capacity \" + (maxCapacity() \/ 1024 \/ 1024) + \"M\");\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XHeap.java","additions":0,"deletions":127,"binary":false,"changes":127,"status":"deleted"},{"patch":"@@ -1,34 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.debugger.OopHandle;\n-\n-class XOop {\n-    static Address to_address(OopHandle oop) {\n-        return oop;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XOop.java","additions":0,"deletions":34,"binary":false,"changes":34,"status":"deleted"},{"patch":"@@ -1,141 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.debugger.OopHandle;\n-import sun.jvm.hotspot.gc.shared.LiveRegionsProvider;\n-import sun.jvm.hotspot.memory.MemRegion;\n-import sun.jvm.hotspot.oops.Oop;\n-import sun.jvm.hotspot.oops.UnknownOopException;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XPage extends VMObject implements LiveRegionsProvider {\n-    private static CIntegerField typeField;\n-    private static CIntegerField seqnumField;\n-    private static long virtualFieldOffset;\n-    private static AddressField topField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XPage\");\n-\n-        typeField = type.getCIntegerField(\"_type\");\n-        seqnumField = type.getCIntegerField(\"_seqnum\");\n-        virtualFieldOffset = type.getField(\"_virtual\").getOffset();\n-        topField = type.getAddressField(\"_top\");\n-    }\n-\n-    public XPage(Address addr) {\n-        super(addr);\n-    }\n-\n-    private byte type() {\n-        return typeField.getJByte(addr);\n-    }\n-\n-    private int seqnum() {\n-        return seqnumField.getJInt(addr);\n-    }\n-\n-    private XVirtualMemory virtual() {\n-        return VMObjectFactory.newObject(XVirtualMemory.class, addr.addOffsetTo(virtualFieldOffset));\n-    }\n-\n-    private Address top() {\n-        return topField.getValue(addr);\n-    }\n-\n-    private boolean is_relocatable() {\n-        return seqnum() < XGlobals.XGlobalSeqNum();\n-    }\n-\n-    long start() {\n-        return virtual().start();\n-    }\n-\n-    long size() {\n-        return virtual().end() - virtual().start();\n-    }\n-\n-    long object_alignment_shift() {\n-        if (type() == XGlobals.XPageTypeSmall) {\n-            return XGlobals.XObjectAlignmentSmallShift();\n-        } else if (type() == XGlobals.XPageTypeMedium) {\n-            return XGlobals.XObjectAlignmentMediumShift;\n-        } else {\n-            assert(type() == XGlobals.XPageTypeLarge);\n-            return XGlobals.XObjectAlignmentLargeShift;\n-        }\n-    }\n-\n-    long objectAlignmentSize() {\n-        return 1 << object_alignment_shift();\n-    }\n-\n-    public boolean isIn(Address addr) {\n-        long offset = XAddress.offset(addr);\n-        \/\/ FIXME: it does not consider the sign.\n-        return (offset >= start()) && (offset < top().asLongValue());\n-    }\n-\n-    private long getObjectSize(Address good) {\n-        OopHandle handle = good.addOffsetToAsOopHandle(0);\n-        Oop obj = null;\n-\n-        try {\n-           obj = VM.getVM().getObjectHeap().newOop(handle);\n-        } catch (UnknownOopException exp) {\n-          throw new RuntimeException(\" UnknownOopException  \" + exp);\n-        }\n-\n-        return VM.getVM().alignUp(obj.getObjectSize(), objectAlignmentSize());\n-    }\n-\n-    public List<MemRegion> getLiveRegions() {\n-        Address start = XAddress.good(XUtils.longToAddress(start()));\n-\n-        \/\/ Can't convert top() to a \"good\" address because it might\n-        \/\/ be at the top of the \"offset\" range, and therefore also\n-        \/\/ looks like one of the color bits. Instead use the \"good\"\n-        \/\/ address and add the size.\n-        long size = top().asLongValue() - start();\n-        Address end = start.addOffsetTo(size);\n-\n-        return List.of(new MemRegion(start, end));\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPage.java","additions":0,"deletions":141,"binary":false,"changes":141,"status":"deleted"},{"patch":"@@ -1,69 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-\/\/ Mirror class for XPageAllocator\n-\n-public class XPageAllocator extends VMObject {\n-\n-    private static CIntegerField maxCapacityField;\n-    private static CIntegerField capacityField;\n-    private static CIntegerField usedField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XPageAllocator\");\n-\n-        maxCapacityField = type.getCIntegerField(\"_max_capacity\");\n-        capacityField = type.getCIntegerField(\"_capacity\");\n-        usedField = type.getCIntegerField(\"_used\");\n-    }\n-\n-    public long maxCapacity() {\n-        return maxCapacityField.getValue(addr);\n-    }\n-\n-    public long capacity() {\n-        return capacityField.getValue(addr);\n-    }\n-\n-    public long used() {\n-        return usedField.getValue(addr);\n-    }\n-\n-    public XPageAllocator(Address addr) {\n-        super(addr);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPageAllocator.java","additions":0,"deletions":69,"binary":false,"changes":69,"status":"deleted"},{"patch":"@@ -1,176 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import java.util.Iterator;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XPageTable extends VMObject {\n-    private static long mapFieldOffset;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XPageTable\");\n-\n-        mapFieldOffset = type.getAddressField(\"_map\").getOffset();\n-    }\n-\n-    public XPageTable(Address addr) {\n-        super(addr);\n-    }\n-\n-    private XGranuleMapForPageTable map() {\n-        return VMObjectFactory.newObject(XGranuleMapForPageTable.class, addr.addOffsetTo(mapFieldOffset));\n-    }\n-\n-    private XPageTableEntry getEntry(Address o) {\n-        return new XPageTableEntry(map().get(o));\n-    }\n-\n-    XPage get(Address o) {\n-        return VMObjectFactory.newObject(XPage.class, map().get(VM.getVM().getDebugger().newAddress(XAddress.offset(o))));\n-    }\n-\n-    boolean is_relocating(Address o) {\n-        return getEntry(o).relocating();\n-    }\n-\n-    private class XPagesIterator implements Iterator<XPage> {\n-        private XGranuleMapForPageTable.Iterator mapIter;\n-        private XPage next;\n-\n-        XPagesIterator() {\n-            mapIter = map().new Iterator();\n-            positionToNext();\n-        }\n-\n-        private XPage positionToNext() {\n-            XPage current = next;\n-\n-            \/\/ Find next\n-            XPage found = null;\n-            while (mapIter.hasNext()) {\n-                XPageTableEntry entry = new XPageTableEntry(mapIter.next());\n-                if (!entry.isEmpty()) {\n-                    XPage page = entry.page();\n-                    \/\/ Medium pages have repeated entries for all covered slots,\n-                    \/\/ therefore we need to compare against the current page.\n-                    if (page != null && !page.equals(current)) {\n-                        found = page;\n-                        break;\n-                    }\n-                }\n-            }\n-\n-            next = found;\n-\n-            return current;\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return next != null;\n-        }\n-\n-        @Override\n-        public XPage next() {\n-            return positionToNext();\n-        }\n-\n-        @Override\n-        public void remove() {\n-            \/* not supported *\/\n-        }\n-    }\n-\n-    abstract class XPageFilter {\n-        public abstract boolean accept(XPage page);\n-    }\n-\n-    class XPagesFilteredIterator implements Iterator<XPage> {\n-        private XPage next;\n-        private XPagesIterator iter = new XPagesIterator();\n-        private XPageFilter filter;\n-\n-        XPagesFilteredIterator(XPageFilter filter) {\n-            this.filter = filter;\n-            positionToNext();\n-        }\n-\n-        public XPage positionToNext() {\n-            XPage current = next;\n-\n-            \/\/ Find next\n-            XPage found = null;\n-            while (iter.hasNext()) {\n-                XPage page = iter.next();\n-                if (filter.accept(page)) {\n-                    found = page;\n-                    break;\n-                }\n-            }\n-\n-            next = found;\n-\n-            return current;\n-        }\n-\n-        @Override\n-        public boolean hasNext() {\n-            return next != null;\n-        }\n-\n-        @Override\n-        public XPage next() {\n-            return positionToNext();\n-        }\n-\n-        @Override\n-        public void remove() {\n-            \/* not supported *\/\n-        }\n-    }\n-\n-    public Iterator<XPage> iterator() {\n-        return new XPagesIterator();\n-    }\n-\n-    public Iterator<XPage> activePagesIterator() {\n-        return new XPagesFilteredIterator(new XPageFilter() {\n-            public boolean accept(XPage page) {\n-                return page != null;\n-            }\n-        });\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPageTable.java","additions":0,"deletions":176,"binary":false,"changes":176,"status":"deleted"},{"patch":"@@ -1,52 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VMObjectFactory;\n-\n-class XPageTableEntry {\n-    Address entry;\n-\n-    XPageTableEntry(Address address) {\n-        entry = address;\n-    }\n-\n-    XPage page() {\n-        return VMObjectFactory.newObject(XPage.class, zPageBits());\n-    }\n-\n-    private Address zPageBits() {\n-        return entry.andWithMask(~1L);\n-    }\n-\n-    boolean relocating() {\n-        return (entry.asLongValue() & 1) == 1;\n-    }\n-\n-    boolean isEmpty() {\n-        return entry == null || zPageBits() == null;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XPageTableEntry.java","additions":0,"deletions":52,"binary":false,"changes":52,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, NTT DATA.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.AddressField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XRelocate  extends VMObject {\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XRelocate\");\n-    }\n-\n-    public XRelocate(Address addr) {\n-        super(addr);\n-    }\n-\n-    private long forwardingIndex(XForwarding forwarding, Address from) {\n-        long fromOffset = XAddress.offset(from);\n-        return (fromOffset - forwarding.start()) >>> forwarding.objectAlignmentShift();\n-    }\n-\n-    private Address forwardingFind(XForwarding forwarding, Address from) {\n-        long fromIndex = forwardingIndex(forwarding, from);\n-        XForwardingEntry entry = forwarding.find(fromIndex);\n-        return entry.populated() ? XAddress.good(VM.getVM().getDebugger().newAddress(entry.toOffset())) : null;\n-    }\n-\n-    public Address forwardObject(XForwarding forwarding, Address from) {\n-        return forwardingFind(forwarding, from);\n-    }\n-\n-    public Address relocateObject(XForwarding forwarding, Address o) {\n-        Address toAddr = forwardingFind(forwarding, o);\n-        if (toAddr != null) {\n-            \/\/ Already relocated.\n-            return toAddr;\n-        } else {\n-            \/\/ Return original address because it is not yet relocated.\n-            return o;\n-        }\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XRelocate.java","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-\n-class XUtils {\n-    static Address longToAddress(long value) {\n-        return VM.getVM().getDebugger().newAddress(value);\n-    }\n-\n-    static long alignUp(long size, long alignment) {\n-        long mask = alignment - 1;\n-        long adjusted = size + mask;\n-        return adjusted & ~mask;\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XUtils.java","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -1,60 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.gc.x;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.runtime.VM;\n-import sun.jvm.hotspot.runtime.VMObject;\n-import sun.jvm.hotspot.types.CIntegerField;\n-import sun.jvm.hotspot.types.Type;\n-import sun.jvm.hotspot.types.TypeDataBase;\n-\n-public class XVirtualMemory extends VMObject {\n-    private static CIntegerField startField;\n-    private static CIntegerField endField;\n-\n-    static {\n-        VM.registerVMInitializedObserver((o, d) -> initialize(VM.getVM().getTypeDataBase()));\n-    }\n-\n-    private static synchronized void initialize(TypeDataBase db) {\n-        Type type = db.lookupType(\"XVirtualMemory\");\n-\n-        startField = type.getCIntegerField(\"_start\");\n-        endField = type.getCIntegerField(\"_end\");\n-    }\n-\n-    public XVirtualMemory(Address addr) {\n-        super(addr);\n-    }\n-\n-    long start() {\n-        return startField.getJLong(addr);\n-    }\n-\n-    long end() {\n-        return endField.getJLong(addr);\n-    }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/x\/XVirtualMemory.java","additions":0,"deletions":60,"binary":false,"changes":60,"status":"deleted"},{"patch":"@@ -39,1 +39,0 @@\n-import sun.jvm.hotspot.gc.x.XCollectedHeap;\n@@ -90,1 +89,0 @@\n-    addHeapTypeIfInDB(db, XCollectedHeap.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/Universe.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-import sun.jvm.hotspot.gc.x.*;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ObjectHeap.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-import sun.jvm.hotspot.gc.x.*;\n@@ -148,3 +147,0 @@\n-      } else if (heap instanceof XCollectedHeap) {\n-         XCollectedHeap zheap = (XCollectedHeap) heap;\n-         zheap.printOn(System.out);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/HeapSummary.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,135 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"unittest.hpp\"\n-\n-class XAddressTest : public ::testing::Test {\n-protected:\n-  static void is_good_bit(uintptr_t bit_mask) {\n-    \/\/ Setup\n-    XAddress::initialize();\n-    XAddress::set_good_mask(bit_mask);\n-\n-    \/\/ Test that a pointer with only the given bit is considered good.\n-    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked0),  (bit_mask == XAddressMetadataMarked0));\n-    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked1),  (bit_mask == XAddressMetadataMarked1));\n-    EXPECT_EQ(XAddress::is_good(XAddressMetadataRemapped), (bit_mask == XAddressMetadataRemapped));\n-\n-    \/\/ Test that a pointer with the given bit and some extra bits is considered good.\n-    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked0  | 0x8),(bit_mask == XAddressMetadataMarked0));\n-    EXPECT_EQ(XAddress::is_good(XAddressMetadataMarked1  | 0x8), (bit_mask == XAddressMetadataMarked1));\n-    EXPECT_EQ(XAddress::is_good(XAddressMetadataRemapped | 0x8), (bit_mask == XAddressMetadataRemapped));\n-\n-    \/\/ Test that null is not considered good.\n-    EXPECT_FALSE(XAddress::is_good(0));\n-  }\n-\n-  static void is_good_or_null_bit(uintptr_t bit_mask) {\n-    \/\/ Setup\n-    XAddress::initialize();\n-    XAddress::set_good_mask(bit_mask);\n-\n-    \/\/ Test that a pointer with only the given bit is considered good.\n-    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked0),  (bit_mask == XAddressMetadataMarked0));\n-    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked1),  (bit_mask == XAddressMetadataMarked1));\n-    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataRemapped), (bit_mask == XAddressMetadataRemapped));\n-\n-    \/\/ Test that a pointer with the given bit and some extra bits is considered good.\n-    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked0  | 0x8), (bit_mask == XAddressMetadataMarked0));\n-    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataMarked1  | 0x8), (bit_mask == XAddressMetadataMarked1));\n-    EXPECT_EQ(XAddress::is_good_or_null(XAddressMetadataRemapped | 0x8), (bit_mask == XAddressMetadataRemapped));\n-\n-    \/\/ Test that null is considered good_or_null.\n-    EXPECT_TRUE(XAddress::is_good_or_null(0));\n-  }\n-\n-  static void finalizable() {\n-    \/\/ Setup\n-    XAddress::initialize();\n-    XAddress::flip_to_marked();\n-\n-    \/\/ Test that a normal good pointer is good and weak good, but not finalizable\n-    const uintptr_t addr1 = XAddress::good(1);\n-    EXPECT_FALSE(XAddress::is_finalizable(addr1));\n-    EXPECT_TRUE(XAddress::is_marked(addr1));\n-    EXPECT_FALSE(XAddress::is_remapped(addr1));\n-    EXPECT_TRUE(XAddress::is_weak_good(addr1));\n-    EXPECT_TRUE(XAddress::is_weak_good_or_null(addr1));\n-    EXPECT_TRUE(XAddress::is_good(addr1));\n-    EXPECT_TRUE(XAddress::is_good_or_null(addr1));\n-\n-    \/\/ Test that a finalizable good pointer is finalizable and weak good, but not good\n-    const uintptr_t addr2 = XAddress::finalizable_good(1);\n-    EXPECT_TRUE(XAddress::is_finalizable(addr2));\n-    EXPECT_TRUE(XAddress::is_marked(addr2));\n-    EXPECT_FALSE(XAddress::is_remapped(addr2));\n-    EXPECT_TRUE(XAddress::is_weak_good(addr2));\n-    EXPECT_TRUE(XAddress::is_weak_good_or_null(addr2));\n-    EXPECT_FALSE(XAddress::is_good(addr2));\n-    EXPECT_FALSE(XAddress::is_good_or_null(addr2));\n-\n-    \/\/ Flip to remapped and test that it's no longer weak good\n-    XAddress::flip_to_remapped();\n-    EXPECT_TRUE(XAddress::is_finalizable(addr2));\n-    EXPECT_TRUE(XAddress::is_marked(addr2));\n-    EXPECT_FALSE(XAddress::is_remapped(addr2));\n-    EXPECT_FALSE(XAddress::is_weak_good(addr2));\n-    EXPECT_FALSE(XAddress::is_weak_good_or_null(addr2));\n-    EXPECT_FALSE(XAddress::is_good(addr2));\n-    EXPECT_FALSE(XAddress::is_good_or_null(addr2));\n-  }\n-};\n-\n-TEST_F(XAddressTest, is_good) {\n-  is_good_bit(XAddressMetadataMarked0);\n-  is_good_bit(XAddressMetadataMarked1);\n-  is_good_bit(XAddressMetadataRemapped);\n-}\n-\n-TEST_F(XAddressTest, is_good_or_null) {\n-  is_good_or_null_bit(XAddressMetadataMarked0);\n-  is_good_or_null_bit(XAddressMetadataMarked1);\n-  is_good_or_null_bit(XAddressMetadataRemapped);\n-}\n-\n-TEST_F(XAddressTest, is_weak_good_or_null) {\n-#define check_is_weak_good_or_null(value)                                        \\\n-  EXPECT_EQ(XAddress::is_weak_good_or_null(value),                               \\\n-            (XAddress::is_good_or_null(value) || XAddress::is_remapped(value)))  \\\n-    << \"is_good_or_null: \" << XAddress::is_good_or_null(value)                   \\\n-    << \" is_remaped: \" << XAddress::is_remapped(value)                           \\\n-    << \" is_good_or_null_or_remapped: \" << XAddress::is_weak_good_or_null(value)\n-\n-  check_is_weak_good_or_null((uintptr_t)nullptr);\n-  check_is_weak_good_or_null(XAddressMetadataMarked0);\n-  check_is_weak_good_or_null(XAddressMetadataMarked1);\n-  check_is_weak_good_or_null(XAddressMetadataRemapped);\n-  check_is_weak_good_or_null((uintptr_t)0x123);\n-}\n-\n-TEST_F(XAddressTest, finalizable) {\n-  finalizable();\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xAddress.cpp","additions":0,"deletions":135,"binary":false,"changes":135,"status":"deleted"},{"patch":"@@ -1,83 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xArray.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-TEST(XArray, sanity) {\n-  XArray<int> a;\n-\n-  \/\/ Add elements\n-  for (int i = 0; i < 10; i++) {\n-    a.append(i);\n-  }\n-\n-  XArray<int> b;\n-\n-  b.swap(&a);\n-\n-  \/\/ Check size\n-  ASSERT_EQ(a.length(), 0);\n-  ASSERT_EQ(a.capacity(), 0);\n-  ASSERT_EQ(a.is_empty(), true);\n-\n-  ASSERT_EQ(b.length(), 10);\n-  ASSERT_GE(b.capacity(), 10);\n-  ASSERT_EQ(b.is_empty(), false);\n-\n-  \/\/ Clear elements\n-  a.clear();\n-\n-  \/\/ Check that b is unaffected\n-  ASSERT_EQ(b.length(), 10);\n-  ASSERT_GE(b.capacity(), 10);\n-  ASSERT_EQ(b.is_empty(), false);\n-\n-  a.append(1);\n-\n-  \/\/ Check that b is unaffected\n-  ASSERT_EQ(b.length(), 10);\n-  ASSERT_GE(b.capacity(), 10);\n-  ASSERT_EQ(b.is_empty(), false);\n-}\n-\n-TEST(XArray, iterator) {\n-  XArray<int> a;\n-\n-  \/\/ Add elements\n-  for (int i = 0; i < 10; i++) {\n-    a.append(i);\n-  }\n-\n-  \/\/ Iterate\n-  int count = 0;\n-  XArrayIterator<int> iter(&a);\n-  for (int value; iter.next(&value);) {\n-    ASSERT_EQ(a.at(count), count);\n-    count++;\n-  }\n-\n-  \/\/ Check count\n-  ASSERT_EQ(count, 10);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xArray.cpp","additions":0,"deletions":83,"binary":false,"changes":83,"status":"deleted"},{"patch":"@@ -1,79 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBitField.hpp\"\n-#include \"unittest.hpp\"\n-\n-TEST(XBitFieldTest, test) {\n-  typedef XBitField<uint64_t, bool,      0,  1>    field_bool;\n-  typedef XBitField<uint64_t, uint8_t,   1,  8>    field_uint8;\n-  typedef XBitField<uint64_t, uint16_t,  2, 16>    field_uint16;\n-  typedef XBitField<uint64_t, uint32_t, 32, 32>    field_uint32;\n-  typedef XBitField<uint64_t, uint64_t,  0, 63>    field_uint64;\n-  typedef XBitField<uint64_t, void*,     1, 61, 3> field_pointer;\n-\n-  uint64_t entry;\n-\n-  {\n-    const bool value = false;\n-    entry = field_bool::encode(value);\n-    EXPECT_EQ(field_bool::decode(entry), value) << \"Should be equal\";\n-  }\n-\n-  {\n-    const bool value = true;\n-    entry = field_bool::encode(value);\n-      EXPECT_EQ(field_bool::decode(entry), value) << \"Should be equal\";\n-  }\n-\n-  {\n-    const uint8_t value = ~(uint8_t)0;\n-    entry = field_uint8::encode(value);\n-    EXPECT_EQ(field_uint8::decode(entry), value) << \"Should be equal\";\n-  }\n-\n-  {\n-    const uint16_t value = ~(uint16_t)0;\n-    entry = field_uint16::encode(value);\n-    EXPECT_EQ(field_uint16::decode(entry), value) << \"Should be equal\";\n-  }\n-\n-  {\n-    const uint32_t value = ~(uint32_t)0;\n-    entry = field_uint32::encode(value);\n-    EXPECT_EQ(field_uint32::decode(entry), value) << \"Should be equal\";\n-  }\n-\n-  {\n-    const uint64_t value = ~(uint64_t)0 >> 1;\n-    entry = field_uint64::encode(value);\n-    EXPECT_EQ(field_uint64::decode(entry), value) << \"Should be equal\";\n-  }\n-\n-  {\n-    void* const value = (void*)(~(uintptr_t)0 << 3);\n-    entry = field_pointer::encode(value);\n-    EXPECT_EQ(field_pointer::decode(entry), value) << \"Should be equal\";\n-  }\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xBitField.cpp","additions":0,"deletions":79,"binary":false,"changes":79,"status":"deleted"},{"patch":"@@ -1,108 +0,0 @@\n-\/*\n- * Copyright (c) 2016, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xBitMap.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-class XBitMapTest : public ::testing::Test {\n-protected:\n-  static void test_set_pair_unset(size_t size, bool finalizable) {\n-    XBitMap bitmap(size);\n-\n-    for (BitMap::idx_t i = 0; i < size - 1; i++) {\n-      if ((i + 1) % BitsPerWord == 0) {\n-        \/\/ Can't set pairs of bits in different words.\n-        continue;\n-      }\n-\n-      \/\/ XBitMaps are not cleared when constructed.\n-      bitmap.clear();\n-\n-      bool inc_live = false;\n-\n-      bool ret = bitmap.par_set_bit_pair(i, finalizable, inc_live);\n-      EXPECT_TRUE(ret) << \"Failed to set bit\";\n-      EXPECT_TRUE(inc_live) << \"Should have set inc_live\";\n-\n-      \/\/ First bit should always be set\n-      EXPECT_TRUE(bitmap.at(i)) << \"Should be set\";\n-\n-      \/\/ Second bit should only be set when marking strong\n-      EXPECT_NE(bitmap.at(i + 1), finalizable);\n-    }\n-  }\n-\n-  static void test_set_pair_set(size_t size, bool finalizable) {\n-    XBitMap bitmap(size);\n-\n-    for (BitMap::idx_t i = 0; i < size - 1; i++) {\n-      if ((i + 1) % BitsPerWord == 0) {\n-        \/\/ Can't set pairs of bits in different words.\n-        continue;\n-      }\n-\n-      \/\/ Fill the bitmap with ones.\n-      bitmap.set_range(0, size);\n-\n-      bool inc_live = false;\n-\n-      bool ret = bitmap.par_set_bit_pair(i, finalizable, inc_live);\n-      EXPECT_FALSE(ret) << \"Should not succeed setting bit\";\n-      EXPECT_FALSE(inc_live) << \"Should not have set inc_live\";\n-\n-      \/\/ Both bits were pre-set.\n-      EXPECT_TRUE(bitmap.at(i)) << \"Should be set\";\n-      EXPECT_TRUE(bitmap.at(i + 1)) << \"Should be set\";\n-    }\n-  }\n-\n-  static void test_set_pair_set(bool finalizable) {\n-    test_set_pair_set(2,   finalizable);\n-    test_set_pair_set(62,  finalizable);\n-    test_set_pair_set(64,  finalizable);\n-    test_set_pair_set(66,  finalizable);\n-    test_set_pair_set(126, finalizable);\n-    test_set_pair_set(128, finalizable);\n-  }\n-\n-  static void test_set_pair_unset(bool finalizable) {\n-    test_set_pair_unset(2,   finalizable);\n-    test_set_pair_unset(62,  finalizable);\n-    test_set_pair_unset(64,  finalizable);\n-    test_set_pair_unset(66,  finalizable);\n-    test_set_pair_unset(126, finalizable);\n-    test_set_pair_unset(128, finalizable);\n-  }\n-\n-};\n-\n-TEST_F(XBitMapTest, test_set_pair_set) {\n-  test_set_pair_set(false);\n-  test_set_pair_set(true);\n-}\n-\n-TEST_F(XBitMapTest, test_set_pair_unset) {\n-  test_set_pair_unset(false);\n-  test_set_pair_unset(true);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xBitMap.cpp","additions":0,"deletions":108,"binary":false,"changes":108,"status":"deleted"},{"patch":"@@ -1,205 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xForwarding.inline.hpp\"\n-#include \"gc\/x\/xForwardingAllocator.inline.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xPage.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-using namespace testing;\n-\n-#define CAPTURE_DELIM \"\\n\"\n-#define CAPTURE1(expression) #expression << \" evaluates to \" << expression\n-#define CAPTURE2(e0, e1)                 CAPTURE1(e0) << CAPTURE_DELIM << CAPTURE1(e1)\n-\n-#define CAPTURE(expression) CAPTURE1(expression)\n-\n-class XForwardingTest : public Test {\n-public:\n-  \/\/ Helper functions\n-\n-  class SequenceToFromIndex : AllStatic {\n-  public:\n-    static uintptr_t even(size_t sequence_number) {\n-      return sequence_number * 2;\n-    }\n-    static uintptr_t odd(size_t sequence_number) {\n-      return even(sequence_number) + 1;\n-    }\n-    static uintptr_t one_to_one(size_t sequence_number) {\n-      return sequence_number;\n-    }\n-  };\n-\n-  \/\/ Test functions\n-\n-  static void setup(XForwarding* forwarding) {\n-    EXPECT_PRED1(is_power_of_2<size_t>, forwarding->_entries.length()) << CAPTURE(forwarding->_entries.length());\n-  }\n-\n-  static void find_empty(XForwarding* forwarding) {\n-    size_t size = forwarding->_entries.length();\n-    size_t entries_to_check = size * 2;\n-\n-    for (size_t i = 0; i < entries_to_check; i++) {\n-      uintptr_t from_index = SequenceToFromIndex::one_to_one(i);\n-\n-      XForwardingCursor cursor;\n-      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n-      EXPECT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n-    }\n-  }\n-\n-  static void find_full(XForwarding* forwarding) {\n-    size_t size = forwarding->_entries.length();\n-    size_t entries_to_populate = size;\n-\n-    \/\/ Populate\n-    for (size_t i = 0; i < entries_to_populate; i++) {\n-      uintptr_t from_index = SequenceToFromIndex::one_to_one(i);\n-\n-      XForwardingCursor cursor;\n-      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n-      ASSERT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n-\n-      forwarding->insert(from_index, from_index, &cursor);\n-    }\n-\n-    \/\/ Verify\n-    for (size_t i = 0; i < entries_to_populate; i++) {\n-      uintptr_t from_index = SequenceToFromIndex::one_to_one(i);\n-\n-      XForwardingCursor cursor;\n-      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n-      ASSERT_TRUE(entry.populated()) << CAPTURE2(from_index, size);\n-\n-      ASSERT_EQ(entry.from_index(), from_index) << CAPTURE(size);\n-      ASSERT_EQ(entry.to_offset(), from_index) << CAPTURE(size);\n-    }\n-  }\n-\n-  static void find_every_other(XForwarding* forwarding) {\n-    size_t size = forwarding->_entries.length();\n-    size_t entries_to_populate = size \/ 2;\n-\n-    \/\/ Populate even from indices\n-    for (size_t i = 0; i < entries_to_populate; i++) {\n-      uintptr_t from_index = SequenceToFromIndex::even(i);\n-\n-      XForwardingCursor cursor;\n-      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n-      ASSERT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n-\n-      forwarding->insert(from_index, from_index, &cursor);\n-    }\n-\n-    \/\/ Verify populated even indices\n-    for (size_t i = 0; i < entries_to_populate; i++) {\n-      uintptr_t from_index = SequenceToFromIndex::even(i);\n-\n-      XForwardingCursor cursor;\n-      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n-      ASSERT_TRUE(entry.populated()) << CAPTURE2(from_index, size);\n-\n-      ASSERT_EQ(entry.from_index(), from_index) << CAPTURE(size);\n-      ASSERT_EQ(entry.to_offset(), from_index) << CAPTURE(size);\n-    }\n-\n-    \/\/ Verify empty odd indices\n-    \/\/\n-    \/\/ This check could be done on a larger range of sequence numbers,\n-    \/\/ but currently entries_to_populate is used.\n-    for (size_t i = 0; i < entries_to_populate; i++) {\n-      uintptr_t from_index = SequenceToFromIndex::odd(i);\n-\n-      XForwardingCursor cursor;\n-      XForwardingEntry entry = forwarding->find(from_index, &cursor);\n-\n-      ASSERT_FALSE(entry.populated()) << CAPTURE2(from_index, size);\n-    }\n-  }\n-\n-  static void test(void (*function)(XForwarding*), uint32_t size) {\n-    \/\/ Create page\n-    const XVirtualMemory vmem(0, XPageSizeSmall);\n-    const XPhysicalMemory pmem(XPhysicalMemorySegment(0, XPageSizeSmall, true));\n-    XPage page(XPageTypeSmall, vmem, pmem);\n-\n-    page.reset();\n-\n-    const size_t object_size = 16;\n-    const uintptr_t object = page.alloc_object(object_size);\n-\n-    XGlobalSeqNum++;\n-\n-    bool dummy = false;\n-    page.mark_object(XAddress::marked(object), dummy, dummy);\n-\n-    const uint32_t live_objects = size;\n-    const size_t live_bytes = live_objects * object_size;\n-    page.inc_live(live_objects, live_bytes);\n-\n-    \/\/ Setup allocator\n-    XForwardingAllocator allocator;\n-    const uint32_t nentries = XForwarding::nentries(&page);\n-    allocator.reset((sizeof(XForwarding)) + (nentries * sizeof(XForwardingEntry)));\n-\n-    \/\/ Setup forwarding\n-    XForwarding* const forwarding = XForwarding::alloc(&allocator, &page);\n-\n-    \/\/ Actual test function\n-    (*function)(forwarding);\n-  }\n-\n-  \/\/ Run the given function with a few different input values.\n-  static void test(void (*function)(XForwarding*)) {\n-    test(function, 1);\n-    test(function, 2);\n-    test(function, 3);\n-    test(function, 4);\n-    test(function, 7);\n-    test(function, 8);\n-    test(function, 1023);\n-    test(function, 1024);\n-    test(function, 1025);\n-  }\n-};\n-\n-TEST_F(XForwardingTest, setup) {\n-  test(&XForwardingTest::setup);\n-}\n-\n-TEST_F(XForwardingTest, find_empty) {\n-  test(&XForwardingTest::find_empty);\n-}\n-\n-TEST_F(XForwardingTest, find_full) {\n-  test(&XForwardingTest::find_full);\n-}\n-\n-TEST_F(XForwardingTest, find_every_other) {\n-  test(&XForwardingTest::find_every_other);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xForwarding.cpp","additions":0,"deletions":205,"binary":false,"changes":205,"status":"deleted"},{"patch":"@@ -1,155 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xList.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-#ifndef PRODUCT\n-\n-class XTestEntry {\n-  friend class XList<XTestEntry>;\n-\n-private:\n-  const int             _id;\n-  XListNode<XTestEntry> _node;\n-\n-public:\n-  XTestEntry(int id) :\n-      _id(id),\n-      _node() {}\n-\n-  int id() const {\n-    return _id;\n-  }\n-};\n-\n-class XListTest : public ::testing::Test {\n-protected:\n-  static void assert_sorted(XList<XTestEntry>* list) {\n-    \/\/ Iterate forward\n-    {\n-      int count = list->first()->id();\n-      XListIterator<XTestEntry> iter(list);\n-      for (XTestEntry* entry; iter.next(&entry);) {\n-        ASSERT_EQ(entry->id(), count);\n-        count++;\n-      }\n-    }\n-\n-    \/\/ Iterate backward\n-    {\n-      int count = list->last()->id();\n-      XListReverseIterator<XTestEntry> iter(list);\n-      for (XTestEntry* entry; iter.next(&entry);) {\n-        EXPECT_EQ(entry->id(), count);\n-        count--;\n-      }\n-    }\n-  }\n-};\n-\n-TEST_F(XListTest, test_insert) {\n-  XList<XTestEntry> list;\n-  XTestEntry e0(0);\n-  XTestEntry e1(1);\n-  XTestEntry e2(2);\n-  XTestEntry e3(3);\n-  XTestEntry e4(4);\n-  XTestEntry e5(5);\n-\n-  list.insert_first(&e2);\n-  list.insert_before(&e2, &e1);\n-  list.insert_after(&e2, &e3);\n-  list.insert_last(&e4);\n-  list.insert_first(&e0);\n-  list.insert_last(&e5);\n-\n-  EXPECT_EQ(list.size(), 6u);\n-  assert_sorted(&list);\n-\n-  for (int i = 0; i < 6; i++) {\n-    XTestEntry* e = list.remove_first();\n-    EXPECT_EQ(e->id(), i);\n-  }\n-\n-  EXPECT_EQ(list.size(), 0u);\n-}\n-\n-TEST_F(XListTest, test_remove) {\n-  \/\/ Remove first\n-  {\n-    XList<XTestEntry> list;\n-    XTestEntry e0(0);\n-    XTestEntry e1(1);\n-    XTestEntry e2(2);\n-    XTestEntry e3(3);\n-    XTestEntry e4(4);\n-    XTestEntry e5(5);\n-\n-    list.insert_last(&e0);\n-    list.insert_last(&e1);\n-    list.insert_last(&e2);\n-    list.insert_last(&e3);\n-    list.insert_last(&e4);\n-    list.insert_last(&e5);\n-\n-    EXPECT_EQ(list.size(), 6u);\n-\n-    for (int i = 0; i < 6; i++) {\n-      XTestEntry* e = list.remove_first();\n-      EXPECT_EQ(e->id(), i);\n-    }\n-\n-    EXPECT_EQ(list.size(), 0u);\n-  }\n-\n-  \/\/ Remove last\n-  {\n-    XList<XTestEntry> list;\n-    XTestEntry e0(0);\n-    XTestEntry e1(1);\n-    XTestEntry e2(2);\n-    XTestEntry e3(3);\n-    XTestEntry e4(4);\n-    XTestEntry e5(5);\n-\n-    list.insert_last(&e0);\n-    list.insert_last(&e1);\n-    list.insert_last(&e2);\n-    list.insert_last(&e3);\n-    list.insert_last(&e4);\n-    list.insert_last(&e5);\n-\n-    EXPECT_EQ(list.size(), 6u);\n-\n-    for (int i = 5; i >= 0; i--) {\n-      XTestEntry* e = list.remove_last();\n-      EXPECT_EQ(e->id(), i);\n-    }\n-\n-    EXPECT_EQ(list.size(), 0u);\n-  }\n-}\n-\n-#endif \/\/ PRODUCT\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xList.cpp","additions":0,"deletions":155,"binary":false,"changes":155,"status":"deleted"},{"patch":"@@ -1,55 +0,0 @@\n-\/*\n- * Copyright (c) 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xLiveMap.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-class XLiveMapTest : public ::testing::Test {\n-protected:\n-  static void strongly_live_for_large_xpage() {\n-    \/\/ Large XPages only have room for one object.\n-    XLiveMap livemap(1);\n-\n-    bool inc_live;\n-    uintptr_t object = 0u;\n-\n-    \/\/ Mark the object strong.\n-    livemap.set(object, false \/* finalizable *\/, inc_live);\n-\n-    \/\/ Check that both bits are in the same segment.\n-    ASSERT_EQ(livemap.index_to_segment(0), livemap.index_to_segment(1));\n-\n-    \/\/ Check that the object was marked.\n-    ASSERT_TRUE(livemap.get(0));\n-\n-    \/\/ Check that the object was strongly marked.\n-    ASSERT_TRUE(livemap.get(1));\n-\n-    ASSERT_TRUE(inc_live);\n-  }\n-};\n-\n-TEST_F(XLiveMapTest, strongly_live_for_large_xpage) {\n-  strongly_live_for_large_xpage();\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xLiveMap.cpp","additions":0,"deletions":55,"binary":false,"changes":55,"status":"deleted"},{"patch":"@@ -1,174 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xPhysicalMemory.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-TEST(XPhysicalMemoryTest, copy) {\n-  const XPhysicalMemorySegment seg0(0, 100, true);\n-  const XPhysicalMemorySegment seg1(200, 100, true);\n-\n-  XPhysicalMemory pmem0;\n-  pmem0.add_segment(seg0);\n-  EXPECT_EQ(pmem0.nsegments(), 1);\n-  EXPECT_EQ(pmem0.segment(0).size(), 100u);\n-\n-  XPhysicalMemory pmem1;\n-  pmem1.add_segment(seg0);\n-  pmem1.add_segment(seg1);\n-  EXPECT_EQ(pmem1.nsegments(), 2);\n-  EXPECT_EQ(pmem1.segment(0).size(), 100u);\n-  EXPECT_EQ(pmem1.segment(1).size(), 100u);\n-\n-  XPhysicalMemory pmem2(pmem0);\n-  EXPECT_EQ(pmem2.nsegments(), 1);\n-  EXPECT_EQ(pmem2.segment(0).size(), 100u);\n-\n-  pmem2 = pmem1;\n-  EXPECT_EQ(pmem2.nsegments(), 2);\n-  EXPECT_EQ(pmem2.segment(0).size(), 100u);\n-  EXPECT_EQ(pmem2.segment(1).size(), 100u);\n-}\n-\n-TEST(XPhysicalMemoryTest, add) {\n-  const XPhysicalMemorySegment seg0(0, 1, true);\n-  const XPhysicalMemorySegment seg1(1, 1, true);\n-  const XPhysicalMemorySegment seg2(2, 1, true);\n-  const XPhysicalMemorySegment seg3(3, 1, true);\n-  const XPhysicalMemorySegment seg4(4, 1, true);\n-  const XPhysicalMemorySegment seg5(5, 1, true);\n-  const XPhysicalMemorySegment seg6(6, 1, true);\n-\n-  XPhysicalMemory pmem0;\n-  EXPECT_EQ(pmem0.nsegments(), 0);\n-  EXPECT_EQ(pmem0.is_null(), true);\n-\n-  XPhysicalMemory pmem1;\n-  pmem1.add_segment(seg0);\n-  pmem1.add_segment(seg1);\n-  pmem1.add_segment(seg2);\n-  pmem1.add_segment(seg3);\n-  pmem1.add_segment(seg4);\n-  pmem1.add_segment(seg5);\n-  pmem1.add_segment(seg6);\n-  EXPECT_EQ(pmem1.nsegments(), 1);\n-  EXPECT_EQ(pmem1.segment(0).size(), 7u);\n-  EXPECT_EQ(pmem1.is_null(), false);\n-\n-  XPhysicalMemory pmem2;\n-  pmem2.add_segment(seg0);\n-  pmem2.add_segment(seg1);\n-  pmem2.add_segment(seg2);\n-  pmem2.add_segment(seg4);\n-  pmem2.add_segment(seg5);\n-  pmem2.add_segment(seg6);\n-  EXPECT_EQ(pmem2.nsegments(), 2);\n-  EXPECT_EQ(pmem2.segment(0).size(), 3u);\n-  EXPECT_EQ(pmem2.segment(1).size(), 3u);\n-  EXPECT_EQ(pmem2.is_null(), false);\n-\n-  XPhysicalMemory pmem3;\n-  pmem3.add_segment(seg0);\n-  pmem3.add_segment(seg2);\n-  pmem3.add_segment(seg3);\n-  pmem3.add_segment(seg4);\n-  pmem3.add_segment(seg6);\n-  EXPECT_EQ(pmem3.nsegments(), 3);\n-  EXPECT_EQ(pmem3.segment(0).size(), 1u);\n-  EXPECT_EQ(pmem3.segment(1).size(), 3u);\n-  EXPECT_EQ(pmem3.segment(2).size(), 1u);\n-  EXPECT_EQ(pmem3.is_null(), false);\n-\n-  XPhysicalMemory pmem4;\n-  pmem4.add_segment(seg0);\n-  pmem4.add_segment(seg2);\n-  pmem4.add_segment(seg4);\n-  pmem4.add_segment(seg6);\n-  EXPECT_EQ(pmem4.nsegments(), 4);\n-  EXPECT_EQ(pmem4.segment(0).size(), 1u);\n-  EXPECT_EQ(pmem4.segment(1).size(), 1u);\n-  EXPECT_EQ(pmem4.segment(2).size(), 1u);\n-  EXPECT_EQ(pmem4.segment(3).size(), 1u);\n-  EXPECT_EQ(pmem4.is_null(), false);\n-}\n-\n-TEST(XPhysicalMemoryTest, remove) {\n-  XPhysicalMemory pmem;\n-\n-  pmem.add_segment(XPhysicalMemorySegment(10, 10, true));\n-  pmem.add_segment(XPhysicalMemorySegment(30, 10, true));\n-  pmem.add_segment(XPhysicalMemorySegment(50, 10, true));\n-  EXPECT_EQ(pmem.nsegments(), 3);\n-  EXPECT_EQ(pmem.size(), 30u);\n-  EXPECT_FALSE(pmem.is_null());\n-\n-  pmem.remove_segments();\n-  EXPECT_EQ(pmem.nsegments(), 0);\n-  EXPECT_EQ(pmem.size(), 0u);\n-  EXPECT_TRUE(pmem.is_null());\n-}\n-\n-TEST(XPhysicalMemoryTest, split) {\n-  XPhysicalMemory pmem;\n-\n-  pmem.add_segment(XPhysicalMemorySegment(0, 10, true));\n-  pmem.add_segment(XPhysicalMemorySegment(10, 10, true));\n-  pmem.add_segment(XPhysicalMemorySegment(30, 10, true));\n-  EXPECT_EQ(pmem.nsegments(), 2);\n-  EXPECT_EQ(pmem.size(), 30u);\n-\n-  XPhysicalMemory pmem0 = pmem.split(1);\n-  EXPECT_EQ(pmem0.nsegments(), 1);\n-  EXPECT_EQ(pmem0.size(), 1u);\n-  EXPECT_EQ(pmem.nsegments(), 2);\n-  EXPECT_EQ(pmem.size(), 29u);\n-\n-  XPhysicalMemory pmem1 = pmem.split(25);\n-  EXPECT_EQ(pmem1.nsegments(), 2);\n-  EXPECT_EQ(pmem1.size(), 25u);\n-  EXPECT_EQ(pmem.nsegments(), 1);\n-  EXPECT_EQ(pmem.size(), 4u);\n-\n-  XPhysicalMemory pmem2 = pmem.split(4);\n-  EXPECT_EQ(pmem2.nsegments(), 1);\n-  EXPECT_EQ(pmem2.size(), 4u);\n-  EXPECT_EQ(pmem.nsegments(), 0);\n-  EXPECT_EQ(pmem.size(), 0u);\n-}\n-\n-TEST(XPhysicalMemoryTest, split_committed) {\n-  XPhysicalMemory pmem0;\n-  pmem0.add_segment(XPhysicalMemorySegment(0, 10, true));\n-  pmem0.add_segment(XPhysicalMemorySegment(10, 10, false));\n-  pmem0.add_segment(XPhysicalMemorySegment(20, 10, true));\n-  pmem0.add_segment(XPhysicalMemorySegment(30, 10, false));\n-  EXPECT_EQ(pmem0.nsegments(), 4);\n-  EXPECT_EQ(pmem0.size(), 40u);\n-\n-  XPhysicalMemory pmem1 = pmem0.split_committed();\n-  EXPECT_EQ(pmem0.nsegments(), 2);\n-  EXPECT_EQ(pmem0.size(), 20u);\n-  EXPECT_EQ(pmem1.nsegments(), 2);\n-  EXPECT_EQ(pmem1.size(), 20u);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xPhysicalMemory.cpp","additions":0,"deletions":174,"binary":false,"changes":174,"status":"deleted"},{"patch":"@@ -1,45 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"unittest.hpp\"\n-\n-TEST(XVirtualMemory, split) {\n-  XVirtualMemory vmem(0, 10);\n-\n-  XVirtualMemory vmem0 = vmem.split(0);\n-  EXPECT_EQ(vmem0.size(), 0u);\n-  EXPECT_EQ(vmem.size(), 10u);\n-\n-  XVirtualMemory vmem1 = vmem.split(5);\n-  EXPECT_EQ(vmem1.size(), 5u);\n-  EXPECT_EQ(vmem.size(), 5u);\n-\n-  XVirtualMemory vmem2 = vmem.split(5);\n-  EXPECT_EQ(vmem2.size(), 5u);\n-  EXPECT_EQ(vmem.size(), 0u);\n-\n-  XVirtualMemory vmem3 = vmem.split(0);\n-  EXPECT_EQ(vmem3.size(), 0u);\n-}\n","filename":"test\/hotspot\/gtest\/gc\/x\/test_xVirtualMemory.cpp","additions":0,"deletions":45,"binary":false,"changes":45,"status":"deleted"},{"patch":"@@ -1,118 +0,0 @@\n-#\n-# Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n-# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-#\n-# This code is free software; you can redistribute it and\/or modify it\n-# under the terms of the GNU General Public License version 2 only, as\n-# published by the Free Software Foundation.\n-#\n-# This code is distributed in the hope that it will be useful, but WITHOUT\n-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-# version 2 for more details (a copy is included in the LICENSE file that\n-# accompanied this code).\n-#\n-# You should have received a copy of the GNU General Public License version\n-# 2 along with this work; if not, write to the Free Software Foundation,\n-# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-#\n-# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-# or visit www.oracle.com if you need additional information or have any\n-# questions.\n-#\n-\n-#############################################################################\n-#\n-# List of quarantined tests for testing with Generational ZGC.\n-#\n-#############################################################################\n-\n-# Quiet all SA tests\n-\n-resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8307393   generic-all\n-serviceability\/sa\/CDSJMapClstats.java                         8307393   generic-all\n-serviceability\/sa\/ClhsdbAttach.java                           8307393   generic-all\n-serviceability\/sa\/ClhsdbAttachDifferentJVMs.java              8307393   generic-all\n-serviceability\/sa\/ClhsdbCDSCore.java                          8307393   generic-all\n-serviceability\/sa\/ClhsdbCDSJstackPrintAll.java                8307393   generic-all\n-serviceability\/sa\/ClhsdbClasses.java                          8307393   generic-all\n-serviceability\/sa\/ClhsdbDumpclass.java                        8307393   generic-all\n-serviceability\/sa\/ClhsdbDumpheap.java                         8307393   generic-all\n-serviceability\/sa\/ClhsdbField.java                            8307393   generic-all\n-serviceability\/sa\/ClhsdbFindPC.java#apa                       8307393   generic-all\n-serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core             8307393   generic-all\n-serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-process          8307393   generic-all\n-serviceability\/sa\/ClhsdbFindPC.java#xcomp-core                8307393   generic-all\n-serviceability\/sa\/ClhsdbFindPC.java#xcomp-process             8307393   generic-all\n-serviceability\/sa\/ClhsdbFlags.java                            8307393   generic-all\n-serviceability\/sa\/ClhsdbHistory.java                          8307393   generic-all\n-serviceability\/sa\/ClhsdbInspect.java                          8307393   generic-all\n-serviceability\/sa\/ClhsdbJdis.java                             8307393   generic-all\n-serviceability\/sa\/ClhsdbJhisto.java                           8307393   generic-all\n-serviceability\/sa\/ClhsdbJstack.java#id0                       8307393   generic-all\n-serviceability\/sa\/ClhsdbJstack.java#id1                       8307393   generic-all\n-serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java         8307393   generic-all\n-serviceability\/sa\/ClhsdbJstackXcompStress.java                8307393   generic-all\n-serviceability\/sa\/ClhsdbLauncher.java                         8307393   generic-all\n-serviceability\/sa\/ClhsdbLongConstant.java                     8307393   generic-all\n-serviceability\/sa\/ClhsdbPmap.java                             8307393   generic-all\n-serviceability\/sa\/ClhsdbPmap.java#core                        8307393   generic-all\n-serviceability\/sa\/ClhsdbPmap.java#process                     8307393   generic-all\n-serviceability\/sa\/ClhsdbPrintAll.java                         8307393   generic-all\n-serviceability\/sa\/ClhsdbPrintAs.java                          8307393   generic-all\n-serviceability\/sa\/ClhsdbPrintStatics.java                     8307393   generic-all\n-serviceability\/sa\/ClhsdbPstack.java#core                      8307393   generic-all\n-serviceability\/sa\/ClhsdbPstack.java#process                   8307393   generic-all\n-serviceability\/sa\/ClhsdbScanOops.java                         8307393   generic-all\n-serviceability\/sa\/ClhsdbSource.java                           8307393   generic-all\n-serviceability\/sa\/ClhsdbSymbol.java                           8307393   generic-all\n-serviceability\/sa\/ClhsdbThread.java                           8307393   generic-all\n-serviceability\/sa\/ClhsdbThreadContext.java                    8307393   generic-all\n-serviceability\/sa\/ClhsdbVmStructsDump.java                    8307393   generic-all\n-serviceability\/sa\/ClhsdbWhere.java                            8307393   generic-all\n-serviceability\/sa\/DeadlockDetectionTest.java                  8307393   generic-all\n-serviceability\/sa\/JhsdbThreadInfoTest.java                    8307393   generic-all\n-serviceability\/sa\/LingeredAppSysProps.java                    8307393   generic-all\n-serviceability\/sa\/LingeredAppWithDefaultMethods.java          8307393   generic-all\n-serviceability\/sa\/LingeredAppWithEnum.java                    8307393   generic-all\n-serviceability\/sa\/LingeredAppWithInterface.java               8307393   generic-all\n-serviceability\/sa\/LingeredAppWithInvokeDynamic.java           8307393   generic-all\n-serviceability\/sa\/LingeredAppWithLock.java                    8307393   generic-all\n-serviceability\/sa\/LingeredAppWithNativeMethod.java            8307393   generic-all\n-serviceability\/sa\/LingeredAppWithRecComputation.java          8307393   generic-all\n-serviceability\/sa\/TestClassDump.java                          8307393   generic-all\n-serviceability\/sa\/TestClhsdbJstackLock.java                   8307393   generic-all\n-serviceability\/sa\/TestCpoolForInvokeDynamic.java              8307393   generic-all\n-serviceability\/sa\/TestDefaultMethods.java                     8307393   generic-all\n-serviceability\/sa\/TestG1HeapRegion.java                       8307393   generic-all\n-serviceability\/sa\/TestHeapDumpForInvokeDynamic.java           8307393   generic-all\n-serviceability\/sa\/TestInstanceKlassSize.java                  8307393   generic-all\n-serviceability\/sa\/TestInstanceKlassSizeForInterface.java      8307393   generic-all\n-serviceability\/sa\/TestIntConstant.java                        8307393   generic-all\n-serviceability\/sa\/TestJhsdbJstackLineNumbers.java             8307393   generic-all\n-serviceability\/sa\/TestJhsdbJstackLock.java                    8307393   generic-all\n-serviceability\/sa\/TestJhsdbJstackMixed.java                   8307393   generic-all\n-serviceability\/sa\/TestJhsdbJstackUpcall.java                  8307393   generic-all\n-serviceability\/sa\/TestJmapCore.java                           8307393   generic-all\n-serviceability\/sa\/TestJmapCoreMetaspace.java                  8307393   generic-all\n-serviceability\/sa\/TestObjectAlignment.java                    8307393   generic-all\n-serviceability\/sa\/TestObjectMonitorIterate.java               8307393   generic-all\n-serviceability\/sa\/TestPrintMdo.java                           8307393   generic-all\n-serviceability\/sa\/TestRevPtrsForInvokeDynamic.java            8307393   generic-all\n-serviceability\/sa\/TestSysProps.java                           8307393   generic-all\n-serviceability\/sa\/TestType.java                               8307393   generic-all\n-serviceability\/sa\/TestUniverse.java                           8307393   generic-all\n-serviceability\/sa\/UniqueVtableTest.java                       8307393   generic-all\n-serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapProc.java      8307393   generic-all\n-serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java      8307393   generic-all\n-serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java     8307393   generic-all\n-serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java     8307393   generic-all\n-serviceability\/sa\/ClhsdbTestAllocationMerge.java              8307393   generic-all\n-serviceability\/sa\/sadebugd\/DebugdConnectTest.java             8307393   generic-all\n-serviceability\/sa\/sadebugd\/DebugdUtils.java                   8307393   generic-all\n-serviceability\/sa\/sadebugd\/DisableRegistryTest.java           8307393   generic-all\n-serviceability\/sa\/sadebugd\/PmapOnDebugdTest.java              8307393   generic-all\n-serviceability\/sa\/sadebugd\/RunCommandOnServerTest.java        8307393   generic-all\n-serviceability\/sa\/sadebugd\/SADebugDTest.java                  8307393   generic-all\n-\n-vmTestbase\/gc\/gctests\/MemoryEaterMT\/MemoryEaterMT.java        8289582   windows-x64\n","filename":"test\/hotspot\/jtreg\/ProblemList-generational-zgc.txt","additions":0,"deletions":118,"binary":false,"changes":118,"status":"deleted"},{"patch":"@@ -30,5 +30,1 @@\n-resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8276539   generic-all\n-serviceability\/sa\/CDSJMapClstats.java                         8276539   generic-all\n-serviceability\/sa\/ClhsdbJhisto.java                           8276539   generic-all\n-serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java         8276539   generic-all\n-serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java      8276539   generic-all\n+# Quiet all SA tests\n@@ -36,11 +32,85 @@\n-serviceability\/sa\/ClhsdbFindPC.java#xcomp-core                8284045   generic-all\n-serviceability\/sa\/TestJmapCore.java                           8268283,8270202   generic-all\n-serviceability\/sa\/TestJmapCoreMetaspace.java                  8268636   generic-all\n-\n-serviceability\/sa\/TestJhsdbJstackMixed.java                   8248912   generic-all\n-serviceability\/sa\/ClhsdbPstack.java#process                   8248912   generic-all\n-serviceability\/sa\/ClhsdbPstack.java#core                      8248912   generic-all\n-\n-serviceability\/sa\/TestSysProps.java                           8302055   generic-all\n-\n-serviceability\/sa\/TestHeapDumpForInvokeDynamic.java           8315646   generic-all\n+resourcehogs\/serviceability\/sa\/TestHeapDumpForLargeArray.java 8307393   generic-all\n+serviceability\/sa\/CDSJMapClstats.java                         8307393   generic-all\n+serviceability\/sa\/ClhsdbAttach.java                           8307393   generic-all\n+serviceability\/sa\/ClhsdbAttachDifferentJVMs.java              8307393   generic-all\n+serviceability\/sa\/ClhsdbCDSCore.java                          8307393   generic-all\n+serviceability\/sa\/ClhsdbCDSJstackPrintAll.java                8307393   generic-all\n+serviceability\/sa\/ClhsdbClasses.java                          8307393   generic-all\n+serviceability\/sa\/ClhsdbDumpclass.java                        8307393   generic-all\n+serviceability\/sa\/ClhsdbDumpheap.java                         8307393   generic-all\n+serviceability\/sa\/ClhsdbField.java                            8307393   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#apa                       8307393   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-core             8307393   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#no-xcomp-process          8307393   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-core                8307393   generic-all\n+serviceability\/sa\/ClhsdbFindPC.java#xcomp-process             8307393   generic-all\n+serviceability\/sa\/ClhsdbFlags.java                            8307393   generic-all\n+serviceability\/sa\/ClhsdbHistory.java                          8307393   generic-all\n+serviceability\/sa\/ClhsdbInspect.java                          8307393   generic-all\n+serviceability\/sa\/ClhsdbJdis.java                             8307393   generic-all\n+serviceability\/sa\/ClhsdbJhisto.java                           8307393   generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id0                       8307393   generic-all\n+serviceability\/sa\/ClhsdbJstack.java#id1                       8307393   generic-all\n+serviceability\/sa\/ClhsdbJstackWithConcurrentLock.java         8307393   generic-all\n+serviceability\/sa\/ClhsdbJstackXcompStress.java                8307393   generic-all\n+serviceability\/sa\/ClhsdbLauncher.java                         8307393   generic-all\n+serviceability\/sa\/ClhsdbLongConstant.java                     8307393   generic-all\n+serviceability\/sa\/ClhsdbPmap.java                             8307393   generic-all\n+serviceability\/sa\/ClhsdbPmap.java#core                        8307393   generic-all\n+serviceability\/sa\/ClhsdbPmap.java#process                     8307393   generic-all\n+serviceability\/sa\/ClhsdbPrintAll.java                         8307393   generic-all\n+serviceability\/sa\/ClhsdbPrintAs.java                          8307393   generic-all\n+serviceability\/sa\/ClhsdbPrintStatics.java                     8307393   generic-all\n+serviceability\/sa\/ClhsdbPstack.java#core                      8307393   generic-all\n+serviceability\/sa\/ClhsdbPstack.java#process                   8307393   generic-all\n+serviceability\/sa\/ClhsdbScanOops.java                         8307393   generic-all\n+serviceability\/sa\/ClhsdbSource.java                           8307393   generic-all\n+serviceability\/sa\/ClhsdbSymbol.java                           8307393   generic-all\n+serviceability\/sa\/ClhsdbThread.java                           8307393   generic-all\n+serviceability\/sa\/ClhsdbThreadContext.java                    8307393   generic-all\n+serviceability\/sa\/ClhsdbVmStructsDump.java                    8307393   generic-all\n+serviceability\/sa\/ClhsdbWhere.java                            8307393   generic-all\n+serviceability\/sa\/DeadlockDetectionTest.java                  8307393   generic-all\n+serviceability\/sa\/JhsdbThreadInfoTest.java                    8307393   generic-all\n+serviceability\/sa\/LingeredAppSysProps.java                    8307393   generic-all\n+serviceability\/sa\/LingeredAppWithDefaultMethods.java          8307393   generic-all\n+serviceability\/sa\/LingeredAppWithEnum.java                    8307393   generic-all\n+serviceability\/sa\/LingeredAppWithInterface.java               8307393   generic-all\n+serviceability\/sa\/LingeredAppWithInvokeDynamic.java           8307393   generic-all\n+serviceability\/sa\/LingeredAppWithLock.java                    8307393   generic-all\n+serviceability\/sa\/LingeredAppWithNativeMethod.java            8307393   generic-all\n+serviceability\/sa\/LingeredAppWithRecComputation.java          8307393   generic-all\n+serviceability\/sa\/TestClassDump.java                          8307393   generic-all\n+serviceability\/sa\/TestClhsdbJstackLock.java                   8307393   generic-all\n+serviceability\/sa\/TestCpoolForInvokeDynamic.java              8307393   generic-all\n+serviceability\/sa\/TestDefaultMethods.java                     8307393   generic-all\n+serviceability\/sa\/TestG1HeapRegion.java                       8307393   generic-all\n+serviceability\/sa\/TestHeapDumpForInvokeDynamic.java           8307393   generic-all\n+serviceability\/sa\/TestInstanceKlassSize.java                  8307393   generic-all\n+serviceability\/sa\/TestInstanceKlassSizeForInterface.java      8307393   generic-all\n+serviceability\/sa\/TestIntConstant.java                        8307393   generic-all\n+serviceability\/sa\/TestJhsdbJstackLineNumbers.java             8307393   generic-all\n+serviceability\/sa\/TestJhsdbJstackLock.java                    8307393   generic-all\n+serviceability\/sa\/TestJhsdbJstackMixed.java                   8307393   generic-all\n+serviceability\/sa\/TestJhsdbJstackUpcall.java                  8307393   generic-all\n+serviceability\/sa\/TestJmapCore.java                           8307393   generic-all\n+serviceability\/sa\/TestJmapCoreMetaspace.java                  8307393   generic-all\n+serviceability\/sa\/TestObjectAlignment.java                    8307393   generic-all\n+serviceability\/sa\/TestObjectMonitorIterate.java               8307393   generic-all\n+serviceability\/sa\/TestPrintMdo.java                           8307393   generic-all\n+serviceability\/sa\/TestRevPtrsForInvokeDynamic.java            8307393   generic-all\n+serviceability\/sa\/TestSysProps.java                           8307393   generic-all\n+serviceability\/sa\/TestType.java                               8307393   generic-all\n+serviceability\/sa\/TestUniverse.java                           8307393   generic-all\n+serviceability\/sa\/UniqueVtableTest.java                       8307393   generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapProc.java      8307393   generic-all\n+serviceability\/sa\/jmap-hprof\/JMapHProfLargeHeapTest.java      8307393   generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbAttachToDebugServer.java     8307393   generic-all\n+serviceability\/sa\/sadebugd\/ClhsdbTestConnectArgument.java     8307393   generic-all\n+serviceability\/sa\/ClhsdbTestAllocationMerge.java              8307393   generic-all\n+serviceability\/sa\/sadebugd\/DebugdConnectTest.java             8307393   generic-all\n+serviceability\/sa\/sadebugd\/DebugdUtils.java                   8307393   generic-all\n+serviceability\/sa\/sadebugd\/DisableRegistryTest.java           8307393   generic-all\n+serviceability\/sa\/sadebugd\/PmapOnDebugdTest.java              8307393   generic-all\n+serviceability\/sa\/sadebugd\/RunCommandOnServerTest.java        8307393   generic-all\n+serviceability\/sa\/sadebugd\/SADebugDTest.java                  8307393   generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":86,"deletions":16,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -63,2 +63,1 @@\n-compiler\/vectorapi\/VectorRebracket128Test.java#ZSinglegen 8330538 generic-all\n-compiler\/vectorapi\/VectorRebracket128Test.java#ZGenerational 8330538 generic-all\n+compiler\/vectorapi\/VectorRebracket128Test.java#Z 8330538 generic-all\n@@ -98,2 +97,1 @@\n-gc\/TestAlwaysPreTouchBehavior.java#ZGenerational 8334513 generic-all\n-gc\/TestAlwaysPreTouchBehavior.java#ZSinglegen 8334513 generic-all\n+gc\/TestAlwaysPreTouchBehavior.java#Z 8334513 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -64,2 +64,0 @@\n-    vm.gc.ZGenerational \\\n-    vm.gc.ZSinglegen \\\n","filename":"test\/hotspot\/jtreg\/TEST.ROOT","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -37,1 +37,1 @@\n- *                   -XX:+UseZGC -XX:+ZGenerational\n+ *                   -XX:+UseZGC\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestArrayCopyWithLargeObjectAlignment.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -46,1 +46,1 @@\n- * @requires vm.gc.ZGenerational & (vm.simpleArch == \"x64\" | vm.simpleArch == \"aarch64\")\n+ * @requires vm.gc.Z & (vm.simpleArch == \"x64\" | vm.simpleArch == \"aarch64\")\n@@ -102,1 +102,1 @@\n-        test.addFlags(\"-XX:+UseZGC\", \"-XX:+ZGenerational\", \"-XX:+UnlockExperimentalVMOptions\",\n+        test.addFlags(\"-XX:+UseZGC\", \"-XX:+UnlockExperimentalVMOptions\",\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCBarrierElision.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -58,2 +58,1 @@\n-        TestFramework.runWithFlags(\"-XX:+UseZGC\", \"-XX:+ZGenerational\",\n-                                   \"-XX:LoopUnrollLimit=24\");\n+        TestFramework.runWithFlags(\"-XX:+UseZGC\", \"-XX:LoopUnrollLimit=24\");\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestZGCUnrolling.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n- * @test id=ZSinglegenDebug\n+ * @test id=ZDebug\n@@ -30,1 +30,1 @@\n- * @requires vm.gc.ZSinglegen & vm.debug\n+ * @requires vm.gc.Z & vm.debug\n@@ -32,1 +32,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational\n+ * @run main\/othervm -XX:+UseZGC\n@@ -41,1 +41,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -46,1 +46,1 @@\n- * @requires vm.gc.ZSinglegen & !vm.debug\n+ * @requires vm.gc.Z & !vm.debug\n@@ -48,33 +48,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational\n- *                   -XX:+UnlockDiagnosticVMOptions\n- *                   -XX:ZCollectionInterval=1\n- *                   -XX:-CreateCoredumpOnCrash\n- *                   -XX:CompileCommand=dontinline,*::mergeImpl*\n- *                   compiler.gcbarriers.UnsafeIntrinsicsTest\n- *\/\n-\n-\/*\n- * @test id=ZGenerationalDebug\n- * @key randomness\n- * @bug 8059022 8271855\n- * @modules java.base\/jdk.internal.misc:+open\n- * @summary Validate barriers after Unsafe getReference, CAS and swap (GetAndSet)\n- * @requires vm.gc.ZGenerational & vm.debug\n- * @library \/test\/lib\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational\n- *                   -XX:+UnlockDiagnosticVMOptions\n- *                   -XX:+ZVerifyOops -XX:ZCollectionInterval=1\n- *                   -XX:-CreateCoredumpOnCrash\n- *                   -XX:CompileCommand=dontinline,*::mergeImpl*\n- *                   compiler.gcbarriers.UnsafeIntrinsicsTest\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @key randomness\n- * @bug 8059022 8271855\n- * @modules java.base\/jdk.internal.misc:+open\n- * @summary Validate barriers after Unsafe getReference, CAS and swap (GetAndSet)\n- * @requires vm.gc.ZGenerational & !vm.debug\n- * @library \/test\/lib\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational\n+ * @run main\/othervm -XX:+UseZGC\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/UnsafeIntrinsicsTest.java","additions":6,"deletions":38,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -27,1 +27,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -31,13 +31,2 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational compiler.loopopts.TestRangeCheckPredicatesControl\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -XX:+UnlockDiagnosticVMOptions -XX:+IgnoreUnrecognizedVMOptions -XX:+StressGCM compiler.loopopts.TestRangeCheckPredicatesControl\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @key stress randomness\n- * @requires vm.gc.ZGenerational\n- * @bug 8237859\n- * @summary A LoadP node has a wrong control input (too early) which results in an out-of-bounds read of an object array with ZGC.\n- *\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational compiler.loopopts.TestRangeCheckPredicatesControl\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -XX:+UnlockDiagnosticVMOptions -XX:+IgnoreUnrecognizedVMOptions -XX:+StressGCM compiler.loopopts.TestRangeCheckPredicatesControl\n+ * @run main\/othervm -XX:+UseZGC compiler.loopopts.TestRangeCheckPredicatesControl\n+ * @run main\/othervm -XX:+UseZGC -XX:+UnlockDiagnosticVMOptions -XX:+IgnoreUnrecognizedVMOptions -XX:+StressGCM compiler.loopopts.TestRangeCheckPredicatesControl\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/TestRangeCheckPredicatesControl.java","additions":4,"deletions":15,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -52,1 +52,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -54,12 +54,1 @@\n- * @run driver TestNoWarningLoopStripMiningIterSet Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8241486\n- * @summary G1\/Z give warning when using LoopStripMiningIter and turn off LoopStripMiningIter (0)\n- * @requires vm.flagless\n- * @requires vm.flavor == \"server\" & !vm.graal.enabled\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @run driver TestNoWarningLoopStripMiningIterSet Z -XX:+ZGenerational\n+ * @run driver TestNoWarningLoopStripMiningIterSet Z\n@@ -109,13 +98,4 @@\n-        if (args.length > 1) {\n-            String extraVMArg = args[1];\n-            testWith(output -> output.shouldNotContain(CLSOffLSMGreaterZero), \"should have CLS and LSM enabled\", true, 100, \"-XX:LoopStripMiningIter=100\", gc, extraVMArg);\n-            testWith(output -> output.shouldContain(CLSOffLSMGreaterZero), \"should have CLS and LSM disabled\", false, 0, \"-XX:-UseCountedLoopSafepoints\", \"-XX:LoopStripMiningIter=100\", gc, extraVMArg);\n-            testWith(output -> output.shouldContain(CLSOnLSMEqualZero), \"should have CLS and LSM enabled\", true, 1, \"-XX:LoopStripMiningIter=0\", gc, extraVMArg);\n-            testWith(output -> output.shouldNotContain(CLSOnLSMEqualZero), \"should have CLS and LSM disabled\", false, 0, \"-XX:-UseCountedLoopSafepoints\", \"-XX:LoopStripMiningIter=0\", gc, extraVMArg);\n-        } else {\n-            testWith(output -> output.shouldNotContain(CLSOffLSMGreaterZero), \"should have CLS and LSM enabled\", true, 100, \"-XX:LoopStripMiningIter=100\", gc);\n-            testWith(output -> output.shouldContain(CLSOffLSMGreaterZero), \"should have CLS and LSM disabled\", false, 0, \"-XX:-UseCountedLoopSafepoints\", \"-XX:LoopStripMiningIter=100\", gc);\n-            testWith(output -> output.shouldContain(CLSOnLSMEqualZero), \"should have CLS and LSM enabled\", true, 1, \"-XX:LoopStripMiningIter=0\", gc);\n-            testWith(output -> output.shouldNotContain(CLSOnLSMEqualZero), \"should have CLS and LSM disabled\", false, 0, \"-XX:-UseCountedLoopSafepoints\", \"-XX:LoopStripMiningIter=0\", gc);\n-\n-        }\n+        testWith(output -> output.shouldNotContain(CLSOffLSMGreaterZero), \"should have CLS and LSM enabled\", true, 100, \"-XX:LoopStripMiningIter=100\", gc);\n+        testWith(output -> output.shouldContain(CLSOffLSMGreaterZero), \"should have CLS and LSM disabled\", false, 0, \"-XX:-UseCountedLoopSafepoints\", \"-XX:LoopStripMiningIter=100\", gc);\n+        testWith(output -> output.shouldContain(CLSOnLSMEqualZero), \"should have CLS and LSM enabled\", true, 1, \"-XX:LoopStripMiningIter=0\", gc);\n+        testWith(output -> output.shouldNotContain(CLSOnLSMEqualZero), \"should have CLS and LSM disabled\", false, 0, \"-XX:-UseCountedLoopSafepoints\", \"-XX:LoopStripMiningIter=0\", gc);\n","filename":"test\/hotspot\/jtreg\/compiler\/loopstripmining\/TestNoWarningLoopStripMiningIterSet.java","additions":7,"deletions":27,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -40,1 +40,1 @@\n- * @requires !vm.graal.enabled & vm.gc.ZSinglegen\n+ * @requires !vm.graal.enabled & vm.gc.Z\n@@ -45,14 +45,1 @@\n- *      -XX:+UseZGC -XX:-ZGenerational -XX:+LogCompilation -XX:+PrintDeoptimizationDetails -XX:+TraceDeoptimization -XX:+Verbose\n- *      compiler.uncommontrap.TestDeoptOOM\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8273456\n- * @summary Test that ttyLock is ranked above StackWatermark_lock\n- * @requires !vm.graal.enabled & vm.gc.ZGenerational\n- * @run main\/othervm -XX:-BackgroundCompilation -Xmx128M -XX:+IgnoreUnrecognizedVMOptions -XX:+VerifyStack\n- *      -XX:CompileCommand=exclude,compiler.uncommontrap.TestDeoptOOM::main\n- *      -XX:CompileCommand=exclude,compiler.uncommontrap.TestDeoptOOM::m9_1\n- *      -XX:+UnlockDiagnosticVMOptions\n- *      -XX:+UseZGC -XX:+ZGenerational -XX:+LogCompilation -XX:+PrintDeoptimizationDetails -XX:+TraceDeoptimization -XX:+Verbose\n+ *      -XX:+UseZGC -XX:+LogCompilation -XX:+PrintDeoptimizationDetails -XX:+TraceDeoptimization -XX:+Verbose\n","filename":"test\/hotspot\/jtreg\/compiler\/uncommontrap\/TestDeoptOOM.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -40,1 +40,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -44,11 +44,1 @@\n- *      -XX:-TieredCompilation -XX:CICompilerCount=1 -XX:+UseZGC -XX:-ZGenerational -Xbatch -Xmx256m VectorRebracket128Test\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8260473\n- * @requires vm.gc.ZGenerational\n- * @modules jdk.incubator.vector\n- * @modules java.base\/jdk.internal.vm.annotation\n- * @run testng\/othervm -XX:CompileCommand=compileonly,jdk\/incubator\/vector\/ByteVector.fromMemorySegment\n- *      -XX:-TieredCompilation -XX:CICompilerCount=1 -XX:+UseZGC -XX:+ZGenerational -Xbatch -Xmx256m VectorRebracket128Test\n+ *      -XX:-TieredCompilation -XX:CICompilerCount=1 -XX:+UseZGC -Xbatch -Xmx256m VectorRebracket128Test\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorRebracket128Test.java","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -76,1 +76,1 @@\n- * @test id=ZGenerational\n+ * @test id=Z\n@@ -78,1 +78,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -84,13 +84,1 @@\n- * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseZGC -XX:+ZGenerational -Xmx512m -Xms512m -XX:+AlwaysPreTouch gc.TestAlwaysPreTouchBehavior\n- *\/\n-\n-\/**\n- * @test id=ZSinglegen\n- * @summary tests AlwaysPreTouch\n- * @requires vm.gc.ZSinglegen\n- * @requires os.maxMemory > 2G\n- * @requires os.family != \"aix\"\n- * @library \/test\/lib\n- * @build jdk.test.whitebox.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n- * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseZGC -XX:-ZGenerational -Xmx512m -Xms512m -XX:+AlwaysPreTouch gc.TestAlwaysPreTouchBehavior\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseZGC -Xmx512m -Xms512m -XX:+AlwaysPreTouch gc.TestAlwaysPreTouchBehavior\n","filename":"test\/hotspot\/jtreg\/gc\/TestAlwaysPreTouchBehavior.java","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-\/* @test id=ZSinglegen\n+\/* @test id=Z\n@@ -41,1 +41,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -47,13 +47,1 @@\n- *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseZGC -XX:-ZGenerational\n- *      gc.TestReferenceClearDuringReferenceProcessing\n- *\/\n-\n-\/* @test id=ZGenerational\n- * @bug 8256517\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @build jdk.test.whitebox.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n- * @run main\/othervm\n- *      -Xbootclasspath\/a:.\n- *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseZGC -XX:+ZGenerational\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UseZGC\n","filename":"test\/hotspot\/jtreg\/gc\/TestReferenceClearDuringReferenceProcessing.java","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -61,2 +61,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.gc.Z\n@@ -66,10 +66,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational gc.TestSystemGC\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @requires vm.gc.ZGenerational\n- * @comment ZGC will not start when LargePages cannot be allocated, therefore\n- *          we do not run such configuration.\n- * @summary Runs System.gc() with different flags.\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational gc.TestSystemGC\n+ * @run main\/othervm -XX:+UseZGC gc.TestSystemGC\n","filename":"test\/hotspot\/jtreg\/gc\/TestSystemGC.java","additions":3,"deletions":12,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,3 +29,3 @@\n- * @comment Generational ZGC can't use the generic Universe::verify\n- *          because there's no guarantee that we will ever have\n- *          a stable snapshot where all roots can be verified.\n+ * @comment ZGC can't use the generic Universe::verify because\n+ *          there's no guarantee that we will ever have a stable\n+ *          snapshot where all roots can be verified.\n","filename":"test\/hotspot\/jtreg\/gc\/TestVerifySubSet.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,5 +36,5 @@\n- * @comment This test assumes that no allocation happens during the sleep loop,   \\\n- *          which is something that we can't guarantee. With Generational ZGC we  \\\n- *          see test timeouts because the main thread allocates and waits for the \\\n- *          GC, which waits for the CSLocker, which waits for the main thread.    \\\n- * @requires !vm.opt.final.ZGenerational\n+ * @comment This test assumes that no allocation happens during the sleep loop,\n+ *          which is something that we can't guarantee. With ZGC we see test\n+ *          timeouts because the main thread allocates and waits for the GC,\n+ *          which waits for the CSLocker, which waits for the main thread.\n+ * @requires vm.gc != \"Z\"\n","filename":"test\/hotspot\/jtreg\/gc\/cslocker\/TestCSLocker.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n- * @test TestGCBasherWithZGenerational\n+ * @test TestGCBasherWithZ\n@@ -33,1 +33,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -36,10 +36,1 @@\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx384m -server -XX:+UseZGC -XX:+ZGenerational  gc.stress.gcbasher.TestGCBasherWithZ 120000\n- *\/\n-\/*\n- * @test TestGCBasherWithZSinglegen\n- * @key stress\n- * @library \/\n- * @requires vm.gc.ZSinglegen\n- * @requires vm.flavor == \"server\" & !vm.emulatedClient\n- * @summary Stress ZGC\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx384m -server -XX:+UseZGC -XX:-ZGenerational gc.stress.gcbasher.TestGCBasherWithZ 120000\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx384m -server -XX:+UseZGC gc.stress.gcbasher.TestGCBasherWithZ 120000\n@@ -49,1 +40,1 @@\n- * @test TestGCBasherDeoptWithZGenerational\n+ * @test TestGCBasherDeoptWithZ\n@@ -52,1 +43,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -55,1 +46,1 @@\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC -XX:+ZGenerational\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC\n@@ -60,11 +51,0 @@\n-\/*\n- * @test TestGCBasherDeoptWithZSinglegen\n- * @key stress\n- * @library \/\n- * @requires vm.gc.ZSinglegen\n- * @requires vm.flavor == \"server\" & !vm.emulatedClient & vm.opt.ClassUnloading != false\n- * @summary Stress ZGC with nmethod barrier forced deoptimization enabled.\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC -XX:-ZGenerational\n- *   -XX:+UnlockDiagnosticVMOptions -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n- *   gc.stress.gcbasher.TestGCBasherWithZ 120000\n- *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcbasher\/TestGCBasherWithZ.java","additions":6,"deletions":26,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @test TestGCOldWithZGenerational\n+ * @test TestGCOldWithZ\n@@ -31,1 +31,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -33,2 +33,2 @@\n- * @run main\/othervm -Xmx384M -XX:+UseZGC -XX:+ZGenerational gc.stress.gcold.TestGCOldWithZ 50 1 20 10 10000\n- * @run main\/othervm -Xmx256m -XX:+UseZGC -XX:+ZGenerational gc.stress.gcold.TestGCOldWithZ 50 5 20 1 5000\n+ * @run main\/othervm -Xmx384M -XX:+UseZGC gc.stress.gcold.TestGCOldWithZ 50 1 20 10 10000\n+ * @run main\/othervm -Xmx256m -XX:+UseZGC gc.stress.gcold.TestGCOldWithZ 50 5 20 1 5000\n@@ -37,9 +37,0 @@\n-\/*\n- * @test TestGCOldWithZSinglegen\n- * @key randomness\n- * @library \/ \/test\/lib\n- * @requires vm.gc.ZSinglegen\n- * @summary Stress the Z\n- * @run main\/othervm -Xmx384M -XX:+UseZGC -XX:-ZGenerational gc.stress.gcold.TestGCOldWithZ 50 1 20 10 10000\n- * @run main\/othervm -Xmx256m -XX:+UseZGC -XX:-ZGenerational gc.stress.gcold.TestGCOldWithZ 50 5 20 1 5000\n- *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcold\/TestGCOldWithZ.java","additions":4,"deletions":13,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -82,1 +82,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -88,14 +88,1 @@\n- * @run driver gc.stringdedup.TestStringDeduplicationAgeThreshold Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @summary Test string deduplication age threshold\n- * @bug 8029075\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc:open\n- * @modules java.base\/java.lang:open\n- *          java.management\n- * @run driver gc.stringdedup.TestStringDeduplicationAgeThreshold Z -XX:+ZGenerational\n+ * @run driver gc.stringdedup.TestStringDeduplicationAgeThreshold Z\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationAgeThreshold.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -82,1 +82,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -88,14 +88,1 @@\n- * @run driver gc.stringdedup.TestStringDeduplicationFullGC Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @summary Test string deduplication during full GC\n- * @bug 8029075\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc:open\n- * @modules java.base\/java.lang:open\n- *          java.management\n- * @run driver gc.stringdedup.TestStringDeduplicationFullGC Z -XX:+ZGenerational\n+ * @run driver gc.stringdedup.TestStringDeduplicationFullGC Z\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationFullGC.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -82,1 +82,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -88,14 +88,1 @@\n- * @run driver gc.stringdedup.TestStringDeduplicationInterned Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @summary Test string deduplication of interned strings\n- * @bug 8029075\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc:open\n- * @modules java.base\/java.lang:open\n- *          java.management\n- * @run driver gc.stringdedup.TestStringDeduplicationInterned Z -XX:+ZGenerational\n+ * @run driver gc.stringdedup.TestStringDeduplicationInterned Z\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationInterned.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -82,1 +82,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -88,14 +88,1 @@\n- * @run driver gc.stringdedup.TestStringDeduplicationPrintOptions Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @summary Test string deduplication print options\n- * @bug 8029075\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc:open\n- * @modules java.base\/java.lang:open\n- *          java.management\n- * @run driver gc.stringdedup.TestStringDeduplicationPrintOptions Z -XX:+ZGenerational\n+ * @run driver gc.stringdedup.TestStringDeduplicationPrintOptions Z\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationPrintOptions.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -82,1 +82,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -88,14 +88,1 @@\n- * @run driver gc.stringdedup.TestStringDeduplicationTableResize Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @summary Test string deduplication table resize\n- * @bug 8029075\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc:open\n- * @modules java.base\/java.lang:open\n- *          java.management\n- * @run driver gc.stringdedup.TestStringDeduplicationTableResize Z -XX:+ZGenerational\n+ * @run driver gc.stringdedup.TestStringDeduplicationTableResize Z\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationTableResize.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-    private static String selectedGCMode = null;\n@@ -77,3 +76,0 @@\n-        if (args.length > 1) {\n-            selectedGCMode = args[1];\n-        }\n@@ -140,1 +136,1 @@\n-                    \/\/ Generational ZGC only triggers string deduplications from major collections\n+                    \/\/ ZGC only triggers string deduplications from major collections\n@@ -144,6 +140,0 @@\n-\n-                    \/\/ Single-gen ZGC\n-                    if (!info.getGcName().startsWith(\"ZGC Major\") && !info.getGcName().startsWith(\"ZGC Minor\") &&\n-                            \"end of GC cycle\".equals(info.getGcAction())) {\n-                        gcCount++;\n-                    }\n@@ -328,3 +318,0 @@\n-        if (selectedGCMode != null) {\n-            args.add(selectedGCMode);\n-        }\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationTools.java","additions":1,"deletions":14,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -82,1 +82,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -88,14 +88,1 @@\n- * @run driver gc.stringdedup.TestStringDeduplicationYoungGC Z -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @summary Test string deduplication during young GC\n- * @bug 8029075\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc:open\n- * @modules java.base\/java.lang:open\n- *          java.management\n- * @run driver gc.stringdedup.TestStringDeduplicationYoungGC Z -XX:+ZGenerational\n+ * @run driver gc.stringdedup.TestStringDeduplicationYoungGC Z\n","filename":"test\/hotspot\/jtreg\/gc\/stringdedup\/TestStringDeduplicationYoungGC.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1,57 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestAllocateHeapAt\n- * @requires vm.gc.ZSinglegen & os.family == \"linux\"\n- * @requires !vm.opt.final.UseLargePages\n- * @summary Test ZGC with -XX:AllocateHeapAt\n- * @library \/test\/lib\n- * @run main\/othervm gc.x.TestAllocateHeapAt . true\n- * @run main\/othervm gc.x.TestAllocateHeapAt non-existing-directory false\n- *\/\n-\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestAllocateHeapAt {\n-    public static void main(String[] args) throws Exception {\n-        final String directory = args[0];\n-        final boolean exists = Boolean.parseBoolean(args[1]);\n-        final String heapBackingFile = \"Heap Backing File: \" + directory;\n-        final String failedToCreateFile = \"Failed to create file \" + directory;\n-\n-        ProcessTools.executeTestJava(\n-            \"-XX:+UseZGC\",\n-            \"-XX:-ZGenerational\",\n-            \"-Xlog:gc*\",\n-            \"-Xms32M\",\n-            \"-Xmx32M\",\n-            \"-XX:AllocateHeapAt=\" + directory,\n-            \"-version\")\n-                .shouldContain(exists ? heapBackingFile : failedToCreateFile)\n-                .shouldNotContain(exists ? failedToCreateFile : heapBackingFile)\n-                .shouldHaveExitValue(exists ? 0 : 1);\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestAllocateHeapAt.java","additions":0,"deletions":57,"binary":false,"changes":57,"status":"deleted"},{"patch":"@@ -1,41 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestAlwaysPreTouch\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC parallel pre-touch\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:-AlwaysPreTouch -Xms128M -Xmx128M gc.x.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms2M -Xmx128M gc.x.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms2M -Xmx128M gc.x.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms128M -Xmx128M gc.x.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms128M -Xmx128M gc.x.TestAlwaysPreTouch\n- *\/\n-\n-public class TestAlwaysPreTouch {\n-    public static void main(String[] args) throws Exception {\n-        System.out.println(\"Success\");\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestAlwaysPreTouch.java","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -1,50 +0,0 @@\n-\/*\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestDeprecated\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGenerational Deprecated\n- * @library \/test\/lib\n- * @run driver gc.x.TestDeprecated\n- *\/\n-\n-import java.util.LinkedList;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestDeprecated {\n-    static class Test {\n-        public static void main(String[] args) throws Exception {}\n-    }\n-    public static void main(String[] args) throws Exception {\n-        ProcessTools.executeLimitedTestJava(\"-XX:+UseZGC\",\n-                                            \"-XX:-ZGenerational\",\n-                                            \"-Xlog:gc+init\",\n-                                            Test.class.getName())\n-                    .shouldContain(\"Option ZGenerational was deprecated\")\n-                    .shouldContain(\"Using deprecated non-generational mode\")\n-                    .shouldHaveExitValue(0);\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestDeprecated.java","additions":0,"deletions":50,"binary":false,"changes":50,"status":"deleted"},{"patch":"@@ -1,221 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/**\n- * @test TestGarbageCollectorMXBean\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC garbage collector MXBean\n- * @modules java.management\n- * @requires vm.compMode != \"Xcomp\"\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms256M -Xmx512M -Xlog:gc gc.x.TestGarbageCollectorMXBean 256 512\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms512M -Xmx512M -Xlog:gc gc.x.TestGarbageCollectorMXBean 512 512\n- *\/\n-\n-import java.lang.management.ManagementFactory;\n-import java.util.concurrent.atomic.AtomicInteger;\n-import java.util.concurrent.atomic.AtomicLong;\n-import javax.management.Notification;\n-import javax.management.NotificationEmitter;\n-import javax.management.NotificationListener;\n-import javax.management.openmbean.CompositeData;\n-\n-import com.sun.management.GarbageCollectionNotificationInfo;\n-\n-public class TestGarbageCollectorMXBean {\n-    private static final long startTime = System.nanoTime();\n-\n-    private static void log(String msg) {\n-        final String elapsedSeconds = String.format(\"%.3fs\", (System.nanoTime() - startTime) \/ 1_000_000_000.0);\n-        System.out.println(\"[\" + elapsedSeconds + \"] (\" + Thread.currentThread().getName() + \") \" + msg);\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        final long M = 1024 * 1024;\n-        final long initialCapacity = Long.parseLong(args[0]) * M;\n-        final long maxCapacity = Long.parseLong(args[1]) * M;\n-        final AtomicInteger cycles = new AtomicInteger();\n-        final AtomicInteger pauses = new AtomicInteger();\n-        final AtomicInteger errors = new AtomicInteger();\n-\n-        final NotificationListener listener = (Notification notification, Object ignored) -> {\n-            final var type = notification.getType();\n-            if (!type.equals(GarbageCollectionNotificationInfo.GARBAGE_COLLECTION_NOTIFICATION)) {\n-                \/\/ Ignore\n-                return;\n-            }\n-\n-            final var data = (CompositeData)notification.getUserData();\n-            final var info = GarbageCollectionNotificationInfo.from(data);\n-            final var name = info.getGcName();\n-            final var id = info.getGcInfo().getId();\n-            final var action = info.getGcAction();\n-            final var cause = info.getGcCause();\n-            final var startTime = info.getGcInfo().getStartTime();\n-            final var endTime = info.getGcInfo().getEndTime();\n-            final var duration = info.getGcInfo().getDuration();\n-            final var memoryUsageBeforeGC = info.getGcInfo().getMemoryUsageBeforeGc().get(\"ZHeap\");\n-            final var memoryUsageAfterGC = info.getGcInfo().getMemoryUsageAfterGc().get(\"ZHeap\");\n-\n-            log(name + \" (\" + type + \")\");\n-            log(\"                  Id: \" + id);\n-            log(\"              Action: \" + action);\n-            log(\"               Cause: \" + cause);\n-            log(\"           StartTime: \" + startTime);\n-            log(\"             EndTime: \" + endTime);\n-            log(\"            Duration: \" + duration);\n-            log(\" MemoryUsageBeforeGC: \" + memoryUsageBeforeGC);\n-            log(\"  MemoryUsageAfterGC: \" + memoryUsageAfterGC);\n-            log(\"\");\n-\n-            if (name.equals(\"ZGC Cycles\")) {\n-                cycles.incrementAndGet();\n-\n-                if (!action.equals(\"end of GC cycle\")) {\n-                    log(\"ERROR: Action\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getInit() != initialCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.init\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getUsed() > initialCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.used\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getCommitted() != initialCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.committed\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getMax() != maxCapacity) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.max\");\n-                    errors.incrementAndGet();\n-                }\n-            } else if (name.equals(\"ZGC Pauses\")) {\n-                pauses.incrementAndGet();\n-\n-                if (!action.equals(\"end of GC pause\")) {\n-                    log(\"ERROR: Action\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getInit() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.init\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getUsed() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.used\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getCommitted() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.committed\");\n-                    errors.incrementAndGet();\n-                }\n-\n-                if (memoryUsageBeforeGC.getMax() != 0) {\n-                    log(\"ERROR: MemoryUsageBeforeGC.max\");\n-                    errors.incrementAndGet();\n-                }\n-            } else {\n-                log(\"ERROR: Name\");\n-                errors.incrementAndGet();\n-            }\n-\n-            if (!cause.equals(\"System.gc()\")) {\n-                log(\"ERROR: Cause\");\n-                errors.incrementAndGet();\n-            }\n-\n-            if (startTime > endTime) {\n-                log(\"ERROR: StartTime\");\n-                errors.incrementAndGet();\n-            }\n-\n-            if (endTime - startTime != duration) {\n-                log(\"ERROR: Duration\");\n-                errors.incrementAndGet();\n-            }\n-        };\n-\n-        \/\/ Collect garbage created at startup\n-        System.gc();\n-\n-        \/\/ Register GC event listener\n-        for (final var collector : ManagementFactory.getGarbageCollectorMXBeans()) {\n-            final NotificationEmitter emitter = (NotificationEmitter)collector;\n-            emitter.addNotificationListener(listener, null, null);\n-        }\n-\n-        final int minCycles = 5;\n-        final int minPauses = minCycles * 3;\n-\n-        \/\/ Run GCs\n-        for (int i = 0; i < minCycles; i++) {\n-            log(\"Starting GC \" + i);\n-            System.gc();\n-        }\n-\n-        \/\/ Wait at most 90 seconds\n-        for (int i = 0; i < 90; i++) {\n-            log(\"Waiting...\");\n-            Thread.sleep(1000);\n-\n-            if (cycles.get() >= minCycles) {\n-                log(\"All events received!\");\n-                break;\n-            }\n-        }\n-\n-        final int actualCycles = cycles.get();\n-        final int actualPauses = pauses.get();\n-        final int actualErrors = errors.get();\n-\n-        log(\"   minCycles: \" + minCycles);\n-        log(\"   minPauses: \" + minPauses);\n-        log(\"actualCycles: \" + actualCycles);\n-        log(\"actualPauses: \" + actualPauses);\n-        log(\"actualErrors: \" + actualErrors);\n-\n-        \/\/ Verify number of cycle events\n-        if (actualCycles < minCycles) {\n-            throw new Exception(\"Unexpected cycles\");\n-        }\n-\n-        \/\/ Verify number of pause events\n-        if (actualPauses < minPauses) {\n-            throw new Exception(\"Unexpected pauses\");\n-        }\n-\n-        \/\/ Verify number of errors\n-        if (actualErrors != 0) {\n-            throw new Exception(\"Unexpected errors\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestGarbageCollectorMXBean.java","additions":0,"deletions":221,"binary":false,"changes":221,"status":"deleted"},{"patch":"@@ -1,101 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestHighUsage\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC \"High Usage\" rule\n- * @library \/test\/lib\n- * @run main\/othervm gc.x.TestHighUsage\n- *\/\n-\n-import java.util.LinkedList;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestHighUsage {\n-    static class Test {\n-        private static final int K = 1024;\n-        private static final int M = K * K;\n-        private static final long maxCapacity = Runtime.getRuntime().maxMemory();\n-        private static final long slowAllocationThreshold = 16 * M;\n-        private static final long highUsageThreshold = maxCapacity \/ 20; \/\/ 5%\n-        private static volatile LinkedList<byte[]> keepAlive;\n-        private static volatile Object dummy;\n-\n-        public static void main(String[] args) throws Exception {\n-            System.out.println(\"Max capacity: \" + (maxCapacity \/ M) + \"M\");\n-            System.out.println(\"High usage threshold: \" + (highUsageThreshold \/ M) + \"M\");\n-            System.out.println(\"Allocating live-set\");\n-\n-            \/\/ Allocate live-set\n-            keepAlive = new LinkedList<>();\n-            while (Runtime.getRuntime().freeMemory() > slowAllocationThreshold) {\n-                while (Runtime.getRuntime().freeMemory() > slowAllocationThreshold) {\n-                    keepAlive.add(new byte[128 * K]);\n-                }\n-\n-                \/\/ Compact live-set and let allocation rate settle down\n-                System.gc();\n-                Thread.sleep(2000);\n-            }\n-\n-            System.out.println(\"Allocating garbage slowly\");\n-\n-            \/\/ Allocate garbage slowly, so that the sampled allocation rate on average\n-            \/\/ becomes zero MB\/s for the last 1 second windows. Once we reach the high\n-            \/\/ usage threshold we idle to allow for a \"High Usage\" GC cycle to happen.\n-            \/\/ We need to allocate slowly to avoid an \"Allocation Rate\" GC cycle.\n-            for (int i = 0; i < 300; i++) {\n-                if (Runtime.getRuntime().freeMemory() > highUsageThreshold) {\n-                    \/\/ Allocate\n-                    dummy = new byte[128 * K];\n-                    System.out.println(\"Free: \" + (Runtime.getRuntime().freeMemory() \/ M) + \"M (Allocating)\");\n-                } else {\n-                    \/\/ Idle\n-                    System.out.println(\"Free: \" + (Runtime.getRuntime().freeMemory() \/ M) + \"M (Idling)\");\n-                }\n-\n-                Thread.sleep(250);\n-            }\n-\n-            System.out.println(\"Done\");\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        ProcessTools.executeTestJava(\"-XX:+UseZGC\",\n-                                     \"-XX:-ZGenerational\",\n-                                     \"-XX:-ZProactive\",\n-                                     \"-Xms128M\",\n-                                     \"-Xmx128M\",\n-                                     \"-XX:ParallelGCThreads=1\",\n-                                     \"-XX:ConcGCThreads=1\",\n-                                     \"-Xlog:gc,gc+start\",\n-                                     Test.class.getName())\n-                    .shouldNotContain(\"Allocation Stall\")\n-                    .shouldContain(\"High Usage\")\n-                    .shouldHaveExitValue(0);\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestHighUsage.java","additions":0,"deletions":101,"binary":false,"changes":101,"status":"deleted"},{"patch":"@@ -1,65 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/**\n- * @test TestMemoryMXBean\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC heap memory MXBean\n- * @modules java.management\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms128M -Xmx256M -Xlog:gc* gc.x.TestMemoryMXBean 128 256\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms256M -Xmx256M -Xlog:gc* gc.x.TestMemoryMXBean 256 256\n- *\/\n-\n-import java.lang.management.ManagementFactory;\n-\n-public class TestMemoryMXBean {\n-    public static void main(String[] args) throws Exception {\n-        final long M = 1024 * 1024;\n-        final long expectedInitialCapacity = Long.parseLong(args[0]) * M;\n-        final long expectedMaxCapacity = Long.parseLong(args[1]) * M;\n-        final var memoryUsage = ManagementFactory.getMemoryMXBean().getHeapMemoryUsage();\n-        final long initialCapacity = memoryUsage.getInit();\n-        final long capacity = memoryUsage.getCommitted();\n-        final long maxCapacity = memoryUsage.getMax();\n-\n-        System.out.println(\"expectedInitialCapacity: \" + expectedInitialCapacity);\n-        System.out.println(\"    expectedMaxCapacity: \" + expectedMaxCapacity);\n-        System.out.println(\"        initialCapacity: \" + initialCapacity);\n-        System.out.println(\"               capacity: \" + capacity);\n-        System.out.println(\"            maxCapacity: \" + maxCapacity);\n-\n-        if (initialCapacity != expectedInitialCapacity) {\n-            throw new Exception(\"Unexpected initial capacity\");\n-        }\n-\n-        if (maxCapacity != expectedMaxCapacity) {\n-            throw new Exception(\"Unexpected max capacity\");\n-        }\n-\n-        if (capacity < initialCapacity || capacity > maxCapacity) {\n-            throw new Exception(\"Unexpected capacity\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestMemoryMXBean.java","additions":0,"deletions":65,"binary":false,"changes":65,"status":"deleted"},{"patch":"@@ -1,92 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/**\n- * @test TestMemoryManagerMXBean\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC memory manager MXBean\n- * @modules java.management\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx128M gc.x.TestMemoryManagerMXBean\n- *\/\n-\n-import java.lang.management.ManagementFactory;\n-\n-public class TestMemoryManagerMXBean {\n-    private static void checkName(String name) throws Exception {\n-        if (name == null || name.length() == 0) {\n-            throw new Exception(\"Invalid name\");\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        int zgcCyclesMemoryManagers = 0;\n-        int zgcPausesMemoryManagers = 0;\n-        int zgcCyclesMemoryPools = 0;\n-        int zgcPausesMemoryPools = 0;\n-\n-        for (final var memoryManager : ManagementFactory.getMemoryManagerMXBeans()) {\n-            final var memoryManagerName = memoryManager.getName();\n-            checkName(memoryManagerName);\n-\n-            System.out.println(\"MemoryManager: \" + memoryManagerName);\n-\n-            if (memoryManagerName.equals(\"ZGC Cycles\")) {\n-                zgcCyclesMemoryManagers++;\n-            } else if (memoryManagerName.equals(\"ZGC Pauses\")) {\n-                zgcPausesMemoryManagers++;\n-            }\n-\n-            for (final var memoryPoolName : memoryManager.getMemoryPoolNames()) {\n-                checkName(memoryPoolName);\n-\n-                System.out.println(\"   MemoryPool:   \" + memoryPoolName);\n-\n-                if (memoryPoolName.equals(\"ZHeap\")) {\n-                    if (memoryManagerName.equals(\"ZGC Cycles\")) {\n-                        zgcCyclesMemoryPools++;\n-                    } else if (memoryManagerName.equals(\"ZGC Pauses\")) {\n-                        zgcPausesMemoryPools++;\n-                    }\n-                }\n-            }\n-        }\n-\n-        if (zgcCyclesMemoryManagers != 1) {\n-            throw new Exception(\"Unexpected number of cycle MemoryManagers\");\n-        }\n-\n-        if (zgcPausesMemoryManagers != 1) {\n-            throw new Exception(\"Unexpected number of pause MemoryManagers\");\n-        }\n-\n-        if (zgcCyclesMemoryPools != 1) {\n-            throw new Exception(\"Unexpected number of cycle MemoryPools\");\n-        }\n-\n-        if (zgcPausesMemoryPools != 1) {\n-            throw new Exception(\"Unexpected number of pause MemoryPools\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestMemoryManagerMXBean.java","additions":0,"deletions":92,"binary":false,"changes":92,"status":"deleted"},{"patch":"@@ -1,63 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestNoUncommit\n- * @requires vm.gc.ZSinglegen & !vm.graal.enabled\n- * @summary Test ZGC uncommit unused memory disabled\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms512M -Xmx512M -XX:ZUncommitDelay=1 gc.x.TestNoUncommit\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=1 -XX:-ZUncommit gc.x.TestNoUncommit\n- *\/\n-\n-public class TestNoUncommit {\n-    private static final int allocSize = 200 * 1024 * 1024; \/\/ 200M\n-    private static volatile Object keepAlive = null;\n-\n-    private static long capacity() {\n-        return Runtime.getRuntime().totalMemory();\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        System.out.println(\"Allocating\");\n-        keepAlive = new byte[allocSize];\n-        final var afterAlloc = capacity();\n-\n-        System.out.println(\"Reclaiming\");\n-        keepAlive = null;\n-        System.gc();\n-\n-        \/\/ Wait longer than the uncommit delay (which is 1 second)\n-        Thread.sleep(5 * 1000);\n-\n-        final var afterDelay = capacity();\n-\n-        \/\/ Verify\n-        if (afterAlloc > afterDelay) {\n-            throw new Exception(\"Should not uncommit\");\n-        }\n-\n-        System.out.println(\"Success\");\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestNoUncommit.java","additions":0,"deletions":63,"binary":false,"changes":63,"status":"deleted"},{"patch":"@@ -1,83 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestPageCacheFlush\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC page cache flushing\n- * @library \/test\/lib\n- * @run driver gc.x.TestPageCacheFlush\n- *\/\n-\n-import java.util.LinkedList;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestPageCacheFlush {\n-    static class Test {\n-        private static final int K = 1024;\n-        private static final int M = K * K;\n-        private static volatile LinkedList<byte[]> keepAlive;\n-\n-        public static void fillPageCache(int size) {\n-            System.out.println(\"Begin allocate (\" + size + \")\");\n-\n-            keepAlive = new LinkedList<>();\n-\n-            try {\n-                for (;;) {\n-                    keepAlive.add(new byte[size]);\n-                }\n-            } catch (OutOfMemoryError e) {\n-                keepAlive = null;\n-                System.gc();\n-            }\n-\n-            System.out.println(\"End allocate (\" + size + \")\");\n-        }\n-\n-        public static void main(String[] args) throws Exception {\n-            \/\/ Allocate small objects to fill the page cache with small pages\n-            fillPageCache(10 * K);\n-\n-            \/\/ Allocate large objects to provoke page cache flushing to rebuild\n-            \/\/ cached small pages into large pages\n-            fillPageCache(10 * M);\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        ProcessTools.executeTestJava(\n-            \"-XX:+UseZGC\",\n-            \"-XX:-ZGenerational\",\n-            \"-Xms128M\",\n-            \"-Xmx128M\",\n-            \"-Xlog:gc,gc+init,gc+heap=debug\",\n-            Test.class.getName())\n-                .outputTo(System.out)\n-                .errorTo(System.out)\n-                .shouldContain(\"Page Cache Flushed:\")\n-                .shouldHaveExitValue(0);\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestPageCacheFlush.java","additions":0,"deletions":83,"binary":false,"changes":83,"status":"deleted"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestRelocateInPlace\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC in-place relocateion\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.x.TestRelocateInPlace\n- *\/\n-\n-import java.util.ArrayList;\n-\n-public class TestRelocateInPlace {\n-    private static final int allocSize = 100 * 1024 * 1024; \/\/ 100M\n-    private static final int smallObjectSize = 4 * 1024; \/\/ 4K\n-    private static final int mediumObjectSize = 2 * 1024 * 1024; \/\/ 2M\n-\n-    private static volatile ArrayList<byte[]> keepAlive;\n-\n-    private static void allocate(int objectSize) {\n-        keepAlive = new ArrayList<>();\n-        for (int i = 0; i < allocSize; i+= objectSize) {\n-            keepAlive.add(new byte[objectSize]);\n-        }\n-    }\n-\n-    private static void fragment() {\n-        \/\/ Release every other reference to cause lots of fragmentation\n-        for (int i = 0; i < keepAlive.size(); i += 2) {\n-            keepAlive.set(i, null);\n-        }\n-    }\n-\n-    private static void test(int objectSize) throws Exception {\n-        System.out.println(\"Allocating\");\n-        allocate(objectSize);\n-\n-        System.out.println(\"Fragmenting\");\n-        fragment();\n-\n-        System.out.println(\"Reclaiming\");\n-        System.gc();\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        for (int i = 0; i < 10; i++) {\n-            System.out.println(\"Iteration \" + i);\n-            test(smallObjectSize);\n-            test(mediumObjectSize);\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestRelocateInPlace.java","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -1,68 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestSmallHeap\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC with small heaps\n- * @library \/ \/test\/lib\n- * @run driver gc.x.TestSmallHeap 8M 16M 32M 64M 128M 256M 512M 1024M\n- *\/\n-\n-import jdk.test.lib.process.ProcessTools;\n-import static gc.testlibrary.Allocation.blackHole;\n-\n-public class TestSmallHeap {\n-    public static class Test {\n-        public static void main(String[] args) throws Exception {\n-            final long maxCapacity = Runtime.getRuntime().maxMemory();\n-            System.out.println(\"Max Capacity \" + maxCapacity + \" bytes\");\n-\n-            \/\/ Allocate byte arrays of increasing length, so that\n-            \/\/ all allocation paths (small\/medium\/large) are tested.\n-            for (int length = 16; length <= maxCapacity \/ 16; length *= 2) {\n-                System.out.println(\"Allocating \" + length + \" bytes\");\n-                blackHole(new byte[length]);\n-            }\n-\n-            System.out.println(\"Success\");\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        for (var maxCapacity: args) {\n-            ProcessTools.executeTestJava(\n-                \"-XX:+UseZGC\",\n-                \"-XX:-ZGenerational\",\n-                \"-Xlog:gc,gc+init,gc+reloc,gc+heap\",\n-                \"-Xmx\" + maxCapacity,\n-                Test.class.getName())\n-                    .outputTo(System.out)\n-                    .errorTo(System.out)\n-                    .shouldContain(\"Success\")\n-                    .shouldHaveExitValue(0);\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestSmallHeap.java","additions":0,"deletions":68,"binary":false,"changes":68,"status":"deleted"},{"patch":"@@ -1,136 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.x;\n-\n-\/*\n- * @test TestUncommit\n- * @requires vm.gc.ZSinglegen\n- * @summary Test ZGC uncommit unused memory\n- * @library \/test\/lib\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.x.TestUncommit\n- *\/\n-\n-import java.util.ArrayList;\n-import jdk.test.lib.Utils;\n-\n-public class TestUncommit {\n-    private static final int delay = 10 * 1000; \/\/ milliseconds\n-    private static final int allocSize = 200 * 1024 * 1024; \/\/ 200M\n-    private static final int smallObjectSize = 4 * 1024; \/\/ 4K\n-    private static final int mediumObjectSize = 2 * 1024 * 1024; \/\/ 2M\n-    private static final int largeObjectSize = allocSize;\n-\n-    private static volatile ArrayList<byte[]> keepAlive;\n-\n-    private static final long startTime = System.nanoTime();\n-\n-    private static void log(String msg) {\n-        final String elapsedSeconds = String.format(\"%.3fs\", (System.nanoTime() - startTime) \/ 1_000_000_000.0);\n-        System.out.println(\"[\" + elapsedSeconds + \"] (\" + Thread.currentThread().getName() + \") \" + msg);\n-    }\n-\n-    private static long capacity() {\n-        return Runtime.getRuntime().totalMemory();\n-    }\n-\n-    private static void allocate(int objectSize) {\n-        keepAlive = new ArrayList<>();\n-        for (int i = 0; i < allocSize; i+= objectSize) {\n-            keepAlive.add(new byte[objectSize]);\n-        }\n-    }\n-\n-    private static void reclaim() {\n-        keepAlive = null;\n-        System.gc();\n-    }\n-\n-    private static void test(int objectSize) throws Exception {\n-        final var beforeAlloc = capacity();\n-        final var timeBeforeAlloc = System.nanoTime();\n-\n-        \/\/ Allocate memory\n-        log(\"Allocating\");\n-        allocate(objectSize);\n-\n-        final var afterAlloc = capacity();\n-\n-        \/\/ Reclaim memory\n-        log(\"Reclaiming\");\n-        reclaim();\n-\n-        log(\"Waiting for uncommit to start\");\n-        while (capacity() >= afterAlloc) {\n-            Thread.sleep(1000);\n-        }\n-\n-        log(\"Uncommit started\");\n-        final var timeUncommitStart = System.nanoTime();\n-        final var actualDelay = (timeUncommitStart - timeBeforeAlloc) \/ 1_000_000;\n-\n-        log(\"Waiting for uncommit to complete\");\n-        while (capacity() > beforeAlloc) {\n-            Thread.sleep(1000);\n-        }\n-\n-        log(\"Uncommit completed\");\n-        final var afterUncommit = capacity();\n-\n-        log(\"        Uncommit Delay: \" + delay);\n-        log(\"           Object Size: \" + objectSize);\n-        log(\"            Alloc Size: \" + allocSize);\n-        log(\"          Before Alloc: \" + beforeAlloc);\n-        log(\"           After Alloc: \" + afterAlloc);\n-        log(\"        After Uncommit: \" + afterUncommit);\n-        log(\" Actual Uncommit Delay: \" + actualDelay);\n-\n-        \/\/ Verify\n-        if (actualDelay < delay) {\n-            throw new Exception(\"Uncommitted too fast\");\n-        }\n-\n-        if (actualDelay > delay * 2 * Utils.TIMEOUT_FACTOR) {\n-            throw new Exception(\"Uncommitted too slow\");\n-        }\n-\n-        if (afterUncommit < beforeAlloc) {\n-            throw new Exception(\"Uncommitted too much\");\n-        }\n-\n-        if (afterUncommit > beforeAlloc) {\n-            throw new Exception(\"Uncommitted too little\");\n-        }\n-\n-        log(\"Success\");\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        for (int i = 0; i < 2; i++) {\n-            log(\"Iteration \" + i);\n-            test(smallObjectSize);\n-            test(mediumObjectSize);\n-            test(largeObjectSize);\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/x\/TestUncommit.java","additions":0,"deletions":136,"binary":false,"changes":136,"status":"deleted"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational & os.family == \"linux\"\n+ * @requires vm.gc.Z & os.family == \"linux\"\n@@ -47,1 +47,0 @@\n-            \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestAllocateHeapAt.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational & os.family == \"linux\"\n+ * @requires vm.gc.Z & os.family == \"linux\"\n@@ -80,1 +80,0 @@\n-            \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestAllocateHeapAtWithHugeTLBFS.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -30,5 +30,5 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:-AlwaysPreTouch -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:-AlwaysPreTouch -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms2M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=1 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc* -XX:+AlwaysPreTouch -XX:ParallelGCThreads=8 -Xms128M -Xmx128M gc.z.TestAlwaysPreTouch\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestAlwaysPreTouch.java","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1,51 +0,0 @@\n-\/*\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.z;\n-\n-\/*\n- * @test TestDefault\n- * @requires vm.gc.ZGenerational\n- * @summary Test that ZGC Generational Mode is Default\n- * @library \/test\/lib\n- * @run driver gc.z.TestDefault\n- *\/\n-\n-import java.util.LinkedList;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestDefault {\n-    static class Test {\n-        public static void main(String[] args) throws Exception {}\n-    }\n-    public static void main(String[] args) throws Exception {\n-        ProcessTools.executeLimitedTestJava(\"-XX:+UseZGC\",\n-                                            \"-Xlog:gc+init\",\n-                                            Test.class.getName())\n-                    .shouldNotContain(\"Option ZGenerational was deprecated\")\n-                    .shouldNotContain(\"Using deprecated non-generational mode\")\n-                    .shouldContain(\"GC Workers for Old Generation\")\n-                    .shouldContain(\"GC Workers for Young Generation\")\n-                    .shouldHaveExitValue(0);\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestDefault.java","additions":0,"deletions":51,"binary":false,"changes":51,"status":"deleted"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -32,2 +32,2 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms256M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 256 512\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms512M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 512 512\n+ * @run main\/othervm -XX:+UseZGC -Xms256M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 256 512\n+ * @run main\/othervm -XX:+UseZGC -Xms512M -Xmx512M -Xlog:gc gc.z.TestGarbageCollectorMXBean 512 512\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestGarbageCollectorMXBean.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -31,2 +31,2 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms128M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 128 256\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms256M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 256 256\n+ * @run main\/othervm -XX:+UseZGC -Xms128M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 128 256\n+ * @run main\/othervm -XX:+UseZGC -Xms256M -Xmx256M -Xlog:gc* gc.z.TestMemoryMXBean 256 256\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestMemoryMXBean.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx128M gc.z.TestMemoryManagerMXBean\n+ * @run main\/othervm -XX:+UseZGC -Xmx128M gc.z.TestMemoryManagerMXBean\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestMemoryManagerMXBean.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational & !vm.graal.enabled\n+ * @requires vm.gc.Z & !vm.graal.enabled\n@@ -30,2 +30,2 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms512M -Xmx512M -XX:ZUncommitDelay=1 gc.z.TestNoUncommit\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=1 -XX:-ZUncommit gc.z.TestNoUncommit\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms512M -Xmx512M -XX:ZUncommitDelay=1 gc.z.TestNoUncommit\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=1 -XX:-ZUncommit gc.z.TestNoUncommit\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestNoUncommit.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -73,1 +73,0 @@\n-            \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestPageCacheFlush.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n- * @requires vm.gc.ZGenerational & vm.debug\n+ * @requires vm.gc.Z & vm.debug\n@@ -319,1 +319,0 @@\n-        command.add(\"-XX:+ZGenerational\");\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestRegistersPushPopAtZGCLoadBarrierStub.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -30,1 +30,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.z.TestRelocateInPlace\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+stats=off -Xmx256M -XX:+UnlockDiagnosticVMOptions -XX:+ZStressRelocateInPlace gc.z.TestRelocateInPlace\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestRelocateInPlace.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -58,1 +58,0 @@\n-                \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestSmallHeap.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -31,1 +31,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.z.TestUncommit\n+ * @run main\/othervm -XX:+UseZGC -Xlog:gc*,gc+heap=debug,gc+stats=off -Xms128M -Xmx512M -XX:ZUncommitDelay=10 gc.z.TestUncommit\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestUncommit.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires vm.gc.ZGenerational & vm.debug\n+ * @requires vm.gc.Z & vm.debug\n@@ -50,1 +50,0 @@\n-            \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZForceDiscontiguousHeapReservations.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,2 +29,2 @@\n- * @requires vm.gc.ZGenerational & vm.debug\n- * @summary Test NMT and ZGenerational heap reservation \/ commits interactions.\n+ * @requires vm.gc.Z & vm.debug\n+ * @summary Test NMT and ZGC heap reservation \/ commits interactions.\n@@ -73,1 +73,0 @@\n-            \"-XX:+ZGenerational\",\n","filename":"test\/hotspot\/jtreg\/gc\/z\/TestZNMT.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-            {\"ZGenerational\", \"false\"},\n","filename":"test\/hotspot\/jtreg\/runtime\/CommandLine\/VMDeprecatedOptions.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n- * @test id=ZSinglegen\n+ * @test\n@@ -29,1 +29,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -34,14 +34,1 @@\n- * @run driver TestZGCWithCDS -XX:-ZGenerational\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8232069\n- * @requires vm.cds\n- * @requires vm.bits == 64\n- * @requires vm.gc.ZGenerational\n- * @requires vm.gc.Serial\n- * @requires vm.gc == null\n- * @library \/test\/lib \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\n- * @compile test-classes\/Hello.java\n- * @run driver TestZGCWithCDS -XX:+ZGenerational\n+ * @run driver TestZGCWithCDS\n@@ -58,1 +45,0 @@\n-         String zGenerational = args[0];\n@@ -65,1 +51,0 @@\n-                                        zGenerational,\n@@ -74,1 +59,0 @@\n-                         zGenerational,\n@@ -154,1 +138,0 @@\n-                         zGenerational,\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":3,"deletions":20,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n- * @test id=custom-cl-zgc-singlegen\n+ * @test id=custom-cl-zgc\n@@ -58,1 +58,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -72,17 +72,0 @@\n-\/**\n- * @test id=custom-cl-zgc-generational\n- * @requires vm.cds.custom.loaders\n- * @requires vm.gc.ZGenerational\n- * @summary Test dumptime_table entries are removed with zgc eager class unloading\n- * @bug 8274935\n- * @library \/test\/lib\n- *          \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\n- *          \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\n- *          \/test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\n- * @modules java.base\/jdk.internal.misc\n- *          jdk.httpserver\n- * @build jdk.test.whitebox.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n- * @run main\/othervm\/timeout=180 -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -Xbootclasspath\/a:. DynamicLoaderConstraintsTest custom-zgc-generational\n- *\/\n-\n@@ -121,1 +104,0 @@\n-    static boolean useZGenerational;\n@@ -125,2 +107,1 @@\n-        useZGenerational = (args.length != 0 && args[0].equals(\"custom-zgc-generational\"));\n-        useZGC = useZGenerational || (args.length != 0 && args[0].equals(\"custom-zgc\"));\n+        useZGC = (args.length != 0 && args[0].equals(\"custom-zgc\"));\n@@ -153,1 +134,1 @@\n-                               \", useZGC: \" + useZGC + \", ZGenerational: \" + useZGenerational + \", case: \" + i);\n+                               \", useZGC: \" + useZGC + \", case: \" + i);\n@@ -167,1 +148,0 @@\n-                    String zGenerational = \"-XX:\" + (useZGenerational ? \"+\" : \"-\") + \"ZGenerational\";\n@@ -170,1 +150,1 @@\n-                                                \"-XX:+UseZGC\", zGenerational, \"-XX:ZCollectionInterval=0.01\",\n+                                                \"-XX:+UseZGC\", \"-XX:ZCollectionInterval=0.01\",\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/loaderConstraints\/DynamicLoaderConstraintsTest.java","additions":5,"deletions":25,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -74,2 +74,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.gc.Z\n@@ -82,13 +82,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational HeapDumpCompressedTest\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @requires vm.gc.ZGenerational\n- * @summary Test of diagnostic command GC.heap_dump with gzipped output (Z GC)\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.compiler\n- *          java.management\n- *          jdk.internal.jvmstat\/sun.jvmstat.monitor\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational HeapDumpCompressedTest\n+ * @run main\/othervm -XX:+UseZGC HeapDumpCompressedTest\n","filename":"test\/hotspot\/jtreg\/serviceability\/dcmd\/gc\/HeapDumpCompressedTest.java","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -64,1 +64,1 @@\n-        TestFramework.runWithFlags(\"-XX:+UseZGC\", \"-XX:+ZGenerational\");\n+        TestFramework.runWithFlags(\"-XX:+UseZGC\");\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/examples\/GCBarrierIRExample.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,40 +0,0 @@\n-#\n-# Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n-# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-#\n-# This code is free software; you can redistribute it and\/or modify it\n-# under the terms of the GNU General Public License version 2 only, as\n-# published by the Free Software Foundation.\n-#\n-# This code is distributed in the hope that it will be useful, but WITHOUT\n-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-# version 2 for more details (a copy is included in the LICENSE file that\n-# accompanied this code).\n-#\n-# You should have received a copy of the GNU General Public License version\n-# 2 along with this work; if not, write to the Free Software Foundation,\n-# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-#\n-# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-# or visit www.oracle.com if you need additional information or have any\n-# questions.\n-#\n-\n-#############################################################################\n-#\n-# List of quarantined tests for testing with Generational ZGC.\n-#\n-#############################################################################\n-\n-# Quiet all SA tests\n-\n-sun\/tools\/jhsdb\/HeapDumpTest.java                  8307393   generic-all\n-sun\/tools\/jhsdb\/BasicLauncherTest.java             8307393   generic-all\n-sun\/tools\/jhsdb\/JStackStressTest.java              8307393   generic-all\n-sun\/tools\/jhsdb\/JShellHeapDumpTest.java            8307393   generic-all\n-sun\/tools\/jhsdb\/SAGetoptTest.java                  8307393   generic-all\n-sun\/tools\/jhsdb\/heapconfig\/JMapHeapConfigTest.java 8307393   generic-all\n-sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8307393   generic-all\n-\n-com\/sun\/jdi\/ThreadMemoryLeakTest.java          8307402 generic-all\n","filename":"test\/jdk\/ProblemList-generational-zgc.txt","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -30,2 +30,11 @@\n-sun\/tools\/jhsdb\/JShellHeapDumpTest.java            8276539 generic-all\n-sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8276539 generic-all\n+# Quiet all SA tests\n+\n+sun\/tools\/jhsdb\/HeapDumpTest.java                  8307393   generic-all\n+sun\/tools\/jhsdb\/BasicLauncherTest.java             8307393   generic-all\n+sun\/tools\/jhsdb\/JStackStressTest.java              8307393   generic-all\n+sun\/tools\/jhsdb\/JShellHeapDumpTest.java            8307393   generic-all\n+sun\/tools\/jhsdb\/SAGetoptTest.java                  8307393   generic-all\n+sun\/tools\/jhsdb\/heapconfig\/JMapHeapConfigTest.java 8307393   generic-all\n+sun\/tools\/jhsdb\/HeapDumpTestWithActiveProcess.java 8307393   generic-all\n+\n+com\/sun\/jdi\/ThreadMemoryLeakTest.java          8307402 generic-all\n","filename":"test\/jdk\/ProblemList-zgc.txt","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -87,2 +87,0 @@\n-    vm.gc.ZGenerational \\\n-    vm.gc.ZSinglegen \\\n","filename":"test\/jdk\/TEST.ROOT","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -41,14 +41,3 @@\n- * @requires vm.gc.ZSinglegen\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Dsun.java2d.uiScale=1 LargeWindowPaintTest\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Dsun.java2d.uiScale=1 -Dsun.java2d.d3d=false LargeWindowPaintTest\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8240654\n- * @summary Test painting a large window works\n- * @key headful\n- * @requires (os.family == \"windows\")\n- * @requires vm.gc.ZGenerational\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Dsun.java2d.uiScale=1 LargeWindowPaintTest\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Dsun.java2d.uiScale=1 -Dsun.java2d.d3d=false LargeWindowPaintTest\n+ * @requires vm.gc.Z\n+ * @run main\/othervm -XX:+UseZGC -Dsun.java2d.uiScale=1 LargeWindowPaintTest\n+ * @run main\/othervm -XX:+UseZGC -Dsun.java2d.uiScale=1 -Dsun.java2d.d3d=false LargeWindowPaintTest\n","filename":"test\/jdk\/java\/awt\/Graphics2D\/LargeWindowPaintTest.java","additions":4,"deletions":15,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -30,12 +30,2 @@\n- * @requires vm.gc.ZSinglegen\n- * @run main\/manual\/othervm -XX:+UseZGC -XX:-ZGenerational -Dsun.java2d.d3d=false AlphaPrintTest\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8240654\n- * @summary Test printing alpha colors - banded printing works with ZGC.\n- * @key headful printer\n- * @requires (os.family == \"windows\")\n- * @requires vm.gc.ZGenerational\n- * @run main\/manual\/othervm -XX:+UseZGC -XX:+ZGenerational -Dsun.java2d.d3d=false AlphaPrintTest\n+ * @requires vm.gc.Z\n+ * @run main\/manual\/othervm -XX:+UseZGC -Dsun.java2d.d3d=false AlphaPrintTest\n","filename":"test\/jdk\/java\/awt\/print\/PrinterJob\/AlphaPrintTest.java","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -42,2 +42,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.gc.Z\n@@ -55,19 +55,1 @@\n- *   -XX:+UseZGC -XX:-ZGenerational\n- *   TestAsyncStackWalk\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library ..\/\n- * @build jdk.test.whitebox.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n- *\n- * @run main\/othervm\n- *   -Xbootclasspath\/a:.\n- *   -XX:+UnlockDiagnosticVMOptions\n- *   -XX:+WhiteBoxAPI\n- *   --enable-native-access=ALL-UNNAMED\n- *   -Xbatch\n- *   -XX:+UseZGC -XX:+ZGenerational\n+ *   -XX:+UseZGC\n","filename":"test\/jdk\/java\/foreign\/stackwalk\/TestAsyncStackWalk.java","additions":3,"deletions":21,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -42,2 +42,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.gc.Z\n@@ -55,19 +55,1 @@\n- *   -XX:+UseZGC -XX:-ZGenerational\n- *   TestStackWalk\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @requires vm.gc.ZGenerational\n- * @library \/test\/lib\n- * @library ..\/\n- * @build jdk.test.whitebox.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n- *\n- * @run main\/othervm\n- *   -Xbootclasspath\/a:.\n- *   -XX:+UnlockDiagnosticVMOptions\n- *   -XX:+WhiteBoxAPI\n- *   --enable-native-access=ALL-UNNAMED\n- *   -Xbatch\n- *   -XX:+UseZGC -XX:+ZGenerational\n+ *   -XX:+UseZGC\n","filename":"test\/jdk\/java\/foreign\/stackwalk\/TestStackWalk.java","additions":3,"deletions":21,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -52,2 +52,2 @@\n- * @test id=ZGenerational\n- * @requires vm.gc.ZGenerational\n+ * @test id=Z\n+ * @requires vm.gc.Z\n@@ -57,1 +57,1 @@\n- * @run testng\/othervm -Xmx64m -XX:+UseZGC -XX:+ZGenerational ObjectStreamClassCaching\n+ * @run testng\/othervm -Xmx64m -XX:+UseZGC ObjectStreamClassCaching\n","filename":"test\/jdk\/java\/io\/ObjectStreamClass\/ObjectStreamClassCaching.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n- * @test\n+ * @test id=Default\n@@ -30,1 +30,1 @@\n- * @requires !vm.opt.final.ZGenerational\n+ * @requires vm.gc != \"Z\"\n@@ -37,1 +37,1 @@\n- * @test\n+ * @test id=Z\n@@ -39,1 +39,1 @@\n- * @requires vm.gc.Z & vm.opt.final.ZGenerational\n+ * @requires vm.gc.Z\n@@ -42,1 +42,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M -Dtest.duration=2 CloseRace\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M -Dtest.duration=2 CloseRace\n","filename":"test\/jdk\/java\/lang\/ProcessBuilder\/CloseRace.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,10 +31,0 @@\n-\n-\/*\n- * @test id=ZSinglegen\n- * @requires vm.debug == true & vm.continuations\n- * @requires vm.gc.ZSinglegen\n- * @run main\/othervm\/timeout=300 -XX:+UnlockDiagnosticVMOptions\n- *     -XX:+UseZGC -XX:-ZGenerational\n- *     -XX:+ZVerifyOops -XX:ZCollectionInterval=0.01 -Xmx1500m Skynet\n- *\/\n-\n@@ -42,1 +32,1 @@\n- * @test id=ZGenerational\n+ * @test id=Z\n@@ -44,1 +34,1 @@\n- * @requires vm.gc.ZGenerational\n+ * @requires vm.gc.Z\n@@ -46,1 +36,1 @@\n- *     -XX:+UseZGC -XX:+ZGenerational\n+ *     -XX:+UseZGC\n","filename":"test\/jdk\/java\/lang\/Thread\/virtual\/stress\/Skynet.java","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -41,1 +41,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -45,13 +45,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational MemoryTest 2 1\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug     4530538\n- * @summary Basic unit test of MemoryMXBean.getMemoryPools() and\n- *          MemoryMXBean.getMemoryManager().\n- * @requires vm.gc.ZGenerational\n- * @author  Mandy Chung\n- *\n- * @modules jdk.management\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational MemoryTest 4 2\n+ * @run main\/othervm -XX:+UseZGC MemoryTest 4 2\n","filename":"test\/jdk\/java\/lang\/management\/MemoryMXBean\/MemoryTest.java","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -49,2 +49,2 @@\n- * @test id=with_ZGC_Singlegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=with_ZGC\n+ * @requires vm.gc.Z\n@@ -53,9 +53,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational TypeConverterFactoryMemoryLeakTest\n- *\/\n-\n-\/*\n- * @test id=with_ZGC_Generational\n- * @requires vm.gc.ZGenerational\n- * @bug 8198540\n- * @summary Test TypeConverterFactory is not leaking method handles (Z GC)\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational TypeConverterFactoryMemoryLeakTest\n+ * @run main\/othervm -XX:+UseZGC TypeConverterFactoryMemoryLeakTest\n","filename":"test\/jdk\/jdk\/dynalink\/TypeConverterFactoryMemoryLeakTest.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -49,2 +49,2 @@\n- * @test id=with_ZGC_Singlegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=with_ZGC\n+ * @requires vm.gc.Z\n@@ -53,9 +53,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational TypeConverterFactoryRetentionTests\n- *\/\n-\n-\/*\n- * @test id=with_ZGC_Generational\n- * @requires vm.gc.ZGenerational\n- * @bug 8198540\n- * @summary Test TypeConverterFactory is not leaking class loaders (Z GC)\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational TypeConverterFactoryRetentionTests\n+ * @run main\/othervm -XX:+UseZGC TypeConverterFactoryRetentionTests\n","filename":"test\/jdk\/jdk\/dynalink\/TypeConverterFactoryRetentionTests.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n- * @test id=ZSinglegen\n+ * @test id=Z\n@@ -47,1 +47,1 @@\n- * @requires vm.gc.ZSinglegen\n+ * @requires vm.gc.Z\n@@ -51,13 +51,1 @@\n- *                      -XX:+UnlockDiagnosticVMOptions -XX:+UseKNLSetting -XX:+UseZGC -XX:-ZGenerational -XX:+IgnoreUnrecognizedVMOptions\n- *                      VectorMaxConversionTests\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @bug 8281544\n- * @summary Test that ZGC and vectorapi with KNL work together.\n- * @requires vm.gc.ZGenerational\n- * @modules jdk.incubator.vector\n- * @modules java.base\/jdk.internal.vm.annotation\n- * @run testng\/othervm  -XX:-TieredCompilation --add-opens jdk.incubator.vector\/jdk.incubator.vector=ALL-UNNAMED\n- *                      -XX:+UnlockDiagnosticVMOptions -XX:+UseKNLSetting -XX:+UseZGC -XX:+ZGenerational -XX:+IgnoreUnrecognizedVMOptions\n+ *                      -XX:+UnlockDiagnosticVMOptions -XX:+UseKNLSetting -XX:+UseZGC -XX:+IgnoreUnrecognizedVMOptions\n","filename":"test\/jdk\/jdk\/incubator\/vector\/VectorMaxConversionTests.java","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -41,1 +41,1 @@\n- * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+ZGenerational -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestGarbageCollectionEventWithZMajor\n+ * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestGarbageCollectionEventWithZMajor\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestGarbageCollectionEventWithZMajor.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -48,1 +48,1 @@\n- * @run main\/othervm -Xbootclasspath\/a:. -XX:+UseZGC  -XX:+ZGenerational -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestGarbageCollectionEventWithZMinor\n+ * @run main\/othervm -Xbootclasspath\/a:. -XX:+UseZGC -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestGarbageCollectionEventWithZMinor\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestGarbageCollectionEventWithZMinor.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -41,1 +41,1 @@\n- * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+ZGenerational -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestZOldGarbageCollectionEvent\n+ * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestZOldGarbageCollectionEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestZOldGarbageCollectionEvent.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -41,1 +41,1 @@\n- * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+ZGenerational -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestZYoungGarbageCollectionEvent\n+ * @run main\/othervm -Xmx50m -XX:+UseZGC -XX:+UnlockExperimentalVMOptions -XX:-UseFastUnorderedTimeStamps -Xlog:gc* jdk.jfr.event.gc.collection.TestZYoungGarbageCollectionEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestZYoungGarbageCollectionEvent.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n- * @test id=ZGenerational\n+ * @test id=Z\n@@ -37,10 +37,2 @@\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent Z\n- *\/\n-\n-\/**\n- * @test id=ZSinglegen\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent X\n+ * @requires vm.hasJFR & vm.gc.Z\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestGCPhaseConcurrent Z\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestGCPhaseConcurrent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -39,9 +39,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx32M -Xlog:gc*:gc.log::filecount=0 jdk.jfr.event.gc.detailed.TestZAllocationStallEvent\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M -Xlog:gc*:gc.log::filecount=0 jdk.jfr.event.gc.detailed.TestZAllocationStallEvent\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M -Xlog:gc*:gc.log::filecount=0 jdk.jfr.event.gc.detailed.TestZAllocationStallEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZAllocationStallEvent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -39,9 +39,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZPageAllocationEvent\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZPageAllocationEvent\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestZPageAllocationEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZPageAllocationEvent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -39,9 +39,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZRelocationSetEvent\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZRelocationSetEvent\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestZRelocationSetEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZRelocationSetEvent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -39,9 +39,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZRelocationSetGroupEvent\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZRelocationSetGroupEvent\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestZRelocationSetGroupEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZRelocationSetGroupEvent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -41,9 +41,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xms32M -Xmx128M -Xlog:gc,gc+heap -XX:+ZUncommit -XX:ZUncommitDelay=1 jdk.jfr.event.gc.detailed.TestZUncommitEvent\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xms32M -Xmx128M -Xlog:gc,gc+heap -XX:+ZUncommit -XX:ZUncommitDelay=1 jdk.jfr.event.gc.detailed.TestZUncommitEvent\n+ * @run main\/othervm -XX:+UseZGC -Xms32M -Xmx128M -Xlog:gc,gc+heap -XX:+ZUncommit -XX:ZUncommitDelay=1 jdk.jfr.event.gc.detailed.TestZUncommitEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZUncommitEvent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -39,9 +39,1 @@\n- * @run main\/othervm -XX:+UseZGC -XX:-ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZUnmapEvent\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @library \/test\/lib \/test\/jdk \/test\/hotspot\/jtreg\n- * @run main\/othervm -XX:+UseZGC -XX:+ZGenerational -Xmx32M jdk.jfr.event.gc.detailed.TestZUnmapEvent\n+ * @run main\/othervm -XX:+UseZGC -Xmx32M jdk.jfr.event.gc.detailed.TestZUnmapEvent\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestZUnmapEvent.java","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -36,2 +36,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.hasJFR & vm.gc.ZSinglegen\n+ * @test\n+ * @requires vm.hasJFR & vm.gc.Z\n@@ -42,11 +42,1 @@\n- * @run main\/othervm  -XX:TLABSize=2k -XX:+UseZGC -XX:-ZGenerational jdk.jfr.event.oldobject.TestZ\n- *\/\n-\n-\/**\n- * @test id=ZGenerational\n- * @requires vm.hasJFR & vm.gc.ZGenerational\n- * @key jfr\n- * @summary Test leak profiler with ZGC\n- * @library \/test\/lib \/test\/jdk\n- * @modules jdk.jfr\/jdk.jfr.internal.test\n- * @run main\/othervm  -XX:TLABSize=2k -XX:+UseZGC -XX:+ZGenerational jdk.jfr.event.oldobject.TestZ\n+ * @run main\/othervm  -XX:TLABSize=2k -XX:+UseZGC jdk.jfr.event.oldobject.TestZ\n","filename":"test\/jdk\/jdk\/jfr\/event\/oldobject\/TestZ.java","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -90,2 +90,2 @@\n- * @test id=ZSinglegen\n- * @requires vm.gc.ZSinglegen\n+ * @test id=Z\n+ * @requires vm.gc.Z\n@@ -99,14 +99,1 @@\n- * @run main\/othervm\/timeout=240 -XX:+UseZGC -XX:-ZGenerational BasicJMapTest\n- *\/\n-\n-\/*\n- * @test id=ZGenerational\n- * @requires vm.gc.ZGenerational\n- * @summary Unit test for jmap utility (Z GC)\n- * @key intermittent\n- * @library \/test\/lib\n- * @build jdk.test.lib.hprof.*\n- * @build jdk.test.lib.hprof.model.*\n- * @build jdk.test.lib.hprof.parser.*\n- * @build jdk.test.lib.hprof.util.*\n- * @run main\/othervm\/timeout=240 -XX:+UseZGC -XX:+ZGenerational BasicJMapTest\n+ * @run main\/othervm\/timeout=240 -XX:+UseZGC BasicJMapTest\n","filename":"test\/jdk\/sun\/tools\/jmap\/BasicJMapTest.java","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -326,11 +326,0 @@\n-\n-        \/\/ Special handling for ZGC modes\n-        var vmGCZ = vmGCProperty.test(GC.Z);\n-        var genZ = WB.getBooleanVMFlag(\"ZGenerational\");\n-        var genZIsDefault = WB.isDefaultVMFlag(\"ZGenerational\");\n-        \/\/ vm.gc.ZGenerational=true means:\n-        \/\/    vm.gc.Z is true and ZGenerational is either explicitly true, or default\n-        map.put(\"vm.gc.ZGenerational\", () -> \"\" + (vmGCZ && (genZ || genZIsDefault)));\n-        \/\/ vm.gc.ZSinglegen=true means:\n-        \/\/    vm.gc.Z is true and ZGenerational is either explicitly false, or default\n-        map.put(\"vm.gc.ZSinglegen\", () -> \"\" + (vmGCZ && (!genZ || genZIsDefault)));\n@@ -391,1 +380,0 @@\n-        vmOptFinalFlag(map, \"ZGenerational\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"}]}