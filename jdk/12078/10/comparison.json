{"files":[{"patch":"@@ -0,0 +1,132 @@\n+\/*\n+ * Copyright (c) 2023, Google and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_ENDIAN_HPP\n+#define SHARE_UTILITIES_ENDIAN_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/byteswap.hpp\"\n+#include \"utilities\/unalignedAccess.hpp\"\n+\n+class Endian final : public AllStatic {\n+public:\n+  enum Order {\n+    LITTLE,\n+    BIG,\n+    JAVA = BIG,\n+    NATIVE =\n+#ifdef VM_LITTLE_ENDIAN\n+    LITTLE\n+#else\n+    BIG\n+#endif\n+  };\n+\n+  \/\/ Returns true, if the byte ordering used by Java is different from\n+  \/\/ the native byte ordering of the underlying machine.\n+  static constexpr bool is_Java_byte_ordering_different() {\n+    return NATIVE != JAVA;\n+  }\n+\n+  template <Order From, Order To>\n+  struct Converter;\n+};\n+\n+template <Endian::Order To>\n+class Endianness final : public AllStatic {\n+ private:\n+  template <typename T, ENABLE_IF(std::is_integral<T>::value)>\n+  static inline T to_native(T x) {\n+    return Endian::Converter<To, Endian::NATIVE>{}(x);\n+  }\n+\n+  template <typename T, ENABLE_IF(std::is_integral<T>::value)>\n+  static inline T from_native(T x) {\n+    return Endian::Converter<Endian::NATIVE, To>{}(x);\n+  }\n+\n+ public:\n+  template <typename T, ENABLE_IF(std::is_integral<T>::value)>\n+  static inline T load(const T* p) {\n+    return to_native(*p);\n+  }\n+\n+  template <typename T, ENABLE_IF(std::is_integral<T>::value)>\n+  static inline void store(T* p, T x) {\n+    *p = from_native(x);\n+  }\n+\n+  template <typename T, ENABLE_IF(std::is_integral<T>::value)>\n+  static inline T load_unaligned(const void* p) {\n+    return to_native(UnalignedAccess::load<T>(p));\n+  }\n+\n+  template <typename T, ENABLE_IF(std::is_integral<T>::value)>\n+  static inline void store_unaligned(void* p, T x) {\n+    UnalignedAccess::store(p, from_native(x));\n+  }\n+};\n+\n+\/\/ Utility for loading and storing 8-bit, 16-bit, 32-bit, and 64-bit integers in big endian. If the\n+\/\/ native endianness is little, then the integers are byteswapped before storing and after loading.\n+\/\/ That is, all integers passed are expected to be in native endianness for storing and are returned\n+\/\/ in native endianness when loading.\n+using BigEndian = Endianness<Endian::BIG>;\n+\n+using JavaEndian = Endianness<Endian::JAVA>;\n+\n+static_assert(std::is_same<BigEndian, JavaEndian>::value, \"BigEndian and JavaEndian are different\");\n+\n+\/\/ Utility for loading and storing 8-bit, 16-bit, 32-bit, and 64-bit integers in little endian. If\n+\/\/ the native endianness is big, then the integers are byteswapped before storing and after loading.\n+\/\/ That is, all integers passed are expected to be in native endianness for storing and are returned\n+\/\/ in native endianness when loading.\n+using LittleEndian = Endianness<Endian::LITTLE>;\n+\n+template <Endian::Order From, Endian::Order To>\n+struct Endian::Converter {\n+  template <typename T>\n+  inline T operator()(T x) const {\n+    return byteswap(x);\n+  }\n+};\n+\n+template <>\n+struct Endian::Converter<Endian::LITTLE, Endian::LITTLE> {\n+  template <typename T>\n+  inline constexpr T operator()(T x) const {\n+    return x;\n+  }\n+};\n+\n+template <>\n+struct Endian::Converter<Endian::BIG, Endian::BIG> {\n+  template <typename T>\n+  inline constexpr T operator()(T x) const {\n+    return x;\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_ENDIAN_HPP\n","filename":"src\/hotspot\/share\/utilities\/endian.hpp","additions":132,"deletions":0,"binary":false,"changes":132,"status":"added"},{"patch":"@@ -0,0 +1,187 @@\n+\/*\n+ * Copyright (c) 2023, Google and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_UNALIGNED_ACCESS_HPP\n+#define SHARE_UTILITIES_UNALIGNED_ACCESS_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#include <cstddef>\n+#include <cstdint>\n+#include <cstring>\n+#include <type_traits>\n+\n+#if defined(ADDRESS_SANITIZER)\n+#include <sanitizer\/common_interface_defs.h>\n+#endif\n+\n+template <typename T, size_t S = sizeof(T), size_t A = alignof(T)>\n+struct UnalignedLoadImpl;\n+template <typename T, size_t S = sizeof(T), size_t A = alignof(T)>\n+struct UnalignedStoreImpl;\n+\n+\/\/ Support for well defined potentially unaligned memory access, regardless of underlying\n+\/\/ architecture support.\n+\/\/\n+\/\/ Unaligned access is undefined behavior according to the standard. Some architectures support\n+\/\/ aligned and unaligned memory access via the same instructions (i.e. x86, AArch64) while some do\n+\/\/ not permit unaligned access at all. Compilers are free to assume that all memory access of a\n+\/\/ type T are done at a suitably aligned address for type T, that is an address aligned to\n+\/\/ alignof(T). This is not always the case, as there are use cases where we may want to access type\n+\/\/ T at a non-suitably aligned address. For example, when serializing scalar types to a buffer\n+\/\/ without padding.\n+\/\/\n+\/\/ IMPORTANT: On some architectures the cost for unaligned accesses is cheap, while on others it is\n+\/\/ expensive. Only use unaligned accesses when necessary.\n+class UnalignedAccess final : public AllStatic {\n+ public:\n+  \/\/ Loads the bits of the value of type T from the specified address. The address may or may not be\n+  \/\/ suitably aligned for type T. T must be trivially copyable and must be default constructible.\n+  template <typename T, ENABLE_IF(std::is_trivially_copyable<T>::value &&\n+                                  std::is_default_constructible<T>::value)>\n+  static inline T load(const void* p) {\n+    return UnalignedLoadImpl<T>{}(p);\n+  }\n+\n+  \/\/ Stores the bits of the value of type T at the specified address. The address may or may not be\n+  \/\/ suitably aligned for type T. T must be trivially copyable and must be default constructible.\n+  template <typename T, ENABLE_IF(std::is_trivially_copyable<T>::value)>\n+  static inline void store(void* p, T x) {\n+    UnalignedStoreImpl<T>{}(p, x);\n+  }\n+};\n+\n+template <typename T, size_t S, size_t A>\n+struct UnalignedLoadImpl {\n+  inline T operator()(const void* p) const {\n+#if defined(TARGET_COMPILER_gcc) || defined(TARGET_COMPILER_xlc)\n+    \/\/ When available, explicitly prefer the builtin memcpy variant. This ensures GCC\/Clang will\n+    \/\/ do its best at generating optimal machine code regardless of build options. For architectures\n+    \/\/ which support unaligned access, this typically results in a single instruction. For other\n+    \/\/ architectures, GCC\/Clang will attempt to determine if the access is aligned first at compile\n+    \/\/ time and generate a single instruction otherwise it will fallback to a more general approach.\n+    T x;\n+    __builtin_memcpy(&x, p, S);\n+    return x;\n+#elif defined(TARGET_COMPILER_visCPP)\n+    return *static_cast<__unaligned const T*>(p);\n+#else\n+    \/\/ Most compilers will generate optimal machine code.\n+    T x;\n+    std::memcpy(&x, p, S);\n+    return x;\n+#endif\n+  }\n+};\n+\n+template <typename T, size_t S, size_t A>\n+struct UnalignedStoreImpl {\n+  inline void operator()(void* p, T x) const {\n+#if defined(TARGET_COMPILER_gcc) || defined(TARGET_COMPILER_xlc)\n+    \/\/ When available, explicitly prefer the builtin memcpy variant. This ensures GCC\/Clang will\n+    \/\/ do its best at generating optimal machine code regardless of build options. For architectures\n+    \/\/ which support unaligned access, this typically results in a single instruction. For other\n+    \/\/ architectures, GCC\/Clang will attempt to determine if the access is aligned first at compile\n+    \/\/ time and generate a single instruction otherwise it will fallback to a more general approach.\n+    __builtin_memcpy(p, &x, S);\n+#elif defined(TARGET_COMPILER_visCPP)\n+    *static_cast<__unaligned T*>(p) = x;\n+#else\n+    \/\/ Most compilers will generate optimal machine code.\n+    std::memcpy(p, &x, S);\n+#endif\n+  }\n+};\n+\n+\/\/ Loads for types with an alignment of 1 byte are always aligned, but for simplicity of\n+\/\/ metaprogramming we accept them in UnalignedAccess.\n+template <typename T, size_t S>\n+struct UnalignedLoadImpl<T, S, 1> {\n+  inline T operator()(const void* p) const {\n+    return *static_cast<const T*>(p);\n+  }\n+};\n+\n+\/\/ Stores for types with an alignment of 1 byte are always aligned, but for simplicity of\n+\/\/ metaprogramming we accept them in UnalignedAccess.\n+template <typename T, size_t S>\n+struct UnalignedStoreImpl<T, S, 1> {\n+  inline void operator()(void* p, T x) const {\n+    *static_cast<T*>(p) = x;\n+  }\n+};\n+\n+#if defined(ADDRESS_SANITIZER)\n+\/\/ Intercept unaligned accesses of size 2, 4, and 8 for ASan which can miss some bugs related to\n+\/\/ unaligned accesses if these are not used.\n+\/\/\n+\/\/ NOTE: these should also be enabled for MSan and TSan as well when\/if we use those.\n+\n+template <typename T, size_t A>\n+struct UnalignedLoadImpl<T, 2, A> {\n+  inline T operator()(const void* p) const {\n+    return PrimitiveConversions::cast<T>(__sanitizer_unaligned_load16(p));\n+  }\n+};\n+\n+template <typename T, size_t A>\n+struct UnalignedStoreImpl<T, 2, A> {\n+  inline void operator()(void* p, T x) const {\n+    __sanitizer_unaligned_store16(p, PrimitiveConversions::cast<uint16_t>(x));\n+  }\n+};\n+\n+template <typename T, size_t A>\n+struct UnalignedLoadImpl<T, 4, A> {\n+  inline T operator()(const void* p) const {\n+    return PrimitiveConversions::cast<T>(__sanitizer_unaligned_load32(p));\n+  }\n+};\n+\n+template <typename T, size_t A>\n+struct UnalignedStoreImpl<T, 4, A> {\n+  inline void operator()(void* p, T x) const {\n+    __sanitizer_unaligned_store32(p, PrimitiveConversions::cast<uint32_t>(x));\n+  }\n+};\n+\n+template <typename T, size_t A>\n+struct UnalignedLoadImpl<T, 8, A> final {\n+  inline T operator()(const void* p) const {\n+    return PrimitiveConversions::cast<T>(__sanitizer_unaligned_load64(p));\n+  }\n+};\n+\n+template <typename T, size_t A>\n+struct UnalignedStoreImpl<T, 8, A> final {\n+  inline void operator()(void* p, T x) const {\n+    __sanitizer_unaligned_store64(p, PrimitiveConversions::cast<uint64_t>(x));\n+  }\n+};\n+#endif\n+\n+#endif \/\/ SHARE_UTILITIES_UNALIGNED_ACCESS_HPP\n","filename":"src\/hotspot\/share\/utilities\/unalignedAccess.hpp","additions":187,"deletions":0,"binary":false,"changes":187,"status":"added"}]}