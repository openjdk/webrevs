{"files":[{"patch":"@@ -62,1 +62,1 @@\n-        thread->tlab().retire(&_tlab_stats);\n+        thread->retire_tlab(&_tlab_stats);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPreEvacuateTasks.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -520,2 +520,2 @@\n-      if (retire_tlabs) {\n-        thread->tlab().retire(&stats);\n+      if (retire_tlabs || ZeroTLAB) {\n+        thread->retire_tlab(&stats);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-  bool                _tlab_end_reset_for_sample;\n@@ -77,2 +76,1 @@\n-      _allocated_tlab_size(0),\n-      _tlab_end_reset_for_sample(false)\n+      _allocated_tlab_size(0)\n@@ -177,5 +175,2 @@\n-  if (!_allocated_outside_tlab && _allocated_tlab_size == 0 && !_tlab_end_reset_for_sample) {\n-    \/\/ Sample if it's a non-TLAB allocation, or a TLAB allocation that either refills the TLAB\n-    \/\/ or expands it due to taking a sampler induced slow path.\n-    return;\n-  }\n+  ThreadHeapSampler& heap_sampler = _thread->heap_sampler();\n+  ThreadLocalAllocBuffer& tlab = _thread->tlab();\n@@ -183,4 +178,2 @@\n-  \/\/ If we want to be sampling, protect the allocated object with a Handle\n-  \/\/ before doing the callback. The callback is done in the destructor of\n-  \/\/ the JvmtiSampledObjectAllocEventCollector.\n-  size_t bytes_since_last = 0;\n+  \/\/ Log sample decision\n+  heap_sampler.log_sample_decision(tlab.top());\n@@ -188,1 +181,4 @@\n-  {\n+  if (heap_sampler.should_sample(tlab.top())) {\n+    \/\/ If we want to be sampling, protect the allocated object with a Handle\n+    \/\/ before doing the callback. The callback is done in the destructor of\n+    \/\/ the JvmtiSampledObjectAllocEventCollector.\n@@ -191,2 +187,0 @@\n-    size_t size_in_bytes = _allocator._word_size * HeapWordSize;\n-    ThreadLocalAllocBuffer& tlab = _thread->tlab();\n@@ -194,3 +188,2 @@\n-    if (!_allocated_outside_tlab) {\n-      bytes_since_last = tlab.bytes_since_last_sample_point();\n-    }\n+    \/\/ Perform the sampling\n+    heap_sampler.sample(obj_h(), tlab.top());\n@@ -198,1 +191,3 @@\n-    _thread->heap_sampler().check_for_sampling(obj_h(), size_in_bytes, bytes_since_last);\n+    \/\/ Note that after this point all the TLAB can have been retired, and agent\n+    \/\/ code can run and allocate, don't rely on earlier calculations involving\n+    \/\/ the TLAB.\n@@ -201,3 +196,4 @@\n-  if (_tlab_end_reset_for_sample || _allocated_tlab_size != 0) {\n-    \/\/ Tell tlab to forget bytes_since_last if we passed it to the heap sampler.\n-    _thread->tlab().set_sample_end(bytes_since_last != 0);\n+  \/\/ Set a new sampling point in the TLAB if it fits in the current TLAB\n+  const size_t words_until_sample = heap_sampler.bytes_until_sample(tlab.top()) \/ HeapWordSize;\n+  if (words_until_sample <= tlab.free()) {\n+    tlab.set_sampling_point(tlab.top() + words_until_sample);\n@@ -252,0 +248,1 @@\n+  _thread->heap_sampler().inc_outside_tlab_bytes(size_in_bytes);\n@@ -265,0 +262,6 @@\n+    \/\/ When sampling we artificially set the TLAB end to the sample point.\n+    \/\/ When we hit that point it looks like the TLAB is full, but it's\n+    \/\/ not necessarily the case. Set the real end and retry the allocation.\n+\n+    \/\/ Undo previous adjustment of end.\n+    \/\/ Note that notify_allocation_jvmti_sampler will set a new sample point.\n@@ -266,1 +269,0 @@\n-    mem = tlab.allocate(_word_size);\n@@ -268,3 +270,2 @@\n-    \/\/ We set back the allocation sample point to try to allocate this, reset it\n-    \/\/ when done.\n-    allocation._tlab_end_reset_for_sample = true;\n+    \/\/ Retry the TLAB allocation with the proper end\n+    mem = tlab.allocate(_word_size);\n@@ -285,0 +286,7 @@\n+\n+  \/\/ Record the amount wasted\n+  tlab.record_refill_waste();\n+\n+  \/\/ Retire the current TLAB\n+  _thread->retire_tlab();\n+\n@@ -288,2 +296,0 @@\n-  tlab.retire_before_allocation();\n-\n@@ -320,1 +326,2 @@\n-  tlab.fill(mem, mem + _word_size, allocation._allocated_tlab_size);\n+  _thread->fill_tlab(mem, _word_size, allocation._allocated_tlab_size);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":36,"deletions":29,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-  _bytes_since_last_sample_point(0),\n@@ -128,5 +127,1 @@\n-    if (ZeroTLAB) {\n-      retire();\n-    } else {\n-      insert_filler();\n-    }\n+    insert_filler();\n@@ -143,1 +138,0 @@\n-    thread()->incr_allocated_bytes(used_bytes());\n@@ -149,1 +143,1 @@\n-void ThreadLocalAllocBuffer::retire_before_allocation() {\n+void ThreadLocalAllocBuffer::record_refill_waste() {\n@@ -151,1 +145,0 @@\n-  retire();\n@@ -315,18 +308,0 @@\n-void ThreadLocalAllocBuffer::set_sample_end(bool reset_byte_accumulation) {\n-  size_t heap_words_remaining = pointer_delta(_end, _top);\n-  size_t bytes_until_sample = thread()->heap_sampler().bytes_until_sample();\n-  size_t words_until_sample = bytes_until_sample \/ HeapWordSize;\n-\n-  if (reset_byte_accumulation) {\n-    _bytes_since_last_sample_point = 0;\n-  }\n-\n-  if (heap_words_remaining > words_until_sample) {\n-    HeapWord* new_end = _top + words_until_sample;\n-    set_end(new_end);\n-    _bytes_since_last_sample_point += bytes_until_sample;\n-  } else {\n-    _bytes_since_last_sample_point += heap_words_remaining * HeapWordSize;\n-  }\n-}\n-\n@@ -341,0 +316,8 @@\n+void ThreadLocalAllocBuffer::set_sampling_point(HeapWord* sampling_point) {\n+  precond(sampling_point >= _top);\n+  precond(sampling_point <= _allocation_end);\n+\n+  \/\/ This will trigger a slow-path, which in turn might take a sample.\n+  _end = sampling_point;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/threadLocalAllocBuffer.cpp","additions":10,"deletions":27,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-  size_t    _bytes_since_last_sample_point;      \/\/ bytes since last sample point.\n@@ -127,1 +126,0 @@\n-  size_t bytes_since_last_sample_point() const   { return _bytes_since_last_sample_point; }\n@@ -161,2 +159,2 @@\n-  \/\/ Retire in-use tlab before allocation of a new tlab\n-  void retire_before_allocation();\n+  \/\/ Record refill waste before allocating (refilling) with a new TLAB.\n+  void record_refill_waste();\n@@ -170,0 +168,1 @@\n+  \/\/ Support for TLAB sampling\n@@ -171,1 +170,1 @@\n-  void set_sample_end(bool reset_byte_accumulation);\n+  void set_sampling_point(HeapWord* sampling_point);\n","filename":"src\/hotspot\/share\/gc\/shared\/threadLocalAllocBuffer.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1502,0 +1502,3 @@\n+    if (ZeroTLAB) {\n+      t->retire_tlab();\n+    }\n@@ -1519,2 +1522,1 @@\n-    ThreadLocalAllocBuffer& tlab = t->tlab();\n-    tlab.retire(&stats);\n+    t->retire_tlab(&stats);\n@@ -1522,1 +1524,1 @@\n-      tlab.resize();\n+      t->tlab().resize();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-    _jt->tlab().retire(&_stats);\n+    _jt->retire_tlab(&_stats);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahStackWatermark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -68,1 +68,1 @@\n-    thread->tlab().retire(stats);\n+    thread->retire_tlab(stats);\n","filename":"src\/hotspot\/share\/gc\/z\/zThreadLocalAllocBuffer.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -971,1 +971,1 @@\n-    tlab().retire();\n+    retire_tlab();\n@@ -1045,1 +1045,1 @@\n-    tlab().retire();\n+    retire_tlab();\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -157,0 +157,19 @@\n+void Thread::retire_tlab(ThreadLocalAllocStats* stats) {\n+  \/\/ Sampling and serviceability support\n+  if (tlab().end() != nullptr) {\n+    incr_allocated_bytes(tlab().used_bytes());\n+    heap_sampler().retire_tlab(tlab().top());\n+  }\n+\n+  \/\/ Retire the TLAB\n+  tlab().retire(stats);\n+}\n+\n+void Thread::fill_tlab(HeapWord* start, size_t pre_reserved, size_t new_size) {\n+  \/\/ Thread allocation sampling support\n+  heap_sampler().set_tlab_top_at_sample_start(start);\n+\n+  \/\/ Fill the TLAB\n+  tlab().fill(start, start + pre_reserved, new_size);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -405,0 +405,2 @@\n+  void retire_tlab(ThreadLocalAllocStats* stats = nullptr);\n+  void fill_tlab(HeapWord* start, size_t pre_reserved, size_t new_size);\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -399,1 +399,1 @@\n-  _bytes_until_sample = interval;\n+  _sample_threshold = interval;\n@@ -402,1 +402,1 @@\n-void ThreadHeapSampler::pick_next_sample(size_t overflowed_bytes) {\n+void ThreadHeapSampler::pick_next_sample() {\n@@ -411,1 +411,1 @@\n-    _bytes_until_sample = 0;\n+    _sample_threshold = 0;\n@@ -418,7 +418,12 @@\n-void ThreadHeapSampler::check_for_sampling(oop obj, size_t allocation_size, size_t bytes_since_allocation) {\n-  size_t total_allocated_bytes = bytes_since_allocation + allocation_size;\n-\n-  \/\/ If not yet time for a sample, skip it.\n-  if (total_allocated_bytes < _bytes_until_sample) {\n-    _bytes_until_sample -= total_allocated_bytes;\n-    return;\n+#ifndef PRODUCT\n+void ThreadHeapSampler::log_sample_decision(HeapWord* tlab_top) {\n+  LogTarget(Debug, gc, tlab) log;\n+  if (log.is_enabled()) {\n+    const bool should_sample = bytes_since_last_sample(tlab_top) >= _sample_threshold;\n+    log_debug(gc, tlab)(\"Should sample: %s _sample_threshold: %zu total: %zu tlab: %zu current tlab: %zu outside_tlab: %zu\",\n+                        should_sample ? \"yes\" : \"no \",\n+                        _sample_threshold,\n+                        bytes_since_last_sample(tlab_top),\n+                        tlab_bytes_since_last_sample(tlab_top),\n+                        current_tlab_bytes_since_last_sample(tlab_top),\n+                        outside_tlab_bytes_since_last_sample());\n@@ -426,0 +431,2 @@\n+}\n+#endif\n@@ -427,0 +434,1 @@\n+void ThreadHeapSampler::sample(oop obj, HeapWord* tlab_top) {\n@@ -429,2 +437,3 @@\n-  size_t overflow_bytes = total_allocated_bytes - _bytes_until_sample;\n-  pick_next_sample(overflow_bytes);\n+  pick_next_sample();\n+\n+  reset_after_sample(tlab_top);\n","filename":"src\/hotspot\/share\/runtime\/threadHeapSampler.cpp","additions":21,"deletions":12,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -33,1 +34,14 @@\n-  size_t _bytes_until_sample;\n+  \/\/ Amount of bytes to allocate before taking the next sample\n+  size_t _sample_threshold;\n+\n+  \/\/ The TLAB top address when the last sampling happened, or\n+  \/\/ TLAB start if a new TLAB is allocated\n+  HeapWord* _tlab_top_at_sample_start;\n+\n+  \/\/ The accumulated amount of allocated bytes in a TLAB since the last sampling\n+  \/\/ excluding the amount between _tlab_sample_start and top\n+  size_t _accumulated_tlab_bytes_since_sample;\n+\n+  \/\/ The accumulated amount of allocated bytes outside TLABs since last sample point\n+  size_t _accumulated_outside_tlab_bytes_since_sample;\n+\n@@ -40,1 +54,1 @@\n-  void pick_next_sample(size_t overflowed_bytes = 0);\n+  void pick_next_sample();\n@@ -45,0 +59,19 @@\n+  size_t current_tlab_bytes_since_last_sample(HeapWord* tlab_top)  const {\n+    \/\/ Both can be nullptr if there's not active TLAB, but otherwise\n+    \/\/ they both should be non-null.\n+    assert((tlab_top != nullptr) == (_tlab_top_at_sample_start != nullptr),\n+           \"Both should either be uninitialized or initialized \"\n+           \"tlab_top: \" PTR_FORMAT \" _tlab_top_at_sample_start: \" PTR_FORMAT,\n+           p2i(tlab_top), p2i(_tlab_top_at_sample_start));\n+\n+    return pointer_delta(tlab_top, _tlab_top_at_sample_start, 1);\n+  }\n+\n+  size_t tlab_bytes_since_last_sample(HeapWord* tlab_top) const {\n+    return _accumulated_tlab_bytes_since_sample + current_tlab_bytes_since_last_sample(tlab_top);\n+  }\n+\n+  size_t outside_tlab_bytes_since_last_sample() const {\n+    return _accumulated_outside_tlab_bytes_since_sample;\n+  }\n+\n@@ -46,1 +79,5 @@\n-  ThreadHeapSampler() {\n+  ThreadHeapSampler() :\n+      _sample_threshold(0),\n+      _tlab_top_at_sample_start(nullptr),\n+      _accumulated_tlab_bytes_since_sample(0),\n+      _accumulated_outside_tlab_bytes_since_sample(0) {\n@@ -52,1 +89,1 @@\n-    \/\/ Call this after _rnd is initialized to initialize _bytes_until_sample.\n+    \/\/ Call this after _rnd is initialized to initialize _sample_threshold.\n@@ -56,1 +93,34 @@\n-  size_t bytes_until_sample()                    { return _bytes_until_sample;   }\n+  size_t bytes_since_last_sample(HeapWord* tlab_top) const {\n+    return tlab_bytes_since_last_sample(tlab_top) +\n+           outside_tlab_bytes_since_last_sample();\n+  }\n+\n+  size_t bytes_until_sample(HeapWord* tlab_top) const {\n+    const size_t since_last_sample = bytes_since_last_sample(tlab_top);\n+    return _sample_threshold - MIN2(since_last_sample, _sample_threshold);\n+  }\n+\n+  bool should_sample(HeapWord* tlab_top) const {\n+    return bytes_until_sample(tlab_top) == 0;\n+  }\n+\n+  void set_tlab_top_at_sample_start(HeapWord* tlab_top) {\n+    _tlab_top_at_sample_start = tlab_top;\n+  }\n+\n+  void reset_after_sample(HeapWord* tlab_top) {\n+    _tlab_top_at_sample_start = tlab_top;\n+    _accumulated_tlab_bytes_since_sample = 0;\n+    _accumulated_outside_tlab_bytes_since_sample = 0;\n+  }\n+\n+  void retire_tlab(HeapWord* tlab_top) {\n+    _accumulated_tlab_bytes_since_sample += current_tlab_bytes_since_last_sample(tlab_top);\n+    _tlab_top_at_sample_start = nullptr;\n+  }\n+\n+  void inc_outside_tlab_bytes(size_t size) {\n+    _accumulated_outside_tlab_bytes_since_sample += size;\n+  }\n+\n+  void log_sample_decision(HeapWord* tlab_top) PRODUCT_RETURN;\n@@ -58,1 +128,1 @@\n-  void check_for_sampling(oop obj, size_t size_in_bytes, size_t bytes_allocated_before);\n+  void sample(oop obj, HeapWord* tlab_top);\n","filename":"src\/hotspot\/share\/runtime\/threadHeapSampler.hpp","additions":77,"deletions":7,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -119,3 +119,0 @@\n-serviceability\/jvmti\/HeapMonitor\/MyPackage\/HeapMonitorInterpreterObjectTest.java                8356372 generic-all\n-serviceability\/jvmti\/HeapMonitor\/MyPackage\/HeapMonitorStatObjectCorrectnessTest.java\t          8356372 generic-all\n-\n","filename":"test\/hotspot\/jtreg\/ProblemList-zgc.txt","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -280,0 +280,3 @@\n+    } else {\n+      System.out.println(\"OK difference percentage: \" + diffPercentage\n+          + \" due to the count being \" + actual + \" instead of \" + expected);\n","filename":"test\/hotspot\/jtreg\/serviceability\/jvmti\/HeapMonitor\/MyPackage\/HeapMonitor.java","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}