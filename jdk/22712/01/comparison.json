{"files":[{"patch":"@@ -50,0 +50,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -196,1 +197,1 @@\n-    _shared_rs.release();\n+    MemoryReserver::release(_shared_rs);\n@@ -350,1 +351,3 @@\n-  ReservedSpace rs(buffer_size, MetaspaceShared::core_region_alignment(), os::vm_page_size());\n+  ReservedSpace rs = MemoryReserver::reserve(buffer_size,\n+                                             MetaspaceShared::core_region_alignment(),\n+                                             os::vm_page_size());\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"memory\/reservedSpace.hpp\"\n+#include \"memory\/virtualspace.hpp\"\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"memory\/virtualspace.hpp\"\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+class ReservedSpace;\n@@ -482,1 +483,0 @@\n-  ReservedSpace reserve_shared_memory();\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -285,1 +286,1 @@\n-  _symbol_rs = ReservedSpace(symbol_rs_size, mtClassShared);\n+  _symbol_rs = MemoryReserver::reserve(symbol_rs_size, mtClassShared);\n@@ -1269,1 +1270,3 @@\n-        archive_space_rs.release();\n+        MemoryReserver::release(archive_space_rs);\n+        \/\/ Mark as not reserved\n+        archive_space_rs = {};\n@@ -1441,2 +1444,4 @@\n-    archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,\n-                                     os::vm_page_size(), (char*)base_address);\n+    archive_space_rs = MemoryReserver::reserve((char*)base_address,\n+                                               archive_space_size,\n+                                               archive_space_alignment,\n+                                               os::vm_page_size());\n@@ -1508,4 +1513,8 @@\n-      archive_space_rs = ReservedSpace(archive_space_size, archive_space_alignment,\n-                                       os::vm_page_size(), (char*)base_address);\n-      class_space_rs   = ReservedSpace(class_space_size, class_space_alignment,\n-                                       os::vm_page_size(), (char*)ccs_base);\n+      archive_space_rs = MemoryReserver::reserve((char*)base_address,\n+                                                 archive_space_size,\n+                                                 archive_space_alignment,\n+                                                 os::vm_page_size());\n+      class_space_rs   = MemoryReserver::reserve((char*)ccs_base,\n+                                                 class_space_size,\n+                                                 class_space_alignment,\n+                                                 os::vm_page_size());\n@@ -1522,2 +1531,4 @@\n-      total_space_rs = ReservedSpace(total_range_size, base_address_alignment,\n-                                     os::vm_page_size(), (char*) base_address);\n+      total_space_rs = MemoryReserver::reserve((char*) base_address,\n+                                               total_range_size,\n+                                               base_address_alignment,\n+                                               os::vm_page_size());\n@@ -1571,1 +1582,2 @@\n-    total_space_rs.release();\n+    MemoryReserver::release(total_space_rs);\n+    total_space_rs = {};\n@@ -1575,1 +1587,2 @@\n-      archive_space_rs.release();\n+      MemoryReserver::release(archive_space_rs);\n+      archive_space_rs = {};\n@@ -1579,1 +1592,2 @@\n-      class_space_rs.release();\n+      MemoryReserver::release(class_space_rs);\n+      class_space_rs = {};\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":27,"deletions":13,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -321,1 +322,1 @@\n-  ReservedCodeSpace rs = reserve_heap_memory(cache_size, ps);\n+  ReservedSpace rs = reserve_heap_memory(cache_size, ps);\n@@ -351,1 +352,1 @@\n-ReservedCodeSpace CodeCache::reserve_heap_memory(size_t size, size_t rs_ps) {\n+ReservedSpace CodeCache::reserve_heap_memory(size_t size, size_t rs_ps) {\n@@ -355,1 +356,2 @@\n-  ReservedCodeSpace rs(rs_size, rs_align, rs_ps);\n+\n+  ReservedSpace rs = CodeMemoryReserver::reserve(rs_size, rs_align, rs_ps);\n@@ -1133,1 +1135,1 @@\n-    ReservedCodeSpace rs = reserve_heap_memory(ReservedCodeCacheSize, page_size(false, min_pages));\n+    ReservedSpace rs = reserve_heap_memory(ReservedCodeCacheSize, page_size(false, min_pages));\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -82,0 +82,1 @@\n+class ReservedSpace;\n@@ -125,1 +126,1 @@\n-  static ReservedCodeSpace reserve_heap_memory(size_t size, size_t rs_ps); \/\/ Reserves one continuous chunk of memory for the CodeHeaps\n+  static ReservedSpace reserve_heap_memory(size_t size, size_t rs_ps); \/\/ Reserves one continuous chunk of memory for the CodeHeaps\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -101,0 +101,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -1215,0 +1216,10 @@\n+\n+  \/\/ When a page size is given we don't want to mix large\n+  \/\/ and normal pages. If the size is not a multiple of the\n+  \/\/ page size it will be aligned up to achieve this.\n+  size_t alignment = os::vm_allocation_granularity();\n+  if (preferred_page_size != os::vm_page_size()) {\n+    alignment = MAX2(preferred_page_size, alignment);\n+    size = align_up(size, alignment);\n+  }\n+\n@@ -1216,1 +1227,4 @@\n-  ReservedSpace rs(size, preferred_page_size);\n+  ReservedSpace rs = MemoryReserver::reserve(size,\n+                                             alignment,\n+                                             preferred_page_size);\n+\n@@ -1291,1 +1305,1 @@\n-  G1CardTable* ct = new G1CardTable(heap_rs.region());\n+  G1CardTable* ct = new G1CardTable(_reserved);\n@@ -1443,1 +1457,1 @@\n-  FullGCForwarding::initialize(heap_rs.region());\n+  FullGCForwarding::initialize(_reserved);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":17,"deletions":3,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"memory\/virtualspace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMarkBitMap.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"memory\/virtualspace.hpp\"\n@@ -33,0 +32,1 @@\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1PageBasedVirtualSpace.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RegionToSpaceMapper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RegionToSpaceMapper.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"nmt\/memTracker.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n@@ -50,1 +50,1 @@\n-  ReservedSpace backing_store(bytes_to_reserve, mtGC);\n+  ReservedSpace backing_store = MemoryReserver::reserve(bytes_to_reserve, mtGC);\n","filename":"src\/hotspot\/share\/gc\/parallel\/objectStartArray.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -45,1 +46,7 @@\n-  _reserved_byte_size = align_up(raw_bytes, MAX2(page_sz, granularity));\n+  const size_t rs_align = MAX2(page_sz, granularity);\n+\n+  _reserved_byte_size = align_up(raw_bytes, rs_align);\n+\n+  ReservedSpace rs = MemoryReserver::reserve(_reserved_byte_size,\n+                                             rs_align,\n+                                             page_sz);\n@@ -47,3 +54,0 @@\n-  const size_t rs_align = page_sz == os::vm_page_size() ? 0 :\n-    MAX2(page_sz, granularity);\n-  ReservedSpace rs(_reserved_byte_size, rs_align, page_sz);\n@@ -71,1 +75,3 @@\n-    rs.release();\n+    if (rs.is_reserved()) {\n+      MemoryReserver::release(rs);\n+    }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n@@ -49,1 +50,0 @@\n-#include \"nmt\/memTracker.hpp\"\n@@ -77,1 +77,1 @@\n-  PSCardTable* card_table = new PSCardTable(heap_rs.region());\n+  PSCardTable* card_table = new PSCardTable(_reserved);\n@@ -133,1 +133,1 @@\n-  FullGCForwarding::initialize(heap_rs.region());\n+  FullGCForwarding::initialize(_reserved);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+class ReservedSpace;\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -243,1 +244,7 @@\n-  _reserved_byte_size = align_up(raw_bytes, MAX2(page_sz, granularity));\n+  const size_t rs_align = MAX2(page_sz, granularity);\n+\n+  _reserved_byte_size = align_up(raw_bytes, rs_align);\n+\n+  ReservedSpace rs = MemoryReserver::reserve(_reserved_byte_size,\n+                                             rs_align,\n+                                             page_sz);\n@@ -245,3 +252,0 @@\n-  const size_t rs_align = page_sz == os::vm_page_size() ? 0 :\n-    MAX2(page_sz, granularity);\n-  ReservedSpace rs(_reserved_byte_size, rs_align, page_sz);\n@@ -260,1 +264,4 @@\n-    rs.release();\n+    if (rs.is_reserved()) {\n+      MemoryReserver::release(rs);\n+      rs = {};\n+    }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psVirtualspace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psVirtualspace.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+class ReservedSpace;\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psYoungGen.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/gc\/serial\/generation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"nmt\/memTracker.hpp\"\n@@ -50,1 +50,1 @@\n-  ReservedSpace rs(size, mtGC);\n+  ReservedSpace rs = MemoryReserver::reserve(size, mtGC);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialBlockOffsetTable.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n@@ -192,1 +193,1 @@\n-  _rem_set = new CardTableRS(heap_rs.region());\n+  _rem_set = new CardTableRS(_reserved);\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n@@ -83,3 +83,2 @@\n-  const size_t rs_align = _page_size == os::vm_page_size() ? 0 :\n-    MAX2(_page_size, os::vm_allocation_granularity());\n-  ReservedSpace heap_rs(_byte_map_size, rs_align, _page_size);\n+  const size_t rs_align = MAX2(_page_size, os::vm_allocation_granularity());\n+  ReservedSpace rs = MemoryReserver::reserve(_byte_map_size, rs_align, _page_size);\n@@ -87,1 +86,1 @@\n-  MemTracker::record_virtual_memory_tag((address)heap_rs.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag((address)rs.base(), mtGC);\n@@ -90,2 +89,2 @@\n-                       heap_rs.base(), heap_rs.size(), _page_size);\n-  if (!heap_rs.is_reserved()) {\n+                       rs.base(), rs.size(), _page_size);\n+  if (!rs.is_reserved()) {\n@@ -100,1 +99,1 @@\n-  _byte_map = (CardValue*) heap_rs.base();\n+  _byte_map = (CardValue*) rs.base();\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"memory\/virtualspace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/virtualspace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/generationCounters.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -31,0 +31,2 @@\n+class VirtualSpace;\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/generationCounters.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,2 @@\n-#include \"runtime\/init.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -44,1 +46,1 @@\n-  const size_t rs_align = _page_size == os::vm_page_size() ? 0 : MAX2(_page_size, granularity);\n+  const size_t rs_align = MAX2(_page_size, granularity);\n@@ -46,1 +48,1 @@\n-  ReservedSpace write_space(_byte_map_size, rs_align, _page_size);\n+  ReservedSpace write_space = MemoryReserver::reserve(_byte_map_size, rs_align, _page_size);\n@@ -61,1 +63,1 @@\n-  ReservedSpace read_space(_byte_map_size, rs_align, _page_size);\n+  ReservedSpace read_space = MemoryReserver::reserve(_byte_map_size, rs_align, _page_size);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-#include \"runtime\/atomic.hpp\"\n@@ -37,0 +36,1 @@\n+#include \"runtime\/atomic.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,2 +29,0 @@\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/virtualspace.hpp\"\n@@ -34,0 +32,3 @@\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n+#include \"memory\/virtualspace.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/codeCache.hpp\"\n@@ -89,1 +88,1 @@\n-\n+#include \"memory\/allocation.hpp\"\n@@ -92,0 +91,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -159,0 +159,13 @@\n+static ReservedSpace reserve(size_t size, size_t preferred_page_size) {\n+  \/\/ When a page size is given we don't want to mix large\n+  \/\/ and normal pages. If the size is not a multiple of the\n+  \/\/ page size it will be aligned up to achieve this.\n+  size_t alignment = os::vm_allocation_granularity();\n+  if (preferred_page_size != os::vm_page_size()) {\n+    alignment = MAX2(preferred_page_size, alignment);\n+    size = align_up(size, alignment);\n+  }\n+\n+  return MemoryReserver::reserve(size, alignment, preferred_page_size);\n+}\n+\n@@ -284,1 +297,1 @@\n-  ReservedSpace bitmap(_bitmap_size, bitmap_page_size);\n+  ReservedSpace bitmap = reserve(_bitmap_size, bitmap_page_size);\n@@ -304,1 +317,1 @@\n-    ReservedSpace verify_bitmap(_bitmap_size, bitmap_page_size);\n+    ReservedSpace verify_bitmap = reserve(_bitmap_size, bitmap_page_size);\n@@ -322,1 +335,1 @@\n-  ReservedSpace aux_bitmap(_bitmap_size, aux_bitmap_page_size);\n+  ReservedSpace aux_bitmap = reserve(_bitmap_size, aux_bitmap_page_size);\n@@ -340,1 +353,1 @@\n-  ReservedSpace region_storage(region_storage_size, region_page_size);\n+  ReservedSpace region_storage = reserve(region_storage_size, region_page_size);\n@@ -366,1 +379,1 @@\n-      cset_rs = ReservedSpace(cset_size, cset_align, cset_page_size, req_addr);\n+      cset_rs = MemoryReserver::reserve(req_addr, cset_size, cset_align, cset_page_size);\n@@ -375,1 +388,1 @@\n-      cset_rs = ReservedSpace(cset_size, cset_align, os::vm_page_size());\n+      cset_rs = MemoryReserver::reserve(cset_size, cset_align, os::vm_page_size());\n@@ -2737,1 +2750,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":21,"deletions":9,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -100,1 +101,3 @@\n-  _rs.release();\n+  if (_rs.is_reserved()) {\n+    MemoryReserver::release(_rs);\n+  }\n@@ -105,3 +108,3 @@\n-  _rs = ReservedSpace(reservation_size_request_bytes,\n-                      os::vm_allocation_granularity(),\n-                      os::vm_page_size());\n+  _rs = MemoryReserver::reserve(reservation_size_request_bytes,\n+                                os::vm_allocation_granularity(),\n+                                os::vm_page_size());\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrVirtualMemory.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"nmt\/memTracker.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n@@ -226,1 +226,1 @@\n-  ReservedSpace seg_rs(reserved_segments_size, mtCode);\n+  ReservedSpace seg_rs = MemoryReserver::reserve(reserved_segments_size, mtCode);\n","filename":"src\/hotspot\/share\/memory\/heap.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+class ReservedSpace;\n+\n","filename":"src\/hotspot\/share\/memory\/heap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,693 @@\n+\/*\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"jvm.h\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+static void sanity_check_size_and_alignment(size_t size, size_t alignment) {\n+  assert(size > 0, \"Precondition\");\n+\n+  DEBUG_ONLY(const size_t granularity = os::vm_allocation_granularity());\n+  assert(is_aligned(size, granularity), \"size not aligned to os::vm_allocation_granularity()\");\n+\n+  assert(alignment >= granularity, \"Must be set\");\n+  assert(is_power_of_2(alignment), \"not a power of 2\");\n+  assert(is_aligned(alignment, granularity), \"alignment not aligned to os::vm_allocation_granularity()\");\n+}\n+\n+static void sanity_check_page_size(size_t page_size) {\n+  assert(page_size >= os::vm_page_size(), \"Invalid page size\");\n+  assert(is_power_of_2(page_size), \"Invalid page size\");\n+}\n+\n+static void sanity_check_arguments(size_t size, size_t alignment, size_t page_size) {\n+  sanity_check_size_and_alignment(size, alignment);\n+  sanity_check_page_size(page_size);\n+}\n+\n+static bool large_pages_requested() {\n+  return UseLargePages &&\n+         (!FLAG_IS_DEFAULT(UseLargePages) || !FLAG_IS_DEFAULT(LargePageSizeInBytes));\n+}\n+\n+static void log_on_large_pages_failure(char* req_addr, size_t bytes) {\n+  if (large_pages_requested()) {\n+    \/\/ Compressed oops logging.\n+    log_debug(gc, heap, coops)(\"Reserve regular memory without large pages\");\n+    \/\/ JVM style warning that we did not succeed in using large pages.\n+    char msg[128];\n+    jio_snprintf(msg, sizeof(msg), \"Failed to reserve and commit memory using large pages. \"\n+                                   \"req_addr: \" PTR_FORMAT \" bytes: \" SIZE_FORMAT,\n+                                   req_addr, bytes);\n+    warning(\"%s\", msg);\n+  }\n+}\n+\n+static bool use_explicit_large_pages(size_t page_size) {\n+  return !os::can_commit_large_page_memory() &&\n+         page_size != os::vm_page_size();\n+}\n+\n+static char* reserve_memory_inner(char* requested_address,\n+                                  size_t size,\n+                                  size_t alignment,\n+                                  bool exec,\n+                                  MemTag mem_tag) {\n+  \/\/ If the memory was requested at a particular address, use\n+  \/\/ os::attempt_reserve_memory_at() to avoid mapping over something\n+  \/\/ important.  If the reservation fails, return null.\n+  if (requested_address != nullptr) {\n+    assert(is_aligned(requested_address, alignment),\n+           \"Requested address \" PTR_FORMAT \" must be aligned to \" SIZE_FORMAT,\n+           p2i(requested_address), alignment);\n+    return os::attempt_reserve_memory_at(requested_address, size, exec, mem_tag);\n+  }\n+\n+  \/\/ Optimistically assume that the OS returns an aligned base pointer.\n+  \/\/ When reserving a large address range, most OSes seem to align to at\n+  \/\/ least 64K.\n+  char* base = os::reserve_memory(size, exec, mem_tag);\n+  if (is_aligned(base, alignment)) {\n+    return base;\n+  }\n+\n+  \/\/ Base not aligned, retry.\n+  if (!os::release_memory(base, size)) {\n+    fatal(\"os::release_memory failed\");\n+  }\n+\n+  \/\/ Map using the requested alignment.\n+  return os::reserve_memory_aligned(size, alignment, exec);\n+}\n+\n+ReservedSpace MemoryReserver::reserve_memory(char* requested_address,\n+                                             size_t size,\n+                                             size_t alignment,\n+                                             bool exec,\n+                                             MemTag mem_tag) {\n+  char* base = reserve_memory_inner(requested_address, size, alignment, exec, mem_tag);\n+\n+  if (base != nullptr) {\n+    return ReservedSpace(base, size, alignment, os::vm_page_size(), exec, false \/* special *\/);\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+ReservedSpace MemoryReserver::reserve_memory_special(char* requested_address,\n+                                                     size_t size,\n+                                                     size_t alignment,\n+                                                     size_t page_size,\n+                                                     bool exec) {\n+  log_trace(pagesize)(\"Attempt special mapping: size: \" SIZE_FORMAT \"%s, \"\n+                      \"alignment: \" SIZE_FORMAT \"%s\",\n+                      byte_size_in_exact_unit(size), exact_unit_for_byte_size(size),\n+                      byte_size_in_exact_unit(alignment), exact_unit_for_byte_size(alignment));\n+\n+  char* base = os::reserve_memory_special(size, alignment, page_size, requested_address, exec);\n+\n+  if (base != nullptr) {\n+    assert(is_aligned(base, alignment),\n+           \"reserve_memory_special() returned an unaligned address, \"\n+           \"base: \" PTR_FORMAT \" alignment: \" SIZE_FORMAT_X,\n+           p2i(base), alignment);\n+\n+    return ReservedSpace(base, size, alignment, page_size, exec, true \/* special *\/);\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+ReservedSpace MemoryReserver::reserve(char* requested_address,\n+                                      size_t size,\n+                                      size_t alignment,\n+                                      size_t page_size,\n+                                      bool executable,\n+                                      MemTag mem_tag) {\n+  sanity_check_arguments(size, alignment, page_size);\n+\n+  \/\/ Reserve the memory.\n+\n+  \/\/ There are basically three different cases that we need to handle:\n+  \/\/ 1. Mapping backed by a file\n+  \/\/ 2. Mapping backed by explicit large pages\n+  \/\/ 3. Mapping backed by normal pages or transparent huge pages\n+  \/\/ The first two have restrictions that requires the whole mapping to be\n+  \/\/ committed up front. To record this the ReservedSpace is marked 'special'.\n+\n+  \/\/ == Case 1 ==\n+  \/\/ This case is contained within the HeapReserver\n+\n+  \/\/ == Case 2 ==\n+  if (use_explicit_large_pages(page_size)) {\n+    \/\/ System can't commit large pages i.e. use transparent huge pages and\n+    \/\/ the caller requested large pages. To satisfy this request we use\n+    \/\/ explicit large pages and these have to be committed up front to ensure\n+    \/\/ no reservations are lost.\n+    do {\n+      ReservedSpace reserved = reserve_memory_special(requested_address, size, alignment, page_size, executable);\n+      if (reserved.is_reserved()) {\n+        \/\/ Successful reservation using large pages.\n+        return reserved;\n+      }\n+      page_size = os::page_sizes().next_smaller(page_size);\n+    } while (page_size > os::vm_page_size());\n+\n+    \/\/ Failed to reserve explicit large pages, do proper logging.\n+    log_on_large_pages_failure(requested_address, size);\n+    \/\/ Now fall back to normal reservation.\n+    assert(page_size == os::vm_page_size(), \"inv\");\n+  }\n+\n+  \/\/ == Case 3 ==\n+  return reserve_memory(requested_address, size, alignment, executable, mem_tag);\n+}\n+\n+ReservedSpace MemoryReserver::reserve(char* requested_address,\n+                                      size_t size,\n+                                      size_t alignment,\n+                                      size_t page_size,\n+                                      MemTag mem_tag) {\n+  return reserve(requested_address,\n+                 size,\n+                 alignment,\n+                 page_size,\n+                 !ExecMem,\n+                 mem_tag);\n+}\n+\n+\n+ReservedSpace MemoryReserver::reserve(size_t size,\n+                                      size_t alignment,\n+                                      size_t page_size,\n+                                      MemTag mem_tag) {\n+  return reserve(nullptr \/* requested_address *\/,\n+                 size,\n+                 alignment,\n+                 page_size,\n+                 mem_tag);\n+}\n+\n+ReservedSpace MemoryReserver::reserve(size_t size,\n+                                      MemTag mem_tag) {\n+  \/\/ Want to use large pages where possible. If the size is\n+  \/\/ not large page aligned the mapping will be a mix of\n+  \/\/ large and normal pages.\n+  size_t page_size = os::page_size_for_region_unaligned(size, 1);\n+  size_t alignment = os::vm_allocation_granularity();\n+\n+  return reserve(size,\n+                 alignment,\n+                 page_size,\n+                 mem_tag);\n+}\n+\n+bool MemoryReserver::release(const ReservedSpace& reserved) {\n+  assert(reserved.is_reserved(), \"Precondition\");\n+\n+  if (reserved.special()) {\n+    return os::release_memory_special(reserved.base(), reserved.size());\n+  } else {\n+    return os::release_memory(reserved.base(), reserved.size());\n+  }\n+}\n+\n+static char* map_memory_to_file(char* requested_address,\n+                                size_t size,\n+                                size_t alignment,\n+                                int fd,\n+                                MemTag mem_tag) {\n+  \/\/ If the memory was requested at a particular address, use\n+  \/\/ os::attempt_reserve_memory_at() to avoid mapping over something\n+  \/\/ important.  If the reservation fails, return null.\n+  if (requested_address != nullptr) {\n+    assert(is_aligned(requested_address, alignment),\n+           \"Requested address \" PTR_FORMAT \" must be aligned to \" SIZE_FORMAT,\n+           p2i(requested_address), alignment);\n+    return os::attempt_map_memory_to_file_at(requested_address, size, fd, mem_tag);\n+  }\n+\n+  \/\/ Optimistically assume that the OS returns an aligned base pointer.\n+  \/\/ When reserving a large address range, most OSes seem to align to at\n+  \/\/ least 64K.\n+  char* base = os::map_memory_to_file(size, fd);\n+  if (is_aligned(base, alignment)) {\n+    return base;\n+  }\n+\n+\n+  \/\/ Base not aligned, retry.\n+  if (!os::unmap_memory(base, size)) {\n+    fatal(\"os::unmap_memory failed\");\n+  }\n+\n+  \/\/ Map using the requested alignment.\n+  return os::map_memory_to_file_aligned(size, alignment, fd, mem_tag);\n+}\n+\n+ReservedSpace FileMappedMemoryReserver::reserve(char* requested_address,\n+                                                size_t size,\n+                                                size_t alignment,\n+                                                int fd,\n+                                                MemTag mem_tag) {\n+  sanity_check_size_and_alignment(size, alignment);\n+\n+  char* base = map_memory_to_file(requested_address, size, alignment, fd, mem_tag);\n+\n+  if (base != nullptr) {\n+    return ReservedSpace(base, size, alignment, os::vm_page_size(), !ExecMem, true \/* special *\/);\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+ReservedSpace CodeMemoryReserver::reserve(size_t size,\n+                                          size_t alignment,\n+                                          size_t page_size) {\n+  return MemoryReserver::reserve(nullptr \/* requested_address *\/,\n+                                 size,\n+                                 alignment,\n+                                 page_size,\n+                                 ExecMem,\n+                                 mtCode);\n+}\n+\n+ReservedHeapSpace HeapReserver::Instance::reserve_uncompressed_oops_heap(size_t size,\n+                                                                         size_t alignment,\n+                                                                         size_t page_size) {\n+  ReservedSpace reserved = reserve_memory(size, alignment, page_size);\n+\n+  if (reserved.is_reserved()) {\n+    return ReservedHeapSpace(reserved, 0 \/* noaccess_prefix *\/);\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+\n+static int maybe_create_file(const char* heap_allocation_directory) {\n+  if (heap_allocation_directory == nullptr) {\n+    return -1;\n+  }\n+\n+  int fd = os::create_file_for_heap(heap_allocation_directory);\n+  if (fd == -1) {\n+    vm_exit_during_initialization(\n+        err_msg(\"Could not create file for Heap at location %s\", heap_allocation_directory));\n+  }\n+\n+  return fd;\n+}\n+\n+HeapReserver::Instance::Instance(const char* heap_allocation_directory)\n+  : _fd(maybe_create_file(heap_allocation_directory)) {}\n+\n+HeapReserver::Instance::~Instance() {\n+  if (_fd != -1) {\n+    ::close(_fd);\n+  }\n+}\n+\n+ReservedSpace HeapReserver::Instance::reserve_memory(size_t size,\n+                                                     size_t alignment,\n+                                                     size_t page_size,\n+                                                     char* requested_address) {\n+\n+  \/\/ There are basically three different cases that we need to handle below:\n+  \/\/ 1. Mapping backed by a file\n+  \/\/ 2. Mapping backed by explicit large pages\n+  \/\/ 3. Mapping backed by normal pages or transparent huge pages\n+  \/\/ The first two have restrictions that requires the whole mapping to be\n+  \/\/ committed up front. To record this the ReservedSpace is marked 'special'.\n+\n+  \/\/ == Case 1 ==\n+  if (_fd != -1) {\n+    \/\/ When there is a backing file directory for this space then whether\n+    \/\/ large pages are allocated is up to the filesystem of the backing file.\n+    \/\/ So UseLargePages is not taken into account for this reservation.\n+    \/\/\n+    \/\/ If requested, let the user know that explicit large pages can't be used.\n+    if (use_explicit_large_pages(page_size) && large_pages_requested()) {\n+      log_debug(gc, heap)(\"Cannot allocate explicit large pages for Java Heap when AllocateHeapAt option is set.\");\n+    }\n+\n+    \/\/ Always return, not possible to fall back to reservation not using a file.\n+    return FileMappedMemoryReserver::reserve(requested_address, size, alignment, _fd, mtJavaHeap);\n+  }\n+\n+  \/\/ == Case 2 & 3 ==\n+  return MemoryReserver::reserve(requested_address, size, alignment, page_size, mtJavaHeap);\n+}\n+\n+\/\/ Compressed oop support is not relevant in 32bit builds.\n+#ifdef _LP64\n+\n+void HeapReserver::Instance::release(const ReservedSpace& reserved) {\n+  if (reserved.is_reserved()) {\n+    if (_fd == -1) {\n+      if (reserved.special()) {\n+        os::release_memory_special(reserved.base(), reserved.size());\n+      } else{\n+        os::release_memory(reserved.base(), reserved.size());\n+      }\n+    } else {\n+      os::unmap_memory(reserved.base(), reserved.size());\n+    }\n+  }\n+}\n+\n+\/\/ Tries to allocate memory of size 'size' at address requested_address with alignment 'alignment'.\n+\/\/ Does not check whether the reserved memory actually is at requested_address, as the memory returned\n+\/\/ might still fulfill the wishes of the caller.\n+\/\/ Assures the memory is aligned to 'alignment'.\n+ReservedSpace HeapReserver::Instance::try_reserve_memory(size_t size,\n+                                                         size_t alignment,\n+                                                         size_t page_size,\n+                                                         char* requested_address) {\n+  \/\/ Try to reserve the memory for the heap.\n+  log_trace(gc, heap, coops)(\"Trying to allocate at address \" PTR_FORMAT\n+                             \" heap of size \" SIZE_FORMAT_X,\n+                             p2i(requested_address),\n+                             size);\n+\n+  ReservedSpace reserved = reserve_memory(size, alignment, page_size, requested_address);\n+\n+  if (reserved.is_reserved()) {\n+    \/\/ Check alignment constraints.\n+    assert(reserved.alignment() == alignment, \"Unexpected\");\n+    assert(is_aligned(reserved.base(), alignment), \"Unexpected\");\n+    return reserved;\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+ReservedSpace HeapReserver::Instance::try_reserve_range(char *highest_start,\n+                                                        char *lowest_start,\n+                                                        size_t attach_point_alignment,\n+                                                        char *aligned_heap_base_min_address,\n+                                                        char *upper_bound,\n+                                                        size_t size,\n+                                                        size_t alignment,\n+                                                        size_t page_size) {\n+  const size_t attach_range = highest_start - lowest_start;\n+  \/\/ Cap num_attempts at possible number.\n+  \/\/ At least one is possible even for 0 sized attach range.\n+  const uint64_t num_attempts_possible = (attach_range \/ attach_point_alignment) + 1;\n+  const uint64_t num_attempts_to_try   = MIN2((uint64_t)HeapSearchSteps, num_attempts_possible);\n+\n+  const size_t stepsize = (attach_range == 0) ? \/\/ Only one try.\n+    (size_t) highest_start : align_up(attach_range \/ num_attempts_to_try, attach_point_alignment);\n+\n+  \/\/ Try attach points from top to bottom.\n+  for (char* attach_point = highest_start;\n+       attach_point >= lowest_start && attach_point <= highest_start;  \/\/ Avoid wrap around.\n+       attach_point -= stepsize) {\n+    ReservedSpace reserved = try_reserve_memory(size, alignment, page_size, attach_point);\n+\n+    if (reserved.is_reserved()) {\n+      if (reserved.base() >= aligned_heap_base_min_address &&\n+          size <= (uintptr_t)(upper_bound - reserved.base())) {\n+        \/\/ Got a successful reservation.\n+        return reserved;\n+      }\n+\n+      release(reserved);\n+    }\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+#define SIZE_64K  ((uint64_t) UCONST64(      0x10000))\n+#define SIZE_256M ((uint64_t) UCONST64(   0x10000000))\n+#define SIZE_32G  ((uint64_t) UCONST64(  0x800000000))\n+\n+\/\/ Helper for heap allocation. Returns an array with addresses\n+\/\/ (OS-specific) which are suited for disjoint base mode. Array is\n+\/\/ null terminated.\n+static char** get_attach_addresses_for_disjoint_mode() {\n+  static uint64_t addresses[] = {\n+     2 * SIZE_32G,\n+     3 * SIZE_32G,\n+     4 * SIZE_32G,\n+     8 * SIZE_32G,\n+    10 * SIZE_32G,\n+     1 * SIZE_64K * SIZE_32G,\n+     2 * SIZE_64K * SIZE_32G,\n+     3 * SIZE_64K * SIZE_32G,\n+     4 * SIZE_64K * SIZE_32G,\n+    16 * SIZE_64K * SIZE_32G,\n+    32 * SIZE_64K * SIZE_32G,\n+    34 * SIZE_64K * SIZE_32G,\n+    0\n+  };\n+\n+  \/\/ Sort out addresses smaller than HeapBaseMinAddress. This assumes\n+  \/\/ the array is sorted.\n+  uint i = 0;\n+  while (addresses[i] != 0 &&\n+         (addresses[i] < OopEncodingHeapMax || addresses[i] < HeapBaseMinAddress)) {\n+    i++;\n+  }\n+  uint start = i;\n+\n+  \/\/ Avoid more steps than requested.\n+  i = 0;\n+  while (addresses[start+i] != 0) {\n+    if (i == HeapSearchSteps) {\n+      addresses[start+i] = 0;\n+      break;\n+    }\n+    i++;\n+  }\n+\n+  return (char**) &addresses[start];\n+}\n+\n+\/\/ Create protection page at the beginning of the space.\n+static ReservedSpace establish_noaccess_prefix(const ReservedSpace& reserved, size_t noaccess_prefix) {\n+  assert(reserved.alignment() >= os::vm_page_size(), \"must be at least page size big\");\n+  assert(reserved.is_reserved(), \"should only be called on a reserved memory area\");\n+\n+  if (reserved.end() > (char *)OopEncodingHeapMax) {\n+    if (true\n+        WIN64_ONLY(&& !UseLargePages)\n+        AIX_ONLY(&& (os::Aix::supports_64K_mmap_pages() || os::vm_page_size() == 4*K))) {\n+      \/\/ Protect memory at the base of the allocated region.\n+      if (!os::protect_memory(reserved.base(), noaccess_prefix, os::MEM_PROT_NONE, reserved.special())) {\n+        fatal(\"cannot protect protection page\");\n+      }\n+      log_debug(gc, heap, coops)(\"Protected page at the reserved heap base: \"\n+                                 PTR_FORMAT \" \/ \" INTX_FORMAT \" bytes\",\n+                                 p2i(reserved.base()),\n+                                 noaccess_prefix);\n+      assert(CompressedOops::use_implicit_null_checks() == true, \"not initialized?\");\n+    } else {\n+      CompressedOops::set_use_implicit_null_checks(false);\n+    }\n+  }\n+\n+  return reserved.last_part(noaccess_prefix);\n+}\n+\n+ReservedHeapSpace HeapReserver::Instance::reserve_compressed_oops_heap(const size_t size, size_t alignment, size_t page_size) {\n+  const size_t noaccess_prefix_size = lcm(os::vm_page_size(), alignment);\n+  const size_t granularity = os::vm_allocation_granularity();\n+\n+  assert(size + noaccess_prefix_size <= OopEncodingHeapMax,  \"can not allocate compressed oop heap for this size\");\n+  assert(is_aligned(size, granularity), \"size not aligned to os::vm_allocation_granularity()\");\n+\n+  assert(alignment >= os::vm_page_size(), \"alignment too small\");\n+  assert(is_aligned(alignment, granularity), \"alignment not aligned to os::vm_allocation_granularity()\");\n+  assert(is_power_of_2(alignment), \"not a power of 2\");\n+\n+  \/\/ The necessary attach point alignment for generated wish addresses.\n+  \/\/ This is needed to increase the chance of attaching for mmap and shmat.\n+  \/\/ AIX is the only platform that uses System V shm for reserving virtual memory.\n+  \/\/ In this case, the required alignment of the allocated size (64K) and the alignment\n+  \/\/ of possible start points of the memory region (256M) differ.\n+  \/\/ This is not reflected by os_allocation_granularity().\n+  \/\/ The logic here is dual to the one in pd_reserve_memory in os_aix.cpp\n+  const size_t os_attach_point_alignment =\n+    AIX_ONLY(os::vm_page_size() == 4*K ? 4*K : 256*M)\n+    NOT_AIX(os::vm_allocation_granularity());\n+\n+  const size_t attach_point_alignment = lcm(alignment, os_attach_point_alignment);\n+\n+  char* aligned_heap_base_min_address = align_up((char*)HeapBaseMinAddress, alignment);\n+  size_t noaccess_prefix = ((aligned_heap_base_min_address + size) > (char*)OopEncodingHeapMax) ?\n+    noaccess_prefix_size : 0;\n+\n+  ReservedSpace reserved{};\n+\n+  \/\/ Attempt to alloc at user-given address.\n+  if (!FLAG_IS_DEFAULT(HeapBaseMinAddress)) {\n+    reserved = try_reserve_memory(size + noaccess_prefix, alignment, page_size, aligned_heap_base_min_address);\n+    if (reserved.base() != aligned_heap_base_min_address) { \/\/ Enforce this exact address.\n+      release(reserved);\n+      reserved = {};\n+    }\n+  }\n+\n+  \/\/ Keep heap at HeapBaseMinAddress.\n+  if (!reserved.is_reserved()) {\n+\n+    \/\/ Try to allocate the heap at addresses that allow efficient oop compression.\n+    \/\/ Different schemes are tried, in order of decreasing optimization potential.\n+    \/\/\n+    \/\/ For this, try_reserve_heap() is called with the desired heap base addresses.\n+    \/\/ A call into the os layer to allocate at a given address can return memory\n+    \/\/ at a different address than requested.  Still, this might be memory at a useful\n+    \/\/ address. try_reserve_heap() always returns this allocated memory, as only here\n+    \/\/ the criteria for a good heap are checked.\n+\n+    \/\/ Attempt to allocate so that we can run without base and scale (32-Bit unscaled compressed oops).\n+    \/\/ Give it several tries from top of range to bottom.\n+    if (aligned_heap_base_min_address + size <= (char *)UnscaledOopHeapMax) {\n+\n+      \/\/ Calc address range within we try to attach (range of possible start addresses).\n+      char* const highest_start = align_down((char *)UnscaledOopHeapMax - size, attach_point_alignment);\n+      char* const lowest_start  = align_up(aligned_heap_base_min_address, attach_point_alignment);\n+      reserved = try_reserve_range(highest_start, lowest_start, attach_point_alignment,\n+                                   aligned_heap_base_min_address, (char *)UnscaledOopHeapMax, size, alignment, page_size);\n+    }\n+\n+    \/\/ zerobased: Attempt to allocate in the lower 32G.\n+    char *zerobased_max = (char *)OopEncodingHeapMax;\n+\n+    \/\/ Give it several tries from top of range to bottom.\n+    if (aligned_heap_base_min_address + size <= zerobased_max && \/\/ Zerobased theoretical possible.\n+        ((!reserved.is_reserved()) ||                            \/\/ No previous try succeeded.\n+         (reserved.end() > zerobased_max))) {                    \/\/ Unscaled delivered an arbitrary address.\n+\n+      \/\/ Release previous reservation\n+      release(reserved);\n+\n+      \/\/ Calc address range within we try to attach (range of possible start addresses).\n+      char *const highest_start = align_down(zerobased_max - size, attach_point_alignment);\n+      \/\/ Need to be careful about size being guaranteed to be less\n+      \/\/ than UnscaledOopHeapMax due to type constraints.\n+      char *lowest_start = aligned_heap_base_min_address;\n+      uint64_t unscaled_end = UnscaledOopHeapMax - size;\n+      if (unscaled_end < UnscaledOopHeapMax) { \/\/ unscaled_end wrapped if size is large\n+        lowest_start = MAX2(lowest_start, (char*)unscaled_end);\n+      }\n+      lowest_start = align_up(lowest_start, attach_point_alignment);\n+      reserved = try_reserve_range(highest_start, lowest_start, attach_point_alignment,\n+                                   aligned_heap_base_min_address, zerobased_max, size, alignment, page_size);\n+    }\n+\n+    \/\/ Now we go for heaps with base != 0.  We need a noaccess prefix to efficiently\n+    \/\/ implement null checks.\n+    noaccess_prefix = noaccess_prefix_size;\n+\n+    \/\/ Try to attach at addresses that are aligned to OopEncodingHeapMax. Disjointbase mode.\n+    char** addresses = get_attach_addresses_for_disjoint_mode();\n+    int i = 0;\n+    while ((addresses[i] != nullptr) &&       \/\/ End of array not yet reached.\n+           ((!reserved.is_reserved()) ||      \/\/ No previous try succeeded.\n+           (reserved.end() > zerobased_max && \/\/ Not zerobased or unscaled address.\n+                                              \/\/ Not disjoint address.\n+            !CompressedOops::is_disjoint_heap_base_address((address)reserved.base())))) {\n+\n+      \/\/ Release previous reservation\n+      release(reserved);\n+\n+      char* const attach_point = addresses[i];\n+      assert(attach_point >= aligned_heap_base_min_address, \"Flag support broken\");\n+      reserved = try_reserve_memory(size + noaccess_prefix, alignment, page_size, attach_point);\n+      i++;\n+    }\n+\n+    \/\/ Last, desperate try without any placement.\n+    if (!reserved.is_reserved()) {\n+      log_trace(gc, heap, coops)(\"Trying to allocate at address null heap of size \" SIZE_FORMAT_X, size + noaccess_prefix);\n+      assert(alignment >= os::vm_page_size(), \"Unexpected\");\n+      reserved = reserve_memory(size + noaccess_prefix, alignment, page_size);\n+    }\n+  }\n+\n+  \/\/ No more reserve attempts\n+\n+  if (reserved.is_reserved()) {\n+    \/\/ Successfully found and reserved memory for the heap.\n+\n+    if (reserved.size() > size) {\n+      \/\/ We reserved heap memory with a noaccess prefix.\n+\n+      assert(reserved.size() == size + noaccess_prefix, \"Prefix should be included\");\n+      \/\/ It can happen we get a zerobased\/unscaled heap with noaccess prefix,\n+      \/\/ if we had to try at arbitrary address.\n+      reserved = establish_noaccess_prefix(reserved, noaccess_prefix);\n+      assert(reserved.size() == size, \"Prefix should be gone\");\n+      return ReservedHeapSpace(reserved, noaccess_prefix);\n+    }\n+\n+    \/\/ We reserved heap memory without a noaccess prefix.\n+    return ReservedHeapSpace(reserved, 0 \/* noaccess_prefix *\/);\n+  }\n+\n+  \/\/ Failed\n+  return {};\n+}\n+\n+#endif \/\/ _LP64\n+\n+ReservedHeapSpace HeapReserver::Instance::reserve_heap(size_t size, size_t alignment, size_t page_size) {\n+  if (UseCompressedOops) {\n+#ifdef _LP64\n+    return reserve_compressed_oops_heap(size, alignment, page_size);\n+#endif\n+  } else {\n+    return reserve_uncompressed_oops_heap(size, alignment, page_size);\n+  }\n+}\n+\n+ReservedHeapSpace HeapReserver::reserve(size_t size, size_t alignment, size_t page_size, const char* heap_allocation_directory) {\n+  sanity_check_arguments(size, alignment, page_size);\n+\n+  assert(alignment != 0, \"Precondition\");\n+  assert(is_aligned(size, alignment), \"Precondition\");\n+\n+  Instance instance(heap_allocation_directory);\n+\n+  return instance.reserve_heap(size, alignment, page_size);\n+}\n","filename":"src\/hotspot\/share\/memory\/memoryReserver.cpp","additions":693,"deletions":0,"binary":false,"changes":693,"status":"added"},{"patch":"@@ -0,0 +1,147 @@\n+\/*\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_MEMORYRESERVER_HPP\n+#define SHARE_MEMORY_MEMORYRESERVER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class MemoryReserver : AllStatic {\n+  static ReservedSpace reserve_memory(char* requested_address,\n+                                      size_t size,\n+                                      size_t alignment,\n+                                      bool exec,\n+                                      MemTag mem_tag);\n+\n+  static ReservedSpace reserve_memory_special(char* requested_address,\n+                                              size_t size,\n+                                              size_t alignment,\n+                                              size_t page_size,\n+                                              bool exec);\n+\n+public:\n+  \/\/ Final destination\n+  static ReservedSpace reserve(char* requested_address,\n+                               size_t size,\n+                               size_t alignment,\n+                               size_t page_size,\n+                               bool executable,\n+                               MemTag mem_tag);\n+\n+  \/\/ Convenience overloads\n+\n+  static ReservedSpace reserve(char* requested_address,\n+                               size_t size,\n+                               size_t alignment,\n+                               size_t page_size,\n+                               MemTag mem_tag = mtNone);\n+\n+  static ReservedSpace reserve(size_t size,\n+                               size_t alignment,\n+                               size_t page_size,\n+                               MemTag mem_tag = mtNone);\n+\n+  static ReservedSpace reserve(size_t size,\n+                               MemTag mem_tag);\n+\n+  \/\/ Release reserved memory\n+  static bool release(const ReservedSpace& reserved);\n+};\n+\n+class CodeMemoryReserver : AllStatic {\n+public:\n+  static ReservedSpace reserve(size_t size,\n+                              size_t alignment,\n+                              size_t page_size);\n+};\n+\n+class FileMappedMemoryReserver : AllStatic {\n+public:\n+  static ReservedSpace reserve(char* requested_address,\n+                               size_t size,\n+                               size_t alignment,\n+                               int fd,\n+                               MemTag mem_tag);\n+};\n+\n+class HeapReserver : AllStatic {\n+  class Instance {\n+    const int _fd;\n+\n+    NONCOPYABLE(Instance);\n+\n+    ReservedSpace reserve_memory(size_t size,\n+                                 size_t alignment,\n+                                 size_t page_size,\n+                                 char* requested_address = nullptr);\n+\n+    void release(const ReservedSpace& reserved);\n+\n+    \/\/ CompressedOops support\n+#ifdef _LP64\n+\n+    ReservedSpace try_reserve_memory(size_t size,\n+                                     size_t alignment,\n+                                     size_t page_size,\n+                                     char* requested_address);\n+\n+    ReservedSpace try_reserve_range(char *highest_start,\n+                                    char *lowest_start,\n+                                    size_t attach_point_alignment,\n+                                    char *aligned_heap_base_min_address,\n+                                    char *upper_bound,\n+                                    size_t size,\n+                                    size_t alignment,\n+                                    size_t page_size);\n+\n+    ReservedHeapSpace reserve_compressed_oops_heap(size_t size,\n+                                                   size_t alignment,\n+                                                   size_t page_size);\n+\n+#endif \/\/ _LP64\n+\n+    ReservedHeapSpace reserve_uncompressed_oops_heap(size_t size,\n+                                                     size_t alignment,\n+                                                     size_t page_size);\n+\n+  public:\n+    Instance(const char* heap_allocation_directory);\n+    ~Instance();\n+\n+    ReservedHeapSpace reserve_heap(size_t size,\n+                                   size_t alignment,\n+                                   size_t page_size);\n+  }; \/\/ Instance\n+\n+public:\n+  static ReservedHeapSpace reserve(size_t size,\n+                                   size_t alignment,\n+                                   size_t page_size,\n+                                   const char* heap_allocation_directory);\n+};\n+\n+#endif \/\/ SHARE_MEMORY_MEMORYRESERVER_HPP\n","filename":"src\/hotspot\/share\/memory\/memoryReserver.hpp","additions":147,"deletions":0,"binary":false,"changes":147,"status":"added"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -64,1 +66,0 @@\n-#include \"virtualspace.hpp\"\n@@ -600,1 +601,0 @@\n-  ReservedSpace rs;\n@@ -604,2 +604,7 @@\n-    rs = ReservedSpace::space_for_range(result, size, Metaspace::reserve_alignment(),\n-                                                      os::vm_page_size(), false, false);\n+\n+    return ReservedSpace(result,\n+                         size,\n+                         Metaspace::reserve_alignment(),\n+                         os::vm_page_size(),\n+                         !ExecMem,\n+                         false \/* special *\/);\n@@ -608,1 +613,1 @@\n-    rs = ReservedSpace();\n+    return {};\n@@ -610,1 +615,0 @@\n-  return rs;\n@@ -763,2 +767,6 @@\n-      rs = ReservedSpace(size, Metaspace::reserve_alignment(),\n-                         os::vm_page_size() \/* large *\/, (char*)base);\n+\n+      rs = MemoryReserver::reserve((char*)base,\n+                                   size,\n+                                   Metaspace::reserve_alignment(),\n+                                   os::vm_page_size());\n+\n@@ -1024,1 +1032,0 @@\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"memory\/virtualspace.hpp\"\n@@ -39,0 +38,1 @@\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"memory\/virtualspace.hpp\"\n@@ -35,0 +34,1 @@\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -85,1 +86,1 @@\n-    _rs = ReservedSpace(reserve_limit * BytesPerWord, Metaspace::reserve_alignment(), os::vm_page_size());\n+    _rs = MemoryReserver::reserve(reserve_limit * BytesPerWord, Metaspace::reserve_alignment(), os::vm_page_size());\n@@ -99,1 +100,1 @@\n-    _rs.release();\n+    MemoryReserver::release(_rs);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -256,3 +257,4 @@\n-  ReservedSpace rs(word_size * BytesPerWord,\n-                   Settings::virtual_space_node_reserve_alignment_words() * BytesPerWord,\n-                   os::vm_page_size());\n+\n+  ReservedSpace rs = MemoryReserver::reserve(word_size * BytesPerWord,\n+                                             Settings::virtual_space_node_reserve_alignment_words() * BytesPerWord,\n+                                             os::vm_page_size());\n@@ -289,1 +291,3 @@\n-    _rs.release();\n+    if (_rs.is_reserved()) {\n+      MemoryReserver::release(_rs);\n+    }\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/align.hpp\"\n+\n+#ifdef ASSERT\n+void ReservedSpace::sanity_checks() {\n+  assert(is_aligned(_base, os::vm_allocation_granularity()), \"Unaligned base\");\n+  assert(is_aligned(_base, _alignment), \"Unaligned base\");\n+  assert(is_aligned(_size, os::vm_page_size()), \"Unaligned size\");\n+  assert(os::page_sizes().contains(_page_size), \"Invalid pagesize\");\n+}\n+#endif\n","filename":"src\/hotspot\/share\/memory\/reservedSpace.cpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -0,0 +1,159 @@\n+\/*\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_RESERVEDSPACE_HPP\n+#define SHARE_MEMORY_RESERVEDSPACE_HPP\n+\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ ReservedSpace is a data structure for describing a reserved contiguous address range.\n+\n+class ReservedSpace {\n+  char*  _base;\n+  size_t _size;\n+  size_t _alignment;\n+  size_t _page_size;\n+  bool   _executable;\n+  bool   _special;\n+\n+  void sanity_checks() NOT_DEBUG_RETURN;\n+\n+public:\n+  \/\/ Constructor for non-reserved memory.\n+  ReservedSpace()\n+    : _base(nullptr),\n+      _size(0),\n+      _alignment(0),\n+      _page_size(0),\n+      _executable(false),\n+      _special(false) {}\n+\n+  \/\/ Main constructor\n+  ReservedSpace(char*  base,\n+                size_t size,\n+                size_t alignment,\n+                size_t page_size,\n+                bool   executable,\n+                bool   special)\n+    : _base(base),\n+      _size(size),\n+      _alignment(alignment),\n+      _page_size(page_size),\n+      _executable(executable),\n+      _special(special) {\n+    sanity_checks();\n+  }\n+\n+  bool is_reserved() const {\n+    return _base != nullptr;\n+  }\n+\n+  char* base() const {\n+    return _base;\n+  }\n+\n+  size_t size() const {\n+    return _size;\n+  }\n+\n+  char* end() const {\n+    return _base + _size;\n+  }\n+\n+  size_t alignment() const {\n+    return _alignment;\n+  }\n+\n+  size_t page_size() const {\n+    return _page_size;\n+  }\n+\n+  bool executable() const {\n+    return _executable;\n+  }\n+\n+  bool special() const {\n+    return _special;\n+  }\n+\n+  ReservedSpace partition(size_t offset, size_t partition_size, size_t alignment) const {\n+    assert(offset + partition_size <= size(), \"partition failed\");\n+\n+    char* const partition_base = base() + offset;\n+    assert(is_aligned(partition_base, alignment), \"partition base must be aligned\");\n+\n+    return ReservedSpace(partition_base,\n+                         partition_size,\n+                         alignment,\n+                         _page_size,\n+                         _executable,\n+                         _special);\n+  }\n+\n+  ReservedSpace partition(size_t offset, size_t partition_size) const {\n+    return partition(offset, partition_size, _alignment);\n+  }\n+\n+  ReservedSpace first_part(size_t split_offset, size_t alignment) const {\n+    return partition(0, split_offset, alignment);\n+  }\n+\n+  ReservedSpace first_part(size_t split_offset) const {\n+    return first_part(split_offset, _alignment);\n+  }\n+\n+  ReservedSpace last_part (size_t split_offset, size_t alignment) const {\n+    return partition(split_offset, _size - split_offset, alignment);\n+  }\n+\n+  ReservedSpace last_part (size_t split_offset) const {\n+    return last_part(split_offset, _alignment);\n+  }\n+};\n+\n+\/\/ Class encapsulating behavior specific to memory reserved for the Java heap.\n+class ReservedHeapSpace : public ReservedSpace {\n+private:\n+  const size_t _noaccess_prefix;\n+\n+public:\n+  \/\/ Constructor for non-reserved memory.\n+  ReservedHeapSpace()\n+    : ReservedSpace(),\n+      _noaccess_prefix() {}\n+\n+  ReservedHeapSpace(const ReservedSpace& reserved, size_t noaccess_prefix)\n+    : ReservedSpace(reserved),\n+      _noaccess_prefix(noaccess_prefix) {}\n+\n+  size_t noaccess_prefix() const { return _noaccess_prefix; }\n+\n+  \/\/ Returns the base to be used for compression, i.e. so that null can be\n+  \/\/ encoded safely and implicit null checks can work.\n+  char* compressed_oop_base() const { return base() - _noaccess_prefix; }\n+};\n+\n+#endif \/\/ SHARE_MEMORY_RESERVEDSPACE_HPP\n","filename":"src\/hotspot\/share\/memory\/reservedSpace.hpp","additions":159,"deletions":0,"binary":false,"changes":159,"status":"added"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -959,1 +960,11 @@\n-  ReservedHeapSpace total_rs(total_reserved, alignment, page_size, AllocateHeapAt);\n+  ReservedHeapSpace rhs = HeapReserver::reserve(total_reserved, alignment, page_size, AllocateHeapAt);\n+\n+  if (rhs.is_reserved()) {\n+    assert(total_reserved == rhs.size(),    \"must be exactly of required size\");\n+    assert(is_aligned(rhs.base(),alignment),\"must be exactly of required alignment\");\n+\n+    assert(markWord::encode_pointer_as_mark(rhs.base()).decode_pointer() == rhs.base(),\n+           \"area must be distinguishable from marks for mark-sweep\");\n+    assert(markWord::encode_pointer_as_mark(&rhs.base()[rhs.size()]).decode_pointer() ==\n+           &rhs.base()[rhs.size()],\n+           \"area must be distinguishable from marks for mark-sweep\");\n@@ -961,3 +972,0 @@\n-  if (total_rs.is_reserved()) {\n-    assert((total_reserved == total_rs.size()) && ((uintptr_t)total_rs.base() % alignment == 0),\n-           \"must be exactly of required size and alignment\");\n@@ -971,1 +979,1 @@\n-      CompressedOops::initialize(total_rs);\n+      CompressedOops::initialize(rhs);\n@@ -974,1 +982,1 @@\n-    Universe::calculate_verify_data((HeapWord*)total_rs.base(), (HeapWord*)total_rs.end());\n+    Universe::calculate_verify_data((HeapWord*)rhs.base(), (HeapWord*)rhs.end());\n@@ -976,1 +984,1 @@\n-    return total_rs;\n+    return rhs;\n@@ -985,1 +993,0 @@\n-  return ReservedHeapSpace(0, 0, os::vm_page_size());\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":15,"deletions":8,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n@@ -45,1 +46,0 @@\n-class ReservedHeapSpace;\n@@ -54,1 +54,0 @@\n-  friend class ReservedHeapSpace;\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,2 +26,2 @@\n-#include \"logging\/log.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"memory\/reservedSpace.hpp\"\n@@ -29,7 +29,0 @@\n-#include \"nmt\/memTracker.hpp\"\n-#include \"oops\/compressedKlass.hpp\"\n-#include \"oops\/compressedOops.hpp\"\n-#include \"oops\/markWord.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/java.hpp\"\n@@ -38,630 +31,2 @@\n-#include \"utilities\/formatBuffer.hpp\"\n-#include \"utilities\/powerOfTwo.hpp\"\n-\n-\/\/ ReservedSpace\n-\n-\/\/ Dummy constructor\n-ReservedSpace::ReservedSpace() : _base(nullptr), _size(0), _noaccess_prefix(0),\n-    _alignment(0), _special(false), _fd_for_heap(-1), _executable(false) {\n-}\n-\n-ReservedSpace::ReservedSpace(size_t size, MemTag mem_tag) : _fd_for_heap(-1) {\n-  \/\/ Want to use large pages where possible. If the size is\n-  \/\/ not large page aligned the mapping will be a mix of\n-  \/\/ large and normal pages.\n-  size_t page_size = os::page_size_for_region_unaligned(size, 1);\n-  size_t alignment = os::vm_allocation_granularity();\n-  initialize(size, alignment, page_size, nullptr, false, mem_tag);\n-}\n-\n-ReservedSpace::ReservedSpace(size_t size, size_t preferred_page_size) : _fd_for_heap(-1) {\n-  \/\/ When a page size is given we don't want to mix large\n-  \/\/ and normal pages. If the size is not a multiple of the\n-  \/\/ page size it will be aligned up to achieve this.\n-  size_t alignment = os::vm_allocation_granularity();\n-  if (preferred_page_size != os::vm_page_size()) {\n-    alignment = MAX2(preferred_page_size, alignment);\n-    size = align_up(size, alignment);\n-  }\n-  initialize(size, alignment, preferred_page_size, nullptr, false);\n-}\n-\n-ReservedSpace::ReservedSpace(size_t size,\n-                             size_t alignment,\n-                             size_t page_size,\n-                             char* requested_address) : _fd_for_heap(-1) {\n-  initialize(size, alignment, page_size, requested_address, false);\n-}\n-\n-ReservedSpace::ReservedSpace(char* base, size_t size, size_t alignment, size_t page_size,\n-                             bool special, bool executable) : _fd_for_heap(-1) {\n-  assert((size % os::vm_allocation_granularity()) == 0,\n-         \"size not allocation aligned\");\n-  initialize_members(base, size, alignment, page_size, special, executable);\n-}\n-\n-\/\/ Helper method\n-static char* attempt_map_or_reserve_memory_at(char* base, size_t size, int fd, bool executable, MemTag mem_tag) {\n-  if (fd != -1) {\n-    return os::attempt_map_memory_to_file_at(base, size, fd);\n-  }\n-  return os::attempt_reserve_memory_at(base, size, executable, mem_tag);\n-}\n-\n-\/\/ Helper method\n-static char* map_or_reserve_memory(size_t size, int fd, bool executable, MemTag mem_tag) {\n-  if (fd != -1) {\n-    return os::map_memory_to_file(size, fd);\n-  }\n-  return os::reserve_memory(size, executable, mem_tag);\n-}\n-\n-\/\/ Helper method\n-static char* map_or_reserve_memory_aligned(size_t size, size_t alignment, int fd, bool executable) {\n-  if (fd != -1) {\n-    return os::map_memory_to_file_aligned(size, alignment, fd);\n-  }\n-  return os::reserve_memory_aligned(size, alignment, executable);\n-}\n-\n-\/\/ Helper method\n-static void unmap_or_release_memory(char* base, size_t size, bool is_file_mapped) {\n-  if (is_file_mapped) {\n-    if (!os::unmap_memory(base, size)) {\n-      fatal(\"os::unmap_memory failed\");\n-    }\n-  } else if (!os::release_memory(base, size)) {\n-    fatal(\"os::release_memory failed\");\n-  }\n-}\n-\n-\/\/ Helper method\n-static bool failed_to_reserve_as_requested(char* base, char* requested_address) {\n-  if (base == requested_address || requested_address == nullptr) {\n-    return false; \/\/ did not fail\n-  }\n-\n-  if (base != nullptr) {\n-    \/\/ Different reserve address may be acceptable in other cases\n-    \/\/ but for compressed oops heap should be at requested address.\n-    assert(UseCompressedOops, \"currently requested address used only for compressed oops\");\n-    log_debug(gc, heap, coops)(\"Reserved memory not at requested address: \" PTR_FORMAT \" vs \" PTR_FORMAT, p2i(base), p2i(requested_address));\n-  }\n-  return true;\n-}\n-\n-static bool use_explicit_large_pages(size_t page_size) {\n-  return !os::can_commit_large_page_memory() &&\n-         page_size != os::vm_page_size();\n-}\n-\n-static bool large_pages_requested() {\n-  return UseLargePages &&\n-         (!FLAG_IS_DEFAULT(UseLargePages) || !FLAG_IS_DEFAULT(LargePageSizeInBytes));\n-}\n-\n-static void log_on_large_pages_failure(char* req_addr, size_t bytes) {\n-  if (large_pages_requested()) {\n-    \/\/ Compressed oops logging.\n-    log_debug(gc, heap, coops)(\"Reserve regular memory without large pages\");\n-    \/\/ JVM style warning that we did not succeed in using large pages.\n-    char msg[128];\n-    jio_snprintf(msg, sizeof(msg), \"Failed to reserve and commit memory using large pages. \"\n-                                   \"req_addr: \" PTR_FORMAT \" bytes: \" SIZE_FORMAT,\n-                                   req_addr, bytes);\n-    warning(\"%s\", msg);\n-  }\n-}\n-\n-static char* reserve_memory(char* requested_address, const size_t size,\n-                            const size_t alignment, int fd, bool exec, MemTag mem_tag) {\n-  char* base;\n-  \/\/ If the memory was requested at a particular address, use\n-  \/\/ os::attempt_reserve_memory_at() to avoid mapping over something\n-  \/\/ important.  If the reservation fails, return null.\n-  if (requested_address != nullptr) {\n-    assert(is_aligned(requested_address, alignment),\n-           \"Requested address \" PTR_FORMAT \" must be aligned to \" SIZE_FORMAT,\n-           p2i(requested_address), alignment);\n-    base = attempt_map_or_reserve_memory_at(requested_address, size, fd, exec, mem_tag);\n-  } else {\n-    \/\/ Optimistically assume that the OS returns an aligned base pointer.\n-    \/\/ When reserving a large address range, most OSes seem to align to at\n-    \/\/ least 64K.\n-    base = map_or_reserve_memory(size, fd, exec, mem_tag);\n-    \/\/ Check alignment constraints. This is only needed when there is\n-    \/\/ no requested address.\n-    if (!is_aligned(base, alignment)) {\n-      \/\/ Base not aligned, retry.\n-      unmap_or_release_memory(base, size, fd != -1 \/*is_file_mapped*\/);\n-      \/\/ Map using the requested alignment.\n-      base = map_or_reserve_memory_aligned(size, alignment, fd, exec);\n-    }\n-  }\n-\n-  return base;\n-}\n-\n-static char* reserve_memory_special(char* requested_address, const size_t size,\n-                                    const size_t alignment, const size_t page_size, bool exec) {\n-\n-  log_trace(pagesize)(\"Attempt special mapping: size: \" SIZE_FORMAT \"%s, \"\n-                      \"alignment: \" SIZE_FORMAT \"%s\",\n-                      byte_size_in_exact_unit(size), exact_unit_for_byte_size(size),\n-                      byte_size_in_exact_unit(alignment), exact_unit_for_byte_size(alignment));\n-\n-  char* base = os::reserve_memory_special(size, alignment, page_size, requested_address, exec);\n-  if (base != nullptr) {\n-    \/\/ Check alignment constraints.\n-    assert(is_aligned(base, alignment),\n-           \"reserve_memory_special() returned an unaligned address, base: \" PTR_FORMAT\n-           \" alignment: \" SIZE_FORMAT_X,\n-           p2i(base), alignment);\n-  }\n-  return base;\n-}\n-\n-void ReservedSpace::clear_members() {\n-  initialize_members(nullptr, 0, 0, 0, false, false);\n-}\n-\n-void ReservedSpace::initialize_members(char* base, size_t size, size_t alignment,\n-                                       size_t page_size, bool special, bool executable) {\n-  _base = base;\n-  _size = size;\n-  _alignment = alignment;\n-  _page_size = page_size;\n-  _special = special;\n-  _executable = executable;\n-  _noaccess_prefix = 0;\n-}\n-\n-void ReservedSpace::reserve(size_t size,\n-                            size_t alignment,\n-                            size_t page_size,\n-                            char* requested_address,\n-                            bool executable,\n-                            MemTag mem_tag) {\n-  assert(is_aligned(size, alignment), \"Size must be aligned to the requested alignment\");\n-\n-  \/\/ There are basically three different cases that we need to handle below:\n-  \/\/ 1. Mapping backed by a file\n-  \/\/ 2. Mapping backed by explicit large pages\n-  \/\/ 3. Mapping backed by normal pages or transparent huge pages\n-  \/\/ The first two have restrictions that requires the whole mapping to be\n-  \/\/ committed up front. To record this the ReservedSpace is marked 'special'.\n-\n-  \/\/ == Case 1 ==\n-  if (_fd_for_heap != -1) {\n-    \/\/ When there is a backing file directory for this space then whether\n-    \/\/ large pages are allocated is up to the filesystem of the backing file.\n-    \/\/ So UseLargePages is not taken into account for this reservation.\n-    char* base = reserve_memory(requested_address, size, alignment, _fd_for_heap, executable, mem_tag);\n-    if (base != nullptr) {\n-      initialize_members(base, size, alignment, os::vm_page_size(), true, executable);\n-    }\n-    \/\/ Always return, not possible to fall back to reservation not using a file.\n-    return;\n-  }\n-\n-  \/\/ == Case 2 ==\n-  if (use_explicit_large_pages(page_size)) {\n-    \/\/ System can't commit large pages i.e. use transparent huge pages and\n-    \/\/ the caller requested large pages. To satisfy this request we use\n-    \/\/ explicit large pages and these have to be committed up front to ensure\n-    \/\/ no reservations are lost.\n-    do {\n-      char* base = reserve_memory_special(requested_address, size, alignment, page_size, executable);\n-      if (base != nullptr) {\n-        \/\/ Successful reservation using large pages.\n-        initialize_members(base, size, alignment, page_size, true, executable);\n-        return;\n-      }\n-      page_size = os::page_sizes().next_smaller(page_size);\n-    } while (page_size > os::vm_page_size());\n-\n-    \/\/ Failed to reserve explicit large pages, do proper logging.\n-    log_on_large_pages_failure(requested_address, size);\n-    \/\/ Now fall back to normal reservation.\n-    assert(page_size == os::vm_page_size(), \"inv\");\n-  }\n-\n-  \/\/ == Case 3 ==\n-  char* base = reserve_memory(requested_address, size, alignment, -1, executable, mem_tag);\n-  if (base != nullptr) {\n-    \/\/ Successful mapping.\n-    initialize_members(base, size, alignment, page_size, false, executable);\n-  }\n-}\n-\n-void ReservedSpace::initialize(size_t size,\n-                               size_t alignment,\n-                               size_t page_size,\n-                               char* requested_address,\n-                               bool executable,\n-                               MemTag mem_tag) {\n-  const size_t granularity = os::vm_allocation_granularity();\n-  assert((size & (granularity - 1)) == 0,\n-         \"size not aligned to os::vm_allocation_granularity()\");\n-  assert((alignment & (granularity - 1)) == 0,\n-         \"alignment not aligned to os::vm_allocation_granularity()\");\n-  assert(alignment == 0 || is_power_of_2((intptr_t)alignment),\n-         \"not a power of 2\");\n-  assert(page_size >= os::vm_page_size(), \"Invalid page size\");\n-  assert(is_power_of_2(page_size), \"Invalid page size\");\n-\n-  clear_members();\n-\n-  if (size == 0) {\n-    return;\n-  }\n-\n-  \/\/ Adjust alignment to not be 0.\n-  alignment = MAX2(alignment, os::vm_page_size());\n-\n-  \/\/ Reserve the memory.\n-  reserve(size, alignment, page_size, requested_address, executable, mem_tag);\n-\n-  \/\/ Check that the requested address is used if given.\n-  if (failed_to_reserve_as_requested(_base, requested_address)) {\n-    \/\/ OS ignored the requested address, release the reservation.\n-    release();\n-    return;\n-  }\n-}\n-\n-ReservedSpace ReservedSpace::first_part(size_t partition_size, size_t alignment) {\n-  assert(partition_size <= size(), \"partition failed\");\n-  ReservedSpace result(base(), partition_size, alignment, page_size(), special(), executable());\n-  return result;\n-}\n-\n-ReservedSpace ReservedSpace::last_part(size_t partition_size, size_t alignment) {\n-  assert(partition_size <= size(), \"partition failed\");\n-  ReservedSpace result(base() + partition_size, size() - partition_size,\n-                       alignment, page_size(), special(), executable());\n-  return result;\n-}\n-\n-ReservedSpace ReservedSpace::partition(size_t offset, size_t partition_size, size_t alignment) {\n-  assert(offset + partition_size <= size(), \"partition failed\");\n-  ReservedSpace result(base() + offset, partition_size, alignment, page_size(), special(), executable());\n-  return result;\n-}\n-\n-void ReservedSpace::release() {\n-  if (is_reserved()) {\n-    char *real_base = _base - _noaccess_prefix;\n-    const size_t real_size = _size + _noaccess_prefix;\n-    if (special()) {\n-      if (_fd_for_heap != -1) {\n-        os::unmap_memory(real_base, real_size);\n-      } else {\n-        os::release_memory_special(real_base, real_size);\n-      }\n-    } else{\n-      os::release_memory(real_base, real_size);\n-    }\n-    clear_members();\n-  }\n-}\n-\n-\/\/ Put a ReservedSpace over an existing range\n-ReservedSpace ReservedSpace::space_for_range(char* base, size_t size, size_t alignment,\n-                                             size_t page_size, bool special, bool executable) {\n-  assert(is_aligned(base, os::vm_allocation_granularity()), \"Unaligned base\");\n-  assert(is_aligned(size, os::vm_page_size()), \"Unaligned size\");\n-  assert(os::page_sizes().contains(page_size), \"Invalid pagesize\");\n-  ReservedSpace space;\n-  space.initialize_members(base, size, alignment, page_size, special, executable);\n-  return space;\n-}\n-\n-\/\/ Compressed oop support is not relevant in 32bit builds.\n-#ifdef _LP64\n-\n-static size_t noaccess_prefix_size(size_t alignment) {\n-  return lcm(os::vm_page_size(), alignment);\n-}\n-\n-void ReservedHeapSpace::establish_noaccess_prefix() {\n-  assert(_alignment >= os::vm_page_size(), \"must be at least page size big\");\n-  _noaccess_prefix = noaccess_prefix_size(_alignment);\n-\n-  if (base() && base() + _size > (char *)OopEncodingHeapMax) {\n-    if (true\n-        WIN64_ONLY(&& !UseLargePages)\n-        AIX_ONLY(&& (os::Aix::supports_64K_mmap_pages() || os::vm_page_size() == 4*K))) {\n-      \/\/ Protect memory at the base of the allocated region.\n-      \/\/ If special, the page was committed (only matters on windows)\n-      if (!os::protect_memory(_base, _noaccess_prefix, os::MEM_PROT_NONE, _special)) {\n-        fatal(\"cannot protect protection page\");\n-      }\n-      log_debug(gc, heap, coops)(\"Protected page at the reserved heap base: \"\n-                                 PTR_FORMAT \" \/ \" INTX_FORMAT \" bytes\",\n-                                 p2i(_base),\n-                                 _noaccess_prefix);\n-      assert(CompressedOops::use_implicit_null_checks() == true, \"not initialized?\");\n-    } else {\n-      CompressedOops::set_use_implicit_null_checks(false);\n-    }\n-  }\n-\n-  _base += _noaccess_prefix;\n-  _size -= _noaccess_prefix;\n-  assert(((uintptr_t)_base % _alignment == 0), \"must be exactly of required alignment\");\n-}\n-\n-\/\/ Tries to allocate memory of size 'size' at address requested_address with alignment 'alignment'.\n-\/\/ Does not check whether the reserved memory actually is at requested_address, as the memory returned\n-\/\/ might still fulfill the wishes of the caller.\n-\/\/ Assures the memory is aligned to 'alignment'.\n-\/\/ NOTE: If ReservedHeapSpace already points to some reserved memory this is freed, first.\n-void ReservedHeapSpace::try_reserve_heap(size_t size,\n-                                         size_t alignment,\n-                                         size_t page_size,\n-                                         char* requested_address) {\n-  if (_base != nullptr) {\n-    \/\/ We tried before, but we didn't like the address delivered.\n-    release();\n-  }\n-\n-  \/\/ Try to reserve the memory for the heap.\n-  log_trace(gc, heap, coops)(\"Trying to allocate at address \" PTR_FORMAT\n-                             \" heap of size \" SIZE_FORMAT_X,\n-                             p2i(requested_address),\n-                             size);\n-\n-  reserve(size, alignment, page_size, requested_address, false, mtJavaHeap);\n-\n-  \/\/ Check alignment constraints.\n-  if (is_reserved() && !is_aligned(_base, _alignment)) {\n-    \/\/ Base not aligned, retry.\n-    release();\n-  }\n-}\n-\n-void ReservedHeapSpace::try_reserve_range(char *highest_start,\n-                                          char *lowest_start,\n-                                          size_t attach_point_alignment,\n-                                          char *aligned_heap_base_min_address,\n-                                          char *upper_bound,\n-                                          size_t size,\n-                                          size_t alignment,\n-                                          size_t page_size) {\n-  const size_t attach_range = highest_start - lowest_start;\n-  \/\/ Cap num_attempts at possible number.\n-  \/\/ At least one is possible even for 0 sized attach range.\n-  const uint64_t num_attempts_possible = (attach_range \/ attach_point_alignment) + 1;\n-  const uint64_t num_attempts_to_try   = MIN2((uint64_t)HeapSearchSteps, num_attempts_possible);\n-\n-  const size_t stepsize = (attach_range == 0) ? \/\/ Only one try.\n-    (size_t) highest_start : align_up(attach_range \/ num_attempts_to_try, attach_point_alignment);\n-\n-  \/\/ Try attach points from top to bottom.\n-  char* attach_point = highest_start;\n-  while (attach_point >= lowest_start  &&\n-         attach_point <= highest_start &&  \/\/ Avoid wrap around.\n-         ((_base == nullptr) ||\n-          (_base < aligned_heap_base_min_address || size > (uintptr_t)(upper_bound - _base)))) {\n-    try_reserve_heap(size, alignment, page_size, attach_point);\n-    attach_point -= stepsize;\n-  }\n-}\n-\n-#define SIZE_64K  ((uint64_t) UCONST64(      0x10000))\n-#define SIZE_256M ((uint64_t) UCONST64(   0x10000000))\n-#define SIZE_32G  ((uint64_t) UCONST64(  0x800000000))\n-\n-\/\/ Helper for heap allocation. Returns an array with addresses\n-\/\/ (OS-specific) which are suited for disjoint base mode. Array is\n-\/\/ null terminated.\n-static char** get_attach_addresses_for_disjoint_mode() {\n-  static uint64_t addresses[] = {\n-     2 * SIZE_32G,\n-     3 * SIZE_32G,\n-     4 * SIZE_32G,\n-     8 * SIZE_32G,\n-    10 * SIZE_32G,\n-     1 * SIZE_64K * SIZE_32G,\n-     2 * SIZE_64K * SIZE_32G,\n-     3 * SIZE_64K * SIZE_32G,\n-     4 * SIZE_64K * SIZE_32G,\n-    16 * SIZE_64K * SIZE_32G,\n-    32 * SIZE_64K * SIZE_32G,\n-    34 * SIZE_64K * SIZE_32G,\n-    0\n-  };\n-\n-  \/\/ Sort out addresses smaller than HeapBaseMinAddress. This assumes\n-  \/\/ the array is sorted.\n-  uint i = 0;\n-  while (addresses[i] != 0 &&\n-         (addresses[i] < OopEncodingHeapMax || addresses[i] < HeapBaseMinAddress)) {\n-    i++;\n-  }\n-  uint start = i;\n-\n-  \/\/ Avoid more steps than requested.\n-  i = 0;\n-  while (addresses[start+i] != 0) {\n-    if (i == HeapSearchSteps) {\n-      addresses[start+i] = 0;\n-      break;\n-    }\n-    i++;\n-  }\n-\n-  return (char**) &addresses[start];\n-}\n-\n-void ReservedHeapSpace::initialize_compressed_heap(const size_t size, size_t alignment, size_t page_size) {\n-  guarantee(size + noaccess_prefix_size(alignment) <= OopEncodingHeapMax,\n-            \"can not allocate compressed oop heap for this size\");\n-  guarantee(alignment == MAX2(alignment, os::vm_page_size()), \"alignment too small\");\n-\n-  const size_t granularity = os::vm_allocation_granularity();\n-  assert((size & (granularity - 1)) == 0,\n-         \"size not aligned to os::vm_allocation_granularity()\");\n-  assert((alignment & (granularity - 1)) == 0,\n-         \"alignment not aligned to os::vm_allocation_granularity()\");\n-  assert(alignment == 0 || is_power_of_2((intptr_t)alignment),\n-         \"not a power of 2\");\n-\n-  \/\/ The necessary attach point alignment for generated wish addresses.\n-  \/\/ This is needed to increase the chance of attaching for mmap and shmat.\n-  \/\/ AIX is the only platform that uses System V shm for reserving virtual memory.\n-  \/\/ In this case, the required alignment of the allocated size (64K) and the alignment\n-  \/\/ of possible start points of the memory region (256M) differ.\n-  \/\/ This is not reflected by os_allocation_granularity().\n-  \/\/ The logic here is dual to the one in pd_reserve_memory in os_aix.cpp\n-  const size_t os_attach_point_alignment =\n-    AIX_ONLY(os::vm_page_size() == 4*K ? 4*K : 256*M)\n-    NOT_AIX(os::vm_allocation_granularity());\n-\n-  const size_t attach_point_alignment = lcm(alignment, os_attach_point_alignment);\n-\n-  char *aligned_heap_base_min_address = (char *)align_up((void *)HeapBaseMinAddress, alignment);\n-  size_t noaccess_prefix = ((aligned_heap_base_min_address + size) > (char*)OopEncodingHeapMax) ?\n-    noaccess_prefix_size(alignment) : 0;\n-\n-  \/\/ Attempt to alloc at user-given address.\n-  if (!FLAG_IS_DEFAULT(HeapBaseMinAddress)) {\n-    try_reserve_heap(size + noaccess_prefix, alignment, page_size, aligned_heap_base_min_address);\n-    if (_base != aligned_heap_base_min_address) { \/\/ Enforce this exact address.\n-      release();\n-    }\n-  }\n-\n-  \/\/ Keep heap at HeapBaseMinAddress.\n-  if (_base == nullptr) {\n-\n-    \/\/ Try to allocate the heap at addresses that allow efficient oop compression.\n-    \/\/ Different schemes are tried, in order of decreasing optimization potential.\n-    \/\/\n-    \/\/ For this, try_reserve_heap() is called with the desired heap base addresses.\n-    \/\/ A call into the os layer to allocate at a given address can return memory\n-    \/\/ at a different address than requested.  Still, this might be memory at a useful\n-    \/\/ address. try_reserve_heap() always returns this allocated memory, as only here\n-    \/\/ the criteria for a good heap are checked.\n-\n-    \/\/ Attempt to allocate so that we can run without base and scale (32-Bit unscaled compressed oops).\n-    \/\/ Give it several tries from top of range to bottom.\n-    if (aligned_heap_base_min_address + size <= (char *)UnscaledOopHeapMax) {\n-\n-      \/\/ Calc address range within we try to attach (range of possible start addresses).\n-      char* const highest_start = align_down((char *)UnscaledOopHeapMax - size, attach_point_alignment);\n-      char* const lowest_start  = align_up(aligned_heap_base_min_address, attach_point_alignment);\n-      try_reserve_range(highest_start, lowest_start, attach_point_alignment,\n-                        aligned_heap_base_min_address, (char *)UnscaledOopHeapMax, size, alignment, page_size);\n-    }\n-\n-    \/\/ zerobased: Attempt to allocate in the lower 32G.\n-    char *zerobased_max = (char *)OopEncodingHeapMax;\n-\n-    \/\/ Give it several tries from top of range to bottom.\n-    if (aligned_heap_base_min_address + size <= zerobased_max &&    \/\/ Zerobased theoretical possible.\n-        ((_base == nullptr) ||                        \/\/ No previous try succeeded.\n-         (_base + size > zerobased_max))) {        \/\/ Unscaled delivered an arbitrary address.\n-\n-      \/\/ Calc address range within we try to attach (range of possible start addresses).\n-      char *const highest_start = align_down(zerobased_max - size, attach_point_alignment);\n-      \/\/ Need to be careful about size being guaranteed to be less\n-      \/\/ than UnscaledOopHeapMax due to type constraints.\n-      char *lowest_start = aligned_heap_base_min_address;\n-      uint64_t unscaled_end = UnscaledOopHeapMax - size;\n-      if (unscaled_end < UnscaledOopHeapMax) { \/\/ unscaled_end wrapped if size is large\n-        lowest_start = MAX2(lowest_start, (char*)unscaled_end);\n-      }\n-      lowest_start = align_up(lowest_start, attach_point_alignment);\n-      try_reserve_range(highest_start, lowest_start, attach_point_alignment,\n-                        aligned_heap_base_min_address, zerobased_max, size, alignment, page_size);\n-    }\n-\n-    \/\/ Now we go for heaps with base != 0.  We need a noaccess prefix to efficiently\n-    \/\/ implement null checks.\n-    noaccess_prefix = noaccess_prefix_size(alignment);\n-\n-    \/\/ Try to attach at addresses that are aligned to OopEncodingHeapMax. Disjointbase mode.\n-    char** addresses = get_attach_addresses_for_disjoint_mode();\n-    int i = 0;\n-    while ((addresses[i] != nullptr) &&                    \/\/ End of array not yet reached.\n-           ((_base == nullptr) ||                          \/\/ No previous try succeeded.\n-            (_base + size >  (char *)OopEncodingHeapMax && \/\/ Not zerobased or unscaled address.\n-             !CompressedOops::is_disjoint_heap_base_address((address)_base)))) {  \/\/ Not disjoint address.\n-      char* const attach_point = addresses[i];\n-      assert(attach_point >= aligned_heap_base_min_address, \"Flag support broken\");\n-      try_reserve_heap(size + noaccess_prefix, alignment, page_size, attach_point);\n-      i++;\n-    }\n-\n-    \/\/ Last, desperate try without any placement.\n-    if (_base == nullptr) {\n-      log_trace(gc, heap, coops)(\"Trying to allocate at address null heap of size \" SIZE_FORMAT_X, size + noaccess_prefix);\n-      initialize(size + noaccess_prefix, alignment, page_size, nullptr, false, mtJavaHeap);\n-    }\n-  }\n-}\n-\n-#endif \/\/ _LP64\n-\n-ReservedHeapSpace::ReservedHeapSpace(size_t size, size_t alignment, size_t page_size, const char* heap_allocation_directory) : ReservedSpace() {\n-\n-  if (size == 0) {\n-    return;\n-  }\n-\n-  if (heap_allocation_directory != nullptr) {\n-    _fd_for_heap = os::create_file_for_heap(heap_allocation_directory);\n-    if (_fd_for_heap == -1) {\n-      vm_exit_during_initialization(\n-        err_msg(\"Could not create file for Heap at location %s\", heap_allocation_directory));\n-    }\n-    \/\/ When there is a backing file directory for this space then whether\n-    \/\/ large pages are allocated is up to the filesystem of the backing file.\n-    \/\/ If requested, let the user know that explicit large pages can't be used.\n-    if (use_explicit_large_pages(page_size) && large_pages_requested()) {\n-      log_debug(gc, heap)(\"Cannot allocate explicit large pages for Java Heap when AllocateHeapAt option is set.\");\n-    }\n-  }\n-\n-  \/\/ Heap size should be aligned to alignment, too.\n-  guarantee(is_aligned(size, alignment), \"set by caller\");\n-\n-  if (UseCompressedOops) {\n-#ifdef _LP64\n-    initialize_compressed_heap(size, alignment, page_size);\n-    if (_size > size) {\n-      \/\/ We allocated heap with noaccess prefix.\n-      \/\/ It can happen we get a zerobased\/unscaled heap with noaccess prefix,\n-      \/\/ if we had to try at arbitrary address.\n-      establish_noaccess_prefix();\n-    }\n-#else\n-    ShouldNotReachHere();\n-#endif \/\/ _LP64\n-  } else {\n-    initialize(size, alignment, page_size, nullptr, false, mtJavaHeap);\n-  }\n-\n-  assert(markWord::encode_pointer_as_mark(_base).decode_pointer() == _base,\n-         \"area must be distinguishable from marks for mark-sweep\");\n-  assert(markWord::encode_pointer_as_mark(&_base[size]).decode_pointer() == &_base[size],\n-         \"area must be distinguishable from marks for mark-sweep\");\n-\n-  if (_fd_for_heap != -1) {\n-    ::close(_fd_for_heap);\n-  }\n-}\n-\n-MemRegion ReservedHeapSpace::region() const {\n-  return MemRegion((HeapWord*)base(), (HeapWord*)end());\n-}\n-\n-\/\/ Reserve space for code segment.  Same as Java heap only we mark this as\n-\/\/ executable.\n-ReservedCodeSpace::ReservedCodeSpace(size_t r_size,\n-                                     size_t rs_align,\n-                                     size_t rs_page_size) : ReservedSpace() {\n-  initialize(r_size, rs_align, rs_page_size, \/*requested address*\/ nullptr, \/*executable*\/ true, mtCode);\n-}\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/ostream.hpp\"\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":4,"deletions":639,"binary":false,"changes":643,"status":"modified"},{"patch":"@@ -28,2 +28,0 @@\n-#include \"memory\/memRegion.hpp\"\n-#include \"nmt\/memTag.hpp\"\n@@ -31,0 +29,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -33,132 +32,1 @@\n-\n-\/\/ ReservedSpace is a data structure for reserving a contiguous address range.\n-\n-class ReservedSpace {\n-  friend class VMStructs;\n- protected:\n-  char*  _base;\n-  size_t _size;\n-  size_t _noaccess_prefix;\n-  size_t _alignment;\n-  size_t _page_size;\n-  bool   _special;\n-  int    _fd_for_heap;\n- private:\n-  bool   _executable;\n-\n-  \/\/ ReservedSpace\n-  ReservedSpace(char* base, size_t size, size_t alignment,\n-                size_t page_size, bool special, bool executable);\n- protected:\n-  \/\/ Helpers to clear and set members during initialization. Two members\n-  \/\/ require special treatment:\n-  \/\/  * _fd_for_heap     - The fd is set once and should not be cleared\n-  \/\/                       even if the reservation has to be retried.\n-  \/\/  * _noaccess_prefix - Used for compressed heaps and updated after\n-  \/\/                       the reservation is initialized. Always set to\n-  \/\/                       0 during initialization.\n-  void clear_members();\n-  void initialize_members(char* base, size_t size, size_t alignment,\n-                          size_t page_size, bool special, bool executable);\n-\n-  void initialize(size_t size, size_t alignment, size_t page_size,\n-                  char* requested_address, bool executable, MemTag mem_tag = mtNone);\n-\n-  void reserve(size_t size, size_t alignment, size_t page_size,\n-               char* requested_address, bool executable, MemTag mem_tag);\n- public:\n-  \/\/ Constructor\n-  ReservedSpace();\n-  \/\/ Initialize the reserved space with the given size. Depending on the size\n-  \/\/ a suitable page size and alignment will be used.\n-  ReservedSpace(size_t size, MemTag mem_tag);\n-  \/\/ Initialize the reserved space with the given size. The preferred_page_size\n-  \/\/ is used as the minimum page size\/alignment. This may waste some space if\n-  \/\/ the given size is not aligned to that value, as the reservation will be\n-  \/\/ aligned up to the final alignment in this case.\n-  ReservedSpace(size_t size, size_t preferred_page_size);\n-  ReservedSpace(size_t size, size_t alignment, size_t page_size,\n-                char* requested_address = nullptr);\n-\n-  \/\/ Accessors\n-  char*  base()            const { return _base;      }\n-  size_t size()            const { return _size;      }\n-  char*  end()             const { return _base + _size; }\n-  size_t alignment()       const { return _alignment; }\n-  size_t page_size()       const { return _page_size; }\n-  bool   special()         const { return _special;   }\n-  bool   executable()      const { return _executable;   }\n-  size_t noaccess_prefix() const { return _noaccess_prefix;   }\n-  bool is_reserved()       const { return _base != nullptr; }\n-  void release();\n-\n-  \/\/ Splitting\n-  \/\/ This splits the space into two spaces, the first part of which will be returned.\n-  ReservedSpace first_part(size_t partition_size, size_t alignment);\n-  ReservedSpace last_part (size_t partition_size, size_t alignment);\n-  ReservedSpace partition (size_t offset, size_t partition_size, size_t alignment);\n-\n-  \/\/ These simply call the above using the default alignment.\n-  inline ReservedSpace first_part(size_t partition_size);\n-  inline ReservedSpace last_part (size_t partition_size);\n-  inline ReservedSpace partition (size_t offset, size_t partition_size);\n-\n-  bool contains(const void* p) const {\n-    return (base() <= ((char*)p)) && (((char*)p) < (base() + size()));\n-  }\n-\n-  \/\/ Put a ReservedSpace over an existing range\n-  static ReservedSpace space_for_range(char* base, size_t size, size_t alignment,\n-                                       size_t page_size, bool special, bool executable);\n-};\n-\n-ReservedSpace ReservedSpace::first_part(size_t partition_size)\n-{\n-  return first_part(partition_size, alignment());\n-}\n-\n-ReservedSpace ReservedSpace::last_part(size_t partition_size)\n-{\n-  return last_part(partition_size, alignment());\n-}\n-\n-ReservedSpace ReservedSpace::partition(size_t offset, size_t partition_size)\n-{\n-  return partition(offset, partition_size, alignment());\n-}\n-\n-\/\/ Class encapsulating behavior specific of memory space reserved for Java heap.\n-class ReservedHeapSpace : public ReservedSpace {\n- private:\n-\n-  \/\/ Compressed oop support is not relevant in 32bit builds.\n-#ifdef _LP64\n-\n-  void try_reserve_heap(size_t size, size_t alignment, size_t page_size,\n-                        char *requested_address);\n-  void try_reserve_range(char *highest_start, char *lowest_start,\n-                         size_t attach_point_alignment, char *aligned_HBMA,\n-                         char *upper_bound, size_t size, size_t alignment, size_t page_size);\n-  void initialize_compressed_heap(const size_t size, size_t alignment, size_t page_size);\n-  \/\/ Create protection page at the beginning of the space.\n-  void establish_noaccess_prefix();\n-\n-#endif \/\/ _LP64\n-\n- public:\n-  \/\/ Constructor. Tries to find a heap that is good for compressed oops.\n-  \/\/ heap_allocation_directory is the path to the backing memory for Java heap. When set, Java heap will be allocated\n-  \/\/ on the device which is managed by the file system where the directory resides.\n-  ReservedHeapSpace(size_t size, size_t forced_base_alignment, size_t page_size, const char* heap_allocation_directory = nullptr);\n-  \/\/ Returns the base to be used for compression, i.e. so that null can be\n-  \/\/ encoded safely and implicit null checks can work.\n-  char *compressed_oop_base() const { return _base - _noaccess_prefix; }\n-  MemRegion region() const;\n-};\n-\n-\/\/ Class encapsulating behavior specific memory space for Code\n-class ReservedCodeSpace : public ReservedSpace {\n- public:\n-  \/\/ Constructor\n-  ReservedCodeSpace(size_t r_size, size_t rs_align, size_t page_size);\n-};\n+class ReservedSpace;\n","filename":"src\/hotspot\/share\/memory\/virtualspace.hpp","additions":2,"deletions":134,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/reservedSpace.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"memory\/virtualspace.hpp\"\n@@ -69,1 +69,1 @@\n-  _heap_address_range = heap_space.region();\n+  _heap_address_range = MemRegion((HeapWord*)heap_space.base(), (HeapWord*)heap_space.end());\n","filename":"src\/hotspot\/share\/oops\/compressedOops.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -302,1 +303,1 @@\n-  ReservedHeapSpace rhs(100 * granularity, granularity, os::vm_page_size());\n+  ReservedHeapSpace rhs = HeapReserver::reserve(100 * granularity, granularity, os::vm_page_size(), nullptr);\n@@ -329,1 +330,1 @@\n-  ReservedHeapSpace rhs(reserved_space_size * granularity, granularity, os::vm_page_size());\n+  ReservedHeapSpace rhs = HeapReserver::reserve(reserved_space_size * granularity, granularity, os::vm_page_size(), nullptr);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/memoryReserver.hpp\"\n@@ -33,1 +34,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+\n@@ -53,1 +54,1 @@\n-  ReservedSpace bot_rs(G1BlockOffsetTable::compute_size(heap.word_size()), mtGC);\n+  ReservedSpace bot_rs = MemoryReserver::reserve(G1BlockOffsetTable::compute_size(heap.word_size()), mtGC);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_freeRegionList.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n@@ -84,1 +84,3 @@\n-  ReservedSpace rs(size, os::vm_page_size());\n+  ReservedSpace rs = MemoryReserver::reserve(size,\n+                                             os::vm_allocation_granularity(),\n+                                             os::vm_page_size());\n@@ -108,2 +110,3 @@\n-  ReservedSpace rs(size, page_size);\n-\n+  ReservedSpace rs = MemoryReserver::reserve(size,\n+                                             os::vm_allocation_granularity(),\n+                                             os::vm_page_size());\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_stressCommitUncommit.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n@@ -38,5 +38,1 @@\n-      if (_rs->special()) {\n-        EXPECT_TRUE(os::release_memory_special(_rs->base(), _rs->size()));\n-      } else {\n-        EXPECT_TRUE(os::release_memory(_rs->base(), _rs->size()));\n-      }\n+      EXPECT_TRUE(MemoryReserver::release(*_rs));\n@@ -67,1 +63,1 @@\n-    ReservedSpace rs(size, mtTest);\n+    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n@@ -81,1 +77,1 @@\n-    ReservedSpace rs(size, alignment, page_size, (char *) nullptr);\n+    ReservedSpace rs = MemoryReserver::reserve(size, alignment, page_size);\n@@ -109,1 +105,1 @@\n-    ReservedSpace rs(size, alignment, page_size);\n+    ReservedSpace rs = MemoryReserver::reserve(size, alignment, page_size);\n@@ -209,1 +205,3 @@\n-      _rs->release();\n+      if (_rs->is_reserved()) {\n+        MemoryReserver::release(*_rs);\n+      }\n@@ -218,1 +216,1 @@\n-        return ReservedSpace(reserve_size_aligned, mtTest);\n+        return MemoryReserver::reserve(reserve_size_aligned, mtTest);\n@@ -221,3 +219,3 @@\n-        return ReservedSpace(reserve_size_aligned,\n-                             os::vm_allocation_granularity(),\n-                             os::vm_page_size());\n+        return MemoryReserver::reserve(reserve_size_aligned,\n+                                       os::vm_allocation_granularity(),\n+                                       os::vm_page_size());\n@@ -302,1 +300,1 @@\n-  ReservedSpace reserved(large_page_size, large_page_size, large_page_size);\n+  ReservedSpace reserved = MemoryReserver::reserve(large_page_size, large_page_size, large_page_size);\n@@ -369,4 +367,3 @@\n-    ReservedSpace rs(size,          \/\/ size\n-                     alignment,     \/\/ alignment\n-                     page_size, \/\/ page size\n-                     (char *)nullptr); \/\/ requested_address\n+    ReservedSpace rs = MemoryReserver::reserve(size,\n+                                               alignment,\n+                                               page_size);\n@@ -390,1 +387,1 @@\n-    ReservedSpace rs(size, mtTest);\n+    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n@@ -415,1 +412,3 @@\n-    ReservedSpace rs(size, alignment, page_size);\n+    ReservedSpace rs = MemoryReserver::reserve(size,\n+                                               alignment,\n+                                               page_size);\n@@ -519,1 +518,1 @@\n-      return ReservedSpace(reserve_size_aligned, mtTest);\n+      return MemoryReserver::reserve(reserve_size_aligned, mtTest);\n@@ -522,3 +521,3 @@\n-      return ReservedSpace(reserve_size_aligned,\n-                           os::vm_allocation_granularity(),\n-                           os::vm_page_size());\n+      return MemoryReserver::reserve(reserve_size_aligned,\n+                                     os::vm_allocation_granularity(),\n+                                     os::vm_page_size());\n@@ -569,1 +568,3 @@\n-    reserved.release();\n+    if (reserved.is_reserved()) {\n+      MemoryReserver::release(reserved);\n+    }\n@@ -579,1 +580,3 @@\n-    ReservedSpace reserved(large_page_size, large_page_size, large_page_size);\n+    ReservedSpace reserved = MemoryReserver::reserve(large_page_size,\n+                                                     large_page_size,\n+                                                     large_page_size);\n@@ -591,1 +594,3 @@\n-    reserved.release();\n+    if (reserved.is_reserved()) {\n+      MemoryReserver::release(reserved);\n+    }\n","filename":"test\/hotspot\/gtest\/memory\/test_virtualspace.cpp","additions":33,"deletions":28,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -34,2 +34,1 @@\n-\n-#include \"memory\/virtualspace.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n@@ -96,1 +95,1 @@\n-    ReservedSpace rs(size, mtTest);\n+    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n@@ -170,1 +169,1 @@\n-    ReservedSpace rs(size, mtTest);\n+    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n@@ -257,1 +256,1 @@\n-    ReservedSpace rs(size, mtTest);\n+    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n@@ -428,1 +427,1 @@\n-    ReservedSpace rs(size, mtTest);\n+    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n","filename":"test\/hotspot\/gtest\/runtime\/test_virtualMemoryTracker.cpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"}]}