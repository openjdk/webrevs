{"files":[{"patch":"@@ -274,0 +274,3 @@\n+  case vmIntrinsics::_SVget:\n+  case vmIntrinsics::_SVslowGet:\n+  case vmIntrinsics::_SVCacheInvalidate:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -297,0 +297,5 @@\n+  do_intrinsic(_SVget,                   java_lang_ScopedValue,   get_name, void_object_signature, F_R)                 \\\n+  do_intrinsic(_SVslowGet,               java_lang_ScopedValue,   slowGet_name, void_object_signature, F_R)             \\\n+   do_name(     slowGet_name,                                    \"slowGet\")                                             \\\n+  do_intrinsic(_SVCacheInvalidate,       java_lang_ScopedValue_Cache, invalidate_name, int_void_signature, F_S)         \\\n+   do_name(     invalidate_name,                                 \"invalidate\")                                          \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -125,0 +125,1 @@\n+  template(java_lang_ScopedValue_Cache,               \"java\/lang\/ScopedValue$Cache\")              \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -108,1 +108,2 @@\n-    MergeMemNode* mm = opt_access.mem();\n+    assert(opt_access.mem()->is_MergeMem(), \"\");\n+    MergeMemNode* mm = opt_access.mem()->as_MergeMem();\n@@ -171,1 +172,1 @@\n-    MergeMemNode* mm = opt_access.mem();\n+    Node* mem = opt_access.mem();\n@@ -173,1 +174,3 @@\n-    Node* mem = mm->memory_at(gvn.C->get_alias_index(adr_type));\n+    if (mem->is_MergeMem()) {\n+      mem = mem->as_MergeMem()->memory_at(gvn.C->get_alias_index(adr_type));\n+    }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -190,1 +190,1 @@\n-  MergeMemNode* _mem;\n+  Node* _mem;\n@@ -194,1 +194,1 @@\n-  C2OptAccess(PhaseGVN& gvn, Node* ctl, MergeMemNode* mem, DecoratorSet decorators,\n+  C2OptAccess(PhaseGVN& gvn, Node* ctl, Node* mem, DecoratorSet decorators,\n@@ -201,1 +201,1 @@\n-  MergeMemNode* mem() const { return _mem; }\n+  Node* mem() const { return _mem; }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -704,0 +706,2 @@\n+    process_result(kit);\n+\n@@ -812,0 +816,418 @@\n+class LateInlineScopedValueCallGenerator : public LateInlineCallGenerator {\n+  Node* _sv;\n+  bool _process_result;\n+\n+public:\n+  LateInlineScopedValueCallGenerator(ciMethod* method, CallGenerator* inline_cg, bool process_result) :\n+          LateInlineCallGenerator(method, inline_cg), _sv(nullptr), _process_result(process_result) {}\n+\n+  virtual JVMState* generate(JVMState* jvms) {\n+    Compile *C = Compile::current();\n+\n+    C->log_inline_id(this);\n+\n+    C->add_scoped_value_late_inline(this);\n+\n+    JVMState* new_jvms = DirectCallGenerator::generate(jvms);\n+    return new_jvms;\n+  }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineScopedValueCallGenerator* cg = new LateInlineScopedValueCallGenerator(method(), _inline_cg, false);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n+\n+  void do_late_inline() {\n+    CallNode* call = call_node();\n+    _sv = call->in(TypeFunc::Parms);\n+    CallGenerator::do_late_inline_helper();\n+  }\n+\n+  virtual void set_process_result(bool v) {\n+    _process_result = v;\n+  }\n+\n+  virtual void process_result(GraphKit& kit) {\n+    if (!_process_result) {\n+      return;\n+    }\n+    \/\/ The call for ScopedValue.get() was just inlined. The code here pattern matches the resulting subgraph. To make it\n+    \/\/ easier:\n+    \/\/ - the slow path call to slowGet() is not inlined. If heuristics decided it should be, it was enqueued for late\n+    \/\/ inlining which will happen later.\n+    \/\/ - The call to Thread.scopedValueCache() is not inlined either.\n+    \/\/ The pattern matching here starts from the current control (end of inlining) and looks for the call for\n+    \/\/ Thread.scopedValueCache() which acts as a marker for the beginning of the subgraph of interest. In the process a\n+    \/\/ number of checks from the java code of ScopedValue.get() are expected to be encountered. They are recorded:\n+    assert(method()->intrinsic_id() == vmIntrinsics::_SVget, \"\");\n+    Compile* C = Compile::current();\n+    CallNode* scoped_value_cache = nullptr; \/\/ call to Thread.scopedValueCache()\n+    IfNode* get_cache_iff = nullptr; \/\/ test that scopedValueCache() is not null\n+    IfNode* get_first_iff = nullptr; \/\/ test for a hit in the cache with first hash\n+    IfNode* get_second_iff = nullptr; \/\/ test for a hit in the cache with second hash\n+    Node* first_index = nullptr; \/\/ index in the cache for first hash\n+    Node* second_index = nullptr; \/\/ index in the cache for second hash\n+    CallStaticJavaNode* slow_call = nullptr; \/\/ slowGet() call if any\n+    {\n+      ResourceMark rm;\n+      Unique_Node_List wq;\n+      wq.push(kit.control());\n+      for (uint i = 0; i < wq.size(); ++i) {\n+        Node* c = wq.at(i);\n+        if (c->is_Region()) {\n+          for (uint j = 1; j < c->req(); ++j) {\n+            Node* in = c->in(j);\n+            if (in != nullptr) {\n+              assert(!in->is_top(), \"\");\n+              wq.push(in);\n+            }\n+          }\n+        } else {\n+          if (c->Opcode() == Op_If) {\n+            Node* bol = c->in(1);\n+            assert(bol->is_Bool(), \"\");\n+            Node* cmp = bol->in(1);\n+            assert(cmp->Opcode() == Op_CmpP, \"\");\n+            Node* in1 = cmp->in(1);\n+            Node* in2 = cmp->in(2);\n+            if (in1->is_Proj() && in1->in(0)->is_Call() &&\n+                in1->in(0)->as_CallJava()->method()->intrinsic_id() == vmIntrinsics::_scopedValueCache) {\n+              assert(in2->bottom_type() == TypePtr::NULL_PTR, \"\");\n+              assert(get_cache_iff == nullptr, \"\");\n+              get_cache_iff = c->as_If();\n+              if (scoped_value_cache == nullptr) {\n+                scoped_value_cache = in1->in(0)->as_Call();\n+              } else {\n+                assert(scoped_value_cache == in1->in(0), \"\");\n+              }\n+              continue;\n+            } else if (in2->is_Proj() && in2->in(0)->is_Call() &&\n+                       in2->in(0)->as_CallJava()->method()->intrinsic_id() == vmIntrinsics::_scopedValueCache) {\n+              assert(in1->bottom_type() == TypePtr::NULL_PTR, \"\");\n+              assert(get_cache_iff == nullptr, \"\");\n+              get_cache_iff = c->as_If();\n+              if (scoped_value_cache == nullptr) {\n+                scoped_value_cache = in1->in(0)->as_Call();\n+              } else {\n+                assert(scoped_value_cache == in1->in(0), \"\");\n+              }\n+              continue;\n+            } else {\n+              Node* in;\n+              if (in1 == _sv) {\n+                in = in2;\n+              } else {\n+                assert(in2 = _sv, \"\");\n+                in = in1;\n+              }\n+              BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+              in = bs->step_over_gc_barrier(in);\n+              if (in->Opcode() == Op_DecodeN) {\n+                in = in->in(1);\n+              }\n+              assert(in->Opcode() == Op_LoadP || in->Opcode() == Op_LoadN, \"\");\n+              assert(C->get_alias_index(in->adr_type()) == C->get_alias_index(TypeAryPtr::OOPS), \"\");\n+              Node* addp1 = in->in(MemNode::Address);\n+              assert(addp1->is_AddP(), \"\");\n+              assert(addp1->in(AddPNode::Base)->uncast()->is_Proj() &&\n+                     addp1->in(AddPNode::Base)->uncast()->in(0)->as_CallJava()->method()->intrinsic_id() ==\n+                     vmIntrinsics::_scopedValueCache, \"\");\n+              if (scoped_value_cache == nullptr) {\n+                scoped_value_cache = addp1->in(AddPNode::Base)->uncast()->in(0)->as_Call();\n+              } else {\n+                assert(scoped_value_cache == addp1->in(AddPNode::Base)->uncast()->in(0), \"\");\n+              }\n+              assert(in->in(MemNode::Memory)->is_Proj() && in->in(MemNode::Memory)->in(0) == scoped_value_cache, \"\");\n+              Node* addp2 = addp1->in(AddPNode::Address);\n+              Node* offset1 = addp1->in(AddPNode::Offset);\n+              intptr_t const_offset = offset1->find_intptr_t_con(-1);\n+              BasicType bt = TypeAryPtr::OOPS->array_element_basic_type();\n+              int shift = exact_log2(type2aelembytes(bt));\n+              int header = arrayOopDesc::base_offset_in_bytes(bt);\n+              assert(const_offset >= header, \"\");\n+              const_offset -= header;\n+\n+              Node* index = kit.gvn().intcon(const_offset >> shift);\n+              if (addp2->is_AddP()) {\n+                assert(!addp2->in(AddPNode::Address)->is_AddP() &&\n+                       addp2->in(AddPNode::Base) == addp1->in(AddPNode::Base),\n+                       \"\");\n+                Node* offset2 = addp2->in(AddPNode::Offset);\n+                assert(offset2->Opcode() == Op_LShiftX && offset2->in(2)->find_int_con(-1) == shift, \"\");\n+                offset2 = offset2->in(1);\n+#ifdef _LP64\n+                assert(offset2->Opcode() == Op_ConvI2L, \"\");\n+                offset2 = offset2->in(1);\n+                if (offset2->Opcode() == Op_CastII && offset2->in(0)->is_Proj() &&\n+                    offset2->in(0)->in(0) == get_cache_iff) {\n+                  ShouldNotReachHere();\n+                  offset2 = offset2->in(1);\n+                }\n+#endif\n+                index = kit.gvn().transform(new AddINode(offset2, index));\n+              }\n+\n+              if (get_first_iff == nullptr) {\n+                get_first_iff = c->as_If();\n+                first_index = index;\n+              } else {\n+                assert(get_second_iff == nullptr, \"\");\n+                get_second_iff = c->as_If();\n+                second_index = index;\n+              }\n+            }\n+          } else if (c->is_RangeCheck()) {\n+            \/\/ Kill the range checks as they are known to always succeed\n+            kit.gvn().hash_delete(c);\n+            c->set_req(1, kit.gvn().intcon(1));\n+            C->record_for_igvn(c);\n+          } else if (c->is_CallStaticJava()) {\n+            assert(slow_call == nullptr, \"\");\n+            slow_call = c->as_CallStaticJava();\n+            assert(slow_call->method()->intrinsic_id() == vmIntrinsics::_SVslowGet, \"\");\n+          } else {\n+            assert(c->is_Proj() || c->is_Catch(), \"\");\n+          }\n+          wq.push(c->in(0));\n+        }\n+      }\n+      \/\/ get_first_iff\/get_second_iff contain the first\/second check we ran into during the graph traversal but they may\n+      \/\/ not be the first\/second one in execution order. Perform another traversal to figure out which is first.\n+      if (get_second_iff != nullptr) {\n+        Node_Stack stack(0);\n+        stack.push(get_cache_iff, 0);\n+        while (stack.is_nonempty()) {\n+          Node* c = stack.node();\n+          uint i = stack.index();\n+          if (i < c->outcnt()) {\n+            stack.set_index(i + 1);\n+            Node* u = c->raw_out(i);\n+            if (wq.member(u) && u != c) {\n+              if (u == get_first_iff) {\n+                break;\n+              } else if (u == get_second_iff) {\n+                swap(get_first_iff, get_second_iff);\n+                swap(first_index, second_index);\n+                break;\n+              }\n+              stack.push(u, 0);\n+            }\n+          } else {\n+            stack.pop();\n+          }\n+        }\n+      }\n+    }\n+\n+    assert(get_cache_iff != nullptr, \"\");\n+    assert(get_second_iff == nullptr || get_first_iff != nullptr, \"\");\n+\n+    if (get_first_iff != nullptr && get_second_iff != nullptr) {\n+      ProjNode* get_first_iff_failure = get_first_iff->proj_out(\n+              get_first_iff->in(1)->as_Bool()->_test._test == BoolTest::ne ? 0 : 1);\n+      CallStaticJavaNode* get_first_iff_unc = get_first_iff_failure->is_uncommon_trap_proj(Deoptimization::Reason_none);\n+      if (get_first_iff_unc != nullptr) {\n+        \/\/ first cache check never hits, keep only the second.\n+        swap(get_first_iff, get_second_iff);\n+        swap(first_index, second_index);\n+        get_second_iff = nullptr;\n+        second_index = nullptr;\n+      }\n+    }\n+\n+    \/\/ Now transform the subgraph in a way that makes is amenable to optimizations\n+\n+    \/\/ The path on exit of the method from parsing ends here\n+    Node* current_ctrl = kit.control();\n+    Node* frame = kit.gvn().transform(new ParmNode(C->start(), TypeFunc::FramePtr));\n+    Node* halt = kit.gvn().transform(new HaltNode(current_ctrl, frame, \"Dead path for ScopedValueCall::get\"));\n+    C->root()->add_req(halt);\n+    current_ctrl = nullptr;\n+\n+    \/\/ Now move right above the scopedValueCache() call\n+    Node* mem = scoped_value_cache->in(TypeFunc::Memory);\n+    Node* c = scoped_value_cache->in(TypeFunc::Control);\n+    Node* io = scoped_value_cache->in(TypeFunc::I_O);\n+\n+    kit.set_control(c);\n+    kit.set_all_memory(mem);\n+    kit.set_i_o(io);\n+\n+    \/\/ remove the scopedValueCache() call\n+    CallProjections scoped_value_cache_projs = CallProjections();\n+    scoped_value_cache->extract_projections(&scoped_value_cache_projs, true);\n+\n+    C->gvn_replace_by(scoped_value_cache_projs.fallthrough_memproj, mem);\n+    C->gvn_replace_by(scoped_value_cache_projs.fallthrough_ioproj, io);\n+\n+    kit.gvn().hash_delete(scoped_value_cache);\n+    scoped_value_cache->set_req(0, C->top());\n+    C->record_for_igvn(scoped_value_cache);\n+\n+    \/\/ replace it with its intrinsic code:\n+    Node* thread = kit.gvn().transform(new ThreadLocalNode());\n+    Node* p = kit.basic_plus_adr(C->top()\/*!oop*\/, thread, in_bytes(JavaThread::scopedValueCache_offset()));\n+    Node* cache_obj_handle = kit.make_load(nullptr, p, p->bottom_type()->is_ptr(), T_ADDRESS, MemNode::unordered);\n+    ciInstanceKlass* object_klass = ciEnv::current()->Object_klass();\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass(object_klass);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAryPtr* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, true, 0);\n+    Node* scoped_value_cache_load = kit.access_load(cache_obj_handle, objects_type, T_OBJECT, IN_NATIVE);\n+\n+    \/\/ A single ScopedValueGetHitsInCache node represents all checks that are needed to probe the cache (cache not null,\n+    \/\/ prob with first hash, prob with second hash)\n+    ScopedValueGetHitsInCacheNode* sv_hits_in_cache = new ScopedValueGetHitsInCacheNode(C, kit.control(),\n+                                                                                        scoped_value_cache_load,\n+                                                                                        kit.gvn().makecon(\n+                                                                                                TypePtr::NULL_PTR),\n+                                                                                        kit.memory(TypeAryPtr::OOPS),\n+                                                                                        _sv,\n+                                                                                        first_index == nullptr ? C->top() : first_index,\n+                                                                                        second_index == nullptr ? C->top() : second_index);\n+\n+    \/\/ It will later be expanded back to all the checks so record profile data\n+    float get_cache_prob = get_cache_iff->_prob;\n+    BoolNode* get_cache_bool = get_cache_iff->in(1)->as_Bool();\n+    if (get_cache_prob != PROB_UNKNOWN && !get_cache_bool->_test.is_canonical()) {\n+      get_cache_prob = 1 - get_cache_prob;\n+    }\n+    sv_hits_in_cache->set_profile_data(0, get_cache_iff->_fcnt, get_cache_prob);\n+    float get_first_prob = 0;\n+    if (get_first_iff != nullptr) {\n+      get_first_prob = get_first_iff->_prob;\n+      if (get_first_prob != PROB_UNKNOWN && !get_first_iff->in(1)->as_Bool()->_test.is_canonical()) {\n+        get_first_prob = 1 - get_first_prob;\n+      }\n+      sv_hits_in_cache->set_profile_data(1, get_first_iff->_fcnt, get_first_prob);\n+    } else {\n+      sv_hits_in_cache->set_profile_data(1, 0, 0);\n+    }\n+    float get_second_prob = 0;\n+    if (get_second_iff != nullptr) {\n+      get_second_prob = get_second_iff->_prob;\n+      if (get_second_prob != PROB_UNKNOWN && !get_second_iff->in(1)->as_Bool()->_test.is_canonical()) {\n+        get_second_prob = 1 - get_second_prob;\n+      }\n+      sv_hits_in_cache->set_profile_data(2, get_second_iff->_fcnt, get_second_prob);\n+    } else {\n+      sv_hits_in_cache->set_profile_data(2, 0, 0);\n+    }\n+    Node* sv_hits_in_cachex = kit.gvn().transform(sv_hits_in_cache);\n+    assert(sv_hits_in_cachex == sv_hits_in_cache, \"\");\n+\n+    \/\/ And compute the probability of a miss in the cache\n+    float prob;\n+    \/\/ get_cache_prob: probability that cache array is not null\n+    \/\/ get_first_prob: probability of a miss\n+    \/\/ get_second_prob: probability of a miss\n+    if (get_cache_prob == PROB_UNKNOWN || get_first_prob == PROB_UNKNOWN || get_second_prob == PROB_UNKNOWN) {\n+      prob = PROB_UNKNOWN;\n+    } else {\n+      prob = (1 - get_cache_prob) + get_cache_prob * (get_first_prob + (1 - get_first_prob) * get_second_prob);\n+    }\n+\n+    \/\/ Add the control flow that checks whether ScopedValueGetHitsInCache succeeds\n+    Node* bol = kit.gvn().transform(new BoolNode(sv_hits_in_cache, BoolTest::ne));\n+    IfNode* iff = new IfNode(kit.control(), bol, 1 - prob, get_cache_iff->_fcnt);\n+    Node* transformed_iff = kit.gvn().transform(iff);\n+    assert(transformed_iff == iff, \"\");\n+    Node* not_in_cache = kit.gvn().transform(new IfFalseNode(iff));\n+    Node* in_cache = kit.gvn().transform(new IfTrueNode(iff));\n+\n+    \/\/ Merge the paths that produce the result (in case there's a slow path)\n+    Node* r = new RegionNode(3);\n+    Node* phi_cache_value = new PhiNode(r, TypeInstPtr::BOTTOM);\n+    Node* phi_mem = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);\n+    Node* phi_io = new PhiNode(r, Type::ABIO);\n+\n+    C->gvn_replace_by(scoped_value_cache_projs.fallthrough_catchproj, not_in_cache);\n+    C->gvn_replace_by(scoped_value_cache_projs.resproj, scoped_value_cache_load);\n+\n+    if (slow_call == nullptr) {\n+      r->init_req(1, C->top());\n+      phi_cache_value->init_req(1, C->top());\n+      phi_mem->init_req(1, C->top());\n+      phi_io->init_req(1, C->top());\n+    } else {\n+      CallProjections slow_projs;\n+      slow_call->extract_projections(&slow_projs, false);\n+      Node* fallthrough = slow_projs.fallthrough_catchproj->clone();\n+      kit.gvn().set_type(fallthrough, fallthrough->bottom_type());\n+      r->init_req(1, fallthrough);\n+      C->gvn_replace_by(slow_projs.fallthrough_catchproj, C->top());\n+      phi_mem->init_req(1, slow_projs.fallthrough_memproj);\n+      phi_io->init_req(1, slow_projs.fallthrough_ioproj);\n+      phi_cache_value->init_req(1, slow_projs.resproj);\n+    }\n+    r->init_req(2, in_cache);\n+\n+    \/\/ ScopedValueGetLoadFromCache is a single that represents the result of a hit in the cache\n+    Node* cache_value = kit.gvn().transform(new ScopedValueGetLoadFromCacheNode(C, in_cache, sv_hits_in_cache));\n+    phi_cache_value->init_req(2, cache_value);\n+    phi_mem->init_req(2, kit.reset_memory());\n+    phi_io->init_req(2, kit.i_o());\n+    kit.set_all_memory(kit.gvn().transform(phi_mem));\n+    kit.set_i_o(kit.gvn().transform(phi_io));\n+    kit.set_control(kit.gvn().transform(r));\n+    C->record_for_igvn(r);\n+    kit.pop();\n+    kit.push(phi_cache_value);\n+    \/\/ Before the transformation of the subgraph we had (some branch may not be present depending on profile data),\n+    \/\/ in pseudo code:\n+    \/\/\n+    \/\/ if (cache == null) {\n+    \/\/   goto slow_call;\n+    \/\/ }\n+    \/\/ if (first_entry_hits) {\n+    \/\/   result = first_entry;\n+    \/\/ } else {\n+    \/\/   if (second_entry_hits) {\n+    \/\/     result = second_entry;\n+    \/\/   } else {\n+    \/\/     goto slow_call;\n+    \/\/   }\n+    \/\/ }\n+    \/\/ continue:\n+    \/\/\n+    \/\/ slow_call:\n+    \/\/ result = slowGet();\n+    \/\/ goto continue;\n+    \/\/\n+    \/\/ After transformation:\n+    \/\/ if (hits_in_the_cache) {\n+    \/\/   result = load_from_cache;\n+    \/\/ } else {\n+    \/\/   if (cache == null) {\n+    \/\/     goto slow_call;\n+    \/\/   }\n+    \/\/   if (first_entry_hits) {\n+    \/\/     halt;\n+    \/\/   } else {\n+    \/\/     if (second_entry_hits) {\n+    \/\/        halt;\n+    \/\/      } else {\n+    \/\/        goto slow_call;\n+    \/\/     }\n+    \/\/   }\n+    \/\/ }\n+    \/\/ continue:\n+    \/\/\n+    \/\/ slow_call:\n+    \/\/ result = slowGet();\n+    \/\/ goto continue;\n+    \/\/\n+    \/\/ the transformed graph includes 2 copies of the cache probing logic. One represented by the\n+    \/\/ ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache pair that is amenable to optimizations. The other from\n+    \/\/ the result of the parsing of the java code where the success path ends with an Halt node. The reason for that is\n+    \/\/ that some paths may end with an uncommon trap and if one traps, we want the trap to be recorded for the right bci.\n+    \/\/ When the ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache pair is expanded, split if finds the duplicate\n+    \/\/ logic and cleans it up.\n+  }\n+};\n+\n+CallGenerator* CallGenerator::for_scoped_value_late_inline(ciMethod* method, CallGenerator* inline_cg,\n+                                                           bool process_result) {\n+  return new LateInlineScopedValueCallGenerator(method, inline_cg, process_result);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":422,"deletions":0,"binary":false,"changes":422,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"graphKit.hpp\"\n@@ -144,0 +145,1 @@\n+  static CallGenerator* for_scoped_value_late_inline(ciMethod* m, CallGenerator* inline_cg, bool process_result);\n@@ -189,0 +191,2 @@\n+  virtual void set_process_result(bool v) {}\n+\n@@ -196,0 +200,2 @@\n+\n+  virtual void process_result(GraphKit& kit) {}\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3052,0 +3052,8 @@\n+\n+const Type* ScopedValueGetResultNode::Value(PhaseGVN* phase) const {\n+  if (phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return Node::Value(phase);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -715,0 +715,37 @@\n+\/\/ The result of a ScopedValue.get()\n+class ScopedValueGetResultNode : public  MultiNode {\n+public:\n+  enum {\n+      Control = 0,\n+      ScopedValue, \/\/ which ScopedValue object is this for?\n+      GetResult \/\/ subgraph that produces the result\n+  };\n+  enum {\n+      ControlOut = 0,\n+      Result \/\/ The ScopedValue.get() result\n+  };\n+  ScopedValueGetResultNode(Compile* C, Node* ctrl, Node* sv, Node* res) : MultiNode(3) {\n+    init_req(Control, ctrl);\n+    init_req(ScopedValue, sv);\n+    init_req(GetResult, res);\n+  }\n+  virtual int   Opcode() const;\n+  virtual const Type* bottom_type() const { return TypeTuple::SV_GET_RESULT; }\n+\n+  ProjNode* result_out() {\n+    return proj_out_or_null(Result);\n+  }\n+\n+  ProjNode* control_out() {\n+    return proj_out(ControlOut);\n+  }\n+\n+  Node* scoped_value() const {\n+    return in(ScopedValue);\n+  }\n+  Node* result_in() const {\n+    return in(GetResult);\n+  }\n+\n+  const Type* Value(PhaseGVN* phase) const;\n+};\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -215,0 +215,3 @@\n+macro(ScopedValueGetLoadFromCache)\n+macro(ScopedValueGetHitsInCache)\n+macro(ScopedValueGetResult)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -406,0 +406,1 @@\n+    remove_useless_late_inlines(   &_scoped_value_late_inlines, dead);\n@@ -464,0 +465,1 @@\n+  remove_useless_late_inlines(          &_scoped_value_late_inlines, useful);\n@@ -666,0 +668,1 @@\n+                  _scoped_value_late_inlines(comp_arena(), 2, 0, nullptr),\n@@ -1111,0 +1114,2 @@\n+  _has_scoped_value_invalidate = false;\n+  _has_scoped_value_get_nodes = false;\n@@ -2024,0 +2029,67 @@\n+void Compile::inline_scoped_value_calls(PhaseIterGVN& igvn) {\n+  if (_scoped_value_late_inlines.length() > 0) {\n+    PhaseGVN* gvn = initial_gvn();\n+    set_inlining_incrementally(true);\n+\n+    igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n+\n+    _late_inlines_pos = _late_inlines.length();\n+\n+    while (_scoped_value_late_inlines.length() > 0) {\n+      CallGenerator* cg = _scoped_value_late_inlines.pop();\n+      if (has_scoped_value_invalidate()) {\n+        \/\/ ScopedValue$Cache.invalidate() is called so pessimistically assume we can't optimize ScopedValue.get() and\n+        \/\/ enqueue the call for regular late inlining\n+        cg->set_process_result(false);\n+        C->add_late_inline(cg);\n+        continue;\n+      }\n+      C->set_has_scoped_value_get_nodes(true);\n+      CallNode* call = cg->call_node();\n+      CallProjections projs;\n+      call->extract_projections(&projs, true);\n+      Node* sv = call->in(TypeFunc::Parms);\n+      Node* control_out = projs.fallthrough_catchproj;\n+      Node* res = projs.resproj;\n+      if (res == nullptr) {\n+        res = gvn->transform(new ProjNode(call, TypeFunc::Parms));\n+      }\n+      control_out = control_out->clone();\n+      gvn->set_type_bottom(control_out);\n+      gvn->record_for_igvn(control_out);\n+      res = res->clone();\n+      gvn->set_type_bottom(res);\n+      gvn->record_for_igvn(res);\n+\n+      \/\/ Add a ScopedValueGetResult node after the call with the result of ScopedValue.get() as input\n+      ScopedValueGetResultNode* sv_get_result = new ScopedValueGetResultNode(C, control_out, sv, res);\n+      Node* sv_get_resultx = gvn->transform(sv_get_result);\n+      assert(sv_get_resultx == sv_get_result, \"\");\n+      Node* control_proj = gvn->transform(new ProjNode(sv_get_result, ScopedValueGetResultNode::ControlOut));\n+      Node* res_proj = gvn->transform(new ProjNode(sv_get_result, ScopedValueGetResultNode::Result));\n+\n+      C->gvn_replace_by(projs.fallthrough_catchproj, control_proj);\n+      if (projs.resproj != nullptr) {\n+        C->gvn_replace_by(projs.resproj, res_proj);\n+      }\n+\n+      Node* control_projx = gvn->transform(control_proj);\n+      assert(control_projx == control_proj, \"\");\n+      Node* res_projx = gvn->transform(res_proj);\n+      assert(res_projx == res_proj, \"\");\n+\n+      \/\/ Inline the call to ScopedValue.get(). That triggers the execution of LateInlineScopedValueCallGenerator::process_result()\n+      cg->do_late_inline();\n+      if (failing()) return;\n+\n+      C->set_has_split_ifs(true);\n+    }\n+\n+    inline_incrementally_cleanup(igvn);\n+\n+    set_inlining_incrementally(false);\n+\n+    inline_incrementally(igvn);\n+  }\n+}\n+\n@@ -2261,0 +2333,6 @@\n+  inline_scoped_value_calls(igvn);\n+\n+  print_method(PHASE_INCREMENTAL_SCOPED_VALUE_INLINE, 2);\n+\n+  if (failing())  return;\n+\n@@ -3845,0 +3923,6 @@\n+  case Op_ScopedValueGetResult:\n+  case Op_ScopedValueGetHitsInCache:\n+  case Op_ScopedValueGetLoadFromCache: {\n+    ShouldNotReachHere();\n+    break;\n+  }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -357,0 +357,2 @@\n+  bool                  _has_scoped_value_invalidate; \/\/ Did we encounter a call to ScopedValue$Cache.invalidate()?\n+  bool                  _has_scoped_value_get_nodes; \/\/ Are we optimizing ScopedValue.get() calls?\n@@ -459,0 +461,1 @@\n+  GrowableArray<CallGenerator*> _scoped_value_late_inlines; \/\/ same but for operations related to ScopedValue.get()\n@@ -674,0 +677,4 @@\n+  bool              has_scoped_value_invalidate() const { return _has_scoped_value_invalidate; }\n+  void          set_has_scoped_value_invalidate(bool v) { _has_scoped_value_invalidate = v; }\n+  bool              has_scoped_value_get_nodes() const { return _has_scoped_value_get_nodes; }\n+  void          set_has_scoped_value_get_nodes(bool v) { _has_scoped_value_get_nodes = v; }\n@@ -1042,0 +1049,4 @@\n+  void              add_scoped_value_late_inline(CallGenerator* cg) {\n+    _scoped_value_late_inlines.push(cg);\n+  }\n+\n@@ -1075,0 +1086,1 @@\n+  void inline_scoped_value_calls(PhaseIterGVN& igvn);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -162,0 +162,2 @@\n+      } else if (callee->intrinsic_id() == vmIntrinsics::_scopedValueCache) {\n+        return CallGenerator::for_late_inline(callee, cg);\n@@ -177,0 +179,4 @@\n+  if (callee->intrinsic_id() == vmIntrinsics::_SVCacheInvalidate) {\n+    C->set_has_scoped_value_invalidate(true);\n+  }\n+\n@@ -215,0 +221,6 @@\n+          } else if (callee->intrinsic_id() == vmIntrinsics::_SVget) {\n+            return CallGenerator::for_scoped_value_late_inline(callee, cg, true);\n+          } else if (callee->intrinsic_id() == vmIntrinsics::_SVslowGet) {\n+            return CallGenerator::for_late_inline(callee, cg);\n+          } else if (should_delay) {\n+            return CallGenerator::for_late_inline(callee, cg);\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -1036,2 +1037,3 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      if ((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n+           n->in(0)->as_Call()->returns_pointer()) ||\n+          (n->as_Proj()->_con == ScopedValueGetResultNode::Result && n->in(0)->Opcode() == Op_ScopedValueGetResult)) {\n@@ -1105,0 +1107,11 @@\n+    case Op_ScopedValueGetLoadFromCache: {\n+      ScopedValueGetLoadFromCacheNode* get_from_cache = (ScopedValueGetLoadFromCacheNode*)n;\n+      map_ideal_node(get_from_cache, phantom_obj);\n+      break;\n+    }\n+    case Op_ScopedValueGetResult: {\n+      ScopedValueGetResultNode* get_result = (ScopedValueGetResultNode*)n;\n+      add_local_var_and_edge(get_result, PointsToNode::NoEscape, get_result->result_in(), delayed_worklist);\n+      break;\n+    }\n+\n@@ -1193,2 +1206,3 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert((n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n+             n->in(0)->as_Call()->returns_pointer()) ||\n+             (n->as_Proj()->_con == ScopedValueGetResultNode::Result && n->in(0)->Opcode() == Op_ScopedValueGetResult), \"Unexpected node type\");\n@@ -1273,0 +1287,5 @@\n+    case Op_ScopedValueGetResult: {\n+      ScopedValueGetResultNode* get_result = (ScopedValueGetResultNode*)n;\n+      add_local_var_and_edge(get_result, PointsToNode::NoEscape, get_result->result_in(), nullptr);\n+      break;\n+    }\n@@ -3922,0 +3941,1 @@\n+              op == Op_ScopedValueGetLoadFromCache || op == Op_ScopedValueGetResult ||\n@@ -4072,3 +4092,4 @@\n-              op == Op_AryEq || op == Op_StrComp || op == Op_CountPositives ||\n-              op == Op_StrCompressedCopy || op == Op_StrInflatedCopy || op == Op_VectorizedHashCode ||\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+                     op == Op_AryEq || op == Op_StrComp || op == Op_CountPositives ||\n+                     op == Op_StrCompressedCopy || op == Op_StrInflatedCopy || op == Op_VectorizedHashCode ||\n+                     op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||\n+                     op == Op_ScopedValueGetLoadFromCache || op == Op_ScopedValueGetResult)) {\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/subnode.hpp\"\n@@ -371,0 +372,19 @@\n+\n+Node* ScopedValueGetLoadFromCacheNode::scoped_value() const {\n+  Node* hits_in_cache = in(1);\n+  assert(hits_in_cache->Opcode() == Op_ScopedValueGetHitsInCache, \"\");\n+  return ((ScopedValueGetHitsInCacheNode*)hits_in_cache)->scoped_value();\n+}\n+\n+IfNode* ScopedValueGetLoadFromCacheNode::iff() const {\n+  return in(0)->in(0)->as_If();\n+}\n+\n+#ifdef ASSERT\n+void ScopedValueGetLoadFromCacheNode::verify() const {\n+  assert(in(0)->Opcode() == Op_IfTrue, \"\");\n+  assert(in(0)->in(0)->in(1)->is_Bool(), \"\");\n+  assert(in(0)->in(0)->in(1)->in(1)->Opcode() == Op_ScopedValueGetHitsInCache, \"\");\n+  assert(in(0)->in(0)->in(1)->in(1) == in(1), \"\");\n+}\n+#endif\n","filename":"src\/hotspot\/share\/opto\/intrinsicnode.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -342,0 +342,19 @@\n+\/\/ The result from a successful load from the ScopedValue cache. Goes in pair with ScopedValueGetHitsInCache\n+class ScopedValueGetLoadFromCacheNode : public Node {\n+public:\n+  ScopedValueGetLoadFromCacheNode(Compile* C, Node* c, Node* hits_in_cache)\n+          : Node(c, hits_in_cache) {\n+  }\n+\n+  Node* scoped_value() const;\n+  IfNode* iff() const;\n+\n+  virtual int Opcode() const;\n+\n+  const Type* bottom_type() const {\n+    return TypeInstPtr::BOTTOM;\n+  }\n+\n+  void verify() const PRODUCT_RETURN;\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/intrinsicnode.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -1150,1 +1152,1 @@\n-                   in->as_Proj()->is_uncommon_trap_if_pattern() &&\n+                   is_uncommon_trap_if_pattern(in->as_IfProj()) &&\n@@ -1279,1 +1281,2 @@\n-  } else {\n+  } else if (!loop_predication_for_scoped_value_get(loop, if_success_proj, parse_predicate_proj, invar, reason, iff,\n+                                                    new_predicate_proj)) {\n@@ -1425,2 +1428,1 @@\n-      CallStaticJavaNode* call = if_proj->is_uncommon_trap_if_pattern();\n-      if (call == nullptr) {\n+      if (!is_uncommon_trap_if_pattern(if_proj)) {\n@@ -1442,2 +1444,2 @@\n-      Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(call->uncommon_trap_request());\n-      if (reason == Deoptimization::Reason_predicate) {\n+      CallStaticJavaNode* call = if_proj->is_uncommon_trap_if_pattern();\n+      if (call != nullptr && Deoptimization::trap_request_reason(call->uncommon_trap_request()) == Deoptimization::Reason_predicate) {\n@@ -1463,1 +1465,1 @@\n-      if (if_proj->as_Proj()->is_uncommon_trap_if_pattern() &&\n+      if (is_uncommon_trap_if_pattern(if_proj->as_IfProj()) &&\n@@ -1549,0 +1551,98 @@\n+\n+bool PhaseIdealLoop::is_uncommon_trap_if_pattern(IfProjNode* proj) {\n+  if (proj->is_uncommon_trap_if_pattern()) {\n+    return true;\n+  }\n+  if (proj->in(0)->in(1)->is_Bool() && proj->in(0)->in(1)->in(1)->Opcode() == Op_ScopedValueGetHitsInCache &&\n+      proj->is_multi_uncommon_trap_if_pattern()) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\n+bool PhaseIdealLoop::loop_predication_for_scoped_value_get(IdealLoopTree* loop, IfProjNode* if_success_proj,\n+                                                           ParsePredicateSuccessProj* parse_predicate_proj,\n+                                                           Invariance &invar, Deoptimization::DeoptReason reason,\n+                                                           IfNode* iff, IfProjNode*&new_predicate_proj) {\n+  \/\/ A ScopedValueGetHitsInCache check is loop invariant if the scoped value object it is applied to is loop invariant\n+  BoolNode* bol = iff->in(1)->as_Bool();\n+  if (bol->in(1)->Opcode() == Op_ScopedValueGetHitsInCache && invar.is_invariant(((ScopedValueGetHitsInCacheNode*)bol->in(1))->scoped_value()) &&\n+      invar.is_invariant(((ScopedValueGetHitsInCacheNode*)bol->in(1))->index1()) && invar.is_invariant(((ScopedValueGetHitsInCacheNode*)bol->in(1))->index2())) {\n+    ScopedValueGetHitsInCacheNode* hits_in_the_cache = (ScopedValueGetHitsInCacheNode*) bol->in(1);\n+    Node* load_from_cache = if_success_proj->find_unique_out_with(Op_ScopedValueGetLoadFromCache);\n+    assert(load_from_cache->in(1) == hits_in_the_cache, \"\");\n+    assert(if_success_proj->is_IfTrue(), \"\");\n+    new_predicate_proj = create_new_if_for_predicate(parse_predicate_proj, nullptr,\n+                                                     reason,\n+                                                     iff->Opcode());\n+    Node* ctrl = new_predicate_proj->in(0)->in(0);\n+    Node* new_bol = bol->clone();\n+    register_new_node(new_bol, ctrl);\n+    Node* new_hits_in_the_cache = hits_in_the_cache->clone();\n+    register_new_node(new_hits_in_the_cache, ctrl);\n+    _igvn.replace_input_of(load_from_cache, 1, new_hits_in_the_cache);\n+\n+    CallStaticJavaNode* call = new_predicate_proj->is_uncommon_trap_if_pattern();\n+    assert(call != nullptr, \"\");\n+\n+    Node* all_mem = call->in(TypeFunc::Memory);\n+    MergeMemNode* mm = all_mem->is_MergeMem() ? all_mem->as_MergeMem() : nullptr;\n+    Node* raw_mem = mm != nullptr ? mm->memory_at(Compile::AliasIdxRaw) : all_mem;\n+\n+    \/\/ It is easier to re-create the cache load subgraph rather than trying to change the inputs of the existing one to\n+    \/\/ move it out of loops\n+    Node* thread = new ThreadLocalNode();\n+    register_new_node(thread, C->root());\n+    Node* scoped_value_cache_offset = _igvn.MakeConX(in_bytes(JavaThread::scopedValueCache_offset()));\n+    set_ctrl(scoped_value_cache_offset, C->root());\n+    Node* p = new AddPNode(C->top(), thread, scoped_value_cache_offset);\n+    register_new_node(p, C->root());\n+    Node* handle_load = LoadNode::make(_igvn, nullptr, raw_mem, p, p->bottom_type()->is_ptr(), TypeRawPtr::NOTNULL,\n+                                       T_ADDRESS, MemNode::unordered);\n+    _igvn.register_new_node_with_optimizer(handle_load);\n+    set_subtree_ctrl(handle_load, true);\n+\n+    ciInstanceKlass* object_klass = ciEnv::current()->Object_klass();\n+    const TypeOopPtr* etype = TypeOopPtr::make_from_klass(object_klass);\n+    const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+    const TypeAryPtr* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, true, 0);\n+\n+    DecoratorSet decorators = C2_READ_ACCESS | IN_NATIVE;\n+    C2AccessValuePtr addr(handle_load, TypeRawPtr::NOTNULL);\n+    C2OptAccess access(_igvn, nullptr, raw_mem, decorators, T_OBJECT, nullptr, addr);\n+    BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+    Node* load_of_cache = bs->load_at(access, objects_type);\n+    set_subtree_ctrl(load_of_cache, true);\n+\n+    _igvn.replace_input_of(new_hits_in_the_cache, 1, load_of_cache);\n+    Node* oop_mem = mm != nullptr ? mm->memory_at(C->get_alias_index(TypeAryPtr::OOPS)) : all_mem;\n+    _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::Memory, oop_mem);\n+    _igvn.replace_input_of(new_hits_in_the_cache, 0, ctrl);\n+    _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::ScopedValue,\n+                           invar.clone(hits_in_the_cache->scoped_value(), ctrl));\n+    _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::Index1,\n+                           invar.clone(hits_in_the_cache->index1(), ctrl));\n+    _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::Index2,\n+                           invar.clone(hits_in_the_cache->index2(), ctrl));\n+\n+    _igvn.replace_input_of(new_bol, 1, new_hits_in_the_cache);\n+\n+    assert(invar.is_invariant(new_bol), \"\");\n+\n+    IfNode* new_predicate_iff = new_predicate_proj->in(0)->as_If();\n+    _igvn.hash_delete(new_predicate_iff);\n+    new_predicate_iff->set_req(1, new_bol);\n+#ifndef PRODUCT\n+    if (TraceLoopPredicate) {\n+      tty->print(\"Predicate invariant if: %d \", new_predicate_iff->_idx);\n+      loop->dump_head();\n+    } else if (TraceLoopOpts) {\n+      tty->print(\"Predicate IC \");\n+      loop->dump_head();\n+    }\n+#endif\n+    return true;\n+  }\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/opto\/loopPredicate.cpp","additions":107,"deletions":7,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -447,2 +447,2 @@\n-bool IdealLoopTree::policy_peeling(PhaseIdealLoop *phase) {\n-  uint estimate = estimate_peeling(phase);\n+bool IdealLoopTree::policy_peeling(PhaseIdealLoop* phase, bool scoped_value_only) {\n+  uint estimate = estimate_peeling(phase, scoped_value_only);\n@@ -456,1 +456,1 @@\n-uint IdealLoopTree::estimate_peeling(PhaseIdealLoop *phase) {\n+uint IdealLoopTree::estimate_peeling(PhaseIdealLoop* phase, bool scoped_value_only) {\n@@ -484,1 +484,1 @@\n-    if (test->is_If()) {    \/\/ Test?\n+    if (test->is_If() && !scoped_value_only) {    \/\/ Test?\n@@ -500,0 +500,2 @@\n+    } else if (test->Opcode() == Op_ScopedValueGetResult && !is_member(phase->get_loop(phase->get_ctrl(((ScopedValueGetResultNode*)test)->scoped_value())))) {\n+      return estimate;\n@@ -3616,1 +3618,1 @@\n-    if (policy_peeling(phase)) {    \/\/ Should we peel?\n+    if (policy_peeling(phase, false)) {    \/\/ Should we peel?\n@@ -3657,1 +3659,1 @@\n-  uint est_peeling = estimate_peeling(phase);\n+  uint est_peeling = estimate_peeling(phase, false);\n@@ -3758,0 +3760,6 @@\n+      \/\/ if the loop body has a ScopedValueGetResult for a loop invariant ScopedValue object that dominates the backedge,\n+      \/\/ then peeling one iteration of the loop body will allow the entire ScopedValue subgraph from the loop body\n+      if (policy_peeling(phase, true)) {\n+        phase->do_peeling(this, old_new);\n+        return false;\n+      }\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -4393,1 +4394,1 @@\n-          !_verify_only && !bs->is_gc_specific_loop_opts_pass(_mode);\n+          !_verify_only && !bs->is_gc_specific_loop_opts_pass(_mode) && !C->has_scoped_value_get_nodes();\n@@ -4493,0 +4494,2 @@\n+  C->set_has_scoped_value_get_nodes(_scoped_value_get_nodes.size() > 0);\n+\n@@ -4603,0 +4606,4 @@\n+  if (!C->major_progress() && optimize_scoped_value_get_nodes()) {\n+    C->set_major_progress();\n+  }\n+\n@@ -4665,0 +4672,4 @@\n+  if (!C->major_progress() && do_split_ifs && expand_scoped_value_get_nodes()) {\n+    C->set_major_progress();\n+  }\n+\n@@ -4698,0 +4709,325 @@\n+\/\/ Expansion of ScopedValue nodes happen during loop opts because their expansion creates an opportunity for\n+\/\/ further loop optimizations (see comment in LateInlineScopedValueCallGenerator::process_result)\n+bool PhaseIdealLoop::expand_scoped_value_get_nodes() {\n+  bool progress = false;\n+  assert(!_igvn.delay_transform(), \"\");\n+  _igvn.set_delay_transform(true);\n+  for (uint i = _scoped_value_get_nodes.size(); i > 0; i--) {\n+    Node* n = _scoped_value_get_nodes.at(i - 1);\n+    if (n->Opcode() == Op_ScopedValueGetResult) {\n+      \/\/ Remove the ScopedValueGetResult entirely\n+      ScopedValueGetResultNode* get_result = (ScopedValueGetResultNode*) n;\n+      Node* result_out = get_result->result_out();\n+      Node* result_in = get_result->in(ScopedValueGetResultNode::GetResult);\n+      if (result_out != nullptr) {\n+        _igvn.replace_node(result_out, result_in);\n+      } else {\n+        _igvn.replace_input_of(get_result, ScopedValueGetResultNode::GetResult, C->top());\n+      }\n+      lazy_replace(get_result->control_out(), get_result->in(ScopedValueGetResultNode::Control));\n+      progress = true;\n+      Node* top_of_stack = _scoped_value_get_nodes.pop();\n+      remove_scoped_value_get_at(i-1);\n+    }\n+  }\n+  while (_scoped_value_get_nodes.size() > 0) {\n+    Node* n = _scoped_value_get_nodes.pop();\n+    assert (n->Opcode() == Op_ScopedValueGetHitsInCache, \"\");\n+    ScopedValueGetHitsInCacheNode* get_from_cache = (ScopedValueGetHitsInCacheNode*) n;\n+    expand_get_from_sv_cache(get_from_cache);\n+    progress = true;\n+  }\n+  _igvn.set_delay_transform(false);\n+  return progress;\n+}\n+\n+void PhaseIdealLoop::expand_get_from_sv_cache(ScopedValueGetHitsInCacheNode* get_from_cache) {\n+  get_from_cache->verify();\n+#ifdef ASSERT\n+  for (DUIterator_Fast imax, i = get_from_cache->fast_outs(imax); i < imax; i++) {\n+    Node* u = get_from_cache->fast_out(i);\n+    assert(u->is_Bool() || u->Opcode() == Op_ScopedValueGetLoadFromCache, \"\");\n+  }\n+#endif\n+  BoolNode* bol = get_from_cache->find_unique_out_with(Op_Bool)->as_Bool();\n+  assert(bol->_test._test == BoolTest::ne, \"\");\n+  IfNode* iff = bol->find_unique_out_with(Op_If)->as_If();\n+  ProjNode* success = iff->proj_out(1);\n+  ProjNode* failure = iff->proj_out(0);\n+\n+\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = (ScopedValueGetLoadFromCacheNode*)success->find_unique_out_with(Op_ScopedValueGetLoadFromCache);\n+  if (load_from_cache != nullptr) {\n+    load_from_cache->verify();\n+  }\n+  Node* first_index = get_from_cache->index1();\n+  Node* second_index = get_from_cache->index2();\n+\n+  if (first_index == C->top() && second_index == C->top()) {\n+    Node* zero = _igvn.intcon(0);\n+    set_ctrl(zero, C->root());\n+    _igvn.replace_input_of(iff, 1, zero);\n+    _igvn.replace_node(get_from_cache, C->top());\n+\n+    return;\n+  }\n+\n+  Node* load_of_cache = get_from_cache->in(1);\n+\n+  Node* null_ptr = get_from_cache->in(2);\n+  Node* cache_not_null_cmp = new CmpPNode(load_of_cache, null_ptr);\n+  _igvn.register_new_node_with_optimizer(cache_not_null_cmp);\n+  Node* cache_not_null_bol = new BoolNode(cache_not_null_cmp, BoolTest::ne);\n+  _igvn.register_new_node_with_optimizer(cache_not_null_bol);\n+  set_subtree_ctrl(cache_not_null_bol, true);\n+  IfNode* cache_not_null_iff = new IfNode(iff->in(0), cache_not_null_bol, get_from_cache->prob(0),\n+                                          get_from_cache->cnt(0));\n+  IdealLoopTree* loop = get_loop(iff->in(0));\n+  register_control(cache_not_null_iff, loop, iff->in(0));\n+  Node* cache_not_null_proj = new IfTrueNode(cache_not_null_iff);\n+  register_control(cache_not_null_proj, loop, cache_not_null_iff);\n+  Node* cache_null_proj = new IfFalseNode(cache_not_null_iff);\n+  register_control(cache_null_proj, loop, cache_not_null_iff);\n+\n+  Node* not_null_load_of_cache = new CastPPNode(load_of_cache, _igvn.type(load_of_cache)->join(TypePtr::NOTNULL));\n+  not_null_load_of_cache->set_req(0, cache_not_null_proj);\n+  register_new_node(not_null_load_of_cache, cache_not_null_proj);\n+\n+  Node* mem = get_from_cache->mem();\n+\n+  Node* sv = get_from_cache->scoped_value();\n+  Node* hit_proj = nullptr;\n+  Node* failure_proj = nullptr;\n+  Node* res = nullptr;\n+  Node* success_region = new RegionNode(3);\n+  Node* success_phi = new PhiNode(success_region, TypeInstPtr::BOTTOM);\n+  Node* failure_region = new RegionNode(3);\n+  float first_prob = get_from_cache->prob(1);\n+  float first_cnt = get_from_cache->cnt(1);\n+  float second_prob = get_from_cache->prob(2);\n+  if (first_prob != PROB_UNKNOWN && second_prob != PROB_UNKNOWN) {\n+    second_prob = (1 - first_prob) * second_prob;\n+  }\n+  float second_cnt = get_from_cache->cnt(2);\n+\n+  if (second_index != C->top() && second_prob < first_prob) {\n+    swap(first_index, second_index);\n+    swap(first_prob, second_prob);\n+    second_prob = (1 - first_prob) * second_prob;\n+    if (first_cnt != COUNT_UNKNOWN && first_prob != PROB_UNKNOWN) {\n+      second_cnt = first_cnt * first_prob;\n+    }\n+  }\n+\n+  test_and_load_from_cache(not_null_load_of_cache, mem, first_index, cache_not_null_proj,\n+                           first_prob, first_cnt, sv, failure_proj, hit_proj, res);\n+  Node* success_region_dom = hit_proj;\n+  success_region->init_req(1, hit_proj);\n+  success_phi->init_req(1, res);\n+  if (second_index != C->top()) {\n+    test_and_load_from_cache(not_null_load_of_cache, mem, second_index, failure_proj,\n+                             second_prob, second_cnt, sv, failure_proj, hit_proj, res);\n+    success_region->init_req(2, hit_proj);\n+    success_phi->init_req(2, res);\n+    success_region_dom = success_region_dom->in(0);\n+  }\n+\n+  failure_region->init_req(1, cache_null_proj);\n+  failure_region->init_req(2, failure_proj);\n+\n+  register_control(success_region, loop, success_region_dom);\n+  register_control(failure_region, loop, cache_not_null_iff);\n+  register_new_node(success_phi, success_region);\n+\n+  Node* failure_path = failure->unique_ctrl_out();\n+\n+  lazy_replace(success, success_region);\n+  lazy_replace(failure, failure_region);\n+  if (load_from_cache != nullptr) {\n+    _igvn.replace_node(load_from_cache, success_phi);\n+  }\n+  _igvn.replace_node(get_from_cache, C->top());\n+}\n+\n+void PhaseIdealLoop::test_and_load_from_cache(Node* load_of_cache, Node* mem, Node* index, Node* c, float prob, float cnt,\n+                                              Node* sv, Node*& failure, Node*& hit, Node*& res) {\n+  BasicType bt = TypeAryPtr::OOPS->array_element_basic_type();\n+  uint shift  = exact_log2(type2aelembytes(bt));\n+  uint header = arrayOopDesc::base_offset_in_bytes(bt);\n+\n+  Node* header_offset = _igvn.MakeConX(header);\n+  set_ctrl(header_offset, C->root());\n+  Node* base  = new AddPNode(load_of_cache, load_of_cache, header_offset);\n+  _igvn.register_new_node_with_optimizer(base);\n+  Node* casted_idx = Compile::conv_I2X_index(&_igvn, index, nullptr, c);\n+  ConINode* shift_node = _igvn.intcon(shift);\n+  set_ctrl(shift_node, C->root());\n+  Node* scale = new LShiftXNode(casted_idx, shift_node);\n+  _igvn.register_new_node_with_optimizer(scale);\n+  Node* adr = new AddPNode(load_of_cache, base, scale);\n+  _igvn.register_new_node_with_optimizer(adr);\n+\n+  DecoratorSet decorators = C2_READ_ACCESS | IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD;\n+  C2AccessValuePtr addr(adr, TypeAryPtr::OOPS);\n+  C2OptAccess access(_igvn, c, mem, decorators, bt, load_of_cache, addr);\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* cache_load = bs->load_at(access, TypeAryPtr::OOPS->elem());\n+\n+  Node* cmp = new CmpPNode(cache_load, sv);\n+  _igvn.register_new_node_with_optimizer(cmp);\n+  Node* bol = new BoolNode(cmp, BoolTest::ne);\n+  _igvn.register_new_node_with_optimizer(bol);\n+  set_subtree_ctrl(bol, true);\n+  IfNode* iff = new IfNode(c, bol, prob, cnt);\n+  IdealLoopTree* loop = get_loop(c);\n+  register_control(iff, loop, c);\n+  failure = new IfTrueNode(iff);\n+  register_control(failure, loop, iff);\n+  hit = new IfFalseNode(iff);\n+  register_control(hit, loop, iff);\n+\n+  index = new AddINode(index, _igvn.intcon(1));\n+  _igvn.register_new_node_with_optimizer(index);\n+  casted_idx = Compile::conv_I2X_index(&_igvn, index, nullptr, hit);\n+  scale = new LShiftXNode(casted_idx, shift_node);\n+  _igvn.register_new_node_with_optimizer(scale);\n+  adr = new AddPNode(load_of_cache, base, scale);\n+  _igvn.register_new_node_with_optimizer(adr);\n+  C2AccessValuePtr addr_res(adr, TypeAryPtr::OOPS);\n+  C2OptAccess access_res(_igvn, c, mem, decorators, bt, load_of_cache, addr_res);\n+  res = bs->load_at(access_res, TypeAryPtr::OOPS->elem());\n+  set_subtree_ctrl(res, true);\n+}\n+\n+bool PhaseIdealLoop::optimize_scoped_value_get_nodes() {\n+  bool progress = false;\n+  for (uint i = _scoped_value_get_nodes.size(); i > 0; i--) {\n+    Node* n = _scoped_value_get_nodes.at(i - 1);\n+    if (n->Opcode() == Op_ScopedValueGetHitsInCache) {\n+      ScopedValueGetHitsInCacheNode* get_from_sv_cache = (ScopedValueGetHitsInCacheNode*)n;\n+      get_from_sv_cache->verify();\n+      ScopedValueGetLoadFromCacheNode* load_from_cache = get_from_sv_cache->load_from_cache();\n+      if (load_from_cache != nullptr) {\n+        load_from_cache->verify();\n+      }\n+      BoolNode* bol = get_from_sv_cache->find_unique_out_with(Op_Bool)->as_Bool();\n+      assert(bol->_test._test == BoolTest::ne, \"\");\n+      IfNode* iff = bol->find_unique_out_with(Op_If)->as_If();\n+      assert(load_from_cache == nullptr || load_from_cache->iff() == iff, \"\");\n+      for (uint j = 0; j < _scoped_value_get_nodes.size(); j++) {\n+        Node* m = _scoped_value_get_nodes.at(j);\n+        if (m == n) {\n+          continue;\n+        }\n+        assert(m != n, \"\");\n+        if (m->Opcode() == Op_ScopedValueGetHitsInCache) {\n+          ScopedValueGetHitsInCacheNode* get_from_sv_cache_dom = (ScopedValueGetHitsInCacheNode*) m;\n+          ScopedValueGetLoadFromCacheNode* load_from_cache_dom = get_from_sv_cache_dom->load_from_cache();\n+          BoolNode* bol_dom = get_from_sv_cache_dom->find_unique_out_with(Op_Bool)->as_Bool();\n+          assert(bol_dom->_test._test == BoolTest::ne, \"\");\n+          IfNode* iff_dom = bol_dom->find_unique_out_with(Op_If)->as_If();\n+          assert(load_from_cache_dom == nullptr || load_from_cache_dom->iff() == iff_dom, \"\");\n+          IfProjNode* dom_proj = iff_dom->proj_out(1)->as_IfProj();\n+          assert(load_from_cache_dom == nullptr || dom_proj == load_from_cache_dom->in(0), \"\");\n+          if (get_from_sv_cache_dom->scoped_value() == get_from_sv_cache->scoped_value() &&\n+              is_dominator(dom_proj, iff)) {\n+            \/\/ The success projection of a dominating ScopedValueGetHitsInCache dominates this ScopedValueGetHitsInCache\n+            \/\/ for the same ScopedValue object: replace this ScopedValueGetHitsInCache by the dominating one\n+            _igvn.replace_node(get_from_sv_cache, get_from_sv_cache_dom);\n+            if (load_from_cache_dom != nullptr && load_from_cache != nullptr) {\n+              _igvn.replace_node(load_from_cache, load_from_cache_dom);\n+            }\n+            dominated_by(dom_proj, iff, false, false);\n+            _igvn.replace_node(bol, C->top());\n+            progress = true;\n+            remove_scoped_value_get_at(i-1);\n+            break;\n+          }\n+        } else {\n+          ScopedValueGetResultNode* sv_get_result_dom = (ScopedValueGetResultNode*) m;\n+          if (sv_get_result_dom->scoped_value() == get_from_sv_cache->scoped_value() &&\n+              is_dominator(sv_get_result_dom, iff)) {\n+            \/\/ A ScopedValueGetResult dominates this ScopedValueGetHitsInCache for the same ScopedValue object:\n+            \/\/ the result of the dominating ScopedValue.get() makes this ScopedValueGetHitsInCache useless\n+            Node* one = _igvn.intcon(1);\n+            set_ctrl(one, C->root());\n+            _igvn.replace_input_of(iff, 1, one);\n+            if (load_from_cache != nullptr) {\n+              Node* result_out = sv_get_result_dom->result_out();\n+              if (result_out == nullptr) {\n+                result_out = new ProjNode(sv_get_result_dom, ScopedValueGetResultNode::Result);\n+                register_new_node(result_out, sv_get_result_dom);\n+              }\n+              _igvn.replace_node(load_from_cache, result_out);\n+            }\n+            _igvn.replace_node(get_from_sv_cache, C->top());\n+            progress = true;\n+            remove_scoped_value_get_at(i-1);\n+            break;\n+          }\n+        }\n+      }\n+    } else {\n+      assert(n->Opcode() == Op_ScopedValueGetResult, \"\");\n+      ScopedValueGetResultNode* sv_get_result = (ScopedValueGetResultNode*)n;\n+      for (uint j = 0; j < _scoped_value_get_nodes.size(); j++) {\n+        Node* m = _scoped_value_get_nodes.at(j);\n+        if (m == n) {\n+          continue;\n+        }\n+        if (m->Opcode() == Op_ScopedValueGetHitsInCache) {\n+          ScopedValueGetHitsInCacheNode* get_from_sv_cache_dom = (ScopedValueGetHitsInCacheNode*) m;\n+          ScopedValueGetLoadFromCacheNode* load_from_cache_dom = get_from_sv_cache_dom->load_from_cache();\n+          BoolNode* bol_dom = get_from_sv_cache_dom->find_unique_out_with(Op_Bool)->as_Bool();\n+          assert(bol_dom->_test._test == BoolTest::ne, \"\");\n+          IfNode* iff_dom = bol_dom->find_unique_out_with(Op_If)->as_If();\n+          assert(load_from_cache_dom == nullptr || load_from_cache_dom->iff() == iff_dom, \"\");\n+          IfProjNode* dom_proj = iff_dom->proj_out(1)->as_IfProj();\n+          assert(load_from_cache_dom == nullptr || dom_proj == load_from_cache_dom->in(0), \"\");\n+          if (get_from_sv_cache_dom->scoped_value() == sv_get_result->scoped_value() &&\n+              is_dominator(dom_proj, sv_get_result)) {\n+            \/\/ This ScopedValueGetResult is dominated by the success projection of ScopedValueGetHitsInCache for the same\n+            \/\/ ScopedValue object: either the ScopedValueGetResult and ScopedValueGetHitsInCache are from the same\n+            \/\/ ScopedValue.get() and we remove the ScopedValueGetResult because it's only useful to optimize\n+            \/\/ ScopedValue.get() where the slow path is taken. Or They are from difference ScopedValue.get() and we\n+            \/\/ remove the ScopedValueGetResult. Its companion ScopedValueGetHitsInCache should be removed as well as part\n+            \/\/ of this round of optimizations.\n+            lazy_replace(sv_get_result->control_out(), sv_get_result->in(0));\n+            ProjNode* result_out = sv_get_result->result_out();\n+            if (result_out != nullptr) {\n+              _igvn.replace_node(result_out, sv_get_result->in(ScopedValueGetResultNode::GetResult));\n+            }\n+            progress = true;\n+            remove_scoped_value_get_at(i-1);\n+            break;\n+          }\n+        } else {\n+          assert(m->Opcode() == Op_ScopedValueGetResult, \"\");\n+          ScopedValueGetResultNode* sv_get_result_dom = (ScopedValueGetResultNode*) m;\n+          if (sv_get_result_dom->scoped_value() == sv_get_result->scoped_value() &&\n+              is_dominator(sv_get_result_dom, sv_get_result)) {\n+            \/\/ This ScopedValueGetResult is dominated by another ScopedValueGetResult for the same ScopedValue object:\n+            \/\/ remove this one and use the result from the dominating ScopedValue.get()\n+            lazy_replace(sv_get_result->control_out(), sv_get_result->in(0));\n+            ProjNode* result_out = sv_get_result->result_out();\n+            if (result_out != nullptr) {\n+              _igvn.replace_node(result_out, sv_get_result->in(ScopedValueGetResultNode::GetResult));\n+            }\n+            progress = true;\n+            remove_scoped_value_get_at(i-1);\n+            break;\n+          }\n+        }\n+      }\n+    }\n+  }\n+  return progress;\n+}\n+\n+void PhaseIdealLoop::remove_scoped_value_get_at(uint i) {\n+  Node* top_of_stack = _scoped_value_get_nodes.pop();\n+  if (i < _scoped_value_get_nodes.size()) {\n+    _scoped_value_get_nodes.map(i, top_of_stack);\n+  }\n+}\n+\n@@ -6080,0 +6416,11 @@\n+  if (!_verify_only && (n->Opcode() == Op_ScopedValueGetResult || n->Opcode() == Op_ScopedValueGetHitsInCache)) {\n+#ifdef ASSERT\n+    if (n->Opcode() == Op_ScopedValueGetHitsInCache) {\n+      ((ScopedValueGetHitsInCacheNode*) n)->verify();\n+    } else if (n->Opcode() == Op_ScopedValueGetLoadFromCache) {\n+      ((ScopedValueGetLoadFromCacheNode*) n)->verify();\n+    }\n+#endif\n+    _scoped_value_get_nodes.push(n);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":348,"deletions":1,"binary":false,"changes":349,"status":"modified"},{"patch":"@@ -704,1 +704,1 @@\n-  bool policy_peeling(PhaseIdealLoop *phase);\n+  bool policy_peeling(PhaseIdealLoop* phase, bool scoped_value_only);\n@@ -706,1 +706,1 @@\n-  uint estimate_peeling(PhaseIdealLoop *phase);\n+  uint estimate_peeling(PhaseIdealLoop* phase, bool scoped_value_only);\n@@ -893,0 +893,1 @@\n+  Node_List _scoped_value_get_nodes;\n@@ -1740,0 +1741,21 @@\n+  void expand_get_from_sv_cache(ScopedValueGetHitsInCacheNode* get_from_cache);\n+  void test_and_load_from_cache(Node* load_of_cache, Node* mem, Node* index, Node* c, float prob, float cnt,\n+                                Node* sv, Node*& failure, Node*& hit, Node*& res);\n+\n+  bool is_uncommon_trap_if_pattern(IfProjNode* proj);\n+\n+  bool optimize_scoped_value_get_nodes();\n+\n+  bool expand_scoped_value_get_nodes();\n+\n+  void remove_scoped_value_get_at(uint i);\n+\n+  bool\n+  loop_predication_for_scoped_value_get(IdealLoopTree* loop, IfProjNode* if_success_proj,\n+                                        ParsePredicateSuccessProj* parse_predicate_proj,\n+                                        Invariance &invar, Deoptimization::DeoptReason reason,\n+                                        IfNode* iff, IfProjNode*&new_predicate_proj);\n+\n+  void\n+  move_scoped_value_nodes_to_not_peel(VectorSet &peel, VectorSet &not_peel, Node_List &peel_list,\n+                                      Node_List &sink_list, uint i) const;\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -1655,1 +1656,3 @@\n-      !n->is_Type()) {\n+      !n->is_Type() &&\n+      \/\/ ScopedValueGetLoadFromCache and companion ScopedValueGetHitsInCacheNode must stay together\n+      n->Opcode() != Op_ScopedValueGetLoadFromCache) {\n@@ -3728,0 +3731,4 @@\n+          } else if (n->Opcode() == Op_ScopedValueGetHitsInCache) {\n+            \/\/ ScopedValueGetLoadFromCache and companion ScopedValueGetHitsInCacheNode must stay together\n+            move_scoped_value_nodes_to_not_peel(peel, not_peel, peel_list, sink_list, i);\n+            incr = false;\n@@ -3925,0 +3932,16 @@\n+void PhaseIdealLoop::move_scoped_value_nodes_to_not_peel(VectorSet &peel, VectorSet &not_peel, Node_List &peel_list,\n+                                                         Node_List &sink_list, uint i) const {\n+  ScopedValueGetHitsInCacheNode* sv_hits_in_cache = (ScopedValueGetHitsInCacheNode*)peel_list.at(i);\n+  sv_hits_in_cache->verify();\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = sv_hits_in_cache->load_from_cache();\n+  assert(load_from_cache == nullptr || not_peel.test(load_from_cache->_idx), \"\");\n+  Node* bol = sv_hits_in_cache->find_unique_out_with(Op_Bool);\n+  assert(not_peel.test(bol->_idx), \"\");\n+  Node* iff = bol->unique_ctrl_out();\n+  assert(not_peel.test(iff->_idx), \"\");\n+  sink_list.push(sv_hits_in_cache);\n+  peel.remove(sv_hits_in_cache->_idx);\n+  not_peel.set(sv_hits_in_cache->_idx);\n+  peel_list.remove(i);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -230,0 +230,41 @@\n+\n+bool ProjNode::is_multi_uncommon_trap_if_pattern() {\n+  Node* iff = in(0);\n+  if (!iff->is_If() || iff->outcnt() < 2) {\n+    \/\/ Not a projection of an If or variation of a dead If node.\n+    return false;\n+  }\n+  return other_if_proj()->is_multi_uncommon_trap_proj();\n+}\n+\n+bool ProjNode::is_multi_uncommon_trap_proj() {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(this);\n+  const int path_limit = 100;\n+  uint unc_count = 0;\n+  for (uint i = 0; i < wq.size(); ++i) {\n+    Node* n = wq.at(i);\n+    if (n->is_CallStaticJava()) {\n+      CallStaticJavaNode* call = n->as_CallStaticJava();\n+      int req = call->uncommon_trap_request();\n+      if (req == 0) {\n+        return false;\n+      }\n+      unc_count++;\n+    } else if (n->is_Region() || n->is_If() || n->is_IfProj()) {\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* u = n->fast_out(j);\n+        if (u->is_CFG()) {\n+          if (wq.size() >= path_limit) {\n+            return false;\n+          }\n+          wq.push(u);\n+        }\n+      }\n+    } else if (n->Opcode() != Op_Halt) {\n+      return false;\n+    }\n+  }\n+  return unc_count > 0;\n+}\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -103,0 +103,4 @@\n+  \/\/ Return true if this projection doesn't end with an uncommon trap but, even though several cfg paths are branching out\n+  \/\/ from here, they all end with an uncommon trap\n+  bool is_multi_uncommon_trap_proj();\n+  bool is_multi_uncommon_trap_if_pattern();\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -977,0 +977,12 @@\n+Node* Node::find_unique_out_with(int opcode) const {\n+  Node* res = nullptr;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* use = fast_out(i);\n+    if (use->Opcode() == opcode) {\n+      assert(res == nullptr, \"only one match\");\n+      res = use;\n+    }\n+  }\n+  return res;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -495,0 +495,1 @@\n+  Node* find_unique_out_with(int opcode) const;\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  flags(INCREMENTAL_SCOPED_VALUE_INLINE, \"Incremental Scoped Value Inline\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -1968,0 +1969,13 @@\n+\n+#ifdef ASSERT\n+void ScopedValueGetHitsInCacheNode::verify() const {\n+  ScopedValueGetLoadFromCacheNode* load = load_from_cache();\n+  if (load != nullptr) {\n+    assert(load->in(0)->Opcode() == Op_IfTrue, \"\");\n+    assert(load->in(0)->in(0)->in(1)->is_Bool(), \"\");\n+    assert(load->in(0)->in(0)->in(1)->in(1) == this, \"\");\n+  }\n+}\n+#endif\n+\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -300,0 +300,83 @@\n+\/\/ Does a ScopedValue.get() hits in the cache?\n+class ScopedValueGetLoadFromCacheNode;\n+class ScopedValueGetHitsInCacheNode : public CmpNode {\n+private:\n+  \/\/ There are multiple checks involved, keep track of their profile data\n+  struct {\n+      float _cnt;\n+      float _prob;\n+  } _profile_data[3];\n+\n+  virtual uint size_of() const { return sizeof(*this); }\n+  uint hash() const { return NO_HASH; }\n+\n+public:\n+  enum {\n+      ScopedValue = 3, \/\/ What ScopedValue object is it for?\n+      Memory, \/\/ Memory for the cache loads\n+      Index1, \/\/ index for the first check\n+      Index2  \/\/ index for the second check\n+  };\n+\n+  ScopedValueGetHitsInCacheNode(Compile* C, Node* c, Node* scoped_value_cache, Node* null_con, Node* mem, Node* sv,\n+                                Node* index1, Node* index2) :\n+          CmpNode(scoped_value_cache, null_con) {\n+    init_req(0, c);\n+    assert(req() == ScopedValue, \"\");\n+    add_req(sv);\n+    assert(req() == Memory, \"\");\n+    add_req(mem);\n+    assert(req() == Index1, \"\");\n+    add_req(index1);\n+    assert(req() == Index2, \"\");\n+    add_req(index2);\n+  }\n+\n+  Node* scoped_value() const {\n+    return in(ScopedValue);\n+  }\n+\n+  Node* mem() const {\n+    return in(Memory);\n+  }\n+\n+  Node* index1() const {\n+    return in(Index1);\n+  }\n+\n+  Node* index2() const {\n+    return in(Index2);\n+  }\n+\n+  ScopedValueGetLoadFromCacheNode* load_from_cache() const {\n+    return (ScopedValueGetLoadFromCacheNode*)find_unique_out_with(Op_ScopedValueGetLoadFromCache);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+\n+  const Type* sub(const Type* type, const Type* type1) const {\n+    return CmpNode::bottom_type();\n+  }\n+  void set_profile_data(uint i, float cnt, float prob) {\n+    assert(i < sizeof(_profile_data) \/ sizeof(_profile_data[0]), \"\");\n+    _profile_data[i]._cnt = cnt;\n+    _profile_data[i]._prob = prob;\n+  }\n+\n+ float prob(uint i) const {\n+    assert(i < sizeof(_profile_data) \/ sizeof(_profile_data[0]), \"\");\n+    return _profile_data[i]._prob;\n+  }\n+\n+ float cnt(uint i) const {\n+    assert(i < sizeof(_profile_data) \/ sizeof(_profile_data[0]), \"\");\n+    return _profile_data[i]._cnt;\n+  }\n+\n+  void verify() const PRODUCT_RETURN;\n+\n+  virtual bool depends_only_on_test() const {\n+    return false;\n+  }\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":83,"deletions":0,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -617,0 +617,10 @@\n+  const Type **fgetfromcache =(const Type**)shared_type_arena->AmallocWords(3*sizeof(Type*));\n+  fgetfromcache[0] = TypeInt::BOOL;\n+  fgetfromcache[1] = TypeInstPtr::BOTTOM;\n+  fgetfromcache[2] = TypeAryPtr::OOPS;\n+  TypeTuple::make(3, fgetfromcache);\n+  const Type **fsvgetresult =(const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n+  fsvgetresult[0] = Type::CONTROL;\n+  fsvgetresult[1] = TypeInstPtr::BOTTOM;\n+  TypeTuple::SV_GET_RESULT = TypeTuple::make(2, fsvgetresult);\n+\n@@ -2125,0 +2135,1 @@\n+const TypeTuple* TypeTuple::SV_GET_RESULT;\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -749,0 +749,1 @@\n+  static const TypeTuple *SV_GET_RESULT;\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -677,0 +678,1 @@\n+    @IntrinsicCandidate\n@@ -697,0 +699,1 @@\n+    @IntrinsicCandidate\n@@ -774,0 +777,1 @@\n+    @ForceInline\n@@ -957,0 +961,1 @@\n+        @IntrinsicCandidate\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ScopedValue.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,712 @@\n+\/*\n+ * Copyright (c) 2023, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.c2.irTests;\n+\n+import compiler.lib.ir_framework.*;\n+import jdk.test.whitebox.WhiteBox;\n+import java.lang.reflect.Method;\n+import compiler.whitebox.CompilerWhiteBoxTest;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\/*\n+ * @test\n+ * @bug 8320649\n+ * @library \/test\/lib \/\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @compile --enable-preview -source ${jdk.version} TestScopedValue.java\n+ * @run main\/othervm --enable-preview -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.c2.irTests.TestScopedValue\n+ *\/\n+\n+public class TestScopedValue {\n+\n+    protected static final WhiteBox WHITE_BOX = WhiteBox.getWhiteBox();\n+\n+    static ScopedValue<MyDouble> sv = ScopedValue.newInstance();\n+    static final ScopedValue<MyDouble> svFinal = ScopedValue.newInstance();\n+    static ScopedValue<Object> svObject = ScopedValue.newInstance();\n+    private static volatile int volatileField;\n+\n+    public static void main(String[] args) {\n+        \/\/ Fast path tests need to be run one at a time to prevent profile pollution\n+        List<String> tests = List.of(\"testFastPath1\", \"testFastPath2\", \"testFastPath3\", \"testFastPath5\",\n+                \"testFastPath6\", \"testFastPath7\", \"testFastPath8\", \"testFastPath9\", \"testFastPath10\",\n+                \"testFastPath11\", \"testFastPath12\", \"testFastPath13\",\"testFastPath14\",\n+                \"testSlowPath1,testSlowPath2,testSlowPath3,testSlowPath4,testSlowPath5,testSlowPath6,testSlowPath7,testSlowPath8,testSlowPath9\");\n+        for (String test : tests) {\n+            TestFramework.runWithFlags(\"--enable-preview\", \"-XX:CompileCommand=dontinline,java.lang.ScopedValue::slowGet\", \"-DTest=\" + test);\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath1() {\n+        MyDouble sv1 = sv.get();\n+        MyDouble sv2 = sv.get(); \/\/ Should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testFastPath1\", mode = RunMode.STANDALONE)\n+    private void testFastPath1Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath1() != 42 + 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath1\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @DontInline\n+    static void notInlined() {\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath2() {\n+        ScopedValue<MyDouble> scopedValue = sv;\n+        MyDouble sv1 = scopedValue.get();\n+        notInlined();\n+        MyDouble sv2 = scopedValue.get(); \/\/ Should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testFastPath2\", mode = RunMode.STANDALONE)\n+    private void testFastPath2Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath2() != 42 + 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath2\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"2\" })\n+    public static double testFastPath3() {\n+        MyDouble sv1 = sv.get();\n+        notInlined();\n+        MyDouble sv2 = sv.get(); \/\/ Doesn't optimize out (load of sv cannot common)\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testFastPath3\", mode = RunMode.STANDALONE)\n+    private void testFastPath3Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath3() != 42 + 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath3\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    \/\/ Split if test but it doesn't trigger at this point\n+    \/\/ @Test\n+    \/\/ @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    \/\/ @IR(counts = {IRNode.LOAD_L, \"2\" })\n+    \/\/ public static long test4(boolean flag) throws Exception {\n+    \/\/     ScopedValue<MyLong> scopedValue;\n+    \/\/     MyLong long1;\n+    \/\/     if (flag) {\n+    \/\/         scopedValue = svFinal;\n+    \/\/         long1 = (MyLong)svFinal.get();\n+    \/\/     } else {\n+    \/\/         scopedValue = svFinal2;\n+    \/\/         long1 = (MyLong)svFinal2.get();\n+    \/\/     }\n+    \/\/     MyLong long2 = (MyLong)scopedValue.get();\n+    \/\/     return long1.getValue() + long2.getValue();\n+    \/\/ }\n+\n+    \/\/ @Run(test = \"test4\", mode = RunMode.STANDALONE)\n+    \/\/ private void test4Runner() throws Exception {\n+    \/\/     ScopedValue.where(svFinal, new MyLong(42)).where(svFinal2, new MyLong(42)).run(\n+    \/\/             () -> {\n+    \/\/                 try {\n+    \/\/                     MyLong unused = (MyLong)svFinal.get();\n+    \/\/                     unused = (MyLong)svFinal2.get();\n+    \/\/                     for (int i = 0; i < 20_000; i++) {\n+    \/\/                         if (test4(true) != 42 + 42) {\n+    \/\/                             throw new RuntimeException();\n+    \/\/                         }\n+    \/\/                         if (test4(false) != 42 + 42) {\n+    \/\/                             throw new RuntimeException();\n+    \/\/                         }\n+    \/\/                     }\n+    \/\/                 } catch(Exception ex) {}\n+    \/\/             });\n+    \/\/     Method m = TestScopedValue.class.getDeclaredMethod(\"test4\", boolean.class);\n+    \/\/     WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+    \/\/     if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+    \/\/         throw new RuntimeException(\"should be compiled\");\n+    \/\/     }\n+    \/\/ }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\", IRNode.LOOP, IRNode.COUNTED_LOOP})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath5() {\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            res = sv.get().getValue(); \/\/ should be hoisted out of loop and loop should optimize out\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath5\", mode = RunMode.STANDALONE)\n+    private void testFastPath5Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath5() != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath5\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 5\" })\n+    public static void testFastPath6() {\n+        Object unused = svObject.get(); \/\/ cannot be removed if result not used\n+    }\n+\n+    @Run(test = \"testFastPath6\", mode = RunMode.STANDALONE)\n+    private void testFastPath6Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath6();\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath6\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    static Object testFastPath7Field;\n+    @ForceInline\n+    static void testFastPath7Helper(int i, Object o) {\n+        if (i != 10) {\n+            testFastPath7Field = o;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 5\" })\n+    public static void testFastPath7() {\n+        Object unused = svObject.get(); \/\/ cannot be removed even if result not used (after opts)\n+        int i;\n+        for (i = 0; i < 10; i++);\n+        testFastPath7Helper(i, unused);\n+    }\n+\n+    @Run(test = \"testFastPath7\", mode = RunMode.STANDALONE)\n+    private void testFastPath7Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath7();\n+                        testFastPath7Helper(9, null);\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath7\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\", IRNode.LOOP, IRNode.COUNTED_LOOP})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath8(boolean[] flags) {\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            if (flags[i]) {\n+                res = sv.get().getValue(); \/\/ Should be hoisted by predication\n+            } else {\n+                res = sv.get().getValue(); \/\/ should be hoisted by predication\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath8\", mode = RunMode.STANDALONE)\n+    private void testFastPath8Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath8(allTrue) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                        if (testFastPath8(allFalse) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath8\", boolean[].class);\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath9(boolean[] flags) {\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            notInlined();\n+            if (flags[i]) {\n+                res = svFinal.get().getValue(); \/\/ should be hoisted by predication\n+            } else {\n+                res = svFinal.get().getValue(); \/\/ should be hoisted by predication\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath9\", mode = RunMode.STANDALONE)\n+    private void testFastPath9Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(svFinal, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = svFinal.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath9(allTrue) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                        if (testFastPath9(allFalse) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath9\", boolean[].class);\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    public static Object testFastPath10(boolean[] flags) {\n+        \/\/ result of get() is candidate for sinking\n+        Object res = null;\n+        for (int i = 0; i < 10_000; i++) {\n+            notInlined();\n+            res = svObject.get();\n+            if (flags[i]) {\n+                break;\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath10\", mode = RunMode.STANDALONE)\n+    private void testFastPath10Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(svObject, new MyDouble(42)).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath10(allTrue);\n+                        testFastPath10(allFalse);\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath10\", boolean[].class);\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    public static Object testFastPath11(boolean[] flags) {\n+        for (int i = 0; i < 10_000; i++) {\n+            volatileField = 0x42;\n+            final boolean flag = flags[i];\n+            Object res = svObject.get(); \/\/ result used out of loop\n+            if (flag) {\n+                return res;\n+            }\n+        }\n+        return null;\n+    }\n+\n+    @Run(test = \"testFastPath11\", mode = RunMode.STANDALONE)\n+    private void testFastPath11Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(svObject, new MyDouble(42)).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath11(allTrue);\n+                        testFastPath11(allFalse);\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath11\", boolean[].class);\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    public static Object testFastPath12() {\n+        \/\/ test commoning when the result of one is unused\n+        Object unused = svObject.get();\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testFastPath12\", mode = RunMode.STANDALONE)\n+    private void testFastPath12Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath12();\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath12\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    public static Object testFastPath13() {\n+        \/\/ test commoning when the result of one is unused\n+        int i;\n+        for (i = 0; i < 10; i++) {\n+\n+        }\n+        final Object result = testFastPath13Inlined(i);\n+        Object unused = svObject.get();\n+        return result;\n+    }\n+\n+    @ForceInline\n+    private static Object testFastPath13Inlined(int i) {\n+        Object result = null;\n+        if (i == 10) {\n+            result = svObject.get();\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"testFastPath13\", mode = RunMode.STANDALONE)\n+    private void testFastPath13Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath13();\n+                        testFastPath13Inlined(0);\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath13\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 6\" })\n+    public static Object testFastPath14() {\n+        \/\/ checks code shape once fully expanded\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testFastPath14\", mode = RunMode.STANDALONE)\n+    private void testFastPath14Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath14();\n+                    }\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath14\");\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        if (!WHITE_BOX.isMethodCompiled(m) || WHITE_BOX.getMethodCompilationLevel(m) != CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION) {\n+            throw new RuntimeException(\"should be compiled\");\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_D, \"1\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static double testSlowPath1() {\n+        ScopedValue<MyDouble> localSV = sv;\n+        MyDouble sv1 = localSV.get();\n+        MyDouble sv2 = localSV.get(); \/\/ should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testSlowPath1\")\n+    private void testSlowPath1Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath1() != 42 + 42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_D, \"1\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static double testSlowPath2() {\n+        ScopedValue<MyDouble> localSV = sv;\n+        MyDouble sv1 = localSV.get();\n+        notInlined();\n+        MyDouble sv2 = localSV.get(); \/\/ should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testSlowPath2\")\n+    private void testSlowPath2Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath2() != 42 + 42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static void testSlowPath3() {\n+        Object unused = svObject.get(); \/\/ Can't be optimized out even tough result is unused\n+    }\n+\n+    @Run(test = \"testSlowPath3\")\n+    private void testSlowPath3Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath3();\n+                });\n+    }\n+\n+    static Object testSlowPath4Field;\n+    @ForceInline\n+    static void testSlowPath4Helper(int i, Object o) {\n+        if (i != 10) {\n+            testSlowPath4Field = o;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static void testSlowPath4() {\n+        Object unused = svObject.get(); \/\/ Can't be optimized out even tough result is unused (after opts)\n+        int i;\n+        for (i = 0; i < 10; i++);\n+        testSlowPath4Helper(i, unused);\n+    }\n+\n+    @Run(test = \"testSlowPath4\")\n+    private void testSlowPath4Runner() throws Exception {\n+        testSlowPath4Helper(9, null);\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath4();\n+                });\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.LOOP, IRNode.COUNTED_LOOP})\n+    @IR(counts = {IRNode.LOAD_D, \"1\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static double testSlowPath5() {\n+        ScopedValue<MyDouble> localSV = sv;\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            res = localSV.get().getValue(); \/\/ one iteration of the loop should be peeled to optimize get() out of loop\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testSlowPath5\")\n+    private void testSlowPath5Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath5() != 42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+\n+    @Test\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"2\" })\n+    public static double testSlowPath6() {\n+        \/\/ Should not optimize because of where() call\n+        ScopedValue<MyDouble> localSV = sv;\n+        MyDouble sv1 = localSV.get();\n+        MyDouble sv2 = ScopedValue.where(sv, new MyDouble(0x42)).get(() -> localSV.get());\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testSlowPath6\")\n+    private void testSlowPath6Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath6() != 42 + 0x42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static Object testSlowPath7() {\n+        \/\/ test optimization of redundant get() when one doesn't use its result\n+        final ScopedValue<Object> scopedValue = svObject;\n+        Object unused = scopedValue.get();\n+        return scopedValue.get();\n+    }\n+\n+    @Run(test = \"testSlowPath7\")\n+    private void testSlowPath7Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath7();\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static Object testSlowPath8() {\n+        \/\/ test optimization of redundant get() when one doesn't use its result\n+        final ScopedValue<Object> scopedValue = svObject;\n+        Object result = scopedValue.get();\n+        Object unused = scopedValue.get();\n+        return result;\n+    }\n+\n+    @Run(test = \"testSlowPath8\")\n+    private void testSlowPath8Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath8();\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">=3\", IRNode.LOAD_P_OR_N, \">=5\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    @IR(counts = {IRNode.IF, \"<=4\", IRNode.LOAD_P_OR_N, \"<=7\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static Object testSlowPath9() {\n+        \/\/ Test right pattern once fully expanded\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testSlowPath9\")\n+    private void testSlowPath9Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath9();\n+                });\n+    }\n+\n+    static class MyDouble {\n+        final private double value;\n+\n+        public MyDouble(long value) {\n+            this.value = value;\n+        }\n+\n+        @ForceInline\n+        public double getValue() {\n+            return value;\n+        }\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestScopedValue.java","additions":712,"deletions":0,"binary":false,"changes":712,"status":"added"},{"patch":"@@ -678,0 +678,5 @@\n+    public static final String LOAD_P_OR_N = PREFIX + \"LOAD_P_OR_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(LOAD_P_OR_N, \"Load[PN]\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}