{"files":[{"patch":"@@ -273,0 +273,3 @@\n+  case vmIntrinsics::_ScopedValue_get:\n+  case vmIntrinsics::_ScopedValue_slowGet:\n+  case vmIntrinsics::_ScopedValueCache_invalidate:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -297,0 +297,5 @@\n+  do_intrinsic(_ScopedValue_get,         java_lang_ScopedValue,   get_name, void_object_signature, F_R)                 \\\n+  do_intrinsic(_ScopedValue_slowGet,     java_lang_ScopedValue,   slowGet_name, void_object_signature, F_R)             \\\n+   do_name(     slowGet_name,                                    \"slowGet\")                                             \\\n+  do_intrinsic(_ScopedValueCache_invalidate, java_lang_ScopedValue_Cache, invalidate_name, int_void_signature, F_S)     \\\n+   do_name(     invalidate_name,                                 \"invalidate\")                                          \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -125,0 +125,1 @@\n+  template(java_lang_ScopedValue_Cache,               \"java\/lang\/ScopedValue$Cache\")              \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-    MergeMemNode* mm = opt_access.mem();\n+    MergeMemNode* mm = opt_access.mem()->as_MergeMem();\n@@ -171,1 +171,1 @@\n-    MergeMemNode* mm = opt_access.mem();\n+    Node* mem = opt_access.mem();\n@@ -173,1 +173,3 @@\n-    Node* mem = mm->memory_at(gvn.C->get_alias_index(adr_type));\n+    if (mem->is_MergeMem()) {\n+      mem = mem->as_MergeMem()->memory_at(gvn.C->get_alias_index(adr_type));\n+    }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -190,1 +190,1 @@\n-  MergeMemNode* _mem;\n+  Node* _mem;\n@@ -194,1 +194,1 @@\n-  C2OptAccess(PhaseGVN& gvn, Node* ctl, MergeMemNode* mem, DecoratorSet decorators,\n+  C2OptAccess(PhaseGVN& gvn, Node* ctl, Node* mem, DecoratorSet decorators,\n@@ -201,1 +201,1 @@\n-  MergeMemNode* mem() const { return _mem; }\n+  Node* mem() const { return _mem; }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -711,0 +713,2 @@\n+    process_result(kit);\n+\n@@ -819,0 +823,683 @@\n+\/\/ Inline ScopedValue.get() call, pattern match the resulting subgraph, transform the subgraph to make it more amenable\n+\/\/ to optimizations.\n+class LateInlineScopedValueCallGenerator : public LateInlineCallGenerator {\n+ private:\n+  bool _process_result;\n+  Node* _scoped_value_object;\n+\n+  class ScopedValueGetPatternMatcher : public StackObj {\n+   private:\n+    GraphKit& _kit;\n+    Node* _scoped_value_object;\n+    CallNode* _scoped_value_cache; \/\/ call to Thread.scopedValueCache()\n+    IfNode* _cache_not_null_iff; \/\/ test that scopedValueCache() is not null\n+    IfNode* _first_cache_probe_iff; \/\/ test for a hit in the cache with first hash\n+    IfNode* _second_cache_probe_iff; \/\/ test for a hit in the cache with second hash\n+    Node* _first_index_in_cache; \/\/ index in the cache for first hash\n+    Node* _second_index_in_cache; \/\/ index in the cache for second hash\n+    CallStaticJavaNode* _slow_call; \/\/ slowGet() call if any\n+\n+    bool match_cache_null_check_with_input(Node* maybe_cache, Node* maybe_nullptr, IfNode* iff) {\n+      if (!maybe_cache->is_Proj() ||\n+          !maybe_cache->in(0)->is_Call() ||\n+          maybe_cache->in(0)->as_CallJava()->method()->intrinsic_id() != vmIntrinsics::_scopedValueCache) {\n+        return false;\n+      }\n+      assert(maybe_nullptr->bottom_type() == TypePtr::NULL_PTR, \"should be a test with null\");\n+      assert(_cache_not_null_iff == nullptr, \"should only find one get_cache_if\");\n+      _cache_not_null_iff = iff;\n+      assert(_scoped_value_cache == nullptr || _scoped_value_cache == maybe_cache->in(0),\n+             \"should only find one scoped_value_cache\");\n+      _scoped_value_cache = maybe_cache->in(0)->as_Call();\n+      return true;\n+    }\n+\n+    \/\/ Pattern matches:\n+    \/\/ if ((objects = scopedValueCache()) != null) {\n+    bool match_cache_null_check(Node* maybe_iff) {\n+      if (maybe_iff->Opcode() != Op_If) {\n+        return false;\n+      }\n+      IfNode* iff = maybe_iff->as_If();\n+      BoolNode* bol = iff->in(1)->as_Bool();\n+      Node* cmp = bol->in(1);\n+      assert(cmp->Opcode() == Op_CmpP, \"only reference comparisons in ScopedValue.get()\");\n+      Node* cmp_in1 = cmp->in(1)->uncast();\n+      Node* cmp_in2 = cmp->in(2)->uncast();\n+      if (match_cache_null_check_with_input(cmp_in1, cmp_in2, iff)) {\n+        return true;\n+      }\n+      if (match_cache_null_check_with_input(cmp_in2, cmp_in1, iff)) {\n+        return true;\n+      }\n+      return false;\n+    }\n+\n+    \/\/ Pattern matches:\n+    \/\/ if (objects[n] == this) {\n+    bool match_cache_probe(Node* maybe_iff) {\n+      if (maybe_iff->Opcode() != Op_If) {\n+        return false;\n+      }\n+      BoolNode* bol = maybe_iff->in(1)->as_Bool();\n+      Node* cmp = bol->in(1);\n+      assert(cmp->Opcode() == Op_CmpP, \"only reference comparisons cache_array_load ScopedValue.get()\");\n+      Node* cmp_in1 = cmp->in(1)->uncast();\n+      Node* cmp_in2 = cmp->in(2)->uncast();\n+      Node* uncasted_scoped_value_object = _scoped_value_object->uncast();\n+      assert(cmp_in1 == uncasted_scoped_value_object || cmp_in2 == uncasted_scoped_value_object,\n+             \"one of the comparison inputs must be the scoped value oop\");\n+      Node* cache_array_load = cmp_in1 == uncasted_scoped_value_object ? cmp_in2 : cmp_in1;\n+      BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+      cache_array_load = bs->step_over_gc_barrier(cache_array_load);\n+      if (cache_array_load->Opcode() == Op_DecodeN) {\n+        cache_array_load = cache_array_load->in(1);\n+      }\n+      assert(cache_array_load->Opcode() == Op_LoadP || cache_array_load->Opcode() == Op_LoadN,\n+             \"load from cache array expected\");\n+      assert(_kit.C->get_alias_index(cache_array_load->adr_type()) == _kit.C->get_alias_index(TypeAryPtr::OOPS),\n+             \"load from cache array expected\");\n+      AddPNode* array_cache_load_adr = cache_array_load->in(MemNode::Address)->as_AddP();\n+      ProjNode* scoped_value_cache_proj = array_cache_load_adr->in(AddPNode::Base)->uncast()->as_Proj();\n+      assert(scoped_value_cache_proj->in(0)->as_CallJava()->method()->intrinsic_id() == vmIntrinsics::_scopedValueCache,\n+             \"should be call to Thread.scopedValueCache()\");\n+      assert(_scoped_value_cache == nullptr || _scoped_value_cache == scoped_value_cache_proj->in(0),\n+             \"only one cache expected\");\n+      _scoped_value_cache = scoped_value_cache_proj->in(0)->as_Call();\n+      assert(cache_array_load->in(MemNode::Memory)->is_Proj() &&\n+             cache_array_load->in(MemNode::Memory)->in(0) == _scoped_value_cache,\n+             \"load from cache expected right after Thread.scopedValueCache() call\");\n+      Node* second_addp_for_array_cache_load_adr = array_cache_load_adr->in(AddPNode::Address);\n+      Node* array_cache_load_offset = array_cache_load_adr->in(AddPNode::Offset);\n+      intptr_t array_cache_load_const_offset = array_cache_load_offset->find_intptr_t_con(-1);\n+      BasicType bt = TypeAryPtr::OOPS->array_element_basic_type();\n+      int shift_for_cache_array_load = exact_log2(type2aelembytes(bt));\n+      int header_size_for_cache_array_load = arrayOopDesc::base_offset_in_bytes(bt);\n+      assert(array_cache_load_const_offset >= header_size_for_cache_array_load,\n+             \"load from cache doesn't access the cache array?\");\n+      intptr_t array_cache_load_offset_in_body = array_cache_load_const_offset - header_size_for_cache_array_load;\n+\n+      Node* index_in_cache_array = _kit.gvn().intcon(\n+              checked_cast<int>(array_cache_load_offset_in_body >> shift_for_cache_array_load));\n+      if (second_addp_for_array_cache_load_adr->is_AddP()) {\n+        assert(!second_addp_for_array_cache_load_adr->in(AddPNode::Address)->is_AddP() &&\n+               second_addp_for_array_cache_load_adr->in(AddPNode::Base) == array_cache_load_adr->in(AddPNode::Base),\n+               \"only 2 AddPs for address computation\");\n+        Node* array_cache_load_offset_from_second_addp = second_addp_for_array_cache_load_adr->in(AddPNode::Offset);\n+        assert(array_cache_load_offset_from_second_addp->Opcode() == Op_LShiftX &&\n+               array_cache_load_offset_from_second_addp->in(2)->find_int_con(-1) == shift_for_cache_array_load,\n+               \"Not an array access?\");\n+        Node* array_cache_load_index_from_second_addp = array_cache_load_offset_from_second_addp->in(1);\n+#ifdef _LP64\n+        assert(array_cache_load_index_from_second_addp->Opcode() == Op_ConvI2L,\n+               \"unexpected address calculation shape\");\n+        array_cache_load_index_from_second_addp = array_cache_load_index_from_second_addp->in(1);\n+        assert(!(array_cache_load_index_from_second_addp->Opcode() == Op_CastII &&\n+                 array_cache_load_index_from_second_addp->in(0)->is_Proj() &&\n+                 array_cache_load_index_from_second_addp->in(0)->in(0) == _cache_not_null_iff),\n+               \"no CastII because index_in_cache_array is known to be positive\");\n+#endif\n+        index_in_cache_array = _kit.gvn().transform(new AddINode(array_cache_load_index_from_second_addp, index_in_cache_array));\n+      }\n+\n+      if (_first_cache_probe_iff == nullptr) {\n+        _first_cache_probe_iff = maybe_iff->as_If();\n+        _first_index_in_cache = index_in_cache_array;\n+      } else {\n+        assert(_second_cache_probe_iff == nullptr, \"no more than 2 cache probes\");\n+        _second_cache_probe_iff = maybe_iff->as_If();\n+        _second_index_in_cache = index_in_cache_array;\n+      }\n+      return true;\n+    }\n+\n+    \/\/ First traversal of the get() subgraph starts from the end of the method and follows control paths until it reaches\n+    \/\/ the Thread.scopedValueCache() call. Given the shape of the method and some paths may have been trimmed and end with\n+    \/\/ an uncommon trap, it could reach either the first or the second cache probe if first. Figure out which is the first\n+    \/\/ here.\n+    void adjust_order_of_first_and_second_probe_if(const Unique_Node_List &scoped_value_get_subgraph) {\n+      if (_second_cache_probe_iff == nullptr) {\n+        return;\n+      }\n+      assert(_first_cache_probe_iff != nullptr, \"can't have a second iff if there's no first one\");\n+      ResourceMark rm;\n+      Node_Stack stack(0);\n+      stack.push(_cache_not_null_iff, 0);\n+      while (stack.is_nonempty()) {\n+        Node* c = stack.node();\n+        assert(c->is_CFG(), \"only cfg nodes\");\n+        uint i = stack.index();\n+        if (i < c->outcnt()) {\n+          stack.set_index(i + 1);\n+          Node* u = c->raw_out(i);\n+          if (scoped_value_get_subgraph.member(u) && u != c) {\n+            if (u == _first_cache_probe_iff) {\n+              return;\n+            } else if (u == _second_cache_probe_iff) {\n+              swap(_first_cache_probe_iff, _second_cache_probe_iff);\n+              swap(_first_index_in_cache, _second_index_in_cache);\n+              return;\n+            }\n+            stack.push(u, 0);\n+          }\n+        } else {\n+          stack.pop();\n+        }\n+      }\n+      fatal(\"should have found the cache probe ifs\");\n+    }\n+\n+    \/\/ ScopedValue.get() probes 2 cache locations. If, when pattern matching the get() subgraph, we found 2 ifs, then the\n+    \/\/ first and second locations were probed. If the first if's other branch is to an uncommon trap, then that location\n+    \/\/ never saw a cache hit. In that case, when the ScopedValueGetHitsInCacheNode is expanded, only code to probe\n+    \/\/ the second location is added back to the IR.\n+    \/\/\n+    \/\/ Before transformation:        After transformation:                      After expansion:\n+    \/\/ cache = scopedValueCache();   cache = currentThread.scopedValueCache;    cache = currentThread.scopedValueCache;\n+    \/\/ if (cache == null) {          if (hits_in_cache(cache)) {                if (cache != null && second_entry_hits) {\n+    \/\/   goto slow_call;               result = load_from_cache;                  result = second_entry;\n+    \/\/ }                             } else {                                   } else {\n+    \/\/ if (first_entry_hits) {         if (cache == null) {                       if (cache == null) {\n+    \/\/   uncommon_trap();                goto slow_call;                            goto slow_call;\n+    \/\/ } else {                        }                                          }\n+    \/\/   if (second_entry_hits) {      if (first_entry_hits) {                    if (first_entry_hits) {\n+    \/\/     result = second_entry;        uncommon_trap();                           uncommon_trap();\n+    \/\/   } else {                      } else {                                   } else {\n+    \/\/     goto slow_call;               if (second_entry_hits) {                   if (second_entry_hits) {\n+    \/\/   }                                  halt;                                      halt;\n+    \/\/ }                                  } else {                                   } else {\n+    \/\/ continue:                            goto slow_call;                            goto slow_call;\n+    \/\/ ...                               }                                          }\n+    \/\/ return;                         }                                          }\n+    \/\/                               }                                          }\n+    \/\/ slow_call:                    continue:                                  continue:\n+    \/\/ result = slowGet();           ...                                        ...\n+    \/\/ goto continue;                return;                                    return;\n+    \/\/\n+    \/\/                               slow_call:                                 slow_call:\n+    \/\/                               result = slowGet();                        result = slowGet();\n+    \/\/                               goto continue;                             goto continue;\n+    \/\/\n+    void remove_first_probe_if_when_it_never_hits() {\n+      if (_first_cache_probe_iff == nullptr || _second_cache_probe_iff == nullptr) {\n+        return;\n+      }\n+      ProjNode* get_first_iff_failure = _first_cache_probe_iff->proj_out(\n+              _first_cache_probe_iff->in(1)->as_Bool()->_test._test == BoolTest::ne ? 0 : 1);\n+      CallStaticJavaNode* get_first_iff_unc = get_first_iff_failure->is_uncommon_trap_proj(Deoptimization::Reason_none);\n+      if (get_first_iff_unc == nullptr) {\n+        return;\n+      }\n+      \/\/ first cache check never hits, keep only the second.\n+      swap(_first_cache_probe_iff, _second_cache_probe_iff);\n+      swap(_first_index_in_cache, _second_index_in_cache);\n+      _second_cache_probe_iff = nullptr;\n+      _second_index_in_cache = nullptr;\n+    }\n+\n+    \/\/ The call for ScopedValue.get() was just inlined. The code here pattern matches the resulting subgraph. To make it\n+    \/\/ easier:\n+    \/\/ - the slow path call to slowGet() is not inlined. If heuristics decided it should be, it was enqueued for late\n+    \/\/ inlining which will happen later.\n+    \/\/ - The call to Thread.scopedValueCache() is not inlined either.\n+    \/\/\n+    \/\/ The pattern matching starts from the current control (end of inlining) and looks for the call for\n+    \/\/ Thread.scopedValueCache() which acts as a marker for the beginning of the subgraph for ScopedValue.get(). That\n+    \/\/ subgraph is connected to the graph of the current compilation but there's no risk of \"escaping\" ScopedValue.get()\n+    \/\/ during pattern matching because the call to Thread.scopedValueCache() dominates the entire subgraph for\n+    \/\/ ScopedValue.get().\n+    \/\/ In the process of pattern matching a number of checks from the java code of ScopedValue.get() are expected to\n+    \/\/ be encountered. They are recorded to be used later when the subgraph for ScopedValue.get() is transformed.\n+    void pattern_match() {\n+      ResourceMark rm;\n+      Unique_Node_List scoped_value_get_subgraph;\n+      scoped_value_get_subgraph.push(_kit.control());\n+      for (uint i = 0; i < scoped_value_get_subgraph.size(); ++i) {\n+        Node* c = scoped_value_get_subgraph.at(i);\n+        assert(c->is_CFG(), \"only control flow here\");\n+        if (c->is_Region()) {\n+          for (uint j = 1; j < c->req(); ++j) {\n+            Node* in = c->in(j);\n+            if (in != nullptr) {\n+              assert(!in->is_top(), \"no dead path here\");\n+              scoped_value_get_subgraph.push(in);\n+            }\n+          }\n+        } else if (match_cache_null_check(c)) {\n+          \/\/ we reached the start of ScopedValue.get()\n+        } else if (match_cache_probe(c)) {\n+          scoped_value_get_subgraph.push(c->in(0));\n+        } else if (c->is_RangeCheck()) {\n+          \/\/ Range checks for:\n+          \/\/ objects = scopedValueCache()\n+          \/\/ int n = (hash & Cache.SLOT_MASK) * 2;\n+          \/\/ if (objects[n] == this) {\n+          \/\/\n+          \/\/ always succeeds because the cache is of size CACHE_TABLE_SIZE * 2, CACHE_TABLE_SIZE is a power of 2 and\n+          \/\/ SLOT_MASK = CACHE_TABLE_SIZE - 1\n+#ifdef ASSERT\n+          \/\/ Verify the range check is against the return value from Thread.scopedValueCache()\n+          BoolNode* rc_bol = c->in(1)->as_Bool();\n+          CmpNode* rc_cmp = rc_bol->in(1)->as_Cmp();\n+          assert(rc_cmp->Opcode() == Op_CmpU, \"unexpected range check shape\");\n+          Node* rc_range = rc_cmp->in(rc_bol->_test.is_less() ? 2 : 1);\n+          assert(rc_range->Opcode() == Op_LoadRange, \"unexpected range check shape\");\n+          AddPNode* rc_range_address = rc_range->in(MemNode::Address)->as_AddP();\n+          ProjNode* rc_range_base = rc_range_address->in(AddPNode::Base)->uncast()->as_Proj();\n+          CallJavaNode* scoped_value_cache = rc_range_base->in(0)->as_CallJava();\n+          assert(scoped_value_cache->method()->intrinsic_id() == vmIntrinsics::_scopedValueCache, \"unexpected range check shape\");\n+#endif\n+          _kit.gvn().hash_delete(c);\n+          c->set_req(1, _kit.gvn().intcon(1));\n+          _kit.C->record_for_igvn(c);\n+          scoped_value_get_subgraph.push(c->in(0));\n+        } else if (c->is_CallStaticJava()) {\n+          assert(_slow_call == nullptr &&\n+                 c->as_CallStaticJava()->method()->intrinsic_id() == vmIntrinsics::_ScopedValue_slowGet,\n+                 \"ScopedValue.slowGet() call expected\");\n+          _slow_call = c->as_CallStaticJava();\n+          scoped_value_get_subgraph.push(c->in(0));\n+        } else {\n+          assert(c->is_Proj() || c->is_Catch(), \"unexpected node when pattern matching ScopedValue.get()\");\n+          scoped_value_get_subgraph.push(c->in(0));\n+        }\n+      }\n+      assert(_cache_not_null_iff != nullptr, \"pattern matching should find cache null check\");\n+      assert(_second_cache_probe_iff == nullptr || _first_cache_probe_iff != nullptr,\n+             \"second cache probe iff only if first one exists\");\n+\n+      \/\/ get_first_iff\/get_second_iff contain the first\/second check we ran into during the graph traversal. They are not\n+      \/\/ guaranteed to be the first\/second one in execution order. Indeed, the graph traversal started from the end of\n+      \/\/ ScopedValue.get() and followed control flow inputs towards the start. In the process and in the general case, it\n+      \/\/ encountered regions merging the results from the 3 paths that can produce the get() result: slowGet() call, first\n+      \/\/ cache location, second cache location. Depending on the order of region inputs, the first or second cache\n+      \/\/ location test can be encountered first or second.\n+      \/\/ Perform another traversal to figure out which is first.\n+      adjust_order_of_first_and_second_probe_if(scoped_value_get_subgraph);\n+      remove_first_probe_if_when_it_never_hits();\n+    }\n+\n+   public:\n+    ScopedValueGetPatternMatcher(GraphKit& kit, Node* scoped_value_object) :\n+            _kit(kit),\n+            _scoped_value_object(scoped_value_object),\n+            _scoped_value_cache(nullptr),\n+            _cache_not_null_iff(nullptr),\n+            _first_cache_probe_iff(nullptr),\n+            _second_cache_probe_iff(nullptr),\n+            _first_index_in_cache(nullptr),\n+            _second_index_in_cache(nullptr),\n+            _slow_call(nullptr)\n+    {\n+      pattern_match();\n+      assert(_scoped_value_cache != nullptr, \"must have found Thread.scopedValueCache() call\");\n+    }\n+    NONCOPYABLE(ScopedValueGetPatternMatcher);\n+\n+    CallNode* scoped_value_cache() const {\n+      return _scoped_value_cache;\n+    }\n+\n+    IfNode* cache_not_null_iff() const {\n+      return _cache_not_null_iff;\n+    }\n+\n+    IfNode* first_cache_probe_iff() const {\n+      return _first_cache_probe_iff;\n+    }\n+\n+    IfNode* second_cache_probe_iff() const {\n+      return _second_cache_probe_iff;\n+    }\n+\n+    Node* first_index_in_cache() const {\n+      return _first_index_in_cache;\n+    }\n+\n+    Node* second_index_in_cache() const {\n+      return _second_index_in_cache;\n+    }\n+\n+    CallStaticJavaNode* slow_call() const {\n+      return _slow_call;\n+    }\n+  };\n+\n+  class ScopedValueTransformer : public StackObj {\n+   private:\n+    GraphKit& _kit;\n+    Node* _scoped_value_object;\n+    const ScopedValueGetPatternMatcher& _pattern_matcher;\n+\n+    \/\/ Before the transformation of the subgraph we have (some branches may not be present depending on profile data),\n+    \/\/ in pseudo code:\n+    \/\/\n+    \/\/ cache = scopedValueCache();\n+    \/\/ if (cache == null) {\n+    \/\/   goto slow_call;\n+    \/\/ }\n+    \/\/ if (first_entry_hits) {\n+    \/\/   result = first_entry;\n+    \/\/ } else {\n+    \/\/   if (second_entry_hits) {\n+    \/\/     result = second_entry;\n+    \/\/   } else {\n+    \/\/     goto slow_call;\n+    \/\/   }\n+    \/\/ }\n+    \/\/ continue:\n+    \/\/ ...\n+    \/\/ return;\n+    \/\/\n+    \/\/ slow_call:\n+    \/\/ result = slowGet();\n+    \/\/ goto continue;\n+    \/\/\n+    \/\/ After transformation:\n+    \/\/ cache = currentThread.scopedValueCache;\n+    \/\/ if (hits_in_cache(cache)) {\n+    \/\/   result = load_from_cache;\n+    \/\/ } else {\n+    \/\/   if (cache == null) {\n+    \/\/     goto slow_call;\n+    \/\/   }\n+    \/\/   if (first_entry_hits) {\n+    \/\/     halt;\n+    \/\/   } else {\n+    \/\/     if (second_entry_hits) {\n+    \/\/        halt;\n+    \/\/      } else {\n+    \/\/        goto slow_call;\n+    \/\/     }\n+    \/\/   }\n+    \/\/ }\n+    \/\/ continue:\n+    \/\/ ...\n+    \/\/ return;\n+    \/\/\n+    \/\/ slow_call:\n+    \/\/ result = slowGet();\n+    \/\/ goto continue;\n+    \/\/\n+    \/\/ the transformed graph includes 2 copies of the cache probing logic. One represented by the\n+    \/\/ ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache pair that is amenable to optimizations. The other from\n+    \/\/ the result of the parsing of the java code where the success path ends with a Halt node. The reason for that is\n+    \/\/ that some paths may end with an uncommon trap and if one traps, we want the trap to be recorded for the right bci.\n+    \/\/ When the ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache pair is expanded, split if finds the duplicate\n+    \/\/ logic and cleans it up.\n+    void transform_get_subgraph() {\n+      Compile* C = _kit.C;\n+      replace_current_exit_of_get_with_halt();\n+\n+      \/\/ Graph now is:\n+      \/\/ cache = scopedValueCache();\n+      \/\/ if (cache == null) {\n+      \/\/   goto slow_call;\n+      \/\/ }\n+      \/\/ if (first_entry_hits) {\n+      \/\/   result = first_entry;\n+      \/\/ } else {\n+      \/\/   if (second_entry_hits) {\n+      \/\/     result = second_entry;\n+      \/\/   } else {\n+      \/\/     goto slow_call;\n+      \/\/   }\n+      \/\/ }\n+      \/\/ continue:\n+      \/\/ halt;\n+      \/\/\n+      \/\/ slow_call:\n+      \/\/ result = slowGet();\n+      \/\/ goto continue;\n+\n+      \/\/ Move right above the scopedValueCache() call\n+      CallNode* scoped_value_cache = _pattern_matcher.scoped_value_cache();\n+      Node* input_mem = scoped_value_cache->in(TypeFunc::Memory);\n+      Node* input_ctrl = scoped_value_cache->in(TypeFunc::Control);\n+      Node* input_io = scoped_value_cache->in(TypeFunc::I_O);\n+\n+      _kit.set_control(input_ctrl);\n+      _kit.set_all_memory(input_mem);\n+      _kit.set_i_o(input_io);\n+\n+      \/\/ replace it with its intrinsic code:\n+      Node* scoped_value_cache_load = _kit.scopedValueCache();\n+      \/\/ A single ScopedValueGetHitsInCache node represents all checks that are needed to probe the cache (cache not null,\n+      \/\/ cache_miss_prob with first hash, cache_miss_prob with second hash)\n+      \/\/ It will later be expanded back to all the checks so record profile data\n+      IfNode* cache_not_null_iff = _pattern_matcher.cache_not_null_iff();\n+      IfNode* first_cache_probe_iff = _pattern_matcher.first_cache_probe_iff();\n+      IfNode* second_cache_probe_iff = _pattern_matcher.second_cache_probe_iff();\n+      float probability_cache_exists = canonical_if_prob(cache_not_null_iff);\n+      float probability_first_cache_probe_fails = canonical_if_prob(first_cache_probe_iff);\n+      float probability_second_cache_probe_fails = canonical_if_prob(second_cache_probe_iff);\n+      Node* first_index_in_cache = _pattern_matcher.first_index_in_cache();\n+      Node* second_index_in_cache = _pattern_matcher.second_index_in_cache();\n+      ScopedValueGetHitsInCacheNode* hits_in_cache = new ScopedValueGetHitsInCacheNode(C, _kit.control(),\n+                                                                                       scoped_value_cache_load,\n+                                                                                       _kit.gvn().makecon(TypePtr::NULL_PTR),\n+                                                                                       _kit.memory(TypeAryPtr::OOPS),\n+                                                                                       _scoped_value_object,\n+                                                                                       first_index_in_cache == nullptr ? C->top() : first_index_in_cache,\n+                                                                                       second_index_in_cache == nullptr ? C->top() : second_index_in_cache,\n+                                                                                       cache_not_null_iff->_fcnt, probability_cache_exists,\n+                                                                                       if_cnt(first_cache_probe_iff), probability_first_cache_probe_fails,\n+                                                                                       if_cnt(second_cache_probe_iff), probability_second_cache_probe_fails);\n+\n+      Node* transformed_sv_hits_in_cache = _kit.gvn().transform(hits_in_cache);\n+      assert(transformed_sv_hits_in_cache == hits_in_cache, \"shouldn't be transformed to new node\");\n+\n+      \/\/ And compute the probability of a miss in the cache\n+      float cache_miss_prob;\n+      \/\/ probability_cache_exists: probability that cache array is not null\n+      \/\/ probability_first_cache_probe_fails: probability of a miss\n+      \/\/ probability_second_cache_probe_fails: probability of a miss\n+      if (probability_cache_exists == PROB_UNKNOWN || probability_first_cache_probe_fails == PROB_UNKNOWN || probability_second_cache_probe_fails == PROB_UNKNOWN) {\n+        cache_miss_prob = PROB_UNKNOWN;\n+      } else {\n+        float probability_cache_does_not_exist = 1 - probability_cache_exists;\n+        cache_miss_prob = probability_cache_does_not_exist + probability_cache_exists * probability_first_cache_probe_fails * probability_second_cache_probe_fails;\n+      }\n+\n+      \/\/ Add the control flow that checks whether ScopedValueGetHitsInCache succeeds\n+      Node* bol = _kit.gvn().transform(new BoolNode(hits_in_cache, BoolTest::ne));\n+      IfNode* iff = new IfNode(_kit.control(), bol, 1 - cache_miss_prob, cache_not_null_iff->_fcnt);\n+      Node* transformed_iff = _kit.gvn().transform(iff);\n+      assert(transformed_iff == iff, \"shouldn't be transformed to new node\");\n+      Node* not_in_cache_proj = _kit.gvn().transform(new IfFalseNode(iff));\n+      Node* in_cache_proj = _kit.gvn().transform(new IfTrueNode(iff));\n+\n+      \/\/ Merge the paths that produce the result (in case there's a slow path)\n+      CallStaticJavaNode* slow_call = _pattern_matcher.slow_call();\n+      Node* region_fast_slow = new RegionNode(slow_call == nullptr ? 2 : 3);\n+      Node* phi_cache_value = new PhiNode(region_fast_slow, TypeInstPtr::BOTTOM);\n+      Node* phi_mem = new PhiNode(region_fast_slow, Type::MEMORY, TypePtr::BOTTOM);\n+      Node* phi_io = new PhiNode(region_fast_slow, Type::ABIO);\n+\n+      \/\/ remove the scopedValueCache() call\n+      remove_scoped_value_cache_call(not_in_cache_proj, scoped_value_cache_load);\n+\n+      \/\/ ScopedValueGetLoadFromCache is a single that represents the result of a hit in the cache\n+      Node* sv_load_from_cache = _kit.gvn().transform(new ScopedValueGetLoadFromCacheNode(C, in_cache_proj, hits_in_cache));\n+      region_fast_slow->init_req(1, in_cache_proj);\n+      phi_cache_value->init_req(1, sv_load_from_cache);\n+      phi_mem->init_req(1, _kit.reset_memory());\n+      phi_io->init_req(1, _kit.i_o());\n+\n+      \/\/ Graph now is:\n+      \/\/\n+      \/\/ cache = currentThread.scopedValueCache;\n+      \/\/ if (hits_in_cache(cache)) {\n+      \/\/   result = load_from_cache;\n+      \/\/   goto region_fast_slow;\n+      \/\/ } else {\n+      \/\/   if (cache == null) {\n+      \/\/     goto slow_call;\n+      \/\/   }\n+      \/\/   if (first_entry_hits) {\n+      \/\/     result = first_entry;\n+      \/\/   } else {\n+      \/\/     if (second_entry_hits) {\n+      \/\/       result = second_entry;\n+      \/\/     } else {\n+      \/\/       goto slow_call;\n+      \/\/     }\n+      \/\/   }\n+      \/\/ }\n+      \/\/ continue:\n+      \/\/ halt;\n+      \/\/ region_fast_slow;\n+      \/\/\n+      \/\/ slow_call:\n+      \/\/ result = slowGet();\n+      \/\/ goto continue;\n+\n+      if (slow_call != nullptr) {\n+        \/\/ At this point, return from slowGet() falls through to a Halt node. Connect it to the new normal exit (region_fast_slow)\n+        CallProjections slow_projs;\n+        slow_call->extract_projections(&slow_projs, false);\n+        Node* fallthrough = slow_projs.fallthrough_catchproj->clone();\n+        _kit.gvn().set_type(fallthrough, fallthrough->bottom_type());\n+        C->gvn_replace_by(slow_projs.fallthrough_catchproj, C->top());\n+        region_fast_slow->init_req(2, fallthrough);\n+        phi_mem->init_req(2, slow_projs.fallthrough_memproj);\n+        phi_io->init_req(2, slow_projs.fallthrough_ioproj);\n+        phi_cache_value->init_req(2, slow_projs.resproj);\n+      }\n+\n+      _kit.set_all_memory(_kit.gvn().transform(phi_mem));\n+      _kit.set_i_o(_kit.gvn().transform(phi_io));\n+      _kit.set_control(_kit.gvn().transform(region_fast_slow));\n+      C->record_for_igvn(region_fast_slow);\n+      _kit.pop();\n+      _kit.push(phi_cache_value);\n+      \/\/ The if nodes from parsing are now only reachable if get() doesn't hit in the cache. Adjust count\/probability for\n+      \/\/ those nodes.\n+      float cache_miss_cnt = cache_miss_prob * cache_not_null_iff->_fcnt;\n+      reset_iff_prob_and_cnt(cache_not_null_iff, true, cache_miss_cnt);\n+      reset_iff_prob_and_cnt(first_cache_probe_iff, false, cache_miss_cnt);\n+      reset_iff_prob_and_cnt(second_cache_probe_iff, false, cache_miss_cnt);\n+    }\n+\n+    float canonical_if_prob(IfNode* iff) const {\n+      if (iff == nullptr) {\n+        return 0;\n+      }\n+      return iff->canonical_prob();\n+    }\n+\n+    float if_cnt(IfNode* iff) const {\n+      if (iff == nullptr) {\n+        return 0;\n+      }\n+      return iff->_fcnt;\n+    }\n+\n+    void remove_scoped_value_cache_call(Node* not_in_cache, Node* scoped_value_cache_load) const {\n+      CallProjections scoped_value_cache_projs;\n+      CallNode* scoped_value_cache = _pattern_matcher.scoped_value_cache();\n+      scoped_value_cache->extract_projections(&scoped_value_cache_projs, true);\n+      Compile* C = _kit.C;\n+      C->gvn_replace_by(scoped_value_cache_projs.fallthrough_memproj, _kit.merged_memory());\n+      C->gvn_replace_by(scoped_value_cache_projs.fallthrough_ioproj, _kit.i_o());\n+      C->gvn_replace_by(scoped_value_cache_projs.fallthrough_catchproj, not_in_cache);\n+      C->gvn_replace_by(scoped_value_cache_projs.resproj, scoped_value_cache_load);\n+\n+      _kit.gvn().hash_delete(scoped_value_cache);\n+      scoped_value_cache->set_req(0, C->top());\n+      C->record_for_igvn(scoped_value_cache);\n+    }\n+\n+    void replace_current_exit_of_get_with_halt() const {\n+      \/\/ The path on exit of the method from parsing ends here\n+      Compile* C = _kit.C;\n+      Node* current_ctrl = _kit.control();\n+      Node* frame = _kit.gvn().transform(new ParmNode(C->start(), TypeFunc::FramePtr));\n+      Node* halt = _kit.gvn().transform(new HaltNode(current_ctrl, frame, \"Dead path for ScopedValueCall::get\"));\n+      C->root()->add_req(halt);\n+    }\n+\n+    \/\/ Either the if leads to a Halt: that branch is never taken or it leads to an uncommon trap and the probability is\n+    \/\/ left unchanged.\n+    static void reset_iff_prob_and_cnt(IfNode* iff, bool expected, float cnt) {\n+      if (iff == nullptr) {\n+        return;\n+      }\n+      if (!iff->in(1)->as_Bool()->_test.is_canonical()) {\n+        ProjNode* proj = iff->proj_out(expected);\n+        if (!proj->is_uncommon_trap_proj()) {\n+          float prob = expected ? PROB_ALWAYS : PROB_NEVER;\n+          iff->_prob = prob;\n+        }\n+      } else {\n+        ProjNode* proj = iff->proj_out(!expected);\n+        if (!proj->is_uncommon_trap_proj()) {\n+          float prob = expected ? PROB_NEVER : PROB_ALWAYS;\n+          iff->_prob = prob;\n+        }\n+      }\n+      iff->_fcnt = cnt;\n+    }\n+\n+   public:\n+    ScopedValueTransformer(GraphKit& kit, Node* scopedValueObject, const ScopedValueGetPatternMatcher &patternMatcher) :\n+            _kit(kit), _scoped_value_object(scopedValueObject), _pattern_matcher(patternMatcher) {\n+      transform_get_subgraph();\n+    }\n+    NONCOPYABLE(ScopedValueTransformer);\n+  };\n+\n+ public:\n+  LateInlineScopedValueCallGenerator(ciMethod* method, CallGenerator* inline_cg, bool process_result) :\n+          LateInlineCallGenerator(method, inline_cg),\n+          _process_result(process_result),\n+          _scoped_value_object(nullptr) {}\n+\n+  virtual JVMState* generate(JVMState* jvms) {\n+    Compile *C = Compile::current();\n+\n+    C->log_inline_id(this);\n+\n+    C->add_scoped_value_late_inline(this);\n+\n+    JVMState* new_jvms = DirectCallGenerator::generate(jvms);\n+    return new_jvms;\n+  }\n+\n+  virtual CallGenerator* with_call_node(CallNode* call) {\n+    LateInlineScopedValueCallGenerator* cg = new LateInlineScopedValueCallGenerator(method(), _inline_cg, false);\n+    cg->set_call_node(call->as_CallStaticJava());\n+    return cg;\n+  }\n+\n+  void do_late_inline() {\n+    CallNode* call = call_node();\n+    _scoped_value_object = call->in(TypeFunc::Parms);\n+    CallGenerator::do_late_inline_helper();\n+  }\n+\n+  virtual void set_process_result(bool v) {\n+    _process_result = v;\n+  }\n+\n+  \/\/ Inlining is finished. Here we first pattern match the resulting subgraph to extract profile data. Then the subgraph\n+  \/\/ is transformed so probing the scoped value cache is handled by a ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache\n+  \/\/ pair of nodes. The resulting shape is better suited for optimization. Profiled data is attached to these nodes.\n+  \/\/ Later, the pair of nodes are expanded back to a subgraph that probes the cache.\n+  virtual void process_result(GraphKit& kit) {\n+    if (!_process_result) {\n+      return;\n+    }\n+    assert(_scoped_value_object != nullptr, \"must have set scoped value to be pattern matched\");\n+    assert(method()->intrinsic_id() == vmIntrinsics::_ScopedValue_get, \"should be run after late inlining of ScopedValue.get()\");\n+    ScopedValueGetPatternMatcher pattern_matcher(kit, _scoped_value_object);\n+    \/\/ Now transform the subgraph in a way that makes it amenable to optimizations\n+    ScopedValueTransformer transformer(kit, _scoped_value_object, pattern_matcher);\n+  }\n+};\n+\n+CallGenerator* CallGenerator::for_scoped_value_get_late_inline(ciMethod* m, CallGenerator* inline_cg,\n+                                                               bool process_result) {\n+  return new LateInlineScopedValueCallGenerator(m, inline_cg, process_result);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":687,"deletions":0,"binary":false,"changes":687,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"opto\/graphKit.hpp\"\n@@ -144,0 +145,1 @@\n+  static CallGenerator* for_scoped_value_get_late_inline(ciMethod* m, CallGenerator* inline_cg, bool process_result);\n@@ -189,0 +191,2 @@\n+  virtual void set_process_result(bool v) {}\n+\n@@ -196,0 +200,2 @@\n+\n+  virtual void process_result(GraphKit& kit) {}\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3048,0 +3048,8 @@\n+\n+const Type* ScopedValueGetResultNode::Value(PhaseGVN* phase) const {\n+  if (phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return Node::Value(phase);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -449,0 +449,1 @@\n+  float canonical_prob() const;\n@@ -720,0 +721,38 @@\n+\/\/ The result of a ScopedValue.get()\n+class ScopedValueGetResultNode : public  MultiNode {\n+public:\n+  enum {\n+      Control = 0,\n+      ScopedValue, \/\/ which ScopedValue object is this for?\n+      GetResult \/\/ subgraph that produces the result\n+  };\n+  enum {\n+      ControlOut = 0,\n+      Result \/\/ The ScopedValue.get() result\n+  };\n+  ScopedValueGetResultNode(Compile* C, Node* ctrl, Node* sv, Node* res) : MultiNode(3) {\n+    init_req(Control, ctrl);\n+    init_req(ScopedValue, sv);\n+    init_req(GetResult, res);\n+    init_class_id(Class_ScopedValueGetResult);\n+  }\n+  virtual int   Opcode() const;\n+  virtual const Type* bottom_type() const { return TypeTuple::SV_GET_RESULT; }\n+\n+  ProjNode* result_out_or_null() {\n+    return proj_out_or_null(Result);\n+  }\n+\n+  ProjNode* control_out() {\n+    return proj_out(ControlOut);\n+  }\n+\n+  Node* scoped_value() const {\n+    return in(ScopedValue);\n+  }\n+  Node* result_in() const {\n+    return in(GetResult);\n+  }\n+\n+  const Type* Value(PhaseGVN* phase) const;\n+};\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -215,0 +215,3 @@\n+macro(ScopedValueGetLoadFromCache)\n+macro(ScopedValueGetHitsInCache)\n+macro(ScopedValueGetResult)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -409,0 +409,1 @@\n+    remove_useless_late_inlines(   &_scoped_value_late_inlines, dead);\n@@ -467,0 +468,1 @@\n+  remove_useless_late_inlines(   &_scoped_value_late_inlines, useful);\n@@ -669,0 +671,1 @@\n+                  _scoped_value_late_inlines(comp_arena(), 2, 0, nullptr),\n@@ -1124,0 +1127,2 @@\n+  _has_scoped_value_invalidate = false;\n+  _has_scoped_value_get_nodes = false;\n@@ -2037,0 +2042,72 @@\n+void Compile::inline_scoped_value_get_calls(PhaseIterGVN& igvn) {\n+  if (_scoped_value_late_inlines.is_empty()) {\n+    return;\n+  }\n+  PhaseGVN* gvn = initial_gvn();\n+  set_inlining_incrementally(true);\n+\n+  igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n+\n+  _late_inlines_pos = _late_inlines.length();\n+\n+  while (_scoped_value_late_inlines.length() > 0) {\n+    CallGenerator* cg = _scoped_value_late_inlines.pop();\n+    assert(cg->method()->intrinsic_id() == vmIntrinsics::_ScopedValue_get, \"only calls to ScopedValue.get() here\");\n+    if (has_scoped_value_invalidate()) {\n+      \/\/ ScopedValue$Cache.invalidate() is called so pessimistically assume we can't optimize ScopedValue.get() and\n+      \/\/ enqueue the call for regular late inlining\n+      cg->set_process_result(false);\n+      C->add_late_inline(cg);\n+      continue;\n+    }\n+    C->set_has_scoped_value_get_nodes(true);\n+    CallNode* call = cg->call_node();\n+    CallProjections call_projs;\n+    call->extract_projections(&call_projs, true);\n+    Node* scoped_value_object = call->in(TypeFunc::Parms);\n+    Node* control_out = call_projs.fallthrough_catchproj;\n+    Node* scoped_value_get_result = call_projs.resproj;\n+    \/\/ Insert a ScopedValueGetResult node after the call with the result of ScopedValue.get() as input\n+    if (scoped_value_get_result == nullptr) {\n+      scoped_value_get_result = gvn->transform(new ProjNode(call, TypeFunc::Parms));\n+    }\n+    \/\/ Clone the control and result projections of the call and add them as input to the ScopedValueGetResult node\n+    \/\/ Updating uses of the call result\/control is then done by replacing the initial control and result projections\n+    \/\/ of the call with the new control and result projections of the ScopedValueGetResult node.\n+    control_out = control_out->clone();\n+    gvn->set_type_bottom(control_out);\n+    gvn->record_for_igvn(control_out);\n+    scoped_value_get_result = scoped_value_get_result->clone();\n+    gvn->set_type_bottom(scoped_value_get_result);\n+    gvn->record_for_igvn(scoped_value_get_result);\n+\n+    ScopedValueGetResultNode* get_result = new ScopedValueGetResultNode(C, control_out, scoped_value_object, scoped_value_get_result);\n+    Node* sv_get_resultx = gvn->transform(get_result);\n+    assert(sv_get_resultx == get_result, \"this breaks if gvn returns new node\");\n+    Node* control_proj = gvn->transform(new ProjNode(get_result, ScopedValueGetResultNode::ControlOut));\n+    Node* res_proj = gvn->transform(new ProjNode(get_result, ScopedValueGetResultNode::Result));\n+\n+    C->gvn_replace_by(call_projs.fallthrough_catchproj, control_proj);\n+    if (call_projs.resproj != nullptr) {\n+      C->gvn_replace_by(call_projs.resproj, res_proj);\n+    }\n+\n+    Node* control_projx = gvn->transform(control_proj);\n+    assert(control_projx == control_proj, \"this breaks if gvn returns new node\");\n+    Node* res_projx = gvn->transform(res_proj);\n+    assert(res_projx == res_proj, \"this breaks if gvn returns new node\");\n+\n+    \/\/ Inline the call to ScopedValue.get(). That triggers the execution of LateInlineScopedValueCallGenerator::process_result()\n+    cg->do_late_inline();\n+    if (failing()) return;\n+\n+    C->set_has_split_ifs(true);\n+  }\n+\n+  inline_incrementally_cleanup(igvn);\n+\n+  set_inlining_incrementally(false);\n+\n+  inline_incrementally(igvn);\n+}\n+\n@@ -2274,0 +2351,6 @@\n+  inline_scoped_value_get_calls(igvn);\n+\n+  print_method(PHASE_INCREMENTAL_SCOPED_VALUE_INLINE, 2);\n+\n+  if (failing())  return;\n+\n@@ -3890,0 +3973,6 @@\n+  case Op_ScopedValueGetResult:\n+  case Op_ScopedValueGetHitsInCache:\n+  case Op_ScopedValueGetLoadFromCache: {\n+    fatal(\"ScopedValue nodes should have been expanded by now\");\n+    break;\n+  }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":89,"deletions":0,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -360,0 +360,2 @@\n+  bool                  _has_scoped_value_invalidate; \/\/ Did we encounter a call to ScopedValue$Cache.invalidate()?\n+  bool                  _has_scoped_value_get_nodes; \/\/ Are we optimizing ScopedValue.get() calls?\n@@ -463,0 +465,1 @@\n+  GrowableArray<CallGenerator*> _scoped_value_late_inlines; \/\/ same but for operations related to ScopedValue.get()\n@@ -679,0 +682,4 @@\n+  bool              has_scoped_value_invalidate() const { return _has_scoped_value_invalidate; }\n+  void          set_has_scoped_value_invalidate(bool v) { _has_scoped_value_invalidate = v; }\n+  bool              has_scoped_value_get_nodes() const { return _has_scoped_value_get_nodes; }\n+  void          set_has_scoped_value_get_nodes(bool v) { _has_scoped_value_get_nodes = v; }\n@@ -1062,0 +1069,4 @@\n+  void              add_scoped_value_late_inline(CallGenerator* cg) {\n+    _scoped_value_late_inlines.push(cg);\n+  }\n+\n@@ -1095,0 +1106,1 @@\n+  void inline_scoped_value_get_calls(PhaseIterGVN& igvn);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -162,0 +162,2 @@\n+      } else if (IncrementalInline && callee->intrinsic_id() == vmIntrinsics::_scopedValueCache) {\n+        return CallGenerator::for_late_inline(callee, cg);\n@@ -177,0 +179,4 @@\n+  if (callee->intrinsic_id() == vmIntrinsics::_ScopedValueCache_invalidate) {\n+    C->set_has_scoped_value_invalidate(true);\n+  }\n+\n@@ -215,0 +221,4 @@\n+          } else if (callee->intrinsic_id() == vmIntrinsics::_ScopedValue_get && IncrementalInline) {\n+            return CallGenerator::for_scoped_value_get_late_inline(callee, cg, true);\n+          } else if (callee->intrinsic_id() == vmIntrinsics::_ScopedValue_slowGet && IncrementalInline) {\n+            return CallGenerator::for_late_inline(callee, cg);\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -1601,2 +1602,2 @@\n-      if (n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-          n->in(0)->as_Call()->returns_pointer()) {\n+      ProjNode* proj = n->as_Proj();\n+      if (proj->returns_pointer_from_call() || proj->is_result_from_scoped_value_get()) {\n@@ -1670,0 +1671,11 @@\n+    case Op_ScopedValueGetLoadFromCache: {\n+      ScopedValueGetLoadFromCacheNode* load_from_cache = n->as_ScopedValueGetLoadFromCache();\n+      map_ideal_node(load_from_cache, phantom_obj);\n+      break;\n+    }\n+    case Op_ScopedValueGetResult: {\n+      ScopedValueGetResultNode* get_result = n->as_ScopedValueGetResult();\n+      add_local_var_and_edge(get_result, PointsToNode::NoEscape, get_result->result_in(), delayed_worklist);\n+      break;\n+    }\n+\n@@ -1758,2 +1770,1 @@\n-      assert(n->as_Proj()->_con == TypeFunc::Parms && n->in(0)->is_Call() &&\n-             n->in(0)->as_Call()->returns_pointer(), \"Unexpected node type\");\n+      assert(n->as_Proj()->returns_pointer_from_call() || n->as_Proj()->is_result_from_scoped_value_get(), \"Unexpected node type\");\n@@ -1838,0 +1849,5 @@\n+    case Op_ScopedValueGetResult: {\n+      ScopedValueGetResultNode* get_result = n->as_ScopedValueGetResult();\n+      add_local_var_and_edge(get_result, PointsToNode::NoEscape, get_result->result_in(), nullptr);\n+      break;\n+    }\n@@ -4531,0 +4547,1 @@\n+              op == Op_ScopedValueGetLoadFromCache || op == Op_ScopedValueGetResult ||\n@@ -4688,3 +4705,4 @@\n-              op == Op_AryEq || op == Op_StrComp || op == Op_CountPositives ||\n-              op == Op_StrCompressedCopy || op == Op_StrInflatedCopy || op == Op_VectorizedHashCode ||\n-              op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {\n+                     op == Op_AryEq || op == Op_StrComp || op == Op_CountPositives ||\n+                     op == Op_StrCompressedCopy || op == Op_StrInflatedCopy || op == Op_VectorizedHashCode ||\n+                     op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||\n+                     op == Op_ScopedValueGetLoadFromCache || op == Op_ScopedValueGetResult)) {\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":25,"deletions":7,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -4274,0 +4274,29 @@\n+\n+const Type* GraphKit::scopedValueCache_type() {\n+  ciKlass* objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n+  const TypeOopPtr* etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+\n+  \/\/ Because we create the scopedValue cache lazily we have to make the\n+  \/\/ type of the result BotPTR.\n+  bool xk = etype->klass_is_exact();\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n+  return objects_type;\n+}\n+\n+Node* GraphKit::scopedValueCache_handle() {\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* p = basic_plus_adr(top()\/*!oop*\/, thread, in_bytes(JavaThread::scopedValueCache_offset()));\n+  \/\/ We cannot use immutable_memory() because we might flip onto a\n+  \/\/ different carrier thread, at which point we'll need to use that\n+  \/\/ carrier thread's cache.\n+  \/\/ return _gvn.transform(LoadNode::make(_gvn, nullptr, immutable_memory(), p, p->bottom_type()->is_ptr(),\n+  \/\/       TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered));\n+  return make_load(nullptr, p, p->bottom_type()->is_ptr(), T_ADDRESS, MemNode::unordered);\n+}\n+\n+Node* GraphKit::scopedValueCache() {\n+  Node* cache_obj_handle = scopedValueCache_handle();\n+  const Type* objects_type = scopedValueCache_type();\n+  return access_load(cache_obj_handle, objects_type, T_OBJECT, IN_NATIVE);\n+}\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -70,0 +70,3 @@\n+  const Type* scopedValueCache_type();\n+  Node* scopedValueCache_handle();\n+\n@@ -910,0 +913,2 @@\n+\n+  Node* scopedValueCache();\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1645,0 +1645,7 @@\n+float IfNode::canonical_prob() const {\n+  float prob = _prob;\n+  if (prob != PROB_UNKNOWN && !in(1)->as_Bool()->_test.is_canonical()) {\n+    prob = 1 - prob;\n+  }\n+  return prob;\n+}\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"opto\/cfgnode.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"opto\/subnode.hpp\"\n@@ -371,0 +373,20 @@\n+\n+Node* ScopedValueGetLoadFromCacheNode::scoped_value() const {\n+  Node* hits_in_cache = in(1);\n+  return hits_in_cache->as_ScopedValueGetHitsInCache()->scoped_value();\n+}\n+\n+IfNode* ScopedValueGetLoadFromCacheNode::iff() const {\n+  return in(0)->in(0)->as_If();\n+}\n+\n+#ifdef ASSERT\n+void ScopedValueGetLoadFromCacheNode::verify() const {\n+  \/\/ check a ScopedValueGetHitsInCache guards this ScopedValueGetLoadFromCache\n+  assert(in(0)->Opcode() == Op_IfTrue, \"unexpected ScopedValueGetLoadFromCache shape\");\n+  IfNode* iff = in(0)->in(0)->as_If();\n+  assert(iff->in(1)->is_Bool(), \"unexpected ScopedValueGetLoadFromCache shape\");\n+  assert(iff->in(1)->in(1)->Opcode() == Op_ScopedValueGetHitsInCache, \"unexpected ScopedValueGetLoadFromCache shape\");\n+  assert(iff->in(1)->in(1) == in(1), \"unexpected ScopedValueGetLoadFromCache shape\");\n+}\n+#endif\n","filename":"src\/hotspot\/share\/opto\/intrinsicnode.cpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -342,0 +342,20 @@\n+\/\/ The result from a successful load from the ScopedValue cache. Goes in pair with ScopedValueGetHitsInCache\n+class ScopedValueGetLoadFromCacheNode : public Node {\n+public:\n+  ScopedValueGetLoadFromCacheNode(Compile* C, Node* ctrl, Node* hits_in_cache)\n+          : Node(ctrl, hits_in_cache) {\n+    init_class_id(Class_ScopedValueGetLoadFromCache);\n+  }\n+\n+  Node* scoped_value() const;\n+  IfNode* iff() const;\n+\n+  virtual int Opcode() const;\n+\n+  const Type* bottom_type() const {\n+    return TypeInstPtr::BOTTOM;\n+  }\n+\n+  void verify() const NOT_DEBUG_RETURN;\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/intrinsicnode.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -3644,23 +3644,0 @@\n-const Type* LibraryCallKit::scopedValueCache_type() {\n-  ciKlass* objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n-  const TypeOopPtr* etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n-  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n-\n-  \/\/ Because we create the scopedValue cache lazily we have to make the\n-  \/\/ type of the result BotPTR.\n-  bool xk = etype->klass_is_exact();\n-  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, 0);\n-  return objects_type;\n-}\n-\n-Node* LibraryCallKit::scopedValueCache_helper() {\n-  Node* thread = _gvn.transform(new ThreadLocalNode());\n-  Node* p = basic_plus_adr(top()\/*!oop*\/, thread, in_bytes(JavaThread::scopedValueCache_offset()));\n-  \/\/ We cannot use immutable_memory() because we might flip onto a\n-  \/\/ different carrier thread, at which point we'll need to use that\n-  \/\/ carrier thread's cache.\n-  \/\/ return _gvn.transform(LoadNode::make(_gvn, nullptr, immutable_memory(), p, p->bottom_type()->is_ptr(),\n-  \/\/       TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered));\n-  return make_load(nullptr, p, p->bottom_type()->is_ptr(), T_ADDRESS, MemNode::unordered);\n-}\n-\n@@ -3669,4 +3646,1 @@\n-  Node* cache_obj_handle = scopedValueCache_helper();\n-  const Type* objects_type = scopedValueCache_type();\n-  set_result(access_load(cache_obj_handle, objects_type, T_OBJECT, IN_NATIVE));\n-\n+  set_result(scopedValueCache());\n@@ -3679,1 +3653,1 @@\n-  Node* cache_obj_handle = scopedValueCache_helper();\n+  Node* cache_obj_handle = scopedValueCache_handle();\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":28,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -240,2 +240,1 @@\n-  const Type* scopedValueCache_type();\n-  Node* scopedValueCache_helper();\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/c2\/barrierSetC2.hpp\"\n@@ -1128,1 +1130,1 @@\n-                   in->as_Proj()->is_uncommon_trap_if_pattern() &&\n+                   is_uncommon_or_multi_uncommon_trap_if_pattern(in->as_IfProj()) &&\n@@ -1266,1 +1268,2 @@\n-  } else {\n+  } else if (!loop_predication_for_scoped_value_get(loop, if_success_proj, parse_predicate_proj, invar, reason, iff,\n+                                                    new_predicate_proj)) {\n@@ -1415,2 +1418,1 @@\n-      CallStaticJavaNode* call = if_proj->is_uncommon_trap_if_pattern();\n-      if (call == nullptr) {\n+      if (!is_uncommon_or_multi_uncommon_trap_if_pattern(if_proj)) {\n@@ -1432,2 +1434,2 @@\n-      Deoptimization::DeoptReason reason = Deoptimization::trap_request_reason(call->uncommon_trap_request());\n-      if (reason == Deoptimization::Reason_predicate) {\n+      CallStaticJavaNode* call = if_proj->is_uncommon_trap_if_pattern();\n+      if (call != nullptr && Deoptimization::trap_request_reason(call->uncommon_trap_request()) == Deoptimization::Reason_predicate) {\n@@ -1453,1 +1455,1 @@\n-      if (if_proj->as_Proj()->is_uncommon_trap_if_pattern() &&\n+      if (is_uncommon_or_multi_uncommon_trap_if_pattern(if_proj->as_IfProj()) &&\n@@ -1539,0 +1541,111 @@\n+\n+bool PhaseIdealLoop::is_uncommon_or_multi_uncommon_trap_if_pattern(IfProjNode* proj) {\n+  if (proj->is_uncommon_trap_if_pattern()) {\n+    return true;\n+  }\n+  if (proj->in(0)->in(1)->is_Bool() &&\n+      proj->in(0)->in(1)->in(1)->Opcode() == Op_ScopedValueGetHitsInCache &&\n+      proj->is_multi_uncommon_trap_if_pattern()) {\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\n+\/\/ A ScopedValueGetHitsInCache check is loop invariant if the scoped value object it is applied to is loop invariant\n+bool PhaseIdealLoop::loop_predication_for_scoped_value_get(IdealLoopTree* loop, IfProjNode* if_success_proj,\n+                                                           ParsePredicateSuccessProj* parse_predicate_proj,\n+                                                           Invariance& invar, Deoptimization::DeoptReason reason,\n+                                                           IfNode* iff, IfProjNode*& new_predicate_proj) {\n+  BoolNode* bol = iff->in(1)->as_Bool();\n+  if (bol->in(1)->Opcode() != Op_ScopedValueGetHitsInCache){\n+    return false;\n+  }\n+  ScopedValueGetHitsInCacheNode* hits_in_the_cache = bol->in(1)->as_ScopedValueGetHitsInCache();\n+  if (!invar.is_invariant(hits_in_the_cache->scoped_value()) ||\n+      !invar.is_invariant(hits_in_the_cache->index1()) ||\n+      !invar.is_invariant(hits_in_the_cache->index2())) {\n+    return false;\n+  }\n+  Node* load_from_cache = if_success_proj->find_unique_out_with(Op_ScopedValueGetLoadFromCache);\n+  assert(load_from_cache->in(1) == hits_in_the_cache, \"unexpected ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+  assert(if_success_proj->is_IfTrue(), \"unexpected ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+  new_predicate_proj = create_new_if_for_predicate(parse_predicate_proj, nullptr,\n+                                                   reason,\n+                                                   iff->Opcode());\n+  Node* ctrl = new_predicate_proj->in(0)->in(0);\n+  Node* new_bol = bol->clone();\n+  register_new_node(new_bol, ctrl);\n+  Node* new_hits_in_the_cache = hits_in_the_cache->clone();\n+  register_new_node(new_hits_in_the_cache, ctrl);\n+  _igvn.replace_input_of(load_from_cache, 1, new_hits_in_the_cache);\n+\n+  CallStaticJavaNode* call = new_predicate_proj->is_uncommon_trap_if_pattern();\n+  assert(call != nullptr, \"Where's the uncommon trap call?\");\n+\n+  Node* all_mem = call->in(TypeFunc::Memory);\n+  MergeMemNode* mm = all_mem->isa_MergeMem();\n+  Node* raw_mem = mm != nullptr ? mm->memory_at(Compile::AliasIdxRaw) : all_mem;\n+\n+  \/\/ The scoped value cache may be loop variant because it depends on raw memory which may keep the\n+  \/\/ ScopedValueGetHitsInCache in the loop. It's legal to hoist it out of loop though but we need to update the scoped\n+  \/\/ value cache to be out of loop as well.\n+  Node* scoped_value_cache_load = scoped_value_cache_node(raw_mem);\n+\n+  _igvn.replace_input_of(new_hits_in_the_cache, 1, scoped_value_cache_load);\n+  Node* oop_mem = mm != nullptr ? mm->memory_at(C->get_alias_index(TypeAryPtr::OOPS)) : all_mem;\n+  _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::Memory, oop_mem);\n+  _igvn.replace_input_of(new_hits_in_the_cache, 0, ctrl);\n+  _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::ScopedValue,\n+                         invar.clone(hits_in_the_cache->scoped_value(), ctrl));\n+  _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::Index1,\n+                         invar.clone(hits_in_the_cache->index1(), ctrl));\n+  _igvn.replace_input_of(new_hits_in_the_cache, ScopedValueGetHitsInCacheNode::Index2,\n+                         invar.clone(hits_in_the_cache->index2(), ctrl));\n+\n+  _igvn.replace_input_of(new_bol, 1, new_hits_in_the_cache);\n+\n+  assert(invar.is_invariant(new_bol), \"should be loop invariant\");\n+\n+  IfNode* new_predicate_iff = new_predicate_proj->in(0)->as_If();\n+  _igvn.hash_delete(new_predicate_iff);\n+  new_predicate_iff->set_req(1, new_bol);\n+#ifndef PRODUCT\n+  if (TraceLoopPredicate) {\n+    tty->print(\"Predicate invariant if: %d \", new_predicate_iff->_idx);\n+    loop->dump_head();\n+  } else if (TraceLoopOpts) {\n+    tty->print(\"Predicate IC \");\n+    loop->dump_head();\n+  }\n+#endif\n+  return true;\n+}\n+\n+\/\/ It is easier to re-create the cache load subgraph rather than trying to change the inputs of the existing one to move\n+\/\/ it out of loops\n+Node* PhaseIdealLoop::scoped_value_cache_node(Node* raw_mem) {\n+  Node* thread = new ThreadLocalNode();\n+  register_new_node(thread, C->root());\n+  Node* scoped_value_cache_offset = _igvn.MakeConX(in_bytes(JavaThread::scopedValueCache_offset()));\n+  set_ctrl(scoped_value_cache_offset, C->root());\n+  Node* p = new AddPNode(C->top(), thread, scoped_value_cache_offset);\n+  register_new_node(p, C->root());\n+  Node* handle_load = LoadNode::make(_igvn, nullptr, raw_mem, p, p->bottom_type()->is_ptr(), TypeRawPtr::NOTNULL,\n+                                     T_ADDRESS, MemNode::unordered);\n+  _igvn.register_new_node_with_optimizer(handle_load);\n+  set_subtree_ctrl(handle_load, true);\n+\n+  ciInstanceKlass* object_klass = ciEnv::current()->Object_klass();\n+  const TypeOopPtr* etype = TypeOopPtr::make_from_klass(object_klass);\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+  const TypeAryPtr* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, true, 0);\n+\n+  DecoratorSet decorators = C2_READ_ACCESS | IN_NATIVE;\n+  C2AccessValuePtr addr(handle_load, TypeRawPtr::NOTNULL);\n+  C2OptAccess access(_igvn, nullptr, raw_mem, decorators, T_OBJECT, nullptr, addr);\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* load_of_cache = bs->load_at(access, objects_type);\n+  set_subtree_ctrl(load_of_cache, true);\n+  return load_of_cache;\n+}\n","filename":"src\/hotspot\/share\/opto\/loopPredicate.cpp","additions":120,"deletions":7,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -484,2 +484,2 @@\n-bool IdealLoopTree::policy_peeling(PhaseIdealLoop *phase) {\n-  uint estimate = estimate_peeling(phase);\n+bool IdealLoopTree::policy_peeling(PhaseIdealLoop* phase, bool scoped_value_only) {\n+  uint estimate = estimate_peeling(phase, scoped_value_only);\n@@ -493,1 +493,1 @@\n-uint IdealLoopTree::estimate_peeling(PhaseIdealLoop *phase) {\n+uint IdealLoopTree::estimate_peeling(PhaseIdealLoop* phase, bool peel_only_if_has_scoped_value) {\n@@ -521,1 +521,1 @@\n-    if (test->is_If()) {    \/\/ Test?\n+    if (test->is_If() && !peel_only_if_has_scoped_value) {    \/\/ Test?\n@@ -537,0 +537,5 @@\n+    } else if (test->Opcode() == Op_ScopedValueGetResult &&\n+               !phase->is_member(this, phase->get_ctrl((test->as_ScopedValueGetResult())->scoped_value()))) {\n+      \/\/ Found a ScopedValueGetResult node: peeling one iteration will allow the elimination of the ScopedValue.get()\n+      \/\/ nodes in the loop body.\n+      return estimate;\n@@ -3607,1 +3612,1 @@\n-    if (policy_peeling(phase)) {    \/\/ Should we peel?\n+    if (policy_peeling(phase, false)) {    \/\/ Should we peel?\n@@ -3648,1 +3653,1 @@\n-  uint est_peeling = estimate_peeling(phase);\n+  uint est_peeling = estimate_peeling(phase, false);\n@@ -3749,0 +3754,7 @@\n+      \/\/ If the loop body has a ScopedValueGetResult for a loop invariant ScopedValue object that dominates the backedge,\n+      \/\/ then peeling one iteration of the loop body will allow the entire ScopedValue subgraph to be hoisted from the\n+      \/\/ loop body\n+      if (policy_peeling(phase, true)) {\n+        phase->do_peeling(this, old_new);\n+        return false;\n+      }\n","filename":"src\/hotspot\/share\/opto\/loopTransform.cpp","additions":18,"deletions":6,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -4605,1 +4606,1 @@\n-          !_verify_only && !bs->is_gc_specific_loop_opts_pass(_mode);\n+          !_verify_only && !bs->is_gc_specific_loop_opts_pass(_mode) && !C->has_scoped_value_get_nodes();\n@@ -4705,0 +4706,4 @@\n+  if (do_split_ifs) {\n+    C->set_has_scoped_value_get_nodes(_scoped_value_get_nodes.size() > 0);\n+  }\n+\n@@ -4815,0 +4820,4 @@\n+  if (!C->major_progress() && optimize_scoped_value_get_nodes()) {\n+    C->set_major_progress();\n+  }\n+\n@@ -4877,0 +4886,4 @@\n+  if (!C->major_progress() && do_split_ifs && expand_scoped_value_get_nodes()) {\n+    C->set_major_progress();\n+  }\n+\n@@ -4911,0 +4924,404 @@\n+\/\/ Expansion of ScopedValue nodes happen during loop opts because their expansion creates an opportunity for\n+\/\/ further loop optimizations (see comment in LateInlineScopedValueCallGenerator::process_result)\n+bool PhaseIdealLoop::expand_scoped_value_get_nodes() {\n+  bool progress = false;\n+  assert(!_igvn.delay_transform(), \"about to delay igvn transform\");\n+  _igvn.set_delay_transform(true);\n+  while (_scoped_value_get_nodes.size() > 0) {\n+    Node* n = _scoped_value_get_nodes.pop();\n+    if (n->Opcode() == Op_ScopedValueGetResult) {\n+      \/\/ Remove the ScopedValueGetResult and its projections entirely\n+      ScopedValueGetResultNode* get_result = n->as_ScopedValueGetResult();\n+      Node* result_out_proj = get_result->result_out_or_null();\n+      Node* result_in = get_result->in(ScopedValueGetResultNode::GetResult);\n+      if (result_out_proj != nullptr) {\n+        _igvn.replace_node(result_out_proj, result_in);\n+      } else {\n+        _igvn.replace_input_of(get_result, ScopedValueGetResultNode::GetResult, C->top());\n+      }\n+      lazy_replace(get_result->control_out(), get_result->in(ScopedValueGetResultNode::Control));\n+    } else {\n+      ScopedValueGetHitsInCacheNode* hits_in_cache = n->as_ScopedValueGetHitsInCache();\n+      expand_sv_get_hits_in_cache_and_load_from_cache(hits_in_cache);\n+    }\n+    progress = true;\n+  }\n+  _igvn.set_delay_transform(false);\n+  return progress;\n+}\n+\n+\/\/ On entry to this, IR shape in pseudo-code:\n+\/\/\n+\/\/ if (hits_in_the_cache) {\n+\/\/   result = load_from_cache;\n+\/\/ } else {\n+\/\/   if (cache == null) {\n+\/\/     goto slow_call;\n+\/\/   }\n+\/\/   if (first_entry_hits) {\n+\/\/     halt;\n+\/\/   } else {\n+\/\/     if (second_entry_hits) {\n+\/\/        halt;\n+\/\/      } else {\n+\/\/        goto slow_call;\n+\/\/     }\n+\/\/   }\n+\/\/ }\n+\/\/ continue:\n+\/\/ ...\n+\/\/ return;\n+\/\/\n+\/\/ slow_call:\n+\/\/ result = slowGet();\n+\/\/ goto continue;\n+\/\/\n+\/\/ The hits_in_the_cache and load_from_cache are expanded back:\n+\/\/\n+\/\/ if (cache == null) {\n+\/\/   goto slow_path;\n+\/\/ }\n+\/\/ if (first_entry_hits) {\n+\/\/   goto continue;\n+\/\/ } else {\n+\/\/   if (second_entry_hits) {\n+\/\/      goto continue;\n+\/\/    } else {\n+\/\/      goto slow_path;\n+\/\/   }\n+\/\/ }\n+\/\/ slow_path:\n+\/\/ if (cache == null) {\n+\/\/   goto slow_call;\n+\/\/ }\n+\/\/ if (first_entry_hits) {\n+\/\/   halt;\n+\/\/ } else {\n+\/\/   if (second_entry_hits) {\n+\/\/      halt;\n+\/\/    } else {\n+\/\/      goto slow_call;\n+\/\/   }\n+\/\/ }\n+\/\/ continue:\n+\/\/ ...\n+\/\/ return;\n+\/\/\n+\/\/ slow_call:\n+\/\/ result = slowGet();\n+\/\/ goto continue;\n+\/\/\n+\/\/ Split if in subsequent loop opts rounds will have a chance to clean the duplicated cache null, first_entry_hits,\n+\/\/ second_entry_hits checks\n+\/\/ The reason for having the duplicate checks is so that, if some checks branch to an uncommon trap, and a trap is hit,\n+\/\/ the right bci in the java method is marked as having trapped.\n+void PhaseIdealLoop::expand_sv_get_hits_in_cache_and_load_from_cache(ScopedValueGetHitsInCacheNode* hits_in_cache) {\n+  hits_in_cache->verify();\n+  BoolNode* bol = hits_in_cache->find_unique_out_with(Op_Bool)->as_Bool();\n+  assert(bol->_test._test == BoolTest::ne, \"unexpected ScopedValueGetHitsInCache shape\");\n+  IfNode* iff = bol->find_unique_out_with(Op_If)->as_If();\n+  ProjNode* success = iff->proj_out(1);\n+  ProjNode* failure = iff->proj_out(0);\n+\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = hits_in_cache->load_from_cache();\n+  if (load_from_cache != nullptr) {\n+    load_from_cache->verify();\n+  }\n+  Node* first_index = hits_in_cache->index1();\n+  Node* second_index = hits_in_cache->index2();\n+\n+  \/\/ The cache was always seen to be null so no code to probe the cache was added to the IR.\n+  if (first_index == C->top() && second_index == C->top()) {\n+    Node* zero = _igvn.intcon(0);\n+    set_ctrl(zero, C->root());\n+    _igvn.replace_input_of(iff, 1, zero);\n+    _igvn.replace_node(hits_in_cache, C->top());\n+    return;\n+  }\n+\n+  Node* load_of_cache = hits_in_cache->in(1);\n+\n+  Node* null_ptr = hits_in_cache->in(2);\n+  Node* cache_not_null_cmp = new CmpPNode(load_of_cache, null_ptr);\n+  _igvn.register_new_node_with_optimizer(cache_not_null_cmp);\n+  Node* cache_not_null_bol = new BoolNode(cache_not_null_cmp, BoolTest::ne);\n+  _igvn.register_new_node_with_optimizer(cache_not_null_bol);\n+  set_subtree_ctrl(cache_not_null_bol, true);\n+  IfNode* cache_not_null_iff = new IfNode(iff->in(0), cache_not_null_bol, hits_in_cache->prob_cache_exists(),\n+                                          hits_in_cache->cnt_cache_exists());\n+  IdealLoopTree* loop = get_loop(iff->in(0));\n+  register_control(cache_not_null_iff, loop, iff->in(0));\n+  Node* cache_not_null_proj = new IfTrueNode(cache_not_null_iff);\n+  register_control(cache_not_null_proj, loop, cache_not_null_iff);\n+  Node* cache_null_proj = new IfFalseNode(cache_not_null_iff);\n+  register_control(cache_null_proj, loop, cache_not_null_iff);\n+\n+  Node* not_null_load_of_cache = new CastPPNode(cache_not_null_proj, load_of_cache, _igvn.type(load_of_cache)->join(TypePtr::NOTNULL));\n+  register_new_node(not_null_load_of_cache, cache_not_null_proj);\n+\n+  Node* mem = hits_in_cache->mem();\n+\n+  Node* sv = hits_in_cache->scoped_value();\n+  Node* hit_proj = nullptr;\n+  Node* failure_proj = nullptr;\n+  Node* res = nullptr;\n+  Node* success_region = new RegionNode(3);\n+  Node* success_phi = new PhiNode(success_region, TypeInstPtr::BOTTOM);\n+  Node* failure_region = new RegionNode(3);\n+  float prob_cache_miss_at_first_if;\n+  float first_if_cnt;\n+  float prob_cache_miss_at_second_if;\n+  float second_if_cnt;\n+  find_most_likely_cache_index(hits_in_cache, first_index, second_index, prob_cache_miss_at_first_if, first_if_cnt,\n+                               prob_cache_miss_at_second_if,\n+                               second_if_cnt);\n+\n+  test_and_load_from_cache(not_null_load_of_cache, mem, first_index, cache_not_null_proj,\n+                           prob_cache_miss_at_first_if, first_if_cnt, sv, failure_proj, hit_proj, res);\n+  Node* success_region_dom = hit_proj;\n+  success_region->init_req(1, hit_proj);\n+  success_phi->init_req(1, res);\n+  if (second_index != C->top()) {\n+    test_and_load_from_cache(not_null_load_of_cache, mem, second_index, failure_proj,\n+                             prob_cache_miss_at_second_if, second_if_cnt, sv, failure_proj, hit_proj, res);\n+    success_region->init_req(2, hit_proj);\n+    success_phi->init_req(2, res);\n+    success_region_dom = success_region_dom->in(0);\n+  }\n+\n+  failure_region->init_req(1, cache_null_proj);\n+  failure_region->init_req(2, failure_proj);\n+\n+  register_control(success_region, loop, success_region_dom);\n+  register_control(failure_region, loop, cache_not_null_iff);\n+  register_new_node(success_phi, success_region);\n+\n+  Node* failure_path = failure->unique_ctrl_out();\n+\n+  lazy_replace(success, success_region);\n+  lazy_replace(failure, failure_region);\n+  if (load_from_cache != nullptr) {\n+    _igvn.replace_node(load_from_cache, success_phi);\n+  }\n+  _igvn.replace_node(hits_in_cache, C->top());\n+  lazy_update(iff, cache_not_null_iff);\n+}\n+\n+\/\/ Java code for ScopedValue.get() probes a first cache location and in case of a miss, a second one. We should have\n+\/\/ probabilities for both tests. If the second location is more likely than the first one, have it be tested first.\n+void PhaseIdealLoop::find_most_likely_cache_index(const ScopedValueGetHitsInCacheNode* hits_in_cache, Node*& first_index,\n+                                                  Node*& second_index, float& prob_cache_miss_at_first_if,\n+                                                  float& first_if_cnt, float& prob_cache_miss_at_second_if,\n+                                                  float& second_if_cnt) const {\n+  prob_cache_miss_at_first_if= hits_in_cache->prob_first_cache_probe_fails();\n+  first_if_cnt= hits_in_cache->cnt_first_cache_probe_fails();\n+  prob_cache_miss_at_second_if= hits_in_cache->prob_second_cache_probe_fails();\n+  second_if_cnt= hits_in_cache->cnt_second_cache_probe_fails();\n+  if (prob_cache_miss_at_first_if != PROB_UNKNOWN && prob_cache_miss_at_second_if != PROB_UNKNOWN) {\n+    float prob_cache_miss_at_first_index = prob_cache_miss_at_first_if;\n+    float prob_cache_hit_at_second_if = 1 - prob_cache_miss_at_second_if;\n+    \/\/ Compute the probability of a hit in the second location. We have the probability that the test at the second\n+    \/\/ location fails once the test at the first location has failed.\n+    float prob_cache_hit_at_second_index = prob_cache_miss_at_first_if * prob_cache_hit_at_second_if;\n+    float prob_cache_miss_at_second_index = 1 - prob_cache_hit_at_second_index;\n+    if (second_index != C->top() && prob_cache_miss_at_second_index < prob_cache_miss_at_first_index) {\n+      \/\/ The second location is more likely to lead to a hit than the first one. Have it be tested first.\n+      swap(first_index, second_index);\n+      swap(prob_cache_miss_at_first_index, prob_cache_miss_at_second_index);\n+      prob_cache_miss_at_first_if = prob_cache_miss_at_first_index;\n+      prob_cache_hit_at_second_index = 1 - prob_cache_miss_at_second_index;\n+      prob_cache_hit_at_second_if = prob_cache_hit_at_second_index \/ prob_cache_miss_at_first_if;\n+      prob_cache_miss_at_second_if = 1 - prob_cache_hit_at_second_if;\n+      if (first_if_cnt != COUNT_UNKNOWN) {\n+        second_if_cnt = first_if_cnt * prob_cache_miss_at_first_if;\n+      }\n+    }\n+  }\n+}\n+\n+void PhaseIdealLoop::test_and_load_from_cache(Node* load_of_cache, Node* mem, Node* index, Node* c, float prob, float cnt,\n+                                              Node* sv, Node*& failure, Node*& hit, Node*& res) {\n+  BasicType bt = TypeAryPtr::OOPS->array_element_basic_type();\n+  uint shift  = exact_log2(type2aelembytes(bt));\n+  uint header = arrayOopDesc::base_offset_in_bytes(bt);\n+\n+  Node* header_offset = _igvn.MakeConX(header);\n+  set_ctrl(header_offset, C->root());\n+  Node* base  = new AddPNode(load_of_cache, load_of_cache, header_offset);\n+  _igvn.register_new_node_with_optimizer(base);\n+  Node* casted_idx = Compile::conv_I2X_index(&_igvn, index, nullptr, c);\n+  ConINode* shift_node = _igvn.intcon(shift);\n+  set_ctrl(shift_node, C->root());\n+  Node* scale = new LShiftXNode(casted_idx, shift_node);\n+  _igvn.register_new_node_with_optimizer(scale);\n+  Node* adr = new AddPNode(load_of_cache, base, scale);\n+  _igvn.register_new_node_with_optimizer(adr);\n+\n+  DecoratorSet decorators = C2_READ_ACCESS | IN_HEAP | IS_ARRAY | C2_CONTROL_DEPENDENT_LOAD;\n+  C2AccessValuePtr addr(adr, TypeAryPtr::OOPS);\n+  C2OptAccess access(_igvn, c, mem, decorators, bt, load_of_cache, addr);\n+  BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();\n+  Node* cache_load = bs->load_at(access, TypeAryPtr::OOPS->elem());\n+\n+  Node* cmp = new CmpPNode(cache_load, sv);\n+  _igvn.register_new_node_with_optimizer(cmp);\n+  Node* bol = new BoolNode(cmp, BoolTest::ne);\n+  _igvn.register_new_node_with_optimizer(bol);\n+  set_subtree_ctrl(bol, true);\n+  IfNode* iff = new IfNode(c, bol, prob, cnt);\n+  IdealLoopTree* loop = get_loop(c);\n+  register_control(iff, loop, c);\n+  failure = new IfTrueNode(iff);\n+  register_control(failure, loop, iff);\n+  hit = new IfFalseNode(iff);\n+  register_control(hit, loop, iff);\n+\n+  index = new AddINode(index, _igvn.intcon(1));\n+  _igvn.register_new_node_with_optimizer(index);\n+  casted_idx = Compile::conv_I2X_index(&_igvn, index, nullptr, hit);\n+  scale = new LShiftXNode(casted_idx, shift_node);\n+  _igvn.register_new_node_with_optimizer(scale);\n+  adr = new AddPNode(load_of_cache, base, scale);\n+  _igvn.register_new_node_with_optimizer(adr);\n+  C2AccessValuePtr addr_res(adr, TypeAryPtr::OOPS);\n+  C2OptAccess access_res(_igvn, c, mem, decorators, bt, load_of_cache, addr_res);\n+  res = bs->load_at(access_res, TypeAryPtr::OOPS->elem());\n+  set_subtree_ctrl(res, true);\n+}\n+\n+bool PhaseIdealLoop::optimize_scoped_value_get_nodes() {\n+  bool progress = false;\n+  \/\/ Iterate in reverse order so we can remove the element we're processing from the `_scoped_value_get_nodes` list.\n+  for (uint i = _scoped_value_get_nodes.size(); i > 0; i--) {\n+    Node* n = _scoped_value_get_nodes.at(i - 1);\n+    \/\/ Look for a node that dominates n and can replace it.\n+    for (uint j = 0; j < _scoped_value_get_nodes.size(); j++) {\n+      Node* m = _scoped_value_get_nodes.at(j);\n+      if (m == n) {\n+        continue;\n+      }\n+\n+      if (hits_in_cache_replaced_by_dominating_hits_in_cache(n, m) ||\n+          hits_in_cache_replaced_by_dominating_get_result(n, m) ||\n+          get_result_replaced_by_dominating_hits_in_cache(n, m) ||\n+          get_result_replaced_by_dominating_get_result(n, m)) {\n+        _scoped_value_get_nodes.delete_at(i - 1);\n+        progress = true;\n+        break;\n+      }\n+    }\n+  }\n+  return progress;\n+}\n+\n+bool PhaseIdealLoop::hits_in_cache_replaced_by_dominating_hits_in_cache(Node* n, Node* m) {\n+  if (!n->is_ScopedValueGetHitsInCache() || !m->is_ScopedValueGetHitsInCache()) {\n+    return false;\n+  }\n+  ScopedValueGetHitsInCacheNode* hits_in_cache = n->as_ScopedValueGetHitsInCache();\n+  hits_in_cache->verify();\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = hits_in_cache->load_from_cache();\n+  if (load_from_cache != nullptr) {\n+    load_from_cache->verify();\n+  }\n+  IfNode* iff = hits_in_cache->success_proj()->in(0)->as_If();\n+  ScopedValueGetHitsInCacheNode* hits_in_cache_dom = m->as_ScopedValueGetHitsInCache();\n+  ScopedValueGetLoadFromCacheNode* load_from_cache_dom = hits_in_cache_dom->load_from_cache();\n+  IfProjNode* dom_proj = hits_in_cache_dom->success_proj();\n+  if (hits_in_cache_dom->scoped_value() != hits_in_cache->scoped_value() ||\n+      !is_dominator(dom_proj, iff)) {\n+    return false;\n+  }\n+  \/\/ The success projection of a dominating ScopedValueGetHitsInCache dominates this ScopedValueGetHitsInCache\n+  \/\/ for the same ScopedValue object: replace this ScopedValueGetHitsInCache by the dominating one\n+  _igvn.replace_node(hits_in_cache, hits_in_cache_dom);\n+  if (load_from_cache_dom != nullptr && load_from_cache != nullptr) {\n+    _igvn.replace_node(load_from_cache, load_from_cache_dom);\n+  }\n+  Node* bol = iff->in(1);\n+  dominated_by(dom_proj, iff, false, false);\n+  _igvn.replace_node(bol, C->top());\n+\n+  return true;\n+}\n+\n+bool PhaseIdealLoop::hits_in_cache_replaced_by_dominating_get_result(Node* n, Node* m) {\n+  if (!n->is_ScopedValueGetHitsInCache() || !m->is_ScopedValueGetResult()) {\n+    return false;\n+  }\n+  ScopedValueGetHitsInCacheNode* hits_in_cache = n->as_ScopedValueGetHitsInCache();\n+  hits_in_cache->verify();\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = hits_in_cache->load_from_cache();\n+  if (load_from_cache != nullptr) {\n+    load_from_cache->verify();\n+  }\n+  IfNode* iff = hits_in_cache->success_proj()->in(0)->as_If();\n+  ScopedValueGetResultNode* get_result_dom = m->as_ScopedValueGetResult();\n+  if (get_result_dom->scoped_value() != hits_in_cache->scoped_value() ||\n+      !is_dominator(get_result_dom, iff)) {\n+    return false;\n+  }\n+  \/\/ A ScopedValueGetResult dominates this ScopedValueGetHitsInCache for the same ScopedValue object:\n+  \/\/ the result of the dominating ScopedValue.get() makes this ScopedValueGetHitsInCache useless\n+  Node* one = _igvn.intcon(1);\n+  set_ctrl(one, C->root());\n+  _igvn.replace_input_of(iff, 1, one);\n+  if (load_from_cache != nullptr) {\n+    Node* result_out = get_result_dom->result_out_or_null();\n+    if (result_out == nullptr) {\n+      result_out = new ProjNode(get_result_dom, ScopedValueGetResultNode::Result);\n+      register_new_node(result_out, get_result_dom);\n+    }\n+    _igvn.replace_node(load_from_cache, result_out);\n+  }\n+  _igvn.replace_node(hits_in_cache, C->top());\n+\n+  return true;\n+}\n+\n+bool PhaseIdealLoop::get_result_replaced_by_dominating_hits_in_cache(Node* n, Node* m) {\n+  if (!n->is_ScopedValueGetResult() || !m->is_ScopedValueGetHitsInCache()) {\n+    return false;\n+  }\n+  ScopedValueGetResultNode* get_result = n->as_ScopedValueGetResult();\n+  ScopedValueGetHitsInCacheNode* hits_in_cache_dom = m->as_ScopedValueGetHitsInCache();\n+  IfProjNode* dom_proj = hits_in_cache_dom->success_proj();\n+  if (replace_scoped_value_result_by_dominator(get_result, hits_in_cache_dom->scoped_value(), dom_proj)) {\n+    \/\/ This ScopedValueGetResult is dominated by the success projection of ScopedValueGetHitsInCache for the same\n+    \/\/ ScopedValue object: either the ScopedValueGetResult and ScopedValueGetHitsInCache are from the same\n+    \/\/ ScopedValue.get() and we remove the ScopedValueGetResult because it's only useful to optimize\n+    \/\/ ScopedValue.get() where the slow path is taken. Or they are from different ScopedValue.get() and we\n+    \/\/ remove the ScopedValueGetResult. Its companion ScopedValueGetHitsInCache should be removed as well as part\n+    \/\/ of this round of optimizations.\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool PhaseIdealLoop::get_result_replaced_by_dominating_get_result(Node* n, Node* m) {\n+  if (!n->is_ScopedValueGetResult() || !m->is_ScopedValueGetResult()) {\n+    return false;\n+  }\n+  ScopedValueGetResultNode* get_result = n->as_ScopedValueGetResult();\n+  ScopedValueGetResultNode* get_result_dom = m->as_ScopedValueGetResult();\n+  if (replace_scoped_value_result_by_dominator(get_result, get_result_dom->scoped_value(), get_result_dom)) {\n+    \/\/ This ScopedValueGetResult is dominated by another ScopedValueGetResult for the same ScopedValue object:\n+    \/\/ remove this one and use the result from the dominating ScopedValue.get()\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool PhaseIdealLoop::replace_scoped_value_result_by_dominator(ScopedValueGetResultNode* get_result, Node* scoped_value_object, Node* dom_ctrl) {\n+  if (scoped_value_object == get_result->scoped_value() &&\n+      is_dominator(dom_ctrl, get_result)) {\n+    lazy_replace(get_result->control_out(), get_result->in(0));\n+    ProjNode* result_out = get_result->result_out_or_null();\n+    if (result_out != nullptr) {\n+      _igvn.replace_node(result_out, get_result->in(ScopedValueGetResultNode::GetResult));\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n@@ -6269,0 +6686,11 @@\n+  if (!_verify_only && (n->Opcode() == Op_ScopedValueGetResult || n->Opcode() == Op_ScopedValueGetHitsInCache)) {\n+#ifdef ASSERT\n+    if (n->Opcode() == Op_ScopedValueGetHitsInCache) {\n+      n->as_ScopedValueGetHitsInCache()->verify();\n+    } else if (n->Opcode() == Op_ScopedValueGetLoadFromCache) {\n+      n->as_ScopedValueGetLoadFromCache()->verify();\n+    }\n+#endif\n+    _scoped_value_get_nodes.push(n);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":429,"deletions":1,"binary":false,"changes":430,"status":"modified"},{"patch":"@@ -701,1 +701,1 @@\n-  bool policy_peeling(PhaseIdealLoop *phase);\n+  bool policy_peeling(PhaseIdealLoop* phase, bool scoped_value_only);\n@@ -703,1 +703,1 @@\n-  uint estimate_peeling(PhaseIdealLoop *phase);\n+  uint estimate_peeling(PhaseIdealLoop* phase, bool peel_only_if_has_scoped_value);\n@@ -898,0 +898,1 @@\n+  Node_List _scoped_value_get_nodes;\n@@ -1246,2 +1247,3 @@\n-  int is_member( const IdealLoopTree *loop, Node *n ) const {\n-    return loop->is_member(get_loop(n)); }\n+  bool is_member(const IdealLoopTree *loop, Node *n) const {\n+    return loop->is_member(get_loop(n));\n+  }\n@@ -1776,0 +1778,26 @@\n+  void expand_sv_get_hits_in_cache_and_load_from_cache(ScopedValueGetHitsInCacheNode* hits_in_cache);\n+  void test_and_load_from_cache(Node* load_of_cache, Node* mem, Node* index, Node* c, float prob, float cnt,\n+                                Node* sv, Node*& failure, Node*& hit, Node*& res);\n+\n+  static bool is_uncommon_or_multi_uncommon_trap_if_pattern(IfProjNode* proj);\n+\n+  bool optimize_scoped_value_get_nodes();\n+\n+  bool expand_scoped_value_get_nodes();\n+\n+  bool loop_predication_for_scoped_value_get(IdealLoopTree* loop, IfProjNode* if_success_proj,\n+                                             ParsePredicateSuccessProj* parse_predicate_proj,\n+                                             Invariance& invar, Deoptimization::DeoptReason reason,\n+                                             IfNode* iff, IfProjNode*& new_predicate_proj);\n+\n+  void move_scoped_value_nodes_to_not_peel(VectorSet &peel, VectorSet &not_peel, Node_List &peel_list,\n+                                           Node_List &sink_list, uint i) const;\n+\n+  Node* scoped_value_cache_node(Node* raw_mem);\n+\n+  void find_most_likely_cache_index(const ScopedValueGetHitsInCacheNode* hits_in_cache, Node*&first_index,\n+                                    Node*&second_index,\n+                                    float &prob_cache_miss_at_first_if, float &first_if_cnt,\n+                                    float &prob_cache_miss_at_second_if, float &second_if_cnt) const;\n+\n+  bool replace_scoped_value_result_by_dominator(ScopedValueGetResultNode* get_result, Node* scoped_value_object, Node* dom_ctrl);\n@@ -1778,0 +1806,8 @@\n+\n+  bool hits_in_cache_replaced_by_dominating_hits_in_cache(Node* n, Node* m);\n+\n+  bool hits_in_cache_replaced_by_dominating_get_result(Node* n, Node* m);\n+\n+  bool get_result_replaced_by_dominating_hits_in_cache(Node* n, Node* m);\n+\n+  bool get_result_replaced_by_dominating_get_result(Node* n, Node* m);\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":40,"deletions":4,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -1666,1 +1667,3 @@\n-      !n->is_Type()) {\n+      !n->is_Type() &&\n+      \/\/ ScopedValueGetLoadFromCache and companion ScopedValueGetHitsInCacheNode must stay together\n+      n->Opcode() != Op_ScopedValueGetLoadFromCache) {\n@@ -3777,0 +3780,4 @@\n+          } else if (n->Opcode() == Op_ScopedValueGetHitsInCache) {\n+            \/\/ ScopedValueGetLoadFromCache and companion ScopedValueGetHitsInCacheNode must stay together\n+            move_scoped_value_nodes_to_not_peel(peel, not_peel, peel_list, sink_list, i);\n+            incr = false;\n@@ -3990,0 +3997,16 @@\n+void PhaseIdealLoop::move_scoped_value_nodes_to_not_peel(VectorSet &peel, VectorSet &not_peel, Node_List &peel_list,\n+                                                         Node_List &sink_list, uint i) const {\n+  ScopedValueGetHitsInCacheNode* hits_in_cache = peel_list.at(i)->as_ScopedValueGetHitsInCache();\n+  hits_in_cache->verify();\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = hits_in_cache->load_from_cache();\n+  assert(load_from_cache == nullptr || not_peel.test(load_from_cache->_idx), \"unexpected ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+  Node* bol = hits_in_cache->find_unique_out_with(Op_Bool);\n+  assert(not_peel.test(bol->_idx), \"should be in not peel subgraph\");\n+  Node* iff = bol->unique_ctrl_out();\n+  assert(not_peel.test(iff->_idx), \"should be in not peel subgraph\");\n+  sink_list.push(hits_in_cache);\n+  peel.remove(hits_in_cache->_idx);\n+  not_peel.set(hits_in_cache->_idx);\n+  peel_list.remove(i);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -230,0 +230,73 @@\n+\n+\/\/ This handles a pattern that may show up with ScopedValue.get():\n+\/\/\n+\/\/ if (hits_in_the_cache) {\n+\/\/   result = load_from_cache;\n+\/\/ } else {\n+\/\/   if (cache == null) {\n+\/\/     unc;\n+\/\/   }\n+\/\/   if (first_entry_hits) {\n+\/\/     halt;\n+\/\/   } else {\n+\/\/     if (second_entry_hits) {\n+\/\/        halt;\n+\/\/      } else {\n+\/\/        unc;\n+\/\/     }\n+\/\/   }\n+\/\/ }\n+\/\/\n+\/\/ The paths that end with a Halt node are never taken. So in practice, all taken paths end with an uncommon trap. Loop\n+\/\/ predication takes advantage of this, to hoist:\n+\/\/ if (hits_in_the_cache) {\n+bool ProjNode::is_multi_uncommon_trap_if_pattern() {\n+  Node* iff = in(0);\n+  if (!iff->is_If() || iff->outcnt() < 2) {\n+    \/\/ Not a projection of an If or variation of a dead If node.\n+    return false;\n+  }\n+  assert(iff->in(1)->is_Bool() &&\n+         iff->in(1)->in(1)->Opcode() == Op_ScopedValueGetHitsInCache, \"this only makes sense for ScopedValueGetHitsInCache\");\n+  return other_if_proj()->is_multi_uncommon_trap_proj();\n+}\n+\n+bool ProjNode::is_multi_uncommon_trap_proj() {\n+  ResourceMark rm;\n+  Unique_Node_List wq;\n+  wq.push(this);\n+  const int path_limit = 100;\n+  uint unc_count = 0;\n+  for (uint i = 0; i < wq.size(); ++i) {\n+    Node* n = wq.at(i);\n+    if (n->is_CallStaticJava()) {\n+      CallStaticJavaNode* call = n->as_CallStaticJava();\n+      int req = call->uncommon_trap_request();\n+      if (req == 0) {\n+        return false;\n+      }\n+      unc_count++;\n+    } else if (n->is_Region() || n->is_If() || n->is_IfProj()) {\n+      for (DUIterator_Fast jmax, j = n->fast_outs(jmax); j < jmax; j++) {\n+        Node* u = n->fast_out(j);\n+        if (u->is_CFG()) {\n+          if (wq.size() >= path_limit) {\n+            return false;\n+          }\n+          wq.push(u);\n+        }\n+      }\n+    } else if (n->Opcode() != Op_Halt) {\n+      return false;\n+    }\n+  }\n+  return unc_count > 0;\n+}\n+\n+bool ProjNode::returns_pointer_from_call() const {\n+  return _con == TypeFunc::Parms && in(0)->is_Call() && in(0)->as_Call()->returns_pointer();\n+}\n+\n+bool ProjNode::is_result_from_scoped_value_get() const {\n+  return _con == ScopedValueGetResultNode::Result && in(0)->Opcode() == Op_ScopedValueGetResult;\n+}\n","filename":"src\/hotspot\/share\/opto\/multnode.cpp","additions":73,"deletions":0,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -103,0 +103,4 @@\n+  \/\/ Check if all cfg paths lead to some (possibly multiple different) uncommon trap or Halt node.\n+  \/\/ Traverse Region, If, IfProj nodes.\n+  bool is_multi_uncommon_trap_proj();\n+  bool is_multi_uncommon_trap_if_pattern();\n@@ -106,0 +110,2 @@\n+  bool returns_pointer_from_call() const;\n+  bool is_result_from_scoped_value_get() const;\n","filename":"src\/hotspot\/share\/opto\/multnode.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -977,0 +977,12 @@\n+Node* Node::find_unique_out_with(int opcode) const {\n+  Node* res = nullptr;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* use = fast_out(i);\n+    if (use->Opcode() == opcode) {\n+      assert(res == nullptr, \"only one match\");\n+      res = use;\n+    }\n+  }\n+  return res;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -194,0 +194,3 @@\n+class ScopedValueGetLoadFromCacheNode;\n+class ScopedValueGetHitsInCacheNode;\n+class ScopedValueGetResultNode;\n@@ -503,0 +506,1 @@\n+  Node* find_unique_out_with(int opcode) const;\n@@ -785,0 +789,1 @@\n+      DEFINE_CLASS_ID(ScopedValueGetHitsInCache, Sub, 1)\n@@ -801,0 +806,2 @@\n+    DEFINE_CLASS_ID(ScopedValueGetLoadFromCache, Node, 21)\n+    DEFINE_CLASS_ID(ScopedValueGetResult, Node, 22)\n@@ -802,1 +809,1 @@\n-    _max_classes  = ClassMask_Neg\n+    _max_classes  = ClassMask_ScopedValueGetResult\n@@ -1003,0 +1010,3 @@\n+  DEFINE_CLASS_QUERY(ScopedValueGetLoadFromCache)\n+  DEFINE_CLASS_QUERY(ScopedValueGetHitsInCache)\n+  DEFINE_CLASS_QUERY(ScopedValueGetResult)\n@@ -1641,4 +1651,12 @@\n-  void insert( uint i, Node *n ) { Node_Array::insert(i,n); _cnt++; }\n-  void remove( uint i ) { Node_Array::remove(i); _cnt--; }\n-  void push( Node *b ) { map(_cnt++,b); }\n-  void yank( Node *n );         \/\/ Find and remove\n+  void insert(uint i, Node *n) { Node_Array::insert(i,n); _cnt++; }\n+  \/\/ preserve order\n+  void remove(uint i) { Node_Array::remove(i); _cnt--; }\n+  \/\/ doesn't preserve order\n+  void delete_at(uint i) {\n+    Node* top_of_stack = pop();\n+    if (i < size()) {\n+      map(i, top_of_stack);\n+    }\n+  }\n+  void push(Node *b) { map(_cnt++,b); }\n+  void yank(Node *n);         \/\/ Find and remove\n@@ -1703,2 +1721,2 @@\n-  void remove( Node *n );\n-  bool member( Node *n ) { return _in_worklist.test(n->_idx) != 0; }\n+  void remove(Node *n);\n+  bool member(Node *n) const { return _in_worklist.test(n->_idx) != 0; }\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":25,"deletions":7,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+  flags(INCREMENTAL_SCOPED_VALUE_INLINE, \"Incremental Scoped Value Inline\") \\\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"opto\/intrinsicnode.hpp\"\n@@ -1976,0 +1977,28 @@\n+\n+IfProjNode* ScopedValueGetHitsInCacheNode::success_proj() const {\n+  ScopedValueGetLoadFromCacheNode* load_from_cache = this->load_from_cache();\n+  BoolNode* bol = find_unique_out_with(Op_Bool)->as_Bool();\n+  assert(bol->_test._test == BoolTest::ne, \"unexpected ScopedValueGetHitsInCache shape\");\n+  IfNode* iff = bol->find_unique_out_with(Op_If)->as_If();\n+  assert(load_from_cache == nullptr || load_from_cache->iff() == iff, \"unexpected ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+  IfProjNode* dom = iff->proj_out(1)->as_IfProj();\n+  assert(load_from_cache == nullptr || dom == load_from_cache->in(0), \"unexpected ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+  return dom;\n+}\n+\n+#ifdef ASSERT\n+void ScopedValueGetHitsInCacheNode::verify() const {\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    Node* u = fast_out(i);\n+    assert(u->is_Bool() || u->Opcode() == Op_ScopedValueGetLoadFromCache, \"wrong ScopedValueGetHitsInCache shape\");\n+  }\n+  ScopedValueGetLoadFromCacheNode* load = load_from_cache();\n+  if (load != nullptr) {\n+    assert(load->in(0)->Opcode() == Op_IfTrue, \"wrong ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+    assert(load->in(0)->in(0)->in(1)->is_Bool(), \"wrong ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+    assert(load->in(0)->in(0)->in(1)->in(1) == this, \"wrong ScopedValueGetHitsInCache\/ScopedValueGetLoadFromCache shape\");\n+  }\n+}\n+#endif\n+\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -300,0 +300,103 @@\n+\/\/ Does a ScopedValue.get() hits in the cache?\n+\/\/ This node returns true in case of cache hit (cache reference not null, and at least one of the indices leads to a hit).\n+class ScopedValueGetLoadFromCacheNode;\n+class ScopedValueGetHitsInCacheNode : public CmpNode {\n+private:\n+  \/\/ There are multiple checks involved, keep track of their profile data\n+  struct ProfileData {\n+      float _cnt;\n+      float _prob;\n+  };\n+  ProfileData _cache_exists;\n+  ProfileData _first_cache_probe_fails;\n+  ProfileData _second_cache_probe_fails;\n+\n+  virtual uint size_of() const { return sizeof(*this); }\n+  uint hash() const { return NO_HASH; }\n+\n+public:\n+  enum {\n+      ScopedValue = 3, \/\/ What ScopedValue object is it for?\n+      Memory, \/\/ Memory for the cache loads\n+      Index1, \/\/ index for the first check\n+      Index2  \/\/ index for the second check\n+  };\n+\n+  ScopedValueGetHitsInCacheNode(Compile* C, Node* c, Node* scoped_value_cache, Node* null_con, Node* mem, Node* sv,\n+                                Node* index1, Node* index2, float cnt_cache_exists, float prob_cache_exists,\n+                                float cnt_first_cache_probe_fails, float prob_first_cache_probe_fails,\n+                                float cnt_second_cache_probe_fails, float prob_second_cache_probe_fails) :\n+          CmpNode(scoped_value_cache, null_con),\n+          _cache_exists({cnt_cache_exists, prob_cache_exists }),\n+          _first_cache_probe_fails({cnt_first_cache_probe_fails, prob_first_cache_probe_fails }),\n+          _second_cache_probe_fails({cnt_second_cache_probe_fails, prob_second_cache_probe_fails }) {\n+    init_class_id(Class_ScopedValueGetHitsInCache);\n+    init_req(0, c);\n+    assert(req() == ScopedValue, \"wrong of inputs for ScopedValueGetHitsInCacheNode\");\n+    add_req(sv);\n+    assert(req() == Memory, \"wrong of inputs for ScopedValueGetHitsInCacheNode\");\n+    add_req(mem);\n+    assert(req() == Index1, \"wrong of inputs for ScopedValueGetHitsInCacheNode\");\n+    add_req(index1);\n+    assert(req() == Index2, \"wrong of inputs for ScopedValueGetHitsInCacheNode\");\n+    add_req(index2);\n+  }\n+\n+  Node* scoped_value() const {\n+    return in(ScopedValue);\n+  }\n+\n+  Node* mem() const {\n+    return in(Memory);\n+  }\n+\n+  Node* index1() const {\n+    return in(Index1);\n+  }\n+\n+  Node* index2() const {\n+    return in(Index2);\n+  }\n+\n+  ScopedValueGetLoadFromCacheNode* load_from_cache() const {\n+    return (ScopedValueGetLoadFromCacheNode*)find_unique_out_with(Op_ScopedValueGetLoadFromCache);\n+  }\n+\n+  virtual int Opcode() const;\n+\n+  const Type* sub(const Type* type, const Type* type1) const {\n+    return CmpNode::bottom_type();\n+  }\n+\n+  float prob_cache_exists() const {\n+    return _cache_exists._prob;\n+  }\n+\n+  float cnt_cache_exists() const {\n+    return _cache_exists._cnt;\n+  }\n+\n+  float prob_first_cache_probe_fails() const {\n+    return _first_cache_probe_fails._prob;\n+  }\n+\n+  float cnt_first_cache_probe_fails() const {\n+    return _first_cache_probe_fails._cnt;\n+  }\n+\n+  float prob_second_cache_probe_fails() const {\n+    return _second_cache_probe_fails._prob;\n+  }\n+\n+  float cnt_second_cache_probe_fails() const {\n+    return _second_cache_probe_fails._cnt;\n+  }\n+\n+  IfProjNode* success_proj() const;\n+\n+  void verify() const NOT_DEBUG_RETURN;\n+\n+  virtual bool depends_only_on_test() const {\n+    return false;\n+  }\n+};\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -617,0 +617,10 @@\n+  const Type** fgetfromcache = (const Type**)shared_type_arena->AmallocWords(3*sizeof(Type*));\n+  fgetfromcache[0] = TypeInt::BOOL;\n+  fgetfromcache[1] = TypeInstPtr::BOTTOM;\n+  fgetfromcache[2] = TypeAryPtr::OOPS;\n+  TypeTuple::make(3, fgetfromcache);\n+  const Type** fsvgetresult = (const Type**)shared_type_arena->AmallocWords(2*sizeof(Type*));\n+  fsvgetresult[0] = Type::CONTROL;\n+  fsvgetresult[1] = TypeInstPtr::BOTTOM;\n+  TypeTuple::SV_GET_RESULT = TypeTuple::make(2, fsvgetresult);\n+\n@@ -2125,0 +2135,1 @@\n+const TypeTuple* TypeTuple::SV_GET_RESULT;\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -749,0 +749,1 @@\n+  static const TypeTuple* SV_GET_RESULT;\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+import jdk.internal.vm.annotation.IntrinsicCandidate;\n@@ -314,1 +315,1 @@\n-        \/\/ Bit masks: a 1 in postion n indicates that this set of bound values\n+        \/\/ Bit masks: a 1 in position n indicates that this set of bound values\n@@ -677,0 +678,1 @@\n+    @IntrinsicCandidate\n@@ -697,0 +699,1 @@\n+    @IntrinsicCandidate\n@@ -774,0 +777,1 @@\n+    @ForceInline\n@@ -957,0 +961,1 @@\n+        @IntrinsicCandidate\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ScopedValue.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,708 @@\n+\/*\n+ * Copyright (c) 2024, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package compiler.c2.irTests;\n+\n+import compiler.lib.ir_framework.*;\n+import compiler.lib.ir_framework.test.TestVM;\n+import jdk.test.whitebox.WhiteBox;\n+import java.lang.reflect.Method;\n+import compiler.whitebox.CompilerWhiteBoxTest;\n+import jdk.test.lib.Platform;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+\/*\n+ * @test\n+ * @bug 8320649\n+ * @summary C2: Optimize scoped values\n+ * @library \/test\/lib \/\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @compile --enable-preview -source ${jdk.version} TestScopedValue.java\n+ * @run main\/othervm --enable-preview -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI compiler.c2.irTests.TestScopedValue\n+ *\/\n+\n+public class TestScopedValue {\n+\n+    private static final WhiteBox WHITE_BOX = WhiteBox.getWhiteBox();\n+\n+    private static long tieredStopAtLevel = (long)WHITE_BOX.getVMFlag(\"TieredStopAtLevel\");\n+\n+    static ScopedValue<MyDouble> sv = ScopedValue.newInstance();\n+    static final ScopedValue<MyDouble> svFinal = ScopedValue.newInstance();\n+    static ScopedValue<Object> svObject = ScopedValue.newInstance();\n+    private static volatile int volatileField;\n+\n+    public static void main(String[] args) {\n+        if (Platform.isComp()) {\n+            TestFramework.runWithFlags(\"--enable-preview\");\n+        } else {\n+            \/\/ Fast path tests need to be run one at a time to prevent profile pollution\n+            List<String> tests = List.of(\"testFastPath1\", \"testFastPath2\", \"testFastPath3\", \"testFastPath4\",\n+                    \"testFastPath5\", \"testFastPath6\", \"testFastPath7\", \"testFastPath8\", \"testFastPath9\",\n+                    \"testFastPath10\", \"testFastPath11\", \"testFastPath12\", \"testFastPath13\", \"testFastPath14\", \"testFastPath15\",\n+                    \"testSlowPath1,testSlowPath2,testSlowPath3,testSlowPath4,testSlowPath5,testSlowPath6,testSlowPath7,testSlowPath8,testSlowPath9,testSlowPath10\");\n+            for (String test : tests) {\n+                TestFramework.runWithFlags(\"-XX:+TieredCompilation\", \"--enable-preview\", \"-XX:CompileCommand=dontinline,java.lang.ScopedValue::slowGet\", \"-DTest=\" + test);\n+            }\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath1() {\n+        MyDouble sv1 = sv.get();\n+        MyDouble sv2 = sv.get(); \/\/ Should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testFastPath1\", mode = RunMode.STANDALONE)\n+    private void testFastPath1Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath1() != 42 + 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        forceCompilation(\"testFastPath1\");\n+    }\n+\n+    private static void forceCompilation(String name, Class<?>... parameterTypes) throws NoSuchMethodException {\n+        Method m = TestScopedValue.class.getDeclaredMethod(name, parameterTypes);\n+        WHITE_BOX.enqueueMethodForCompilation(m, CompilerWhiteBoxTest.COMP_LEVEL_FULL_OPTIMIZATION);\n+        TestFramework.assertCompiledByC2(m);\n+    }\n+\n+    @DontInline\n+    static void notInlined() {\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath2() {\n+        ScopedValue<MyDouble> scopedValue = sv;\n+        MyDouble sv1 = scopedValue.get();\n+        notInlined();\n+        MyDouble sv2 = scopedValue.get(); \/\/ Should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testFastPath2\", mode = RunMode.STANDALONE)\n+    private void testFastPath2Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath2() != 42 + 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        forceCompilation(\"testFastPath2\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"2\" })\n+    public static double testFastPath3() {\n+        MyDouble sv1 = sv.get();\n+        notInlined();\n+        MyDouble sv2 = sv.get(); \/\/ Doesn't optimize out (load of sv cannot common)\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testFastPath3\", mode = RunMode.STANDALONE)\n+    private void testFastPath3Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath3() != 42 + 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        forceCompilation(\"testFastPath3\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\", IRNode.LOOP, IRNode.COUNTED_LOOP})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath4() {\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            res = sv.get().getValue(); \/\/ should be hoisted out of loop and loop should optimize out\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath4\", mode = RunMode.STANDALONE)\n+    private void testFastPath4Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath4() != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        forceCompilation(\"testFastPath4\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 5\" })\n+    public static void testFastPath5() {\n+        Object unused = svObject.get(); \/\/ cannot be removed if result not used\n+    }\n+\n+    @Run(test = \"testFastPath5\", mode = RunMode.STANDALONE)\n+    private void testFastPath5Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath5();\n+                    }\n+                });\n+        forceCompilation(\"testFastPath5\");\n+    }\n+\n+    static Object testFastPath6Field;\n+    @ForceInline\n+    static void testFastPath6Helper(int i, Object o) {\n+        if (i != 10) {\n+            testFastPath6Field = o;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 5\" })\n+    public static void testFastPath6() {\n+        Object unused = svObject.get(); \/\/ cannot be removed even if result not used (after opts)\n+        int i;\n+        for (i = 0; i < 10; i++);\n+        testFastPath6Helper(i, unused);\n+    }\n+\n+    @Run(test = \"testFastPath6\", mode = RunMode.STANDALONE)\n+    private void testFastPath6Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath6();\n+                        testFastPath6Helper(9, null);\n+                    }\n+                });\n+        forceCompilation(\"testFastPath6\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\", IRNode.LOOP, IRNode.COUNTED_LOOP})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath7(boolean[] flags) {\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            if (flags[i]) {\n+                res = sv.get().getValue(); \/\/ Should be hoisted by predication\n+            } else {\n+                res = sv.get().getValue(); \/\/ should be hoisted by predication\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath7\", mode = RunMode.STANDALONE)\n+    private void testFastPath7Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath7(allTrue) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                        if (testFastPath7(allFalse) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        forceCompilation(\"testFastPath7\", boolean[].class);\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_D, \"1\" })\n+    public static double testFastPath8(boolean[] flags) {\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            notInlined();\n+            if (flags[i]) {\n+                res = svFinal.get().getValue(); \/\/ should be hoisted by predication\n+            } else {\n+                res = svFinal.get().getValue(); \/\/ should be hoisted by predication\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath8\", mode = RunMode.STANDALONE)\n+    private void testFastPath8Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(svFinal, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = svFinal.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        if (testFastPath8(allTrue) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                        if (testFastPath8(allFalse) != 42) {\n+                            throw new RuntimeException();\n+                        }\n+                    }\n+                });\n+        forceCompilation(\"testFastPath8\", boolean[].class);\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    public static Object testFastPath9(boolean[] flags) {\n+        \/\/ result of get() is candidate for sinking\n+        Object res = null;\n+        for (int i = 0; i < 10_000; i++) {\n+            notInlined();\n+            res = svObject.get();\n+            if (flags[i]) {\n+                break;\n+            }\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testFastPath9\", mode = RunMode.STANDALONE)\n+    private void testFastPath9Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(svObject, new MyDouble(42)).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath9(allTrue);\n+                        testFastPath9(allFalse);\n+                    }\n+                });\n+        forceCompilation(\"testFastPath9\", boolean[].class);\n+    }\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    public static Object testFastPath10(boolean[] flags) {\n+        for (int i = 0; i < 10_000; i++) {\n+            volatileField = 0x42;\n+            final boolean flag = flags[i];\n+            Object res = svObject.get(); \/\/ result used out of loop\n+            if (flag) {\n+                return res;\n+            }\n+        }\n+        return null;\n+    }\n+\n+    @Run(test = \"testFastPath10\", mode = RunMode.STANDALONE)\n+    private void testFastPath10Runner() throws Exception {\n+        boolean[] allTrue = new boolean[10_000];\n+        Arrays.fill(allTrue, true);\n+        boolean[] allFalse = new boolean[10_000];\n+        ScopedValue.where(svObject, new MyDouble(42)).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath10(allTrue);\n+                        testFastPath10(allFalse);\n+                    }\n+                });\n+        forceCompilation(\"testFastPath10\", boolean[].class);\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    public static Object testFastPath11() {\n+        \/\/ test commoning when the result of one is unused\n+        Object unused = svObject.get();\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testFastPath11\", mode = RunMode.STANDALONE)\n+    private void testFastPath11Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath11();\n+                    }\n+                });\n+        forceCompilation(\"testFastPath11\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    public static Object testFastPath12() {\n+        \/\/ test commoning when the result of one is unused\n+        int i;\n+        for (i = 0; i < 10; i++) {\n+\n+        }\n+        final Object result = testFastPath12Inlined(i);\n+        Object unused = svObject.get();\n+        return result;\n+    }\n+\n+    @ForceInline\n+    private static Object testFastPath12Inlined(int i) {\n+        Object result = null;\n+        if (i == 10) {\n+            result = svObject.get();\n+        }\n+        return result;\n+    }\n+\n+    @Run(test = \"testFastPath12\", mode = RunMode.STANDALONE)\n+    private void testFastPath12Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath12();\n+                        testFastPath12Inlined(0);\n+                    }\n+                });\n+        forceCompilation(\"testFastPath12\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 6\" })\n+    public static Object testFastPath13() {\n+        \/\/ checks code shape once fully expanded\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testFastPath13\", mode = RunMode.STANDALONE)\n+    private void testFastPath13Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath13();\n+                    }\n+                });\n+        forceCompilation(\"testFastPath13\");\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.CALL_OF_METHOD, \"slowGet\"})\n+    @IR(counts = {IRNode.LOAD_VECTOR_D, \">=1\" })\n+    public static void testFastPath14(double[] src, double[] dst) {\n+        for (int i = 0; i < 10_000; i++) {\n+            dst[i] = src[i] * sv.get().getValue();\n+        }\n+    }\n+\n+    @Run(test = \"testFastPath14\", mode = RunMode.STANDALONE)\n+    private void testFastPath14Runner() throws Exception {\n+        double[] array = new double[10_000];\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    MyDouble unused = sv.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath14(array, array);\n+                    }\n+                });\n+        forceCompilation(\"testFastPath14\", double[].class, double[].class);\n+    }\n+\n+    \/\/ Check uncommon trap is recorded at the right byte code (a cache miss) so on re-compilation,\n+    \/\/ the cache null check still branches to an uncommon trap\n+    @Test\n+    @IR(counts = {IRNode.UNSTABLE_IF_TRAP, \">= 1\" })\n+    public static Object testFastPath15() {\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testFastPath15\", mode = RunMode.STANDALONE)\n+    private void testFastPath15Runner() throws Exception {\n+        \/\/ Profile data will report a single of the 2 cache locations as a hit\n+        runAndCompile15();\n+        \/\/ Force a cache miss\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testFastPath15();\n+                });\n+        Method m = TestScopedValue.class.getDeclaredMethod(\"testFastPath15\");\n+        TestFramework.assertDeoptimizedByC2(m);\n+        \/\/ Compile again\n+        runAndCompile15();\n+    }\n+\n+    private static void runAndCompile15() throws NoSuchMethodException {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    Object unused = svObject.get();\n+                    for (int i = 0; i < 20_000; i++) {\n+                        testFastPath15();\n+                    }\n+                });\n+        forceCompilation(\"testFastPath15\");\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_D, \"1\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static double testSlowPath1() {\n+        ScopedValue<MyDouble> localSV = sv;\n+        MyDouble sv1 = localSV.get();\n+        MyDouble sv2 = localSV.get(); \/\/ should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testSlowPath1\")\n+    private void testSlowPath1Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath1() != 42 + 42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_D, \"1\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static double testSlowPath2() {\n+        ScopedValue<MyDouble> localSV = sv;\n+        MyDouble sv1 = localSV.get();\n+        notInlined();\n+        MyDouble sv2 = localSV.get(); \/\/ should optimize out\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testSlowPath2\")\n+    private void testSlowPath2Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath2() != 42 + 42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static void testSlowPath3() {\n+        Object unused = svObject.get(); \/\/ Can't be optimized out even tough result is unused\n+    }\n+\n+    @Run(test = \"testSlowPath3\")\n+    private void testSlowPath3Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath3();\n+                });\n+    }\n+\n+    static Object testSlowPath4Field;\n+    @ForceInline\n+    static void testSlowPath4Helper(int i, Object o) {\n+        if (i != 10) {\n+            testSlowPath4Field = o;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 4\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static void testSlowPath4() {\n+        Object unused = svObject.get(); \/\/ Can't be optimized out even tough result is unused (after opts)\n+        int i;\n+        for (i = 0; i < 10; i++);\n+        testSlowPath4Helper(i, unused);\n+    }\n+\n+    @Run(test = \"testSlowPath4\")\n+    private void testSlowPath4Runner() throws Exception {\n+        testSlowPath4Helper(9, null);\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath4();\n+                });\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.LOOP, IRNode.COUNTED_LOOP})\n+    @IR(counts = {IRNode.LOAD_D, \"1\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static double testSlowPath5() {\n+        ScopedValue<MyDouble> localSV = sv;\n+        double res = 0;\n+        for (int i = 0; i < 10_000; i++) {\n+            res = localSV.get().getValue(); \/\/ one iteration of the loop should be peeled to optimize get() out of loop\n+        }\n+        return res;\n+    }\n+\n+    @Run(test = \"testSlowPath5\")\n+    private void testSlowPath5Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath5() != 42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+\n+    @Test\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"2\" })\n+    public static double testSlowPath6() {\n+        \/\/ Should not optimize because of where() call\n+        ScopedValue<MyDouble> localSV = sv;\n+        MyDouble sv1 = localSV.get();\n+        MyDouble sv2 = ScopedValue.where(sv, new MyDouble(0x42)).get(() -> localSV.get());\n+        return sv1.getValue() + sv2.getValue();\n+    }\n+\n+    @Run(test = \"testSlowPath6\")\n+    private void testSlowPath6Runner() throws Exception {\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    if (testSlowPath6() != 42 + 0x42) {\n+                        throw new RuntimeException();\n+                    }\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static Object testSlowPath7() {\n+        \/\/ test optimization of redundant get() when one doesn't use its result\n+        final ScopedValue<Object> scopedValue = svObject;\n+        Object unused = scopedValue.get();\n+        return scopedValue.get();\n+    }\n+\n+    @Run(test = \"testSlowPath7\")\n+    private void testSlowPath7Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath7();\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">= 3\", IRNode.LOAD_P_OR_N, \">= 5\" })\n+    @IR(counts = {IRNode.IF, \"<= 4\", IRNode.LOAD_P_OR_N, \"<= 7\" })\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static Object testSlowPath8() {\n+        \/\/ test optimization of redundant get() when one doesn't use its result\n+        final ScopedValue<Object> scopedValue = svObject;\n+        Object result = scopedValue.get();\n+        Object unused = scopedValue.get();\n+        return result;\n+    }\n+\n+    @Run(test = \"testSlowPath8\")\n+    private void testSlowPath8Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath8();\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.IF, \">=3\", IRNode.LOAD_P_OR_N, \">=5\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    @IR(counts = {IRNode.IF, \"<=4\", IRNode.LOAD_P_OR_N, \"<=7\", IRNode.CALL_OF_METHOD, \"slowGet\", \"1\" })\n+    public static Object testSlowPath9() {\n+        \/\/ Test right pattern once fully expanded\n+        return svObject.get();\n+    }\n+\n+    @Run(test = \"testSlowPath9\")\n+    private void testSlowPath9Runner() throws Exception {\n+        ScopedValue.where(svObject, new Object()).run(\n+                () -> {\n+                    testSlowPath9();\n+                });\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.CALL_OF_METHOD, \"slowGet\", \"1\", IRNode.LOAD_VECTOR_D, \">=1\" })\n+    public static void testSlowPath10(double[] src, double[] dst) {\n+        ScopedValue<MyDouble> localSV = sv;\n+        for (int i = 0; i < 10_000; i++) {\n+            dst[i] = src[i] * localSV.get().getValue();\n+        }\n+    }\n+\n+    @Run(test = \"testSlowPath10\")\n+    private void testSlowPath10Runner() throws Exception {\n+        double[] array = new double[10_000];\n+        ScopedValue.where(sv, new MyDouble(42)).run(\n+                () -> {\n+                    testSlowPath10(array, array);\n+                });\n+    }\n+\n+\n+    static class MyDouble {\n+        final private double value;\n+\n+        public MyDouble(long value) {\n+            this.value = value;\n+        }\n+\n+        @ForceInline\n+        public double getValue() {\n+            return value;\n+        }\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestScopedValue.java","additions":708,"deletions":0,"binary":false,"changes":708,"status":"added"},{"patch":"@@ -683,0 +683,5 @@\n+    public static final String LOAD_P_OR_N = PREFIX + \"LOAD_P_OR_N\" + POSTFIX;\n+    static {\n+        beforeMatchingNameRegex(LOAD_P_OR_N, \"Load[PN]\");\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright (c) 2024, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8320649\n+ * @requires vm.gc.Parallel\n+ * @summary SIGSEGV in PhaseIdealLoop::get_early_ctrl()\n+ * @compile --enable-preview -source ${jdk.version} TestScopedValueBadDominatorAfterExpansion.java\n+ * @run main\/othervm --enable-preview -XX:-BackgroundCompilation -XX:-TieredCompilation -XX:+UseParallelGC TestScopedValueBadDominatorAfterExpansion\n+ *\/\n+\n+public class TestScopedValueBadDominatorAfterExpansion {\n+    static ScopedValue<Object> sv1 = ScopedValue.newInstance();\n+    static ScopedValue<Object> sv2 = ScopedValue.newInstance();\n+    private static Object field1;\n+    private static Object field2;\n+\n+    public static void main(String[] args) {\n+        Object o = new Object();\n+        for (int i = 0; i < 20_000; i++) {\n+            ScopedValue.where(sv1, o).where(sv2, o).run(\n+                    () -> {\n+                        test();\n+                    }\n+            );\n+        }\n+    }\n+\n+    private static void test() {\n+        final ScopedValue<Object> localSv2 = sv2;\n+        final ScopedValue<Object> localSv1 = sv1;\n+        if (localSv2 == null) {\n+        }\n+        Object v1 = localSv1.get();\n+        field1 = v1;\n+        Object v2 = localSv2.get();\n+        field2 = v2;\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/scoped_value\/TestScopedValueBadDominatorAfterExpansion.java","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"}]}