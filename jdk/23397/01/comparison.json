{"files":[{"patch":"@@ -4344,3 +4344,3 @@\n-  \/\/ ChaCha20 block function.  This version parallelizes by loading\n-  \/\/ individual 32-bit state elements into vectors for four blocks\n-  \/\/ (e.g. all four blocks' worth of state[0] in one register, etc.)\n+  \/\/ ChaCha20 block function.  This version parallelizes 4 quarter\n+  \/\/ round operations at a time.  It uses 16 SIMD registers to\n+  \/\/ produce 4 blocks of key stream.\n@@ -4349,1 +4349,1 @@\n-  \/\/ keystream (byte[1024]) = c_rarg1\n+  \/\/ keystream (byte[256]) = c_rarg1\n@@ -4351,2 +4351,27 @@\n-  address generate_chacha20Block_blockpar() {\n-    Label L_twoRounds, L_cc20_const;\n+  \/\/\n+  \/\/ In this approach, we load the 512-bit start state sequentially into\n+  \/\/ 4 128-bit vectors.  We then make 4 4-vector copies of that starting\n+  \/\/ state, with each successive set of 4 vectors having a +1 added into\n+  \/\/ the first 32-bit lane of the 4th vector in that group (the counter).\n+  \/\/ By doing this, we can perform the block function on 4 512-bit blocks\n+  \/\/ within one run of this intrinsic.\n+  \/\/ The alignment of the data across the 4-vector group is such that at\n+  \/\/ the start it is already aligned for the first round of each two-round\n+  \/\/ loop iteration.  In other words, the corresponding lanes of each vector\n+  \/\/ will contain the values needed for that quarter round operation (e.g.\n+  \/\/ elements 0\/4\/8\/12, 1\/5\/9\/13, 2\/6\/10\/14, etc.).\n+  \/\/ In between each full round, a lane shift must occur.  Within a loop\n+  \/\/ iteration, between the first and second rounds, the 2nd, 3rd, and 4th\n+  \/\/ vectors are rotated left 32, 64 and 96 bits, respectively.  The result\n+  \/\/ is effectively a diagonal orientation in columnar form.  After the\n+  \/\/ second full round, those registers are left-rotated again, this time\n+  \/\/ 96, 64, and 32 bits - returning the vectors to their columnar organization.\n+  \/\/ After all 10 iterations, the original state is added to each 4-vector\n+  \/\/ working state along with the add mask, and the 4 vector groups are\n+  \/\/ sequentially written to the memory dedicated for the output key stream.\n+  \/\/\n+  \/\/ For a more detailed explanation, see Goll and Gueron, \"Vectorization of\n+  \/\/ ChaCha Stream Cipher\", 2014 11th Int. Conf. on Information Technology:\n+  \/\/ New Generations, Las Vegas, NV, USA, April 2014, DOI: 10.1109\/ITNG.2014.33\n+  address generate_chacha20Block_qrpar() {\n+    Label L_Q_twoRounds, L_Q_cc20_const;\n@@ -4354,2 +4379,2 @@\n-    \/\/ onto FloatRegisters.  The first 128 bits are a counter add overlay\n-    \/\/ that adds +0\/+1\/+2\/+3 to the vector holding replicated state[12].\n+    \/\/ onto SIMD registers.  The first 128 bits are a counter add overlay\n+    \/\/ that adds +1\/+0\/+0\/+0 to the vectors holding replicated state[12].\n@@ -4357,3 +4382,4 @@\n-    __ BIND(L_cc20_const);\n-    __ emit_int64(0x0000000100000000UL);\n-    __ emit_int64(0x0000000300000002UL);\n+    \/\/ on 32-bit lanes within a SIMD register.\n+    __ BIND(L_Q_cc20_const);\n+    __ emit_int64(0x0000000000000001UL);\n+    __ emit_int64(0x0000000000000000UL);\n@@ -4369,1 +4395,0 @@\n-    int i, j;\n@@ -4375,6 +4400,23 @@\n-    const FloatRegister stateFirst = v0;\n-    const FloatRegister stateSecond = v1;\n-    const FloatRegister stateThird = v2;\n-    const FloatRegister stateFourth = v3;\n-    const FloatRegister origCtrState = v28;\n-    const FloatRegister scratch = v29;\n+    const FloatRegister aState = v0;\n+    const FloatRegister bState = v1;\n+    const FloatRegister cState = v2;\n+    const FloatRegister dState = v3;\n+    const FloatRegister a1Vec = v4;\n+    const FloatRegister b1Vec = v5;\n+    const FloatRegister c1Vec = v6;\n+    const FloatRegister d1Vec = v7;\n+    \/\/ Skip the callee-saved registers v8 - v15\n+    const FloatRegister a2Vec = v16;\n+    const FloatRegister b2Vec = v17;\n+    const FloatRegister c2Vec = v18;\n+    const FloatRegister d2Vec = v19;\n+    const FloatRegister a3Vec = v20;\n+    const FloatRegister b3Vec = v21;\n+    const FloatRegister c3Vec = v22;\n+    const FloatRegister d3Vec = v23;\n+    const FloatRegister a4Vec = v24;\n+    const FloatRegister b4Vec = v25;\n+    const FloatRegister c4Vec = v26;\n+    const FloatRegister d4Vec = v27;\n+    const FloatRegister scratch = v28;\n+    const FloatRegister addMask = v29;\n@@ -4383,30 +4425,32 @@\n-    \/\/ Organize SIMD registers in an array that facilitates\n-    \/\/ putting repetitive opcodes into loop structures.  It is\n-    \/\/ important that each grouping of 4 registers is monotonically\n-    \/\/ increasing to support the requirements of multi-register\n-    \/\/ instructions (e.g. ld4r, st4, etc.)\n-    const FloatRegister workSt[16] = {\n-         v4,  v5,  v6,  v7, v16, v17, v18, v19,\n-        v20, v21, v22, v23, v24, v25, v26, v27\n-    };\n-\n-    \/\/ Load from memory and interlace across 16 SIMD registers,\n-    \/\/ With each word from memory being broadcast to all lanes of\n-    \/\/ each successive SIMD register.\n-    \/\/      Addr(0) -> All lanes in workSt[i]\n-    \/\/      Addr(4) -> All lanes workSt[i + 1], etc.\n-    __ mov(tmpAddr, state);\n-    for (i = 0; i < 16; i += 4) {\n-      __ ld4r(workSt[i], workSt[i + 1], workSt[i + 2], workSt[i + 3], __ T4S,\n-          __ post(tmpAddr, 16));\n-    }\n-\n-    \/\/ Pull in constant data.  The first 16 bytes are the add overlay\n-    \/\/ which is applied to the vector holding the counter (state[12]).\n-    \/\/ The second 16 bytes is the index register for the 8-bit left\n-    \/\/ rotation tbl instruction.\n-    __ adr(tmpAddr, L_cc20_const);\n-    __ ldpq(origCtrState, lrot8Tbl, Address(tmpAddr));\n-    __ addv(workSt[12], __ T4S, workSt[12], origCtrState);\n-\n-    \/\/ Set up the 10 iteration loop and perform all 8 quarter round ops\n+    \/\/ Load the initial state in the first 4 quadword registers,\n+    \/\/ then copy the initial state into the next 4 quadword registers\n+    \/\/ that will be used for the working state.\n+    __ ld1(aState, bState, cState, dState, __ T16B, Address(state));\n+\n+    \/\/ Load the index register for 2 constant 128-bit data fields.\n+    \/\/ The first represents the +1\/+0\/+0\/+0 add mask.  The second is\n+    \/\/ the 8-bit left rotation.\n+    __ adr(tmpAddr, L_Q_cc20_const);\n+    __ ldpq(addMask, lrot8Tbl, Address(tmpAddr));\n+\n+    __ mov(a1Vec, __ T16B, aState);\n+    __ mov(b1Vec, __ T16B, bState);\n+    __ mov(c1Vec, __ T16B, cState);\n+    __ mov(d1Vec, __ T16B, dState);\n+\n+    __ mov(a2Vec, __ T16B, aState);\n+    __ mov(b2Vec, __ T16B, bState);\n+    __ mov(c2Vec, __ T16B, cState);\n+    __ addv(d2Vec, __ T4S, d1Vec, addMask);\n+\n+    __ mov(a3Vec, __ T16B, aState);\n+    __ mov(b3Vec, __ T16B, bState);\n+    __ mov(c3Vec, __ T16B, cState);\n+    __ addv(d3Vec, __ T4S, d2Vec, addMask);\n+\n+    __ mov(a4Vec, __ T16B, aState);\n+    __ mov(b4Vec, __ T16B, bState);\n+    __ mov(c4Vec, __ T16B, cState);\n+    __ addv(d4Vec, __ T4S, d3Vec, addMask);\n+\n+    \/\/ Set up the 10 iteration loop\n@@ -4414,19 +4458,38 @@\n-    __ BIND(L_twoRounds);\n-\n-    __ cc20_quarter_round(workSt[0], workSt[4], workSt[8], workSt[12],\n-        scratch, lrot8Tbl);\n-    __ cc20_quarter_round(workSt[1], workSt[5], workSt[9], workSt[13],\n-        scratch, lrot8Tbl);\n-    __ cc20_quarter_round(workSt[2], workSt[6], workSt[10], workSt[14],\n-        scratch, lrot8Tbl);\n-    __ cc20_quarter_round(workSt[3], workSt[7], workSt[11], workSt[15],\n-        scratch, lrot8Tbl);\n-\n-    __ cc20_quarter_round(workSt[0], workSt[5], workSt[10], workSt[15],\n-        scratch, lrot8Tbl);\n-    __ cc20_quarter_round(workSt[1], workSt[6], workSt[11], workSt[12],\n-        scratch, lrot8Tbl);\n-    __ cc20_quarter_round(workSt[2], workSt[7], workSt[8], workSt[13],\n-        scratch, lrot8Tbl);\n-    __ cc20_quarter_round(workSt[3], workSt[4], workSt[9], workSt[14],\n-        scratch, lrot8Tbl);\n+    __ BIND(L_Q_twoRounds);\n+\n+    \/\/ The first set of operations on the vectors covers the first 4 quarter\n+    \/\/ round operations:\n+    \/\/  Qround(state, 0, 4, 8,12)\n+    \/\/  Qround(state, 1, 5, 9,13)\n+    \/\/  Qround(state, 2, 6,10,14)\n+    \/\/  Qround(state, 3, 7,11,15)\n+    __ cc20_quarter_round(a1Vec, b1Vec, c1Vec, d1Vec, scratch, lrot8Tbl);\n+    __ cc20_quarter_round(a2Vec, b2Vec, c2Vec, d2Vec, scratch, lrot8Tbl);\n+    __ cc20_quarter_round(a3Vec, b3Vec, c3Vec, d3Vec, scratch, lrot8Tbl);\n+    __ cc20_quarter_round(a4Vec, b4Vec, c4Vec, d4Vec, scratch, lrot8Tbl);\n+\n+    \/\/ Shuffle the b1Vec\/c1Vec\/d1Vec to reorganize the state vectors to\n+    \/\/ diagonals. The a1Vec does not need to change orientation.\n+    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, true);\n+    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, true);\n+    __ cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, true);\n+    __ cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, true);\n+\n+    \/\/ The second set of operations on the vectors covers the second 4 quarter\n+    \/\/ round operations, now acting on the diagonals:\n+    \/\/  Qround(state, 0, 5,10,15)\n+    \/\/  Qround(state, 1, 6,11,12)\n+    \/\/  Qround(state, 2, 7, 8,13)\n+    \/\/  Qround(state, 3, 4, 9,14)\n+    __ cc20_quarter_round(a1Vec, b1Vec, c1Vec, d1Vec, scratch, lrot8Tbl);\n+    __ cc20_quarter_round(a2Vec, b2Vec, c2Vec, d2Vec, scratch, lrot8Tbl);\n+    __ cc20_quarter_round(a3Vec, b3Vec, c3Vec, d3Vec, scratch, lrot8Tbl);\n+    __ cc20_quarter_round(a4Vec, b4Vec, c4Vec, d4Vec, scratch, lrot8Tbl);\n+\n+    \/\/ Before we start the next iteration, we need to perform shuffles\n+    \/\/ on the b\/c\/d vectors to move them back to columnar organizations\n+    \/\/ from their current diagonal orientation.\n+    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, false);\n+    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, false);\n+    __ cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, false);\n+    __ cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, false);\n@@ -4436,27 +4499,34 @@\n-    __ cbnz(loopCtr, L_twoRounds);\n-\n-    __ mov(tmpAddr, state);\n-\n-    \/\/ Add the starting state back to the post-loop keystream\n-    \/\/ state.  We read\/interlace the state array from memory into\n-    \/\/ 4 registers similar to what we did in the beginning.  Then\n-    \/\/ add the counter overlay onto workSt[12] at the end.\n-    for (i = 0; i < 16; i += 4) {\n-      __ ld4r(stateFirst, stateSecond, stateThird, stateFourth, __ T4S,\n-          __ post(tmpAddr, 16));\n-      __ addv(workSt[i], __ T4S, workSt[i], stateFirst);\n-      __ addv(workSt[i + 1], __ T4S, workSt[i + 1], stateSecond);\n-      __ addv(workSt[i + 2], __ T4S, workSt[i + 2], stateThird);\n-      __ addv(workSt[i + 3], __ T4S, workSt[i + 3], stateFourth);\n-    }\n-    __ addv(workSt[12], __ T4S, workSt[12], origCtrState);    \/\/ Add ctr mask\n-\n-    \/\/ Write to key stream, storing the same element out of workSt[0..15]\n-    \/\/ to consecutive 4-byte offsets in the key stream buffer, then repeating\n-    \/\/ for the next element position.\n-    for (i = 0; i < 4; i++) {\n-      for (j = 0; j < 16; j += 4) {\n-        __ st4(workSt[j], workSt[j + 1], workSt[j + 2], workSt[j + 3], __ S, i,\n-            __ post(keystream, 16));\n-      }\n-    }\n+    __ cbnz(loopCtr, L_Q_twoRounds);\n+\n+    \/\/ Once the counter reaches zero, we fall out of the loop\n+    \/\/ and need to add the initial state back into the working state\n+    \/\/ represented by the a\/b\/c\/d1Vec registers.  This is destructive\n+    \/\/ on the dState register but we no longer will need it.\n+    __ addv(a1Vec, __ T4S, a1Vec, aState);\n+    __ addv(b1Vec, __ T4S, b1Vec, bState);\n+    __ addv(c1Vec, __ T4S, c1Vec, cState);\n+    __ addv(d1Vec, __ T4S, d1Vec, dState);\n+\n+    __ addv(a2Vec, __ T4S, a2Vec, aState);\n+    __ addv(b2Vec, __ T4S, b2Vec, bState);\n+    __ addv(c2Vec, __ T4S, c2Vec, cState);\n+    __ addv(dState, __ T4S, dState, addMask);\n+    __ addv(d2Vec, __ T4S, d2Vec, dState);\n+\n+    __ addv(a3Vec, __ T4S, a3Vec, aState);\n+    __ addv(b3Vec, __ T4S, b3Vec, bState);\n+    __ addv(c3Vec, __ T4S, c3Vec, cState);\n+    __ addv(dState, __ T4S, dState, addMask);\n+    __ addv(d3Vec, __ T4S, d3Vec, dState);\n+\n+    __ addv(a4Vec, __ T4S, a4Vec, aState);\n+    __ addv(b4Vec, __ T4S, b4Vec, bState);\n+    __ addv(c4Vec, __ T4S, c4Vec, cState);\n+    __ addv(dState, __ T4S, dState, addMask);\n+    __ addv(d4Vec, __ T4S, d4Vec, dState);\n+\n+    \/\/ Write the final state back to the result buffer\n+    __ st1(a1Vec, b1Vec, c1Vec, d1Vec, __ T16B, __ post(keystream, 64));\n+    __ st1(a2Vec, b2Vec, c2Vec, d2Vec, __ T16B, __ post(keystream, 64));\n+    __ st1(a3Vec, b3Vec, c3Vec, d3Vec, __ T16B, __ post(keystream, 64));\n+    __ st1(a4Vec, b4Vec, c4Vec, d4Vec, __ T16B, __ post(keystream, 64));\n@@ -8869,1 +8939,1 @@\n-      StubRoutines::_chacha20Block = generate_chacha20Block_blockpar();\n+      StubRoutines::_chacha20Block = generate_chacha20Block_qrpar();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":165,"deletions":95,"binary":false,"changes":260,"status":"modified"}]}