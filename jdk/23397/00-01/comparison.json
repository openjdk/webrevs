{"files":[{"patch":"@@ -4351,0 +4351,25 @@\n+  \/\/\n+  \/\/ In this approach, we load the 512-bit start state sequentially into\n+  \/\/ 4 128-bit vectors.  We then make 4 4-vector copies of that starting\n+  \/\/ state, with each successive set of 4 vectors having a +1 added into\n+  \/\/ the first 32-bit lane of the 4th vector in that group (the counter).\n+  \/\/ By doing this, we can perform the block function on 4 512-bit blocks\n+  \/\/ within one run of this intrinsic.\n+  \/\/ The alignment of the data across the 4-vector group is such that at\n+  \/\/ the start it is already aligned for the first round of each two-round\n+  \/\/ loop iteration.  In other words, the corresponding lanes of each vector\n+  \/\/ will contain the values needed for that quarter round operation (e.g.\n+  \/\/ elements 0\/4\/8\/12, 1\/5\/9\/13, 2\/6\/10\/14, etc.).\n+  \/\/ In between each full round, a lane shift must occur.  Within a loop\n+  \/\/ iteration, between the first and second rounds, the 2nd, 3rd, and 4th\n+  \/\/ vectors are rotated left 32, 64 and 96 bits, respectively.  The result\n+  \/\/ is effectively a diagonal orientation in columnar form.  After the\n+  \/\/ second full round, those registers are left-rotated again, this time\n+  \/\/ 96, 64, and 32 bits - returning the vectors to their columnar organization.\n+  \/\/ After all 10 iterations, the original state is added to each 4-vector\n+  \/\/ working state along with the add mask, and the 4 vector groups are\n+  \/\/ sequentially written to the memory dedicated for the output key stream.\n+  \/\/\n+  \/\/ For a more detailed explanation, see Goll and Gueron, \"Vectorization of\n+  \/\/ ChaCha Stream Cipher\", 2014 11th Int. Conf. on Information Technology:\n+  \/\/ New Generations, Las Vegas, NV, USA, April 2014, DOI: 10.1109\/ITNG.2014.33\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"}]}