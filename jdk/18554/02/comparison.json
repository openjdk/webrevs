{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n+  assert(cb->as_nmethod()->method()->is_continuation_enter_intrinsic(), \"\");\n","filename":"src\/hotspot\/cpu\/aarch64\/continuationEntry_aarch64.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -225,1 +225,1 @@\n-    CompiledMethod* nm = sender_blob->as_compiled_method_or_null();\n+    nmethod* nm = sender_blob->as_nmethod_or_null();\n@@ -237,1 +237,1 @@\n-      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      assert(!sender_blob->is_nmethod(), \"should count return address at least\");\n@@ -246,1 +246,1 @@\n-    if (!sender_blob->is_compiled()) {\n+    if (!sender_blob->is_nmethod()) {\n@@ -300,1 +300,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -429,1 +429,1 @@\n-void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+void frame::verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp) {\n@@ -452,2 +452,2 @@\n-    CompiledMethod* sender_cm = _cb->as_compiled_method_or_null();\n-    if (sender_cm != nullptr) {\n+    nmethod* sender_nm = _cb->as_nmethod_or_null();\n+    if (sender_nm != nullptr) {\n@@ -455,3 +455,3 @@\n-      if (sender_cm->is_deopt_entry(_pc) ||\n-          sender_cm->is_deopt_mh_entry(_pc)) {\n-        verify_deopt_original_pc(sender_cm, _unextended_sp);\n+      if (sender_nm->is_deopt_entry(_pc) ||\n+          sender_nm->is_deopt_mh_entry(_pc)) {\n+        verify_deopt_original_pc(sender_nm, _unextended_sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -156,1 +156,1 @@\n-  static void verify_deopt_original_pc(   CompiledMethod* nm, intptr_t* unextended_sp);\n+  static void verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp);\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -78,1 +78,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n@@ -181,1 +181,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -243,2 +243,2 @@\n-  assert(cb()->is_compiled(), \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n@@ -420,1 +420,1 @@\n-    if (!_cb->is_compiled()) { \/\/ compiled frames do not use callee-saved registers\n+    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -870,1 +870,1 @@\n-          !CodeCache::find_blob(target)->is_compiled(),\n+          !CodeCache::find_blob(target)->is_nmethod(),\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"code\/compiledMethod.hpp\"\n+#include \"code\/nmethod.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/relocInfo_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  int argsize = is_compiled() ? (_cb->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n+  int argsize = is_compiled() ? (_cb->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n","filename":"src\/hotspot\/cpu\/aarch64\/stackChunkFrameStream_aarch64.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -83,1 +83,1 @@\n-      if (_cb->is_compiled() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n+      if (_cb->is_nmethod() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n@@ -182,1 +182,1 @@\n-      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      assert(!sender_blob->is_nmethod(), \"should count return address at least\");\n@@ -191,1 +191,1 @@\n-    if (!sender_blob->is_compiled()) {\n+    if (!sender_blob->is_nmethod()) {\n@@ -232,1 +232,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -335,1 +335,1 @@\n-void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp, bool is_method_handle_return) {\n+void frame::verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp, bool is_method_handle_return) {\n@@ -360,2 +360,2 @@\n-  CompiledMethod* sender_cm = (_cb == nullptr) ? nullptr : _cb->as_compiled_method_or_null();\n-  if (sender_cm != nullptr) {\n+  nmethod* sender_nm = (_cb == nullptr) ? nullptr : _cb->as_nmethod_or_null();\n+  if (sender_nm != nullptr) {\n@@ -365,2 +365,2 @@\n-    if (sender_cm->is_deopt_mh_entry(_pc)) {\n-      DEBUG_ONLY(verify_deopt_mh_original_pc(sender_cm, _fp));\n+    if (sender_nm->is_deopt_mh_entry(_pc)) {\n+      DEBUG_ONLY(verify_deopt_mh_original_pc(sender_nm, _fp));\n@@ -369,2 +369,2 @@\n-    else if (sender_cm->is_deopt_entry(_pc)) {\n-      DEBUG_ONLY(verify_deopt_original_pc(sender_cm, _unextended_sp));\n+    else if (sender_nm->is_deopt_entry(_pc)) {\n+      DEBUG_ONLY(verify_deopt_original_pc(sender_nm, _unextended_sp));\n@@ -372,1 +372,1 @@\n-    else if (sender_cm->is_method_handle_return(_pc)) {\n+    else if (sender_nm->is_method_handle_return(_pc)) {\n","filename":"src\/hotspot\/cpu\/arm\/frame_arm.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,2 +96,2 @@\n-  static void verify_deopt_original_pc(   CompiledMethod* nm, intptr_t* unextended_sp, bool is_method_handle_return = false);\n-  static void verify_deopt_mh_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+  static void verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp, bool is_method_handle_return = false);\n+  static void verify_deopt_mh_original_pc(nmethod* nm, intptr_t* unextended_sp) {\n","filename":"src\/hotspot\/cpu\/arm\/frame_arm.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -64,1 +64,1 @@\n-    assert(_cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb->as_nmethod()->insts_contains_inclusive(_pc),\n","filename":"src\/hotspot\/cpu\/arm\/frame_arm.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n+  assert(cb->as_nmethod()->method()->is_continuation_enter_intrinsic(), \"\");\n","filename":"src\/hotspot\/cpu\/ppc\/continuationEntry_ppc.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -93,1 +93,1 @@\n-      if (_cb->is_compiled() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n+      if (_cb->is_nmethod() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n@@ -283,1 +283,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -291,1 +291,1 @@\n-  assert(!is_compiled_frame() || !_cb->as_compiled_method()->is_deopt_entry(_pc), \"must be\");\n+  assert(!is_compiled_frame() || !_cb->as_nmethod()->is_deopt_entry(_pc), \"must be\");\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -68,1 +68,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n@@ -332,1 +332,1 @@\n-    if (!_cb->is_compiled()) { \/\/ compiled frames do not use callee-saved registers\n+    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n@@ -371,2 +371,2 @@\n-  assert(cb()->is_compiled(), \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -463,1 +463,1 @@\n-  if (cb == nullptr || !cb->is_compiled()) return false;\n+  if (cb == nullptr || !cb->is_nmethod()) return false;\n","filename":"src\/hotspot\/cpu\/ppc\/nativeInst_ppc.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  int argsize = (_cb->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  int argsize = (_cb->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n","filename":"src\/hotspot\/cpu\/ppc\/stackChunkFrameStream_ppc.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n+  assert(cb->as_nmethod()->method()->is_continuation_enter_intrinsic(), \"\");\n","filename":"src\/hotspot\/cpu\/riscv\/continuationEntry_riscv.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -214,1 +214,1 @@\n-    CompiledMethod* nm = sender_blob->as_compiled_method_or_null();\n+    nmethod* nm = sender_blob->as_nmethod_or_null();\n@@ -225,1 +225,1 @@\n-      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      assert(!sender_blob->is_nmethod(), \"should count return address at least\");\n@@ -233,1 +233,1 @@\n-    if (!sender_blob->is_compiled()) {\n+    if (!sender_blob->is_nmethod()) {\n@@ -276,1 +276,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -402,1 +402,1 @@\n-void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+void frame::verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp) {\n@@ -426,2 +426,2 @@\n-    CompiledMethod* sender_cm = _cb->as_compiled_method_or_null();\n-    if (sender_cm != nullptr) {\n+    nmethod* sender_nm = _cb->as_nmethod_or_null();\n+    if (sender_nm != nullptr) {\n@@ -429,3 +429,3 @@\n-      if (sender_cm->is_deopt_entry(_pc) ||\n-          sender_cm->is_deopt_mh_entry(_pc)) {\n-        verify_deopt_original_pc(sender_cm, _unextended_sp);\n+      if (sender_nm->is_deopt_entry(_pc) ||\n+          sender_nm->is_deopt_mh_entry(_pc)) {\n+        verify_deopt_original_pc(sender_nm, _unextended_sp);\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -189,1 +189,1 @@\n-  static void verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp);\n+  static void verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp);\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -76,1 +76,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n@@ -173,1 +173,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -234,2 +234,2 @@\n-  assert(cb()->is_compiled(), \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n@@ -416,1 +416,1 @@\n-    if (!_cb->is_compiled()) { \/\/ compiled frames do not use callee-saved registers\n+    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n","filename":"src\/hotspot\/cpu\/riscv\/frame_riscv.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-  int argsize = is_compiled() ? (_cb->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n+  int argsize = is_compiled() ? (_cb->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n","filename":"src\/hotspot\/cpu\/riscv\/stackChunkFrameStream_riscv.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -270,1 +270,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -278,1 +278,1 @@\n-  assert(!is_compiled_frame() || !_cb->as_compiled_method()->is_deopt_entry(_pc), \"must be\");\n+  assert(!is_compiled_frame() || !_cb->as_nmethod()->is_deopt_entry(_pc), \"must be\");\n","filename":"src\/hotspot\/cpu\/s390\/frame_s390.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,1 +59,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -63,1 +63,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n","filename":"src\/hotspot\/cpu\/s390\/frame_s390.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -543,1 +543,1 @@\n-void NativeMovConstReg::set_pcrel_addr(intptr_t newTarget, CompiledMethod *passed_nm \/* = nullptr *\/) {\n+void NativeMovConstReg::set_pcrel_addr(intptr_t newTarget, nmethod *passed_nm \/* = nullptr *\/) {\n@@ -568,1 +568,1 @@\n-void NativeMovConstReg::set_pcrel_data(intptr_t newData, CompiledMethod *passed_nm \/* = nullptr *\/) {\n+void NativeMovConstReg::set_pcrel_data(intptr_t newData, nmethod *passed_nm \/* = nullptr *\/) {\n","filename":"src\/hotspot\/cpu\/s390\/nativeInst_s390.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -489,2 +489,2 @@\n-  void set_pcrel_addr(intptr_t addr, CompiledMethod *nm = nullptr);\n-  void set_pcrel_data(intptr_t data, CompiledMethod *nm = nullptr);\n+  void set_pcrel_addr(intptr_t addr, nmethod *nm = nullptr);\n+  void set_pcrel_data(intptr_t data, nmethod *nm = nullptr);\n","filename":"src\/hotspot\/cpu\/s390\/nativeInst_s390.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n+  assert(cb->as_nmethod()->method()->is_continuation_enter_intrinsic(), \"\");\n","filename":"src\/hotspot\/cpu\/x86\/continuationEntry_x86.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -98,1 +98,1 @@\n-      if (_cb->is_compiled() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n+      if (_cb->is_nmethod() || _cb->is_adapter_blob() || _cb->is_runtime_stub()) {\n@@ -216,1 +216,1 @@\n-    CompiledMethod* nm = sender_blob->as_compiled_method_or_null();\n+    nmethod* nm = sender_blob->as_nmethod_or_null();\n@@ -228,1 +228,1 @@\n-      assert(!sender_blob->is_compiled(), \"should count return address at least\");\n+      assert(!sender_blob->is_nmethod(), \"should count return address at least\");\n@@ -237,1 +237,1 @@\n-    if (!sender_blob->is_compiled()) {\n+    if (!sender_blob->is_nmethod()) {\n@@ -286,1 +286,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -294,1 +294,1 @@\n-  assert(!is_compiled_frame() || !_cb->as_compiled_method()->is_deopt_entry(_pc), \"must be\");\n+  assert(!is_compiled_frame() || !_cb->as_nmethod()->is_deopt_entry(_pc), \"must be\");\n@@ -418,1 +418,1 @@\n-void frame::verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp) {\n+void frame::verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp) {\n@@ -441,2 +441,2 @@\n-    CompiledMethod* sender_cm = _cb->as_compiled_method_or_null();\n-    if (sender_cm != nullptr) {\n+    nmethod* sender_nm = _cb->as_nmethod_or_null();\n+    if (sender_nm != nullptr) {\n@@ -444,3 +444,3 @@\n-      if (sender_cm->is_deopt_entry(_pc) ||\n-          sender_cm->is_deopt_mh_entry(_pc)) {\n-        verify_deopt_original_pc(sender_cm, _unextended_sp);\n+      if (sender_nm->is_deopt_entry(_pc) ||\n+          sender_nm->is_deopt_mh_entry(_pc)) {\n+        verify_deopt_original_pc(sender_nm, _unextended_sp);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -149,1 +149,1 @@\n-  static void verify_deopt_original_pc(CompiledMethod* nm, intptr_t* unextended_sp);\n+  static void verify_deopt_original_pc(nmethod* nm, intptr_t* unextended_sp);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -73,1 +73,1 @@\n-    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_nmethod()->insts_contains_inclusive(_pc),\n@@ -167,1 +167,1 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n+  address original_pc = nmethod::get_deopt_original_pc(this);\n@@ -229,2 +229,2 @@\n-  assert(cb()->is_compiled(), \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n@@ -400,1 +400,1 @@\n-    if (!_cb->is_compiled()) { \/\/ compiled frames do not use callee-saved registers\n+    if (!_cb->is_nmethod()) { \/\/ compiled frames do not use callee-saved registers\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-  int argsize = is_compiled() ? (_cb->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n+  int argsize = is_compiled() ? (_cb->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord : 0;\n","filename":"src\/hotspot\/cpu\/x86\/stackChunkFrameStream_x86.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"code\/compiledMethod.hpp\"\n@@ -29,0 +28,1 @@\n+#include \"code\/nmethod.hpp\"\n@@ -616,1 +616,1 @@\n-      if (cb != nullptr && cb->is_compiled()) {\n+      if (cb != nullptr && cb->is_nmethod()) {\n@@ -618,5 +618,5 @@\n-        CompiledMethod* cm = cb->as_compiled_method();\n-        assert(cm->insts_contains_inclusive(pc), \"\");\n-        address deopt = cm->is_method_handle_return(pc) ?\n-          cm->deopt_mh_handler_begin() :\n-          cm->deopt_handler_begin();\n+        nmethod* nm = cb->as_nmethod();\n+        assert(nm->insts_contains_inclusive(pc), \"\");\n+        address deopt = nm->is_method_handle_return(pc) ?\n+          nm->deopt_mh_handler_begin() :\n+          nm->deopt_handler_begin();\n@@ -626,1 +626,1 @@\n-        cm->set_original_pc(&fr, pc);\n+        nm->set_original_pc(&fr, pc);\n","filename":"src\/hotspot\/os\/posix\/signals_posix.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2784,1 +2784,1 @@\n-      CompiledMethod* nm = nullptr;\n+      nmethod* nm = nullptr;\n@@ -2787,1 +2787,1 @@\n-        nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n@@ -2836,2 +2836,2 @@\n-        if (cb != nullptr && cb->is_compiled()) {\n-          CompiledMethod* cm = cb->as_compiled_method();\n+        if (cb != nullptr && cb->is_nmethod()) {\n+          nmethod* nm = cb->as_nmethod();\n@@ -2839,5 +2839,5 @@\n-          address deopt = cm->is_method_handle_return(pc) ?\n-            cm->deopt_mh_handler_begin() :\n-            cm->deopt_handler_begin();\n-          assert(cm->insts_contains_inclusive(pc), \"\");\n-          cm->set_original_pc(&fr, pc);\n+          address deopt = nm->is_method_handle_return(pc) ?\n+            nm->deopt_mh_handler_begin() :\n+            nm->deopt_handler_begin();\n+          assert(nm->insts_contains_inclusive(pc), \"\");\n+          nm->set_original_pc(&fr, pc);\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -239,1 +239,1 @@\n-               cb->is_compiled()) {\n+               cb->is_nmethod()) {\n@@ -252,1 +252,1 @@\n-               cb->is_compiled()) {\n+               cb->is_nmethod()) {\n@@ -342,1 +342,1 @@\n-        CompiledMethod* nm = cb ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = cb ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/os_aix_ppc.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -259,1 +259,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -443,1 +443,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -242,1 +242,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -326,1 +326,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -266,1 +266,1 @@\n-               cb->is_compiled()) {\n+               cb->is_nmethod()) {\n@@ -278,1 +278,1 @@\n-               cb->is_compiled()) {\n+               cb->is_nmethod()) {\n@@ -357,1 +357,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/os_linux_ppc.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -232,1 +232,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -312,1 +312,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/os_linux_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -262,1 +262,1 @@\n-        CompiledMethod* nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+        nmethod* nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1145,1 +1145,1 @@\n-        CompiledMethod* old = method->code();\n+        nmethod* old = method->code();\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1132,1 +1132,1 @@\n-      CompiledMethod* code = get_Method()->code();\n+      nmethod* code = get_Method()->code();\n@@ -1148,1 +1148,1 @@\n-    CompiledMethod* code = get_Method()->code();\n+    nmethod* code = get_Method()->code();\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -807,1 +807,1 @@\n-    CompiledMethod* nm = (entry_bci != InvocationEntryBci) ? method->lookup_osr_nmethod_for(entry_bci, comp_level, true) : method->code();\n+    nmethod* nm = (entry_bci != InvocationEntryBci) ? method->lookup_osr_nmethod_for(entry_bci, comp_level, true) : method->code();\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2420,1 +2420,1 @@\n-      CompiledMethod* nm = method->code();\n+      nmethod* nm = method->code();\n@@ -2546,1 +2546,1 @@\n-  CompiledMethod* nm = nullptr;\n+  nmethod* nm = nullptr;\n@@ -2590,1 +2590,1 @@\n-        if (cb == nullptr || !cb->is_compiled()) {\n+        if (cb == nullptr || !cb->is_nmethod()) {\n@@ -2593,1 +2593,1 @@\n-        nm = cb->as_compiled_method();\n+        nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-bool DefaultICProtectionBehaviour::lock(CompiledMethod* method) {\n+bool DefaultICProtectionBehaviour::lock(nmethod* method) {\n@@ -40,1 +40,1 @@\n-void DefaultICProtectionBehaviour::unlock(CompiledMethod* method) {\n+void DefaultICProtectionBehaviour::unlock(nmethod* method) {\n@@ -44,1 +44,1 @@\n-bool DefaultICProtectionBehaviour::is_safe(CompiledMethod* method) {\n+bool DefaultICProtectionBehaviour::is_safe(nmethod* method) {\n","filename":"src\/hotspot\/share\/code\/codeBehaviours.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -36,3 +36,3 @@\n-  virtual bool lock(CompiledMethod* method) = 0;\n-  virtual void unlock(CompiledMethod* method) = 0;\n-  virtual bool is_safe(CompiledMethod* method) = 0;\n+  virtual bool lock(nmethod* method) = 0;\n+  virtual void unlock(nmethod* method) = 0;\n+  virtual bool is_safe(nmethod* method) = 0;\n@@ -45,3 +45,3 @@\n-  virtual bool lock(CompiledMethod* method);\n-  virtual void unlock(CompiledMethod* method);\n-  virtual bool is_safe(CompiledMethod* method);\n+  virtual bool lock(nmethod* method);\n+  virtual void unlock(nmethod* method);\n+  virtual bool is_safe(nmethod* method);\n","filename":"src\/hotspot\/share\/code\/codeBehaviours.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -57,3 +57,0 @@\n-const char* CodeBlob::compiler_name() const {\n-  return compilertype2name(_type);\n-}\n@@ -67,1 +64,0 @@\n-\n@@ -80,7 +76,17 @@\n-CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled) :\n-  _code_begin(layout.code_begin()),\n-  _code_end(layout.code_end()),\n-  _content_begin(layout.content_begin()),\n-  _data_end(layout.data_end()),\n-  _relocation_begin(layout.relocation_begin()),\n-  _relocation_end(layout.relocation_end()),\n+#ifdef ASSERT\n+void CodeBlob::verify_parameters() {\n+  assert(is_aligned(_size,            oopSize), \"unaligned size\");\n+  assert(is_aligned(_header_size,     oopSize), \"unaligned size\");\n+  assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n+  assert(_data_offset <= size(), \"codeBlob is too small\");\n+  assert(code_end() == content_end(), \"must be the same - see code_end()\");\n+#ifdef COMPILER1\n+  \/\/ probably wrong for tiered\n+  assert(frame_size() >= -1, \"must use frame size or -1 for runtime stubs\");\n+#endif \/\/ COMPILER1\n+}\n+#endif\n+\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size, int relocation_size,\n+                   int content_offset, int code_offset, int frame_complete_offset, int data_offset,\n+                   int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments) :\n@@ -89,2 +95,5 @@\n-  _size(layout.size()),\n-  _header_size(layout.header_size()),\n+  _size(size),\n+  _header_size(header_size),\n+  _relocation_size(relocation_size),\n+  _content_offset(content_offset),\n+  _code_offset(code_offset),\n@@ -92,1 +101,1 @@\n-  _data_offset(layout.data_offset()),\n+  _data_offset(data_offset),\n@@ -94,3 +103,3 @@\n-  _caller_must_gc_arguments(caller_must_gc_arguments),\n-  _is_compiled(compiled),\n-  _type(type)\n+  S390_ONLY(_ctable_offset(0) COMMA)\n+  _kind(kind),\n+  _caller_must_gc_arguments(caller_must_gc_arguments)\n@@ -98,9 +107,1 @@\n-  assert(is_aligned(layout.size(),            oopSize), \"unaligned size\");\n-  assert(is_aligned(layout.header_size(),     oopSize), \"unaligned size\");\n-  assert(is_aligned(layout.relocation_size(), oopSize), \"unaligned size\");\n-  assert(layout.code_end() == layout.content_end(), \"must be the same - see code_end()\");\n-#ifdef COMPILER1\n-  \/\/ probably wrong for tiered\n-  assert(_frame_size >= -1, \"must use frame size or -1 for runtime stubs\");\n-#endif \/\/ COMPILER1\n-  S390_ONLY(_ctable_offset = 0;) \/\/ avoid uninitialized fields\n+  DEBUG_ONLY( verify_parameters(); )\n@@ -109,7 +110,3 @@\n-CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb \/*UNUSED*\/, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled) :\n-  _code_begin(layout.code_begin()),\n-  _code_end(layout.code_end()),\n-  _content_begin(layout.content_begin()),\n-  _data_end(layout.data_end()),\n-  _relocation_begin(layout.relocation_begin()),\n-  _relocation_end(layout.relocation_end()),\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size,\n+                   int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+  _oop_maps(nullptr), \/\/ will be set by set_oop_maps() call\n@@ -117,2 +114,5 @@\n-  _size(layout.size()),\n-  _header_size(layout.header_size()),\n+  _size(size),\n+  _header_size(header_size),\n+  _relocation_size(align_up(cb->total_relocation_size(), oopSize)),\n+  _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n+  _code_offset(_content_offset + cb->total_offset_of(cb->insts())),\n@@ -120,1 +120,1 @@\n-  _data_offset(layout.data_offset()),\n+  _data_offset(_content_offset + align_up(cb->total_content_size(), oopSize)),\n@@ -122,3 +122,3 @@\n-  _caller_must_gc_arguments(caller_must_gc_arguments),\n-  _is_compiled(compiled),\n-  _type(type)\n+  S390_ONLY(_ctable_offset(0) COMMA)\n+  _kind(kind),\n+  _caller_must_gc_arguments(caller_must_gc_arguments)\n@@ -126,4 +126,1 @@\n-  assert(is_aligned(_size,        oopSize), \"unaligned size\");\n-  assert(is_aligned(_header_size, oopSize), \"unaligned size\");\n-  assert(_data_offset <= _size, \"codeBlob is too small\");\n-  assert(layout.code_end() == layout.content_end(), \"must be the same - see code_end()\");\n+  DEBUG_ONLY( verify_parameters(); )\n@@ -132,5 +129,0 @@\n-#ifdef COMPILER1\n-  \/\/ probably wrong for tiered\n-  assert(_frame_size >= -1, \"must use frame size or -1 for runtime stubs\");\n-#endif \/\/ COMPILER1\n-  S390_ONLY(_ctable_offset = 0;) \/\/ avoid uninitialized fields\n@@ -139,4 +131,15 @@\n-\n-\/\/ Creates a simple CodeBlob. Sets up the size of the different regions.\n-RuntimeBlob::RuntimeBlob(const char* name, int header_size, int size, int frame_complete, int locs_size)\n-  : CodeBlob(name, compiler_none, CodeBlobLayout((address) this, size, header_size, locs_size, size), frame_complete, 0, nullptr, false \/* caller_must_gc_arguments *\/)\n+\/\/ Simple CodeBlob used for simple BufferBlob.\n+CodeBlob::CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size) :\n+  _oop_maps(nullptr),\n+  _name(name),\n+  _size(size),\n+  _header_size(header_size),\n+  _relocation_size(0),\n+  _content_offset(CodeBlob::align_code_offset(header_size)),\n+  _code_offset(_content_offset),\n+  _frame_complete_offset(CodeOffsets::frame_never_safe),\n+  _data_offset(size),\n+  _frame_size(0),\n+  S390_ONLY(_ctable_offset(0) COMMA)\n+  _kind(kind),\n+  _caller_must_gc_arguments(false)\n@@ -144,1 +147,2 @@\n-  assert(is_aligned(locs_size, oopSize), \"unaligned size\");\n+  assert(is_aligned(size,            oopSize), \"unaligned size\");\n+  assert(is_aligned(header_size,     oopSize), \"unaligned size\");\n@@ -147,0 +151,31 @@\n+void CodeBlob::purge(bool free_code_cache_data, bool unregister_nmethod) {\n+  if (_oop_maps != nullptr) {\n+    delete _oop_maps;\n+    _oop_maps = nullptr;\n+  }\n+  NOT_PRODUCT(_asm_remarks.clear());\n+  NOT_PRODUCT(_dbg_strings.clear());\n+}\n+\n+void CodeBlob::set_oop_maps(OopMapSet* p) {\n+  \/\/ Danger Will Robinson! This method allocates a big\n+  \/\/ chunk of memory, its your job to free it.\n+  if (p != nullptr) {\n+    _oop_maps = ImmutableOopMapSet::build_from(p);\n+  } else {\n+    _oop_maps = nullptr;\n+  }\n+}\n+\n+const ImmutableOopMap* CodeBlob::oop_map_for_return_address(address return_address) const {\n+  assert(_oop_maps != nullptr, \"nope\");\n+  return _oop_maps->find_map_at_offset((intptr_t) return_address - (intptr_t) code_begin());\n+}\n+\n+void CodeBlob::print_code_on(outputStream* st) {\n+  ResourceMark m;\n+  Disassembler::decode(this, st);\n+}\n+\n+\/\/-----------------------------------------------------------------------------------------\n+\/\/ Creates a RuntimeBlob from a CodeBuffer and copy code and relocation info.\n@@ -148,2 +183,0 @@\n-\/\/ Creates a RuntimeBlob from a CodeBuffer\n-\/\/ and copy code and relocation info.\n@@ -152,0 +185,1 @@\n+  CodeBlobKind kind,\n@@ -153,1 +187,0 @@\n-  int         header_size,\n@@ -155,0 +188,1 @@\n+  int         header_size,\n@@ -158,2 +192,3 @@\n-  bool        caller_must_gc_arguments\n-) : CodeBlob(name, compiler_none, CodeBlobLayout((address) this, size, header_size, cb), cb, frame_complete, frame_size, oop_maps, caller_must_gc_arguments) {\n+  bool        caller_must_gc_arguments)\n+  : CodeBlob(name, kind, cb, size, header_size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+{\n@@ -175,19 +210,0 @@\n-void CodeBlob::purge(bool free_code_cache_data, bool unregister_nmethod) {\n-  if (_oop_maps != nullptr) {\n-    delete _oop_maps;\n-    _oop_maps = nullptr;\n-  }\n-  NOT_PRODUCT(_asm_remarks.clear());\n-  NOT_PRODUCT(_dbg_strings.clear());\n-}\n-\n-void CodeBlob::set_oop_maps(OopMapSet* p) {\n-  \/\/ Danger Will Robinson! This method allocates a big\n-  \/\/ chunk of memory, its your job to free it.\n-  if (p != nullptr) {\n-    _oop_maps = ImmutableOopMapSet::build_from(p);\n-  } else {\n-    _oop_maps = nullptr;\n-  }\n-}\n-\n@@ -233,10 +249,0 @@\n-const ImmutableOopMap* CodeBlob::oop_map_for_return_address(address return_address) const {\n-  assert(_oop_maps != nullptr, \"nope\");\n-  return _oop_maps->find_map_at_offset((intptr_t) return_address - (intptr_t) code_begin());\n-}\n-\n-void CodeBlob::print_code_on(outputStream* st) {\n-  ResourceMark m;\n-  Disassembler::decode(this, st);\n-}\n-\n@@ -246,3 +252,2 @@\n-\n-BufferBlob::BufferBlob(const char* name, int size)\n-: RuntimeBlob(name, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, \/*locs_size:*\/ 0)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, int size)\n+: RuntimeBlob(name, kind, size, sizeof(BufferBlob))\n@@ -262,1 +267,1 @@\n-    blob = new (size) BufferBlob(name, size);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Blob_Buffer, size);\n@@ -271,2 +276,2 @@\n-BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb)\n-  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, 0, nullptr)\n+BufferBlob::BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size)\n+  : RuntimeBlob(name, kind, cb, size, sizeof(BufferBlob), CodeOffsets::frame_never_safe, 0, nullptr)\n@@ -275,0 +280,1 @@\n+\/\/ Used by gtest\n@@ -283,1 +289,1 @@\n-    blob = new (size) BufferBlob(name, size, cb);\n+    blob = new (size) BufferBlob(name, CodeBlobKind::Blob_Buffer, cb, size);\n@@ -304,1 +310,1 @@\n-  BufferBlob(\"I2C\/C2I adapters\", size, cb) {\n+  BufferBlob(\"I2C\/C2I adapters\", CodeBlobKind::Blob_Adapter, cb, size) {\n@@ -325,0 +331,3 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of VtableBlob\n+\n@@ -336,1 +345,1 @@\n-  BufferBlob(name, size) {\n+  BufferBlob(name, CodeBlobKind::Blob_Vtable, size) {\n@@ -407,1 +416,2 @@\n-: RuntimeBlob(name, cb, sizeof(RuntimeStub), size, frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n+: RuntimeBlob(name, CodeBlobKind::Blob_Runtime_Stub, cb, size, sizeof(RuntimeStub),\n+              frame_complete, frame_size, oop_maps, caller_must_gc_arguments)\n@@ -463,1 +473,2 @@\n-: SingletonBlob(\"DeoptimizationBlob\", cb, sizeof(DeoptimizationBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"DeoptimizationBlob\", CodeBlobKind::Blob_Deoptimization, cb,\n+                size, sizeof(DeoptimizationBlob), frame_size, oop_maps)\n@@ -512,1 +523,2 @@\n-: SingletonBlob(\"UncommonTrapBlob\", cb, sizeof(UncommonTrapBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"UncommonTrapBlob\", CodeBlobKind::Blob_Uncommon_Trap, cb,\n+                size, sizeof(UncommonTrapBlob), frame_size, oop_maps)\n@@ -548,1 +560,2 @@\n-: SingletonBlob(\"ExceptionBlob\", cb, sizeof(ExceptionBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"ExceptionBlob\", CodeBlobKind::Blob_Exception, cb,\n+                size, sizeof(ExceptionBlob), frame_size, oop_maps)\n@@ -583,1 +596,2 @@\n-: SingletonBlob(\"SafepointBlob\", cb, sizeof(SafepointBlob), size, frame_size, oop_maps)\n+: SingletonBlob(\"SafepointBlob\", CodeBlobKind::Blob_Safepoint, cb,\n+                size, sizeof(SafepointBlob), frame_size, oop_maps)\n@@ -605,0 +619,55 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of UpcallStub\n+\n+UpcallStub::UpcallStub(const char* name, CodeBuffer* cb, int size, jobject receiver, ByteSize frame_data_offset) :\n+  RuntimeBlob(name, CodeBlobKind::Blob_Upcall, cb, size, sizeof(UpcallStub),\n+              CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n+              \/* oop maps = *\/ nullptr, \/* caller must gc arguments = *\/ false),\n+  _receiver(receiver),\n+  _frame_data_offset(frame_data_offset)\n+{\n+  CodeCache::commit(this);\n+}\n+\n+void* UpcallStub::operator new(size_t s, unsigned size) throw() {\n+  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n+}\n+\n+UpcallStub* UpcallStub::create(const char* name, CodeBuffer* cb, jobject receiver, ByteSize frame_data_offset) {\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+\n+  UpcallStub* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(UpcallStub));\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) UpcallStub(name, cb, size, receiver, frame_data_offset);\n+  }\n+  if (blob == nullptr) {\n+    return nullptr; \/\/ caller must handle this\n+  }\n+\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+\n+  trace_new_stub(blob, \"UpcallStub\");\n+\n+  return blob;\n+}\n+\n+void UpcallStub::oops_do(OopClosure* f, const frame& frame) {\n+  frame_data_for_frame(frame)->old_handles->oops_do(f);\n+}\n+\n+JavaFrameAnchor* UpcallStub::jfa_for_frame(const frame& frame) const {\n+  return &frame_data_for_frame(frame)->jfa;\n+}\n+\n+void UpcallStub::free(UpcallStub* blob) {\n+  assert(blob != nullptr, \"caller must check for nullptr\");\n+  JNIHandles::destroy_global(blob->receiver());\n+  RuntimeBlob::free(blob);\n+}\n+\n+void UpcallStub::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n+  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n+}\n@@ -681,4 +750,0 @@\n-void RuntimeBlob::verify() {\n-  ShouldNotReachHere();\n-}\n-\n@@ -733,54 +798,0 @@\n-\/\/ Implementation of UpcallStub\n-\n-UpcallStub::UpcallStub(const char* name, CodeBuffer* cb, int size, jobject receiver, ByteSize frame_data_offset) :\n-  RuntimeBlob(name, cb, sizeof(UpcallStub), size, CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n-              \/* oop maps = *\/ nullptr, \/* caller must gc arguments = *\/ false),\n-  _receiver(receiver),\n-  _frame_data_offset(frame_data_offset) {\n-  CodeCache::commit(this);\n-}\n-\n-void* UpcallStub::operator new(size_t s, unsigned size) throw() {\n-  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n-}\n-\n-UpcallStub* UpcallStub::create(const char* name, CodeBuffer* cb, jobject receiver, ByteSize frame_data_offset) {\n-  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n-\n-  UpcallStub* blob = nullptr;\n-  unsigned int size = CodeBlob::allocation_size(cb, sizeof(UpcallStub));\n-  {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    blob = new (size) UpcallStub(name, cb, size, receiver, frame_data_offset);\n-  }\n-  if (blob == nullptr) {\n-    return nullptr; \/\/ caller must handle this\n-  }\n-\n-  \/\/ Track memory usage statistic after releasing CodeCache_lock\n-  MemoryService::track_code_cache_memory_usage();\n-\n-  trace_new_stub(blob, \"UpcallStub\");\n-\n-  return blob;\n-}\n-\n-void UpcallStub::oops_do(OopClosure* f, const frame& frame) {\n-  frame_data_for_frame(frame)->old_handles->oops_do(f);\n-}\n-\n-JavaFrameAnchor* UpcallStub::jfa_for_frame(const frame& frame) const {\n-  return &frame_data_for_frame(frame)->jfa;\n-}\n-\n-void UpcallStub::free(UpcallStub* blob) {\n-  assert(blob != nullptr, \"caller must check for nullptr\");\n-  JNIHandles::destroy_global(blob->receiver());\n-  RuntimeBlob::free(blob);\n-}\n-\n-void UpcallStub::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n-  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n-}\n-\n-\/\/ Misc.\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":170,"deletions":159,"binary":false,"changes":329,"status":"modified"},{"patch":"@@ -55,2 +55,1 @@\n-\/\/  CompiledMethod       : Compiled Java methods (include method that calls to native code)\n-\/\/   nmethod             : JIT Compiled Java methods\n+\/\/  nmethod              : JIT Compiled Java methods\n@@ -78,0 +77,15 @@\n+enum class CodeBlobKind : u1 {\n+  Blob_None,\n+  Blob_Nmethod,\n+  Blob_Buffer,\n+  Blob_Adapter,\n+  Blob_Vtable,\n+  Blob_MH_Adapter,\n+  Blob_Runtime_Stub,\n+  Blob_Deoptimization,\n+  Blob_Exception,\n+  Blob_Safepoint,\n+  Blob_Uncommon_Trap,\n+  Blob_Upcall,\n+  Blob_Number_Of_Kinds\n+};\n@@ -79,1 +93,0 @@\n-class CodeBlobLayout;\n@@ -90,1 +103,0 @@\n-\n@@ -92,8 +104,0 @@\n-  address    _code_begin;\n-  address    _code_end;\n-  address    _content_begin;                     \/\/ address to where content region begins (this includes consts, insts, stubs)\n-                                                 \/\/ address    _content_end - not required, for all CodeBlobs _code_end == _content_end for now\n-  address    _data_end;\n-  address    _relocation_begin;\n-  address    _relocation_end;\n-\n@@ -101,1 +105,0 @@\n-\n@@ -103,1 +106,0 @@\n-  S390_ONLY(int       _ctable_offset;)\n@@ -107,0 +109,3 @@\n+  int        _relocation_size;                   \/\/ size of relocation\n+  int        _content_offset;                    \/\/ offset to where content region begins (this includes consts, insts, stubs)\n+  int        _code_offset;                       \/\/ offset to where instructions region begins (this includes insts, stubs)\n@@ -114,1 +119,1 @@\n-  bool                _caller_must_gc_arguments;\n+  S390_ONLY(int       _ctable_offset;)\n@@ -116,2 +121,3 @@\n-  bool                _is_compiled;\n-  const CompilerType  _type;                     \/\/ CompilerType\n+  CodeBlobKind        _kind;                     \/\/ Kind of this code blob\n+\n+  bool                _caller_must_gc_arguments;\n@@ -124,6 +130,11 @@\n-  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset,\n-           int frame_size, ImmutableOopMapSet* oop_maps,\n-           bool caller_must_gc_arguments, bool compiled = false);\n-  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb, int frame_complete_offset,\n-           int frame_size, OopMapSet* oop_maps,\n-           bool caller_must_gc_arguments, bool compiled = false);\n+  DEBUG_ONLY( void verify_parameters() );\n+\n+  CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size, int relocation_size,\n+           int content_offset, int code_offset, int data_offset, int frame_complete_offset,\n+           int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments);\n+\n+  CodeBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size, int header_size,\n+           int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments);\n+\n+  \/\/ Simple CodeBlob used for simple BufferBlob.\n+  CodeBlob(const char* name, CodeBlobKind kind, int size, int header_size);\n@@ -134,2 +145,0 @@\n-  \/\/ Only used by unit test.\n-  CodeBlob() : _type(compiler_none) {}\n@@ -149,19 +158,11 @@\n-  virtual bool is_buffer_blob() const                 { return false; }\n-  virtual bool is_nmethod() const                     { return false; }\n-  virtual bool is_runtime_stub() const                { return false; }\n-  virtual bool is_deoptimization_stub() const         { return false; }\n-  virtual bool is_uncommon_trap_stub() const          { return false; }\n-  virtual bool is_exception_stub() const              { return false; }\n-  virtual bool is_safepoint_stub() const              { return false; }\n-  virtual bool is_adapter_blob() const                { return false; }\n-  virtual bool is_vtable_blob() const                 { return false; }\n-  virtual bool is_method_handles_adapter_blob() const { return false; }\n-  virtual bool is_upcall_stub() const                 { return false; }\n-  bool is_compiled() const                            { return _is_compiled; }\n-  const bool* is_compiled_addr() const                { return &_is_compiled; }\n-\n-  inline bool is_compiled_by_c1() const    { return _type == compiler_c1; };\n-  inline bool is_compiled_by_c2() const    { return _type == compiler_c2; };\n-  inline bool is_compiled_by_jvmci() const { return _type == compiler_jvmci; };\n-  const char* compiler_name() const;\n-  CompilerType compiler_type() const { return _type; }\n+  bool is_nmethod() const                     { return _kind == CodeBlobKind::Blob_Nmethod; }\n+  bool is_buffer_blob() const                 { return _kind == CodeBlobKind::Blob_Buffer; }\n+  bool is_runtime_stub() const                { return _kind == CodeBlobKind::Blob_Runtime_Stub; }\n+  bool is_deoptimization_stub() const         { return _kind == CodeBlobKind::Blob_Deoptimization; }\n+  bool is_uncommon_trap_stub() const          { return _kind == CodeBlobKind::Blob_Uncommon_Trap; }\n+  bool is_exception_stub() const              { return _kind == CodeBlobKind::Blob_Exception; }\n+  bool is_safepoint_stub() const              { return _kind == CodeBlobKind::Blob_Safepoint; }\n+  bool is_adapter_blob() const                { return _kind == CodeBlobKind::Blob_Adapter; }\n+  bool is_vtable_blob() const                 { return _kind == CodeBlobKind::Blob_Vtable; }\n+  bool is_method_handles_adapter_blob() const { return _kind == CodeBlobKind::Blob_MH_Adapter; }\n+  bool is_upcall_stub() const                 { return _kind == CodeBlobKind::Blob_Upcall; }\n@@ -170,7 +171,5 @@\n-  nmethod* as_nmethod_or_null()                { return is_nmethod() ? (nmethod*) this : nullptr; }\n-  nmethod* as_nmethod()                        { assert(is_nmethod(), \"must be nmethod\"); return (nmethod*) this; }\n-  CompiledMethod* as_compiled_method_or_null() { return is_compiled() ? (CompiledMethod*) this : nullptr; }\n-  CompiledMethod* as_compiled_method()         { assert(is_compiled(), \"must be compiled\"); return (CompiledMethod*) this; }\n-  CodeBlob* as_codeblob_or_null() const        { return (CodeBlob*) this; }\n-  UpcallStub* as_upcall_stub() const           { assert(is_upcall_stub(), \"must be upcall stub\"); return (UpcallStub*) this; }\n-  RuntimeStub* as_runtime_stub() const         { assert(is_runtime_stub(), \"must be runtime blob\"); return (RuntimeStub*) this; }\n+  nmethod* as_nmethod_or_null()               { return is_nmethod() ? (nmethod*) this : nullptr; }\n+  nmethod* as_nmethod()                       { assert(is_nmethod(), \"must be nmethod\"); return (nmethod*) this; }\n+  CodeBlob* as_codeblob_or_null() const       { return (CodeBlob*) this; }\n+  UpcallStub* as_upcall_stub() const          { assert(is_upcall_stub(), \"must be upcall stub\"); return (UpcallStub*) this; }\n+  RuntimeStub* as_runtime_stub() const        { assert(is_runtime_stub(), \"must be runtime blob\"); return (RuntimeStub*) this; }\n@@ -179,8 +178,16 @@\n-  address header_begin() const        { return (address) this; }\n-  relocInfo* relocation_begin() const { return (relocInfo*) _relocation_begin; };\n-  relocInfo* relocation_end() const   { return (relocInfo*) _relocation_end; }\n-  address content_begin() const       { return _content_begin; }\n-  address content_end() const         { return _code_end; } \/\/ _code_end == _content_end is true for all types of blobs for now, it is also checked in the constructor\n-  address code_begin() const          { return _code_begin;    }\n-  address code_end() const            { return _code_end; }\n-  address data_end() const            { return _data_end;      }\n+  address    header_begin() const             { return (address)    this; }\n+  address    header_end() const               { return ((address)   this) + _header_size; }\n+  relocInfo* relocation_begin() const         { return (relocInfo*) header_end(); }\n+  relocInfo* relocation_end() const           { return (relocInfo*)(header_end()   + _relocation_size); }\n+  address    content_begin() const            { return (address)    header_begin() + _content_offset; }\n+  address    content_end() const              { return (address)    header_begin() + _data_offset; }\n+  address    code_begin() const               { return (address)    header_begin() + _code_offset; }\n+  \/\/ code_end == content_end is true for all types of blobs for now, it is also checked in the constructor\n+  address    code_end() const                 { return (address)    header_begin() + _data_offset; }\n+  address    data_begin() const               { return (address)    header_begin() + _data_offset; }\n+  address    data_end() const                 { return (address)    header_begin() + _size; }\n+\n+  \/\/ Offsets\n+  int content_offset() const                  { return _content_offset; }\n+  int code_offset() const                     { return _code_offset; }\n+  int data_offset() const                     { return _data_offset; }\n@@ -195,5 +202,6 @@\n-  int size() const                               { return _size; }\n-  int header_size() const                        { return _header_size; }\n-  int relocation_size() const                    { return pointer_delta_as_int((address) relocation_end(), (address) relocation_begin()); }\n-  int content_size() const                       { return pointer_delta_as_int(content_end(), content_begin()); }\n-  int code_size() const                          { return pointer_delta_as_int(code_end(), code_begin()); }\n+  int size() const               { return _size; }\n+  int header_size() const        { return _header_size; }\n+  int relocation_size() const    { return pointer_delta_as_int((address) relocation_end(), (address) relocation_begin()); }\n+  int content_size() const       { return pointer_delta_as_int(content_end(), content_begin()); }\n+  int code_size() const          { return pointer_delta_as_int(code_end(), code_begin()); }\n+\n@@ -204,2 +212,0 @@\n-    _code_end = (address)this + used;\n-    _data_end = (address)this + used;\n@@ -216,2 +222,0 @@\n-  virtual bool is_not_entrant() const            { return false; }\n-\n@@ -263,91 +267,2 @@\n-class CodeBlobLayout : public StackObj {\n-private:\n-  int _size;\n-  int _header_size;\n-  int _relocation_size;\n-  int _content_offset;\n-  int _code_offset;\n-  int _data_offset;\n-  address _code_begin;\n-  address _code_end;\n-  address _content_begin;\n-  address _content_end;\n-  address _data_end;\n-  address _relocation_begin;\n-  address _relocation_end;\n-\n-public:\n-  CodeBlobLayout(address code_begin, address code_end, address content_begin, address content_end, address data_end, address relocation_begin, address relocation_end) :\n-    _size(0),\n-    _header_size(0),\n-    _relocation_size(0),\n-    _content_offset(0),\n-    _code_offset(0),\n-    _data_offset(0),\n-    _code_begin(code_begin),\n-    _code_end(code_end),\n-    _content_begin(content_begin),\n-    _content_end(content_end),\n-    _data_end(data_end),\n-    _relocation_begin(relocation_begin),\n-    _relocation_end(relocation_end)\n-  {\n-  }\n-\n-  CodeBlobLayout(const address start, int size, int header_size, int relocation_size, int data_offset) :\n-    _size(size),\n-    _header_size(header_size),\n-    _relocation_size(relocation_size),\n-    _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n-    _code_offset(_content_offset),\n-    _data_offset(data_offset)\n-  {\n-    assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n-\n-    _code_begin = (address) start + _code_offset;\n-    _code_end = (address) start + _data_offset;\n-\n-    _content_begin = (address) start + _content_offset;\n-    _content_end = (address) start + _data_offset;\n-\n-    _data_end = (address) start + _size;\n-    _relocation_begin = (address) start + _header_size;\n-    _relocation_end = _relocation_begin + _relocation_size;\n-  }\n-\n-  CodeBlobLayout(const address start, int size, int header_size, const CodeBuffer* cb) :\n-    _size(size),\n-    _header_size(header_size),\n-    _relocation_size(align_up(cb->total_relocation_size(), oopSize)),\n-    _content_offset(CodeBlob::align_code_offset(_header_size + _relocation_size)),\n-    _code_offset(_content_offset + cb->total_offset_of(cb->insts())),\n-    _data_offset(_content_offset + align_up(cb->total_content_size(), oopSize))\n-  {\n-    assert(is_aligned(_relocation_size, oopSize), \"unaligned size\");\n-\n-    _code_begin = (address) start + _code_offset;\n-    _code_end = (address) start + _data_offset;\n-\n-    _content_begin = (address) start + _content_offset;\n-    _content_end = (address) start + _data_offset;\n-\n-    _data_end = (address) start + _size;\n-    _relocation_begin = (address) start + _header_size;\n-    _relocation_end = _relocation_begin + _relocation_size;\n-  }\n-\n-  int size() const { return _size; }\n-  int header_size() const { return _header_size; }\n-  int relocation_size() const { return _relocation_size; }\n-  int content_offset() const { return _content_offset; }\n-  int code_offset() const { return _code_offset; }\n-  int data_offset() const { return _data_offset; }\n-  address code_begin() const { return _code_begin; }\n-  address code_end() const { return _code_end; }\n-  address data_end() const { return _data_end; }\n-  address relocation_begin() const { return _relocation_begin; }\n-  address relocation_end() const { return _relocation_end; }\n-  address content_begin() const { return _content_begin; }\n-  address content_end() const { return _content_end; }\n-};\n-\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ RuntimeBlob: used for non-compiled method code (adapters, stubs, blobs)\n@@ -361,3 +276,3 @@\n-  \/\/ frame_complete is the offset from the beginning of the instructions\n-  \/\/ to where the frame setup (from stackwalk viewpoint) is complete.\n-  RuntimeBlob(const char* name, int header_size, int size, int frame_complete, int locs_size);\n+  RuntimeBlob(const char* name, CodeBlobKind kind, int size, int header_size)\n+    : CodeBlob(name, kind, size, header_size)\n+  {}\n@@ -366,0 +281,2 @@\n+  \/\/ frame_complete is the offset from the beginning of the instructions\n+  \/\/ to where the frame setup (from stackwalk viewpoint) is complete.\n@@ -368,0 +285,1 @@\n+    CodeBlobKind kind,\n@@ -369,1 +287,0 @@\n-    int         header_size,\n@@ -371,0 +288,1 @@\n+    int         header_size,\n@@ -379,9 +297,0 @@\n-  void verify();\n-\n-  \/\/ OopMap for frame\n-  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { ShouldNotReachHere(); }\n-\n-  \/\/ Debugging\n-  virtual void print_on(outputStream* st) const { CodeBlob::print_on(st); }\n-  virtual void print_value_on(outputStream* st) const { CodeBlob::print_value_on(st); }\n-\n@@ -406,2 +315,2 @@\n-  BufferBlob(const char* name, int size);\n-  BufferBlob(const char* name, int size, CodeBuffer* cb);\n+  BufferBlob(const char* name, CodeBlobKind kind, int size);\n+  BufferBlob(const char* name, CodeBlobKind kind, CodeBuffer* cb, int size);\n@@ -418,3 +327,0 @@\n-  \/\/ Typing\n-  virtual bool is_buffer_blob() const            { return true; }\n-\n@@ -422,1 +328,1 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n+  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n@@ -424,3 +330,3 @@\n-  void verify();\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n+  virtual void verify() override;\n+  virtual void print_on(outputStream* st) const override;\n+  virtual void print_value_on(outputStream* st) const override;\n@@ -440,3 +346,0 @@\n-\n-  \/\/ Typing\n-  virtual bool is_adapter_blob() const { return true; }\n@@ -455,3 +358,0 @@\n-\n-  \/\/ Typing\n-  virtual bool is_vtable_blob() const { return true; }\n@@ -465,1 +365,1 @@\n-  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", size) {}\n+  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", CodeBlobKind::Blob_MH_Adapter, size) {}\n@@ -470,3 +370,0 @@\n-\n-  \/\/ Typing\n-  virtual bool is_method_handles_adapter_blob() const { return true; }\n@@ -509,3 +406,0 @@\n-  \/\/ Typing\n-  bool is_runtime_stub() const                   { return true; }\n-\n@@ -515,1 +409,1 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n+  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n@@ -517,3 +411,3 @@\n-  void verify();\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n+  virtual void verify() override;\n+  virtual void print_on(outputStream* st) const override;\n+  virtual void print_value_on(outputStream* st) const override;\n@@ -534,6 +428,7 @@\n-     const char* name,\n-     CodeBuffer* cb,\n-     int         header_size,\n-     int         size,\n-     int         frame_size,\n-     OopMapSet*  oop_maps\n+     const char*  name,\n+     CodeBlobKind kind,\n+     CodeBuffer*  cb,\n+     int          size,\n+     int          header_size,\n+     int          frame_size,\n+     OopMapSet*   oop_maps\n@@ -541,1 +436,1 @@\n-   : RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, frame_size, oop_maps)\n+   : RuntimeBlob(name, kind, cb, size, header_size, CodeOffsets::frame_never_safe, frame_size, oop_maps)\n@@ -547,4 +442,4 @@\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-  void verify(); \/\/ does nothing\n-  void print_on(outputStream* st) const;\n-  void print_value_on(outputStream* st) const;\n+  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override { \/* nothing to do *\/ }\n+  virtual void verify() override; \/\/ does nothing\n+  virtual void print_on(outputStream* st) const override;\n+  virtual void print_value_on(outputStream* st) const override;\n@@ -595,6 +490,0 @@\n-  \/\/ Typing\n-  bool is_deoptimization_stub() const { return true; }\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) { \/* Nothing to do *\/ }\n-\n@@ -602,1 +491,1 @@\n-  void print_value_on(outputStream* st) const;\n+  virtual void print_value_on(outputStream* st) const override;\n@@ -659,6 +548,0 @@\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-\n-  \/\/ Typing\n-  bool is_uncommon_trap_stub() const             { return true; }\n@@ -689,6 +572,0 @@\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-\n-  \/\/ Typing\n-  bool is_exception_stub() const                 { return true; }\n@@ -720,6 +597,0 @@\n-\n-  \/\/ GC for args\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f)  { \/* nothing to do *\/ }\n-\n-  \/\/ Typing\n-  bool is_safepoint_stub() const                 { return true; }\n@@ -762,3 +633,0 @@\n-  \/\/ Typing\n-  virtual bool is_upcall_stub() const override { return true; }\n-\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":105,"deletions":237,"binary":false,"changes":342,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -164,1 +164,0 @@\n-#define FOR_ALL_NMETHOD_HEAPS(heap) for (GrowableArrayIterator<CodeHeap*> heap = _nmethod_heaps->begin(); heap != _nmethod_heaps->end(); ++heap)\n@@ -177,1 +176,0 @@\n-GrowableArray<CodeHeap*>* CodeCache::_compiled_heaps = new(mtCode) GrowableArray<CodeHeap*> (static_cast<int>(CodeBlobType::All), mtCode);\n@@ -427,3 +425,0 @@\n-  if (code_blob_type_accepts_compiled(type)) {\n-    _compiled_heaps->insert_sorted<code_heap_compare>(heap);\n-  }\n@@ -672,2 +667,2 @@\n-  assert(cb->is_nmethod(), \"did not find an nmethod\");\n-  return (nmethod*)cb;\n+  assert(cb != nullptr, \"did not find an nmethod\");\n+  return cb->as_nmethod();\n@@ -885,1 +880,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n@@ -1014,1 +1009,1 @@\n-  FOR_ALL_NMETHOD_HEAPS(heap) {\n+  for (GrowableArrayIterator<CodeHeap*> heap = _nmethod_heaps->begin(); heap != _nmethod_heaps->end(); ++heap) {\n@@ -1181,1 +1176,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1274,6 +1269,0 @@\n-CompiledMethod* CodeCache::find_compiled(void* start) {\n-  CodeBlob *cb = find_blob(start);\n-  assert(cb == nullptr || cb->is_compiled(), \"did not find an compiled_method\");\n-  return (CompiledMethod*)cb;\n-}\n-\n@@ -1283,1 +1272,1 @@\n-static GrowableArray<CompiledMethod*>* old_compiled_method_table = nullptr;\n+static GrowableArray<nmethod*>* old_nmethod_table = nullptr;\n@@ -1285,3 +1274,3 @@\n-static void add_to_old_table(CompiledMethod* c) {\n-  if (old_compiled_method_table == nullptr) {\n-    old_compiled_method_table = new (mtCode) GrowableArray<CompiledMethod*>(100, mtCode);\n+static void add_to_old_table(nmethod* c) {\n+  if (old_nmethod_table == nullptr) {\n+    old_nmethod_table = new (mtCode) GrowableArray<nmethod*>(100, mtCode);\n@@ -1289,1 +1278,1 @@\n-  old_compiled_method_table->push(c);\n+  old_nmethod_table->push(c);\n@@ -1293,3 +1282,3 @@\n-  if (old_compiled_method_table != nullptr) {\n-    delete old_compiled_method_table;\n-    old_compiled_method_table = nullptr;\n+  if (old_nmethod_table != nullptr) {\n+    delete old_nmethod_table;\n+    old_nmethod_table = nullptr;\n@@ -1300,1 +1289,1 @@\n-void CodeCache::unregister_old_nmethod(CompiledMethod* c) {\n+void CodeCache::unregister_old_nmethod(nmethod* c) {\n@@ -1302,2 +1291,2 @@\n-  if (old_compiled_method_table != nullptr) {\n-    int index = old_compiled_method_table->find(c);\n+  if (old_nmethod_table != nullptr) {\n+    int index = old_nmethod_table->find(c);\n@@ -1305,1 +1294,1 @@\n-      old_compiled_method_table->delete_at(index);\n+      old_nmethod_table->delete_at(index);\n@@ -1313,2 +1302,2 @@\n-  if (old_compiled_method_table != nullptr) {\n-    length = old_compiled_method_table->length();\n+  if (old_nmethod_table != nullptr) {\n+    length = old_nmethod_table->length();\n@@ -1319,1 +1308,1 @@\n-      old_compiled_method_table->at(i)->metadata_do(f);\n+      old_nmethod_table->at(i)->metadata_do(f);\n@@ -1332,1 +1321,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n@@ -1334,1 +1323,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1347,1 +1336,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n@@ -1349,1 +1338,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1368,1 +1357,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1370,1 +1359,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1386,1 +1375,1 @@\n-  RelaxedCompiledMethodIterator iter(RelaxedCompiledMethodIterator::only_not_unloading);\n+  RelaxedNMethodIterator iter(RelaxedNMethodIterator::only_not_unloading);\n@@ -1388,1 +1377,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1427,1 +1416,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1429,1 +1418,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1439,1 +1428,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1441,1 +1430,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1449,1 +1438,1 @@\n-  RelaxedCompiledMethodIterator iter(RelaxedCompiledMethodIterator::only_not_unloading);\n+  RelaxedNMethodIterator iter(RelaxedNMethodIterator::only_not_unloading);\n@@ -1451,1 +1440,1 @@\n-    CompiledMethod* nm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1852,1 +1841,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::only_not_unloading);\n+  NMethodIterator iter(NMethodIterator::only_not_unloading);\n@@ -1854,1 +1843,1 @@\n-    CompiledMethod* cm = iter.method();\n+    nmethod* nm = iter.method();\n@@ -1856,1 +1845,1 @@\n-    char* method_name = cm->method()->name_and_sig_as_C_string();\n+    char* method_name = nm->method()->name_and_sig_as_C_string();\n@@ -1858,1 +1847,1 @@\n-                 cm->compile_id(), cm->comp_level(), cm->get_state(),\n+                 nm->compile_id(), nm->comp_level(), nm->get_state(),\n@@ -1860,1 +1849,1 @@\n-                 (intptr_t)cm->header_begin(), (intptr_t)cm->code_begin(), (intptr_t)cm->code_end());\n+                 (intptr_t)nm->header_begin(), (intptr_t)nm->code_begin(), (intptr_t)nm->code_end());\n@@ -1900,2 +1889,2 @@\n-      cb->is_compiled() ? cb->as_compiled_method()->method()->external_name()\n-                        : cb->name();\n+      cb->is_nmethod() ? cb->as_nmethod()->method()->external_name()\n+                       : cb->name();\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":42,"deletions":53,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -86,1 +86,1 @@\n-  template <class T, class Filter, bool is_compiled_method> friend class CodeBlobIterator;\n+  template <class T, class Filter, bool is_relaxed> friend class CodeBlobIterator;\n@@ -93,1 +93,0 @@\n-  static GrowableArray<CodeHeap*>* _compiled_heaps;\n@@ -147,1 +146,0 @@\n-  static const GrowableArray<CodeHeap*>* compiled_heaps() { return _compiled_heaps; }\n@@ -168,1 +166,0 @@\n-  static CompiledMethod* find_compiled(void* start);\n@@ -261,8 +258,3 @@\n-  \/\/ Returns the CodeBlobType for the given CompiledMethod\n-  static CodeBlobType get_code_blob_type(CompiledMethod* cm) {\n-    return get_code_heap(cm)->code_blob_type();\n-  }\n-\n-  static bool code_blob_type_accepts_compiled(CodeBlobType code_blob_type) {\n-    bool result = code_blob_type == CodeBlobType::All || code_blob_type <= CodeBlobType::MethodProfiled;\n-    return result;\n+  \/\/ Returns the CodeBlobType for the given nmethod\n+  static CodeBlobType get_code_blob_type(nmethod* nm) {\n+    return get_code_heap(nm)->code_blob_type();\n@@ -318,1 +310,1 @@\n-  static void unregister_old_nmethod(CompiledMethod* c) NOT_JVMTI_RETURN;\n+  static void unregister_old_nmethod(nmethod* c) NOT_JVMTI_RETURN;\n@@ -372,2 +364,2 @@\n-        CompiledMethod* cm = _code_blob->as_compiled_method_or_null();\n-        if (cm != nullptr && cm->is_unloading()) {\n+        nmethod* nm = _code_blob->as_nmethod_or_null();\n+        if (nm != nullptr && nm->is_unloading()) {\n@@ -445,6 +437,0 @@\n-struct CompiledMethodFilter {\n-  static bool apply(CodeBlob* cb) { return cb->is_compiled(); }\n-  static const GrowableArray<CodeHeap*>* heaps() { return CodeCache::compiled_heaps(); }\n-};\n-\n-\n@@ -461,2 +447,0 @@\n-typedef CodeBlobIterator<CompiledMethod, CompiledMethodFilter, false \/* is_relaxed *\/> CompiledMethodIterator;\n-typedef CodeBlobIterator<CompiledMethod, CompiledMethodFilter, true \/* is_relaxed *\/> RelaxedCompiledMethodIterator;\n@@ -464,0 +448,1 @@\n+typedef CodeBlobIterator<nmethod, NMethodFilter, true  \/* is_relaxed *\/> RelaxedNMethodIterator;\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":9,"deletions":24,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,2 +47,2 @@\n-    \/\/ The nMethod_* values correspond to the CompiledMethod enum values.\n-    \/\/ We can't use the CompiledMethod values 1:1 because we depend on noType == 0.\n+    \/\/ The nMethod_* values correspond to the nmethod enum values.\n+    \/\/ We can't use the nmethod values 1:1 because we depend on noType == 0.\n","filename":"src\/hotspot\/share\/code\/codeHeapState.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,1 @@\n-CompiledICLocker::CompiledICLocker(CompiledMethod* method)\n+CompiledICLocker::CompiledICLocker(nmethod* method)\n@@ -59,1 +59,1 @@\n-bool CompiledICLocker::is_safe(CompiledMethod* method) {\n+bool CompiledICLocker::is_safe(nmethod* method) {\n@@ -65,3 +65,3 @@\n-  assert(cb != nullptr && cb->is_compiled(), \"must be compiled\");\n-  CompiledMethod* cm = cb->as_compiled_method();\n-  return CompiledICProtectionBehaviour::current()->is_safe(cm);\n+  assert(cb != nullptr && cb->is_nmethod(), \"must be compiled\");\n+  nmethod* nm = cb->as_nmethod();\n+  return CompiledICProtectionBehaviour::current()->is_safe(nm);\n@@ -170,1 +170,1 @@\n-CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr) {\n+CompiledIC* CompiledIC_before(nmethod* nm, address return_addr) {\n@@ -175,1 +175,1 @@\n-CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site) {\n+CompiledIC* CompiledIC_at(nmethod* nm, address call_site) {\n@@ -183,2 +183,2 @@\n-  CompiledMethod* cm = CodeCache::find_blob(call_reloc->addr())->as_compiled_method();\n-  return CompiledIC_at(cm, call_site);\n+  nmethod* nm = CodeCache::find_blob(call_reloc->addr())->as_nmethod();\n+  return CompiledIC_at(nm, call_site);\n@@ -207,1 +207,1 @@\n-  CompiledMethod* code = method->code();\n+  nmethod* code = method->code();\n@@ -324,1 +324,1 @@\n-  \/\/ in_use is unused but needed to match template function in CompiledMethod\n+  \/\/ in_use is unused but needed to match template function in nmethod\n@@ -346,2 +346,2 @@\n-  CompiledMethod* code = callee_method->code();\n-  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n+  nmethod* code = callee_method->code();\n+  nmethod* caller = CodeCache::find_nmethod(instruction_address());\n@@ -380,2 +380,2 @@\n-  CompiledMethod* cm = CodeCache::find_compiled(instruction_address());\n-  return cm->stub_contains(destination());\n+  nmethod* nm = CodeCache::find_nmethod(instruction_address());\n+  return nm->stub_contains(destination());\n@@ -385,1 +385,1 @@\n-  CompiledMethod* caller = CodeCache::find_compiled(instruction_address());\n+  nmethod* caller = CodeCache::find_nmethod(instruction_address());\n@@ -387,1 +387,1 @@\n-  return !caller->stub_contains(destination()) && dest_cb->is_compiled();\n+  return !caller->stub_contains(destination()) && dest_cb->is_nmethod();\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -45,1 +45,1 @@\n-  CompiledMethod* _method;\n+  nmethod* _method;\n@@ -51,1 +51,1 @@\n-  CompiledICLocker(CompiledMethod* method);\n+  CompiledICLocker(nmethod* method);\n@@ -53,1 +53,1 @@\n-  static bool is_safe(CompiledMethod* method);\n+  static bool is_safe(nmethod* method);\n@@ -101,1 +101,1 @@\n-  CompiledMethod* _method;\n+  nmethod* _method;\n@@ -117,2 +117,2 @@\n-  friend CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr);\n-  friend CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site);\n+  friend CompiledIC* CompiledIC_before(nmethod* nm, address return_addr);\n+  friend CompiledIC* CompiledIC_at(nmethod* nm, address call_site);\n@@ -149,2 +149,2 @@\n-CompiledIC* CompiledIC_before(CompiledMethod* nm, address return_addr);\n-CompiledIC* CompiledIC_at(CompiledMethod* nm, address call_site);\n+CompiledIC* CompiledIC_before(nmethod* nm, address return_addr);\n+CompiledIC* CompiledIC_at(nmethod* nm, address call_site);\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1,647 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/compiledIC.hpp\"\n-#include \"code\/compiledMethod.inline.hpp\"\n-#include \"code\/exceptionHandlerTable.hpp\"\n-#include \"code\/scopeDesc.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"gc\/shared\/gcBehaviours.hpp\"\n-#include \"interpreter\/bytecode.inline.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"logging\/logTag.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"oops\/klass.inline.hpp\"\n-#include \"oops\/methodData.hpp\"\n-#include \"oops\/method.inline.hpp\"\n-#include \"oops\/weakHandle.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/deoptimization.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/jniHandles.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-\n-CompiledMethod::CompiledMethod(Method* method, const char* name, CompilerType type, const CodeBlobLayout& layout,\n-                               int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps,\n-                               bool caller_must_gc_arguments, bool compiled)\n-  : CodeBlob(name, type, layout, frame_complete_offset, frame_size, oop_maps, caller_must_gc_arguments, compiled),\n-    _deoptimization_status(not_marked),\n-    _deoptimization_generation(0),\n-    _method(method),\n-    _gc_data(nullptr)\n-{\n-  init_defaults();\n-}\n-\n-CompiledMethod::CompiledMethod(Method* method, const char* name, CompilerType type, int size,\n-                               int header_size, CodeBuffer* cb, int frame_complete_offset, int frame_size,\n-                               OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled)\n-  : CodeBlob(name, type, CodeBlobLayout((address) this, size, header_size, cb), cb,\n-             frame_complete_offset, frame_size, oop_maps, caller_must_gc_arguments, compiled),\n-    _deoptimization_status(not_marked),\n-    _deoptimization_generation(0),\n-    _method(method),\n-    _gc_data(nullptr)\n-{\n-  init_defaults();\n-}\n-\n-void CompiledMethod::init_defaults() {\n-  { \/\/ avoid uninitialized fields, even for short time periods\n-    _scopes_data_begin          = nullptr;\n-    _deopt_handler_begin        = nullptr;\n-    _deopt_mh_handler_begin     = nullptr;\n-    _exception_cache            = nullptr;\n-  }\n-  _has_unsafe_access          = 0;\n-  _has_method_handle_invokes  = 0;\n-  _has_wide_vectors           = 0;\n-  _has_monitors               = 0;\n-}\n-\n-bool CompiledMethod::is_method_handle_return(address return_pc) {\n-  if (!has_method_handle_invokes())  return false;\n-  PcDesc* pd = pc_desc_at(return_pc);\n-  if (pd == nullptr)\n-    return false;\n-  return pd->is_method_handle_invoke();\n-}\n-\n-\/\/ Returns a string version of the method state.\n-const char* CompiledMethod::state() const {\n-  int state = get_state();\n-  switch (state) {\n-  case not_installed:\n-    return \"not installed\";\n-  case in_use:\n-    return \"in use\";\n-  case not_used:\n-    return \"not_used\";\n-  case not_entrant:\n-    return \"not_entrant\";\n-  default:\n-    fatal(\"unexpected method state: %d\", state);\n-    return nullptr;\n-  }\n-}\n-\n-\/\/-----------------------------------------------------------------------------\n-void CompiledMethod::set_deoptimized_done() {\n-  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n-  if (_deoptimization_status != deoptimize_done) { \/\/ can't go backwards\n-    Atomic::store(&_deoptimization_status, deoptimize_done);\n-  }\n-}\n-\n-\/\/-----------------------------------------------------------------------------\n-\n-ExceptionCache* CompiledMethod::exception_cache_acquire() const {\n-  return Atomic::load_acquire(&_exception_cache);\n-}\n-\n-void CompiledMethod::add_exception_cache_entry(ExceptionCache* new_entry) {\n-  assert(ExceptionCache_lock->owned_by_self(),\"Must hold the ExceptionCache_lock\");\n-  assert(new_entry != nullptr,\"Must be non null\");\n-  assert(new_entry->next() == nullptr, \"Must be null\");\n-\n-  for (;;) {\n-    ExceptionCache *ec = exception_cache();\n-    if (ec != nullptr) {\n-      Klass* ex_klass = ec->exception_type();\n-      if (!ex_klass->is_loader_alive()) {\n-        \/\/ We must guarantee that entries are not inserted with new next pointer\n-        \/\/ edges to ExceptionCache entries with dead klasses, due to bad interactions\n-        \/\/ with concurrent ExceptionCache cleanup. Therefore, the inserts roll\n-        \/\/ the head pointer forward to the first live ExceptionCache, so that the new\n-        \/\/ next pointers always point at live ExceptionCaches, that are not removed due\n-        \/\/ to concurrent ExceptionCache cleanup.\n-        ExceptionCache* next = ec->next();\n-        if (Atomic::cmpxchg(&_exception_cache, ec, next) == ec) {\n-          CodeCache::release_exception_cache(ec);\n-        }\n-        continue;\n-      }\n-      ec = exception_cache();\n-      if (ec != nullptr) {\n-        new_entry->set_next(ec);\n-      }\n-    }\n-    if (Atomic::cmpxchg(&_exception_cache, ec, new_entry) == ec) {\n-      return;\n-    }\n-  }\n-}\n-\n-void CompiledMethod::clean_exception_cache() {\n-  \/\/ For each nmethod, only a single thread may call this cleanup function\n-  \/\/ at the same time, whether called in STW cleanup or concurrent cleanup.\n-  \/\/ Note that if the GC is processing exception cache cleaning in a concurrent phase,\n-  \/\/ then a single writer may contend with cleaning up the head pointer to the\n-  \/\/ first ExceptionCache node that has a Klass* that is alive. That is fine,\n-  \/\/ as long as there is no concurrent cleanup of next pointers from concurrent writers.\n-  \/\/ And the concurrent writers do not clean up next pointers, only the head.\n-  \/\/ Also note that concurrent readers will walk through Klass* pointers that are not\n-  \/\/ alive. That does not cause ABA problems, because Klass* is deleted after\n-  \/\/ a handshake with all threads, after all stale ExceptionCaches have been\n-  \/\/ unlinked. That is also when the CodeCache::exception_cache_purge_list()\n-  \/\/ is deleted, with all ExceptionCache entries that were cleaned concurrently.\n-  \/\/ That similarly implies that CAS operations on ExceptionCache entries do not\n-  \/\/ suffer from ABA problems as unlinking and deletion is separated by a global\n-  \/\/ handshake operation.\n-  ExceptionCache* prev = nullptr;\n-  ExceptionCache* curr = exception_cache_acquire();\n-\n-  while (curr != nullptr) {\n-    ExceptionCache* next = curr->next();\n-\n-    if (!curr->exception_type()->is_loader_alive()) {\n-      if (prev == nullptr) {\n-        \/\/ Try to clean head; this is contended by concurrent inserts, that\n-        \/\/ both lazily clean the head, and insert entries at the head. If\n-        \/\/ the CAS fails, the operation is restarted.\n-        if (Atomic::cmpxchg(&_exception_cache, curr, next) != curr) {\n-          prev = nullptr;\n-          curr = exception_cache_acquire();\n-          continue;\n-        }\n-      } else {\n-        \/\/ It is impossible to during cleanup connect the next pointer to\n-        \/\/ an ExceptionCache that has not been published before a safepoint\n-        \/\/ prior to the cleanup. Therefore, release is not required.\n-        prev->set_next(next);\n-      }\n-      \/\/ prev stays the same.\n-\n-      CodeCache::release_exception_cache(curr);\n-    } else {\n-      prev = curr;\n-    }\n-\n-    curr = next;\n-  }\n-}\n-\n-\/\/ public method for accessing the exception cache\n-\/\/ These are the public access methods.\n-address CompiledMethod::handler_for_exception_and_pc(Handle exception, address pc) {\n-  \/\/ We never grab a lock to read the exception cache, so we may\n-  \/\/ have false negatives. This is okay, as it can only happen during\n-  \/\/ the first few exception lookups for a given nmethod.\n-  ExceptionCache* ec = exception_cache_acquire();\n-  while (ec != nullptr) {\n-    address ret_val;\n-    if ((ret_val = ec->match(exception,pc)) != nullptr) {\n-      return ret_val;\n-    }\n-    ec = ec->next();\n-  }\n-  return nullptr;\n-}\n-\n-void CompiledMethod::add_handler_for_exception_and_pc(Handle exception, address pc, address handler) {\n-  \/\/ There are potential race conditions during exception cache updates, so we\n-  \/\/ must own the ExceptionCache_lock before doing ANY modifications. Because\n-  \/\/ we don't lock during reads, it is possible to have several threads attempt\n-  \/\/ to update the cache with the same data. We need to check for already inserted\n-  \/\/ copies of the current data before adding it.\n-\n-  MutexLocker ml(ExceptionCache_lock);\n-  ExceptionCache* target_entry = exception_cache_entry_for_exception(exception);\n-\n-  if (target_entry == nullptr || !target_entry->add_address_and_handler(pc,handler)) {\n-    target_entry = new ExceptionCache(exception,pc,handler);\n-    add_exception_cache_entry(target_entry);\n-  }\n-}\n-\n-\/\/ private method for handling exception cache\n-\/\/ These methods are private, and used to manipulate the exception cache\n-\/\/ directly.\n-ExceptionCache* CompiledMethod::exception_cache_entry_for_exception(Handle exception) {\n-  ExceptionCache* ec = exception_cache_acquire();\n-  while (ec != nullptr) {\n-    if (ec->match_exception_with_space(exception)) {\n-      return ec;\n-    }\n-    ec = ec->next();\n-  }\n-  return nullptr;\n-}\n-\n-\/\/-------------end of code for ExceptionCache--------------\n-\n-bool CompiledMethod::is_at_poll_return(address pc) {\n-  RelocIterator iter(this, pc, pc+1);\n-  while (iter.next()) {\n-    if (iter.type() == relocInfo::poll_return_type)\n-      return true;\n-  }\n-  return false;\n-}\n-\n-\n-bool CompiledMethod::is_at_poll_or_poll_return(address pc) {\n-  RelocIterator iter(this, pc, pc+1);\n-  while (iter.next()) {\n-    relocInfo::relocType t = iter.type();\n-    if (t == relocInfo::poll_return_type || t == relocInfo::poll_type)\n-      return true;\n-  }\n-  return false;\n-}\n-\n-void CompiledMethod::verify_oop_relocations() {\n-  \/\/ Ensure sure that the code matches the current oop values\n-  RelocIterator iter(this, nullptr, nullptr);\n-  while (iter.next()) {\n-    if (iter.type() == relocInfo::oop_type) {\n-      oop_Relocation* reloc = iter.oop_reloc();\n-      if (!reloc->oop_is_immediate()) {\n-        reloc->verify_oop_relocation();\n-      }\n-    }\n-  }\n-}\n-\n-\n-ScopeDesc* CompiledMethod::scope_desc_at(address pc) {\n-  PcDesc* pd = pc_desc_at(pc);\n-  guarantee(pd != nullptr, \"scope must be present\");\n-  return new ScopeDesc(this, pd);\n-}\n-\n-ScopeDesc* CompiledMethod::scope_desc_near(address pc) {\n-  PcDesc* pd = pc_desc_near(pc);\n-  guarantee(pd != nullptr, \"scope must be present\");\n-  return new ScopeDesc(this, pd);\n-}\n-\n-address CompiledMethod::oops_reloc_begin() const {\n-  \/\/ If the method is not entrant then a JMP is plastered over the\n-  \/\/ first few bytes.  If an oop in the old code was there, that oop\n-  \/\/ should not get GC'd.  Skip the first few bytes of oops on\n-  \/\/ not-entrant methods.\n-  if (frame_complete_offset() != CodeOffsets::frame_never_safe &&\n-      code_begin() + frame_complete_offset() >\n-      verified_entry_point() + NativeJump::instruction_size)\n-  {\n-    \/\/ If we have a frame_complete_offset after the native jump, then there\n-    \/\/ is no point trying to look for oops before that. This is a requirement\n-    \/\/ for being allowed to scan oops concurrently.\n-    return code_begin() + frame_complete_offset();\n-  }\n-\n-  \/\/ It is not safe to read oops concurrently using entry barriers, if their\n-  \/\/ location depend on whether the nmethod is entrant or not.\n-  \/\/ assert(BarrierSet::barrier_set()->barrier_set_nmethod() == nullptr, \"Not safe oop scan\");\n-\n-  address low_boundary = verified_entry_point();\n-  if (!is_in_use() && is_nmethod()) {\n-    low_boundary += NativeJump::instruction_size;\n-    \/\/ %%% Note:  On SPARC we patch only a 4-byte trap, not a full NativeJump.\n-    \/\/ This means that the low_boundary is going to be a little too high.\n-    \/\/ This shouldn't matter, since oops of non-entrant methods are never used.\n-    \/\/ In fact, why are we bothering to look at oops in a non-entrant method??\n-  }\n-  return low_boundary;\n-}\n-\n-\/\/ Method that knows how to preserve outgoing arguments at call. This method must be\n-\/\/ called with a frame corresponding to a Java invoke\n-void CompiledMethod::preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) {\n-  if (method() == nullptr) {\n-    return;\n-  }\n-\n-  \/\/ handle the case of an anchor explicitly set in continuation code that doesn't have a callee\n-  JavaThread* thread = reg_map->thread();\n-  if (thread->has_last_Java_frame() && fr.sp() == thread->last_Java_sp()) {\n-    return;\n-  }\n-\n-  if (!method()->is_native()) {\n-    address pc = fr.pc();\n-    bool has_receiver, has_appendix;\n-    Symbol* signature;\n-\n-    \/\/ The method attached by JIT-compilers should be used, if present.\n-    \/\/ Bytecode can be inaccurate in such case.\n-    Method* callee = attached_method_before_pc(pc);\n-    if (callee != nullptr) {\n-      has_receiver = !(callee->access_flags().is_static());\n-      has_appendix = false;\n-      signature    = callee->signature();\n-    } else {\n-      SimpleScopeDesc ssd(this, pc);\n-\n-      Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());\n-      has_receiver = call.has_receiver();\n-      has_appendix = call.has_appendix();\n-      signature    = call.signature();\n-    }\n-\n-    fr.oops_compiled_arguments_do(signature, has_receiver, has_appendix, reg_map, f);\n-  } else if (method()->is_continuation_enter_intrinsic()) {\n-    \/\/ This method only calls Continuation.enter()\n-    Symbol* signature = vmSymbols::continuationEnter_signature();\n-    fr.oops_compiled_arguments_do(signature, false, false, reg_map, f);\n-  }\n-}\n-\n-Method* CompiledMethod::attached_method(address call_instr) {\n-  assert(code_contains(call_instr), \"not part of the nmethod\");\n-  RelocIterator iter(this, call_instr, call_instr + 1);\n-  while (iter.next()) {\n-    if (iter.addr() == call_instr) {\n-      switch(iter.type()) {\n-        case relocInfo::static_call_type:      return iter.static_call_reloc()->method_value();\n-        case relocInfo::opt_virtual_call_type: return iter.opt_virtual_call_reloc()->method_value();\n-        case relocInfo::virtual_call_type:     return iter.virtual_call_reloc()->method_value();\n-        default:                               break;\n-      }\n-    }\n-  }\n-  return nullptr; \/\/ not found\n-}\n-\n-Method* CompiledMethod::attached_method_before_pc(address pc) {\n-  if (NativeCall::is_call_before(pc)) {\n-    NativeCall* ncall = nativeCall_before(pc);\n-    return attached_method(ncall->instruction_address());\n-  }\n-  return nullptr; \/\/ not a call\n-}\n-\n-void CompiledMethod::clear_inline_caches() {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"clearing of IC's only allowed at safepoint\");\n-  RelocIterator iter(this);\n-  while (iter.next()) {\n-    iter.reloc()->clear_inline_cache();\n-  }\n-}\n-\n-#ifdef ASSERT\n-\/\/ Check class_loader is alive for this bit of metadata.\n-class CheckClass : public MetadataClosure {\n-  void do_metadata(Metadata* md) {\n-    Klass* klass = nullptr;\n-    if (md->is_klass()) {\n-      klass = ((Klass*)md);\n-    } else if (md->is_method()) {\n-      klass = ((Method*)md)->method_holder();\n-    } else if (md->is_methodData()) {\n-      klass = ((MethodData*)md)->method()->method_holder();\n-    } else {\n-      md->print();\n-      ShouldNotReachHere();\n-    }\n-    assert(klass->is_loader_alive(), \"must be alive\");\n-  }\n-};\n-#endif \/\/ ASSERT\n-\n-\n-static void clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n-  ic->clean_metadata();\n-}\n-\n-\/\/ Clean references to unloaded nmethods at addr from this one, which is not unloaded.\n-template <typename CallsiteT>\n-static void clean_if_nmethod_is_unloaded(CallsiteT* callsite, CompiledMethod* from,\n-                                         bool clean_all) {\n-  CodeBlob* cb = CodeCache::find_blob(callsite->destination());\n-  if (!cb->is_compiled()) {\n-    return;\n-  }\n-  CompiledMethod* cm = cb->as_compiled_method();\n-  if (clean_all || !cm->is_in_use() || cm->is_unloading() || cm->method()->code() != cm) {\n-    callsite->set_to_clean();\n-  }\n-}\n-\n-\/\/ Cleans caches in nmethods that point to either classes that are unloaded\n-\/\/ or nmethods that are unloaded.\n-\/\/\n-\/\/ Can be called either in parallel by G1 currently or after all\n-\/\/ nmethods are unloaded.  Return postponed=true in the parallel case for\n-\/\/ inline caches found that point to nmethods that are not yet visited during\n-\/\/ the do_unloading walk.\n-void CompiledMethod::unload_nmethod_caches(bool unloading_occurred) {\n-  ResourceMark rm;\n-\n-  \/\/ Exception cache only needs to be called if unloading occurred\n-  if (unloading_occurred) {\n-    clean_exception_cache();\n-  }\n-\n-  cleanup_inline_caches_impl(unloading_occurred, false);\n-\n-#ifdef ASSERT\n-  \/\/ Check that the metadata embedded in the nmethod is alive\n-  CheckClass check_class;\n-  metadata_do(&check_class);\n-#endif\n-}\n-\n-void CompiledMethod::run_nmethod_entry_barrier() {\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  if (bs_nm != nullptr) {\n-    \/\/ We want to keep an invariant that nmethods found through iterations of a Thread's\n-    \/\/ nmethods found in safepoints have gone through an entry barrier and are not armed.\n-    \/\/ By calling this nmethod entry barrier, it plays along and acts\n-    \/\/ like any other nmethod found on the stack of a thread (fewer surprises).\n-    nmethod* nm = as_nmethod_or_null();\n-    if (nm != nullptr && bs_nm->is_armed(nm)) {\n-      bool alive = bs_nm->nmethod_entry_barrier(nm);\n-      assert(alive, \"should be alive\");\n-    }\n-  }\n-}\n-\n-\/\/ Only called by whitebox test\n-void CompiledMethod::cleanup_inline_caches_whitebox() {\n-  assert_locked_or_safepoint(CodeCache_lock);\n-  CompiledICLocker ic_locker(this);\n-  cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/);\n-}\n-\n-address* CompiledMethod::orig_pc_addr(const frame* fr) {\n-  return (address*) ((address)fr->unextended_sp() + orig_pc_offset());\n-}\n-\n-\/\/ Called to clean up after class unloading for live nmethods\n-void CompiledMethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n-  assert(CompiledICLocker::is_safe(this), \"mt unsafe call\");\n-  ResourceMark rm;\n-\n-  \/\/ Find all calls in an nmethod and clear the ones that point to bad nmethods.\n-  RelocIterator iter(this, oops_reloc_begin());\n-  bool is_in_static_stub = false;\n-  while(iter.next()) {\n-\n-    switch (iter.type()) {\n-\n-    case relocInfo::virtual_call_type:\n-      if (unloading_occurred) {\n-        \/\/ If class unloading occurred we first clear ICs where the cached metadata\n-        \/\/ is referring to an unloaded klass or method.\n-        clean_ic_if_metadata_is_dead(CompiledIC_at(&iter));\n-      }\n-\n-      clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all);\n-      break;\n-\n-    case relocInfo::opt_virtual_call_type:\n-    case relocInfo::static_call_type:\n-      clean_if_nmethod_is_unloaded(CompiledDirectCall::at(iter.reloc()), this, clean_all);\n-      break;\n-\n-    case relocInfo::static_stub_type: {\n-      is_in_static_stub = true;\n-      break;\n-    }\n-\n-    case relocInfo::metadata_type: {\n-      \/\/ Only the metadata relocations contained in static\/opt virtual call stubs\n-      \/\/ contains the Method* passed to c2i adapters. It is the only metadata\n-      \/\/ relocation that needs to be walked, as it is the one metadata relocation\n-      \/\/ that violates the invariant that all metadata relocations have an oop\n-      \/\/ in the compiled method (due to deferred resolution and code patching).\n-\n-      \/\/ This causes dead metadata to remain in compiled methods that are not\n-      \/\/ unloading. Unless these slippery metadata relocations of the static\n-      \/\/ stubs are at least cleared, subsequent class redefinition operations\n-      \/\/ will access potentially free memory, and JavaThread execution\n-      \/\/ concurrent to class unloading may call c2i adapters with dead methods.\n-      if (!is_in_static_stub) {\n-        \/\/ The first metadata relocation after a static stub relocation is the\n-        \/\/ metadata relocation of the static stub used to pass the Method* to\n-        \/\/ c2i adapters.\n-        continue;\n-      }\n-      is_in_static_stub = false;\n-      if (is_unloading()) {\n-        \/\/ If the nmethod itself is dying, then it may point at dead metadata.\n-        \/\/ Nobody should follow that metadata; it is strictly unsafe.\n-        continue;\n-      }\n-      metadata_Relocation* r = iter.metadata_reloc();\n-      Metadata* md = r->metadata_value();\n-      if (md != nullptr && md->is_method()) {\n-        Method* method = static_cast<Method*>(md);\n-        if (!method->method_holder()->is_loader_alive()) {\n-          Atomic::store(r->metadata_addr(), (Method*)nullptr);\n-\n-          if (!r->metadata_is_immediate()) {\n-            r->fix_metadata_relocation();\n-          }\n-        }\n-      }\n-      break;\n-    }\n-\n-    default:\n-      break;\n-    }\n-  }\n-}\n-\n-address CompiledMethod::continuation_for_implicit_exception(address pc, bool for_div0_check) {\n-  \/\/ Exception happened outside inline-cache check code => we are inside\n-  \/\/ an active nmethod => use cpc to determine a return address\n-  int exception_offset = int(pc - code_begin());\n-  int cont_offset = ImplicitExceptionTable(this).continuation_offset( exception_offset );\n-#ifdef ASSERT\n-  if (cont_offset == 0) {\n-    Thread* thread = Thread::current();\n-    ResourceMark rm(thread);\n-    CodeBlob* cb = CodeCache::find_blob(pc);\n-    assert(cb != nullptr && cb == this, \"\");\n-\n-    \/\/ Keep tty output consistent. To avoid ttyLocker, we buffer in stream, and print all at once.\n-    stringStream ss;\n-    ss.print_cr(\"implicit exception happened at \" INTPTR_FORMAT, p2i(pc));\n-    print_on(&ss);\n-    method()->print_codes_on(&ss);\n-    print_code_on(&ss);\n-    print_pcs_on(&ss);\n-    tty->print(\"%s\", ss.as_string()); \/\/ print all at once\n-  }\n-#endif\n-  if (cont_offset == 0) {\n-    \/\/ Let the normal error handling report the exception\n-    return nullptr;\n-  }\n-  if (cont_offset == exception_offset) {\n-#if INCLUDE_JVMCI\n-    Deoptimization::DeoptReason deopt_reason = for_div0_check ? Deoptimization::Reason_div0_check : Deoptimization::Reason_null_check;\n-    JavaThread *thread = JavaThread::current();\n-    thread->set_jvmci_implicit_exception_pc(pc);\n-    thread->set_pending_deoptimization(Deoptimization::make_trap_request(deopt_reason,\n-                                                                         Deoptimization::Action_reinterpret));\n-    return (SharedRuntime::deopt_blob()->implicit_exception_uncommon_trap());\n-#else\n-    ShouldNotReachHere();\n-#endif\n-  }\n-  return code_begin() + cont_offset;\n-}\n-\n-class HasEvolDependency : public MetadataClosure {\n-  bool _has_evol_dependency;\n- public:\n-  HasEvolDependency() : _has_evol_dependency(false) {}\n-  void do_metadata(Metadata* md) {\n-    if (md->is_method()) {\n-      Method* method = (Method*)md;\n-      if (method->is_old()) {\n-        _has_evol_dependency = true;\n-      }\n-    }\n-  }\n-  bool has_evol_dependency() const { return _has_evol_dependency; }\n-};\n-\n-bool CompiledMethod::has_evol_metadata() {\n-  \/\/ Check the metadata in relocIter and CompiledIC and also deoptimize\n-  \/\/ any nmethod that has reference to old methods.\n-  HasEvolDependency check_evol;\n-  metadata_do(&check_evol);\n-  if (check_evol.has_evol_dependency() && log_is_enabled(Debug, redefine, class, nmethod)) {\n-    ResourceMark rm;\n-    log_debug(redefine, class, nmethod)\n-            (\"Found evol dependency of nmethod %s.%s(%s) compile_id=%d on in nmethod metadata\",\n-             _method->method_holder()->external_name(),\n-             _method->name()->as_C_string(),\n-             _method->signature()->as_C_string(),\n-             compile_id());\n-  }\n-  return check_evol.has_evol_dependency();\n-}\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":0,"deletions":647,"binary":false,"changes":647,"status":"deleted"},{"patch":"@@ -1,415 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_CODE_COMPILEDMETHOD_HPP\n-#define SHARE_CODE_COMPILEDMETHOD_HPP\n-\n-#include \"code\/codeBlob.hpp\"\n-#include \"code\/pcDesc.hpp\"\n-#include \"oops\/metadata.hpp\"\n-#include \"oops\/method.hpp\"\n-\n-class Dependencies;\n-class ExceptionHandlerTable;\n-class ImplicitExceptionTable;\n-class AbstractCompiler;\n-class xmlStream;\n-class CompiledDirectCall;\n-class NativeCallWrapper;\n-class ScopeDesc;\n-class CompiledIC;\n-class MetadataClosure;\n-\n-\/\/ This class is used internally by nmethods, to cache\n-\/\/ exception\/pc\/handler information.\n-\n-class ExceptionCache : public CHeapObj<mtCode> {\n-  friend class VMStructs;\n- private:\n-  enum { cache_size = 16 };\n-  Klass*   _exception_type;\n-  address  _pc[cache_size];\n-  address  _handler[cache_size];\n-  volatile int _count;\n-  ExceptionCache* volatile _next;\n-  ExceptionCache* _purge_list_next;\n-\n-  inline address pc_at(int index);\n-  void set_pc_at(int index, address a)      { assert(index >= 0 && index < cache_size,\"\"); _pc[index] = a; }\n-\n-  inline address handler_at(int index);\n-  void set_handler_at(int index, address a) { assert(index >= 0 && index < cache_size,\"\"); _handler[index] = a; }\n-\n-  inline int count();\n-  \/\/ increment_count is only called under lock, but there may be concurrent readers.\n-  void increment_count();\n-\n- public:\n-\n-  ExceptionCache(Handle exception, address pc, address handler);\n-\n-  Klass*    exception_type()                { return _exception_type; }\n-  ExceptionCache* next();\n-  void      set_next(ExceptionCache *ec);\n-  ExceptionCache* purge_list_next()                 { return _purge_list_next; }\n-  void      set_purge_list_next(ExceptionCache *ec) { _purge_list_next = ec; }\n-\n-  address match(Handle exception, address pc);\n-  bool    match_exception_with_space(Handle exception) ;\n-  address test_address(address addr);\n-  bool    add_address_and_handler(address addr, address handler) ;\n-};\n-\n-class nmethod;\n-\n-\/\/ cache pc descs found in earlier inquiries\n-class PcDescCache {\n-  friend class VMStructs;\n- private:\n-  enum { cache_size = 4 };\n-  \/\/ The array elements MUST be volatile! Several threads may modify\n-  \/\/ and read from the cache concurrently. find_pc_desc_internal has\n-  \/\/ returned wrong results. C++ compiler (namely xlC12) may duplicate\n-  \/\/ C++ field accesses if the elements are not volatile.\n-  typedef PcDesc* PcDescPtr;\n-  volatile PcDescPtr _pc_descs[cache_size]; \/\/ last cache_size pc_descs found\n- public:\n-  PcDescCache() { debug_only(_pc_descs[0] = nullptr); }\n-  void    reset_to(PcDesc* initial_pc_desc);\n-  PcDesc* find_pc_desc(int pc_offset, bool approximate);\n-  void    add_pc_desc(PcDesc* pc_desc);\n-  PcDesc* last_pc_desc() { return _pc_descs[0]; }\n-};\n-\n-class PcDescSearch {\n-private:\n-  address _code_begin;\n-  PcDesc* _lower;\n-  PcDesc* _upper;\n-public:\n-  PcDescSearch(address code, PcDesc* lower, PcDesc* upper) :\n-    _code_begin(code), _lower(lower), _upper(upper)\n-  {\n-  }\n-\n-  address code_begin() const { return _code_begin; }\n-  PcDesc* scopes_pcs_begin() const { return _lower; }\n-  PcDesc* scopes_pcs_end() const { return _upper; }\n-};\n-\n-class PcDescContainer {\n-private:\n-  PcDescCache _pc_desc_cache;\n-public:\n-  PcDescContainer() {}\n-\n-  PcDesc* find_pc_desc_internal(address pc, bool approximate, const PcDescSearch& search);\n-  void    reset_to(PcDesc* initial_pc_desc) { _pc_desc_cache.reset_to(initial_pc_desc); }\n-\n-  PcDesc* find_pc_desc(address pc, bool approximate, const PcDescSearch& search) {\n-    address base_address = search.code_begin();\n-    PcDesc* desc = _pc_desc_cache.last_pc_desc();\n-    if (desc != nullptr && desc->pc_offset() == pc - base_address) {\n-      return desc;\n-    }\n-    return find_pc_desc_internal(pc, approximate, search);\n-  }\n-};\n-\n-\n-class CompiledMethod : public CodeBlob {\n-  friend class VMStructs;\n-  friend class DeoptimizationScope;\n-  void init_defaults();\n-protected:\n-  enum DeoptimizationStatus : u1 {\n-    not_marked,\n-    deoptimize,\n-    deoptimize_noupdate,\n-    deoptimize_done\n-  };\n-\n-  volatile DeoptimizationStatus _deoptimization_status; \/\/ Used for stack deoptimization\n-  \/\/ Used to track in which deoptimize handshake this method will be deoptimized.\n-  uint64_t                      _deoptimization_generation;\n-\n-  \/\/ set during construction\n-  unsigned int _has_unsafe_access:1;         \/\/ May fault due to unsafe access.\n-  unsigned int _has_method_handle_invokes:1; \/\/ Has this method MethodHandle invokes?\n-  unsigned int _has_wide_vectors:1;          \/\/ Preserve wide vectors at safepoints\n-  unsigned int _has_monitors:1;              \/\/ Fastpath monitor detection for continuations\n-\n-  Method*   _method;\n-  address _scopes_data_begin;\n-  \/\/ All deoptee's will resume execution at this location described by\n-  \/\/ this address.\n-  address _deopt_handler_begin;\n-  \/\/ All deoptee's at a MethodHandle call site will resume execution\n-  \/\/ at this location described by this offset.\n-  address _deopt_mh_handler_begin;\n-\n-  PcDescContainer _pc_desc_container;\n-  ExceptionCache * volatile _exception_cache;\n-\n-  void* _gc_data;\n-\n-  virtual void purge(bool free_code_cache_data, bool unregister_nmethod) = 0;\n-\n-private:\n-  DeoptimizationStatus deoptimization_status() const {\n-    return Atomic::load(&_deoptimization_status);\n-  }\n-\n-protected:\n-  CompiledMethod(Method* method, const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled);\n-  CompiledMethod(Method* method, const char* name, CompilerType type, int size, int header_size, CodeBuffer* cb, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled);\n-\n-public:\n-  \/\/ Only used by unit test.\n-  CompiledMethod() {}\n-\n-  template<typename T>\n-  T* gc_data() const                              { return reinterpret_cast<T*>(_gc_data); }\n-  template<typename T>\n-  void set_gc_data(T* gc_data)                    { _gc_data = reinterpret_cast<void*>(gc_data); }\n-\n-  bool  has_unsafe_access() const                 { return _has_unsafe_access; }\n-  void  set_has_unsafe_access(bool z)             { _has_unsafe_access = z; }\n-\n-  bool  has_monitors() const                      { return _has_monitors; }\n-  void  set_has_monitors(bool z)                  { _has_monitors = z; }\n-\n-  bool  has_method_handle_invokes() const         { return _has_method_handle_invokes; }\n-  void  set_has_method_handle_invokes(bool z)     { _has_method_handle_invokes = z; }\n-\n-  bool  has_wide_vectors() const                  { return _has_wide_vectors; }\n-  void  set_has_wide_vectors(bool z)              { _has_wide_vectors = z; }\n-\n-  enum : signed char { not_installed = -1, \/\/ in construction, only the owner doing the construction is\n-                                           \/\/ allowed to advance state\n-                       in_use        = 0,  \/\/ executable nmethod\n-                       not_used      = 1,  \/\/ not entrant, but revivable\n-                       not_entrant   = 2,  \/\/ marked for deoptimization but activations may still exist\n-  };\n-\n-  virtual bool  is_in_use() const = 0;\n-  virtual int   comp_level() const = 0;\n-  virtual int   compile_id() const = 0;\n-\n-  virtual address verified_entry_point() const = 0;\n-  virtual void log_identity(xmlStream* log) const = 0;\n-  virtual void log_state_change() const = 0;\n-  virtual bool make_not_used() = 0;\n-  virtual bool make_not_entrant() = 0;\n-  virtual bool make_entrant() = 0;\n-  virtual address entry_point() const = 0;\n-  virtual bool is_osr_method() const = 0;\n-  virtual int osr_entry_bci() const = 0;\n-  Method* method() const                          { return _method; }\n-  virtual void print_pcs_on(outputStream* st) = 0;\n-  bool is_native_method() const { return _method != nullptr && _method->is_native(); }\n-  bool is_java_method() const { return _method != nullptr && !_method->is_native(); }\n-\n-  \/\/ ScopeDesc retrieval operation\n-  PcDesc* pc_desc_at(address pc)   { return find_pc_desc(pc, false); }\n-  \/\/ pc_desc_near returns the first PcDesc at or after the given pc.\n-  PcDesc* pc_desc_near(address pc) { return find_pc_desc(pc, true); }\n-\n-  \/\/ ScopeDesc for an instruction\n-  ScopeDesc* scope_desc_at(address pc);\n-  ScopeDesc* scope_desc_near(address pc);\n-\n-  bool is_at_poll_return(address pc);\n-  bool is_at_poll_or_poll_return(address pc);\n-\n-  bool  is_marked_for_deoptimization() const { return deoptimization_status() != not_marked; }\n-  bool  has_been_deoptimized() const { return deoptimization_status() == deoptimize_done; }\n-  void  set_deoptimized_done();\n-\n-  virtual void  make_deoptimized() { assert(false, \"not supported\"); };\n-\n-  bool update_recompile_counts() const {\n-    \/\/ Update recompile counts when either the update is explicitly requested (deoptimize)\n-    \/\/ or the nmethod is not marked for deoptimization at all (not_marked).\n-    \/\/ The latter happens during uncommon traps when deoptimized nmethod is made not entrant.\n-    DeoptimizationStatus status = deoptimization_status();\n-    return status != deoptimize_noupdate && status != deoptimize_done;\n-  }\n-\n-  \/\/ tells whether frames described by this nmethod can be deoptimized\n-  \/\/ note: native wrappers cannot be deoptimized.\n-  bool can_be_deoptimized() const { return is_java_method(); }\n-\n-  virtual oop oop_at(int index) const = 0;\n-  virtual Metadata* metadata_at(int index) const = 0;\n-\n-  address scopes_data_begin() const { return _scopes_data_begin; }\n-  virtual address scopes_data_end() const = 0;\n-  int scopes_data_size() const { return int(scopes_data_end() - scopes_data_begin()); }\n-\n-  virtual PcDesc* scopes_pcs_begin() const = 0;\n-  virtual PcDesc* scopes_pcs_end() const = 0;\n-  int scopes_pcs_size() const { return int((intptr_t) scopes_pcs_end() - (intptr_t) scopes_pcs_begin()); }\n-\n-  address insts_begin() const { return code_begin(); }\n-  address insts_end() const { return stub_begin(); }\n-  \/\/ Returns true if a given address is in the 'insts' section. The method\n-  \/\/ insts_contains_inclusive() is end-inclusive.\n-  bool insts_contains(address addr) const { return insts_begin() <= addr && addr < insts_end(); }\n-  bool insts_contains_inclusive(address addr) const { return insts_begin() <= addr && addr <= insts_end(); }\n-\n-  int insts_size() const { return int(insts_end() - insts_begin()); }\n-\n-  virtual address consts_begin() const = 0;\n-  virtual address consts_end() const = 0;\n-  bool consts_contains(address addr) const { return consts_begin() <= addr && addr < consts_end(); }\n-  int consts_size() const { return int(consts_end() - consts_begin()); }\n-\n-  virtual int skipped_instructions_size() const = 0;\n-\n-  virtual address stub_begin() const = 0;\n-  virtual address stub_end() const = 0;\n-  bool stub_contains(address addr) const { return stub_begin() <= addr && addr < stub_end(); }\n-  int stub_size() const { return int(stub_end() - stub_begin()); }\n-\n-  virtual address handler_table_begin() const = 0;\n-  virtual address handler_table_end() const = 0;\n-  bool handler_table_contains(address addr) const { return handler_table_begin() <= addr && addr < handler_table_end(); }\n-  int handler_table_size() const { return int(handler_table_end() - handler_table_begin()); }\n-\n-  virtual address exception_begin() const = 0;\n-\n-  virtual address nul_chk_table_begin() const = 0;\n-  virtual address nul_chk_table_end() const = 0;\n-  bool nul_chk_table_contains(address addr) const { return nul_chk_table_begin() <= addr && addr < nul_chk_table_end(); }\n-  int nul_chk_table_size() const { return int(nul_chk_table_end() - nul_chk_table_begin()); }\n-\n-  virtual oop* oop_addr_at(int index) const = 0;\n-  virtual Metadata** metadata_addr_at(int index) const = 0;\n-\n-protected:\n-  \/\/ Exception cache support\n-  \/\/ Note: _exception_cache may be read and cleaned concurrently.\n-  ExceptionCache* exception_cache() const         { return _exception_cache; }\n-  ExceptionCache* exception_cache_acquire() const;\n-  void set_exception_cache(ExceptionCache *ec)    { _exception_cache = ec; }\n-\n-public:\n-  address handler_for_exception_and_pc(Handle exception, address pc);\n-  void add_handler_for_exception_and_pc(Handle exception, address pc, address handler);\n-  void clean_exception_cache();\n-\n-  void add_exception_cache_entry(ExceptionCache* new_entry);\n-  ExceptionCache* exception_cache_entry_for_exception(Handle exception);\n-\n-  \/\/ MethodHandle\n-  bool is_method_handle_return(address return_pc);\n-  address deopt_mh_handler_begin() const  { return _deopt_mh_handler_begin; }\n-\n-  address deopt_handler_begin() const { return _deopt_handler_begin; }\n-  address* deopt_handler_begin_addr() { return &_deopt_handler_begin; }\n-  \/\/ Deopt\n-  \/\/ Return true is the PC is one would expect if the frame is being deopted.\n-  inline bool is_deopt_pc(address pc);\n-  inline bool is_deopt_mh_entry(address pc);\n-  inline bool is_deopt_entry(address pc);\n-\n-  \/\/ Accessor\/mutator for the original pc of a frame before a frame was deopted.\n-  address get_original_pc(const frame* fr) { return *orig_pc_addr(fr); }\n-  void    set_original_pc(const frame* fr, address pc) { *orig_pc_addr(fr) = pc; }\n-\n-  virtual int orig_pc_offset() = 0;\n-\n-private:\n-  address* orig_pc_addr(const frame* fr);\n-\n-public:\n-  virtual const char* compile_kind() const = 0;\n-  virtual int get_state() const = 0;\n-\n-  const char* state() const;\n-\n-  bool inlinecache_check_contains(address addr) const {\n-    return (addr >= code_begin() && addr < verified_entry_point());\n-  }\n-\n-  void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f);\n-\n-  \/\/ implicit exceptions support\n-  address continuation_for_implicit_div0_exception(address pc) { return continuation_for_implicit_exception(pc, true); }\n-  address continuation_for_implicit_null_exception(address pc) { return continuation_for_implicit_exception(pc, false); }\n-\n-  static address get_deopt_original_pc(const frame* fr);\n-\n-  \/\/ Inline cache support for class unloading and nmethod unloading\n- private:\n-  void cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n-\n-  address continuation_for_implicit_exception(address pc, bool for_div0_check);\n-\n- public:\n-  \/\/ Serial version used by whitebox test\n-  void cleanup_inline_caches_whitebox();\n-\n-  virtual void clear_inline_caches();\n-\n-  \/\/ Execute nmethod barrier code, as if entering through nmethod call.\n-  void run_nmethod_entry_barrier();\n-\n-  void verify_oop_relocations();\n-\n-  bool has_evol_metadata();\n-\n-  \/\/ Fast breakpoint support. Tells if this compiled method is\n-  \/\/ dependent on the given method. Returns true if this nmethod\n-  \/\/ corresponds to the given method as well.\n-  virtual bool is_dependent_on_method(Method* dependee) = 0;\n-\n-  virtual address call_instruction_address(address pc) const = 0;\n-\n-  Method* attached_method(address call_pc);\n-  Method* attached_method_before_pc(address pc);\n-\n-  virtual void metadata_do(MetadataClosure* f) = 0;\n-\n-  \/\/ GC support\n- protected:\n-  address oops_reloc_begin() const;\n-\n- public:\n-  \/\/ GC unloading support\n-  \/\/ Cleans unloaded klasses and unloaded nmethods in inline caches\n-\n-  virtual bool is_unloading() = 0;\n-\n-  void unload_nmethod_caches(bool class_unloading_occurred);\n-  virtual void do_unloading(bool unloading_occurred) = 0;\n-\n-private:\n-  PcDesc* find_pc_desc(address pc, bool approximate) {\n-    return _pc_desc_container.find_pc_desc(pc, approximate, PcDescSearch(code_begin(), scopes_pcs_begin(), scopes_pcs_end()));\n-  }\n-};\n-\n-#endif \/\/ SHARE_CODE_COMPILEDMETHOD_HPP\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":0,"deletions":415,"binary":false,"changes":415,"status":"deleted"},{"patch":"@@ -1,89 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_CODE_COMPILEDMETHOD_INLINE_HPP\n-#define SHARE_CODE_COMPILEDMETHOD_INLINE_HPP\n-\n-#include \"code\/compiledMethod.hpp\"\n-\n-#include \"code\/nativeInst.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/frame.hpp\"\n-\n-inline bool CompiledMethod::is_deopt_pc(address pc) { return is_deopt_entry(pc) || is_deopt_mh_entry(pc); }\n-\n-\/\/ When using JVMCI the address might be off by the size of a call instruction.\n-inline bool CompiledMethod::is_deopt_entry(address pc) {\n-  return pc == deopt_handler_begin()\n-#if INCLUDE_JVMCI\n-    || (is_compiled_by_jvmci() && pc == (deopt_handler_begin() + NativeCall::instruction_size))\n-#endif\n-    ;\n-}\n-\n-inline bool CompiledMethod::is_deopt_mh_entry(address pc) {\n-  return pc == deopt_mh_handler_begin()\n-#if INCLUDE_JVMCI\n-    || (is_compiled_by_jvmci() && pc == (deopt_mh_handler_begin() + NativeCall::instruction_size))\n-#endif\n-    ;\n-}\n-\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ CompiledMethod::get_deopt_original_pc\n-\/\/\n-\/\/ Return the original PC for the given PC if:\n-\/\/ (a) the given PC belongs to a nmethod and\n-\/\/ (b) it is a deopt PC\n-\n-inline address CompiledMethod::get_deopt_original_pc(const frame* fr) {\n-  if (fr->cb() == nullptr)  return nullptr;\n-\n-  CompiledMethod* cm = fr->cb()->as_compiled_method_or_null();\n-  if (cm != nullptr && cm->is_deopt_pc(fr->pc()))\n-    return cm->get_original_pc(fr);\n-\n-  return nullptr;\n-}\n-\n-\n-\/\/ class ExceptionCache methods\n-\n-inline int ExceptionCache::count() { return Atomic::load_acquire(&_count); }\n-\n-address ExceptionCache::pc_at(int index) {\n-  assert(index >= 0 && index < count(),\"\");\n-  return _pc[index];\n-}\n-\n-address ExceptionCache::handler_at(int index) {\n-  assert(index >= 0 && index < count(),\"\");\n-  return _handler[index];\n-}\n-\n-\/\/ increment_count is only called under lock, but there may be concurrent readers.\n-inline void ExceptionCache::increment_count() { Atomic::release_store(&_count, _count + 1); }\n-\n-\n-#endif \/\/ SHARE_CODE_COMPILEDMETHOD_INLINE_HPP\n","filename":"src\/hotspot\/share\/code\/compiledMethod.inline.hpp","additions":0,"deletions":89,"binary":false,"changes":89,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,9 +56,3 @@\n-  nmethod* nm = const_cast<CompiledMethod*>(code())->as_nmethod_or_null();\n-  oop o;\n-  if (nm != nullptr) {\n-    \/\/ Despite these oops being found inside nmethods that are on-stack,\n-    \/\/ they are not kept alive by all GCs (e.g. G1 and Shenandoah).\n-    o = nm->oop_at_phantom(read_int());\n-  } else {\n-    o = code()->oop_at(read_int());\n-  }\n+  \/\/ Despite these oops being found inside nmethods that are on-stack,\n+  \/\/ they are not kept alive by all GCs (e.g. G1 and Shenandoah).\n+  oop o = code()->oop_at_phantom(read_int());\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -375,2 +375,2 @@\n-  const CompiledMethod* _code;\n-  const CompiledMethod* code() const { return _code; }\n+  const nmethod* _code;\n+  const nmethod* code() const { return _code; }\n@@ -379,1 +379,1 @@\n-  DebugInfoReadStream(const CompiledMethod* code, int offset, GrowableArray<ScopeValue*>* obj_pool = nullptr) :\n+  DebugInfoReadStream(const nmethod* code, int offset, GrowableArray<ScopeValue*>* obj_pool = nullptr) :\n","filename":"src\/hotspot\/share\/code\/debugInfo.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -178,1 +178,1 @@\n-         \"must specify a new, larger pc offset\");\n+         \"must specify a new, larger pc offset: %d >= %d\", last_pc()->pc_offset(), pc_offset);\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -68,3 +68,3 @@\n-ExceptionHandlerTable::ExceptionHandlerTable(const CompiledMethod* cm) {\n-  _table  = (HandlerTableEntry*)cm->handler_table_begin();\n-  _length = cm->handler_table_size() \/ sizeof(HandlerTableEntry);\n+ExceptionHandlerTable::ExceptionHandlerTable(const nmethod* nm) {\n+  _table  = (HandlerTableEntry*)nm->handler_table_begin();\n+  _length = nm->handler_table_size() \/ sizeof(HandlerTableEntry);\n@@ -101,3 +101,3 @@\n-void ExceptionHandlerTable::copy_to(CompiledMethod* cm) {\n-  assert(size_in_bytes() == cm->handler_table_size(), \"size of space allocated in compiled method incorrect\");\n-  copy_bytes_to(cm->handler_table_begin());\n+void ExceptionHandlerTable::copy_to(nmethod* nm) {\n+  assert(size_in_bytes() == nm->handler_table_size(), \"size of space allocated in compiled method incorrect\");\n+  copy_bytes_to(nm->handler_table_begin());\n@@ -218,1 +218,1 @@\n-ImplicitExceptionTable::ImplicitExceptionTable(const CompiledMethod* nm) {\n+ImplicitExceptionTable::ImplicitExceptionTable(const nmethod* nm) {\n","filename":"src\/hotspot\/share\/code\/exceptionHandlerTable.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -102,1 +102,1 @@\n-  ExceptionHandlerTable(const CompiledMethod* nm);\n+  ExceptionHandlerTable(const nmethod* nm);\n@@ -119,1 +119,1 @@\n-  void copy_to(CompiledMethod* nm);\n+  void copy_to(nmethod* nm);\n@@ -153,1 +153,1 @@\n-  ImplicitExceptionTable( const CompiledMethod *nm );\n+  ImplicitExceptionTable(const nmethod *nm);\n","filename":"src\/hotspot\/share\/code\/exceptionHandlerTable.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n@@ -32,1 +31,1 @@\n-#include \"code\/nmethod.hpp\"\n+#include \"code\/nmethod.inline.hpp\"\n@@ -47,1 +46,1 @@\n-#include \"interpreter\/bytecode.hpp\"\n+#include \"interpreter\/bytecode.inline.hpp\"\n@@ -101,3 +100,3 @@\n-        (char *) klass_name->bytes(), klass_name->utf8_length(),                   \\\n-        (char *) name->bytes(), name->utf8_length(),                               \\\n-        (char *) signature->bytes(), signature->utf8_length());                    \\\n+        (char *) klass_name->bytes(), klass_name->utf8_length(),          \\\n+        (char *) name->bytes(), name->utf8_length(),                      \\\n+        (char *) signature->bytes(), signature->utf8_length());           \\\n@@ -141,0 +140,3 @@\n+  uint size_gt_32k;\n+  int size_max;\n+\n@@ -159,0 +161,3 @@\n+    int short_pos_max = ((1<<15) - 1);\n+    if (nm->size() > short_pos_max) size_gt_32k++;\n+    if (nm->size() > size_max) size_max = nm->size();\n@@ -163,13 +168,14 @@\n-    if (total_size != 0)          tty->print_cr(\" total in heap  = %u\", total_size);\n-    if (nmethod_count != 0)       tty->print_cr(\" header         = \" SIZE_FORMAT, nmethod_count * sizeof(nmethod));\n-    if (relocation_size != 0)     tty->print_cr(\" relocation     = %u\", relocation_size);\n-    if (consts_size != 0)         tty->print_cr(\" constants      = %u\", consts_size);\n-    if (insts_size != 0)          tty->print_cr(\" main code      = %u\", insts_size);\n-    if (stub_size != 0)           tty->print_cr(\" stub code      = %u\", stub_size);\n-    if (oops_size != 0)           tty->print_cr(\" oops           = %u\", oops_size);\n-    if (metadata_size != 0)       tty->print_cr(\" metadata       = %u\", metadata_size);\n-    if (scopes_data_size != 0)    tty->print_cr(\" scopes data    = %u\", scopes_data_size);\n-    if (scopes_pcs_size != 0)     tty->print_cr(\" scopes pcs     = %u\", scopes_pcs_size);\n-    if (dependencies_size != 0)   tty->print_cr(\" dependencies   = %u\", dependencies_size);\n-    if (handler_table_size != 0)  tty->print_cr(\" handler table  = %u\", handler_table_size);\n-    if (nul_chk_table_size != 0)  tty->print_cr(\" nul chk table  = %u\", nul_chk_table_size);\n+    if (total_size != 0)          tty->print_cr(\" total in heap  = %u (100%%)\", total_size);\n+    uint header_size = (uint)(nmethod_count * sizeof(nmethod));\n+    if (nmethod_count != 0)       tty->print_cr(\" header         = %u (%f%%)\", header_size, (header_size * 100.0f)\/total_size);\n+    if (relocation_size != 0)     tty->print_cr(\" relocation     = %u (%f%%)\", relocation_size, (relocation_size * 100.0f)\/total_size);\n+    if (consts_size != 0)         tty->print_cr(\" constants      = %u (%f%%)\", consts_size, (consts_size * 100.0f)\/total_size);\n+    if (insts_size != 0)          tty->print_cr(\" main code      = %u (%f%%)\", insts_size, (insts_size * 100.0f)\/total_size);\n+    if (stub_size != 0)           tty->print_cr(\" stub code      = %u (%f%%)\", stub_size, (stub_size * 100.0f)\/total_size);\n+    if (oops_size != 0)           tty->print_cr(\" oops           = %u (%f%%)\", oops_size, (oops_size * 100.0f)\/total_size);\n+    if (metadata_size != 0)       tty->print_cr(\" metadata       = %u (%f%%)\", metadata_size, (metadata_size * 100.0f)\/total_size);\n+    if (scopes_data_size != 0)    tty->print_cr(\" scopes data    = %u (%f%%)\", scopes_data_size, (scopes_data_size * 100.0f)\/total_size);\n+    if (scopes_pcs_size != 0)     tty->print_cr(\" scopes pcs     = %u (%f%%)\", scopes_pcs_size, (scopes_pcs_size * 100.0f)\/total_size);\n+    if (dependencies_size != 0)   tty->print_cr(\" dependencies   = %u (%f%%)\", dependencies_size, (dependencies_size * 100.0f)\/total_size);\n+    if (handler_table_size != 0)  tty->print_cr(\" handler table  = %u (%f%%)\", handler_table_size, (handler_table_size * 100.0f)\/total_size);\n+    if (nul_chk_table_size != 0)  tty->print_cr(\" nul chk table  = %u (%f%%)\", nul_chk_table_size, (nul_chk_table_size * 100.0f)\/total_size);\n@@ -177,2 +183,2 @@\n-    if (speculations_size != 0)   tty->print_cr(\" speculations   = %u\", speculations_size);\n-    if (jvmci_data_size != 0)     tty->print_cr(\" JVMCI data     = %u\", jvmci_data_size);\n+    if (speculations_size != 0)   tty->print_cr(\" speculations   = %u (%f%%)\", speculations_size, (speculations_size * 100.0f)\/total_size);\n+    if (jvmci_data_size != 0)     tty->print_cr(\" JVMCI data     = %u (%f%%)\", jvmci_data_size, (jvmci_data_size * 100.0f)\/total_size);\n@@ -180,0 +186,2 @@\n+    if (size_gt_32k != 0)         tty->print_cr(\" size > 32k     = %u\", size_gt_32k);\n+    if (size_max != 0)            tty->print_cr(\" max size       = %d\", size_max);\n@@ -420,0 +428,552 @@\n+bool nmethod::is_method_handle_return(address return_pc) {\n+  if (!has_method_handle_invokes())  return false;\n+  PcDesc* pd = pc_desc_at(return_pc);\n+  if (pd == nullptr)\n+    return false;\n+  return pd->is_method_handle_invoke();\n+}\n+\n+\/\/ Returns a string version of the method state.\n+const char* nmethod::state() const {\n+  int state = get_state();\n+  switch (state) {\n+  case not_installed:\n+    return \"not installed\";\n+  case in_use:\n+    return \"in use\";\n+  case not_entrant:\n+    return \"not_entrant\";\n+  default:\n+    fatal(\"unexpected method state: %d\", state);\n+    return nullptr;\n+  }\n+}\n+\n+void nmethod::set_deoptimized_done() {\n+  ConditionalMutexLocker ml(CompiledMethod_lock, !CompiledMethod_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  if (_deoptimization_status != deoptimize_done) { \/\/ can't go backwards\n+    Atomic::store(&_deoptimization_status, deoptimize_done);\n+  }\n+}\n+\n+ExceptionCache* nmethod::exception_cache_acquire() const {\n+  return Atomic::load_acquire(&_exception_cache);\n+}\n+\n+void nmethod::add_exception_cache_entry(ExceptionCache* new_entry) {\n+  assert(ExceptionCache_lock->owned_by_self(),\"Must hold the ExceptionCache_lock\");\n+  assert(new_entry != nullptr,\"Must be non null\");\n+  assert(new_entry->next() == nullptr, \"Must be null\");\n+\n+  for (;;) {\n+    ExceptionCache *ec = exception_cache();\n+    if (ec != nullptr) {\n+      Klass* ex_klass = ec->exception_type();\n+      if (!ex_klass->is_loader_alive()) {\n+        \/\/ We must guarantee that entries are not inserted with new next pointer\n+        \/\/ edges to ExceptionCache entries with dead klasses, due to bad interactions\n+        \/\/ with concurrent ExceptionCache cleanup. Therefore, the inserts roll\n+        \/\/ the head pointer forward to the first live ExceptionCache, so that the new\n+        \/\/ next pointers always point at live ExceptionCaches, that are not removed due\n+        \/\/ to concurrent ExceptionCache cleanup.\n+        ExceptionCache* next = ec->next();\n+        if (Atomic::cmpxchg(&_exception_cache, ec, next) == ec) {\n+          CodeCache::release_exception_cache(ec);\n+        }\n+        continue;\n+      }\n+      ec = exception_cache();\n+      if (ec != nullptr) {\n+        new_entry->set_next(ec);\n+      }\n+    }\n+    if (Atomic::cmpxchg(&_exception_cache, ec, new_entry) == ec) {\n+      return;\n+    }\n+  }\n+}\n+\n+void nmethod::clean_exception_cache() {\n+  \/\/ For each nmethod, only a single thread may call this cleanup function\n+  \/\/ at the same time, whether called in STW cleanup or concurrent cleanup.\n+  \/\/ Note that if the GC is processing exception cache cleaning in a concurrent phase,\n+  \/\/ then a single writer may contend with cleaning up the head pointer to the\n+  \/\/ first ExceptionCache node that has a Klass* that is alive. That is fine,\n+  \/\/ as long as there is no concurrent cleanup of next pointers from concurrent writers.\n+  \/\/ And the concurrent writers do not clean up next pointers, only the head.\n+  \/\/ Also note that concurrent readers will walk through Klass* pointers that are not\n+  \/\/ alive. That does not cause ABA problems, because Klass* is deleted after\n+  \/\/ a handshake with all threads, after all stale ExceptionCaches have been\n+  \/\/ unlinked. That is also when the CodeCache::exception_cache_purge_list()\n+  \/\/ is deleted, with all ExceptionCache entries that were cleaned concurrently.\n+  \/\/ That similarly implies that CAS operations on ExceptionCache entries do not\n+  \/\/ suffer from ABA problems as unlinking and deletion is separated by a global\n+  \/\/ handshake operation.\n+  ExceptionCache* prev = nullptr;\n+  ExceptionCache* curr = exception_cache_acquire();\n+\n+  while (curr != nullptr) {\n+    ExceptionCache* next = curr->next();\n+\n+    if (!curr->exception_type()->is_loader_alive()) {\n+      if (prev == nullptr) {\n+        \/\/ Try to clean head; this is contended by concurrent inserts, that\n+        \/\/ both lazily clean the head, and insert entries at the head. If\n+        \/\/ the CAS fails, the operation is restarted.\n+        if (Atomic::cmpxchg(&_exception_cache, curr, next) != curr) {\n+          prev = nullptr;\n+          curr = exception_cache_acquire();\n+          continue;\n+        }\n+      } else {\n+        \/\/ It is impossible to during cleanup connect the next pointer to\n+        \/\/ an ExceptionCache that has not been published before a safepoint\n+        \/\/ prior to the cleanup. Therefore, release is not required.\n+        prev->set_next(next);\n+      }\n+      \/\/ prev stays the same.\n+\n+      CodeCache::release_exception_cache(curr);\n+    } else {\n+      prev = curr;\n+    }\n+\n+    curr = next;\n+  }\n+}\n+\n+\/\/ public method for accessing the exception cache\n+\/\/ These are the public access methods.\n+address nmethod::handler_for_exception_and_pc(Handle exception, address pc) {\n+  \/\/ We never grab a lock to read the exception cache, so we may\n+  \/\/ have false negatives. This is okay, as it can only happen during\n+  \/\/ the first few exception lookups for a given nmethod.\n+  ExceptionCache* ec = exception_cache_acquire();\n+  while (ec != nullptr) {\n+    address ret_val;\n+    if ((ret_val = ec->match(exception,pc)) != nullptr) {\n+      return ret_val;\n+    }\n+    ec = ec->next();\n+  }\n+  return nullptr;\n+}\n+\n+void nmethod::add_handler_for_exception_and_pc(Handle exception, address pc, address handler) {\n+  \/\/ There are potential race conditions during exception cache updates, so we\n+  \/\/ must own the ExceptionCache_lock before doing ANY modifications. Because\n+  \/\/ we don't lock during reads, it is possible to have several threads attempt\n+  \/\/ to update the cache with the same data. We need to check for already inserted\n+  \/\/ copies of the current data before adding it.\n+\n+  MutexLocker ml(ExceptionCache_lock);\n+  ExceptionCache* target_entry = exception_cache_entry_for_exception(exception);\n+\n+  if (target_entry == nullptr || !target_entry->add_address_and_handler(pc,handler)) {\n+    target_entry = new ExceptionCache(exception,pc,handler);\n+    add_exception_cache_entry(target_entry);\n+  }\n+}\n+\n+\/\/ private method for handling exception cache\n+\/\/ These methods are private, and used to manipulate the exception cache\n+\/\/ directly.\n+ExceptionCache* nmethod::exception_cache_entry_for_exception(Handle exception) {\n+  ExceptionCache* ec = exception_cache_acquire();\n+  while (ec != nullptr) {\n+    if (ec->match_exception_with_space(exception)) {\n+      return ec;\n+    }\n+    ec = ec->next();\n+  }\n+  return nullptr;\n+}\n+\n+bool nmethod::is_at_poll_return(address pc) {\n+  RelocIterator iter(this, pc, pc+1);\n+  while (iter.next()) {\n+    if (iter.type() == relocInfo::poll_return_type)\n+      return true;\n+  }\n+  return false;\n+}\n+\n+\n+bool nmethod::is_at_poll_or_poll_return(address pc) {\n+  RelocIterator iter(this, pc, pc+1);\n+  while (iter.next()) {\n+    relocInfo::relocType t = iter.type();\n+    if (t == relocInfo::poll_return_type || t == relocInfo::poll_type)\n+      return true;\n+  }\n+  return false;\n+}\n+\n+void nmethod::verify_oop_relocations() {\n+  \/\/ Ensure sure that the code matches the current oop values\n+  RelocIterator iter(this, nullptr, nullptr);\n+  while (iter.next()) {\n+    if (iter.type() == relocInfo::oop_type) {\n+      oop_Relocation* reloc = iter.oop_reloc();\n+      if (!reloc->oop_is_immediate()) {\n+        reloc->verify_oop_relocation();\n+      }\n+    }\n+  }\n+}\n+\n+\n+ScopeDesc* nmethod::scope_desc_at(address pc) {\n+  PcDesc* pd = pc_desc_at(pc);\n+  guarantee(pd != nullptr, \"scope must be present\");\n+  return new ScopeDesc(this, pd);\n+}\n+\n+ScopeDesc* nmethod::scope_desc_near(address pc) {\n+  PcDesc* pd = pc_desc_near(pc);\n+  guarantee(pd != nullptr, \"scope must be present\");\n+  return new ScopeDesc(this, pd);\n+}\n+\n+address nmethod::oops_reloc_begin() const {\n+  \/\/ If the method is not entrant then a JMP is plastered over the\n+  \/\/ first few bytes.  If an oop in the old code was there, that oop\n+  \/\/ should not get GC'd.  Skip the first few bytes of oops on\n+  \/\/ not-entrant methods.\n+  if (frame_complete_offset() != CodeOffsets::frame_never_safe &&\n+      code_begin() + frame_complete_offset() >\n+      verified_entry_point() + NativeJump::instruction_size)\n+  {\n+    \/\/ If we have a frame_complete_offset after the native jump, then there\n+    \/\/ is no point trying to look for oops before that. This is a requirement\n+    \/\/ for being allowed to scan oops concurrently.\n+    return code_begin() + frame_complete_offset();\n+  }\n+\n+  \/\/ It is not safe to read oops concurrently using entry barriers, if their\n+  \/\/ location depend on whether the nmethod is entrant or not.\n+  \/\/ assert(BarrierSet::barrier_set()->barrier_set_nmethod() == nullptr, \"Not safe oop scan\");\n+\n+  address low_boundary = verified_entry_point();\n+  if (!is_in_use()) {\n+    low_boundary += NativeJump::instruction_size;\n+    \/\/ %%% Note:  On SPARC we patch only a 4-byte trap, not a full NativeJump.\n+    \/\/ This means that the low_boundary is going to be a little too high.\n+    \/\/ This shouldn't matter, since oops of non-entrant methods are never used.\n+    \/\/ In fact, why are we bothering to look at oops in a non-entrant method??\n+  }\n+  return low_boundary;\n+}\n+\n+\/\/ Method that knows how to preserve outgoing arguments at call. This method must be\n+\/\/ called with a frame corresponding to a Java invoke\n+void nmethod::preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) {\n+  if (method() == nullptr) {\n+    return;\n+  }\n+\n+  \/\/ handle the case of an anchor explicitly set in continuation code that doesn't have a callee\n+  JavaThread* thread = reg_map->thread();\n+  if (thread->has_last_Java_frame() && fr.sp() == thread->last_Java_sp()) {\n+    return;\n+  }\n+\n+  if (!method()->is_native()) {\n+    address pc = fr.pc();\n+    bool has_receiver, has_appendix;\n+    Symbol* signature;\n+\n+    \/\/ The method attached by JIT-compilers should be used, if present.\n+    \/\/ Bytecode can be inaccurate in such case.\n+    Method* callee = attached_method_before_pc(pc);\n+    if (callee != nullptr) {\n+      has_receiver = !(callee->access_flags().is_static());\n+      has_appendix = false;\n+      signature    = callee->signature();\n+    } else {\n+      SimpleScopeDesc ssd(this, pc);\n+\n+      Bytecode_invoke call(methodHandle(Thread::current(), ssd.method()), ssd.bci());\n+      has_receiver = call.has_receiver();\n+      has_appendix = call.has_appendix();\n+      signature    = call.signature();\n+    }\n+\n+    fr.oops_compiled_arguments_do(signature, has_receiver, has_appendix, reg_map, f);\n+  } else if (method()->is_continuation_enter_intrinsic()) {\n+    \/\/ This method only calls Continuation.enter()\n+    Symbol* signature = vmSymbols::continuationEnter_signature();\n+    fr.oops_compiled_arguments_do(signature, false, false, reg_map, f);\n+  }\n+}\n+\n+Method* nmethod::attached_method(address call_instr) {\n+  assert(code_contains(call_instr), \"not part of the nmethod\");\n+  RelocIterator iter(this, call_instr, call_instr + 1);\n+  while (iter.next()) {\n+    if (iter.addr() == call_instr) {\n+      switch(iter.type()) {\n+        case relocInfo::static_call_type:      return iter.static_call_reloc()->method_value();\n+        case relocInfo::opt_virtual_call_type: return iter.opt_virtual_call_reloc()->method_value();\n+        case relocInfo::virtual_call_type:     return iter.virtual_call_reloc()->method_value();\n+        default:                               break;\n+      }\n+    }\n+  }\n+  return nullptr; \/\/ not found\n+}\n+\n+Method* nmethod::attached_method_before_pc(address pc) {\n+  if (NativeCall::is_call_before(pc)) {\n+    NativeCall* ncall = nativeCall_before(pc);\n+    return attached_method(ncall->instruction_address());\n+  }\n+  return nullptr; \/\/ not a call\n+}\n+\n+void nmethod::clear_inline_caches() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"clearing of IC's only allowed at safepoint\");\n+  RelocIterator iter(this);\n+  while (iter.next()) {\n+    iter.reloc()->clear_inline_cache();\n+  }\n+}\n+\n+#ifdef ASSERT\n+\/\/ Check class_loader is alive for this bit of metadata.\n+class CheckClass : public MetadataClosure {\n+  void do_metadata(Metadata* md) {\n+    Klass* klass = nullptr;\n+    if (md->is_klass()) {\n+      klass = ((Klass*)md);\n+    } else if (md->is_method()) {\n+      klass = ((Method*)md)->method_holder();\n+    } else if (md->is_methodData()) {\n+      klass = ((MethodData*)md)->method()->method_holder();\n+    } else {\n+      md->print();\n+      ShouldNotReachHere();\n+    }\n+    assert(klass->is_loader_alive(), \"must be alive\");\n+  }\n+};\n+#endif \/\/ ASSERT\n+\n+\n+static void clean_ic_if_metadata_is_dead(CompiledIC *ic) {\n+  ic->clean_metadata();\n+}\n+\n+\/\/ Clean references to unloaded nmethods at addr from this one, which is not unloaded.\n+template <typename CallsiteT>\n+static void clean_if_nmethod_is_unloaded(CallsiteT* callsite, nmethod* from,\n+                                         bool clean_all) {\n+  CodeBlob* cb = CodeCache::find_blob(callsite->destination());\n+  if (!cb->is_nmethod()) {\n+    return;\n+  }\n+  nmethod* nm = cb->as_nmethod();\n+  if (clean_all || !nm->is_in_use() || nm->is_unloading() || nm->method()->code() != nm) {\n+    callsite->set_to_clean();\n+  }\n+}\n+\n+\/\/ Cleans caches in nmethods that point to either classes that are unloaded\n+\/\/ or nmethods that are unloaded.\n+\/\/\n+\/\/ Can be called either in parallel by G1 currently or after all\n+\/\/ nmethods are unloaded.  Return postponed=true in the parallel case for\n+\/\/ inline caches found that point to nmethods that are not yet visited during\n+\/\/ the do_unloading walk.\n+void nmethod::unload_nmethod_caches(bool unloading_occurred) {\n+  ResourceMark rm;\n+\n+  \/\/ Exception cache only needs to be called if unloading occurred\n+  if (unloading_occurred) {\n+    clean_exception_cache();\n+  }\n+\n+  cleanup_inline_caches_impl(unloading_occurred, false);\n+\n+#ifdef ASSERT\n+  \/\/ Check that the metadata embedded in the nmethod is alive\n+  CheckClass check_class;\n+  metadata_do(&check_class);\n+#endif\n+}\n+\n+void nmethod::run_nmethod_entry_barrier() {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n+    \/\/ We want to keep an invariant that nmethods found through iterations of a Thread's\n+    \/\/ nmethods found in safepoints have gone through an entry barrier and are not armed.\n+    \/\/ By calling this nmethod entry barrier, it plays along and acts\n+    \/\/ like any other nmethod found on the stack of a thread (fewer surprises).\n+    nmethod* nm = this;\n+    if (bs_nm->is_armed(nm)) {\n+      bool alive = bs_nm->nmethod_entry_barrier(nm);\n+      assert(alive, \"should be alive\");\n+    }\n+  }\n+}\n+\n+\/\/ Only called by whitebox test\n+void nmethod::cleanup_inline_caches_whitebox() {\n+  assert_locked_or_safepoint(CodeCache_lock);\n+  CompiledICLocker ic_locker(this);\n+  cleanup_inline_caches_impl(false \/* unloading_occurred *\/, true \/* clean_all *\/);\n+}\n+\n+address* nmethod::orig_pc_addr(const frame* fr) {\n+  return (address*) ((address)fr->unextended_sp() + orig_pc_offset());\n+}\n+\n+\/\/ Called to clean up after class unloading for live nmethods\n+void nmethod::cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all) {\n+  assert(CompiledICLocker::is_safe(this), \"mt unsafe call\");\n+  ResourceMark rm;\n+\n+  \/\/ Find all calls in an nmethod and clear the ones that point to bad nmethods.\n+  RelocIterator iter(this, oops_reloc_begin());\n+  bool is_in_static_stub = false;\n+  while(iter.next()) {\n+\n+    switch (iter.type()) {\n+\n+    case relocInfo::virtual_call_type:\n+      if (unloading_occurred) {\n+        \/\/ If class unloading occurred we first clear ICs where the cached metadata\n+        \/\/ is referring to an unloaded klass or method.\n+        clean_ic_if_metadata_is_dead(CompiledIC_at(&iter));\n+      }\n+\n+      clean_if_nmethod_is_unloaded(CompiledIC_at(&iter), this, clean_all);\n+      break;\n+\n+    case relocInfo::opt_virtual_call_type:\n+    case relocInfo::static_call_type:\n+      clean_if_nmethod_is_unloaded(CompiledDirectCall::at(iter.reloc()), this, clean_all);\n+      break;\n+\n+    case relocInfo::static_stub_type: {\n+      is_in_static_stub = true;\n+      break;\n+    }\n+\n+    case relocInfo::metadata_type: {\n+      \/\/ Only the metadata relocations contained in static\/opt virtual call stubs\n+      \/\/ contains the Method* passed to c2i adapters. It is the only metadata\n+      \/\/ relocation that needs to be walked, as it is the one metadata relocation\n+      \/\/ that violates the invariant that all metadata relocations have an oop\n+      \/\/ in the compiled method (due to deferred resolution and code patching).\n+\n+      \/\/ This causes dead metadata to remain in compiled methods that are not\n+      \/\/ unloading. Unless these slippery metadata relocations of the static\n+      \/\/ stubs are at least cleared, subsequent class redefinition operations\n+      \/\/ will access potentially free memory, and JavaThread execution\n+      \/\/ concurrent to class unloading may call c2i adapters with dead methods.\n+      if (!is_in_static_stub) {\n+        \/\/ The first metadata relocation after a static stub relocation is the\n+        \/\/ metadata relocation of the static stub used to pass the Method* to\n+        \/\/ c2i adapters.\n+        continue;\n+      }\n+      is_in_static_stub = false;\n+      if (is_unloading()) {\n+        \/\/ If the nmethod itself is dying, then it may point at dead metadata.\n+        \/\/ Nobody should follow that metadata; it is strictly unsafe.\n+        continue;\n+      }\n+      metadata_Relocation* r = iter.metadata_reloc();\n+      Metadata* md = r->metadata_value();\n+      if (md != nullptr && md->is_method()) {\n+        Method* method = static_cast<Method*>(md);\n+        if (!method->method_holder()->is_loader_alive()) {\n+          Atomic::store(r->metadata_addr(), (Method*)nullptr);\n+\n+          if (!r->metadata_is_immediate()) {\n+            r->fix_metadata_relocation();\n+          }\n+        }\n+      }\n+      break;\n+    }\n+\n+    default:\n+      break;\n+    }\n+  }\n+}\n+\n+address nmethod::continuation_for_implicit_exception(address pc, bool for_div0_check) {\n+  \/\/ Exception happened outside inline-cache check code => we are inside\n+  \/\/ an active nmethod => use cpc to determine a return address\n+  int exception_offset = int(pc - code_begin());\n+  int cont_offset = ImplicitExceptionTable(this).continuation_offset( exception_offset );\n+#ifdef ASSERT\n+  if (cont_offset == 0) {\n+    Thread* thread = Thread::current();\n+    ResourceMark rm(thread);\n+    CodeBlob* cb = CodeCache::find_blob(pc);\n+    assert(cb != nullptr && cb == this, \"\");\n+\n+    \/\/ Keep tty output consistent. To avoid ttyLocker, we buffer in stream, and print all at once.\n+    stringStream ss;\n+    ss.print_cr(\"implicit exception happened at \" INTPTR_FORMAT, p2i(pc));\n+    print_on(&ss);\n+    method()->print_codes_on(&ss);\n+    print_code_on(&ss);\n+    print_pcs_on(&ss);\n+    tty->print(\"%s\", ss.as_string()); \/\/ print all at once\n+  }\n+#endif\n+  if (cont_offset == 0) {\n+    \/\/ Let the normal error handling report the exception\n+    return nullptr;\n+  }\n+  if (cont_offset == exception_offset) {\n+#if INCLUDE_JVMCI\n+    Deoptimization::DeoptReason deopt_reason = for_div0_check ? Deoptimization::Reason_div0_check : Deoptimization::Reason_null_check;\n+    JavaThread *thread = JavaThread::current();\n+    thread->set_jvmci_implicit_exception_pc(pc);\n+    thread->set_pending_deoptimization(Deoptimization::make_trap_request(deopt_reason,\n+                                                                         Deoptimization::Action_reinterpret));\n+    return (SharedRuntime::deopt_blob()->implicit_exception_uncommon_trap());\n+#else\n+    ShouldNotReachHere();\n+#endif\n+  }\n+  return code_begin() + cont_offset;\n+}\n+\n+class HasEvolDependency : public MetadataClosure {\n+  bool _has_evol_dependency;\n+ public:\n+  HasEvolDependency() : _has_evol_dependency(false) {}\n+  void do_metadata(Metadata* md) {\n+    if (md->is_method()) {\n+      Method* method = (Method*)md;\n+      if (method->is_old()) {\n+        _has_evol_dependency = true;\n+      }\n+    }\n+  }\n+  bool has_evol_dependency() const { return _has_evol_dependency; }\n+};\n+\n+bool nmethod::has_evol_metadata() {\n+  \/\/ Check the metadata in relocIter and CompiledIC and also deoptimize\n+  \/\/ any nmethod that has reference to old methods.\n+  HasEvolDependency check_evol;\n+  metadata_do(&check_evol);\n+  if (check_evol.has_evol_dependency() && log_is_enabled(Debug, redefine, class, nmethod)) {\n+    ResourceMark rm;\n+    log_debug(redefine, class, nmethod)\n+            (\"Found evol dependency of nmethod %s.%s(%s) compile_id=%d on in nmethod metadata\",\n+             _method->method_holder()->external_name(),\n+             _method->name()->as_C_string(),\n+             _method->signature()->as_C_string(),\n+             compile_id());\n+  }\n+  return check_evol.has_evol_dependency();\n+}\n@@ -443,0 +1003,4 @@\n+const char* nmethod::compiler_name() const {\n+  return compilertype2name(_compiler_type);\n+}\n+\n@@ -445,0 +1009,8 @@\n+  \/\/ avoid uninitialized fields, even for short time periods\n+  _exception_cache            = nullptr;\n+\n+  _has_unsafe_access          = 0;\n+  _has_method_handle_invokes  = 0;\n+  _has_wide_vectors           = 0;\n+  _has_monitors               = 0;\n+\n@@ -449,2 +1021,2 @@\n-  _oops_do_mark_link       = nullptr;\n-  _osr_link                = nullptr;\n+  _oops_do_mark_link          = nullptr;\n+  _osr_link                   = nullptr;\n@@ -452,1 +1024,1 @@\n-  _rtm_state               = NoRTM;\n+  _rtm_state                  = NoRTM;\n@@ -642,1 +1214,5 @@\n-  : CompiledMethod(method, \"native nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false, true),\n+  : CodeBlob(\"native nmethod\", CodeBlobKind::Blob_Nmethod, code_buffer, nmethod_size, sizeof(nmethod),\n+             offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false),\n+  _deoptimization_generation(0),\n+  _method(method),\n+  _gc_data(nullptr),\n@@ -647,1 +1223,2 @@\n-  _is_unloading_state(0)\n+  _is_unloading_state(0),\n+  _deoptimization_status(not_marked)\n@@ -650,4 +1227,0 @@\n-    int scopes_data_offset   = 0;\n-    int deoptimize_offset    = 0;\n-    int deoptimize_mh_offset = 0;\n-\n@@ -664,0 +1237,2 @@\n+    _deopt_handler_offset    = 0;\n+    _deopt_mh_handler_offset = 0;\n@@ -670,2 +1245,2 @@\n-    scopes_data_offset       = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n-    _scopes_pcs_offset       = scopes_data_offset;\n+    _scopes_data_offset      = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n+    _scopes_pcs_offset       = _scopes_data_offset;\n@@ -684,0 +1259,1 @@\n+    _compiler_type           = type;\n@@ -692,4 +1268,0 @@\n-    _scopes_data_begin = (address) this + scopes_data_offset;\n-    _deopt_handler_begin = (address) this + deoptimize_offset;\n-    _deopt_mh_handler_begin = (address) this + deoptimize_mh_offset;\n-\n@@ -787,1 +1359,5 @@\n-  : CompiledMethod(method, \"nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false, true),\n+  : CodeBlob(\"nmethod\", CodeBlobKind::Blob_Nmethod, code_buffer, nmethod_size, sizeof(nmethod),\n+             offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false),\n+  _deoptimization_generation(0),\n+  _method(method),\n+  _gc_data(nullptr),\n@@ -792,1 +1368,2 @@\n-  _is_unloading_state(0)\n+  _is_unloading_state(0),\n+  _deoptimization_status(not_marked)\n@@ -799,3 +1376,0 @@\n-    _deopt_handler_begin = (address) this;\n-    _deopt_mh_handler_begin = (address) this;\n-\n@@ -803,5 +1377,6 @@\n-    _entry_bci               = entry_bci;\n-    _compile_id              = compile_id;\n-    _comp_level              = comp_level;\n-    _orig_pc_offset          = orig_pc_offset;\n-    _gc_epoch                = CodeCache::gc_epoch();\n+    _entry_bci      = entry_bci;\n+    _compile_id     = compile_id;\n+    _compiler_type  = type;\n+    _comp_level     = comp_level;\n+    _orig_pc_offset = orig_pc_offset;\n+    _gc_epoch       = CodeCache::gc_epoch();\n@@ -810,2 +1385,2 @@\n-    _consts_offset           = content_offset()      + code_buffer->total_offset_of(code_buffer->consts());\n-    _stub_offset             = content_offset()      + code_buffer->total_offset_of(code_buffer->stubs());\n+    _consts_offset  = content_offset() + code_buffer->total_offset_of(code_buffer->consts());\n+    _stub_offset    = content_offset() + code_buffer->total_offset_of(code_buffer->stubs());\n@@ -813,1 +1388,1 @@\n-    _skipped_instructions_size      = code_buffer->total_skipped_instructions_size();\n+    _skipped_instructions_size = code_buffer->total_skipped_instructions_size();\n@@ -819,1 +1394,1 @@\n-        _exception_offset        = code_offset()          + offsets->value(CodeOffsets::Exceptions);\n+        _exception_offset        = code_offset() + offsets->value(CodeOffsets::Exceptions);\n@@ -821,1 +1396,1 @@\n-        _exception_offset = -1;\n+        _exception_offset        = -1;\n@@ -824,1 +1399,1 @@\n-        _deopt_handler_begin       = (address) this + code_offset()          + offsets->value(CodeOffsets::Deopt);\n+        _deopt_handler_offset    = code_offset() + offsets->value(CodeOffsets::Deopt);\n@@ -826,1 +1401,1 @@\n-        _deopt_handler_begin = nullptr;\n+        _deopt_handler_offset    = -1;\n@@ -829,1 +1404,1 @@\n-        _deopt_mh_handler_begin  = (address) this + code_offset()          + offsets->value(CodeOffsets::DeoptMH);\n+        _deopt_mh_handler_offset = code_offset() + offsets->value(CodeOffsets::DeoptMH);\n@@ -831,1 +1406,1 @@\n-        _deopt_mh_handler_begin = nullptr;\n+        _deopt_mh_handler_offset = -1;\n@@ -840,2 +1415,2 @@\n-      _exception_offset       = _stub_offset          + offsets->value(CodeOffsets::Exceptions);\n-      _deopt_handler_begin    = (address) this + _stub_offset          + offsets->value(CodeOffsets::Deopt);\n+      _exception_offset          = _stub_offset + offsets->value(CodeOffsets::Exceptions);\n+      _deopt_handler_offset      = _stub_offset + offsets->value(CodeOffsets::Deopt);\n@@ -843,1 +1418,1 @@\n-        _deopt_mh_handler_begin  = (address) this + _stub_offset          + offsets->value(CodeOffsets::DeoptMH);\n+        _deopt_mh_handler_offset = _stub_offset + offsets->value(CodeOffsets::DeoptMH);\n@@ -845,1 +1420,1 @@\n-        _deopt_mh_handler_begin  = nullptr;\n+        _deopt_mh_handler_offset = -1;\n@@ -849,1 +1424,1 @@\n-      _unwind_handler_offset = code_offset()         + offsets->value(CodeOffsets::UnwindHandler);\n+      _unwind_handler_offset = code_offset() + offsets->value(CodeOffsets::UnwindHandler);\n@@ -856,1 +1431,1 @@\n-    int scopes_data_offset   = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n+    _scopes_data_offset      = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n@@ -858,1 +1433,1 @@\n-    _scopes_pcs_offset       = scopes_data_offset    + align_up(debug_info->data_size       (), oopSize);\n+    _scopes_pcs_offset       = _scopes_data_offset   + align_up(debug_info->data_size       (), oopSize);\n@@ -874,1 +1449,0 @@\n-    _scopes_data_begin       = (address) this + scopes_data_offset;\n@@ -1512,1 +2086,1 @@\n-    set_has_flushed_dependencies();\n+    set_has_flushed_dependencies(true);\n@@ -2029,1 +2603,1 @@\n-  assert(has_method_handle_invokes() == (_deopt_mh_handler_begin != nullptr), \"must have deopt mh handler\");\n+  assert(has_method_handle_invokes() == (_deopt_mh_handler_offset != -1), \"must have deopt mh handler\");\n@@ -2992,2 +3566,2 @@\n-  if (JVMCI_ONLY(_exception_offset >= 0 &&) pos == exception_begin())           label = \"[Exception Handler]\";\n-  if (JVMCI_ONLY(_deopt_handler_begin != nullptr &&) pos == deopt_handler_begin()) label = \"[Deopt Handler Code]\";\n+  if (JVMCI_ONLY(_exception_offset >= 0 &&) pos == exception_begin())          label = \"[Exception Handler]\";\n+  if (JVMCI_ONLY(_deopt_handler_offset != -1 &&) pos == deopt_handler_begin()) label = \"[Deopt Handler Code]\";\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":641,"deletions":67,"binary":false,"changes":708,"status":"modified"},{"patch":"@@ -28,2 +28,8 @@\n-#include \"code\/compiledMethod.hpp\"\n-\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/pcDesc.hpp\"\n+#include \"oops\/metadata.hpp\"\n+#include \"oops\/method.hpp\"\n+\n+class AbstractCompiler;\n+class CompiledDirectCall;\n+class CompiledIC;\n@@ -33,0 +39,1 @@\n+class Dependencies;\n@@ -35,0 +42,2 @@\n+class ExceptionHandlerTable;\n+class ImplicitExceptionTable;\n@@ -36,0 +45,2 @@\n+class MetadataClosure;\n+class NativeCallWrapper;\n@@ -37,0 +48,96 @@\n+class ScopeDesc;\n+class xmlStream;\n+\n+\/\/ This class is used internally by nmethods, to cache\n+\/\/ exception\/pc\/handler information.\n+\n+class ExceptionCache : public CHeapObj<mtCode> {\n+  friend class VMStructs;\n+ private:\n+  enum { cache_size = 16 };\n+  Klass*   _exception_type;\n+  address  _pc[cache_size];\n+  address  _handler[cache_size];\n+  volatile int _count;\n+  ExceptionCache* volatile _next;\n+  ExceptionCache* _purge_list_next;\n+\n+  inline address pc_at(int index);\n+  void set_pc_at(int index, address a)      { assert(index >= 0 && index < cache_size,\"\"); _pc[index] = a; }\n+\n+  inline address handler_at(int index);\n+  void set_handler_at(int index, address a) { assert(index >= 0 && index < cache_size,\"\"); _handler[index] = a; }\n+\n+  inline int count();\n+  \/\/ increment_count is only called under lock, but there may be concurrent readers.\n+  void increment_count();\n+\n+ public:\n+\n+  ExceptionCache(Handle exception, address pc, address handler);\n+\n+  Klass*    exception_type()                { return _exception_type; }\n+  ExceptionCache* next();\n+  void      set_next(ExceptionCache *ec);\n+  ExceptionCache* purge_list_next()                 { return _purge_list_next; }\n+  void      set_purge_list_next(ExceptionCache *ec) { _purge_list_next = ec; }\n+\n+  address match(Handle exception, address pc);\n+  bool    match_exception_with_space(Handle exception) ;\n+  address test_address(address addr);\n+  bool    add_address_and_handler(address addr, address handler) ;\n+};\n+\n+\/\/ cache pc descs found in earlier inquiries\n+class PcDescCache {\n+  friend class VMStructs;\n+ private:\n+  enum { cache_size = 4 };\n+  \/\/ The array elements MUST be volatile! Several threads may modify\n+  \/\/ and read from the cache concurrently. find_pc_desc_internal has\n+  \/\/ returned wrong results. C++ compiler (namely xlC12) may duplicate\n+  \/\/ C++ field accesses if the elements are not volatile.\n+  typedef PcDesc* PcDescPtr;\n+  volatile PcDescPtr _pc_descs[cache_size]; \/\/ last cache_size pc_descs found\n+ public:\n+  PcDescCache() { debug_only(_pc_descs[0] = nullptr); }\n+  void    reset_to(PcDesc* initial_pc_desc);\n+  PcDesc* find_pc_desc(int pc_offset, bool approximate);\n+  void    add_pc_desc(PcDesc* pc_desc);\n+  PcDesc* last_pc_desc() { return _pc_descs[0]; }\n+};\n+\n+class PcDescSearch {\n+private:\n+  address _code_begin;\n+  PcDesc* _lower;\n+  PcDesc* _upper;\n+public:\n+  PcDescSearch(address code, PcDesc* lower, PcDesc* upper) :\n+    _code_begin(code), _lower(lower), _upper(upper)\n+  {\n+  }\n+\n+  address code_begin() const { return _code_begin; }\n+  PcDesc* scopes_pcs_begin() const { return _lower; }\n+  PcDesc* scopes_pcs_end() const { return _upper; }\n+};\n+\n+class PcDescContainer {\n+private:\n+  PcDescCache _pc_desc_cache;\n+public:\n+  PcDescContainer() {}\n+\n+  PcDesc* find_pc_desc_internal(address pc, bool approximate, const PcDescSearch& search);\n+  void    reset_to(PcDesc* initial_pc_desc) { _pc_desc_cache.reset_to(initial_pc_desc); }\n+\n+  PcDesc* find_pc_desc(address pc, bool approximate, const PcDescSearch& search) {\n+    address base_address = search.code_begin();\n+    PcDesc* desc = _pc_desc_cache.last_pc_desc();\n+    if (desc != nullptr && desc->pc_offset() == pc - base_address) {\n+      return desc;\n+    }\n+    return find_pc_desc_internal(pc, approximate, search);\n+  }\n+};\n@@ -68,1 +175,1 @@\n-class nmethod : public CompiledMethod {\n+class nmethod : public CodeBlob {\n@@ -73,0 +180,1 @@\n+  friend class DeoptimizationScope;\n@@ -76,0 +184,3 @@\n+  \/\/ Used to track in which deoptimize handshake this method will be deoptimized.\n+  uint64_t  _deoptimization_generation;\n+\n@@ -78,0 +189,2 @@\n+  Method*   _method;\n+\n@@ -81,60 +194,2 @@\n-  \/\/ STW two-phase nmethod root processing helpers.\n-  \/\/\n-  \/\/ When determining liveness of a given nmethod to do code cache unloading,\n-  \/\/ some collectors need to do different things depending on whether the nmethods\n-  \/\/ need to absolutely be kept alive during root processing; \"strong\"ly reachable\n-  \/\/ nmethods are known to be kept alive at root processing, but the liveness of\n-  \/\/ \"weak\"ly reachable ones is to be determined later.\n-  \/\/\n-  \/\/ We want to allow strong and weak processing of nmethods by different threads\n-  \/\/ at the same time without heavy synchronization. Additional constraints are\n-  \/\/ to make sure that every nmethod is processed a minimal amount of time, and\n-  \/\/ nmethods themselves are always iterated at most once at a particular time.\n-  \/\/\n-  \/\/ Note that strong processing work must be a superset of weak processing work\n-  \/\/ for this code to work.\n-  \/\/\n-  \/\/ We store state and claim information in the _oops_do_mark_link member, using\n-  \/\/ the two LSBs for the state and the remaining upper bits for linking together\n-  \/\/ nmethods that were already visited.\n-  \/\/ The last element is self-looped, i.e. points to itself to avoid some special\n-  \/\/ \"end-of-list\" sentinel value.\n-  \/\/\n-  \/\/ _oops_do_mark_link special values:\n-  \/\/\n-  \/\/   _oops_do_mark_link == nullptr: the nmethod has not been visited at all yet, i.e.\n-  \/\/      is Unclaimed.\n-  \/\/\n-  \/\/ For other values, its lowest two bits indicate the following states of the nmethod:\n-  \/\/\n-  \/\/   weak_request (WR): the nmethod has been claimed by a thread for weak processing\n-  \/\/   weak_done (WD): weak processing has been completed for this nmethod.\n-  \/\/   strong_request (SR): the nmethod has been found to need strong processing while\n-  \/\/       being weak processed.\n-  \/\/   strong_done (SD): strong processing has been completed for this nmethod .\n-  \/\/\n-  \/\/ The following shows the _only_ possible progressions of the _oops_do_mark_link\n-  \/\/ pointer.\n-  \/\/\n-  \/\/ Given\n-  \/\/   N as the nmethod\n-  \/\/   X the current next value of _oops_do_mark_link\n-  \/\/\n-  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD: the nmethod has been processed weakly by\n-  \/\/   a single thread.\n-  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD (O)-> X|SD: after weak processing has been\n-  \/\/   completed (as above) another thread found that the nmethod needs strong\n-  \/\/   processing after all.\n-  \/\/ Unclaimed (C)-> N|WR (O)-> N|SR (C)-> X|SD: during weak processing another\n-  \/\/   thread finds that the nmethod needs strong processing, marks it as such and\n-  \/\/   terminates. The original thread completes strong processing.\n-  \/\/ Unclaimed (C)-> N|SD (C)-> X|SD: the nmethod has been processed strongly from\n-  \/\/   the beginning by a single thread.\n-  \/\/\n-  \/\/ \"|\" describes the concatenation of bits in _oops_do_mark_link.\n-  \/\/\n-  \/\/ The diagram also describes the threads responsible for changing the nmethod to\n-  \/\/ the next state by marking the _transition_ with (C) and (O), which mean \"current\"\n-  \/\/ and \"other\" thread respectively.\n-  \/\/\n-  struct oops_do_mark_link; \/\/ Opaque data type.\n+  PcDescContainer _pc_desc_container;\n+  ExceptionCache* volatile _exception_cache;\n@@ -142,11 +197,1 @@\n-  \/\/ States used for claiming nmethods during root processing.\n-  static const uint claim_weak_request_tag = 0;\n-  static const uint claim_weak_done_tag = 1;\n-  static const uint claim_strong_request_tag = 2;\n-  static const uint claim_strong_done_tag = 3;\n-\n-  static oops_do_mark_link* mark_link(nmethod* nm, uint tag) {\n-    assert(tag <= claim_strong_done_tag, \"invalid tag %u\", tag);\n-    assert(is_aligned(nm, 4), \"nmethod pointer must have zero lower two LSB\");\n-    return (oops_do_mark_link*)(((uintptr_t)nm & ~0x3) | tag);\n-  }\n+  void* _gc_data;\n@@ -154,39 +199,2 @@\n-  static uint extract_state(oops_do_mark_link* link) {\n-    return (uint)((uintptr_t)link & 0x3);\n-  }\n-\n-  static nmethod* extract_nmethod(oops_do_mark_link* link) {\n-    return (nmethod*)((uintptr_t)link & ~0x3);\n-  }\n-\n-  void oops_do_log_change(const char* state);\n-\n-  static bool oops_do_has_weak_request(oops_do_mark_link* next) {\n-    return extract_state(next) == claim_weak_request_tag;\n-  }\n-\n-  static bool oops_do_has_any_strong_state(oops_do_mark_link* next) {\n-    return extract_state(next) >= claim_strong_request_tag;\n-  }\n-\n-  \/\/ Attempt Unclaimed -> N|WR transition. Returns true if successful.\n-  bool oops_do_try_claim_weak_request();\n-\n-  \/\/ Attempt Unclaimed -> N|SD transition. Returns the current link.\n-  oops_do_mark_link* oops_do_try_claim_strong_done();\n-  \/\/ Attempt N|WR -> X|WD transition. Returns nullptr if successful, X otherwise.\n-  nmethod* oops_do_try_add_to_list_as_weak_done();\n-\n-  \/\/ Attempt X|WD -> N|SR transition. Returns the current link.\n-  oops_do_mark_link* oops_do_try_add_strong_request(oops_do_mark_link* next);\n-  \/\/ Attempt X|WD -> X|SD transition. Returns true if successful.\n-  bool oops_do_try_claim_weak_done_as_strong_done(oops_do_mark_link* next);\n-\n-  \/\/ Do the N|SD -> X|SD transition.\n-  void oops_do_add_to_list_as_strong_done();\n-\n-  \/\/ Sets this nmethod as strongly claimed (as part of N|SD -> X|SD and N|SR -> X|SD\n-  \/\/ transitions).\n-  void oops_do_set_strong_done(nmethod* old_head);\n-\n-  static nmethod* volatile _oops_do_mark_nmethods;\n+  struct oops_do_mark_link; \/\/ Opaque data type.\n+  static nmethod*    volatile _oops_do_mark_nmethods;\n@@ -201,1 +209,0 @@\n-  bool _is_unlinked;\n@@ -208,0 +215,6 @@\n+  \/\/ All deoptee's will resume execution at this location described by\n+  \/\/ this offset.\n+  int _deopt_handler_offset;\n+  \/\/ All deoptee's at a MethodHandle call site will resume execution\n+  \/\/ at this location described by this offset.\n+  int _deopt_mh_handler_offset;\n@@ -225,2 +238,1 @@\n-\n-  int code_offset() const { return int(code_begin() - header_begin()); }\n+  int _skipped_instructions_size;\n@@ -232,1 +244,5 @@\n-  int _compile_id;                           \/\/ which compilation made this nmethod\n+  int _compile_id;                        \/\/ which compilation made this nmethod\n+\n+  CompilerType _compiler_type;            \/\/ which compiler made this nmethod (u1)\n+\n+  bool _is_unlinked;\n@@ -251,1 +267,1 @@\n-  CompLevel _comp_level;               \/\/ compilation level\n+  CompLevel _comp_level;               \/\/ compilation level (s1)\n@@ -256,3 +272,0 @@\n-  \/\/ protected by CodeCache_lock\n-  bool _has_flushed_dependencies;      \/\/ Used for maintenance of dependencies (CodeCache_lock)\n-\n@@ -263,1 +276,15 @@\n-  volatile signed char _state;         \/\/ {not_installed, in_use, not_used, not_entrant}\n+  volatile signed char _state;         \/\/ {not_installed, in_use, not_entrant}\n+\n+  \/\/ set during construction\n+  uint8_t _has_unsafe_access:1,        \/\/ May fault due to unsafe access.\n+          _has_method_handle_invokes:1,\/\/ Has this method MethodHandle invokes?\n+          _has_wide_vectors:1,         \/\/ Preserve wide vectors at safepoints\n+          _has_monitors:1,             \/\/ Fastpath monitor detection for continuations\n+          _has_flushed_dependencies:1; \/\/ Used for maintenance of dependencies (under CodeCache_lock)\n+\n+  enum DeoptimizationStatus : u1 {\n+    not_marked,\n+    deoptimize,\n+    deoptimize_noupdate,\n+    deoptimize_done\n+  };\n@@ -265,1 +292,5 @@\n-  int _skipped_instructions_size;\n+  volatile DeoptimizationStatus _deoptimization_status; \/\/ Used for stack deoptimization\n+\n+  DeoptimizationStatus deoptimization_status() const {\n+    return Atomic::load(&_deoptimization_status);\n+  }\n@@ -305,0 +336,1 @@\n+\n@@ -325,3 +357,3 @@\n-  \/\/ Offsets\n-  int content_offset() const                  { return int(content_begin() - header_begin()); }\n-  int data_offset() const                     { return _data_offset; }\n+  PcDesc* find_pc_desc(address pc, bool approximate) {\n+    return _pc_desc_container.find_pc_desc(pc, approximate, PcDescSearch(code_begin(), scopes_pcs_begin(), scopes_pcs_end()));\n+  }\n@@ -329,1 +361,59 @@\n-  address header_end() const                  { return (address)    header_begin() + header_size(); }\n+  \/\/ STW two-phase nmethod root processing helpers.\n+  \/\/\n+  \/\/ When determining liveness of a given nmethod to do code cache unloading,\n+  \/\/ some collectors need to do different things depending on whether the nmethods\n+  \/\/ need to absolutely be kept alive during root processing; \"strong\"ly reachable\n+  \/\/ nmethods are known to be kept alive at root processing, but the liveness of\n+  \/\/ \"weak\"ly reachable ones is to be determined later.\n+  \/\/\n+  \/\/ We want to allow strong and weak processing of nmethods by different threads\n+  \/\/ at the same time without heavy synchronization. Additional constraints are\n+  \/\/ to make sure that every nmethod is processed a minimal amount of time, and\n+  \/\/ nmethods themselves are always iterated at most once at a particular time.\n+  \/\/\n+  \/\/ Note that strong processing work must be a superset of weak processing work\n+  \/\/ for this code to work.\n+  \/\/\n+  \/\/ We store state and claim information in the _oops_do_mark_link member, using\n+  \/\/ the two LSBs for the state and the remaining upper bits for linking together\n+  \/\/ nmethods that were already visited.\n+  \/\/ The last element is self-looped, i.e. points to itself to avoid some special\n+  \/\/ \"end-of-list\" sentinel value.\n+  \/\/\n+  \/\/ _oops_do_mark_link special values:\n+  \/\/\n+  \/\/   _oops_do_mark_link == nullptr: the nmethod has not been visited at all yet, i.e.\n+  \/\/      is Unclaimed.\n+  \/\/\n+  \/\/ For other values, its lowest two bits indicate the following states of the nmethod:\n+  \/\/\n+  \/\/   weak_request (WR): the nmethod has been claimed by a thread for weak processing\n+  \/\/   weak_done (WD): weak processing has been completed for this nmethod.\n+  \/\/   strong_request (SR): the nmethod has been found to need strong processing while\n+  \/\/       being weak processed.\n+  \/\/   strong_done (SD): strong processing has been completed for this nmethod .\n+  \/\/\n+  \/\/ The following shows the _only_ possible progressions of the _oops_do_mark_link\n+  \/\/ pointer.\n+  \/\/\n+  \/\/ Given\n+  \/\/   N as the nmethod\n+  \/\/   X the current next value of _oops_do_mark_link\n+  \/\/\n+  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD: the nmethod has been processed weakly by\n+  \/\/   a single thread.\n+  \/\/ Unclaimed (C)-> N|WR (C)-> X|WD (O)-> X|SD: after weak processing has been\n+  \/\/   completed (as above) another thread found that the nmethod needs strong\n+  \/\/   processing after all.\n+  \/\/ Unclaimed (C)-> N|WR (O)-> N|SR (C)-> X|SD: during weak processing another\n+  \/\/   thread finds that the nmethod needs strong processing, marks it as such and\n+  \/\/   terminates. The original thread completes strong processing.\n+  \/\/ Unclaimed (C)-> N|SD (C)-> X|SD: the nmethod has been processed strongly from\n+  \/\/   the beginning by a single thread.\n+  \/\/\n+  \/\/ \"|\" describes the concatenation of bits in _oops_do_mark_link.\n+  \/\/\n+  \/\/ The diagram also describes the threads responsible for changing the nmethod to\n+  \/\/ the next state by marking the _transition_ with (C) and (O), which mean \"current\"\n+  \/\/ and \"other\" thread respectively.\n+  \/\/\n@@ -331,1 +421,51 @@\n- public:\n+  \/\/ States used for claiming nmethods during root processing.\n+  static const uint claim_weak_request_tag = 0;\n+  static const uint claim_weak_done_tag = 1;\n+  static const uint claim_strong_request_tag = 2;\n+  static const uint claim_strong_done_tag = 3;\n+\n+  static oops_do_mark_link* mark_link(nmethod* nm, uint tag) {\n+    assert(tag <= claim_strong_done_tag, \"invalid tag %u\", tag);\n+    assert(is_aligned(nm, 4), \"nmethod pointer must have zero lower two LSB\");\n+    return (oops_do_mark_link*)(((uintptr_t)nm & ~0x3) | tag);\n+  }\n+\n+  static uint extract_state(oops_do_mark_link* link) {\n+    return (uint)((uintptr_t)link & 0x3);\n+  }\n+\n+  static nmethod* extract_nmethod(oops_do_mark_link* link) {\n+    return (nmethod*)((uintptr_t)link & ~0x3);\n+  }\n+\n+  void oops_do_log_change(const char* state);\n+\n+  static bool oops_do_has_weak_request(oops_do_mark_link* next) {\n+    return extract_state(next) == claim_weak_request_tag;\n+  }\n+\n+  static bool oops_do_has_any_strong_state(oops_do_mark_link* next) {\n+    return extract_state(next) >= claim_strong_request_tag;\n+  }\n+\n+  \/\/ Attempt Unclaimed -> N|WR transition. Returns true if successful.\n+  bool oops_do_try_claim_weak_request();\n+\n+  \/\/ Attempt Unclaimed -> N|SD transition. Returns the current link.\n+  oops_do_mark_link* oops_do_try_claim_strong_done();\n+  \/\/ Attempt N|WR -> X|WD transition. Returns nullptr if successful, X otherwise.\n+  nmethod* oops_do_try_add_to_list_as_weak_done();\n+\n+  \/\/ Attempt X|WD -> N|SR transition. Returns the current link.\n+  oops_do_mark_link* oops_do_try_add_strong_request(oops_do_mark_link* next);\n+  \/\/ Attempt X|WD -> X|SD transition. Returns true if successful.\n+  bool oops_do_try_claim_weak_done_as_strong_done(oops_do_mark_link* next);\n+\n+  \/\/ Do the N|SD -> X|SD transition.\n+  void oops_do_add_to_list_as_strong_done();\n+\n+  \/\/ Sets this nmethod as strongly claimed (as part of N|SD -> X|SD and N|SR -> X|SD\n+  \/\/ transitions).\n+  void oops_do_set_strong_done(nmethod* old_head);\n+\n+public:\n@@ -354,8 +494,0 @@\n-  \/\/ Only used for unit tests.\n-  nmethod()\n-    : CompiledMethod(),\n-      _native_receiver_sp_offset(in_ByteSize(-1)),\n-      _native_basic_lock_sp_offset(in_ByteSize(-1)),\n-      _is_unloading_state(0) {}\n-\n-\n@@ -373,3 +505,17 @@\n-  \/\/ type info\n-  bool is_nmethod() const                         { return true; }\n-  bool is_osr_method() const                      { return _entry_bci != InvocationEntryBci; }\n+  Method* method       () const { return _method; }\n+  bool is_native_method() const { return _method != nullptr && _method->is_native(); }\n+  bool is_java_method  () const { return _method != nullptr && !_method->is_native(); }\n+  bool is_osr_method   () const { return _entry_bci != InvocationEntryBci; }\n+\n+  \/\/ Compiler task identification.  Note that all OSR methods\n+  \/\/ are numbered in an independent sequence if CICountOSR is true,\n+  \/\/ and native method wrappers are also numbered independently if\n+  \/\/ CICountNative is true.\n+  int compile_id() const { return _compile_id; }\n+  const char* compile_kind() const;\n+\n+  inline bool  is_compiled_by_c1   () const { return _compiler_type == compiler_c1; }\n+  inline bool  is_compiled_by_c2   () const { return _compiler_type == compiler_c2; }\n+  inline bool  is_compiled_by_jvmci() const { return _compiler_type == compiler_jvmci; }\n+  CompilerType compiler_type       () const { return _compiler_type; }\n+  const char*  compiler_name       () const;\n@@ -378,22 +524,25 @@\n-  address consts_begin          () const          { return           header_begin() + _consts_offset        ; }\n-  address consts_end            () const          { return           code_begin()                           ; }\n-  address stub_begin            () const          { return           header_begin() + _stub_offset          ; }\n-  address stub_end              () const          { return           header_begin() + _oops_offset          ; }\n-  address exception_begin       () const          { return           header_begin() + _exception_offset     ; }\n-  address unwind_handler_begin  () const          { return _unwind_handler_offset != -1 ? (header_begin() + _unwind_handler_offset) : nullptr; }\n-  oop*    oops_begin            () const          { return (oop*)   (header_begin() + _oops_offset)         ; }\n-  oop*    oops_end              () const          { return (oop*)   (header_begin() + _metadata_offset)     ; }\n-\n-  Metadata** metadata_begin   () const            { return (Metadata**)  (header_begin() + _metadata_offset)     ; }\n-  Metadata** metadata_end     () const            { return (Metadata**)  _scopes_data_begin; }\n-\n-  address scopes_data_end       () const          { return           header_begin() + _scopes_pcs_offset    ; }\n-  PcDesc* scopes_pcs_begin      () const          { return (PcDesc*)(header_begin() + _scopes_pcs_offset   ); }\n-  PcDesc* scopes_pcs_end        () const          { return (PcDesc*)(header_begin() + _dependencies_offset) ; }\n-  address dependencies_begin    () const          { return           header_begin() + _dependencies_offset  ; }\n-  address dependencies_end      () const          { return           header_begin() + _handler_table_offset ; }\n-  address handler_table_begin   () const          { return           header_begin() + _handler_table_offset ; }\n-  address handler_table_end     () const          { return           header_begin() + _nul_chk_table_offset ; }\n-  address nul_chk_table_begin   () const          { return           header_begin() + _nul_chk_table_offset ; }\n-\n-  int skipped_instructions_size () const          { return           _skipped_instructions_size             ; }\n+  address consts_begin          () const { return           header_begin() + _consts_offset           ; }\n+  address consts_end            () const { return           header_begin() +  code_offset()           ; }\n+  address insts_begin           () const { return           header_begin() +  code_offset()           ; }\n+  address insts_end             () const { return           header_begin() + _stub_offset             ; }\n+  address stub_begin            () const { return           header_begin() + _stub_offset             ; }\n+  address stub_end              () const { return           header_begin() + _oops_offset             ; }\n+  address exception_begin       () const { return           header_begin() + _exception_offset        ; }\n+  address deopt_handler_begin   () const { return           header_begin() + _deopt_handler_offset    ; }\n+  address deopt_mh_handler_begin() const { return           header_begin() + _deopt_mh_handler_offset ; }\n+  address unwind_handler_begin  () const { return _unwind_handler_offset != -1 ? (header_begin() + _unwind_handler_offset) : nullptr; }\n+  oop*    oops_begin            () const { return (oop*)   (header_begin() + _oops_offset)            ; }\n+  oop*    oops_end              () const { return (oop*)   (header_begin() + _metadata_offset)        ; }\n+\n+  Metadata** metadata_begin     () const { return (Metadata**) (header_begin() + _metadata_offset)    ; }\n+  Metadata** metadata_end       () const { return (Metadata**) (header_begin() + _scopes_data_offset) ; }\n+\n+  address scopes_data_begin     () const { return           header_begin() + _scopes_data_offset      ; }\n+  address scopes_data_end       () const { return           header_begin() + _scopes_pcs_offset       ; }\n+  PcDesc* scopes_pcs_begin      () const { return (PcDesc*)(header_begin() + _scopes_pcs_offset)      ; }\n+  PcDesc* scopes_pcs_end        () const { return (PcDesc*)(header_begin() + _dependencies_offset)    ; }\n+  address dependencies_begin    () const { return           header_begin() + _dependencies_offset     ; }\n+  address dependencies_end      () const { return           header_begin() + _handler_table_offset    ; }\n+  address handler_table_begin   () const { return           header_begin() + _handler_table_offset    ; }\n+  address handler_table_end     () const { return           header_begin() + _nul_chk_table_offset    ; }\n+  address nul_chk_table_begin   () const { return           header_begin() + _nul_chk_table_offset    ; }\n@@ -402,5 +551,5 @@\n-  address nul_chk_table_end     () const          { return           header_begin() + _speculations_offset  ; }\n-  address speculations_begin    () const          { return           header_begin() + _speculations_offset  ; }\n-  address speculations_end      () const          { return           header_begin() + _jvmci_data_offset   ; }\n-  address jvmci_data_begin      () const          { return           header_begin() + _jvmci_data_offset    ; }\n-  address jvmci_data_end        () const          { return           header_begin() + _nmethod_end_offset   ; }\n+  address nul_chk_table_end     () const { return           header_begin() + _speculations_offset     ; }\n+  address speculations_begin    () const { return           header_begin() + _speculations_offset     ; }\n+  address speculations_end      () const { return           header_begin() + _jvmci_data_offset       ; }\n+  address jvmci_data_begin      () const { return           header_begin() + _jvmci_data_offset       ; }\n+  address jvmci_data_end        () const { return           header_begin() + _nmethod_end_offset      ; }\n@@ -408,1 +557,1 @@\n-  address nul_chk_table_end     () const          { return           header_begin() + _nmethod_end_offset   ; }\n+  address nul_chk_table_end     () const { return           header_begin() + _nmethod_end_offset      ; }\n@@ -412,3 +561,10 @@\n-  int oops_size         () const                  { return int((address)  oops_end         () - (address)  oops_begin         ()); }\n-  int metadata_size     () const                  { return int((address)  metadata_end     () - (address)  metadata_begin     ()); }\n-  int dependencies_size () const                  { return int(           dependencies_end () -            dependencies_begin ()); }\n+  int consts_size       () const { return int(          consts_end       () -           consts_begin       ()); }\n+  int insts_size        () const { return int(          insts_end        () -           insts_begin        ()); }\n+  int stub_size         () const { return int(          stub_end         () -           stub_begin         ()); }\n+  int oops_size         () const { return int((address) oops_end         () - (address) oops_begin         ()); }\n+  int metadata_size     () const { return int((address) metadata_end     () - (address) metadata_begin     ()); }\n+  int scopes_data_size  () const { return int(          scopes_data_end  () -           scopes_data_begin  ()); }\n+  int scopes_pcs_size   () const { return int((intptr_t)scopes_pcs_end   () - (intptr_t)scopes_pcs_begin   ()); }\n+  int dependencies_size () const { return int(          dependencies_end () -           dependencies_begin ()); }\n+  int handler_table_size() const { return int(          handler_table_end() -           handler_table_begin()); }\n+  int nul_chk_table_size() const { return int(          nul_chk_table_end() -           nul_chk_table_begin()); }\n@@ -416,2 +572,2 @@\n-  int speculations_size () const                  { return int(           speculations_end () -            speculations_begin ()); }\n-  int jvmci_data_size   () const                  { return int(           jvmci_data_end   () -            jvmci_data_begin   ()); }\n+  int speculations_size () const { return int(          speculations_end () -           speculations_begin ()); }\n+  int jvmci_data_size   () const { return int(          jvmci_data_end   () -           jvmci_data_begin   ()); }\n@@ -423,1 +579,2 @@\n-  int total_size        () const;\n+  int skipped_instructions_size () const { return _skipped_instructions_size; }\n+  int total_size() const;\n@@ -426,4 +583,12 @@\n-  bool oops_contains         (oop*    addr) const { return oops_begin         () <= addr && addr < oops_end         (); }\n-  bool metadata_contains     (Metadata** addr) const   { return metadata_begin     () <= addr && addr < metadata_end     (); }\n-  bool scopes_data_contains  (address addr) const { return scopes_data_begin  () <= addr && addr < scopes_data_end  (); }\n-  bool scopes_pcs_contains   (PcDesc* addr) const { return scopes_pcs_begin   () <= addr && addr < scopes_pcs_end   (); }\n+  bool consts_contains         (address addr) const { return consts_begin       () <= addr && addr < consts_end       (); }\n+  \/\/ Returns true if a given address is in the 'insts' section. The method\n+  \/\/ insts_contains_inclusive() is end-inclusive.\n+  bool insts_contains          (address addr) const { return insts_begin        () <= addr && addr < insts_end        (); }\n+  bool insts_contains_inclusive(address addr) const { return insts_begin        () <= addr && addr <= insts_end       (); }\n+  bool stub_contains           (address addr) const { return stub_begin         () <= addr && addr < stub_end         (); }\n+  bool oops_contains           (oop*    addr) const { return oops_begin         () <= addr && addr < oops_end         (); }\n+  bool metadata_contains       (Metadata** addr) const { return metadata_begin  () <= addr && addr < metadata_end     (); }\n+  bool scopes_data_contains    (address addr) const { return scopes_data_begin  () <= addr && addr < scopes_data_end  (); }\n+  bool scopes_pcs_contains     (PcDesc* addr) const { return scopes_pcs_begin   () <= addr && addr < scopes_pcs_end   (); }\n+  bool handler_table_contains  (address addr) const { return handler_table_begin() <= addr && addr < handler_table_end(); }\n+  bool nul_chk_table_contains  (address addr) const { return nul_chk_table_begin() <= addr && addr < nul_chk_table_end(); }\n@@ -432,2 +597,8 @@\n-  address entry_point() const                     { return _entry_point;             } \/\/ normal entry point\n-  address verified_entry_point() const            { return _verified_entry_point;    } \/\/ if klass is correct\n+  address entry_point() const          { return _entry_point;          } \/\/ normal entry point\n+  address verified_entry_point() const { return _verified_entry_point; } \/\/ if klass is correct\n+\n+  enum : signed char { not_installed = -1, \/\/ in construction, only the owner doing the construction is\n+                                           \/\/ allowed to advance state\n+                       in_use        = 0,  \/\/ executable nmethod\n+                       not_entrant   = 1   \/\/ marked for deoptimization but activations may still exist\n+  };\n@@ -436,3 +607,4 @@\n-  bool  is_not_installed() const                  { return _state == not_installed; }\n-  bool  is_in_use() const                         { return _state <= in_use; }\n-  bool  is_not_entrant() const                    { return _state == not_entrant; }\n+  bool is_not_installed() const        { return _state == not_installed; }\n+  bool is_in_use() const               { return _state <= in_use; }\n+  bool is_not_entrant() const          { return _state == not_entrant; }\n+  int  get_state() const               { return _state; }\n@@ -443,2 +615,2 @@\n-  virtual bool is_unloading();\n-  virtual void do_unloading(bool unloading_occurred);\n+  bool is_unloading();\n+  void do_unloading(bool unloading_occurred);\n@@ -446,2 +618,2 @@\n-  bool is_unlinked() const                        { return _is_unlinked; }\n-  void set_is_unlinked()                          { assert(!_is_unlinked, \"already unlinked\"); _is_unlinked = true; }\n+  bool is_unlinked() const             { return _is_unlinked; }\n+  void set_is_unlinked()               { assert(!_is_unlinked, \"already unlinked\"); _is_unlinked = true; }\n@@ -451,2 +623,2 @@\n-  RTMState  rtm_state() const                     { return _rtm_state; }\n-  void set_rtm_state(RTMState state)              { _rtm_state = state; }\n+  RTMState  rtm_state() const          { return _rtm_state; }\n+  void set_rtm_state(RTMState state)   { _rtm_state = state; }\n@@ -465,2 +637,10 @@\n-  int get_state() const {\n-    return _state;\n+  bool  is_marked_for_deoptimization() const { return deoptimization_status() != not_marked; }\n+  bool  has_been_deoptimized() const { return deoptimization_status() == deoptimize_done; }\n+  void  set_deoptimized_done();\n+\n+  bool update_recompile_counts() const {\n+    \/\/ Update recompile counts when either the update is explicitly requested (deoptimize)\n+    \/\/ or the nmethod is not marked for deoptimization at all (not_marked).\n+    \/\/ The latter happens during uncommon traps when deoptimized nmethod is made not entrant.\n+    DeoptimizationStatus status = deoptimization_status();\n+    return status != deoptimize_noupdate && status != deoptimize_done;\n@@ -469,0 +649,4 @@\n+  \/\/ tells whether frames described by this nmethod can be deoptimized\n+  \/\/ note: native wrappers cannot be deoptimized.\n+  bool can_be_deoptimized() const { return is_java_method(); }\n+\n@@ -472,2 +656,20 @@\n-  bool has_flushed_dependencies()                 { return _has_flushed_dependencies; }\n-  void set_has_flushed_dependencies()             {\n+\n+  template<typename T>\n+  T* gc_data() const                              { return reinterpret_cast<T*>(_gc_data); }\n+  template<typename T>\n+  void set_gc_data(T* gc_data)                    { _gc_data = reinterpret_cast<void*>(gc_data); }\n+\n+  bool  has_unsafe_access() const                 { return _has_unsafe_access; }\n+  void  set_has_unsafe_access(bool z)             { _has_unsafe_access = z; }\n+\n+  bool  has_monitors() const                      { return _has_monitors; }\n+  void  set_has_monitors(bool z)                  { _has_monitors = z; }\n+\n+  bool  has_method_handle_invokes() const         { return _has_method_handle_invokes; }\n+  void  set_has_method_handle_invokes(bool z)     { _has_method_handle_invokes = z; }\n+\n+  bool  has_wide_vectors() const                  { return _has_wide_vectors; }\n+  void  set_has_wide_vectors(bool z)              { _has_wide_vectors = z; }\n+\n+  bool  has_flushed_dependencies() const          { return _has_flushed_dependencies; }\n+  void  set_has_flushed_dependencies(bool z)      {\n@@ -475,1 +677,1 @@\n-    _has_flushed_dependencies = 1;\n+    _has_flushed_dependencies = z;\n@@ -480,2 +682,0 @@\n-  void unlink_from_method();\n-\n@@ -494,1 +694,1 @@\n-  Metadata*     metadata_at(int index) const      { return index == 0 ? nullptr: *metadata_addr_at(index); }\n+  Metadata*   metadata_at(int index) const      { return index == 0 ? nullptr: *metadata_addr_at(index); }\n@@ -509,0 +709,3 @@\n+protected:\n+  address oops_reloc_begin() const;\n+\n@@ -513,0 +716,74 @@\n+  bool is_at_poll_return(address pc);\n+  bool is_at_poll_or_poll_return(address pc);\n+\n+protected:\n+  \/\/ Exception cache support\n+  \/\/ Note: _exception_cache may be read and cleaned concurrently.\n+  ExceptionCache* exception_cache() const         { return _exception_cache; }\n+  ExceptionCache* exception_cache_acquire() const;\n+  void set_exception_cache(ExceptionCache *ec)    { _exception_cache = ec; }\n+\n+public:\n+  address handler_for_exception_and_pc(Handle exception, address pc);\n+  void add_handler_for_exception_and_pc(Handle exception, address pc, address handler);\n+  void clean_exception_cache();\n+\n+  void add_exception_cache_entry(ExceptionCache* new_entry);\n+  ExceptionCache* exception_cache_entry_for_exception(Handle exception);\n+\n+\n+  \/\/ MethodHandle\n+  bool is_method_handle_return(address return_pc);\n+  \/\/ Deopt\n+  \/\/ Return true is the PC is one would expect if the frame is being deopted.\n+  inline bool is_deopt_pc(address pc);\n+  inline bool is_deopt_mh_entry(address pc);\n+  inline bool is_deopt_entry(address pc);\n+\n+  \/\/ Accessor\/mutator for the original pc of a frame before a frame was deopted.\n+  address get_original_pc(const frame* fr) { return *orig_pc_addr(fr); }\n+  void    set_original_pc(const frame* fr, address pc) { *orig_pc_addr(fr) = pc; }\n+\n+  const char* state() const;\n+\n+  bool inlinecache_check_contains(address addr) const {\n+    return (addr >= code_begin() && addr < verified_entry_point());\n+  }\n+\n+  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap *reg_map, OopClosure* f) override;\n+\n+  \/\/ implicit exceptions support\n+  address continuation_for_implicit_div0_exception(address pc) { return continuation_for_implicit_exception(pc, true); }\n+  address continuation_for_implicit_null_exception(address pc) { return continuation_for_implicit_exception(pc, false); }\n+\n+  static address get_deopt_original_pc(const frame* fr);\n+\n+  \/\/ Inline cache support for class unloading and nmethod unloading\n+ private:\n+  void cleanup_inline_caches_impl(bool unloading_occurred, bool clean_all);\n+\n+  address continuation_for_implicit_exception(address pc, bool for_div0_check);\n+\n+ public:\n+  \/\/ Serial version used by whitebox test\n+  void cleanup_inline_caches_whitebox();\n+\n+  virtual void clear_inline_caches();\n+\n+  \/\/ Execute nmethod barrier code, as if entering through nmethod call.\n+  void run_nmethod_entry_barrier();\n+\n+  void verify_oop_relocations();\n+\n+  bool has_evol_metadata();\n+\n+  Method* attached_method(address call_pc);\n+  Method* attached_method_before_pc(address pc);\n+\n+  \/\/ GC unloading support\n+  \/\/ Cleans unloaded klasses and unloaded nmethods in inline caches\n+\n+  void unload_nmethod_caches(bool class_unloading_occurred);\n+\n+  void unlink_from_method();\n+\n@@ -527,1 +804,1 @@\n-  void purge(bool free_code_cache_data, bool unregister_nmethod);\n+  virtual void purge(bool free_code_cache_data, bool unregister_nmethod) override;\n@@ -552,1 +829,0 @@\n- public:\n@@ -594,0 +870,9 @@\n+  \/\/ ScopeDesc retrieval operation\n+  PcDesc* pc_desc_at(address pc)   { return find_pc_desc(pc, false); }\n+  \/\/ pc_desc_near returns the first PcDesc at or after the given pc.\n+  PcDesc* pc_desc_near(address pc) { return find_pc_desc(pc, true); }\n+\n+  \/\/ ScopeDesc for an instruction\n+  ScopeDesc* scope_desc_at(address pc);\n+  ScopeDesc* scope_desc_near(address pc);\n+\n@@ -607,1 +892,1 @@\n-  void verify();\n+  virtual void verify() override;\n@@ -619,1 +904,1 @@\n-  void print()                          const;\n+  virtual void print()                  const override;\n@@ -629,1 +914,1 @@\n-  void print_value_on(outputStream* st) const;\n+  void print_value_on(outputStream* st) const override;\n@@ -649,1 +934,1 @@\n-  virtual void print_on(outputStream* st) const { CodeBlob::print_on(st); }\n+  virtual void print_on(outputStream* st) const override { CodeBlob::print_on(st); }\n@@ -658,1 +943,1 @@\n-  virtual void print_block_comment(outputStream* stream, address block_begin) const {\n+  virtual void print_block_comment(outputStream* stream, address block_begin) const override {\n@@ -673,7 +958,0 @@\n-  \/\/ Compiler task identification.  Note that all OSR methods\n-  \/\/ are numbered in an independent sequence if CICountOSR is true,\n-  \/\/ and native method wrappers are also numbered independently if\n-  \/\/ CICountNative is true.\n-  virtual int compile_id() const { return _compile_id; }\n-  const char* compile_kind() const;\n-\n@@ -687,1 +965,1 @@\n-  virtual bool is_dependent_on_method(Method* dependee);\n+  bool is_dependent_on_method(Method* dependee);\n@@ -702,1 +980,1 @@\n-  virtual void metadata_do(MetadataClosure* f);\n+  void metadata_do(MetadataClosure* f);\n@@ -706,1 +984,1 @@\n-  virtual void  make_deoptimized();\n+  void make_deoptimized();\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":491,"deletions":213,"binary":false,"changes":704,"status":"modified"},{"patch":"@@ -0,0 +1,89 @@\n+\/*\n+ * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_CODE_NMETHOD_INLINE_HPP\n+#define SHARE_CODE_NMETHOD_INLINE_HPP\n+\n+#include \"code\/nmethod.hpp\"\n+\n+#include \"code\/nativeInst.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/frame.hpp\"\n+\n+inline bool nmethod::is_deopt_pc(address pc) { return is_deopt_entry(pc) || is_deopt_mh_entry(pc); }\n+\n+\/\/ When using JVMCI the address might be off by the size of a call instruction.\n+inline bool nmethod::is_deopt_entry(address pc) {\n+  return pc == deopt_handler_begin()\n+#if INCLUDE_JVMCI\n+    || (is_compiled_by_jvmci() && pc == (deopt_handler_begin() + NativeCall::instruction_size))\n+#endif\n+    ;\n+}\n+\n+inline bool nmethod::is_deopt_mh_entry(address pc) {\n+  return pc == deopt_mh_handler_begin()\n+#if INCLUDE_JVMCI\n+    || (is_compiled_by_jvmci() && pc == (deopt_mh_handler_begin() + NativeCall::instruction_size))\n+#endif\n+    ;\n+}\n+\n+\/\/ -----------------------------------------------------------------------------\n+\/\/ nmethod::get_deopt_original_pc\n+\/\/\n+\/\/ Return the original PC for the given PC if:\n+\/\/ (a) the given PC belongs to a nmethod and\n+\/\/ (b) it is a deopt PC\n+\n+inline address nmethod::get_deopt_original_pc(const frame* fr) {\n+  if (fr->cb() == nullptr)  return nullptr;\n+\n+  nmethod* nm = fr->cb()->as_nmethod_or_null();\n+  if (nm != nullptr && nm->is_deopt_pc(fr->pc())) {\n+    return nm->get_original_pc(fr);\n+  }\n+  return nullptr;\n+}\n+\n+\n+\/\/ class ExceptionCache methods\n+\n+inline int ExceptionCache::count() { return Atomic::load_acquire(&_count); }\n+\n+address ExceptionCache::pc_at(int index) {\n+  assert(index >= 0 && index < count(),\"\");\n+  return _pc[index];\n+}\n+\n+address ExceptionCache::handler_at(int index) {\n+  assert(index >= 0 && index < count(),\"\");\n+  return _handler[index];\n+}\n+\n+\/\/ increment_count is only called under lock, but there may be concurrent readers.\n+inline void ExceptionCache::increment_count() { Atomic::release_store(&_count, _count + 1); }\n+\n+\n+#endif \/\/ SHARE_CODE_NMETHOD_INLINE_HPP\n","filename":"src\/hotspot\/share\/code\/nmethod.inline.hpp","additions":89,"deletions":0,"binary":false,"changes":89,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-address PcDesc::real_pc(const CompiledMethod* code) const {\n+address PcDesc::real_pc(const nmethod* code) const {\n@@ -43,1 +43,1 @@\n-void PcDesc::print_on(outputStream* st, CompiledMethod* code) {\n+void PcDesc::print_on(outputStream* st, nmethod* code) {\n@@ -60,1 +60,1 @@\n-bool PcDesc::verify(CompiledMethod* code) {\n+bool PcDesc::verify(nmethod* code) {\n","filename":"src\/hotspot\/share\/code\/pcDesc.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -105,1 +105,1 @@\n-  address real_pc(const CompiledMethod* code) const;\n+  address real_pc(const nmethod* code) const;\n@@ -107,3 +107,3 @@\n-  void print(CompiledMethod* code) { print_on(tty, code); }\n-  void print_on(outputStream* st, CompiledMethod* code);\n-  bool verify(CompiledMethod* code);\n+  void print(nmethod* code) { print_on(tty, code); }\n+  void print_on(outputStream* st, nmethod* code);\n+  bool verify(nmethod* code);\n","filename":"src\/hotspot\/share\/code\/pcDesc.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-void RelocIterator::initialize(CompiledMethod* nm, address begin, address limit) {\n+void RelocIterator::initialize(nmethod* nm, address begin, address limit) {\n@@ -126,1 +126,1 @@\n-    nm = (cb != nullptr) ? cb->as_compiled_method_or_null() : nullptr;\n+    nm = (cb != nullptr) ? cb->as_nmethod_or_null() : nullptr;\n@@ -636,3 +636,3 @@\n-  CompiledMethod* cm = code();\n-  if (cm == nullptr) return (Method*)nullptr;\n-  Metadata* m = cm->metadata_at(_method_index);\n+  nmethod* nm = code();\n+  if (nm == nullptr) return (Method*)nullptr;\n+  Metadata* m = nm->metadata_at(_method_index);\n@@ -662,3 +662,3 @@\n-  CompiledMethod* cm = code();\n-  if (cm == nullptr) return (Method*)nullptr;\n-  Metadata* m = cm->metadata_at(_method_index);\n+  nmethod* nm = code();\n+  if (nm == nullptr) return (Method*)nullptr;\n+  Metadata* m = nm->metadata_at(_method_index);\n@@ -692,3 +692,3 @@\n-  CompiledMethod* cm = code();\n-  if (cm == nullptr) return (Method*)nullptr;\n-  Metadata* m = cm->metadata_at(_method_index);\n+  nmethod* nm = code();\n+  if (nm == nullptr) return (Method*)nullptr;\n+  Metadata* m = nm->metadata_at(_method_index);\n","filename":"src\/hotspot\/share\/code\/relocInfo.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,0 @@\n-class nmethod;\n@@ -39,1 +38,0 @@\n-class CompiledMethod;\n@@ -42,0 +40,1 @@\n+class nmethod;\n@@ -574,1 +573,1 @@\n-  CompiledMethod* _code;    \/\/ compiled method containing _addr\n+  nmethod*        _code;    \/\/ compiled method containing _addr\n@@ -604,1 +603,1 @@\n-  void initialize(CompiledMethod* nm, address begin, address limit);\n+  void initialize(nmethod* nm, address begin, address limit);\n@@ -610,1 +609,1 @@\n-  RelocIterator(CompiledMethod* nm, address begin = nullptr, address limit = nullptr);\n+  RelocIterator(nmethod* nm, address begin = nullptr, address limit = nullptr);\n@@ -643,1 +642,1 @@\n-  CompiledMethod*     code()  const { return _code; }\n+  nmethod*     code()         const { return _code; }\n@@ -830,1 +829,1 @@\n-  CompiledMethod* code()            const { return binding()->code(); }\n+  nmethod*        code()            const { return binding()->code(); }\n@@ -1466,1 +1465,1 @@\n-inline RelocIterator::RelocIterator(CompiledMethod* nm, address begin, address limit) {\n+inline RelocIterator::RelocIterator(nmethod* nm, address begin, address limit) {\n","filename":"src\/hotspot\/share\/code\/relocInfo.hpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-ScopeDesc::ScopeDesc(const CompiledMethod* code, PcDesc* pd, bool ignore_objects) {\n+ScopeDesc::ScopeDesc(const nmethod* code, PcDesc* pd, bool ignore_objects) {\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,1 +44,1 @@\n-  SimpleScopeDesc(CompiledMethod* code, address pc) {\n+  SimpleScopeDesc(nmethod* code, address pc) {\n@@ -64,1 +64,1 @@\n-  ScopeDesc(const CompiledMethod* code, PcDesc* pd, bool ignore_objects = false);\n+  ScopeDesc(const nmethod* code, PcDesc* pd, bool ignore_objects = false);\n@@ -123,1 +123,1 @@\n-  const CompiledMethod* _code;\n+  const nmethod* _code;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -215,1 +215,1 @@\n-  CompiledMethod *nm = method->code();\n+  nmethod *nm = method->code();\n@@ -711,1 +711,1 @@\n-                                      int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS) {\n+                                      int branch_bci, int bci, CompLevel comp_level, nmethod* nm, TRAPS) {\n@@ -1140,1 +1140,1 @@\n-                                                      CompLevel level, CompiledMethod* nm, TRAPS) {\n+                                                      CompLevel level, nmethod* nm, TRAPS) {\n@@ -1155,1 +1155,1 @@\n-                                                     int bci, CompLevel level, CompiledMethod* nm, TRAPS) {\n+                                                     int bci, CompLevel level, nmethod* nm, TRAPS) {\n","filename":"src\/hotspot\/share\/compiler\/compilationPolicy.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -238,1 +238,1 @@\n-                               CompLevel level, CompiledMethod* nm, TRAPS);\n+                                      CompLevel level, nmethod* nm, TRAPS);\n@@ -240,1 +240,1 @@\n-                                int bci, CompLevel level, CompiledMethod* nm, TRAPS);\n+                                      int bci, CompLevel level, nmethod* nm, TRAPS);\n@@ -268,1 +268,1 @@\n-                 int branch_bci, int bci, CompLevel comp_level, CompiledMethod* nm, TRAPS);\n+                        int branch_bci, int bci, CompLevel comp_level, nmethod* nm, TRAPS);\n","filename":"src\/hotspot\/share\/compiler\/compilationPolicy.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1379,3 +1379,2 @@\n-    CompiledMethod* method_code = method->code();\n-    if (method_code != nullptr && method_code->is_nmethod()\n-                      && (compile_reason != CompileTask::Reason_DirectivesChanged)) {\n+    nmethod* method_code = method->code();\n+    if (method_code != nullptr && (compile_reason != CompileTask::Reason_DirectivesChanged)) {\n@@ -1484,6 +1483,1 @@\n-    CompiledMethod* code = method->code();\n-    if (code == nullptr) {\n-      return (nmethod*) code;\n-    } else {\n-      return code->as_nmethod_or_null();\n-    }\n+    return method->code();\n@@ -1514,1 +1508,1 @@\n-      CompiledMethod* result = method->code();\n+      nmethod* result = method->code();\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -501,1 +501,0 @@\n-      \/\/DEBUG_ONLY(nof_callee++;)\n@@ -523,1 +522,0 @@\n-  DEBUG_ONLY(int nof_callee = 0;)\n@@ -525,7 +523,0 @@\n-\n-  \/\/ Check that runtime stubs save all callee-saved registers\n-#ifdef COMPILER2\n-  assert(cb == nullptr || cb->is_compiled_by_c1() || cb->is_compiled_by_jvmci() || !cb->is_runtime_stub() ||\n-         (nof_callee >= SAVED_ON_ENTRY_REG_COUNT || nof_callee >= C_SAVED_ON_ENTRY_REG_COUNT),\n-         \"must save all\");\n-#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":10,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -339,1 +339,1 @@\n-    nmethod* nm = (cb == nullptr) ? nullptr : cb->as_compiled_method()->as_nmethod_or_null();\n+    nmethod* nm = (cb == nullptr) ? nullptr : cb->as_nmethod_or_null();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,0 @@\n-#include \"code\/compiledMethod.hpp\"\n@@ -32,2 +31,2 @@\n-bool IsUnloadingBehaviour::is_unloading(CompiledMethod* cm) {\n-  if (cm->method()->can_be_allocated_in_NonNMethod_space()) {\n+bool IsUnloadingBehaviour::is_unloading(nmethod* nm) {\n+  if (nm->method()->can_be_allocated_in_NonNMethod_space()) {\n@@ -38,1 +37,1 @@\n-  return _current->has_dead_oop(cm) || cm->as_nmethod()->is_cold();\n+  return _current->has_dead_oop(nm) || nm->is_cold();\n@@ -73,8 +72,4 @@\n-bool ClosureIsUnloadingBehaviour::has_dead_oop(CompiledMethod* cm) const {\n-  if (cm->is_nmethod()) {\n-    IsCompiledMethodUnloadingOopClosure cl(_cl);\n-    static_cast<nmethod*>(cm)->oops_do(&cl, true \/* allow_dead *\/);\n-    return cl.is_unloading();\n-  } else {\n-    return false;\n-  }\n+bool ClosureIsUnloadingBehaviour::has_dead_oop(nmethod* nm) const {\n+  IsCompiledMethodUnloadingOopClosure cl(_cl);\n+  nm->oops_do(&cl, true \/* allow_dead *\/);\n+  return cl.is_unloading();\n","filename":"src\/hotspot\/share\/gc\/shared\/gcBehaviours.cpp","additions":8,"deletions":13,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-\/\/ This is the behaviour for checking if a CompiledMethod is unloading\n+\/\/ This is the behaviour for checking if a nmethod is unloading\n@@ -37,2 +37,2 @@\n-  static bool is_unloading(CompiledMethod* cm);\n-  virtual bool has_dead_oop(CompiledMethod* cm) const = 0;\n+  static bool is_unloading(nmethod* nm);\n+  virtual bool has_dead_oop(nmethod* nm) const = 0;\n@@ -51,1 +51,1 @@\n-  virtual bool has_dead_oop(CompiledMethod* cm) const;\n+  virtual bool has_dead_oop(nmethod* nm) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/gcBehaviours.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,1 @@\n-  CompiledMethodIterator iter(CompiledMethodIterator::all_blobs);\n+  NMethodIterator iter(NMethodIterator::all_blobs);\n@@ -52,3 +52,3 @@\n-void CodeCacheUnloadingTask::claim_nmethods(CompiledMethod** claimed_nmethods, int *num_claimed_nmethods) {\n-  CompiledMethod* first;\n-  CompiledMethodIterator last(CompiledMethodIterator::all_blobs);\n+void CodeCacheUnloadingTask::claim_nmethods(nmethod** claimed_nmethods, int *num_claimed_nmethods) {\n+  nmethod* first;\n+  NMethodIterator last(NMethodIterator::all_blobs);\n@@ -60,1 +60,1 @@\n-    last = CompiledMethodIterator(CompiledMethodIterator::all_blobs, first);\n+    last = NMethodIterator(NMethodIterator::all_blobs, first);\n@@ -84,1 +84,1 @@\n-  CompiledMethod* claimed_nmethods[MaxClaimNmethods];\n+  nmethod* claimed_nmethods[MaxClaimNmethods];\n","filename":"src\/hotspot\/share\/gc\/shared\/parallelCleaning.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,2 +39,2 @@\n-  CompiledMethod* _first_nmethod;\n-  CompiledMethod* volatile _claimed_nmethod;\n+  nmethod* _first_nmethod;\n+  nmethod* volatile _claimed_nmethod;\n@@ -48,1 +48,1 @@\n-  void claim_nmethods(CompiledMethod** claimed_nmethods, int *num_claimed_nmethods);\n+  void claim_nmethods(nmethod** claimed_nmethods, int *num_claimed_nmethods);\n","filename":"src\/hotspot\/share\/gc\/shared\/parallelCleaning.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,0 +2,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,2 +81,1 @@\n-  virtual bool has_dead_oop(CompiledMethod* method) const {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual bool has_dead_oop(nmethod* const nm) const {\n@@ -93,2 +93,1 @@\n-  virtual bool lock(CompiledMethod* method) {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual bool lock(nmethod* const nm) {\n@@ -101,2 +100,1 @@\n-  virtual void unlock(CompiledMethod* method) {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual void unlock(nmethod* const nm) {\n@@ -108,2 +106,2 @@\n-  virtual bool is_safe(CompiledMethod* method) {\n-    if (SafepointSynchronize::is_at_safepoint() || method->is_unloading()) {\n+  virtual bool is_safe(nmethod* const nm) {\n+    if (SafepointSynchronize::is_at_safepoint() || nm->is_unloading()) {\n@@ -113,1 +111,0 @@\n-    nmethod* const nm = method->as_nmethod();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,2 +78,1 @@\n-  virtual bool has_dead_oop(CompiledMethod* method) const {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual bool has_dead_oop(nmethod* const nm) const {\n@@ -90,2 +89,1 @@\n-  virtual bool lock(CompiledMethod* method) {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual bool lock(nmethod* const nm) {\n@@ -97,2 +95,1 @@\n-  virtual void unlock(CompiledMethod* method) {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual void unlock(nmethod* const nm) {\n@@ -103,2 +100,2 @@\n-  virtual bool is_safe(CompiledMethod* method) {\n-    if (SafepointSynchronize::is_at_safepoint() || method->is_unloading()) {\n+  virtual bool is_safe(nmethod* const nm) {\n+    if (SafepointSynchronize::is_at_safepoint() || nm->is_unloading()) {\n@@ -108,1 +105,0 @@\n-    nmethod* const nm = method->as_nmethod();\n","filename":"src\/hotspot\/share\/gc\/x\/xUnload.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -77,2 +77,1 @@\n-  virtual bool has_dead_oop(CompiledMethod* method) const {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual bool has_dead_oop(nmethod* const nm) const {\n@@ -93,2 +92,1 @@\n-  virtual bool lock(CompiledMethod* method) {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual bool lock(nmethod* const nm) {\n@@ -100,2 +98,1 @@\n-  virtual void unlock(CompiledMethod* method) {\n-    nmethod* const nm = method->as_nmethod();\n+  virtual void unlock(nmethod* const nm) {\n@@ -106,2 +103,2 @@\n-  virtual bool is_safe(CompiledMethod* method) {\n-    if (SafepointSynchronize::is_at_safepoint() || method->is_unloading()) {\n+  virtual bool is_safe(nmethod* const nm) {\n+    if (SafepointSynchronize::is_at_safepoint() || nm->is_unloading()) {\n@@ -111,1 +108,0 @@\n-    nmethod* const nm = method->as_nmethod();\n","filename":"src\/hotspot\/share\/gc\/z\/zUnload.cpp","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1308,1 +1308,1 @@\n-  CompiledMethod* code = method->code();\n+  nmethod* code = method->code();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -259,1 +259,1 @@\n-JRT_ENTRY_NO_ASYNC(static address, exception_handler_for_pc_helper(JavaThread* current, oopDesc* ex, address pc, CompiledMethod*& cm))\n+JRT_ENTRY_NO_ASYNC(static address, exception_handler_for_pc_helper(JavaThread* current, oopDesc* ex, address pc, nmethod*& nm))\n@@ -270,2 +270,1 @@\n-  cm = CodeCache::find_compiled(pc);\n-  assert(cm != nullptr, \"this is not a compiled method\");\n+  nm = CodeCache::find_nmethod(pc);\n@@ -273,1 +272,1 @@\n-  if (cm->is_deopt_pc(pc)) {\n+  if (nm->is_deopt_pc(pc)) {\n@@ -294,1 +293,1 @@\n-    assert(cm->method() != nullptr, \"Unexpected null method()\");\n+    assert(nm->method() != nullptr, \"Unexpected null method()\");\n@@ -297,1 +296,1 @@\n-                 cm->method()->print_value_string(), p2i(pc), p2i(current));\n+                 nm->method()->print_value_string(), p2i(pc), p2i(current));\n@@ -335,1 +334,1 @@\n-    address fast_continuation = cm->handler_for_exception_and_pc(exception, pc);\n+    address fast_continuation = nm->handler_for_exception_and_pc(exception, pc);\n@@ -338,1 +337,1 @@\n-      current->set_is_method_handle_return(cm->is_method_handle_return(pc));\n+      current->set_is_method_handle_return(nm->is_method_handle_return(pc));\n@@ -359,1 +358,1 @@\n-    continuation = SharedRuntime::compute_compiled_exc_handler(cm, pc, exception, false, false, recursive_exception);\n+    continuation = SharedRuntime::compute_compiled_exc_handler(nm, pc, exception, false, false, recursive_exception);\n@@ -371,1 +370,1 @@\n-      cm->add_handler_for_exception_and_pc(exception, pc, continuation);\n+      nm->add_handler_for_exception_and_pc(exception, pc, continuation);\n@@ -376,1 +375,1 @@\n-  current->set_is_method_handle_return(cm->is_method_handle_return(pc));\n+  current->set_is_method_handle_return(nm->is_method_handle_return(pc));\n@@ -398,1 +397,1 @@\n-  CompiledMethod* cm = nullptr;\n+  nmethod* nm = nullptr;\n@@ -403,1 +402,1 @@\n-    continuation = exception_handler_for_pc_helper(current, exception, pc, cm);\n+    continuation = exception_handler_for_pc_helper(current, exception, pc, nm);\n@@ -409,1 +408,1 @@\n-  if (cm != nullptr && caller_is_deopted()) {\n+  if (nm != nullptr && caller_is_deopted()) {\n@@ -678,1 +677,1 @@\n-        tty->print(\"%s [\" INTPTR_FORMAT \"+\" JLONG_FORMAT \"]\", cb->as_nmethod_or_null()->method()->name_and_sig_as_C_string(buf, O_BUFLEN), p2i(cb->code_begin()), (jlong)((address)v - cb->code_begin()));\n+        tty->print(\"%s [\" INTPTR_FORMAT \"+\" JLONG_FORMAT \"]\", cb->as_nmethod()->method()->name_and_sig_as_C_string(buf, O_BUFLEN), p2i(cb->code_begin()), (jlong)((address)v - cb->code_begin()));\n@@ -2211,1 +2210,1 @@\n-            CompiledMethod* old = method->code();\n+            nmethod* old = method->code();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -269,1 +269,1 @@\n-  volatile_nonstatic_field(Method,             _code,                                         CompiledMethod*)                       \\\n+  volatile_nonstatic_field(Method,             _code,                                         nmethod*)                              \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1014,1 +1014,1 @@\n-  CompiledMethod* nm = code(); \/\/ Put it into local variable to guard against concurrent updates\n+  nmethod* nm = code(); \/\/ Put it into local variable to guard against concurrent updates\n@@ -1162,1 +1162,1 @@\n-void Method::unlink_code(CompiledMethod *compare) {\n+void Method::unlink_code(nmethod *compare) {\n@@ -1306,1 +1306,1 @@\n-  CompiledMethod *code = Atomic::load_acquire(&_code);\n+  nmethod *code = Atomic::load_acquire(&_code);\n@@ -1311,1 +1311,1 @@\n-void Method::set_code(const methodHandle& mh, CompiledMethod *code) {\n+void Method::set_code(const methodHandle& mh, nmethod *code) {\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-class CompiledMethod;\n+class nmethod;\n@@ -96,1 +96,1 @@\n-  volatile address _from_compiled_entry;        \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n+  volatile address _from_compiled_entry;     \/\/ Cache of: _code ? _code->entry_point() : _adapter->c2i_entry()\n@@ -102,2 +102,2 @@\n-  CompiledMethod* volatile _code;                       \/\/ Points to the corresponding piece of native code\n-  volatile address           _from_interpreted_entry; \/\/ Cache of _code ? _adapter->i2c_entry() : _i2i_entry\n+  nmethod* volatile _code;                   \/\/ Points to the corresponding piece of native code\n+  volatile address  _from_interpreted_entry; \/\/ Cache of _code ? _adapter->i2c_entry() : _i2i_entry\n@@ -360,1 +360,1 @@\n-  CompiledMethod* code() const;\n+  nmethod* code() const;\n@@ -363,1 +363,1 @@\n-  void unlink_code(CompiledMethod *compare);\n+  void unlink_code(nmethod *compare);\n@@ -376,1 +376,1 @@\n-  static void set_code(const methodHandle& mh, CompiledMethod* code);\n+  static void set_code(const methodHandle& mh, nmethod* code);\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,1 @@\n-inline CompiledMethod* Method::code() const {\n+inline nmethod* Method::code() const {\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,1 @@\n-#include \"code\/compiledMethod.hpp\"\n+#include \"code\/nmethod.hpp\"\n@@ -111,1 +111,1 @@\n-static int num_java_frames(CompiledMethod* cm, address pc) {\n+static int num_java_frames(nmethod* nm, address pc) {\n@@ -113,1 +113,1 @@\n-  for (ScopeDesc* scope = cm->scope_desc_at(pc); scope != nullptr; scope = scope->sender()) {\n+  for (ScopeDesc* scope = nm->scope_desc_at(pc); scope != nullptr; scope = scope->sender()) {\n@@ -121,2 +121,2 @@\n-         || (f.cb() != nullptr && f.cb()->is_compiled() && f.cb()->as_compiled_method()->is_java_method()), \"\");\n-  return f.is_interpreted() ? 1 : num_java_frames(f.cb()->as_compiled_method(), f.orig_pc());\n+         || (f.cb() != nullptr && f.cb()->is_nmethod() && f.cb()->as_nmethod()->is_java_method()), \"\");\n+  return f.is_interpreted() ? 1 : num_java_frames(f.cb()->as_nmethod(), f.orig_pc());\n@@ -563,1 +563,1 @@\n-  if (closure._cb != nullptr && closure._cb->is_compiled()) {\n+  if (closure._cb != nullptr && closure._cb->is_nmethod()) {\n@@ -565,1 +565,1 @@\n-      (closure._cb->as_compiled_method()->method()->num_stack_arg_slots()*VMRegImpl::stack_slot_size) >>LogBytesPerWord,\n+      (closure._cb->as_nmethod()->method()->num_stack_arg_slots()*VMRegImpl::stack_slot_size) >>LogBytesPerWord,\n@@ -567,1 +567,1 @@\n-      (closure._cb->as_compiled_method()->method()->num_stack_arg_slots()*VMRegImpl::stack_slot_size) >>LogBytesPerWord);\n+      (closure._cb->as_nmethod()->method()->num_stack_arg_slots()*VMRegImpl::stack_slot_size) >>LogBytesPerWord);\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n@@ -1854,3 +1853,2 @@\n-  if (blob->is_compiled()) {\n-    CompiledMethod* cm = blob->as_compiled_method_or_null();\n-    cm->method()->print_value_on(&tempst);\n+  if (blob->is_nmethod()) {\n+    blob->as_nmethod()->method()->print_value_on(&tempst);\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -83,1 +83,1 @@\n-static bool is_decipherable_compiled_frame(JavaThread* thread, frame* fr, CompiledMethod* nm);\n+static bool is_decipherable_compiled_frame(JavaThread* thread, frame* fr, nmethod* nm);\n@@ -153,1 +153,1 @@\n-static bool is_decipherable_compiled_frame(JavaThread* thread, frame* fr, CompiledMethod* nm) {\n+static bool is_decipherable_compiled_frame(JavaThread* thread, frame* fr, nmethod* nm) {\n@@ -416,1 +416,1 @@\n-    if (candidate.cb()->is_compiled()) {\n+    if (candidate.cb()->is_nmethod()) {\n@@ -418,1 +418,1 @@\n-      CompiledMethod* nm = candidate.cb()->as_compiled_method();\n+      nmethod* nm = candidate.cb()->as_nmethod();\n","filename":"src\/hotspot\/share\/prims\/forte.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -771,3 +771,2 @@\n-                CompiledMethod* cm = CodeCache::find_compiled(f->pc());\n-                assert(cm != nullptr, \"sanity check\");\n-                cm->make_not_entrant();\n+                nmethod* nm = CodeCache::find_nmethod(f->pc());\n+                nm->make_not_entrant();\n@@ -842,1 +841,1 @@\n-  CompiledMethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n+  nmethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n@@ -941,1 +940,1 @@\n-  CompiledMethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n+  nmethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n@@ -1026,1 +1025,1 @@\n-  CompiledMethod* code = mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false);\n+  nmethod* code = mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false);\n@@ -1100,2 +1099,2 @@\n-    CompiledMethod* code = mh->code();\n-    if (code != nullptr && code->as_nmethod_or_null() != nullptr) {\n+    nmethod* code = mh->code();\n+    if (code != nullptr) {\n@@ -1559,1 +1558,1 @@\n-  CompiledMethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n+  nmethod* code = is_osr ? mh->lookup_osr_nmethod_for(InvocationEntryBci, CompLevel_none, false) : mh->code();\n@@ -1611,1 +1610,1 @@\n-      ::new (blob) BufferBlob(\"WB::DummyBlob\", full_size);\n+      ::new (blob) BufferBlob(\"WB::DummyBlob\", CodeBlobKind::Blob_Buffer, full_size);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":9,"deletions":10,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -108,1 +108,1 @@\n-  if (f.cb() == nullptr || !f.cb()->is_compiled()) {\n+  if (f.cb() == nullptr || !f.cb()->is_nmethod()) {\n@@ -111,1 +111,1 @@\n-  Method* m = f.cb()->as_compiled_method()->method();\n+  Method* m = f.cb()->as_nmethod()->method();\n","filename":"src\/hotspot\/share\/runtime\/continuation.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-CompiledMethod* ContinuationEntry::_enter_special = nullptr;\n+nmethod* ContinuationEntry::_enter_special = nullptr;\n@@ -43,1 +43,1 @@\n-void ContinuationEntry::set_enter_code(CompiledMethod* cm, int interpreted_entry_offset) {\n+void ContinuationEntry::set_enter_code(nmethod* nm, int interpreted_entry_offset) {\n@@ -45,1 +45,1 @@\n-  _return_pc = cm->code_begin() + _return_pc_offset;\n+  _return_pc = nm->code_begin() + _return_pc_offset;\n@@ -47,1 +47,1 @@\n-  _enter_special = cm;\n+  _enter_special = nm;\n@@ -144,1 +144,1 @@\n-    assert(cb->as_compiled_method()->method()->is_continuation_enter_intrinsic(), \"\");\n+    assert(cb->as_nmethod()->method()->is_continuation_enter_intrinsic(), \"\");\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,0 @@\n-class CompiledMethod;\n@@ -37,0 +36,1 @@\n+class nmethod;\n@@ -59,1 +59,1 @@\n-  static void set_enter_code(CompiledMethod* cm, int interpreted_entry_offset);\n+  static void set_enter_code(nmethod* nm, int interpreted_entry_offset);\n@@ -64,1 +64,1 @@\n-  static CompiledMethod* _enter_special;\n+  static nmethod* _enter_special;\n","filename":"src\/hotspot\/share\/runtime\/continuationEntry.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n+#include \"code\/nmethod.inline.hpp\"\n@@ -1073,2 +1073,2 @@\n-      assert(f.cb()->as_compiled_method()->is_deopt_pc(f.raw_pc()), \"\");\n-      assert(f.cb()->as_compiled_method()->is_deopt_pc(ContinuationHelper::Frame::real_pc(f)), \"\");\n+      assert(f.cb()->as_nmethod()->is_deopt_pc(f.raw_pc()), \"\");\n+      assert(f.cb()->as_nmethod()->is_deopt_pc(ContinuationHelper::Frame::real_pc(f)), \"\");\n@@ -1473,1 +1473,1 @@\n-  ResourceMark rm; \/\/ used for scope traversal in num_java_frames(CompiledMethod*, address)\n+  ResourceMark rm; \/\/ used for scope traversal in num_java_frames(nmethod*, address)\n@@ -2293,1 +2293,1 @@\n-              || (_cont.is_preempted() && f.cb()->as_compiled_method()->is_marked_for_deoptimization())) {\n+              || (_cont.is_preempted() && f.cb()->as_nmethod()->is_marked_for_deoptimization())) {\n@@ -2312,1 +2312,1 @@\n-    int stack_args_slots = f.cb()->as_compiled_method()->method()->num_stack_arg_slots(false \/* rounded *\/);\n+    int stack_args_slots = f.cb()->as_nmethod()->method()->num_stack_arg_slots(false \/* rounded *\/);\n@@ -2407,1 +2407,1 @@\n-  assert(!f.is_compiled_frame() || f.is_deoptimized_frame() == f.cb()->as_compiled_method()->is_deopt_pc(f.raw_pc()), \"\");\n+  assert(!f.is_compiled_frame() || f.is_deoptimized_frame() == f.cb()->as_nmethod()->is_deopt_pc(f.raw_pc()), \"\");\n@@ -2494,4 +2494,4 @@\n-    if (fst.current()->cb()->is_compiled()) {\n-      CompiledMethod* cm = fst.current()->cb()->as_compiled_method();\n-      if (!cm->method()->is_continuation_native_intrinsic()) {\n-        cm->make_deoptimized();\n+    if (fst.current()->cb()->is_nmethod()) {\n+      nmethod* nm = fst.current()->cb()->as_nmethod();\n+      if (!nm->method()->is_continuation_native_intrinsic()) {\n+        nm->make_deoptimized();\n@@ -2543,1 +2543,1 @@\n-    if (fst.current()->cb()->is_compiled() && fst.current()->cb()->as_compiled_method()->is_marked_for_deoptimization()) {\n+    if (fst.current()->cb()->is_nmethod() && fst.current()->cb()->as_nmethod()->is_marked_for_deoptimization()) {\n","filename":"src\/hotspot\/share\/runtime\/continuationFreezeThaw.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -60,1 +60,1 @@\n-  return f.is_interpreted_frame() ? f.interpreter_frame_method() : f.cb()->as_compiled_method()->method();\n+  return f.is_interpreted_frame() ? f.interpreter_frame_method() : f.cb()->as_nmethod()->method();\n@@ -82,2 +82,2 @@\n-  CompiledMethod* cm = sender.cb()->as_compiled_method();\n-  return cm->is_deopt_pc(pc);\n+  nmethod* nm = sender.cb()->as_nmethod();\n+  return nm->is_deopt_pc(pc);\n@@ -165,2 +165,2 @@\n-  CompiledMethod* cm = f.cb()->as_compiled_method();\n-  assert(!cm->is_compiled() || !cm->as_compiled_method()->is_native_method(), \"\"); \/\/ See compiledVFrame::compiledVFrame(...) in vframe_hp.cpp\n+  nmethod* nm = f.cb()->as_nmethod();\n+  assert(!nm->is_native_method(), \"\"); \/\/ See compiledVFrame::compiledVFrame(...) in vframe_hp.cpp\n@@ -168,1 +168,1 @@\n-  if (!cm->has_monitors()) {\n+  if (!nm->has_monitors()) {\n@@ -174,1 +174,1 @@\n-  for (ScopeDesc* scope = cm->scope_desc_at(f.pc()); scope != nullptr; scope = scope->sender()) {\n+  for (ScopeDesc* scope = nm->scope_desc_at(f.pc()); scope != nullptr; scope = scope->sender()) {\n@@ -189,1 +189,1 @@\n-        \/\/assert(cm->has_monitors(), \"\");\n+        \/\/assert(nm->has_monitors(), \"\");\n","filename":"src\/hotspot\/share\/runtime\/continuationHelper.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-void DeoptimizationScope::mark(CompiledMethod* cm, bool inc_recompile_counts) {\n+void DeoptimizationScope::mark(nmethod* nm, bool inc_recompile_counts) {\n@@ -124,2 +124,2 @@\n-  if (cm->is_marked_for_deoptimization()) {\n-    dependent(cm);\n+  if (nm->is_marked_for_deoptimization()) {\n+    dependent(nm);\n@@ -129,3 +129,3 @@\n-  CompiledMethod::DeoptimizationStatus status =\n-    inc_recompile_counts ? CompiledMethod::deoptimize : CompiledMethod::deoptimize_noupdate;\n-  Atomic::store(&cm->_deoptimization_status, status);\n+  nmethod::DeoptimizationStatus status =\n+    inc_recompile_counts ? nmethod::deoptimize : nmethod::deoptimize_noupdate;\n+  Atomic::store(&nm->_deoptimization_status, status);\n@@ -135,1 +135,1 @@\n-  assert(cm->_deoptimization_generation == 0, \"Is already marked\");\n+  assert(nm->_deoptimization_generation == 0, \"Is already marked\");\n@@ -137,1 +137,1 @@\n-  cm->_deoptimization_generation = DeoptimizationScope::_active_deopt_gen;\n+  nm->_deoptimization_generation = DeoptimizationScope::_active_deopt_gen;\n@@ -141,1 +141,1 @@\n-void DeoptimizationScope::dependent(CompiledMethod* cm) {\n+void DeoptimizationScope::dependent(nmethod* nm) {\n@@ -146,2 +146,2 @@\n-  if (_required_gen < cm->_deoptimization_generation) {\n-    _required_gen = cm->_deoptimization_generation;\n+  if (_required_gen < nm->_deoptimization_generation) {\n+    _required_gen = nm->_deoptimization_generation;\n@@ -324,1 +324,1 @@\n-static bool rematerialize_objects(JavaThread* thread, int exec_mode, CompiledMethod* compiled_method,\n+static bool rematerialize_objects(JavaThread* thread, int exec_mode, nmethod* compiled_method,\n@@ -442,1 +442,1 @@\n-  CompiledMethod* cm = deoptee.cb()->as_compiled_method_or_null();\n+  nmethod* nm = deoptee.cb()->as_nmethod_or_null();\n@@ -451,1 +451,1 @@\n-    realloc_failures = rematerialize_objects(thread, Unpack_none, cm, deoptee, map, chunk, deoptimized_objects);\n+    realloc_failures = rematerialize_objects(thread, Unpack_none, nm, deoptee, map, chunk, deoptimized_objects);\n@@ -495,2 +495,2 @@\n-  CompiledMethod* cm = deoptee.cb()->as_compiled_method_or_null();\n-  current->set_deopt_compiled_method(cm);\n+  nmethod* nm = deoptee.cb()->as_nmethod_or_null();\n+  current->set_deopt_compiled_method(nm);\n@@ -525,1 +525,1 @@\n-    realloc_failures = rematerialize_objects(current, exec_mode, cm, deoptee, map, chunk, unused);\n+    realloc_failures = rematerialize_objects(current, exec_mode, nm, deoptee, map, chunk, unused);\n@@ -1223,2 +1223,2 @@\n-      CompiledMethod* cm = fr->cb()->as_compiled_method_or_null();\n-      if (cm->is_compiled_by_jvmci() && sv->is_auto_box()) {\n+      nmethod* nm = fr->cb()->as_nmethod_or_null();\n+      if (nm->is_compiled_by_jvmci() && sv->is_auto_box()) {\n@@ -1750,2 +1750,2 @@\n-    CompiledMethod* cm = fr.cb()->as_compiled_method_or_null();\n-    assert(cm != nullptr, \"only compiled methods can deopt\");\n+    nmethod* nm = fr.cb()->as_nmethod_or_null();\n+    assert(nm != nullptr, \"only compiled methods can deopt\");\n@@ -1755,1 +1755,1 @@\n-    cm->log_identity(xtty);\n+    nm->log_identity(xtty);\n@@ -1757,1 +1757,1 @@\n-    for (ScopeDesc* sd = cm->scope_desc_at(fr.pc()); ; sd = sd->sender()) {\n+    for (ScopeDesc* sd = nm->scope_desc_at(fr.pc()); ; sd = sd->sender()) {\n@@ -1785,1 +1785,1 @@\n-address Deoptimization::deoptimize_for_missing_exception_handler(CompiledMethod* cm) {\n+address Deoptimization::deoptimize_for_missing_exception_handler(nmethod* nm) {\n@@ -1787,1 +1787,1 @@\n-  cm->make_not_entrant();\n+  nm->make_not_entrant();\n@@ -1800,1 +1800,1 @@\n-  assert(caller_frame.cb()->as_compiled_method_or_null() == cm, \"expect top frame compiled method\");\n+  assert(caller_frame.cb()->as_nmethod_or_null() == nm, \"expect top frame compiled method\");\n@@ -1818,1 +1818,1 @@\n-  MethodData* trap_mdo = get_method_data(thread, methodHandle(thread, cm->method()), true);\n+  MethodData* trap_mdo = get_method_data(thread, methodHandle(thread, nm->method()), true);\n@@ -1953,1 +1953,1 @@\n-static void post_deoptimization_event(CompiledMethod* nm,\n+static void post_deoptimization_event(nmethod* nm,\n@@ -1982,1 +1982,1 @@\n-static void log_deopt(CompiledMethod* nm, Method* tm, intptr_t pc, frame& fr, int trap_bci,\n+static void log_deopt(nmethod* nm, Method* tm, intptr_t pc, frame& fr, int trap_bci,\n@@ -2044,1 +2044,1 @@\n-    CompiledMethod* nm = cvf->code();\n+    nmethod* nm = cvf->code();\n@@ -2061,1 +2061,1 @@\n-      nm->as_nmethod()->update_speculation(current);\n+      nm->update_speculation(current);\n@@ -2181,2 +2181,2 @@\n-        if (nm->is_nmethod()) {\n-          const char* installed_code_name = nm->as_nmethod()->jvmci_name();\n+        if (nm->is_compiled_by_jvmci()) {\n+          const char* installed_code_name = nm->jvmci_name();\n@@ -2436,1 +2436,1 @@\n-          UseRTMDeopt && (nm->as_nmethod()->rtm_state() != ProfileRTM)) {\n+          UseRTMDeopt && (nm->rtm_state() != ProfileRTM)) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":34,"deletions":34,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -60,1 +60,1 @@\n-  void mark(CompiledMethod* cm, bool inc_recompile_counts = true);\n+  void mark(nmethod* nm, bool inc_recompile_counts = true);\n@@ -62,1 +62,1 @@\n-  void dependent(CompiledMethod* cm);\n+  void dependent(nmethod* nm);\n@@ -187,1 +187,1 @@\n-  static address deoptimize_for_missing_exception_handler(CompiledMethod* cm);\n+  static address deoptimize_for_missing_exception_handler(nmethod* nm);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -208,3 +208,4 @@\n-    CompiledMethod* cm = cb()->as_compiled_method_or_null();\n-    if (cm->is_method_handle_return(pc()))\n-      return cm->deopt_mh_handler_begin() - pc_return_offset;\n+    nmethod* nm = cb()->as_nmethod_or_null();\n+    assert(nm != nullptr, \"only nmethod is expected here\");\n+    if (nm->is_method_handle_return(pc()))\n+      return nm->deopt_mh_handler_begin() - pc_return_offset;\n@@ -212,1 +213,1 @@\n-      return cm->deopt_handler_begin() - pc_return_offset;\n+      return nm->deopt_handler_begin() - pc_return_offset;\n@@ -316,2 +317,2 @@\n-  assert(_cb != nullptr && _cb->is_compiled(), \"must be an nmethod\");\n-  CompiledMethod* nm = (CompiledMethod *)_cb;\n+  assert(_cb != nullptr && _cb->is_nmethod(), \"must be an nmethod\");\n+  nmethod* nm = _cb->as_nmethod();\n@@ -336,1 +337,1 @@\n-  CompiledMethod* nm = (CompiledMethod*)_cb;\n+  nmethod* nm = _cb->as_nmethod();\n@@ -349,1 +350,1 @@\n-  assert(_cb != nullptr && _cb->is_compiled(), \"must be\");\n+  assert(_cb != nullptr && _cb->is_nmethod(), \"must be\");\n@@ -352,4 +353,4 @@\n-  CompiledMethod* cm = (CompiledMethod*) _cb;\n-  address deopt = cm->is_method_handle_return(pc()) ?\n-                        cm->deopt_mh_handler_begin() :\n-                        cm->deopt_handler_begin();\n+  nmethod* nm = _cb->as_nmethod();\n+  address deopt = nm->is_method_handle_return(pc()) ?\n+                        nm->deopt_mh_handler_begin() :\n+                        nm->deopt_handler_begin();\n@@ -360,1 +361,1 @@\n-  cm->set_original_pc(this, pc());\n+  nm->set_original_pc(this, pc());\n@@ -677,3 +678,3 @@\n-    } else if (_cb->is_compiled()) {\n-      CompiledMethod* cm = (CompiledMethod*)_cb;\n-      Method* m = cm->method();\n+    } else if (_cb->is_nmethod()) {\n+      nmethod* nm = _cb->as_nmethod();\n+      Method* m = nm->method();\n@@ -681,5 +682,2 @@\n-        if (cm->is_nmethod()) {\n-          nmethod* nm = cm->as_nmethod();\n-          st->print(\"J %d%s\", nm->compile_id(), (nm->is_osr_method() ? \"%\" : \"\"));\n-          st->print(\" %s\", nm->compiler_name());\n-        }\n+        st->print(\"J %d%s\", nm->compile_id(), (nm->is_osr_method() ? \"%\" : \"\"));\n+        st->print(\" %s\", nm->compiler_name());\n@@ -700,6 +698,3 @@\n-        if (cm->is_nmethod()) {\n-          nmethod* nm = cm->as_nmethod();\n-          const char* jvmciName = nm->jvmci_name();\n-          if (jvmciName != nullptr) {\n-            st->print(\" (%s)\", jvmciName);\n-          }\n+        const char* jvmciName = nm->jvmci_name();\n+        if (jvmciName != nullptr) {\n+          st->print(\" (%s)\", jvmciName);\n@@ -1406,1 +1401,1 @@\n-  } else if (cb()->is_compiled()) {\n+  } else if (cb()->is_nmethod()) {\n@@ -1408,1 +1403,1 @@\n-    CompiledMethod* cm = cb()->as_compiled_method();\n+    nmethod* nm = cb()->as_nmethod();\n@@ -1411,2 +1406,2 @@\n-                                       p2i(cm),\n-                                       cm->method()->name_and_sig_as_C_string(),\n+                                       p2i(nm),\n+                                       nm->method()->name_and_sig_as_C_string(),\n@@ -1419,1 +1414,1 @@\n-      Method* m = cm->method();\n+      Method* m = nm->method();\n@@ -1421,1 +1416,1 @@\n-      int stack_slot_offset = cm->frame_size() * wordSize; \/\/ offset, in bytes, to caller sp\n+      int stack_slot_offset = nm->frame_size() * wordSize; \/\/ offset, in bytes, to caller sp\n@@ -1472,1 +1467,1 @@\n-      for (ScopeDesc* scope = cm->scope_desc_at(pc()); scope != nullptr; scope = scope->sender(), scope_no++) {\n+      for (ScopeDesc* scope = nm->scope_desc_at(pc()); scope != nullptr; scope = scope->sender(), scope_no++) {\n@@ -1510,1 +1505,1 @@\n-    if (cm->method()->is_continuation_enter_intrinsic()) {\n+    if (nm->method()->is_continuation_enter_intrinsic()) {\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":30,"deletions":35,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,0 @@\n-class CompiledMethod;\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n+#include \"code\/nmethod.inline.hpp\"\n@@ -67,2 +67,2 @@\n-      _cb->is_compiled() &&\n-      ((CompiledMethod*)_cb)->is_java_method()) {\n+      _cb->is_nmethod() &&\n+      _cb->as_nmethod()->is_java_method()) {\n","filename":"src\/hotspot\/share\/runtime\/frame.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-  CompiledMethod*       _deopt_nmethod;         \/\/ CompiledMethod that is currently being deoptimized\n+  nmethod*       _deopt_nmethod;                 \/\/ nmethod that is currently being deoptimized\n@@ -689,2 +689,2 @@\n-  void set_deopt_compiled_method(CompiledMethod* nm)  { _deopt_nmethod = nm; }\n-  CompiledMethod* deopt_compiled_method()        { return _deopt_nmethod; }\n+  void set_deopt_compiled_method(nmethod* nm)    { _deopt_nmethod = nm; }\n+  nmethod* deopt_compiled_method()               { return _deopt_nmethod; }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -847,2 +847,2 @@\n-  assert(cb != nullptr && cb->is_compiled(), \"return address should be in nmethod\");\n-  CompiledMethod* nm = (CompiledMethod*)cb;\n+  assert(cb != nullptr && cb->is_nmethod(), \"return address should be in nmethod\");\n+  nmethod* nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"code\/compiledMethod.inline.hpp\"\n+#include \"code\/nmethod.inline.hpp\"\n@@ -488,1 +488,1 @@\n-  CompiledMethod* nm = (blob != nullptr) ? blob->as_compiled_method_or_null() : nullptr;\n+  nmethod* nm = (blob != nullptr) ? blob->as_nmethod_or_null() : nullptr;\n@@ -561,1 +561,1 @@\n-  guarantee(cb != nullptr && cb->is_compiled(), \"safepoint polling: pc must refer to an nmethod\");\n+  guarantee(cb != nullptr && cb->is_nmethod(), \"safepoint polling: pc must refer to an nmethod\");\n@@ -564,1 +564,1 @@\n-  assert(((CompiledMethod*)cb)->is_at_poll_or_poll_return(pc),\n+  assert(cb->as_nmethod()->is_at_poll_or_poll_return(pc),\n@@ -575,2 +575,2 @@\n-  bool at_poll_return = ((CompiledMethod*)cb)->is_at_poll_return(pc);\n-  bool has_wide_vectors = ((CompiledMethod*)cb)->has_wide_vectors();\n+  bool at_poll_return = cb->as_nmethod()->is_at_poll_return(pc);\n+  bool has_wide_vectors = cb->as_nmethod()->has_wide_vectors();\n@@ -686,1 +686,1 @@\n-address SharedRuntime::compute_compiled_exc_handler(CompiledMethod* cm, address ret_pc, Handle& exception,\n+address SharedRuntime::compute_compiled_exc_handler(nmethod* nm, address ret_pc, Handle& exception,\n@@ -688,1 +688,1 @@\n-  assert(cm != nullptr, \"must exist\");\n+  assert(nm != nullptr, \"must exist\");\n@@ -692,1 +692,1 @@\n-  if (cm->is_compiled_by_jvmci()) {\n+  if (nm->is_compiled_by_jvmci()) {\n@@ -694,2 +694,2 @@\n-    int catch_pco = pointer_delta_as_int(ret_pc, cm->code_begin());\n-    ExceptionHandlerTable table(cm);\n+    int catch_pco = pointer_delta_as_int(ret_pc, nm->code_begin());\n+    ExceptionHandlerTable table(nm);\n@@ -698,1 +698,1 @@\n-      return cm->code_begin() + t->pco();\n+      return nm->code_begin() + t->pco();\n@@ -700,1 +700,1 @@\n-      return Deoptimization::deoptimize_for_missing_exception_handler(cm);\n+      return Deoptimization::deoptimize_for_missing_exception_handler(nm);\n@@ -705,1 +705,0 @@\n-  nmethod* nm = cm->as_nmethod();\n@@ -916,1 +915,1 @@\n-          if (!cb->is_compiled()) {\n+          if (!cb->is_nmethod()) {\n@@ -928,2 +927,2 @@\n-          CompiledMethod* cm = (CompiledMethod*)cb;\n-          if (cm->inlinecache_check_contains(pc)) {\n+          nmethod* nm = cb->as_nmethod();\n+          if (nm->inlinecache_check_contains(pc)) {\n@@ -938,1 +937,1 @@\n-          if (cm->method()->is_method_handle_intrinsic()) {\n+          if (nm->method()->is_method_handle_intrinsic()) {\n@@ -947,1 +946,1 @@\n-          target_pc = cm->continuation_for_implicit_null_exception(pc);\n+          target_pc = nm->continuation_for_implicit_null_exception(pc);\n@@ -958,2 +957,2 @@\n-        CompiledMethod* cm = CodeCache::find_compiled(pc);\n-        guarantee(cm != nullptr, \"must have containing compiled method for implicit division-by-zero exceptions\");\n+        nmethod* nm = CodeCache::find_nmethod(pc);\n+        guarantee(nm != nullptr, \"must have containing compiled method for implicit division-by-zero exceptions\");\n@@ -963,1 +962,1 @@\n-        target_pc = cm->continuation_for_implicit_div0_exception(pc);\n+        target_pc = nm->continuation_for_implicit_div0_exception(pc);\n@@ -1112,1 +1111,1 @@\n-  CompiledMethod* caller = vfst.nm();\n+  nmethod* caller = vfst.nm();\n@@ -1298,2 +1297,2 @@\n-  guarantee(caller_cb != nullptr && caller_cb->is_compiled(), \"must be called from compiled method\");\n-  CompiledMethod* caller_nm = caller_cb->as_compiled_method();\n+  guarantee(caller_cb != nullptr && caller_cb->is_nmethod(), \"must be called from compiled method\");\n+  nmethod* caller_nm = caller_cb->as_nmethod();\n@@ -1509,2 +1508,2 @@\n-      enter_special = caller.cb() != nullptr && caller.cb()->is_compiled()\n-        && caller.cb()->as_compiled_method()->method()->is_continuation_enter_intrinsic();\n+      enter_special = caller.cb() != nullptr && caller.cb()->is_nmethod()\n+        && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic();\n@@ -1606,1 +1605,1 @@\n-  CompiledMethod* caller_nm = cb->as_compiled_method();\n+  nmethod* caller_nm = cb->as_nmethod();\n@@ -1637,1 +1636,1 @@\n-      (caller.is_native_frame() && ((CompiledMethod*)caller.cb())->method()->is_continuation_enter_intrinsic())) {\n+      (caller.is_native_frame() && caller.cb()->as_nmethod()->method()->is_continuation_enter_intrinsic())) {\n@@ -1641,1 +1640,1 @@\n-    CompiledMethod* caller_nm = CodeCache::find_compiled(pc);\n+    nmethod* caller_nm = CodeCache::find_nmethod(pc);\n@@ -1769,1 +1768,1 @@\n-  CompiledMethod* callee = method->code();\n+  nmethod* callee = method->code();\n@@ -1778,1 +1777,1 @@\n-  if (cb == nullptr || !cb->is_compiled() || !callee->is_in_use() || callee->is_unloading()) {\n+  if (cb == nullptr || !cb->is_nmethod() || !callee->is_in_use() || callee->is_unloading()) {\n@@ -1783,1 +1782,1 @@\n-  CompiledMethod* caller = cb->as_compiled_method();\n+  nmethod* caller = cb->as_nmethod();\n@@ -3046,1 +3045,1 @@\n-  CompiledMethod* nm = nullptr;\n+  nmethod* nm = nullptr;\n@@ -3069,2 +3068,2 @@\n-      if (cb != nullptr && cb->is_compiled()) {\n-        nm = cb->as_compiled_method();\n+      if (cb != nullptr && cb->is_nmethod()) {\n+        nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":35,"deletions":36,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -184,1 +184,1 @@\n-  static address compute_compiled_exc_handler(CompiledMethod* nm, address ret_pc, Handle& exception,\n+  static address compute_compiled_exc_handler(nmethod* nm, address ret_pc, Handle& exception,\n@@ -331,1 +331,1 @@\n-  static bool handle_ic_miss_helper_internal(Handle receiver, CompiledMethod* caller_nm, const frame& caller_frame,\n+  static bool handle_ic_miss_helper_internal(Handle receiver, nmethod* caller_nm, const frame& caller_frame,\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,1 +1,2 @@\n-\/* Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+\/*\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -113,1 +114,1 @@\n-  return cb() != nullptr && _cb->is_compiled();\n+  return cb() != nullptr && _cb->is_nmethod();\n@@ -191,3 +192,3 @@\n-  assert(cb()->is_compiled(), \"\");\n-  assert(cb()->as_compiled_method()->method() != nullptr, \"\");\n-  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+  assert(cb()->is_nmethod(), \"\");\n+  assert(cb()->as_nmethod()->method() != nullptr, \"\");\n+  return (cb()->as_nmethod()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n@@ -268,1 +269,1 @@\n-  assert(!is_compiled() || !cb()->as_compiled_method()->is_deopt_pc(pc), \"\");\n+  assert(!is_compiled() || !cb()->as_nmethod()->is_deopt_pc(pc), \"\");\n@@ -320,3 +321,3 @@\n-  CompiledMethod* cm = cb()->as_compiled_method();\n-  if (cm->is_deopt_pc(pc1)) {\n-    pc1 = *(address*)((address)unextended_sp() + cm->orig_pc_offset());\n+  nmethod* nm = cb()->as_nmethod();\n+  if (nm->is_deopt_pc(pc1)) {\n+    pc1 = *(address*)((address)unextended_sp() + nm->orig_pc_offset());\n@@ -326,1 +327,1 @@\n-  assert(!cm->is_deopt_pc(pc1), \"\");\n+  assert(!nm->is_deopt_pc(pc1), \"\");\n@@ -347,1 +348,1 @@\n-    if (cb()->as_compiled_method()->is_deopt_pc(pc1)) {\n+    if (cb()->as_nmethod()->is_deopt_pc(pc1)) {\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.inline.hpp","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -74,2 +74,2 @@\n-    if (cb->is_compiled()) {\n-      CompiledMethod* nm = (CompiledMethod*)cb;\n+    if (cb->is_nmethod()) {\n+      nmethod* nm = cb->as_nmethod();\n","filename":"src\/hotspot\/share\/runtime\/vframe.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -295,4 +295,4 @@\n-  CodeBlob*         cb()         const { return _frame.cb();  }\n-  CompiledMethod*   nm()         const {\n-      assert( cb() != nullptr && cb()->is_compiled(), \"usage\");\n-      return (CompiledMethod*) cb();\n+  CodeBlob* cb() const { return _frame.cb();  }\n+  nmethod*  nm() const {\n+    assert(cb() != nullptr, \"usage\");\n+    return cb()->as_nmethod();\n","filename":"src\/hotspot\/share\/runtime\/vframe.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-  if (cb() != nullptr && cb()->is_compiled()) {\n+  if (cb() != nullptr && cb()->is_nmethod()) {\n","filename":"src\/hotspot\/share\/runtime\/vframe.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -245,1 +245,1 @@\n-    CompiledMethod* nm = code();\n+    nmethod* nm = code();\n@@ -302,1 +302,1 @@\n-compiledVFrame::compiledVFrame(const frame* fr, const RegisterMap* reg_map, JavaThread* thread, CompiledMethod* nm)\n+compiledVFrame::compiledVFrame(const frame* fr, const RegisterMap* reg_map, JavaThread* thread, nmethod* nm)\n@@ -308,1 +308,1 @@\n-  if (!nm->is_compiled() || !nm->as_compiled_method()->is_native_method()) {\n+  if (!nm->is_native_method()) {\n@@ -336,2 +336,2 @@\n-CompiledMethod* compiledVFrame::code() const {\n-  return CodeCache::find_compiled(_fr.pc());\n+nmethod* compiledVFrame::code() const {\n+  return CodeCache::find_nmethod(_fr.pc());\n@@ -344,1 +344,1 @@\n-    nmethod* nm = code()->as_nmethod();\n+    nmethod* nm = code();\n@@ -361,1 +361,1 @@\n-    nmethod* nm = code()->as_nmethod();\n+    nmethod* nm = code();\n@@ -371,1 +371,1 @@\n-    nmethod* nm = code()->as_nmethod();\n+    nmethod* nm = code();\n@@ -381,1 +381,1 @@\n-    assert(code()->as_nmethod()->is_native_method(), \"must be native\");\n+    assert(code()->is_native_method(), \"must be native\");\n@@ -390,1 +390,1 @@\n-    assert(code()->as_nmethod()->is_native_method(), \"must be native\");\n+    assert(code()->is_native_method(), \"must be native\");\n@@ -400,1 +400,1 @@\n-    nmethod* nm = code()->as_nmethod();\n+    nmethod* nm = code();\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -68,1 +68,1 @@\n-  compiledVFrame(const frame* fr, const RegisterMap* reg_map, JavaThread* thread, CompiledMethod* nm);\n+  compiledVFrame(const frame* fr, const RegisterMap* reg_map, JavaThread* thread, nmethod* nm);\n@@ -80,1 +80,1 @@\n-  CompiledMethod*  code() const;\n+  nmethod*  code() const;\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -310,1 +310,1 @@\n-  volatile_nonstatic_field(Method,             _code,                                         CompiledMethod*)                       \\\n+  volatile_nonstatic_field(Method,             _code,                                         nmethod*)                              \\\n@@ -552,11 +552,11 @@\n-  nonstatic_field(CodeBlob,                 _name,                                   const char*)                                    \\\n-  nonstatic_field(CodeBlob,                 _size,                                   int)                                            \\\n-  nonstatic_field(CodeBlob,                 _header_size,                            int)                                            \\\n-  nonstatic_field(CodeBlob,                 _frame_complete_offset,                  int)                                            \\\n-  nonstatic_field(CodeBlob,                 _data_offset,                            int)                                            \\\n-  nonstatic_field(CodeBlob,                 _frame_size,                             int)                                            \\\n-  nonstatic_field(CodeBlob,                 _oop_maps,                               ImmutableOopMapSet*)                            \\\n-  nonstatic_field(CodeBlob,                 _code_begin,                             address)                                        \\\n-  nonstatic_field(CodeBlob,                 _code_end,                               address)                                        \\\n-  nonstatic_field(CodeBlob,                 _content_begin,                          address)                                        \\\n-  nonstatic_field(CodeBlob,                 _data_end,                               address)                                        \\\n+  nonstatic_field(CodeBlob,                    _name,                                         const char*)                           \\\n+  nonstatic_field(CodeBlob,                    _size,                                         int)                                   \\\n+  nonstatic_field(CodeBlob,                    _header_size,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _relocation_size,                              int)                                   \\\n+  nonstatic_field(CodeBlob,                    _content_offset,                               int)                                   \\\n+  nonstatic_field(CodeBlob,                    _code_offset,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_complete_offset,                        int)                                   \\\n+  nonstatic_field(CodeBlob,                    _data_offset,                                  int)                                   \\\n+  nonstatic_field(CodeBlob,                    _frame_size,                                   int)                                   \\\n+  nonstatic_field(CodeBlob,                    _oop_maps,                                     ImmutableOopMapSet*)                   \\\n+  nonstatic_field(CodeBlob,                    _caller_must_gc_arguments,                     bool)                                  \\\n@@ -566,12 +566,0 @@\n-  nonstatic_field(RuntimeStub,                 _caller_must_gc_arguments,                     bool)                                  \\\n-                                                                                                                                     \\\n-  \/********************************************************\/                                                                         \\\n-  \/* CompiledMethod (NOTE: incomplete, but only a little) *\/                                                                         \\\n-  \/********************************************************\/                                                                         \\\n-                                                                                                                                     \\\n-  nonstatic_field(CompiledMethod,                     _method,                                       Method*)                        \\\n-  volatile_nonstatic_field(CompiledMethod,            _exception_cache,                              ExceptionCache*)                \\\n-  nonstatic_field(CompiledMethod,                     _scopes_data_begin,                            address)                        \\\n-  nonstatic_field(CompiledMethod,                     _deopt_handler_begin,                          address)                        \\\n-  nonstatic_field(CompiledMethod,                     _deopt_mh_handler_begin,                       address)                        \\\n-                                                                                                                                     \\\n@@ -582,0 +570,1 @@\n+  nonstatic_field(nmethod,                     _method,                                       Method*)                               \\\n@@ -586,0 +575,2 @@\n+  nonstatic_field(nmethod,                     _deopt_handler_offset,                         int)                                   \\\n+  nonstatic_field(nmethod,                     _deopt_mh_handler_offset,                      int)                                   \\\n@@ -591,0 +582,1 @@\n+  nonstatic_field(nmethod,                     _scopes_data_offset,                           int)                                   \\\n@@ -601,0 +593,1 @@\n+  volatile_nonstatic_field(nmethod,            _exception_cache,                              ExceptionCache*)                       \\\n@@ -1314,2 +1307,1 @@\n-  declare_type(CompiledMethod,           CodeBlob)                        \\\n-  declare_type(nmethod,                  CompiledMethod)                  \\\n+  declare_type(nmethod,                  CodeBlob)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":18,"deletions":26,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,4 +45,3 @@\n-  private static AddressField  contentBeginField;\n-  private static AddressField  codeBeginField;\n-  private static AddressField  codeEndField;\n-  private static AddressField  dataEndField;\n+  private static CIntegerField relocationSizeField;\n+  private static CIntegerField contentOffsetField;\n+  private static CIntegerField codeOffsetField;\n@@ -66,0 +65,3 @@\n+    relocationSizeField      = type.getCIntegerField(\"_relocation_size\");\n+    contentOffsetField       = type.getCIntegerField(\"_content_offset\");\n+    codeOffsetField          = type.getCIntegerField(\"_code_offset\");\n@@ -67,4 +69,0 @@\n-    contentBeginField        = type.getAddressField(\"_content_begin\");\n-    codeBeginField           = type.getAddressField(\"_code_begin\");\n-    codeEndField             = type.getAddressField(\"_code_end\");\n-    dataEndField             = type.getAddressField(\"_data_end\");\n@@ -89,1 +87,1 @@\n-  public Address headerBegin() { return getAddress(); }\n+  public Address headerBegin()    { return getAddress(); }\n@@ -91,1 +89,1 @@\n-  public Address headerEnd() { return getAddress().addOffsetTo(getHeaderSize()); }\n+  public Address headerEnd()      { return getAddress().addOffsetTo(getHeaderSize()); }\n@@ -93,1 +91,1 @@\n-  public Address contentBegin() { return contentBeginField.getValue(addr); }\n+  public Address contentBegin()   { return headerBegin().addOffsetTo(getContentOffset()); }\n@@ -95,1 +93,1 @@\n-  public Address contentEnd() { return headerBegin().addOffsetTo(getDataOffset()); }\n+  public Address contentEnd()     { return headerBegin().addOffsetTo(getDataOffset()); }\n@@ -97,1 +95,1 @@\n-  public Address codeBegin() { return codeBeginField.getValue(addr); }\n+  public Address codeBegin()      { return headerBegin().addOffsetTo(getCodeOffset()); }\n@@ -99,1 +97,1 @@\n-  public Address codeEnd() { return codeEndField.getValue(addr); }\n+  public Address codeEnd()        { return headerBegin().addOffsetTo(getDataOffset()); }\n@@ -101,1 +99,1 @@\n-  public Address dataBegin() { return headerBegin().addOffsetTo(getDataOffset()); }\n+  public Address dataBegin()      { return headerBegin().addOffsetTo(getDataOffset()); }\n@@ -103,1 +101,6 @@\n-  public Address dataEnd() { return dataEndField.getValue(addr); }\n+  public Address dataEnd()        { return headerBegin().addOffsetTo(getSize()); }\n+\n+  \/\/ Offsets\n+  public int getContentOffset()   { return (int) contentOffsetField.getValue(addr); }\n+\n+  public int getCodeOffset()      { return (int) codeOffsetField   .getValue(addr); }\n@@ -107,1 +110,1 @@\n-  public int getDataOffset()       { return (int) dataOffsetField.getValue(addr); }\n+  public int getDataOffset()      { return (int) dataOffsetField.getValue(addr); }\n@@ -110,1 +113,1 @@\n-  public int getSize()             { return (int) sizeField.getValue(addr); }\n+  public int getSize()            { return (int) sizeField.getValue(addr); }\n@@ -112,1 +115,1 @@\n-  public int getHeaderSize()       { return (int) headerSizeField.getValue(addr); }\n+  public int getHeaderSize()      { return (int) headerSizeField.getValue(addr); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/code\/CodeBlob.java","additions":23,"deletions":20,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -1,76 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-package sun.jvm.hotspot.code;\n-\n-import java.util.*;\n-\n-import sun.jvm.hotspot.debugger.Address;\n-import sun.jvm.hotspot.oops.*;\n-import sun.jvm.hotspot.runtime.*;\n-import sun.jvm.hotspot.types.*;\n-import sun.jvm.hotspot.utilities.*;\n-import sun.jvm.hotspot.utilities.Observable;\n-import sun.jvm.hotspot.utilities.Observer;\n-\n-public abstract class CompiledMethod extends CodeBlob {\n-  private static AddressField  methodField;\n-  private static AddressField  deoptHandlerBeginField;\n-  private static AddressField  deoptMhHandlerBeginField;\n-  private static AddressField  scopesDataBeginField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"CompiledMethod\");\n-\n-    methodField                 = type.getAddressField(\"_method\");\n-    deoptHandlerBeginField      = type.getAddressField(\"_deopt_handler_begin\");\n-    deoptMhHandlerBeginField    = type.getAddressField(\"_deopt_mh_handler_begin\");\n-    scopesDataBeginField        = type.getAddressField(\"_scopes_data_begin\");\n-  }\n-\n-  public CompiledMethod(Address addr) {\n-    super(addr);\n-  }\n-\n-  public Method getMethod() {\n-    return (Method)Metadata.instantiateWrapperFor(methodField.getValue(addr));\n-  }\n-\n-  public Address deoptHandlerBegin()    { return deoptHandlerBeginField.getValue(addr);              }\n-  public Address deoptMhHandlerBegin()  { return deoptMhHandlerBeginField.getValue(addr);            }\n-  public Address scopesDataBegin()      { return scopesDataBeginField.getValue(addr);                }\n-\n-  public static int getMethodOffset()                { return (int) methodField.getOffset();                }\n-\n-  @Override\n-  public boolean isCompiled() {\n-    return true;\n-  }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/code\/CompiledMethod.java","additions":0,"deletions":76,"binary":false,"changes":76,"status":"deleted"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+import sun.jvm.hotspot.memory.*;\n@@ -37,1 +38,1 @@\n-public class NMethod extends CompiledMethod {\n+public class NMethod extends CodeBlob {\n@@ -39,0 +40,1 @@\n+  private static AddressField  methodField;\n@@ -46,0 +48,2 @@\n+  private static CIntegerField deoptHandlerOffsetField;\n+  private static CIntegerField deoptMhHandlerOffsetField;\n@@ -50,0 +54,1 @@\n+  private static CIntegerField scopesDataOffsetField;\n@@ -79,0 +84,1 @@\n+    methodField                 = type.getAddressField(\"_method\");\n@@ -83,0 +89,2 @@\n+    deoptHandlerOffsetField     = type.getCIntegerField(\"_deopt_handler_offset\");\n+    deoptMhHandlerOffsetField   = type.getCIntegerField(\"_deopt_mh_handler_offset\");\n@@ -87,0 +95,1 @@\n+    scopesDataOffsetField       = type.getCIntegerField(\"_scopes_data_offset\");\n@@ -108,0 +117,4 @@\n+  public Method getMethod() {\n+    return (Method)Metadata.instantiateWrapperFor(methodField.getValue(addr));\n+  }\n+\n@@ -120,0 +133,2 @@\n+  public Address deoptHandlerBegin()    { return headerBegin().addOffsetTo(getDeoptHandlerOffset());   }\n+  public Address deoptMhHandlerBegin()  { return headerBegin().addOffsetTo(getDeoptMhHandlerOffset()); }\n@@ -125,1 +140,2 @@\n-  public Address metadataEnd()          { return scopesDataBegin();                                  }\n+  public Address metadataEnd()          { return headerBegin().addOffsetTo(getScopesDataOffset());   }\n+  public Address scopesDataBegin()      { return headerBegin().addOffsetTo(getScopesDataOffset());   }\n@@ -423,0 +439,1 @@\n+  public static int getMethodOffset()                { return (int) methodField.getOffset();                }\n@@ -500,0 +517,2 @@\n+  private int getDeoptHandlerOffset()   { return (int) deoptHandlerOffsetField  .getValue(addr); }\n+  private int getDeoptMhHandlerOffset() { return (int) deoptMhHandlerOffsetField.getValue(addr); }\n@@ -503,0 +522,1 @@\n+  private int getScopesDataOffset()   { return (int) scopesDataOffsetField  .getValue(addr); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/code\/NMethod.java","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -143,1 +143,1 @@\n-                                  out.print(((CompiledMethod)cb).getMethod().externalNameAndSignature());\n+                                  out.print(((NMethod)cb).getMethod().externalNameAndSignature());\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/PStack.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,1 +152,1 @@\n-    final int methodCodeOffset = getFieldOffset(\"Method::_code\", Integer.class, \"CompiledMethod*\");\n+    final int methodCodeOffset = getFieldOffset(\"Method::_code\", Integer.class, \"nmethod*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}