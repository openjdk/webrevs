{"files":[{"patch":"@@ -1399,4 +1399,4 @@\n-void C2_MacroAssembler::vgather8b_masked(BasicType elem_bt, XMMRegister dst,\n-                                         Register base, Register idx_base,\n-                                         Register mask, Register mask_idx,\n-                                         Register rtmp, int vlen_enc) {\n+void C2_MacroAssembler::vgather_le8b_masked(BasicType elem_bt, XMMRegister dst,\n+                                            Register base, Register idx_base,\n+                                            Register mask, Register mask_idx,\n+                                            Register rtmp, int vector_len, int vlen_enc) {\n@@ -1404,7 +1404,7 @@\n-  if (elem_bt == T_SHORT) {\n-    for (int i = 0; i < 4; i++) {\n-      \/\/ dst[i] = mask[i] ? src[idx_base[i]] : 0\n-      Label skip_load;\n-      btq(mask, mask_idx);\n-      jccb(Assembler::carryClear, skip_load);\n-      movl(rtmp, Address(idx_base, i * 4));\n+  for (int i = 0; i < vector_len; i++) {\n+    \/\/ dst[i] = mask[i] ? src[idx_base[i]] : 0\n+    Label skip_load;\n+    btq(mask, mask_idx);\n+    jccb(Assembler::carryClear, skip_load);\n+    movl(rtmp, Address(idx_base, i * 4));\n+    if (elem_bt == T_SHORT) {\n@@ -1412,11 +1412,2 @@\n-      bind(skip_load);\n-      incq(mask_idx);\n-    }\n-  } else {\n-    assert(elem_bt == T_BYTE, \"\");\n-    for (int i = 0; i < 8; i++) {\n-      \/\/ dst[i] = mask[i] ? src[idx_base[i]] : 0\n-      Label skip_load;\n-      btq(mask, mask_idx);\n-      jccb(Assembler::carryClear, skip_load);\n-      movl(rtmp, Address(idx_base, i * 4));\n+    } else {\n+      assert(elem_bt == T_BYTE, \"\");\n@@ -1424,2 +1415,0 @@\n-      bind(skip_load);\n-      incq(mask_idx);\n@@ -1427,0 +1416,2 @@\n+    bind(skip_load);\n+    incq(mask_idx);\n@@ -1430,3 +1421,3 @@\n-void C2_MacroAssembler::vgather8b(BasicType elem_bt, XMMRegister dst,\n-                                  Register base, Register idx_base,\n-                                  Register rtmp, int vlen_enc) {\n+void C2_MacroAssembler::vgather_le8b(BasicType elem_bt, XMMRegister dst,\n+                                     Register base, Register idx_base,\n+                                     Register rtmp, int vector_len, int vlen_enc) {\n@@ -1434,4 +1425,4 @@\n-  if (elem_bt == T_SHORT) {\n-    for (int i = 0; i < 4; i++) {\n-      \/\/ dst[i] = src[idx_base[i]]\n-      movl(rtmp, Address(idx_base, i * 4));\n+  for (int i = 0; i < vector_len; i++) {\n+    \/\/ dst[i] = src[idx_base[i]]\n+    movl(rtmp, Address(idx_base, i * 4));\n+    if (elem_bt == T_SHORT) {\n@@ -1439,6 +1430,2 @@\n-    }\n-  } else {\n-    assert(elem_bt == T_BYTE, \"\");\n-    for (int i = 0; i < 8; i++) {\n-      \/\/ dst[i] = src[idx_base[i]]\n-      movl(rtmp, Address(idx_base, i * 4));\n+    } else {\n+      assert(elem_bt == T_BYTE, \"\");\n@@ -1487,0 +1474,2 @@\n+  int gather8b_vlen = 8 \/ type2aelembytes(elem_ty);\n+\n@@ -1490,1 +1479,1 @@\n-      vgather8b(elem_ty, temp_dst, base, idx_base, rtmp, vlen_enc);\n+      vgather_le8b(elem_ty, temp_dst, base, idx_base, rtmp, gather8b_vlen, vlen_enc);\n@@ -1492,1 +1481,1 @@\n-      vgather8b_masked(elem_ty, temp_dst, base, idx_base, mask, mask_idx, rtmp, vlen_enc);\n+      vgather_le8b_masked(elem_ty, temp_dst, base, idx_base, mask, mask_idx, rtmp, gather8b_vlen, vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":28,"deletions":39,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -504,4 +504,4 @@\n-  void vgather8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                        Register mask, Register midx, Register rtmp, int vlen_enc);\n-  void vgather8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n-                 Register rtmp, int vlen_enc);\n+  void vgather_le8b_masked(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                           Register mask, Register midx, Register rtmp, int vector_len, int vlen_enc);\n+  void vgather_le8b(BasicType elem_bt, XMMRegister dst, Register base, Register idx_base,\n+                    Register rtmp, int vector_len, int vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3436,1 +3436,0 @@\n-          (size_in_bits < 64)                                      ||\n@@ -3453,3 +3452,0 @@\n-      if (is_subword_type(bt) && size_in_bits < 64) {\n-        return false;\n-      }\n@@ -17990,0 +17986,13 @@\n+instruct reinterpret_mask_same_esize(kReg dst) %{\n+  predicate(n->bottom_type()->isa_vectmask() &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) ==\n+            type2aelembytes(Matcher::vector_element_basic_type(n->in(1))));\n+  match(Set dst (VectorReinterpret dst));\n+  ins_cost(125);\n+  format %{ \"reinterpret_mask_same_esize $dst\\t!\" %}\n+  ins_encode %{\n+    \/\/ empty\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -18369,0 +18378,1 @@\n+    int vector_len = Matcher::vector_length(this);\n@@ -18371,1 +18381,1 @@\n-    __ vgather8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp$$Register, vlen_enc);\n+    __ vgather_le8b(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp$$Register, vector_len, vlen_enc);\n@@ -18401,0 +18411,1 @@\n+    int vector_len = Matcher::vector_length(this);\n@@ -18405,1 +18416,1 @@\n-    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+    __ vgather_le8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vector_len, vlen_enc);\n@@ -18437,0 +18448,1 @@\n+    int vector_len = Matcher::vector_length(this);\n@@ -18445,1 +18457,1 @@\n-    __ vgather8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vlen_enc);\n+    __ vgather_le8b_masked(elem_bt, $dst$$XMMRegister, $tmp$$Register, $idx_base$$Register, $rtmp2$$Register, $mask_idx$$Register, $rtmp$$Register, vector_len, vlen_enc);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1276,3 +1276,0 @@\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n-                                     \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                           \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1195,2 +1195,2 @@\n-\/\/                 W indexVector1, W indexVector2, W indexVector3, W indexVector4,\n-\/\/                 M m, C container, int index, int[] indexMap, int indexM, S s,\n+\/\/                 W indexVector, M m, C container,\n+\/\/                 int index, int[] indexMap, int indexM, S s,\n@@ -1247,0 +1247,7 @@\n+  int gather_scatter_num_elem = num_elem;\n+  \/\/ Adjust the vector length to length of the index for subword types.\n+  if (is_subword_type(elem_bt) && idx_num_elem != num_elem) {\n+    assert(!is_scatter, \"Only supports gather operation for subword types now\");\n+    assert(idx_num_elem < num_elem, \"must be\");\n+    gather_scatter_num_elem = idx_num_elem;\n+  }\n@@ -1248,1 +1255,1 @@\n-  Node* m = is_scatter ? argument(11) : argument(13);\n+  Node* m = is_scatter ? argument(11) : argument(10);\n@@ -1269,1 +1276,1 @@\n-    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, num_elem, elem_bt, mask)) {\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatterMasked : Op_LoadVectorGatherMasked, gather_scatter_num_elem, elem_bt, mask)) {\n@@ -1272,1 +1279,1 @@\n-                      num_elem, type2name(elem_bt));\n+                      gather_scatter_num_elem, type2name(elem_bt));\n@@ -1277,1 +1284,1 @@\n-    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, num_elem, elem_bt, VecMaskNotUsed)) {\n+    if (!arch_supports_vector(is_scatter ? Op_StoreVectorScatter : Op_LoadVectorGather, gather_scatter_num_elem, elem_bt, VecMaskNotUsed)) {\n@@ -1280,1 +1287,1 @@\n-                      num_elem, type2name(elem_bt));\n+                      gather_scatter_num_elem, type2name(elem_bt));\n@@ -1304,1 +1311,0 @@\n-    assert(!is_scatter, \"Only supports gather operation for subword types now\");\n@@ -1308,1 +1314,1 @@\n-    Node* index = argument(15);\n+    Node* index = argument(12);\n@@ -1334,2 +1340,3 @@\n-    Node* indexMap = argument(16);\n-    Node* indexM   = argument(17);\n+    \/\/ Get the index map address for subword types.\n+    Node* indexMap = argument(13);\n+    Node* indexM   = argument(14);\n@@ -1338,1 +1345,1 @@\n-    \/\/ Get the first index vector.\n+    \/\/ Get the index vector.\n@@ -1357,1 +1364,0 @@\n-  const TypeVect* vector_type = TypeVect::make(elem_bt, num_elem);\n@@ -1359,1 +1365,1 @@\n-    Node* val = unbox_vector(argument(10), vbox_type, elem_bt, num_elem);\n+    Node* val = unbox_vector(argument(10), vbox_type, elem_bt, gather_scatter_num_elem);\n@@ -1373,0 +1379,2 @@\n+    const TypeVect* vector_type = TypeVect::make(elem_bt, num_elem);\n+    const TypeVect* load_vector_type =  TypeVect::make(elem_bt, gather_scatter_num_elem);\n@@ -1374,0 +1382,1 @@\n+\n@@ -1375,1 +1384,6 @@\n-      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, vector_type, indexes, mask));\n+      \/\/ Resize the mask to the target vector type.\n+      if (gather_scatter_num_elem != num_elem) {\n+        const TypeVect* resize_type = TypeVect::makemask(elem_bt, gather_scatter_num_elem);\n+        mask = gvn().transform(new VectorReinterpretNode(mask, mask->bottom_type()->is_vect(), resize_type));\n+      }\n+      vload = gvn().transform(new LoadVectorGatherMaskedNode(control(), memory(addr), addr, addr_type, load_vector_type, indexes, mask));\n@@ -1377,1 +1391,6 @@\n-      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, vector_type, indexes));\n+      vload = gvn().transform(new LoadVectorGatherNode(control(), memory(addr), addr, addr_type, load_vector_type, indexes));\n+    }\n+\n+    \/\/ Resize the load vector to the target vector length.\n+    if (gather_scatter_num_elem != num_elem) {\n+      vload = gvn().transform(new VectorReinterpretNode(vload, vload->bottom_type()->is_vect(), vector_type));\n","filename":"src\/hotspot\/share\/opto\/vectorIntrinsics.cpp","additions":35,"deletions":16,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -480,1 +480,1 @@\n-        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s, M m);\n+        V loadWithMap(C container, int index, int[] indexMap, int indexM, S s, M m, int elem_num);\n@@ -495,2 +495,2 @@\n-                  W indexVector1, W indexVector2, W indexVector3, W indexVector4,\n-                  M m, C container, int index, int[] indexMap, int indexM, S s,\n+                  W indexVector, M m,\n+                  C container, int index, int[] indexMap, int indexM, S s,\n@@ -499,1 +499,1 @@\n-        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s, m);\n+        return defaultImpl.loadWithMap(container, index, indexMap, indexM, s, m, indexLength);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/vm\/vector\/VectorSupport.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3073,0 +3073,17 @@\n+    @ForceInline\n+    private static\n+    ByteVector loadWithMap(VectorSpecies<Byte> species,\n+                            VectorSpecies<Integer> lsp,\n+                            IntVector vix, byte[] a, int offset,\n+                            int[] indexMap, int mapOffset) {\n+        ByteSpecies vsp = (ByteSpecies) species;\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, byte.class, vsp.laneCount(),\n+            lsp.vectorType(), lsp.length(),\n+            a, ARRAY_BASE, vix, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm, num) ->\n+            s.vOp(n -> (n < num) ? c[idx + iMap[idy + n]] : 0));\n+    }\n+\n@@ -3113,2 +3130,0 @@\n-        Class<? extends ByteVector> vectorType = vsp.vectorType();\n-\n@@ -3124,0 +3139,17 @@\n+        int vlen = vsp.length();\n+        int idx_vlen = lsp.length();\n+        if (vlen > idx_vlen * 4) {\n+            \/\/ Fall back to scalar version when the vector is too large relative to the\n+            \/\/ index vector. This occurs when the vector size exceeds the max vector size\n+            \/\/ supported by the hardware. For example, when loading a Byte256Vector on\n+            \/\/ hardware with MaxVectorSize of 16 bytes (128 bits), the index species \"lsp\"\n+            \/\/ would be IntVector.SPECIES_128 with 4 int elements, while the byte vector\n+            \/\/ has 32 elements (256\/8). Since 32 > 4*4, the condition matches. We disable\n+            \/\/ intrinsification in such cases because the compiler will eventually fall back\n+            \/\/ to the Java implementation anyway since the vector size exceeds the hardware\n+            \/\/ limit. While calling the intrinsic would also work, it would negatively impact\n+            \/\/ performance because the implementation would need to split the operation into\n+            \/\/ multiple smaller sub-operations.\n+            return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        }\n+\n@@ -3128,2 +3160,0 @@\n-        int vlen = vsp.length();\n-        int idx_vlen = lsp.length();\n@@ -3145,7 +3175,20 @@\n-        return VectorSupport.loadWithMap(\n-            vectorType, null, byte.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, vix2, vix3, null,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(n -> c[idx + iMap[idy+n]]));\n+        \/\/ The first time of gather-load.\n+        ByteVector vec = loadWithMap(vsp, lsp, vix0, a, offset, indexMap, mapOffset);\n+\n+        \/\/ The second time of gather-load.\n+        if (vix1 != null) {\n+            ByteVector vec1 = loadWithMap(vsp, lsp, vix1, a, offset, indexMap, mapOffset + idx_vlen);\n+            \/\/ Merge with previous gather-load result: vec = [vec, vec1]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec1));\n+        }\n+\n+        \/\/ The third and fourth time of gather-load.\n+        if (vix2 != null && vix3 != null) {\n+            ByteVector vec2 = loadWithMap(vsp, lsp, vix2, a, offset, indexMap, mapOffset + idx_vlen * 2);\n+            ByteVector vec3 = loadWithMap(vsp, lsp, vix3, a, offset, indexMap, mapOffset + idx_vlen * 3);\n+            \/\/ Merge the third and fourth gather-load results: vec2 = [vec2, vec3]\n+            vec2 = vec2.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec3));\n+            \/\/ Merge with previous gather-load results: vec = [vec, vec2]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - 2 * idx_vlen, vec2));\n+        }\n+        return vec;\n@@ -3872,0 +3915,28 @@\n+    @ForceInline\n+    private static\n+    <M extends VectorMask<Byte>>\n+    ByteVector loadWithMap(Class<M> maskClass,\n+                            VectorSpecies<Byte> species,\n+                            VectorSpecies<Integer> lsp,\n+                            IntVector vix, byte[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        ByteSpecies vsp = (ByteSpecies) species;\n+        Class<? extends ByteVector> vectorType = vsp.vectorType();\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, byte.class, vsp.laneCount(),\n+            lsp.vectorType(), lsp.length(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm, num) ->\n+            s.vOp(vm, n -> (n < num) ? c[idx + iMap[idy + n]] : 0));\n+    }\n+\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    private static\n+    <M extends VectorMask<Byte>>\n+    M slideDownVectorMask(M m, int origin) {\n+        Vector<Byte> vector = m.toVector();\n+        return (M) vector.slice(origin).compare(NE, 0);\n+    }\n+\n@@ -3887,2 +3958,0 @@\n-        Class<? extends ByteVector> vectorType = vsp.vectorType();\n-\n@@ -3898,0 +3967,17 @@\n+        int vlen = vsp.length();\n+        int idx_vlen = lsp.length();\n+        if (vlen > idx_vlen * 4) {\n+            \/\/ Fall back to scalar version when the vector is too large relative to the\n+            \/\/ index vector. This occurs when the vector size exceeds the max vector size\n+            \/\/ supported by the hardware. For example, when loading a Byte256Vector on\n+            \/\/ hardware with MaxVectorSize of 16 bytes (128 bits), the index species \"lsp\"\n+            \/\/ would be IntVector.SPECIES_128 with 4 int elements, while the byte vector\n+            \/\/ has 32 elements (256\/8). Since 32 > 4*4, the condition matches. We disable\n+            \/\/ intrinsification in such cases because the compiler will eventually fall back\n+            \/\/ to the Java implementation anyway since the vector size exceeds the hardware\n+            \/\/ limit. While calling the intrinsic would also work, it would negatively impact\n+            \/\/ performance because the implementation would need to split the operation into\n+            \/\/ multiple smaller sub-operations.\n+            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        }\n+\n@@ -3903,2 +3989,0 @@\n-        int vlen = vsp.length();\n-        int idx_vlen = lsp.length();\n@@ -3920,7 +4004,28 @@\n-        return VectorSupport.loadWithMap(\n-            vectorType, maskClass, byte.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, vix2, vix3, m,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+        \/\/ The first time of gather-load.\n+        ByteVector vec = loadWithMap(maskClass, vsp, lsp, vix0, a, offset, indexMap, mapOffset, m);\n+\n+        \/\/ The second time of gather-load.\n+        if (vix1 != null) {\n+            M m1 = slideDownVectorMask(m, idx_vlen);\n+            ByteVector vec1 = loadWithMap(maskClass, vsp, lsp, vix1, a, offset, indexMap, mapOffset + idx_vlen, m1);\n+            \/\/ Merge with previous gather load result: vec = [vec, vec1]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec1));\n+        }\n+\n+        \/\/ More times of gather-load.\n+        if (vix2 != null && vix3 != null) {\n+            \/\/ The third time of gather-load.\n+            M m2 = slideDownVectorMask(m, idx_vlen * 2);\n+            ByteVector vec2 = loadWithMap(maskClass, vsp, lsp, vix2, a, offset, indexMap, mapOffset + idx_vlen * 2, m2);\n+\n+            \/\/ The fourth time of gather-load.\n+            M m3 = slideDownVectorMask(m2, idx_vlen);\n+            ByteVector vec3 = loadWithMap(maskClass, vsp, lsp, vix3, a, offset, indexMap, mapOffset + idx_vlen * 3, m3);\n+\n+            \/\/ Merge the third and fourth gather-load results: vec2 = [vec2, vec3]\n+            vec2 = vec2.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec3));\n+\n+            \/\/ Merge with previous gather-load results: vec = [vec, vec2]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - 2 * idx_vlen, vec2));\n+        }\n+        return vec;\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ByteVector.java","additions":127,"deletions":22,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -2941,1 +2941,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, null,\n+            a, ARRAY_BASE, vix, null,\n@@ -2943,1 +2943,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n@@ -3427,1 +3427,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, m,\n+            a, ARRAY_BASE, vix, m,\n@@ -3429,1 +3429,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/DoubleVector.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2947,1 +2947,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, null,\n+            a, ARRAY_BASE, vix, null,\n@@ -2949,1 +2949,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n@@ -3396,1 +3396,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, m,\n+            a, ARRAY_BASE, vix, m,\n@@ -3398,1 +3398,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/FloatVector.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3103,1 +3103,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, null,\n+            a, ARRAY_BASE, vix, null,\n@@ -3105,1 +3105,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n@@ -3552,1 +3552,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, m,\n+            a, ARRAY_BASE, vix, m,\n@@ -3554,1 +3554,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/IntVector.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2982,1 +2982,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, null,\n+            a, ARRAY_BASE, vix, null,\n@@ -2984,1 +2984,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n@@ -3468,1 +3468,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, m,\n+            a, ARRAY_BASE, vix, m,\n@@ -3470,1 +3470,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/LongVector.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3074,0 +3074,17 @@\n+    @ForceInline\n+    private static\n+    ShortVector loadWithMap(VectorSpecies<Short> species,\n+                            VectorSpecies<Integer> lsp,\n+                            IntVector vix, short[] a, int offset,\n+                            int[] indexMap, int mapOffset) {\n+        ShortSpecies vsp = (ShortSpecies) species;\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, short.class, vsp.laneCount(),\n+            lsp.vectorType(), lsp.length(),\n+            a, ARRAY_BASE, vix, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm, num) ->\n+            s.vOp(n -> (n < num) ? c[idx + iMap[idy + n]] : 0));\n+    }\n+\n@@ -3114,2 +3131,0 @@\n-        Class<? extends ShortVector> vectorType = vsp.vectorType();\n-\n@@ -3125,0 +3140,17 @@\n+        int vlen = vsp.length();\n+        int idx_vlen = lsp.length();\n+        if (vlen > idx_vlen * 2) {\n+            \/\/ Fall back to scalar version when the vector is too large relative to the\n+            \/\/ index vector. This occurs when the vector size exceeds the max vector size\n+            \/\/ supported by the hardware. For example, when loading a Byte256Vector on\n+            \/\/ hardware with MaxVectorSize of 16 bytes (128 bits), the index species \"lsp\"\n+            \/\/ would be IntVector.SPECIES_128 with 4 int elements, while the byte vector\n+            \/\/ has 32 elements (256\/8). Since 32 > 4*4, the condition matches. We disable\n+            \/\/ intrinsification in such cases because the compiler will eventually fall back\n+            \/\/ to the Java implementation anyway since the vector size exceeds the hardware\n+            \/\/ limit. While calling the intrinsic would also work, it would negatively impact\n+            \/\/ performance because the implementation would need to split the operation into\n+            \/\/ multiple smaller sub-operations.\n+            return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        }\n+\n@@ -3129,2 +3161,0 @@\n-        int vlen = vsp.length();\n-        int idx_vlen = lsp.length();\n@@ -3137,7 +3167,10 @@\n-        return VectorSupport.loadWithMap(\n-            vectorType, null, short.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, null, null, null,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(n -> c[idx + iMap[idy+n]]));\n+        \/\/ The first time of gather-load.\n+        ShortVector vec = loadWithMap(vsp, lsp, vix0, a, offset, indexMap, mapOffset);\n+\n+        \/\/ The second time of gather-load.\n+        if (vix1 != null) {\n+            ShortVector vec1 = loadWithMap(vsp, lsp, vix1, a, offset, indexMap, mapOffset + idx_vlen);\n+            \/\/ Merge with previous gather-load result: vec = [vec, vec1]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec1));\n+        }\n+        return vec;\n@@ -3849,0 +3882,28 @@\n+    @ForceInline\n+    private static\n+    <M extends VectorMask<Short>>\n+    ShortVector loadWithMap(Class<M> maskClass,\n+                            VectorSpecies<Short> species,\n+                            VectorSpecies<Integer> lsp,\n+                            IntVector vix, short[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        ShortSpecies vsp = (ShortSpecies) species;\n+        Class<? extends ShortVector> vectorType = vsp.vectorType();\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, short.class, vsp.laneCount(),\n+            lsp.vectorType(), lsp.length(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm, num) ->\n+            s.vOp(vm, n -> (n < num) ? c[idx + iMap[idy + n]] : 0));\n+    }\n+\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    private static\n+    <M extends VectorMask<Short>>\n+    M slideDownVectorMask(M m, int origin) {\n+        Vector<Short> vector = m.toVector();\n+        return (M) vector.slice(origin).compare(NE, 0);\n+    }\n+\n@@ -3864,2 +3925,0 @@\n-        Class<? extends ShortVector> vectorType = vsp.vectorType();\n-\n@@ -3875,0 +3934,17 @@\n+        int vlen = vsp.length();\n+        int idx_vlen = lsp.length();\n+        if (vlen > idx_vlen * 2) {\n+            \/\/ Fall back to scalar version when the vector is too large relative to the\n+            \/\/ index vector. This occurs when the vector size exceeds the max vector size\n+            \/\/ supported by the hardware. For example, when loading a Byte256Vector on\n+            \/\/ hardware with MaxVectorSize of 16 bytes (128 bits), the index species \"lsp\"\n+            \/\/ would be IntVector.SPECIES_128 with 4 int elements, while the byte vector\n+            \/\/ has 32 elements (256\/8). Since 32 > 4*4, the condition matches. We disable\n+            \/\/ intrinsification in such cases because the compiler will eventually fall back\n+            \/\/ to the Java implementation anyway since the vector size exceeds the hardware\n+            \/\/ limit. While calling the intrinsic would also work, it would negatively impact\n+            \/\/ performance because the implementation would need to split the operation into\n+            \/\/ multiple smaller sub-operations.\n+            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        }\n+\n@@ -3880,2 +3956,0 @@\n-        int vlen = vsp.length();\n-        int idx_vlen = lsp.length();\n@@ -3888,7 +3962,11 @@\n-        return VectorSupport.loadWithMap(\n-            vectorType, maskClass, short.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, null, null, m,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n+        \/\/ The first time of gather-load.\n+        ShortVector vec = loadWithMap(maskClass, vsp, lsp, vix0, a, offset, indexMap, mapOffset, m);\n+\n+        \/\/ The second time of gather-load.\n+        if (vix1 != null) {\n+            M m1 = slideDownVectorMask(m, idx_vlen);\n+            ShortVector vec1 = loadWithMap(maskClass, vsp, lsp, vix1, a, offset, indexMap, mapOffset + idx_vlen, m1);\n+            \/\/ Merge with previous gather load result: vec = [vec, vec1]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec1));\n+        }\n+        return vec;\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/ShortVector.java","additions":100,"deletions":22,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -3714,0 +3714,19 @@\n+#if[byteOrShort]\n+\n+    @ForceInline\n+    private static\n+    $abstractvectortype$ loadWithMap(VectorSpecies<$Boxtype$> species,\n+                            VectorSpecies<Integer> lsp,\n+                            IntVector vix, $type$[] a, int offset,\n+                            int[] indexMap, int mapOffset) {\n+        $Type$Species vsp = ($Type$Species) species;\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+        return VectorSupport.loadWithMap(\n+            vectorType, null, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(), lsp.length(),\n+            a, ARRAY_BASE, vix, null,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm, num) ->\n+            s.vOp(n -> (n < num) ? c[idx + iMap[idy + n]] : 0));\n+    }\n+#end[byteOrShort]\n@@ -3756,2 +3775,0 @@\n-        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n-\n@@ -3767,0 +3784,21 @@\n+        int vlen = vsp.length();\n+        int idx_vlen = lsp.length();\n+#if[byte]\n+        if (vlen > idx_vlen * 4) {\n+#else[byte]\n+        if (vlen > idx_vlen * 2) {\n+#end[byte]\n+            \/\/ Fall back to scalar version when the vector is too large relative to the\n+            \/\/ index vector. This occurs when the vector size exceeds the max vector size\n+            \/\/ supported by the hardware. For example, when loading a Byte256Vector on\n+            \/\/ hardware with MaxVectorSize of 16 bytes (128 bits), the index species \"lsp\"\n+            \/\/ would be IntVector.SPECIES_128 with 4 int elements, while the byte vector\n+            \/\/ has 32 elements (256\/8). Since 32 > 4*4, the condition matches. We disable\n+            \/\/ intrinsification in such cases because the compiler will eventually fall back\n+            \/\/ to the Java implementation anyway since the vector size exceeds the hardware\n+            \/\/ limit. While calling the intrinsic would also work, it would negatively impact\n+            \/\/ performance because the implementation would need to split the operation into\n+            \/\/ multiple smaller sub-operations.\n+            return vsp.vOp(n -> a[offset + indexMap[mapOffset + n]]);\n+        }\n+\n@@ -3771,2 +3809,0 @@\n-        int vlen = vsp.length();\n-        int idx_vlen = lsp.length();\n@@ -3789,15 +3825,0 @@\n-        return VectorSupport.loadWithMap(\n-            vectorType, null, $type$.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, vix2, vix3, null,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(n -> c[idx + iMap[idy+n]]));\n-#else[byte]\n-        return VectorSupport.loadWithMap(\n-            vectorType, null, $type$.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, null, null, null,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(n -> c[idx + iMap[idy+n]]));\n@@ -3805,0 +3826,22 @@\n+        \/\/ The first time of gather-load.\n+        $abstractvectortype$ vec = loadWithMap(vsp, lsp, vix0, a, offset, indexMap, mapOffset);\n+\n+        \/\/ The second time of gather-load.\n+        if (vix1 != null) {\n+            $abstractvectortype$ vec1 = loadWithMap(vsp, lsp, vix1, a, offset, indexMap, mapOffset + idx_vlen);\n+            \/\/ Merge with previous gather-load result: vec = [vec, vec1]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec1));\n+        }\n+#if[byte]\n+\n+        \/\/ The third and fourth time of gather-load.\n+        if (vix2 != null && vix3 != null) {\n+            $abstractvectortype$ vec2 = loadWithMap(vsp, lsp, vix2, a, offset, indexMap, mapOffset + idx_vlen * 2);\n+            $abstractvectortype$ vec3 = loadWithMap(vsp, lsp, vix3, a, offset, indexMap, mapOffset + idx_vlen * 3);\n+            \/\/ Merge the third and fourth gather-load results: vec2 = [vec2, vec3]\n+            vec2 = vec2.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec3));\n+            \/\/ Merge with previous gather-load results: vec = [vec, vec2]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - 2 * idx_vlen, vec2));\n+        }\n+#end[byte]\n+        return vec;\n@@ -3853,1 +3896,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, null,\n+            a, ARRAY_BASE, vix, null,\n@@ -3855,1 +3898,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n@@ -4969,0 +5012,30 @@\n+#if[byteOrShort]\n+\n+    @ForceInline\n+    private static\n+    <M extends VectorMask<$Boxtype$>>\n+    $abstractvectortype$ loadWithMap(Class<M> maskClass,\n+                            VectorSpecies<$Boxtype$> species,\n+                            VectorSpecies<Integer> lsp,\n+                            IntVector vix, $type$[] a, int offset,\n+                            int[] indexMap, int mapOffset, M m) {\n+        $Type$Species vsp = ($Type$Species) species;\n+        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n+        return VectorSupport.loadWithMap(\n+            vectorType, maskClass, $type$.class, vsp.laneCount(),\n+            lsp.vectorType(), lsp.length(),\n+            a, ARRAY_BASE, vix, m,\n+            a, offset, indexMap, mapOffset, vsp,\n+            (c, idx, iMap, idy, s, vm, num) ->\n+            s.vOp(vm, n -> (n < num) ? c[idx + iMap[idy + n]] : 0));\n+    }\n+\n+    @ForceInline\n+    @SuppressWarnings(\"unchecked\")\n+    private static\n+    <M extends VectorMask<$Boxtype$>>\n+    M slideDownVectorMask(M m, int origin) {\n+        Vector<$Boxtype$> vector = m.toVector();\n+        return (M) vector.slice(origin).compare(NE, 0);\n+    }\n+#end[byteOrShort]\n@@ -4986,2 +5059,0 @@\n-        Class<? extends $abstractvectortype$> vectorType = vsp.vectorType();\n-\n@@ -4997,0 +5068,21 @@\n+        int vlen = vsp.length();\n+        int idx_vlen = lsp.length();\n+#if[byte]\n+        if (vlen > idx_vlen * 4) {\n+#else[byte]\n+        if (vlen > idx_vlen * 2) {\n+#end[byte]\n+            \/\/ Fall back to scalar version when the vector is too large relative to the\n+            \/\/ index vector. This occurs when the vector size exceeds the max vector size\n+            \/\/ supported by the hardware. For example, when loading a Byte256Vector on\n+            \/\/ hardware with MaxVectorSize of 16 bytes (128 bits), the index species \"lsp\"\n+            \/\/ would be IntVector.SPECIES_128 with 4 int elements, while the byte vector\n+            \/\/ has 32 elements (256\/8). Since 32 > 4*4, the condition matches. We disable\n+            \/\/ intrinsification in such cases because the compiler will eventually fall back\n+            \/\/ to the Java implementation anyway since the vector size exceeds the hardware\n+            \/\/ limit. While calling the intrinsic would also work, it would negatively impact\n+            \/\/ performance because the implementation would need to split the operation into\n+            \/\/ multiple smaller sub-operations.\n+            return vsp.vOp(m, n -> a[offset + indexMap[mapOffset + n]]);\n+        }\n+\n@@ -5002,2 +5094,0 @@\n-        int vlen = vsp.length();\n-        int idx_vlen = lsp.length();\n@@ -5020,15 +5110,0 @@\n-        return VectorSupport.loadWithMap(\n-            vectorType, maskClass, $type$.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, vix2, vix3, m,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n-#else[byte]\n-        return VectorSupport.loadWithMap(\n-            vectorType, maskClass, $type$.class, vsp.laneCount(),\n-            lsp.vectorType(), lsp.length(),\n-            a, ARRAY_BASE, vix0, vix1, null, null, m,\n-            a, offset, indexMap, mapOffset, vsp,\n-            (c, idx, iMap, idy, s, vm) ->\n-            s.vOp(vm, n -> c[idx + iMap[idy+n]]));\n@@ -5036,0 +5111,30 @@\n+        \/\/ The first time of gather-load.\n+        $abstractvectortype$ vec = loadWithMap(maskClass, vsp, lsp, vix0, a, offset, indexMap, mapOffset, m);\n+\n+        \/\/ The second time of gather-load.\n+        if (vix1 != null) {\n+            M m1 = slideDownVectorMask(m, idx_vlen);\n+            $abstractvectortype$ vec1 = loadWithMap(maskClass, vsp, lsp, vix1, a, offset, indexMap, mapOffset + idx_vlen, m1);\n+            \/\/ Merge with previous gather load result: vec = [vec, vec1]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec1));\n+        }\n+#if[byte]\n+\n+        \/\/ More times of gather-load.\n+        if (vix2 != null && vix3 != null) {\n+            \/\/ The third time of gather-load.\n+            M m2 = slideDownVectorMask(m, idx_vlen * 2);\n+            $abstractvectortype$ vec2 = loadWithMap(maskClass, vsp, lsp, vix2, a, offset, indexMap, mapOffset + idx_vlen * 2, m2);\n+\n+            \/\/ The fourth time of gather-load.\n+            M m3 = slideDownVectorMask(m2, idx_vlen);\n+            $abstractvectortype$ vec3 = loadWithMap(maskClass, vsp, lsp, vix3, a, offset, indexMap, mapOffset + idx_vlen * 3, m3);\n+\n+            \/\/ Merge the third and fourth gather-load results: vec2 = [vec2, vec3]\n+            vec2 = vec2.or(vsp.broadcast(0).slice(vlen - idx_vlen, vec3));\n+\n+            \/\/ Merge with previous gather-load results: vec = [vec, vec2]\n+            vec = vec.or(vsp.broadcast(0).slice(vlen - 2 * idx_vlen, vec2));\n+        }\n+#end[byte]\n+        return vec;\n@@ -5086,1 +5191,1 @@\n-            a, ARRAY_BASE, vix, null, null, null, m,\n+            a, ARRAY_BASE, vix, m,\n@@ -5088,1 +5193,1 @@\n-            (c, idx, iMap, idy, s, vm) ->\n+            (c, idx, iMap, idy, s, vm, num) ->\n","filename":"src\/jdk.incubator.vector\/share\/classes\/jdk\/incubator\/vector\/X-Vector.java.template","additions":147,"deletions":42,"binary":false,"changes":189,"status":"modified"}]}