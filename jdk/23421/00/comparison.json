{"files":[{"patch":"@@ -315,4 +315,2 @@\n-  \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-  ldr(rscratch1, Address(tmp, ObjectMonitor::EntryList_offset()));\n-  ldr(tmpReg, Address(tmp, ObjectMonitor::cxq_offset()));\n-  orr(rscratch1, rscratch1, tmpReg);\n+  \/\/ Check if the entry_list is empty.\n+  ldr(rscratch1, Address(tmp, ObjectMonitor::entry_list_offset()));\n@@ -638,4 +636,2 @@\n-    \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-    ldr(rscratch1, Address(t1_monitor, ObjectMonitor::EntryList_offset()));\n-    ldr(t3_t, Address(t1_monitor, ObjectMonitor::cxq_offset()));\n-    orr(rscratch1, rscratch1, t3_t);\n+    \/\/ Check if the entry_list is empty.\n+    ldr(rscratch1, Address(t1_monitor, ObjectMonitor::entry_list_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2960,4 +2960,2 @@\n-  \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-  ld(temp,             in_bytes(ObjectMonitor::EntryList_offset()), current_header);\n-  ld(displaced_header, in_bytes(ObjectMonitor::cxq_offset()), current_header);\n-  orr(temp, temp, displaced_header); \/\/ Will be 0 if both are 0.\n+  \/\/ Check if the entry_list is empty.\n+  ld(temp, in_bytes(ObjectMonitor::entry_list_offset()), current_header);\n@@ -3301,2 +3299,0 @@\n-    const Register t2 = tmp2;\n-\n@@ -3312,4 +3308,2 @@\n-    \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-    ld(t, in_bytes(ObjectMonitor::EntryList_offset()), monitor);\n-    ld(t2, in_bytes(ObjectMonitor::cxq_offset()), monitor);\n-    orr(t, t, t2);\n+    \/\/ Check if the entry_list is empty.\n+    ld(t, in_bytes(ObjectMonitor::entry_list_offset()), monitor);\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -236,4 +236,2 @@\n-  \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-  ld(t0, Address(tmp, ObjectMonitor::EntryList_offset()));\n-  ld(tmp1Reg, Address(tmp, ObjectMonitor::cxq_offset()));\n-  orr(t0, t0, tmp1Reg);\n+  \/\/ Check if the entry_list is empty.\n+  ld(t0, Address(tmp, ObjectMonitor::entry_list_offset()));\n@@ -572,4 +570,2 @@\n-    \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-    ld(t0, Address(tmp1_monitor, ObjectMonitor::EntryList_offset()));\n-    ld(tmp3_t, Address(tmp1_monitor, ObjectMonitor::cxq_offset()));\n-    orr(t0, t0, tmp3_t);\n+    \/\/ Check if the entry_list is empty.\n+    ld(t0, Address(tmp1_monitor, ObjectMonitor::entry_list_offset()));\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3934,1 +3934,1 @@\n-  NearLabel check_succ, set_eq_unlocked;\n+  NearLabel set_eq_unlocked;\n@@ -3944,4 +3944,2 @@\n-  \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-  load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n-  z_brne(check_succ);\n-  load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+  \/\/ Check if the entry_list is empty.\n+  load_and_test_long(temp, Address(currentHeader, OM_OFFSET_NO_MONITOR_VALUE_TAG(entry_list)));\n@@ -3950,2 +3948,0 @@\n-  bind(check_succ);\n-\n@@ -6797,1 +6793,0 @@\n-    const Address cxq_address{monitor, ObjectMonitor::cxq_offset() - monitor_tag};\n@@ -6799,1 +6794,1 @@\n-    const Address EntryList_address{monitor, ObjectMonitor::EntryList_offset() - monitor_tag};\n+    const Address entry_list_address{monitor, ObjectMonitor::entry_list_offset() - monitor_tag};\n@@ -6816,1 +6811,1 @@\n-    NearLabel check_succ, set_eq_unlocked;\n+    NearLabel set_eq_unlocked;\n@@ -6826,4 +6821,2 @@\n-    \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-    load_and_test_long(tmp2, EntryList_address);\n-    z_brne(check_succ);\n-    load_and_test_long(tmp2, cxq_address);\n+    \/\/ Check if the entry_list is empty.\n+    load_and_test_long(tmp2, entry_list_address);\n@@ -6832,2 +6825,0 @@\n-    bind(check_succ);\n-\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":7,"deletions":16,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -418,1 +418,1 @@\n-  \/\/ state in _succ so we can avoid fetching EntryList|cxq.\n+  \/\/ state in _succ so we can avoid fetching entry_list.\n@@ -450,3 +450,2 @@\n-  \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-  movptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(EntryList)));\n-  orptr(boxReg, Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(cxq)));\n+  \/\/ Check if the entry_list is empty.\n+  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(entry_list)), NULL_WORD);\n@@ -770,1 +769,0 @@\n-    const Address cxq_address{monitor, ObjectMonitor::cxq_offset() - monitor_tag};\n@@ -772,1 +770,1 @@\n-    const Address EntryList_address{monitor, ObjectMonitor::EntryList_offset() - monitor_tag};\n+    const Address entry_list_address{monitor, ObjectMonitor::entry_list_offset() - monitor_tag};\n@@ -788,3 +786,2 @@\n-    \/\/ Check if the entry lists are empty (EntryList first - by convention).\n-    movptr(reg_rax, EntryList_address);\n-    orptr(reg_rax, cxq_address);\n+    \/\/ Check if the entry_list is empty.\n+    cmpptr(entry_list_address, NULL_WORD);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -331,2 +331,1 @@\n-  volatile_nonstatic_field(ObjectMonitor,      _cxq,                                          ObjectWaiter*)                         \\\n-  volatile_nonstatic_field(ObjectMonitor,      _EntryList,                                    ObjectWaiter*)                         \\\n+  volatile_nonstatic_field(ObjectMonitor,      _entry_list,                                    ObjectWaiter*)                        \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-\/\/   cxq, EntryList or WaitSet -- at any one time.\n+\/\/   entry_list or WaitSet -- at any one time.\n@@ -132,1 +132,1 @@\n-\/\/ * Contending threads \"push\" themselves onto the cxq with CAS\n+\/\/ * Contending threads \"push\" themselves onto the entry_list with CAS\n@@ -138,1 +138,1 @@\n-\/\/   succeeds, it will push itself onto the cxq with CAS and then\n+\/\/   succeeds, it will push itself onto the entry_list with CAS and then\n@@ -142,1 +142,1 @@\n-\/\/   dequeue itself from either the EntryList or the cxq.\n+\/\/   dequeue itself from the entry_list.\n@@ -145,1 +145,1 @@\n-\/\/   tentative successor thread on the EntryList. In case the successor\n+\/\/   tentative successor thread on the entry_list. In case the successor\n@@ -151,1 +151,1 @@\n-\/\/   thread from the EntryList. After having been unparked\/re-scheduled,\n+\/\/   thread from the entry_list. After having been unparked\/re-scheduled,\n@@ -157,1 +157,1 @@\n-\/\/   successor thread.  (This is also referred to as \"handoff\" succession\").\n+\/\/   successor thread.  (This is also referred to as \"handoff succession\").\n@@ -160,6 +160,49 @@\n-\/\/   If the EntryList is empty but the cxq is populated the exiting\n-\/\/   thread will drain the cxq into the EntryList.  It does so by\n-\/\/   by detaching the cxq (installing null with CAS) and folding\n-\/\/   the threads from the cxq into the EntryList.  The EntryList is\n-\/\/   doubly linked, while the cxq is singly linked because of the\n-\/\/   CAS-based \"push\" used to enqueue recently arrived threads (RATs).\n+\/\/\n+\/\/   The entry_list is a linked list, the first contending thread that\n+\/\/   \"pushed\" itself onto entry_list, will be the last thread in the\n+\/\/   list. Each newly pushed thread in entry_list will be linked trough\n+\/\/   its next pointer, and have its prev pointer set to null. Thus\n+\/\/   pushing six threads A-F (in that order) onto entry_list, will\n+\/\/   form a singly-linked list, see 1) below.\n+\/\/\n+\/\/   Since the successor is chosen in FIFO order, the exiting thread\n+\/\/   needs to find the tail of the entry_list. This is done by walking\n+\/\/   from the entry_list head. While walking the list we also assign\n+\/\/   the prev pointers of each thread, essentially forming a doubly\n+\/\/   linked list, see 2) below.\n+\/\/\n+\/\/   Once we have formed a doubly linked list it's easy to find the\n+\/\/   successor, wake it up, have it remove itself, and update the\n+\/\/   tail pointer, as seen in 2) and 3) below.\n+\/\/\n+\/\/   At any time new threads can add themselves to the entry_list, see\n+\/\/   4) and 5).\n+\/\/\n+\/\/   If the thread that removes itself from the end of the list hasn't\n+\/\/   got any prev pointer, we just set the tail pointer to null, see\n+\/\/   5) and 6).\n+\/\/\n+\/\/   Next time we need to find the successor and the tail is null, we\n+\/\/   just start walking from the entry_list head again forming a new\n+\/\/   doubly linked list, see 6) and 7) below.\n+\/\/\n+\/\/      1)  entry_list       ->F->E->D->C->B->A->null\n+\/\/          entry_list_tail  ->null\n+\/\/\n+\/\/      2)  entry_list       ->F<=>E<=>D<=>C<=>B<=>A->null\n+\/\/          entry_list_tail  ----------------------^\n+\/\/\n+\/\/      3)  entry_list       ->F<=>E<=>D<=>C<=>B->null\n+\/\/          entry_list_tail  ------------------^\n+\/\/\n+\/\/      4)  entry_list       ->F->null\n+\/\/          entry_list_tail  --^\n+\/\/\n+\/\/      5)  entry_list       ->I->H->G->F->null\n+\/\/          entry_list_tail  -----------^\n+\/\/\n+\/\/      6)  entry_list       ->I->H->G->null\n+\/\/          entry_list_tail  ->null\n+\/\/\n+\/\/      7)  entry_list       ->I<=>H<=>G->null\n+\/\/          entry_list_tail  ----------^\n@@ -169,2 +212,2 @@\n-\/\/   -- only the monitor owner may access or mutate the EntryList.\n-\/\/      The mutex property of the monitor itself protects the EntryList\n+\/\/   -- only the monitor owner may assign prev pointers in entry_list.\n+\/\/      The mutex property of the monitor itself protects the entry_list\n@@ -172,1 +215,1 @@\n-\/\/   -- Only the monitor owner may detach the cxq.\n+\/\/   -- Only the monitor owner may detach nodes from the entry_list.\n@@ -179,3 +222,4 @@\n-\/\/ * The cxq can have multiple concurrent \"pushers\" but only one concurrent\n-\/\/   detaching thread.  This mechanism is immune from the ABA corruption.\n-\/\/   More precisely, the CAS-based \"push\" onto cxq is ABA-oblivious.\n+\/\/ * The entry_list can have multiple concurrent \"pushers\" but only one\n+\/\/   concurrent detaching thread. This mechanism is immune from the\n+\/\/   ABA corruption. More precisely, the CAS-based \"push\" onto\n+\/\/   entry_list is ABA-oblivious.\n@@ -183,11 +227,6 @@\n-\/\/ * Taken together, the cxq and the EntryList constitute or form a\n-\/\/   single logical queue of threads stalled trying to acquire the lock.\n-\/\/   We use two distinct lists to improve the odds of a constant-time\n-\/\/   dequeue operation after acquisition (in the ::enter() epilogue) and\n-\/\/   to reduce heat on the list ends.  (c.f. Michael Scott's \"2Q\" algorithm).\n-\/\/   A key desideratum is to minimize queue & monitor metadata manipulation\n-\/\/   that occurs while holding the monitor lock -- that is, we want to\n-\/\/   minimize monitor lock holds times.  Note that even a small amount of\n-\/\/   fixed spinning will greatly reduce the # of enqueue-dequeue operations\n-\/\/   on EntryList|cxq.  That is, spinning relieves contention on the \"inner\"\n-\/\/   locks and monitor metadata.\n+\/\/ * The entry_list form a queue of threads stalled trying to acquire\n+\/\/   the lock. Within the entry_list the next pointers always form a\n+\/\/   consistent singly-linked list. At unlock-time when the unlocking\n+\/\/   thread notices that the tail of the entry_list is not known, we\n+\/\/   convert the singly-linked entry_list into a doubly linked list by\n+\/\/   assigning the prev pointers and the entry_list_tail pointer.\n@@ -195,4 +234,9 @@\n-\/\/   Cxq points to the set of Recently Arrived Threads attempting entry.\n-\/\/   Because we push threads onto _cxq with CAS, the RATs must take the form of\n-\/\/   a singly-linked LIFO.  We drain _cxq into EntryList at unlock-time when\n-\/\/   the unlocking thread notices that EntryList is null but _cxq is != null.\n+\/\/   As long as the entry_list_tail is known the odds are good that we\n+\/\/   should be able to dequeue after acquisition (in the ::enter()\n+\/\/   epilogue) in constant-time. This is good since a key desideratum\n+\/\/   is to minimize queue & monitor metadata manipulation that occurs\n+\/\/   while holding the monitor lock -- that is, we want to minimize\n+\/\/   monitor lock holds times.  Note that even a small amount of fixed\n+\/\/   spinning will greatly reduce the # of enqueue-dequeue operations\n+\/\/   on entry_list. That is, spinning relieves contention on the\n+\/\/   \"inner\" locks and monitor metadata.\n@@ -200,12 +244,5 @@\n-\/\/   The EntryList is ordered by the prevailing queue discipline and\n-\/\/   can be organized in any convenient fashion, such as a doubly-linked list or\n-\/\/   a circular doubly-linked list.  Critically, we want insert and delete operations\n-\/\/   to operate in constant-time.  If we need a priority queue then something akin\n-\/\/   to Solaris' sleepq would work nicely.  Viz.,\n-\/\/   http:\/\/agg.eng\/ws\/on10_nightly\/source\/usr\/src\/uts\/common\/os\/sleepq.c.\n-\/\/   Queue discipline is enforced at ::exit() time, when the unlocking thread\n-\/\/   drains the cxq into the EntryList, and orders or reorders the threads on the\n-\/\/   EntryList accordingly.\n-\/\/\n-\/\/   Barring \"lock barging\", this mechanism provides fair cyclic ordering,\n-\/\/   somewhat similar to an elevator-scan.\n+\/\/   Insert and delete operations may not operate in constant-time if\n+\/\/   we have interference because some other thread is adding or\n+\/\/   removing the head element of entry_list or if we need to convert\n+\/\/   the singly-linked entry_list into a doubly linked list to find the\n+\/\/   tail.\n@@ -222,5 +259,5 @@\n-\/\/ * notify() or notifyAll() simply transfers threads from the WaitSet to\n-\/\/   either the EntryList or cxq.  Subsequent exit() operations will\n-\/\/   unpark\/re-schedule the notifyee. Unparking\/re-scheduling a notifyee in\n-\/\/   notify() is inefficient - it's likely the notifyee would simply impale\n-\/\/   itself on the lock held by the notifier.\n+\/\/ * notify() or notifyAll() simply transfers threads from the WaitSet\n+\/\/   to either the entry_list. Subsequent exit() operations will\n+\/\/   unpark\/re-schedule the notifyee. Unparking\/re-scheduling a\n+\/\/   notifyee in notify() is inefficient - it's likely the notifyee\n+\/\/   would simply impale itself on the lock held by the notifier.\n@@ -258,2 +295,2 @@\n-  _EntryList(nullptr),\n-  _cxq(nullptr),\n+  _entry_list(nullptr),\n+  _entry_list_tail(nullptr),\n@@ -476,1 +513,1 @@\n-void ObjectMonitor::notify_contended_enter(JavaThread *current) {\n+void ObjectMonitor::notify_contended_enter(JavaThread* current) {\n@@ -491,1 +528,1 @@\n-void ObjectMonitor::enter_with_contention_mark(JavaThread *current, ObjectMonitorContentionMark &cm) {\n+void ObjectMonitor::enter_with_contention_mark(JavaThread* current, ObjectMonitorContentionMark &cm) {\n@@ -521,1 +558,1 @@\n-        \/\/ _cxq so cancel preemption. We will still go through the preempt stub\n+        \/\/ _entry_list so cancel preemption. We will still go through the preempt stub\n@@ -660,0 +697,43 @@\n+\/\/ Push \"current\" onto the front of the _entry_list. Once on _entry_list,\n+\/\/ current stays on-queue until it acquires the lock.\n+void ObjectMonitor::add_to_entry_list(JavaThread* current, ObjectWaiter* node) {\n+  node->_prev   = nullptr;\n+  node->TState  = ObjectWaiter::TS_ENTER;\n+\n+  for (;;) {\n+    ObjectWaiter* front = Atomic::load(&_entry_list);\n+\n+    node->_next = front;\n+    if (Atomic::cmpxchg(&_entry_list, front, node) == front) {\n+      return;\n+    }\n+  }\n+}\n+\n+\/\/ Push \"current\" onto the front of the entry_list.\n+\/\/ If the _entry_list was changed during our push operation, we try to\n+\/\/ lock the monitor. Returns true if we locked the monitor, and false\n+\/\/ if we added current to _entry_list. Once on _entry_list, current\n+\/\/ stays on-queue until it acquires the lock.\n+bool ObjectMonitor::try_lock_or_add_to_entry_list(JavaThread* current, ObjectWaiter* node) {\n+  node->_prev   = nullptr;\n+  node->TState  = ObjectWaiter::TS_ENTER;\n+\n+  for (;;) {\n+    ObjectWaiter* front = Atomic::load(&_entry_list);\n+\n+    node->_next = front;\n+    if (Atomic::cmpxchg(&_entry_list, front, node) == front) {\n+      return false;\n+    }\n+\n+    \/\/ Interference - the CAS failed because _entry_list changed.  Just retry.\n+    \/\/ As an optional optimization we retry the lock.\n+    if (TryLock(current) == TryLockResult::Success) {\n+      assert(!has_successor(current), \"invariant\");\n+      assert(has_owner(current), \"invariant\");\n+      return true;\n+    }\n+  }\n+}\n+\n@@ -730,5 +810,3 @@\n-  guarantee(_cxq == nullptr, \"must be no contending threads: cxq=\"\n-            INTPTR_FORMAT, p2i(_cxq));\n-  guarantee(_EntryList == nullptr,\n-            \"must be no entering threads: EntryList=\" INTPTR_FORMAT,\n-            p2i(_EntryList));\n+  guarantee(_entry_list == nullptr,\n+            \"must be no entering threads: entry_list=\" INTPTR_FORMAT,\n+            p2i(_entry_list));\n@@ -819,2 +897,1 @@\n-            \", cxq=\" PTR_FORMAT\n-            \", EntryList=\" PTR_FORMAT,\n+            \", entry_list=\" PTR_FORMAT,\n@@ -828,2 +905,1 @@\n-            p2i(_cxq),\n-            p2i(_EntryList));\n+            p2i(_entry_list));\n@@ -862,1 +938,1 @@\n-  \/\/ Enqueue \"current\" on ObjectMonitor's _cxq.\n+  \/\/ Enqueue \"current\" on ObjectMonitor's _entry_list.\n@@ -873,11 +949,0 @@\n-  node._prev   = (ObjectWaiter*) 0xBAD;\n-  node.TState  = ObjectWaiter::TS_CXQ;\n-\n-  \/\/ Push \"current\" onto the front of the _cxq.\n-  \/\/ Once on cxq\/EntryList, current stays on-queue until it acquires the lock.\n-  \/\/ Note that spinning tends to reduce the rate at which threads\n-  \/\/ enqueue and dequeue on EntryList|cxq.\n-  ObjectWaiter* nxt;\n-  for (;;) {\n-    node._next = nxt = _cxq;\n-    if (Atomic::cmpxchg(&_cxq, nxt, &node) == nxt) break;\n@@ -885,7 +950,2 @@\n-    \/\/ Interference - the CAS failed because _cxq changed.  Just retry.\n-    \/\/ As an optional optimization we retry the lock.\n-    if (TryLock(current) == TryLockResult::Success) {\n-      assert(!has_successor(current), \"invariant\");\n-      assert(has_owner(current), \"invariant\");\n-      return;\n-    }\n+  if (try_lock_or_add_to_entry_list(current, &node)) {\n+    return; \/\/ We got the lock.\n@@ -893,0 +953,1 @@\n+  \/\/ This thread is now added to the _entry_list.\n@@ -895,1 +956,1 @@\n-  \/\/ itself onto _cxq.  To close the race and avoid \"stranding\" and\n+  \/\/ itself onto _entry_list.  To close the race and avoid \"stranding\" and\n@@ -897,1 +958,1 @@\n-  \/\/ Note the Dekker\/Lamport duality: ST cxq; MEMBAR; LD Owner.\n+  \/\/ Note the Dekker\/Lamport duality: ST _entry_list; MEMBAR; LD Owner.\n@@ -973,9 +1034,1 @@\n-  \/\/ current has acquired the lock -- Unlink current from the cxq or EntryList.\n-  \/\/ Normally we'll find current on the EntryList .\n-  \/\/ From the perspective of the lock owner (this thread), the\n-  \/\/ EntryList is stable and cxq is prepend-only.\n-  \/\/ The head of cxq is volatile but the interior is stable.\n-  \/\/ In addition, current.TState is stable.\n-\n-  assert(has_owner(current), \"invariant\");\n-\n+  \/\/ Current has acquired the lock -- Unlink current from the _entry_list.\n@@ -991,3 +1044,3 @@\n-  \/\/ But since the CAS() this thread may have also stored into _succ,\n-  \/\/ EntryList or cxq.  These meta-data updates must be\n-  \/\/ visible __before this thread subsequently drops the lock.\n+  \/\/ But since the CAS() this thread may have also stored into _succ\n+  \/\/ or entry_list.  These meta-data updates must be visible __before\n+  \/\/ this thread subsequently drops the lock.\n@@ -1007,1 +1060,1 @@\n-  \/\/ Critically, any prior STs to _succ or EntryList must be visible before\n+  \/\/ Critically, any prior STs to _succ or entry_list must be visible before\n@@ -1030,1 +1083,1 @@\n-    guarantee(v == ObjectWaiter::TS_ENTER || v == ObjectWaiter::TS_CXQ, \"invariant\");\n+    guarantee(v == ObjectWaiter::TS_ENTER, \"invariant\");\n@@ -1080,8 +1133,1 @@\n-  \/\/ current has acquired the lock -- Unlink current from the cxq or EntryList .\n-  \/\/ Normally we'll find current on the EntryList.\n-  \/\/ Unlinking from the EntryList is constant-time and atomic-free.\n-  \/\/ From the perspective of the lock owner (this thread), the\n-  \/\/ EntryList is stable and cxq is prepend-only.\n-  \/\/ The head of cxq is volatile but the interior is stable.\n-  \/\/ In addition, current.TState is stable.\n-\n+  \/\/ Current has acquired the lock -- Unlink current from the _entry_list.\n@@ -1112,17 +1158,4 @@\n-  node->_prev   = (ObjectWaiter*) 0xBAD;\n-  node->TState  = ObjectWaiter::TS_CXQ;\n-\n-  \/\/ Push node associated with vthread onto the front of the _cxq.\n-  ObjectWaiter* nxt;\n-  for (;;) {\n-    node->_next = nxt = _cxq;\n-    if (Atomic::cmpxchg(&_cxq, nxt, node) == nxt) break;\n-\n-    \/\/ Interference - the CAS failed because _cxq changed.  Just retry.\n-    \/\/ As an optional optimization we retry the lock.\n-    if (TryLock(current) == TryLockResult::Success) {\n-      assert(has_owner(current), \"invariant\");\n-      assert(!has_successor(current), \"invariant\");\n-      if (waiter == nullptr) delete node;  \/\/ for Object.wait() don't delete yet\n-      return true;\n-    }\n+  if (try_lock_or_add_to_entry_list(current, node)) {\n+    \/\/ We got the lock.\n+    if (waiter == nullptr) delete node;  \/\/ for Object.wait() don't delete yet\n+    return true;\n@@ -1130,0 +1163,1 @@\n+  \/\/ This thread is now added to the entry_list.\n@@ -1132,1 +1166,1 @@\n-  \/\/ _cxq before we added the node to the queue.\n+  \/\/ _entry_list before we added the node to the queue.\n@@ -1166,1 +1200,1 @@\n-  guarantee(state == ObjectWaiter::TS_ENTER || state == ObjectWaiter::TS_CXQ, \"invariant\");\n+  guarantee(state == ObjectWaiter::TS_ENTER, \"invariant\");\n@@ -1221,1 +1255,29 @@\n-\/\/ By convention we unlink a contending thread from EntryList|cxq immediately\n+\/\/ Return the tail of the _entry_list. If the tail is currently not\n+\/\/ known, find it by walking from the head of _entry_list, and while\n+\/\/ doing so assign the _prev pointers to create a doubly linked list.\n+ObjectWaiter* ObjectMonitor::entry_list_tail(JavaThread* current) {\n+  assert(has_owner(current), \"invariant\");\n+  ObjectWaiter* w = _entry_list_tail;\n+  if (w != nullptr) {\n+    return w;\n+  }\n+  \/\/ Need acquire here to match the implicit release of the cmpxchg\n+  \/\/ that updated _entry_list, so we can access w->_next.\n+  w = Atomic::load_acquire(&_entry_list);\n+  assert(w != nullptr, \"invariant\");\n+  if (w->next() == nullptr) {\n+    _entry_list_tail = w;\n+    return w;\n+  }\n+  ObjectWaiter* prev = nullptr;\n+  while (w != nullptr) {\n+    assert(w->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n+    w->_prev = prev;\n+    prev = w;\n+    w = w->next();\n+  }\n+  _entry_list_tail = prev;\n+  return prev;\n+}\n+\n+\/\/ By convention we unlink a contending thread from _entry_list immediately\n@@ -1224,0 +1286,2 @@\n+\/\/ The head of _entry_list is volatile but the interior is stable.\n+\/\/ In addition, current.TState is stable.\n@@ -1230,40 +1294,12 @@\n-  if (currentNode->TState == ObjectWaiter::TS_ENTER) {\n-    \/\/ Normal case: remove current from the DLL EntryList .\n-    \/\/ This is a constant-time operation.\n-    ObjectWaiter* nxt = currentNode->_next;\n-    ObjectWaiter* prv = currentNode->_prev;\n-    if (nxt != nullptr) nxt->_prev = prv;\n-    if (prv != nullptr) prv->_next = nxt;\n-    if (currentNode == _EntryList) _EntryList = nxt;\n-    assert(nxt == nullptr || nxt->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n-    assert(prv == nullptr || prv->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n-  } else {\n-    assert(currentNode->TState == ObjectWaiter::TS_CXQ, \"invariant\");\n-    \/\/ Inopportune interleaving -- current is still on the cxq.\n-    \/\/ This usually means the enqueue of self raced an exiting thread.\n-    \/\/ Normally we'll find current near the front of the cxq, so\n-    \/\/ dequeueing is typically fast.  If needbe we can accelerate\n-    \/\/ this with some MCS\/CHL-like bidirectional list hints and advisory\n-    \/\/ back-links so dequeueing from the interior will normally operate\n-    \/\/ in constant-time.\n-    \/\/ Dequeue current from either the head (with CAS) or from the interior\n-    \/\/ with a linear-time scan and normal non-atomic memory operations.\n-    \/\/ CONSIDER: if current is on the cxq then simply drain cxq into EntryList\n-    \/\/ and then unlink current from EntryList.  We have to drain eventually,\n-    \/\/ so it might as well be now.\n-\n-    ObjectWaiter* v = _cxq;\n-    assert(v != nullptr, \"invariant\");\n-    if (v != currentNode || Atomic::cmpxchg(&_cxq, v, currentNode->_next) != v) {\n-      \/\/ The CAS above can fail from interference IFF a \"RAT\" arrived.\n-      \/\/ In that case current must be in the interior and can no longer be\n-      \/\/ at the head of cxq.\n-      if (v == currentNode) {\n-        assert(_cxq != v, \"invariant\");\n-        v = _cxq;          \/\/ CAS above failed - start scan at head of list\n-      }\n-      ObjectWaiter* p;\n-      ObjectWaiter* q = nullptr;\n-      for (p = v; p != nullptr && p != currentNode; p = p->_next) {\n-        q = p;\n-        assert(p->TState == ObjectWaiter::TS_CXQ, \"invariant\");\n+  \/\/ Check if we are unlinking the last element in the _entry_list.\n+  \/\/ This is by far the most common case.\n+  if (currentNode->next() == nullptr) {\n+    assert(_entry_list_tail == nullptr || _entry_list_tail == currentNode, \"invariant\");\n+\n+    ObjectWaiter* v = Atomic::load(&_entry_list);\n+    if (v == currentNode) {\n+      \/\/ The currentNode is the only element in _entry_list.\n+      if (Atomic::cmpxchg(&_entry_list, v, (ObjectWaiter*)nullptr) == v) {\n+        _entry_list_tail = nullptr;\n+        currentNode->set_bad_pointers();\n+        return;\n@@ -1271,6 +1307,4 @@\n-      assert(v != currentNode, \"invariant\");\n-      assert(p == currentNode, \"Node not found on cxq\");\n-      assert(p != _cxq, \"invariant\");\n-      assert(q != nullptr, \"invariant\");\n-      assert(q->_next == p, \"invariant\");\n-      q->_next = p->_next;\n+      \/\/ The CAS above can fail from interference IFF a contending\n+      \/\/ thread \"pushed\" itself onto entry_list. So fall-through to\n+      \/\/ building the doubly-linked list.\n+      assert(currentNode->prev() == nullptr, \"invariant\");\n@@ -1278,0 +1312,14 @@\n+    if (currentNode->prev() == nullptr) {\n+      \/\/ Build the doubly linked list to get hold of\n+      \/\/ currentNode->prev().\n+      _entry_list_tail = nullptr;\n+      entry_list_tail(current);\n+      assert(currentNode->prev() != nullptr, \"must be\");\n+    }\n+    \/\/ The currentNode is the last element in _entry_list and we know\n+    \/\/ which element is the previous one.\n+    assert(_entry_list != currentNode, \"invariant\");\n+    _entry_list_tail = currentNode->prev();\n+    _entry_list_tail->_next = nullptr;\n+    currentNode->set_bad_pointers();\n+    return;\n@@ -1280,6 +1328,53 @@\n-#ifdef ASSERT\n-  \/\/ Diagnostic hygiene ...\n-  currentNode->_prev  = (ObjectWaiter*) 0xBAD;\n-  currentNode->_next  = (ObjectWaiter*) 0xBAD;\n-  currentNode->TState = ObjectWaiter::TS_RUN;\n-#endif\n+  \/\/ If we get here it means the current thread enqueued itself on the\n+  \/\/ _entry_list but was then able to \"steal\" the lock before the\n+  \/\/ chosen successor was able to. Consequently currentNode must be an\n+  \/\/ interior node in the _entry_list, or the head.\n+  assert(currentNode->next() != nullptr, \"invariant\");\n+  assert(currentNode != _entry_list_tail, \"invariant\");\n+\n+  \/\/ Check if we are in the singly-linked portion of the\n+  \/\/ _entry_list. If we are the head then we try to remove ourselves,\n+  \/\/ else we convert to the doubly-linked list.\n+  if (currentNode->prev() == nullptr) {\n+    ObjectWaiter* v = Atomic::load(&_entry_list);\n+\n+    assert(v != nullptr, \"invariant\");\n+    if (v == currentNode) {\n+      ObjectWaiter* next = currentNode->next();\n+      \/\/ currentNode is at the head of _entry_list.\n+      if (Atomic::cmpxchg(&_entry_list, v, next) == v) {\n+        \/\/ The CAS above sucsessfully unlinked currentNode from the\n+        \/\/ head of the _entry_list.\n+        assert(_entry_list != v, \"invariant\");\n+        next->_prev = nullptr;\n+        currentNode->set_bad_pointers();\n+        return;\n+      } else {\n+        \/\/ The CAS above can fail from interference IFF a contending\n+        \/\/ thread \"pushed\" itself onto _entry_list, in which case\n+        \/\/ currentNode must now be in the interior of the\n+        \/\/ list. Fall-through to building the doubly-linked list.\n+        assert(_entry_list != currentNode, \"invariant\");\n+      }\n+    }\n+    \/\/ Build the doubly linked list to get hold of currentNode->prev().\n+    _entry_list_tail = nullptr;\n+    entry_list_tail(current);\n+    assert(currentNode->prev() != nullptr, \"must be\");\n+  }\n+\n+  \/\/ We now know we are unlinking currentNode from the interior of a\n+  \/\/ doubly linked list.\n+  assert(currentNode->next() != nullptr, \"\");\n+  assert(currentNode->prev() != nullptr, \"\");\n+  assert(currentNode != _entry_list, \"\");\n+  assert(currentNode != _entry_list_tail, \"\");\n+\n+  ObjectWaiter* nxt = currentNode->next();\n+  ObjectWaiter* prv = currentNode->prev();\n+  assert(nxt->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n+  assert(prv->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n+\n+  nxt->_prev = prv;\n+  prv->_next = nxt;\n+  currentNode->set_bad_pointers();\n@@ -1309,1 +1404,1 @@\n-\/\/      (_succ, _EntryList, _cxq) and data protected by the lock will be\n+\/\/      (_succ, _entry_list) and data protected by the lock will be\n@@ -1315,1 +1410,1 @@\n-\/\/   4. If both _EntryList and _cxq are null, we are done, since there is no\n+\/\/   4. If _entry_list is null, we are done, since there is no\n@@ -1321,2 +1416,2 @@\n-\/\/   6. There are waiters in the entry list (_EntryList and\/or cxq are\n-\/\/      non-null), but there is no successor (_succ is null), so we need to\n+\/\/   6. There are waiters in the entry list (_entry_list is non-null),\n+\/\/      but there is no successor (_succ is null), so we need to\n@@ -1325,11 +1420,3 @@\n-\/\/ Note that since only the current lock owner can manipulate the _EntryList\n-\/\/ or drain _cxq, we need to reacquire the lock before we can wake up\n-\/\/ (unpark) a waiting thread.\n-\/\/\n-\/\/ Note that we read the EntryList and then the cxq after dropping the\n-\/\/ lock, so the values need not form a stable snapshot. In particular,\n-\/\/ after reading the (empty) EntryList, another thread could acquire\n-\/\/ and release the lock, moving any entries in the cxq to the\n-\/\/ EntryList, causing the current thread to see an empty cxq and\n-\/\/ conclude there are no waiters. But this is okay as the thread that\n-\/\/ moved the cxq is responsible for waking the successor.\n+\/\/ Note that since only the current lock owner can manipulate the\n+\/\/ _entry_list (except for pushing new threads to the head), we need to\n+\/\/ reacquire the lock before we can wake up (unpark) a waiting thread.\n@@ -1393,5 +1480,0 @@\n-    if ((intptr_t(_EntryList)|intptr_t(_cxq)) == 0 || has_successor()) {\n-      return;\n-    }\n-    \/\/ Other threads are blocked trying to acquire the lock.\n-\n@@ -1423,5 +1505,10 @@\n-    \/\/ It appears that an heir-presumptive (successor) must be made ready.\n-    \/\/ Only the current lock owner can manipulate the EntryList or\n-    \/\/ drain _cxq, so we need to reacquire the lock.  If we fail\n-    \/\/ to reacquire the lock the responsibility for ensuring succession\n-    \/\/ falls to the new owner.\n+    if (_entry_list == nullptr || has_successor()) {\n+      return;\n+    }\n+\n+    \/\/ Other threads are blocked trying to acquire the lock and there\n+    \/\/ is no successor, so it appears that an heir-presumptive\n+    \/\/ (successor) must be made ready. Only the current lock owner can\n+    \/\/ detach threads from the entry_list, therefore we need to\n+    \/\/ reacquire the lock. If we fail to reacquire the lock the\n+    \/\/ responsibility for ensuring succession falls to the new owner.\n@@ -1439,1 +1526,1 @@\n-    w = _EntryList;\n+    w = Atomic::load(&_entry_list);\n@@ -1441,0 +1528,1 @@\n+      w = entry_list_tail(current);\n@@ -1442,1 +1530,1 @@\n-      \/\/ But in practice an exiting thread may find itself on the EntryList.\n+      \/\/ But in practice an exiting thread may find itself on the entry_list.\n@@ -1446,1 +1534,1 @@\n-      \/\/ notify() operation moves T1 from O's waitset to O's EntryList. T2 then\n+      \/\/ notify() operation moves T1 from O's waitset to O's entry_list. T2 then\n@@ -1448,2 +1536,2 @@\n-      \/\/ _owner, above.  T2 notices that the EntryList is populated, so it\n-      \/\/ reacquires the lock and then finds itself on the EntryList.\n+      \/\/ _owner, above.  T2 notices that the entry_list is populated, so it\n+      \/\/ reacquires the lock and then finds itself on the entry_list.\n@@ -1456,52 +1544,0 @@\n-\n-    \/\/ If we find that both _cxq and EntryList are null then just\n-    \/\/ re-run the exit protocol from the top.\n-    w = _cxq;\n-    if (w == nullptr) continue;\n-\n-    \/\/ Drain _cxq into EntryList - bulk transfer.\n-    \/\/ First, detach _cxq.\n-    \/\/ The following loop is tantamount to: w = swap(&cxq, nullptr)\n-    for (;;) {\n-      assert(w != nullptr, \"Invariant\");\n-      ObjectWaiter* u = Atomic::cmpxchg(&_cxq, w, (ObjectWaiter*)nullptr);\n-      if (u == w) break;\n-      w = u;\n-    }\n-\n-    assert(w != nullptr, \"invariant\");\n-    assert(_EntryList == nullptr, \"invariant\");\n-\n-    \/\/ Convert the LIFO SLL anchored by _cxq into a DLL.\n-    \/\/ The list reorganization step operates in O(LENGTH(w)) time.\n-    \/\/ It's critical that this step operate quickly as\n-    \/\/ \"current\" still holds the outer-lock, restricting parallelism\n-    \/\/ and effectively lengthening the critical section.\n-    \/\/ Invariant: s chases t chases u.\n-    \/\/ TODO-FIXME: consider changing EntryList from a DLL to a CDLL so\n-    \/\/ we have faster access to the tail.\n-\n-    _EntryList = w;\n-    ObjectWaiter* q = nullptr;\n-    ObjectWaiter* p;\n-    for (p = w; p != nullptr; p = p->_next) {\n-      guarantee(p->TState == ObjectWaiter::TS_CXQ, \"Invariant\");\n-      p->TState = ObjectWaiter::TS_ENTER;\n-      p->_prev = q;\n-      q = p;\n-    }\n-\n-    \/\/ We need to: ST EntryList; MEMBAR #storestore; ST _owner = nullptr\n-    \/\/ The MEMBAR is satisfied by the release_store() operation in ExitEpilog().\n-\n-    \/\/ See if we can abdicate to a spinner instead of waking a thread.\n-    \/\/ A primary goal of the implementation is to reduce the\n-    \/\/ context-switch rate.\n-    if (has_successor()) continue;\n-\n-    w = _EntryList;\n-    if (w != nullptr) {\n-      guarantee(w->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n-      ExitEpilog(current, w);\n-      return;\n-    }\n@@ -1626,1 +1662,1 @@\n-static void vthread_monitor_waited_event(JavaThread *current, ObjectWaiter* node, ContinuationWrapper& cont, EventJavaMonitorWait* event, jboolean timed_out) {\n+static void vthread_monitor_waited_event(JavaThread* current, ObjectWaiter* node, ContinuationWrapper& cont, EventJavaMonitorWait* event, jboolean timed_out) {\n@@ -1783,2 +1819,2 @@\n-    \/\/ Node may be on the WaitSet, the EntryList (or cxq), or in transition\n-    \/\/ from the WaitSet to the EntryList.\n+    \/\/ Node may be on the WaitSet, or on the entry_list, or in transition\n+    \/\/ from the WaitSet to the entry_list.\n@@ -1809,1 +1845,1 @@\n-    \/\/ on the EntryList (TS_ENTER), or on the cxq (TS_CXQ).\n+    \/\/ or on the entry_list (TS_ENTER).\n@@ -1863,1 +1899,1 @@\n-      guarantee(v == ObjectWaiter::TS_ENTER || v == ObjectWaiter::TS_CXQ, \"invariant\");\n+      guarantee(v == ObjectWaiter::TS_ENTER, \"invariant\");\n@@ -1909,2 +1945,2 @@\n-\/\/ If the lock is cool (cxq == null && succ == null) and we're on an MP system\n-\/\/ then instead of transferring a thread from the WaitSet to the EntryList\n+\/\/ If the lock is cool (entry_list == null && succ == null) and we're on an MP system\n+\/\/ then instead of transferring a thread from the WaitSet to the entry_list\n@@ -1919,5 +1955,0 @@\n-    \/\/ Disposition - what might we do with iterator ?\n-    \/\/ a.  add it directly to the EntryList - either tail (policy == 1)\n-    \/\/     or head (policy == 0).\n-    \/\/ b.  push it onto the front of the _cxq (policy == 2).\n-    \/\/ For now we use (b).\n@@ -1941,2 +1972,0 @@\n-    iterator->TState = ObjectWaiter::TS_ENTER;\n-\n@@ -1945,0 +1974,1 @@\n+    add_to_entry_list(current, iterator);\n@@ -1946,24 +1976,2 @@\n-    ObjectWaiter* list = _EntryList;\n-    if (list != nullptr) {\n-      assert(list->_prev == nullptr, \"invariant\");\n-      assert(list->TState == ObjectWaiter::TS_ENTER, \"invariant\");\n-      assert(list != iterator, \"invariant\");\n-    }\n-\n-    \/\/ prepend to cxq\n-    if (list == nullptr) {\n-      iterator->_next = iterator->_prev = nullptr;\n-      _EntryList = iterator;\n-    } else {\n-      iterator->TState = ObjectWaiter::TS_CXQ;\n-      for (;;) {\n-        ObjectWaiter* front = _cxq;\n-        iterator->_next = front;\n-        if (Atomic::cmpxchg(&_cxq, front, iterator) == front) {\n-          break;\n-        }\n-      }\n-    }\n-\n-    \/\/ _WaitSetLock protects the wait queue, not the EntryList.  We could\n-    \/\/ move the add-to-EntryList operation, above, outside the critical section\n+    \/\/ _WaitSetLock protects the wait queue, not the entry_list.  We could\n+    \/\/ move the add-to-entry_list operation, above, outside the critical section\n@@ -2006,1 +2014,1 @@\n-\/\/ from the waitset to the EntryList. This could be done more efficiently with a\n+\/\/ from the waitset to the entry_list. This could be done more efficiently with a\n@@ -2009,2 +2017,2 @@\n-\/\/ waitset is \"ABCD\" and the EntryList is \"XYZ\". After a notifyAll() in prepend\n-\/\/ mode the waitset will be empty and the EntryList will be \"DCBAXYZ\".\n+\/\/ waitset is \"ABCD\" and the entry_list is \"XYZ\". After a notifyAll() in prepend\n+\/\/ mode the waitset will be empty and the entry_list will be \"DCBAXYZ\".\n@@ -2078,1 +2086,1 @@\n-  bool was_notified = state == ObjectWaiter::TS_ENTER || state == ObjectWaiter::TS_CXQ;\n+  bool was_notified = state == ObjectWaiter::TS_ENTER;\n@@ -2109,1 +2117,1 @@\n-    \/\/ Already moved to _cxq or _EntryList by notifier, so just add to contentions.\n+    \/\/ Already moved to _entry_list by notifier, so just add to contentions.\n@@ -2121,7 +2129,1 @@\n-\/\/ algorithm.  On high order SMP systems it would be better to start with\n-\/\/ a brief global spin and then revert to spinning locally.  In the spirit of MCS\/CLH,\n-\/\/ a contending thread could enqueue itself on the cxq and then spin locally\n-\/\/ on a thread-specific variable such as its ParkEvent._Event flag.\n-\/\/ That's left as an exercise for the reader.  Note that global spinning is\n-\/\/ not problematic on Niagara, as the L2 cache serves the interconnect and\n-\/\/ has both low latency and massive bandwidth.\n+\/\/ algorithm.\n@@ -2555,2 +2557,2 @@\n-\/\/   _EntryList = 0x0000000000000000\n-\/\/   _cxq = 0x0000000000000000\n+\/\/   _entry_list = 0x0000000000000000\n+\/\/   _entry_list_tail = 0x0000000000000000\n@@ -2583,2 +2585,2 @@\n-  st->print_cr(\"  _EntryList = \" INTPTR_FORMAT, p2i(_EntryList));\n-  st->print_cr(\"  _cxq = \" INTPTR_FORMAT, p2i(_cxq));\n+  st->print_cr(\"  _entry_list = \" INTPTR_FORMAT, p2i(_entry_list));\n+  st->print_cr(\"  _entry_list_tail = \" INTPTR_FORMAT, p2i(_entry_list_tail));\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":318,"deletions":316,"binary":false,"changes":634,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  enum TStates : uint8_t { TS_UNDEF, TS_READY, TS_RUN, TS_WAIT, TS_ENTER, TS_CXQ };\n+  enum TStates : uint8_t { TS_UNDEF, TS_READY, TS_RUN, TS_WAIT, TS_ENTER };\n@@ -75,0 +75,18 @@\n+\n+  ObjectWaiter* const badObjectWaiterPtr = (ObjectWaiter*) 0xBAD;\n+  void set_bad_pointers() {\n+#ifdef ASSERT\n+    \/\/ Diagnostic hygiene ...\n+    this->_prev  = badObjectWaiterPtr;\n+    this->_next  = badObjectWaiterPtr;\n+    this->TState = ObjectWaiter::TS_RUN;\n+#endif\n+  }\n+  ObjectWaiter* next() {\n+    assert (_next != badObjectWaiterPtr, \"corrupted list!\");\n+    return _next;\n+  }\n+  ObjectWaiter* prev() {\n+    assert (_prev != badObjectWaiterPtr, \"corrupted list!\");\n+    return _prev;\n+  }\n@@ -123,1 +141,1 @@\n-\/\/   _recursions, _EntryList, _cxq, and _succ, all of which may be\n+\/\/   _recursions, _entry_list and _succ, all of which may be\n@@ -176,5 +194,4 @@\n-  ObjectWaiter* volatile _EntryList;  \/\/ Threads blocked on entry or reentry.\n-                                      \/\/ The list is actually composed of WaitNodes,\n-                                      \/\/ acting as proxies for Threads.\n-\n-  ObjectWaiter* volatile _cxq;      \/\/ LL of recently-arrived threads blocked on entry.\n+  ObjectWaiter* volatile _entry_list;  \/\/ Threads blocked on entry or reentry.\n+                                       \/\/ The list is actually composed of WaitNodes,\n+                                       \/\/ acting as proxies for Threads.\n+  ObjectWaiter* volatile _entry_list_tail; \/\/ _entry_list is the head, this is the tail.\n@@ -248,1 +265,0 @@\n-  static ByteSize cxq_offset()         { return byte_offset_of(ObjectMonitor, _cxq); }\n@@ -250,1 +266,1 @@\n-  static ByteSize EntryList_offset()   { return byte_offset_of(ObjectMonitor, _EntryList); }\n+  static ByteSize entry_list_offset()  { return byte_offset_of(ObjectMonitor, _entry_list); }\n@@ -278,1 +294,1 @@\n-    intptr_t ret_code = intptr_t(_waiters) | intptr_t(_cxq) | intptr_t(_EntryList);\n+    intptr_t ret_code = intptr_t(_waiters) | intptr_t(_entry_list);\n@@ -320,1 +336,1 @@\n-  \/\/ from _cxq\/_EntryList by the current owner when releasing the monitor,\n+  \/\/ from _entry_list by the current owner when releasing the monitor,\n@@ -422,0 +438,1 @@\n+  void      add_to_entry_list(JavaThread* current, ObjectWaiter* node);\n@@ -429,0 +446,1 @@\n+  ObjectWaiter* entry_list_tail(JavaThread* current);\n@@ -438,0 +456,1 @@\n+  bool           try_lock_or_add_to_entry_list(JavaThread* current, ObjectWaiter* node);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":30,"deletions":11,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -369,1 +369,1 @@\n-      \/\/ to the entrylist here and now, avoiding the slow-path.\n+      \/\/ to the entry_list here and now, avoiding the slow-path.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}