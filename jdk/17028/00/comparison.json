{"files":[{"patch":"@@ -1476,19 +1476,0 @@\n-\/\/ Support class used to generate HPROF_GC_CLASS_DUMP records\n-\n-class ClassDumper : public KlassClosure {\n- private:\n-  AbstractDumpWriter* _writer;\n-  AbstractDumpWriter* writer() const { return _writer; }\n-\n- public:\n-  ClassDumper(AbstractDumpWriter* writer) : _writer(writer) {}\n-\n-  void do_klass(Klass* k) {\n-    if (k->is_instance_klass()) {\n-      DumperSupport::dump_instance_class(writer(), k);\n-    } else {\n-      DumperSupport::dump_array_class(writer(), k);\n-    }\n-  }\n-};\n-\n@@ -1882,6 +1863,0 @@\n-\/\/ Callback to dump thread-related data for unmounted virtual threads;\n-\/\/ implemented by VM_HeapDumper.\n-class UnmountedVThreadDumper {\n- public:\n-  virtual void dump_vthread(oop vt, AbstractDumpWriter* segment_writer) = 0;\n-};\n@@ -1889,1 +1864,3 @@\n-\/\/ Support class used when iterating over the heap.\n+class VM_HeapDumper;\n+\n+\/\/ Support class using when iterating over the heap.\n@@ -1894,1 +1871,0 @@\n-  UnmountedVThreadDumper* _vthread_dumper;\n@@ -1899,2 +1875,3 @@\n-  HeapObjectDumper(AbstractDumpWriter* writer, UnmountedVThreadDumper* vthread_dumper)\n-    : _writer(writer), _vthread_dumper(vthread_dumper) {}\n+  HeapObjectDumper(AbstractDumpWriter* writer) {\n+    _writer = writer;\n+  }\n@@ -1921,3 +1898,0 @@\n-    if (java_lang_VirtualThread::is_instance(o) && ThreadDumper::should_dump_vthread(o)) {\n-      _vthread_dumper->dump_vthread(o, writer());\n-    }\n@@ -1937,2 +1911,0 @@\n-   Mutex* _global_writer_lock;\n-\n@@ -1942,2 +1914,0 @@\n-   bool   _started; \/\/ VM dumper started and acquired global writer lock\n-\n@@ -1946,6 +1916,1 @@\n-     \/\/ _lock and _global_writer_lock are used for synchronization between GC worker threads inside safepoint,\n-     \/\/ so we lock with _no_safepoint_check_flag.\n-     \/\/ signal_start() acquires _lock when global writer is locked,\n-     \/\/ its rank must be less than _global_writer_lock rank.\n-     _lock(new (std::nothrow) PaddedMonitor(Mutex::nosafepoint - 1, \"DumperController_lock\")),\n-     _global_writer_lock(new (std::nothrow) Mutex(Mutex::nosafepoint, \"DumpWriter_lock\")),\n+     _lock(new (std::nothrow) PaddedMonitor(Mutex::safepoint, \"DumperController_lock\")),\n@@ -1953,16 +1918,1 @@\n-     _complete_number(0),\n-     _started(false)\n-   {}\n-\n-   ~DumperController() {\n-     delete _lock;\n-     delete _global_writer_lock;\n-   }\n-\n-   \/\/ parallel (non VM) dumpers must wait until VM dumper acquires global writer lock\n-   void wait_for_start_signal() {\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     while (_started == false) {\n-       ml.wait();\n-     }\n-   }\n+     _complete_number(0) { }\n@@ -1970,13 +1920,1 @@\n-   void signal_start() {\n-     MonitorLocker ml(_lock, Mutex::_no_safepoint_check_flag);\n-     _started = true;\n-     ml.notify_all();\n-   }\n-\n-   void lock_global_writer() {\n-     _global_writer_lock->lock_without_safepoint_check();\n-   }\n-\n-   void unlock_global_writer() {\n-     _global_writer_lock->unlock();\n-   }\n+   ~DumperController() { delete _lock; }\n@@ -2011,1 +1949,1 @@\n-  void merge_file(const char* path);\n+  void merge_file(char* path);\n@@ -2023,4 +1961,0 @@\n-\n-  \/\/ returns path for the parallel DumpWriter (resource allocated)\n-  static char* get_writer_path(const char* base_path, int seq);\n-\n@@ -2029,16 +1963,0 @@\n-char* DumpMerger::get_writer_path(const char* base_path, int seq) {\n-  \/\/ approximate required buffer size\n-  size_t buf_size = strlen(base_path)\n-                    + 2                 \/\/ \".p\"\n-                    + 10                \/\/ number (that's enough for 2^32 parallel dumpers)\n-                    + 1;                \/\/ '\\0'\n-\n-  char* path = NEW_RESOURCE_ARRAY(char, buf_size);\n-  memset(path, 0, buf_size);\n-\n-  os::snprintf(path, buf_size, \"%s.p%d\", base_path, seq);\n-\n-  return path;\n-}\n-\n-\n@@ -2065,1 +1983,1 @@\n-void DumpMerger::merge_file(const char* path) {\n+void DumpMerger::merge_file(char* path) {\n@@ -2103,1 +2021,1 @@\n-void DumpMerger::merge_file(const char* path) {\n+void DumpMerger::merge_file(char* path) {\n@@ -2139,0 +2057,1 @@\n+  char path[JVM_MAXPATHLEN];\n@@ -2140,2 +2059,2 @@\n-    ResourceMark rm;\n-    const char* path = get_writer_path(_path, i);\n+    memset(path, 0, JVM_MAXPATHLEN);\n+    os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", _path, i);\n@@ -2171,1 +2090,1 @@\n-class VM_HeapDumper : public VM_GC_Operation, public WorkerTask, public UnmountedVThreadDumper {\n+class VM_HeapDumper : public VM_GC_Operation, public WorkerTask {\n@@ -2191,3 +2110,2 @@\n-\n-  \/\/ Dumper id of VMDumper thread.\n-  static const int VMDumperId = 0;\n+  \/\/ worker id of VMDumper thread.\n+  static const size_t VMDumperWorkerId = 0;\n@@ -2195,5 +2113,1 @@\n-  static bool is_vm_dumper(int dumper_id) { return dumper_id == VMDumperId; }\n-  \/\/ the 1st dumper calling get_next_dumper_id becomes VM dumper\n-  int get_next_dumper_id() {\n-    return Atomic::fetch_then_add(&_dump_seq, 1);\n-  }\n+  static bool is_vm_dumper(uint worker_id) { return worker_id == VMDumperWorkerId; }\n@@ -2218,1 +2132,4 @@\n-  \/\/ writes a HPROF_LOAD_CLASS record to global writer\n+  \/\/ create dump writer for every parallel dump thread\n+  DumpWriter* create_local_writer();\n+\n+  \/\/ writes a HPROF_LOAD_CLASS record\n@@ -2221,0 +2138,3 @@\n+  \/\/ writes a HPROF_GC_CLASS_DUMP record for the given class\n+  static void do_class_dump(Klass* k);\n+\n@@ -2222,1 +2142,1 @@\n-  void dump_threads(AbstractDumpWriter* writer);\n+  void dump_threads();\n@@ -2233,1 +2153,1 @@\n-  void dump_stack_traces(AbstractDumpWriter* writer);\n+  void dump_stack_traces();\n@@ -2251,1 +2171,1 @@\n-    _dump_seq = VMDumperId;\n+    _dump_seq = 0;\n@@ -2285,1 +2205,1 @@\n-  void prepare_parallel_dump(WorkerThreads* workers);\n+  bool can_parallel_dump(WorkerThreads* workers);\n@@ -2291,3 +2211,0 @@\n-\n-  \/\/ UnmountedVThreadDumper implementation\n-  void dump_vthread(oop vt, AbstractDumpWriter* segment_writer);\n@@ -2337,0 +2254,9 @@\n+\/\/ writes a HPROF_GC_CLASS_DUMP record for the given class\n+void VM_HeapDumper::do_class_dump(Klass* k) {\n+  if (k->is_instance_klass()) {\n+    DumperSupport::dump_instance_class(writer(), k);\n+  } else {\n+    DumperSupport::dump_array_class(writer(), k);\n+  }\n+}\n+\n@@ -2339,5 +2265,5 @@\n-void VM_HeapDumper::dump_threads(AbstractDumpWriter* writer) {\n-  for (int i = 0; i < _thread_dumpers_count; i++) {\n-    _thread_dumpers[i]->dump_thread_obj(writer);\n-    _thread_dumpers[i]->dump_stack_refs(writer);\n-  }\n+void VM_HeapDumper::dump_threads() {\n+    for (int i = 0; i < _thread_dumpers_count; i++) {\n+        _thread_dumpers[i]->dump_thread_obj(writer());\n+        _thread_dumpers[i]->dump_stack_refs(writer());\n+    }\n@@ -2357,1 +2283,2 @@\n-void VM_HeapDumper::prepare_parallel_dump(WorkerThreads* workers) {\n+bool VM_HeapDumper::can_parallel_dump(WorkerThreads* workers) {\n+  bool can_parallel = true;\n@@ -2363,0 +2290,1 @@\n+    can_parallel = false;\n@@ -2364,1 +2292,9 @@\n-    _num_dumper_threads = clamp(num_requested_dump_threads, 2U, num_active_workers);\n+    \/\/ check if we have extra path room to accommodate segmented heap files\n+    const char* base_path = writer()->get_file_path();\n+    assert(base_path != nullptr, \"sanity check\");\n+    if ((strlen(base_path) + 7\/*.p\\d\\d\\d\\d\\0*\/) >= JVM_MAXPATHLEN) {\n+      _num_dumper_threads = 1;\n+      can_parallel = false;\n+    } else {\n+      _num_dumper_threads = clamp(num_requested_dump_threads, 2U, num_active_workers);\n+    }\n@@ -2366,2 +2302,1 @@\n-  _dumper_controller = new (std::nothrow) DumperController(_num_dumper_threads);\n-  bool can_parallel = _num_dumper_threads > 1;\n+\n@@ -2372,0 +2307,1 @@\n+  return can_parallel;\n@@ -2419,4 +2355,2 @@\n-  prepare_parallel_dump(workers);\n-\n-  if (!is_parallel_dump()) {\n-    work(VMDumperId);\n+  if (!can_parallel_dump(workers)) {\n+    work(VMDumperWorkerId);\n@@ -2424,0 +2358,2 @@\n+    uint heap_only_dumper_threads = _num_dumper_threads - 1 \/* VMDumper thread *\/;\n+    _dumper_controller = new (std::nothrow) DumperController(heap_only_dumper_threads);\n@@ -2435,3 +2371,4 @@\n-void VM_HeapDumper::work(uint worker_id) {\n-  \/\/ VM Dumper works on all non-heap data dumping and part of heap iteration.\n-  int dumper_id = get_next_dumper_id();\n+\/\/ prepare DumpWriter for every parallel dump thread\n+DumpWriter* VM_HeapDumper::create_local_writer() {\n+  char* path = NEW_RESOURCE_ARRAY(char, JVM_MAXPATHLEN);\n+  memset(path, 0, JVM_MAXPATHLEN);\n@@ -2439,7 +2376,11 @@\n-  if (is_vm_dumper(dumper_id)) {\n-    \/\/ lock global writer, it will be unlocked after VM Dumper finishes with non-heap data\n-    _dumper_controller->lock_global_writer();\n-    _dumper_controller->signal_start();\n-  } else {\n-    _dumper_controller->wait_for_start_signal();\n-  }\n+  \/\/ generate segmented heap file path\n+  const char* base_path = writer()->get_file_path();\n+  \/\/ share global compressor, local DumpWriter is not responsible for its life cycle\n+  AbstractCompressor* compressor = writer()->compressor();\n+  int seq = Atomic::fetch_then_add(&_dump_seq, 1);\n+  os::snprintf(path, JVM_MAXPATHLEN, \"%s.p%d\", base_path, seq);\n+\n+  \/\/ create corresponding writer for that\n+  DumpWriter* local_writer = new DumpWriter(path, writer()->is_overwrite(), compressor);\n+  return local_writer;\n+}\n@@ -2447,1 +2388,3 @@\n-  if (is_vm_dumper(dumper_id)) {\n+void VM_HeapDumper::work(uint worker_id) {\n+  \/\/ VM Dumper works on all non-heap data dumping and part of heap iteration.\n+  if (is_vm_dumper(worker_id)) {\n@@ -2469,5 +2412,1 @@\n-    dump_stack_traces(writer());\n-\n-    \/\/ unlock global writer, so parallel dumpers can dump stack traces of unmounted virtual threads\n-    _dumper_controller->unlock_global_writer();\n-  }\n+    dump_stack_traces();\n@@ -2475,1 +2414,1 @@\n-  \/\/ HPROF_HEAP_DUMP\/HPROF_HEAP_DUMP_SEGMENT starts here\n+    \/\/ HPROF_HEAP_DUMP\/HPROF_HEAP_DUMP_SEGMENT starts here\n@@ -2477,26 +2416,4 @@\n-  ResourceMark rm;\n-  \/\/ share global compressor, local DumpWriter is not responsible for its life cycle\n-  DumpWriter segment_writer(DumpMerger::get_writer_path(writer()->get_file_path(), dumper_id),\n-                            writer()->is_overwrite(), writer()->compressor());\n-  if (!segment_writer.has_error()) {\n-    if (is_vm_dumper(dumper_id)) {\n-      \/\/ dump some non-heap subrecords to heap dump segment\n-      TraceTime timer(\"Dump non-objects (part 2)\", TRACETIME_LOG(Info, heapdump));\n-      \/\/ Writes HPROF_GC_CLASS_DUMP records\n-      ClassDumper class_dumper(&segment_writer);\n-      ClassLoaderDataGraph::classes_do(&class_dumper);\n-\n-      \/\/ HPROF_GC_ROOT_THREAD_OBJ + frames + jni locals\n-      dump_threads(&segment_writer);\n-\n-      \/\/ HPROF_GC_ROOT_JNI_GLOBAL\n-      JNIGlobalsDumper jni_dumper(&segment_writer);\n-      JNIHandles::oops_do(&jni_dumper);\n-      \/\/ technically not jni roots, but global roots\n-      \/\/ for things like preallocated throwable backtraces\n-      Universe::vm_global()->oops_do(&jni_dumper);\n-      \/\/ HPROF_GC_ROOT_STICKY_CLASS\n-      \/\/ These should be classes in the null class loader data, and not all classes\n-      \/\/ if !ClassUnloading\n-      StickyClassDumper stiky_class_dumper(&segment_writer);\n-      ClassLoaderData::the_null_class_loader_data()->classes_do(&stiky_class_dumper);\n+    \/\/ Writes HPROF_GC_CLASS_DUMP records\n+    {\n+      LockedClassesDo locked_dump_class(&do_class_dump);\n+      ClassLoaderDataGraph::classes_do(&locked_dump_class);\n@@ -2505,14 +2422,41 @@\n-    \/\/ Heap iteration.\n-    \/\/ writes HPROF_GC_INSTANCE_DUMP records.\n-    \/\/ After each sub-record is written check_segment_length will be invoked\n-    \/\/ to check if the current segment exceeds a threshold. If so, a new\n-    \/\/ segment is started.\n-    \/\/ The HPROF_GC_CLASS_DUMP and HPROF_GC_INSTANCE_DUMP are the vast bulk\n-    \/\/ of the heap dump.\n-\n-    TraceTime timer(is_parallel_dump() ? \"Dump heap objects in parallel\" : \"Dump heap objects\", TRACETIME_LOG(Info, heapdump));\n-    HeapObjectDumper obj_dumper(&segment_writer, this);\n-    if (!is_parallel_dump()) {\n-      Universe::heap()->object_iterate(&obj_dumper);\n-    } else {\n-      \/\/ == Parallel dump\n+    \/\/ HPROF_GC_ROOT_THREAD_OBJ + frames + jni locals\n+    dump_threads();\n+\n+    \/\/ HPROF_GC_ROOT_JNI_GLOBAL\n+    JNIGlobalsDumper jni_dumper(writer());\n+    JNIHandles::oops_do(&jni_dumper);\n+    \/\/ technically not jni roots, but global roots\n+    \/\/ for things like preallocated throwable backtraces\n+    Universe::vm_global()->oops_do(&jni_dumper);\n+    \/\/ HPROF_GC_ROOT_STICKY_CLASS\n+    \/\/ These should be classes in the null class loader data, and not all classes\n+    \/\/ if !ClassUnloading\n+    StickyClassDumper class_dumper(writer());\n+    ClassLoaderData::the_null_class_loader_data()->classes_do(&class_dumper);\n+  }\n+\n+  \/\/ Heap iteration.\n+  \/\/ writes HPROF_GC_INSTANCE_DUMP records.\n+  \/\/ After each sub-record is written check_segment_length will be invoked\n+  \/\/ to check if the current segment exceeds a threshold. If so, a new\n+  \/\/ segment is started.\n+  \/\/ The HPROF_GC_CLASS_DUMP and HPROF_GC_INSTANCE_DUMP are the vast bulk\n+  \/\/ of the heap dump.\n+  if (!is_parallel_dump()) {\n+    assert(is_vm_dumper(worker_id), \"must be\");\n+    \/\/ == Serial dump\n+    ResourceMark rm;\n+    TraceTime timer(\"Dump heap objects\", TRACETIME_LOG(Info, heapdump));\n+    HeapObjectDumper obj_dumper(writer());\n+    Universe::heap()->object_iterate(&obj_dumper);\n+    writer()->finish_dump_segment();\n+    \/\/ Writes the HPROF_HEAP_DUMP_END record because merge does not happen in serial dump\n+    DumperSupport::end_of_dump(writer());\n+    writer()->flush();\n+  } else {\n+    \/\/ == Parallel dump\n+    ResourceMark rm;\n+    TraceTime timer(\"Dump heap objects in parallel\", TRACETIME_LOG(Info, heapdump));\n+    DumpWriter* local_writer = is_vm_dumper(worker_id) ? writer() : create_local_writer();\n+    if (!local_writer->has_error()) {\n+      HeapObjectDumper obj_dumper(local_writer);\n@@ -2520,0 +2464,9 @@\n+      local_writer->finish_dump_segment();\n+      local_writer->flush();\n+    }\n+    if (is_vm_dumper(worker_id)) {\n+      _dumper_controller->wait_all_dumpers_complete();\n+    } else {\n+      _dumper_controller->dumper_complete(local_writer, writer());\n+      delete local_writer;\n+      return;\n@@ -2521,15 +2474,0 @@\n-\n-    segment_writer.finish_dump_segment();\n-    segment_writer.flush();\n-  }\n-\n-  _dumper_controller->dumper_complete(&segment_writer, writer());\n-\n-  if (is_vm_dumper(dumper_id)) {\n-    _dumper_controller->wait_all_dumpers_complete();\n-\n-    \/\/ flush global writer\n-    writer()->flush();\n-\n-    \/\/ At this point, all fragments of the heapdump have been written to separate files.\n-    \/\/ We need to merge them into a complete heapdump and write HPROF_HEAP_DUMP_END at that time.\n@@ -2537,0 +2475,2 @@\n+  \/\/ At this point, all fragments of the heapdump have been written to separate files.\n+  \/\/ We need to merge them into a complete heapdump and write HPROF_HEAP_DUMP_END at that time.\n@@ -2539,1 +2479,1 @@\n-void VM_HeapDumper::dump_stack_traces(AbstractDumpWriter* writer) {\n+void VM_HeapDumper::dump_stack_traces() {\n@@ -2541,4 +2481,4 @@\n-  DumperSupport::write_header(writer, HPROF_TRACE, 3 * sizeof(u4));\n-  writer->write_u4((u4)STACK_TRACE_ID);\n-  writer->write_u4(0);                    \/\/ thread number\n-  writer->write_u4(0);                    \/\/ frame count\n+  DumperSupport::write_header(writer(), HPROF_TRACE, 3 * sizeof(u4));\n+  writer()->write_u4((u4)STACK_TRACE_ID);\n+  writer()->write_u4(0);                    \/\/ thread number\n+  writer()->write_u4(0);                    \/\/ frame count\n@@ -2568,1 +2508,1 @@\n-        thread_dumper->dump_stack_traces(writer, _klass_map);\n+        thread_dumper->dump_stack_traces(writer(), _klass_map);\n@@ -2578,1 +2518,1 @@\n-      thread_dumper->dump_stack_traces(writer, _klass_map);\n+      thread_dumper->dump_stack_traces(writer(), _klass_map);\n@@ -2583,16 +2523,0 @@\n-void VM_HeapDumper::dump_vthread(oop vt, AbstractDumpWriter* segment_writer) {\n-  \/\/ unmounted vthread has no JavaThread\n-  ThreadDumper thread_dumper(ThreadDumper::ThreadType::UnmountedVirtual, nullptr, vt);\n-  thread_dumper.init_serial_nums(&_thread_serial_num, &_frame_serial_num);\n-\n-  \/\/ write HPROF_TRACE\/HPROF_FRAME records to global writer\n-  _dumper_controller->lock_global_writer();\n-  thread_dumper.dump_stack_traces(writer(), _klass_map);\n-  _dumper_controller->unlock_global_writer();\n-\n-  \/\/ write HPROF_GC_ROOT_THREAD_OBJ\/HPROF_GC_ROOT_JAVA_FRAME\/HPROF_GC_ROOT_JNI_LOCAL subrecord\n-  \/\/ to segment writer\n-  thread_dumper.dump_thread_obj(segment_writer);\n-  thread_dumper.dump_stack_refs(segment_writer);\n-}\n-\n@@ -2640,1 +2564,3 @@\n-  \/\/ Heap dump process is done in two phases\n+  \/\/ For serial dump, once VM_HeapDumper completes, the whole heap dump process\n+  \/\/ is done, no further phases needed. For parallel dump, the whole heap dump\n+  \/\/ process is done in two phases\n@@ -2647,14 +2573,13 @@\n-\n-  DumpMerger merger(path, &writer, dumper.dump_seq());\n-  Thread* current_thread = Thread::current();\n-  if (current_thread->is_AttachListener_thread()) {\n-    \/\/ perform heapdump file merge operation in the current thread prevents us\n-    \/\/ from occupying the VM Thread, which in turn affects the occurrence of\n-    \/\/ GC and other VM operations.\n-    merger.do_merge();\n-  } else {\n-    \/\/ otherwise, performs it by VM thread\n-    VM_HeapDumpMerge op(&merger);\n-    VMThread::execute(&op);\n-  }\n-  if (writer.error() != nullptr) {\n+  if (dumper.is_parallel_dump()) {\n+    DumpMerger merger(path, &writer, dumper.dump_seq());\n+    Thread* current_thread = Thread::current();\n+    if (current_thread->is_AttachListener_thread()) {\n+      \/\/ perform heapdump file merge operation in the current thread prevents us\n+      \/\/ from occupying the VM Thread, which in turn affects the occurrence of\n+      \/\/ GC and other VM operations.\n+      merger.do_merge();\n+    } else {\n+      \/\/ otherwise, performs it by VM thread\n+      VM_HeapDumpMerge op(&merger);\n+      VMThread::execute(&op);\n+    }\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":161,"deletions":236,"binary":false,"changes":397,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+            appOut.shouldNotContain(\"Merge heap files complete\");\n","filename":"test\/hotspot\/jtreg\/serviceability\/dcmd\/gc\/HeapDumpParallelTest.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -186,1 +186,0 @@\n-            extraVMArgs.add(\"-Xlog:heapdump\");\n@@ -256,1 +255,2 @@\n-            test(snapshot, VThreadInHeapDumpTarg.VThreadUnmountedReferenced.class);\n+            \/\/ Dumping of unmounted vthreads is not implemented yet\n+            \/\/test(snapshot, VThreadInHeapDumpTarg.VThreadUnmountedReferenced.class);\n","filename":"test\/hotspot\/jtreg\/serviceability\/jvmti\/vthread\/HeapDump\/VThreadInHeapDump.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}