{"files":[{"patch":"@@ -8239,0 +8239,10 @@\n+instruct castHH(vRegF dst)\n+%{\n+  match(Set dst (CastHH dst));\n+  size(0);\n+  format %{ \"# castHH of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(pipe_class_empty);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n-\/\/ Copyright (c) 2020, 2024, Arm Limited. All rights reserved.\n+\/\/ Copyright (c) 2020, 2025, Arm Limited. All rights reserved.\n@@ -228,0 +228,20 @@\n+      case Op_AddVHF:\n+      case Op_SubVHF:\n+      case Op_MulVHF:\n+      case Op_DivVHF:\n+      case Op_MinVHF:\n+      case Op_MaxVHF:\n+      case Op_SqrtVHF:\n+        \/\/ FEAT_FP16 is enabled if both \"fphp\" and \"asimdhp\" features are supported.\n+        \/\/ Only the Neon instructions need this check. SVE supports half-precision floats\n+        \/\/ by default.\n+        if (UseSVE == 0 && !is_feat_fp16_supported()) {\n+          return false;\n+        }\n+        break;\n+      case Op_FmaVHF:\n+        \/\/ UseFMA flag needs to be checked along with FEAT_FP16\n+        if (!UseFMA || (UseSVE == 0 && !is_feat_fp16_supported())) {\n+          return false;\n+        }\n+        break;\n@@ -586,0 +606,16 @@\n+instruct vaddHF(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (AddVHF src1 src2));\n+  format %{ \"vaddHF $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ fadd($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_fadd($dst$$FloatRegister, __ H, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -660,0 +696,10 @@\n+instruct vaddHF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (AddVHF (Binary dst_src1 src2) pg));\n+  format %{ \"vaddHF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fadd($dst_src1$$FloatRegister, __ H, $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -810,0 +856,16 @@\n+instruct vsubHF(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (SubVHF src1 src2));\n+  format %{ \"vsubHF $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ fsub($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_fsub($dst$$FloatRegister, __ H, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -884,0 +946,10 @@\n+instruct vsubHF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (SubVHF (Binary dst_src1 src2) pg));\n+  format %{ \"vsubHF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fsub($dst_src1$$FloatRegister, __ H, $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1007,0 +1079,16 @@\n+instruct vmulHF(vReg dst, vReg src1, vReg src2) %{\n+  match(Set dst (MulVHF src1 src2));\n+  format %{ \"vmulHF $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ fmul($dst$$FloatRegister, get_arrangement(this),\n+              $src1$$FloatRegister, $src2$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_fmul($dst$$FloatRegister, __ H, $src1$$FloatRegister, $src2$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1081,0 +1169,10 @@\n+instruct vmulHF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MulVHF (Binary dst_src1 src2) pg));\n+  format %{ \"vmulHF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fmul($dst_src1$$FloatRegister, __ H, $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1105,0 +1203,22 @@\n+instruct vdivHF_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst (DivVHF src1 src2));\n+  format %{ \"vdivHF_neon $dst, $src1, $src2\" %}\n+  ins_encode %{\n+    __ fdiv($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vdivHF_sve(vReg dst_src1, vReg src2) %{\n+  predicate(!VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst_src1 (DivVHF dst_src1 src2));\n+  format %{ \"vdivHF_sve $dst_src1, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_fdiv($dst_src1$$FloatRegister, __ H, ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1151,0 +1271,10 @@\n+instruct vdivHF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (DivVHF (Binary dst_src1 src2) pg));\n+  format %{ \"vdivHF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fdiv($dst_src1$$FloatRegister, __ H, $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -1994,0 +2124,15 @@\n+instruct vsqrtHF(vReg dst, vReg src) %{\n+  match(Set dst (SqrtVHF src));\n+  format %{ \"vsqrtHF $dst, $src\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ fsqrt($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+    } else {\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_fsqrt($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2026,0 +2171,10 @@\n+instruct vsqrtHF_masked(vReg dst_src, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src (SqrtVHF dst_src pg));\n+  format %{ \"vsqrtHF_masked $dst_src, $pg, $dst_src\" %}\n+  ins_encode %{\n+    __ sve_fsqrt($dst_src$$FloatRegister, __ H, $pg$$PRegister, $dst_src$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2072,1 +2227,1 @@\n-\/\/ vector min - B\/S\/I\/F\/D\n+\/\/ vector min - B\/S\/I\/HF\/F\/D\n@@ -2113,0 +2268,23 @@\n+instruct vmin_HF_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst (MinVHF src1 src2));\n+  format %{ \"vmin_HF_neon $dst, $src1, $src2\\t# Half float\" %}\n+  ins_encode %{\n+    __ fmin($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmin_HF_sve(vReg dst_src1, vReg src2) %{\n+  predicate(!VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst_src1 (MinVHF dst_src1 src2));\n+  format %{ \"vmin_HF_sve $dst_src1, $dst_src1, $src2\\t# Half float\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_fmin($dst_src1$$FloatRegister, __ H,\n+                ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2133,0 +2311,11 @@\n+instruct vmin_HF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MinVHF (Binary dst_src1 src2) pg));\n+  format %{ \"vmin_HF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fmin($dst_src1$$FloatRegister, __ H,\n+                $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2229,1 +2418,1 @@\n-\/\/ vector max - B\/S\/I\/F\/D\n+\/\/ vector max - B\/S\/I\/HF\/F\/D\n@@ -2270,0 +2459,23 @@\n+instruct vmax_HF_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst (MaxVHF src1 src2));\n+  format %{ \"vmax_HF_neon $dst, $src1, $src2\\t# Half float\" %}\n+  ins_encode %{\n+    __ fmax($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct vmax_HF_sve(vReg dst_src1, vReg src2) %{\n+  predicate(!VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst_src1 (MaxVHF dst_src1 src2));\n+  format %{ \"vmax_HF_sve $dst_src1, $dst_src1, $src2\\t# Half float\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ sve_fmax($dst_src1$$FloatRegister, __ H,\n+                ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2290,0 +2502,11 @@\n+instruct vmax_HF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 (MaxVHF (Binary dst_src1 src2) pg));\n+  format %{ \"vmax_HF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ sve_fmax($dst_src1$$FloatRegister, __ H,\n+                $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -2416,2 +2639,3 @@\n-  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 src3)));\n-  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVHF dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVF  dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVD  dst_src1 (Binary src2 src3)));\n@@ -2440,2 +2664,3 @@\n-  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n-  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVHF (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVF  (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVD  (Binary dst_src1 src2) (Binary src3 pg)));\n@@ -4616,0 +4841,17 @@\n+\/\/ Replicate a half-precision float value held in a floating point register\n+instruct replicateHF(vReg dst, vRegF src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (Replicate src));\n+  format %{ \"replicateHF $dst, $src\\t# replicate half-precision float\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ dup($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+    } else { \/\/ length_in_bytes must be > 16 and SVE should be enabled\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_cpy($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -4684,0 +4926,17 @@\n+\/\/ Replicate a 16-bit half precision float value\n+instruct replicateHF_imm(vReg dst, immH con) %{\n+  match(Set dst (Replicate con));\n+  format %{ \"replicateHF_imm $dst, $con\\t# replicate immediate half-precision float\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    int imm = (int)($con$$constant) & 0xffff;\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ mov($dst$$FloatRegister, get_arrangement(this), imm);\n+    } else { \/\/ length_in_bytes must be > 16 and SVE should be enabled\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_dup($dst$$FloatRegister, __ H, imm);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector.ad","additions":266,"deletions":7,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n-\/\/ Copyright (c) 2020, 2024, Arm Limited. All rights reserved.\n+\/\/ Copyright (c) 2020, 2025, Arm Limited. All rights reserved.\n@@ -218,0 +218,20 @@\n+      case Op_AddVHF:\n+      case Op_SubVHF:\n+      case Op_MulVHF:\n+      case Op_DivVHF:\n+      case Op_MinVHF:\n+      case Op_MaxVHF:\n+      case Op_SqrtVHF:\n+        \/\/ FEAT_FP16 is enabled if both \"fphp\" and \"asimdhp\" features are supported.\n+        \/\/ Only the Neon instructions need this check. SVE supports half-precision floats\n+        \/\/ by default.\n+        if (UseSVE == 0 && !is_feat_fp16_supported()) {\n+          return false;\n+        }\n+        break;\n+      case Op_FmaVHF:\n+        \/\/ UseFMA flag needs to be checked along with FEAT_FP16\n+        if (!UseFMA || (UseSVE == 0 && !is_feat_fp16_supported())) {\n+          return false;\n+        }\n+        break;\n@@ -511,6 +531,7 @@\n-BINARY_OP(vaddB, AddVB, addv, sve_add,  B)\n-BINARY_OP(vaddS, AddVS, addv, sve_add,  H)\n-BINARY_OP(vaddI, AddVI, addv, sve_add,  S)\n-BINARY_OP(vaddL, AddVL, addv, sve_add,  D)\n-BINARY_OP(vaddF, AddVF, fadd, sve_fadd, S)\n-BINARY_OP(vaddD, AddVD, fadd, sve_fadd, D)\n+BINARY_OP(vaddB,  AddVB,  addv, sve_add,  B)\n+BINARY_OP(vaddS,  AddVS,  addv, sve_add,  H)\n+BINARY_OP(vaddI,  AddVI,  addv, sve_add,  S)\n+BINARY_OP(vaddL,  AddVL,  addv, sve_add,  D)\n+BINARY_OP(vaddHF, AddVHF, fadd, sve_fadd, H)\n+BINARY_OP(vaddF,  AddVF,  fadd, sve_fadd, S)\n+BINARY_OP(vaddD,  AddVD,  fadd, sve_fadd, D)\n@@ -519,6 +540,7 @@\n-BINARY_OP_PREDICATE(vaddB, AddVB, sve_add,  B)\n-BINARY_OP_PREDICATE(vaddS, AddVS, sve_add,  H)\n-BINARY_OP_PREDICATE(vaddI, AddVI, sve_add,  S)\n-BINARY_OP_PREDICATE(vaddL, AddVL, sve_add,  D)\n-BINARY_OP_PREDICATE(vaddF, AddVF, sve_fadd, S)\n-BINARY_OP_PREDICATE(vaddD, AddVD, sve_fadd, D)\n+BINARY_OP_PREDICATE(vaddB,  AddVB,  sve_add,  B)\n+BINARY_OP_PREDICATE(vaddS,  AddVS,  sve_add,  H)\n+BINARY_OP_PREDICATE(vaddI,  AddVI,  sve_add,  S)\n+BINARY_OP_PREDICATE(vaddL,  AddVL,  sve_add,  D)\n+BINARY_OP_PREDICATE(vaddHF, AddVHF, sve_fadd, H)\n+BINARY_OP_PREDICATE(vaddF,  AddVF,  sve_fadd, S)\n+BINARY_OP_PREDICATE(vaddD,  AddVD,  sve_fadd, D)\n@@ -535,6 +557,7 @@\n-BINARY_OP(vsubB, SubVB, subv, sve_sub,  B)\n-BINARY_OP(vsubS, SubVS, subv, sve_sub,  H)\n-BINARY_OP(vsubI, SubVI, subv, sve_sub,  S)\n-BINARY_OP(vsubL, SubVL, subv, sve_sub,  D)\n-BINARY_OP(vsubF, SubVF, fsub, sve_fsub, S)\n-BINARY_OP(vsubD, SubVD, fsub, sve_fsub, D)\n+BINARY_OP(vsubB,  SubVB,  subv, sve_sub,  B)\n+BINARY_OP(vsubS,  SubVS,  subv, sve_sub,  H)\n+BINARY_OP(vsubI,  SubVI,  subv, sve_sub,  S)\n+BINARY_OP(vsubL,  SubVL,  subv, sve_sub,  D)\n+BINARY_OP(vsubHF, SubVHF, fsub, sve_fsub, H)\n+BINARY_OP(vsubF,  SubVF,  fsub, sve_fsub, S)\n+BINARY_OP(vsubD,  SubVD,  fsub, sve_fsub, D)\n@@ -543,6 +566,7 @@\n-BINARY_OP_PREDICATE(vsubB, SubVB, sve_sub,  B)\n-BINARY_OP_PREDICATE(vsubS, SubVS, sve_sub,  H)\n-BINARY_OP_PREDICATE(vsubI, SubVI, sve_sub,  S)\n-BINARY_OP_PREDICATE(vsubL, SubVL, sve_sub,  D)\n-BINARY_OP_PREDICATE(vsubF, SubVF, sve_fsub, S)\n-BINARY_OP_PREDICATE(vsubD, SubVD, sve_fsub, D)\n+BINARY_OP_PREDICATE(vsubB,  SubVB,  sve_sub,  B)\n+BINARY_OP_PREDICATE(vsubS,  SubVS,  sve_sub,  H)\n+BINARY_OP_PREDICATE(vsubI,  SubVI,  sve_sub,  S)\n+BINARY_OP_PREDICATE(vsubL,  SubVL,  sve_sub,  D)\n+BINARY_OP_PREDICATE(vsubHF, SubVHF, sve_fsub, H)\n+BINARY_OP_PREDICATE(vsubF,  SubVF,  sve_fsub, S)\n+BINARY_OP_PREDICATE(vsubD,  SubVD,  sve_fsub, D)\n@@ -615,2 +639,3 @@\n-BINARY_OP(vmulF, MulVF, fmul, sve_fmul, S)\n-BINARY_OP(vmulD, MulVD, fmul, sve_fmul, D)\n+BINARY_OP(vmulHF, MulVHF, fmul, sve_fmul, H)\n+BINARY_OP(vmulF,  MulVF,  fmul, sve_fmul, S)\n+BINARY_OP(vmulD,  MulVD,  fmul, sve_fmul, D)\n@@ -619,6 +644,7 @@\n-BINARY_OP_PREDICATE(vmulB, MulVB, sve_mul,  B)\n-BINARY_OP_PREDICATE(vmulS, MulVS, sve_mul,  H)\n-BINARY_OP_PREDICATE(vmulI, MulVI, sve_mul,  S)\n-BINARY_OP_PREDICATE(vmulL, MulVL, sve_mul,  D)\n-BINARY_OP_PREDICATE(vmulF, MulVF, sve_fmul, S)\n-BINARY_OP_PREDICATE(vmulD, MulVD, sve_fmul, D)\n+BINARY_OP_PREDICATE(vmulB,  MulVB,  sve_mul,  B)\n+BINARY_OP_PREDICATE(vmulS,  MulVS,  sve_mul,  H)\n+BINARY_OP_PREDICATE(vmulI,  MulVI,  sve_mul,  S)\n+BINARY_OP_PREDICATE(vmulL,  MulVL,  sve_mul,  D)\n+BINARY_OP_PREDICATE(vmulHF, MulVHF, sve_fmul, H)\n+BINARY_OP_PREDICATE(vmulF,  MulVF,  sve_fmul, S)\n+BINARY_OP_PREDICATE(vmulD,  MulVD,  sve_fmul, D)\n@@ -629,2 +655,3 @@\n-BINARY_OP_NEON_SVE_PAIRWISE(vdivF, DivVF, fdiv, sve_fdiv, S)\n-BINARY_OP_NEON_SVE_PAIRWISE(vdivD, DivVD, fdiv, sve_fdiv, D)\n+BINARY_OP_NEON_SVE_PAIRWISE(vdivHF, DivVHF, fdiv, sve_fdiv, H)\n+BINARY_OP_NEON_SVE_PAIRWISE(vdivF,  DivVF,  fdiv, sve_fdiv, S)\n+BINARY_OP_NEON_SVE_PAIRWISE(vdivD,  DivVD,  fdiv, sve_fdiv, D)\n@@ -633,2 +660,3 @@\n-BINARY_OP_PREDICATE(vdivF, DivVF, sve_fdiv, S)\n-BINARY_OP_PREDICATE(vdivD, DivVD, sve_fdiv, D)\n+BINARY_OP_PREDICATE(vdivHF, DivVHF, sve_fdiv, H)\n+BINARY_OP_PREDICATE(vdivF,  DivVF,  sve_fdiv, S)\n+BINARY_OP_PREDICATE(vdivD,  DivVD,  sve_fdiv, D)\n@@ -1019,2 +1047,3 @@\n-UNARY_OP(vsqrtF, SqrtVF, fsqrt, sve_fsqrt, S)\n-UNARY_OP(vsqrtD, SqrtVD, fsqrt, sve_fsqrt, D)\n+UNARY_OP(vsqrtHF, SqrtVHF, fsqrt, sve_fsqrt, H)\n+UNARY_OP(vsqrtF,  SqrtVF,  fsqrt, sve_fsqrt, S)\n+UNARY_OP(vsqrtD,  SqrtVD,  fsqrt, sve_fsqrt, D)\n@@ -1023,2 +1052,3 @@\n-UNARY_OP_PREDICATE_WITH_SIZE(vsqrtF, SqrtVF, sve_fsqrt, S)\n-UNARY_OP_PREDICATE_WITH_SIZE(vsqrtD, SqrtVD, sve_fsqrt, D)\n+UNARY_OP_PREDICATE_WITH_SIZE(vsqrtHF, SqrtVHF, sve_fsqrt, H)\n+UNARY_OP_PREDICATE_WITH_SIZE(vsqrtF,  SqrtVF,  sve_fsqrt, S)\n+UNARY_OP_PREDICATE_WITH_SIZE(vsqrtD,  SqrtVD,  sve_fsqrt, D)\n@@ -1077,0 +1107,14 @@\n+dnl VMINMAX_HF_NEON($1,   $2,      $3     )\n+dnl VMINMAX_HF_NEON(type, op_name, insn_fp)\n+define(`VMINMAX_HF_NEON', `\n+instruct v$1_HF_neon(vReg dst, vReg src1, vReg src2) %{\n+  predicate(VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst ($2 src1 src2));\n+  format %{ \"v$1_HF_neon $dst, $src1, $src2\\t# Half float\" %}\n+  ins_encode %{\n+    __ $3($dst$$FloatRegister, get_arrangement(this),\n+            $src1$$FloatRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -1100,0 +1144,15 @@\n+dnl VMINMAX_HF_SVE($1,   $2,      $3     )\n+dnl VMINMAX_HF_SVE(type, op_name, insn_fp)\n+define(`VMINMAX_HF_SVE', `\n+instruct v$1_HF_sve(vReg dst_src1, vReg src2) %{\n+  predicate(!VM_Version::use_neon_for_vector(Matcher::vector_length_in_bytes(n)));\n+  match(Set dst_src1 ($2 dst_src1 src2));\n+  format %{ \"v$1_HF_sve $dst_src1, $dst_src1, $src2\\t# Half float\" %}\n+  ins_encode %{\n+    assert(UseSVE > 0, \"must be sve\");\n+    __ $3($dst_src1$$FloatRegister, __ H,\n+                ptrue, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -1172,0 +1231,14 @@\n+dnl VMINMAX_HF_PREDICATE($1,   $2,      $3     )\n+dnl VMINMAX_HF_PREDICATE(type, op_name, insn_fp)\n+define(`VMINMAX_HF_PREDICATE', `\n+instruct v$1_HF_masked(vReg dst_src1, vReg src2, pRegGov pg) %{\n+  predicate(UseSVE > 0);\n+  match(Set dst_src1 ($2 (Binary dst_src1 src2) pg));\n+  format %{ \"v$1_HF_masked $dst_src1, $pg, $dst_src1, $src2\" %}\n+  ins_encode %{\n+    __ $3($dst_src1$$FloatRegister, __ H,\n+                $pg$$PRegister, $src2$$FloatRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}')dnl\n+dnl\n@@ -1178,1 +1251,1 @@\n-\/\/ vector min - B\/S\/I\/F\/D\n+\/\/ vector min - B\/S\/I\/HF\/F\/D\n@@ -1181,0 +1254,2 @@\n+VMINMAX_HF_NEON(min, MinVHF, fmin)\n+VMINMAX_HF_SVE(min, MinVHF, sve_fmin)\n@@ -1184,0 +1259,1 @@\n+VMINMAX_HF_PREDICATE(min, MinVHF, sve_fmin)\n@@ -1202,1 +1278,1 @@\n-\/\/ vector max - B\/S\/I\/F\/D\n+\/\/ vector max - B\/S\/I\/HF\/F\/D\n@@ -1205,0 +1281,2 @@\n+VMINMAX_HF_NEON(max, MaxVHF, fmax)\n+VMINMAX_HF_SVE(max, MaxVHF, sve_fmax)\n@@ -1208,0 +1286,1 @@\n+VMINMAX_HF_PREDICATE(max, MaxVHF, sve_fmax)\n@@ -1276,2 +1355,3 @@\n-  match(Set dst_src1 (FmaVF dst_src1 (Binary src2 src3)));\n-  match(Set dst_src1 (FmaVD dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVHF dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVF  dst_src1 (Binary src2 src3)));\n+  match(Set dst_src1 (FmaVD  dst_src1 (Binary src2 src3)));\n@@ -1300,2 +1380,3 @@\n-  match(Set dst_src1 (FmaVF (Binary dst_src1 src2) (Binary src3 pg)));\n-  match(Set dst_src1 (FmaVD (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVHF (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVF  (Binary dst_src1 src2) (Binary src3 pg)));\n+  match(Set dst_src1 (FmaVD  (Binary dst_src1 src2) (Binary src3 pg)));\n@@ -2941,0 +3022,17 @@\n+\/\/ Replicate a half-precision float value held in a floating point register\n+instruct replicateHF(vReg dst, vRegF src) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_SHORT);\n+  match(Set dst (Replicate src));\n+  format %{ \"replicateHF $dst, $src\\t# replicate half-precision float\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ dup($dst$$FloatRegister, get_arrangement(this), $src$$FloatRegister);\n+    } else { \/\/ length_in_bytes must be > 16 and SVE should be enabled\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_cpy($dst$$FloatRegister, __ H, ptrue, $src$$FloatRegister);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -3009,0 +3107,17 @@\n+\/\/ Replicate a 16-bit half precision float value\n+instruct replicateHF_imm(vReg dst, immH con) %{\n+  match(Set dst (Replicate con));\n+  format %{ \"replicateHF_imm $dst, $con\\t# replicate immediate half-precision float\" %}\n+  ins_encode %{\n+    uint length_in_bytes = Matcher::vector_length_in_bytes(this);\n+    int imm = (int)($con$$constant) & 0xffff;\n+    if (VM_Version::use_neon_for_vector(length_in_bytes)) {\n+      __ mov($dst$$FloatRegister, get_arrangement(this), imm);\n+    } else { \/\/ length_in_bytes must be > 16 and SVE should be enabled\n+      assert(UseSVE > 0, \"must be sve\");\n+      __ sve_dup($dst$$FloatRegister, __ H, imm);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64_vector_ad.m4","additions":162,"deletions":47,"binary":false,"changes":209,"status":"modified"},{"patch":"@@ -243,0 +243,13 @@\n+\/\/ This method is used to generate Advanced SIMD data processing instructions\n+void Assembler::adv_simd_three_same(Instruction_aarch64 &current_insn, FloatRegister Vd,\n+                                    SIMD_Arrangement T, FloatRegister Vn, FloatRegister Vm,\n+                                    int op1, int op2, int op3) {\n+  assert(T == T4H || T == T8H || T == T2S || T == T4S || T == T2D, \"invalid arrangement\");\n+  int op22 = (T == T2S || T == T4S) ? 0b0 : 0b1;\n+  int op21 = (T == T4H || T == T8H) ? 0b0 : 0b1;\n+  int op14 = (T == T4H || T == T8H) ? 0b00 : 0b11;\n+  f(0, 31), f((int)T & 1, 30), f(op1, 29), f(0b01110, 28, 24), f(op2, 23);\n+  f(op22, 22); f(op21, 21), rf(Vm, 16), f(op14, 15, 14), f(op3, 13, 10),\n+  rf(Vn, 5), rf(Vd, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2746,20 +2746,20 @@\n-\/\/ Advanced SIMD three same\n-#define INSN(NAME, op1, op2, op3)                                                       \\\n-  void NAME(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn, FloatRegister Vm) { \\\n-    starti;                                                                             \\\n-    assert(T == T2S || T == T4S || T == T2D, \"invalid arrangement\");                    \\\n-    f(0, 31), f((int)T & 1, 30), f(op1, 29), f(0b01110, 28, 24), f(op2, 23);            \\\n-    f(T==T2D ? 1:0, 22); f(1, 21), rf(Vm, 16), f(op3, 15, 10), rf(Vn, 5), rf(Vd, 0);    \\\n-  }\n-\n-  INSN(fabd, 1, 1, 0b110101);\n-  INSN(fadd, 0, 0, 0b110101);\n-  INSN(fdiv, 1, 0, 0b111111);\n-  INSN(faddp, 1, 0, 0b110101);\n-  INSN(fmul, 1, 0, 0b110111);\n-  INSN(fsub, 0, 1, 0b110101);\n-  INSN(fmla, 0, 0, 0b110011);\n-  INSN(fmls, 0, 1, 0b110011);\n-  INSN(fmax, 0, 0, 0b111101);\n-  INSN(fmin, 0, 1, 0b111101);\n-  INSN(facgt, 1, 1, 0b111011);\n+  \/\/ Advanced SIMD three same\n+  void adv_simd_three_same(Instruction_aarch64 &current_insn, FloatRegister Vd,\n+                           SIMD_Arrangement T, FloatRegister Vn, FloatRegister Vm,\n+                           int op1, int op2, int op3);\n+#define INSN(NAME, op1, op2, op3)                                                             \\\n+  void NAME(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn, FloatRegister Vm) {       \\\n+    starti;                                                                                   \\\n+    adv_simd_three_same(current_insn, Vd, T, Vn, Vm, op1, op2, op3);                          \\\n+  }\n+  INSN(fabd,  1, 1, 0b0101);\n+  INSN(fadd,  0, 0, 0b0101);\n+  INSN(fdiv,  1, 0, 0b1111);\n+  INSN(faddp, 1, 0, 0b0101);\n+  INSN(fmul,  1, 0, 0b0111);\n+  INSN(fsub,  0, 1, 0b0101);\n+  INSN(fmla,  0, 0, 0b0011);\n+  INSN(fmls,  0, 1, 0b0011);\n+  INSN(fmax,  0, 0, 0b1101);\n+  INSN(fmin,  0, 1, 0b1101);\n+  INSN(facgt, 1, 1, 0b1011);\n@@ -3265,7 +3265,13 @@\n-#define INSN(NAME, U, size, tmask, opcode)                                          \\\n-  void NAME(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn) {               \\\n-       starti;                                                                      \\\n-       assert((ASSERTION), MSG);                                                    \\\n-       f(0, 31), f((int)T & 1, 30), f(U, 29), f(0b01110, 28, 24);                   \\\n-       f(size | ((int)(T >> 1) & tmask), 23, 22), f(0b10000, 21, 17);               \\\n-       f(opcode, 16, 12), f(0b10, 11, 10), rf(Vn, 5), rf(Vd, 0);                    \\\n+#define INSN(NAME, U, size, tmask, opcode)                                      \\\n+  void NAME(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn) {           \\\n+    starti;                                                                     \\\n+    assert((ASSERTION), MSG);                                                   \\\n+    int op22 = (int)(T >> 1) & tmask;                                           \\\n+    int op19 = 0b00;                                                            \\\n+    if (tmask == 0b01 && (T == T4H || T == T8H)) {                              \\\n+      op22 = 0b1;                                                               \\\n+      op19 = 0b11;                                                              \\\n+    }                                                                           \\\n+    f(0, 31), f((int)T & 1, 30), f(U, 29), f(0b01110, 28, 24);                  \\\n+    f(size | op22, 23, 22), f(1, 21), f(op19, 20, 19), f(0b00, 18, 17);         \\\n+    f(opcode, 16, 12), f(0b10, 11, 10), rf(Vn, 5), rf(Vd, 0);                   \\\n@@ -3276,1 +3282,1 @@\n-#define ASSERTION (T == T2S || T == T4S || T == T2D)\n+#define ASSERTION (T == T4H || T == T8H || T == T2S || T == T4S || T == T2D)\n@@ -3403,1 +3409,1 @@\n-    assert(T == S || T == D, \"invalid register variant\");                              \\\n+    assert(T == H || T == S || T == D, \"invalid register variant\");                    \\\n@@ -3488,1 +3494,1 @@\n-    assert(T == S || T == D, \"invalid register variant\");                                             \\\n+    assert(T == H || T == S || T == D, \"invalid register variant\");                                   \\\n","filename":"src\/hotspot\/cpu\/aarch64\/assembler_aarch64.hpp","additions":36,"deletions":30,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -1783,1 +1783,2 @@\n-          [\"fabs\", \"fabs\", \"2D\"],\n+          [\"fabs\", \"fabs\", \"2D\"], [\"fabs\", \"fabs\", \"4H\"],\n+          [\"fabs\", \"fabs\", \"8H\"],\n@@ -1785,1 +1786,2 @@\n-          [\"fneg\", \"fneg\", \"2D\"],\n+          [\"fneg\", \"fneg\", \"2D\"], [\"fneg\", \"fneg\", \"4H\"],\n+          [\"fneg\", \"fneg\", \"8H\"],\n@@ -1787,1 +1789,2 @@\n-          [\"fsqrt\", \"fsqrt\", \"2D\"],\n+          [\"fsqrt\", \"fsqrt\", \"2D\"], [\"fsqrt\", \"fsqrt\", \"4H\"],\n+          [\"fsqrt\", \"fsqrt\", \"8H\"],\n@@ -1808,1 +1811,2 @@\n-          [\"fadd\", \"fadd\", \"2D\"],\n+          [\"fadd\", \"fadd\", \"2D\"], [\"fadd\", \"fadd\", \"4H\"],\n+          [\"fadd\", \"fadd\", \"8H\"],\n@@ -1822,1 +1826,2 @@\n-          [\"fsub\", \"fsub\", \"2D\"],\n+          [\"fsub\", \"fsub\", \"2D\"], [\"fsub\", \"fsub\", \"4H\"],\n+          [\"fsub\", \"fsub\", \"8H\"],\n@@ -1827,1 +1832,2 @@\n-          [\"fabd\", \"fabd\", \"2D\"],\n+          [\"fabd\", \"fabd\", \"2D\"], [\"fabd\", \"fabd\", \"4H\"],\n+          [\"fabd\", \"fabd\", \"8H\"],\n@@ -1829,1 +1835,2 @@\n-          [\"faddp\", \"faddp\", \"2D\"],\n+          [\"faddp\", \"faddp\", \"2D\"], [\"faddp\", \"faddp\", \"4H\"],\n+          [\"faddp\", \"faddp\", \"8H\"],\n@@ -1831,1 +1838,2 @@\n-          [\"fmul\", \"fmul\", \"2D\"],\n+          [\"fmul\", \"fmul\", \"2D\"], [\"fmul\", \"fmul\", \"4H\"],\n+          [\"fmul\", \"fmul\", \"8H\"],\n@@ -1835,1 +1843,2 @@\n-          [\"fmla\", \"fmla\", \"2D\"],\n+          [\"fmla\", \"fmla\", \"2D\"], [\"fmla\", \"fmla\", \"4H\"],\n+          [\"fmla\", \"fmla\", \"8H\"],\n@@ -1839,1 +1848,2 @@\n-          [\"fmls\", \"fmls\", \"2D\"],\n+          [\"fmls\", \"fmls\", \"2D\"], [\"fmls\", \"fmls\", \"4H\"],\n+          [\"fmls\", \"fmls\", \"8H\"],\n@@ -1841,1 +1851,2 @@\n-          [\"fdiv\", \"fdiv\", \"2D\"],\n+          [\"fdiv\", \"fdiv\", \"2D\"], [\"fdiv\", \"fdiv\", \"4H\"],\n+          [\"fdiv\", \"fdiv\", \"8H\"],\n@@ -1852,1 +1863,2 @@\n-          [\"fmax\", \"fmax\", \"2D\"],\n+          [\"fmax\", \"fmax\", \"2D\"], [\"fmax\", \"fmax\", \"4H\"],\n+          [\"fmax\", \"fmax\", \"8H\"],\n@@ -1868,1 +1880,2 @@\n-          [\"fmin\", \"fmin\", \"2D\"],\n+          [\"fmin\", \"fmin\", \"2D\"], [\"fmin\", \"fmin\", \"4H\"],\n+          [\"fmin\", \"fmin\", \"8H\"],\n@@ -1870,1 +1883,2 @@\n-          [\"facgt\", \"facgt\", \"2D\"],\n+          [\"facgt\", \"facgt\", \"2D\"], [\"facgt\", \"facgt\", \"4H\"],\n+          [\"facgt\", \"facgt\", \"8H\"],\n@@ -1931,0 +1945,2 @@\n+                        [\"fcvtzs\", \"__ fcvtzs(v0, __ T4H, v1);\",                         \"fcvtzs\\tv0.4h, v1.4h\"],\n+                        [\"fcvtzs\", \"__ fcvtzs(v0, __ T8H, v1);\",                         \"fcvtzs\\tv0.8h, v1.8h\"],\n@@ -1932,0 +1948,2 @@\n+                        [\"fcvtas\", \"__ fcvtas(v2, __ T4H, v3);\",                         \"fcvtas\\tv2.4h, v3.4h\"],\n+                        [\"fcvtas\", \"__ fcvtas(v2, __ T8H, v3);\",                         \"fcvtas\\tv2.8h, v3.8h\"],\n@@ -1933,0 +1951,2 @@\n+                        [\"fcvtms\", \"__ fcvtms(v4, __ T4H, v5);\",                         \"fcvtms\\tv4.4h, v5.4h\"],\n+                        [\"fcvtms\", \"__ fcvtms(v4, __ T8H, v5);\",                         \"fcvtms\\tv4.8h, v5.8h\"],\n","filename":"test\/hotspot\/gtest\/aarch64\/aarch64-asmtest.py","additions":34,"deletions":14,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -669,8 +669,14 @@\n-    __ fneg(v1, __ T2S, v2);                           \/\/       fneg    v1.2S, v2.2S\n-    __ fneg(v0, __ T4S, v1);                           \/\/       fneg    v0.4S, v1.4S\n-    __ fneg(v24, __ T2D, v25);                         \/\/       fneg    v24.2D, v25.2D\n-    __ fsqrt(v4, __ T2S, v5);                          \/\/       fsqrt   v4.2S, v5.2S\n-    __ fsqrt(v3, __ T4S, v4);                          \/\/       fsqrt   v3.4S, v4.4S\n-    __ fsqrt(v12, __ T2D, v13);                        \/\/       fsqrt   v12.2D, v13.2D\n-    __ notr(v31, __ T8B, v0);                          \/\/       not     v31.8B, v0.8B\n-    __ notr(v28, __ T16B, v29);                        \/\/       not     v28.16B, v29.16B\n+    __ fabs(v1, __ T4H, v2);                           \/\/       fabs    v1.4H, v2.4H\n+    __ fabs(v0, __ T8H, v1);                           \/\/       fabs    v0.8H, v1.8H\n+    __ fneg(v24, __ T2S, v25);                         \/\/       fneg    v24.2S, v25.2S\n+    __ fneg(v4, __ T4S, v5);                           \/\/       fneg    v4.4S, v5.4S\n+    __ fneg(v3, __ T2D, v4);                           \/\/       fneg    v3.2D, v4.2D\n+    __ fneg(v12, __ T4H, v13);                         \/\/       fneg    v12.4H, v13.4H\n+    __ fneg(v31, __ T8H, v0);                          \/\/       fneg    v31.8H, v0.8H\n+    __ fsqrt(v28, __ T2S, v29);                        \/\/       fsqrt   v28.2S, v29.2S\n+    __ fsqrt(v10, __ T4S, v11);                        \/\/       fsqrt   v10.4S, v11.4S\n+    __ fsqrt(v26, __ T2D, v27);                        \/\/       fsqrt   v26.2D, v27.2D\n+    __ fsqrt(v2, __ T4H, v3);                          \/\/       fsqrt   v2.4H, v3.4H\n+    __ fsqrt(v12, __ T8H, v13);                        \/\/       fsqrt   v12.8H, v13.8H\n+    __ notr(v18, __ T8B, v19);                         \/\/       not     v18.8B, v19.8B\n+    __ notr(v31, __ T16B, v0);                         \/\/       not     v31.16B, v0.16B\n@@ -679,16 +685,16 @@\n-    __ andr(v10, __ T8B, v11, v12);                    \/\/       and     v10.8B, v11.8B, v12.8B\n-    __ andr(v26, __ T16B, v27, v28);                   \/\/       and     v26.16B, v27.16B, v28.16B\n-    __ orr(v2, __ T8B, v3, v4);                        \/\/       orr     v2.8B, v3.8B, v4.8B\n-    __ orr(v12, __ T16B, v13, v14);                    \/\/       orr     v12.16B, v13.16B, v14.16B\n-    __ eor(v18, __ T8B, v19, v20);                     \/\/       eor     v18.8B, v19.8B, v20.8B\n-    __ eor(v31, __ T16B, v0, v1);                      \/\/       eor     v31.16B, v0.16B, v1.16B\n-    __ addv(v1, __ T8B, v2, v3);                       \/\/       add     v1.8B, v2.8B, v3.8B\n-    __ addv(v13, __ T16B, v14, v15);                   \/\/       add     v13.16B, v14.16B, v15.16B\n-    __ addv(v29, __ T4H, v30, v31);                    \/\/       add     v29.4H, v30.4H, v31.4H\n-    __ addv(v0, __ T8H, v1, v2);                       \/\/       add     v0.8H, v1.8H, v2.8H\n-    __ addv(v19, __ T2S, v20, v21);                    \/\/       add     v19.2S, v20.2S, v21.2S\n-    __ addv(v12, __ T4S, v13, v14);                    \/\/       add     v12.4S, v13.4S, v14.4S\n-    __ addv(v17, __ T2D, v18, v19);                    \/\/       add     v17.2D, v18.2D, v19.2D\n-    __ sqaddv(v22, __ T8B, v23, v24);                  \/\/       sqadd   v22.8B, v23.8B, v24.8B\n-    __ sqaddv(v13, __ T16B, v14, v15);                 \/\/       sqadd   v13.16B, v14.16B, v15.16B\n-    __ sqaddv(v28, __ T4H, v29, v30);                  \/\/       sqadd   v28.4H, v29.4H, v30.4H\n+    __ andr(v1, __ T8B, v2, v3);                       \/\/       and     v1.8B, v2.8B, v3.8B\n+    __ andr(v13, __ T16B, v14, v15);                   \/\/       and     v13.16B, v14.16B, v15.16B\n+    __ orr(v29, __ T8B, v30, v31);                     \/\/       orr     v29.8B, v30.8B, v31.8B\n+    __ orr(v0, __ T16B, v1, v2);                       \/\/       orr     v0.16B, v1.16B, v2.16B\n+    __ eor(v19, __ T8B, v20, v21);                     \/\/       eor     v19.8B, v20.8B, v21.8B\n+    __ eor(v12, __ T16B, v13, v14);                    \/\/       eor     v12.16B, v13.16B, v14.16B\n+    __ addv(v17, __ T8B, v18, v19);                    \/\/       add     v17.8B, v18.8B, v19.8B\n+    __ addv(v22, __ T16B, v23, v24);                   \/\/       add     v22.16B, v23.16B, v24.16B\n+    __ addv(v13, __ T4H, v14, v15);                    \/\/       add     v13.4H, v14.4H, v15.4H\n+    __ addv(v28, __ T8H, v29, v30);                    \/\/       add     v28.8H, v29.8H, v30.8H\n+    __ addv(v30, __ T2S, v31, v0);                     \/\/       add     v30.2S, v31.2S, v0.2S\n+    __ addv(v31, __ T4S, v0, v1);                      \/\/       add     v31.4S, v0.4S, v1.4S\n+    __ addv(v1, __ T2D, v2, v3);                       \/\/       add     v1.2D, v2.2D, v3.2D\n+    __ sqaddv(v26, __ T8B, v27, v28);                  \/\/       sqadd   v26.8B, v27.8B, v28.8B\n+    __ sqaddv(v28, __ T16B, v29, v30);                 \/\/       sqadd   v28.16B, v29.16B, v30.16B\n+    __ sqaddv(v4, __ T4H, v5, v6);                     \/\/       sqadd   v4.4H, v5.4H, v6.4H\n@@ -696,26 +702,28 @@\n-    __ sqaddv(v31, __ T2S, v0, v1);                    \/\/       sqadd   v31.2S, v0.2S, v1.2S\n-    __ sqaddv(v1, __ T4S, v2, v3);                     \/\/       sqadd   v1.4S, v2.4S, v3.4S\n-    __ sqaddv(v26, __ T2D, v27, v28);                  \/\/       sqadd   v26.2D, v27.2D, v28.2D\n-    __ uqaddv(v28, __ T8B, v29, v30);                  \/\/       uqadd   v28.8B, v29.8B, v30.8B\n-    __ uqaddv(v4, __ T16B, v5, v6);                    \/\/       uqadd   v4.16B, v5.16B, v6.16B\n-    __ uqaddv(v30, __ T4H, v31, v0);                   \/\/       uqadd   v30.4H, v31.4H, v0.4H\n-    __ uqaddv(v4, __ T8H, v5, v6);                     \/\/       uqadd   v4.8H, v5.8H, v6.8H\n-    __ uqaddv(v6, __ T2S, v7, v8);                     \/\/       uqadd   v6.2S, v7.2S, v8.2S\n-    __ uqaddv(v30, __ T4S, v31, v0);                   \/\/       uqadd   v30.4S, v31.4S, v0.4S\n-    __ uqaddv(v26, __ T2D, v27, v28);                  \/\/       uqadd   v26.2D, v27.2D, v28.2D\n-    __ fadd(v18, __ T2S, v19, v20);                    \/\/       fadd    v18.2S, v19.2S, v20.2S\n-    __ fadd(v9, __ T4S, v10, v11);                     \/\/       fadd    v9.4S, v10.4S, v11.4S\n-    __ fadd(v8, __ T2D, v9, v10);                      \/\/       fadd    v8.2D, v9.2D, v10.2D\n-    __ subv(v12, __ T8B, v13, v14);                    \/\/       sub     v12.8B, v13.8B, v14.8B\n-    __ subv(v0, __ T16B, v1, v2);                      \/\/       sub     v0.16B, v1.16B, v2.16B\n-    __ subv(v20, __ T4H, v21, v22);                    \/\/       sub     v20.4H, v21.4H, v22.4H\n-    __ subv(v1, __ T8H, v2, v3);                       \/\/       sub     v1.8H, v2.8H, v3.8H\n-    __ subv(v24, __ T2S, v25, v26);                    \/\/       sub     v24.2S, v25.2S, v26.2S\n-    __ subv(v2, __ T4S, v3, v4);                       \/\/       sub     v2.4S, v3.4S, v4.4S\n-    __ subv(v0, __ T2D, v1, v2);                       \/\/       sub     v0.2D, v1.2D, v2.2D\n-    __ sqsubv(v9, __ T8B, v10, v11);                   \/\/       sqsub   v9.8B, v10.8B, v11.8B\n-    __ sqsubv(v24, __ T16B, v25, v26);                 \/\/       sqsub   v24.16B, v25.16B, v26.16B\n-    __ sqsubv(v26, __ T4H, v27, v28);                  \/\/       sqsub   v26.4H, v27.4H, v28.4H\n-    __ sqsubv(v16, __ T8H, v17, v18);                  \/\/       sqsub   v16.8H, v17.8H, v18.8H\n-    __ sqsubv(v30, __ T2S, v31, v0);                   \/\/       sqsub   v30.2S, v31.2S, v0.2S\n-    __ sqsubv(v3, __ T4S, v4, v5);                     \/\/       sqsub   v3.4S, v4.4S, v5.4S\n+    __ sqaddv(v4, __ T2S, v5, v6);                     \/\/       sqadd   v4.2S, v5.2S, v6.2S\n+    __ sqaddv(v6, __ T4S, v7, v8);                     \/\/       sqadd   v6.4S, v7.4S, v8.4S\n+    __ sqaddv(v30, __ T2D, v31, v0);                   \/\/       sqadd   v30.2D, v31.2D, v0.2D\n+    __ uqaddv(v26, __ T8B, v27, v28);                  \/\/       uqadd   v26.8B, v27.8B, v28.8B\n+    __ uqaddv(v18, __ T16B, v19, v20);                 \/\/       uqadd   v18.16B, v19.16B, v20.16B\n+    __ uqaddv(v9, __ T4H, v10, v11);                   \/\/       uqadd   v9.4H, v10.4H, v11.4H\n+    __ uqaddv(v8, __ T8H, v9, v10);                    \/\/       uqadd   v8.8H, v9.8H, v10.8H\n+    __ uqaddv(v12, __ T2S, v13, v14);                  \/\/       uqadd   v12.2S, v13.2S, v14.2S\n+    __ uqaddv(v0, __ T4S, v1, v2);                     \/\/       uqadd   v0.4S, v1.4S, v2.4S\n+    __ uqaddv(v20, __ T2D, v21, v22);                  \/\/       uqadd   v20.2D, v21.2D, v22.2D\n+    __ fadd(v1, __ T2S, v2, v3);                       \/\/       fadd    v1.2S, v2.2S, v3.2S\n+    __ fadd(v24, __ T4S, v25, v26);                    \/\/       fadd    v24.4S, v25.4S, v26.4S\n+    __ fadd(v2, __ T2D, v3, v4);                       \/\/       fadd    v2.2D, v3.2D, v4.2D\n+    __ fadd(v0, __ T4H, v1, v2);                       \/\/       fadd    v0.4H, v1.4H, v2.4H\n+    __ fadd(v9, __ T8H, v10, v11);                     \/\/       fadd    v9.8H, v10.8H, v11.8H\n+    __ subv(v24, __ T8B, v25, v26);                    \/\/       sub     v24.8B, v25.8B, v26.8B\n+    __ subv(v26, __ T16B, v27, v28);                   \/\/       sub     v26.16B, v27.16B, v28.16B\n+    __ subv(v16, __ T4H, v17, v18);                    \/\/       sub     v16.4H, v17.4H, v18.4H\n+    __ subv(v30, __ T8H, v31, v0);                     \/\/       sub     v30.8H, v31.8H, v0.8H\n+    __ subv(v3, __ T2S, v4, v5);                       \/\/       sub     v3.2S, v4.2S, v5.2S\n+    __ subv(v10, __ T4S, v11, v12);                    \/\/       sub     v10.4S, v11.4S, v12.4S\n+    __ subv(v23, __ T2D, v24, v25);                    \/\/       sub     v23.2D, v24.2D, v25.2D\n+    __ sqsubv(v10, __ T8B, v11, v12);                  \/\/       sqsub   v10.8B, v11.8B, v12.8B\n+    __ sqsubv(v4, __ T16B, v5, v6);                    \/\/       sqsub   v4.16B, v5.16B, v6.16B\n+    __ sqsubv(v18, __ T4H, v19, v20);                  \/\/       sqsub   v18.4H, v19.4H, v20.4H\n+    __ sqsubv(v2, __ T8H, v3, v4);                     \/\/       sqsub   v2.8H, v3.8H, v4.8H\n+    __ sqsubv(v11, __ T2S, v12, v13);                  \/\/       sqsub   v11.2S, v12.2S, v13.2S\n+    __ sqsubv(v8, __ T4S, v9, v10);                    \/\/       sqsub   v8.4S, v9.4S, v10.4S\n@@ -723,27 +731,35 @@\n-    __ uqsubv(v23, __ T8B, v24, v25);                  \/\/       uqsub   v23.8B, v24.8B, v25.8B\n-    __ uqsubv(v10, __ T16B, v11, v12);                 \/\/       uqsub   v10.16B, v11.16B, v12.16B\n-    __ uqsubv(v4, __ T4H, v5, v6);                     \/\/       uqsub   v4.4H, v5.4H, v6.4H\n-    __ uqsubv(v18, __ T8H, v19, v20);                  \/\/       uqsub   v18.8H, v19.8H, v20.8H\n-    __ uqsubv(v2, __ T2S, v3, v4);                     \/\/       uqsub   v2.2S, v3.2S, v4.2S\n-    __ uqsubv(v11, __ T4S, v12, v13);                  \/\/       uqsub   v11.4S, v12.4S, v13.4S\n-    __ uqsubv(v8, __ T2D, v9, v10);                    \/\/       uqsub   v8.2D, v9.2D, v10.2D\n-    __ fsub(v10, __ T2S, v11, v12);                    \/\/       fsub    v10.2S, v11.2S, v12.2S\n-    __ fsub(v15, __ T4S, v16, v17);                    \/\/       fsub    v15.4S, v16.4S, v17.4S\n-    __ fsub(v17, __ T2D, v18, v19);                    \/\/       fsub    v17.2D, v18.2D, v19.2D\n-    __ mulv(v2, __ T8B, v3, v4);                       \/\/       mul     v2.8B, v3.8B, v4.8B\n-    __ mulv(v10, __ T16B, v11, v12);                   \/\/       mul     v10.16B, v11.16B, v12.16B\n-    __ mulv(v12, __ T4H, v13, v14);                    \/\/       mul     v12.4H, v13.4H, v14.4H\n-    __ mulv(v12, __ T8H, v13, v14);                    \/\/       mul     v12.8H, v13.8H, v14.8H\n-    __ mulv(v15, __ T2S, v16, v17);                    \/\/       mul     v15.2S, v16.2S, v17.2S\n-    __ mulv(v13, __ T4S, v14, v15);                    \/\/       mul     v13.4S, v14.4S, v15.4S\n-    __ fabd(v2, __ T2S, v3, v4);                       \/\/       fabd    v2.2S, v3.2S, v4.2S\n-    __ fabd(v7, __ T4S, v8, v9);                       \/\/       fabd    v7.4S, v8.4S, v9.4S\n-    __ fabd(v20, __ T2D, v21, v22);                    \/\/       fabd    v20.2D, v21.2D, v22.2D\n-    __ faddp(v26, __ T2S, v27, v28);                   \/\/       faddp   v26.2S, v27.2S, v28.2S\n-    __ faddp(v16, __ T4S, v17, v18);                   \/\/       faddp   v16.4S, v17.4S, v18.4S\n-    __ faddp(v4, __ T2D, v5, v6);                      \/\/       faddp   v4.2D, v5.2D, v6.2D\n-    __ fmul(v2, __ T2S, v3, v4);                       \/\/       fmul    v2.2S, v3.2S, v4.2S\n-    __ fmul(v4, __ T4S, v5, v6);                       \/\/       fmul    v4.4S, v5.4S, v6.4S\n-    __ fmul(v12, __ T2D, v13, v14);                    \/\/       fmul    v12.2D, v13.2D, v14.2D\n-    __ mlav(v18, __ T4H, v19, v20);                    \/\/       mla     v18.4H, v19.4H, v20.4H\n-    __ mlav(v21, __ T8H, v22, v23);                    \/\/       mla     v21.8H, v22.8H, v23.8H\n+    __ uqsubv(v15, __ T8B, v16, v17);                  \/\/       uqsub   v15.8B, v16.8B, v17.8B\n+    __ uqsubv(v17, __ T16B, v18, v19);                 \/\/       uqsub   v17.16B, v18.16B, v19.16B\n+    __ uqsubv(v2, __ T4H, v3, v4);                     \/\/       uqsub   v2.4H, v3.4H, v4.4H\n+    __ uqsubv(v10, __ T8H, v11, v12);                  \/\/       uqsub   v10.8H, v11.8H, v12.8H\n+    __ uqsubv(v12, __ T2S, v13, v14);                  \/\/       uqsub   v12.2S, v13.2S, v14.2S\n+    __ uqsubv(v12, __ T4S, v13, v14);                  \/\/       uqsub   v12.4S, v13.4S, v14.4S\n+    __ uqsubv(v15, __ T2D, v16, v17);                  \/\/       uqsub   v15.2D, v16.2D, v17.2D\n+    __ fsub(v13, __ T2S, v14, v15);                    \/\/       fsub    v13.2S, v14.2S, v15.2S\n+    __ fsub(v2, __ T4S, v3, v4);                       \/\/       fsub    v2.4S, v3.4S, v4.4S\n+    __ fsub(v7, __ T2D, v8, v9);                       \/\/       fsub    v7.2D, v8.2D, v9.2D\n+    __ fsub(v20, __ T4H, v21, v22);                    \/\/       fsub    v20.4H, v21.4H, v22.4H\n+    __ fsub(v26, __ T8H, v27, v28);                    \/\/       fsub    v26.8H, v27.8H, v28.8H\n+    __ mulv(v16, __ T8B, v17, v18);                    \/\/       mul     v16.8B, v17.8B, v18.8B\n+    __ mulv(v4, __ T16B, v5, v6);                      \/\/       mul     v4.16B, v5.16B, v6.16B\n+    __ mulv(v2, __ T4H, v3, v4);                       \/\/       mul     v2.4H, v3.4H, v4.4H\n+    __ mulv(v4, __ T8H, v5, v6);                       \/\/       mul     v4.8H, v5.8H, v6.8H\n+    __ mulv(v12, __ T2S, v13, v14);                    \/\/       mul     v12.2S, v13.2S, v14.2S\n+    __ mulv(v18, __ T4S, v19, v20);                    \/\/       mul     v18.4S, v19.4S, v20.4S\n+    __ fabd(v21, __ T2S, v22, v23);                    \/\/       fabd    v21.2S, v22.2S, v23.2S\n+    __ fabd(v16, __ T4S, v17, v18);                    \/\/       fabd    v16.4S, v17.4S, v18.4S\n+    __ fabd(v18, __ T2D, v19, v20);                    \/\/       fabd    v18.2D, v19.2D, v20.2D\n+    __ fabd(v11, __ T4H, v12, v13);                    \/\/       fabd    v11.4H, v12.4H, v13.4H\n+    __ fabd(v21, __ T8H, v22, v23);                    \/\/       fabd    v21.8H, v22.8H, v23.8H\n+    __ faddp(v23, __ T2S, v24, v25);                   \/\/       faddp   v23.2S, v24.2S, v25.2S\n+    __ faddp(v12, __ T4S, v13, v14);                   \/\/       faddp   v12.4S, v13.4S, v14.4S\n+    __ faddp(v26, __ T2D, v27, v28);                   \/\/       faddp   v26.2D, v27.2D, v28.2D\n+    __ faddp(v23, __ T4H, v24, v25);                   \/\/       faddp   v23.4H, v24.4H, v25.4H\n+    __ faddp(v28, __ T8H, v29, v30);                   \/\/       faddp   v28.8H, v29.8H, v30.8H\n+    __ fmul(v14, __ T2S, v15, v16);                    \/\/       fmul    v14.2S, v15.2S, v16.2S\n+    __ fmul(v11, __ T4S, v12, v13);                    \/\/       fmul    v11.4S, v12.4S, v13.4S\n+    __ fmul(v24, __ T2D, v25, v26);                    \/\/       fmul    v24.2D, v25.2D, v26.2D\n+    __ fmul(v1, __ T4H, v2, v3);                       \/\/       fmul    v1.4H, v2.4H, v3.4H\n+    __ fmul(v12, __ T8H, v13, v14);                    \/\/       fmul    v12.8H, v13.8H, v14.8H\n+    __ mlav(v31, __ T4H, v0, v1);                      \/\/       mla     v31.4H, v0.4H, v1.4H\n+    __ mlav(v10, __ T8H, v11, v12);                    \/\/       mla     v10.8H, v11.8H, v12.8H\n@@ -751,22 +767,28 @@\n-    __ mlav(v18, __ T4S, v19, v20);                    \/\/       mla     v18.4S, v19.4S, v20.4S\n-    __ fmla(v11, __ T2S, v12, v13);                    \/\/       fmla    v11.2S, v12.2S, v13.2S\n-    __ fmla(v21, __ T4S, v22, v23);                    \/\/       fmla    v21.4S, v22.4S, v23.4S\n-    __ fmla(v23, __ T2D, v24, v25);                    \/\/       fmla    v23.2D, v24.2D, v25.2D\n-    __ mlsv(v12, __ T4H, v13, v14);                    \/\/       mls     v12.4H, v13.4H, v14.4H\n-    __ mlsv(v26, __ T8H, v27, v28);                    \/\/       mls     v26.8H, v27.8H, v28.8H\n-    __ mlsv(v23, __ T2S, v24, v25);                    \/\/       mls     v23.2S, v24.2S, v25.2S\n-    __ mlsv(v28, __ T4S, v29, v30);                    \/\/       mls     v28.4S, v29.4S, v30.4S\n-    __ fmls(v14, __ T2S, v15, v16);                    \/\/       fmls    v14.2S, v15.2S, v16.2S\n-    __ fmls(v11, __ T4S, v12, v13);                    \/\/       fmls    v11.4S, v12.4S, v13.4S\n-    __ fmls(v24, __ T2D, v25, v26);                    \/\/       fmls    v24.2D, v25.2D, v26.2D\n-    __ fdiv(v1, __ T2S, v2, v3);                       \/\/       fdiv    v1.2S, v2.2S, v3.2S\n-    __ fdiv(v12, __ T4S, v13, v14);                    \/\/       fdiv    v12.4S, v13.4S, v14.4S\n-    __ fdiv(v31, __ T2D, v0, v1);                      \/\/       fdiv    v31.2D, v0.2D, v1.2D\n-    __ maxv(v10, __ T8B, v11, v12);                    \/\/       smax    v10.8B, v11.8B, v12.8B\n-    __ maxv(v16, __ T16B, v17, v18);                   \/\/       smax    v16.16B, v17.16B, v18.16B\n-    __ maxv(v7, __ T4H, v8, v9);                       \/\/       smax    v7.4H, v8.4H, v9.4H\n-    __ maxv(v2, __ T8H, v3, v4);                       \/\/       smax    v2.8H, v3.8H, v4.8H\n-    __ maxv(v3, __ T2S, v4, v5);                       \/\/       smax    v3.2S, v4.2S, v5.2S\n-    __ maxv(v13, __ T4S, v14, v15);                    \/\/       smax    v13.4S, v14.4S, v15.4S\n-    __ umaxv(v19, __ T8B, v20, v21);                   \/\/       umax    v19.8B, v20.8B, v21.8B\n-    __ umaxv(v17, __ T16B, v18, v19);                  \/\/       umax    v17.16B, v18.16B, v19.16B\n+    __ mlav(v7, __ T4S, v8, v9);                       \/\/       mla     v7.4S, v8.4S, v9.4S\n+    __ fmla(v2, __ T2S, v3, v4);                       \/\/       fmla    v2.2S, v3.2S, v4.2S\n+    __ fmla(v3, __ T4S, v4, v5);                       \/\/       fmla    v3.4S, v4.4S, v5.4S\n+    __ fmla(v13, __ T2D, v14, v15);                    \/\/       fmla    v13.2D, v14.2D, v15.2D\n+    __ fmla(v19, __ T4H, v20, v21);                    \/\/       fmla    v19.4H, v20.4H, v21.4H\n+    __ fmla(v17, __ T8H, v18, v19);                    \/\/       fmla    v17.8H, v18.8H, v19.8H\n+    __ mlsv(v16, __ T4H, v17, v18);                    \/\/       mls     v16.4H, v17.4H, v18.4H\n+    __ mlsv(v3, __ T8H, v4, v5);                       \/\/       mls     v3.8H, v4.8H, v5.8H\n+    __ mlsv(v1, __ T2S, v2, v3);                       \/\/       mls     v1.2S, v2.2S, v3.2S\n+    __ mlsv(v11, __ T4S, v12, v13);                    \/\/       mls     v11.4S, v12.4S, v13.4S\n+    __ fmls(v30, __ T2S, v31, v0);                     \/\/       fmls    v30.2S, v31.2S, v0.2S\n+    __ fmls(v5, __ T4S, v6, v7);                       \/\/       fmls    v5.4S, v6.4S, v7.4S\n+    __ fmls(v8, __ T2D, v9, v10);                      \/\/       fmls    v8.2D, v9.2D, v10.2D\n+    __ fmls(v15, __ T4H, v16, v17);                    \/\/       fmls    v15.4H, v16.4H, v17.4H\n+    __ fmls(v29, __ T8H, v30, v31);                    \/\/       fmls    v29.8H, v30.8H, v31.8H\n+    __ fdiv(v30, __ T2S, v31, v0);                     \/\/       fdiv    v30.2S, v31.2S, v0.2S\n+    __ fdiv(v0, __ T4S, v1, v2);                       \/\/       fdiv    v0.4S, v1.4S, v2.4S\n+    __ fdiv(v20, __ T2D, v21, v22);                    \/\/       fdiv    v20.2D, v21.2D, v22.2D\n+    __ fdiv(v7, __ T4H, v8, v9);                       \/\/       fdiv    v7.4H, v8.4H, v9.4H\n+    __ fdiv(v20, __ T8H, v21, v22);                    \/\/       fdiv    v20.8H, v21.8H, v22.8H\n+    __ maxv(v23, __ T8B, v24, v25);                    \/\/       smax    v23.8B, v24.8B, v25.8B\n+    __ maxv(v28, __ T16B, v29, v30);                   \/\/       smax    v28.16B, v29.16B, v30.16B\n+    __ maxv(v21, __ T4H, v22, v23);                    \/\/       smax    v21.4H, v22.4H, v23.4H\n+    __ maxv(v27, __ T8H, v28, v29);                    \/\/       smax    v27.8H, v28.8H, v29.8H\n+    __ maxv(v25, __ T2S, v26, v27);                    \/\/       smax    v25.2S, v26.2S, v27.2S\n+    __ maxv(v5, __ T4S, v6, v7);                       \/\/       smax    v5.4S, v6.4S, v7.4S\n+    __ umaxv(v1, __ T8B, v2, v3);                      \/\/       umax    v1.8B, v2.8B, v3.8B\n+    __ umaxv(v23, __ T16B, v24, v25);                  \/\/       umax    v23.16B, v24.16B, v25.16B\n@@ -774,46 +796,52 @@\n-    __ umaxv(v3, __ T8H, v4, v5);                      \/\/       umax    v3.8H, v4.8H, v5.8H\n-    __ umaxv(v1, __ T2S, v2, v3);                      \/\/       umax    v1.2S, v2.2S, v3.2S\n-    __ umaxv(v11, __ T4S, v12, v13);                   \/\/       umax    v11.4S, v12.4S, v13.4S\n-    __ smaxp(v30, __ T8B, v31, v0);                    \/\/       smaxp   v30.8B, v31.8B, v0.8B\n-    __ smaxp(v5, __ T16B, v6, v7);                     \/\/       smaxp   v5.16B, v6.16B, v7.16B\n-    __ smaxp(v8, __ T4H, v9, v10);                     \/\/       smaxp   v8.4H, v9.4H, v10.4H\n-    __ smaxp(v15, __ T8H, v16, v17);                   \/\/       smaxp   v15.8H, v16.8H, v17.8H\n-    __ smaxp(v29, __ T2S, v30, v31);                   \/\/       smaxp   v29.2S, v30.2S, v31.2S\n-    __ smaxp(v30, __ T4S, v31, v0);                    \/\/       smaxp   v30.4S, v31.4S, v0.4S\n-    __ fmax(v0, __ T2S, v1, v2);                       \/\/       fmax    v0.2S, v1.2S, v2.2S\n-    __ fmax(v20, __ T4S, v21, v22);                    \/\/       fmax    v20.4S, v21.4S, v22.4S\n-    __ fmax(v7, __ T2D, v8, v9);                       \/\/       fmax    v7.2D, v8.2D, v9.2D\n-    __ minv(v20, __ T8B, v21, v22);                    \/\/       smin    v20.8B, v21.8B, v22.8B\n-    __ minv(v23, __ T16B, v24, v25);                   \/\/       smin    v23.16B, v24.16B, v25.16B\n-    __ minv(v28, __ T4H, v29, v30);                    \/\/       smin    v28.4H, v29.4H, v30.4H\n-    __ minv(v21, __ T8H, v22, v23);                    \/\/       smin    v21.8H, v22.8H, v23.8H\n-    __ minv(v27, __ T2S, v28, v29);                    \/\/       smin    v27.2S, v28.2S, v29.2S\n-    __ minv(v25, __ T4S, v26, v27);                    \/\/       smin    v25.4S, v26.4S, v27.4S\n-    __ uminv(v5, __ T8B, v6, v7);                      \/\/       umin    v5.8B, v6.8B, v7.8B\n-    __ uminv(v1, __ T16B, v2, v3);                     \/\/       umin    v1.16B, v2.16B, v3.16B\n-    __ uminv(v23, __ T4H, v24, v25);                   \/\/       umin    v23.4H, v24.4H, v25.4H\n-    __ uminv(v16, __ T8H, v17, v18);                   \/\/       umin    v16.8H, v17.8H, v18.8H\n-    __ uminv(v31, __ T2S, v0, v1);                     \/\/       umin    v31.2S, v0.2S, v1.2S\n-    __ uminv(v5, __ T4S, v6, v7);                      \/\/       umin    v5.4S, v6.4S, v7.4S\n-    __ sminp(v12, __ T8B, v13, v14);                   \/\/       sminp   v12.8B, v13.8B, v14.8B\n-    __ sminp(v9, __ T16B, v10, v11);                   \/\/       sminp   v9.16B, v10.16B, v11.16B\n-    __ sminp(v28, __ T4H, v29, v30);                   \/\/       sminp   v28.4H, v29.4H, v30.4H\n-    __ sminp(v15, __ T8H, v16, v17);                   \/\/       sminp   v15.8H, v16.8H, v17.8H\n-    __ sminp(v29, __ T2S, v30, v31);                   \/\/       sminp   v29.2S, v30.2S, v31.2S\n-    __ sminp(v22, __ T4S, v23, v24);                   \/\/       sminp   v22.4S, v23.4S, v24.4S\n-    __ sqdmulh(v31, __ T4H, v0, v1);                   \/\/       sqdmulh v31.4H, v0.4H, v1.4H\n-    __ sqdmulh(v19, __ T8H, v20, v21);                 \/\/       sqdmulh v19.8H, v20.8H, v21.8H\n-    __ sqdmulh(v31, __ T2S, v0, v1);                   \/\/       sqdmulh v31.2S, v0.2S, v1.2S\n-    __ sqdmulh(v5, __ T4S, v6, v7);                    \/\/       sqdmulh v5.4S, v6.4S, v7.4S\n-    __ shsubv(v14, __ T8B, v15, v16);                  \/\/       shsub   v14.8B, v15.8B, v16.8B\n-    __ shsubv(v18, __ T16B, v19, v20);                 \/\/       shsub   v18.16B, v19.16B, v20.16B\n-    __ shsubv(v31, __ T4H, v0, v1);                    \/\/       shsub   v31.4H, v0.4H, v1.4H\n-    __ shsubv(v18, __ T8H, v19, v20);                  \/\/       shsub   v18.8H, v19.8H, v20.8H\n-    __ shsubv(v27, __ T2S, v28, v29);                  \/\/       shsub   v27.2S, v28.2S, v29.2S\n-    __ shsubv(v20, __ T4S, v21, v22);                  \/\/       shsub   v20.4S, v21.4S, v22.4S\n-    __ fmin(v16, __ T2S, v17, v18);                    \/\/       fmin    v16.2S, v17.2S, v18.2S\n-    __ fmin(v12, __ T4S, v13, v14);                    \/\/       fmin    v12.4S, v13.4S, v14.4S\n-    __ fmin(v11, __ T2D, v12, v13);                    \/\/       fmin    v11.2D, v12.2D, v13.2D\n-    __ facgt(v9, __ T2S, v10, v11);                    \/\/       facgt   v9.2S, v10.2S, v11.2S\n-    __ facgt(v6, __ T4S, v7, v8);                      \/\/       facgt   v6.4S, v7.4S, v8.4S\n-    __ facgt(v30, __ T2D, v31, v0);                    \/\/       facgt   v30.2D, v31.2D, v0.2D\n+    __ umaxv(v31, __ T8H, v0, v1);                     \/\/       umax    v31.8H, v0.8H, v1.8H\n+    __ umaxv(v5, __ T2S, v6, v7);                      \/\/       umax    v5.2S, v6.2S, v7.2S\n+    __ umaxv(v12, __ T4S, v13, v14);                   \/\/       umax    v12.4S, v13.4S, v14.4S\n+    __ smaxp(v9, __ T8B, v10, v11);                    \/\/       smaxp   v9.8B, v10.8B, v11.8B\n+    __ smaxp(v28, __ T16B, v29, v30);                  \/\/       smaxp   v28.16B, v29.16B, v30.16B\n+    __ smaxp(v15, __ T4H, v16, v17);                   \/\/       smaxp   v15.4H, v16.4H, v17.4H\n+    __ smaxp(v29, __ T8H, v30, v31);                   \/\/       smaxp   v29.8H, v30.8H, v31.8H\n+    __ smaxp(v22, __ T2S, v23, v24);                   \/\/       smaxp   v22.2S, v23.2S, v24.2S\n+    __ smaxp(v31, __ T4S, v0, v1);                     \/\/       smaxp   v31.4S, v0.4S, v1.4S\n+    __ fmax(v19, __ T2S, v20, v21);                    \/\/       fmax    v19.2S, v20.2S, v21.2S\n+    __ fmax(v31, __ T4S, v0, v1);                      \/\/       fmax    v31.4S, v0.4S, v1.4S\n+    __ fmax(v5, __ T2D, v6, v7);                       \/\/       fmax    v5.2D, v6.2D, v7.2D\n+    __ fmax(v14, __ T4H, v15, v16);                    \/\/       fmax    v14.4H, v15.4H, v16.4H\n+    __ fmax(v18, __ T8H, v19, v20);                    \/\/       fmax    v18.8H, v19.8H, v20.8H\n+    __ minv(v31, __ T8B, v0, v1);                      \/\/       smin    v31.8B, v0.8B, v1.8B\n+    __ minv(v18, __ T16B, v19, v20);                   \/\/       smin    v18.16B, v19.16B, v20.16B\n+    __ minv(v27, __ T4H, v28, v29);                    \/\/       smin    v27.4H, v28.4H, v29.4H\n+    __ minv(v20, __ T8H, v21, v22);                    \/\/       smin    v20.8H, v21.8H, v22.8H\n+    __ minv(v16, __ T2S, v17, v18);                    \/\/       smin    v16.2S, v17.2S, v18.2S\n+    __ minv(v12, __ T4S, v13, v14);                    \/\/       smin    v12.4S, v13.4S, v14.4S\n+    __ uminv(v11, __ T8B, v12, v13);                   \/\/       umin    v11.8B, v12.8B, v13.8B\n+    __ uminv(v9, __ T16B, v10, v11);                   \/\/       umin    v9.16B, v10.16B, v11.16B\n+    __ uminv(v6, __ T4H, v7, v8);                      \/\/       umin    v6.4H, v7.4H, v8.4H\n+    __ uminv(v30, __ T8H, v31, v0);                    \/\/       umin    v30.8H, v31.8H, v0.8H\n+    __ uminv(v17, __ T2S, v18, v19);                   \/\/       umin    v17.2S, v18.2S, v19.2S\n+    __ uminv(v27, __ T4S, v28, v29);                   \/\/       umin    v27.4S, v28.4S, v29.4S\n+    __ sminp(v28, __ T8B, v29, v30);                   \/\/       sminp   v28.8B, v29.8B, v30.8B\n+    __ sminp(v30, __ T16B, v31, v0);                   \/\/       sminp   v30.16B, v31.16B, v0.16B\n+    __ sminp(v7, __ T4H, v8, v9);                      \/\/       sminp   v7.4H, v8.4H, v9.4H\n+    __ sminp(v10, __ T8H, v11, v12);                   \/\/       sminp   v10.8H, v11.8H, v12.8H\n+    __ sminp(v20, __ T2S, v21, v22);                   \/\/       sminp   v20.2S, v21.2S, v22.2S\n+    __ sminp(v10, __ T4S, v11, v12);                   \/\/       sminp   v10.4S, v11.4S, v12.4S\n+    __ sqdmulh(v4, __ T4H, v5, v6);                    \/\/       sqdmulh v4.4H, v5.4H, v6.4H\n+    __ sqdmulh(v24, __ T8H, v25, v26);                 \/\/       sqdmulh v24.8H, v25.8H, v26.8H\n+    __ sqdmulh(v17, __ T2S, v18, v19);                 \/\/       sqdmulh v17.2S, v18.2S, v19.2S\n+    __ sqdmulh(v17, __ T4S, v18, v19);                 \/\/       sqdmulh v17.4S, v18.4S, v19.4S\n+    __ shsubv(v22, __ T8B, v23, v24);                  \/\/       shsub   v22.8B, v23.8B, v24.8B\n+    __ shsubv(v3, __ T16B, v4, v5);                    \/\/       shsub   v3.16B, v4.16B, v5.16B\n+    __ shsubv(v29, __ T4H, v30, v31);                  \/\/       shsub   v29.4H, v30.4H, v31.4H\n+    __ shsubv(v15, __ T8H, v16, v17);                  \/\/       shsub   v15.8H, v16.8H, v17.8H\n+    __ shsubv(v22, __ T2S, v23, v24);                  \/\/       shsub   v22.2S, v23.2S, v24.2S\n+    __ shsubv(v19, __ T4S, v20, v21);                  \/\/       shsub   v19.4S, v20.4S, v21.4S\n+    __ fmin(v19, __ T2S, v20, v21);                    \/\/       fmin    v19.2S, v20.2S, v21.2S\n+    __ fmin(v22, __ T4S, v23, v24);                    \/\/       fmin    v22.4S, v23.4S, v24.4S\n+    __ fmin(v2, __ T2D, v3, v4);                       \/\/       fmin    v2.2D, v3.2D, v4.2D\n+    __ fmin(v15, __ T4H, v16, v17);                    \/\/       fmin    v15.4H, v16.4H, v17.4H\n+    __ fmin(v6, __ T8H, v7, v8);                       \/\/       fmin    v6.8H, v7.8H, v8.8H\n+    __ facgt(v12, __ T2S, v13, v14);                   \/\/       facgt   v12.2S, v13.2S, v14.2S\n+    __ facgt(v16, __ T4S, v17, v18);                   \/\/       facgt   v16.4S, v17.4S, v18.4S\n+    __ facgt(v11, __ T2D, v12, v13);                   \/\/       facgt   v11.2D, v12.2D, v13.2D\n+    __ facgt(v13, __ T4H, v14, v15);                   \/\/       facgt   v13.4H, v14.4H, v15.4H\n+    __ facgt(v23, __ T8H, v24, v25);                   \/\/       facgt   v23.8H, v24.8H, v25.8H\n@@ -822,13 +850,13 @@\n-    __ fmlavs(v13, __ T2S, v14, v15, 1);               \/\/       fmla    v13.2S, v14.2S, v15.S[1]\n-    __ mulvs(v15, __ T4S, v0, v1, 3);                  \/\/       mul     v15.4S, v0.4S, v1.S[3]\n-    __ fmlavs(v5, __ T2D, v6, v7, 0);                  \/\/       fmla    v5.2D, v6.2D, v7.D[0]\n-    __ fmlsvs(v5, __ T2S, v6, v7, 1);                  \/\/       fmls    v5.2S, v6.2S, v7.S[1]\n-    __ mulvs(v12, __ T4S, v13, v14, 0);                \/\/       mul     v12.4S, v13.4S, v14.S[0]\n-    __ fmlsvs(v8, __ T2D, v9, v10, 1);                 \/\/       fmls    v8.2D, v9.2D, v10.D[1]\n-    __ fmulxvs(v1, __ T2S, v2, v3, 1);                 \/\/       fmulx   v1.2S, v2.2S, v3.S[1]\n-    __ mulvs(v7, __ T4S, v8, v9, 3);                   \/\/       mul     v7.4S, v8.4S, v9.S[3]\n-    __ fmulxvs(v9, __ T2D, v10, v11, 1);               \/\/       fmulx   v9.2D, v10.2D, v11.D[1]\n-    __ mulvs(v11, __ T4H, v12, v13, 2);                \/\/       mul     v11.4H, v12.4H, v13.H[2]\n-    __ mulvs(v7, __ T8H, v8, v9, 0);                   \/\/       mul     v7.8H, v8.8H, v9.H[0]\n-    __ mulvs(v6, __ T2S, v7, v8, 0);                   \/\/       mul     v6.2S, v7.2S, v8.S[0]\n-    __ mulvs(v5, __ T4S, v6, v7, 2);                   \/\/       mul     v5.4S, v6.4S, v7.S[2]\n+    __ fmlavs(v15, __ T2S, v0, v1, 0);                 \/\/       fmla    v15.2S, v0.2S, v1.S[0]\n+    __ mulvs(v2, __ T4S, v3, v4, 2);                   \/\/       mul     v2.4S, v3.4S, v4.S[2]\n+    __ fmlavs(v1, __ T2D, v2, v3, 1);                  \/\/       fmla    v1.2D, v2.2D, v3.D[1]\n+    __ fmlsvs(v11, __ T2S, v12, v13, 1);               \/\/       fmls    v11.2S, v12.2S, v13.S[1]\n+    __ mulvs(v5, __ T4S, v6, v7, 1);                   \/\/       mul     v5.4S, v6.4S, v7.S[1]\n+    __ fmlsvs(v14, __ T2D, v15, v16, 1);               \/\/       fmls    v14.2D, v15.2D, v16.D[1]\n+    __ fmulxvs(v6, __ T2S, v7, v8, 1);                 \/\/       fmulx   v6.2S, v7.2S, v8.S[1]\n+    __ mulvs(v1, __ T4S, v2, v3, 3);                   \/\/       mul     v1.4S, v2.4S, v3.S[3]\n+    __ fmulxvs(v15, __ T2D, v0, v1, 0);                \/\/       fmulx   v15.2D, v0.2D, v1.D[0]\n+    __ mulvs(v9, __ T4H, v10, v11, 3);                 \/\/       mul     v9.4H, v10.4H, v11.H[3]\n+    __ mulvs(v4, __ T8H, v5, v6, 4);                   \/\/       mul     v4.8H, v5.8H, v6.H[4]\n+    __ mulvs(v13, __ T2S, v14, v15, 1);                \/\/       mul     v13.2S, v14.2S, v15.S[1]\n+    __ mulvs(v3, __ T4S, v4, v5, 1);                   \/\/       mul     v3.4S, v4.4S, v5.S[1]\n@@ -837,1 +865,1 @@\n-    __ cm(Assembler::GT, v13, __ T8B, v14, v15);       \/\/       cmgt    v13.8B, v14.8B, v15.8B\n+    __ cm(Assembler::GT, v21, __ T8B, v22, v23);       \/\/       cmgt    v21.8B, v22.8B, v23.8B\n@@ -839,23 +867,23 @@\n-    __ cm(Assembler::GT, v1, __ T4H, v2, v3);          \/\/       cmgt    v1.4H, v2.4H, v3.4H\n-    __ cm(Assembler::GT, v30, __ T8H, v31, v0);        \/\/       cmgt    v30.8H, v31.8H, v0.8H\n-    __ cm(Assembler::GT, v19, __ T2S, v20, v21);       \/\/       cmgt    v19.2S, v20.2S, v21.2S\n-    __ cm(Assembler::GT, v5, __ T4S, v6, v7);          \/\/       cmgt    v5.4S, v6.4S, v7.4S\n-    __ cm(Assembler::GT, v17, __ T2D, v18, v19);       \/\/       cmgt    v17.2D, v18.2D, v19.2D\n-    __ cm(Assembler::GE, v2, __ T8B, v3, v4);          \/\/       cmge    v2.8B, v3.8B, v4.8B\n-    __ cm(Assembler::GE, v16, __ T16B, v17, v18);      \/\/       cmge    v16.16B, v17.16B, v18.16B\n-    __ cm(Assembler::GE, v22, __ T4H, v23, v24);       \/\/       cmge    v22.4H, v23.4H, v24.4H\n-    __ cm(Assembler::GE, v13, __ T8H, v14, v15);       \/\/       cmge    v13.8H, v14.8H, v15.8H\n-    __ cm(Assembler::GE, v10, __ T2S, v11, v12);       \/\/       cmge    v10.2S, v11.2S, v12.2S\n-    __ cm(Assembler::GE, v21, __ T4S, v22, v23);       \/\/       cmge    v21.4S, v22.4S, v23.4S\n-    __ cm(Assembler::GE, v29, __ T2D, v30, v31);       \/\/       cmge    v29.2D, v30.2D, v31.2D\n-    __ cm(Assembler::EQ, v27, __ T8B, v28, v29);       \/\/       cmeq    v27.8B, v28.8B, v29.8B\n-    __ cm(Assembler::EQ, v12, __ T16B, v13, v14);      \/\/       cmeq    v12.16B, v13.16B, v14.16B\n-    __ cm(Assembler::EQ, v27, __ T4H, v28, v29);       \/\/       cmeq    v27.4H, v28.4H, v29.4H\n-    __ cm(Assembler::EQ, v3, __ T8H, v4, v5);          \/\/       cmeq    v3.8H, v4.8H, v5.8H\n-    __ cm(Assembler::EQ, v1, __ T2S, v2, v3);          \/\/       cmeq    v1.2S, v2.2S, v3.2S\n-    __ cm(Assembler::EQ, v31, __ T4S, v0, v1);         \/\/       cmeq    v31.4S, v0.4S, v1.4S\n-    __ cm(Assembler::EQ, v24, __ T2D, v25, v26);       \/\/       cmeq    v24.2D, v25.2D, v26.2D\n-    __ cm(Assembler::HI, v19, __ T8B, v20, v21);       \/\/       cmhi    v19.8B, v20.8B, v21.8B\n-    __ cm(Assembler::HI, v17, __ T16B, v18, v19);      \/\/       cmhi    v17.16B, v18.16B, v19.16B\n-    __ cm(Assembler::HI, v9, __ T4H, v10, v11);        \/\/       cmhi    v9.4H, v10.4H, v11.4H\n-    __ cm(Assembler::HI, v28, __ T8H, v29, v30);       \/\/       cmhi    v28.8H, v29.8H, v30.8H\n+    __ cm(Assembler::GT, v31, __ T4H, v0, v1);         \/\/       cmgt    v31.4H, v0.4H, v1.4H\n+    __ cm(Assembler::GT, v25, __ T8H, v26, v27);       \/\/       cmgt    v25.8H, v26.8H, v27.8H\n+    __ cm(Assembler::GT, v2, __ T2S, v3, v4);          \/\/       cmgt    v2.2S, v3.2S, v4.2S\n+    __ cm(Assembler::GT, v31, __ T4S, v0, v1);         \/\/       cmgt    v31.4S, v0.4S, v1.4S\n+    __ cm(Assembler::GT, v27, __ T2D, v28, v29);       \/\/       cmgt    v27.2D, v28.2D, v29.2D\n+    __ cm(Assembler::GE, v18, __ T8B, v19, v20);       \/\/       cmge    v18.8B, v19.8B, v20.8B\n+    __ cm(Assembler::GE, v10, __ T16B, v11, v12);      \/\/       cmge    v10.16B, v11.16B, v12.16B\n+    __ cm(Assembler::GE, v23, __ T4H, v24, v25);       \/\/       cmge    v23.4H, v24.4H, v25.4H\n+    __ cm(Assembler::GE, v19, __ T8H, v20, v21);       \/\/       cmge    v19.8H, v20.8H, v21.8H\n+    __ cm(Assembler::GE, v3, __ T2S, v4, v5);          \/\/       cmge    v3.2S, v4.2S, v5.2S\n+    __ cm(Assembler::GE, v18, __ T4S, v19, v20);       \/\/       cmge    v18.4S, v19.4S, v20.4S\n+    __ cm(Assembler::GE, v0, __ T2D, v1, v2);          \/\/       cmge    v0.2D, v1.2D, v2.2D\n+    __ cm(Assembler::EQ, v25, __ T8B, v26, v27);       \/\/       cmeq    v25.8B, v26.8B, v27.8B\n+    __ cm(Assembler::EQ, v26, __ T16B, v27, v28);      \/\/       cmeq    v26.16B, v27.16B, v28.16B\n+    __ cm(Assembler::EQ, v23, __ T4H, v24, v25);       \/\/       cmeq    v23.4H, v24.4H, v25.4H\n+    __ cm(Assembler::EQ, v2, __ T8H, v3, v4);          \/\/       cmeq    v2.8H, v3.8H, v4.8H\n+    __ cm(Assembler::EQ, v18, __ T2S, v19, v20);       \/\/       cmeq    v18.2S, v19.2S, v20.2S\n+    __ cm(Assembler::EQ, v12, __ T4S, v13, v14);       \/\/       cmeq    v12.4S, v13.4S, v14.4S\n+    __ cm(Assembler::EQ, v4, __ T2D, v5, v6);          \/\/       cmeq    v4.2D, v5.2D, v6.2D\n+    __ cm(Assembler::HI, v28, __ T8B, v29, v30);       \/\/       cmhi    v28.8B, v29.8B, v30.8B\n+    __ cm(Assembler::HI, v30, __ T16B, v31, v0);       \/\/       cmhi    v30.16B, v31.16B, v0.16B\n+    __ cm(Assembler::HI, v29, __ T4H, v30, v31);       \/\/       cmhi    v29.4H, v30.4H, v31.4H\n+    __ cm(Assembler::HI, v16, __ T8H, v17, v18);       \/\/       cmhi    v16.8H, v17.8H, v18.8H\n@@ -863,18 +891,18 @@\n-    __ cm(Assembler::HI, v15, __ T4S, v16, v17);       \/\/       cmhi    v15.4S, v16.4S, v17.4S\n-    __ cm(Assembler::HI, v7, __ T2D, v8, v9);          \/\/       cmhi    v7.2D, v8.2D, v9.2D\n-    __ cm(Assembler::HS, v21, __ T8B, v22, v23);       \/\/       cmhs    v21.8B, v22.8B, v23.8B\n-    __ cm(Assembler::HS, v23, __ T16B, v24, v25);      \/\/       cmhs    v23.16B, v24.16B, v25.16B\n-    __ cm(Assembler::HS, v31, __ T4H, v0, v1);         \/\/       cmhs    v31.4H, v0.4H, v1.4H\n-    __ cm(Assembler::HS, v25, __ T8H, v26, v27);       \/\/       cmhs    v25.8H, v26.8H, v27.8H\n-    __ cm(Assembler::HS, v2, __ T2S, v3, v4);          \/\/       cmhs    v2.2S, v3.2S, v4.2S\n-    __ cm(Assembler::HS, v31, __ T4S, v0, v1);         \/\/       cmhs    v31.4S, v0.4S, v1.4S\n-    __ cm(Assembler::HS, v27, __ T2D, v28, v29);       \/\/       cmhs    v27.2D, v28.2D, v29.2D\n-    __ fcm(Assembler::EQ, v18, __ T2S, v19, v20);      \/\/       fcmeq   v18.2S, v19.2S, v20.2S\n-    __ fcm(Assembler::EQ, v10, __ T4S, v11, v12);      \/\/       fcmeq   v10.4S, v11.4S, v12.4S\n-    __ fcm(Assembler::EQ, v23, __ T2D, v24, v25);      \/\/       fcmeq   v23.2D, v24.2D, v25.2D\n-    __ fcm(Assembler::GT, v19, __ T2S, v20, v21);      \/\/       fcmgt   v19.2S, v20.2S, v21.2S\n-    __ fcm(Assembler::GT, v3, __ T4S, v4, v5);         \/\/       fcmgt   v3.4S, v4.4S, v5.4S\n-    __ fcm(Assembler::GT, v18, __ T2D, v19, v20);      \/\/       fcmgt   v18.2D, v19.2D, v20.2D\n-    __ fcm(Assembler::GE, v0, __ T2S, v1, v2);         \/\/       fcmge   v0.2S, v1.2S, v2.2S\n-    __ fcm(Assembler::GE, v25, __ T4S, v26, v27);      \/\/       fcmge   v25.4S, v26.4S, v27.4S\n-    __ fcm(Assembler::GE, v26, __ T2D, v27, v28);      \/\/       fcmge   v26.2D, v27.2D, v28.2D\n+    __ cm(Assembler::HI, v6, __ T4S, v7, v8);          \/\/       cmhi    v6.4S, v7.4S, v8.4S\n+    __ cm(Assembler::HI, v9, __ T2D, v10, v11);        \/\/       cmhi    v9.2D, v10.2D, v11.2D\n+    __ cm(Assembler::HS, v29, __ T8B, v30, v31);       \/\/       cmhs    v29.8B, v30.8B, v31.8B\n+    __ cm(Assembler::HS, v18, __ T16B, v19, v20);      \/\/       cmhs    v18.16B, v19.16B, v20.16B\n+    __ cm(Assembler::HS, v7, __ T4H, v8, v9);          \/\/       cmhs    v7.4H, v8.4H, v9.4H\n+    __ cm(Assembler::HS, v4, __ T8H, v5, v6);          \/\/       cmhs    v4.8H, v5.8H, v6.8H\n+    __ cm(Assembler::HS, v7, __ T2S, v8, v9);          \/\/       cmhs    v7.2S, v8.2S, v9.2S\n+    __ cm(Assembler::HS, v15, __ T4S, v16, v17);       \/\/       cmhs    v15.4S, v16.4S, v17.4S\n+    __ cm(Assembler::HS, v9, __ T2D, v10, v11);        \/\/       cmhs    v9.2D, v10.2D, v11.2D\n+    __ fcm(Assembler::EQ, v23, __ T2S, v24, v25);      \/\/       fcmeq   v23.2S, v24.2S, v25.2S\n+    __ fcm(Assembler::EQ, v8, __ T4S, v9, v10);        \/\/       fcmeq   v8.4S, v9.4S, v10.4S\n+    __ fcm(Assembler::EQ, v2, __ T2D, v3, v4);         \/\/       fcmeq   v2.2D, v3.2D, v4.2D\n+    __ fcm(Assembler::GT, v28, __ T2S, v29, v30);      \/\/       fcmgt   v28.2S, v29.2S, v30.2S\n+    __ fcm(Assembler::GT, v21, __ T4S, v22, v23);      \/\/       fcmgt   v21.4S, v22.4S, v23.4S\n+    __ fcm(Assembler::GT, v31, __ T2D, v0, v1);        \/\/       fcmgt   v31.2D, v0.2D, v1.2D\n+    __ fcm(Assembler::GE, v5, __ T2S, v6, v7);         \/\/       fcmge   v5.2S, v6.2S, v7.2S\n+    __ fcm(Assembler::GE, v27, __ T4S, v28, v29);      \/\/       fcmge   v27.4S, v28.4S, v29.4S\n+    __ fcm(Assembler::GE, v0, __ T2D, v1, v2);         \/\/       fcmge   v0.2D, v1.2D, v2.2D\n@@ -883,6 +911,6 @@\n-    __ sve_fcm(Assembler::EQ, p11, __ D, p3, z2, 0.0); \/\/       fcmeq   p11.d, p3\/z, z2.d, #0.0\n-    __ sve_fcm(Assembler::GT, p2, __ D, p7, z28, 0.0); \/\/       fcmgt   p2.d, p7\/z, z28.d, #0.0\n-    __ sve_fcm(Assembler::GE, p8, __ S, p2, z27, 0.0); \/\/       fcmge   p8.s, p2\/z, z27.s, #0.0\n-    __ sve_fcm(Assembler::LT, p14, __ S, p1, z18, 0.0); \/\/      fcmlt   p14.s, p1\/z, z18.s, #0.0\n-    __ sve_fcm(Assembler::LE, p3, __ S, p5, z15, 0.0); \/\/       fcmle   p3.s, p5\/z, z15.s, #0.0\n-    __ sve_fcm(Assembler::NE, p4, __ D, p5, z2, 0.0);  \/\/       fcmne   p4.d, p5\/z, z2.d, #0.0\n+    __ sve_fcm(Assembler::EQ, p8, __ S, p6, z15, 0.0); \/\/       fcmeq   p8.s, p6\/z, z15.s, #0.0\n+    __ sve_fcm(Assembler::GT, p4, __ D, p6, z28, 0.0); \/\/       fcmgt   p4.d, p6\/z, z28.d, #0.0\n+    __ sve_fcm(Assembler::GE, p13, __ D, p0, z25, 0.0); \/\/      fcmge   p13.d, p0\/z, z25.d, #0.0\n+    __ sve_fcm(Assembler::LT, p2, __ D, p0, z6, 0.0);  \/\/       fcmlt   p2.d, p0\/z, z6.d, #0.0\n+    __ sve_fcm(Assembler::LE, p2, __ S, p2, z15, 0.0); \/\/       fcmle   p2.s, p2\/z, z15.s, #0.0\n+    __ sve_fcm(Assembler::NE, p3, __ S, p7, z5, 0.0);  \/\/       fcmne   p3.s, p7\/z, z5.s, #0.0\n@@ -891,10 +919,10 @@\n-    __ sve_cmp(Assembler::EQ, p15, __ D, p0, z5, 1);   \/\/       cmpeq   p15.d, p0\/z, z5.d, #1\n-    __ sve_cmp(Assembler::GT, p7, __ D, p2, z4, 12);   \/\/       cmpgt   p7.d, p2\/z, z4.d, #12\n-    __ sve_cmp(Assembler::GE, p11, __ D, p6, z27, 7);  \/\/       cmpge   p11.d, p6\/z, z27.d, #7\n-    __ sve_cmp(Assembler::LT, p0, __ B, p4, z4, -16);  \/\/       cmplt   p0.b, p4\/z, z4.b, #-16\n-    __ sve_cmp(Assembler::LE, p2, __ B, p2, z15, -9);  \/\/       cmple   p2.b, p2\/z, z15.b, #-9\n-    __ sve_cmp(Assembler::NE, p2, __ D, p1, z10, 4);   \/\/       cmpne   p2.d, p1\/z, z10.d, #4\n-    __ sve_cmp(Assembler::HS, p11, __ B, p2, z21, 34); \/\/       cmphs   p11.b, p2\/z, z21.b, #34\n-    __ sve_cmp(Assembler::HI, p8, __ B, p4, z31, 8);   \/\/       cmphi   p8.b, p4\/z, z31.b, #8\n-    __ sve_cmp(Assembler::LS, p6, __ D, p0, z30, 109); \/\/       cmpls   p6.d, p0\/z, z30.d, #109\n-    __ sve_cmp(Assembler::LO, p11, __ H, p3, z29, 114); \/\/      cmplo   p11.h, p3\/z, z29.h, #114\n+    __ sve_cmp(Assembler::EQ, p3, __ S, p5, z20, -10); \/\/       cmpeq   p3.s, p5\/z, z20.s, #-10\n+    __ sve_cmp(Assembler::GT, p5, __ S, p7, z8, -10);  \/\/       cmpgt   p5.s, p7\/z, z8.s, #-10\n+    __ sve_cmp(Assembler::GE, p8, __ H, p7, z2, 13);   \/\/       cmpge   p8.h, p7\/z, z2.h, #13\n+    __ sve_cmp(Assembler::LT, p1, __ S, p7, z27, -2);  \/\/       cmplt   p1.s, p7\/z, z27.s, #-2\n+    __ sve_cmp(Assembler::LE, p6, __ S, p6, z28, -11); \/\/       cmple   p6.s, p6\/z, z28.s, #-11\n+    __ sve_cmp(Assembler::NE, p1, __ H, p4, z14, -5);  \/\/       cmpne   p1.h, p4\/z, z14.h, #-5\n+    __ sve_cmp(Assembler::HS, p13, __ H, p1, z23, 90); \/\/       cmphs   p13.h, p1\/z, z23.h, #90\n+    __ sve_cmp(Assembler::HI, p8, __ B, p4, z4, 66);   \/\/       cmphi   p8.b, p4\/z, z4.b, #66\n+    __ sve_cmp(Assembler::LS, p9, __ H, p3, z13, 11);  \/\/       cmpls   p9.h, p3\/z, z13.h, #11\n+    __ sve_cmp(Assembler::LO, p8, __ S, p5, z3, 21);   \/\/       cmplo   p8.s, p5\/z, z3.s, #21\n@@ -930,0 +958,2 @@\n+    __ fcvtzs(v0, __ T4H, v1);                         \/\/       fcvtzs  v0.4h, v1.4h\n+    __ fcvtzs(v0, __ T8H, v1);                         \/\/       fcvtzs  v0.8h, v1.8h\n@@ -931,0 +961,2 @@\n+    __ fcvtas(v2, __ T4H, v3);                         \/\/       fcvtas  v2.4h, v3.4h\n+    __ fcvtas(v2, __ T8H, v3);                         \/\/       fcvtas  v2.8h, v3.8h\n@@ -932,0 +964,2 @@\n+    __ fcvtms(v4, __ T4H, v5);                         \/\/       fcvtms  v4.4h, v5.4h\n+    __ fcvtms(v4, __ T8H, v5);                         \/\/       fcvtms  v4.8h, v5.8h\n@@ -1155,9 +1189,9 @@\n-    __ swp(Assembler::xword, r17, r24, r5);            \/\/       swp     x17, x24, [x5]\n-    __ ldadd(Assembler::xword, r2, r14, r10);          \/\/       ldadd   x2, x14, [x10]\n-    __ ldbic(Assembler::xword, r16, r11, r27);         \/\/       ldclr   x16, x11, [x27]\n-    __ ldeor(Assembler::xword, r23, r12, r4);          \/\/       ldeor   x23, x12, [x4]\n-    __ ldorr(Assembler::xword, r22, r17, r4);          \/\/       ldset   x22, x17, [x4]\n-    __ ldsmin(Assembler::xword, r1, r19, r16);         \/\/       ldsmin  x1, x19, [x16]\n-    __ ldsmax(Assembler::xword, r16, r13, r14);        \/\/       ldsmax  x16, x13, [x14]\n-    __ ldumin(Assembler::xword, r12, r2, r17);         \/\/       ldumin  x12, x2, [x17]\n-    __ ldumax(Assembler::xword, r3, r21, r23);         \/\/       ldumax  x3, x21, [x23]\n+    __ swp(Assembler::xword, r6, r7, r19);             \/\/       swp     x6, x7, [x19]\n+    __ ldadd(Assembler::xword, r13, r28, r17);         \/\/       ldadd   x13, x28, [x17]\n+    __ ldbic(Assembler::xword, r16, r6, r2);           \/\/       ldclr   x16, x6, [x2]\n+    __ ldeor(Assembler::xword, r29, r3, r4);           \/\/       ldeor   x29, x3, [x4]\n+    __ ldorr(Assembler::xword, r6, r16, r20);          \/\/       ldset   x6, x16, [x20]\n+    __ ldsmin(Assembler::xword, r13, r12, r20);        \/\/       ldsmin  x13, x12, [x20]\n+    __ ldsmax(Assembler::xword, r8, r25, r20);         \/\/       ldsmax  x8, x25, [x20]\n+    __ ldumin(Assembler::xword, r19, r0, r11);         \/\/       ldumin  x19, x0, [x11]\n+    __ ldumax(Assembler::xword, r24, r6, r20);         \/\/       ldumax  x24, x6, [x20]\n@@ -1166,9 +1200,9 @@\n-    __ swpa(Assembler::xword, r5, r6, r7);             \/\/       swpa    x5, x6, [x7]\n-    __ ldadda(Assembler::xword, r19, r13, r28);        \/\/       ldadda  x19, x13, [x28]\n-    __ ldbica(Assembler::xword, r17, r16, r6);         \/\/       ldclra  x17, x16, [x6]\n-    __ ldeora(Assembler::xword, r2, r29, r3);          \/\/       ldeora  x2, x29, [x3]\n-    __ ldorra(Assembler::xword, r4, r6, r15);          \/\/       ldseta  x4, x6, [x15]\n-    __ ldsmina(Assembler::xword, r20, r13, r12);       \/\/       ldsmina x20, x13, [x12]\n-    __ ldsmaxa(Assembler::xword, r20, r8, r25);        \/\/       ldsmaxa x20, x8, [x25]\n-    __ ldumina(Assembler::xword, r20, r19, r0);        \/\/       ldumina x20, x19, [x0]\n-    __ ldumaxa(Assembler::xword, r11, r24, r6);        \/\/       ldumaxa x11, x24, [x6]\n+    __ swpa(Assembler::xword, zr, r14, r16);           \/\/       swpa    xzr, x14, [x16]\n+    __ ldadda(Assembler::xword, r6, r0, r7);           \/\/       ldadda  x6, x0, [x7]\n+    __ ldbica(Assembler::xword, r15, r19, r26);        \/\/       ldclra  x15, x19, [x26]\n+    __ ldeora(Assembler::xword, r9, r10, r23);         \/\/       ldeora  x9, x10, [x23]\n+    __ ldorra(Assembler::xword, r21, r22, r28);        \/\/       ldseta  x21, x22, [x28]\n+    __ ldsmina(Assembler::xword, r2, r3, r15);         \/\/       ldsmina x2, x3, [x15]\n+    __ ldsmaxa(Assembler::xword, r19, r20, r7);        \/\/       ldsmaxa x19, x20, [x7]\n+    __ ldumina(Assembler::xword, r4, r29, r7);         \/\/       ldumina x4, x29, [x7]\n+    __ ldumaxa(Assembler::xword, r0, r9, r16);         \/\/       ldumaxa x0, x9, [x16]\n@@ -1177,9 +1211,9 @@\n-    __ swpal(Assembler::xword, r20, zr, r14);          \/\/       swpal   x20, xzr, [x14]\n-    __ ldaddal(Assembler::xword, r16, r6, r0);         \/\/       ldaddal x16, x6, [x0]\n-    __ ldbical(Assembler::xword, r7, r15, r19);        \/\/       ldclral x7, x15, [x19]\n-    __ ldeoral(Assembler::xword, r26, r9, r10);        \/\/       ldeoral x26, x9, [x10]\n-    __ ldorral(Assembler::xword, r23, r21, r22);       \/\/       ldsetal x23, x21, [x22]\n-    __ ldsminal(Assembler::xword, r28, r2, r3);        \/\/       ldsminal        x28, x2, [x3]\n-    __ ldsmaxal(Assembler::xword, r15, r19, r20);      \/\/       ldsmaxal        x15, x19, [x20]\n-    __ lduminal(Assembler::xword, r7, r4, r29);        \/\/       lduminal        x7, x4, [x29]\n-    __ ldumaxal(Assembler::xword, r7, r0, r9);         \/\/       ldumaxal        x7, x0, [x9]\n+    __ swpal(Assembler::xword, r20, r23, r4);          \/\/       swpal   x20, x23, [x4]\n+    __ ldaddal(Assembler::xword, r16, r10, r23);       \/\/       ldaddal x16, x10, [x23]\n+    __ ldbical(Assembler::xword, r11, r25, r6);        \/\/       ldclral x11, x25, [x6]\n+    __ ldeoral(Assembler::xword, zr, r16, r13);        \/\/       ldeoral xzr, x16, [x13]\n+    __ ldorral(Assembler::xword, r23, r12, r1);        \/\/       ldsetal x23, x12, [x1]\n+    __ ldsminal(Assembler::xword, r14, r9, r21);       \/\/       ldsminal        x14, x9, [x21]\n+    __ ldsmaxal(Assembler::xword, r16, r26, r15);      \/\/       ldsmaxal        x16, x26, [x15]\n+    __ lduminal(Assembler::xword, r4, r4, r15);        \/\/       lduminal        x4, x4, [x15]\n+    __ ldumaxal(Assembler::xword, r8, r6, r30);        \/\/       ldumaxal        x8, x6, [x30]\n@@ -1188,9 +1222,9 @@\n-    __ swpl(Assembler::xword, r16, r20, r23);          \/\/       swpl    x16, x20, [x23]\n-    __ ldaddl(Assembler::xword, r4, r16, r10);         \/\/       ldaddl  x4, x16, [x10]\n-    __ ldbicl(Assembler::xword, r23, r11, r25);        \/\/       ldclrl  x23, x11, [x25]\n-    __ ldeorl(Assembler::xword, r6, zr, r16);          \/\/       ldeorl  x6, xzr, [x16]\n-    __ ldorrl(Assembler::xword, r13, r23, r12);        \/\/       ldsetl  x13, x23, [x12]\n-    __ ldsminl(Assembler::xword, r1, r14, r9);         \/\/       ldsminl x1, x14, [x9]\n-    __ ldsmaxl(Assembler::xword, r21, r16, r26);       \/\/       ldsmaxl x21, x16, [x26]\n-    __ lduminl(Assembler::xword, r15, r4, r4);         \/\/       lduminl x15, x4, [x4]\n-    __ ldumaxl(Assembler::xword, r16, r8, r6);         \/\/       ldumaxl x16, x8, [x6]\n+    __ swpl(Assembler::xword, r4, r29, r17);           \/\/       swpl    x4, x29, [x17]\n+    __ ldaddl(Assembler::xword, r29, r26, r9);         \/\/       ldaddl  x29, x26, [x9]\n+    __ ldbicl(Assembler::xword, r15, r2, r11);         \/\/       ldclrl  x15, x2, [x11]\n+    __ ldeorl(Assembler::xword, r29, r3, r7);          \/\/       ldeorl  x29, x3, [x7]\n+    __ ldorrl(Assembler::xword, r1, r27, r21);         \/\/       ldsetl  x1, x27, [x21]\n+    __ ldsminl(Assembler::xword, r16, r14, r8);        \/\/       ldsminl x16, x14, [x8]\n+    __ ldsmaxl(Assembler::xword, r16, r22, r25);       \/\/       ldsmaxl x16, x22, [x25]\n+    __ lduminl(Assembler::xword, r5, r20, r21);        \/\/       lduminl x5, x20, [x21]\n+    __ ldumaxl(Assembler::xword, r16, r23, r16);       \/\/       ldumaxl x16, x23, [x16]\n@@ -1199,9 +1233,9 @@\n-    __ swp(Assembler::word, r30, r4, r29);             \/\/       swp     w30, w4, [x29]\n-    __ ldadd(Assembler::word, r17, r29, r26);          \/\/       ldadd   w17, w29, [x26]\n-    __ ldbic(Assembler::word, r9, r15, r2);            \/\/       ldclr   w9, w15, [x2]\n-    __ ldeor(Assembler::word, r11, r29, r3);           \/\/       ldeor   w11, w29, [x3]\n-    __ ldorr(Assembler::word, r7, r1, r27);            \/\/       ldset   w7, w1, [x27]\n-    __ ldsmin(Assembler::word, r21, r16, r14);         \/\/       ldsmin  w21, w16, [x14]\n-    __ ldsmax(Assembler::word, r8, r16, r22);          \/\/       ldsmax  w8, w16, [x22]\n-    __ ldumin(Assembler::word, r25, r5, r20);          \/\/       ldumin  w25, w5, [x20]\n-    __ ldumax(Assembler::word, r21, r16, r23);         \/\/       ldumax  w21, w16, [x23]\n+    __ swp(Assembler::word, r30, r20, r20);            \/\/       swp     w30, w20, [x20]\n+    __ ldadd(Assembler::word, r0, r4, r19);            \/\/       ldadd   w0, w4, [x19]\n+    __ ldbic(Assembler::word, r24, r4, r20);           \/\/       ldclr   w24, w4, [x20]\n+    __ ldeor(Assembler::word, r4, r24, r26);           \/\/       ldeor   w4, w24, [x26]\n+    __ ldorr(Assembler::word, r19, r2, r8);            \/\/       ldset   w19, w2, [x8]\n+    __ ldsmin(Assembler::word, r8, r14, r24);          \/\/       ldsmin  w8, w14, [x24]\n+    __ ldsmax(Assembler::word, r16, zr, r22);          \/\/       ldsmax  w16, wzr, [x22]\n+    __ ldumin(Assembler::word, r4, zr, r1);            \/\/       ldumin  w4, wzr, [x1]\n+    __ ldumax(Assembler::word, r10, r20, r12);         \/\/       ldumax  w10, w20, [x12]\n@@ -1210,9 +1244,9 @@\n-    __ swpa(Assembler::word, r16, r30, r20);           \/\/       swpa    w16, w30, [x20]\n-    __ ldadda(Assembler::word, r20, r0, r4);           \/\/       ldadda  w20, w0, [x4]\n-    __ ldbica(Assembler::word, r19, r24, r4);          \/\/       ldclra  w19, w24, [x4]\n-    __ ldeora(Assembler::word, r20, r4, r24);          \/\/       ldeora  w20, w4, [x24]\n-    __ ldorra(Assembler::word, r26, r19, r2);          \/\/       ldseta  w26, w19, [x2]\n-    __ ldsmina(Assembler::word, r8, r8, r14);          \/\/       ldsmina w8, w8, [x14]\n-    __ ldsmaxa(Assembler::word, r24, r16, sp);         \/\/       ldsmaxa w24, w16, [sp]\n-    __ ldumina(Assembler::word, r22, r4, sp);          \/\/       ldumina w22, w4, [sp]\n-    __ ldumaxa(Assembler::word, r1, r10, r20);         \/\/       ldumaxa w1, w10, [x20]\n+    __ swpa(Assembler::word, r0, r9, r7);              \/\/       swpa    w0, w9, [x7]\n+    __ ldadda(Assembler::word, r24, r16, r4);          \/\/       ldadda  w24, w16, [x4]\n+    __ ldbica(Assembler::word, r27, r6, r10);          \/\/       ldclra  w27, w6, [x10]\n+    __ ldeora(Assembler::word, r27, r24, r13);         \/\/       ldeora  w27, w24, [x13]\n+    __ ldorra(Assembler::word, r16, zr, r22);          \/\/       ldseta  w16, wzr, [x22]\n+    __ ldsmina(Assembler::word, r22, r20, sp);         \/\/       ldsmina w22, w20, [sp]\n+    __ ldsmaxa(Assembler::word, r29, r9, r14);         \/\/       ldsmaxa w29, w9, [x14]\n+    __ ldumina(Assembler::word, r20, r7, r20);         \/\/       ldumina w20, w7, [x20]\n+    __ ldumaxa(Assembler::word, r28, r9, r11);         \/\/       ldumaxa w28, w9, [x11]\n@@ -1221,9 +1255,9 @@\n-    __ swpal(Assembler::word, r12, r0, r9);            \/\/       swpal   w12, w0, [x9]\n-    __ ldaddal(Assembler::word, r7, r24, r15);         \/\/       ldaddal w7, w24, [x15]\n-    __ ldbical(Assembler::word, r4, r27, r6);          \/\/       ldclral w4, w27, [x6]\n-    __ ldeoral(Assembler::word, r10, r27, r24);        \/\/       ldeoral w10, w27, [x24]\n-    __ ldorral(Assembler::word, r13, r16, sp);         \/\/       ldsetal w13, w16, [sp]\n-    __ ldsminal(Assembler::word, r22, r22, r20);       \/\/       ldsminal        w22, w22, [x20]\n-    __ ldsmaxal(Assembler::word, zr, r29, r9);         \/\/       ldsmaxal        wzr, w29, [x9]\n-    __ lduminal(Assembler::word, r14, r20, r7);        \/\/       lduminal        w14, w20, [x7]\n-    __ ldumaxal(Assembler::word, r20, r28, r9);        \/\/       ldumaxal        w20, w28, [x9]\n+    __ swpal(Assembler::word, r14, r12, r20);          \/\/       swpal   w14, w12, [x20]\n+    __ ldaddal(Assembler::word, r1, r24, r9);          \/\/       ldaddal w1, w24, [x9]\n+    __ ldbical(Assembler::word, r19, r13, r19);        \/\/       ldclral w19, w13, [x19]\n+    __ ldeoral(Assembler::word, r16, r16, r5);         \/\/       ldeoral w16, w16, [x5]\n+    __ ldorral(Assembler::word, r0, r3, r12);          \/\/       ldsetal w0, w3, [x12]\n+    __ ldsminal(Assembler::word, r8, r15, r15);        \/\/       ldsminal        w8, w15, [x15]\n+    __ ldsmaxal(Assembler::word, r16, r4, r15);        \/\/       ldsmaxal        w16, w4, [x15]\n+    __ lduminal(Assembler::word, r30, r5, r0);         \/\/       lduminal        w30, w5, [x0]\n+    __ ldumaxal(Assembler::word, r10, r22, r27);       \/\/       ldumaxal        w10, w22, [x27]\n@@ -1232,9 +1266,9 @@\n-    __ swpl(Assembler::word, r11, r14, r12);           \/\/       swpl    w11, w14, [x12]\n-    __ ldaddl(Assembler::word, r20, r1, r24);          \/\/       ldaddl  w20, w1, [x24]\n-    __ ldbicl(Assembler::word, r9, r19, r13);          \/\/       ldclrl  w9, w19, [x13]\n-    __ ldeorl(Assembler::word, r19, r16, r16);         \/\/       ldeorl  w19, w16, [x16]\n-    __ ldorrl(Assembler::word, r5, r0, r3);            \/\/       ldsetl  w5, w0, [x3]\n-    __ ldsminl(Assembler::word, r12, r8, r15);         \/\/       ldsminl w12, w8, [x15]\n-    __ ldsmaxl(Assembler::word, r15, r16, r4);         \/\/       ldsmaxl w15, w16, [x4]\n-    __ lduminl(Assembler::word, r15, r30, r5);         \/\/       lduminl w15, w30, [x5]\n-    __ ldumaxl(Assembler::word, r0, r10, r22);         \/\/       ldumaxl w0, w10, [x22]\n+    __ swpl(Assembler::word, r3, r0, r9);              \/\/       swpl    w3, w0, [x9]\n+    __ ldaddl(Assembler::word, r19, r29, r10);         \/\/       ldaddl  w19, w29, [x10]\n+    __ ldbicl(Assembler::word, r24, r4, r20);          \/\/       ldclrl  w24, w4, [x20]\n+    __ ldeorl(Assembler::word, r7, r24, r29);          \/\/       ldeorl  w7, w24, [x29]\n+    __ ldorrl(Assembler::word, r14, r21, r11);         \/\/       ldsetl  w14, w21, [x11]\n+    __ ldsminl(Assembler::word, r27, r13, r15);        \/\/       ldsminl w27, w13, [x15]\n+    __ ldsmaxl(Assembler::word, zr, r17, r14);         \/\/       ldsmaxl wzr, w17, [x14]\n+    __ lduminl(Assembler::word, r3, r30, r16);         \/\/       lduminl w3, w30, [x16]\n+    __ ldumaxl(Assembler::word, r22, r20, r7);         \/\/       ldumaxl w22, w20, [x7]\n@@ -1243,4 +1277,4 @@\n-    __ bcax(v27, __ T16B, v3, v0, v9);                 \/\/       bcax            v27.16B, v3.16B, v0.16B, v9.16B\n-    __ eor3(v19, __ T16B, v29, v10, v24);              \/\/       eor3            v19.16B, v29.16B, v10.16B, v24.16B\n-    __ rax1(v4, __ T2D, v20, v7);                      \/\/       rax1            v4.2D, v20.2D, v7.2D\n-    __ xar(v24, __ T2D, v29, v14, 43);                 \/\/       xar             v24.2D, v29.2D, v14.2D, #43\n+    __ bcax(v20, __ T16B, v3, v1, v26);                \/\/       bcax            v20.16B, v3.16B, v1.16B, v26.16B\n+    __ eor3(v19, __ T16B, v9, v16, v17);               \/\/       eor3            v19.16B, v9.16B, v16.16B, v17.16B\n+    __ rax1(v21, __ T2D, v0, v4);                      \/\/       rax1            v21.2D, v0.2D, v4.2D\n+    __ xar(v2, __ T2D, v24, v14, 12);                  \/\/       xar             v2.2D, v24.2D, v14.2D, #12\n@@ -1249,4 +1283,4 @@\n-    __ sha512h(v11, __ T2D, v27, v13);                 \/\/       sha512h         q11, q27, v13.2D\n-    __ sha512h2(v18, __ T2D, v31, v17);                \/\/       sha512h2                q18, q31, v17.2D\n-    __ sha512su0(v14, __ T2D, v3);                     \/\/       sha512su0               v14.2D, v3.2D\n-    __ sha512su1(v30, __ T2D, v16, v22);               \/\/       sha512su1               v30.2D, v16.2D, v22.2D\n+    __ sha512h(v11, __ T2D, v21, v14);                 \/\/       sha512h         q11, q21, v14.2D\n+    __ sha512h2(v17, __ T2D, v30, v12);                \/\/       sha512h2                q17, q30, v12.2D\n+    __ sha512su0(v3, __ T2D, v3);                      \/\/       sha512su0               v3.2D, v3.2D\n+    __ sha512su1(v23, __ T2D, v9, v3);                 \/\/       sha512su1               v23.2D, v9.2D, v3.2D\n@@ -1255,5 +1289,5 @@\n-    __ sve_add(z20, __ B, 163u);                       \/\/       add     z20.b, z20.b, #0xa3\n-    __ sve_sub(z3, __ B, 215u);                        \/\/       sub     z3.b, z3.b, #0xd7\n-    __ sve_and(z19, __ H, 33279u);                     \/\/       and     z19.h, z19.h, #0x81ff\n-    __ sve_eor(z21, __ B, 12u);                        \/\/       eor     z21.b, z21.b, #0xc\n-    __ sve_orr(z24, __ H, 8064u);                      \/\/       orr     z24.h, z24.h, #0x1f80\n+    __ sve_add(z24, __ D, 26u);                        \/\/       add     z24.d, z24.d, #0x1a\n+    __ sve_sub(z19, __ S, 62u);                        \/\/       sub     z19.s, z19.s, #0x3e\n+    __ sve_and(z26, __ S, 917504u);                    \/\/       and     z26.s, z26.s, #0xe0000\n+    __ sve_eor(z8, __ D, 18442240474082197503u);       \/\/       eor     z8.d, z8.d, #0xfff0000000003fff\n+    __ sve_orr(z18, __ S, 253952u);                    \/\/       orr     z18.s, z18.s, #0x3e000\n@@ -1262,5 +1296,5 @@\n-    __ sve_add(z21, __ H, 139u);                       \/\/       add     z21.h, z21.h, #0x8b\n-    __ sve_sub(z30, __ H, 26u);                        \/\/       sub     z30.h, z30.h, #0x1a\n-    __ sve_and(z3, __ S, 122880u);                     \/\/       and     z3.s, z3.s, #0x1e000\n-    __ sve_eor(z24, __ D, 18158513714670600195u);      \/\/       eor     z24.d, z24.d, #0xfc000003fc000003\n-    __ sve_orr(z23, __ B, 191u);                       \/\/       orr     z23.b, z23.b, #0xbf\n+    __ sve_add(z9, __ S, 97u);                         \/\/       add     z9.s, z9.s, #0x61\n+    __ sve_sub(z8, __ H, 118u);                        \/\/       sub     z8.h, z8.h, #0x76\n+    __ sve_and(z19, __ S, 1056980736u);                \/\/       and     z19.s, z19.s, #0x3f003f00\n+    __ sve_eor(z25, __ S, 3758350339u);                \/\/       eor     z25.s, z25.s, #0xe003e003\n+    __ sve_orr(z9, __ S, 4294459391u);                 \/\/       orr     z9.s, z9.s, #0xfff83fff\n@@ -1269,5 +1303,5 @@\n-    __ sve_add(z14, __ B, 66u);                        \/\/       add     z14.b, z14.b, #0x42\n-    __ sve_sub(z26, __ B, 180u);                       \/\/       sub     z26.b, z26.b, #0xb4\n-    __ sve_and(z18, __ S, 253952u);                    \/\/       and     z18.s, z18.s, #0x3e000\n-    __ sve_eor(z9, __ S, 16744448u);                   \/\/       eor     z9.s, z9.s, #0xff8000\n-    __ sve_orr(z12, __ H, 33279u);                     \/\/       orr     z12.h, z12.h, #0x81ff\n+    __ sve_add(z23, __ D, 183u);                       \/\/       add     z23.d, z23.d, #0xb7\n+    __ sve_sub(z8, __ H, 41u);                         \/\/       sub     z8.h, z8.h, #0x29\n+    __ sve_and(z28, __ D, 8064u);                      \/\/       and     z28.d, z28.d, #0x1f80\n+    __ sve_eor(z15, __ D, 18428729675200069887u);      \/\/       eor     z15.d, z15.d, #0xffc00000000000ff\n+    __ sve_orr(z0, __ B, 239u);                        \/\/       orr     z0.b, z0.b, #0xef\n@@ -1276,5 +1310,5 @@\n-    __ sve_add(z11, __ H, 206u);                       \/\/       add     z11.h, z11.h, #0xce\n-    __ sve_sub(z18, __ D, 154u);                       \/\/       sub     z18.d, z18.d, #0x9a\n-    __ sve_and(z9, __ S, 4294459391u);                 \/\/       and     z9.s, z9.s, #0xfff83fff\n-    __ sve_eor(z23, __ D, 562675075514368u);           \/\/       eor     z23.d, z23.d, #0x1ffc000000000\n-    __ sve_orr(z8, __ B, 243u);                        \/\/       orr     z8.b, z8.b, #0xf3\n+    __ sve_add(z5, __ D, 243u);                        \/\/       add     z5.d, z5.d, #0xf3\n+    __ sve_sub(z19, __ S, 8u);                         \/\/       sub     z19.s, z19.s, #0x8\n+    __ sve_and(z13, __ H, 32256u);                     \/\/       and     z13.h, z13.h, #0x7e00\n+    __ sve_eor(z0, __ S, 4294967293u);                 \/\/       eor     z0.s, z0.s, #0xfffffffd\n+    __ sve_orr(z21, __ S, 4294965263u);                \/\/       orr     z21.s, z21.s, #0xfffff80f\n@@ -1283,5 +1317,5 @@\n-    __ sve_add(z10, __ B, 121u);                       \/\/       add     z10.b, z10.b, #0x79\n-    __ sve_sub(z25, __ S, 172u);                       \/\/       sub     z25.s, z25.s, #0xac\n-    __ sve_and(z0, __ B, 239u);                        \/\/       and     z0.b, z0.b, #0xef\n-    __ sve_eor(z5, __ D, 17870287719452639231u);       \/\/       eor     z5.d, z5.d, #0xf80003ffffffffff\n-    __ sve_orr(z17, __ B, 128u);                       \/\/       orr     z17.b, z17.b, #0x80\n+    __ sve_add(z12, __ H, 20u);                        \/\/       add     z12.h, z12.h, #0x14\n+    __ sve_sub(z0, __ H, 190u);                        \/\/       sub     z0.h, z0.h, #0xbe\n+    __ sve_and(z23, __ B, 239u);                       \/\/       and     z23.b, z23.b, #0xef\n+    __ sve_eor(z27, __ D, 18442240474082197503u);      \/\/       eor     z27.d, z27.d, #0xfff0000000003fff\n+    __ sve_orr(z22, __ B, 124u);                       \/\/       orr     z22.b, z22.b, #0x7c\n@@ -1290,5 +1324,5 @@\n-    __ sve_add(z30, __ H, 3u);                         \/\/       add     z30.h, z30.h, #0x3\n-    __ sve_sub(z18, __ B, 253u);                       \/\/       sub     z18.b, z18.b, #0xfd\n-    __ sve_and(z21, __ S, 4294965263u);                \/\/       and     z21.s, z21.s, #0xfffff80f\n-    __ sve_eor(z12, __ H, 1u);                         \/\/       eor     z12.h, z12.h, #0x1\n-    __ sve_orr(z15, __ S, 1u);                         \/\/       orr     z15.s, z15.s, #0x1\n+    __ sve_add(z20, __ H, 165u);                       \/\/       add     z20.h, z20.h, #0xa5\n+    __ sve_sub(z24, __ D, 72u);                        \/\/       sub     z24.d, z24.d, #0x48\n+    __ sve_and(z31, __ S, 4026535935u);                \/\/       and     z31.s, z31.s, #0xf0000fff\n+    __ sve_eor(z21, __ B, 128u);                       \/\/       eor     z21.b, z21.b, #0x80\n+    __ sve_orr(z30, __ S, 4294967293u);                \/\/       orr     z30.s, z30.s, #0xfffffffd\n@@ -1297,66 +1331,66 @@\n-    __ sve_add(z19, __ D, z26, z27);                   \/\/       add     z19.d, z26.d, z27.d\n-    __ sve_sub(z13, __ B, z22, z22);                   \/\/       sub     z13.b, z22.b, z22.b\n-    __ sve_fadd(z1, __ S, z11, z20);                   \/\/       fadd    z1.s, z11.s, z20.s\n-    __ sve_fmul(z20, __ S, z24, z24);                  \/\/       fmul    z20.s, z24.s, z24.s\n-    __ sve_fsub(z31, __ D, z17, z20);                  \/\/       fsub    z31.d, z17.d, z20.d\n-    __ sve_sqadd(z21, __ H, z4, z21);                  \/\/       sqadd   z21.h, z4.h, z21.h\n-    __ sve_sqsub(z30, __ D, z22, z31);                 \/\/       sqsub   z30.d, z22.d, z31.d\n-    __ sve_uqadd(z26, __ H, z18, z19);                 \/\/       uqadd   z26.h, z18.h, z19.h\n-    __ sve_uqsub(z11, __ S, z13, z29);                 \/\/       uqsub   z11.s, z13.s, z29.s\n-    __ sve_abs(z5, __ H, p0, z14);                     \/\/       abs     z5.h, p0\/m, z14.h\n-    __ sve_add(z2, __ H, p1, z10);                     \/\/       add     z2.h, p1\/m, z2.h, z10.h\n-    __ sve_and(z19, __ H, p1, z26);                    \/\/       and     z19.h, p1\/m, z19.h, z26.h\n-    __ sve_asr(z2, __ B, p0, z30);                     \/\/       asr     z2.b, p0\/m, z2.b, z30.b\n-    __ sve_bic(z20, __ D, p1, z20);                    \/\/       bic     z20.d, p1\/m, z20.d, z20.d\n-    __ sve_clz(z29, __ H, p3, z13);                    \/\/       clz     z29.h, p3\/m, z13.h\n-    __ sve_cnt(z14, __ H, p7, z1);                     \/\/       cnt     z14.h, p7\/m, z1.h\n-    __ sve_eor(z28, __ D, p0, z3);                     \/\/       eor     z28.d, p0\/m, z28.d, z3.d\n-    __ sve_lsl(z9, __ B, p6, z9);                      \/\/       lsl     z9.b, p6\/m, z9.b, z9.b\n-    __ sve_lsr(z26, __ B, p2, z14);                    \/\/       lsr     z26.b, p2\/m, z26.b, z14.b\n-    __ sve_mul(z20, __ D, p6, z7);                     \/\/       mul     z20.d, p6\/m, z20.d, z7.d\n-    __ sve_neg(z20, __ D, p4, z6);                     \/\/       neg     z20.d, p4\/m, z6.d\n-    __ sve_not(z13, __ H, p0, z29);                    \/\/       not     z13.h, p0\/m, z29.h\n-    __ sve_orr(z9, __ B, p0, z1);                      \/\/       orr     z9.b, p0\/m, z9.b, z1.b\n-    __ sve_rbit(z27, __ B, p6, z15);                   \/\/       rbit    z27.b, p6\/m, z15.b\n-    __ sve_revb(z4, __ D, p7, z17);                    \/\/       revb    z4.d, p7\/m, z17.d\n-    __ sve_smax(z2, __ B, p0, z24);                    \/\/       smax    z2.b, p0\/m, z2.b, z24.b\n-    __ sve_smin(z26, __ B, p7, z13);                   \/\/       smin    z26.b, p7\/m, z26.b, z13.b\n-    __ sve_umax(z22, __ D, p3, z16);                   \/\/       umax    z22.d, p3\/m, z22.d, z16.d\n-    __ sve_umin(z17, __ D, p1, z11);                   \/\/       umin    z17.d, p1\/m, z17.d, z11.d\n-    __ sve_sub(z16, __ B, p0, z16);                    \/\/       sub     z16.b, p0\/m, z16.b, z16.b\n-    __ sve_fabs(z28, __ D, p1, z23);                   \/\/       fabs    z28.d, p1\/m, z23.d\n-    __ sve_fadd(z28, __ D, p4, z10);                   \/\/       fadd    z28.d, p4\/m, z28.d, z10.d\n-    __ sve_fdiv(z17, __ D, p7, z7);                    \/\/       fdiv    z17.d, p7\/m, z17.d, z7.d\n-    __ sve_fmax(z4, __ S, p3, z24);                    \/\/       fmax    z4.s, p3\/m, z4.s, z24.s\n-    __ sve_fmin(z9, __ S, p2, z11);                    \/\/       fmin    z9.s, p2\/m, z9.s, z11.s\n-    __ sve_fmul(z4, __ D, p5, z22);                    \/\/       fmul    z4.d, p5\/m, z4.d, z22.d\n-    __ sve_fneg(z4, __ S, p0, z15);                    \/\/       fneg    z4.s, p0\/m, z15.s\n-    __ sve_frintm(z4, __ D, p7, z26);                  \/\/       frintm  z4.d, p7\/m, z26.d\n-    __ sve_frintn(z5, __ S, p5, z26);                  \/\/       frintn  z5.s, p5\/m, z26.s\n-    __ sve_frintp(z31, __ S, p0, z25);                 \/\/       frintp  z31.s, p0\/m, z25.s\n-    __ sve_fsqrt(z8, __ D, p1, z3);                    \/\/       fsqrt   z8.d, p1\/m, z3.d\n-    __ sve_fsub(z7, __ D, p6, z24);                    \/\/       fsub    z7.d, p6\/m, z7.d, z24.d\n-    __ sve_fmad(z24, __ S, p7, z17, z1);               \/\/       fmad    z24.s, p7\/m, z17.s, z1.s\n-    __ sve_fmla(z12, __ D, p7, z13, z8);               \/\/       fmla    z12.d, p7\/m, z13.d, z8.d\n-    __ sve_fmls(z29, __ D, p0, z31, z23);              \/\/       fmls    z29.d, p0\/m, z31.d, z23.d\n-    __ sve_fmsb(z20, __ D, p0, z21, z7);               \/\/       fmsb    z20.d, p0\/m, z21.d, z7.d\n-    __ sve_fnmad(z29, __ D, p6, z22, z8);              \/\/       fnmad   z29.d, p6\/m, z22.d, z8.d\n-    __ sve_fnmsb(z26, __ D, p5, z5, z6);               \/\/       fnmsb   z26.d, p5\/m, z5.d, z6.d\n-    __ sve_fnmla(z18, __ S, p3, z26, z21);             \/\/       fnmla   z18.s, p3\/m, z26.s, z21.s\n-    __ sve_fnmls(z0, __ S, p4, z10, z28);              \/\/       fnmls   z0.s, p4\/m, z10.s, z28.s\n-    __ sve_mla(z17, __ D, p1, z30, z20);               \/\/       mla     z17.d, p1\/m, z30.d, z20.d\n-    __ sve_mls(z28, __ S, p3, z17, z14);               \/\/       mls     z28.s, p3\/m, z17.s, z14.s\n-    __ sve_and(z10, z26, z11);                         \/\/       and     z10.d, z26.d, z11.d\n-    __ sve_eor(z0, z11, z15);                          \/\/       eor     z0.d, z11.d, z15.d\n-    __ sve_orr(z23, z23, z20);                         \/\/       orr     z23.d, z23.d, z20.d\n-    __ sve_bic(z23, z20, z29);                         \/\/       bic     z23.d, z20.d, z29.d\n-    __ sve_uzp1(z0, __ S, z27, z6);                    \/\/       uzp1    z0.s, z27.s, z6.s\n-    __ sve_uzp2(z13, __ H, z12, z4);                   \/\/       uzp2    z13.h, z12.h, z4.h\n-    __ sve_fabd(z31, __ D, p6, z23);                   \/\/       fabd    z31.d, p6\/m, z31.d, z23.d\n-    __ sve_bext(z6, __ D, z2, z29);                    \/\/       bext    z6.d, z2.d, z29.d\n-    __ sve_bdep(z0, __ B, z29, z23);                   \/\/       bdep    z0.b, z29.b, z23.b\n-    __ sve_eor3(z4, z5, z8);                           \/\/       eor3    z4.d, z4.d, z5.d, z8.d\n-    __ sve_sqadd(z13, __ H, p4, z13);                  \/\/       sqadd   z13.h, p4\/m, z13.h, z13.h\n-    __ sve_sqsub(z8, __ H, p2, z8);                    \/\/       sqsub   z8.h, p2\/m, z8.h, z8.h\n-    __ sve_uqadd(z19, __ S, p0, z29);                  \/\/       uqadd   z19.s, p0\/m, z19.s, z29.s\n-    __ sve_uqsub(z16, __ D, p3, z23);                  \/\/       uqsub   z16.d, p3\/m, z16.d, z23.d\n+    __ sve_add(z26, __ H, z18, z19);                   \/\/       add     z26.h, z18.h, z19.h\n+    __ sve_sub(z11, __ S, z13, z29);                   \/\/       sub     z11.s, z13.s, z29.s\n+    __ sve_fadd(z5, __ S, z1, z14);                    \/\/       fadd    z5.s, z1.s, z14.s\n+    __ sve_fmul(z2, __ S, z7, z10);                    \/\/       fmul    z2.s, z7.s, z10.s\n+    __ sve_fsub(z19, __ S, z4, z26);                   \/\/       fsub    z19.s, z4.s, z26.s\n+    __ sve_sqadd(z2, __ B, z3, z30);                   \/\/       sqadd   z2.b, z3.b, z30.b\n+    __ sve_sqsub(z20, __ D, z5, z20);                  \/\/       sqsub   z20.d, z5.d, z20.d\n+    __ sve_uqadd(z29, __ H, z13, z13);                 \/\/       uqadd   z29.h, z13.h, z13.h\n+    __ sve_uqsub(z14, __ H, z30, z1);                  \/\/       uqsub   z14.h, z30.h, z1.h\n+    __ sve_abs(z28, __ D, p0, z3);                     \/\/       abs     z28.d, p0\/m, z3.d\n+    __ sve_add(z9, __ B, p6, z9);                      \/\/       add     z9.b, p6\/m, z9.b, z9.b\n+    __ sve_and(z26, __ B, p2, z14);                    \/\/       and     z26.b, p2\/m, z26.b, z14.b\n+    __ sve_asr(z20, __ D, p6, z7);                     \/\/       asr     z20.d, p6\/m, z20.d, z7.d\n+    __ sve_bic(z20, __ D, p4, z6);                     \/\/       bic     z20.d, p4\/m, z20.d, z6.d\n+    __ sve_clz(z13, __ H, p0, z29);                    \/\/       clz     z13.h, p0\/m, z29.h\n+    __ sve_cnt(z9, __ B, p0, z1);                      \/\/       cnt     z9.b, p0\/m, z1.b\n+    __ sve_eor(z27, __ B, p6, z15);                    \/\/       eor     z27.b, p6\/m, z27.b, z15.b\n+    __ sve_lsl(z4, __ D, p7, z17);                     \/\/       lsl     z4.d, p7\/m, z4.d, z17.d\n+    __ sve_lsr(z2, __ B, p0, z24);                     \/\/       lsr     z2.b, p0\/m, z2.b, z24.b\n+    __ sve_mul(z26, __ B, p7, z13);                    \/\/       mul     z26.b, p7\/m, z26.b, z13.b\n+    __ sve_neg(z22, __ D, p3, z16);                    \/\/       neg     z22.d, p3\/m, z16.d\n+    __ sve_not(z17, __ D, p1, z11);                    \/\/       not     z17.d, p1\/m, z11.d\n+    __ sve_orr(z16, __ B, p0, z16);                    \/\/       orr     z16.b, p0\/m, z16.b, z16.b\n+    __ sve_rbit(z28, __ D, p1, z23);                   \/\/       rbit    z28.d, p1\/m, z23.d\n+    __ sve_revb(z28, __ D, p4, z10);                   \/\/       revb    z28.d, p4\/m, z10.d\n+    __ sve_smax(z17, __ S, p7, z7);                    \/\/       smax    z17.s, p7\/m, z17.s, z7.s\n+    __ sve_smin(z4, __ H, p3, z24);                    \/\/       smin    z4.h, p3\/m, z4.h, z24.h\n+    __ sve_umax(z9, __ B, p2, z11);                    \/\/       umax    z9.b, p2\/m, z9.b, z11.b\n+    __ sve_umin(z4, __ S, p5, z22);                    \/\/       umin    z4.s, p5\/m, z4.s, z22.s\n+    __ sve_sub(z4, __ H, p0, z15);                     \/\/       sub     z4.h, p0\/m, z4.h, z15.h\n+    __ sve_fabs(z4, __ D, p7, z26);                    \/\/       fabs    z4.d, p7\/m, z26.d\n+    __ sve_fadd(z5, __ S, p5, z26);                    \/\/       fadd    z5.s, p5\/m, z5.s, z26.s\n+    __ sve_fdiv(z31, __ S, p0, z25);                   \/\/       fdiv    z31.s, p0\/m, z31.s, z25.s\n+    __ sve_fmax(z8, __ D, p1, z3);                     \/\/       fmax    z8.d, p1\/m, z8.d, z3.d\n+    __ sve_fmin(z7, __ D, p6, z24);                    \/\/       fmin    z7.d, p6\/m, z7.d, z24.d\n+    __ sve_fmul(z24, __ S, p7, z17);                   \/\/       fmul    z24.s, p7\/m, z24.s, z17.s\n+    __ sve_fneg(z10, __ S, p3, z30);                   \/\/       fneg    z10.s, p3\/m, z30.s\n+    __ sve_frintm(z8, __ S, p6, z29);                  \/\/       frintm  z8.s, p6\/m, z29.s\n+    __ sve_frintn(z31, __ D, p5, z31);                 \/\/       frintn  z31.d, p5\/m, z31.d\n+    __ sve_frintp(z0, __ D, p5, z7);                   \/\/       frintp  z0.d, p5\/m, z7.d\n+    __ sve_fsqrt(z29, __ S, p6, z22);                  \/\/       fsqrt   z29.s, p6\/m, z22.s\n+    __ sve_fsub(z29, __ S, p6, z20);                   \/\/       fsub    z29.s, p6\/m, z29.s, z20.s\n+    __ sve_fmad(z6, __ D, p4, z18, z13);               \/\/       fmad    z6.d, p4\/m, z18.d, z13.d\n+    __ sve_fmla(z21, __ S, p2, z0, z19);               \/\/       fmla    z21.s, p2\/m, z0.s, z19.s\n+    __ sve_fmls(z28, __ D, p1, z17, z6);               \/\/       fmls    z28.d, p1\/m, z17.d, z6.d\n+    __ sve_fmsb(z20, __ D, p6, z28, z14);              \/\/       fmsb    z20.d, p6\/m, z28.d, z14.d\n+    __ sve_fnmad(z14, __ S, p4, z10, z26);             \/\/       fnmad   z14.s, p4\/m, z10.s, z26.s\n+    __ sve_fnmsb(z24, __ D, p0, z11, z15);             \/\/       fnmsb   z24.d, p0\/m, z11.d, z15.d\n+    __ sve_fnmla(z23, __ D, p5, z20, z28);             \/\/       fnmla   z23.d, p5\/m, z20.d, z28.d\n+    __ sve_fnmls(z20, __ D, p7, z24, z0);              \/\/       fnmls   z20.d, p7\/m, z24.d, z0.d\n+    __ sve_mla(z6, __ B, p5, z13, z12);                \/\/       mla     z6.b, p5\/m, z13.b, z12.b\n+    __ sve_mls(z13, __ S, p7, z26, z23);               \/\/       mls     z13.s, p7\/m, z26.s, z23.s\n+    __ sve_and(z6, z2, z29);                           \/\/       and     z6.d, z2.d, z29.d\n+    __ sve_eor(z0, z29, z23);                          \/\/       eor     z0.d, z29.d, z23.d\n+    __ sve_orr(z4, z5, z8);                            \/\/       orr     z4.d, z5.d, z8.d\n+    __ sve_bic(z13, z17, z13);                         \/\/       bic     z13.d, z17.d, z13.d\n+    __ sve_uzp1(z8, __ H, z10, z8);                    \/\/       uzp1    z8.h, z10.h, z8.h\n+    __ sve_uzp2(z19, __ S, z0, z29);                   \/\/       uzp2    z19.s, z0.s, z29.s\n+    __ sve_fabd(z16, __ D, p3, z23);                   \/\/       fabd    z16.d, p3\/m, z16.d, z23.d\n+    __ sve_bext(z23, __ B, z30, z13);                  \/\/       bext    z23.b, z30.b, z13.b\n+    __ sve_bdep(z25, __ H, z22, z0);                   \/\/       bdep    z25.h, z22.h, z0.h\n+    __ sve_eor3(z25, z30, z11);                        \/\/       eor3    z25.d, z25.d, z30.d, z11.d\n+    __ sve_sqadd(z14, __ H, p5, z22);                  \/\/       sqadd   z14.h, p5\/m, z14.h, z22.h\n+    __ sve_sqsub(z5, __ H, p4, z0);                    \/\/       sqsub   z5.h, p4\/m, z5.h, z0.h\n+    __ sve_uqadd(z9, __ D, p0, z3);                    \/\/       uqadd   z9.d, p0\/m, z9.d, z3.d\n+    __ sve_uqsub(z14, __ H, p1, z29);                  \/\/       uqsub   z14.h, p1\/m, z14.h, z29.h\n@@ -1365,9 +1399,9 @@\n-    __ sve_andv(v23, __ B, p7, z13);                   \/\/       andv b23, p7, z13.b\n-    __ sve_orv(v25, __ H, p5, z0);                     \/\/       orv h25, p5, z0.h\n-    __ sve_eorv(v25, __ H, p7, z11);                   \/\/       eorv h25, p7, z11.h\n-    __ sve_smaxv(v14, __ H, p5, z22);                  \/\/       smaxv h14, p5, z22.h\n-    __ sve_sminv(v5, __ H, p4, z0);                    \/\/       sminv h5, p4, z0.h\n-    __ sve_fminv(v9, __ D, p0, z3);                    \/\/       fminv d9, p0, z3.d\n-    __ sve_fmaxv(v14, __ S, p1, z29);                  \/\/       fmaxv s14, p1, z29.s\n-    __ sve_fadda(v14, __ D, p5, z4);                   \/\/       fadda d14, p5, d14, z4.d\n-    __ sve_uaddv(v27, __ S, p3, z22);                  \/\/       uaddv d27, p3, z22.s\n+    __ sve_andv(v14, __ D, p5, z4);                    \/\/       andv d14, p5, z4.d\n+    __ sve_orv(v27, __ S, p3, z22);                    \/\/       orv s27, p3, z22.s\n+    __ sve_eorv(v31, __ S, p6, z11);                   \/\/       eorv s31, p6, z11.s\n+    __ sve_smaxv(v12, __ B, p4, z28);                  \/\/       smaxv b12, p4, z28.b\n+    __ sve_sminv(v28, __ D, p4, z4);                   \/\/       sminv d28, p4, z4.d\n+    __ sve_fminv(v6, __ D, p0, z15);                   \/\/       fminv d6, p0, z15.d\n+    __ sve_fmaxv(v1, __ D, p5, z18);                   \/\/       fmaxv d1, p5, z18.d\n+    __ sve_fadda(v2, __ S, p2, z4);                    \/\/       fadda s2, p2, s2, z4.s\n+    __ sve_uaddv(v11, __ S, p2, z28);                  \/\/       uaddv d11, p2, z28.s\n@@ -1376,12 +1410,12 @@\n-    __ saddwv(v31, v0, __ T8H, v1, __ T8B);            \/\/       saddw   v31.8H, v0.8H, v1.8B\n-    __ saddwv2(v24, v25, __ T8H, v26, __ T16B);        \/\/       saddw2  v24.8H, v25.8H, v26.16B\n-    __ saddwv(v11, v12, __ T4S, v13, __ T4H);          \/\/       saddw   v11.4S, v12.4S, v13.4H\n-    __ saddwv2(v16, v17, __ T4S, v18, __ T8H);         \/\/       saddw2  v16.4S, v17.4S, v18.8H\n-    __ saddwv(v12, v13, __ T2D, v14, __ T2S);          \/\/       saddw   v12.2D, v13.2D, v14.2S\n-    __ saddwv2(v17, v18, __ T2D, v19, __ T4S);         \/\/       saddw2  v17.2D, v18.2D, v19.4S\n-    __ uaddwv(v28, v29, __ T8H, v30, __ T8B);          \/\/       uaddw   v28.8H, v29.8H, v30.8B\n-    __ uaddwv2(v3, v4, __ T8H, v5, __ T16B);           \/\/       uaddw2  v3.8H, v4.8H, v5.16B\n-    __ uaddwv(v28, v29, __ T4S, v30, __ T4H);          \/\/       uaddw   v28.4S, v29.4S, v30.4H\n-    __ uaddwv2(v16, v17, __ T4S, v18, __ T8H);         \/\/       uaddw2  v16.4S, v17.4S, v18.8H\n-    __ uaddwv(v4, v5, __ T2D, v6, __ T2S);             \/\/       uaddw   v4.2D, v5.2D, v6.2S\n-    __ uaddwv2(v29, v30, __ T2D, v31, __ T4S);         \/\/       uaddw2  v29.2D, v30.2D, v31.4S\n+    __ saddwv(v3, v4, __ T8H, v5, __ T8B);             \/\/       saddw   v3.8H, v4.8H, v5.8B\n+    __ saddwv2(v21, v22, __ T8H, v23, __ T16B);        \/\/       saddw2  v21.8H, v22.8H, v23.16B\n+    __ saddwv(v31, v0, __ T4S, v1, __ T4H);            \/\/       saddw   v31.4S, v0.4S, v1.4H\n+    __ saddwv2(v11, v12, __ T4S, v13, __ T8H);         \/\/       saddw2  v11.4S, v12.4S, v13.8H\n+    __ saddwv(v24, v25, __ T2D, v26, __ T2S);          \/\/       saddw   v24.2D, v25.2D, v26.2S\n+    __ saddwv2(v21, v22, __ T2D, v23, __ T4S);         \/\/       saddw2  v21.2D, v22.2D, v23.4S\n+    __ uaddwv(v15, v16, __ T8H, v17, __ T8B);          \/\/       uaddw   v15.8H, v16.8H, v17.8B\n+    __ uaddwv2(v12, v13, __ T8H, v14, __ T16B);        \/\/       uaddw2  v12.8H, v13.8H, v14.16B\n+    __ uaddwv(v6, v7, __ T4S, v8, __ T4H);             \/\/       uaddw   v6.4S, v7.4S, v8.4H\n+    __ uaddwv2(v13, v14, __ T4S, v15, __ T8H);         \/\/       uaddw2  v13.4S, v14.4S, v15.8H\n+    __ uaddwv(v8, v9, __ T2D, v10, __ T2S);            \/\/       uaddw   v8.2D, v9.2D, v10.2S\n+    __ uaddwv2(v15, v16, __ T2D, v17, __ T4S);         \/\/       uaddw2  v15.2D, v16.2D, v17.4S\n@@ -1406,7 +1440,7 @@\n-    0x14000000,     0x17ffffd7,     0x1400048d,     0x94000000,\n-    0x97ffffd4,     0x9400048a,     0x3400000a,     0x34fffa2a,\n-    0x340090ea,     0x35000008,     0x35fff9c8,     0x35009088,\n-    0xb400000b,     0xb4fff96b,     0xb400902b,     0xb500001d,\n-    0xb5fff91d,     0xb5008fdd,     0x10000013,     0x10fff8b3,\n-    0x10008f73,     0x90000013,     0x36300016,     0x3637f836,\n-    0x36308ef6,     0x3758000c,     0x375ff7cc,     0x37588e8c,\n+    0x14000000,     0x17ffffd7,     0x140004af,     0x94000000,\n+    0x97ffffd4,     0x940004ac,     0x3400000a,     0x34fffa2a,\n+    0x3400952a,     0x35000008,     0x35fff9c8,     0x350094c8,\n+    0xb400000b,     0xb4fff96b,     0xb400946b,     0xb500001d,\n+    0xb5fff91d,     0xb500941d,     0x10000013,     0x10fff8b3,\n+    0x100093b3,     0x90000013,     0x36300016,     0x3637f836,\n+    0x36309336,     0x3758000c,     0x375ff7cc,     0x375892cc,\n@@ -1417,13 +1451,13 @@\n-    0x54008c60,     0x54000001,     0x54fff541,     0x54008c01,\n-    0x54000002,     0x54fff4e2,     0x54008ba2,     0x54000002,\n-    0x54fff482,     0x54008b42,     0x54000003,     0x54fff423,\n-    0x54008ae3,     0x54000003,     0x54fff3c3,     0x54008a83,\n-    0x54000004,     0x54fff364,     0x54008a24,     0x54000005,\n-    0x54fff305,     0x540089c5,     0x54000006,     0x54fff2a6,\n-    0x54008966,     0x54000007,     0x54fff247,     0x54008907,\n-    0x54000008,     0x54fff1e8,     0x540088a8,     0x54000009,\n-    0x54fff189,     0x54008849,     0x5400000a,     0x54fff12a,\n-    0x540087ea,     0x5400000b,     0x54fff0cb,     0x5400878b,\n-    0x5400000c,     0x54fff06c,     0x5400872c,     0x5400000d,\n-    0x54fff00d,     0x540086cd,     0x5400000e,     0x54ffefae,\n-    0x5400866e,     0x5400000f,     0x54ffef4f,     0x5400860f,\n+    0x540090a0,     0x54000001,     0x54fff541,     0x54009041,\n+    0x54000002,     0x54fff4e2,     0x54008fe2,     0x54000002,\n+    0x54fff482,     0x54008f82,     0x54000003,     0x54fff423,\n+    0x54008f23,     0x54000003,     0x54fff3c3,     0x54008ec3,\n+    0x54000004,     0x54fff364,     0x54008e64,     0x54000005,\n+    0x54fff305,     0x54008e05,     0x54000006,     0x54fff2a6,\n+    0x54008da6,     0x54000007,     0x54fff247,     0x54008d47,\n+    0x54000008,     0x54fff1e8,     0x54008ce8,     0x54000009,\n+    0x54fff189,     0x54008c89,     0x5400000a,     0x54fff12a,\n+    0x54008c2a,     0x5400000b,     0x54fff0cb,     0x54008bcb,\n+    0x5400000c,     0x54fff06c,     0x54008b6c,     0x5400000d,\n+    0x54fff00d,     0x54008b0d,     0x5400000e,     0x54ffefae,\n+    0x54008aae,     0x5400000f,     0x54ffef4f,     0x54008a4f,\n@@ -1531,56 +1565,63 @@\n-    0x2ea0f841,     0x6ea0f820,     0x6ee0fb38,     0x2ea1f8a4,\n-    0x6ea1f883,     0x6ee1f9ac,     0x2e20581f,     0x6e205bbc,\n-    0x0e2c1d6a,     0x4e3c1f7a,     0x0ea41c62,     0x4eae1dac,\n-    0x2e341e72,     0x6e211c1f,     0x0e238441,     0x4e2f85cd,\n-    0x0e7f87dd,     0x4e628420,     0x0eb58693,     0x4eae85ac,\n-    0x4ef38651,     0x0e380ef6,     0x4e2f0dcd,     0x0e7e0fbc,\n-    0x4e600ffe,     0x0ea10c1f,     0x4ea30c41,     0x4efc0f7a,\n-    0x2e3e0fbc,     0x6e260ca4,     0x2e600ffe,     0x6e660ca4,\n-    0x2ea80ce6,     0x6ea00ffe,     0x6efc0f7a,     0x0e34d672,\n-    0x4e2bd549,     0x4e6ad528,     0x2e2e85ac,     0x6e228420,\n-    0x2e7686b4,     0x6e638441,     0x2eba8738,     0x6ea48462,\n-    0x6ee28420,     0x0e2b2d49,     0x4e3a2f38,     0x0e7c2f7a,\n-    0x4e722e30,     0x0ea02ffe,     0x4ea52c83,     0x4eec2d6a,\n-    0x2e392f17,     0x6e2c2d6a,     0x2e662ca4,     0x6e742e72,\n-    0x2ea42c62,     0x6ead2d8b,     0x6eea2d28,     0x0eacd56a,\n-    0x4eb1d60f,     0x4ef3d651,     0x0e249c62,     0x4e2c9d6a,\n-    0x0e6e9dac,     0x4e6e9dac,     0x0eb19e0f,     0x4eaf9dcd,\n-    0x2ea4d462,     0x6ea9d507,     0x6ef6d6b4,     0x2e3cd77a,\n-    0x6e32d630,     0x6e66d4a4,     0x2e24dc62,     0x6e26dca4,\n-    0x6e6eddac,     0x0e749672,     0x4e7796d5,     0x0eb29630,\n-    0x4eb49672,     0x0e2dcd8b,     0x4e37ced5,     0x4e79cf17,\n-    0x2e6e95ac,     0x6e7c977a,     0x2eb99717,     0x6ebe97bc,\n-    0x0eb0cdee,     0x4eadcd8b,     0x4efacf38,     0x2e23fc41,\n-    0x6e2efdac,     0x6e61fc1f,     0x0e2c656a,     0x4e326630,\n-    0x0e696507,     0x4e646462,     0x0ea56483,     0x4eaf65cd,\n-    0x2e356693,     0x6e336651,     0x2e726630,     0x6e656483,\n-    0x2ea36441,     0x6ead658b,     0x0e20a7fe,     0x4e27a4c5,\n-    0x0e6aa528,     0x4e71a60f,     0x0ebfa7dd,     0x4ea0a7fe,\n-    0x0e22f420,     0x4e36f6b4,     0x4e69f507,     0x0e366eb4,\n-    0x4e396f17,     0x0e7e6fbc,     0x4e776ed5,     0x0ebd6f9b,\n-    0x4ebb6f59,     0x2e276cc5,     0x6e236c41,     0x2e796f17,\n-    0x6e726e30,     0x2ea16c1f,     0x6ea76cc5,     0x0e2eadac,\n-    0x4e2bad49,     0x0e7eafbc,     0x4e71ae0f,     0x0ebfafdd,\n-    0x4eb8aef6,     0x0e61b41f,     0x4e75b693,     0x0ea1b41f,\n-    0x4ea7b4c5,     0x0e3025ee,     0x4e342672,     0x0e61241f,\n-    0x4e742672,     0x0ebd279b,     0x4eb626b4,     0x0eb2f630,\n-    0x4eaef5ac,     0x4eedf58b,     0x2eabed49,     0x6ea8ece6,\n-    0x6ee0effe,     0x0faf11cd,     0x4fa1880f,     0x4fc710c5,\n-    0x0fa750c5,     0x4f8e81ac,     0x4fca5928,     0x2fa39041,\n-    0x4fa98907,     0x6fcb9949,     0x0f6d818b,     0x4f498107,\n-    0x0f8880e6,     0x4f8788c5,     0x0e2f35cd,     0x4e393717,\n-    0x0e633441,     0x4e6037fe,     0x0eb53693,     0x4ea734c5,\n-    0x4ef33651,     0x0e243c62,     0x4e323e30,     0x0e783ef6,\n-    0x4e6f3dcd,     0x0eac3d6a,     0x4eb73ed5,     0x4eff3fdd,\n-    0x2e3d8f9b,     0x6e2e8dac,     0x2e7d8f9b,     0x6e658c83,\n-    0x2ea38c41,     0x6ea18c1f,     0x6efa8f38,     0x2e353693,\n-    0x6e333651,     0x2e6b3549,     0x6e7e37bc,     0x2ebd379b,\n-    0x6eb1360f,     0x6ee93507,     0x2e373ed5,     0x6e393f17,\n-    0x2e613c1f,     0x6e7b3f59,     0x2ea43c62,     0x6ea13c1f,\n-    0x6efd3f9b,     0x0e34e672,     0x4e2ce56a,     0x4e79e717,\n-    0x2eb5e693,     0x6ea5e483,     0x6ef4e672,     0x2e22e420,\n-    0x6e3be759,     0x6e7ce77a,     0x65d22c4b,     0x65d03f92,\n-    0x65902b68,     0x6591264e,     0x659135f3,     0x65d33444,\n-    0x25c180af,     0x25cc0897,     0x25c71b6b,     0x25103080,\n-    0x251729f2,     0x25c48552,     0x24288aab,     0x242213f8,\n-    0x24fb63d6,     0x247cafab,     0xba5fd3e3,     0x3a5f03e5,\n+    0x0ef8f841,     0x4ef8f820,     0x2ea0fb38,     0x6ea0f8a4,\n+    0x6ee0f883,     0x2ef8f9ac,     0x6ef8f81f,     0x2ea1fbbc,\n+    0x6ea1f96a,     0x6ee1fb7a,     0x2ef9f862,     0x6ef9f9ac,\n+    0x2e205a72,     0x6e20581f,     0x0e231c41,     0x4e2f1dcd,\n+    0x0ebf1fdd,     0x4ea21c20,     0x2e351e93,     0x6e2e1dac,\n+    0x0e338651,     0x4e3886f6,     0x0e6f85cd,     0x4e7e87bc,\n+    0x0ea087fe,     0x4ea1841f,     0x4ee38441,     0x0e3c0f7a,\n+    0x4e3e0fbc,     0x0e660ca4,     0x4e600ffe,     0x0ea60ca4,\n+    0x4ea80ce6,     0x4ee00ffe,     0x2e3c0f7a,     0x6e340e72,\n+    0x2e6b0d49,     0x6e6a0d28,     0x2eae0dac,     0x6ea20c20,\n+    0x6ef60eb4,     0x0e23d441,     0x4e3ad738,     0x4e64d462,\n+    0x0e421420,     0x4e4b1549,     0x2e3a8738,     0x6e3c877a,\n+    0x2e728630,     0x6e6087fe,     0x2ea58483,     0x6eac856a,\n+    0x6ef98717,     0x0e2c2d6a,     0x4e262ca4,     0x0e742e72,\n+    0x4e642c62,     0x0ead2d8b,     0x4eaa2d28,     0x4eec2d6a,\n+    0x2e312e0f,     0x6e332e51,     0x2e642c62,     0x6e6c2d6a,\n+    0x2eae2dac,     0x6eae2dac,     0x6ef12e0f,     0x0eafd5cd,\n+    0x4ea4d462,     0x4ee9d507,     0x0ed616b4,     0x4edc177a,\n+    0x0e329e30,     0x4e269ca4,     0x0e649c62,     0x4e669ca4,\n+    0x0eae9dac,     0x4eb49e72,     0x2eb7d6d5,     0x6eb2d630,\n+    0x6ef4d672,     0x2ecd158b,     0x6ed716d5,     0x2e39d717,\n+    0x6e2ed5ac,     0x6e7cd77a,     0x2e591717,     0x6e5e17bc,\n+    0x2e30ddee,     0x6e2ddd8b,     0x6e7adf38,     0x2e431c41,\n+    0x6e4e1dac,     0x0e61941f,     0x4e6c956a,     0x0eb29630,\n+    0x4ea99507,     0x0e24cc62,     0x4e25cc83,     0x4e6fcdcd,\n+    0x0e550e93,     0x4e530e51,     0x2e729630,     0x6e659483,\n+    0x2ea39441,     0x6ead958b,     0x0ea0cffe,     0x4ea7ccc5,\n+    0x4eeacd28,     0x0ed10e0f,     0x4edf0fdd,     0x2e20fffe,\n+    0x6e22fc20,     0x6e76feb4,     0x2e493d07,     0x6e563eb4,\n+    0x0e396717,     0x4e3e67bc,     0x0e7766d5,     0x4e7d679b,\n+    0x0ebb6759,     0x4ea764c5,     0x2e236441,     0x6e396717,\n+    0x2e726630,     0x6e61641f,     0x2ea764c5,     0x6eae65ac,\n+    0x0e2ba549,     0x4e3ea7bc,     0x0e71a60f,     0x4e7fa7dd,\n+    0x0eb8a6f6,     0x4ea1a41f,     0x0e35f693,     0x4e21f41f,\n+    0x4e67f4c5,     0x0e5035ee,     0x4e543672,     0x0e216c1f,\n+    0x4e346e72,     0x0e7d6f9b,     0x4e766eb4,     0x0eb26e30,\n+    0x4eae6dac,     0x2e2d6d8b,     0x6e2b6d49,     0x2e686ce6,\n+    0x6e606ffe,     0x2eb36e51,     0x6ebd6f9b,     0x0e3eafbc,\n+    0x4e20affe,     0x0e69ad07,     0x4e6cad6a,     0x0eb6aeb4,\n+    0x4eacad6a,     0x0e66b4a4,     0x4e7ab738,     0x0eb3b651,\n+    0x4eb3b651,     0x0e3826f6,     0x4e252483,     0x0e7f27dd,\n+    0x4e71260f,     0x0eb826f6,     0x4eb52693,     0x0eb5f693,\n+    0x4eb8f6f6,     0x4ee4f462,     0x0ed1360f,     0x4ec834e6,\n+    0x2eaeedac,     0x6eb2ee30,     0x6eeded8b,     0x2ecf2dcd,\n+    0x6ed92f17,     0x0f81100f,     0x4f848862,     0x4fc31841,\n+    0x0fad518b,     0x4fa780c5,     0x4fd059ee,     0x2fa890e6,\n+    0x4fa38841,     0x6fc1900f,     0x0f7b8149,     0x4f4688a4,\n+    0x0faf81cd,     0x4fa58083,     0x0e3736d5,     0x4e393717,\n+    0x0e61341f,     0x4e7b3759,     0x0ea43462,     0x4ea1341f,\n+    0x4efd379b,     0x0e343e72,     0x4e2c3d6a,     0x0e793f17,\n+    0x4e753e93,     0x0ea53c83,     0x4eb43e72,     0x4ee23c20,\n+    0x2e3b8f59,     0x6e3c8f7a,     0x2e798f17,     0x6e648c62,\n+    0x2eb48e72,     0x6eae8dac,     0x6ee68ca4,     0x2e3e37bc,\n+    0x6e2037fe,     0x2e7f37dd,     0x6e723630,     0x2ebd379b,\n+    0x6ea834e6,     0x6eeb3549,     0x2e3f3fdd,     0x6e343e72,\n+    0x2e693d07,     0x6e663ca4,     0x2ea93d07,     0x6eb13e0f,\n+    0x6eeb3d49,     0x0e39e717,     0x4e2ae528,     0x4e64e462,\n+    0x2ebee7bc,     0x6eb7e6d5,     0x6ee1e41f,     0x2e27e4c5,\n+    0x6e3de79b,     0x6e62e420,     0x659239e8,     0x65d03b94,\n+    0x65d0232d,     0x65d120c2,     0x659129f2,     0x65933ca3,\n+    0x25969683,     0x25961d15,     0x254d1c48,     0x259e3f61,\n+    0x25953b96,     0x255b91d1,     0x247686ed,     0x24309098,\n+    0x2462edb9,     0x24a57468,     0xba5fd3e3,     0x3a5f03e5,\n@@ -1593,105 +1634,107 @@\n-    0x0ea1b820,     0x4e21c862,     0x4e61b8a4,     0x05a08020,\n-    0x05104fe0,     0x05505001,     0x05906fe2,     0x05d03005,\n-    0x05101fea,     0x05901feb,     0x04b0e3e0,     0x0470e7e1,\n-    0x042f9c20,     0x043f9c35,     0x047f9c20,     0x04ff9c20,\n-    0x04299420,     0x04319160,     0x0461943e,     0x04a19020,\n-    0x04038100,     0x040381a0,     0x040387e1,     0x04438be2,\n-    0x04c38fe3,     0x040181e0,     0x04018100,     0x04018621,\n-    0x04418b22,     0x04418822,     0x04818c23,     0x040081e0,\n-    0x04008120,     0x04008761,     0x04008621,     0x04408822,\n-    0x04808c23,     0x042053ff,     0x047f5401,     0x25208028,\n-    0x2538cfe0,     0x2578d001,     0x25b8efe2,     0x25f8f007,\n-    0x2538dfea,     0x25b8dfeb,     0xa400a3e0,     0xa420a7e0,\n-    0xa4484be0,     0xa467afe0,     0xa4a8a7ea,     0xa547a814,\n-    0xa4084ffe,     0xa55c53e0,     0xa5e1540b,     0xe400fbf6,\n-    0xe408ffff,     0xe420e7e0,     0xe4484be0,     0xe460efe0,\n-    0xe547e400,     0xe4014be0,     0xe4a84fe0,     0xe5f15000,\n-    0x858043e0,     0x85a043ff,     0xe59f5d08,     0x0420e3e9,\n-    0x0460e3ea,     0x04a0e3eb,     0x04e0e3ec,     0x25104042,\n-    0x25104871,     0x25904861,     0x25904c92,     0x05344020,\n-    0x05744041,     0x05b44062,     0x05f44083,     0x252c8840,\n-    0x253c1420,     0x25681572,     0x25a21ce3,     0x25ea1e34,\n-    0x253c0421,     0x25680572,     0x25a20ce3,     0x25ea0e34,\n-    0x0522c020,     0x05e6c0a4,     0x2401a001,     0x2443a051,\n-    0x24858881,     0x24c78cd1,     0x24850891,     0x24c70cc1,\n-    0x250f9001,     0x25508051,     0x25802491,     0x25df28c1,\n-    0x25850c81,     0x251e10d1,     0x65816001,     0x65c36051,\n-    0x65854891,     0x65c74cc1,     0x05733820,     0x05b238a4,\n-    0x05f138e6,     0x0570396a,     0x65d0a001,     0x65d6a443,\n-    0x65d4a826,     0x6594ac26,     0x6554ac26,     0x6556ac26,\n-    0x6552ac26,     0x65cbac85,     0x65caac01,     0x6589ac85,\n-    0x6588ac01,     0x65c9ac85,     0x65c8ac01,     0x65dea833,\n-    0x659ca509,     0x65d8a801,     0x65dcac01,     0x655cb241,\n-    0x0520a1e0,     0x0521a601,     0x052281e0,     0x05238601,\n-    0x04a14026,     0x042244a6,     0x046344a6,     0x04a444a6,\n-    0x04e544a7,     0x0568aca7,     0x05b23230,     0x853040af,\n-    0xc5b040af,     0xe57080af,     0xe5b080af,     0x25034440,\n-    0x254054c4,     0x25034640,     0x25415a05,     0x25834440,\n-    0x25c54489,     0x250b5d3a,     0x2550dc20,     0x2518e3e1,\n-    0x2518e021,     0x2518e0a1,     0x2518e121,     0x2518e1a1,\n-    0x2558e3e2,     0x2558e042,     0x2558e0c2,     0x2558e142,\n-    0x2598e3e3,     0x2598e063,     0x2598e0e3,     0x2598e163,\n-    0x25d8e3e4,     0x25d8e084,     0x25d8e104,     0x25d8e184,\n-    0x2518e407,     0x05214800,     0x05614800,     0x05a14800,\n-    0x05e14800,     0x05214c00,     0x05614c00,     0x05a14c00,\n-    0x05e14c00,     0x05304001,     0x05314001,     0x05a18610,\n-    0x05e18610,     0x05271e11,     0x6545e891,     0x6585e891,\n-    0x65c5e891,     0x6545c891,     0x6585c891,     0x65c5c891,\n-    0x45b0c210,     0x45f1c231,     0x1e601000,     0x1e603000,\n-    0x1e621000,     0x1e623000,     0x1e641000,     0x1e643000,\n-    0x1e661000,     0x1e663000,     0x1e681000,     0x1e683000,\n-    0x1e6a1000,     0x1e6a3000,     0x1e6c1000,     0x1e6c3000,\n-    0x1e6e1000,     0x1e6e3000,     0x1e701000,     0x1e703000,\n-    0x1e721000,     0x1e723000,     0x1e741000,     0x1e743000,\n-    0x1e761000,     0x1e763000,     0x1e781000,     0x1e783000,\n-    0x1e7a1000,     0x1e7a3000,     0x1e7c1000,     0x1e7c3000,\n-    0x1e7e1000,     0x1e7e3000,     0xf83180b8,     0xf822014e,\n-    0xf830136b,     0xf837208c,     0xf8363091,     0xf8215213,\n-    0xf83041cd,     0xf82c7222,     0xf82362f5,     0xf8a580e6,\n-    0xf8b3038d,     0xf8b110d0,     0xf8a2207d,     0xf8a431e6,\n-    0xf8b4518d,     0xf8b44328,     0xf8b47013,     0xf8ab60d8,\n-    0xf8f481df,     0xf8f00006,     0xf8e7126f,     0xf8fa2149,\n-    0xf8f732d5,     0xf8fc5062,     0xf8ef4293,     0xf8e773a4,\n-    0xf8e76120,     0xf87082f4,     0xf8640150,     0xf877132b,\n-    0xf866221f,     0xf86d3197,     0xf861512e,     0xf8754350,\n-    0xf86f7084,     0xf87060c8,     0xb83e83a4,     0xb831035d,\n-    0xb829104f,     0xb82b207d,     0xb8273361,     0xb83551d0,\n-    0xb82842d0,     0xb8397285,     0xb83562f0,     0xb8b0829e,\n-    0xb8b40080,     0xb8b31098,     0xb8b42304,     0xb8ba3053,\n-    0xb8a851c8,     0xb8b843f0,     0xb8b673e4,     0xb8a1628a,\n-    0xb8ec8120,     0xb8e701f8,     0xb8e410db,     0xb8ea231b,\n-    0xb8ed33f0,     0xb8f65296,     0xb8ff413d,     0xb8ee70f4,\n-    0xb8f4613c,     0xb86b818e,     0xb8740301,     0xb86911b3,\n-    0xb8732210,     0xb8653060,     0xb86c51e8,     0xb86f4090,\n-    0xb86f70be,     0xb86062ca,     0xce20247b,     0xce0a63b3,\n-    0xce678e84,     0xce8eafb8,     0xce6d836b,     0xce7187f2,\n-    0xcec0806e,     0xce768a1e,     0x2520d474,     0x2521dae3,\n-    0x05800d33,     0x05403635,     0x05004cb8,     0x2560d175,\n-    0x2561c35e,     0x05809863,     0x054030f8,     0x05000ed7,\n-    0x2520c84e,     0x2521d69a,     0x05809892,     0x05408909,\n-    0x05000d2c,     0x2560d9cb,     0x25e1d352,     0x05806b49,\n-    0x0542d157,     0x050026a8,     0x2520cf2a,     0x25a1d599,\n-    0x05801ec0,     0x05422dc5,     0x05000e11,     0x2560c07e,\n-    0x2521dfb2,     0x0580ab15,     0x0540040c,     0x0500000f,\n-    0x04fb0353,     0x043606cd,     0x65940161,     0x65980b14,\n-    0x65d4063f,     0x04751095,     0x04ff1ade,     0x0473165a,\n-    0x04bd1dab,     0x0456a1c5,     0x04400542,     0x045a0753,\n-    0x041083c2,     0x04db0694,     0x0459adbd,     0x045abc2e,\n-    0x04d9007c,     0x04139929,     0x041189da,     0x04d018f4,\n-    0x04d7b0d4,     0x045ea3ad,     0x04180029,     0x052799fb,\n-    0x05e49e24,     0x04080302,     0x040a1dba,     0x04c90e16,\n-    0x04cb0571,     0x04010210,     0x04dca6fc,     0x65c0915c,\n-    0x65cd9cf1,     0x65868f04,     0x65878969,     0x65c296c4,\n-    0x049da1e4,     0x65c2bf44,     0x6580b745,     0x6581a33f,\n-    0x65cda468,     0x65c19b07,     0x65a19e38,     0x65e81dac,\n-    0x65f723fd,     0x65e7a2b4,     0x65e8dadd,     0x65e6f4ba,\n-    0x65b54f52,     0x65bc7140,     0x04d447d1,     0x048e6e3c,\n-    0x042b334a,     0x04af3160,     0x047432f7,     0x04fd3297,\n-    0x05a66b60,     0x05646d8d,     0x65c89aff,     0x45ddb046,\n-    0x4517b7a0,     0x04253904,     0x445891ad,     0x445a8908,\n-    0x449983b3,     0x44db8ef0,     0x041a3db7,     0x04583419,\n-    0x04593d79,     0x044836ce,     0x044a3005,     0x65c72069,\n-    0x658627ae,     0x65d8348e,     0x04812edb,     0x0e21101f,\n-    0x4e3a1338,     0x0e6d118b,     0x4e721230,     0x0eae11ac,\n-    0x4eb31251,     0x2e3e13bc,     0x6e251083,     0x2e7e13bc,\n-    0x6e721230,     0x2ea610a4,     0x6ebf13dd,\n+    0x0ea1b820,     0x0ef9b820,     0x4ef9b820,     0x4e21c862,\n+    0x0e79c862,     0x4e79c862,     0x4e61b8a4,     0x0e79b8a4,\n+    0x4e79b8a4,     0x05a08020,     0x05104fe0,     0x05505001,\n+    0x05906fe2,     0x05d03005,     0x05101fea,     0x05901feb,\n+    0x04b0e3e0,     0x0470e7e1,     0x042f9c20,     0x043f9c35,\n+    0x047f9c20,     0x04ff9c20,     0x04299420,     0x04319160,\n+    0x0461943e,     0x04a19020,     0x04038100,     0x040381a0,\n+    0x040387e1,     0x04438be2,     0x04c38fe3,     0x040181e0,\n+    0x04018100,     0x04018621,     0x04418b22,     0x04418822,\n+    0x04818c23,     0x040081e0,     0x04008120,     0x04008761,\n+    0x04008621,     0x04408822,     0x04808c23,     0x042053ff,\n+    0x047f5401,     0x25208028,     0x2538cfe0,     0x2578d001,\n+    0x25b8efe2,     0x25f8f007,     0x2538dfea,     0x25b8dfeb,\n+    0xa400a3e0,     0xa420a7e0,     0xa4484be0,     0xa467afe0,\n+    0xa4a8a7ea,     0xa547a814,     0xa4084ffe,     0xa55c53e0,\n+    0xa5e1540b,     0xe400fbf6,     0xe408ffff,     0xe420e7e0,\n+    0xe4484be0,     0xe460efe0,     0xe547e400,     0xe4014be0,\n+    0xe4a84fe0,     0xe5f15000,     0x858043e0,     0x85a043ff,\n+    0xe59f5d08,     0x0420e3e9,     0x0460e3ea,     0x04a0e3eb,\n+    0x04e0e3ec,     0x25104042,     0x25104871,     0x25904861,\n+    0x25904c92,     0x05344020,     0x05744041,     0x05b44062,\n+    0x05f44083,     0x252c8840,     0x253c1420,     0x25681572,\n+    0x25a21ce3,     0x25ea1e34,     0x253c0421,     0x25680572,\n+    0x25a20ce3,     0x25ea0e34,     0x0522c020,     0x05e6c0a4,\n+    0x2401a001,     0x2443a051,     0x24858881,     0x24c78cd1,\n+    0x24850891,     0x24c70cc1,     0x250f9001,     0x25508051,\n+    0x25802491,     0x25df28c1,     0x25850c81,     0x251e10d1,\n+    0x65816001,     0x65c36051,     0x65854891,     0x65c74cc1,\n+    0x05733820,     0x05b238a4,     0x05f138e6,     0x0570396a,\n+    0x65d0a001,     0x65d6a443,     0x65d4a826,     0x6594ac26,\n+    0x6554ac26,     0x6556ac26,     0x6552ac26,     0x65cbac85,\n+    0x65caac01,     0x6589ac85,     0x6588ac01,     0x65c9ac85,\n+    0x65c8ac01,     0x65dea833,     0x659ca509,     0x65d8a801,\n+    0x65dcac01,     0x655cb241,     0x0520a1e0,     0x0521a601,\n+    0x052281e0,     0x05238601,     0x04a14026,     0x042244a6,\n+    0x046344a6,     0x04a444a6,     0x04e544a7,     0x0568aca7,\n+    0x05b23230,     0x853040af,     0xc5b040af,     0xe57080af,\n+    0xe5b080af,     0x25034440,     0x254054c4,     0x25034640,\n+    0x25415a05,     0x25834440,     0x25c54489,     0x250b5d3a,\n+    0x2550dc20,     0x2518e3e1,     0x2518e021,     0x2518e0a1,\n+    0x2518e121,     0x2518e1a1,     0x2558e3e2,     0x2558e042,\n+    0x2558e0c2,     0x2558e142,     0x2598e3e3,     0x2598e063,\n+    0x2598e0e3,     0x2598e163,     0x25d8e3e4,     0x25d8e084,\n+    0x25d8e104,     0x25d8e184,     0x2518e407,     0x05214800,\n+    0x05614800,     0x05a14800,     0x05e14800,     0x05214c00,\n+    0x05614c00,     0x05a14c00,     0x05e14c00,     0x05304001,\n+    0x05314001,     0x05a18610,     0x05e18610,     0x05271e11,\n+    0x6545e891,     0x6585e891,     0x65c5e891,     0x6545c891,\n+    0x6585c891,     0x65c5c891,     0x45b0c210,     0x45f1c231,\n+    0x1e601000,     0x1e603000,     0x1e621000,     0x1e623000,\n+    0x1e641000,     0x1e643000,     0x1e661000,     0x1e663000,\n+    0x1e681000,     0x1e683000,     0x1e6a1000,     0x1e6a3000,\n+    0x1e6c1000,     0x1e6c3000,     0x1e6e1000,     0x1e6e3000,\n+    0x1e701000,     0x1e703000,     0x1e721000,     0x1e723000,\n+    0x1e741000,     0x1e743000,     0x1e761000,     0x1e763000,\n+    0x1e781000,     0x1e783000,     0x1e7a1000,     0x1e7a3000,\n+    0x1e7c1000,     0x1e7c3000,     0x1e7e1000,     0x1e7e3000,\n+    0xf8268267,     0xf82d023c,     0xf8301046,     0xf83d2083,\n+    0xf8263290,     0xf82d528c,     0xf8284299,     0xf8337160,\n+    0xf8386286,     0xf8bf820e,     0xf8a600e0,     0xf8af1353,\n+    0xf8a922ea,     0xf8b53396,     0xf8a251e3,     0xf8b340f4,\n+    0xf8a470fd,     0xf8a06209,     0xf8f48097,     0xf8f002ea,\n+    0xf8eb10d9,     0xf8ff21b0,     0xf8f7302c,     0xf8ee52a9,\n+    0xf8f041fa,     0xf8e471e4,     0xf8e863c6,     0xf864823d,\n+    0xf87d013a,     0xf86f1162,     0xf87d20e3,     0xf86132bb,\n+    0xf870510e,     0xf8704336,     0xf86572b4,     0xf8706217,\n+    0xb83e8294,     0xb8200264,     0xb8381284,     0xb8242358,\n+    0xb8333102,     0xb828530e,     0xb83042df,     0xb824703f,\n+    0xb82a6194,     0xb8a080e9,     0xb8b80090,     0xb8bb1146,\n+    0xb8bb21b8,     0xb8b032df,     0xb8b653f4,     0xb8bd41c9,\n+    0xb8b47287,     0xb8bc6169,     0xb8ee828c,     0xb8e10138,\n+    0xb8f3126d,     0xb8f020b0,     0xb8e03183,     0xb8e851ef,\n+    0xb8f041e4,     0xb8fe7005,     0xb8ea6376,     0xb8638120,\n+    0xb873015d,     0xb8781284,     0xb86723b8,     0xb86e3175,\n+    0xb87b51ed,     0xb87f41d1,     0xb863721e,     0xb87660f4,\n+    0xce216874,     0xce104533,     0xce648c15,     0xce8e3302,\n+    0xce6e82ab,     0xce6c87d1,     0xcec08063,     0xce638937,\n+    0x25e0c358,     0x25a1c7d3,     0x0580785a,     0x05426328,\n+    0x05009892,     0x25a0cc29,     0x2561cec8,     0x058044b3,\n+    0x05401c99,     0x05006b49,     0x25e0d6f7,     0x2561c528,\n+    0x0583c8bc,     0x0542522f,     0x05001ec0,     0x25e0de65,\n+    0x25a1c113,     0x05803cad,     0x0540f3c0,     0x0500ab15,\n+    0x2560c28c,     0x2561d7c0,     0x05801ed7,     0x0542633b,\n+    0x05003696,     0x2560d4b4,     0x25e1c918,     0x058021ff,\n+    0x05400e15,     0x0500f3de,     0x0473025a,     0x04bd05ab,\n+    0x658e0025,     0x658a08e2,     0x659a0493,     0x043e1062,\n+    0x04f418b4,     0x046d15bd,     0x04611fce,     0x04d6a07c,\n+    0x04001929,     0x041a09da,     0x04d098f4,     0x04db10d4,\n+    0x0459a3ad,     0x041aa029,     0x041919fb,     0x04d39e24,\n+    0x04118302,     0x04101dba,     0x04d7ae16,     0x04dea571,\n+    0x04180210,     0x05e786fc,     0x05e4915c,     0x04881cf1,\n+    0x044a0f04,     0x04090969,     0x048b16c4,     0x044101e4,\n+    0x04dcbf44,     0x65809745,     0x658d833f,     0x65c68468,\n+    0x65c79b07,     0x65829e38,     0x049dafca,     0x6582bba8,\n+    0x65c0b7ff,     0x65c1b4e0,     0x658dbadd,     0x65819a9d,\n+    0x65ed9246,     0x65b30815,     0x65e6263c,     0x65eebb94,\n+    0x65bad14e,     0x65efe178,     0x65fc5697,     0x65e07f14,\n+    0x040c55a6,     0x04977f4d,     0x043d3046,     0x04b733a0,\n+    0x046830a4,     0x04ed322d,     0x05686948,     0x05bd6c13,\n+    0x65c88ef0,     0x450db3d7,     0x4540b6d9,     0x043e3979,\n+    0x445896ce,     0x445a9005,     0x44d98069,     0x445b87ae,\n+    0x04da348e,     0x04982edb,     0x0499397f,     0x0408338c,\n+    0x04ca309c,     0x65c721e6,     0x65c63641,     0x65982882,\n+    0x04812b8b,     0x0e251083,     0x4e3712d5,     0x0e61101f,\n+    0x4e6d118b,     0x0eba1338,     0x4eb712d5,     0x2e31120f,\n+    0x6e2e11ac,     0x2e6810e6,     0x6e6f11cd,     0x2eaa1128,\n+    0x6eb1120f,\n","filename":"test\/hotspot\/gtest\/aarch64\/asmtest.out.h","additions":637,"deletions":594,"binary":false,"changes":1231,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+        \/\/ Test with default MaxVectorSize\n@@ -54,0 +55,6 @@\n+\n+        \/\/ Test with different values of MaxVectorSize\n+        TestFramework.runWithFlags(\"--add-modules=jdk.incubator.vector\", \"-XX:MaxVectorSize=8\");\n+        TestFramework.runWithFlags(\"--add-modules=jdk.incubator.vector\", \"-XX:MaxVectorSize=16\");\n+        TestFramework.runWithFlags(\"--add-modules=jdk.incubator.vector\", \"-XX:MaxVectorSize=32\");\n+        TestFramework.runWithFlags(\"--add-modules=jdk.incubator.vector\", \"-XX:MaxVectorSize=64\");\n@@ -81,1 +88,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -102,1 +111,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.SUB_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -123,1 +134,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.MUL_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -144,1 +157,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.DIV_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -165,1 +180,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.MIN_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -186,1 +203,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.MAX_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -207,1 +226,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.SQRT_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -228,1 +249,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.FMA_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -251,1 +274,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.FMA_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -275,1 +300,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.FMA_VHF, \">= 1\"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n@@ -298,1 +325,3 @@\n-        applyIfCPUFeature = {\"avx512_fp16\", \"true\"})\n+        applyIfCPUFeatureOr = {\"avx512_fp16\", \"true\", \"sve\", \"true\"})\n+    @IR(counts = {IRNode.FMA_VHF, \" 0 \"},\n+        applyIfCPUFeatureAnd = {\"fphp\", \"true\", \"asimdhp\", \"true\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloat16VectorOperations.java","additions":40,"deletions":11,"binary":false,"changes":51,"status":"modified"}]}