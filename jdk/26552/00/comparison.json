{"files":[{"patch":"@@ -416,5 +416,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(*stub->entry());\n-    } else {\n-      __ unlock_object(rdi, rsi, rax, *stub->entry());\n-    }\n+    __ unlock_object(rdi, rsi, rax, *stub->entry());\n@@ -2736,7 +2732,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    if (op->info() != nullptr) {\n-      add_debug_info_for_null_check_here(op->info());\n-      __ null_check(obj);\n-    }\n-    __ jmp(*op->stub()->entry());\n-  } else if (op->code() == lir_lock) {\n+  if (op->code() == lir_lock) {\n@@ -2744,1 +2734,1 @@\n-    Register tmp = LockingMode == LM_LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n+    Register tmp = op->scratch_opr()->as_register();\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -292,1 +292,1 @@\n-  LIR_Opr tmp = LockingMode == LM_LIGHTWEIGHT ? new_register(T_ADDRESS) : LIR_OprFact::illegalOpr;\n+  LIR_Opr tmp = new_register(T_ADDRESS);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,48 +58,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_lock(disp_hdr, obj, hdr, tmp, slow_case);\n-  } else  if (LockingMode == LM_LEGACY) {\n-    Label done;\n-\n-    if (DiagnoseSyncOnValueBasedClasses != 0) {\n-      load_klass(hdr, obj, rscratch1);\n-      testb(Address(hdr, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n-      jcc(Assembler::notZero, slow_case);\n-    }\n-\n-    \/\/ Load object header\n-    movptr(hdr, Address(obj, hdr_offset));\n-    \/\/ and mark it as unlocked\n-    orptr(hdr, markWord::unlocked_value);\n-    \/\/ save unlocked object header into the displaced header location on the stack\n-    movptr(Address(disp_hdr, 0), hdr);\n-    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-    \/\/ displaced header address in the object header - if it is not the same, get the\n-    \/\/ object header instead\n-    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-    \/\/ if the object header was the same, we're done\n-    jcc(Assembler::equal, done);\n-    \/\/ if the object header was not the same, it is now in the hdr register\n-    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-    \/\/\n-    \/\/ 1) (hdr & aligned_mask) == 0\n-    \/\/ 2) rsp <= hdr\n-    \/\/ 3) hdr <= rsp + page_size\n-    \/\/\n-    \/\/ these 3 tests can be done by evaluating the following expression:\n-    \/\/\n-    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-    \/\/\n-    \/\/ assuming both the stack pointer and page_size have their least\n-    \/\/ significant 2 bits cleared and page_size is a power of 2\n-    subptr(hdr, rsp);\n-    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-    \/\/ for recursive locking, the result is zero => save it in the displaced header\n-    \/\/ location (null in the displaced hdr location indicates recursive locking)\n-    movptr(Address(disp_hdr, 0), hdr);\n-    \/\/ otherwise we don't care about the result and handle locking via runtime call\n-    jcc(Assembler::notZero, slow_case);\n-    \/\/ done\n-    bind(done);\n-    inc_held_monitor_count();\n-  }\n+  lightweight_lock(disp_hdr, obj, hdr, tmp, slow_case);\n@@ -117,9 +70,0 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ load displaced header\n-    movptr(hdr, Address(disp_hdr, 0));\n-    \/\/ if the loaded hdr is null we had recursive locking\n-    testptr(hdr, hdr);\n-    \/\/ if we had recursive locking, we are done\n-    jcc(Assembler::zero, done);\n-  }\n-\n@@ -130,15 +74,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n-  } else if (LockingMode == LM_LEGACY) {\n-    \/\/ test if object header is pointing to the displaced header, and if so, restore\n-    \/\/ the displaced header in the object - if the object header is not pointing to\n-    \/\/ the displaced header, get the object header instead\n-    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-    cmpxchgptr(hdr, Address(obj, hdr_offset));\n-    \/\/ if the object header was not pointing to the displaced header,\n-    \/\/ we do unlocking via runtime call\n-    jcc(Assembler::notEqual, slow_case);\n-    \/\/ done\n-    bind(done);\n-    dec_held_monitor_count();\n-  }\n+  lightweight_unlock(obj, disp_hdr, hdr, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":72,"binary":false,"changes":74,"status":"modified"},{"patch":"@@ -222,235 +222,3 @@\n-\/\/ box: on-stack box address (displaced header location) - KILLED\n-\/\/ rax,: tmp -- KILLED\n-\/\/ scr: tmp -- KILLED\n-void C2_MacroAssembler::fast_lock(Register objReg, Register boxReg, Register tmpReg,\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n-                                 Metadata* method_data) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n-  \/\/ Ensure the register assignments are disjoint\n-  assert(tmpReg == rax, \"\");\n-  assert(cx1Reg == noreg, \"\");\n-  assert(cx2Reg == noreg, \"\");\n-  assert_different_registers(objReg, boxReg, tmpReg, scrReg);\n-\n-  \/\/ Possible cases that we'll encounter in fast_lock\n-  \/\/ ------------------------------------------------\n-  \/\/ * Inflated\n-  \/\/    -- unlocked\n-  \/\/    -- Locked\n-  \/\/       = by self\n-  \/\/       = by other\n-  \/\/ * neutral\n-  \/\/ * stack-locked\n-  \/\/    -- by self\n-  \/\/       = sp-proximity test hits\n-  \/\/       = sp-proximity test generates false-negative\n-  \/\/    -- by other\n-  \/\/\n-\n-  Label IsInflated, DONE_LABEL, NO_COUNT, COUNT;\n-\n-  if (DiagnoseSyncOnValueBasedClasses != 0) {\n-    load_klass(tmpReg, objReg, scrReg);\n-    testb(Address(tmpReg, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n-    jcc(Assembler::notZero, DONE_LABEL);\n-  }\n-\n-  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));          \/\/ [FETCH]\n-  testptr(tmpReg, markWord::monitor_value); \/\/ inflated vs stack-locked|neutral\n-  jcc(Assembler::notZero, IsInflated);\n-\n-  if (LockingMode == LM_MONITOR) {\n-    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n-    testptr(objReg, objReg);\n-  } else {\n-    assert(LockingMode == LM_LEGACY, \"must be\");\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (7 - (int)os::vm_page_size()) );\n-    movptr(Address(boxReg, 0), tmpReg);\n-  }\n-  jmp(DONE_LABEL);\n-\n-  bind(IsInflated);\n-  \/\/ The object is inflated. tmpReg contains pointer to ObjectMonitor* + markWord::monitor_value\n-\n-  \/\/ Unconditionally set box->_displaced_header = markWord::unused_mark().\n-  \/\/ Without cast to int32_t this style of movptr will destroy r10 which is typically obj.\n-  movptr(Address(boxReg, 0), checked_cast<int32_t>(markWord::unused_mark().value()));\n-\n-  \/\/ It's inflated and we use scrReg for ObjectMonitor* in this section.\n-  movptr(boxReg, Address(r15_thread, JavaThread::monitor_owner_id_offset()));\n-  movq(scrReg, tmpReg);\n-  xorq(tmpReg, tmpReg);\n-  lock();\n-  cmpxchgptr(boxReg, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n-\n-  \/\/ Propagate ICC.ZF from CAS above into DONE_LABEL.\n-  jccb(Assembler::equal, COUNT);    \/\/ CAS above succeeded; propagate ZF = 1 (success)\n-\n-  cmpptr(boxReg, rax);                \/\/ Check if we are already the owner (recursive lock)\n-  jccb(Assembler::notEqual, NO_COUNT);    \/\/ If not recursive, ZF = 0 at this point (fail)\n-  incq(Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-  xorq(rax, rax); \/\/ Set ZF = 1 (success) for recursive lock, denoting locking success\n-  bind(DONE_LABEL);\n-\n-  \/\/ ZFlag == 1 count in fast path\n-  \/\/ ZFlag == 0 count in slow path\n-  jccb(Assembler::notZero, NO_COUNT); \/\/ jump if ZFlag == 0\n-\n-  bind(COUNT);\n-  if (LockingMode == LM_LEGACY) {\n-    \/\/ Count monitors in fast path\n-    increment(Address(thread, JavaThread::held_monitor_count_offset()));\n-  }\n-  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n-\n-  bind(NO_COUNT);\n-\n-  \/\/ At NO_COUNT the icc ZFlag is set as follows ...\n-  \/\/ fast_unlock uses the same protocol.\n-  \/\/ ZFlag == 1 -> Success\n-  \/\/ ZFlag == 0 -> Failure - force control through the slow path\n-}\n-\n-\/\/ obj: object to unlock\n-\/\/ box: box address (displaced header location), killed.  Must be EAX.\n-\/\/ tmp: killed, cannot be obj nor box.\n-\/\/\n-\/\/ Some commentary on balanced locking:\n-\/\/\n-\/\/ fast_lock and fast_unlock are emitted only for provably balanced lock sites.\n-\/\/ Methods that don't have provably balanced locking are forced to run in the\n-\/\/ interpreter - such methods won't be compiled to use fast_lock and fast_unlock.\n-\/\/ The interpreter provides two properties:\n-\/\/ I1:  At return-time the interpreter automatically and quietly unlocks any\n-\/\/      objects acquired the current activation (frame).  Recall that the\n-\/\/      interpreter maintains an on-stack list of locks currently held by\n-\/\/      a frame.\n-\/\/ I2:  If a method attempts to unlock an object that is not held by the\n-\/\/      the frame the interpreter throws IMSX.\n-\/\/\n-\/\/ Lets say A(), which has provably balanced locking, acquires O and then calls B().\n-\/\/ B() doesn't have provably balanced locking so it runs in the interpreter.\n-\/\/ Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O\n-\/\/ is still locked by A().\n-\/\/\n-\/\/ The only other source of unbalanced locking would be JNI.  The \"Java Native Interface:\n-\/\/ Programmer's Guide and Specification\" claims that an object locked by jni_monitorenter\n-\/\/ should not be unlocked by \"normal\" java-level locking and vice-versa.  The specification\n-\/\/ doesn't specify what will occur if a program engages in such mixed-mode locking, however.\n-\/\/ Arguably given that the spec legislates the JNI case as undefined our implementation\n-\/\/ could reasonably *avoid* checking owner in fast_unlock().\n-\/\/ In the interest of performance we elide m->Owner==Self check in unlock.\n-\/\/ A perfectly viable alternative is to elide the owner check except when\n-\/\/ Xcheck:jni is enabled.\n-\n-void C2_MacroAssembler::fast_unlock(Register objReg, Register boxReg, Register tmpReg) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n-  assert(boxReg == rax, \"\");\n-  assert_different_registers(objReg, boxReg, tmpReg);\n-\n-  Label DONE_LABEL, Stacked, COUNT, NO_COUNT;\n-\n-  if (LockingMode == LM_LEGACY) {\n-    cmpptr(Address(boxReg, 0), NULL_WORD);                            \/\/ Examine the displaced header\n-    jcc   (Assembler::zero, COUNT);                                   \/\/ 0 indicates recursive stack-lock\n-  }\n-  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));   \/\/ Examine the object's markword\n-  if (LockingMode != LM_MONITOR) {\n-    testptr(tmpReg, markWord::monitor_value);                         \/\/ Inflated?\n-    jcc(Assembler::zero, Stacked);\n-  }\n-\n-  \/\/ It's inflated.\n-\n-  \/\/ Despite our balanced locking property we still check that m->_owner == Self\n-  \/\/ as java routines or native JNI code called by this thread might\n-  \/\/ have released the lock.\n-  \/\/\n-  \/\/ If there's no contention try a 1-0 exit.  That is, exit without\n-  \/\/ a costly MEMBAR or CAS.  See synchronizer.cpp for details on how\n-  \/\/ we detect and recover from the race that the 1-0 exit admits.\n-  \/\/\n-  \/\/ Conceptually fast_unlock() must execute a STST|LDST \"release\" barrier\n-  \/\/ before it STs null into _owner, releasing the lock.  Updates\n-  \/\/ to data protected by the critical section must be visible before\n-  \/\/ we drop the lock (and thus before any other thread could acquire\n-  \/\/ the lock and observe the fields protected by the lock).\n-  \/\/ IA32's memory-model is SPO, so STs are ordered with respect to\n-  \/\/ each other and there's no need for an explicit barrier (fence).\n-  \/\/ See also http:\/\/gee.cs.oswego.edu\/dl\/jmm\/cookbook.html.\n-  Label LSuccess, LNotRecursive;\n-\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)), 0);\n-  jccb(Assembler::equal, LNotRecursive);\n-\n-  \/\/ Recursive inflated unlock\n-  decrement(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(recursions)));\n-  jmpb(LSuccess);\n-\n-  bind(LNotRecursive);\n-\n-  \/\/ Set owner to null.\n-  \/\/ Release to satisfy the JMM\n-  movptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), NULL_WORD);\n-  \/\/ We need a full fence after clearing owner to avoid stranding.\n-  \/\/ StoreLoad achieves this.\n-  membar(StoreLoad);\n-\n-  \/\/ Check if the entry_list is empty.\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(entry_list)), NULL_WORD);\n-  jccb(Assembler::zero, LSuccess);    \/\/ If so we are done.\n-\n-  \/\/ Check if there is a successor.\n-  cmpptr(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(succ)), NULL_WORD);\n-  jccb(Assembler::notZero, LSuccess); \/\/ If so we are done.\n-\n-  \/\/ Save the monitor pointer in the current thread, so we can try to\n-  \/\/ reacquire the lock in SharedRuntime::monitor_exit_helper().\n-  andptr(tmpReg, ~(int32_t)markWord::monitor_value);\n-  movptr(Address(r15_thread, JavaThread::unlocked_inflated_monitor_offset()), tmpReg);\n-\n-  orl   (boxReg, 1);                      \/\/ set ICC.ZF=0 to indicate failure\n-  jmpb  (DONE_LABEL);\n-\n-  bind  (LSuccess);\n-  testl (boxReg, 0);                      \/\/ set ICC.ZF=1 to indicate success\n-  jmpb  (DONE_LABEL);\n-\n-  if (LockingMode == LM_LEGACY) {\n-    bind  (Stacked);\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n-    \/\/ Intentional fall-thru into DONE_LABEL\n-  }\n-\n-  bind(DONE_LABEL);\n-\n-  \/\/ ZFlag == 1 count in fast path\n-  \/\/ ZFlag == 0 count in slow path\n-  jccb(Assembler::notZero, NO_COUNT);\n-\n-  bind(COUNT);\n-\n-  if (LockingMode == LM_LEGACY) {\n-    \/\/ Count monitors in fast path\n-    decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-  }\n-\n-  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n-\n-  bind(NO_COUNT);\n-}\n-\n+\/\/ box: on-stack box address (displaced header location) - Must be EBX -- KILLED\n+\/\/ rax: tmp -- KILLED\n+\/\/ t  : tmp -- KILLED\n@@ -459,1 +227,1 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(box == rbx, \"Used for displaced header location\");\n@@ -619,0 +387,32 @@\n+\/\/ obj: object to lock\n+\/\/ rax: tmp -- KILLED\n+\/\/ t  : tmp - cannot be obj nor rax -- KILLED\n+\/\/\n+\/\/ Some commentary on balanced locking:\n+\/\/\n+\/\/ fast_lock and fast_unlock are emitted only for provably balanced lock sites.\n+\/\/ Methods that don't have provably balanced locking are forced to run in the\n+\/\/ interpreter - such methods won't be compiled to use fast_lock and fast_unlock.\n+\/\/ The interpreter provides two properties:\n+\/\/ I1:  At return-time the interpreter automatically and quietly unlocks any\n+\/\/      objects acquired the current activation (frame).  Recall that the\n+\/\/      interpreter maintains an on-stack list of locks currently held by\n+\/\/      a frame.\n+\/\/ I2:  If a method attempts to unlock an object that is not held by the\n+\/\/      the frame the interpreter throws IMSX.\n+\/\/\n+\/\/ Lets say A(), which has provably balanced locking, acquires O and then calls B().\n+\/\/ B() doesn't have provably balanced locking so it runs in the interpreter.\n+\/\/ Control returns to A() and A() unlocks O.  By I1 and I2, above, we know that O\n+\/\/ is still locked by A().\n+\/\/\n+\/\/ The only other source of unbalanced locking would be JNI.  The \"Java Native Interface:\n+\/\/ Programmer's Guide and Specification\" claims that an object locked by jni_monitorenter\n+\/\/ should not be unlocked by \"normal\" java-level locking and vice-versa.  The specification\n+\/\/ doesn't specify what will occur if a program engages in such mixed-mode locking, however.\n+\/\/ Arguably given that the spec legislates the JNI case as undefined our implementation\n+\/\/ could reasonably *avoid* checking owner in fast_unlock().\n+\/\/ In the interest of performance we elide m->Owner==Self check in unlock.\n+\/\/ A perfectly viable alternative is to elide the owner check except when\n+\/\/ Xcheck:jni is enabled.\n+\n@@ -620,1 +420,0 @@\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":36,"deletions":237,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -37,6 +37,1 @@\n-  \/\/ See full description in macroAssembler_x86.cpp.\n-  void fast_lock(Register obj, Register box, Register tmp,\n-                 Register scr, Register cx1, Register cx2, Register thread,\n-                 Metadata* method_data);\n-  void fast_unlock(Register obj, Register box, Register tmp);\n-\n+  \/\/ See full description in c2_MacroAssembler_x86.cpp.\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1027,28 +1027,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-  } else {\n-    Label count_locking, done, slow_case;\n-\n-    const Register swap_reg = rax; \/\/ Must use rax for cmpxchg instruction\n-    const Register tmp_reg = rbx;\n-    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n-    const Register rklass_decode_tmp = rscratch1;\n-\n-    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n-    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n-    const int mark_offset = lock_offset +\n-                            BasicLock::displaced_header_offset_in_bytes();\n-\n-    \/\/ Load object pointer into obj_reg\n-    movptr(obj_reg, Address(lock_reg, obj_offset));\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_lock(lock_reg, obj_reg, swap_reg, tmp_reg, slow_case);\n-    } else if (LockingMode == LM_LEGACY) {\n-      if (DiagnoseSyncOnValueBasedClasses != 0) {\n-        load_klass(tmp_reg, obj_reg, rklass_decode_tmp);\n-        testb(Address(tmp_reg, Klass::misc_flags_offset()), KlassFlags::_misc_is_value_based_class);\n-        jcc(Assembler::notZero, slow_case);\n-      }\n+  Label done, slow_case;\n@@ -1056,56 +1029,4 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-      assert(lock_offset == 0,\n-             \"displaced header must be first word in BasicObjectLock\");\n-\n-      lock();\n-      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      jcc(Assembler::zero, count_locking);\n-\n-      const int zero_bits = 7;\n-\n-      \/\/ Fast check for recursive lock.\n-      \/\/\n-      \/\/ Can apply the optimization only if this is a stack lock\n-      \/\/ allocated in this thread. For efficiency, we can focus on\n-      \/\/ recently allocated stack locks (instead of reading the stack\n-      \/\/ base and checking whether 'mark' points inside the current\n-      \/\/ thread stack):\n-      \/\/  1) (mark & zero_bits) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/\n-      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-      \/\/ neither apply the optimization for an inflated lock allocated\n-      \/\/ just above the thread stack (this is why condition 1 matters)\n-      \/\/ nor apply the optimization if the stack lock is inside the stack\n-      \/\/ of another thread. The latter is avoided even in case of overflow\n-      \/\/ because we have guard pages at the end of all stacks. Hence, if\n-      \/\/ we go over the stack base and hit the stack of another thread,\n-      \/\/ this should not be in a writeable area that could contain a\n-      \/\/ stack lock allocated by that thread. As a consequence, a stack\n-      \/\/ lock less than page size away from rsp is guaranteed to be\n-      \/\/ owned by the current thread.\n-      \/\/\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant bits clear.\n-      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-      subptr(swap_reg, rsp);\n-      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      movptr(Address(lock_reg, mark_offset), swap_reg);\n-      jcc(Assembler::notZero, slow_case);\n-\n-      bind(count_locking);\n-      inc_held_monitor_count();\n-    }\n-    jmp(done);\n+  const Register swap_reg = rax; \/\/ Must use rax for cmpxchg instruction\n+  const Register tmp_reg = rbx;\n+  const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n+  const Register rklass_decode_tmp = rscratch1;\n@@ -1113,1 +1034,4 @@\n-    bind(slow_case);\n+  const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n+  const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n+  const int mark_offset = lock_offset +\n+                          BasicLock::displaced_header_offset_in_bytes();\n@@ -1115,6 +1039,13 @@\n-    \/\/ Call the runtime routine for slow case\n-    call_VM_preemptable(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-    bind(done);\n-  }\n+  \/\/ Load object pointer into obj_reg\n+  movptr(obj_reg, Address(lock_reg, obj_offset));\n+\n+  lightweight_lock(lock_reg, obj_reg, swap_reg, tmp_reg, slow_case);\n+  jmp(done);\n+\n+  bind(slow_case);\n+\n+  \/\/ Call the runtime routine for slow case\n+  call_VM_preemptable(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+          lock_reg);\n+  bind(done);\n@@ -1139,8 +1070,1 @@\n-  if (LockingMode == LM_MONITOR) {\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n-  } else {\n-    Label count_locking, done, slow_case;\n-\n-    const Register swap_reg   = rax;  \/\/ Must use rax for cmpxchg instruction\n-    const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n-    const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n+  Label done, slow_case;\n@@ -1148,1 +1072,3 @@\n-    save_bcp(); \/\/ Save in case of exception\n+  const Register swap_reg   = rax;  \/\/ Must use rax for cmpxchg instruction\n+  const Register header_reg = c_rarg2;  \/\/ Will contain the old oopMark\n+  const Register obj_reg    = c_rarg3;  \/\/ Will contain the oop\n@@ -1150,24 +1076,1 @@\n-    if (LockingMode != LM_LIGHTWEIGHT) {\n-      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-      \/\/ structure Store the BasicLock address into %rax\n-      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset()));\n-    }\n-\n-    \/\/ Load oop into obj_reg(%c_rarg3)\n-    movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n-\n-    \/\/ Free entry\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset()), NULL_WORD);\n-\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load the old header from BasicLock structure\n-      movptr(header_reg, Address(swap_reg,\n-                                 BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Test for recursion\n-      testptr(header_reg, header_reg);\n-\n-      \/\/ zero for recursive case\n-      jcc(Assembler::zero, count_locking);\n+  save_bcp(); \/\/ Save in case of exception\n@@ -1175,3 +1078,2 @@\n-      \/\/ Atomic swap back the old header\n-      lock();\n-      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+  \/\/ Load oop into obj_reg(%c_rarg3)\n+  movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -1179,2 +1081,2 @@\n-      \/\/ zero for simple unlock of a stack-lock case\n-      jcc(Assembler::notZero, slow_case);\n+  \/\/ Free entry\n+  movptr(Address(lock_reg, BasicObjectLock::obj_offset()), NULL_WORD);\n@@ -1182,4 +1084,2 @@\n-      bind(count_locking);\n-      dec_held_monitor_count();\n-    }\n-    jmp(done);\n+  lightweight_unlock(obj_reg, swap_reg, header_reg, slow_case);\n+  jmp(done);\n@@ -1187,4 +1087,4 @@\n-    bind(slow_case);\n-    \/\/ Call the runtime routine for slow case.\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset()), obj_reg); \/\/ restore obj\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n+  bind(slow_case);\n+  \/\/ Call the runtime routine for slow case.\n+  movptr(Address(lock_reg, BasicObjectLock::obj_offset()), obj_reg); \/\/ restore obj\n+  call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorexit), lock_reg);\n@@ -1192,1 +1092,1 @@\n-    bind(done);\n+  bind(done);\n@@ -1194,2 +1094,1 @@\n-    restore_bcp();\n-  }\n+  restore_bcp();\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":39,"deletions":140,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -62,11 +62,4 @@\n-\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    if (!UseObjectMonitorTable) {\n-      \/\/ check if monitor\n-      __ testptr(result, markWord::monitor_value);\n-      __ jcc(Assembler::notZero, slowCase);\n-    }\n-  } else {\n-    \/\/ check if locked\n-    __ testptr(result, markWord::unlocked_value);\n-    __ jcc(Assembler::zero, slowCase);\n+  if (!UseObjectMonitorTable) {\n+    \/\/ check if monitor\n+    __ testptr(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, slowCase);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2136,1 +2136,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -2177,1 +2177,0 @@\n-  const Register old_hdr  = r13;  \/\/ value of old header at unlock time\n@@ -2197,41 +2196,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(slow_path_lock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n-\n-      __ bind(count_mon);\n-      __ inc_held_monitor_count();\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_lock(lock_reg, obj_reg, swap_reg, rscratch1, slow_path_lock);\n-    }\n+    __ lightweight_lock(lock_reg, obj_reg, swap_reg, rscratch1, slow_path_lock);\n@@ -2325,1 +2284,1 @@\n-  if (LockingMode != LM_LEGACY && method->is_object_wait0()) {\n+  if (method->is_object_wait0()) {\n@@ -2357,10 +2316,0 @@\n-    if (LockingMode == LM_LEGACY) {\n-      Label not_recur;\n-      \/\/ Simple recursive lock?\n-      __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), NULL_WORD);\n-      __ jcc(Assembler::notEqual, not_recur);\n-      __ dec_held_monitor_count();\n-      __ jmpb(fast_done);\n-      __ bind(not_recur);\n-    }\n-\n@@ -2372,17 +2321,1 @@\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(slow_path_unlock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n-      __ dec_held_monitor_count();\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n-    }\n+    __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":4,"deletions":71,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -1020,15 +1020,10 @@\n-  if (LockingMode != LM_LEGACY) {\n-    \/\/ Check preemption for Object.wait()\n-    Label not_preempted;\n-    __ movptr(rscratch1, Address(r15_thread, JavaThread::preempt_alternate_return_offset()));\n-    __ cmpptr(rscratch1, NULL_WORD);\n-    __ jccb(Assembler::equal, not_preempted);\n-    __ movptr(Address(r15_thread, JavaThread::preempt_alternate_return_offset()), NULL_WORD);\n-    __ jmp(rscratch1);\n-    __ bind(native_return);\n-    __ restore_after_resume(true \/* is_native *\/);\n-    __ bind(not_preempted);\n-  } else {\n-    \/\/ any pc will do so just use this one for LM_LEGACY to keep code together.\n-    __ bind(native_return);\n-  }\n+  \/\/ Check preemption for Object.wait()\n+  Label not_preempted;\n+  __ movptr(rscratch1, Address(r15_thread, JavaThread::preempt_alternate_return_offset()));\n+  __ cmpptr(rscratch1, NULL_WORD);\n+  __ jccb(Assembler::equal, not_preempted);\n+  __ movptr(Address(r15_thread, JavaThread::preempt_alternate_return_offset()), NULL_WORD);\n+  __ jmp(rscratch1);\n+  __ bind(native_return);\n+  __ restore_after_resume(true \/* is_native *\/);\n+  __ bind(not_preempted);\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":10,"deletions":15,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -14076,25 +14076,0 @@\n-instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr) %{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n-  ins_cost(300);\n-  format %{ \"fastlock $object,$box\\t! kills $box,$tmp,$scr\" %}\n-  ins_encode %{\n-    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register,\n-                 $scr$$Register, noreg, noreg, r15_thread, nullptr);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpFastUnlock(rFlagsReg cr, rRegP object, rax_RegP box, rRegP tmp) %{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp, USE_KILL box);\n-  ins_cost(300);\n-  format %{ \"fastunlock $object,$box\\t! kills $box,$tmp\" %}\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -14102,1 +14077,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -14114,1 +14088,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"}]}