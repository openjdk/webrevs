{"files":[{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -62,0 +63,22 @@\n+\n+\/\/ To enable detection of GC time trends, we keep separate track of the recent history of gc time.  During initialization,\n+\/\/ for example, the amount of live memory may be increasing, which is likely to cause the GC times to increase.  This history\n+\/\/ allows us to predict increasing GC times rather than always assuming average recent GC time is the best predictor.\n+const size_t ShenandoahAdaptiveHeuristics::GC_TIME_SAMPLE_SIZE = 3;\n+\n+\/\/ We also keep separate track of recently sampled allocation rates for two purposes:\n+\/\/  1. The number of samples examined to determine acceleration of allocation is represented by\n+\/\/     ShenandoahRateAccelerationSampleSize\n+\/\/  2. The number of most recent samples averaged to determine a momentary allocation spike is represented by\n+\/\/     ShenandoahMomentaryAllocationRateSpikeSampleSize\n+\n+\/\/ Allocation rates are sampled by the regulator thread, which typically runs every ms.  There may be jitter in the scheduling\n+\/\/ of the regulator thread.  To reduce signal noise and synchronization overhead, we do not sample allocation rate with every\n+\/\/ iteration of the regulator.  We prefer sample time longer than 1 ms so that there can be a statistically significant number\n+\/\/ of allocations occuring within each sample period.  The regulator thread samples allocation rate only if at least\n+\/\/ ShenandoahAccelerationSamplePeriod seconds have passed since it previously sampled the allocation rate.\n+\/\/\n+\/\/ This trigger responds much more quickly than the traditional trigger, which monitors 100 ms spans.  When acceleration is\n+\/\/ detected, the impact of acceleration on anticipated consumption of available memory is also much more impactful\n+\/\/ than the assumed constant allocation rate consumption of available memory.\n+\n@@ -67,1 +90,98 @@\n-  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor) { }\n+  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor),\n+  _free_set(nullptr),\n+  _is_generational(ShenandoahHeap::heap()->mode()->is_generational()),\n+  _regulator_thread(nullptr),\n+  _previous_allocation_timestamp(0.0),\n+  _gc_time_first_sample_index(0),\n+  _gc_time_num_samples(0),\n+  _gc_time_timestamps(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_samples(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_xy(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_xx(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_sum_of_timestamps(0),\n+  _gc_time_sum_of_samples(0),\n+  _gc_time_sum_of_xy(0),\n+  _gc_time_sum_of_xx(0),\n+  _gc_time_m(0.0),\n+  _gc_time_b(0.0),\n+  _gc_time_sd(0.0),\n+  _spike_acceleration_buffer_size(MAX2(ShenandoahRateAccelerationSampleSize, 1+ShenandoahMomentaryAllocationRateSpikeSampleSize)),\n+  _spike_acceleration_first_sample_index(0),\n+  _spike_acceleration_num_samples(0),\n+  _spike_acceleration_rate_samples(NEW_C_HEAP_ARRAY(double, _spike_acceleration_buffer_size, mtGC)),\n+  _spike_acceleration_rate_timestamps(NEW_C_HEAP_ARRAY(double, _spike_acceleration_buffer_size, mtGC)),\n+  _most_recent_headroom_at_start_of_idle((size_t) 0) {\n+  }\n+\n+ShenandoahAdaptiveHeuristics::~ShenandoahAdaptiveHeuristics() {\n+  FREE_C_HEAP_ARRAY(double, _spike_acceleration_rate_samples);\n+  FREE_C_HEAP_ARRAY(double, _spike_acceleration_rate_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_samples);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_xy);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_xx);\n+}\n+\n+void ShenandoahAdaptiveHeuristics::initialize() {\n+  ShenandoahHeuristics::initialize();\n+}\n+\n+void ShenandoahAdaptiveHeuristics::post_initialize() {\n+  ShenandoahHeuristics::post_initialize();\n+  _free_set = ShenandoahHeap::heap()->free_set();\n+  assert(!_is_generational, \"ShenandoahGenerationalHeuristics overrides this method\");\n+  _control_thread = ShenandoahHeap::heap()->control_thread();\n+  size_t global_available = (ShenandoahHeap::heap()->global_generation()->max_capacity() -\n+                             (ShenandoahHeap::heap()->global_generation()->used() + _free_set->reserved()));\n+  recalculate_trigger_threshold(global_available);\n+}\n+\n+void ShenandoahAdaptiveHeuristics::recalculate_trigger_threshold(size_t mutator_available) {\n+  \/\/ The trigger threshold represents mutator available - \"head room\".\n+  \/\/ We plan for GC to finish before the amount of allocated memory exceeds trigger threshold.  This is the same  as saying we\n+  \/\/ intend to finish GC before the amount of available memory is less than the allocation headroom.  Headroom is the planned\n+  \/\/ safety buffer to allow a small amount of additional allocation to take place in case we were overly optimistic in delaying\n+  \/\/ our trigger.\n+  size_t capacity = ShenandoahHeap::heap()->soft_max_capacity();\n+  size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n+  size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n+\n+  size_t bytes_allocated_at_start_of_idle_span = _free_set->get_bytes_allocated_since_gc_start();\n+\n+  \/\/ make headroom adjustments\n+  size_t headroom_adjustments = spike_headroom + penalties;\n+  if (mutator_available >= headroom_adjustments) {\n+    mutator_available -= headroom_adjustments;;\n+  } else {\n+    mutator_available = 0;\n+  }\n+\n+  assert(!_is_generational || !strcmp(_space_info->name(), \"Young\") || !strcmp(_space_info->name(), \"Global\"),\n+         \"Assumed young or global space, but got: %s\", _space_info->name());\n+  assert(_is_generational || !strcmp(_space_info->name(), \"\"), \"Assumed global (unnamed) space, but got: %s\", _space_info->name());\n+  log_info(gc)(\"At start or resumption of idle gc span for %s, mutator available set to: \" PROPERFMT\n+               \" after adjusting for spike_headroom: \" PROPERFMT \" and penalties: \" PROPERFMT,\n+               _is_generational? _space_info->name(): \"Global\",\n+               PROPERFMTARGS(mutator_available), PROPERFMTARGS(spike_headroom), PROPERFMTARGS(penalties));\n+\n+  _most_recent_headroom_at_start_of_idle = mutator_available;\n+  \/\/ _trigger_threshold is expressed in words\n+  _trigger_threshold = (bytes_allocated_at_start_of_idle_span + mutator_available) \/ HeapWordSize;\n+}\n+\n+void ShenandoahAdaptiveHeuristics::start_idle_span() {\n+  size_t mutator_available = _free_set->available();\n+  recalculate_trigger_threshold(mutator_available);\n+}\n+\n+void ShenandoahAdaptiveHeuristics::resume_idle_span() {\n+  size_t mutator_available = _free_set->available_holding_lock();\n+  recalculate_trigger_threshold(mutator_available);\n+}\n+\n+\/\/ There is no headroom during evacuation and update refs.  This information is not used to trigger the next GC.\n+\/\/ In future implementations, this information may feed into worker surge calculations.\n+void ShenandoahAdaptiveHeuristics::start_evac_span() {\n+  size_t mutator_available = _free_set->capacity() - _free_set->used();\n+  _trigger_threshold = mutator_available;\n+}\n@@ -69,1 +189,3 @@\n-ShenandoahAdaptiveHeuristics::~ShenandoahAdaptiveHeuristics() {}\n+void ShenandoahAdaptiveHeuristics::adjust_penalty(intx step) {\n+  ShenandoahHeuristics::adjust_penalty(step);\n+}\n@@ -79,2 +201,2 @@\n-  \/\/      during evacuation, and thus guarantee full GC. In practice, we also want to let\n-  \/\/      application to allocate something. This is why we limit CSet to some fraction of\n+  \/\/      during evacuation, and thus guarantee full GC. In practice, we also want to let the\n+  \/\/      application allocate during concurrent GC. This is why we limit CSet to some fraction of\n@@ -111,0 +233,1 @@\n+  \/\/ Regions are sorted in order of decreasing garbage\n@@ -129,0 +252,82 @@\n+void ShenandoahAdaptiveHeuristics::add_degenerated_gc_time(double timestamp, double gc_time) {\n+  \/\/ Conservatively add sample into linear model If this time is above the predicted concurrent gc time\n+  if (predict_gc_time(timestamp) < gc_time) {\n+    add_gc_time(timestamp, gc_time);\n+  }\n+}\n+\n+void ShenandoahAdaptiveHeuristics::add_gc_time(double timestamp, double gc_time) {\n+  \/\/ Update best-fit linear predictor of GC time\n+  uint index = (_gc_time_first_sample_index + _gc_time_num_samples) % GC_TIME_SAMPLE_SIZE;\n+  if (_gc_time_num_samples == GC_TIME_SAMPLE_SIZE) {\n+    _gc_time_sum_of_timestamps -= _gc_time_timestamps[index];\n+    _gc_time_sum_of_samples -= _gc_time_samples[index];\n+    _gc_time_sum_of_xy -= _gc_time_xy[index];\n+    _gc_time_sum_of_xx -= _gc_time_xx[index];\n+  }\n+  _gc_time_timestamps[index] = timestamp;\n+  _gc_time_samples[index] = gc_time;\n+  _gc_time_xy[index] = timestamp * gc_time;\n+  _gc_time_xx[index] = timestamp * timestamp;\n+\n+  _gc_time_sum_of_timestamps += _gc_time_timestamps[index];\n+  _gc_time_sum_of_samples += _gc_time_samples[index];\n+  _gc_time_sum_of_xy += _gc_time_xy[index];\n+  _gc_time_sum_of_xx += _gc_time_xx[index];\n+\n+  if (_gc_time_num_samples < GC_TIME_SAMPLE_SIZE) {\n+    _gc_time_num_samples++;\n+  } else {\n+    _gc_time_first_sample_index = (_gc_time_first_sample_index + 1) % GC_TIME_SAMPLE_SIZE;\n+  }\n+\n+  if (_gc_time_num_samples == 1) {\n+    \/\/ The predictor is constant (horizontal line)\n+    _gc_time_m = 0;\n+    _gc_time_b = gc_time;\n+    _gc_time_sd = 0.0;\n+  } else if (_gc_time_num_samples == 2) {\n+    \/\/ Two points define a line\n+    double delta_y = gc_time - _gc_time_samples[_gc_time_first_sample_index];\n+    double delta_x = timestamp - _gc_time_timestamps[_gc_time_first_sample_index];\n+    _gc_time_m = delta_y \/ delta_x;\n+\n+    \/\/ y = mx + b\n+    \/\/ so b = y0 - mx0\n+    _gc_time_b = gc_time - _gc_time_m * timestamp;\n+    _gc_time_sd = 0.0;\n+  } else {\n+    _gc_time_m = ((_gc_time_num_samples * _gc_time_sum_of_xy - _gc_time_sum_of_timestamps * _gc_time_sum_of_samples) \/\n+                  (_gc_time_num_samples * _gc_time_sum_of_xx - _gc_time_sum_of_timestamps * _gc_time_sum_of_timestamps));\n+    _gc_time_b = (_gc_time_sum_of_samples - _gc_time_m * _gc_time_sum_of_timestamps) \/ _gc_time_num_samples;\n+    double sum_of_squared_deviations = 0.0;\n+    for (size_t i = 0; i < _gc_time_num_samples; i++) {\n+      uint index = (_gc_time_first_sample_index + i) % GC_TIME_SAMPLE_SIZE;\n+      double x = _gc_time_timestamps[index];\n+      double predicted_y = _gc_time_m * x + _gc_time_b;\n+      double deviation = predicted_y - _gc_time_samples[index];\n+      sum_of_squared_deviations += deviation * deviation;\n+    }\n+    _gc_time_sd = sqrt(sum_of_squared_deviations \/ _gc_time_num_samples);\n+  }\n+}\n+\n+double ShenandoahAdaptiveHeuristics::predict_gc_time(double timestamp_at_start) {\n+  return _gc_time_m * timestamp_at_start + _gc_time_b + _gc_time_sd * _margin_of_error_sd;;\n+}\n+\n+void ShenandoahAdaptiveHeuristics::add_rate_to_acceleration_history(double timestamp, double rate) {\n+  uint new_sample_index =\n+    (_spike_acceleration_first_sample_index + _spike_acceleration_num_samples) % _spike_acceleration_buffer_size;\n+  _spike_acceleration_rate_timestamps[new_sample_index] = timestamp;\n+  _spike_acceleration_rate_samples[new_sample_index] = rate;\n+  if (_spike_acceleration_num_samples == _spike_acceleration_buffer_size) {\n+    _spike_acceleration_first_sample_index++;\n+    if (_spike_acceleration_first_sample_index == _spike_acceleration_buffer_size) {\n+      _spike_acceleration_first_sample_index = 0;\n+    }\n+  } else {\n+    _spike_acceleration_num_samples++;\n+  }\n+}\n+\n@@ -136,0 +341,4 @@\n+  double now = os::elapsedTime();\n+\n+  \/\/ Should we not add GC time if this was an abbreviated cycle?\n+  add_gc_time(_cycle_start, elapsed_cycle_time());\n@@ -188,0 +397,1 @@\n+  add_degenerated_gc_time(_precursor_cycle_start, elapsed_degenerated_cycle_time());\n@@ -239,0 +449,19 @@\n+  double avg_cycle_time = 0;\n+  double avg_alloc_rate = 0;\n+  double now = get_most_recent_wake_time();\n+  size_t allocatable_words = this->allocatable();\n+  double predicted_future_accelerated_gc_time = 0.0;\n+  size_t allocated_bytes_since_last_sample = 0;\n+  double instantaneous_rate_words_per_second = 0.0;\n+  size_t consumption_accelerated = 0;\n+  double acceleration = 0.0;\n+  double current_rate_by_acceleration = 0.0;\n+  size_t min_threshold = min_free_threshold();\n+  double predicted_future_gc_time = 0;\n+  double future_planned_gc_time = 0;\n+  bool future_planned_gc_time_is_average = false;\n+  double avg_time_to_deplete_available = 0.0;\n+  bool is_spiking = false;\n+  double spike_time_to_deplete_available = 0.0;\n+  double spike_rate = 0.0;\n+\n@@ -251,0 +480,4 @@\n+  \/\/ Track allocation rate even if we decide to start a cycle for other reasons.  With default value of 10 for\n+  \/\/ ShenandoahDaptiveSampleSizeSeconds, the allocation rate is only updated if 100 ms have accumulated since the\n+  \/\/ last update.  Otherwise, allocated is ignored and spike_rate is reported as 0.\n+  spike_rate = _allocation_rate.sample(allocated);\n@@ -253,1 +486,0 @@\n-  size_t min_threshold = min_free_threshold();\n@@ -274,4 +506,0 @@\n-  \/\/ Check if allocation headroom is still okay. This also factors in:\n-  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n-  \/\/   2. Accumulated penalties from Degenerated and Full GC\n-  size_t allocation_headroom = available;\n@@ -279,2 +507,141 @@\n-  size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n-  size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n+  avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+  avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+\n+  if ((now - _previous_allocation_timestamp) >= ShenandoahAccelerationSamplePeriod) {\n+    predicted_future_accelerated_gc_time =\n+      predict_gc_time(now + MAX2(get_planned_sleep_interval(), ShenandoahAccelerationSamplePeriod));\n+    double future_accelerated_planned_gc_time;\n+    bool future_accelerated_planned_gc_time_is_average;\n+    if (predicted_future_accelerated_gc_time > avg_cycle_time) {\n+      future_accelerated_planned_gc_time = predicted_future_accelerated_gc_time;\n+      future_accelerated_planned_gc_time_is_average = false;\n+    } else {\n+      future_accelerated_planned_gc_time = avg_cycle_time;\n+      future_accelerated_planned_gc_time_is_average = true;\n+    }\n+    allocated_bytes_since_last_sample = _free_set->get_bytes_allocated_since_previous_sample();\n+    instantaneous_rate_words_per_second =\n+      (allocated_bytes_since_last_sample \/ HeapWordSize) \/ (now - _previous_allocation_timestamp);\n+\n+    _previous_allocation_timestamp = now;\n+    add_rate_to_acceleration_history(now, instantaneous_rate_words_per_second);\n+    current_rate_by_acceleration = instantaneous_rate_words_per_second;\n+    consumption_accelerated =\n+      accelerated_consumption(acceleration, current_rate_by_acceleration, avg_alloc_rate \/ HeapWordSize,\n+                              ShenandoahAccelerationSamplePeriod + future_accelerated_planned_gc_time);\n+\n+    \/\/ Note that even a single thread that wakes up and begins to allocate excessively can manifest as accelerating allocation\n+    \/\/ rate. This thread will initially allocate a TLAB of minimum size.  Then it will allocate a TLAB twice as big a bit later,\n+    \/\/ and then twice as big again after another short delay.  When a phase change causes many threads to increase their\n+    \/\/ allocation behavior, this effect is multiplied, and compounded by jitter in the times that individual threads experience\n+    \/\/ the phase change.\n+    \/\/\n+    \/\/ The following trace represents an actual workload, with allocation rates sampled at 10 Hz, the default behavior before\n+    \/\/ introduction of accelerated allocation rate detection.  Though the allocation rate is seen to be increasing at times\n+    \/\/ 101.907 and 102.007 and 102.108, the newly sampled allocation rate is not enough to trigger GC because the headroom is\n+    \/\/ still quite large.  In fact, GC is not triggered until time 102.409s, and this GC degenerates.\n+    \/\/\n+    \/\/    Sample Time (s)      Allocation Rate (MB\/s)       Headroom (GB)\n+    \/\/       101.807                       0.0                  26.93\n+    \/\/                                                                  <--- accelerated spike can trigger here, around time 101.9s\n+    \/\/       101.907                     477.6                  26.85\n+    \/\/       102.007                   3,206.0                  26.35\n+    \/\/       102.108                  23,797.8                  24.19\n+    \/\/       102.208                  24,164.5                  21.83\n+    \/\/       102.309                  23,965.0                  19.47\n+    \/\/       102.409                  24,624.35                 17.05   <--- without accelerated rate detection, we trigger here\n+    \/\/\n+    \/\/ Though the above measurements are from actual workload, the following details regarding sampled allocation rates at 3ms\n+    \/\/ period were not measured directly for this run-time sample.  These are hypothetical, though they represent a plausible\n+    \/\/ result that correlates with the actual measurements.\n+    \/\/\n+    \/\/ For most of the 100 ms time span that precedes the sample at 101.907, the allocation rate still remains at zero.  The phase\n+    \/\/ change that causes increasing allocations occurs near the end ot this time segment.  When sampled with a 3 ms period,\n+    \/\/ acceration of allocation can be triggered at approximately time 101.88s.\n+    \/\/\n+    \/\/ In the default configuration, accelerated allocation rate is detected by examining a sequence of 5 allocation rate samples.\n+    \/\/\n+    \/\/ Even a single allocation rate sample above the norm can be interpreted as acceleration of allocation rate.  For example, the\n+    \/\/ the best-fit line for the following samples has an acceleration rate of 3,553.3 MB\/s\/s.  This is not enough to trigger GC,\n+    \/\/ especially given the abundance of Headroom at this moment in time.\n+    \/\/\n+    \/\/    TimeStamp (s)     Alloc rate (MB\/s)\n+    \/\/    101.857                 0\n+    \/\/    101.860                 0\n+    \/\/    101.863                 0\n+    \/\/    101.866                 0\n+    \/\/    101.869                53.3\n+    \/\/\n+    \/\/ At the next sample time, we will compute a slightly higher acceration, 9,150 MB\/s\/s.  This is also insufficient to trigger\n+    \/\/ GC.\n+    \/\/\n+    \/\/    TimeStamp (s)     Alloc rate (MB\/s)\n+    \/\/    101.860                 0\n+    \/\/    101.863                 0\n+    \/\/    101.866                 0\n+    \/\/    101.869                53.3\n+    \/\/    101.872               110.6\n+    \/\/\n+    \/\/ Eventually, we will observe a full history of accelerating rate samples, computing acceleration of 18,500 MB\/s\/s.  This will\n+    \/\/ trigger GC over 500 ms earlier than was previously possible.\n+    \/\/\n+    \/\/    TimeStamp (s)     Alloc rate (MB\/s)\n+    \/\/    101.866                 0\n+    \/\/    101.869                53.3\n+    \/\/    101.872               110.6\n+    \/\/    101.875               165.9\n+    \/\/    101.878               221.2\n+    \/\/\n+    \/\/ The accelerated rate heuristic is based on the following idea:\n+    \/\/\n+    \/\/    Assume allocation rate is accelerating at a constant rate.  If we postpone the spike trigger until the subsequent\n+    \/\/    sample point, will there be enough memory to satisfy allocations that occur during the anticipated concurrent GC\n+    \/\/    cycle?  If not, we should trigger right now.\n+    \/\/\n+    \/\/ Outline of this heuristic triggering technique:\n+    \/\/\n+    \/\/  1. We remember the N (e.g. N=3) most recent samples of spike allocation rate r0, r1, r2 samples at t0, t1, and t2\n+    \/\/  2. if r1 < r0 or r2 < r1, approximate Acceleration = 0.0, Rate = Average(r0, r1, r2)\n+    \/\/  3. Otherwise, use least squares method to compute best-fit line of rate vs time\n+    \/\/  4. The slope of this line represents Acceleration. The y-intercept of this line represents \"initial rate\"\n+    \/\/  5. Use r2 to rrpresent CurrentRate\n+    \/\/  6. Use Consumption = CurrentRate * GCTime + 1\/2 * Acceleration * GCTime * GCTime\n+    \/\/     (See High School physics discussions on constant acceleration: D = v0 * t + 1\/2 * a * t^2)\n+    \/\/  7. if Consumption exceeds headroom, trigger now\n+    \/\/\n+    \/\/ Though larger sample size may improve quality of predictor, it would delay our trigger response as well.  Smaller sample\n+    \/\/ sizes are more susceptible to false triggers based on random noise.  The default configuration uses a sample size of 5,\n+    \/\/ spanning 15ms of execution.\n+\n+    if (consumption_accelerated > allocatable_words) {\n+      size_t size_t_alloc_rate = (size_t) current_rate_by_acceleration * HeapWordSize;\n+      if (acceleration > 0) {\n+        size_t size_t_acceleration = (size_t) acceleration * HeapWordSize;\n+        log_trigger(\"Accelerated consumption (%zu%s) exceeds free headroom (%zu%s) at \"\n+                    \"current rate (%zu%s\/s) with acceleration (%zu%s\/s\/s) for planned %s GC time (%.2f ms)\",\n+                    byte_size_in_proper_unit(consumption_accelerated * HeapWordSize), proper_unit_for_byte_size(consumption_accelerated * HeapWordSize),\n+                    byte_size_in_proper_unit(allocatable_words * HeapWordSize), proper_unit_for_byte_size(allocatable_words * HeapWordSize),\n+                    byte_size_in_proper_unit(size_t_alloc_rate), proper_unit_for_byte_size(size_t_alloc_rate),\n+                    byte_size_in_proper_unit(size_t_acceleration), proper_unit_for_byte_size(size_t_acceleration),\n+                    future_accelerated_planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\",\n+                    future_accelerated_planned_gc_time * 1000);\n+      } else {\n+        log_trigger(\"Momentary spike consumption (%zu%s) exceeds free headroom (%zu%s) at \"\n+                    \"current rate (%zu%s\/s) for planned %s GC time (%.2f ms) (spike threshold = %.2f)\",\n+                    byte_size_in_proper_unit(consumption_accelerated * HeapWordSize), proper_unit_for_byte_size(consumption_accelerated * HeapWordSize),\n+                    byte_size_in_proper_unit(allocatable_words * HeapWordSize), proper_unit_for_byte_size(allocatable_words * HeapWordSize),\n+                    byte_size_in_proper_unit(size_t_alloc_rate), proper_unit_for_byte_size(size_t_alloc_rate),\n+                    future_accelerated_planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\",\n+                    future_accelerated_planned_gc_time * 1000, _spike_threshold_sd);\n+\n+\n+      }\n+      _spike_acceleration_num_samples = 0;\n+      _spike_acceleration_first_sample_index = 0;\n+\n+      \/\/ Count this as a form of RATE trigger for purposes of adjusting heuristic triggering configuration because this\n+      \/\/ trigger is influenced more by margin_of_error_sd than by spike_threshold_sd.\n+      accept_trigger_with_type(RATE);\n+      return true;\n+    }\n+  }\n@@ -282,15 +649,29 @@\n-  allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n-  allocation_headroom -= MIN2(allocation_headroom, penalties);\n-\n-  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n-  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n-\n-  log_debug(gc)(\"average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n-          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n-  if (avg_cycle_time * avg_alloc_rate > allocation_headroom) {\n-    log_trigger(\"Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n-                 \" to deplete free headroom (%zu%s) (margin of error = %.2f)\",\n-                 avg_cycle_time * 1000,\n-                 byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                 _margin_of_error_sd);\n+  \/\/ Suppose we don't trigger now, but decide to trigger in the next regulator cycle.  What will be the GC time then?\n+  predicted_future_gc_time = predict_gc_time(now + get_planned_sleep_interval());\n+  if (predicted_future_gc_time > avg_cycle_time) {\n+    future_planned_gc_time = predicted_future_gc_time;\n+    future_planned_gc_time_is_average = false;\n+  } else {\n+    future_planned_gc_time = avg_cycle_time;\n+    future_planned_gc_time_is_average = true;\n+  }\n+\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, predicted GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                _space_info->name(), avg_cycle_time * 1000, predicted_future_gc_time * 1000,\n+                byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n+  size_t allocatable_bytes = allocatable_words * HeapWordSize;\n+  avg_time_to_deplete_available = allocatable_bytes \/ avg_alloc_rate;\n+\n+  if (future_planned_gc_time > avg_time_to_deplete_available) {\n+    log_trigger(\"%s GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                \" to deplete free headroom (%zu%s) (margin of error = %.2f)\",\n+                future_planned_gc_time_is_average? \"Average\": \"Linear prediction of\", future_planned_gc_time * 1000,\n+                byte_size_in_proper_unit(avg_alloc_rate),    proper_unit_for_byte_size(avg_alloc_rate),\n+                byte_size_in_proper_unit(allocatable_bytes), proper_unit_for_byte_size(allocatable_bytes),\n+                _margin_of_error_sd);\n+\n+    size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n+    size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n+    size_t allocation_headroom = available;\n+    allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n+    allocation_headroom -= MIN2(allocation_headroom, penalties);\n@@ -306,7 +687,9 @@\n-  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n-  if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n-    log_trigger(\"Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (%zu%s) (spike threshold = %.2f)\",\n-                 avg_cycle_time * 1000,\n-                 byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                 _spike_threshold_sd);\n+  is_spiking = _allocation_rate.is_spiking(spike_rate, _spike_threshold_sd);\n+  spike_time_to_deplete_available = (spike_rate == 0)? future_planned_gc_time: allocatable_bytes \/ spike_rate;\n+  if (is_spiking && future_planned_gc_time > spike_time_to_deplete_available) {\n+    log_trigger(\"%s GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s)\"\n+                \" to deplete free headroom (%zu%s) (spike threshold = %.2f)\",\n+                future_planned_gc_time_is_average? \"Average\": \"Linear prediction of\", future_planned_gc_time * 1000,\n+                byte_size_in_proper_unit(spike_rate),        proper_unit_for_byte_size(spike_rate),\n+                byte_size_in_proper_unit(allocatable_bytes), proper_unit_for_byte_size(allocatable_bytes),\n+                _spike_threshold_sd);\n@@ -316,7 +699,1 @@\n-\n-  if (ShenandoahHeuristics::should_start_gc()) {\n-    _start_gc_is_pending = true;\n-    return true;\n-  } else {\n-    return false;\n-  }\n+  return ShenandoahHeuristics::should_start_gc();\n@@ -355,0 +732,102 @@\n+\/\/ This is called each time a new rate sample has been gathered, as governed by MINMUM_ALLOC_RATE_SAMPLE_INTERVAL.\n+\/\/ There is no adjustment for standard deviation of the accelerated rate prediction.\n+size_t ShenandoahAdaptiveHeuristics::accelerated_consumption(double& acceleration, double& current_rate,\n+                                                             double avg_alloc_rate_words_per_second,\n+                                                             double predicted_cycle_time) const\n+{\n+  double *x_array = (double *) alloca(ShenandoahRateAccelerationSampleSize * sizeof(double));\n+  double *y_array = (double *) alloca(ShenandoahRateAccelerationSampleSize * sizeof(double));\n+  double x_sum = 0.0;\n+  double y_sum = 0.0;\n+\n+  assert(_spike_acceleration_num_samples > 0, \"At minimum, we should have sample from this period\");\n+\n+  double weighted_average_alloc;\n+  if (_spike_acceleration_num_samples >= ShenandoahRateAccelerationSampleSize) {\n+    double weighted_y_sum = 0;\n+    double total_weight = 0;\n+    double previous_x = 0;\n+    uint delta = _spike_acceleration_num_samples - ShenandoahRateAccelerationSampleSize;\n+    for (uint i = 0; i < ShenandoahRateAccelerationSampleSize; i++) {\n+      uint index = (_spike_acceleration_first_sample_index + delta + i) % _spike_acceleration_buffer_size;\n+      x_array[i] = _spike_acceleration_rate_timestamps[index];\n+      x_sum += x_array[i];\n+      y_array[i] = _spike_acceleration_rate_samples[index];\n+      if (i > 0) {\n+        \/\/ first sample not included in weighted average because it has no weight.\n+        double sample_weight = x_array[i] - x_array[i-1];\n+        weighted_y_sum = y_array[i] * sample_weight;\n+        total_weight += sample_weight;\n+      }\n+      y_sum += y_array[i];\n+    }\n+    weighted_average_alloc = (total_weight > 0)? weighted_y_sum \/ total_weight: 0;\n+  } else {\n+    weighted_average_alloc = 0;\n+  }\n+\n+  double momentary_rate;\n+  if (_spike_acceleration_num_samples > ShenandoahMomentaryAllocationRateSpikeSampleSize) {\n+    \/\/ Num samples must be strictly greater than sample size, because we need one extra sample to compute rate and weights\n+    double weighted_y_sum = 0;\n+    double total_weight = 0;\n+    double sum_for_average = 0.0;\n+    uint delta = _spike_acceleration_num_samples - ShenandoahMomentaryAllocationRateSpikeSampleSize;\n+    for (uint i = 0; i < ShenandoahMomentaryAllocationRateSpikeSampleSize; i++) {\n+      uint sample_index = (_spike_acceleration_first_sample_index + delta + i) % _spike_acceleration_buffer_size;\n+      uint preceding_index = (sample_index == 0)? _spike_acceleration_buffer_size - 1: sample_index - 1;\n+      double sample_weight = (_spike_acceleration_rate_timestamps[sample_index]\n+                              - _spike_acceleration_rate_timestamps[preceding_index]);\n+      weighted_y_sum += _spike_acceleration_rate_samples[sample_index] * sample_weight;\n+      total_weight += sample_weight;\n+    }\n+    momentary_rate = weighted_y_sum \/ total_weight;\n+    bool is_spiking = _allocation_rate.is_spiking(momentary_rate, _spike_threshold_sd);\n+    if (!is_spiking) {\n+      \/\/ Disable momentary spike trigger unless allocation rate delta from average exceeds sd\n+      momentary_rate = 0.0;\n+    }\n+  } else {\n+    momentary_rate = 0.0;\n+  }\n+\n+  \/\/ By default, use momentary_rate for current rate and zero acceleration. Overwrite iff best-fit line has positive slope.\n+  current_rate = momentary_rate;\n+  acceleration = 0.0;\n+  if ((_spike_acceleration_num_samples >= ShenandoahRateAccelerationSampleSize)\n+      && (weighted_average_alloc >= avg_alloc_rate_words_per_second))  {\n+    \/\/ If the average rate across the acceleration samples is below the overall average, this sample is not eligible to\n+    \/\/  represent acceleration of allocation rate.  We may just be catching up with allocations after a lull.\n+\n+    double *xy_array = (double *) alloca(ShenandoahRateAccelerationSampleSize * sizeof(double));\n+    double *x2_array = (double *) alloca(ShenandoahRateAccelerationSampleSize * sizeof(double));\n+    double xy_sum = 0.0;\n+    double x2_sum = 0.0;\n+    for (uint i = 0; i < ShenandoahRateAccelerationSampleSize; i++) {\n+      xy_array[i] = x_array[i] * y_array[i];\n+      xy_sum += xy_array[i];\n+      x2_array[i] = x_array[i] * x_array[i];\n+      x2_sum += x2_array[i];\n+    }\n+    \/\/ Find the best-fit least-squares linear representation of rate vs time\n+    double m;                 \/* slope *\/\n+    double b;                 \/* y-intercept *\/\n+\n+    m = ((ShenandoahRateAccelerationSampleSize * xy_sum - x_sum * y_sum)\n+         \/ (ShenandoahRateAccelerationSampleSize * x2_sum - x_sum * x_sum));\n+    b = (y_sum - m * x_sum) \/ ShenandoahRateAccelerationSampleSize;\n+\n+    if (m > 0) {\n+      double proposed_current_rate = m * x_array[ShenandoahRateAccelerationSampleSize - 1] + b;\n+      acceleration = m;\n+      current_rate = proposed_current_rate;\n+    }\n+    \/\/ else, leave current_rate = y_max, acceleration = 0\n+  }\n+  \/\/ and here also, leave current_rate = y_max, acceleration = 0\n+\n+  double time_delta = get_planned_sleep_interval() + predicted_cycle_time;\n+  size_t words_to_be_consumed = (size_t) (current_rate * time_delta + 0.5 * acceleration * time_delta * time_delta);\n+  return words_to_be_consumed;\n+}\n+\n@@ -415,2 +894,4 @@\n-    \/\/ There is a small chance that that rate has already been sampled, but it\n-    \/\/ seems not to matter in practice.\n+    \/\/ There is a small chance that that rate has already been sampled, but it seems not to matter in practice.\n+    \/\/ Note that z_score reports how close the rate is to the average.  A value between -1 and 1 means we are within one\n+    \/\/ standard deviation.  A value between -3 and +3 means we are within 3 standard deviations.  We only check for z_score\n+    \/\/ greater than threshold because we are looking for an allocation spike which is greater than the mean.\n@@ -432,0 +913,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":524,"deletions":42,"binary":false,"changes":566,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -71,0 +73,7 @@\n+  double interval() const {\n+    return _interval_sec;\n+  }\n+  double last_sample_time() const {\n+    return _last_sample_time;\n+  }\n+\n@@ -111,0 +120,4 @@\n+  virtual void initialize() override;\n+\n+  virtual void post_initialize() override;\n+\n@@ -115,0 +128,25 @@\n+  virtual void adjust_penalty(intx step) override;\n+\n+  \/\/ At the end of GC(N), we idle GC until necessary to start the next GC.  Compute the threshold of memory that can be allocated\n+  \/\/ before we need to start the next GC.\n+  void start_idle_span() override;\n+\n+  \/\/ If old-generation marking finishes during an idle span and immediate old-generation garbage is identified, we will rebuild\n+  \/\/ the free set.  If this happens, resume_idle_span() recomputes the threshold of memory that can be allocated before we need\n+  \/\/ to start the next GC.\n+  void resume_idle_span() override;\n+\n+  \/\/ As we begin to do evacuation, adjust the trigger threshold to not account for headroom, as we are now free to allocate\n+  \/\/ everything that remains in the mutator set up until that is exhausted.  Our hope is that we finish GC before the\n+  \/\/ remaining mutator memory is fully depleted.\n+  void start_evac_span() override;\n+\n+  \/\/ Having observed a new allocation rate sample, add this to the acceleration history so that we can determine if allocation\n+  \/\/ rate is accelerating.\n+  void add_rate_to_acceleration_history(double timestamp, double rate);\n+\n+  \/\/ Compute and return the current allocation rate, the current rate of acceleration, and the amount of memory that we expect\n+  \/\/ to consume if we start GC right now and gc takes predicted_cycle_time to complete.\n+  size_t accelerated_consumption(double& acceleration, double& current_rate,\n+                                 double avg_rate_words_per_sec, double predicted_cycle_time) const;\n+\n@@ -139,0 +177,2 @@\n+  const static size_t GC_TIME_SAMPLE_SIZE;\n+\n@@ -153,0 +193,7 @@\n+  \/\/ Returns number of words that can be allocated before we need to trigger next GC.\n+  inline size_t allocatable() const {\n+    size_t allocated_bytes = _free_set->get_bytes_allocated_since_gc_start();\n+    size_t allocated_words = allocated_bytes \/ HeapWordSize;\n+    return (allocated_words < _trigger_threshold)? _trigger_threshold - allocated_words: 0;\n+  }\n+\n@@ -156,0 +203,7 @@\n+  \/\/ Invocations of should_start_gc() happen approximately once per ms.  Approximately every third invocation  of should_start_gc()\n+  \/\/ queries the allocation rate.\n+  size_t _allocated_at_previous_query;\n+\n+  double _time_of_previous_allocation_query;\n+\n+\n@@ -182,0 +236,46 @@\n+  ShenandoahFreeSet* _free_set;\n+  bool _is_generational;\n+  ShenandoahRegulatorThread* _regulator_thread;\n+  ShenandoahController* _control_thread;\n+\n+  double _previous_allocation_timestamp;\n+  size_t _total_allocations_at_start_of_idle;\n+  size_t _trigger_threshold;\n+\n+  \/\/ Keep track of GC_TIME_SAMPLE_SIZE most recent concurrent GC cycle times\n+  uint _gc_time_first_sample_index;\n+  uint _gc_time_num_samples;\n+  double* const _gc_time_timestamps;\n+  double* const _gc_time_samples;\n+  double* const _gc_time_xy;    \/\/ timestamp * sample\n+  double* const _gc_time_xx;    \/\/ timestamp squared\n+  double _gc_time_sum_of_timestamps;\n+  double _gc_time_sum_of_samples;\n+  double _gc_time_sum_of_xy;\n+  double _gc_time_sum_of_xx;\n+\n+  double _gc_time_m;            \/\/ slope\n+  double _gc_time_b;            \/\/ y-intercept\n+  double _gc_time_sd;           \/\/ sd on deviance from prediction\n+\n+  \/\/ _trigger_threshold, represented in words, is the amount of memory that we allow ourselves to allocate while concurrent\n+  \/\/ GC is running.  If anticipated consumption of mutator memory during GC (e.g. average alloc rate * average GC time)\n+  \/\/ exceeds _trigger_threshold, we need to start GC now.  Note that we intend NOT to allocate the headroom reserve,\n+  \/\/ so this is not included in the _trigger_threshold.\n+  void recalculate_trigger_threshold(size_t mutator_available);\n+\n+  void add_gc_time(double timestamp_at_start, double duration);\n+  void add_degenerated_gc_time(double timestamp_at_start, double duration);\n+  double predict_gc_time(double timestamp_at_start);\n+\n+  \/\/ Keep track of SPIKE_ACCELERATION_SAMPLE_SIZE most recent spike allocation rate measurements. Note that it is\n+  \/\/ typical to experience a small spike following end of GC cycle, as mutator threads refresh their TLABs.  But\n+  \/\/ there is generally an abundance of memory at this time as well, so this will not generally trigger GC.\n+  uint _spike_acceleration_buffer_size;\n+  uint _spike_acceleration_first_sample_index;\n+  uint _spike_acceleration_num_samples;\n+  double* const _spike_acceleration_rate_samples; \/\/ holds rates in words\/second\n+  double* const _spike_acceleration_rate_timestamps;\n+\n+  size_t _most_recent_headroom_at_start_of_idle;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -40,0 +41,9 @@\n+void ShenandoahGenerationalHeuristics::post_initialize() {\n+  ShenandoahHeuristics::post_initialize();\n+  _free_set = ShenandoahHeap::heap()->free_set();\n+  _regulator_thread = ShenandoahGenerationalHeap::heap()->regulator_thread();\n+  size_t young_available = (ShenandoahGenerationalHeap::heap()->young_generation()->max_capacity() -\n+                            (ShenandoahGenerationalHeap::heap()->young_generation()->used() + _free_set->reserved()));\n+  recalculate_trigger_threshold(young_available);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -48,0 +48,3 @@\n+\n+  virtual void post_initialize() override;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+  _most_recent_trigger_evaluation_time(os::elapsedTime()),\n+  _most_recent_planned_sleep_interval(0.0),\n@@ -55,1 +57,2 @@\n-  _cycle_start(os::elapsedTime()),\n+  _precursor_cycle_start(os::elapsedTime()),\n+  _cycle_start(_precursor_cycle_start),\n@@ -160,0 +163,21 @@\n+void ShenandoahHeuristics::start_idle_span() {\n+  \/\/ do nothing\n+}\n+\n+void ShenandoahHeuristics::start_evac_span() {\n+  \/\/ do nothing\n+}\n+\n+void ShenandoahHeuristics::resume_idle_span() {\n+  \/\/ do nothing\n+}\n+\n+void ShenandoahHeuristics::record_degenerated_cycle_start(bool out_of_cycle) {\n+  if (out_of_cycle) {\n+    _precursor_cycle_start = _cycle_start = os::elapsedTime();\n+  } else {\n+    _precursor_cycle_start = _cycle_start;\n+    _cycle_start = os::elapsedTime();\n+  }\n+}\n+\n@@ -201,1 +225,0 @@\n-\n@@ -278,0 +301,4 @@\n+void ShenandoahHeuristics::post_initialize() {\n+  \/\/ Nothing to do by default.\n+}\n+\n@@ -281,0 +308,7 @@\n+\n+\n+\/\/ Includes the time spent in abandoned concurrent GC cycle that may have triggered this degenerated cycle.\n+double ShenandoahHeuristics::elapsed_degenerated_cycle_time() const {\n+  double now = os::elapsedTime();\n+  return now - _precursor_cycle_start;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -81,0 +81,4 @@\n+private:\n+  double _most_recent_trigger_evaluation_time;\n+  double _most_recent_planned_sleep_interval;\n+\n@@ -88,1 +92,1 @@\n-                                       ;  \/\/ This represents the value of _declined_trigger_count as captured at the\n+                                          \/\/ This represents the value of _declined_trigger_count as captured at the\n@@ -95,1 +99,0 @@\n-\n@@ -106,0 +109,1 @@\n+\n@@ -167,0 +171,1 @@\n+  double _precursor_cycle_start;\n@@ -183,1 +188,1 @@\n-  void adjust_penalty(intx step);\n+  virtual void adjust_penalty(intx step);\n@@ -195,0 +200,8 @@\n+  inline double get_most_recent_wake_time() const {\n+    return _most_recent_trigger_evaluation_time;\n+  }\n+\n+  inline double get_planned_sleep_interval() const {\n+    return _most_recent_planned_sleep_interval;\n+  }\n+\n@@ -207,0 +220,4 @@\n+  virtual void start_idle_span();\n+  virtual void start_evac_span();\n+  virtual void resume_idle_span();\n+\n@@ -209,0 +226,2 @@\n+  void record_degenerated_cycle_start(bool out_of_cycle);\n+\n@@ -211,0 +230,5 @@\n+  void update_should_start_query_times(double now, double planned_sleep_interval) {\n+    _most_recent_trigger_evaluation_time = now;\n+    _most_recent_planned_sleep_interval = planned_sleep_interval;\n+  }\n+\n@@ -241,0 +265,1 @@\n+  virtual void post_initialize();\n@@ -243,0 +268,1 @@\n+  double elapsed_degenerated_cycle_time() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -129,0 +129,1 @@\n+    \/\/ ShenandoahAdaptiveHeuristics::should_start_gc() has already accepted trigger, or declined it.\n@@ -170,1 +171,1 @@\n-  size_t allocated = _space_info->bytes_allocated_since_gc_start();\n+  size_t allocated = _free_set->get_bytes_allocated_since_gc_start();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -99,0 +99,10 @@\n+void ShenandoahCollectorPolicy::record_success_degenerated(bool is_young, bool is_abbreviated) {\n+  update_young(is_young);\n+\n+  _success_degenerated_gcs++;\n+  _consecutive_degenerated_gcs++;\n+  if (is_abbreviated) {\n+    _abbreviated_degenerated_gcs++;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+  void record_success_degenerated(bool is_young, bool is_abbreviated);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1214,0 +1214,1 @@\n+  _generation->heuristics()->start_idle_span();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+  double most_recent_wake_time = os::elapsedTime();\n@@ -225,1 +226,1 @@\n-    const double current = os::elapsedTime();\n+    const double before_sleep = most_recent_wake_time;\n@@ -228,1 +229,1 @@\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+    } else if ((before_sleep - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n@@ -230,1 +231,1 @@\n-      last_sleep_adjust_time = current;\n+      last_sleep_adjust_time = before_sleep;\n@@ -232,1 +233,0 @@\n-\n@@ -235,0 +235,11 @@\n+    \/\/ Record a conservative estimate of the longest anticipated sleep duration until we sample again.\n+    double planned_sleep_interval = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2)) \/ 1000.0;\n+    most_recent_wake_time = os::elapsedTime();\n+    heuristics->update_should_start_query_times(most_recent_wake_time, planned_sleep_interval);\n+    if (LogTarget(Debug, gc, thread)::is_enabled()) {\n+      double elapsed = most_recent_wake_time - before_sleep;\n+      double hiccup = elapsed - double(sleep);\n+      if (hiccup > 0.001) {\n+        log_debug(gc, thread)(\"Control Thread hiccup time: %.3fs\", hiccup);\n+      }\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -316,0 +316,1 @@\n+    heap->shenandoah_policy()->record_success_degenerated(_generation->is_young(), _abbreviated);\n@@ -317,0 +318,1 @@\n+    heap->start_idle_span();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -290,0 +290,1 @@\n+\n@@ -292,0 +293,14 @@\n+  \/\/ Future inquiries of get_total_bytes_allocated() will return the sum of\n+  \/\/    _total_bytes_previously_allocated and _mutator_bytes_allocated_since_gc_start.\n+  \/\/ Since _mutator_bytes_allocated_since_gc_start does not start at zero, we subtract initial_bytes_allocated so as\n+  \/\/ to not double count these allocated bytes.\n+  size_t original_mutator_bytes_allocated_since_gc_start = _mutator_bytes_allocated_since_gc_start;\n+\n+  \/\/ Setting _mutator_bytes_allocated_since_gc_start before _total_bytes_previously_allocated reduces the damage\n+  \/\/ in the case that the control or regulator thread queries get_bytes_allocated_since_previous_sample() between\n+  \/\/ the two assignments.\n+  \/\/\n+  \/\/ These are not declared as volatile so the compiler or hardware may reorder the assignments.  The implementation of\n+  \/\/ get_bytes_allocated_since_previous_cycle() is robust to this possibility, as are triggering heuristics.  The current\n+  \/\/ implementation assumes we are better off to tolerate the very rare race rather than impose a synchronization penalty\n+  \/\/ on every update and fetch.  (Perhaps it would be better to make the opposite tradeoff for improved maintainability.)\n@@ -293,0 +308,1 @@\n+  _total_bytes_previously_allocated += original_mutator_bytes_allocated_since_gc_start - initial_bytes_allocated;\n@@ -1192,0 +1208,2 @@\n+  _total_bytes_previously_allocated(0),\n+  _mutator_bytes_at_last_sample(0),\n@@ -1618,3 +1636,0 @@\n-    \/\/ Also, if this allocation request failed and the consumed within this region * ShenandoahEvacWaste > region size,\n-    \/\/ then retire the region so that subsequent searches can find available memory more quickly.\n-\n@@ -1736,1 +1751,0 @@\n-\n@@ -1973,1 +1987,2 @@\n-void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_trashed_regions, size_t &old_trashed_regions,\n+\/\/ Returns total allocatable words in Mutator partition\n+size_t ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_trashed_regions, size_t &old_trashed_regions,\n@@ -1991,0 +2006,2 @@\n+  size_t mutator_alloc_capacity_in_words = 0;\n+\n@@ -2053,0 +2070,1 @@\n+          mutator_alloc_capacity_in_words += ac \/ HeapWordSize;\n@@ -2200,0 +2218,1 @@\n+  return mutator_alloc_capacity_in_words;\n@@ -2500,4 +2519,5 @@\n-  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n-  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n-  find_regions_with_alloc_capacity(young_trashed_regions, old_trashed_regions,\n-                                   first_old_region, last_old_region, old_region_count);\n+  \/\/ Place regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.  Save the\n+  \/\/ allocatable words in mutator partition in state variable.\n+  _prepare_to_rebuild_mutator_free = find_regions_with_alloc_capacity(young_trashed_regions, old_trashed_regions,\n+                                                                      first_old_region, last_old_region, old_region_count);\n@@ -2506,1 +2526,2 @@\n-void ShenandoahFreeSet::finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t old_region_count,\n+\/\/ Return mutator free\n+size_t ShenandoahFreeSet::finish_rebuild(size_t young_trashed_regions, size_t old_trashed_regions, size_t old_region_count,\n@@ -2522,2 +2543,2 @@\n-  reserve_regions(young_reserve, old_reserve, old_region_count, young_used_regions, old_used_regions,\n-                  young_used_bytes, old_used_bytes);\n+  size_t mutator_free = reserve_regions(young_reserve, old_reserve, old_region_count, young_used_regions, old_used_regions,\n+                                        young_used_bytes, old_used_bytes);\n@@ -2529,0 +2550,1 @@\n+  return mutator_free;\n@@ -2636,3 +2658,5 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count,\n-                                        size_t &young_used_regions, size_t &old_used_regions,\n-                                        size_t &young_used_bytes, size_t &old_used_bytes) {\n+\/\/\n+\/\/ Returns total mutator alloc capacity, in words.\n+size_t ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count,\n+                                          size_t &young_used_regions, size_t &old_used_regions,\n+                                          size_t &young_used_bytes, size_t &old_used_bytes) {\n@@ -2640,0 +2664,1 @@\n+  size_t mutator_allocatable_words = _prepare_to_rebuild_mutator_free;\n@@ -2717,0 +2742,2 @@\n+          assert(ac = ShenandoahHeapRegion::region_size_bytes(), \"Cannot move to old unless entire region is in alloc capacity\");\n+          mutator_allocatable_words -= ShenandoahHeapRegion::region_size_words();\n@@ -2760,2 +2787,4 @@\n-                            _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n-                            _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n+                            _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                            _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+        mutator_allocatable_words -= ac \/ HeapWordSize;\n@@ -2881,0 +2910,1 @@\n+  return mutator_allocatable_words;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":47,"deletions":17,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -443,1 +443,5 @@\n-  size_t _total_humongous_waste;\n+  size_t _total_bytes_previously_allocated;\n+  size_t _mutator_bytes_at_last_sample;\n+\n+  \/\/ Temporarily holds mutator_Free allocatable bytes between prepare_to_rebuild() and finish_rebuild()\n+  size_t _prepare_to_rebuild_mutator_free;\n@@ -447,0 +451,2 @@\n+  size_t _total_humongous_waste;\n+\n@@ -653,0 +659,3 @@\n+  \/\/ Return an approximation of the bytes allocated since GC start.  The value returned is monotonically non-decreasing\n+  \/\/ in time within each GC cycle.  For certain GC cycles, the value returned may include some bytes allocated before\n+  \/\/ the start of the current GC cycle.\n@@ -657,0 +666,34 @@\n+  inline size_t get_total_bytes_allocated() {\n+    return  _mutator_bytes_allocated_since_gc_start + _total_bytes_previously_allocated;\n+  }\n+\n+  inline size_t get_bytes_allocated_since_previous_sample() {\n+    size_t total_bytes = get_total_bytes_allocated();\n+    size_t result;\n+    if (total_bytes < _mutator_bytes_at_last_sample) {\n+      \/\/ This rare condition may occur if bytes allocated overflows (wraps around) size_t tally of allocations.\n+      \/\/ This may also occur in the very rare situation that get_total_bytes_allocated() is queried in the middle of\n+      \/\/ reset_bytes_allocated_since_gc_start().  Note that there is no lock to assure that the two global variables\n+      \/\/ it modifies are modified atomically (_total_bytes_previously_allocated and _mutator_byts_allocated_since_gc_start)\n+      \/\/ This has been observed to occur when an out-of-cycle degenerated cycle is starting (and thus calls\n+      \/\/ reset_bytes_allocated_since_gc_start()) at the same time that the control (non-generational mode) or\n+      \/\/ regulator (generational-mode) thread calls should_start_gc() (which invokes get_bytes_allocated_since_previous_sample()).\n+      \/\/\n+      \/\/ Handle this rare situation by responding with the \"innocent\" value 0 and resetting internal state so that the\n+      \/\/ the next query can recalibrate.\n+      result = 0;\n+    } else {\n+      \/\/ Note: there's always the possibility that the tally of total allocations exceeds the 64-bit capacity of our size_t\n+      \/\/ counter.  We assume that the difference between relevant samples does not exceed this count.  Example:\n+      \/\/   Suppose _mutator_words_at_last_sample is 0xffff_ffff_ffff_fff0 (18,446,744,073,709,551,600 Decimal)\n+      \/\/                        and _total_words is 0x0000_0000_0000_0800 (                    32,768 Decimal)\n+      \/\/ Then, total_words - _mutator_words_at_last_sample can be done adding 1's complement of subtrahend:\n+      \/\/   1's complement of _mutator_words_at_last_sample is: 0x0000_0000_0000_0010 (    16 Decimal))\n+      \/\/                                     plus total_words: 0x0000_0000_0000_0800 (32,768 Decimal)\n+      \/\/                                                  sum: 0x0000_0000_0000_0810 (32,784 Decimal)\n+      result = total_bytes - _mutator_bytes_at_last_sample;\n+    }\n+    _mutator_bytes_at_last_sample = total_bytes;\n+    return result;\n+  }\n+\n@@ -750,2 +793,4 @@\n-  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n-                      bool have_evacuation_reserves = false);\n+  \/\/\n+  \/\/ Returns allocatable memory within Mutator partition, in words.\n+  size_t finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n+                        bool have_evacuation_reserves = false);\n@@ -770,5 +815,1 @@\n-  \/\/ Note that capacity is the number of regions that had available memory at most recent rebuild.  It is not the\n-  \/\/ entire size of the young or global generation.  (Regions within the generation that were fully utilized at time of\n-  \/\/ rebuild are not counted as part of capacity.)\n-\n-  \/\/ All three of the following functions may produce stale data if called without owning the global heap lock.\n+  \/\/ All four of the following functions may produce stale data if called without owning the global heap lock.\n@@ -779,0 +820,4 @@\n+\n+  \/\/ Note that capacity is the number of regions that had available memory at most recent rebuild.  It is not the\n+  \/\/ entire size of the young or global generation.  (Regions within the generation that were fully utilized at time of\n+  \/\/ rebuild are not counted as part of capacity.)\n@@ -780,0 +825,1 @@\n+\n@@ -781,0 +827,1 @@\n+  inline size_t reserved()  const { return _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector);           }\n@@ -782,0 +829,2 @@\n+  inline size_t available_holding_lock() const\n+                                  { return _partitions.available_in(ShenandoahFreeSetPartitionId::Mutator); }\n@@ -840,2 +889,4 @@\n-  void find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n-                                        size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+  \/\/\n+  \/\/ Returns allocatable memory within Mutator partition, in words.\n+  size_t find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                          size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n@@ -846,1 +897,3 @@\n-  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count,\n+  \/\/\n+  \/\/ Returns allocatable memory within Mutator partition, in words.\n+  size_t reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":64,"deletions":11,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -255,0 +255,1 @@\n+  heap->start_idle_span();\n@@ -1126,1 +1127,2 @@\n-    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    size_t mutator_free = heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    heap->set_mutator_free_after_updaterefs(mutator_free);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -155,0 +155,4 @@\n+void ShenandoahGeneration::post_initialize_heuristics() {\n+  _heuristics->post_initialize();\n+}\n+\n@@ -831,0 +835,1 @@\n+    heap->heuristics()->start_evac_span();\n@@ -873,2 +878,1 @@\n-ShenandoahGeneration::ShenandoahGeneration(ShenandoahGenerationType type,\n-                                           uint max_workers) :\n+ShenandoahGeneration::ShenandoahGeneration(ShenandoahGenerationType type, uint max_workers) :\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -110,0 +110,1 @@\n+  virtual void post_initialize_heuristics();\n@@ -113,1 +114,0 @@\n-  virtual size_t bytes_allocated_since_gc_start() const override = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -625,0 +625,2 @@\n+  request.generation->heuristics()->record_degenerated_cycle_start(ShenandoahGC::ShenandoahDegenPoint::_degenerated_outside_cycle\n+                                                                  == _degen_point);\n@@ -628,1 +630,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -93,0 +93,6 @@\n+void ShenandoahGenerationalHeap::initialize_generations() {\n+  ShenandoahHeap::initialize_generations();\n+  _young_generation->post_initialize(this);\n+  _old_generation->post_initialize(this);\n+}\n+\n@@ -98,0 +104,6 @@\n+void ShenandoahGenerationalHeap::post_initialize_heuristics() {\n+  ShenandoahHeap::post_initialize_heuristics();\n+  _young_generation->post_initialize_heuristics();\n+  _old_generation->post_initialize_heuristics();\n+}\n+\n@@ -124,6 +136,0 @@\n-void ShenandoahGenerationalHeap::post_initialize_heuristics() {\n-  ShenandoahHeap::post_initialize_heuristics();\n-  _young_generation->post_initialize(this);\n-  _old_generation->post_initialize(this);\n-}\n-\n@@ -166,0 +172,4 @@\n+void ShenandoahGenerationalHeap::start_idle_span() {\n+  young_generation()->heuristics()->start_idle_span();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+  void initialize_generations() override;\n@@ -85,0 +86,2 @@\n+  void start_idle_span() override;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -429,0 +429,7 @@\n+    initialize_generations();\n+    if (mode()->is_generational()) {\n+      size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      young_generation()->set_evacuation_reserve(young_reserve);\n+      old_generation()->set_evacuation_reserve((size_t) 0);\n+      old_generation()->set_promoted_reserve((size_t) 0);\n+    }\n@@ -431,1 +438,0 @@\n-    post_initialize_heuristics();\n@@ -433,2 +439,2 @@\n-    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n-    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    size_t young_trash_regions, old_trash_regions, first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_trash_regions, old_trash_regions, first_old, last_old, num_old);\n@@ -441,1 +447,1 @@\n-      gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_trash_regions);\n@@ -443,1 +449,1 @@\n-    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+    _free_set->finish_rebuild(young_trash_regions, old_trash_regions, num_old);\n@@ -487,0 +493,1 @@\n+  \/\/ Initialization of controller markes use of varaibles esstablished by initialize_heuristics.\n@@ -489,0 +496,3 @@\n+  \/\/ Certain initialization of heuristics must be deferred until after controller is initialized.\n+  post_initialize_heuristics();\n+  start_idle_span();\n@@ -492,1 +502,0 @@\n-\n@@ -494,1 +503,0 @@\n-\n@@ -496,1 +504,0 @@\n-\n@@ -540,4 +547,0 @@\n-void ShenandoahHeap::post_initialize_heuristics() {\n-  _global_generation->post_initialize(this);\n-}\n-\n@@ -685,0 +688,5 @@\n+void ShenandoahHeap::initialize_generations() {\n+  _global_generation->post_initialize(this);\n+}\n+\n+\/\/ We do not call this explicitly  It is called by Hotspot infrastructure.\n@@ -712,0 +720,4 @@\n+void ShenandoahHeap::post_initialize_heuristics() {\n+  _global_generation->post_initialize_heuristics();\n+}\n+\n@@ -830,0 +842,4 @@\n+void ShenandoahHeap::start_idle_span() {\n+  heuristics()->start_idle_span();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":28,"deletions":12,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -184,0 +184,1 @@\n+  virtual void initialize_generations();\n@@ -382,0 +383,2 @@\n+  virtual void start_idle_span();\n+\n@@ -682,0 +685,1 @@\n+  size_t _mutator_free_after_updaterefs;\n@@ -708,0 +712,3 @@\n+  inline void set_mutator_free_after_updaterefs(size_t val) { _mutator_free_after_updaterefs = val; };\n+  inline size_t get_mutator_free_after_updaterefs() const   { return _mutator_free_after_updaterefs; };\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -416,2 +416,1 @@\n-    size_t young_trash_regions, old_trash_regions;\n-    size_t first_old, last_old, num_old;\n+    size_t young_trash_regions, old_trash_regions, first_old, last_old, num_old;\n@@ -429,1 +428,2 @@\n-    heap->free_set()->finish_rebuild(young_trash_regions, old_trash_regions, num_old);\n+    size_t mutator_free = heap->free_set()->finish_rebuild(young_trash_regions, old_trash_regions, num_old);\n+    ((ShenandoahAdaptiveHeuristics *) (heap->young_generation()->heuristics()))->resume_idle_span();\n@@ -766,0 +766,1 @@\n+\/\/ For the old generation, max_capacity() equals soft_max_capacity()\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -39,1 +39,2 @@\n-  _last_sleep_adjust_time(os::elapsedTime()) {\n+  _most_recent_wake_time(os::elapsedTime()),\n+  _last_sleep_adjust_time(_most_recent_wake_time) {\n@@ -118,2 +119,1 @@\n-  double current = os::elapsedTime();\n-\n+  double before_sleep_time = _most_recent_wake_time;\n@@ -122,1 +122,1 @@\n-  } else if ((current - _last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+  } else if ((before_sleep_time - _last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n@@ -124,1 +124,1 @@\n-    _last_sleep_adjust_time = current;\n+    _last_sleep_adjust_time = before_sleep_time;\n@@ -129,0 +129,4 @@\n+  double wake_time = os::elapsedTime();\n+  _most_recent_period = wake_time - _most_recent_wake_time;\n+  _most_recent_wake_time = wake_time;\n+  _young_heuristics->update_should_start_query_times(_most_recent_wake_time, double(_sleep) \/ 1000.0);\n@@ -130,1 +134,1 @@\n-    double elapsed = os::elapsedTime() - current;\n+    double elapsed = _most_recent_wake_time - before_sleep_time;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -82,0 +82,1 @@\n+  \/\/ duration of planned regulator sleep period, in ms\n@@ -83,0 +84,2 @@\n+  double _most_recent_wake_time;\n+  double _most_recent_period;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,53 @@\n+  product(double, ShenandoahAccelerationSamplePeriod, 0.0145, EXPERIMENTAL, \\\n+          \"When at least this much time (measured in seconds) has passed \"  \\\n+          \"since the allocation rate was most recently sampled, capture \"   \\\n+          \"another allocation rate sample for the purpose of detecting \"    \\\n+          \"acceleration or momentary spikes in allocation rate.  A \"        \\\n+          \"smaller value allows quicker response to changes in allocation \" \\\n+          \"rates but is more vulnerable to noise and requires more \"        \\\n+          \"monitoring effort.\")                                             \\\n+          range(0.001, 1.00)                                                \\\n+                                                                            \\\n+  product(uint, ShenandoahRateAccelerationSampleSize, 16, EXPERIMENTAL,     \\\n+          \"In selected ShenandoahControlIntervals \"                         \\\n+          \"(if ShenandoahAccelerationSamplePeriod seconds have passed \"     \\\n+          \"since previous allocation rate sample), \"                        \\\n+          \"we compute the allocation rate since the previous rate was \"     \\\n+          \"sampled.  This many samples are analyzed to determine whether \"  \\\n+          \"allocation rates are accelerating.  Acceleration may occur \"     \\\n+          \"due to increasing client demand or due to phase changes in \"     \\\n+          \"an application.  A larger value reduces sensitivity to \"         \\\n+          \"noise and delays recognition of the accelerating trend.  A \"     \\\n+          \"larger value may also cause the heuristic to miss detection \"    \\\n+          \"of very quick accelerations.  Smaller values may cause random \"  \\\n+          \"noise to be perceived as acceleration of allocation rate, \"      \\\n+          \"triggering excess collections.  Note that the acceleration \"     \\\n+          \"need not last the entire span of the sampled duration to be \"    \\\n+          \"detected.  If the last several of all samples are signficantly \" \\\n+          \"larger than the other samples, the best fit line through all \"   \\\n+          \"sampled values will have an upward slope, manifesting as \"       \\\n+          \"acceleration.\")                                                  \\\n+          range(1,64)                                                       \\\n+                                                                            \\\n+  product(uint, ShenandoahMomentaryAllocationRateSpikeSampleSize,           \\\n+          2, EXPERIMENTAL,                                                  \\\n+          \"In selected ShenandoahControlIntervals \"                         \\\n+          \"(if ShenandoahAccelerationSamplePeriod seconds have passed \"     \\\n+          \"since previous allocation rate sample), we compute \"             \\\n+          \"the allocation rate since the previous rate was sampled. \"       \\\n+          \"The weighted average of this \"                                   \\\n+          \"many most recent momentary allocation rate samples is compared \" \\\n+          \"against current allocation runway and anticipated GC time to \"   \\\n+          \"determine whether a spike in momentary allocation rate \"         \\\n+          \"justifies an early GC trigger.  Momentary allocation spike \"     \\\n+          \"detection is in addition to previously implemented \"             \\\n+          \"ShenandoahAdaptiveInitialSpikeThreshold, the latter of which \"   \\\n+          \"is more effective at detecting slower spikes.  The latter \"      \\\n+          \"spike detection samples at the rate specifieid by \"              \\\n+          \"ShenandoahAdaptiveSampleFrequencyHz.  The value of this \"        \\\n+          \"parameter must be less than the value of \"                       \\\n+          \"ShenandoahRateAccelerationSampleSize.  A larger value makes \"    \\\n+          \"momentary spike detection less sensitive.  A smaller value \"     \\\n+          \"may result in excessive GC triggers.\")                           \\\n+          range(1,64)                                                       \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"}]}