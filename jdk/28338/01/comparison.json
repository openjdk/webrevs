{"files":[{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomicAccess.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -125,1 +125,1 @@\n-  assert(_refcount == 0, \"precondition\");\n+  assert(_refcount.load_relaxed() == 0, \"precondition\");\n@@ -147,1 +147,1 @@\n-  return _block_count;\n+  return _block_count.load_relaxed();\n@@ -151,1 +151,1 @@\n-  return AtomicAccess::load_acquire(&_block_count);\n+  return _block_count.load_acquire();\n@@ -155,2 +155,2 @@\n-  int new_value = AtomicAccess::add(&_refcount, 1);\n-  assert(new_value >= 1, \"negative refcount %d\", new_value - 1);\n+  int old_value = _refcount.fetch_then_add(1);\n+  assert(old_value >= 0, \"negative refcount %d\", old_value);\n@@ -160,1 +160,1 @@\n-  int new_value = AtomicAccess::sub(&_refcount, 1);\n+  int new_value = _refcount.sub_then_fetch(1);\n@@ -166,1 +166,1 @@\n-  size_t index = _block_count;\n+  size_t index = _block_count.load_relaxed();\n@@ -172,1 +172,1 @@\n-    AtomicAccess::release_store(&_block_count, index + 1);\n+    _block_count.release_store(index + 1);\n@@ -180,1 +180,1 @@\n-  assert(_block_count > 0, \"array is empty\");\n+  assert(_block_count.load_relaxed() > 0, \"array is empty\");\n@@ -183,1 +183,1 @@\n-  size_t last_index = _block_count - 1;\n+  size_t last_index = _block_count.load_relaxed() - 1;\n@@ -187,1 +187,1 @@\n-  _block_count = last_index;\n+  _block_count.store_relaxed(last_index);\n@@ -191,2 +191,2 @@\n-  assert(_block_count == 0, \"array must be empty\");\n-  size_t count = from->_block_count;\n+  assert(_block_count.load_relaxed() == 0, \"array must be empty\");\n+  size_t count = from->_block_count.load_relaxed();\n@@ -201,1 +201,1 @@\n-  _block_count = count;\n+  _block_count.store_relaxed(count);\n@@ -233,0 +233,1 @@\n+#ifdef ASSERT\n@@ -234,2 +235,2 @@\n-  assert(_release_refcount == 0, \"deleting block while releasing\");\n-  assert(_deferred_updates_next == nullptr, \"deleting block with deferred update\");\n+  assert(_release_refcount.load_relaxed() == 0, \"deleting block while releasing\");\n+  assert(_deferred_updates_next.load_relaxed() == nullptr, \"deleting block with deferred update\");\n@@ -237,2 +238,3 @@\n-  \/\/ might help catch bugs.  Volatile to prevent dead-store elimination.\n-  const_cast<uintx volatile&>(_allocated_bitmask) = 0;\n+  \/\/ might help catch bugs.\n+  _allocated_bitmask.store_relaxed(0);\n+  \/\/ Volatile to prevent dead-store elimination.\n@@ -241,0 +243,1 @@\n+#endif \/\/ ASSERT\n@@ -275,2 +278,2 @@\n-  return (AtomicAccess::load_acquire(&_release_refcount) == 0) &&\n-         (AtomicAccess::load_acquire(&_deferred_updates_next) == nullptr);\n+  return ((_release_refcount.load_acquire() == 0) &&\n+          (_deferred_updates_next.load_acquire() == nullptr));\n@@ -280,1 +283,1 @@\n-  return _deferred_updates_next;\n+  return _deferred_updates_next.load_relaxed();\n@@ -284,1 +287,1 @@\n-  _deferred_updates_next = block;\n+  _deferred_updates_next.store_relaxed(block);\n@@ -324,3 +327,2 @@\n-  uintx sum = AtomicAccess::add(&_allocated_bitmask, add);\n-  assert((sum & add) == add, \"some already present: %zu:%zu\",\n-         sum, add);\n+  uintx sum = _allocated_bitmask.add_then_fetch(add);\n+  assert((sum & add) == add, \"some already present: %zu:%zu\", sum, add);\n@@ -455,1 +457,1 @@\n-  AtomicAccess::inc(&_allocation_count); \/\/ release updates outside lock.\n+  _allocation_count.add_then_fetch(1u); \/\/ release updates outside lock.\n@@ -493,1 +495,1 @@\n-  AtomicAccess::add(&_allocation_count, num_taken);\n+  _allocation_count.add_then_fetch(num_taken);\n@@ -509,1 +511,1 @@\n-    AtomicAccess::sub(&_allocation_count, num_taken - limit);\n+    _allocation_count.sub_then_fetch(num_taken - limit);\n@@ -530,1 +532,1 @@\n-  if (!_active_array->push(block)) {\n+  if (!_active_array.load_relaxed()->push(block)) {\n@@ -532,1 +534,1 @@\n-      guarantee(_active_array->push(block), \"push failed after expansion\");\n+      guarantee(_active_array.load_relaxed()->push(block), \"push failed after expansion\");\n@@ -579,1 +581,1 @@\n-  ActiveArray* old_array = _active_array;\n+  ActiveArray* old_array = _active_array.load_relaxed();\n@@ -602,1 +604,1 @@\n-  AtomicAccess::release_store(&_active_array, new_array);\n+  _active_array.release_store(new_array);\n@@ -620,1 +622,1 @@\n-  ActiveArray* result = AtomicAccess::load_acquire(&_active_array);\n+  ActiveArray* result = _active_array.load_acquire();\n@@ -628,1 +630,1 @@\n-    assert(array != _active_array, \"invariant\");\n+    assert(array != _active_array.load_relaxed(), \"invariant\");\n@@ -675,1 +677,1 @@\n-  AtomicAccess::inc(&_release_refcount);\n+  _release_refcount.add_then_fetch(1u);\n@@ -678,1 +680,1 @@\n-  uintx old_allocated = _allocated_bitmask;\n+  uintx old_allocated = _allocated_bitmask.load_relaxed();\n@@ -682,1 +684,1 @@\n-    uintx fetched = AtomicAccess::cmpxchg(&_allocated_bitmask, old_allocated, new_value);\n+    uintx fetched = _allocated_bitmask.compare_exchange(old_allocated, new_value);\n@@ -701,1 +703,1 @@\n-    if (AtomicAccess::replace_if_null(&_deferred_updates_next, this)) {\n+    if (_deferred_updates_next.compare_exchange(nullptr, this) == nullptr) {\n@@ -703,1 +705,1 @@\n-      Block* head = owner->_deferred_updates;\n+      Block* head = owner->_deferred_updates.load_relaxed();\n@@ -705,2 +707,2 @@\n-        _deferred_updates_next = (head == nullptr) ? this : head;\n-        Block* fetched = AtomicAccess::cmpxchg(&owner->_deferred_updates, head, this);\n+        _deferred_updates_next.store_relaxed((head == nullptr) ? this : head);\n+        Block* fetched = owner->_deferred_updates.compare_exchange(head, this);\n@@ -723,1 +725,1 @@\n-  AtomicAccess::dec(&_release_refcount);\n+  _release_refcount.sub_then_fetch(1u);\n@@ -732,1 +734,1 @@\n-  Block* block = AtomicAccess::load_acquire(&_deferred_updates);\n+  Block* block = _deferred_updates.load_acquire();\n@@ -738,1 +740,1 @@\n-    Block* fetched = AtomicAccess::cmpxchg(&_deferred_updates, block, tail);\n+    Block* fetched = _deferred_updates.compare_exchange(block, tail);\n@@ -783,1 +785,1 @@\n-  AtomicAccess::dec(&_allocation_count);\n+  _allocation_count.sub_then_fetch(1u);\n@@ -809,1 +811,1 @@\n-    AtomicAccess::sub(&_allocation_count, count);\n+    _allocation_count.sub_then_fetch(count);\n@@ -840,1 +842,1 @@\n-  _active_array->increment_refcount();\n+  _active_array.load_relaxed()->increment_refcount();\n@@ -855,2 +857,2 @@\n-  while ((block = _deferred_updates) != nullptr) {\n-    _deferred_updates = block->deferred_updates_next();\n+  while ((block = _deferred_updates.load_relaxed()) != nullptr) {\n+    _deferred_updates.store_relaxed(block->deferred_updates_next());\n@@ -862,1 +864,2 @@\n-  bool unreferenced = _active_array->decrement_refcount();\n+  ActiveArray* array = _active_array.load_relaxed();\n+  bool unreferenced = array->decrement_refcount();\n@@ -864,2 +867,2 @@\n-  for (size_t i = _active_array->block_count(); 0 < i; ) {\n-    block = _active_array->at(--i);\n+  for (size_t i = array->block_count(); 0 < i; ) {\n+    block = array->at(--i);\n@@ -868,1 +871,1 @@\n-  ActiveArray::destroy(_active_array);\n+  ActiveArray::destroy(array);\n@@ -897,1 +900,1 @@\n-static volatile bool needs_cleanup_requested = false;\n+static Atomic<bool> needs_cleanup_requested{false};\n@@ -909,4 +912,3 @@\n-  if (AtomicAccess::load_acquire(&needs_cleanup_requested) &&\n-      os::javaTimeNanos() > cleanup_permit_time) {\n-    cleanup_permit_time =\n-      os::javaTimeNanos() + cleanup_defer_period;\n+  if (needs_cleanup_requested.load_acquire() &&\n+      (os::javaTimeNanos() > cleanup_permit_time)) {\n+    cleanup_permit_time = os::javaTimeNanos() + cleanup_defer_period;\n@@ -914,1 +916,1 @@\n-    AtomicAccess::release_store(&needs_cleanup_requested, false);\n+    needs_cleanup_requested.release_store(false);\n@@ -926,2 +928,2 @@\n-  AtomicAccess::release_store(&_needs_cleanup, true);\n-  AtomicAccess::release_store_fence(&needs_cleanup_requested, true);\n+  _needs_cleanup.release_store(true);\n+  needs_cleanup_requested.release_store_fence(true);\n@@ -933,2 +935,2 @@\n-  if (!AtomicAccess::load_acquire(&_needs_cleanup) &&\n-      (AtomicAccess::load_acquire(&_deferred_updates) == nullptr)) {\n+  if (!_needs_cleanup.load_acquire() &&\n+      (_deferred_updates.load_acquire() == nullptr)) {\n@@ -941,1 +943,1 @@\n-  AtomicAccess::release_store_fence(&_needs_cleanup, false);\n+  _needs_cleanup.release_store_fence(false);\n@@ -980,1 +982,1 @@\n-        _active_array->remove(block);\n+        _active_array.load_relaxed()->remove(block);\n@@ -1004,2 +1006,3 @@\n-    if ((index < _active_array->block_count()) &&\n-        (block == _active_array->at(index)) &&\n+    ActiveArray* array = _active_array.load_relaxed();\n+    if ((index < array->block_count()) &&\n+        (block == array->at(index)) &&\n@@ -1018,1 +1021,1 @@\n-  return _allocation_count;\n+  return _allocation_count.load_relaxed();\n@@ -1087,1 +1090,1 @@\n-  size_t start = AtomicAccess::load_acquire(&_next_block);\n+  size_t start = _next_block.load_acquire();\n@@ -1100,1 +1103,1 @@\n-  \/\/ AtomicAccess::add with possible overshoot.  This can perform better\n+  \/\/ Atomic add with possible overshoot.  This can perform better\n@@ -1104,1 +1107,1 @@\n-  size_t end = AtomicAccess::add(&_next_block, step);\n+  size_t end = _next_block.add_then_fetch(step);\n@@ -1131,1 +1134,1 @@\n-  return AtomicAccess::load(&_num_dead);\n+  return _num_dead.load_relaxed();\n@@ -1135,1 +1138,1 @@\n-  AtomicAccess::add(&_num_dead, num_dead);\n+  _num_dead.add_then_fetch(num_dead);\n@@ -1139,1 +1142,1 @@\n-  _storage->report_num_dead(AtomicAccess::load(&_num_dead));\n+  _storage->report_num_dead(_num_dead.load_relaxed());\n@@ -1167,2 +1170,2 @@\n-  size_t allocations = _allocation_count;\n-  size_t blocks = _active_array->block_count();\n+  size_t allocations = _allocation_count.load_relaxed();\n+  size_t blocks = _active_array.load_relaxed()->block_count();\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.cpp","additions":80,"deletions":77,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -261,1 +262,1 @@\n-  ActiveArray* _active_array;\n+  Atomic<ActiveArray*> _active_array;\n@@ -263,1 +264,1 @@\n-  Block* volatile _deferred_updates;\n+  Atomic<Block*> _deferred_updates;\n@@ -268,2 +269,2 @@\n-  \/\/ Volatile for racy unlocked accesses.\n-  volatile size_t _allocation_count;\n+  \/\/ Atomic for racy unlocked accesses.\n+  Atomic<size_t> _allocation_count;\n@@ -281,1 +282,1 @@\n-  volatile bool _needs_cleanup;\n+  Atomic<bool> _needs_cleanup;\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -45,2 +46,2 @@\n-  volatile size_t _block_count;\n-  mutable volatile int _refcount;\n+  Atomic<size_t> _block_count;\n+  mutable Atomic<int> _refcount;\n@@ -107,1 +108,1 @@\n-  assert(index < _block_count, \"precondition\");\n+  assert(index < _block_count.load_relaxed(), \"precondition\");\n@@ -138,1 +139,1 @@\n-  volatile uintx _allocated_bitmask; \/\/ One bit per _data element.\n+  Atomic<uintx> _allocated_bitmask; \/\/ One bit per _data element.\n@@ -143,2 +144,2 @@\n-  Block* volatile _deferred_updates_next;\n-  volatile uintx _release_refcount;\n+  Atomic<Block*> _deferred_updates_next;\n+  Atomic<uintx> _release_refcount;\n@@ -147,1 +148,1 @@\n-  ~Block();\n+  ~Block() NOT_DEBUG(= default);\n@@ -325,1 +326,1 @@\n-  return _allocated_bitmask;\n+  return _allocated_bitmask.load_relaxed();\n@@ -369,1 +370,1 @@\n-  ActiveArray* blocks = storage->_active_array;\n+  ActiveArray* blocks = storage->_active_array.load_relaxed();\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -134,1 +135,1 @@\n-  volatile size_t _next_block;\n+  Atomic<size_t> _next_block;\n@@ -137,1 +138,1 @@\n-  volatile size_t _num_dead;\n+  Atomic<size_t> _num_dead;\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorageParState.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-    return *storage._active_array;\n+    return *storage._active_array.load_relaxed();\n@@ -93,1 +93,1 @@\n-    blocks->_block_count = count;\n+    blocks->_block_count.store_relaxed(count);\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_oopStorage.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}