{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1614,5 +1614,9 @@\n-  void cc20_quarter_round(FloatRegister aVec, FloatRegister bVec,\n-          FloatRegister cVec, FloatRegister dVec, FloatRegister scratch,\n-          FloatRegister tbl);\n-  void cc20_shift_lane_org(FloatRegister bVec, FloatRegister cVec,\n-          FloatRegister dVec, bool colToDiag);\n+  void cc20_qr_add4(FloatRegister (&addFirst)[4],\n+          FloatRegister (&addSecond)[4]);\n+  void cc20_qr_xor4(FloatRegister (&firstElem)[4],\n+          FloatRegister (&secondElem)[4], FloatRegister (&result)[4]);\n+  void cc20_qr_lrot4(FloatRegister (&sourceReg)[4],\n+          FloatRegister (&destReg)[4], int bits, FloatRegister table);\n+  void cc20_set_qr_registers(FloatRegister (&vectorSet)[4],\n+          const FloatRegister (&stateVectors)[16], int idx1, int idx2,\n+          int idx3, int idx4);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -31,2 +31,6 @@\n- * Perform the quarter round calculations on values contained within\n- * four SIMD registers.\n+ * Perform the vectorized add for a group of 4 quarter round operations.\n+ * In the ChaCha20 quarter round, there are two add ops: a += b and c += d.\n+ * Each parameter is a set of 4 registers representing the 4 registers\n+ * for the each addend in the add operation for each of the quarter rounds.\n+ * (e.g. for \"a\" it would consist of v0\/v1\/v2\/v3).  The result of the add\n+ * is placed into the vectors in the \"addFirst\" array.\n@@ -34,6 +38,2 @@\n- * @param aVec the SIMD register containing only the \"a\" values\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param scratch scratch SIMD register used for 12 and 7 bit left rotations\n- * @param table the SIMD register used as a table for 8 bit left rotations\n+ * @param addFirst array of SIMD registers representing the first addend.\n+ * @param addSecond array of SIMD registers representing the second addend.\n@@ -41,3 +41,52 @@\n-void MacroAssembler::cc20_quarter_round(FloatRegister aVec, FloatRegister bVec,\n-    FloatRegister cVec, FloatRegister dVec, FloatRegister scratch,\n-     FloatRegister table) {\n+void MacroAssembler::cc20_qr_add4(FloatRegister (&addFirst)[4],\n+    FloatRegister (&addSecond)[4]) {\n+  for (int i = 0; i < 4; i++) {\n+      addv(addFirst[i], T4S, addFirst[i], addSecond[i]);\n+  }\n+}\n+\n+\n+\/**\n+ * Perform the vectorized XOR for a group of 4 quarter round operations.\n+ * In the ChaCha20 quarter round, there are two XOR ops: d ^= a and b ^= c\n+ * Each parameter is a set of 4 registers representing the 4 registers\n+ * for the each element in the xor operation for each of the quarter rounds.\n+ * (e.g. for \"a\" it would consist of v0\/v1\/v2\/v3)\n+ * Note: because the b ^= c ops precede a non-byte-aligned left-rotation,\n+ *       there is a third parameter which can take a set of scratch registers\n+ *       for the result, which facilitates doing the subsequent operations for\n+ *       the left rotation.\n+ *\n+ * @param firstElem array of SIMD registers representing the first element.\n+ * @param secondElem array of SIMD registers representing the second element.\n+ * @param result array of SIMD registers representing the destination.\n+ *        May be the same as firstElem or secondElem, or a separate array.\n+ *\/\n+void MacroAssembler::cc20_qr_xor4(FloatRegister (&firstElem)[4],\n+    FloatRegister (&secondElem)[4], FloatRegister (&result)[4]) {\n+  for (int i = 0; i < 4; i++) {\n+    eor(result[i], T16B, firstElem[i], secondElem[i]);\n+  }\n+}\n+\n+\/**\n+ * Perform the vectorized left-rotation on 32-bit lanes for a group of\n+ * 4 quarter round operations.\n+ * Each parameter is a set of 4 registers representing the 4 registers\n+ * for the each element in the source and destination for each of the quarter\n+ * rounds (e.g. for \"d\" it would consist of v12\/v13\/v14\/v15 on columns and\n+ * v15\/v12\/v13\/v14 on diagonal alignments).\n+ *\n+ * @param sourceReg array of SIMD registers representing the source\n+ * @param destReg array of SIMD registers representing the destination\n+ * @param bits the distance of the rotation in bits, must be 16\/12\/8\/7 per\n+ *        the ChaCha20 specification.\n+ *\/\n+void MacroAssembler::cc20_qr_lrot4(FloatRegister (&sourceReg)[4],\n+    FloatRegister (&destReg)[4], int bits, FloatRegister table) {\n+  switch (bits) {\n+  case 16:      \/\/ reg <<<= 16, in-place swap of half-words\n+    for (int i = 0; i < 4; i++) {\n+      rev32(destReg[i], T8H, sourceReg[i]);\n+    }\n+    break;\n@@ -45,4 +94,5 @@\n-  \/\/ a += b, d ^= a, d <<<= 16\n-  addv(aVec, T4S, aVec, bVec);\n-  eor(dVec, T16B, dVec, aVec);\n-  rev32(dVec, T8H, dVec);\n+  case 7:       \/\/ reg <<<= (12 || 7)\n+  case 12:      \/\/ r-shift src -> dest, l-shift src & ins to dest\n+    for (int i = 0; i < 4; i++) {\n+      ushr(destReg[i], T4S, sourceReg[i], 32 - bits);\n+    }\n@@ -50,5 +100,4 @@\n-  \/\/ c += d, b ^= c, b <<<= 12\n-  addv(cVec, T4S, cVec, dVec);\n-  eor(scratch, T16B, bVec, cVec);\n-  ushr(bVec, T4S, scratch, 20);\n-  sli(bVec, T4S, scratch, 12);\n+    for (int i = 0; i < 4; i++) {\n+      sli(destReg[i], T4S, sourceReg[i], bits);\n+    }\n+    break;\n@@ -56,4 +105,5 @@\n-  \/\/ a += b, d ^= a, d <<<= 8\n-  addv(aVec, T4S, aVec, bVec);\n-  eor(dVec, T16B, dVec, aVec);\n-  tbl(dVec, T16B, dVec,  1, table);\n+  case 8:       \/\/ reg <<<= 8, simulate left rotation with table reorg\n+    for (int i = 0; i < 4; i++) {\n+      tbl(destReg[i], T16B, sourceReg[i], 1, table);\n+    }\n+    break;\n@@ -61,5 +111,5 @@\n-  \/\/ c += d, b ^= c, b <<<= 7\n-  addv(cVec, T4S, cVec, dVec);\n-  eor(scratch, T16B, bVec, cVec);\n-  ushr(bVec, T4S, scratch, 25);\n-  sli(bVec, T4S, scratch, 7);\n+  default:\n+    \/\/ The caller shouldn't be sending bit rotation values outside\n+    \/\/ of the 16\/12\/8\/7 as defined in the specification.\n+    ShouldNotReachHere();\n+  }\n@@ -69,2 +119,5 @@\n- * Shift the b, c, and d vectors between columnar and diagonal representations.\n- * Note that the \"a\" vector does not shift.\n+ * Set the FloatRegisters for a 4-vector register set.  These will be used\n+ * during various quarter round transformations (adds, xors and left-rotations).\n+ * This method itself does not result in the output of any assembly\n+ * instructions.  It just organizes the vectors so they can be in columnar or\n+ * diagonal alignments.\n@@ -72,5 +125,13 @@\n- * @param bVec the SIMD register containing only the \"b\" values\n- * @param cVec the SIMD register containing only the \"c\" values\n- * @param dVec the SIMD register containing only the \"d\" values\n- * @param colToDiag true if moving columnar to diagonal, false if\n- *                  moving diagonal back to columnar.\n+ * @param vectorSet a 4-vector array to be altered into a new alignment\n+ * @param stateVectors the 16-vector array that represents the current\n+ *        working state.  The indices of this array match up with the\n+ *        organization of the ChaCha20 state per RFC 7539 (e.g. stateVectors[12]\n+ *        would contain the vector that holds the 32-bit counter, etc.)\n+ * @param idx1 the index of the stateVectors array to be assigned to the\n+ *        first vectorSet element.\n+ * @param idx2 the index of the stateVectors array to be assigned to the\n+ *        second vectorSet element.\n+ * @param idx3 the index of the stateVectors array to be assigned to the\n+ *        third vectorSet element.\n+ * @param idx4 the index of the stateVectors array to be assigned to the\n+ *        fourth vectorSet element.\n@@ -78,9 +139,7 @@\n-void MacroAssembler::cc20_shift_lane_org(FloatRegister bVec, FloatRegister cVec,\n-    FloatRegister dVec, bool colToDiag) {\n-  int bShift = colToDiag ? 4 : 12;\n-  int cShift = 8;\n-  int dShift = colToDiag ? 12 : 4;\n-\n-  ext(bVec, T16B, bVec, bVec, bShift);\n-  ext(cVec, T16B, cVec, cVec, cShift);\n-  ext(dVec, T16B, dVec, dVec, dShift);\n+void MacroAssembler::cc20_set_qr_registers(FloatRegister (&vectorSet)[4],\n+    const FloatRegister (&stateVectors)[16], int idx1, int idx2,\n+    int idx3, int idx4) {\n+  vectorSet[0] = stateVectors[idx1];\n+  vectorSet[1] = stateVectors[idx2];\n+  vectorSet[2] = stateVectors[idx3];\n+  vectorSet[3] = stateVectors[idx4];\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64_chacha.cpp","additions":104,"deletions":45,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -4408,0 +4408,191 @@\n+  \/\/ ChaCha20 block function.  This version parallelizes the 32-bit\n+  \/\/ state elements on each of 16 vectors, producing 4 blocks of\n+  \/\/ keystream at a time.\n+  \/\/\n+  \/\/ state (int[16]) = c_rarg0\n+  \/\/ keystream (byte[256]) = c_rarg1\n+  \/\/ return - number of bytes of produced keystream (always 256)\n+  \/\/\n+  \/\/ This implementation takes each 32-bit integer from the state\n+  \/\/ array and broadcasts it across all 4 32-bit lanes of a vector register\n+  \/\/ (e.g. state[0] is replicated on all 4 lanes of v4, state[1] to all 4 lanes\n+  \/\/ of v5, etc.).  Once all 16 elements have been broadcast onto 16 vectors,\n+  \/\/ the quarter round schedule is implemented as outlined in RFC 7539 section\n+  \/\/ 2.3.  However, instead of sequentially processing the 3 quarter round\n+  \/\/ operations represented by one QUARTERROUND function, we instead stack all\n+  \/\/ the adds, xors and left-rotations from the first 4 quarter rounds together\n+  \/\/ and then do the same for the second set of 4 quarter rounds.  This removes\n+  \/\/ some latency that would otherwise be incurred by waiting for an add to\n+  \/\/ complete before performing an xor (which depends on the result of the\n+  \/\/ add), etc. An adjustment happens between the first and second groups of 4\n+  \/\/ quarter rounds, but this is done only in the inputs to the macro functions\n+  \/\/ that generate the assembly instructions - these adjustments themselves are\n+  \/\/ not part of the resulting assembly.\n+  \/\/ The 4 registers v0-v3 are used during the quarter round operations as\n+  \/\/ scratch registers.  Once the 20 rounds are complete, these 4 scratch\n+  \/\/ registers become the vectors involved in adding the start state back onto\n+  \/\/ the post-QR working state.  After the adds are complete, each of the 16\n+  \/\/ vectors write their first lane back to the keystream buffer, followed\n+  \/\/ by the second lane from all vectors and so on.\n+  address generate_chacha20Block_blockpar() {\n+    Label L_twoRounds, L_cc20_const;\n+    \/\/ The constant data is broken into two 128-bit segments to be loaded\n+    \/\/ onto FloatRegisters.  The first 128 bits are a counter add overlay\n+    \/\/ that adds +0\/+1\/+2\/+3 to the vector holding replicated state[12].\n+    \/\/ The second 128-bits is a table constant used for 8-bit left rotations.\n+    __ BIND(L_cc20_const);\n+    __ emit_int64(0x0000000100000000UL);\n+    __ emit_int64(0x0000000300000002UL);\n+    __ emit_int64(0x0605040702010003UL);\n+    __ emit_int64(0x0E0D0C0F0A09080BUL);\n+\n+    __ align(CodeEntryAlignment);\n+    StubGenStubId stub_id = StubGenStubId::chacha20Block_id;\n+    StubCodeMark mark(this, stub_id);\n+    address start = __ pc();\n+    __ enter();\n+\n+    int i, j;\n+    const Register state = c_rarg0;\n+    const Register keystream = c_rarg1;\n+    const Register loopCtr = r10;\n+    const Register tmpAddr = r11;\n+    const FloatRegister ctrAddOverlay = v28;\n+    const FloatRegister lrot8Tbl = v29;\n+\n+    \/\/ Organize SIMD registers in an array that facilitates\n+    \/\/ putting repetitive opcodes into loop structures.  It is\n+    \/\/ important that each grouping of 4 registers is monotonically\n+    \/\/ increasing to support the requirements of multi-register\n+    \/\/ instructions (e.g. ld4r, st4, etc.)\n+    const FloatRegister workSt[16] = {\n+         v4,  v5,  v6,  v7, v16, v17, v18, v19,\n+        v20, v21, v22, v23, v24, v25, v26, v27\n+    };\n+\n+    \/\/ Pull in constant data.  The first 16 bytes are the add overlay\n+    \/\/ which is applied to the vector holding the counter (state[12]).\n+    \/\/ The second 16 bytes is the index register for the 8-bit left\n+    \/\/ rotation tbl instruction.\n+    __ adr(tmpAddr, L_cc20_const);\n+    __ ldpq(ctrAddOverlay, lrot8Tbl, Address(tmpAddr));\n+\n+    \/\/ Load from memory and interlace across 16 SIMD registers,\n+    \/\/ With each word from memory being broadcast to all lanes of\n+    \/\/ each successive SIMD register.\n+    \/\/      Addr(0) -> All lanes in workSt[i]\n+    \/\/      Addr(4) -> All lanes workSt[i + 1], etc.\n+    __ mov(tmpAddr, state);\n+    for (i = 0; i < 16; i += 4) {\n+      __ ld4r(workSt[i], workSt[i + 1], workSt[i + 2], workSt[i + 3], __ T4S,\n+          __ post(tmpAddr, 16));\n+    }\n+    __ addv(workSt[12], __ T4S, workSt[12], ctrAddOverlay); \/\/ Add ctr overlay\n+\n+    \/\/ Before entering the loop, create 5 4-register arrays.  These\n+    \/\/ will hold the 4 registers that represent the a\/b\/c\/d fields\n+    \/\/ in the quarter round operation.  For instance the \"b\" field\n+    \/\/ for the first 4 quarter round operations is the set of v16\/v17\/v18\/v19,\n+    \/\/ but in the second 4 quarter rounds it gets adjusted to v17\/v18\/v19\/v16\n+    \/\/ since it is part of a diagonal organization.  The aSet and scratch\n+    \/\/ register sets are defined at declaration time because they do not change\n+    \/\/ organization at any point during the 20-round processing.\n+    FloatRegister aSet[4] = { v4, v5, v6, v7 };\n+    FloatRegister bSet[4];\n+    FloatRegister cSet[4];\n+    FloatRegister dSet[4];\n+    FloatRegister scratch[4] = { v0, v1, v2, v3 };\n+\n+    \/\/ Set up the 10 iteration loop and perform all 8 quarter round ops\n+    __ mov(loopCtr, 10);\n+    __ BIND(L_twoRounds);\n+\n+    \/\/ Set to columnar organization and do the following 4 quarter-rounds:\n+    \/\/ QUARTERROUND(0, 4, 8, 12)\n+    \/\/ QUARTERROUND(1, 5, 9, 13)\n+    \/\/ QUARTERROUND(2, 6, 10, 14)\n+    \/\/ QUARTERROUND(3, 7, 11, 15)\n+    __ cc20_set_qr_registers(bSet, workSt, 4, 5, 6, 7);\n+    __ cc20_set_qr_registers(cSet, workSt, 8, 9, 10, 11);\n+    __ cc20_set_qr_registers(dSet, workSt, 12, 13, 14, 15);\n+\n+    __ cc20_qr_add4(aSet, bSet);                    \/\/ a += b\n+    __ cc20_qr_xor4(dSet, aSet, dSet);              \/\/ d ^= a\n+    __ cc20_qr_lrot4(dSet, dSet, 16, lrot8Tbl);     \/\/ d <<<= 16\n+\n+    __ cc20_qr_add4(cSet, dSet);                    \/\/ c += d\n+    __ cc20_qr_xor4(bSet, cSet, scratch);           \/\/ b ^= c (scratch)\n+    __ cc20_qr_lrot4(scratch, bSet, 12, lrot8Tbl);  \/\/ b <<<= 12\n+\n+    __ cc20_qr_add4(aSet, bSet);                    \/\/ a += b\n+    __ cc20_qr_xor4(dSet, aSet, dSet);              \/\/ d ^= a\n+    __ cc20_qr_lrot4(dSet, dSet, 8, lrot8Tbl);      \/\/ d <<<= 8\n+\n+    __ cc20_qr_add4(cSet, dSet);                    \/\/ c += d\n+    __ cc20_qr_xor4(bSet, cSet, scratch);           \/\/ b ^= c (scratch)\n+    __ cc20_qr_lrot4(scratch, bSet, 7, lrot8Tbl);   \/\/ b <<<= 12\n+\n+    \/\/ Set to diagonal organization and do the next 4 quarter-rounds:\n+    \/\/ QUARTERROUND(0, 5, 10, 15)\n+    \/\/ QUARTERROUND(1, 6, 11, 12)\n+    \/\/ QUARTERROUND(2, 7, 8, 13)\n+    \/\/ QUARTERROUND(3, 4, 9, 14)\n+    __ cc20_set_qr_registers(bSet, workSt, 5, 6, 7, 4);\n+    __ cc20_set_qr_registers(cSet, workSt, 10, 11, 8, 9);\n+    __ cc20_set_qr_registers(dSet, workSt, 15, 12, 13, 14);\n+\n+    __ cc20_qr_add4(aSet, bSet);                    \/\/ a += b\n+    __ cc20_qr_xor4(dSet, aSet, dSet);              \/\/ d ^= a\n+    __ cc20_qr_lrot4(dSet, dSet, 16, lrot8Tbl);     \/\/ d <<<= 16\n+\n+    __ cc20_qr_add4(cSet, dSet);                    \/\/ c += d\n+    __ cc20_qr_xor4(bSet, cSet, scratch);           \/\/ b ^= c (scratch)\n+    __ cc20_qr_lrot4(scratch, bSet, 12, lrot8Tbl);  \/\/ b <<<= 12\n+\n+    __ cc20_qr_add4(aSet, bSet);                    \/\/ a += b\n+    __ cc20_qr_xor4(dSet, aSet, dSet);              \/\/ d ^= a\n+    __ cc20_qr_lrot4(dSet, dSet, 8, lrot8Tbl);      \/\/ d <<<= 8\n+\n+    __ cc20_qr_add4(cSet, dSet);                    \/\/ c += d\n+    __ cc20_qr_xor4(bSet, cSet, scratch);           \/\/ b ^= c (scratch)\n+    __ cc20_qr_lrot4(scratch, bSet, 7, lrot8Tbl);   \/\/ b <<<= 12\n+\n+    \/\/ Decrement and iterate\n+    __ sub(loopCtr, loopCtr, 1);\n+    __ cbnz(loopCtr, L_twoRounds);\n+\n+    __ mov(tmpAddr, state);\n+\n+    \/\/ Add the starting state back to the post-loop keystream\n+    \/\/ state.  We read\/interlace the state array from memory into\n+    \/\/ 4 registers similar to what we did in the beginning.  Then\n+    \/\/ add the counter overlay onto workSt[12] at the end.\n+    for (i = 0; i < 16; i += 4) {\n+      __ ld4r(v0, v1, v2, v3, __ T4S, __ post(tmpAddr, 16));\n+      __ addv(workSt[i], __ T4S, workSt[i], v0);\n+      __ addv(workSt[i + 1], __ T4S, workSt[i + 1], v1);\n+      __ addv(workSt[i + 2], __ T4S, workSt[i + 2], v2);\n+      __ addv(workSt[i + 3], __ T4S, workSt[i + 3], v3);\n+    }\n+    __ addv(workSt[12], __ T4S, workSt[12], ctrAddOverlay); \/\/ Add ctr overlay\n+\n+    \/\/ Write working state into the keystream buffer.  This is accomplished\n+    \/\/ by taking the lane \"i\" from each of the four vectors and writing\n+    \/\/ it to consecutive 4-byte offsets, then post-incrementing by 16 and\n+    \/\/ repeating with the next 4 vectors until all 16 vectors have been used.\n+    \/\/ Then move to the next lane and repeat the process until all lanes have\n+    \/\/ been written.\n+    for (i = 0; i < 4; i++) {\n+      for (j = 0; j < 16; j += 4) {\n+        __ st4(workSt[j], workSt[j + 1], workSt[j + 2], workSt[j + 3], __ S, i,\n+            __ post(keystream, 16));\n+      }\n+    }\n+\n+    __ mov(r0, 256);             \/\/ Return length of output keystream\n+    __ leave();\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -4449,197 +4640,0 @@\n-  \/\/ ChaCha20 block function.  This version parallelizes 4 quarter\n-  \/\/ round operations at a time.  It uses 16 SIMD registers to\n-  \/\/ produce 4 blocks of key stream.\n-  \/\/\n-  \/\/ state (int[16]) = c_rarg0\n-  \/\/ keystream (byte[256]) = c_rarg1\n-  \/\/ return - number of bytes of keystream (always 256)\n-  \/\/\n-  \/\/ In this approach, we load the 512-bit start state sequentially into\n-  \/\/ 4 128-bit vectors.  We then make 4 4-vector copies of that starting\n-  \/\/ state, with each successive set of 4 vectors having a +1 added into\n-  \/\/ the first 32-bit lane of the 4th vector in that group (the counter).\n-  \/\/ By doing this, we can perform the block function on 4 512-bit blocks\n-  \/\/ within one run of this intrinsic.\n-  \/\/ The alignment of the data across the 4-vector group is such that at\n-  \/\/ the start it is already aligned for the first round of each two-round\n-  \/\/ loop iteration.  In other words, the corresponding lanes of each vector\n-  \/\/ will contain the values needed for that quarter round operation (e.g.\n-  \/\/ elements 0\/4\/8\/12, 1\/5\/9\/13, 2\/6\/10\/14, etc.).\n-  \/\/ In between each full round, a lane shift must occur.  Within a loop\n-  \/\/ iteration, between the first and second rounds, the 2nd, 3rd, and 4th\n-  \/\/ vectors are rotated left 32, 64 and 96 bits, respectively.  The result\n-  \/\/ is effectively a diagonal orientation in columnar form.  After the\n-  \/\/ second full round, those registers are left-rotated again, this time\n-  \/\/ 96, 64, and 32 bits - returning the vectors to their columnar organization.\n-  \/\/ After all 10 iterations, the original state is added to each 4-vector\n-  \/\/ working state along with the add mask, and the 4 vector groups are\n-  \/\/ sequentially written to the memory dedicated for the output key stream.\n-  \/\/\n-  \/\/ For a more detailed explanation, see Goll and Gueron, \"Vectorization of\n-  \/\/ ChaCha Stream Cipher\", 2014 11th Int. Conf. on Information Technology:\n-  \/\/ New Generations, Las Vegas, NV, USA, April 2014, DOI: 10.1109\/ITNG.2014.33\n-  address generate_chacha20Block_qrpar() {\n-    Label L_Q_twoRounds, L_Q_cc20_const;\n-    \/\/ The constant data is broken into two 128-bit segments to be loaded\n-    \/\/ onto SIMD registers.  The first 128 bits are a counter add overlay\n-    \/\/ that adds +1\/+0\/+0\/+0 to the vectors holding replicated state[12].\n-    \/\/ The second 128-bits is a table constant used for 8-bit left rotations.\n-    \/\/ on 32-bit lanes within a SIMD register.\n-    __ BIND(L_Q_cc20_const);\n-    __ emit_int64(0x0000000000000001UL);\n-    __ emit_int64(0x0000000000000000UL);\n-    __ emit_int64(0x0605040702010003UL);\n-    __ emit_int64(0x0E0D0C0F0A09080BUL);\n-\n-    __ align(CodeEntryAlignment);\n-    StubGenStubId stub_id = StubGenStubId::chacha20Block_id;\n-    StubCodeMark mark(this, stub_id);\n-    address start = __ pc();\n-    __ enter();\n-\n-    const Register state = c_rarg0;\n-    const Register keystream = c_rarg1;\n-    const Register loopCtr = r10;\n-    const Register tmpAddr = r11;\n-\n-    const FloatRegister aState = v0;\n-    const FloatRegister bState = v1;\n-    const FloatRegister cState = v2;\n-    const FloatRegister dState = v3;\n-    const FloatRegister a1Vec = v4;\n-    const FloatRegister b1Vec = v5;\n-    const FloatRegister c1Vec = v6;\n-    const FloatRegister d1Vec = v7;\n-    \/\/ Skip the callee-saved registers v8 - v15\n-    const FloatRegister a2Vec = v16;\n-    const FloatRegister b2Vec = v17;\n-    const FloatRegister c2Vec = v18;\n-    const FloatRegister d2Vec = v19;\n-    const FloatRegister a3Vec = v20;\n-    const FloatRegister b3Vec = v21;\n-    const FloatRegister c3Vec = v22;\n-    const FloatRegister d3Vec = v23;\n-    const FloatRegister a4Vec = v24;\n-    const FloatRegister b4Vec = v25;\n-    const FloatRegister c4Vec = v26;\n-    const FloatRegister d4Vec = v27;\n-    const FloatRegister scratch = v28;\n-    const FloatRegister addMask = v29;\n-    const FloatRegister lrot8Tbl = v30;\n-\n-    \/\/ Load the initial state in the first 4 quadword registers,\n-    \/\/ then copy the initial state into the next 4 quadword registers\n-    \/\/ that will be used for the working state.\n-    __ ld1(aState, bState, cState, dState, __ T16B, Address(state));\n-\n-    \/\/ Load the index register for 2 constant 128-bit data fields.\n-    \/\/ The first represents the +1\/+0\/+0\/+0 add mask.  The second is\n-    \/\/ the 8-bit left rotation.\n-    __ adr(tmpAddr, L_Q_cc20_const);\n-    __ ldpq(addMask, lrot8Tbl, Address(tmpAddr));\n-\n-    __ mov(a1Vec, __ T16B, aState);\n-    __ mov(b1Vec, __ T16B, bState);\n-    __ mov(c1Vec, __ T16B, cState);\n-    __ mov(d1Vec, __ T16B, dState);\n-\n-    __ mov(a2Vec, __ T16B, aState);\n-    __ mov(b2Vec, __ T16B, bState);\n-    __ mov(c2Vec, __ T16B, cState);\n-    __ addv(d2Vec, __ T4S, d1Vec, addMask);\n-\n-    __ mov(a3Vec, __ T16B, aState);\n-    __ mov(b3Vec, __ T16B, bState);\n-    __ mov(c3Vec, __ T16B, cState);\n-    __ addv(d3Vec, __ T4S, d2Vec, addMask);\n-\n-    __ mov(a4Vec, __ T16B, aState);\n-    __ mov(b4Vec, __ T16B, bState);\n-    __ mov(c4Vec, __ T16B, cState);\n-    __ addv(d4Vec, __ T4S, d3Vec, addMask);\n-\n-    \/\/ Set up the 10 iteration loop\n-    __ mov(loopCtr, 10);\n-    __ BIND(L_Q_twoRounds);\n-\n-    \/\/ The first set of operations on the vectors covers the first 4 quarter\n-    \/\/ round operations:\n-    \/\/  Qround(state, 0, 4, 8,12)\n-    \/\/  Qround(state, 1, 5, 9,13)\n-    \/\/  Qround(state, 2, 6,10,14)\n-    \/\/  Qround(state, 3, 7,11,15)\n-    __ cc20_quarter_round(a1Vec, b1Vec, c1Vec, d1Vec, scratch, lrot8Tbl);\n-    __ cc20_quarter_round(a2Vec, b2Vec, c2Vec, d2Vec, scratch, lrot8Tbl);\n-    __ cc20_quarter_round(a3Vec, b3Vec, c3Vec, d3Vec, scratch, lrot8Tbl);\n-    __ cc20_quarter_round(a4Vec, b4Vec, c4Vec, d4Vec, scratch, lrot8Tbl);\n-\n-    \/\/ Shuffle the b1Vec\/c1Vec\/d1Vec to reorganize the state vectors to\n-    \/\/ diagonals. The a1Vec does not need to change orientation.\n-    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, true);\n-    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, true);\n-    __ cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, true);\n-    __ cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, true);\n-\n-    \/\/ The second set of operations on the vectors covers the second 4 quarter\n-    \/\/ round operations, now acting on the diagonals:\n-    \/\/  Qround(state, 0, 5,10,15)\n-    \/\/  Qround(state, 1, 6,11,12)\n-    \/\/  Qround(state, 2, 7, 8,13)\n-    \/\/  Qround(state, 3, 4, 9,14)\n-    __ cc20_quarter_round(a1Vec, b1Vec, c1Vec, d1Vec, scratch, lrot8Tbl);\n-    __ cc20_quarter_round(a2Vec, b2Vec, c2Vec, d2Vec, scratch, lrot8Tbl);\n-    __ cc20_quarter_round(a3Vec, b3Vec, c3Vec, d3Vec, scratch, lrot8Tbl);\n-    __ cc20_quarter_round(a4Vec, b4Vec, c4Vec, d4Vec, scratch, lrot8Tbl);\n-\n-    \/\/ Before we start the next iteration, we need to perform shuffles\n-    \/\/ on the b\/c\/d vectors to move them back to columnar organizations\n-    \/\/ from their current diagonal orientation.\n-    __ cc20_shift_lane_org(b1Vec, c1Vec, d1Vec, false);\n-    __ cc20_shift_lane_org(b2Vec, c2Vec, d2Vec, false);\n-    __ cc20_shift_lane_org(b3Vec, c3Vec, d3Vec, false);\n-    __ cc20_shift_lane_org(b4Vec, c4Vec, d4Vec, false);\n-\n-    \/\/ Decrement and iterate\n-    __ sub(loopCtr, loopCtr, 1);\n-    __ cbnz(loopCtr, L_Q_twoRounds);\n-\n-    \/\/ Once the counter reaches zero, we fall out of the loop\n-    \/\/ and need to add the initial state back into the working state\n-    \/\/ represented by the a\/b\/c\/d1Vec registers.  This is destructive\n-    \/\/ on the dState register but we no longer will need it.\n-    __ addv(a1Vec, __ T4S, a1Vec, aState);\n-    __ addv(b1Vec, __ T4S, b1Vec, bState);\n-    __ addv(c1Vec, __ T4S, c1Vec, cState);\n-    __ addv(d1Vec, __ T4S, d1Vec, dState);\n-\n-    __ addv(a2Vec, __ T4S, a2Vec, aState);\n-    __ addv(b2Vec, __ T4S, b2Vec, bState);\n-    __ addv(c2Vec, __ T4S, c2Vec, cState);\n-    __ addv(dState, __ T4S, dState, addMask);\n-    __ addv(d2Vec, __ T4S, d2Vec, dState);\n-\n-    __ addv(a3Vec, __ T4S, a3Vec, aState);\n-    __ addv(b3Vec, __ T4S, b3Vec, bState);\n-    __ addv(c3Vec, __ T4S, c3Vec, cState);\n-    __ addv(dState, __ T4S, dState, addMask);\n-    __ addv(d3Vec, __ T4S, d3Vec, dState);\n-\n-    __ addv(a4Vec, __ T4S, a4Vec, aState);\n-    __ addv(b4Vec, __ T4S, b4Vec, bState);\n-    __ addv(c4Vec, __ T4S, c4Vec, cState);\n-    __ addv(dState, __ T4S, dState, addMask);\n-    __ addv(d4Vec, __ T4S, d4Vec, dState);\n-\n-    \/\/ Write the final state back to the result buffer\n-    __ st1(a1Vec, b1Vec, c1Vec, d1Vec, __ T16B, __ post(keystream, 64));\n-    __ st1(a2Vec, b2Vec, c2Vec, d2Vec, __ T16B, __ post(keystream, 64));\n-    __ st1(a3Vec, b3Vec, c3Vec, d3Vec, __ T16B, __ post(keystream, 64));\n-    __ st1(a4Vec, b4Vec, c4Vec, d4Vec, __ T16B, __ post(keystream, 64));\n-\n-    __ mov(r0, 256);             \/\/ Return length of output keystream\n-    __ leave();\n-    __ ret(lr);\n-\n-    return start;\n-  }\n-\n@@ -10003,1 +9997,1 @@\n-      StubRoutines::_chacha20Block = generate_chacha20Block_qrpar();\n+      StubRoutines::_chacha20Block = generate_chacha20Block_blockpar();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":192,"deletions":198,"binary":false,"changes":390,"status":"modified"}]}