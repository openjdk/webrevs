[{"commit":{"message":"8370863: VectorAPI: Optimize the VectorMaskCast chain in specific patterns\n\n`VectorMaskCastNode` is used to cast a vector mask from one type to\nanother type. The cast may be generated by calling the vector API `cast`\nor generated by the compiler. For example, some vector mask operations\nlike `trueCount` require the input mask to be integer types, so for\nfloating point type masks, the compiler will cast the mask to the\ncorresponding integer type mask automatically before doing the mask\noperation. This kind of cast is very common.\n\nIf the vector element size is not changed, the `VectorMaskCastNode`\ndon't generate code, otherwise code will be generated to extend or narrow\nthe mask. This IR node is not free no matter it generates code or not\nbecause it may block some optimizations. For example:\n1. `(VectorStoremask (VectorMaskCast (VectorLoadMask x)))`\nThe middle `VectorMaskCast` prevented the following optimization:\n`(VectorStoremask (VectorLoadMask x)) => (x)`\n2. `(VectorMaskToLong (VectorMaskCast (VectorLongToMask x)))`, which\nblocks the optimization `(VectorMaskToLong (VectorLongToMask x)) => (x)`.\n\nIn these IR patterns, the value of the input `x` is not changed, so we\ncan safely do the optimization. But if the input value is changed, we\ncan't eliminate the cast.\n\nThe general idea of this PR is introducing an `uncast_mask` helper\nfunction, which can be used to uncast a chain of `VectorMaskCastNode`,\nlike the existing `Node::uncast(bool)` function. The funtion returns\nthe first non `VectorMaskCastNode`.\n\nThe intended use case is when the IR pattern to be optimized may\ncontain one or more consecutive `VectorMaskCastNode` and this does not\naffect the correctness of the optimization. Then this function can be\ncalled to eliminate the `VectorMaskCastNode` chain.\n\nCurrent optimizations related to `VectorMaskCastNode` include:\n1. `(VectorMaskCast (VectorMaskCast x)) => (x)`, see JDK-8356760.\n2. `(XorV (VectorMaskCast (VectorMaskCmp src1 src2 cond)) (Replicate -1))\n    => (VectorMaskCast (VectorMaskCmp src1 src2 ncond))`, see JDK-8354242.\n\nThis PR does the following optimizations:\n1. Extends the optimization pattern `(VectorMaskCast (VectorMaskCast x)) => (x)`\nas `(VectorMaskCast (VectorMaskCastÂ  ... (VectorMaskCast x))) => (x)`.\nBecause as long as types of the head and tail `VectorMaskCastNode` are\nconsistent, the optimization is correct.\n2. Supports a new optimization pattern\n`(VectorStoreMask (VectorMaskCast ... (VectorLoadMask x))) => (x)`.\nSince the value before and after the pattern is a boolean vector, it\nremains unchanged as long as the vector length remains the same, and\nthis is guranteed in the api level.\n\nI conducted some simple research on different mask generation methods\nand mask operations, and obtained the following table, which includes\nsome potential optimization opportunities that may use this `uncast_mask`\nfunction.\n\n```\nmask_gen\\op    toLong   anyTrue allTrue trueCount firstTrue lastTrue\ncompare        N\/A      N\/A     N\/A     N\/A       N\/A       N\/A\nmaskAll        TBI      TBI     TBI     TBI       TBI       TBI\nfromLong       TBI      TBI     N\/A     TBI       TBI       TBI\n\nmask_gen\\op    and      or      xor     andNot    not       laneIsSet\ncompare        N\/A      N\/A     N\/A     N\/A       TBI       N\/A\nmaskAll        TBI      TBI     TBI     TBI       TBI       TBI\nfromLong       N\/A      N\/A     N\/A     N\/A       TBI       TBI\n```\n`TBI` indicated that there may be potential optimizations here that\nrequire further investigation.\n\nBenchmarks:\n\nOn a Nvidia Grace machine with 128-bit SVE2:\n```\nBenchmark\t\t\tUnit\tBefore\tError\tAfter\tError\tUplift\nmicroMaskLoadCastStoreByte64\tops\/us\t59.23\t0.21\t148.12\t0.07\t2.50\nmicroMaskLoadCastStoreDouble128\tops\/us\t2.43\t0.00\t38.31\t0.01\t15.73\nmicroMaskLoadCastStoreFloat128\tops\/us\t6.19\t0.00\t75.67\t0.11\t12.22\nmicroMaskLoadCastStoreInt128\tops\/us\t6.19\t0.00\t75.67\t0.03\t12.22\nmicroMaskLoadCastStoreLong128\tops\/us\t2.43\t0.00\t38.32\t0.01\t15.74\nmicroMaskLoadCastStoreShort64\tops\/us\t28.89\t0.02\t75.60\t0.09\t2.62\n```\n\nOn a Nvidia Grace machine with 128-bit NEON:\n```\nBenchmark\t\t\tUnit\tBefore\tError\tAfter\tError\tUplift\nmicroMaskLoadCastStoreByte64\tops\/us\t75.75\t0.19\t149.74\t0.08\t1.98\nmicroMaskLoadCastStoreDouble128\tops\/us\t8.71\t0.03\t38.71\t0.05\t4.44\nmicroMaskLoadCastStoreFloat128\tops\/us\t24.05\t0.03\t76.49\t0.05\t3.18\nmicroMaskLoadCastStoreInt128\tops\/us\t24.06\t0.02\t76.51\t0.05\t3.18\nmicroMaskLoadCastStoreLong128\tops\/us\t8.72\t0.01\t38.71\t0.02\t4.44\nmicroMaskLoadCastStoreShort64\tops\/us\t24.64\t0.01\t76.43\t0.06\t3.10\n```\n\nOn an AMD EPYC 9124 16-Core Processor with AVX3:\n```\nBenchmark\t\t\tUnit\tBefore\tError\tAfter\tError\tUplift\nmicroMaskLoadCastStoreByte64\tops\/us\t82.13\t0.31\t115.14\t0.08\t1.40\nmicroMaskLoadCastStoreDouble128\tops\/us\t0.32\t0.00\t0.32\t0.00\t1.01\nmicroMaskLoadCastStoreFloat128\tops\/us\t42.18\t0.05\t57.56\t0.07\t1.36\nmicroMaskLoadCastStoreInt128\tops\/us\t42.19\t0.01\t57.53\t0.08\t1.36\nmicroMaskLoadCastStoreLong128\tops\/us\t0.30\t0.01\t0.32\t0.00\t1.05\nmicroMaskLoadCastStoreShort64\tops\/us\t42.18\t0.05\t57.59\t0.01\t1.37\n```\n\nOn an AMD EPYC 9124 16-Core Processor with AVX2:\n```\nBenchmark\t\t\tUnit\tBefore\tError\tAfter\tError\tUplift\nmicroMaskLoadCastStoreByte64\tops\/us\t73.53\t0.20\t114.98\t0.03\t1.56\nmicroMaskLoadCastStoreDouble128\tops\/us\t0.29\t0.01\t0.30\t0.00\t1.00\nmicroMaskLoadCastStoreFloat128\tops\/us\t30.78\t0.14\t57.50\t0.01\t1.87\nmicroMaskLoadCastStoreInt128\tops\/us\t30.65\t0.26\t57.50\t0.01\t1.88\nmicroMaskLoadCastStoreLong128\tops\/us\t0.30\t0.00\t0.30\t0.00\t0.99\nmicroMaskLoadCastStoreShort64\tops\/us\t24.92\t0.00\t57.49\t0.01\t2.31\n```\n\nOn an AMD EPYC 9124 16-Core Processor with AVX1:\n```\nBenchmark\t\t\tUnit\tBefore\tError\tAfter\tError\tUplift\nmicroMaskLoadCastStoreByte64\tops\/us\t79.68\t0.01\t248.49\t0.91\t3.12\nmicroMaskLoadCastStoreDouble128\tops\/us\t0.28\t0.00\t0.28\t0.00\t1.00\nmicroMaskLoadCastStoreFloat128\tops\/us\t31.11\t0.04\t95.48\t2.27\t3.07\nmicroMaskLoadCastStoreInt128\tops\/us\t31.10\t0.03\t99.94\t1.87\t3.21\nmicroMaskLoadCastStoreLong128\tops\/us\t0.28\t0.00\t0.28\t0.00\t0.99\nmicroMaskLoadCastStoreShort64\tops\/us\t31.11\t0.02\t94.97\t2.30\t3.05\n```\n\nThis PR was tested on 128-bit, 256-bit, and 512-bit (QEMU) aarch64\nenvironments, and two 512-bit x64 machines with various configurations,\nincluding sve2, sve1, neon, avx3, avx2, avx1, sse4 and sse3, all tests\npassed."},"files":[{"filename":"src\/hotspot\/share\/opto\/vectornode.cpp"},{"filename":"src\/hotspot\/share\/opto\/vectornode.hpp"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskCastIdentityTest.java"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskCastTest.java"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorMaskToLongTest.java"},{"filename":"test\/hotspot\/jtreg\/compiler\/vectorapi\/VectorStoreMaskIdentityTest.java"},{"filename":"test\/micro\/org\/openjdk\/bench\/jdk\/incubator\/vector\/VectorStoreMaskBenchmark.java"}],"sha":"fca9b3e513d357e28d90617da958ec9e05a7ac1a"}]