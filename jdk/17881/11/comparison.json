{"files":[{"patch":"@@ -5145,0 +5145,20 @@\n+void Assembler::vpmadd52luq(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {\n+  assert ((VM_Version::supports_avxifma() && vector_len <= AVX_256bit) || (VM_Version::supports_avx512ifma() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl())), \"\");\n+\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xB4);\n+  emit_operand(dst, src2, 0);\n+}\n+\n+void Assembler::vpmadd52luq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  assert ((VM_Version::supports_avxifma() && vector_len <= AVX_256bit) || (VM_Version::supports_avx512ifma() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl())), \"\");\n+\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xB4, (0xC0 | encode));\n+}\n+\n@@ -5162,0 +5182,20 @@\n+void Assembler::vpmadd52huq(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {\n+  assert ((VM_Version::supports_avxifma() && vector_len <= AVX_256bit) || (VM_Version::supports_avx512ifma() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl())), \"\");\n+\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8((unsigned char)0xB5);\n+  emit_operand(dst, src2, 0);\n+}\n+\n+void Assembler::vpmadd52huq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len) {\n+  assert ((VM_Version::supports_avxifma() && vector_len <= AVX_256bit) || (VM_Version::supports_avx512ifma() && (vector_len == AVX_512bit || VM_Version::supports_avx512vl())), \"\");\n+\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xB5, (0xC0 | encode));\n+}\n+\n@@ -9064,0 +9104,7 @@\n+void Assembler::vpunpckhqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6D, (0xC0 | encode));\n+}\n+\n@@ -9071,0 +9118,7 @@\n+void Assembler::vpunpcklqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x6C, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -1917,0 +1917,2 @@\n+  void vpmadd52luq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void vpmadd52luq(XMMRegister dst, XMMRegister src1, Address src2, int vector_len);\n@@ -1919,0 +1921,2 @@\n+  void vpmadd52huq(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n+  void vpmadd52huq(XMMRegister dst, XMMRegister src1, Address src2, int vector_len);\n@@ -2013,0 +2017,2 @@\n+  void vpunpcklqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n+\n@@ -2022,0 +2028,1 @@\n+  void vpunpckhqdq(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -9404,0 +9404,11 @@\n+void MacroAssembler::vpor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::vpor(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpor(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1808,0 +1808,3 @@\n+  using Assembler::vpor;\n+  void vpor(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -456,0 +456,21 @@\n+  \/\/Poly305 AVX2 implementation\n+  void poly1305_process_blocks_avx2(const Register input, const Register length,\n+    const Register a0, const Register a1, const Register a2,\n+    const Register r0, const Register r1, const Register c1);\n+  void poly1305_msg_mul_reduce_vec4_avx2(const XMMRegister A0, const XMMRegister A1, const XMMRegister A2,\n+                                   const Address R0, const Address R1, const Address R2,\n+                                   const Address R1P, const Address R2P,\n+                                   const XMMRegister P0L, const XMMRegister P0H,\n+                                   const XMMRegister P1L, const XMMRegister P1H,\n+                                   const XMMRegister P2L, const XMMRegister P2H,\n+                                   const XMMRegister YTMP1, const XMMRegister YTMP2,\n+                                   const XMMRegister YTMP3, const XMMRegister YTMP4,\n+                                   const XMMRegister YTMP5, const XMMRegister YTMP6,\n+                                   const Register input, const Register length, const Register rscratch);\n+  void poly1305_mul_reduce_vec4_avx2(const XMMRegister A0, const XMMRegister A1, const XMMRegister A2,\n+                               const XMMRegister R0, const XMMRegister R1, const XMMRegister R2,\n+                               const XMMRegister R1P, const XMMRegister R2P,\n+                               const XMMRegister P0L, const XMMRegister P0H,\n+                               const XMMRegister P1L, const XMMRegister P1H,\n+                               const XMMRegister P2L, const XMMRegister P2H,\n+                               const XMMRegister YTMP1, const Register rscratch);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2023, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2022, 2024, Intel Corporation. All rights reserved.\n@@ -930,2 +930,2 @@\n-  const Register input        = rdi;\n-  const Register length       = rbx;\n+  const Register input        = rdi; \/\/ msg\n+  const Register length       = rbx; \/\/ msg length in bytes\n@@ -988,1 +988,2 @@\n-  poly1305_process_blocks_avx512(input, length,\n+  if (UseAVX > 2) {\n+    poly1305_process_blocks_avx512(input, length,\n@@ -991,0 +992,6 @@\n+  } else {\n+    poly1305_process_blocks_avx2(input, length,\n+                                  a0, a1, a2,\n+                                  r0, r1, c1);\n+  }\n+\n@@ -1026,0 +1033,665 @@\n+\n+\/*\n+  The AVX2 implementation below is directly based on the AVX2 Poly1305 hash computation as\n+  implemented in Intel(R) Multi-Buffer Crypto for IPsec Library.\n+  (url: https:\/\/github.com\/intel\/intel-ipsec-mb\/blob\/main\/lib\/avx2_t3\/poly_fma_avx2.asm)\n+\n+  Additional references:\n+  [1] Goll M, Gueron S., \"Vectorization of Poly1305 message authentication code\",\n+      12th International Conference on Information Technology-New Generations,\n+      2015 Apr 13 (pp. 145-150). IEEE.\n+  [2] Bhattacharyya S, Sarkar P., \"Improved SIMD implementation of Poly1305\",\n+      IET Information Security. 2020 Sep;14(5):521-30.\n+  Note: a compact summary of the Goll-Gueron AVX2 algorithm developed in [1] is presented in [2].\n+  [3] Wikipedia, \"Parallel evaluation of Horner's method\",\n+      (url: https:\/\/en.wikipedia.org\/wiki\/Horner%27s_method)\n+ ----------------------------------------------------------\n+\n+  Poly1305 AVX2 algorithm:\n+  Let the 32-byte one-time key be partitioned into two equal parts R and K.\n+  Let R be the 16-byte secret key used for polynomial evaluation.\n+  Let K be the 16-byte secret key.\n+  Let Z_P be prime field over which the polynomial is evaluated. Let P = 2^130 - 5 be the prime.\n+  Let M be the message which can be represented as a concatenation (||) of 'l' 16-byte blocks M[i].\n+  i.e., M = M[0] || M[1] || ... || M[i] || ... || M[l-2] || M[l-1]\n+  To create the coefficients C[i] for polynomial evaluation over Z_P, each 16-byte (i.e., 128-bit)\n+  message block M[i] is concatenated with bits '10' to make a 130-bit block.\n+  The last block (<= 16-byte length) is concatenated with 1 followed by 0s to make a 130-bit block.\n+  Therefore, we define\n+  C[i]   = M[i] || '10' for 0 <= i <= l-2 ;\n+  C[l-1] = M[i] || '10...0'\n+  such that, length(C[i]) = 130 bits, for i ∈ [0, l).\n+\n+  Let * indicate scalar multiplication (i.e., w = u * v);\n+  Let × indicate scalar multiplication followed by reduction modulo P (i.e., z = u × v = {(u * v) mod P})\n+\n+  POLY1305_MAC = (POLY1305_EVAL_POLYNOMIAL(C, R, P) + K) mod 2^128; where,\n+\n+  POLY1305_EVAL_POLYNOMIAL(C, R, P) = {C[0] * R^l + C[1] * R^(l-1) + ... + C[l-2] * R^2 + C[l-1] * R} mod P\n+    = R × {C[0] × R^(l-1) + C[1] × R^(l-2) + ... + C[l-2] × R + C[l-1]}\n+    = R × Polynomial(R; C[0], C[1], ... ,C[l-2], C[l-1])\n+  Where,\n+  Polynomial(R; C[0], C[1], ... ,C[l-2], C[l-1]) = Σ{C[i] × R^(l-i-1)} for i ∈ [0, l)\n+  ----------------------------------------------------------\n+\n+  Parallel evaluation of POLY1305_EVAL_POLYNOMIAL(C, R, P):\n+  Let the number of message blocks l = 4*l' + ρ where ρ = l mod 4.\n+  Using k-way parallel Horner's evaluation [3], for k = 4, we define SUM below:\n+\n+  SUM = R^4 × Polynomial(R^4; C[0], C[4], C[8]  ... , C[4l'-4]) +\n+        R^3 × Polynomial(R^4; C[1], C[5], C[9]  ... , C[4l'-3]) +\n+        R^2 × Polynomial(R^4; C[2], C[6], C[10] ... , C[4l'-2]) +\n+        R^1 × Polynomial(R^4; C[3], C[7], C[11] ... , C[4l'-1]) +\n+\n+  Then,\n+  POLY1305_EVAL_POLYNOMIAL(C, R, P) = SUM if ρ = 0 (i.e., l is multiple of 4)\n+                        = R × Polynomial(R; SUM + C[l-ρ], C[l-ρ+1], ... , C[l-1]) if ρ > 0\n+  ----------------------------------------------------------\n+\n+  Gall-Gueron[1] 4-way SIMD Algorithm[2] for POLY1305_EVAL_POLYNOMIAL(C, R, P):\n+\n+  Define mathematical vectors (not same as SIMD vector lanes) as below:\n+  R4321   = [R^4, R^3, R^2, R^1];\n+  R4444   = [R^4, R^4, R^4, R^4];\n+  COEF[i] = [C[4i], C[4i+1], C[4i+2], C[4i+3]] for i ∈ [0, l'). For example, COEF[0] and COEF[1] shown below.\n+  COEF[0] = [C0, C1, C2, C3]\n+  COEF[1] = [C4, C5, C6, C7]\n+  T       = [T0, T1, T2, T3] be a temporary vector\n+  ACC     = [acc, 0, 0, 0]; acc has hash from previous computations (if any), otherwise 0.\n+  ⊗ indicates component-wise vector multiplication followed by modulo reduction\n+  ⊕ indicates component-wise vector addition, + indicates scalar addition\n+\n+  POLY1305_EVAL_POLYNOMIAL(C, R, P) {\n+    T ← ACC; # load accumulator\n+    T ← T ⊕ COEF[0]; # add accumulator to the first 4 blocks\n+    Compute R4321, R4444;\n+    # SIMD loop\n+    l' = floor(l\/4); # operate on 4 blocks at a time\n+    for (i = 1 to l'-1):\n+      T ← (R4444 ⊗ T) ⊕ COEF[i];\n+    T ← R4321 ⊗ T;\n+    SUM ← T0 + T1 + T2 + T3;\n+\n+    # Scalar tail processing\n+    if (ρ > 0):\n+      SUM ← R × Polynomial(R; SUM + C[l-ρ], C[l-ρ+1], ... , C[l-1]);\n+    return SUM;\n+  }\n+\n+  Notes:\n+  (1) Each 130-bit block is represented using three 44-bit limbs (most significant limb is only 42-bit).\n+      (The Goll-Gueron implementation[1] uses five 26-bit limbs instead).\n+  (2) Each component of the mathematical vectors is a 130-bit value. The above mathemetical vectors are not to be confused with SIMD vector lanes.\n+  (3) Each AVX2 YMM register can store four 44-bit limbs in quadwords. Since each 130-bit message block is represented using 3 limbs,\n+      to store all the limbs of 4 different 130-bit message blocks, we need 3 YMM registers in total.\n+  (4) In the AVX2 implementation, multiplication followed by modulo reduction and addition are performed for 4 blocks at a time.\n+\n+*\/\n+\n+void StubGenerator::poly1305_process_blocks_avx2(\n+    const Register input, const Register length,\n+    const Register a0, const Register a1, const Register a2,\n+    const Register r0, const Register r1, const Register c1)\n+{\n+  Label L_process256Loop, L_process256LoopDone;\n+  const Register t0 = r13;\n+  const Register t1 = r14;\n+  const Register t2 = r15;\n+  const Register mulql = rax;\n+  const Register mulqh = rdx;\n+\n+  const XMMRegister YMM_ACC0 = xmm0;\n+  const XMMRegister YMM_ACC1 = xmm1;\n+  const XMMRegister YMM_ACC2 = xmm2;\n+\n+  const XMMRegister YTMP1 = xmm3;\n+  const XMMRegister YTMP2 = xmm4;\n+  const XMMRegister YTMP3 = xmm5;\n+  const XMMRegister YTMP4 = xmm6;\n+  const XMMRegister YTMP5 = xmm7;\n+  const XMMRegister YTMP6 = xmm8;\n+  const XMMRegister YTMP7 = xmm9;\n+  const XMMRegister YTMP8 = xmm10;\n+  const XMMRegister YTMP9 = xmm11;\n+  const XMMRegister YTMP10 = xmm12;\n+  const XMMRegister YTMP11 = xmm13;\n+  const XMMRegister YTMP12 = xmm14;\n+  const XMMRegister YTMP13 = xmm15;\n+\n+  const XMMRegister YMM_R0 = YTMP11;\n+  const XMMRegister YMM_R1 = YTMP12;\n+  const XMMRegister YMM_R2 = YTMP13;\n+\n+  \/\/ XWORD aliases of YMM registers (for convenience)\n+  const XMMRegister XTMP1 = YTMP1;\n+  const XMMRegister XTMP2 = YTMP2;\n+  const XMMRegister XTMP3 = YTMP3;\n+\n+  \/\/ Setup stack frame\n+  \/\/ Save rbp and rsp\n+  __ push(rbp);\n+  __ movq(rbp, rsp);\n+  \/\/ Align stack and reserve space\n+  __ andq(rsp, -32);\n+  __ subptr(rsp, 32*8);\n+\n+  \/* Compute the following steps of POLY1305_EVAL_POLYNOMIAL algorithm\n+    T ← ACC\n+    T ← T ⊕ COEF[0];\n+  *\/\n+\n+  \/\/ Spread accumulator into 44-bit limbs in quadwords\n+  \/\/ Accumulator limbs to be stored in YTMP1,YTMP2,YTMP3\n+  \/\/ First limb (Acc[43:0])\n+  __ movq(t0, a0);\n+  __ andq(t0, ExternalAddress(poly1305_mask44()), t1 \/*rscratch*\/);\n+  __ movq(XTMP1, t0);\n+  \/\/ Second limb (Acc[87:44])\n+  __ movq(t0, a1);\n+  __ shrdq(a0, t0, 44);\n+  __ andq(a0, ExternalAddress(poly1305_mask44()), t1 \/*rscratch*\/);\n+  __ movq(XTMP2, a0);\n+  \/\/ Third limb (Acc[129:88])\n+  __ shrdq(a1, a2, 24);\n+  __ andq(a1, ExternalAddress(poly1305_mask42()), t1 \/*rscratch*\/);\n+  __ movq(XTMP3, a1);\n+  \/\/ --- end of spread accumulator\n+\n+  \/\/ To add accumulator, we must unroll first loop iteration\n+  \/\/ Load first four 16-byte message blocks of data (64 bytes)\n+  __ vmovdqu(YTMP4, Address(input, 0));\n+  __ vmovdqu(YTMP5, Address(input, 32));\n+\n+  \/\/ Interleave the input message data to form 44-bit limbs\n+  \/\/ YMM_ACC0 to have bits 0-43 of all 4 blocks in 4 qwords\n+  \/\/ YMM_ACC1 to have bits 87-44 of all 4 blocks in 4 qwords\n+  \/\/ YMM_ACC2 to have bits 127-88 of all 4 blocks in 4 qwords\n+  \/\/ Interleave blocks of data\n+  __ vpunpckhqdq(YMM_ACC2, YTMP4, YTMP5, Assembler::AVX_256bit);\n+  __ vpunpcklqdq(YMM_ACC0, YTMP4, YTMP5, Assembler::AVX_256bit);\n+\n+  \/\/ Middle 44-bit limbs of new blocks\n+  __ vpsrlq(YMM_ACC1, YMM_ACC0, 44, Assembler::AVX_256bit);\n+  __ vpsllq(YTMP4, YMM_ACC2, 20, Assembler::AVX_256bit);\n+  __ vpor(YMM_ACC1, YMM_ACC1, YTMP4, Assembler::AVX_256bit);\n+  __ vpand(YMM_ACC1, YMM_ACC1, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, t1);\n+\n+  \/\/ Lowest 44-bit limbs of new blocks\n+  __ vpand(YMM_ACC0, YMM_ACC0, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, t1);\n+\n+  \/\/ Highest 42-bit limbs of new blocks; pad the msg with 2^128\n+  __ vpsrlq(YMM_ACC2, YMM_ACC2, 24, Assembler::AVX_256bit);\n+\n+  \/\/ Add 2^128 to all 4 final qwords for the message\n+  __ vpor(YMM_ACC2, YMM_ACC2, ExternalAddress(poly1305_pad_msg()), Assembler::AVX_256bit, t1);\n+  \/\/ --- end of input interleaving and message padding\n+\n+  \/\/ Add accumulator to the fist message block\n+  \/\/ Accumulator limbs in YTMP1,YTMP2,YTMP3\n+  __ vpaddq(YMM_ACC0, YMM_ACC0, YTMP1, Assembler::AVX_256bit);\n+  __ vpaddq(YMM_ACC1, YMM_ACC1, YTMP2, Assembler::AVX_256bit);\n+  __ vpaddq(YMM_ACC2, YMM_ACC2, YTMP3, Assembler::AVX_256bit);\n+\n+  \/* Compute the following steps of POLY1305_EVAL_POLYNOMIAL algorithm\n+    Compute R4321, R4444;\n+    R4321   = [R^4, R^3, R^2, R^1];\n+    R4444   = [R^4, R^4, R^4, R^4];\n+  *\/\n+\n+  \/\/ Compute the powers of R^1..R^4 and form 44-bit limbs of each\n+  \/\/ YTMP5 to have bits 0-127 for R^1 and R^2\n+  \/\/ YTMP6 to have bits 128-129 for R^1 and R^2\n+  __ movq(XTMP1, r0);\n+  __ vpinsrq(XTMP1, XTMP1, r1, 1);\n+  __ vinserti128(YTMP5, YTMP5, XTMP1, 1);\n+  \/\/ clear registers\n+  __ vpxor(YTMP10, YTMP10, YTMP10, Assembler::AVX_256bit);\n+  __ vpxor(YTMP6, YTMP6, YTMP6, Assembler::AVX_256bit);\n+\n+  \/\/ Calculate R^2\n+  \/\/ a ← R\n+  __ movq(a0, r0);\n+  __ movq(a1, r1);\n+  \/\/ a ← a * R = R^2\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, true,\n+                           t0, t1, t2, mulql, mulqh);\n+  \/\/ Store R^2 in YTMP5, YTM6\n+  __ movq(XTMP1, a0);\n+  __ vpinsrq(XTMP1, XTMP1, a1, 1);\n+  __ vinserti128(YTMP5, YTMP5, XTMP1, 0);\n+  __ movq(XTMP1, a2);\n+  __ vinserti128(YTMP6, YTMP6, XTMP1, 0);\n+\n+  \/\/ Calculate R^3\n+  \/\/ a ← a * R = R^3\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, false,\n+                           t0, t1, t2, mulql, mulqh);\n+  \/\/ Store R^3 in YTMP7, YTM2\n+  __ movq(XTMP1, a0);\n+  __ vpinsrq(XTMP1, XTMP1, a1, 1);\n+  __ vinserti128(YTMP7, YTMP7, XTMP1, 1);\n+  __ movq(XTMP1, a2);\n+  __ vinserti128(YTMP2, YTMP2, XTMP1, 1);\n+\n+  \/\/ Calculate R^4\n+  \/\/ a ← a * R = R^4\n+  poly1305_multiply_scalar(a0, a1, a2,\n+                           r0, r1, c1, false,\n+                           t0, t1, t2, mulql, mulqh);\n+  \/\/ Store R^4 in YTMP7, YTM2\n+  __ movq(XTMP1, a0);\n+  __ vpinsrq(XTMP1, XTMP1, a1, 1);\n+  __ vinserti128(YTMP7, YTMP7, XTMP1, 0);\n+  __ movq(XTMP1, a2);\n+  __ vinserti128(YTMP2, YTMP2, XTMP1, 0);\n+\n+  \/\/ Interleave the powers of R^1..R^4 to form 44-bit limbs (half-empty)\n+  __ vpunpckhqdq(YMM_R2, YTMP5, YTMP10, Assembler::AVX_256bit);\n+  __ vpunpcklqdq(YMM_R0, YTMP5, YTMP10, Assembler::AVX_256bit);\n+  __ vpunpckhqdq(YTMP3, YTMP7, YTMP10, Assembler::AVX_256bit);\n+  __ vpunpcklqdq(YTMP4, YTMP7, YTMP10, Assembler::AVX_256bit);\n+\n+  __ vpslldq(YMM_R2, YMM_R2, 8, Assembler::AVX_256bit);\n+  __ vpslldq(YTMP6, YTMP6, 8, Assembler::AVX_256bit);\n+  __ vpslldq(YMM_R0, YMM_R0, 8, Assembler::AVX_256bit);\n+  __ vpor(YMM_R2, YMM_R2, YTMP3, Assembler::AVX_256bit);\n+  __ vpor(YMM_R0, YMM_R0, YTMP4, Assembler::AVX_256bit);\n+  __ vpor(YTMP6, YTMP6, YTMP2, Assembler::AVX_256bit);\n+  \/\/ Move 2 MSbits to top 24 bits, to be OR'ed later\n+  __ vpsllq(YTMP6, YTMP6, 40, Assembler::AVX_256bit);\n+\n+  __ vpsrlq(YMM_R1, YMM_R0, 44, Assembler::AVX_256bit);\n+  __ vpsllq(YTMP5, YMM_R2, 20, Assembler::AVX_256bit);\n+  __ vpor(YMM_R1, YMM_R1, YTMP5, Assembler::AVX_256bit);\n+  __ vpand(YMM_R1, YMM_R1, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, t1);\n+\n+  __ vpand(YMM_R0, YMM_R0, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, t1);\n+  __ vpsrlq(YMM_R2, YMM_R2, 24, Assembler::AVX_256bit);\n+\n+  __ vpor(YMM_R2, YMM_R2, YTMP6, Assembler::AVX_256bit);\n+  \/\/ YMM_R0, YMM_R1, YMM_R2 have the limbs of R^1, R^2, R^3, R^4\n+\n+  \/\/ Store R^4-R on stack for later use\n+  int _r4_r1_save = 0;\n+  __ vmovdqu(Address(rsp, _r4_r1_save + 0), YMM_R0);\n+  __ vmovdqu(Address(rsp, _r4_r1_save + 32), YMM_R1);\n+  __ vmovdqu(Address(rsp, _r4_r1_save + 32*2), YMM_R2);\n+\n+  \/\/ Broadcast 44-bit limbs of R^4\n+  __ mov(t0, a0);\n+  __ andq(t0, ExternalAddress(poly1305_mask44()), t1 \/*rscratch*\/); \/\/ First limb (R^4[43:0])\n+  __ movq(YMM_R0, t0);\n+  __ vpermq(YMM_R0, YMM_R0, 0x0, Assembler::AVX_256bit);\n+\n+  __ movq(t0, a1);\n+  __ shrdq(a0, t0, 44);\n+  __ andq(a0, ExternalAddress(poly1305_mask44()), t1 \/*rscratch*\/); \/\/ Second limb (R^4[87:44])\n+  __ movq(YMM_R1, a0);\n+  __ vpermq(YMM_R1, YMM_R1, 0x0, Assembler::AVX_256bit);\n+\n+  __ shrdq(a1, a2, 24);\n+  __ andq(a1, ExternalAddress(poly1305_mask42()), t1 \/*rscratch*\/); \/\/ Third limb (R^4[129:88])\n+  __ movq(YMM_R2, a1);\n+  __ vpermq(YMM_R2, YMM_R2, 0x0, Assembler::AVX_256bit);\n+  \/\/ YMM_R0, YMM_R1, YMM_R2 have the limbs of R^4, R^4, R^4, R^4\n+\n+  \/\/ Generate 4*5*R^4\n+  \/\/ 4*R^4\n+  __ vpsllq(YTMP1, YMM_R1, 2, Assembler::AVX_256bit);\n+  __ vpsllq(YTMP2, YMM_R2, 2, Assembler::AVX_256bit);\n+  \/\/ 5*R^4\n+  __ vpaddq(YTMP1, YTMP1, YMM_R1, Assembler::AVX_256bit);\n+  __ vpaddq(YTMP2, YTMP2, YMM_R2, Assembler::AVX_256bit);\n+  \/\/ 4*5*R^4\n+  __ vpsllq(YTMP1, YTMP1, 2, Assembler::AVX_256bit);\n+  __ vpsllq(YTMP2, YTMP2, 2, Assembler::AVX_256bit);\n+\n+  \/\/Store broadcasted R^4 and 4*5*R^4 on stack for later use\n+  int _r4_save = 32*3;\n+  int _r4p_save = 32*6;\n+  __ vmovdqu(Address(rsp, _r4_save + 0), YMM_R0);\n+  __ vmovdqu(Address(rsp, _r4_save + 32), YMM_R1);\n+  __ vmovdqu(Address(rsp, _r4_save + 32*2), YMM_R2);\n+  __ vmovdqu(Address(rsp, _r4p_save), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4p_save + 32), YTMP2);\n+\n+  \/\/ Get the number of multiples of 4 message blocks (64 bytes) for vectorization\n+  __ movq(t0, length);\n+  __ andq(t0, 0xffffffc0); \/\/ 0xffffffffffffffc0 after sign extension\n+\n+  \/\/ VECTOR LOOP: process 4 * 16-byte message blocks at a time\n+  __ bind(L_process256Loop);\n+  __ cmpl(t0, 16*4); \/\/64 bytes (4 blocks at a time)\n+  __ jcc(Assembler::belowEqual, L_process256LoopDone);\n+\n+  \/*\n+    Compute the following steps of POLY1305_EVAL_POLYNOMIAL algorithm\n+    l' = floor(l\/4)\n+    for (i = 1 to l'-1):\n+      T ← (R4444 ⊗ T) ⊕ COEF[i];\n+  *\/\n+\n+  \/\/ Perform multiply and reduce while loading the next block and adding it in interleaved manner\n+  \/\/ The logic to advance the SIMD loop counter (i.e. length -= 64) is inside the function below.\n+  \/\/ The function below also includes the logic to load the next 4 blocks of data for efficient port utilization.\n+  poly1305_msg_mul_reduce_vec4_avx2(YMM_ACC0, YMM_ACC1, YMM_ACC2,\n+                          Address(rsp, _r4_save + 0), Address(rsp, _r4_save + 32), Address(rsp, _r4_save + 32*2),\n+                          Address(rsp, _r4p_save), Address(rsp, _r4p_save + 32),\n+                          YTMP1, YTMP2, YTMP3, YTMP4, YTMP5, YTMP6,\n+                          YTMP7, YTMP8, YTMP9, YTMP10, YTMP11, YTMP12,\n+                          input, t0, t1 \/*rscratch*\/);\n+  __ jmp(L_process256Loop);\n+  \/\/ end of vector loop\n+  __ bind(L_process256LoopDone);\n+\n+  \/*\n+    Compute the following steps of POLY1305_EVAL_POLYNOMIAL algorithm\n+    T ← R4321 ⊗ T;\n+  *\/\n+\n+  \/\/ Need to multiply by R^4, R^3, R^2, R\n+  \/\/Read R^4-R;\n+  __ vmovdqu(YMM_R0, Address(rsp, _r4_r1_save + 0));\n+  __ vmovdqu(YMM_R1, Address(rsp, _r4_r1_save + 32));\n+  __ vmovdqu(YMM_R2, Address(rsp, _r4_r1_save + 32*2));\n+\n+  \/\/ Generate 4*5*[R^4..R^1] (ignore lowest limb)\n+  \/\/ YTMP1 to have bits 87-44 of all 1-4th powers of R' in 4 qwords\n+  \/\/ YTMP2 to have bits 129-88 of all 1-4th powers of R' in 4 qwords\n+  __ vpsllq(YTMP10, YMM_R1, 2, Assembler::AVX_256bit);\n+  __ vpaddq(YTMP1, YMM_R1, YTMP10, Assembler::AVX_256bit);  \/\/R1' (R1*5)\n+  __ vpsllq(YTMP10, YMM_R2, 2, Assembler::AVX_256bit);\n+  __ vpaddq(YTMP2, YMM_R2, YTMP10, Assembler::AVX_256bit);  \/\/R2' (R2*5)\n+\n+  \/\/ 4*5*R\n+  __ vpsllq(YTMP1, YTMP1, 2, Assembler::AVX_256bit);\n+  __ vpsllq(YTMP2, YTMP2, 2, Assembler::AVX_256bit);\n+\n+  poly1305_mul_reduce_vec4_avx2(YMM_ACC0, YMM_ACC1, YMM_ACC2,\n+                          YMM_R0, YMM_R1, YMM_R2, YTMP1, YTMP2,\n+                          YTMP3, YTMP4, YTMP5, YTMP6,\n+                          YTMP7, YTMP8, YTMP9, t1);\n+  \/*\n+    Compute the following steps of POLY1305_EVAL_POLYNOMIAL algorithm\n+    SUM ← T0 + T1 + T2 + T3;\n+  *\/\n+\n+  \/\/ 4 -> 2 blocks\n+  __ vextracti128(YTMP1, YMM_ACC0, 1);\n+  __ vextracti128(YTMP2, YMM_ACC1, 1);\n+  __ vextracti128(YTMP3, YMM_ACC2, 1);\n+\n+  __ vpaddq(YMM_ACC0, YMM_ACC0, YTMP1, Assembler::AVX_128bit);\n+  __ vpaddq(YMM_ACC1, YMM_ACC1, YTMP2, Assembler::AVX_128bit);\n+  __ vpaddq(YMM_ACC2, YMM_ACC2, YTMP3, Assembler::AVX_128bit);\n+  \/\/ 2 -> 1 blocks\n+  __ vpsrldq(YTMP1, YMM_ACC0, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(YTMP2, YMM_ACC1, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(YTMP3, YMM_ACC2, 8, Assembler::AVX_128bit);\n+\n+  \/\/ Finish folding\n+  __ vpaddq(YMM_ACC0, YMM_ACC0, YTMP1, Assembler::AVX_128bit);\n+  __ vpaddq(YMM_ACC1, YMM_ACC1, YTMP2, Assembler::AVX_128bit);\n+  __ vpaddq(YMM_ACC2, YMM_ACC2, YTMP3, Assembler::AVX_128bit);\n+\n+  __ movq(YMM_ACC0, YMM_ACC0);\n+  __ movq(YMM_ACC1, YMM_ACC1);\n+  __ movq(YMM_ACC2, YMM_ACC2);\n+\n+  __ lea(input, Address(input,16*4));\n+  __ andq(length, 63); \/\/ remaining bytes < length 64\n+  \/\/ carry propagation\n+  __ vpsrlq(YTMP1, YMM_ACC0, 44, Assembler::AVX_128bit);\n+  __ vpand(YMM_ACC0, YMM_ACC0, ExternalAddress(poly1305_mask44()), Assembler::AVX_128bit, t1); \/\/ Clear top 20 bits\n+  __ vpaddq(YMM_ACC1, YMM_ACC1, YTMP1, Assembler::AVX_128bit);\n+  __ vpsrlq(YTMP1, YMM_ACC1, 44, Assembler::AVX_128bit);\n+  __ vpand(YMM_ACC1, YMM_ACC1, ExternalAddress(poly1305_mask44()), Assembler::AVX_128bit, t1); \/\/ Clear top 20 bits\n+  __ vpaddq(YMM_ACC2, YMM_ACC2, YTMP1, Assembler::AVX_128bit);\n+  __ vpsrlq(YTMP1, YMM_ACC2, 42, Assembler::AVX_128bit);\n+  __ vpand(YMM_ACC2, YMM_ACC2, ExternalAddress(poly1305_mask42()), Assembler::AVX_128bit, t1); \/\/ Clear top 20 bits\n+  __ vpsllq(YTMP2, YTMP1, 2, Assembler::AVX_128bit);\n+  __ vpaddq(YTMP1, YTMP1, YTMP2, Assembler::AVX_128bit);\n+  __ vpaddq(YMM_ACC0, YMM_ACC0, YTMP1, Assembler::AVX_128bit);\n+\n+  \/\/ Put together A\n+  __ movq(a0, YMM_ACC0);\n+  __ movq(t0, YMM_ACC1);\n+  __ movq(t1, t0);\n+  __ shlq(t1, 44);\n+  __ orq(a0, t1);\n+  __ shrq(t0, 20);\n+  __ movq(a2, YMM_ACC2);\n+  __ movq(a1, a2);\n+  __ shlq(a1, 24);\n+  __ orq(a1, t0);\n+  __ shrq(a2, 40);\n+\n+  \/\/ cleanup\n+  __ vzeroall(); \/\/ clears all ymm registers (ymm0 through ymm15)\n+\n+  \/\/ SAFE DATA (clear powers of R)\n+  __ vmovdqu(Address(rsp, _r4_r1_save + 0), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4_r1_save + 32), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4_r1_save + 32*2), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4_save + 0), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4_save + 32), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4_save + 32*2), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4p_save), YTMP1);\n+  __ vmovdqu(Address(rsp, _r4p_save + 32), YTMP1);\n+\n+  \/\/ Save rbp and rsp; clear stack frame\n+    __ movq(rsp, rbp);\n+    __ pop(rbp);\n+\n+}\n+\n+\/\/ Compute component-wise product for 4 16-byte message  blocks,\n+\/\/ i.e. For each block, compute [a2 a1 a0] = [a2 a1 a0] x [r2 r1 r0]\n+\/\/\n+\/\/ Each block\/number is represented by 3 44-bit limb digits, start with multiplication\n+\/\/\n+\/\/      a2       a1       a0\n+\/\/ x    r2       r1       r0\n+\/\/ ----------------------------------\n+\/\/     a2xr0    a1xr0    a0xr0\n+\/\/ +   a1xr1    a0xr1  5xa2xr1'     (r1' = r1<<2)\n+\/\/ +   a0xr2  5xa2xr2' 5xa1xr2'     (r2' = r2<<2)\n+\/\/ ----------------------------------\n+\/\/        p2       p1       p0\n+\/\/\n+void StubGenerator::poly1305_mul_reduce_vec4_avx2(\n+  const XMMRegister A0, const XMMRegister A1, const XMMRegister A2,\n+  const XMMRegister R0, const XMMRegister R1, const XMMRegister R2,\n+  const XMMRegister R1P, const XMMRegister R2P,\n+  const XMMRegister P0L, const XMMRegister P0H,\n+  const XMMRegister P1L, const XMMRegister P1H,\n+  const XMMRegister P2L, const XMMRegister P2H,\n+  const XMMRegister YTMP1, const Register rscratch)\n+{\n+  \/\/ Reset accumulator\n+  __ vpxor(P0L, P0L, P0L, Assembler::AVX_256bit);\n+  __ vpxor(P0H, P0H, P0H, Assembler::AVX_256bit);\n+  __ vpxor(P1L, P1L, P1L, Assembler::AVX_256bit);\n+  __ vpxor(P1H, P1H, P1H, Assembler::AVX_256bit);\n+  __ vpxor(P2L, P2L, P2L, Assembler::AVX_256bit);\n+  __ vpxor(P2H, P2H, P2H, Assembler::AVX_256bit);\n+\n+  \/\/ Calculate partial products\n+  \/\/ p0 = a2xr1'\n+  \/\/ p1 = a2xr2'\n+  \/\/ p0 += a0xr0\n+  __ vpmadd52luq(P0L, A2, R1P, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P0H, A2, R1P, Assembler::AVX_256bit);\n+\n+  __ vpmadd52luq(P1L, A2, R2P, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P1H, A2, R2P, Assembler::AVX_256bit);\n+\n+  __ vpmadd52luq(P0L, A0, R0, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P0H, A0, R0, Assembler::AVX_256bit);\n+\n+  \/\/ p2 = a2xr0\n+  \/\/ p1 += a0xr1\n+  \/\/ p0 += a1xr2'\n+  \/\/ p2 += a0Xr2\n+  __ vpmadd52luq(P2L, A2, R0, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P2H, A2, R0, Assembler::AVX_256bit);\n+\n+  __ vpmadd52luq(P1L, A0, R1, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P1H, A0, R1, Assembler::AVX_256bit);\n+\n+  __ vpmadd52luq(P0L, A1, R2P, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P0H, A1, R2P, Assembler::AVX_256bit);\n+\n+  __ vpmadd52luq(P2L, A0, R2, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P2H, A0, R2, Assembler::AVX_256bit);\n+\n+  \/\/ Carry propgation (first pass)\n+  __ vpsrlq(YTMP1, P0L, 44, Assembler::AVX_256bit);\n+  __ vpsllq(P0H, P0H, 8, Assembler::AVX_256bit);\n+  __ vpmadd52luq(P1L, A1, R0, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P1H, A1, R0, Assembler::AVX_256bit);\n+  \/\/ Carry propagation (first pass) - continue\n+  __ vpand(A0, P0L, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 20 bits\n+  __ vpaddq(P0H, P0H, YTMP1, Assembler::AVX_256bit);\n+  __ vpmadd52luq(P2L, A1, R1, Assembler::AVX_256bit);\n+  __ vpmadd52huq(P2H, A1, R1, Assembler::AVX_256bit);\n+\n+  \/\/ Carry propagation (first pass) - continue 2\n+  __ vpaddq(P1L, P1L, P0H, Assembler::AVX_256bit);\n+  __ vpsllq(P1H, P1H, 8, Assembler::AVX_256bit);\n+  __ vpsrlq(YTMP1, P1L, 44, Assembler::AVX_256bit);\n+  __ vpand(A1, P1L, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 20 bits\n+\n+  __ vpaddq(P2L, P2L, P1H, Assembler::AVX_256bit);\n+  __ vpaddq(P2L, P2L, YTMP1, Assembler::AVX_256bit);\n+  __ vpand(A2, P2L, ExternalAddress(poly1305_mask42()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 22 bits\n+  __ vpsrlq(YTMP1, P2L, 42, Assembler::AVX_256bit);\n+  __ vpsllq(P2H, P2H, 10, Assembler::AVX_256bit);\n+  __ vpaddq(P2H, P2H, YTMP1, Assembler::AVX_256bit);\n+\n+  \/\/ Carry propagation (second pass)\n+  \/\/ Multiply by 5 the highest bits (above 130 bits)\n+  __ vpaddq(A0, A0, P2H, Assembler::AVX_256bit);\n+  __ vpsllq(P2H, P2H, 2, Assembler::AVX_256bit);\n+  __ vpaddq(A0, A0, P2H, Assembler::AVX_256bit);\n+\n+  __ vpsrlq(YTMP1, A0, 44, Assembler::AVX_256bit);\n+  __ vpand(A0, A0, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 20 bits\n+  __ vpaddq(A1, A1, YTMP1, Assembler::AVX_256bit);\n+}\n+\n+\/\/ Compute component-wise product for 4 16-byte message  blocks and adds the next 4 blocks\n+\/\/ i.e. For each block, compute [a2 a1 a0] = [a2 a1 a0] x [r2 r1 r0],\n+\/\/ followed by [a2 a1 a0] += [n2 n1 n0], where n contains the next 4 blocks of the message.\n+\/\/\n+\/\/ Each block\/number is represented by 3 44-bit limb digits, start with multiplication\n+\/\/\n+\/\/      a2       a1       a0\n+\/\/ x    r2       r1       r0\n+\/\/ ----------------------------------\n+\/\/     a2xr0    a1xr0    a0xr0\n+\/\/ +   a1xr1    a0xr1  5xa2xr1'     (r1' = r1<<2)\n+\/\/ +   a0xr2  5xa2xr2' 5xa1xr2'     (r2' = r2<<2)\n+\/\/ ----------------------------------\n+\/\/        p2       p1       p0\n+\/\/\n+void StubGenerator::poly1305_msg_mul_reduce_vec4_avx2(\n+  const XMMRegister A0, const XMMRegister A1, const XMMRegister A2,\n+  const Address R0, const Address R1, const Address R2,\n+  const Address R1P, const Address R2P,\n+  const XMMRegister P0L, const XMMRegister P0H,\n+  const XMMRegister P1L, const XMMRegister P1H,\n+  const XMMRegister P2L, const XMMRegister P2H,\n+  const XMMRegister YTMP1, const XMMRegister YTMP2,\n+  const XMMRegister YTMP3, const XMMRegister YTMP4,\n+  const XMMRegister YTMP5, const XMMRegister YTMP6,\n+  const Register input, const Register length, const Register rscratch)\n+{\n+  \/\/ Reset accumulator\n+  __ vpxor(P0L, P0L, P0L, Assembler::AVX_256bit);\n+  __ vpxor(P0H, P0H, P0H, Assembler::AVX_256bit);\n+  __ vpxor(P1L, P1L, P1L, Assembler::AVX_256bit);\n+  __ vpxor(P1H, P1H, P1H, Assembler::AVX_256bit);\n+  __ vpxor(P2L, P2L, P2L, Assembler::AVX_256bit);\n+  __ vpxor(P2H, P2H, P2H, Assembler::AVX_256bit);\n+\n+  \/\/ Calculate partial products\n+  \/\/ p0 = a2xr1'\n+  \/\/ p1 = a2xr2'\n+  \/\/ p2 = a2xr0\n+      __ vpmadd52luq(P0L, A2, R1P, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P0H, A2, R1P, Assembler::AVX_256bit);\n+  \/\/ Interleave input loading with hash computation\n+  __ lea(input, Address(input,16*4));\n+  __ subl(length, 16*4);\n+      __ vpmadd52luq(P1L, A2, R2P, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P1H, A2, R2P, Assembler::AVX_256bit);\n+  \/\/ Load next block of data (64 bytes)\n+  __ vmovdqu(YTMP1, Address(input, 0));\n+  __ vmovdqu(YTMP2, Address(input, 32));\n+  \/\/ interleave new blocks of data\n+  __ vpunpckhqdq(YTMP3, YTMP1, YTMP2, Assembler::AVX_256bit);\n+  __ vpunpcklqdq(YTMP1, YTMP1, YTMP2, Assembler::AVX_256bit);\n+      __ vpmadd52luq(P0L, A0, R0, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P0H, A0, R0, Assembler::AVX_256bit);\n+  \/\/ Highest 42-bit limbs of new blocks\n+  __ vpsrlq(YTMP6, YTMP3, 24, Assembler::AVX_256bit);\n+  __ vpor(YTMP6, YTMP6, ExternalAddress(poly1305_pad_msg()), Assembler::AVX_256bit, rscratch);\n+\n+  \/\/Middle 44-bit limbs of new blocks\n+  __ vpsrlq(YTMP2, YTMP1, 44, Assembler::AVX_256bit);\n+  __ vpsllq(YTMP4, YTMP3, 20, Assembler::AVX_256bit);\n+      \/\/ p2 = a2xr0\n+      __ vpmadd52luq(P2L, A2, R0, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P2H, A2, R0, Assembler::AVX_256bit);\n+  __ vpor(YTMP2, YTMP2, YTMP4, Assembler::AVX_256bit);\n+  __ vpand(YTMP2, YTMP2, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch);\n+  \/\/ Lowest 44-bit limbs of new blocks\n+  __ vpand(YTMP1, YTMP1, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch);\n+\n+      __ vpmadd52luq(P1L, A0, R1, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P1H, A0, R1, Assembler::AVX_256bit);\n+      __ vpmadd52luq(P0L, A1, R2P, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P0H, A1, R2P, Assembler::AVX_256bit);\n+      __ vpmadd52luq(P2L, A0, R2, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P2H, A0, R2, Assembler::AVX_256bit);\n+\n+  \/\/ Carry propgation (first pass)\n+  __ vpsrlq(YTMP5, P0L, 44, Assembler::AVX_256bit);\n+  __ vpsllq(P0H, P0H, 8, Assembler::AVX_256bit);\n+      __ vpmadd52luq(P1L, A1, R0, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P1H, A1, R0, Assembler::AVX_256bit);\n+  \/\/ Carry propagation (first pass) - continue\n+  __ vpand(A0, P0L, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 20 bits\n+  __ vpaddq(P0H, P0H, YTMP5, Assembler::AVX_256bit);\n+      __ vpmadd52luq(P2L, A1, R1, Assembler::AVX_256bit);\n+      __ vpmadd52huq(P2H, A1, R1, Assembler::AVX_256bit);\n+\n+  \/\/ Carry propagation (first pass) - continue 2\n+  __ vpaddq(P1L, P1L, P0H, Assembler::AVX_256bit);\n+  __ vpsllq(P1H, P1H, 8, Assembler::AVX_256bit);\n+  __ vpsrlq(YTMP5, P1L, 44, Assembler::AVX_256bit);\n+  __ vpand(A1, P1L, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 20 bits\n+\n+  __ vpaddq(P2L, P2L, P1H, Assembler::AVX_256bit);\n+  __ vpaddq(P2L, P2L, YTMP5, Assembler::AVX_256bit);\n+  __ vpand(A2, P2L, ExternalAddress(poly1305_mask42()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 22 bits\n+  __ vpaddq(A2, A2, YTMP6, Assembler::AVX_256bit); \/\/ Add highest bits from new blocks to accumulator\n+  __ vpsrlq(YTMP5, P2L, 42, Assembler::AVX_256bit);\n+  __ vpsllq(P2H, P2H, 10, Assembler::AVX_256bit);\n+  __ vpaddq(P2H, P2H, YTMP5, Assembler::AVX_256bit);\n+\n+  \/\/ Carry propagation (second pass)\n+  \/\/ Multiply by 5 the highest bits (above 130 bits)\n+  __ vpaddq(A0, A0, P2H, Assembler::AVX_256bit);\n+  __ vpsllq(P2H, P2H, 2, Assembler::AVX_256bit);\n+  __ vpaddq(A0, A0, P2H, Assembler::AVX_256bit);\n+\n+  __ vpsrlq(YTMP5, A0, 44, Assembler::AVX_256bit);\n+  __ vpand(A0, A0, ExternalAddress(poly1305_mask44()), Assembler::AVX_256bit, rscratch); \/\/ Clear top 20 bits\n+  __ vpaddq(A0, A0, YTMP1, Assembler::AVX_256bit); \/\/Add low 42-bit bits from new blocks to accumulator\n+  __ vpaddq(A1, A1, YTMP2, Assembler::AVX_256bit); \/\/Add medium 42-bit bits from new blocks to accumulator\n+  __ vpaddq(A1, A1, YTMP5, Assembler::AVX_256bit);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_poly.cpp","additions":676,"deletions":4,"binary":false,"changes":680,"status":"modified"},{"patch":"@@ -297,1 +297,1 @@\n-\n+    \/\/ ECX = 0\n@@ -306,0 +306,10 @@\n+    \/\/ ECX = 1\n+    __ movl(rax, 7);\n+    __ movl(rcx, 1);\n+    __ cpuid();\n+    __ lea(rsi, Address(rbp, in_bytes(VM_Version::sef_cpuid7_ecx1_offset())));\n+    __ movl(Address(rsi, 0), rax);\n+    __ movl(Address(rsi, 4), rbx);\n+    __ movl(Address(rsi, 8), rcx);\n+    __ movl(Address(rsi, 12), rdx);\n+\n@@ -961,1 +971,1 @@\n-  if (UseAVX < 2)\n+  if (UseAVX < 2) {\n@@ -963,0 +973,2 @@\n+    _features &= ~CPU_AVX_IFMA;\n+  }\n@@ -992,0 +1004,1 @@\n+      _features &= ~CPU_AVX_IFMA;\n@@ -1348,1 +1361,1 @@\n-  if (supports_avx512ifma() && supports_avx512vlbw() && MaxVectorSize >= 64) {\n+  if ((supports_avx512ifma() && supports_avx512vlbw()) || supports_avxifma())  {\n@@ -2939,1 +2952,1 @@\n-    if (sef_cpuid7_ebx.bits.avx2 != 0)\n+    if (sef_cpuid7_ebx.bits.avx2 != 0) {\n@@ -2941,0 +2954,3 @@\n+      if (sef_cpuid7_ecx1_eax.bits.avx_ifma != 0)\n+        result |= CPU_AVX_IFMA;\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -282,0 +282,9 @@\n+  union SefCpuid7Ecx1Eax {\n+    uint32_t value;\n+    struct {\n+      uint32_t             : 23,\n+                  avx_ifma : 1,\n+                           : 8;\n+    } bits;\n+  };\n+\n@@ -393,1 +402,2 @@\n-    decl(AVX512_IFMA,       \"avx512_ifma\",       58) \/* Integer Vector FMA instructions*\/\n+    decl(AVX512_IFMA,       \"avx512_ifma\",       58) \/* Integer Vector FMA instructions*\/ \\\n+    decl(AVX_IFMA,          \"avx_ifma\",          59) \/* 256-bit VEX-coded variant of AVX512-IFMA*\/\n@@ -452,0 +462,1 @@\n+    \/\/ ECX = 0 before calling cpuid()\n@@ -456,0 +467,2 @@\n+    \/\/ ECX = 1 before calling cpuid()\n+    SefCpuid7Ecx1Eax sef_cpuid7_ecx1_eax;\n@@ -574,0 +587,1 @@\n+  static ByteSize sef_cpuid7_ecx1_offset() { return byte_offset_of(CpuidInfo, sef_cpuid7_ecx1_eax); }\n@@ -680,0 +694,1 @@\n+  static bool supports_avxifma()      { return (_features & CPU_AVX_IFMA) != 0; }\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -235,0 +235,1 @@\n+        AVX_IFMA,\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/amd64\/AMD64.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -68,1 +68,1 @@\n-                    \"cet_ss\",       \"avx512_ifma\",      \"serialize\"\n+                    \"cet_ss\",       \"avx512_ifma\",      \"serialize\",         \"avx_ifma\"\n","filename":"test\/lib-test\/jdk\/test\/whitebox\/CPUInfoTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}