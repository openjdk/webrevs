{"files":[{"patch":"@@ -532,0 +532,49 @@\n+void G1BarrierSetC2::elide_dominated_barrier(MachNode* mach) const {\n+  uint8_t barrier_data = mach->barrier_data();\n+  barrier_data &= ~G1C2BarrierPre;\n+  if (CardTableBarrierSetC2::use_ReduceInitialCardMarks()) {\n+    barrier_data &= ~G1C2BarrierPost;\n+    barrier_data &= ~G1C2BarrierPostNotNull;\n+  }\n+  mach->set_barrier_data(barrier_data);\n+}\n+\n+void G1BarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  PhaseCFG* const cfg = Compile::current()->cfg();\n+\n+  \/\/ Find stores and allocations, and track them in lists.\n+  Node_List stores;\n+  Node_List allocations;\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      Node* const node = block->get_node(j);\n+      if (node->is_Phi()) {\n+        if (BarrierSetC2::is_allocation(node)) {\n+          allocations.push(node);\n+        }\n+        continue;\n+      } else if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_StoreP:\n+      case Op_StoreN:\n+        if (mach->barrier_data() != 0) {\n+          stores.push(mach);\n+        }\n+        break;\n+      default:\n+        break;\n+      }\n+    }\n+  }\n+\n+  \/\/ Find dominating allocations for each store and elide barriers if there is\n+  \/\/ no safepoint poll in between.\n+  elide_dominated_barriers(stores, allocations);\n+}\n+\n@@ -534,0 +583,1 @@\n+  analyze_dominating_barriers();\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -89,0 +89,3 @@\n+private:\n+  void analyze_dominating_barriers() const;\n+\n@@ -120,0 +123,1 @@\n+  virtual void elide_dominated_barrier(MachNode* mach) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -899,0 +899,257 @@\n+static bool block_has_safepoint(const Block* block, uint from, uint to) {\n+  for (uint i = from; i < to; i++) {\n+    if (block->get_node(i)->is_MachSafePoint()) {\n+      \/\/ Safepoint found\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Safepoint not found\n+  return false;\n+}\n+\n+static bool block_has_safepoint(const Block* block) {\n+  return block_has_safepoint(block, 0, block->number_of_nodes());\n+}\n+\n+static uint block_index(const Block* block, const Node* node) {\n+  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+    if (block->get_node(j) == node) {\n+      return j;\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+\/\/ Look through various node aliases\n+static const Node* look_through_node(const Node* node) {\n+  while (node != nullptr) {\n+    const Node* new_node = node;\n+    if (node->is_Mach()) {\n+      const MachNode* const node_mach = node->as_Mach();\n+      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n+        new_node = node->in(1);\n+      }\n+      if (node_mach->is_SpillCopy()) {\n+        new_node = node->in(1);\n+      }\n+    }\n+    if (new_node == node || new_node == nullptr) {\n+      break;\n+    } else {\n+      node = new_node;\n+    }\n+  }\n+\n+  return node;\n+}\n+\n+\/\/ Whether the given offset is undefined.\n+static bool is_undefined(intptr_t offset) {\n+  return offset == Type::OffsetTop;\n+}\n+\n+\/\/ Whether the given offset is unknown.\n+static bool is_unknown(intptr_t offset) {\n+  return offset == Type::OffsetBot;\n+}\n+\n+\/\/ Whether the given offset is concrete (defined and compile-time known).\n+static bool is_concrete(intptr_t offset) {\n+  return !is_undefined(offset) && !is_unknown(offset);\n+}\n+\n+\/\/ Compute base + offset components of the memory address accessed by mach.\n+\/\/ Return a node representing the base address, or null if the base cannot be\n+\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n+\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n+static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n+  const TypePtr* adr_type = nullptr;\n+  offset = 0;\n+  const Node* base = mach->get_base_and_disp(offset, adr_type);\n+\n+  if (base == nullptr || base == NodeSentinel) {\n+    return nullptr;\n+  }\n+\n+  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n+    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n+    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n+    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n+    const TypeOopPtr* oopptr = base->bottom_type()->isa_oopptr();\n+    if (oopptr == nullptr) return nullptr;\n+    offset = oopptr->offset();\n+    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n+    \/\/ guarantees that the base address is still available at the same slot.\n+    base = base->in(AddPNode::Base);\n+    assert(base != nullptr, \"\");\n+  }\n+\n+  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n+    return nullptr;\n+  }\n+\n+  return look_through_node(base);\n+}\n+\n+\/\/ Whether a phi node corresponds to an array allocation.\n+\/\/ This test is incomplete: in some edge cases, it might return false even\n+\/\/ though the node does correspond to an array allocation.\n+static bool is_array_allocation(const Node* phi) {\n+  precond(phi->is_Phi());\n+  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n+  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n+  \/\/ a single path from the phi node consisting of these node types.\n+  const Node* current = phi;\n+  while (true) {\n+    const Node* next = nullptr;\n+    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n+      if (!current->fast_out(i)->isa_Mach()) {\n+        continue;\n+      }\n+      const MachNode* succ = current->fast_out(i)->as_Mach();\n+      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n+        if (succ->get_ptr_type()->isa_aryptr()) {\n+          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n+          return true;\n+        }\n+        \/\/ Other cast: record as candidate for further exploration.\n+        next = succ;\n+      } else if (succ->is_SpillCopy() && next == nullptr) {\n+        \/\/ Spill copy, and no better candidate found: record as candidate.\n+        next = succ;\n+      }\n+    }\n+    if (next == nullptr) {\n+      \/\/ No evidence found that phi corresponds to an array allocation, and no\n+      \/\/ candidates available to continue exploring.\n+      return false;\n+    }\n+    \/\/ Continue exploring from the best candidate found.\n+    current = next;\n+  }\n+  ShouldNotReachHere();\n+}\n+\n+bool BarrierSetC2::is_allocation(const Node* node) {\n+  assert(node->is_Phi(), \"expected phi node\");\n+  if (node->req() != 3) {\n+    return false;\n+  }\n+  const Node* const fast_node = node->in(2);\n+  if (!fast_node->is_Mach()) {\n+    return false;\n+  }\n+  const MachNode* const fast_mach = fast_node->as_Mach();\n+  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n+    return false;\n+  }\n+  const TypePtr* const adr_type = nullptr;\n+  intptr_t offset;\n+  const Node* const base = get_base_and_offset(fast_mach, offset);\n+  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n+    return false;\n+  }\n+  const MachNode* const base_mach = base->as_Mach();\n+  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n+    return false;\n+  }\n+  return offset == in_bytes(Thread::tlab_top_offset());\n+}\n+\n+void BarrierSetC2::elide_dominated_barriers(Node_List& accesses, Node_List& access_dominators) const {\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  for (uint i = 0; i < accesses.size(); i++) {\n+    MachNode* const access = accesses.at(i)->as_Mach();\n+    intptr_t access_offset;\n+    const Node* const access_obj = get_base_and_offset(access, access_offset);\n+    Block* const access_block = cfg->get_block_for_node(access);\n+    const uint access_index = block_index(access_block, access);\n+\n+    if (access_obj == nullptr) {\n+      \/\/ No information available\n+      continue;\n+    }\n+\n+    for (uint j = 0; j < access_dominators.size(); j++) {\n+     const  Node* const mem = access_dominators.at(j);\n+      if (mem->is_Phi()) {\n+        assert(is_allocation(mem), \"expected allocation phi node\");\n+        if (mem != access_obj) {\n+          continue;\n+        }\n+        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n+          \/\/ The accessed address has an unknown offset, but the allocated\n+          \/\/ object cannot be determined to be an array. Avoid eliding in this\n+          \/\/ case, to be on the safe side.\n+          continue;\n+        }\n+        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n+               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n+      } else {\n+        \/\/ Access node\n+        const MachNode* const mem_mach = mem->as_Mach();\n+        intptr_t mem_offset;\n+        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n+\n+        if (mem_obj == nullptr ||\n+            !is_concrete(access_offset) ||\n+            !is_concrete(mem_offset)) {\n+          \/\/ No information available\n+          continue;\n+        }\n+\n+        if (mem_obj != access_obj || mem_offset != access_offset) {\n+          \/\/ Not the same addresses, not a candidate\n+          continue;\n+        }\n+        assert(is_concrete(access_offset) && access_offset >= 0,\n+               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n+      }\n+\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      const uint mem_index = block_index(mem_block, mem);\n+\n+      if (access_block == mem_block) {\n+        \/\/ Earlier accesses in the same block\n+        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n+          elide_dominated_barrier(access);\n+        }\n+      } else if (mem_block->dominates(access_block)) {\n+        \/\/ Dominating block? Look around for safepoints\n+        ResourceMark rm;\n+        Block_List stack;\n+        VectorSet visited;\n+        stack.push(access_block);\n+        bool safepoint_found = block_has_safepoint(access_block);\n+        while (!safepoint_found && stack.size() > 0) {\n+          const Block* const block = stack.pop();\n+          if (visited.test_set(block->_pre_order)) {\n+            continue;\n+          }\n+          if (block_has_safepoint(block)) {\n+            safepoint_found = true;\n+            break;\n+          }\n+          if (block == mem_block) {\n+            continue;\n+          }\n+\n+          \/\/ Push predecessor blocks\n+          for (uint p = 1; p < block->num_preds(); ++p) {\n+            Block* const pred = cfg->get_block_for_node(block->pred(p));\n+            stack.push(pred);\n+          }\n+        }\n+\n+        if (!safepoint_found) {\n+          elide_dominated_barrier(access);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":257,"deletions":0,"binary":false,"changes":257,"status":"modified"},{"patch":"@@ -367,0 +367,8 @@\n+  \/\/ Whether the given phi node joins OOPs from fast and slow allocation paths.\n+  static bool is_allocation(const Node* node);\n+  \/\/ Elide GC barriers from a Mach node according to elide_dominated_barriers().\n+  virtual void elide_dominated_barrier(MachNode* mach) const { }\n+  \/\/ Elide GC barriers from instructions in 'accesses' if they are dominated by\n+  \/\/ instructions in 'access_dominators' (according to elide_mach_barrier()) and\n+  \/\/ there is no safepoint poll in between.\n+  void elide_dominated_barriers(Node_List& accesses, Node_List& access_dominators) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-bool CardTableBarrierSetC2::use_ReduceInitialCardMarks() const {\n+bool CardTableBarrierSetC2::use_ReduceInitialCardMarks() {\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  bool use_ReduceInitialCardMarks() const;\n+  static bool use_ReduceInitialCardMarks();\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/cardTableBarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"opto\/addnode.hpp\"\n@@ -41,1 +40,0 @@\n-#include \"opto\/rootnode.hpp\"\n@@ -478,165 +476,1 @@\n-\/\/ == Dominating barrier elision ==\n-\n-static bool block_has_safepoint(const Block* block, uint from, uint to) {\n-  for (uint i = from; i < to; i++) {\n-    if (block->get_node(i)->is_MachSafePoint()) {\n-      \/\/ Safepoint found\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Safepoint not found\n-  return false;\n-}\n-\n-static bool block_has_safepoint(const Block* block) {\n-  return block_has_safepoint(block, 0, block->number_of_nodes());\n-}\n-\n-static uint block_index(const Block* block, const Node* node) {\n-  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-    if (block->get_node(j) == node) {\n-      return j;\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return 0;\n-}\n-\n-\/\/ Look through various node aliases\n-static const Node* look_through_node(const Node* node) {\n-  while (node != nullptr) {\n-    const Node* new_node = node;\n-    if (node->is_Mach()) {\n-      const MachNode* const node_mach = node->as_Mach();\n-      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n-        new_node = node->in(1);\n-      }\n-      if (node_mach->is_SpillCopy()) {\n-        new_node = node->in(1);\n-      }\n-    }\n-    if (new_node == node || new_node == nullptr) {\n-      break;\n-    } else {\n-      node = new_node;\n-    }\n-  }\n-\n-  return node;\n-}\n-\n-\/\/ Whether the given offset is undefined.\n-static bool is_undefined(intptr_t offset) {\n-  return offset == Type::OffsetTop;\n-}\n-\n-\/\/ Whether the given offset is unknown.\n-static bool is_unknown(intptr_t offset) {\n-  return offset == Type::OffsetBot;\n-}\n-\n-\/\/ Whether the given offset is concrete (defined and compile-time known).\n-static bool is_concrete(intptr_t offset) {\n-  return !is_undefined(offset) && !is_unknown(offset);\n-}\n-\n-\/\/ Compute base + offset components of the memory address accessed by mach.\n-\/\/ Return a node representing the base address, or null if the base cannot be\n-\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n-\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n-static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n-  const TypePtr* adr_type = nullptr;\n-  offset = 0;\n-  const Node* base = mach->get_base_and_disp(offset, adr_type);\n-\n-  if (base == nullptr || base == NodeSentinel) {\n-    return nullptr;\n-  }\n-\n-  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n-    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n-    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n-    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n-    const TypeOopPtr* oopptr = base->bottom_type()->isa_oopptr();\n-    if (oopptr == nullptr) return nullptr;\n-    offset = oopptr->offset();\n-    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n-    \/\/ guarantees that the base address is still available at the same slot.\n-    base = base->in(AddPNode::Base);\n-    assert(base != nullptr, \"\");\n-  }\n-\n-  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n-    return nullptr;\n-  }\n-\n-  return look_through_node(base);\n-}\n-\n-\/\/ Whether a phi node corresponds to an array allocation.\n-\/\/ This test is incomplete: in some edge cases, it might return false even\n-\/\/ though the node does correspond to an array allocation.\n-static bool is_array_allocation(const Node* phi) {\n-  precond(phi->is_Phi());\n-  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n-  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n-  \/\/ a single path from the phi node consisting of these node types.\n-  const Node* current = phi;\n-  while (true) {\n-    const Node* next = nullptr;\n-    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n-      if (!current->fast_out(i)->isa_Mach()) {\n-        continue;\n-      }\n-      const MachNode* succ = current->fast_out(i)->as_Mach();\n-      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n-        if (succ->get_ptr_type()->isa_aryptr()) {\n-          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n-          return true;\n-        }\n-        \/\/ Other cast: record as candidate for further exploration.\n-        next = succ;\n-      } else if (succ->is_SpillCopy() && next == nullptr) {\n-        \/\/ Spill copy, and no better candidate found: record as candidate.\n-        next = succ;\n-      }\n-    }\n-    if (next == nullptr) {\n-      \/\/ No evidence found that phi corresponds to an array allocation, and no\n-      \/\/ candidates available to continue exploring.\n-      return false;\n-    }\n-    \/\/ Continue exploring from the best candidate found.\n-    current = next;\n-  }\n-  ShouldNotReachHere();\n-}\n-\n-\/\/ Match the phi node that connects a TLAB allocation fast path with its slowpath\n-static bool is_allocation(const Node* node) {\n-  if (node->req() != 3) {\n-    return false;\n-  }\n-  const Node* const fast_node = node->in(2);\n-  if (!fast_node->is_Mach()) {\n-    return false;\n-  }\n-  const MachNode* const fast_mach = fast_node->as_Mach();\n-  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n-    return false;\n-  }\n-  const TypePtr* const adr_type = nullptr;\n-  intptr_t offset;\n-  const Node* const base = get_base_and_offset(fast_mach, offset);\n-  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n-    return false;\n-  }\n-  const MachNode* const base_mach = base->as_Mach();\n-  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n-    return false;\n-  }\n-  return offset == in_bytes(Thread::tlab_top_offset());\n-}\n-\n-static void elide_mach_barrier(MachNode* mach) {\n+void ZBarrierSetC2::elide_dominated_barrier(MachNode* mach) const {\n@@ -646,95 +480,0 @@\n-void ZBarrierSetC2::analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const {\n-  Compile* const C = Compile::current();\n-  PhaseCFG* const cfg = C->cfg();\n-\n-  for (uint i = 0; i < accesses.size(); i++) {\n-    MachNode* const access = accesses.at(i)->as_Mach();\n-    intptr_t access_offset;\n-    const Node* const access_obj = get_base_and_offset(access, access_offset);\n-    Block* const access_block = cfg->get_block_for_node(access);\n-    const uint access_index = block_index(access_block, access);\n-\n-    if (access_obj == nullptr) {\n-      \/\/ No information available\n-      continue;\n-    }\n-\n-    for (uint j = 0; j < access_dominators.size(); j++) {\n-     const  Node* const mem = access_dominators.at(j);\n-      if (mem->is_Phi()) {\n-        \/\/ Allocation node\n-        if (mem != access_obj) {\n-          continue;\n-        }\n-        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n-          \/\/ The accessed address has an unknown offset, but the allocated\n-          \/\/ object cannot be determined to be an array. Avoid eliding in this\n-          \/\/ case, to be on the safe side.\n-          continue;\n-        }\n-        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n-               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n-      } else {\n-        \/\/ Access node\n-        const MachNode* const mem_mach = mem->as_Mach();\n-        intptr_t mem_offset;\n-        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n-\n-        if (mem_obj == nullptr ||\n-            !is_concrete(access_offset) ||\n-            !is_concrete(mem_offset)) {\n-          \/\/ No information available\n-          continue;\n-        }\n-\n-        if (mem_obj != access_obj || mem_offset != access_offset) {\n-          \/\/ Not the same addresses, not a candidate\n-          continue;\n-        }\n-        assert(is_concrete(access_offset) && access_offset >= 0,\n-               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n-      }\n-\n-      Block* mem_block = cfg->get_block_for_node(mem);\n-      const uint mem_index = block_index(mem_block, mem);\n-\n-      if (access_block == mem_block) {\n-        \/\/ Earlier accesses in the same block\n-        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n-          elide_mach_barrier(access);\n-        }\n-      } else if (mem_block->dominates(access_block)) {\n-        \/\/ Dominating block? Look around for safepoints\n-        ResourceMark rm;\n-        Block_List stack;\n-        VectorSet visited;\n-        stack.push(access_block);\n-        bool safepoint_found = block_has_safepoint(access_block);\n-        while (!safepoint_found && stack.size() > 0) {\n-          const Block* const block = stack.pop();\n-          if (visited.test_set(block->_pre_order)) {\n-            continue;\n-          }\n-          if (block_has_safepoint(block)) {\n-            safepoint_found = true;\n-            break;\n-          }\n-          if (block == mem_block) {\n-            continue;\n-          }\n-\n-          \/\/ Push predecessor blocks\n-          for (uint p = 1; p < block->num_preds(); ++p) {\n-            Block* const pred = cfg->get_block_for_node(block->pred(p));\n-            stack.push(pred);\n-          }\n-        }\n-\n-        if (!safepoint_found) {\n-          elide_mach_barrier(access);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n@@ -810,3 +549,3 @@\n-  analyze_dominating_barriers_impl(loads, load_dominators);\n-  analyze_dominating_barriers_impl(stores, store_dominators);\n-  analyze_dominating_barriers_impl(atomics, atomic_dominators);\n+  elide_dominated_barriers(loads, load_dominators);\n+  elide_dominated_barriers(stores, store_dominators);\n+  elide_dominated_barriers(atomics, atomic_dominators);\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":4,"deletions":265,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -102,1 +102,0 @@\n-  void analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const;\n@@ -131,0 +130,1 @@\n+  virtual void elide_dominated_barrier(MachNode* mach) const;\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+    static final String ANY = \".*\";\n@@ -93,0 +94,3 @@\n+    @DontInline\n+    static void nonInlinedMethod() {}\n+\n@@ -197,1 +201,1 @@\n-        failOn = {IRNode.G1_STORE_P},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n@@ -200,1 +204,2 @@\n-        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        failOn = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n@@ -225,1 +230,1 @@\n-        failOn = {IRNode.G1_STORE_P},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n@@ -228,1 +233,2 @@\n-        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        failOn = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n@@ -247,1 +253,1 @@\n-        failOn = {IRNode.G1_STORE_P},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n@@ -250,1 +256,2 @@\n-        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        failOn = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n@@ -264,0 +271,57 @@\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreConditionallyOnNewObject(Object o1, boolean c) {\n+        Outer o = new Outer();\n+        if (c) {\n+            o.f = o1;\n+        }\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreOnNewObjectAfterException(Object o1, boolean c) throws Exception {\n+        Outer o = new Outer();\n+        if (c) {\n+            throw new Exception(\"\");\n+        }\n+        o.f = o1;\n+        return o;\n+    }\n+\n+    @Test\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Outer testStoreOnNewObjectAfterCall(Object o1) {\n+        Outer o = new Outer();\n+        nonInlinedMethod();\n+        o.f = o1;\n+        return o;\n+    }\n+\n@@ -273,1 +337,4 @@\n-                 \"testStoreOnNewObjectInTwoPaths\"})\n+                 \"testStoreOnNewObjectInTwoPaths\",\n+                 \"testStoreConditionallyOnNewObject\",\n+                 \"testStoreOnNewObjectAfterException\",\n+                 \"testStoreOnNewObjectAfterCall\"})\n@@ -331,0 +398,18 @@\n+        {\n+            Object o1 = new Object();\n+            boolean c = ThreadLocalRandom.current().nextBoolean();\n+            Outer o = testStoreConditionallyOnNewObject(o1, c);\n+            Asserts.assertTrue(o.f == (c ? o1 : null));\n+        }\n+        {\n+            Object o1 = new Object();\n+            boolean c = ThreadLocalRandom.current().nextBoolean();\n+            try {\n+                Outer o = testStoreOnNewObjectAfterException(o1, c);\n+            } catch (Exception e) {}\n+        }\n+        {\n+            Object o1 = new Object();\n+            Outer o = testStoreOnNewObjectAfterCall(o1);\n+            Asserts.assertEquals(o1, o.f);\n+        }\n@@ -382,0 +467,6 @@\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n@@ -383,1 +474,1 @@\n-        failOn = {IRNode.G1_STORE_P},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n@@ -386,1 +477,2 @@\n-        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        failOn = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n@@ -388,1 +480,1 @@\n-    public static Object[] testStoreOnNewArray(Object o1) {\n+    public static Object[] testStoreOnNewArrayAtKnownIndex(Object o1) {\n@@ -390,2 +482,0 @@\n-        \/\/ The index needs to be concrete for C2 to detect that it is safe to\n-        \/\/ remove the pre-barrier.\n@@ -396,0 +486,58 @@\n+    @Test\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Object[] testStoreOnNewArrayAtUnknownIndex(Object o1, int index) {\n+        Object[] a = new Object[10];\n+        a[index] = o1;\n+        return a;\n+    }\n+\n+    @Test\n+    @IR(failOn = IRNode.SAFEPOINT)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"false\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, POST_ONLY, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"false\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIfAnd = {\"UseCompressedOops\", \"true\", \"ReduceInitialCardMarks\", \"true\"},\n+        failOn = {IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Object[] testStoreAllOnNewSmallArray(Object o1) {\n+        Object[] a = new Object[64];\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = o1;\n+        }\n+        return a;\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.SAFEPOINT, \"1\"})\n+    @IR(applyIf = {\"UseCompressedOops\", \"false\"},\n+        counts = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    @IR(applyIf = {\"UseCompressedOops\", \"true\"},\n+        counts = {IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, PRE_AND_POST, \"1\"},\n+        phase = CompilePhase.FINAL_CODE)\n+    public static Object[] testStoreAllOnNewLargeArray(Object o1) {\n+        Object[] a = new Object[1024];\n+        for (int i = 0; i < a.length; i++) {\n+            a[i] = o1;\n+        }\n+        return a;\n+    }\n+\n@@ -400,1 +548,4 @@\n-                 \"testStoreOnNewArray\"})\n+                 \"testStoreOnNewArrayAtKnownIndex\",\n+                 \"testStoreOnNewArrayAtUnknownIndex\",\n+                 \"testStoreAllOnNewSmallArray\",\n+                 \"testStoreAllOnNewLargeArray\"})\n@@ -429,1 +580,1 @@\n-            Object[] a = testStoreOnNewArray(o1);\n+            Object[] a = testStoreOnNewArrayAtKnownIndex(o1);\n@@ -432,0 +583,19 @@\n+        {\n+            Object o1 = new Object();\n+            Object[] a = testStoreOnNewArrayAtUnknownIndex(o1, 5);\n+            Asserts.assertEquals(o1, a[5]);\n+        }\n+        {\n+            Object o1 = new Object();\n+            Object[] a = testStoreAllOnNewSmallArray(o1);\n+            for (int i = 0; i < a.length; i++) {\n+                Asserts.assertEquals(o1, a[i]);\n+            }\n+        }\n+        {\n+            Object o1 = new Object();\n+            Object[] a = testStoreAllOnNewLargeArray(o1);\n+            for (int i = 0; i < a.length; i++) {\n+                Asserts.assertEquals(o1, a[i]);\n+            }\n+        }\n@@ -445,1 +615,3 @@\n-        failOn = {IRNode.G1_STORE_P, IRNode.G1_STORE_N, IRNode.G1_ENCODE_P_AND_STORE_N},\n+        failOn = {IRNode.G1_STORE_P_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_STORE_N_WITH_BARRIER_FLAG, ANY,\n+                  IRNode.G1_ENCODE_P_AND_STORE_N_WITH_BARRIER_FLAG, ANY},\n","filename":"test\/hotspot\/jtreg\/compiler\/gcbarriers\/TestG1BarrierGeneration.java","additions":187,"deletions":15,"binary":false,"changes":202,"status":"modified"}]}