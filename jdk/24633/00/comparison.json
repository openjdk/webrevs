{"files":[{"patch":"@@ -85,2 +85,2 @@\n-#define PUSH { __ push(temp); LP64_ONLY(  __ push(rscratch1); )               }\n-#define POP  {                LP64_ONLY(  __ pop(rscratch1);  ) __ pop(temp); }\n+#define PUSH { __ push(temp); __ push(rscratch1);               }\n+#define POP  {                __ pop(rscratch1);  __ pop(temp); }\n@@ -142,6 +142,0 @@\n-#ifdef _LP64\n-    Register rthread = r15_thread;\n-#else\n-    Register rthread = temp;\n-    __ get_thread(rthread);\n-#endif\n@@ -150,1 +144,1 @@\n-    __ cmpb(Address(rthread, JavaThread::interp_only_mode_offset()), 0);\n+    __ cmpb(Address(r15_thread, JavaThread::interp_only_mode_offset()), 0);\n@@ -327,1 +321,0 @@\n-#ifdef _LP64\n@@ -336,13 +329,1 @@\n-  }\n-#else\n-  Register temp1 = (for_compiler_entry ? rsi : rdx);\n-  Register temp2 = rdi;\n-  Register temp3 = rax;\n-  if (for_compiler_entry) {\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n-    assert_different_registers(temp1,        rcx, rdx);\n-    assert_different_registers(temp2,        rcx, rdx);\n-    assert_different_registers(temp3,        rcx, rdx);\n-  }\n-#endif\n-  else {\n+  } else {\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":4,"deletions":23,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-    return LP64_ONLY(r13) NOT_LP64(rsi);\n+    return r13;\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -76,1 +76,0 @@\n-#ifdef _LP64\n@@ -82,3 +81,0 @@\n-#else\n-  __ andptr(result, markWord::hash_mask_in_place);\n-#endif \/\/_LP64\n@@ -87,4 +83,1 @@\n-  __ jcc(Assembler::zero, slowCase);\n-#ifndef _LP64\n-  __ shrptr(result, markWord::hash_shift);\n-#endif\n+  __ jccb(Assembler::zero, slowCase);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,26 +37,24 @@\n-  LP64_ONLY(                                                            \\\n-    do_stub(initial, get_previous_sp)                                   \\\n-    do_arch_entry(x86, initial, get_previous_sp,                        \\\n-                  get_previous_sp_entry,                                \\\n-                  get_previous_sp_entry)                                \\\n-    do_stub(initial, f2i_fixup)                                         \\\n-    do_arch_entry(x86, initial, f2i_fixup, f2i_fixup, f2i_fixup)        \\\n-    do_stub(initial, f2l_fixup)                                         \\\n-    do_arch_entry(x86, initial, f2l_fixup, f2l_fixup, f2l_fixup)        \\\n-    do_stub(initial, d2i_fixup)                                         \\\n-    do_arch_entry(x86, initial, d2i_fixup, d2i_fixup, d2i_fixup)        \\\n-    do_stub(initial, d2l_fixup)                                         \\\n-    do_arch_entry(x86, initial, d2l_fixup, d2l_fixup, d2l_fixup)        \\\n-    do_stub(initial, float_sign_mask)                                   \\\n-    do_arch_entry(x86, initial, float_sign_mask, float_sign_mask,       \\\n-                  float_sign_mask)                                      \\\n-    do_stub(initial, float_sign_flip)                                   \\\n-    do_arch_entry(x86, initial, float_sign_flip, float_sign_flip,       \\\n-                  float_sign_flip)                                      \\\n-    do_stub(initial, double_sign_mask)                                  \\\n-    do_arch_entry(x86, initial, double_sign_mask, double_sign_mask,     \\\n-                  double_sign_mask)                                     \\\n-    do_stub(initial, double_sign_flip)                                  \\\n-    do_arch_entry(x86, initial, double_sign_flip, double_sign_flip,     \\\n-                  double_sign_flip)                                     \\\n-  )                                                                     \\\n+  do_stub(initial, get_previous_sp)                                     \\\n+  do_arch_entry(x86, initial, get_previous_sp,                          \\\n+                get_previous_sp_entry,                                  \\\n+                get_previous_sp_entry)                                  \\\n+  do_stub(initial, f2i_fixup)                                           \\\n+  do_arch_entry(x86, initial, f2i_fixup, f2i_fixup, f2i_fixup)          \\\n+  do_stub(initial, f2l_fixup)                                           \\\n+  do_arch_entry(x86, initial, f2l_fixup, f2l_fixup, f2l_fixup)          \\\n+  do_stub(initial, d2i_fixup)                                           \\\n+  do_arch_entry(x86, initial, d2i_fixup, d2i_fixup, d2i_fixup)          \\\n+  do_stub(initial, d2l_fixup)                                           \\\n+  do_arch_entry(x86, initial, d2l_fixup, d2l_fixup, d2l_fixup)          \\\n+  do_stub(initial, float_sign_mask)                                     \\\n+  do_arch_entry(x86, initial, float_sign_mask, float_sign_mask,         \\\n+                float_sign_mask)                                        \\\n+  do_stub(initial, float_sign_flip)                                     \\\n+  do_arch_entry(x86, initial, float_sign_flip, float_sign_flip,         \\\n+                float_sign_flip)                                        \\\n+  do_stub(initial, double_sign_mask)                                    \\\n+  do_arch_entry(x86, initial, double_sign_mask, double_sign_mask,       \\\n+                double_sign_mask)                                       \\\n+  do_stub(initial, double_sign_flip)                                    \\\n+  do_arch_entry(x86, initial, double_sign_flip, double_sign_flip,       \\\n+                double_sign_flip)                                       \\\n@@ -68,1 +66,1 @@\n-  do_arch_blob(continuation, 1000 LP64_ONLY(+2000))                     \\\n+  do_arch_blob(continuation, 3000)                                      \\\n@@ -75,1 +73,1 @@\n-  do_arch_blob(compiler, 20000 LP64_ONLY(+89000) WINDOWS_ONLY(+2000))   \\\n+  do_arch_blob(compiler, 109000 WINDOWS_ONLY(+2000))                    \\\n@@ -163,77 +161,75 @@\n-  LP64_ONLY(                                                            \\\n-    \/* x86_64 exposes these 3 stubs via a generic entry array *\/        \\\n-    \/* oher arches use arch-specific entries *\/                         \\\n-    \/* this really needs rationalising *\/                               \\\n-    do_stub(compiler, string_indexof_linear_ll)                         \\\n-    do_stub(compiler, string_indexof_linear_uu)                         \\\n-    do_stub(compiler, string_indexof_linear_ul)                         \\\n-    do_stub(compiler, pshuffle_byte_flip_mask_sha512)                   \\\n-    do_arch_entry(x86, compiler, pshuffle_byte_flip_mask_sha512,        \\\n-                  pshuffle_byte_flip_mask_addr_sha512,                  \\\n-                  pshuffle_byte_flip_mask_addr_sha512)                  \\\n-    do_stub(compiler, compress_perm_table32)                            \\\n-    do_arch_entry(x86, compiler, compress_perm_table32,                 \\\n-                  compress_perm_table32, compress_perm_table32)         \\\n-    do_stub(compiler, compress_perm_table64)                            \\\n-    do_arch_entry(x86, compiler, compress_perm_table64,                 \\\n-                  compress_perm_table64, compress_perm_table64)         \\\n-    do_stub(compiler, expand_perm_table32)                              \\\n-    do_arch_entry(x86, compiler, expand_perm_table32,                   \\\n-                  expand_perm_table32, expand_perm_table32)             \\\n-    do_stub(compiler, expand_perm_table64)                              \\\n-    do_arch_entry(x86, compiler, expand_perm_table64,                   \\\n-                  expand_perm_table64, expand_perm_table64)             \\\n-    do_stub(compiler, avx2_shuffle_base64)                              \\\n-    do_arch_entry(x86, compiler, avx2_shuffle_base64,                   \\\n-                  avx2_shuffle_base64, base64_avx2_shuffle_addr)        \\\n-    do_stub(compiler, avx2_input_mask_base64)                           \\\n-    do_arch_entry(x86, compiler, avx2_input_mask_base64,                \\\n-                  avx2_input_mask_base64,                               \\\n-                  base64_avx2_input_mask_addr)                          \\\n-    do_stub(compiler, avx2_lut_base64)                                  \\\n-    do_arch_entry(x86, compiler, avx2_lut_base64,                       \\\n-                  avx2_lut_base64, base64_avx2_lut_addr)                \\\n-    do_stub(compiler, avx2_decode_tables_base64)                        \\\n-    do_arch_entry(x86, compiler, avx2_decode_tables_base64,             \\\n-                  avx2_decode_tables_base64,                            \\\n-                  base64_AVX2_decode_tables_addr)                       \\\n-    do_stub(compiler, avx2_decode_lut_tables_base64)                    \\\n-    do_arch_entry(x86, compiler, avx2_decode_lut_tables_base64,         \\\n-                  avx2_decode_lut_tables_base64,                        \\\n-                  base64_AVX2_decode_LUT_tables_addr)                   \\\n-    do_stub(compiler, shuffle_base64)                                   \\\n-    do_arch_entry(x86, compiler, shuffle_base64, shuffle_base64,        \\\n-                  base64_shuffle_addr)                                  \\\n-    do_stub(compiler, lookup_lo_base64)                                 \\\n-    do_arch_entry(x86, compiler, lookup_lo_base64, lookup_lo_base64,    \\\n-                  base64_vbmi_lookup_lo_addr)                           \\\n-    do_stub(compiler, lookup_hi_base64)                                 \\\n-    do_arch_entry(x86, compiler, lookup_hi_base64, lookup_hi_base64,    \\\n-                  base64_vbmi_lookup_hi_addr)                           \\\n-    do_stub(compiler, lookup_lo_base64url)                              \\\n-    do_arch_entry(x86, compiler, lookup_lo_base64url,                   \\\n-                  lookup_lo_base64url,                                  \\\n-                  base64_vbmi_lookup_lo_url_addr)                       \\\n-    do_stub(compiler, lookup_hi_base64url)                              \\\n-    do_arch_entry(x86, compiler, lookup_hi_base64url,                   \\\n-                  lookup_hi_base64url,                                  \\\n-                  base64_vbmi_lookup_hi_url_addr)                       \\\n-    do_stub(compiler, pack_vec_base64)                                  \\\n-    do_arch_entry(x86, compiler, pack_vec_base64, pack_vec_base64,      \\\n-                  base64_vbmi_pack_vec_addr)                            \\\n-    do_stub(compiler, join_0_1_base64)                                  \\\n-    do_arch_entry(x86, compiler, join_0_1_base64, join_0_1_base64,      \\\n-                  base64_vbmi_join_0_1_addr)                            \\\n-    do_stub(compiler, join_1_2_base64)                                  \\\n-    do_arch_entry(x86, compiler, join_1_2_base64, join_1_2_base64,      \\\n-                  base64_vbmi_join_1_2_addr)                            \\\n-    do_stub(compiler, join_2_3_base64)                                  \\\n-    do_arch_entry(x86, compiler, join_2_3_base64, join_2_3_base64,      \\\n-                  base64_vbmi_join_2_3_addr)                            \\\n-    do_stub(compiler, encoding_table_base64)                            \\\n-    do_arch_entry(x86, compiler, encoding_table_base64,                 \\\n-                  encoding_table_base64, base64_encoding_table_addr)    \\\n-    do_stub(compiler, decoding_table_base64)                            \\\n-    do_arch_entry(x86, compiler, decoding_table_base64,                 \\\n-                  decoding_table_base64, base64_decoding_table_addr)    \\\n-  )                                                                     \\\n+  \/* x86_64 exposes these 3 stubs via a generic entry array *\/          \\\n+  \/* other arches use arch-specific entries *\/                          \\\n+  \/* this really needs rationalising *\/                                 \\\n+  do_stub(compiler, string_indexof_linear_ll)                           \\\n+  do_stub(compiler, string_indexof_linear_uu)                           \\\n+  do_stub(compiler, string_indexof_linear_ul)                           \\\n+  do_stub(compiler, pshuffle_byte_flip_mask_sha512)                     \\\n+  do_arch_entry(x86, compiler, pshuffle_byte_flip_mask_sha512,          \\\n+                pshuffle_byte_flip_mask_addr_sha512,                    \\\n+                pshuffle_byte_flip_mask_addr_sha512)                    \\\n+  do_stub(compiler, compress_perm_table32)                              \\\n+  do_arch_entry(x86, compiler, compress_perm_table32,                   \\\n+                compress_perm_table32, compress_perm_table32)           \\\n+  do_stub(compiler, compress_perm_table64)                              \\\n+  do_arch_entry(x86, compiler, compress_perm_table64,                   \\\n+                compress_perm_table64, compress_perm_table64)           \\\n+  do_stub(compiler, expand_perm_table32)                                \\\n+  do_arch_entry(x86, compiler, expand_perm_table32,                     \\\n+                expand_perm_table32, expand_perm_table32)               \\\n+  do_stub(compiler, expand_perm_table64)                                \\\n+  do_arch_entry(x86, compiler, expand_perm_table64,                     \\\n+                expand_perm_table64, expand_perm_table64)               \\\n+  do_stub(compiler, avx2_shuffle_base64)                                \\\n+  do_arch_entry(x86, compiler, avx2_shuffle_base64,                     \\\n+                avx2_shuffle_base64, base64_avx2_shuffle_addr)          \\\n+  do_stub(compiler, avx2_input_mask_base64)                             \\\n+  do_arch_entry(x86, compiler, avx2_input_mask_base64,                  \\\n+                avx2_input_mask_base64,                                 \\\n+                base64_avx2_input_mask_addr)                            \\\n+  do_stub(compiler, avx2_lut_base64)                                    \\\n+  do_arch_entry(x86, compiler, avx2_lut_base64,                         \\\n+                avx2_lut_base64, base64_avx2_lut_addr)                  \\\n+  do_stub(compiler, avx2_decode_tables_base64)                          \\\n+  do_arch_entry(x86, compiler, avx2_decode_tables_base64,               \\\n+                avx2_decode_tables_base64,                              \\\n+                base64_AVX2_decode_tables_addr)                         \\\n+  do_stub(compiler, avx2_decode_lut_tables_base64)                      \\\n+  do_arch_entry(x86, compiler, avx2_decode_lut_tables_base64,           \\\n+                avx2_decode_lut_tables_base64,                          \\\n+                base64_AVX2_decode_LUT_tables_addr)                     \\\n+  do_stub(compiler, shuffle_base64)                                     \\\n+  do_arch_entry(x86, compiler, shuffle_base64, shuffle_base64,          \\\n+                base64_shuffle_addr)                                    \\\n+  do_stub(compiler, lookup_lo_base64)                                   \\\n+  do_arch_entry(x86, compiler, lookup_lo_base64, lookup_lo_base64,      \\\n+                base64_vbmi_lookup_lo_addr)                             \\\n+  do_stub(compiler, lookup_hi_base64)                                   \\\n+  do_arch_entry(x86, compiler, lookup_hi_base64, lookup_hi_base64,      \\\n+                base64_vbmi_lookup_hi_addr)                             \\\n+  do_stub(compiler, lookup_lo_base64url)                                \\\n+  do_arch_entry(x86, compiler, lookup_lo_base64url,                     \\\n+                lookup_lo_base64url,                                    \\\n+                base64_vbmi_lookup_lo_url_addr)                         \\\n+  do_stub(compiler, lookup_hi_base64url)                                \\\n+  do_arch_entry(x86, compiler, lookup_hi_base64url,                     \\\n+                lookup_hi_base64url,                                    \\\n+                base64_vbmi_lookup_hi_url_addr)                         \\\n+  do_stub(compiler, pack_vec_base64)                                    \\\n+  do_arch_entry(x86, compiler, pack_vec_base64, pack_vec_base64,        \\\n+                base64_vbmi_pack_vec_addr)                              \\\n+  do_stub(compiler, join_0_1_base64)                                    \\\n+  do_arch_entry(x86, compiler, join_0_1_base64, join_0_1_base64,        \\\n+                base64_vbmi_join_0_1_addr)                              \\\n+  do_stub(compiler, join_1_2_base64)                                    \\\n+  do_arch_entry(x86, compiler, join_1_2_base64, join_1_2_base64,        \\\n+                base64_vbmi_join_1_2_addr)                              \\\n+  do_stub(compiler, join_2_3_base64)                                    \\\n+  do_arch_entry(x86, compiler, join_2_3_base64, join_2_3_base64,        \\\n+                base64_vbmi_join_2_3_addr)                              \\\n+  do_stub(compiler, encoding_table_base64)                              \\\n+  do_arch_entry(x86, compiler, encoding_table_base64,                   \\\n+                encoding_table_base64, base64_encoding_table_addr)      \\\n+  do_stub(compiler, decoding_table_base64)                              \\\n+  do_arch_entry(x86, compiler, decoding_table_base64,                   \\\n+                decoding_table_base64, base64_decoding_table_addr)      \\\n@@ -246,2 +242,2 @@\n-  do_arch_blob(final, 11000 LP64_ONLY(+20000)                           \\\n-               WINDOWS_ONLY(+22000) ZGC_ONLY(+20000))                    \\\n+  do_arch_blob(final, 31000                                             \\\n+               WINDOWS_ONLY(+22000) ZGC_ONLY(+20000))                   \\\n","filename":"src\/hotspot\/cpu\/x86\/stubDeclarations_x86.hpp","additions":103,"deletions":107,"binary":false,"changes":210,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-#ifdef _LP64\n@@ -52,1 +51,0 @@\n-#endif\n@@ -149,1 +147,0 @@\n-#ifdef _LP64\n@@ -196,1 +193,0 @@\n-#endif \/\/ _LP64\n@@ -359,1 +355,0 @@\n-#ifdef _LP64\n@@ -408,1 +403,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -78,19 +78,0 @@\n-\n-#ifndef _LP64\n-\n-  static jint    _fpu_cntrl_wrd_std;\n-  static jint    _fpu_cntrl_wrd_24;\n-  static jint    _fpu_cntrl_wrd_trunc;\n-\n-  static jint    _fpu_subnormal_bias1[3];\n-  static jint    _fpu_subnormal_bias2[3];\n-\n-  static address addr_fpu_cntrl_wrd_std()     { return (address)&_fpu_cntrl_wrd_std;   }\n-  static address addr_fpu_cntrl_wrd_24()      { return (address)&_fpu_cntrl_wrd_24;    }\n-  static address addr_fpu_cntrl_wrd_trunc()   { return (address)&_fpu_cntrl_wrd_trunc; }\n-  static address addr_fpu_subnormal_bias1()   { return (address)&_fpu_subnormal_bias1; }\n-  static address addr_fpu_subnormal_bias2()   { return (address)&_fpu_subnormal_bias2; }\n-\n-  static jint    fpu_cntrl_wrd_std()          { return _fpu_cntrl_wrd_std; }\n-#endif \/\/ !LP64\n-\n@@ -99,1 +80,0 @@\n-#ifdef _LP64\n@@ -101,1 +81,0 @@\n-#endif \/\/ _LP64\n@@ -105,1 +84,0 @@\n-#ifdef _LP64\n@@ -110,1 +88,0 @@\n-#endif \/\/ _LP64\n@@ -118,1 +95,0 @@\n-#ifdef _LP64\n@@ -123,1 +99,0 @@\n-#endif\n@@ -127,1 +102,0 @@\n-#ifdef _LP64\n@@ -129,1 +103,0 @@\n-#endif \/\/ _LP64\n@@ -131,1 +104,0 @@\n-#ifdef _LP64\n@@ -136,1 +108,0 @@\n-#endif \/\/ _LP64\n@@ -138,1 +109,0 @@\n-#ifdef _LP64\n@@ -141,1 +111,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"}]}