{"files":[{"patch":"@@ -1,207 +0,0 @@\n-\/*\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shared\/gcLogPrecious.hpp\"\n-#include \"gc\/x\/xAddress.inline.hpp\"\n-#include \"gc\/x\/xAddressSpaceLimit.hpp\"\n-#include \"gc\/x\/xGlobals.hpp\"\n-#include \"gc\/x\/xVirtualMemory.inline.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-XVirtualMemoryManager::XVirtualMemoryManager(size_t max_capacity) :\n-    _manager(),\n-    _reserved(0),\n-    _initialized(false) {\n-\n-  \/\/ Check max supported heap size\n-  if (max_capacity > XAddressOffsetMax) {\n-    log_error_p(gc)(\"Java heap too large (max supported heap size is \" SIZE_FORMAT \"G)\",\n-                    XAddressOffsetMax \/ G);\n-    return;\n-  }\n-\n-  \/\/ Initialize platform specific parts before reserving address space\n-  pd_initialize_before_reserve();\n-\n-  \/\/ Reserve address space\n-  if (!reserve(max_capacity)) {\n-    log_error_pd(gc)(\"Failed to reserve enough address space for Java heap\");\n-    return;\n-  }\n-\n-  \/\/ Initialize platform specific parts after reserving address space\n-  pd_initialize_after_reserve();\n-\n-  \/\/ Successfully initialized\n-  _initialized = true;\n-}\n-\n-size_t XVirtualMemoryManager::reserve_discontiguous(uintptr_t start, size_t size, size_t min_range) {\n-  if (size < min_range) {\n-    \/\/ Too small\n-    return 0;\n-  }\n-\n-  assert(is_aligned(size, XGranuleSize), \"Misaligned\");\n-\n-  if (reserve_contiguous(start, size)) {\n-    return size;\n-  }\n-\n-  const size_t half = size \/ 2;\n-  if (half < min_range) {\n-    \/\/ Too small\n-    return 0;\n-  }\n-\n-  \/\/ Divide and conquer\n-  const size_t first_part = align_down(half, XGranuleSize);\n-  const size_t second_part = size - first_part;\n-  return reserve_discontiguous(start, first_part, min_range) +\n-         reserve_discontiguous(start + first_part, second_part, min_range);\n-}\n-\n-size_t XVirtualMemoryManager::reserve_discontiguous(size_t size) {\n-  \/\/ Don't try to reserve address ranges smaller than 1% of the requested size.\n-  \/\/ This avoids an explosion of reservation attempts in case large parts of the\n-  \/\/ address space is already occupied.\n-  const size_t min_range = align_up(size \/ 100, XGranuleSize);\n-  size_t start = 0;\n-  size_t reserved = 0;\n-\n-  \/\/ Reserve size somewhere between [0, XAddressOffsetMax)\n-  while (reserved < size && start < XAddressOffsetMax) {\n-    const size_t remaining = MIN2(size - reserved, XAddressOffsetMax - start);\n-    reserved += reserve_discontiguous(start, remaining, min_range);\n-    start += remaining;\n-  }\n-\n-  return reserved;\n-}\n-\n-bool XVirtualMemoryManager::reserve_contiguous(uintptr_t start, size_t size) {\n-  assert(is_aligned(size, XGranuleSize), \"Must be granule aligned\");\n-\n-  \/\/ Reserve address views\n-  const uintptr_t marked0 = XAddress::marked0(start);\n-  const uintptr_t marked1 = XAddress::marked1(start);\n-  const uintptr_t remapped = XAddress::remapped(start);\n-\n-  \/\/ Reserve address space\n-  if (!pd_reserve(marked0, size)) {\n-    return false;\n-  }\n-\n-  if (!pd_reserve(marked1, size)) {\n-    pd_unreserve(marked0, size);\n-    return false;\n-  }\n-\n-  if (!pd_reserve(remapped, size)) {\n-    pd_unreserve(marked0, size);\n-    pd_unreserve(marked1, size);\n-    return false;\n-  }\n-\n-  \/\/ Register address views with native memory tracker\n-  nmt_reserve(marked0, size);\n-  nmt_reserve(marked1, size);\n-  nmt_reserve(remapped, size);\n-\n-  \/\/ Make the address range free\n-  _manager.free(start, size);\n-\n-  return true;\n-}\n-\n-bool XVirtualMemoryManager::reserve_contiguous(size_t size) {\n-  \/\/ Allow at most 8192 attempts spread evenly across [0, XAddressOffsetMax)\n-  const size_t unused = XAddressOffsetMax - size;\n-  const size_t increment = MAX2(align_up(unused \/ 8192, XGranuleSize), XGranuleSize);\n-\n-  for (size_t start = 0; start + size <= XAddressOffsetMax; start += increment) {\n-    if (reserve_contiguous(start, size)) {\n-      \/\/ Success\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Failed\n-  return false;\n-}\n-\n-bool XVirtualMemoryManager::reserve(size_t max_capacity) {\n-  const size_t limit = MIN2(XAddressOffsetMax, XAddressSpaceLimit::heap_view());\n-  const size_t size = MIN2(max_capacity * XVirtualToPhysicalRatio, limit);\n-\n-  size_t reserved = size;\n-  bool contiguous = true;\n-\n-  \/\/ Prefer a contiguous address space\n-  if (!reserve_contiguous(size)) {\n-    \/\/ Fall back to a discontiguous address space\n-    reserved = reserve_discontiguous(size);\n-    contiguous = false;\n-  }\n-\n-  log_info_p(gc, init)(\"Address Space Type: %s\/%s\/%s\",\n-                       (contiguous ? \"Contiguous\" : \"Discontiguous\"),\n-                       (limit == XAddressOffsetMax ? \"Unrestricted\" : \"Restricted\"),\n-                       (reserved == size ? \"Complete\" : \"Degraded\"));\n-  log_info_p(gc, init)(\"Address Space Size: \" SIZE_FORMAT \"M x \" SIZE_FORMAT \" = \" SIZE_FORMAT \"M\",\n-                       reserved \/ M, XHeapViews, (reserved * XHeapViews) \/ M);\n-\n-  \/\/ Record reserved\n-  _reserved = reserved;\n-\n-  return reserved >= max_capacity;\n-}\n-\n-void XVirtualMemoryManager::nmt_reserve(uintptr_t start, size_t size) {\n-  MemTracker::record_virtual_memory_reserve((void*)start, size, CALLER_PC, mtJavaHeap);\n-}\n-\n-bool XVirtualMemoryManager::is_initialized() const {\n-  return _initialized;\n-}\n-\n-XVirtualMemory XVirtualMemoryManager::alloc(size_t size, bool force_low_address) {\n-  uintptr_t start;\n-\n-  \/\/ Small pages are allocated at low addresses, while medium\/large pages\n-  \/\/ are allocated at high addresses (unless forced to be at a low address).\n-  if (force_low_address || size <= XPageSizeSmall) {\n-    start = _manager.alloc_low_address(size);\n-  } else {\n-    start = _manager.alloc_high_address(size);\n-  }\n-\n-  return XVirtualMemory(start, size);\n-}\n-\n-void XVirtualMemoryManager::free(const XVirtualMemory& vmem) {\n-  _manager.free(vmem.start(), vmem.size());\n-}\n","filename":"src\/hotspot\/share\/gc\/x\/xVirtualMemory.cpp","additions":0,"deletions":207,"binary":false,"changes":207,"status":"deleted"}]}