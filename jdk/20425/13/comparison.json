{"files":[{"patch":"@@ -1454,1 +1454,1 @@\n-      MemTracker::record_virtual_memory_tag(archive_space_rs.base(), mtClassShared);\n+      MemTracker::record_virtual_memory_tag(archive_space_rs.base(), archive_space_rs.size(), mtClassShared);\n@@ -1529,2 +1529,2 @@\n-    MemTracker::record_virtual_memory_tag(archive_space_rs.base(), mtClassShared);\n-    MemTracker::record_virtual_memory_tag(class_space_rs.base(), mtClass);\n+    MemTracker::record_virtual_memory_tag(archive_space_rs.base(), archive_space_rs.size(), mtClassShared);\n+    MemTracker::record_virtual_memory_tag(class_space_rs.base(), class_space_rs.size(), mtClass);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-  MemTracker::record_virtual_memory_tag((address)rs.base(), mem_tag);\n+  MemTracker::record_virtual_memory_tag((address)rs.base(), rs.size(), mem_tag);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RegionToSpaceMapper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-  MemTracker::record_virtual_memory_tag((address)rs.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag((address)rs.base(), rs.size(), mtGC);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -255,1 +255,1 @@\n-  MemTracker::record_virtual_memory_tag((address)rs.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag((address)rs.base(), rs.size(), mtGC);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -86,2 +86,0 @@\n-  MemTracker::record_virtual_memory_tag((address)rs.base(), mtGC);\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  MemTracker::record_virtual_memory_tag((address)card_table.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag((address)card_table.base(), card_table.size(), mtGC);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-  MemTracker::record_virtual_memory_tag(_map_space.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag(_map_space.base(), _map_space.size(), mtGC);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -302,1 +302,1 @@\n-  MemTracker::record_virtual_memory_tag(bitmap.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag(bitmap.base(), bitmap.size(), mtGC);\n@@ -326,1 +326,1 @@\n-    MemTracker::record_virtual_memory_tag(verify_bitmap.base(), mtGC);\n+    MemTracker::record_virtual_memory_tag(verify_bitmap.base(), verify_bitmap.size(), mtGC);\n@@ -340,1 +340,1 @@\n-  MemTracker::record_virtual_memory_tag(aux_bitmap.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag(aux_bitmap.base(), aux_bitmap.size(), mtGC);\n@@ -358,1 +358,1 @@\n-  MemTracker::record_virtual_memory_tag(region_storage.base(), mtGC);\n+  MemTracker::record_virtual_memory_tag(region_storage.base(), region_storage.size(), mtGC);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-  MemTracker::record_virtual_memory_tag((address)_rs.base(), mtTracing);\n+  MemTracker::record_virtual_memory_tag((address)_rs.base(), _rs.size(), mtTracing);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrVirtualMemory.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -801,1 +801,1 @@\n-    MemTracker::record_virtual_memory_tag((address)rs.base(), mtClass);\n+    MemTracker::record_virtual_memory_tag((address)rs.base(), rs.size(), mtClass);\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -264,1 +264,2 @@\n-  MemTracker::record_virtual_memory_tag(rs.base(), mtMetaspace);\n+  assert(rs.size() == word_size * BytesPerWord, \"must be\");\n+  MemTracker::record_virtual_memory_tag(rs.base(), rs.size(), mtMetaspace);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -158,1 +158,1 @@\n-  inline MallocMemory* by_type(MemTag mem_tag) {\n+  inline MallocMemory* by_tag(MemTag mem_tag) {\n@@ -163,1 +163,1 @@\n-  inline const MallocMemory* by_type(MemTag mem_tag) const {\n+  inline const MallocMemory* by_tag(MemTag mem_tag) const {\n@@ -223,1 +223,1 @@\n-     as_snapshot()->by_type(mem_tag)->record_malloc(size);\n+     as_snapshot()->by_tag(mem_tag)->record_malloc(size);\n@@ -228,1 +228,1 @@\n-     as_snapshot()->by_type(mem_tag)->record_free(size);\n+     as_snapshot()->by_tag(mem_tag)->record_free(size);\n@@ -233,1 +233,1 @@\n-     as_snapshot()->by_type(mem_tag)->record_new_arena();\n+     as_snapshot()->by_tag(mem_tag)->record_new_arena();\n@@ -237,1 +237,1 @@\n-     as_snapshot()->by_type(mem_tag)->record_arena_free();\n+     as_snapshot()->by_tag(mem_tag)->record_arena_free();\n@@ -241,1 +241,1 @@\n-     as_snapshot()->by_type(mem_tag)->record_arena_size_change(size);\n+     as_snapshot()->by_tag(mem_tag)->record_arena_size_change(size);\n","filename":"src\/hotspot\/share\/nmt\/mallocTracker.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-        const MallocMemory* mm = as_snapshot()->by_type(mem_tag);\n+        const MallocMemory* mm = as_snapshot()->by_tag(mem_tag);\n","filename":"src\/hotspot\/share\/nmt\/mallocTracker.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-int compare_malloc_site_and_type(const MallocSite& s1, const MallocSite& s2) {\n+int compare_malloc_site_and_tag(const MallocSite& s1, const MallocSite& s2) {\n@@ -164,1 +164,1 @@\n-  if (!VirtualMemoryTracker::walk_virtual_memory(&virtual_memory_walker)) {\n+  if (!MemTracker::walk_virtual_memory(&virtual_memory_walker)) {\n@@ -235,2 +235,2 @@\n-    case by_site_and_type:\n-      malloc_sites_to_allocation_site_and_type_order();\n+    case by_site_and_tag:\n+      malloc_sites_to_allocation_site_and_tag_order();\n@@ -276,1 +276,1 @@\n-  if (_malloc_sites_order != by_site && _malloc_sites_order != by_site_and_type) {\n+  if (_malloc_sites_order != by_site && _malloc_sites_order != by_site_and_tag) {\n@@ -286,3 +286,3 @@\n-void MemBaseline::malloc_sites_to_allocation_site_and_type_order() {\n-  if (_malloc_sites_order != by_site_and_type) {\n-    SortedLinkedList<MallocSite, compare_malloc_site_and_type> tmp;\n+void MemBaseline::malloc_sites_to_allocation_site_and_tag_order() {\n+  if (_malloc_sites_order != by_site_and_tag) {\n+    SortedLinkedList<MallocSite, compare_malloc_site_and_tag> tmp;\n@@ -293,1 +293,1 @@\n-    _malloc_sites_order = by_site_and_type;\n+    _malloc_sites_order = by_site_and_tag;\n","filename":"src\/hotspot\/share\/nmt\/memBaseline.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -56,1 +56,1 @@\n-    by_site_and_type \/\/ by call site and memory tag\n+    by_site_and_tag \/\/ by call site and memory tag\n@@ -149,1 +149,1 @@\n-    return _malloc_memory_snapshot.by_type(mem_tag);\n+    return _malloc_memory_snapshot.by_tag(mem_tag);\n@@ -154,1 +154,1 @@\n-    return _virtual_memory_snapshot.by_type(mem_tag);\n+    return _virtual_memory_snapshot.by_tag(mem_tag);\n@@ -207,1 +207,1 @@\n-  void malloc_sites_to_allocation_site_and_type_order();\n+  void malloc_sites_to_allocation_site_and_tag_order();\n","filename":"src\/hotspot\/share\/nmt\/memBaseline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -161,1 +161,1 @@\n-    return VirtualMemoryTracker::walk_virtual_memory(this);\n+    return MemTracker::walk_virtual_memory(this);\n","filename":"src\/hotspot\/share\/nmt\/memMapPrinter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"logging\/log.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"nmt\/memTracker.hpp\"\n@@ -33,1 +35,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -184,2 +186,2 @@\n-    MallocMemory* malloc_memory = _malloc_snapshot->by_type(mem_tag);\n-    VirtualMemory* virtual_memory = _vm_snapshot->by_type(mem_tag);\n+    MallocMemory* malloc_memory = _malloc_snapshot->by_tag(mem_tag);\n+    VirtualMemory* virtual_memory = _vm_snapshot->by_tag(mem_tag);\n@@ -187,1 +189,1 @@\n-    report_summary_of_type(mem_tag, malloc_memory, virtual_memory);\n+    report_summary_of_tag(mem_tag, malloc_memory, virtual_memory);\n@@ -191,1 +193,1 @@\n-void MemSummaryReporter::report_summary_of_type(MemTag mem_tag,\n+void MemSummaryReporter::report_summary_of_tag(MemTag mem_tag,\n@@ -200,1 +202,1 @@\n-      (const VirtualMemory*)_vm_snapshot->by_type(mtThreadStack);\n+      (const VirtualMemory*)_vm_snapshot->by_tag(mtThreadStack);\n@@ -242,1 +244,1 @@\n-     _vm_snapshot->by_type(mtThreadStack);\n+     _vm_snapshot->by_tag(mtThreadStack);\n@@ -435,7 +437,14 @@\n-    CommittedRegionIterator itr = reserved_rgn->iterate_committed_regions();\n-    const CommittedMemoryRegion* committed_rgn = itr.next();\n-    if (committed_rgn->size() == reserved_rgn->size() && committed_rgn->call_stack()->equals(*stack)) {\n-      \/\/ One region spanning the entire reserved region, with the same stack trace.\n-      \/\/ Don't print this regions because the \"reserved and committed\" line above\n-      \/\/ already indicates that the region is committed.\n-      assert(itr.next() == nullptr, \"Unexpectedly more than one regions\");\n+    bool reserved_and_committed = false;\n+    VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*reserved_rgn,\n+                                                                  [&](CommittedMemoryRegion& committed_rgn) {\n+      if (committed_rgn.size() == reserved_rgn->size() && committed_rgn.call_stack()->equals(*stack)) {\n+        \/\/ One region spanning the entire reserved region, with the same stack trace.\n+        \/\/ Don't print this regions because the \"reserved and committed\" line above\n+        \/\/ already indicates that the region is committed.\n+        reserved_and_committed = true;\n+        return false;\n+      }\n+      return true;\n+    });\n+\n+    if (reserved_and_committed)\n@@ -443,1 +452,0 @@\n-    }\n@@ -446,3 +454,1 @@\n-  CommittedRegionIterator itr = reserved_rgn->iterate_committed_regions();\n-  const CommittedMemoryRegion* committed_rgn;\n-  while ((committed_rgn = itr.next()) != nullptr) {\n+  auto print_committed_rgn = [&](const CommittedMemoryRegion& crgn) {\n@@ -450,2 +456,2 @@\n-    if (amount_in_current_scale(committed_rgn->size()) == 0) continue;\n-    stack = committed_rgn->call_stack();\n+    if (amount_in_current_scale(crgn.size()) == 0) return;\n+    stack = crgn.call_stack();\n@@ -454,1 +460,1 @@\n-      print_virtual_memory_region(\"committed\", committed_rgn->base(), committed_rgn->size());\n+      print_virtual_memory_region(\"committed\", crgn.base(), crgn.size());\n@@ -459,1 +465,1 @@\n-        INDENT_BY(4, stack->print_on(out);)\n+        INDENT_BY(4, _stackprinter.print_stack(stack);)\n@@ -462,1 +468,7 @@\n-  }\n+  };\n+\n+  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*reserved_rgn,\n+                                                                  [&](CommittedMemoryRegion& crgn) {\n+    print_committed_rgn(crgn);\n+    return true;\n+  });\n@@ -527,1 +539,1 @@\n-    diff_summary_of_type(mem_tag,\n+    diff_summary_of_tag(mem_tag,\n@@ -597,1 +609,1 @@\n-void MemSummaryDiffReporter::diff_summary_of_type(MemTag mem_tag,\n+void MemSummaryDiffReporter::diff_summary_of_tag(MemTag mem_tag,\n@@ -798,2 +810,2 @@\n-  MallocSiteIterator early_itr = _early_baseline.malloc_sites(MemBaseline::by_site_and_type);\n-  MallocSiteIterator current_itr = _current_baseline.malloc_sites(MemBaseline::by_site_and_type);\n+  MallocSiteIterator early_itr = _early_baseline.malloc_sites(MemBaseline::by_site_and_tag);\n+  MallocSiteIterator current_itr = _current_baseline.malloc_sites(MemBaseline::by_site_and_tag);\n","filename":"src\/hotspot\/share\/nmt\/memReporter.cpp","additions":39,"deletions":27,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -142,1 +142,1 @@\n-  void report_summary_of_type(MemTag mem_tag, MallocMemory* malloc_memory,\n+  void report_summary_of_tag(MemTag mem_tag, MallocMemory* malloc_memory,\n@@ -207,1 +207,1 @@\n-  void diff_summary_of_type(MemTag mem_tag,\n+  void diff_summary_of_tag(MemTag mem_tag,\n","filename":"src\/hotspot\/share\/nmt\/memReporter.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-  \/\/ Memory type is encoded into tracking header as a byte field,\n+  \/\/ Memory tag is encoded into tracking header as a byte field,\n@@ -71,1 +71,1 @@\n-        !VirtualMemoryTracker::initialize(level)) {\n+        !VirtualMemoryTracker::Instance::initialize(level)) {\n@@ -126,1 +126,1 @@\n-       VirtualMemoryTracker::print_containing_region(p, out));\n+       VirtualMemoryTracker::Instance::print_containing_region(p, out));\n","filename":"src\/hotspot\/share\/nmt\/memTracker.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -55,0 +56,8 @@\n+  static void snapshot_thread_stacks() {\n+    VirtualMemoryTracker::Instance::snapshot_thread_stacks();\n+  }\n+\n+  static bool walk_virtual_memory(VirtualMemoryWalker* walker) {\n+    return VirtualMemoryTracker::Instance::walk_virtual_memory(walker);\n+  }\n+\n@@ -129,1 +138,1 @@\n-      VirtualMemoryTracker::add_reserved_region((address)addr, size, stack, mem_tag);\n+      VirtualMemoryTracker::Instance::add_reserved_region((address)addr, size, stack, mem_tag);\n@@ -137,1 +146,1 @@\n-      VirtualMemoryTracker::remove_released_region((address)addr, size);\n+      VirtualMemoryTracker::Instance::remove_released_region((address)addr, size);\n@@ -145,1 +154,1 @@\n-      VirtualMemoryTracker::remove_uncommitted_region((address)addr, size);\n+      VirtualMemoryTracker::Instance::remove_uncommitted_region((address)addr, size);\n@@ -155,2 +164,2 @@\n-      VirtualMemoryTracker::add_reserved_region((address)addr, size, stack, mem_tag);\n-      VirtualMemoryTracker::add_committed_region((address)addr, size, stack);\n+      VirtualMemoryTracker::Instance::add_reserved_region((address)addr, size, stack, mem_tag);\n+      VirtualMemoryTracker::Instance::add_committed_region((address)addr, size, stack);\n@@ -166,1 +175,1 @@\n-      VirtualMemoryTracker::add_committed_region((address)addr, size, stack);\n+      VirtualMemoryTracker::Instance::add_committed_region((address)addr, size, stack);\n@@ -208,1 +217,1 @@\n-  \/\/  memory flags of the original region.\n+  \/\/  memory tags of the original region.\n@@ -214,1 +223,1 @@\n-      VirtualMemoryTracker::split_reserved_region((address)addr, size, split, mem_tag, split_tag);\n+      VirtualMemoryTracker::Instance::split_reserved_region((address)addr, size, split, mem_tag, split_tag);\n@@ -218,1 +227,1 @@\n-  static inline void record_virtual_memory_tag(void* addr, MemTag mem_tag) {\n+  static inline void record_virtual_memory_tag(void* addr, size_t size, MemTag mem_tag) {\n@@ -223,1 +232,1 @@\n-      VirtualMemoryTracker::set_reserved_region_type((address)addr, mem_tag);\n+      VirtualMemoryTracker::Instance::set_reserved_region_tag((address)addr, size, mem_tag);\n","filename":"src\/hotspot\/share\/nmt\/memTracker.hpp","additions":19,"deletions":10,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-    VirtualMemory* summary = file->_summary.by_type(NMTUtil::index_to_tag(i));\n+    VirtualMemory* summary = file->_summary.by_tag(NMTUtil::index_to_tag(i));\n@@ -59,1 +59,1 @@\n-    VirtualMemory* summary = file->_summary.by_type(NMTUtil::index_to_tag(i));\n+    VirtualMemory* summary = file->_summary.by_tag(NMTUtil::index_to_tag(i));\n@@ -79,1 +79,1 @@\n-      return;\n+      return true;\n@@ -102,0 +102,1 @@\n+    return true;\n@@ -182,5 +183,9 @@\n-  iterate_summary([&](MemTag tag, const VirtualMemory* current) {\n-    VirtualMemory* snap = snapshot->by_type(tag);\n-    \/\/ Only account the committed memory.\n-    snap->commit_memory(current->committed());\n-  });\n+  for (int d = 0; d < _files.length(); d++) {\n+    const MemoryFile* file = _files.at(d);\n+    for (int i = 0; i < mt_number_of_tags; i++) {\n+      VirtualMemory* snap = snapshot->by_tag(NMTUtil::index_to_tag(i));\n+      const VirtualMemory* current = file->_summary.by_tag(NMTUtil::index_to_tag(i));\n+      \/\/ Only account the committed memory.\n+      snap->commit_memory(current->committed());\n+    }\n+  }\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n@@ -33,0 +32,1 @@\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -82,1 +82,1 @@\n-        f(NMTUtil::index_to_tag(i), file->_summary.by_type(NMTUtil::index_to_tag(i)));\n+        f(NMTUtil::index_to_tag(i), file->_summary.by_tag(NMTUtil::index_to_tag(i)));\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,2 +32,2 @@\n-#define MEMORY_TAG_DECLARE_NAME(type, human_readable) \\\n-  { #type, human_readable },\n+#define MEMORY_TAG_DECLARE_NAME(tag, human_readable) \\\n+  { #tag, human_readable },\n","filename":"src\/hotspot\/share\/nmt\/nmtCommon.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -91,1 +91,1 @@\n-    assert(tag_is_valid(mem_tag), \"Invalid type (%u)\", (unsigned)mem_tag);\n+    assert(tag_is_valid(mem_tag), \"Invalid tag (%u)\", (unsigned)mem_tag);\n","filename":"src\/hotspot\/share\/nmt\/nmtCommon.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+  friend class VMTWithVMATreeTest;\n@@ -212,1 +213,1 @@\n-        return;\n+        return true;\n@@ -218,0 +219,1 @@\n+      return true;\n@@ -354,1 +356,2 @@\n-      f(head);\n+      if (!f(head))\n+        return;\n@@ -381,1 +384,2 @@\n-        f(head);\n+        if (!f(head))\n+          return;\n","filename":"src\/hotspot\/share\/nmt\/nmtTreap.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"nmt\/memTracker.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n@@ -51,1 +51,1 @@\n-  VirtualMemoryTracker::snapshot_thread_stacks();\n+  MemTracker::snapshot_thread_stacks();\n@@ -63,1 +63,1 @@\n-    const MallocMemory* mm = ms->by_type(mem_tag);\n+    const MallocMemory* mm = ms->by_tag(mem_tag);\n@@ -87,1 +87,1 @@\n-    const VirtualMemory* vm = vms->by_type(mem_tag);\n+    const VirtualMemory* vm = vms->by_tag(mem_tag);\n","filename":"src\/hotspot\/share\/nmt\/nmtUsage.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+#include \"nmt\/regionsTree.hpp\"\n+\n+ReservedMemoryRegion RegionsTree::find_reserved_region(address addr) {\n+    ReservedMemoryRegion rmr;\n+    auto contain_region = [&](ReservedMemoryRegion& region_in_tree) {\n+      if (region_in_tree.contain_address(addr)) {\n+        rmr = region_in_tree;\n+        return false;\n+      }\n+      return true;\n+    };\n+    visit_reserved_regions(contain_region);\n+    return rmr;\n+}\n+\n+VMATree::SummaryDiff RegionsTree::commit_region(address addr, size_t size, const NativeCallStack& stack) {\n+  return commit_mapping((VMATree::position)addr, size, make_region_data(stack, mtNone), \/*use tag inplace*\/ true);\n+}\n+\n+VMATree::SummaryDiff RegionsTree::uncommit_region(address addr, size_t size) {\n+  return uncommit_mapping((VMATree::position)addr, size, make_region_data(NativeCallStack::empty_stack(), mtNone));\n+}\n+\/\/ The nodes for the regions may look like this:\n+\/\/ small letters are existing nodes, capital A and B are the region we are going to find the summary.\n+\/\/ ...--------a-----A----b---c---d----e---B---f---....\n+\/\/ calling visit_range_in_order for [A,B) is not enough to find regions between a---...---f\n+VMATree::SummaryDiff RegionsTree::region_summary(address addr, size_t size) {\n+  NodeHelper prev;\n+  SummaryDiff summary;\n+  VMATree::position A = (VMATree::position)addr;\n+  VMATree::position B = (VMATree::position)A + size;\n+  VMATree::VMATreap::Range ab = tree().find_enclosing_range(A);\n+  VMATree::VMATreap::Range ef = tree().find_enclosing_range(B);\n+  VMATree::position a = ab.start == nullptr ? A : ab.start->key();\n+  VMATree::position f = ef.end == nullptr ? B : ef.end->key();\n+\n+\n+  visit_range_in_order(a, f, [&](Node* node) {\n+    NodeHelper curr(node);\n+    if (prev.is_valid()) {\n+      SingleDiff& single = summary.tag[NMTUtil::tag_to_index(prev.out_tag())];\n+      size_t dist = curr.distance_from(prev);\n+      if (prev.is_reserved_begin())\n+        single.reserve += dist;\n+      if (prev.is_committed_begin()) {\n+        single.reserve += dist;\n+        single.commit += dist;\n+      }\n+    }\n+    prev = curr;\n+    return true;\n+  });\n+  return summary;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/nmt\/regionsTree.cpp","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -0,0 +1,161 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef NMT_REGIONSTREE_HPP\n+#define NMT_REGIONSTREE_HPP\n+\n+#include \"logging\/log.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n+\n+\/\/ RegionsTree extends VMATree to add some more specific API and also defines a helper\n+\/\/ for processing the tree nodes in a shorter and more meaningful way.\n+class RegionsTree : public VMATree {\n+ private:\n+  NativeCallStackStorage _ncs_storage;\n+  bool _with_storage;\n+\n+ public:\n+  RegionsTree(bool with_storage) : VMATree() , _ncs_storage(with_storage), _with_storage(with_storage) { }\n+\n+  ReservedMemoryRegion find_reserved_region(address addr);\n+\n+  SummaryDiff commit_region(address addr, size_t size, const NativeCallStack& stack);\n+  SummaryDiff uncommit_region(address addr, size_t size);\n+  SummaryDiff region_summary(address addr, size_t size);\n+\n+  using Node = VMATree::TreapNode;\n+\n+  class NodeHelper {\n+      Node* _node;\n+      public:\n+      NodeHelper() : _node(nullptr) { }\n+      NodeHelper(Node* node) : _node(node) { }\n+      inline bool is_valid() { return _node != nullptr; }\n+      inline void clear_node() { _node = nullptr; }\n+      inline VMATree::position position() { return _node->key(); }\n+      inline bool is_committed_begin() { return ((uint8_t)out_state() & (uint8_t)VMATree::StateType::Committed) >= 2; }\n+      inline bool is_released_begin() { return out_state() == VMATree::StateType::Released; }\n+      inline bool is_reserved_begin() { return ((uint8_t)out_state() & (uint8_t)VMATree::StateType::Reserved) == 1; }\n+      inline VMATree::StateType in_state() { return _node->val().in.type(); }\n+      inline VMATree::StateType out_state() { return _node->val().out.type(); }\n+      inline size_t distance_from(NodeHelper& other) { return position() - other.position(); }\n+      inline NativeCallStackStorage::StackIndex out_stack_index() { return _node->val().out.stack(); }\n+      inline MemTag in_tag() { return _node->val().in.mem_tag(); }\n+      inline MemTag out_tag() { return _node->val().out.mem_tag(); }\n+      inline void set_in_tag(MemTag tag) { _node->val().in.set_tag(tag); }\n+      inline void set_out_tag(MemTag tag) { _node->val().out.set_tag(tag); }\n+      inline void print_on(outputStream* st) {\n+        auto st_str = [&](int s){\n+          return s == (int)VMATree::StateType::Released ? \"Rl\" :\n+                 s ==  (int)VMATree::StateType::Reserved ? \"Rv\" : \"Cm\";\n+        };\n+        st->print_cr(\"pos: \" INTPTR_FORMAT \" \"\n+                     \"%s, %s <|> %s, %s\",\n+                     p2i((address)position()),\n+                     st_str((int)in_state()),\n+                     NMTUtil::tag_to_name(in_tag()),\n+                     st_str((int)out_state()),\n+                     NMTUtil::tag_to_name(out_tag())\n+                     );\n+      }\n+    };\n+\n+  void print_on(outputStream* st) {\n+    visit_in_order([&](Node* node) {\n+      NodeHelper curr(node);\n+      curr.print_on(st);\n+      return true;\n+    });\n+  }\n+\n+  template<typename F>\n+  void visit_committed_regions(const ReservedMemoryRegion& rgn, F func) {\n+    position start = (position)rgn.base();\n+    size_t end = (size_t)rgn.end() + 1;\n+    size_t comm_size = 0;\n+\n+    NodeHelper prev;\n+    visit_range_in_order(start, end, [&](Node* node) {\n+      NodeHelper curr(node);\n+      if (prev.is_valid() && prev.is_committed_begin()) {\n+        CommittedMemoryRegion cmr((address)prev.position(), curr.distance_from(prev), stack(curr));\n+        if (!func(cmr))\n+          return false;\n+      }\n+      prev = curr;\n+      return true;\n+    });\n+  }\n+\n+  template<typename F>\n+  void visit_reserved_regions(F func) {\n+    NodeHelper begin_node, prev;\n+    size_t rgn_size = 0;\n+\n+    visit_in_order([&](Node* node) {\n+      NodeHelper curr(node);\n+      if (prev.is_valid()) {\n+        rgn_size += curr.distance_from(prev);\n+      } else {\n+        begin_node = curr;\n+        rgn_size = 0;\n+      }\n+      prev = curr;\n+      if (curr.is_released_begin() || begin_node.out_tag() != curr.out_tag()) {\n+        auto st = stack(curr);\n+        if (rgn_size == 0) {\n+          prev.clear_node();\n+          return true;\n+        }\n+        ReservedMemoryRegion rmr((address)begin_node.position(), rgn_size, st, begin_node.out_tag());\n+        if (!func(rmr))\n+          return false;\n+        rgn_size = 0;\n+        if (!curr.is_released_begin())\n+          begin_node = curr;\n+        else {\n+          begin_node.clear_node();\n+          prev.clear_node();\n+        }\n+      }\n+\n+      return true;\n+    });\n+  }\n+\n+  inline RegionData make_region_data(const NativeCallStack& ncs, MemTag tag) {\n+    return RegionData(_ncs_storage.push(ncs), tag);\n+  }\n+\n+  inline const NativeCallStack stack(NodeHelper& node) {\n+    if (!_with_storage) {\n+      return NativeCallStack::empty_stack();\n+    }\n+    NativeCallStackStorage::StackIndex si = node.out_stack_index();\n+    return _ncs_storage.get(si);\n+  }\n+};\n+\n+#endif \/\/ NMT_REGIONSTREE_HPP\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/nmt\/regionsTree.hpp","additions":161,"deletions":0,"binary":false,"changes":161,"status":"added"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -57,1 +57,1 @@\n-  VirtualMemoryTracker::add_reserved_region((address)base, size, stack, mtThreadStack);\n+  MemTracker::record_virtual_memory_reserve((address)base, size, stack, mtThreadStack);\n@@ -67,1 +67,1 @@\n-  VirtualMemoryTracker::remove_released_region((address)base, size);\n+  MemTracker::record_virtual_memory_release((address)base, size);\n","filename":"src\/hotspot\/share\/nmt\/threadStackTracker.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -24,0 +24,1 @@\n+\n@@ -26,5 +27,0 @@\n-#include \"memory\/metaspaceStats.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"nmt\/nativeCallStackPrinter.hpp\"\n-#include \"nmt\/threadStackTracker.hpp\"\n@@ -32,3 +28,1 @@\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/threadCritical.hpp\"\n-#include \"utilities\/ostream.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n@@ -36,0 +30,1 @@\n+VirtualMemoryTracker* VirtualMemoryTracker::Instance::_tracker = nullptr;\n@@ -52,1 +47,1 @@\n-  VirtualMemoryTracker::snapshot_thread_stacks();\n+  VirtualMemoryTracker::Instance::snapshot_thread_stacks();\n@@ -56,196 +51,7 @@\n-SortedLinkedList<ReservedMemoryRegion, compare_reserved_region_base>* VirtualMemoryTracker::_reserved_regions;\n-\n-int compare_committed_region(const CommittedMemoryRegion& r1, const CommittedMemoryRegion& r2) {\n-  return r1.compare(r2);\n-}\n-\n-int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2) {\n-  return r1.compare(r2);\n-}\n-\n-static bool is_mergeable_with(CommittedMemoryRegion* rgn, address addr, size_t size, const NativeCallStack& stack) {\n-  return rgn->adjacent_to(addr, size) && rgn->call_stack()->equals(stack);\n-}\n-\n-static bool is_same_as(CommittedMemoryRegion* rgn, address addr, size_t size, const NativeCallStack& stack) {\n-  \/\/ It would have made sense to use rgn->equals(...), but equals returns true for overlapping regions.\n-  return rgn->same_region(addr, size) && rgn->call_stack()->equals(stack);\n-}\n-\n-static LinkedListNode<CommittedMemoryRegion>* find_preceding_node_from(LinkedListNode<CommittedMemoryRegion>* from, address addr) {\n-  LinkedListNode<CommittedMemoryRegion>* preceding = nullptr;\n-\n-  for (LinkedListNode<CommittedMemoryRegion>* node = from; node != nullptr; node = node->next()) {\n-    CommittedMemoryRegion* rgn = node->data();\n-\n-    \/\/ We searched past the region start.\n-    if (rgn->end() > addr) {\n-      break;\n-    }\n-\n-    preceding = node;\n-  }\n-\n-  return preceding;\n-}\n-\n-static bool try_merge_with(LinkedListNode<CommittedMemoryRegion>* node, address addr, size_t size, const NativeCallStack& stack) {\n-  if (node != nullptr) {\n-    CommittedMemoryRegion* rgn = node->data();\n-\n-    if (is_mergeable_with(rgn, addr, size, stack)) {\n-      rgn->expand_region(addr, size);\n-      return true;\n-    }\n-  }\n-\n-  return false;\n-}\n-\n-static bool try_merge_with(LinkedListNode<CommittedMemoryRegion>* node, LinkedListNode<CommittedMemoryRegion>* other) {\n-  if (other == nullptr) {\n-    return false;\n-  }\n-\n-  CommittedMemoryRegion* rgn = other->data();\n-  return try_merge_with(node, rgn->base(), rgn->size(), *rgn->call_stack());\n-}\n-\n-bool ReservedMemoryRegion::add_committed_region(address addr, size_t size, const NativeCallStack& stack) {\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(size > 0, \"Invalid size\");\n-  assert(contain_region(addr, size), \"Not contain this region\");\n-\n-  \/\/ Find the region that fully precedes the [addr, addr + size) region.\n-  LinkedListNode<CommittedMemoryRegion>* prev = find_preceding_node_from(_committed_regions.head(), addr);\n-  LinkedListNode<CommittedMemoryRegion>* next = (prev != nullptr ? prev->next() : _committed_regions.head());\n-\n-  if (next != nullptr) {\n-    \/\/ Ignore request if region already exists.\n-    if (is_same_as(next->data(), addr, size, stack)) {\n-      return true;\n-    }\n-\n-    \/\/ The new region is after prev, and either overlaps with the\n-    \/\/ next region (and maybe more regions), or overlaps with no region.\n-    if (next->data()->overlap_region(addr, size)) {\n-      \/\/ Remove _all_ overlapping regions, and parts of regions,\n-      \/\/ in preparation for the addition of this new region.\n-      remove_uncommitted_region(addr, size);\n-\n-      \/\/ The remove could have split a region into two and created a\n-      \/\/ new prev region. Need to reset the prev and next pointers.\n-      prev = find_preceding_node_from((prev != nullptr ? prev : _committed_regions.head()), addr);\n-      next = (prev != nullptr ? prev->next() : _committed_regions.head());\n-    }\n-  }\n-\n-  \/\/ At this point the previous overlapping regions have been\n-  \/\/ cleared, and the full region is guaranteed to be inserted.\n-  VirtualMemorySummary::record_committed_memory(size, mem_tag());\n-\n-  \/\/ Try to merge with prev and possibly next.\n-  if (try_merge_with(prev, addr, size, stack)) {\n-    if (try_merge_with(prev, next)) {\n-      \/\/ prev was expanded to contain the new region\n-      \/\/ and next, need to remove next from the list\n-      _committed_regions.remove_after(prev);\n-    }\n-\n-    return true;\n-  }\n-\n-  \/\/ Didn't merge with prev, try with next.\n-  if (try_merge_with(next, addr, size, stack)) {\n-    return true;\n-  }\n-\n-  \/\/ Couldn't merge with any regions - create a new region.\n-  return add_committed_region(CommittedMemoryRegion(addr, size, stack));\n-}\n-\n-bool ReservedMemoryRegion::remove_uncommitted_region(LinkedListNode<CommittedMemoryRegion>* node,\n-  address addr, size_t size) {\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(size > 0, \"Invalid size\");\n-\n-  CommittedMemoryRegion* rgn = node->data();\n-  assert(rgn->contain_region(addr, size), \"Has to be contained\");\n-  assert(!rgn->same_region(addr, size), \"Can not be the same region\");\n-\n-  if (rgn->base() == addr ||\n-      rgn->end() == addr + size) {\n-    rgn->exclude_region(addr, size);\n-    return true;\n-  } else {\n-    \/\/ split this region\n-    address top =rgn->end();\n-    \/\/ use this region for lower part\n-    size_t exclude_size = rgn->end() - addr;\n-    rgn->exclude_region(addr, exclude_size);\n-\n-    \/\/ higher part\n-    address high_base = addr + size;\n-    size_t  high_size = top - high_base;\n-\n-    CommittedMemoryRegion high_rgn(high_base, high_size, *rgn->call_stack());\n-    LinkedListNode<CommittedMemoryRegion>* high_node = _committed_regions.add(high_rgn);\n-    assert(high_node == nullptr || node->next() == high_node, \"Should be right after\");\n-    return (high_node != nullptr);\n-  }\n-\n-  return false;\n-}\n-\n-bool ReservedMemoryRegion::remove_uncommitted_region(address addr, size_t sz) {\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(sz > 0, \"Invalid size\");\n-\n-  CommittedMemoryRegion del_rgn(addr, sz, *call_stack());\n-  address end = addr + sz;\n-\n-  LinkedListNode<CommittedMemoryRegion>* head = _committed_regions.head();\n-  LinkedListNode<CommittedMemoryRegion>* prev = nullptr;\n-  CommittedMemoryRegion* crgn;\n-\n-  while (head != nullptr) {\n-    crgn = head->data();\n-\n-    if (crgn->same_region(addr, sz)) {\n-      VirtualMemorySummary::record_uncommitted_memory(crgn->size(), mem_tag());\n-      _committed_regions.remove_after(prev);\n-      return true;\n-    }\n-\n-    \/\/ del_rgn contains crgn\n-    if (del_rgn.contain_region(crgn->base(), crgn->size())) {\n-      VirtualMemorySummary::record_uncommitted_memory(crgn->size(), mem_tag());\n-      head = head->next();\n-      _committed_regions.remove_after(prev);\n-      continue;  \/\/ don't update head or prev\n-    }\n-\n-    \/\/ Found addr in the current crgn. There are 2 subcases:\n-    if (crgn->contain_address(addr)) {\n-\n-      \/\/ (1) Found addr+size in current crgn as well. (del_rgn is contained in crgn)\n-      if (crgn->contain_address(end - 1)) {\n-        VirtualMemorySummary::record_uncommitted_memory(sz, mem_tag());\n-        return remove_uncommitted_region(head, addr, sz); \/\/ done!\n-      } else {\n-        \/\/ (2) Did not find del_rgn's end in crgn.\n-        size_t size = crgn->end() - del_rgn.base();\n-        crgn->exclude_region(addr, size);\n-        VirtualMemorySummary::record_uncommitted_memory(size, mem_tag());\n-      }\n-\n-    } else if (crgn->contain_address(end - 1)) {\n-      \/\/ Found del_rgn's end, but not its base addr.\n-      size_t size = del_rgn.end() - crgn->base();\n-      crgn->exclude_region(crgn->base(), size);\n-      VirtualMemorySummary::record_uncommitted_memory(size, mem_tag());\n-      return true;  \/\/ should be done if the list is sorted properly!\n-    }\n-\n-    prev = head;\n-    head = head->next();\n+bool VirtualMemoryTracker::Instance::initialize(NMT_TrackingLevel level) {\n+  assert(_tracker == nullptr, \"only call once\");\n+  if (level >= NMT_summary) {\n+    _tracker = static_cast<VirtualMemoryTracker*>(os::malloc(sizeof(VirtualMemoryTracker), mtNMT));\n+    if (_tracker == nullptr) return false;\n+    new (_tracker) VirtualMemoryTracker(level == NMT_detail);\n+    return _tracker->tree() != nullptr;\n@@ -253,1 +59,0 @@\n-\n@@ -257,15 +62,0 @@\n-void ReservedMemoryRegion::move_committed_regions(address addr, ReservedMemoryRegion& rgn) {\n-  assert(addr != nullptr, \"Invalid address\");\n-\n-  \/\/ split committed regions\n-  LinkedListNode<CommittedMemoryRegion>* head =\n-    _committed_regions.head();\n-  LinkedListNode<CommittedMemoryRegion>* prev = nullptr;\n-\n-  while (head != nullptr) {\n-    if (head->data()->base() >= addr) {\n-      break;\n-    }\n-    prev = head;\n-    head = head->next();\n-  }\n@@ -273,61 +63,4 @@\n-  if (head != nullptr) {\n-    if (prev != nullptr) {\n-      prev->set_next(head->next());\n-    } else {\n-      _committed_regions.set_head(nullptr);\n-    }\n-  }\n-\n-  rgn._committed_regions.set_head(head);\n-}\n-\n-size_t ReservedMemoryRegion::committed_size() const {\n-  size_t committed = 0;\n-  LinkedListNode<CommittedMemoryRegion>* head =\n-    _committed_regions.head();\n-  while (head != nullptr) {\n-    committed += head->data()->size();\n-    head = head->next();\n-  }\n-  return committed;\n-}\n-\n-void ReservedMemoryRegion::set_mem_tag(MemTag new_mem_tag) {\n-  assert((mem_tag() == mtNone || mem_tag() == new_mem_tag),\n-         \"Overwrite memory tag for region [\" INTPTR_FORMAT \"-\" INTPTR_FORMAT \"), %u->%u.\",\n-         p2i(base()), p2i(end()), (unsigned)mem_tag(), (unsigned)new_mem_tag);\n-  if (mem_tag() != new_mem_tag) {\n-    VirtualMemorySummary::move_reserved_memory(mem_tag(), new_mem_tag, size());\n-    VirtualMemorySummary::move_committed_memory(mem_tag(), new_mem_tag, committed_size());\n-    _mem_tag = new_mem_tag;\n-  }\n-}\n-\n-address ReservedMemoryRegion::thread_stack_uncommitted_bottom() const {\n-  assert(mem_tag() == mtThreadStack, \"Only for thread stack\");\n-  LinkedListNode<CommittedMemoryRegion>* head = _committed_regions.head();\n-  address bottom = base();\n-  address top = base() + size();\n-  while (head != nullptr) {\n-    address committed_top = head->data()->base() + head->data()->size();\n-    if (committed_top < top) {\n-      \/\/ committed stack guard pages, skip them\n-      bottom = head->data()->base() + head->data()->size();\n-      head = head->next();\n-    } else {\n-      assert(top == committed_top, \"Sanity\");\n-      break;\n-    }\n-  }\n-\n-  return bottom;\n-}\n-\n-bool VirtualMemoryTracker::initialize(NMT_TrackingLevel level) {\n-  assert(_reserved_regions == nullptr, \"only call once\");\n-  if (level >= NMT_summary) {\n-    _reserved_regions = new (std::nothrow, mtNMT)\n-      SortedLinkedList<ReservedMemoryRegion, compare_reserved_region_base>();\n-    return (_reserved_regions != nullptr);\n-  }\n-  return true;\n+bool VirtualMemoryTracker::Instance::add_reserved_region(address base_addr, size_t size,\n+  const NativeCallStack& stack, MemTag mem_tag) {\n+    assert(_tracker != nullptr, \"Sanity check\");\n+    return _tracker->add_reserved_region(base_addr, size, stack, mem_tag);\n@@ -337,38 +70,27 @@\n-    const NativeCallStack& stack, MemTag mem_tag) {\n-  assert(base_addr != nullptr, \"Invalid address\");\n-  assert(size > 0, \"Invalid size\");\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n-  ReservedMemoryRegion  rgn(base_addr, size, stack, mem_tag);\n-  ReservedMemoryRegion* reserved_rgn = _reserved_regions->find(rgn);\n-\n-  log_debug(nmt)(\"Add reserved region \\'%s\\' (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")\",\n-                rgn.mem_tag_name(), p2i(rgn.base()), rgn.size());\n-  if (reserved_rgn == nullptr) {\n-    VirtualMemorySummary::record_reserved_memory(size, mem_tag);\n-    return _reserved_regions->add(rgn) != nullptr;\n-  } else {\n-    \/\/ Deal with recursive reservation\n-    \/\/ os::reserve_memory() -> pd_reserve_memory() -> os::reserve_memory()\n-    \/\/ See JDK-8198226.\n-    if (reserved_rgn->same_region(base_addr, size) &&\n-        (reserved_rgn->mem_tag() == mem_tag || reserved_rgn->mem_tag() == mtNone)) {\n-      reserved_rgn->set_call_stack(stack);\n-      reserved_rgn->set_mem_tag(mem_tag);\n-      return true;\n-    } else {\n-      assert(reserved_rgn->overlap_region(base_addr, size), \"Must be\");\n-\n-      \/\/ Overlapped reservation.\n-      \/\/ It can happen when the regions are thread stacks, as JNI\n-      \/\/ thread does not detach from VM before exits, and leads to\n-      \/\/ leak JavaThread object\n-      if (reserved_rgn->mem_tag() == mtThreadStack) {\n-        guarantee(!CheckJNICalls, \"Attached JNI thread exited without being detached\");\n-        \/\/ Overwrite with new region\n-\n-        \/\/ Release old region\n-        VirtualMemorySummary::record_uncommitted_memory(reserved_rgn->committed_size(), reserved_rgn->mem_tag());\n-        VirtualMemorySummary::record_released_memory(reserved_rgn->size(), reserved_rgn->mem_tag());\n-\n-        \/\/ Add new region\n-        VirtualMemorySummary::record_reserved_memory(rgn.size(), mem_tag);\n+  const NativeCallStack& stack, MemTag mem_tag) {\n+  \/\/ Check overlap\n+  VMATree::SummaryDiff summary = tree()->region_summary(base_addr, size);\n+  VMATree::SingleDiff total{0, 0};\n+  for (int tag = 0; tag < mt_number_of_tags; tag++) {\n+    total.reserve += summary.tag[tag].reserve;\n+    total.commit += summary.tag[tag].commit;\n+  }\n+  bool overlap_accepted = total.reserve == 0;\n+  if (total.reserve != 0) {\n+    \/\/ Overlap with stack region\n+    if (summary.tag[NMTUtil::tag_to_index(mtThreadStack)].reserve != 0) {\n+      guarantee(!CheckJNICalls, \"Attached JNI thread exited without being detached\");\n+      overlap_accepted = true;\n+    }\n+    if (summary.tag[NMTUtil::tag_to_index(mtClassShared)].reserve != 0 ||\n+        summary.tag[NMTUtil::tag_to_index(mtJavaHeap)].reserve != 0 ||\n+        summary.tag[NMTUtil::tag_to_index(mtNone)].reserve != 0\n+        ) {\n+      overlap_accepted = true;\n+    }\n+  }\n+  assert(overlap_accepted, \"overlap regions, total reserved area= \" SIZE_FORMAT \", new region: base= \" INTPTR_FORMAT \", end=\" INTPTR_FORMAT,\n+         (size_t)total.reserve, p2i(base_addr), p2i(base_addr + size));\n+  VMATree::SummaryDiff diff = tree()->reserve_mapping((size_t)base_addr, size, tree()->make_region_data(stack, mem_tag));\n+  apply_summary_diff(diff);\n+  return true;\n@@ -376,3 +98,1 @@\n-        *reserved_rgn = rgn;\n-        return true;\n-      }\n+}\n@@ -380,8 +100,43 @@\n-      \/\/ CDS mapping region.\n-      \/\/ CDS reserves the whole region for mapping CDS archive, then maps each section into the region.\n-      \/\/ NMT reports CDS as a whole.\n-      if (reserved_rgn->mem_tag() == mtClassShared) {\n-        log_debug(nmt)(\"CDS reserved region \\'%s\\' as a whole (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")\",\n-                      reserved_rgn->mem_tag_name(), p2i(reserved_rgn->base()), reserved_rgn->size());\n-        assert(reserved_rgn->contain_region(base_addr, size), \"Reserved CDS region should contain this mapping region\");\n-        return true;\n+void VirtualMemoryTracker::Instance::set_reserved_region_tag(address addr, size_t size, MemTag mem_tag) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  _tracker->set_reserved_region_tag(addr, size, mem_tag);\n+}\n+\n+void VirtualMemoryTracker::set_reserved_region_tag(address addr, size_t size, MemTag mem_tag) {\n+    VMATree::RegionData rd(NativeCallStackStorage::StackIndex(), mem_tag);\n+    VMATree::SummaryDiff diff = tree()->set_tag((VMATree::position) addr, size, mem_tag);\n+    apply_summary_diff(diff);\n+}\n+\n+void VirtualMemoryTracker::Instance::apply_summary_diff(VMATree::SummaryDiff diff) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  _tracker->apply_summary_diff(diff);\n+}\n+\n+void VirtualMemoryTracker::apply_summary_diff(VMATree::SummaryDiff diff) {\n+  VMATree::SingleDiff::delta reserve_delta, commit_delta;\n+  size_t reserved, committed;\n+  MemTag tag = mtNone;\n+  auto print_err = [&](const char* str) {\n+    log_warning(cds)(\"summary mismatch, at %s, for %s,\"\n+                    \" diff-reserved: \" SSIZE_FORMAT\n+                    \" diff-committed: \" SSIZE_FORMAT\n+                    \" vms-reserved: \"  SIZE_FORMAT\n+                    \" vms-committed: \" SIZE_FORMAT,\n+                    str, NMTUtil::tag_to_name(tag), (ssize_t)reserve_delta, (ssize_t)commit_delta, reserved, committed);\n+  };\n+\n+  for (int i = 0; i < mt_number_of_tags; i++) {\n+    reserve_delta = diff.tag[i].reserve;\n+    commit_delta = diff.tag[i].commit;\n+    tag = NMTUtil::index_to_tag(i);\n+    reserved = VirtualMemorySummary::as_snapshot()->by_tag(tag)->reserved();\n+    committed = VirtualMemorySummary::as_snapshot()->by_tag(tag)->committed();\n+    if (reserve_delta != 0) {\n+      if (reserve_delta > 0)\n+        VirtualMemorySummary::record_reserved_memory(reserve_delta, tag);\n+      else {\n+        if ((size_t)-reserve_delta <= reserved)\n+          VirtualMemorySummary::record_released_memory(-reserve_delta, tag);\n+        else\n+          print_err(\"release\");\n@@ -389,8 +144,8 @@\n-\n-      \/\/ Mapped CDS string region.\n-      \/\/ The string region(s) is part of the java heap.\n-      if (reserved_rgn->mem_tag() == mtJavaHeap) {\n-        log_debug(nmt)(\"CDS reserved region \\'%s\\' as a whole (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")\",\n-                      reserved_rgn->mem_tag_name(), p2i(reserved_rgn->base()), reserved_rgn->size());\n-        assert(reserved_rgn->contain_region(base_addr, size), \"Reserved heap region should contain this mapping region\");\n-        return true;\n+    }\n+    if (commit_delta != 0) {\n+      if (commit_delta > 0) {\n+        if ((size_t)commit_delta <= ((size_t)reserve_delta + reserved)) {\n+          VirtualMemorySummary::record_committed_memory(commit_delta, tag);\n+        }\n+        else\n+          print_err(\"commit\");\n@@ -398,11 +153,5 @@\n-\n-      \/\/ Print some more details. Don't use UL here to avoid circularities.\n-      tty->print_cr(\"Error: existing region: [\" INTPTR_FORMAT \"-\" INTPTR_FORMAT \"), memory tag %u.\\n\"\n-                    \"       new region: [\" INTPTR_FORMAT \"-\" INTPTR_FORMAT \"), memory tag %u.\",\n-                    p2i(reserved_rgn->base()), p2i(reserved_rgn->end()), (unsigned)reserved_rgn->mem_tag(),\n-                    p2i(base_addr), p2i(base_addr + size), (unsigned)mem_tag);\n-      if (MemTracker::tracking_level() == NMT_detail) {\n-        tty->print_cr(\"Existing region allocated from:\");\n-        reserved_rgn->call_stack()->print_on(tty);\n-        tty->print_cr(\"New region allocated from:\");\n-        stack.print_on(tty);\n+      else {\n+        if ((size_t)-commit_delta <= committed)\n+          VirtualMemorySummary::record_uncommitted_memory(-commit_delta, tag);\n+        else\n+          print_err(\"uncommit\");\n@@ -410,2 +159,0 @@\n-      ShouldNotReachHere();\n-      return false;\n@@ -416,14 +163,4 @@\n-void VirtualMemoryTracker::set_reserved_region_type(address addr, MemTag mem_tag) {\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n-\n-  ReservedMemoryRegion   rgn(addr, 1);\n-  ReservedMemoryRegion*  reserved_rgn = _reserved_regions->find(rgn);\n-  if (reserved_rgn != nullptr) {\n-    assert(reserved_rgn->contain_address(addr), \"Containment\");\n-    if (reserved_rgn->mem_tag() != mem_tag) {\n-      assert(reserved_rgn->mem_tag() == mtNone, \"Overwrite memory tag (should be mtNone, is: \\\"%s\\\")\",\n-             NMTUtil::tag_to_name(reserved_rgn->mem_tag()));\n-      reserved_rgn->set_mem_tag(mem_tag);\n-    }\n-  }\n+bool VirtualMemoryTracker::Instance::add_committed_region(address addr, size_t size,\n+  const NativeCallStack& stack) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  return _tracker->add_committed_region(addr, size, stack);\n@@ -434,3 +171,8 @@\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(size > 0, \"Invalid size\");\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n+    VMATree::SummaryDiff summary = tree()->region_summary(addr, size);\n+    VMATree::SingleDiff total{0, 0};\n+    for (int tag = 0; tag < mt_number_of_tags; tag++) {\n+      total.reserve += summary.tag[tag].reserve;\n+      total.commit += summary.tag[tag].commit;\n+    }\n+    \/\/assert(!stack.is_empty() || (size_t)total.reserve >= size, \"committing non-reserved region\");\n+    \/\/assert(stack.is_empty() || (size_t)total.commit ==  0, \"committing already committed region\");\n@@ -438,2 +180,4 @@\n-  ReservedMemoryRegion  rgn(addr, size);\n-  ReservedMemoryRegion* reserved_rgn = _reserved_regions->find(rgn);\n+    VMATree::SummaryDiff diff = tree()->commit_region(addr, size, stack);\n+    apply_summary_diff(diff);\n+    return true;\n+}\n@@ -441,10 +185,3 @@\n-  if (reserved_rgn == nullptr) {\n-    log_debug(nmt)(\"Add committed region \\'%s\\', No reserved region found for  (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")\",\n-                  rgn.mem_tag_name(),  p2i(rgn.base()), rgn.size());\n-  }\n-  assert(reserved_rgn != nullptr, \"Add committed region, No reserved region found\");\n-  assert(reserved_rgn->contain_region(addr, size), \"Not completely contained\");\n-  bool result = reserved_rgn->add_committed_region(addr, size, stack);\n-  log_debug(nmt)(\"Add committed region \\'%s\\'(\" INTPTR_FORMAT \", \" SIZE_FORMAT \") %s\",\n-                reserved_rgn->mem_tag_name(),  p2i(rgn.base()), rgn.size(), (result ? \"Succeeded\" : \"Failed\"));\n-  return result;\n+bool VirtualMemoryTracker::Instance::remove_uncommitted_region(address addr, size_t size) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  return _tracker->remove_uncommitted_region(addr, size);\n@@ -454,13 +191,4 @@\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(size > 0, \"Invalid size\");\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n-\n-  ReservedMemoryRegion  rgn(addr, size);\n-  ReservedMemoryRegion* reserved_rgn = _reserved_regions->find(rgn);\n-  assert(reserved_rgn != nullptr, \"No reserved region (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")\", p2i(addr), size);\n-  assert(reserved_rgn->contain_region(addr, size), \"Not completely contained\");\n-  const char* type_name = reserved_rgn->mem_tag_name();  \/\/ after remove, info is not complete\n-  bool result = reserved_rgn->remove_uncommitted_region(addr, size);\n-  log_debug(nmt)(\"Removed uncommitted region \\'%s\\' (\" INTPTR_FORMAT \", \" SIZE_FORMAT \") %s\",\n-                 type_name,  p2i(addr), size, (result ? \" Succeeded\" : \"Failed\"));\n-  return result;\n+  ThreadCritical tc;\n+  VMATree::SummaryDiff diff = tree()->uncommit_region(addr, size);\n+  apply_summary_diff(diff);\n+  return true;\n@@ -469,18 +197,3 @@\n-bool VirtualMemoryTracker::remove_released_region(ReservedMemoryRegion* rgn) {\n-  assert(rgn != nullptr, \"Sanity check\");\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n-\n-  \/\/ uncommit regions within the released region\n-  ReservedMemoryRegion backup(*rgn);\n-  bool result = rgn->remove_uncommitted_region(rgn->base(), rgn->size());\n-  log_debug(nmt)(\"Remove uncommitted region \\'%s\\' (\" INTPTR_FORMAT \", \" SIZE_FORMAT \") %s\",\n-                backup.mem_tag_name(), p2i(backup.base()), backup.size(), (result ? \"Succeeded\" : \"Failed\"));\n-  if (!result) {\n-    return false;\n-  }\n-\n-  VirtualMemorySummary::record_released_memory(rgn->size(), rgn->mem_tag());\n-  result =  _reserved_regions->remove(*rgn);\n-  log_debug(nmt)(\"Removed region \\'%s\\' (\" INTPTR_FORMAT \", \" SIZE_FORMAT \") from _reserved_regions %s\" ,\n-                backup.mem_tag_name(), p2i(backup.base()), backup.size(), (result ? \"Succeeded\" : \"Failed\"));\n-  return result;\n+bool VirtualMemoryTracker::Instance::remove_released_region(address addr, size_t size) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  return _tracker->remove_released_region(addr, size);\n@@ -490,55 +203,3 @@\n-  assert(addr != nullptr, \"Invalid address\");\n-  assert(size > 0, \"Invalid size\");\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n-\n-  ReservedMemoryRegion  rgn(addr, size);\n-  ReservedMemoryRegion* reserved_rgn = _reserved_regions->find(rgn);\n-\n-  if (reserved_rgn == nullptr) {\n-    log_debug(nmt)(\"No reserved region found for (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")!\",\n-                  p2i(rgn.base()), rgn.size());\n-  }\n-  assert(reserved_rgn != nullptr, \"No reserved region\");\n-  if (reserved_rgn->same_region(addr, size)) {\n-    return remove_released_region(reserved_rgn);\n-  }\n-\n-  \/\/ uncommit regions within the released region\n-  if (!reserved_rgn->remove_uncommitted_region(addr, size)) {\n-    return false;\n-  }\n-\n-  if (reserved_rgn->mem_tag() == mtClassShared) {\n-    if (reserved_rgn->contain_region(addr, size)) {\n-      \/\/ This is an unmapped CDS region, which is part of the reserved shared\n-      \/\/ memory region.\n-      \/\/ See special handling in VirtualMemoryTracker::add_reserved_region also.\n-      return true;\n-    }\n-\n-    if (size > reserved_rgn->size()) {\n-      \/\/ This is from release the whole region spanning from archive space to class space,\n-      \/\/ so we release them altogether.\n-      ReservedMemoryRegion class_rgn(addr + reserved_rgn->size(),\n-                                     (size - reserved_rgn->size()));\n-      ReservedMemoryRegion* cls_rgn = _reserved_regions->find(class_rgn);\n-      assert(cls_rgn != nullptr, \"Class space region  not recorded?\");\n-      assert(cls_rgn->mem_tag() == mtClass, \"Must be class mem tag\");\n-      remove_released_region(reserved_rgn);\n-      remove_released_region(cls_rgn);\n-      return true;\n-    }\n-  }\n-\n-  VirtualMemorySummary::record_released_memory(size, reserved_rgn->mem_tag());\n-\n-  assert(reserved_rgn->contain_region(addr, size), \"Not completely contained\");\n-  if (reserved_rgn->base() == addr ||\n-      reserved_rgn->end() == addr + size) {\n-      reserved_rgn->exclude_region(addr, size);\n-    return true;\n-  } else {\n-    address top = reserved_rgn->end();\n-    address high_base = addr + size;\n-    ReservedMemoryRegion high_rgn(high_base, top - high_base,\n-      *reserved_rgn->call_stack(), reserved_rgn->mem_tag());\n+  VMATree::SummaryDiff diff = tree()->release_mapping((VMATree::position)addr, size);\n+  apply_summary_diff(diff);\n+  return true;\n@@ -546,10 +207,0 @@\n-    \/\/ use original region for lower region\n-    reserved_rgn->exclude_region(addr, top - addr);\n-    LinkedListNode<ReservedMemoryRegion>* new_rgn = _reserved_regions->add(high_rgn);\n-    if (new_rgn == nullptr) {\n-      return false;\n-    } else {\n-      reserved_rgn->move_committed_regions(addr, *new_rgn->data());\n-      return true;\n-    }\n-  }\n@@ -558,21 +209,4 @@\n-\/\/ Given an existing memory mapping registered with NMT, split the mapping in\n-\/\/  two. The newly created two mappings will be registered under the call\n-\/\/  stack and the memory tags of the original section.\n-bool VirtualMemoryTracker::split_reserved_region(address addr, size_t size, size_t split, MemTag mem_tag, MemTag split_tag) {\n-\n-  ReservedMemoryRegion  rgn(addr, size);\n-  ReservedMemoryRegion* reserved_rgn = _reserved_regions->find(rgn);\n-  assert(reserved_rgn->same_region(addr, size), \"Must be identical region\");\n-  assert(reserved_rgn != nullptr, \"No reserved region\");\n-  assert(reserved_rgn->committed_size() == 0, \"Splitting committed region?\");\n-\n-  NativeCallStack original_stack = *reserved_rgn->call_stack();\n-  MemTag original_tag = reserved_rgn->mem_tag();\n-\n-  const char* name = reserved_rgn->mem_tag_name();\n-  remove_released_region(reserved_rgn);\n-  log_debug(nmt)(\"Split region \\'%s\\' (\" INTPTR_FORMAT \", \" SIZE_FORMAT \")  with size \" SIZE_FORMAT,\n-                name, p2i(rgn.base()), rgn.size(), split);\n-  \/\/ Now, create two new regions.\n-  add_reserved_region(addr, split, original_stack, mem_tag);\n-  add_reserved_region(addr + split, size - split, original_stack, split_tag);\n+bool VirtualMemoryTracker::Instance::split_reserved_region(address addr, size_t size, size_t split, MemTag mem_tag, MemTag split_mem_tag) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  return _tracker->split_reserved_region(addr, size, split, mem_tag, split_mem_tag);\n+}\n@@ -580,0 +214,3 @@\n+bool VirtualMemoryTracker::split_reserved_region(address addr, size_t size, size_t split, MemTag mem_tag, MemTag split_mem_tag) {\n+  add_reserved_region(addr, split, NativeCallStack::empty_stack(), mem_tag);\n+  add_reserved_region(addr + split, size - split, NativeCallStack::empty_stack(), split_mem_tag);\n@@ -583,0 +220,4 @@\n+bool VirtualMemoryTracker::Instance::print_containing_region(const void* p, outputStream* st) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  return _tracker->print_containing_region(p, st);\n+}\n@@ -584,30 +225,4 @@\n-\/\/ Iterate the range, find committed region within its bound.\n-class RegionIterator : public StackObj {\n-private:\n-  const address _start;\n-  const size_t  _size;\n-\n-  address _current_start;\n-public:\n-  RegionIterator(address start, size_t size) :\n-    _start(start), _size(size), _current_start(start) {\n-  }\n-\n-  \/\/ return true if committed region is found\n-  bool next_committed(address& start, size_t& size);\n-private:\n-  address end() const { return _start + _size; }\n-};\n-\n-bool RegionIterator::next_committed(address& committed_start, size_t& committed_size) {\n-  if (end() <= _current_start) return false;\n-\n-  const size_t page_sz = os::vm_page_size();\n-  const size_t current_size = end() - _current_start;\n-  if (os::committed_in_range(_current_start, current_size, committed_start, committed_size)) {\n-    assert(committed_start != nullptr, \"Must be\");\n-    assert(committed_size > 0 && is_aligned(committed_size, os::vm_page_size()), \"Must be\");\n-\n-    _current_start = committed_start + committed_size;\n-    return true;\n-  } else {\n+bool VirtualMemoryTracker::print_containing_region(const void* p, outputStream* st) {\n+  ReservedMemoryRegion rmr = tree()->find_reserved_region((address)p);\n+  log_debug(nmt)(\"containing rgn: base=\" INTPTR_FORMAT, p2i(rmr.base()));\n+  if (!rmr.contain_address((address)p))\n@@ -615,0 +230,4 @@\n+  st->print_cr(PTR_FORMAT \" in mmap'd memory region [\" PTR_FORMAT \" - \" PTR_FORMAT \"], tag %s\",\n+               p2i(p), p2i(rmr.base()), p2i(rmr.end()), NMTUtil::tag_to_enum_name(rmr.mem_tag()));\n+  if (MemTracker::tracking_level() == NMT_detail) {\n+    rmr.call_stack()->print_on(st);\n@@ -616,0 +235,2 @@\n+  st->cr();\n+  return true;\n@@ -618,42 +239,3 @@\n-\/\/ Walk all known thread stacks, snapshot their committed ranges.\n-class SnapshotThreadStackWalker : public VirtualMemoryWalker {\n-public:\n-  SnapshotThreadStackWalker() {}\n-\n-  bool do_allocation_site(const ReservedMemoryRegion* rgn) {\n-    if (rgn->mem_tag() == mtThreadStack) {\n-      address stack_bottom = rgn->thread_stack_uncommitted_bottom();\n-      address committed_start;\n-      size_t  committed_size;\n-      size_t stack_size = rgn->base() + rgn->size() - stack_bottom;\n-      \/\/ Align the size to work with full pages (Alpine and AIX stack top is not page aligned)\n-      size_t aligned_stack_size = align_up(stack_size, os::vm_page_size());\n-\n-      ReservedMemoryRegion* region = const_cast<ReservedMemoryRegion*>(rgn);\n-      NativeCallStack ncs; \/\/ empty stack\n-\n-      RegionIterator itr(stack_bottom, aligned_stack_size);\n-      DEBUG_ONLY(bool found_stack = false;)\n-      while (itr.next_committed(committed_start, committed_size)) {\n-        assert(committed_start != nullptr, \"Should not be null\");\n-        assert(committed_size > 0, \"Should not be 0\");\n-        \/\/ unaligned stack_size case: correct the region to fit the actual stack_size\n-        if (stack_bottom + stack_size < committed_start + committed_size) {\n-          committed_size = stack_bottom + stack_size - committed_start;\n-        }\n-        region->add_committed_region(committed_start, committed_size, ncs);\n-        DEBUG_ONLY(found_stack = true;)\n-      }\n-#ifdef ASSERT\n-      if (!found_stack) {\n-        log_debug(thread)(\"Thread exited without proper cleanup, may leak thread object\");\n-      }\n-#endif\n-    }\n-    return true;\n-  }\n-};\n-\n-void VirtualMemoryTracker::snapshot_thread_stacks() {\n-  SnapshotThreadStackWalker walker;\n-  walk_virtual_memory(&walker);\n+bool VirtualMemoryTracker::Instance::walk_virtual_memory(VirtualMemoryWalker* walker) {\n+  assert(_tracker != nullptr, \"Sanity check\");\n+  return _tracker->walk_virtual_memory(walker);\n@@ -663,1 +245,0 @@\n-  assert(_reserved_regions != nullptr, \"Sanity check\");\n@@ -665,31 +246,4 @@\n-  \/\/ Check that the _reserved_regions haven't been deleted.\n-  if (_reserved_regions != nullptr) {\n-    LinkedListNode<ReservedMemoryRegion>* head = _reserved_regions->head();\n-    while (head != nullptr) {\n-      const ReservedMemoryRegion* rgn = head->peek();\n-      if (!walker->do_allocation_site(rgn)) {\n-        return false;\n-      }\n-      head = head->next();\n-    }\n-   }\n-  return true;\n-}\n-\n-class PrintRegionWalker : public VirtualMemoryWalker {\n-private:\n-  const address               _p;\n-  outputStream*               _st;\n-  NativeCallStackPrinter      _stackprinter;\n-public:\n-  PrintRegionWalker(const void* p, outputStream* st) :\n-    _p((address)p), _st(st), _stackprinter(st) { }\n-\n-  bool do_allocation_site(const ReservedMemoryRegion* rgn) {\n-    if (rgn->contain_address(_p)) {\n-      _st->print_cr(PTR_FORMAT \" in mmap'd memory region [\" PTR_FORMAT \" - \" PTR_FORMAT \"], tag %s\",\n-        p2i(_p), p2i(rgn->base()), p2i(rgn->base() + rgn->size()), NMTUtil::tag_to_enum_name(rgn->mem_tag()));\n-      if (MemTracker::tracking_level() == NMT_detail) {\n-        _stackprinter.print_stack(rgn->call_stack());\n-        _st->cr();\n-      }\n+  tree()->visit_reserved_regions([&](ReservedMemoryRegion& rgn) {\n+    log_info(nmt)(\"region in walker vmem, base: \" INTPTR_FORMAT \" size: \" SIZE_FORMAT \" , %s, committed: \" SIZE_FORMAT,\n+     p2i(rgn.base()), rgn.size(), rgn.tag_name(), rgn.committed_size());\n+    if (!walker->do_allocation_site(&rgn))\n@@ -697,1 +251,0 @@\n-    }\n@@ -699,9 +252,2 @@\n-  }\n-};\n-\n-\/\/ If p is contained within a known memory region, print information about it to the\n-\/\/ given stream and return true; false otherwise.\n-bool VirtualMemoryTracker::print_containing_region(const void* p, outputStream* st) {\n-  PrintRegionWalker walker(p, st);\n-  return !walk_virtual_memory(&walker);\n-\n+  });\n+  return true;\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.cpp","additions":159,"deletions":613,"binary":false,"changes":772,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,2 +26,2 @@\n-#ifndef SHARE_NMT_VIRTUALMEMORYTRACKER_HPP\n-#define SHARE_NMT_VIRTUALMEMORYTRACKER_HPP\n+#ifndef NMT_VIRTUALMEMORYTRACKER_HPP\n+#define NMT_VIRTUALMEMORYTRACKER_HPP\n@@ -28,4 +29,0 @@\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/metaspace.hpp\" \/\/ For MetadataType\n-#include \"memory\/metaspaceStats.hpp\"\n-#include \"nmt\/allocationSite.hpp\"\n@@ -33,0 +30,1 @@\n+#include \"nmt\/regionsTree.hpp\"\n@@ -34,1 +32,0 @@\n-#include \"utilities\/linkedlist.hpp\"\n@@ -38,4 +35,17 @@\n-\/*\n- * Virtual memory counter\n- *\/\n-class VirtualMemory {\n+\/\/ VirtualMemoryTracker (VMT) is the internal class of NMT that only the MemTracker class uses it for performing the NMT operations.\n+\/\/ All the Hotspot code use only the MemTracker interface to register the memory operations in NMT.\n+\/\/ Memory regions can be reserved\/committed\/uncommitted\/released by calling MemTracker API which in turn call the corresponding functions in VMT.\n+\/\/ VMT uses RegionsTree to hold and manage the memory regions. Each region has two nodes that each one has address of the region (start\/end) and\n+\/\/ state (reserved\/released\/committed) and MemTag of the regions before and after it.\n+\/\/\n+\/\/ The memory operations of Reserve\/Commit\/Uncommit\/Release (RCUR) are tracked by updating\/inserting\/deleting the nodes in the tree. When an operation\n+\/\/ changes nodes in the tree, the summary of the changes is returned back in a SummaryDiff struct. This struct shows that how much reserve\/commit amount\n+\/\/ of any specific MemTag is changed. The summary of every operation is accumulated in VirtualMemorySummary class.\n+\/\/\n+\/\/ Not all operations are valid in VMT. The following predicates are checked before the operation is applied to the tree nad\/or VirtualMemorySummary:\n+\/\/   - committed size of a MemTag should be <= of its reserved size\n+\/\/   - uncommitted size of a MemTag should be <= of its committed size\n+\/\/   - released size of a MemTag should be <= of its reserved size\n+\/\/   - reserving an already reserved\/committed region is not valid\n+\n+class VirtualMemoryTracker {\n@@ -43,20 +53,1 @@\n-  size_t     _reserved;\n-  size_t     _committed;\n-\n-  volatile size_t _peak_size;\n-  void update_peak(size_t size);\n-\n- public:\n-  VirtualMemory() : _reserved(0), _committed(0), _peak_size(0) {}\n-\n-  inline void reserve_memory(size_t sz) { _reserved += sz; }\n-  inline void commit_memory (size_t sz) {\n-    _committed += sz;\n-    assert(_committed <= _reserved, \"Sanity check\");\n-    update_peak(_committed);\n-  }\n-\n-  inline void release_memory (size_t sz) {\n-    assert(_reserved >= sz, \"Negative amount\");\n-    _reserved -= sz;\n-  }\n+  RegionsTree _tree;\n@@ -64,15 +55,0 @@\n-  inline void uncommit_memory(size_t sz) {\n-    assert(_committed >= sz, \"Negative amount\");\n-    _committed -= sz;\n-  }\n-\n-  inline size_t reserved()  const { return _reserved;  }\n-  inline size_t committed() const { return _committed; }\n-  inline size_t peak_size() const {\n-    return Atomic::load(&_peak_size);\n-  }\n-};\n-\n-\/\/ Virtual memory allocation site, keeps track where the virtual memory is reserved.\n-class VirtualMemoryAllocationSite : public AllocationSite {\n-  VirtualMemory _c;\n@@ -80,143 +56,1 @@\n-  VirtualMemoryAllocationSite(const NativeCallStack& stack, MemTag mem_tag) :\n-    AllocationSite(stack, mem_tag) { }\n-\n-  inline void reserve_memory(size_t sz)  { _c.reserve_memory(sz);  }\n-  inline void commit_memory (size_t sz)  { _c.commit_memory(sz);   }\n-  inline size_t reserved() const  { return _c.reserved(); }\n-  inline size_t committed() const { return _c.committed(); }\n-  inline size_t peak_size() const { return _c.peak_size(); }\n-};\n-\n-class VirtualMemorySummary;\n-\n-\/\/ This class represents a snapshot of virtual memory at a given time.\n-\/\/ The latest snapshot is saved in a static area.\n-class VirtualMemorySnapshot : public ResourceObj {\n-  friend class VirtualMemorySummary;\n-\n- private:\n-  VirtualMemory  _virtual_memory[mt_number_of_tags];\n-\n- public:\n-  inline VirtualMemory* by_type(MemTag mem_tag) {\n-    int index = NMTUtil::tag_to_index(mem_tag);\n-    return &_virtual_memory[index];\n-  }\n-\n-  inline const VirtualMemory* by_type(MemTag mem_tag) const {\n-    int index = NMTUtil::tag_to_index(mem_tag);\n-    return &_virtual_memory[index];\n-  }\n-\n-  inline size_t total_reserved() const {\n-    size_t amount = 0;\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      amount += _virtual_memory[index].reserved();\n-    }\n-    return amount;\n-  }\n-\n-  inline size_t total_committed() const {\n-    size_t amount = 0;\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      amount += _virtual_memory[index].committed();\n-    }\n-    return amount;\n-  }\n-\n-  void copy_to(VirtualMemorySnapshot* s) {\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      s->_virtual_memory[index] = _virtual_memory[index];\n-    }\n-  }\n-};\n-\n-class VirtualMemorySummary : AllStatic {\n- public:\n-\n-  static inline void record_reserved_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_type(mem_tag)->reserve_memory(size);\n-  }\n-\n-  static inline void record_committed_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_type(mem_tag)->commit_memory(size);\n-  }\n-\n-  static inline void record_uncommitted_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_type(mem_tag)->uncommit_memory(size);\n-  }\n-\n-  static inline void record_released_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_type(mem_tag)->release_memory(size);\n-  }\n-\n-  \/\/ Move virtual memory from one memory tag to another.\n-  \/\/ Virtual memory can be reserved before it is associated with a memory tag, and tagged\n-  \/\/ as 'unknown'. Once the memory is tagged, the virtual memory will be moved from 'unknown'\n-  \/\/ type to specified memory tag.\n-  static inline void move_reserved_memory(MemTag from, MemTag to, size_t size) {\n-    as_snapshot()->by_type(from)->release_memory(size);\n-    as_snapshot()->by_type(to)->reserve_memory(size);\n-  }\n-\n-  static inline void move_committed_memory(MemTag from, MemTag to, size_t size) {\n-    as_snapshot()->by_type(from)->uncommit_memory(size);\n-    as_snapshot()->by_type(to)->commit_memory(size);\n-  }\n-\n-  static void snapshot(VirtualMemorySnapshot* s);\n-\n-  static VirtualMemorySnapshot* as_snapshot() {\n-    return &_snapshot;\n-  }\n-\n- private:\n-  static VirtualMemorySnapshot _snapshot;\n-};\n-\n-\n-\n-\/*\n- * A virtual memory region\n- *\/\n-class VirtualMemoryRegion {\n- private:\n-  address      _base_address;\n-  size_t       _size;\n-\n- public:\n-  VirtualMemoryRegion(address addr, size_t size) :\n-    _base_address(addr), _size(size) {\n-     assert(addr != nullptr, \"Invalid address\");\n-     assert(size > 0, \"Invalid size\");\n-   }\n-\n-  inline address base() const { return _base_address;   }\n-  inline address end()  const { return base() + size(); }\n-  inline size_t  size() const { return _size;           }\n-\n-  inline bool is_empty() const { return size() == 0; }\n-\n-  inline bool contain_address(address addr) const {\n-    return (addr >= base() && addr < end());\n-  }\n-\n-\n-  inline bool contain_region(address addr, size_t size) const {\n-    return contain_address(addr) && contain_address(addr + size - 1);\n-  }\n-\n-  inline bool same_region(address addr, size_t sz) const {\n-    return (addr == base() && sz == size());\n-  }\n-\n-\n-  inline bool overlap_region(address addr, size_t sz) const {\n-    assert(sz > 0, \"Invalid size\");\n-    assert(size() > 0, \"Invalid size\");\n-    return MAX2(addr, base()) < MIN2(addr + sz, end());\n-  }\n-\n-  inline bool adjacent_to(address addr, size_t sz) const {\n-    return (addr == end() || (addr + sz) == base());\n-  }\n+  VirtualMemoryTracker(bool is_detailed_mode) : _tree(is_detailed_mode) { }\n@@ -224,166 +58,6 @@\n-  void exclude_region(address addr, size_t sz) {\n-    assert(contain_region(addr, sz), \"Not containment\");\n-    assert(addr == base() || addr + sz == end(), \"Can not exclude from middle\");\n-    size_t new_size = size() - sz;\n-\n-    if (addr == base()) {\n-      set_base(addr + sz);\n-    }\n-    set_size(new_size);\n-  }\n-\n-  void expand_region(address addr, size_t sz) {\n-    assert(adjacent_to(addr, sz), \"Not adjacent regions\");\n-    if (base() == addr + sz) {\n-      set_base(addr);\n-    }\n-    set_size(size() + sz);\n-  }\n-\n-  \/\/ Returns 0 if regions overlap; 1 if this region follows rgn;\n-  \/\/  -1 if this region precedes rgn.\n-  inline int compare(const VirtualMemoryRegion& rgn) const {\n-    if (overlap_region(rgn.base(), rgn.size())) {\n-      return 0;\n-    } else if (base() >= rgn.end()) {\n-      return 1;\n-    } else {\n-      assert(rgn.base() >= end(), \"Sanity\");\n-      return -1;\n-    }\n-  }\n-\n-  \/\/ Returns true if regions overlap, false otherwise.\n-  inline bool equals(const VirtualMemoryRegion& rgn) const {\n-    return compare(rgn) == 0;\n-  }\n-\n- protected:\n-  void set_base(address base) {\n-    assert(base != nullptr, \"Sanity check\");\n-    _base_address = base;\n-  }\n-\n-  void set_size(size_t  size) {\n-    assert(size > 0, \"Sanity check\");\n-    _size = size;\n-  }\n-};\n-\n-\n-class CommittedMemoryRegion : public VirtualMemoryRegion {\n- private:\n-  NativeCallStack  _stack;\n-\n- public:\n-  CommittedMemoryRegion(address addr, size_t size, const NativeCallStack& stack) :\n-    VirtualMemoryRegion(addr, size), _stack(stack) { }\n-\n-  inline void set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n-  inline const NativeCallStack* call_stack() const         { return &_stack; }\n-};\n-\n-\n-typedef LinkedListIterator<CommittedMemoryRegion> CommittedRegionIterator;\n-\n-int compare_committed_region(const CommittedMemoryRegion&, const CommittedMemoryRegion&);\n-class ReservedMemoryRegion : public VirtualMemoryRegion {\n- private:\n-  SortedLinkedList<CommittedMemoryRegion, compare_committed_region>\n-    _committed_regions;\n-\n-  NativeCallStack  _stack;\n-  MemTag           _mem_tag;\n-\n- public:\n-  ReservedMemoryRegion(address base, size_t size, const NativeCallStack& stack,\n-    MemTag mem_tag = mtNone) :\n-    VirtualMemoryRegion(base, size), _stack(stack), _mem_tag(mem_tag) { }\n-\n-\n-  ReservedMemoryRegion(address base, size_t size) :\n-    VirtualMemoryRegion(base, size), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n-\n-  \/\/ Copy constructor\n-  ReservedMemoryRegion(const ReservedMemoryRegion& rr) :\n-    VirtualMemoryRegion(rr.base(), rr.size()) {\n-    *this = rr;\n-  }\n-\n-  inline void  set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n-  inline const NativeCallStack* call_stack() const          { return &_stack;  }\n-\n-  void  set_mem_tag(MemTag mem_tag);\n-  inline MemTag mem_tag() const            { return _mem_tag;  }\n-\n-  \/\/ uncommitted thread stack bottom, above guard pages if there is any.\n-  address thread_stack_uncommitted_bottom() const;\n-\n-  bool    add_committed_region(address addr, size_t size, const NativeCallStack& stack);\n-  bool    remove_uncommitted_region(address addr, size_t size);\n-\n-  size_t  committed_size() const;\n-\n-  \/\/ move committed regions that higher than specified address to\n-  \/\/ the new region\n-  void    move_committed_regions(address addr, ReservedMemoryRegion& rgn);\n-\n-  CommittedRegionIterator iterate_committed_regions() const {\n-    return CommittedRegionIterator(_committed_regions.head());\n-  }\n-\n-  ReservedMemoryRegion& operator= (const ReservedMemoryRegion& other) {\n-    set_base(other.base());\n-    set_size(other.size());\n-\n-    _stack   = *other.call_stack();\n-    _mem_tag = other.mem_tag();\n-    _committed_regions.clear();\n-\n-    CommittedRegionIterator itr = other.iterate_committed_regions();\n-    const CommittedMemoryRegion* rgn = itr.next();\n-    while (rgn != nullptr) {\n-      _committed_regions.add(*rgn);\n-      rgn = itr.next();\n-    }\n-\n-    return *this;\n-  }\n-\n-  const char* mem_tag_name() const { return NMTUtil::tag_to_name(_mem_tag); }\n-\n- private:\n-  \/\/ The committed region contains the uncommitted region, subtract the uncommitted\n-  \/\/ region from this committed region\n-  bool remove_uncommitted_region(LinkedListNode<CommittedMemoryRegion>* node,\n-    address addr, size_t sz);\n-\n-  bool add_committed_region(const CommittedMemoryRegion& rgn) {\n-    assert(rgn.base() != nullptr, \"Invalid base address\");\n-    assert(size() > 0, \"Invalid size\");\n-    return _committed_regions.add(rgn) != nullptr;\n-  }\n-};\n-\n-int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2);\n-\n-class VirtualMemoryWalker : public StackObj {\n- public:\n-   virtual bool do_allocation_site(const ReservedMemoryRegion* rgn) { return false; }\n-};\n-\n-\/\/ Main class called from MemTracker to track virtual memory allocations, commits and releases.\n-class VirtualMemoryTracker : AllStatic {\n-  friend class VirtualMemoryTrackerTest;\n-  friend class CommittedVirtualMemoryTest;\n-\n- public:\n-  static bool initialize(NMT_TrackingLevel level);\n-\n-  static bool add_reserved_region (address base_addr, size_t size, const NativeCallStack& stack, MemTag mem_tag = mtNone);\n-\n-  static bool add_committed_region      (address base_addr, size_t size, const NativeCallStack& stack);\n-  static bool remove_uncommitted_region (address base_addr, size_t size);\n-  static bool remove_released_region    (address base_addr, size_t size);\n-  static bool remove_released_region    (ReservedMemoryRegion* rgn);\n-  static void set_reserved_region_type  (address addr, MemTag mem_tag);\n+  bool add_reserved_region       (address base_addr, size_t size, const NativeCallStack& stack, MemTag mem_tag = mtNone);\n+  bool add_committed_region      (address base_addr, size_t size, const NativeCallStack& stack);\n+  bool remove_uncommitted_region (address base_addr, size_t size);\n+  bool remove_released_region    (address base_addr, size_t size);\n+  bool remove_released_region    (ReservedMemoryRegion* rgn);\n+  void set_reserved_region_tag   (address addr, size_t size, MemTag mem_tag);\n@@ -393,2 +67,2 @@\n-  \/\/  stack and the memory tag of the original section.\n-  static bool split_reserved_region(address addr, size_t size, size_t split, MemTag mem_tag, MemTag split_type);\n+  \/\/  stack and the memory tags of the original section.\n+  bool split_reserved_region(address addr, size_t size, size_t split, MemTag mem_tag, MemTag split_mem_tag);\n@@ -397,1 +71,1 @@\n-  static bool walk_virtual_memory(VirtualMemoryWalker* walker);\n+  bool walk_virtual_memory(VirtualMemoryWalker* walker);\n@@ -401,1 +75,1 @@\n-  static bool print_containing_region(const void* p, outputStream* st);\n+  bool print_containing_region(const void* p, outputStream* st);\n@@ -404,4 +78,28 @@\n-  static void snapshot_thread_stacks();\n-\n- private:\n-  static SortedLinkedList<ReservedMemoryRegion, compare_reserved_region_base>* _reserved_regions;\n+  void snapshot_thread_stacks();\n+  void apply_summary_diff(VMATree::SummaryDiff diff);\n+  RegionsTree* tree() { return &_tree; }\n+\n+  class Instance : public AllStatic {\n+    friend class VirtualMemoryTrackerTest;\n+    friend class CommittedVirtualMemoryTest;\n+\n+    static VirtualMemoryTracker* _tracker;\n+\n+   public:\n+    using RegionData = VMATree::RegionData;\n+    static bool initialize(NMT_TrackingLevel level);\n+\n+    static bool add_reserved_region       (address base_addr, size_t size, const NativeCallStack& stack, MemTag mem_tag = mtNone);\n+    static bool add_committed_region      (address base_addr, size_t size, const NativeCallStack& stack);\n+    static bool remove_uncommitted_region (address base_addr, size_t size);\n+    static bool remove_released_region    (address base_addr, size_t size);\n+    static bool remove_released_region    (ReservedMemoryRegion* rgn);\n+    static void set_reserved_region_tag   (address addr, size_t size, MemTag mem_tag);\n+    static bool split_reserved_region(address addr, size_t size, size_t split, MemTag mem_tag, MemTag split_mem_tag);\n+    static bool walk_virtual_memory(VirtualMemoryWalker* walker);\n+    static bool print_containing_region(const void* p, outputStream* st);\n+    static void snapshot_thread_stacks();\n+    static void apply_summary_diff(VMATree::SummaryDiff diff);\n+\n+    static RegionsTree* tree() { return _tracker->tree(); }\n+  };\n@@ -410,2 +108,1 @@\n-#endif \/\/ SHARE_NMT_VIRTUALMEMORYTRACKER_HPP\n-\n+#endif \/\/ NMT_VIRTUALMEMORYTRACKER_HPP\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.hpp","additions":62,"deletions":365,"binary":false,"changes":427,"status":"modified"},{"patch":"@@ -34,2 +34,2 @@\n-const char* VMATree::statetype_strings[3] = {\n-  \"reserved\", \"committed\", \"released\",\n+const char* VMATree::statetype_strings[4] = {\n+  \"released\",\"reserved\", \"only-committed\", \"committed\",\n@@ -78,1 +78,1 @@\n-      assert(leqA_n->val().out.type() != StateType::Released, \"Should not use inplace the tag of a released region\");\n+      \/\/assert(state != StateType::Committed && leqA_n->val().out.type() != StateType::Released, \"Should not use inplace the tag of a released region\");\n@@ -81,0 +81,1 @@\n+      LEQ_A.state.out.set_tag(tag);\n@@ -152,0 +153,1 @@\n+    return true;\n@@ -165,1 +167,0 @@\n-\n@@ -210,1 +211,2 @@\n-  SingleDiff& rescom = diff.tag[NMTUtil::tag_to_index(metadata.mem_tag)];\n+  MemTag tag_to_change = use_tag_inplace ? stA.out.mem_tag() : metadata.mem_tag;\n+  SingleDiff& rescom = diff.tag[NMTUtil::tag_to_index(tag_to_change)];\n@@ -214,1 +216,0 @@\n-    rescom.commit += B - A;\n@@ -216,0 +217,1 @@\n+    rescom.commit += B - A;\n@@ -225,0 +227,1 @@\n+    return true;\n","filename":"src\/hotspot\/share\/nmt\/vmatree.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"nmt\/memTag.hpp\"\n@@ -43,0 +44,1 @@\n+  friend class VMTWithVMATreeTest;\n@@ -58,1 +60,3 @@\n-  enum class StateType : uint8_t { Reserved, Committed, Released, LAST };\n+  \/\/ Bit fields view: bit 0 for Reserved, bit 1 for Committed.\n+  \/\/ Setting a region as Committed preserves the Reserved state.\n+  enum class StateType : uint8_t { Reserved = 1, Committed = 3, Released = 0, COUNT = 4 };\n@@ -61,1 +65,1 @@\n-  static const char* statetype_strings[static_cast<uint8_t>(StateType::LAST)];\n+  static const char* statetype_strings[static_cast<uint8_t>(StateType::COUNT)];\n@@ -67,1 +71,1 @@\n-    assert(type != StateType::LAST, \"must be\");\n+    assert(type < StateType::COUNT, \"must be\");\n@@ -194,0 +198,18 @@\n+\n+    SummaryDiff apply(SummaryDiff other) {\n+      SummaryDiff out;\n+      for (int i = 0; i < mt_number_of_tags; i++) {\n+        out.tag[i] = SingleDiff {\n+          this->tag[i].reserve + other.tag[i].reserve,\n+          this->tag[i].commit + other.tag[i].commit\n+        };\n+      }\n+      return out;\n+    }\n+\n+    void print_self() {\n+      for (int i = 0; i < mt_number_of_tags; i++) {\n+        if (tag[i].reserve == 0 && tag[i].commit == 0) { continue; }\n+        tty->print_cr(\"Flag %s R: \" INT64_FORMAT \" C: \" INT64_FORMAT, NMTUtil::tag_to_enum_name((MemTag)i), tag[i].reserve, tag[i].commit);\n+      }\n+    }\n@@ -218,6 +240,2 @@\n-  SummaryDiff release_mapping(position from, size size) {\n-    return register_mapping(from, from + size, StateType::Released, VMATree::empty_regiondata);\n-  }\n-\n-  VMATreap& tree() {\n-    return _tree;\n+  SummaryDiff release_mapping(position from, position sz) {\n+    return register_mapping(from, from + sz, StateType::Released, VMATree::empty_regiondata);\n@@ -236,0 +254,14 @@\n+  template<typename F>\n+  void visit_range_in_order(const position& from, const position& to, F f) {\n+    _tree.visit_range_in_order(from, to, f);\n+  }\n+\n+  VMATreap& tree() { return _tree; }\n+\n+  void print_self() {\n+    visit_in_order([&](TreapNode* current) {\n+      tty->print(\"(%s) - %s - \", NMTUtil::tag_to_name(current->val().out.mem_tag()), statetype_to_string(current->val().out.type()));\n+      return true;\n+    });\n+    tty->cr();\n+  }\n","filename":"src\/hotspot\/share\/nmt\/vmatree.hpp","additions":41,"deletions":9,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -0,0 +1,150 @@\n+\/*\n+ * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/metaspaceStats.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"nmt\/nativeCallStackPrinter.hpp\"\n+#include \"nmt\/threadStackTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/threadCritical.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+\n+int compare_committed_region(const CommittedMemoryRegion& r1, const CommittedMemoryRegion& r2) {\n+  return r1.compare(r2);\n+}\n+\n+int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2) {\n+  return r1.compare(r2);\n+}\n+\n+size_t ReservedMemoryRegion::committed_size() const {\n+  size_t committed = 0;\n+  size_t result = 0;\n+  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*this, [&](CommittedMemoryRegion& crgn) {\n+    result += crgn.size();\n+    return true;\n+  });\n+  return result;\n+}\n+\n+address ReservedMemoryRegion::thread_stack_uncommitted_bottom() const {\n+  address bottom = base();\n+  address top = base() + size();\n+  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*this, [&](CommittedMemoryRegion& crgn) {\n+    address committed_top = crgn.base() + crgn.size();\n+    if (committed_top < top) {\n+      \/\/ committed stack guard pages, skip them\n+      bottom = crgn.base() + crgn.size();\n+    } else {\n+      assert(top == committed_top, \"Sanity, top=\" INTPTR_FORMAT \" , com-top=\" INTPTR_FORMAT, p2i(top), p2i(committed_top));\n+      return false;;\n+    }\n+    return true;\n+  });\n+\n+  return bottom;\n+}\n+\n+\/\/ Iterate the range, find committed region within its bound.\n+class RegionIterator : public StackObj {\n+private:\n+  const address _start;\n+  const size_t  _size;\n+\n+  address _current_start;\n+public:\n+  RegionIterator(address start, size_t size) :\n+    _start(start), _size(size), _current_start(start) {\n+  }\n+\n+  \/\/ return true if committed region is found\n+  bool next_committed(address& start, size_t& size);\n+private:\n+  address end() const { return _start + _size; }\n+};\n+\n+bool RegionIterator::next_committed(address& committed_start, size_t& committed_size) {\n+  if (end() <= _current_start) return false;\n+\n+  const size_t page_sz = os::vm_page_size();\n+  const size_t current_size = end() - _current_start;\n+  if (os::committed_in_range(_current_start, current_size, committed_start, committed_size)) {\n+    assert(committed_start != nullptr, \"Must be\");\n+    assert(committed_size > 0 && is_aligned(committed_size, os::vm_page_size()), \"Must be\");\n+\n+    _current_start = committed_start + committed_size;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Walk all known thread stacks, snapshot their committed ranges.\n+class SnapshotThreadStackWalker : public VirtualMemoryWalker {\n+public:\n+  SnapshotThreadStackWalker() {}\n+\n+  bool do_allocation_site(const ReservedMemoryRegion* rgn) {\n+    if (rgn->mem_tag() == mtThreadStack) {\n+      address stack_bottom = rgn->thread_stack_uncommitted_bottom();\n+      address committed_start;\n+      size_t  committed_size;\n+      size_t stack_size = rgn->base() + rgn->size() - stack_bottom;\n+      \/\/ Align the size to work with full pages (Alpine and AIX stack top is not page aligned)\n+      size_t aligned_stack_size = align_up(stack_size, os::vm_page_size());\n+\n+      NativeCallStack ncs; \/\/ empty stack\n+\n+      RegionIterator itr(stack_bottom, aligned_stack_size);\n+      DEBUG_ONLY(bool found_stack = false;)\n+      while (itr.next_committed(committed_start, committed_size)) {\n+        assert(committed_start != nullptr, \"Should not be null\");\n+        assert(committed_size > 0, \"Should not be 0\");\n+        \/\/ unaligned stack_size case: correct the region to fit the actual stack_size\n+        if (stack_bottom + stack_size < committed_start + committed_size) {\n+          committed_size = stack_bottom + stack_size - committed_start;\n+        }\n+        VirtualMemoryTracker::Instance::add_committed_region(committed_start, committed_size, ncs);\n+        \/\/log_warning(cds)(\"st start: \" INTPTR_FORMAT \" size: \" SIZE_FORMAT, p2i(committed_start), committed_size);\n+        DEBUG_ONLY(found_stack = true;)\n+      }\n+#ifdef ASSERT\n+      if (!found_stack) {\n+        log_debug(thread)(\"Thread exited without proper cleanup, may leak thread object\");\n+      }\n+#endif\n+    }\n+    return true;\n+  }\n+};\n+\n+void VirtualMemoryTracker::Instance::snapshot_thread_stacks() {\n+  SnapshotThreadStackWalker walker;\n+  walk_virtual_memory(&walker);\n+}\n+\n","filename":"src\/hotspot\/share\/nmt\/vmtCommon.cpp","additions":150,"deletions":0,"binary":false,"changes":150,"status":"added"},{"patch":"@@ -0,0 +1,357 @@\n+\/*\n+ * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef NMT_VMTCOMMON_HPP\n+#define NMT_VMTCOMMON_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/metaspace.hpp\" \/\/ For MetadataType\n+#include \"memory\/metaspaceStats.hpp\"\n+#include \"nmt\/allocationSite.hpp\"\n+#include \"nmt\/nmtCommon.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/nativeCallStack.hpp\"\n+#include \"utilities\/linkedlist.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+\/*\n+ * Virtual memory counter\n+ *\/\n+class VirtualMemory {\n+ private:\n+  size_t     _reserved;\n+  size_t     _committed;\n+\n+  volatile size_t _peak_size;\n+  void update_peak(size_t size);\n+\n+ public:\n+  VirtualMemory() : _reserved(0), _committed(0), _peak_size(0) {}\n+\n+  inline void reserve_memory(size_t sz) { _reserved += sz; }\n+  inline void commit_memory (size_t sz) {\n+    _committed += sz;\n+    assert(_committed <= _reserved, \"Sanity check\");\n+    update_peak(_committed);\n+  }\n+\n+  inline void release_memory (size_t sz) {\n+    assert(_reserved >= sz, \"Negative amount\");\n+    _reserved -= sz;\n+  }\n+\n+  inline void uncommit_memory(size_t sz) {\n+    assert(_committed >= sz, \"Negative amount\");\n+    _committed -= sz;\n+  }\n+\n+  inline size_t reserved()  const { return _reserved;  }\n+  inline size_t committed() const { return _committed; }\n+  inline size_t peak_size() const {\n+    return Atomic::load(&_peak_size);\n+  }\n+};\n+\n+\/\/ Virtual memory allocation site, keeps track where the virtual memory is reserved.\n+class VirtualMemoryAllocationSite : public AllocationSite {\n+  VirtualMemory _c;\n+ public:\n+  VirtualMemoryAllocationSite(const NativeCallStack& stack, MemTag mem_tag) :\n+    AllocationSite(stack, mem_tag) { }\n+\n+  inline void reserve_memory(size_t sz)  { _c.reserve_memory(sz);  }\n+  inline void commit_memory (size_t sz)  { _c.commit_memory(sz);   }\n+  inline size_t reserved() const  { return _c.reserved(); }\n+  inline size_t committed() const { return _c.committed(); }\n+  inline size_t peak_size() const { return _c.peak_size(); }\n+};\n+\n+class VirtualMemorySummary;\n+\n+\/\/ This class represents a snapshot of virtual memory at a given time.\n+\/\/ The latest snapshot is saved in a static area.\n+class VirtualMemorySnapshot : public ResourceObj {\n+  friend class VirtualMemorySummary;\n+\n+ private:\n+  VirtualMemory  _virtual_memory[mt_number_of_tags];\n+\n+ public:\n+  inline VirtualMemory* by_tag(MemTag mem_tag) {\n+    int index = NMTUtil::tag_to_index(mem_tag);\n+    return &_virtual_memory[index];\n+  }\n+\n+  inline const VirtualMemory* by_tag(MemTag mem_tag) const {\n+    int index = NMTUtil::tag_to_index(mem_tag);\n+    return &_virtual_memory[index];\n+  }\n+\n+  inline void clean() {\n+\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      if (index != NMTUtil::tag_to_index(mtThreadStack))\n+        _virtual_memory[index] = VirtualMemory();\n+    }\n+  }\n+\n+  inline size_t total_reserved() const {\n+    size_t amount = 0;\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      amount += _virtual_memory[index].reserved();\n+    }\n+    return amount;\n+  }\n+\n+  inline size_t total_committed() const {\n+    size_t amount = 0;\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      amount += _virtual_memory[index].committed();\n+    }\n+    return amount;\n+  }\n+\n+  void copy_to(VirtualMemorySnapshot* s) {\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      s->_virtual_memory[index] = _virtual_memory[index];\n+    }\n+  }\n+};\n+\n+class VirtualMemorySummary : AllStatic {\n+ public:\n+\n+  static inline void record_reserved_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->reserve_memory(size);\n+  }\n+\n+  static inline void record_committed_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->commit_memory(size);\n+  }\n+\n+  static inline void record_uncommitted_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->uncommit_memory(size);\n+  }\n+\n+  static inline void record_released_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->release_memory(size);\n+  }\n+\n+  \/\/ Move virtual memory from one memory tag to another.\n+  \/\/ Virtual memory can be reserved before it is associated with a memory tag, and tagged\n+  \/\/ as 'unknown'. Once the memory is tagged, the virtual memory will be moved from 'unknown'\n+  \/\/ tag to specified memory tag.\n+  static inline void move_reserved_memory(MemTag from, MemTag to, size_t size) {\n+    as_snapshot()->by_tag(from)->release_memory(size);\n+    as_snapshot()->by_tag(to)->reserve_memory(size);\n+  }\n+\n+  static inline void move_committed_memory(MemTag from, MemTag to, size_t size) {\n+    as_snapshot()->by_tag(from)->uncommit_memory(size);\n+    as_snapshot()->by_tag(to)->commit_memory(size);\n+  }\n+\n+  static void snapshot(VirtualMemorySnapshot* s);\n+\n+  static VirtualMemorySnapshot* as_snapshot() {\n+    return &_snapshot;\n+  }\n+\n+ private:\n+  static VirtualMemorySnapshot _snapshot;\n+};\n+\n+\n+\n+\/*\n+ * A virtual memory region\n+ *\/\n+class VirtualMemoryRegion {\n+ private:\n+  address      _base_address;\n+  size_t       _size;\n+\n+ public:\n+  VirtualMemoryRegion(address addr, size_t size) :\n+    _base_address(addr), _size(size) {\n+     assert(addr != nullptr, \"Invalid address\");\n+     assert(size > 0, \"Invalid size\");\n+   }\n+\n+  inline address base() const { return _base_address;   }\n+  inline address end()  const { return base() + size(); }\n+  inline size_t  size() const { return _size;           }\n+\n+  inline bool is_empty() const { return size() == 0; }\n+\n+  inline bool contain_address(address addr) const {\n+    return (addr >= base() && addr < end());\n+  }\n+\n+\n+  inline bool contain_region(address addr, size_t size) const {\n+    return contain_address(addr) && contain_address(addr + size - 1);\n+  }\n+\n+  inline bool same_region(address addr, size_t sz) const {\n+    return (addr == base() && sz == size());\n+  }\n+\n+\n+  inline bool overlap_region(address addr, size_t sz) const {\n+    assert(sz > 0, \"Invalid size\");\n+    assert(size() > 0, \"Invalid size\");\n+    return MAX2(addr, base()) < MIN2(addr + sz, end());\n+  }\n+\n+  inline bool adjacent_to(address addr, size_t sz) const {\n+    return (addr == end() || (addr + sz) == base());\n+  }\n+\n+  void exclude_region(address addr, size_t sz) {\n+    assert(contain_region(addr, sz), \"Not containment\");\n+    assert(addr == base() || addr + sz == end(), \"Can not exclude from middle\");\n+    size_t new_size = size() - sz;\n+\n+    if (addr == base()) {\n+      set_base(addr + sz);\n+    }\n+    set_size(new_size);\n+  }\n+\n+  void expand_region(address addr, size_t sz) {\n+    assert(adjacent_to(addr, sz), \"Not adjacent regions\");\n+    if (base() == addr + sz) {\n+      set_base(addr);\n+    }\n+    set_size(size() + sz);\n+  }\n+\n+  \/\/ Returns 0 if regions overlap; 1 if this region follows rgn;\n+  \/\/  -1 if this region precedes rgn.\n+  inline int compare(const VirtualMemoryRegion& rgn) const {\n+    if (overlap_region(rgn.base(), rgn.size())) {\n+      return 0;\n+    } else if (base() >= rgn.end()) {\n+      return 1;\n+    } else {\n+      assert(rgn.base() >= end(), \"Sanity\");\n+      return -1;\n+    }\n+  }\n+\n+  \/\/ Returns true if regions overlap, false otherwise.\n+  inline bool equals(const VirtualMemoryRegion& rgn) const {\n+    return compare(rgn) == 0;\n+  }\n+\n+ protected:\n+  void set_base(address base) {\n+    assert(base != nullptr, \"Sanity check\");\n+    _base_address = base;\n+  }\n+\n+  void set_size(size_t  size) {\n+    assert(size > 0, \"Sanity check\");\n+    _size = size;\n+  }\n+};\n+\n+\n+class CommittedMemoryRegion : public VirtualMemoryRegion {\n+ private:\n+  NativeCallStack  _stack;\n+\n+ public:\n+  CommittedMemoryRegion() :\n+    VirtualMemoryRegion((address)1, 1), _stack(NativeCallStack::empty_stack()) { }\n+\n+  CommittedMemoryRegion(address addr, size_t size, const NativeCallStack& stack) :\n+    VirtualMemoryRegion(addr, size), _stack(stack) { }\n+\n+  inline void set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n+  inline const NativeCallStack* call_stack() const         { return &_stack; }\n+};\n+\n+\n+typedef LinkedListIterator<CommittedMemoryRegion> CommittedRegionIterator;\n+\n+int compare_committed_region(const CommittedMemoryRegion&, const CommittedMemoryRegion&);\n+class ReservedMemoryRegion : public VirtualMemoryRegion {\n+ private:\n+  NativeCallStack  _stack;\n+  MemTag         _mem_tag;\n+\n+ public:\n+  bool is_valid() { return base() != (address)1 && size() != 1;}\n+  ReservedMemoryRegion() :\n+    VirtualMemoryRegion((address)1, 1), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n+\n+  ReservedMemoryRegion(address base, size_t size, const NativeCallStack& stack,\n+    MemTag mem_tag = mtNone) :\n+    VirtualMemoryRegion(base, size), _stack(stack), _mem_tag(mem_tag) { }\n+\n+\n+  ReservedMemoryRegion(address base, size_t size) :\n+    VirtualMemoryRegion(base, size), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n+\n+  \/\/ Copy constructor\n+  ReservedMemoryRegion(const ReservedMemoryRegion& rr) :\n+    VirtualMemoryRegion(rr.base(), rr.size()) {\n+    *this = rr;\n+  }\n+\n+  inline void  set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n+  inline const NativeCallStack* call_stack() const          { return &_stack;  }\n+\n+  inline MemTag mem_tag() const            { return _mem_tag;  }\n+\n+  \/\/ uncommitted thread stack bottom, above guard pages if there is any.\n+  address thread_stack_uncommitted_bottom() const;\n+\n+  size_t committed_size() const;\n+\n+\n+  ReservedMemoryRegion& operator= (const ReservedMemoryRegion& other) {\n+    set_base(other.base());\n+    set_size(other.size());\n+\n+    _stack = *other.call_stack();\n+    _mem_tag = other.mem_tag();\n+\n+    return *this;\n+  }\n+\n+  const char* tag_name() const { return NMTUtil::tag_to_name(_mem_tag); }\n+};\n+\n+int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2);\n+\n+class VirtualMemoryWalker : public StackObj {\n+ public:\n+   virtual bool do_allocation_site(const ReservedMemoryRegion* rgn) { return false; }\n+};\n+\n+#endif \/\/ NMT_VMTCOMMON_HPP\n+\n","filename":"src\/hotspot\/share\/nmt\/vmtCommon.hpp","additions":357,"deletions":0,"binary":false,"changes":357,"status":"added"},{"patch":"@@ -2228,1 +2228,0 @@\n-    ThreadCritical tc;\n@@ -2231,0 +2230,1 @@\n+      ThreadCritical tc;\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/debug.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+  friend class VMTWithVMATreeTest;\n","filename":"src\/hotspot\/share\/utilities\/nativeCallStack.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(100));\n+    EXPECT_EQ(file->_summary.by_tag(mtTest)->committed(), sz(100));\n@@ -39,1 +39,1 @@\n-    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(200));\n+    EXPECT_EQ(file->_summary.by_tag(mtTest)->committed(), sz(200));\n@@ -41,1 +41,1 @@\n-    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(300));\n+    EXPECT_EQ(file->_summary.by_tag(mtTest)->committed(), sz(300));\n@@ -43,1 +43,1 @@\n-    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(0));\n+    EXPECT_EQ(file->_summary.by_tag(mtTest)->committed(), sz(0));\n@@ -45,1 +45,1 @@\n-    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(100));\n+    EXPECT_EQ(file->_summary.by_tag(mtTest)->committed(), sz(100));\n@@ -47,1 +47,1 @@\n-    EXPECT_EQ(file->_summary.by_type(mtTest)->committed(), sz(90));\n+    EXPECT_EQ(file->_summary.by_tag(mtTest)->committed(), sz(90));\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_memoryfiletracker.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1,55 +0,0 @@\n-\/*\n- * Copyright (c) 2023 SAP SE. All rights reserved.\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"nmt\/nmtCommon.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"unittest.hpp\"\n-\n-\/\/ Tests the assignment operator of ReservedMemoryRegion\n-TEST_VM(NMT, ReservedRegionCopy) {\n-  address dummy1 = (address)0x10000000;\n-  NativeCallStack stack1(&dummy1, 1);\n-  ReservedMemoryRegion region1(dummy1, os::vm_page_size(), stack1, mtThreadStack);\n-  VirtualMemorySummary::record_reserved_memory(os::vm_page_size(), region1.mem_tag());\n-  region1.add_committed_region(dummy1, os::vm_page_size(), stack1);\n-  address dummy2 = (address)0x20000000;\n-  NativeCallStack stack2(&dummy2, 1);\n-  ReservedMemoryRegion region2(dummy2, os::vm_page_size(), stack2, mtCode);\n-  VirtualMemorySummary::record_reserved_memory(os::vm_page_size(), region2.mem_tag());\n-  region2.add_committed_region(dummy2, os::vm_page_size(), stack2);\n-\n-  region2 = region1;\n-\n-  CommittedRegionIterator itr = region2.iterate_committed_regions();\n-  const CommittedMemoryRegion* rgn = itr.next();\n-  ASSERT_EQ(rgn->base(), dummy1); \/\/ Now we should see dummy1\n-  ASSERT_EQ(region2.mem_tag(), mtThreadStack); \/\/ Should be correct memory tag\n-  ASSERT_EQ(region2.call_stack()->get_frame(0), dummy1); \/\/ Check the stack\n-  rgn = itr.next();\n-  ASSERT_EQ(rgn, (const CommittedMemoryRegion*)nullptr); \/\/ and nothing else\n-}\n-\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_reserved_region.cpp","additions":0,"deletions":55,"binary":false,"changes":55,"status":"deleted"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -76,0 +77,1 @@\n+      return true;\n@@ -148,0 +150,90 @@\n+\n+  static double treap_upsert(int n) {\n+    TreapCHeap<int, int, Cmp> treap;\n+    double st = os::elapsedTime();\n+    for (int i = 0; i < n; i++) {\n+      int a = (os::random() % n) * 100;\n+      treap.upsert(a, 0);\n+    }\n+    double d = os::elapsedTime() - st;\n+    return d;\n+  }\n+\n+  static double treap_remove(int n) {\n+    TreapCHeap<int, int, Cmp> treap;\n+    for (int i = 0; i < (n + 100); i++) {\n+      treap.upsert(i * 100, 0);\n+    }\n+    double st = os::elapsedTime();\n+    for (int i = 0; i < n; i++) {\n+      int a = (os::random() % n) * 100;\n+      treap.remove(a);\n+    }\n+    double d = os::elapsedTime() - st;\n+    return d;\n+  }\n+\n+\n+  static double sorted_list_insert(int n) {\n+    SortedLinkedList<ReservedMemoryRegion, compare_reserved_region_base> regions;\n+    const size_t page_size = 1024;\n+    double st = os::elapsedTime();\n+    for (int i = 0; i < n; i++) {\n+      int page_no = os::random() % n;\n+      ReservedMemoryRegion rmr((address) (1000UL + page_no * page_size), page_size - 128);\n+      regions.add(rmr);\n+    }\n+    return os::elapsedTime() - st;\n+  }\n+\n+  static double sorted_list_remove(int n) {\n+    SortedLinkedList<ReservedMemoryRegion, compare_reserved_region_base> regions;\n+    const size_t page_size = 1024;\n+    for (int i = 0; i < n; i++) {\n+      int page_no = i;\n+      ReservedMemoryRegion rmr((address) (1000UL + page_no * page_size), page_size - 128);\n+      regions.add(rmr);\n+    }\n+    double st = os::elapsedTime();\n+    for (int i = 0; i < n; i++) {\n+      int page_no = os::random() % n;\n+      ReservedMemoryRegion rmr((address) (1000UL + page_no * page_size), page_size - 128);\n+      regions.remove(rmr);\n+    }\n+    return os::elapsedTime() - st;\n+  }\n+\n+  static void compare_insert_SLL_with_Treap() {\n+    const int N = 10000;\n+    const int REPEATS = 10;\n+    double sll_sum = 0;\n+    double treap_sum = 0;\n+    int unexpected_count = 0;\n+    for (int i = 0; i < REPEATS; i++) {\n+      double d_sll = sorted_list_insert(N);\n+      double d_treap = treap_upsert(N);\n+      if (d_sll < d_treap) unexpected_count++;\n+      sll_sum += d_sll;\n+      treap_sum += d_treap;\n+    }\n+    tty->print_cr(\"Insert Test *** Unexp.Cnt: %d, SSL Avg: %lf, Treap Avg: %lf, SLL\/Treap: %lf\", unexpected_count, sll_sum \/ REPEATS, treap_sum \/ REPEATS, sll_sum \/ treap_sum);\n+    EXPECT_LE(unexpected_count, REPEATS \/ 2) << \"SSL Avg: \" << sll_sum \/ REPEATS << \" Treap Avg: \" << treap_sum \/ REPEATS;\n+  }\n+\n+  static void compare_remove_SLL_with_Treap() {\n+    const int N = 10000;\n+    const int REPEATS = 10;\n+    double sll_sum = 0;\n+    double treap_sum = 0;\n+    int unexpected_count = 0;\n+    for (int i = 0; i < REPEATS; i++) {\n+      double d_sll = sorted_list_remove(N);\n+      double d_treap = treap_remove(N);\n+      if (d_sll < d_treap) unexpected_count++;\n+      sll_sum += d_sll;\n+      treap_sum += d_treap;\n+    }\n+    tty->print_cr(\"Remove Test *** Unexp.Cnt: %d, SSL Avg: %lf, Treap Avg: %lf, SLL\/Treap: %lf\", unexpected_count, sll_sum \/ REPEATS, treap_sum \/ REPEATS, sll_sum \/ treap_sum);\n+    EXPECT_LE(unexpected_count, REPEATS \/ 2) << \"SSL Avg: \" << sll_sum \/ REPEATS << \" Treap Avg: \" << treap_sum \/ REPEATS;\n+  }\n+\n@@ -165,0 +257,1 @@\n+      return true;\n@@ -172,0 +265,1 @@\n+      return true;\n@@ -178,0 +272,1 @@\n+      return true;\n@@ -188,0 +283,1 @@\n+      return true;\n@@ -194,0 +290,1 @@\n+      return true;\n@@ -201,0 +298,1 @@\n+      return true;\n@@ -212,0 +310,1 @@\n+      return true;\n@@ -221,0 +320,1 @@\n+      return true;\n@@ -230,0 +330,1 @@\n+      return true;\n@@ -245,0 +346,1 @@\n+      return true;\n@@ -254,0 +356,1 @@\n+      return true;\n@@ -290,0 +393,5 @@\n+TEST_VM_F(NMTTreapTest, PerformanceComparison) {\n+  compare_insert_SLL_with_Treap();\n+  compare_remove_SLL_with_Treap();\n+}\n+\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_treap.cpp","additions":108,"deletions":0,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -0,0 +1,109 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"nmt\/nmtNativeCallStackStorage.hpp\"\n+#include \"nmt\/regionsTree.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+\n+class NMTRegionsTreeTest : public testing::Test {\n+ public:\n+  RegionsTree rt;\n+  NMTRegionsTreeTest() : rt(true) { }\n+};\n+\n+TEST_VM_F(NMTRegionsTreeTest, CommitUncommitRegion) {\n+  NativeCallStack ncs;\n+  VMATree::RegionData rd = rt.make_region_data(ncs, mtTest);\n+  rt.reserve_mapping(0, 100, rd);\n+  VMATree::SummaryDiff diff = rt.commit_region(0, 50, ncs);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(50, diff.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+  diff = rt.commit_region((address)60, 10, ncs);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(10, diff.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+  diff = rt.uncommit_region(0, 50);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(-50, diff.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+}\n+\n+TEST_VM_F(NMTRegionsTreeTest, FindReservedRegion) {\n+  NativeCallStack ncs;\n+  VMATree::RegionData rd = rt.make_region_data(ncs, mtTest);\n+  rt.reserve_mapping(1000, 50, rd);\n+  rt.reserve_mapping(1200, 50, rd);\n+  rt.reserve_mapping(1300, 50, rd);\n+  rt.reserve_mapping(1400, 50, rd);\n+  ReservedMemoryRegion rmr;\n+  rmr = rt.find_reserved_region((address)1205);\n+  EXPECT_EQ(rmr.base(), (address)1200);\n+  rmr = rt.find_reserved_region((address)1305);\n+  EXPECT_EQ(rmr.base(), (address)1300);\n+  rmr = rt.find_reserved_region((address)1405);\n+  EXPECT_EQ(rmr.base(), (address)1400);\n+  rmr = rt.find_reserved_region((address)1005);\n+  EXPECT_EQ(rmr.base(), (address)1000);\n+}\n+\n+TEST_VM_F(NMTRegionsTreeTest, VisitReservedRegions) {\n+  NativeCallStack ncs;\n+  VMATree::RegionData rd = rt.make_region_data(ncs, mtTest);\n+  rt.reserve_mapping(1000, 50, rd);\n+  rt.reserve_mapping(1200, 50, rd);\n+  rt.reserve_mapping(1300, 50, rd);\n+  rt.reserve_mapping(1400, 50, rd);\n+\n+  rt.visit_reserved_regions([&](const ReservedMemoryRegion& rgn) {\n+    EXPECT_EQ(((size_t)rgn.base()) % 100, 0UL);\n+    EXPECT_EQ(rgn.size(), 50UL);\n+    return true;\n+  });\n+}\n+\n+TEST_VM_F(NMTRegionsTreeTest, VisitCommittedRegions) {\n+  NativeCallStack ncs;\n+  VMATree::RegionData rd = rt.make_region_data(ncs, mtTest);\n+  rt.reserve_mapping(1000, 50, rd);\n+  rt.reserve_mapping(1200, 50, rd);\n+  rt.reserve_mapping(1300, 50, rd);\n+  rt.reserve_mapping(1400, 50, rd);\n+\n+  rt.commit_region((address)1010, 5UL, ncs);\n+  rt.commit_region((address)1020, 5UL, ncs);\n+  rt.commit_region((address)1030, 5UL, ncs);\n+  rt.commit_region((address)1040, 5UL, ncs);\n+  ReservedMemoryRegion rmr((address)1000, 50);\n+  size_t count = 0;\n+  rt.visit_committed_regions(rmr, [&](CommittedMemoryRegion& crgn) {\n+    count++;\n+    EXPECT_EQ((((size_t)crgn.base()) % 100) \/ 10, count);\n+    EXPECT_EQ(crgn.size(), 5UL);\n+    return true;\n+  });\n+  EXPECT_EQ(count, 4UL);\n+}\n\\ No newline at end of file\n","filename":"test\/hotspot\/gtest\/nmt\/test_regions_tree.cpp","additions":109,"deletions":0,"binary":false,"changes":109,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"nmt\/memTracker.hpp\"\n@@ -82,0 +83,1 @@\n+      return true;\n@@ -138,0 +140,1 @@\n+      return true;\n@@ -163,0 +166,1 @@\n+      return true;\n@@ -197,0 +201,1 @@\n+    return true;\n@@ -200,0 +205,16 @@\n+TEST_VM_F(NMTVMATreeTest, CommitUseFlagInplace) {\n+  Tree tree;\n+  VMATree::RegionData rd1(si[0], mtTest);\n+  VMATree::RegionData rd2(si[1], mtNone);\n+  tree.reserve_mapping(0, 100, rd1);\n+  VMATree::SummaryDiff diff = tree.commit_mapping(0, 50, rd2, true);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(50, diff.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+  diff = tree.commit_mapping(60, 10, rd2, true);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(10, diff.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+  diff = tree.uncommit_mapping(0, 50, rd2);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(-50, diff.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+}\n+\n@@ -235,0 +256,1 @@\n+      return true;\n@@ -274,0 +296,1 @@\n+      return true;\n@@ -541,0 +564,12 @@\n+TEST_VM_F(NMTVMATreeTest, SummaryAccountingReserveAsUncommit) {\n+  Tree tree;\n+  Tree::RegionData rd(NCS::StackIndex(), mtTest);\n+  VMATree::SummaryDiff diff1 = tree.reserve_mapping(1200, 100, rd);\n+  VMATree::SummaryDiff diff2 = tree.commit_mapping(1210, 50, rd);\n+  EXPECT_EQ(100, diff1.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+  EXPECT_EQ(50, diff2.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+  VMATree::SummaryDiff diff3 = tree.reserve_mapping(1220, 20, rd);\n+  EXPECT_EQ(-20, diff3.tag[NMTUtil::tag_to_index(mtTest)].commit);\n+  EXPECT_EQ(0, diff3.tag[NMTUtil::tag_to_index(mtTest)].reserve);\n+}\n+\n@@ -728,0 +763,100 @@\n+\n+TEST_VM_F(NMTVMATreeTest, SetFlag) {\n+  \/\/ The gc\/cds case with only reserved data\n+  {\n+    VMATree::SummaryDiff diff;\n+    Tree::RegionData rd(NCS::StackIndex(), mtNone);\n+    VMATree tree;\n+    diff = diff.apply(tree.reserve_mapping(0, 500, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = diff.apply(tree.reserve_mapping(500, 100, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = diff.apply(tree.set_tag(0, 500, mtGC));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = diff.apply(tree.set_tag(500, 100, mtClassShared));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+  }\n+\n+  \/\/ Now let's add in some committed data\n+  {\n+    VMATree::SummaryDiff diff;\n+    Tree::RegionData rd(NCS::StackIndex(), mtNone);\n+    VMATree tree;\n+    diff = diff.apply(tree.reserve_mapping(0, 500, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = diff.apply(tree.reserve_mapping(500, 100, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    \/\/ The committed areas\n+\n+    diff = diff.apply(tree.commit_mapping(100, 125, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+    diff = diff.apply(tree.commit_mapping(550, 10, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = diff.apply(tree.commit_mapping(565, 10, rd));\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = tree.set_tag(500, 100, mtClassShared);\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+\n+    diff = tree.set_tag(0, 500, mtGC);\n+    diff.print_self();\n+    tree.print_self();\n+    tty->cr();\n+  }\n+}\n+\n+TEST_VM_F(NMTVMATreeTest, SetMemTypeOfRegions) {\n+  Tree tree;\n+  Tree::RegionData rd(NCS::StackIndex(), mtNone);\n+  int count = 0;\n+  auto dump_and_count_nodes = [&](TNode* n){\n+    tty->print_cr(SIZE_FORMAT \",in.type: %d, in.tag: %s, out.type: %d, out.tag: %s\" ,\n+    (size_t)n->key(), (int)n->val().in.type(), NMTUtil::tag_to_name(n->val().out.mem_tag()),\n+    (int)n->val().out.type(), NMTUtil::tag_to_name(n->val().in.mem_tag()));\n+    count++;\n+    return true;\n+  };\n+  tree.reserve_mapping(1200, 100, rd); \/\/ nodes in tree: 1200, 1300\n+  tree.commit_mapping(1210, 50, rd);   \/\/ nodes in tree: 1200, 1210, 1260, 1300\n+  tree.reserve_mapping(1100, 100, rd); \/\/ nodes in tree: 1100, 1210, 1260, 1300\n+\n+\n+  VMATree::SummaryDiff diff = tree.set_tag(1200, 100, mtClassShared);\n+  EXPECT_EQ(100, diff.tag[NMTUtil::tag_to_index(mtClassShared)].reserve);\n+  EXPECT_EQ(50, diff.tag[NMTUtil::tag_to_index(mtClassShared)].commit);\n+  EXPECT_EQ(-100, diff.tag[NMTUtil::tag_to_index(mtNone)].reserve);\n+  EXPECT_EQ(-50, diff.tag[NMTUtil::tag_to_index(mtNone)].commit);\n+\n+  diff = tree.set_tag(1100, 100, mtGC);\n+  EXPECT_EQ(100, diff.tag[NMTUtil::tag_to_index(mtGC)].reserve);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtGC)].commit);\n+  EXPECT_EQ(-100, diff.tag[NMTUtil::tag_to_index(mtNone)].reserve);\n+  EXPECT_EQ(0, diff.tag[NMTUtil::tag_to_index(mtNone)].commit);\n+}\n+\n","filename":"test\/hotspot\/gtest\/nmt\/test_vmatree.cpp","additions":135,"deletions":0,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n+#include \"nmt\/vmtCommon.hpp\"\n@@ -41,1 +41,1 @@\n-    VirtualMemoryTracker::add_reserved_region(stack_end, stack_size, CALLER_PC, mtThreadStack);\n+    VirtualMemoryTracker::Instance::add_reserved_region(stack_end, stack_size, CALLER_PC, mtThreadStack);\n@@ -44,1 +44,1 @@\n-    VirtualMemoryTracker::snapshot_thread_stacks();\n+    VirtualMemoryTracker::Instance::snapshot_thread_stacks();\n@@ -46,2 +46,3 @@\n-    ReservedMemoryRegion* rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion(stack_end, stack_size));\n-    ASSERT_TRUE(rmr != nullptr);\n+    ReservedMemoryRegion rmr_found = VirtualMemoryTracker::Instance::tree()->find_reserved_region(stack_end);\n+    ASSERT_TRUE(rmr_found.is_valid());\n+    ASSERT_EQ(rmr_found.base(), stack_end);\n@@ -49,4 +50,0 @@\n-    ASSERT_EQ(rmr->base(), stack_end);\n-    ASSERT_EQ(rmr->size(), stack_size);\n-\n-    CommittedRegionIterator iter = rmr->iterate_committed_regions();\n@@ -60,4 +57,3 @@\n-\n-    for (const CommittedMemoryRegion* region = iter.next(); region != nullptr; region = iter.next()) {\n-      if (region->base() + region->size() == stack_top) {\n-        ASSERT_TRUE(region->size() <= stack_size);\n+    VirtualMemoryTracker::Instance::tree()->visit_committed_regions(rmr_found, [&](const CommittedMemoryRegion& cmr) {\n+      if (cmr.base() + cmr.size() == stack_top) {\n+        EXPECT_TRUE(cmr.size() <= stack_size);\n@@ -66,2 +62,1 @@\n-\n-      if(i_addr < stack_top && i_addr >= region->base()) {\n+      if(i_addr < stack_top && i_addr >= cmr.base()) {\n@@ -70,1 +65,0 @@\n-\n@@ -72,1 +66,3 @@\n-    }\n+      return true;\n+    });\n+\n@@ -76,1 +72,0 @@\n-    ASSERT_TRUE(found_stack_top);\n@@ -78,0 +73,1 @@\n+    ASSERT_TRUE(found_stack_top);\n@@ -106,1 +102,1 @@\n-    VirtualMemoryTracker::add_reserved_region((address)base, size, stack, mtThreadStack);\n+    VirtualMemoryTracker::Instance::add_reserved_region((address)base, size, stack, mtThreadStack);\n@@ -109,1 +105,5 @@\n-    VirtualMemoryTracker::snapshot_thread_stacks();\n+    VirtualMemoryTracker::Instance::snapshot_thread_stacks();\n+\n+    ReservedMemoryRegion rmr_found = VirtualMemoryTracker::Instance::tree()->find_reserved_region((address)base);\n+    ASSERT_TRUE(rmr_found.is_valid());\n+    ASSERT_EQ(rmr_found.base(), (address)base);\n@@ -111,2 +111,0 @@\n-    ReservedMemoryRegion* rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion((address)base, size));\n-    ASSERT_TRUE(rmr != nullptr);\n@@ -115,6 +113,3 @@\n-    CommittedRegionIterator iter = rmr->iterate_committed_regions();\n-    for (const CommittedMemoryRegion* region = iter.next(); region != nullptr; region = iter.next()) {\n-      if (region->size() == size) {\n-        \/\/ platforms that do not support precise tracking.\n-        ASSERT_TRUE(iter.next() == nullptr);\n-        break;\n+    VirtualMemoryTracker::Instance::tree()->visit_committed_regions(rmr_found, [&](const CommittedMemoryRegion& cmr){\n+      if (cmr.size() == size) {\n+        return false;\n@@ -123,1 +118,1 @@\n-        check_covered_pages(region->base(), region->size(), (address)base, touch_pages, page_num);\n+        check_covered_pages(cmr.base(), cmr.size(), (address)base, touch_pages, page_num);\n@@ -125,1 +120,2 @@\n-    }\n+      return true;\n+    });\n@@ -136,4 +132,3 @@\n-    VirtualMemoryTracker::remove_released_region((address)base, size);\n-\n-    rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion((address)base, size));\n-    ASSERT_TRUE(rmr == nullptr);\n+    VirtualMemoryTracker::Instance::remove_released_region((address)base, size);\n+    rmr_found = VirtualMemoryTracker::Instance::tree()->find_reserved_region((address)base);\n+    ASSERT_TRUE(!rmr_found.is_valid());\n@@ -237,2 +232,1 @@\n-TEST_VM(CommittedVirtualMemoryTracker, test_committed_virtualmemory_region) {\n-\n+TEST_VM(NMTCommittedVirtualMemoryTracker, test_committed_virtualmemory_region) {\n@@ -255,1 +249,1 @@\n-TEST_VM(CommittedVirtualMemory, test_committed_in_range){\n+TEST_VM(NMTCommittedVirtualMemory, test_committed_in_range){\n","filename":"test\/hotspot\/gtest\/runtime\/test_committed_virtualmemory.cpp","additions":31,"deletions":37,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -1,564 +0,0 @@\n-\/*\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/\/ Tests here test the VM-global NMT facility.\n-\/\/  The tests must *not* modify global state! E.g. switch NMT on or off. Instead, they\n-\/\/  should work passively with whatever setting the gtestlauncher had been started with\n-\/\/  - if NMT is enabled, test NMT, otherwise do whatever minimal tests make sense if NMT\n-\/\/  is off.\n-\/\/\n-\/\/ The gtestLauncher then are called with various levels of -XX:NativeMemoryTracking during\n-\/\/  jtreg-controlled gtests (see test\/hotspot\/jtreg\/gtest\/NMTGtests.java)\n-\n-#include \"precompiled.hpp\"\n-#include \"memory\/memoryReserver.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"nmt\/virtualMemoryTracker.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#include \"unittest.hpp\"\n-\n-#include <stdio.h>\n-\n-\/\/ #define LOG(...) printf(__VA_ARGS__); printf(\"\\n\"); fflush(stdout);\n-#define LOG(...)\n-\n-namespace {\n-  struct R {\n-    address _addr;\n-    size_t  _size;\n-  };\n-}\n-\n-#define check(rmr, regions) check_inner((rmr), (regions), ARRAY_SIZE(regions), __FILE__, __LINE__)\n-\n-#define check_empty(rmr)                              \\\n-  do {                                                \\\n-    check_inner((rmr), nullptr, 0, __FILE__, __LINE__);  \\\n-  } while (false)\n-\n-static void diagnostic_print(ReservedMemoryRegion* rmr) {\n-  CommittedRegionIterator iter = rmr->iterate_committed_regions();\n-  LOG(\"In reserved region \" PTR_FORMAT \", size \" SIZE_FORMAT_HEX \":\", p2i(rmr->base()), rmr->size());\n-  for (const CommittedMemoryRegion* region = iter.next(); region != nullptr; region = iter.next()) {\n-    LOG(\"   committed region: \" PTR_FORMAT \", size \" SIZE_FORMAT_HEX, p2i(region->base()), region->size());\n-  }\n-}\n-\n-static void check_inner(ReservedMemoryRegion* rmr, R* regions, size_t regions_size, const char* file, int line) {\n-  CommittedRegionIterator iter = rmr->iterate_committed_regions();\n-  size_t i = 0;\n-  size_t size = 0;\n-\n-  \/\/ Helpful log\n-  diagnostic_print(rmr);\n-\n-#define WHERE \" from \" << file << \":\" << line\n-\n-  for (const CommittedMemoryRegion* region = iter.next(); region != nullptr; region = iter.next()) {\n-    EXPECT_LT(i, regions_size) << WHERE;\n-    EXPECT_EQ(region->base(), regions[i]._addr) << WHERE;\n-    EXPECT_EQ(region->size(), regions[i]._size) << WHERE;\n-    size += region->size();\n-    i++;\n-  }\n-\n-  EXPECT_EQ(i, regions_size) << WHERE;\n-  EXPECT_EQ(size, rmr->committed_size()) << WHERE;\n-}\n-\n-class VirtualMemoryTrackerTest {\n-public:\n-  static void test_add_committed_region_adjacent() {\n-\n-    size_t size  = 0x01000000;\n-    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n-    address addr = (address)rs.base();\n-\n-    address frame1 = (address)0x1234;\n-    address frame2 = (address)0x1235;\n-\n-    NativeCallStack stack(&frame1, 1);\n-    NativeCallStack stack2(&frame2, 1);\n-\n-    \/\/ Fetch the added RMR for the space\n-    ReservedMemoryRegion* rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion(addr, size));\n-\n-    ASSERT_EQ(rmr->size(), size);\n-    ASSERT_EQ(rmr->base(), addr);\n-\n-    \/\/ Commit Size Granularity\n-    const size_t cs = 0x1000;\n-\n-    \/\/ Commit adjacent regions with same stack\n-\n-    { \/\/ Commit one region\n-      rmr->add_committed_region(addr + cs, cs, stack);\n-      R r[] = { {addr + cs, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit adjacent - lower address\n-      rmr->add_committed_region(addr, cs, stack);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit adjacent - higher address\n-      rmr->add_committed_region(addr + 2 * cs, cs, stack);\n-      R r[] = { {addr, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ Cleanup\n-    rmr->remove_uncommitted_region(addr, 3 * cs);\n-    ASSERT_EQ(rmr->committed_size(), 0u);\n-\n-\n-    \/\/ Commit adjacent regions with different stacks\n-\n-    { \/\/ Commit one region\n-      rmr->add_committed_region(addr + cs, cs, stack);\n-      R r[] = { {addr + cs, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit adjacent - lower address\n-      rmr->add_committed_region(addr, cs, stack2);\n-      R r[] = { {addr,      cs},\n-                {addr + cs, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit adjacent - higher address\n-      rmr->add_committed_region(addr + 2 * cs, cs, stack2);\n-      R r[] = { {addr,          cs},\n-                {addr +     cs, cs},\n-                {addr + 2 * cs, cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ Cleanup\n-    rmr->remove_uncommitted_region(addr, 3 * cs);\n-    ASSERT_EQ(rmr->committed_size(), 0u);\n-  }\n-\n-  static void test_add_committed_region_adjacent_overlapping() {\n-\n-    size_t size  = 0x01000000;\n-    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n-    address addr = (address)rs.base();\n-\n-    address frame1 = (address)0x1234;\n-    address frame2 = (address)0x1235;\n-\n-    NativeCallStack stack(&frame1, 1);\n-    NativeCallStack stack2(&frame2, 1);\n-\n-    \/\/ Add the reserved memory\n-    VirtualMemoryTracker::add_reserved_region(addr, size, stack, mtTest);\n-\n-    \/\/ Fetch the added RMR for the space\n-    ReservedMemoryRegion* rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion(addr, size));\n-\n-    ASSERT_EQ(rmr->size(), size);\n-    ASSERT_EQ(rmr->base(), addr);\n-\n-    \/\/ Commit Size Granularity\n-    const size_t cs = 0x1000;\n-\n-    \/\/ Commit adjacent and overlapping regions with same stack\n-\n-    { \/\/ Commit two non-adjacent regions\n-      rmr->add_committed_region(addr, 2 * cs, stack);\n-      rmr->add_committed_region(addr + 3 * cs, 2 * cs, stack);\n-      R r[] = { {addr,          2 * cs},\n-                {addr + 3 * cs, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit adjacent and overlapping\n-      rmr->add_committed_region(addr + 2 * cs, 2 * cs, stack);\n-      R r[] = { {addr, 5 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ revert to two non-adjacent regions\n-    rmr->remove_uncommitted_region(addr + 2 * cs, cs);\n-    ASSERT_EQ(rmr->committed_size(), 4 * cs);\n-\n-    { \/\/ Commit overlapping and adjacent\n-      rmr->add_committed_region(addr + cs, 2 * cs, stack);\n-      R r[] = { {addr, 5 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ Cleanup\n-    rmr->remove_uncommitted_region(addr, 5 * cs);\n-    ASSERT_EQ(rmr->committed_size(), 0u);\n-\n-\n-    \/\/ Commit adjacent and overlapping regions with different stacks\n-\n-    { \/\/ Commit two non-adjacent regions\n-      rmr->add_committed_region(addr, 2 * cs, stack);\n-      rmr->add_committed_region(addr + 3 * cs, 2 * cs, stack);\n-      R r[] = { {addr,          2 * cs},\n-                {addr + 3 * cs, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit adjacent and overlapping\n-      rmr->add_committed_region(addr + 2 * cs, 2 * cs, stack2);\n-      R r[] = { {addr,          2 * cs},\n-                {addr + 2 * cs, 2 * cs},\n-                {addr + 4 * cs,     cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ revert to two non-adjacent regions\n-    rmr->add_committed_region(addr, 5 * cs, stack);\n-    rmr->remove_uncommitted_region(addr + 2 * cs, cs);\n-    ASSERT_EQ(rmr->committed_size(), 4 * cs);\n-\n-    { \/\/ Commit overlapping and adjacent\n-      rmr->add_committed_region(addr + cs, 2 * cs, stack2);\n-      R r[] = { {addr,              cs},\n-                {addr +     cs, 2 * cs},\n-                {addr + 3 * cs, 2 * cs} };\n-      check(rmr, r);\n-    }\n-  }\n-\n-  static void test_add_committed_region_overlapping() {\n-\n-    size_t size  = 0x01000000;\n-    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n-    address addr = (address)rs.base();\n-\n-    address frame1 = (address)0x1234;\n-    address frame2 = (address)0x1235;\n-\n-    NativeCallStack stack(&frame1, 1);\n-    NativeCallStack stack2(&frame2, 1);\n-\n-    \/\/ Fetch the added RMR for the space\n-    ReservedMemoryRegion* rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion(addr, size));\n-\n-    ASSERT_EQ(rmr->size(), size);\n-    ASSERT_EQ(rmr->base(), addr);\n-\n-    \/\/ Commit Size Granularity\n-    const size_t cs = 0x1000;\n-\n-    \/\/ With same stack\n-\n-    { \/\/ Commit one region\n-      rmr->add_committed_region(addr, cs, stack);\n-      R r[] = { {addr, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit the same region\n-      rmr->add_committed_region(addr, cs, stack);\n-      R r[] = { {addr, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit a succeeding region\n-      rmr->add_committed_region(addr + cs, cs, stack);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit  over two regions\n-      rmr->add_committed_region(addr, 2 * cs, stack);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    {\/\/ Commit first part of a region\n-      rmr->add_committed_region(addr, cs, stack);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit second part of a region\n-      rmr->add_committed_region(addr + cs, cs, stack);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit a third part\n-      rmr->add_committed_region(addr + 2 * cs, cs, stack);\n-      R r[] = { {addr, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit in the middle of a region\n-      rmr->add_committed_region(addr + 1 * cs, cs, stack);\n-      R r[] = { {addr, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ Cleanup\n-    rmr->remove_uncommitted_region(addr, 3 * cs);\n-    ASSERT_EQ(rmr->committed_size(), 0u);\n-\n-    \/\/ With preceding region\n-\n-    rmr->add_committed_region(addr,              cs, stack);\n-    rmr->add_committed_region(addr + 2 * cs, 3 * cs, stack);\n-\n-    rmr->add_committed_region(addr + 2 * cs,     cs, stack);\n-    {\n-      R r[] = { {addr,              cs},\n-                {addr + 2 * cs, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    rmr->add_committed_region(addr + 3 * cs,     cs, stack);\n-    {\n-      R r[] = { {addr,              cs},\n-                {addr + 2 * cs, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    rmr->add_committed_region(addr + 4 * cs,     cs, stack);\n-    {\n-      R r[] = { {addr,              cs},\n-                {addr + 2 * cs, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    \/\/ Cleanup\n-    rmr->remove_uncommitted_region(addr, 5 * cs);\n-    ASSERT_EQ(rmr->committed_size(), 0u);\n-\n-    \/\/ With different stacks\n-\n-    { \/\/ Commit one region\n-      rmr->add_committed_region(addr, cs, stack);\n-      R r[] = { {addr, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit the same region\n-      rmr->add_committed_region(addr, cs, stack2);\n-      R r[] = { {addr, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit a succeeding region\n-      rmr->add_committed_region(addr + cs, cs, stack);\n-      R r[] = { {addr,      cs},\n-                {addr + cs, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit  over two regions\n-      rmr->add_committed_region(addr, 2 * cs, stack);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    {\/\/ Commit first part of a region\n-      rmr->add_committed_region(addr, cs, stack2);\n-      R r[] = { {addr,      cs},\n-                {addr + cs, cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit second part of a region\n-      rmr->add_committed_region(addr + cs, cs, stack2);\n-      R r[] = { {addr, 2 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit a third part\n-      rmr->add_committed_region(addr + 2 * cs, cs, stack2);\n-      R r[] = { {addr, 3 * cs} };\n-      check(rmr, r);\n-    }\n-\n-    { \/\/ Commit in the middle of a region\n-      rmr->add_committed_region(addr + 1 * cs, cs, stack);\n-      R r[] = { {addr,          cs},\n-                {addr +     cs, cs},\n-                {addr + 2 * cs, cs} };\n-      check(rmr, r);\n-    }\n-  }\n-\n-  static void test_add_committed_region() {\n-    test_add_committed_region_adjacent();\n-    test_add_committed_region_adjacent_overlapping();\n-    test_add_committed_region_overlapping();\n-  }\n-\n-  template <size_t S>\n-  static void fix(R r[S]) {\n-\n-  }\n-\n-  static void test_remove_uncommitted_region() {\n-\n-    size_t size  = 0x01000000;\n-    ReservedSpace rs = MemoryReserver::reserve(size, mtTest);\n-    address addr = (address)rs.base();\n-\n-    address frame1 = (address)0x1234;\n-    address frame2 = (address)0x1235;\n-\n-    NativeCallStack stack(&frame1, 1);\n-    NativeCallStack stack2(&frame2, 1);\n-\n-    \/\/ Fetch the added RMR for the space\n-    ReservedMemoryRegion* rmr = VirtualMemoryTracker::_reserved_regions->find(ReservedMemoryRegion(addr, size));\n-\n-    ASSERT_EQ(rmr->size(), size);\n-    ASSERT_EQ(rmr->base(), addr);\n-\n-    \/\/ Commit Size Granularity\n-    const size_t cs = 0x1000;\n-\n-    { \/\/ Commit regions\n-      rmr->add_committed_region(addr, 3 * cs, stack);\n-      R r[] = { {addr, 3 * cs} };\n-      check(rmr, r);\n-\n-      \/\/ Remove only existing\n-      rmr->remove_uncommitted_region(addr, 3 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    {\n-      rmr->add_committed_region(addr + 0 * cs, cs, stack);\n-      rmr->add_committed_region(addr + 2 * cs, cs, stack);\n-      rmr->add_committed_region(addr + 4 * cs, cs, stack);\n-\n-      { \/\/ Remove first\n-        rmr->remove_uncommitted_region(addr, cs);\n-        R r[] = { {addr + 2 * cs, cs},\n-                  {addr + 4 * cs, cs} };\n-        check(rmr, r);\n-      }\n-\n-      \/\/ add back\n-      rmr->add_committed_region(addr,          cs, stack);\n-\n-      { \/\/ Remove middle\n-        rmr->remove_uncommitted_region(addr + 2 * cs, cs);\n-        R r[] = { {addr + 0 * cs, cs},\n-                  {addr + 4 * cs, cs} };\n-        check(rmr, r);\n-      }\n-\n-      \/\/ add back\n-      rmr->add_committed_region(addr + 2 * cs, cs, stack);\n-\n-      { \/\/ Remove end\n-        rmr->remove_uncommitted_region(addr + 4 * cs, cs);\n-        R r[] = { {addr + 0 * cs, cs},\n-                  {addr + 2 * cs, cs} };\n-        check(rmr, r);\n-      }\n-\n-      rmr->remove_uncommitted_region(addr, 5 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    { \/\/ Remove larger region\n-      rmr->add_committed_region(addr + 1 * cs, cs, stack);\n-      rmr->remove_uncommitted_region(addr, 3 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    { \/\/ Remove smaller region - in the middle\n-      rmr->add_committed_region(addr, 3 * cs, stack);\n-      rmr->remove_uncommitted_region(addr + 1 * cs, cs);\n-      R r[] = { { addr + 0 * cs, cs},\n-                { addr + 2 * cs, cs} };\n-      check(rmr, r);\n-\n-      rmr->remove_uncommitted_region(addr, 3 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    { \/\/ Remove smaller region - at the beginning\n-      rmr->add_committed_region(addr, 3 * cs, stack);\n-      rmr->remove_uncommitted_region(addr + 0 * cs, cs);\n-      R r[] = { { addr + 1 * cs, 2 * cs} };\n-      check(rmr, r);\n-\n-      rmr->remove_uncommitted_region(addr, 3 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    { \/\/ Remove smaller region - at the end\n-      rmr->add_committed_region(addr, 3 * cs, stack);\n-      rmr->remove_uncommitted_region(addr + 2 * cs, cs);\n-      R r[] = { { addr, 2 * cs} };\n-      check(rmr, r);\n-\n-      rmr->remove_uncommitted_region(addr, 3 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    { \/\/ Remove smaller, overlapping region - at the beginning\n-      rmr->add_committed_region(addr + 1 * cs, 4 * cs, stack);\n-      rmr->remove_uncommitted_region(addr, 2 * cs);\n-      R r[] = { { addr + 2 * cs, 3 * cs} };\n-      check(rmr, r);\n-\n-      rmr->remove_uncommitted_region(addr + 1 * cs, 4 * cs);\n-      check_empty(rmr);\n-    }\n-\n-    { \/\/ Remove smaller, overlapping region - at the end\n-      rmr->add_committed_region(addr, 3 * cs, stack);\n-      rmr->remove_uncommitted_region(addr + 2 * cs, 2 * cs);\n-      R r[] = { { addr, 2 * cs} };\n-      check(rmr, r);\n-\n-      rmr->remove_uncommitted_region(addr, 3 * cs);\n-      check_empty(rmr);\n-    }\n-  }\n-};\n-\n-TEST_VM(NMT_VirtualMemoryTracker, add_committed_region) {\n-  if (MemTracker::tracking_level() >= NMT_detail) {\n-    VirtualMemoryTrackerTest::test_add_committed_region();\n-  } else {\n-    tty->print_cr(\"skipped.\");\n-  }\n-}\n-\n-TEST_VM(NMT_VirtualMemoryTracker, remove_uncommitted_region) {\n-  if (MemTracker::tracking_level() >= NMT_detail) {\n-    VirtualMemoryTrackerTest::test_remove_uncommitted_region();\n-  } else {\n-    tty->print_cr(\"skipped.\");\n-  }\n-}\n","filename":"test\/hotspot\/gtest\/runtime\/test_virtualMemoryTracker.cpp","additions":0,"deletions":564,"binary":false,"changes":564,"status":"deleted"},{"patch":"@@ -323,4 +323,0 @@\n-        \/\/ On ARM Thumb the stack is not walkable, so the location is not available and\n-        \/\/ \"from\" string will not be present in the output.\n-        \/\/ Disable assertion for ARM32.\n-        String fromString = Platform.isARM() ? \"\" : \"from.*\";\n@@ -328,2 +324,2 @@\n-                           + Long.toHexString(addr + size)\n-                           + \"\\\\] committed \" + sizeString + \" \" + fromString);\n+                            + Long.toHexString(addr + size)\n+                            + \"\\\\] committed \" + sizeString);\n","filename":"test\/hotspot\/jtreg\/runtime\/NMT\/VirtualAllocCommitMerge.java","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -170,1 +170,1 @@\n-          long expected_delta = numThreads * (max_stack_usage_with_pretouch - min_stack_usage_with_pretouch);\n+          long expected_delta = numThreads * (max_stack_usage_with_pretouch - min_stack_usage_with_pretouch) \/ 2;\n@@ -172,7 +172,9 @@\n-          if (pretouch_committed <= (no_pretouch_committed + expected_delta)) {\n-            throw new RuntimeException(\"Expected a higher amount of committed with pretouch stacks\" +\n-                                       \"PreTouch amount: \" + pretouch_committed +\n-                                       \"NoPreTouch amount: \" + (no_pretouch_committed + expected_delta));\n-          }\n-          if (actual_delta < expected_delta) {\n-            throw new RuntimeException(\"Expected a higher delta between stack committed of with and without pretouch.\" +\n+          if (pretouch_committed \/ no_pretouch_committed < 10) {\n+            if (pretouch_committed <= (no_pretouch_committed + expected_delta)) {\n+              throw new RuntimeException(\"Expected a higher amount of committed with pretouch stacks\" +\n+                                       \" PreTouch amount: \" + pretouch_committed +\n+                                       \" NoPreTouch amount: \" + no_pretouch_committed +\n+                                       \" Expected Delta calculated as: \" + expected_delta);\n+            }\n+            if (actual_delta < expected_delta) {\n+              throw new RuntimeException(\"Expected a higher delta between stack committed of with and without pretouch.\" +\n@@ -180,0 +182,1 @@\n+            }\n","filename":"test\/hotspot\/jtreg\/runtime\/Thread\/TestAlwaysPreTouchStacks.java","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"}]}