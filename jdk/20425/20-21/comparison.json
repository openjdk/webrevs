{"files":[{"patch":"@@ -32,1 +32,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/memBaseline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/memMapPrinter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"nmt\/regionsTree.hpp\"\n@@ -34,1 +35,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/memReporter.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/memReporter.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"nmt\/vmtCommon.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/memTracker.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -179,10 +179,5 @@\n-  for (int d = 0; d < _files.length(); d++) {\n-    const MemoryFile* file = _files.at(d);\n-    for (int i = 0; i < mt_number_of_tags; i++) {\n-      VirtualMemory* snap = snapshot->by_tag(NMTUtil::index_to_tag(i));\n-      const VirtualMemory* current = file->_summary.by_tag(NMTUtil::index_to_tag(i));\n-      \/\/ Only account the committed memory.\n-      snap->commit_memory(current->committed());\n-    }\n-  }\n-}\n+  iterate_summary([&](MemTag tag, const VirtualMemory* current) {\n+    VirtualMemory* snap = snapshot->by_tag(tag);\n+    \/\/ Only account the committed memory.\n+    snap->commit_memory(current->committed());\n+  });}\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.cpp","additions":5,"deletions":10,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"nmt\/vmtCommon.hpp\"\n@@ -33,0 +32,1 @@\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/memoryFileTracker.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/regionsTree.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"nmt\/vmtCommon.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/threadStackTracker.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"nmt\/regionsTree.hpp\"\n@@ -224,0 +225,122 @@\n+\n+int compare_committed_region(const CommittedMemoryRegion& r1, const CommittedMemoryRegion& r2) {\n+  return r1.compare(r2);\n+}\n+\n+int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2) {\n+  return r1.compare(r2);\n+}\n+\n+size_t ReservedMemoryRegion::committed_size() const {\n+  size_t committed = 0;\n+  size_t result = 0;\n+  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*this, [&](CommittedMemoryRegion& crgn) {\n+    result += crgn.size();\n+    return true;\n+  });\n+  return result;\n+}\n+\n+address ReservedMemoryRegion::thread_stack_uncommitted_bottom() const {\n+  address bottom = base();\n+  address top = base() + size();\n+  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*this, [&](CommittedMemoryRegion& crgn) {\n+    address committed_top = crgn.base() + crgn.size();\n+    if (committed_top < top) {\n+      \/\/ committed stack guard pages, skip them\n+      bottom = crgn.base() + crgn.size();\n+    } else {\n+      assert(top == committed_top, \"Sanity, top=\" INTPTR_FORMAT \" , com-top=\" INTPTR_FORMAT, p2i(top), p2i(committed_top));\n+      return false;;\n+    }\n+    return true;\n+  });\n+\n+  return bottom;\n+}\n+\n+\/\/ Iterate the range, find committed region within its bound.\n+class RegionIterator : public StackObj {\n+private:\n+  const address _start;\n+  const size_t  _size;\n+\n+  address _current_start;\n+public:\n+  RegionIterator(address start, size_t size) :\n+    _start(start), _size(size), _current_start(start) {\n+  }\n+\n+  \/\/ return true if committed region is found\n+  bool next_committed(address& start, size_t& size);\n+private:\n+  address end() const { return _start + _size; }\n+};\n+\n+bool RegionIterator::next_committed(address& committed_start, size_t& committed_size) {\n+  if (end() <= _current_start) return false;\n+\n+  const size_t page_sz = os::vm_page_size();\n+  const size_t current_size = end() - _current_start;\n+  if (os::committed_in_range(_current_start, current_size, committed_start, committed_size)) {\n+    assert(committed_start != nullptr, \"Must be\");\n+    assert(committed_size > 0 && is_aligned(committed_size, os::vm_page_size()), \"Must be\");\n+\n+    _current_start = committed_start + committed_size;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Walk all known thread stacks, snapshot their committed ranges.\n+class SnapshotThreadStackWalker : public VirtualMemoryWalker {\n+public:\n+  SnapshotThreadStackWalker() {}\n+\n+  bool do_allocation_site(const ReservedMemoryRegion* rgn) {\n+    if (MemTracker::NmtVirtualMemoryLocker::is_safe_to_use()) {\n+      assert_lock_strong(NmtVirtualMemory_lock);\n+    }\n+    if (rgn->mem_tag() == mtThreadStack) {\n+      address stack_bottom = rgn->thread_stack_uncommitted_bottom();\n+      address committed_start;\n+      size_t  committed_size;\n+      size_t stack_size = rgn->base() + rgn->size() - stack_bottom;\n+      \/\/ Align the size to work with full pages (Alpine and AIX stack top is not page aligned)\n+      size_t aligned_stack_size = align_up(stack_size, os::vm_page_size());\n+\n+      NativeCallStack ncs; \/\/ empty stack\n+\n+      RegionIterator itr(stack_bottom, aligned_stack_size);\n+      DEBUG_ONLY(bool found_stack = false;)\n+      while (itr.next_committed(committed_start, committed_size)) {\n+        assert(committed_start != nullptr, \"Should not be null\");\n+        assert(committed_size > 0, \"Should not be 0\");\n+        \/\/ unaligned stack_size case: correct the region to fit the actual stack_size\n+        if (stack_bottom + stack_size < committed_start + committed_size) {\n+          committed_size = stack_bottom + stack_size - committed_start;\n+        }\n+        VirtualMemoryTracker::Instance::add_committed_region(committed_start, committed_size, ncs);\n+        \/\/log_warning(cds)(\"st start: \" INTPTR_FORMAT \" size: \" SIZE_FORMAT, p2i(committed_start), committed_size);\n+        DEBUG_ONLY(found_stack = true;)\n+      }\n+#ifdef ASSERT\n+      if (!found_stack) {\n+        log_debug(thread)(\"Thread exited without proper cleanup, may leak thread object\");\n+      }\n+#endif\n+    }\n+    return true;\n+  }\n+};\n+\n+void VirtualMemoryTracker::Instance::snapshot_thread_stacks() {\n+  SnapshotThreadStackWalker walker;\n+  walk_virtual_memory(&walker);\n+}\n+\n+  VirtualMemoryTracker::VirtualMemoryTracker(bool is_detailed_mode) {\n+    _tree = static_cast<RegionsTree*>(os::malloc(sizeof(RegionsTree), mtNMT));\n+    new(_tree) RegionsTree(is_detailed_mode);\n+  }\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.cpp","additions":123,"deletions":0,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -28,2 +28,2 @@\n-#include \"nmt\/nmtCommon.hpp\"\n-#include \"nmt\/regionsTree.hpp\"\n+#include \"nmt\/allocationSite.hpp\"\n+#include \"nmt\/vmatree.hpp\"\n@@ -31,0 +31,1 @@\n+#include \"utilities\/linkedlist.hpp\"\n@@ -49,0 +50,320 @@\n+\n+\/*\n+ * Virtual memory counter\n+ *\/\n+class VirtualMemory {\n+ private:\n+  size_t     _reserved;\n+  size_t     _committed;\n+\n+  volatile size_t _peak_size;\n+  void update_peak(size_t size);\n+\n+ public:\n+  VirtualMemory() : _reserved(0), _committed(0), _peak_size(0) {}\n+\n+  inline void reserve_memory(size_t sz) { _reserved += sz; }\n+  inline void commit_memory (size_t sz) {\n+    _committed += sz;\n+    assert(_committed <= _reserved, \"Sanity check\");\n+    update_peak(_committed);\n+  }\n+\n+  inline void release_memory (size_t sz) {\n+    assert(_reserved >= sz, \"Negative amount\");\n+    _reserved -= sz;\n+  }\n+\n+  inline void uncommit_memory(size_t sz) {\n+    assert(_committed >= sz, \"Negative amount\");\n+    _committed -= sz;\n+  }\n+\n+  inline size_t reserved()  const { return _reserved;  }\n+  inline size_t committed() const { return _committed; }\n+  inline size_t peak_size() const {\n+    return Atomic::load(&_peak_size);\n+  }\n+};\n+\n+\/\/ Virtual memory allocation site, keeps track where the virtual memory is reserved.\n+class VirtualMemoryAllocationSite : public AllocationSite {\n+  VirtualMemory _c;\n+ public:\n+  VirtualMemoryAllocationSite(const NativeCallStack& stack, MemTag mem_tag) :\n+    AllocationSite(stack, mem_tag) { }\n+\n+  inline void reserve_memory(size_t sz)  { _c.reserve_memory(sz);  }\n+  inline void commit_memory (size_t sz)  { _c.commit_memory(sz);   }\n+  inline size_t reserved() const  { return _c.reserved(); }\n+  inline size_t committed() const { return _c.committed(); }\n+  inline size_t peak_size() const { return _c.peak_size(); }\n+};\n+\n+class VirtualMemorySummary;\n+\n+\/\/ This class represents a snapshot of virtual memory at a given time.\n+\/\/ The latest snapshot is saved in a static area.\n+class VirtualMemorySnapshot : public ResourceObj {\n+  friend class VirtualMemorySummary;\n+\n+ private:\n+  VirtualMemory  _virtual_memory[mt_number_of_tags];\n+\n+ public:\n+  inline VirtualMemory* by_tag(MemTag mem_tag) {\n+    int index = NMTUtil::tag_to_index(mem_tag);\n+    return &_virtual_memory[index];\n+  }\n+\n+  inline const VirtualMemory* by_tag(MemTag mem_tag) const {\n+    int index = NMTUtil::tag_to_index(mem_tag);\n+    return &_virtual_memory[index];\n+  }\n+\n+  inline void clean() {\n+\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      if (index != NMTUtil::tag_to_index(mtThreadStack))\n+        _virtual_memory[index] = VirtualMemory();\n+    }\n+  }\n+\n+  inline size_t total_reserved() const {\n+    size_t amount = 0;\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      amount += _virtual_memory[index].reserved();\n+    }\n+    return amount;\n+  }\n+\n+  inline size_t total_committed() const {\n+    size_t amount = 0;\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      amount += _virtual_memory[index].committed();\n+    }\n+    return amount;\n+  }\n+\n+  void copy_to(VirtualMemorySnapshot* s) {\n+    for (int index = 0; index < mt_number_of_tags; index ++) {\n+      s->_virtual_memory[index] = _virtual_memory[index];\n+    }\n+  }\n+};\n+\n+class VirtualMemorySummary : AllStatic {\n+ public:\n+\n+  static inline void record_reserved_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->reserve_memory(size);\n+  }\n+\n+  static inline void record_committed_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->commit_memory(size);\n+  }\n+\n+  static inline void record_uncommitted_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->uncommit_memory(size);\n+  }\n+\n+  static inline void record_released_memory(size_t size, MemTag mem_tag) {\n+    as_snapshot()->by_tag(mem_tag)->release_memory(size);\n+  }\n+\n+  \/\/ Move virtual memory from one memory tag to another.\n+  \/\/ Virtual memory can be reserved before it is associated with a memory tag, and tagged\n+  \/\/ as 'unknown'. Once the memory is tagged, the virtual memory will be moved from 'unknown'\n+  \/\/ tag to specified memory tag.\n+  static inline void move_reserved_memory(MemTag from, MemTag to, size_t size) {\n+    as_snapshot()->by_tag(from)->release_memory(size);\n+    as_snapshot()->by_tag(to)->reserve_memory(size);\n+  }\n+\n+  static inline void move_committed_memory(MemTag from, MemTag to, size_t size) {\n+    as_snapshot()->by_tag(from)->uncommit_memory(size);\n+    as_snapshot()->by_tag(to)->commit_memory(size);\n+  }\n+\n+  static void snapshot(VirtualMemorySnapshot* s);\n+\n+  static VirtualMemorySnapshot* as_snapshot() {\n+    return &_snapshot;\n+  }\n+\n+ private:\n+  static VirtualMemorySnapshot _snapshot;\n+};\n+\n+\n+\n+\/*\n+ * A virtual memory region\n+ *\/\n+class VirtualMemoryRegion {\n+ private:\n+  address      _base_address;\n+  size_t       _size;\n+\n+ public:\n+  VirtualMemoryRegion(address addr, size_t size) :\n+    _base_address(addr), _size(size) {\n+     assert(addr != nullptr, \"Invalid address\");\n+     assert(size > 0, \"Invalid size\");\n+   }\n+\n+  inline address base() const { return _base_address;   }\n+  inline address end()  const { return base() + size(); }\n+  inline size_t  size() const { return _size;           }\n+\n+  inline bool is_empty() const { return size() == 0; }\n+\n+  inline bool contain_address(address addr) const {\n+    return (addr >= base() && addr < end());\n+  }\n+\n+\n+  inline bool contain_region(address addr, size_t size) const {\n+    return contain_address(addr) && contain_address(addr + size - 1);\n+  }\n+\n+  inline bool same_region(address addr, size_t sz) const {\n+    return (addr == base() && sz == size());\n+  }\n+\n+\n+  inline bool overlap_region(address addr, size_t sz) const {\n+    assert(sz > 0, \"Invalid size\");\n+    assert(size() > 0, \"Invalid size\");\n+    return MAX2(addr, base()) < MIN2(addr + sz, end());\n+  }\n+\n+  inline bool adjacent_to(address addr, size_t sz) const {\n+    return (addr == end() || (addr + sz) == base());\n+  }\n+\n+  void exclude_region(address addr, size_t sz) {\n+    assert(contain_region(addr, sz), \"Not containment\");\n+    assert(addr == base() || addr + sz == end(), \"Can not exclude from middle\");\n+    size_t new_size = size() - sz;\n+\n+    if (addr == base()) {\n+      set_base(addr + sz);\n+    }\n+    set_size(new_size);\n+  }\n+\n+  void expand_region(address addr, size_t sz) {\n+    assert(adjacent_to(addr, sz), \"Not adjacent regions\");\n+    if (base() == addr + sz) {\n+      set_base(addr);\n+    }\n+    set_size(size() + sz);\n+  }\n+\n+  \/\/ Returns 0 if regions overlap; 1 if this region follows rgn;\n+  \/\/  -1 if this region precedes rgn.\n+  inline int compare(const VirtualMemoryRegion& rgn) const {\n+    if (overlap_region(rgn.base(), rgn.size())) {\n+      return 0;\n+    } else if (base() >= rgn.end()) {\n+      return 1;\n+    } else {\n+      assert(rgn.base() >= end(), \"Sanity\");\n+      return -1;\n+    }\n+  }\n+\n+  \/\/ Returns true if regions overlap, false otherwise.\n+  inline bool equals(const VirtualMemoryRegion& rgn) const {\n+    return compare(rgn) == 0;\n+  }\n+\n+ protected:\n+  void set_base(address base) {\n+    assert(base != nullptr, \"Sanity check\");\n+    _base_address = base;\n+  }\n+\n+  void set_size(size_t  size) {\n+    assert(size > 0, \"Sanity check\");\n+    _size = size;\n+  }\n+};\n+\n+\n+class CommittedMemoryRegion : public VirtualMemoryRegion {\n+ private:\n+  NativeCallStack  _stack;\n+\n+ public:\n+  CommittedMemoryRegion() :\n+    VirtualMemoryRegion((address)1, 1), _stack(NativeCallStack::empty_stack()) { }\n+\n+  CommittedMemoryRegion(address addr, size_t size, const NativeCallStack& stack) :\n+    VirtualMemoryRegion(addr, size), _stack(stack) { }\n+\n+  inline void set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n+  inline const NativeCallStack* call_stack() const         { return &_stack; }\n+};\n+\n+\n+typedef LinkedListIterator<CommittedMemoryRegion> CommittedRegionIterator;\n+\n+int compare_committed_region(const CommittedMemoryRegion&, const CommittedMemoryRegion&);\n+class ReservedMemoryRegion : public VirtualMemoryRegion {\n+ private:\n+  NativeCallStack  _stack;\n+  MemTag         _mem_tag;\n+\n+ public:\n+  bool is_valid() { return base() != (address)1 && size() != 1;}\n+  ReservedMemoryRegion() :\n+    VirtualMemoryRegion((address)1, 1), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n+\n+  ReservedMemoryRegion(address base, size_t size, const NativeCallStack& stack,\n+    MemTag mem_tag = mtNone) :\n+    VirtualMemoryRegion(base, size), _stack(stack), _mem_tag(mem_tag) { }\n+\n+\n+  ReservedMemoryRegion(address base, size_t size) :\n+    VirtualMemoryRegion(base, size), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n+\n+  \/\/ Copy constructor\n+  ReservedMemoryRegion(const ReservedMemoryRegion& rr) :\n+    VirtualMemoryRegion(rr.base(), rr.size()) {\n+    *this = rr;\n+  }\n+\n+  inline void  set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n+  inline const NativeCallStack* call_stack() const          { return &_stack;  }\n+\n+  inline MemTag mem_tag() const            { return _mem_tag;  }\n+\n+  \/\/ uncommitted thread stack bottom, above guard pages if there is any.\n+  address thread_stack_uncommitted_bottom() const;\n+\n+  size_t committed_size() const;\n+\n+\n+  ReservedMemoryRegion& operator= (const ReservedMemoryRegion& other) {\n+    set_base(other.base());\n+    set_size(other.size());\n+\n+    _stack = *other.call_stack();\n+    _mem_tag = other.mem_tag();\n+\n+    return *this;\n+  }\n+\n+  const char* tag_name() const { return NMTUtil::tag_to_name(_mem_tag); }\n+};\n+\n+int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2);\n+\n+class VirtualMemoryWalker : public StackObj {\n+ public:\n+   virtual bool do_allocation_site(const ReservedMemoryRegion* rgn) { return false; }\n+};\n+\n+class RegionsTree;\n@@ -51,1 +372,1 @@\n-  RegionsTree _tree;\n+  RegionsTree *_tree;\n@@ -54,1 +375,1 @@\n-  VirtualMemoryTracker(bool is_detailed_mode) : _tree(is_detailed_mode) { }\n+  VirtualMemoryTracker(bool is_detailed_mode);\n@@ -78,1 +399,1 @@\n-  RegionsTree* tree() { return &_tree; }\n+  RegionsTree* tree() { return _tree; }\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.hpp","additions":326,"deletions":5,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -1,153 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"logging\/log.hpp\"\n-#include \"memory\/metaspaceStats.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"nmt\/memTracker.hpp\"\n-#include \"nmt\/nativeCallStackPrinter.hpp\"\n-#include \"nmt\/threadStackTracker.hpp\"\n-#include \"nmt\/vmtCommon.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/threadCritical.hpp\"\n-#include \"utilities\/ostream.hpp\"\n-\n-\n-int compare_committed_region(const CommittedMemoryRegion& r1, const CommittedMemoryRegion& r2) {\n-  return r1.compare(r2);\n-}\n-\n-int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2) {\n-  return r1.compare(r2);\n-}\n-\n-size_t ReservedMemoryRegion::committed_size() const {\n-  size_t committed = 0;\n-  size_t result = 0;\n-  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*this, [&](CommittedMemoryRegion& crgn) {\n-    result += crgn.size();\n-    return true;\n-  });\n-  return result;\n-}\n-\n-address ReservedMemoryRegion::thread_stack_uncommitted_bottom() const {\n-  address bottom = base();\n-  address top = base() + size();\n-  VirtualMemoryTracker::Instance::tree()->visit_committed_regions(*this, [&](CommittedMemoryRegion& crgn) {\n-    address committed_top = crgn.base() + crgn.size();\n-    if (committed_top < top) {\n-      \/\/ committed stack guard pages, skip them\n-      bottom = crgn.base() + crgn.size();\n-    } else {\n-      assert(top == committed_top, \"Sanity, top=\" INTPTR_FORMAT \" , com-top=\" INTPTR_FORMAT, p2i(top), p2i(committed_top));\n-      return false;;\n-    }\n-    return true;\n-  });\n-\n-  return bottom;\n-}\n-\n-\/\/ Iterate the range, find committed region within its bound.\n-class RegionIterator : public StackObj {\n-private:\n-  const address _start;\n-  const size_t  _size;\n-\n-  address _current_start;\n-public:\n-  RegionIterator(address start, size_t size) :\n-    _start(start), _size(size), _current_start(start) {\n-  }\n-\n-  \/\/ return true if committed region is found\n-  bool next_committed(address& start, size_t& size);\n-private:\n-  address end() const { return _start + _size; }\n-};\n-\n-bool RegionIterator::next_committed(address& committed_start, size_t& committed_size) {\n-  if (end() <= _current_start) return false;\n-\n-  const size_t page_sz = os::vm_page_size();\n-  const size_t current_size = end() - _current_start;\n-  if (os::committed_in_range(_current_start, current_size, committed_start, committed_size)) {\n-    assert(committed_start != nullptr, \"Must be\");\n-    assert(committed_size > 0 && is_aligned(committed_size, os::vm_page_size()), \"Must be\");\n-\n-    _current_start = committed_start + committed_size;\n-    return true;\n-  } else {\n-    return false;\n-  }\n-}\n-\n-\/\/ Walk all known thread stacks, snapshot their committed ranges.\n-class SnapshotThreadStackWalker : public VirtualMemoryWalker {\n-public:\n-  SnapshotThreadStackWalker() {}\n-\n-  bool do_allocation_site(const ReservedMemoryRegion* rgn) {\n-    if (MemTracker::NmtVirtualMemoryLocker::is_safe_to_use()) {\n-      assert_lock_strong(NmtVirtualMemory_lock);\n-    }\n-    if (rgn->mem_tag() == mtThreadStack) {\n-      address stack_bottom = rgn->thread_stack_uncommitted_bottom();\n-      address committed_start;\n-      size_t  committed_size;\n-      size_t stack_size = rgn->base() + rgn->size() - stack_bottom;\n-      \/\/ Align the size to work with full pages (Alpine and AIX stack top is not page aligned)\n-      size_t aligned_stack_size = align_up(stack_size, os::vm_page_size());\n-\n-      NativeCallStack ncs; \/\/ empty stack\n-\n-      RegionIterator itr(stack_bottom, aligned_stack_size);\n-      DEBUG_ONLY(bool found_stack = false;)\n-      while (itr.next_committed(committed_start, committed_size)) {\n-        assert(committed_start != nullptr, \"Should not be null\");\n-        assert(committed_size > 0, \"Should not be 0\");\n-        \/\/ unaligned stack_size case: correct the region to fit the actual stack_size\n-        if (stack_bottom + stack_size < committed_start + committed_size) {\n-          committed_size = stack_bottom + stack_size - committed_start;\n-        }\n-        VirtualMemoryTracker::Instance::add_committed_region(committed_start, committed_size, ncs);\n-        \/\/log_warning(cds)(\"st start: \" INTPTR_FORMAT \" size: \" SIZE_FORMAT, p2i(committed_start), committed_size);\n-        DEBUG_ONLY(found_stack = true;)\n-      }\n-#ifdef ASSERT\n-      if (!found_stack) {\n-        log_debug(thread)(\"Thread exited without proper cleanup, may leak thread object\");\n-      }\n-#endif\n-    }\n-    return true;\n-  }\n-};\n-\n-void VirtualMemoryTracker::Instance::snapshot_thread_stacks() {\n-  SnapshotThreadStackWalker walker;\n-  walk_virtual_memory(&walker);\n-}\n-\n","filename":"src\/hotspot\/share\/nmt\/vmtCommon.cpp","additions":0,"deletions":153,"binary":false,"changes":153,"status":"deleted"},{"patch":"@@ -1,357 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef NMT_VMTCOMMON_HPP\n-#define NMT_VMTCOMMON_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/metaspace.hpp\" \/\/ For MetadataType\n-#include \"memory\/metaspaceStats.hpp\"\n-#include \"nmt\/allocationSite.hpp\"\n-#include \"nmt\/nmtCommon.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/nativeCallStack.hpp\"\n-#include \"utilities\/linkedlist.hpp\"\n-#include \"utilities\/ostream.hpp\"\n-\n-\/*\n- * Virtual memory counter\n- *\/\n-class VirtualMemory {\n- private:\n-  size_t     _reserved;\n-  size_t     _committed;\n-\n-  volatile size_t _peak_size;\n-  void update_peak(size_t size);\n-\n- public:\n-  VirtualMemory() : _reserved(0), _committed(0), _peak_size(0) {}\n-\n-  inline void reserve_memory(size_t sz) { _reserved += sz; }\n-  inline void commit_memory (size_t sz) {\n-    _committed += sz;\n-    assert(_committed <= _reserved, \"Sanity check\");\n-    update_peak(_committed);\n-  }\n-\n-  inline void release_memory (size_t sz) {\n-    assert(_reserved >= sz, \"Negative amount\");\n-    _reserved -= sz;\n-  }\n-\n-  inline void uncommit_memory(size_t sz) {\n-    assert(_committed >= sz, \"Negative amount\");\n-    _committed -= sz;\n-  }\n-\n-  inline size_t reserved()  const { return _reserved;  }\n-  inline size_t committed() const { return _committed; }\n-  inline size_t peak_size() const {\n-    return Atomic::load(&_peak_size);\n-  }\n-};\n-\n-\/\/ Virtual memory allocation site, keeps track where the virtual memory is reserved.\n-class VirtualMemoryAllocationSite : public AllocationSite {\n-  VirtualMemory _c;\n- public:\n-  VirtualMemoryAllocationSite(const NativeCallStack& stack, MemTag mem_tag) :\n-    AllocationSite(stack, mem_tag) { }\n-\n-  inline void reserve_memory(size_t sz)  { _c.reserve_memory(sz);  }\n-  inline void commit_memory (size_t sz)  { _c.commit_memory(sz);   }\n-  inline size_t reserved() const  { return _c.reserved(); }\n-  inline size_t committed() const { return _c.committed(); }\n-  inline size_t peak_size() const { return _c.peak_size(); }\n-};\n-\n-class VirtualMemorySummary;\n-\n-\/\/ This class represents a snapshot of virtual memory at a given time.\n-\/\/ The latest snapshot is saved in a static area.\n-class VirtualMemorySnapshot : public ResourceObj {\n-  friend class VirtualMemorySummary;\n-\n- private:\n-  VirtualMemory  _virtual_memory[mt_number_of_tags];\n-\n- public:\n-  inline VirtualMemory* by_tag(MemTag mem_tag) {\n-    int index = NMTUtil::tag_to_index(mem_tag);\n-    return &_virtual_memory[index];\n-  }\n-\n-  inline const VirtualMemory* by_tag(MemTag mem_tag) const {\n-    int index = NMTUtil::tag_to_index(mem_tag);\n-    return &_virtual_memory[index];\n-  }\n-\n-  inline void clean() {\n-\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      if (index != NMTUtil::tag_to_index(mtThreadStack))\n-        _virtual_memory[index] = VirtualMemory();\n-    }\n-  }\n-\n-  inline size_t total_reserved() const {\n-    size_t amount = 0;\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      amount += _virtual_memory[index].reserved();\n-    }\n-    return amount;\n-  }\n-\n-  inline size_t total_committed() const {\n-    size_t amount = 0;\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      amount += _virtual_memory[index].committed();\n-    }\n-    return amount;\n-  }\n-\n-  void copy_to(VirtualMemorySnapshot* s) {\n-    for (int index = 0; index < mt_number_of_tags; index ++) {\n-      s->_virtual_memory[index] = _virtual_memory[index];\n-    }\n-  }\n-};\n-\n-class VirtualMemorySummary : AllStatic {\n- public:\n-\n-  static inline void record_reserved_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_tag(mem_tag)->reserve_memory(size);\n-  }\n-\n-  static inline void record_committed_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_tag(mem_tag)->commit_memory(size);\n-  }\n-\n-  static inline void record_uncommitted_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_tag(mem_tag)->uncommit_memory(size);\n-  }\n-\n-  static inline void record_released_memory(size_t size, MemTag mem_tag) {\n-    as_snapshot()->by_tag(mem_tag)->release_memory(size);\n-  }\n-\n-  \/\/ Move virtual memory from one memory tag to another.\n-  \/\/ Virtual memory can be reserved before it is associated with a memory tag, and tagged\n-  \/\/ as 'unknown'. Once the memory is tagged, the virtual memory will be moved from 'unknown'\n-  \/\/ tag to specified memory tag.\n-  static inline void move_reserved_memory(MemTag from, MemTag to, size_t size) {\n-    as_snapshot()->by_tag(from)->release_memory(size);\n-    as_snapshot()->by_tag(to)->reserve_memory(size);\n-  }\n-\n-  static inline void move_committed_memory(MemTag from, MemTag to, size_t size) {\n-    as_snapshot()->by_tag(from)->uncommit_memory(size);\n-    as_snapshot()->by_tag(to)->commit_memory(size);\n-  }\n-\n-  static void snapshot(VirtualMemorySnapshot* s);\n-\n-  static VirtualMemorySnapshot* as_snapshot() {\n-    return &_snapshot;\n-  }\n-\n- private:\n-  static VirtualMemorySnapshot _snapshot;\n-};\n-\n-\n-\n-\/*\n- * A virtual memory region\n- *\/\n-class VirtualMemoryRegion {\n- private:\n-  address      _base_address;\n-  size_t       _size;\n-\n- public:\n-  VirtualMemoryRegion(address addr, size_t size) :\n-    _base_address(addr), _size(size) {\n-     assert(addr != nullptr, \"Invalid address\");\n-     assert(size > 0, \"Invalid size\");\n-   }\n-\n-  inline address base() const { return _base_address;   }\n-  inline address end()  const { return base() + size(); }\n-  inline size_t  size() const { return _size;           }\n-\n-  inline bool is_empty() const { return size() == 0; }\n-\n-  inline bool contain_address(address addr) const {\n-    return (addr >= base() && addr < end());\n-  }\n-\n-\n-  inline bool contain_region(address addr, size_t size) const {\n-    return contain_address(addr) && contain_address(addr + size - 1);\n-  }\n-\n-  inline bool same_region(address addr, size_t sz) const {\n-    return (addr == base() && sz == size());\n-  }\n-\n-\n-  inline bool overlap_region(address addr, size_t sz) const {\n-    assert(sz > 0, \"Invalid size\");\n-    assert(size() > 0, \"Invalid size\");\n-    return MAX2(addr, base()) < MIN2(addr + sz, end());\n-  }\n-\n-  inline bool adjacent_to(address addr, size_t sz) const {\n-    return (addr == end() || (addr + sz) == base());\n-  }\n-\n-  void exclude_region(address addr, size_t sz) {\n-    assert(contain_region(addr, sz), \"Not containment\");\n-    assert(addr == base() || addr + sz == end(), \"Can not exclude from middle\");\n-    size_t new_size = size() - sz;\n-\n-    if (addr == base()) {\n-      set_base(addr + sz);\n-    }\n-    set_size(new_size);\n-  }\n-\n-  void expand_region(address addr, size_t sz) {\n-    assert(adjacent_to(addr, sz), \"Not adjacent regions\");\n-    if (base() == addr + sz) {\n-      set_base(addr);\n-    }\n-    set_size(size() + sz);\n-  }\n-\n-  \/\/ Returns 0 if regions overlap; 1 if this region follows rgn;\n-  \/\/  -1 if this region precedes rgn.\n-  inline int compare(const VirtualMemoryRegion& rgn) const {\n-    if (overlap_region(rgn.base(), rgn.size())) {\n-      return 0;\n-    } else if (base() >= rgn.end()) {\n-      return 1;\n-    } else {\n-      assert(rgn.base() >= end(), \"Sanity\");\n-      return -1;\n-    }\n-  }\n-\n-  \/\/ Returns true if regions overlap, false otherwise.\n-  inline bool equals(const VirtualMemoryRegion& rgn) const {\n-    return compare(rgn) == 0;\n-  }\n-\n- protected:\n-  void set_base(address base) {\n-    assert(base != nullptr, \"Sanity check\");\n-    _base_address = base;\n-  }\n-\n-  void set_size(size_t  size) {\n-    assert(size > 0, \"Sanity check\");\n-    _size = size;\n-  }\n-};\n-\n-\n-class CommittedMemoryRegion : public VirtualMemoryRegion {\n- private:\n-  NativeCallStack  _stack;\n-\n- public:\n-  CommittedMemoryRegion() :\n-    VirtualMemoryRegion((address)1, 1), _stack(NativeCallStack::empty_stack()) { }\n-\n-  CommittedMemoryRegion(address addr, size_t size, const NativeCallStack& stack) :\n-    VirtualMemoryRegion(addr, size), _stack(stack) { }\n-\n-  inline void set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n-  inline const NativeCallStack* call_stack() const         { return &_stack; }\n-};\n-\n-\n-typedef LinkedListIterator<CommittedMemoryRegion> CommittedRegionIterator;\n-\n-int compare_committed_region(const CommittedMemoryRegion&, const CommittedMemoryRegion&);\n-class ReservedMemoryRegion : public VirtualMemoryRegion {\n- private:\n-  NativeCallStack  _stack;\n-  MemTag         _mem_tag;\n-\n- public:\n-  bool is_valid() { return base() != (address)1 && size() != 1;}\n-  ReservedMemoryRegion() :\n-    VirtualMemoryRegion((address)1, 1), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n-\n-  ReservedMemoryRegion(address base, size_t size, const NativeCallStack& stack,\n-    MemTag mem_tag = mtNone) :\n-    VirtualMemoryRegion(base, size), _stack(stack), _mem_tag(mem_tag) { }\n-\n-\n-  ReservedMemoryRegion(address base, size_t size) :\n-    VirtualMemoryRegion(base, size), _stack(NativeCallStack::empty_stack()), _mem_tag(mtNone) { }\n-\n-  \/\/ Copy constructor\n-  ReservedMemoryRegion(const ReservedMemoryRegion& rr) :\n-    VirtualMemoryRegion(rr.base(), rr.size()) {\n-    *this = rr;\n-  }\n-\n-  inline void  set_call_stack(const NativeCallStack& stack) { _stack = stack; }\n-  inline const NativeCallStack* call_stack() const          { return &_stack;  }\n-\n-  inline MemTag mem_tag() const            { return _mem_tag;  }\n-\n-  \/\/ uncommitted thread stack bottom, above guard pages if there is any.\n-  address thread_stack_uncommitted_bottom() const;\n-\n-  size_t committed_size() const;\n-\n-\n-  ReservedMemoryRegion& operator= (const ReservedMemoryRegion& other) {\n-    set_base(other.base());\n-    set_size(other.size());\n-\n-    _stack = *other.call_stack();\n-    _mem_tag = other.mem_tag();\n-\n-    return *this;\n-  }\n-\n-  const char* tag_name() const { return NMTUtil::tag_to_name(_mem_tag); }\n-};\n-\n-int compare_reserved_region_base(const ReservedMemoryRegion& r1, const ReservedMemoryRegion& r2);\n-\n-class VirtualMemoryWalker : public StackObj {\n- public:\n-   virtual bool do_allocation_site(const ReservedMemoryRegion* rgn) { return false; }\n-};\n-\n-#endif \/\/ NMT_VMTCOMMON_HPP\n-\n","filename":"src\/hotspot\/share\/nmt\/vmtCommon.hpp","additions":0,"deletions":357,"binary":false,"changes":357,"status":"deleted"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"nmt\/vmtCommon.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/debug.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/virtualMemoryTracker.hpp\"\n@@ -30,0 +30,1 @@\n+#include \"utilities\/linkedlist.hpp\"\n","filename":"test\/hotspot\/gtest\/nmt\/test_nmt_treap.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"nmt\/vmtCommon.hpp\"\n+#include \"nmt\/regionsTree.hpp\"\n","filename":"test\/hotspot\/gtest\/runtime\/test_committed_virtualmemory.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}