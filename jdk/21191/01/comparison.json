{"files":[{"patch":"@@ -243,1 +243,1 @@\n-  free_page(page);\n+  free_page(page, false \/* allow_defragment *\/);\n@@ -246,1 +246,1 @@\n-void ZHeap::free_page(ZPage* page) {\n+void ZHeap::free_page(ZPage* page, bool allow_defragment) {\n@@ -256,1 +256,1 @@\n-  _page_allocator.free_page(page);\n+  _page_allocator.free_page(page, allow_defragment);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-  void free_page(ZPage* page);\n+  void free_page(ZPage* page, bool allow_defragment);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -278,1 +278,1 @@\n-  free_page(page);\n+  free_page(page, false \/* allow_defragment *\/);\n@@ -465,0 +465,32 @@\n+bool ZPageAllocator::should_defragment(const ZPage* page) const {\n+  \/\/ A small page can end up at a high address (second half of the address space)\n+  \/\/ if we've split a larger page or we have a constrained address space. To help\n+  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n+  \/\/ a lower address is available.\n+  return page->type() == ZPageType::small &&\n+         page->start() >= to_zoffset(_virtual.reserved() \/ 2) &&\n+         page->start() > _virtual.lowest_available_address();\n+}\n+\n+ZPage* ZPageAllocator::defragment_page(ZPage* page) {\n+  \/\/ Harvest the physical memory (which is committed)\n+  ZPhysicalMemory pmem;\n+  ZPhysicalMemory& old_pmem = page->physical_memory();\n+  pmem.add_segments(old_pmem);\n+  old_pmem.remove_segments();\n+\n+  _unmapper->unmap_and_destroy_page(page);\n+\n+  \/\/ Allocate new virtual memory at a low address\n+  const ZVirtualMemory vmem = _virtual.alloc(pmem.size(), true \/* force_low_address *\/);\n+\n+  \/\/ Create the new page and map it\n+  ZPage* new_page = new ZPage(ZPageType::small, vmem, pmem);\n+  map_page(new_page);\n+\n+  \/\/ Update statistics\n+  ZStatInc(ZCounterDefragment);\n+\n+  return new_page;\n+}\n+\n@@ -626,10 +658,0 @@\n-bool ZPageAllocator::should_defragment(const ZPage* page) const {\n-  \/\/ A small page can end up at a high address (second half of the address space)\n-  \/\/ if we've split a larger page or we have a constrained address space. To help\n-  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n-  \/\/ a lower address is available.\n-  return page->type() == ZPageType::small &&\n-         page->start() >= to_zoffset(_virtual.reserved() \/ 2) &&\n-         page->start() > _virtual.lowest_available_address();\n-}\n-\n@@ -655,6 +677,0 @@\n-  if (should_defragment(page)) {\n-    \/\/ Defragment address space\n-    ZStatInc(ZCounterDefragment);\n-    return false;\n-  }\n-\n@@ -776,0 +792,12 @@\n+ZPage* ZPageAllocator::prepare_to_recycle(ZPage* page, bool allow_defragment) {\n+  \/\/ Make sure we have a page that is safe to recycle\n+  ZPage* const to_recycle = _safe_recycle.register_and_clone_if_activated(page);\n+\n+  \/\/ Defragment the page before recycle if allowed and needed\n+  if (allow_defragment && should_defragment(to_recycle)) {\n+    return defragment_page(to_recycle);\n+  }\n+\n+  return to_recycle;\n+}\n+\n@@ -784,1 +812,1 @@\n-void ZPageAllocator::free_page(ZPage* page) {\n+void ZPageAllocator::free_page(ZPage* page, bool allow_defragment) {\n@@ -786,1 +814,3 @@\n-  ZPage* const to_recycle = _safe_recycle.register_and_clone_if_activated(page);\n+\n+  \/\/ Prepare page for recycling before taking the lock\n+  ZPage* const to_recycle = prepare_to_recycle(page, allow_defragment);\n@@ -803,1 +833,1 @@\n-  ZArray<ZPage*> to_recycle;\n+  ZArray<ZPage*> to_recycle_pages;\n@@ -808,0 +838,1 @@\n+  \/\/ Prepare pages for recycling before taking the lock\n@@ -815,1 +846,6 @@\n-    to_recycle.push(_safe_recycle.register_and_clone_if_activated(page));\n+\n+    \/\/ Prepare to recycle\n+    ZPage* const to_recycle = prepare_to_recycle(page, true \/* allow_defragment *\/);\n+\n+    \/\/ Register for recycling\n+    to_recycle_pages.push(to_recycle);\n@@ -826,1 +862,1 @@\n-  ZArrayIterator<ZPage*> iter(&to_recycle);\n+  ZArrayIterator<ZPage*> iter(&to_recycle_pages);\n@@ -836,1 +872,1 @@\n-  ZArray<ZPage*> to_recycle;\n+  ZArray<ZPage*> to_recycle_pages;\n@@ -838,0 +874,1 @@\n+  \/\/ Prepare pages for recycling before taking the lock\n@@ -840,1 +877,5 @@\n-    to_recycle.push(_safe_recycle.register_and_clone_if_activated(page));\n+    \/\/ Prepare to recycle\n+    ZPage* const to_recycle = prepare_to_recycle(page, false \/* allow_defragment *\/);\n+\n+    \/\/ Register for recycling\n+    to_recycle_pages.push(to_recycle);\n@@ -852,1 +893,1 @@\n-  ZArrayIterator<ZPage*> iter(&to_recycle);\n+  ZArrayIterator<ZPage*> iter(&to_recycle_pages);\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":66,"deletions":25,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -107,0 +107,3 @@\n+  bool should_defragment(const ZPage* page) const;\n+  ZPage* defragment_page(ZPage* page);\n+\n@@ -113,1 +116,0 @@\n-  bool should_defragment(const ZPage* page) const;\n@@ -152,0 +154,1 @@\n+  ZPage* prepare_to_recycle(ZPage* page, bool allow_defragment);\n@@ -154,1 +157,1 @@\n-  void free_page(ZPage* page);\n+  void free_page(ZPage* page, bool allow_defragment);\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -414,1 +414,1 @@\n-    ZHeap::heap()->free_page(page);\n+    ZHeap::heap()->free_page(page, true \/* allow_defragment *\/);\n@@ -1040,1 +1040,1 @@\n-      ZHeap::heap()->free_page(page);\n+      ZHeap::heap()->free_page(page, true \/* allow_defragment *\/);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}