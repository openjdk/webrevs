{"files":[{"patch":"@@ -278,1 +278,1 @@\n-  Atomic::inc(&_patching_epoch);\n+  AtomicAccess::inc(&_patching_epoch);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -117,1 +117,1 @@\n-    return Atomic::load_acquire(guard_addr());\n+    return AtomicAccess::load_acquire(guard_addr());\n@@ -121,1 +121,1 @@\n-    Atomic::release_store(guard_addr(), value);\n+    AtomicAccess::release_store(guard_addr(), value);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -10268,1 +10268,1 @@\n-  \/\/ ARMv8.1 LSE versions of the atomic stubs used by Atomic::PlatformXX.\n+  \/\/ ARMv8.1 LSE versions of the atomic stubs used by AtomicAccess::PlatformXX.\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    return Atomic::load_acquire(guard_addr());\n+    return AtomicAccess::load_acquire(guard_addr());\n@@ -55,1 +55,1 @@\n-    Atomic::release_store(guard_addr(), value);\n+    AtomicAccess::release_store(guard_addr(), value);\n","filename":"src\/hotspot\/cpu\/arm\/gc\/shared\/barrierSetNMethod_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -424,1 +424,2 @@\n- \/\/ As per atomic.hpp the Atomic read-modify-write operations must be logically implemented as:\n+ \/\/ As per atomicAccess.hpp the atomic read-modify-write operations must be\n+ \/\/ logically implemented as:\n@@ -443,1 +444,1 @@\n-  \/\/ used by Atomic::add(volatile jint* dest, jint add_value)\n+  \/\/ used by AtomicAccess::add(volatile jint* dest, jint add_value)\n@@ -495,1 +496,1 @@\n-  \/\/ used by Atomic::add(volatile jint* dest, jint exchange_value)\n+  \/\/ used by AtomicAccess::add(volatile jint* dest, jint exchange_value)\n@@ -545,1 +546,1 @@\n-  \/\/ used by Atomic::cmpxchg(volatile jint *dest, jint compare_value, jint exchange_value)\n+  \/\/ used by AtomicAccess::cmpxchg(volatile jint *dest, jint compare_value, jint exchange_value)\n@@ -585,1 +586,1 @@\n-  \/\/ Support for jlong Atomic::cmpxchg(jlong exchange_value, volatile jlong *dest, jlong compare_value)\n+  \/\/ Support for jlong AtomicAccess::cmpxchg(jlong exchange_value, volatile jlong *dest, jlong compare_value)\n","filename":"src\/hotspot\/cpu\/arm\/stubGenerator_arm.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -350,1 +350,1 @@\n-  \/\/Atomic::release_store(jump_addr, *((juint*)code_buffer));\n+  \/\/AtomicAccess::release_store(jump_addr, *((juint*)code_buffer));\n","filename":"src\/hotspot\/cpu\/ppc\/nativeInst_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -220,1 +220,1 @@\n-  Atomic::inc(&_patching_epoch);\n+  AtomicAccess::inc(&_patching_epoch);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-    return Atomic::load_acquire(guard_addr());\n+    return AtomicAccess::load_acquire(guard_addr());\n@@ -115,1 +115,1 @@\n-    Atomic::release_store(guard_addr(), value);\n+    AtomicAccess::release_store(guard_addr(), value);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetNMethod_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -812,1 +812,1 @@\n-  const uint64_t obsv = Atomic::cmpxchg(&Bsd::_max_abstime, prev, now);\n+  const uint64_t obsv = AtomicAccess::cmpxchg(&Bsd::_max_abstime, prev, now);\n@@ -2138,1 +2138,1 @@\n-  int processor_id = Atomic::load(&processor_id_map[apic_id]);\n+  int processor_id = AtomicAccess::load(&processor_id_map[apic_id]);\n@@ -2142,1 +2142,1 @@\n-    processor_id = Atomic::cmpxchg(&processor_id_map[apic_id], processor_id_unassigned, processor_id_assigning);\n+    processor_id = AtomicAccess::cmpxchg(&processor_id_map[apic_id], processor_id_unassigned, processor_id_assigning);\n@@ -2144,2 +2144,2 @@\n-      processor_id = Atomic::fetch_then_add(&processor_id_next, 1) % os::processor_count();\n-      Atomic::store(&processor_id_map[apic_id], processor_id);\n+      processor_id = AtomicAccess::fetch_then_add(&processor_id_next, 1) % os::processor_count();\n+      AtomicAccess::store(&processor_id_map[apic_id], processor_id);\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -4789,2 +4789,2 @@\n-  if (Atomic::load(&warn_once) == 0 ||\n-      Atomic::xchg(&warn_once, 0) == 0) {\n+  if (AtomicAccess::load(&warn_once) == 0 ||\n+      AtomicAccess::xchg(&warn_once, 0) == 0) {\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1695,1 +1695,1 @@\n-    if (Atomic::cmpxchg(&_event, v, v - 1) == v) break;\n+    if (AtomicAccess::cmpxchg(&_event, v, v - 1) == v) break;\n@@ -1742,1 +1742,1 @@\n-    if (Atomic::cmpxchg(&_event, v, v - 1) == v) break;\n+    if (AtomicAccess::cmpxchg(&_event, v, v - 1) == v) break;\n@@ -1798,1 +1798,1 @@\n-  if (Atomic::xchg(&_event, 1) >= 0) return;\n+  if (AtomicAccess::xchg(&_event, 1) >= 0) return;\n@@ -1851,1 +1851,1 @@\n-  \/\/ We depend on Atomic::xchg() having full barrier semantics\n+  \/\/ We depend on AtomicAccess::xchg() having full barrier semantics\n@@ -1853,1 +1853,1 @@\n-  if (Atomic::xchg(&_counter, 0) > 0) return;\n+  if (AtomicAccess::xchg(&_counter, 0) > 0) return;\n","filename":"src\/hotspot\/os\/posix\/os_posix.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -359,1 +359,1 @@\n-    Atomic::inc(&pending_signals[sig]);\n+    AtomicAccess::inc(&pending_signals[sig]);\n@@ -372,1 +372,1 @@\n-      if (n > 0 && n == Atomic::cmpxchg(&pending_signals[i], n, n - 1)) {\n+      if (n > 0 && n == AtomicAccess::cmpxchg(&pending_signals[i], n, n - 1)) {\n","filename":"src\/hotspot\/os\/posix\/signals_posix.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -34,1 +34,1 @@\n-  SuspendResume::State result = Atomic::cmpxchg(&_state, from, to);\n+  SuspendResume::State result = AtomicAccess::cmpxchg(&_state, from, to);\n","filename":"src\/hotspot\/os\/posix\/suspendResume_posix.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -2465,1 +2465,1 @@\n-    Atomic::inc(&pending_signals[sig]);\n+    AtomicAccess::inc(&pending_signals[sig]);\n@@ -2478,1 +2478,1 @@\n-      if (n > 0 && n == Atomic::cmpxchg(&pending_signals[i], n, n - 1)) {\n+      if (n > 0 && n == AtomicAccess::cmpxchg(&pending_signals[i], n, n - 1)) {\n@@ -4301,1 +4301,1 @@\n-    } else if (Atomic::load_acquire(&process_exiting) == 0) {\n+    } else if (AtomicAccess::load_acquire(&process_exiting) == 0) {\n@@ -4305,1 +4305,1 @@\n-        Atomic::cmpxchg(&process_exiting, (DWORD)0, GetCurrentThreadId());\n+        AtomicAccess::cmpxchg(&process_exiting, (DWORD)0, GetCurrentThreadId());\n@@ -4309,1 +4309,1 @@\n-      if (what == EPT_THREAD && Atomic::load_acquire(&process_exiting) == 0) {\n+      if (what == EPT_THREAD && AtomicAccess::load_acquire(&process_exiting) == 0) {\n@@ -4422,1 +4422,1 @@\n-        Atomic::load_acquire(&process_exiting) != 0 &&\n+        AtomicAccess::load_acquire(&process_exiting) != 0 &&\n@@ -5588,1 +5588,1 @@\n-    if (Atomic::cmpxchg(&_Event, v, v-1) == v) break;\n+    if (AtomicAccess::cmpxchg(&_Event, v, v-1) == v) break;\n@@ -5651,1 +5651,1 @@\n-    if (Atomic::cmpxchg(&_Event, v, v-1) == v) break;\n+    if (AtomicAccess::cmpxchg(&_Event, v, v-1) == v) break;\n@@ -5698,1 +5698,1 @@\n-  if (Atomic::xchg(&_Event, 1) >= 0) return;\n+  if (AtomicAccess::xchg(&_Event, 1) >= 0) return;\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,1 +96,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -108,2 +108,2 @@\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                               atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -134,2 +134,2 @@\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                               atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -159,3 +159,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -198,3 +198,3 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -238,4 +238,4 @@\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -246,1 +246,1 @@\n-  \/\/ specified otherwise (see atomic.hpp).\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n@@ -308,4 +308,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -316,1 +316,1 @@\n-  \/\/ specified otherwise (see atomic.hpp).\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n@@ -358,4 +358,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -366,1 +366,1 @@\n-  \/\/ specified otherwise (see atomic.hpp).\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n@@ -407,1 +407,1 @@\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE> {\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE> {\n@@ -410,1 +410,1 @@\n-    T t = Atomic::load(p);\n+    T t = AtomicAccess::load(p);\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/atomic_aix_ppc.hpp","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -57,3 +57,3 @@\n-inline T Atomic::PlatformXchg<byte_size>::operator()(T volatile* dest,\n-                                                     T exchange_value,\n-                                                     atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<byte_size>::operator()(T volatile* dest,\n+                                                           T exchange_value,\n+                                                           atomic_memory_order order) const {\n@@ -68,4 +68,4 @@\n-inline T Atomic::PlatformCmpxchg<byte_size>::operator()(T volatile* dest,\n-                                                        T compare_value,\n-                                                        T exchange_value,\n-                                                        atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<byte_size>::operator()(T volatile* dest,\n+                                                              T compare_value,\n+                                                              T exchange_value,\n+                                                              atomic_memory_order order) const {\n@@ -112,1 +112,1 @@\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n@@ -119,1 +119,1 @@\n-struct Atomic::PlatformOrderedStore<byte_size, RELEASE_X>\n+struct AtomicAccess::PlatformOrderedStore<byte_size, RELEASE_X>\n@@ -126,1 +126,1 @@\n-struct Atomic::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/atomic_bsd_aarch64.hpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -43,1 +43,1 @@\n-inline D Atomic::PlatformAdd<4>::fetch_then_add(D volatile* dest, I add_value,\n+inline D AtomicAccess::PlatformAdd<4>::fetch_then_add(D volatile* dest, I add_value,\n@@ -57,1 +57,1 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n@@ -70,1 +70,1 @@\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest,\n@@ -84,1 +84,1 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n@@ -99,1 +99,1 @@\n-inline D Atomic::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n+inline D AtomicAccess::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n@@ -113,1 +113,1 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n@@ -126,1 +126,1 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n@@ -148,1 +148,1 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n@@ -158,1 +158,1 @@\n-struct Atomic::PlatformXchg<8> : Atomic::XchgUsingCmpxchg<8> {};\n+struct AtomicAccess::PlatformXchg<8> : AtomicAccess::XchgUsingCmpxchg<8> {};\n@@ -162,1 +162,1 @@\n-struct Atomic::PlatformAdd<8> : Atomic::AddUsingCmpxchg<8> {};\n+struct AtomicAccess::PlatformAdd<8> : AtomicAccess::AddUsingCmpxchg<8> {};\n@@ -166,1 +166,1 @@\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n+inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n@@ -175,1 +175,1 @@\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n+inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n@@ -184,1 +184,1 @@\n-struct Atomic::PlatformOrderedStore<1, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<1, RELEASE_X_FENCE>\n@@ -196,1 +196,1 @@\n-struct Atomic::PlatformOrderedStore<2, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<2, RELEASE_X_FENCE>\n@@ -208,1 +208,1 @@\n-struct Atomic::PlatformOrderedStore<4, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<4, RELEASE_X_FENCE>\n@@ -221,1 +221,1 @@\n-struct Atomic::PlatformOrderedStore<8, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<8, RELEASE_X_FENCE>\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/atomic_bsd_x86.hpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -47,2 +47,2 @@\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -59,2 +59,2 @@\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -71,3 +71,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -83,3 +83,3 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -95,1 +95,1 @@\n-struct Atomic::PlatformCmpxchg<1> : Atomic::CmpxchgByteUsingInt {};\n+struct AtomicAccess::PlatformCmpxchg<1> : AtomicAccess::CmpxchgByteUsingInt {};\n@@ -99,4 +99,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -114,4 +114,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -137,1 +137,1 @@\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n+inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n@@ -146,2 +146,2 @@\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n-                                                 T store_value) const {\n+inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n+                                                       T store_value) const {\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/atomic_bsd_zero.hpp","additions":24,"deletions":24,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,1 +73,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -86,2 +86,2 @@\n-inline D Atomic::PlatformAdd<4>::fetch_then_add(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::fetch_then_add(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -102,2 +102,2 @@\n-inline D Atomic::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -118,3 +118,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -128,2 +128,2 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest, T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest, T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -137,4 +137,4 @@\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -155,4 +155,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -178,4 +178,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -200,1 +200,1 @@\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n@@ -207,1 +207,1 @@\n-struct Atomic::PlatformOrderedStore<byte_size, RELEASE_X>\n+struct AtomicAccess::PlatformOrderedStore<byte_size, RELEASE_X>\n@@ -214,1 +214,1 @@\n-struct Atomic::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/atomic_linux_aarch64.hpp","additions":26,"deletions":26,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,1 @@\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n+inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n@@ -89,2 +89,2 @@\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n-                                                 T store_value) const {\n+inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n+                                                       T store_value) const {\n@@ -96,1 +96,1 @@\n-\/\/ As per atomic.hpp all read-modify-write operations have to provide two-way\n+\/\/ As per atomicAccess.hpp all read-modify-write operations have to provide two-way\n@@ -102,1 +102,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -114,2 +114,2 @@\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                               atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -124,3 +124,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -133,1 +133,1 @@\n-struct Atomic::PlatformXchg<8> : Atomic::XchgUsingCmpxchg<8> {};\n+struct AtomicAccess::PlatformXchg<8> : AtomicAccess::XchgUsingCmpxchg<8> {};\n@@ -137,1 +137,1 @@\n-struct Atomic::PlatformAdd<8> : Atomic::AddUsingCmpxchg<8> {};\n+struct AtomicAccess::PlatformAdd<8> : AtomicAccess::AddUsingCmpxchg<8> {};\n@@ -143,1 +143,1 @@\n-struct Atomic::PlatformCmpxchg<1> : Atomic::CmpxchgByteUsingInt {};\n+struct AtomicAccess::PlatformCmpxchg<1> : AtomicAccess::CmpxchgByteUsingInt {};\n@@ -163,4 +163,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -173,4 +173,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/atomic_linux_arm.hpp","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,1 +96,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -108,2 +108,2 @@\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -134,2 +134,2 @@\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -159,3 +159,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -198,3 +198,3 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -238,4 +238,4 @@\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -246,1 +246,1 @@\n-  \/\/ specified otherwise (see atomic.hpp).\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n@@ -285,4 +285,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -293,1 +293,1 @@\n-  \/\/ specified otherwise (see atomic.hpp).\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n@@ -335,4 +335,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -343,1 +343,1 @@\n-  \/\/ specified otherwise (see atomic.hpp).\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n@@ -384,1 +384,1 @@\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n@@ -388,1 +388,1 @@\n-    T t = Atomic::load(p);\n+    T t = AtomicAccess::load(p);\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/atomic_linux_ppc.hpp","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -74,4 +74,4 @@\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest __attribute__((unused)),\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest __attribute__((unused)),\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -125,4 +125,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest __attribute__((unused)),\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest __attribute__((unused)),\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -157,3 +157,3 @@\n-inline T Atomic::PlatformXchg<byte_size>::operator()(T volatile* dest,\n-                                                     T exchange_value,\n-                                                     atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<byte_size>::operator()(T volatile* dest,\n+                                                           T exchange_value,\n+                                                           atomic_memory_order order) const {\n@@ -183,4 +183,4 @@\n-inline T Atomic::PlatformCmpxchg<byte_size>::operator()(T volatile* dest __attribute__((unused)),\n-                                                        T compare_value,\n-                                                        T exchange_value,\n-                                                        atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<byte_size>::operator()(T volatile* dest __attribute__((unused)),\n+                                                              T compare_value,\n+                                                              T exchange_value,\n+                                                              atomic_memory_order order) const {\n@@ -207,1 +207,1 @@\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n@@ -214,1 +214,1 @@\n-struct Atomic::PlatformOrderedStore<byte_size, RELEASE_X>\n+struct AtomicAccess::PlatformOrderedStore<byte_size, RELEASE_X>\n@@ -221,1 +221,1 @@\n-struct Atomic::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<byte_size, RELEASE_X_FENCE>\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/atomic_linux_riscv.hpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -58,1 +58,1 @@\n-\/\/ Atomic::add\n+\/\/ AtomicAccess::add\n@@ -78,1 +78,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -90,2 +90,2 @@\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I inc,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I inc,\n+                                                      atomic_memory_order order) const {\n@@ -144,2 +144,2 @@\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I inc,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::add_then_fetch(D volatile* dest, I inc,\n+                                                      atomic_memory_order order) const {\n@@ -197,1 +197,1 @@\n-\/\/ Atomic::xchg\n+\/\/ AtomicAccess::xchg\n@@ -214,3 +214,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order unused) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order unused) const {\n@@ -238,3 +238,3 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order unused) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order unused) const {\n@@ -261,1 +261,1 @@\n-\/\/ Atomic::cmpxchg\n+\/\/ AtomicAccess::cmpxchg\n@@ -291,1 +291,1 @@\n-struct Atomic::PlatformCmpxchg<1> : Atomic::CmpxchgByteUsingInt {};\n+struct AtomicAccess::PlatformCmpxchg<1> : AtomicAccess::CmpxchgByteUsingInt {};\n@@ -295,4 +295,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T cmp_val,\n-                                                T xchg_val,\n-                                                atomic_memory_order unused) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T cmp_val,\n+                                                      T xchg_val,\n+                                                      atomic_memory_order unused) const {\n@@ -319,4 +319,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T cmp_val,\n-                                                T xchg_val,\n-                                                atomic_memory_order unused) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T cmp_val,\n+                                                      T xchg_val,\n+                                                      atomic_memory_order unused) const {\n@@ -342,1 +342,1 @@\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/atomic_linux_s390.hpp","additions":26,"deletions":26,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -43,2 +43,2 @@\n-inline D Atomic::PlatformAdd<4>::fetch_then_add(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::fetch_then_add(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -57,3 +57,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -70,4 +70,4 @@\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order \/* order *\/) const {\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order \/* order *\/) const {\n@@ -84,4 +84,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order \/* order *\/) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order \/* order *\/) const {\n@@ -100,2 +100,2 @@\n-inline D Atomic::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::fetch_then_add(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -114,2 +114,2 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest, T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest, T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -126,4 +126,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order \/* order *\/) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order \/* order *\/) const {\n@@ -148,4 +148,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -158,1 +158,1 @@\n-struct Atomic::PlatformXchg<8> : Atomic::XchgUsingCmpxchg<8> {};\n+struct AtomicAccess::PlatformXchg<8> : AtomicAccess::XchgUsingCmpxchg<8> {};\n@@ -162,1 +162,1 @@\n-struct Atomic::PlatformAdd<8> : Atomic::AddUsingCmpxchg<8> {};\n+struct AtomicAccess::PlatformAdd<8> : AtomicAccess::AddUsingCmpxchg<8> {};\n@@ -166,1 +166,1 @@\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n+inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n@@ -175,2 +175,2 @@\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n-                                                 T store_value) const {\n+inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n+                                                       T store_value) const {\n@@ -184,1 +184,1 @@\n-struct Atomic::PlatformOrderedStore<1, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<1, RELEASE_X_FENCE>\n@@ -196,1 +196,1 @@\n-struct Atomic::PlatformOrderedStore<2, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<2, RELEASE_X_FENCE>\n@@ -208,1 +208,1 @@\n-struct Atomic::PlatformOrderedStore<4, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<4, RELEASE_X_FENCE>\n@@ -221,1 +221,1 @@\n-struct Atomic::PlatformOrderedStore<8, RELEASE_X_FENCE>\n+struct AtomicAccess::PlatformOrderedStore<8, RELEASE_X_FENCE>\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/atomic_linux_x86.hpp","additions":36,"deletions":36,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -46,2 +46,2 @@\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -58,2 +58,2 @@\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n+inline D AtomicAccess::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n@@ -70,3 +70,3 @@\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -82,3 +82,3 @@\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n@@ -94,1 +94,1 @@\n-struct Atomic::PlatformCmpxchg<1> : Atomic::CmpxchgByteUsingInt {};\n+struct AtomicAccess::PlatformCmpxchg<1> : AtomicAccess::CmpxchgByteUsingInt {};\n@@ -98,4 +98,4 @@\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -114,4 +114,4 @@\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n@@ -137,1 +137,1 @@\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n+inline T AtomicAccess::PlatformLoad<8>::operator()(T const volatile* src) const {\n@@ -146,2 +146,2 @@\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n-                                                 T store_value) const {\n+inline void AtomicAccess::PlatformStore<8>::operator()(T volatile* dest,\n+                                                       T store_value) const {\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/atomic_linux_zero.hpp","additions":24,"deletions":24,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,0 +2,1 @@\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +34,1 @@\n-\/\/ As per atomic.hpp all read-modify-write operations have to provide two-way\n+\/\/ As per atomicAccess.hpp all read-modify-write operations have to provide two-way\n@@ -40,1 +41,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -56,3 +57,3 @@\n-  inline D Atomic::PlatformAdd<sizeof(IntrinsicType)>::add_then_fetch(D volatile* dest, \\\n-                                                                      I add_value, \\\n-                                                                      atomic_memory_order order) const { \\\n+  inline D AtomicAccess::PlatformAdd<sizeof(IntrinsicType)>::add_then_fetch(D volatile* dest, \\\n+                                                                            I add_value, \\\n+                                                                            atomic_memory_order order) const { \\\n@@ -73,3 +74,3 @@\n-  inline T Atomic::PlatformXchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n-                                                                   T exchange_value, \\\n-                                                                   atomic_memory_order order) const { \\\n+  inline T AtomicAccess::PlatformXchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n+                                                                         T exchange_value, \\\n+                                                                         atomic_memory_order order) const { \\\n@@ -88,1 +89,1 @@\n-\/\/ Atomic::PlatformCmpxchg<*>::operator() and the\n+\/\/ AtomicAccess::PlatformCmpxchg<*>::operator() and the\n@@ -94,4 +95,4 @@\n-  inline T Atomic::PlatformCmpxchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n-                                                                      T compare_value, \\\n-                                                                      T exchange_value, \\\n-                                                                      atomic_memory_order order) const { \\\n+  inline T AtomicAccess::PlatformCmpxchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n+                                                                            T compare_value, \\\n+                                                                            T exchange_value, \\\n+                                                                            atomic_memory_order order) const { \\\n","filename":"src\/hotspot\/os_cpu\/windows_aarch64\/atomic_windows_aarch64.hpp","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -38,1 +38,1 @@\n-      Atomic::store(to++, Atomic::load(from++));\n+      AtomicAccess::store(to++, AtomicAccess::load(from++));\n@@ -45,1 +45,1 @@\n-      Atomic::store(to--, Atomic::load(from--));\n+      AtomicAccess::store(to--, AtomicAccess::load(from--));\n","filename":"src\/hotspot\/os_cpu\/windows_aarch64\/copy_windows_aarch64.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,2 +35,2 @@\n-\/\/ bound calls like release_store go through Atomic::load\n-\/\/ and Atomic::store which do volatile memory accesses.\n+\/\/ bound calls like release_store go through AtomicAccess::load\n+\/\/ and AtomicAccess::store which do volatile memory accesses.\n@@ -43,1 +43,1 @@\n-struct Atomic::PlatformAdd {\n+struct AtomicAccess::PlatformAdd {\n@@ -59,3 +59,3 @@\n-  inline D Atomic::PlatformAdd<sizeof(IntrinsicType)>::add_then_fetch(D volatile* dest, \\\n-                                                                      I add_value, \\\n-                                                                      atomic_memory_order order) const { \\\n+  inline D AtomicAccess::PlatformAdd<sizeof(IntrinsicType)>::add_then_fetch(D volatile* dest, \\\n+                                                                            I add_value, \\\n+                                                                            atomic_memory_order order) const { \\\n@@ -76,3 +76,3 @@\n-  inline T Atomic::PlatformXchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n-                                                                   T exchange_value, \\\n-                                                                   atomic_memory_order order) const { \\\n+  inline T AtomicAccess::PlatformXchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n+                                                                         T exchange_value, \\\n+                                                                         atomic_memory_order order) const { \\\n@@ -91,1 +91,1 @@\n-\/\/ Atomic::PlatformCmpxchg<*>::operator() and the\n+\/\/ AtomicAccess::PlatformCmpxchg<*>::operator() and the\n@@ -97,4 +97,4 @@\n-  inline T Atomic::PlatformCmpxchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n-                                                                      T compare_value, \\\n-                                                                      T exchange_value, \\\n-                                                                      atomic_memory_order order) const { \\\n+  inline T AtomicAccess::PlatformCmpxchg<sizeof(IntrinsicType)>::operator()(T volatile* dest, \\\n+                                                                            T compare_value, \\\n+                                                                            T exchange_value, \\\n+                                                                            atomic_memory_order order) const { \\\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/atomic_windows_x86.hpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -35,1 +35,1 @@\n-      Atomic::store(to++, Atomic::load(from++));\n+      AtomicAccess::store(to++, AtomicAccess::load(from++));\n@@ -42,1 +42,1 @@\n-      Atomic::store(to--, Atomic::load(from--));\n+      AtomicAccess::store(to--, AtomicAccess::load(from--));\n","filename":"src\/hotspot\/os_cpu\/windows_x86\/copy_windows_x86.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-    return Atomic::load_acquire(&_all_completed);\n+    return AtomicAccess::load_acquire(&_all_completed);\n@@ -93,1 +93,1 @@\n-  Atomic::release_store(&_all_completed, true);\n+  AtomicAccess::release_store(&_all_completed, true);\n","filename":"src\/hotspot\/share\/cds\/aotLinkedClassBulkLoader.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -421,1 +421,1 @@\n-  assert(Atomic::load(&_state) != WORKING, \"Should not be working\");\n+  assert(AtomicAccess::load(&_state) != WORKING, \"Should not be working\");\n@@ -438,1 +438,1 @@\n-    int cur = Atomic::load(&_started_workers);\n+    int cur = AtomicAccess::load(&_started_workers);\n@@ -442,1 +442,1 @@\n-    if (Atomic::cmpxchg(&_started_workers, cur, cur + 1, memory_order_relaxed) == cur) {\n+    if (AtomicAccess::cmpxchg(&_started_workers, cur, cur + 1, memory_order_relaxed) == cur) {\n@@ -450,3 +450,3 @@\n-  assert(Atomic::load(&_state) == UNUSED, \"Should be unused yet\");\n-  assert(Atomic::load(&_task) == nullptr, \"Should not have running tasks\");\n-  Atomic::store(&_state, WORKING);\n+  assert(AtomicAccess::load(&_state) == UNUSED, \"Should be unused yet\");\n+  assert(AtomicAccess::load(&_task) == nullptr, \"Should not have running tasks\");\n+  AtomicAccess::store(&_state, WORKING);\n@@ -460,2 +460,2 @@\n-  assert(Atomic::load(&_state) == WORKING, \"Should be working\");\n-  Atomic::store(&_state, SHUTDOWN);\n+  assert(AtomicAccess::load(&_state) == WORKING, \"Should be working\");\n+  AtomicAccess::store(&_state, SHUTDOWN);\n@@ -478,2 +478,2 @@\n-  Atomic::store(&_finish_tokens, _num_workers + 1);\n-  Atomic::release_store(&_task, task);\n+  AtomicAccess::store(&_finish_tokens, _num_workers + 1);\n+  AtomicAccess::release_store(&_task, task);\n@@ -497,1 +497,1 @@\n-  while (Atomic::load(&_finish_tokens) != 0) {\n+  while (AtomicAccess::load(&_finish_tokens) != 0) {\n@@ -503,1 +503,1 @@\n-  assert(Atomic::load(&_finish_tokens) == 0, \"All tokens are consumed\");\n+  assert(AtomicAccess::load(&_finish_tokens) == 0, \"All tokens are consumed\");\n@@ -509,1 +509,1 @@\n-  ArchiveWorkerTask* task = Atomic::load_acquire(&_task);\n+  ArchiveWorkerTask* task = AtomicAccess::load_acquire(&_task);\n@@ -517,1 +517,1 @@\n-  if (Atomic::sub(&_finish_tokens, 1, memory_order_relaxed) == 1) {\n+  if (AtomicAccess::sub(&_finish_tokens, 1, memory_order_relaxed) == 1) {\n@@ -521,1 +521,1 @@\n-    int last = Atomic::sub(&_finish_tokens, 1, memory_order_relaxed);\n+    int last = AtomicAccess::sub(&_finish_tokens, 1, memory_order_relaxed);\n@@ -528,1 +528,1 @@\n-    int chunk = Atomic::load(&_chunk);\n+    int chunk = AtomicAccess::load(&_chunk);\n@@ -532,1 +532,1 @@\n-    if (Atomic::cmpxchg(&_chunk, chunk, chunk + 1, memory_order_relaxed) == chunk) {\n+    if (AtomicAccess::cmpxchg(&_chunk, chunk, chunk + 1, memory_order_relaxed) == chunk) {\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -90,1 +90,1 @@\n-  Atomic::store(&_parsing_thread, Thread::current());\n+  AtomicAccess::store(&_parsing_thread, Thread::current());\n@@ -107,1 +107,1 @@\n-  return Atomic::load(&_parsing_thread) == Thread::current();\n+  return AtomicAccess::load(&_parsing_thread) == Thread::current();\n@@ -111,1 +111,1 @@\n-  Atomic::store(&_parsing_thread, (Thread*)nullptr);\n+  AtomicAccess::store(&_parsing_thread, (Thread*)nullptr);\n","filename":"src\/hotspot\/share\/cds\/classListParser.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -5731,2 +5731,2 @@\n-    Atomic::cmpxchg(&counter, (size_t)0, Arguments::default_SharedBaseAddress()); \/\/ initialize it\n-    size_t new_id = Atomic::add(&counter, (size_t)1);\n+    AtomicAccess::cmpxchg(&counter, (size_t)0, Arguments::default_SharedBaseAddress()); \/\/ initialize it\n+    size_t new_id = AtomicAccess::add(&counter, (size_t)1);\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -753,1 +753,1 @@\n-      Atomic::release_store(&_first_append_entry_list, new_entry);\n+      AtomicAccess::release_store(&_first_append_entry_list, new_entry);\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -215,1 +215,1 @@\n-    return Atomic::load_acquire(&_first_append_entry_list);\n+    return AtomicAccess::load_acquire(&_first_append_entry_list);\n","filename":"src\/hotspot\/share\/classfile\/classLoader.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -33,1 +33,1 @@\n-inline ClassPathEntry* ClassPathEntry::next() const { return Atomic::load_acquire(&_next); }\n+inline ClassPathEntry* ClassPathEntry::next() const { return AtomicAccess::load_acquire(&_next); }\n@@ -37,1 +37,1 @@\n-  Atomic::release_store(&_next, next);\n+  AtomicAccess::release_store(&_next, next);\n","filename":"src\/hotspot\/share\/classfile\/classLoader.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -195,1 +195,1 @@\n-    Atomic::release_store(&_head, next);\n+    AtomicAccess::release_store(&_head, next);\n@@ -199,1 +199,1 @@\n-  Atomic::release_store(&_head->_size, _head->_size + 1);\n+  AtomicAccess::release_store(&_head->_size, _head->_size + 1);\n@@ -205,1 +205,1 @@\n-  Chunk* chunk = Atomic::load_acquire(&_head);\n+  Chunk* chunk = AtomicAccess::load_acquire(&_head);\n@@ -207,1 +207,1 @@\n-    count += Atomic::load(&chunk->_size);\n+    count += AtomicAccess::load(&chunk->_size);\n@@ -220,1 +220,1 @@\n-  Chunk* head = Atomic::load_acquire(&_head);\n+  Chunk* head = AtomicAccess::load_acquire(&_head);\n@@ -223,1 +223,1 @@\n-    oops_do_chunk(f, head, Atomic::load_acquire(&head->_size));\n+    oops_do_chunk(f, head, AtomicAccess::load_acquire(&head->_size));\n@@ -261,1 +261,1 @@\n-  Chunk* chunk = Atomic::load_acquire(&_head);\n+  Chunk* chunk = AtomicAccess::load_acquire(&_head);\n@@ -263,1 +263,1 @@\n-    if (&(chunk->_data[0]) <= oop_handle && oop_handle < &(chunk->_data[Atomic::load(&chunk->_size)])) {\n+    if (&(chunk->_data[0]) <= oop_handle && oop_handle < &(chunk->_data[AtomicAccess::load(&chunk->_size)])) {\n@@ -274,1 +274,1 @@\n-    int old_claim = Atomic::load(&_claim);\n+    int old_claim = AtomicAccess::load(&_claim);\n@@ -279,1 +279,1 @@\n-    if (Atomic::cmpxchg(&_claim, old_claim, new_claim) == old_claim) {\n+    if (AtomicAccess::cmpxchg(&_claim, old_claim, new_claim) == old_claim) {\n@@ -293,1 +293,1 @@\n-    int old_claim = Atomic::load(&_claim);\n+    int old_claim = AtomicAccess::load(&_claim);\n@@ -298,1 +298,1 @@\n-    if (Atomic::cmpxchg(&_claim, old_claim, new_claim) == old_claim) {\n+    if (AtomicAccess::cmpxchg(&_claim, old_claim, new_claim) == old_claim) {\n@@ -386,1 +386,1 @@\n-  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+  for (Klass* k = AtomicAccess::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n@@ -394,1 +394,1 @@\n-  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+  for (Klass* k = AtomicAccess::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n@@ -402,1 +402,1 @@\n-  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+  for (Klass* k = AtomicAccess::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n@@ -411,1 +411,1 @@\n-  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+  for (Klass* k = AtomicAccess::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n@@ -439,1 +439,1 @@\n-  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+  for (Klass* k = AtomicAccess::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n@@ -501,1 +501,1 @@\n-    NOT_PRODUCT(Atomic::inc(&_dependency_count));\n+    NOT_PRODUCT(AtomicAccess::inc(&_dependency_count));\n@@ -526,1 +526,1 @@\n-    Atomic::release_store(&_klasses, k);\n+    AtomicAccess::release_store(&_klasses, k);\n@@ -638,1 +638,1 @@\n-  ModuleEntryTable* modules = Atomic::load_acquire(&_modules);\n+  ModuleEntryTable* modules = AtomicAccess::load_acquire(&_modules);\n@@ -648,1 +648,1 @@\n-        Atomic::release_store(&_modules, modules);\n+        AtomicAccess::release_store(&_modules, modules);\n@@ -822,1 +822,1 @@\n-  ClassLoaderMetaspace* metaspace = Atomic::load_acquire(&_metaspace);\n+  ClassLoaderMetaspace* metaspace = AtomicAccess::load_acquire(&_metaspace);\n@@ -836,1 +836,1 @@\n-      Atomic::release_store(&_metaspace, metaspace);\n+      AtomicAccess::release_store(&_metaspace, metaspace);\n@@ -1123,1 +1123,1 @@\n-  for (Klass* k = Atomic::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n+  for (Klass* k = AtomicAccess::load_acquire(&_klasses); k != nullptr; k = k->next_link()) {\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":25,"deletions":25,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  Atomic::store(&_next, next);\n+  AtomicAccess::store(&_next, next);\n@@ -42,1 +42,1 @@\n-  return Atomic::load(&_next);\n+  return AtomicAccess::load(&_next);\n@@ -47,1 +47,1 @@\n-  Atomic::store(&_next, _next->_next);\n+  AtomicAccess::store(&_next, _next->_next);\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -64,1 +64,1 @@\n-  for (ClassLoaderData* cld = Atomic::load_acquire(&_head); cld != nullptr; cld = cld->next()) {\n+  for (ClassLoaderData* cld = AtomicAccess::load_acquire(&_head); cld != nullptr; cld = cld->next()) {\n@@ -70,1 +70,1 @@\n- for (ClassLoaderData* cld = Atomic::load_acquire(&_head); cld != nullptr; cld = cld->next()) {\n+ for (ClassLoaderData* cld = AtomicAccess::load_acquire(&_head); cld != nullptr; cld = cld->next()) {\n@@ -77,1 +77,1 @@\n- for (ClassLoaderData* cld = Atomic::load_acquire(&_head); cld != nullptr; cld = cld->next()) {\n+ for (ClassLoaderData* cld = AtomicAccess::load_acquire(&_head); cld != nullptr; cld = cld->next()) {\n@@ -158,1 +158,1 @@\n-  Atomic::release_store(&_head, cld);\n+  AtomicAccess::release_store(&_head, cld);\n@@ -195,1 +195,1 @@\n-  for (ClassLoaderData* cld = Atomic::load_acquire(&_head);  cld != nullptr; cld = cld->next()) {\n+  for (ClassLoaderData* cld = AtomicAccess::load_acquire(&_head);  cld != nullptr; cld = cld->next()) {\n@@ -202,1 +202,1 @@\n-  for (ClassLoaderData* cld = Atomic::load_acquire(&_head);  cld != nullptr; cld = cld->next()) {\n+  for (ClassLoaderData* cld = AtomicAccess::load_acquire(&_head);  cld != nullptr; cld = cld->next()) {\n@@ -431,1 +431,1 @@\n-        Atomic::store(&_head, data->next());\n+        AtomicAccess::store(&_head, data->next());\n@@ -536,1 +536,1 @@\n-    Klass* old_head = Atomic::cmpxchg(&_next_klass, head, next);\n+    Klass* old_head = AtomicAccess::cmpxchg(&_next_klass, head, next);\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataGraph.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -47,1 +47,1 @@\n-  return Atomic::load(&_num_instance_classes);\n+  return AtomicAccess::load(&_num_instance_classes);\n@@ -51,1 +51,1 @@\n-  return Atomic::load(&_num_array_classes);\n+  return AtomicAccess::load(&_num_array_classes);\n@@ -55,1 +55,1 @@\n-  Atomic::add(&_num_instance_classes, count, memory_order_relaxed);\n+  AtomicAccess::add(&_num_instance_classes, count, memory_order_relaxed);\n@@ -59,1 +59,1 @@\n-  size_t old_count = Atomic::fetch_then_add(&_num_instance_classes, -count, memory_order_relaxed);\n+  size_t old_count = AtomicAccess::fetch_then_add(&_num_instance_classes, -count, memory_order_relaxed);\n@@ -64,1 +64,1 @@\n-  Atomic::add(&_num_array_classes, count, memory_order_relaxed);\n+  AtomicAccess::add(&_num_array_classes, count, memory_order_relaxed);\n@@ -68,1 +68,1 @@\n-  size_t old_count = Atomic::fetch_then_add(&_num_array_classes, -count, memory_order_relaxed);\n+  size_t old_count = AtomicAccess::fetch_then_add(&_num_array_classes, -count, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/classfile\/classLoaderDataGraph.inline.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -207,1 +207,1 @@\n-  uint8_t value = Atomic::load(addr);\n+  uint8_t value = AtomicAccess::load(addr);\n@@ -211,1 +211,1 @@\n-    value = Atomic::cmpxchg(addr, old_value, value);\n+    value = AtomicAccess::cmpxchg(addr, old_value, value);\n@@ -2143,1 +2143,1 @@\n-  int res = Atomic::cmpxchg(addr, old_state, new_state);\n+  int res = AtomicAccess::cmpxchg(addr, old_state, new_state);\n@@ -2161,1 +2161,1 @@\n-  jboolean vthread_on_list = Atomic::load(addr);\n+  jboolean vthread_on_list = AtomicAccess::load(addr);\n@@ -2163,1 +2163,1 @@\n-    vthread_on_list = Atomic::cmpxchg(addr, (jboolean)JNI_FALSE, (jboolean)JNI_TRUE);\n+    vthread_on_list = AtomicAccess::cmpxchg(addr, (jboolean)JNI_FALSE, (jboolean)JNI_TRUE);\n@@ -4784,1 +4784,1 @@\n-  return Atomic::load_acquire(loader->field_addr<ClassLoaderData*>(_loader_data_offset));\n+  return AtomicAccess::load_acquire(loader->field_addr<ClassLoaderData*>(_loader_data_offset));\n@@ -4796,1 +4796,1 @@\n-  Atomic::release_store(loader->field_addr<ClassLoaderData*>(_loader_data_offset), new_data);\n+  AtomicAccess::release_store(loader->field_addr<ClassLoaderData*>(_loader_data_offset), new_data);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  return (Atomic::load(flags_addr(java_string)) & flag_mask) != 0;\n+  return (AtomicAccess::load(flags_addr(java_string)) & flag_mask) != 0;\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -225,1 +225,1 @@\n-    return((Atomic::load(&_defined_by_cds_in_class_path) & ((int)1 << idx)) != 0);\n+    return((AtomicAccess::load(&_defined_by_cds_in_class_path) & ((int)1 << idx)) != 0);\n@@ -229,1 +229,1 @@\n-    Atomic::fetch_then_or(&_defined_by_cds_in_class_path, ((int)1 << idx));\n+    AtomicAccess::fetch_then_or(&_defined_by_cds_in_class_path, ((int)1 << idx));\n","filename":"src\/hotspot\/share\/classfile\/packageEntry.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -328,1 +328,1 @@\n-  Atomic::inc(&_items_count);\n+  AtomicAccess::inc(&_items_count);\n@@ -332,1 +332,1 @@\n-  Atomic::dec(&_items_count);\n+  AtomicAccess::dec(&_items_count);\n@@ -348,1 +348,1 @@\n-  return Atomic::load_acquire(&_has_work);\n+  return AtomicAccess::load_acquire(&_has_work);\n@@ -352,1 +352,1 @@\n-  return Atomic::load_acquire(&_items_count);\n+  return AtomicAccess::load_acquire(&_items_count);\n@@ -359,1 +359,1 @@\n-    Atomic::store(&_has_work, true);\n+    AtomicAccess::store(&_has_work, true);\n@@ -513,1 +513,1 @@\n-  assert(!Atomic::load_acquire(&_disable_interning_during_cds_dump),\n+  assert(!AtomicAccess::load_acquire(&_disable_interning_during_cds_dump),\n@@ -669,1 +669,1 @@\n-    Atomic::release_store(&_has_work, false);\n+    AtomicAccess::release_store(&_has_work, false);\n@@ -679,1 +679,1 @@\n-  Atomic::release_store(&_has_work, false);\n+  AtomicAccess::release_store(&_has_work, false);\n@@ -969,1 +969,1 @@\n-  DEBUG_ONLY(Atomic::release_store(&_disable_interning_during_cds_dump, true));\n+  DEBUG_ONLY(AtomicAccess::release_store(&_disable_interning_during_cds_dump, true));\n@@ -1108,1 +1108,1 @@\n-  DEBUG_ONLY(Atomic::release_store(&_disable_interning_during_cds_dump, false));\n+  DEBUG_ONLY(AtomicAccess::release_store(&_disable_interning_during_cds_dump, false));\n","filename":"src\/hotspot\/share\/classfile\/stringTable.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -219,3 +219,3 @@\n-void SymbolTable::reset_has_items_to_clean() { Atomic::store(&_has_items_to_clean, false); }\n-void SymbolTable::mark_has_items_to_clean()  { Atomic::store(&_has_items_to_clean, true); }\n-bool SymbolTable::has_items_to_clean()       { return Atomic::load(&_has_items_to_clean); }\n+void SymbolTable::reset_has_items_to_clean() { AtomicAccess::store(&_has_items_to_clean, false); }\n+void SymbolTable::mark_has_items_to_clean()  { AtomicAccess::store(&_has_items_to_clean, true); }\n+bool SymbolTable::has_items_to_clean()       { return AtomicAccess::load(&_has_items_to_clean); }\n@@ -224,1 +224,1 @@\n-  Atomic::inc(&_items_count);\n+  AtomicAccess::inc(&_items_count);\n@@ -228,2 +228,2 @@\n-  Atomic::inc(&(_symbols_removed));\n-  Atomic::dec(&_items_count);\n+  AtomicAccess::inc(&(_symbols_removed));\n+  AtomicAccess::dec(&_items_count);\n@@ -240,1 +240,1 @@\n-bool SymbolTable::has_work() { return Atomic::load_acquire(&_has_work); }\n+bool SymbolTable::has_work() { return AtomicAccess::load_acquire(&_has_work); }\n@@ -789,1 +789,1 @@\n-  Atomic::add(&_symbols_counted, stdc._processed);\n+  AtomicAccess::add(&_symbols_counted, stdc._processed);\n@@ -817,1 +817,1 @@\n-    Atomic::release_store(&_has_work, false);\n+    AtomicAccess::release_store(&_has_work, false);\n@@ -827,1 +827,1 @@\n-  Atomic::release_store(&_has_work, false);\n+  AtomicAccess::release_store(&_has_work, false);\n","filename":"src\/hotspot\/share\/classfile\/symbolTable.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1082,1 +1082,1 @@\n-  assert(Atomic::add(&ik->_shared_class_load_count, 1) == 1, \"shared class loaded more than once\");\n+  assert(AtomicAccess::add(&ik->_shared_class_load_count, 1) == 1, \"shared class loaded more than once\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -583,1 +583,1 @@\n-      Atomic::dec(&_number_of_nmethods_with_dependencies);\n+      AtomicAccess::dec(&_number_of_nmethods_with_dependencies);\n@@ -619,1 +619,1 @@\n-      Atomic::inc(&_number_of_nmethods_with_dependencies);\n+      AtomicAccess::inc(&_number_of_nmethods_with_dependencies);\n@@ -789,1 +789,1 @@\n-    if (Atomic::cmpxchg(&_unloading_threshold_gc_requested, false, true) == false) {\n+    if (AtomicAccess::cmpxchg(&_unloading_threshold_gc_requested, false, true) == false) {\n@@ -815,1 +815,1 @@\n-    if (Atomic::cmpxchg(&_unloading_threshold_gc_requested, false, true) == false) {\n+    if (AtomicAccess::cmpxchg(&_unloading_threshold_gc_requested, false, true) == false) {\n@@ -902,1 +902,1 @@\n-      ExceptionCache* purge_list_head = Atomic::load(&_exception_cache_purge_list);\n+      ExceptionCache* purge_list_head = AtomicAccess::load(&_exception_cache_purge_list);\n@@ -904,1 +904,1 @@\n-      if (Atomic::cmpxchg(&_exception_cache_purge_list, purge_list_head, entry) == purge_list_head) {\n+      if (AtomicAccess::cmpxchg(&_exception_cache_purge_list, purge_list_head, entry) == purge_list_head) {\n@@ -1155,1 +1155,1 @@\n-  return Atomic::load_acquire(&_number_of_nmethods_with_dependencies) != 0;\n+  return AtomicAccess::load_acquire(&_number_of_nmethods_with_dependencies) != 0;\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -107,2 +107,2 @@\n-    Atomic::store(&_speculated_klass, (uintptr_t)0);\n-    Atomic::store(&_speculated_method, (Method*)nullptr);\n+    AtomicAccess::store(&_speculated_klass, (uintptr_t)0);\n+    AtomicAccess::store(&_speculated_method, (Method*)nullptr);\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -110,1 +110,1 @@\n-  nmethodBucket* head = Atomic::load(_dependency_context_addr);\n+  nmethodBucket* head = AtomicAccess::load(_dependency_context_addr);\n@@ -124,1 +124,1 @@\n-    if (Atomic::cmpxchg(_dependency_context_addr, head, new_head) == head) {\n+    if (AtomicAccess::cmpxchg(_dependency_context_addr, head, new_head) == head) {\n@@ -127,1 +127,1 @@\n-    head = Atomic::load(_dependency_context_addr);\n+    head = AtomicAccess::load(_dependency_context_addr);\n@@ -145,1 +145,1 @@\n-      nmethodBucket* purge_list_head = Atomic::load(&_purge_list);\n+      nmethodBucket* purge_list_head = AtomicAccess::load(&_purge_list);\n@@ -147,1 +147,1 @@\n-      if (Atomic::cmpxchg(&_purge_list, purge_list_head, b) == purge_list_head) {\n+      if (AtomicAccess::cmpxchg(&_purge_list, purge_list_head, b) == purge_list_head) {\n@@ -199,1 +199,1 @@\n-  nmethodBucket* first = Atomic::load_acquire(_dependency_context_addr);\n+  nmethodBucket* first = AtomicAccess::load_acquire(_dependency_context_addr);\n@@ -214,1 +214,1 @@\n-  nmethodBucket* old_purge_list_head = Atomic::load(&_purge_list);\n+  nmethodBucket* old_purge_list_head = AtomicAccess::load(&_purge_list);\n@@ -217,1 +217,1 @@\n-    nmethodBucket* next_purge_list_head = Atomic::cmpxchg(&_purge_list, old_purge_list_head, first);\n+    nmethodBucket* next_purge_list_head = AtomicAccess::cmpxchg(&_purge_list, old_purge_list_head, first);\n@@ -267,2 +267,2 @@\n-  uint64_t cleaning_epoch = Atomic::load(&_cleaning_epoch);\n-  uint64_t last_cleanup = Atomic::load(_last_cleanup_addr);\n+  uint64_t cleaning_epoch = AtomicAccess::load(&_cleaning_epoch);\n+  uint64_t last_cleanup = AtomicAccess::load(_last_cleanup_addr);\n@@ -272,1 +272,1 @@\n-  return Atomic::cmpxchg(_last_cleanup_addr, last_cleanup, cleaning_epoch) == last_cleanup;\n+  return AtomicAccess::cmpxchg(_last_cleanup_addr, last_cleanup, cleaning_epoch) == last_cleanup;\n@@ -276,1 +276,1 @@\n-  return Atomic::load(&_cleaning_epoch) == 0;\n+  return AtomicAccess::load(&_cleaning_epoch) == 0;\n@@ -285,1 +285,1 @@\n-    nmethodBucket* head = Atomic::load_acquire(_dependency_context_addr);\n+    nmethodBucket* head = AtomicAccess::load_acquire(_dependency_context_addr);\n@@ -291,1 +291,1 @@\n-    if (Atomic::load(_dependency_context_addr) != head) {\n+    if (AtomicAccess::load(_dependency_context_addr) != head) {\n@@ -295,1 +295,1 @@\n-    if (Atomic::cmpxchg(_dependency_context_addr, head, head_next) == head) {\n+    if (AtomicAccess::cmpxchg(_dependency_context_addr, head, head_next) == head) {\n@@ -304,1 +304,1 @@\n-  Atomic::store(_dependency_context_addr, b);\n+  AtomicAccess::store(_dependency_context_addr, b);\n@@ -308,1 +308,1 @@\n-  return Atomic::load(_dependency_context_addr);\n+  return AtomicAccess::load(_dependency_context_addr);\n@@ -317,1 +317,1 @@\n-  Atomic::store(&_cleaning_epoch, epoch);\n+  AtomicAccess::store(&_cleaning_epoch, epoch);\n@@ -327,1 +327,1 @@\n-  Atomic::store(&_cleaning_epoch, epoch);\n+  AtomicAccess::store(&_cleaning_epoch, epoch);\n@@ -339,1 +339,1 @@\n-    nmethodBucket* next = Atomic::load(&_next);\n+    nmethodBucket* next = AtomicAccess::load(&_next);\n@@ -343,1 +343,1 @@\n-    nmethodBucket* next_next = Atomic::load(&next->_next);\n+    nmethodBucket* next_next = AtomicAccess::load(&next->_next);\n@@ -345,1 +345,1 @@\n-    if (Atomic::load(&_next) != next) {\n+    if (AtomicAccess::load(&_next) != next) {\n@@ -349,1 +349,1 @@\n-    if (Atomic::cmpxchg(&_next, next, next_next) == next) {\n+    if (AtomicAccess::cmpxchg(&_next, next, next_next) == next) {\n@@ -358,1 +358,1 @@\n-  return Atomic::load(&_next);\n+  return AtomicAccess::load(&_next);\n@@ -362,1 +362,1 @@\n-  Atomic::store(&_next, b);\n+  AtomicAccess::store(&_next, b);\n@@ -366,1 +366,1 @@\n-  return Atomic::load(&_purge_list_next);\n+  return AtomicAccess::load(&_purge_list_next);\n@@ -370,1 +370,1 @@\n-  Atomic::store(&_purge_list_next, b);\n+  AtomicAccess::store(&_purge_list_next, b);\n","filename":"src\/hotspot\/share\/code\/dependencyContext.cpp","additions":28,"deletions":28,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -379,1 +379,1 @@\n-  return Atomic::load(&_next);\n+  return AtomicAccess::load(&_next);\n@@ -383,1 +383,1 @@\n-  Atomic::store(&_next, ec);\n+  AtomicAccess::store(&_next, ec);\n@@ -495,1 +495,1 @@\n-    Atomic::store(&_deoptimization_status, deoptimize_done);\n+    AtomicAccess::store(&_deoptimization_status, deoptimize_done);\n@@ -500,1 +500,1 @@\n-  return Atomic::load_acquire(&_exception_cache);\n+  return AtomicAccess::load_acquire(&_exception_cache);\n@@ -520,1 +520,1 @@\n-        if (Atomic::cmpxchg(&_exception_cache, ec, next) == ec) {\n+        if (AtomicAccess::cmpxchg(&_exception_cache, ec, next) == ec) {\n@@ -530,1 +530,1 @@\n-    if (Atomic::cmpxchg(&_exception_cache, ec, new_entry) == ec) {\n+    if (AtomicAccess::cmpxchg(&_exception_cache, ec, new_entry) == ec) {\n@@ -563,1 +563,1 @@\n-        if (Atomic::cmpxchg(&_exception_cache, curr, next) != curr) {\n+        if (AtomicAccess::cmpxchg(&_exception_cache, curr, next) != curr) {\n@@ -922,1 +922,1 @@\n-          Atomic::store(r->metadata_addr(), (Method*)nullptr);\n+          AtomicAccess::store(r->metadata_addr(), (Method*)nullptr);\n@@ -1926,1 +1926,1 @@\n-  Atomic::store(&_gc_epoch, CodeCache::gc_epoch());\n+  AtomicAccess::store(&_gc_epoch, CodeCache::gc_epoch());\n@@ -1932,1 +1932,1 @@\n-  return Atomic::load(&_gc_epoch) >= CodeCache::previous_completed_gc_marking_cycle();\n+  return AtomicAccess::load(&_gc_epoch) >= CodeCache::previous_completed_gc_marking_cycle();\n@@ -1959,1 +1959,1 @@\n-  Atomic::store(&_state, new_state);\n+  AtomicAccess::store(&_state, new_state);\n@@ -2010,1 +2010,1 @@\n-  if (Atomic::load(&_state) == not_entrant) {\n+  if (AtomicAccess::load(&_state) == not_entrant) {\n@@ -2022,1 +2022,1 @@\n-    if (Atomic::load(&_state) == not_entrant) {\n+    if (AtomicAccess::load(&_state) == not_entrant) {\n@@ -2393,1 +2393,1 @@\n-  uint8_t state = Atomic::load(&_is_unloading_state);\n+  uint8_t state = AtomicAccess::load(&_is_unloading_state);\n@@ -2416,1 +2416,1 @@\n-  uint8_t found_state = Atomic::cmpxchg(&_is_unloading_state, state, new_state, memory_order_relaxed);\n+  uint8_t found_state = AtomicAccess::cmpxchg(&_is_unloading_state, state, new_state, memory_order_relaxed);\n@@ -2429,1 +2429,1 @@\n-  Atomic::store(&_is_unloading_state, state);\n+  AtomicAccess::store(&_is_unloading_state, state);\n@@ -2514,1 +2514,1 @@\n-      (Atomic::replace_if_null(&_oops_do_mark_link, mark_link(this, claim_weak_request_tag)))) {\n+      (AtomicAccess::replace_if_null(&_oops_do_mark_link, mark_link(this, claim_weak_request_tag)))) {\n@@ -2528,1 +2528,1 @@\n-  oops_do_mark_link* old_next = Atomic::cmpxchg(&_oops_do_mark_link, mark_link(nullptr, claim_weak_request_tag), mark_link(this, claim_strong_done_tag));\n+  oops_do_mark_link* old_next = AtomicAccess::cmpxchg(&_oops_do_mark_link, mark_link(nullptr, claim_weak_request_tag), mark_link(this, claim_strong_done_tag));\n@@ -2539,1 +2539,1 @@\n-  oops_do_mark_link* old_next = Atomic::cmpxchg(&_oops_do_mark_link, next, mark_link(this, claim_strong_request_tag));\n+  oops_do_mark_link* old_next = AtomicAccess::cmpxchg(&_oops_do_mark_link, next, mark_link(this, claim_strong_request_tag));\n@@ -2550,1 +2550,1 @@\n-  oops_do_mark_link* old_next = Atomic::cmpxchg(&_oops_do_mark_link, next, mark_link(extract_nmethod(next), claim_strong_done_tag));\n+  oops_do_mark_link* old_next = AtomicAccess::cmpxchg(&_oops_do_mark_link, next, mark_link(extract_nmethod(next), claim_strong_done_tag));\n@@ -2565,1 +2565,1 @@\n-  nmethod* old_head = Atomic::xchg(&_oops_do_mark_nmethods, this);\n+  nmethod* old_head = AtomicAccess::xchg(&_oops_do_mark_nmethods, this);\n@@ -2571,1 +2571,1 @@\n-  if (Atomic::cmpxchg(&_oops_do_mark_link, mark_link(this, claim_weak_request_tag), mark_link(old_head, claim_weak_done_tag)) == mark_link(this, claim_weak_request_tag)) {\n+  if (AtomicAccess::cmpxchg(&_oops_do_mark_link, mark_link(this, claim_weak_request_tag), mark_link(old_head, claim_weak_done_tag)) == mark_link(this, claim_weak_request_tag)) {\n@@ -2582,1 +2582,1 @@\n-  nmethod* old_head = Atomic::xchg(&_oops_do_mark_nmethods, this);\n+  nmethod* old_head = AtomicAccess::xchg(&_oops_do_mark_nmethods, this);\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":24,"deletions":24,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -289,1 +289,1 @@\n-    return Atomic::load(&_deoptimization_status);\n+    return AtomicAccess::load(&_deoptimization_status);\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -46,1 +46,1 @@\n-inline int ExceptionCache::count() { return Atomic::load_acquire(&_count); }\n+inline int ExceptionCache::count() { return AtomicAccess::load_acquire(&_count); }\n@@ -59,1 +59,1 @@\n-inline void ExceptionCache::increment_count() { Atomic::release_store(&_count, _count + 1); }\n+inline void ExceptionCache::increment_count() { AtomicAccess::release_store(&_count, _count + 1); }\n","filename":"src\/hotspot\/share\/code\/nmethod.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-      Atomic::store(&_table[i], (VtableStub*)nullptr);\n+      AtomicAccess::store(&_table[i], (VtableStub*)nullptr);\n@@ -271,1 +271,1 @@\n-  VtableStub* s = Atomic::load(&_table[hash]);\n+  VtableStub* s = AtomicAccess::load(&_table[hash]);\n@@ -282,1 +282,1 @@\n-  s->set_next(Atomic::load(&_table[h]));\n+  s->set_next(AtomicAccess::load(&_table[h]));\n@@ -284,1 +284,1 @@\n-  Atomic::release_store(&_table[h], s);\n+  AtomicAccess::release_store(&_table[h], s);\n@@ -295,1 +295,1 @@\n-  for (s = Atomic::load(&_table[hash]); s != nullptr && s->entry_point() != pc; s = s->next()) {}\n+  for (s = AtomicAccess::load(&_table[hash]); s != nullptr && s->entry_point() != pc; s = s->next()) {}\n@@ -308,1 +308,1 @@\n-    for (VtableStub* s = Atomic::load_acquire(&_table[i]); s != nullptr; s = s->next()) {\n+    for (VtableStub* s = AtomicAccess::load_acquire(&_table[i]); s != nullptr; s = s->next()) {\n@@ -321,1 +321,1 @@\n-    for (VtableStub* s = Atomic::load_acquire(&_table[i]); s != nullptr; s = s->next()) {\n+    for (VtableStub* s = AtomicAccess::load_acquire(&_table[i]); s != nullptr; s = s->next()) {\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -905,1 +905,1 @@\n-        if (Atomic::cmpxchg(&_arenastat_oom_crash, (ArenaStatCounter*) nullptr, arena_stat) != nullptr) {\n+        if (AtomicAccess::cmpxchg(&_arenastat_oom_crash, (ArenaStatCounter*) nullptr, arena_stat) != nullptr) {\n@@ -995,1 +995,1 @@\n-  return Atomic::load(&_arenastat_oom_crash) != nullptr;\n+  return AtomicAccess::load(&_arenastat_oom_crash) != nullptr;\n@@ -1003,1 +1003,1 @@\n-  const ArenaStatCounter* const oom_stats = Atomic::load(&_arenastat_oom_crash);\n+  const ArenaStatCounter* const oom_stats = AtomicAccess::load(&_arenastat_oom_crash);\n","filename":"src\/hotspot\/share\/compiler\/compilationMemoryStatistic.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1590,1 +1590,1 @@\n-    return Atomic::add(CICountNative ? &_native_compilation_id : &_compilation_id, 1);\n+    return AtomicAccess::add(CICountNative ? &_native_compilation_id : &_compilation_id, 1);\n@@ -1592,1 +1592,1 @@\n-    id = Atomic::add(&_osr_compilation_id, 1);\n+    id = AtomicAccess::add(&_osr_compilation_id, 1);\n@@ -1597,1 +1597,1 @@\n-    id = Atomic::add(&_compilation_id, 1);\n+    id = AtomicAccess::add(&_compilation_id, 1);\n@@ -1609,1 +1609,1 @@\n-  return Atomic::add(&_compilation_id, 1);\n+  return AtomicAccess::add(&_compilation_id, 1);\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -365,1 +365,1 @@\n-    jint old = Atomic::cmpxchg(&_should_compile_new_jobs, 1-new_state, new_state);\n+    jint old = AtomicAccess::cmpxchg(&_should_compile_new_jobs, 1-new_state, new_state);\n@@ -380,1 +380,1 @@\n-    Atomic::xchg(&_should_compile_new_jobs, jint(shutdown_compilation));\n+    AtomicAccess::xchg(&_should_compile_new_jobs, jint(shutdown_compilation));\n@@ -384,1 +384,1 @@\n-    return Atomic::load(&_should_compile_new_jobs) == shutdown_compilation;\n+    return AtomicAccess::load(&_should_compile_new_jobs) == shutdown_compilation;\n@@ -392,1 +392,1 @@\n-    jint old = Atomic::cmpxchg(&_print_compilation_warning, 0, 1);\n+    jint old = AtomicAccess::cmpxchg(&_print_compilation_warning, 0, 1);\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -54,1 +54,1 @@\n-    CompileLog* head = Atomic::load_acquire(&_list_head);\n+    CompileLog* head = AtomicAccess::load_acquire(&_list_head);\n@@ -56,1 +56,1 @@\n-    if (Atomic::cmpxchg(&_list_head, head, this) == head) {\n+    if (AtomicAccess::cmpxchg(&_list_head, head, this) == head) {\n@@ -209,1 +209,1 @@\n-  CompileLog* log = Atomic::load_acquire(&_list_head);\n+  CompileLog* log = AtomicAccess::load_acquire(&_list_head);\n@@ -297,1 +297,1 @@\n-  Atomic::store(&_list_head, (CompileLog*)nullptr);\n+  AtomicAccess::store(&_list_head, (CompileLog*)nullptr);\n","filename":"src\/hotspot\/share\/compiler\/compileLog.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  Atomic::add(&_active_tasks, 1, memory_order_relaxed);\n+  AtomicAccess::add(&_active_tasks, 1, memory_order_relaxed);\n@@ -94,1 +94,1 @@\n-  if (Atomic::sub(&_active_tasks, 1, memory_order_relaxed) == 0) {\n+  if (AtomicAccess::sub(&_active_tasks, 1, memory_order_relaxed) == 0) {\n@@ -102,1 +102,1 @@\n-  while (Atomic::load(&_active_tasks) > 0) {\n+  while (AtomicAccess::load(&_active_tasks) > 0) {\n","filename":"src\/hotspot\/share\/compiler\/compileTask.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -157,1 +157,1 @@\n-    if ((used - last >= _step_counter_update) && Atomic::cmpxchg(&_last_counter_update, last, used) == last) {\n+    if ((used - last >= _step_counter_update) && AtomicAccess::cmpxchg(&_last_counter_update, last, used) == last) {\n@@ -165,1 +165,1 @@\n-    if ((used - last >= _step_heap_print) && Atomic::cmpxchg(&_last_heap_print, last, used) == last) {\n+    if ((used - last >= _step_heap_print) && AtomicAccess::cmpxchg(&_last_heap_print, last, used) == last) {\n","filename":"src\/hotspot\/share\/gc\/epsilon\/epsilonHeap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -43,1 +43,1 @@\n-  task = Atomic::fetch_then_add(&_num_serial_tasks_done, 1);\n+  task = AtomicAccess::fetch_then_add(&_num_serial_tasks_done, 1);\n@@ -99,2 +99,2 @@\n-  assert(Atomic::load(&_num_serial_tasks_done) >= _serial_tasks.length(),\n-         \"Only %d tasks of %d claimed\", Atomic::load(&_num_serial_tasks_done), _serial_tasks.length());\n+  assert(AtomicAccess::load(&_num_serial_tasks_done) >= _serial_tasks.length(),\n+         \"Only %d tasks of %d claimed\", AtomicAccess::load(&_num_serial_tasks_done), _serial_tasks.length());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BatchedTask.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-  Atomic::store(addr, offset);\n+  AtomicAccess::store(addr, offset);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -55,1 +55,1 @@\n-  return Atomic::load(addr);\n+  return AtomicAccess::load(addr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BlockOffsetTable.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -218,1 +218,1 @@\n-  Atomic::inc(&_coarsen_from[tag], memory_order_relaxed);\n+  AtomicAccess::inc(&_coarsen_from[tag], memory_order_relaxed);\n@@ -220,1 +220,1 @@\n-    Atomic::inc(&_coarsen_collision[tag], memory_order_relaxed);\n+    AtomicAccess::inc(&_coarsen_collision[tag], memory_order_relaxed);\n@@ -317,1 +317,1 @@\n-      Atomic::store(&_inserted_card, true);\n+      AtomicAccess::store(&_inserted_card, true);\n@@ -346,1 +346,1 @@\n-    if (Atomic::load(&_inserted_card)) {\n+    if (AtomicAccess::load(&_inserted_card)) {\n@@ -348,1 +348,1 @@\n-      Atomic::store(&_inserted_card, false);\n+      AtomicAccess::store(&_inserted_card, false);\n@@ -465,1 +465,1 @@\n-    ContainerPtr container = Atomic::load_acquire(container_addr);\n+    ContainerPtr container = AtomicAccess::load_acquire(container_addr);\n@@ -508,1 +508,1 @@\n-      ContainerPtr cur_container = Atomic::load_acquire(container_addr);\n+      ContainerPtr cur_container = AtomicAccess::load_acquire(container_addr);\n@@ -514,1 +514,1 @@\n-      ContainerPtr old_value = Atomic::cmpxchg(container_addr, cur_container, G1CardSet::FullCardSet);\n+      ContainerPtr old_value = AtomicAccess::cmpxchg(container_addr, cur_container, G1CardSet::FullCardSet);\n@@ -550,1 +550,1 @@\n-    if (Atomic::load(&howl->_num_entries) >= _config->cards_in_howl_threshold()) {\n+    if (AtomicAccess::load(&howl->_num_entries) >= _config->cards_in_howl_threshold()) {\n@@ -574,1 +574,1 @@\n-    Atomic::inc(&howl->_num_entries, memory_order_relaxed);\n+    AtomicAccess::inc(&howl->_num_entries, memory_order_relaxed);\n@@ -643,1 +643,1 @@\n-  ContainerPtr old_value = Atomic::cmpxchg(container_addr, cur_container, new_container); \/\/ Memory order?\n+  ContainerPtr old_value = AtomicAccess::cmpxchg(container_addr, cur_container, new_container); \/\/ Memory order?\n@@ -690,1 +690,1 @@\n-    Atomic::add(&_num_occupied, _config->max_cards_in_region() - table_entry->_num_occupied, memory_order_relaxed);\n+    AtomicAccess::add(&_num_occupied, _config->max_cards_in_region() - table_entry->_num_occupied, memory_order_relaxed);\n@@ -716,1 +716,1 @@\n-    Atomic::add(&howling_array->_num_entries, diff, memory_order_relaxed);\n+    AtomicAccess::add(&howling_array->_num_entries, diff, memory_order_relaxed);\n@@ -721,1 +721,1 @@\n-    Atomic::add(&table_entry->_num_occupied, diff, memory_order_relaxed);\n+    AtomicAccess::add(&table_entry->_num_occupied, diff, memory_order_relaxed);\n@@ -723,1 +723,1 @@\n-    Atomic::add(&_num_occupied, diff, memory_order_relaxed);\n+    AtomicAccess::add(&_num_occupied, diff, memory_order_relaxed);\n@@ -830,2 +830,2 @@\n-    Atomic::inc(&table_entry->_num_occupied, memory_order_relaxed);\n-    Atomic::inc(&_num_occupied, memory_order_relaxed);\n+    AtomicAccess::inc(&table_entry->_num_occupied, memory_order_relaxed);\n+    AtomicAccess::inc(&_num_occupied, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSet.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -154,1 +154,1 @@\n-  uintptr_t refcount() const { return Atomic::load_acquire(&_ref_count); }\n+  uintptr_t refcount() const { return AtomicAccess::load_acquire(&_ref_count); }\n@@ -195,1 +195,1 @@\n-      Atomic::release_store(_num_entries_addr, _local_num_entries);\n+      AtomicAccess::release_store(_num_entries_addr, _local_num_entries);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetContainers.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -70,1 +70,1 @@\n-    ContainerPtr old_value = Atomic::cmpxchg(_value_addr, _value, new_value, memory_order_relaxed);\n+    ContainerPtr old_value = AtomicAccess::cmpxchg(_value_addr, _value, new_value, memory_order_relaxed);\n@@ -129,1 +129,1 @@\n-    uintptr_t ref_count = Atomic::cmpxchg(&_ref_count, old_value, new_value);\n+    uintptr_t ref_count = AtomicAccess::cmpxchg(&_ref_count, old_value, new_value);\n@@ -140,1 +140,1 @@\n-  return Atomic::sub(&_ref_count, 2u);\n+  return AtomicAccess::sub(&_ref_count, 2u);\n@@ -155,1 +155,1 @@\n-  EntryCountType num_entries = Atomic::load(_num_entries_addr) & EntryMask;\n+  EntryCountType num_entries = AtomicAccess::load(_num_entries_addr) & EntryMask;\n@@ -157,3 +157,3 @@\n-    EntryCountType old_value = Atomic::cmpxchg(_num_entries_addr,\n-                                               num_entries,\n-                                               (EntryCountType)(num_entries | LockBitMask));\n+    EntryCountType old_value = AtomicAccess::cmpxchg(_num_entries_addr,\n+                                                     num_entries,\n+                                                     (EntryCountType)(num_entries | LockBitMask));\n@@ -192,1 +192,1 @@\n-  EntryCountType num_entries = Atomic::load_acquire(&_num_entries) & EntryMask;\n+  EntryCountType num_entries = AtomicAccess::load_acquire(&_num_entries) & EntryMask;\n@@ -226,1 +226,1 @@\n-  EntryCountType num_entries = Atomic::load_acquire(&_num_entries) & EntryMask;\n+  EntryCountType num_entries = AtomicAccess::load_acquire(&_num_entries) & EntryMask;\n@@ -238,1 +238,1 @@\n-  EntryCountType num_entries = Atomic::load_acquire(&_num_entries) & EntryMask;\n+  EntryCountType num_entries = AtomicAccess::load_acquire(&_num_entries) & EntryMask;\n@@ -263,1 +263,1 @@\n-    Atomic::inc(&_num_bits_set, memory_order_relaxed);\n+    AtomicAccess::inc(&_num_bits_set, memory_order_relaxed);\n@@ -314,1 +314,1 @@\n-  ContainerPtr container = Atomic::load_acquire(array_entry);\n+  ContainerPtr container = AtomicAccess::load_acquire(array_entry);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetContainers.inline.hpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardSetMemory.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -123,1 +123,1 @@\n-      Atomic::inc(&_num_entries);\n+      AtomicAccess::inc(&_num_entries);\n@@ -134,1 +134,1 @@\n-      Atomic::dec(&_num_entries);\n+      AtomicAccess::dec(&_num_entries);\n@@ -185,1 +185,1 @@\n-      size_t current_size = Atomic::sub(&_num_entries, num_deleted);\n+      size_t current_size = AtomicAccess::sub(&_num_entries, num_deleted);\n@@ -229,1 +229,1 @@\n-  size_t number_of_entries() const { return Atomic::load(&_num_entries); }\n+  size_t number_of_entries() const { return AtomicAccess::load(&_num_entries); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CodeRootSet.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -56,1 +56,1 @@\n-  if (Atomic::load(&_cur_claim) >= _list.length()) {\n+  if (AtomicAccess::load(&_cur_claim) >= _list.length()) {\n@@ -59,1 +59,1 @@\n-  uint claim = Atomic::fetch_then_add(&_cur_claim, _claim_step);\n+  uint claim = AtomicAccess::fetch_then_add(&_cur_claim, _claim_step);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -108,1 +108,1 @@\n-      uint result = Atomic::add(&_cur_claim_idx, _chunk_size);\n+      uint result = AtomicAccess::add(&_cur_claim_idx, _chunk_size);\n@@ -211,1 +211,1 @@\n-      Atomic::add(&_num_regions_added, num_regions);\n+      AtomicAccess::add(&_num_regions_added, num_regions);\n@@ -223,1 +223,1 @@\n-    uint num_candidates = Atomic::load(&_num_regions_added);\n+    uint num_candidates = AtomicAccess::load(&_num_regions_added);\n@@ -256,1 +256,1 @@\n-    Atomic::sub(&_num_regions_added, num_pruned, memory_order_relaxed);\n+    AtomicAccess::sub(&_num_regions_added, num_pruned, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectionSetChooser.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -154,1 +154,1 @@\n-  size_t cur_idx = Atomic::fetch_then_add(&_size, 1u);\n+  size_t cur_idx = AtomicAccess::fetch_then_add(&_size, 1u);\n@@ -161,1 +161,1 @@\n-  if (Atomic::load_acquire(&_buckets[bucket]) == nullptr) {\n+  if (AtomicAccess::load_acquire(&_buckets[bucket]) == nullptr) {\n@@ -168,1 +168,1 @@\n-    if (Atomic::load_acquire(&_buckets[bucket]) == nullptr) {\n+    if (AtomicAccess::load_acquire(&_buckets[bucket]) == nullptr) {\n@@ -261,1 +261,1 @@\n-    if (Atomic::load_acquire(&_buckets[i]) != nullptr) {\n+    if (AtomicAccess::load_acquire(&_buckets[i]) != nullptr) {\n@@ -281,1 +281,1 @@\n-    Atomic::release_store(&_buckets[i], bucket_base);\n+    AtomicAccess::release_store(&_buckets[i], bucket_base);\n@@ -386,1 +386,1 @@\n-  size_t idx = Atomic::fetch_then_add(&_num_root_regions, 1u);\n+  size_t idx = AtomicAccess::fetch_then_add(&_num_root_regions, 1u);\n@@ -414,1 +414,1 @@\n-  size_t claimed_index = Atomic::fetch_then_add(&_claimed_root_regions, 1u);\n+  size_t claimed_index = AtomicAccess::fetch_then_add(&_claimed_root_regions, 1u);\n@@ -1112,1 +1112,1 @@\n-  return Atomic::load(&_completed_mark_cycles);\n+  return AtomicAccess::load(&_completed_mark_cycles);\n@@ -1121,1 +1121,1 @@\n-    Atomic::inc(&_completed_mark_cycles, memory_order_relaxed);\n+    AtomicAccess::inc(&_completed_mark_cycles, memory_order_relaxed);\n@@ -1323,1 +1323,1 @@\n-    Atomic::add(&_total_selected_for_rebuild, on_region_cl._num_selected_for_rebuild);\n+    AtomicAccess::add(&_total_selected_for_rebuild, on_region_cl._num_selected_for_rebuild);\n@@ -1906,1 +1906,1 @@\n-    HeapWord* res = Atomic::cmpxchg(&_finger, finger, end);\n+    HeapWord* res = AtomicAccess::cmpxchg(&_finger, finger, end);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -341,1 +341,1 @@\n-  uint old_wanted = Atomic::load(&_threads_wanted);\n+  uint old_wanted = AtomicAccess::load(&_threads_wanted);\n@@ -363,1 +363,1 @@\n-  Atomic::store(&_threads_wanted, new_wanted);\n+  AtomicAccess::store(&_threads_wanted, new_wanted);\n@@ -377,1 +377,1 @@\n-      Atomic::store(&_threads_wanted, i);\n+      AtomicAccess::store(&_threads_wanted, i);\n@@ -387,1 +387,1 @@\n-    uint wanted = Atomic::load(&_threads_wanted);\n+    uint wanted = AtomicAccess::load(&_threads_wanted);\n@@ -389,1 +389,1 @@\n-      Atomic::store(&_threads_wanted, --wanted);\n+      AtomicAccess::store(&_threads_wanted, --wanted);\n@@ -401,1 +401,1 @@\n-  return worker_id < Atomic::load(&_threads_wanted);\n+  return worker_id < AtomicAccess::load(&_threads_wanted);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -120,1 +120,1 @@\n-  return Atomic::load(&_num_cards);\n+  return AtomicAccess::load(&_num_cards);\n@@ -127,1 +127,1 @@\n-  Atomic::add(&_num_cards, cbn->size());\n+  AtomicAccess::add(&_num_cards, cbn->size());\n@@ -163,1 +163,1 @@\n-  Atomic::sub(&_num_cards, result->size());\n+  AtomicAccess::sub(&_num_cards, result->size());\n@@ -175,1 +175,1 @@\n-  assert(actual == Atomic::load(&_num_cards),\n+  assert(actual == AtomicAccess::load(&_num_cards),\n@@ -177,1 +177,1 @@\n-         Atomic::load(&_num_cards), actual);\n+         AtomicAccess::load(&_num_cards), actual);\n@@ -188,1 +188,1 @@\n-  assert(Atomic::load(&_head) == nullptr, \"precondition\");\n+  assert(AtomicAccess::load(&_head) == nullptr, \"precondition\");\n@@ -201,1 +201,1 @@\n-  BufferNode* old_head = Atomic::xchg(&_head, node);\n+  BufferNode* old_head = AtomicAccess::xchg(&_head, node);\n@@ -211,1 +211,1 @@\n-  BufferNode* head = Atomic::load(&_head);\n+  BufferNode* head = AtomicAccess::load(&_head);\n@@ -213,1 +213,1 @@\n-  Atomic::store(&_head, (BufferNode*)nullptr);\n+  AtomicAccess::store(&_head, (BufferNode*)nullptr);\n@@ -222,1 +222,1 @@\n-  assert(Atomic::load(&_plist) == nullptr, \"invariant\");\n+  assert(AtomicAccess::load(&_plist) == nullptr, \"invariant\");\n@@ -228,1 +228,1 @@\n-  PausedList* plist = Atomic::load_acquire(&_plist);\n+  PausedList* plist = AtomicAccess::load_acquire(&_plist);\n@@ -232,1 +232,1 @@\n-    PausedList* old_plist = Atomic::cmpxchg(&_plist, (PausedList*)nullptr, plist);\n+    PausedList* old_plist = AtomicAccess::cmpxchg(&_plist, (PausedList*)nullptr, plist);\n@@ -250,1 +250,1 @@\n-    previous = Atomic::load_acquire(&_plist);\n+    previous = AtomicAccess::load_acquire(&_plist);\n@@ -254,1 +254,1 @@\n-        (Atomic::cmpxchg(&_plist, previous, (PausedList*)nullptr) != previous)) {\n+        (AtomicAccess::cmpxchg(&_plist, previous, (PausedList*)nullptr) != previous)) {\n@@ -271,1 +271,1 @@\n-  PausedList* plist = Atomic::load(&_plist);\n+  PausedList* plist = AtomicAccess::load(&_plist);\n@@ -273,1 +273,1 @@\n-    Atomic::store(&_plist, (PausedList*)nullptr);\n+    AtomicAccess::store(&_plist, (PausedList*)nullptr);\n@@ -289,1 +289,1 @@\n-  Atomic::add(&_num_cards, node->size());\n+  AtomicAccess::add(&_num_cards, node->size());\n@@ -328,1 +328,1 @@\n-    Atomic::add(&_num_cards, from._entry_count);\n+    AtomicAccess::add(&_num_cards, from._entry_count);\n@@ -337,2 +337,2 @@\n-  size_t num_cards = Atomic::load(&_num_cards);\n-  Atomic::store(&_num_cards, size_t(0));\n+  size_t num_cards = AtomicAccess::load(&_num_cards);\n+  AtomicAccess::store(&_num_cards, size_t(0));\n@@ -483,1 +483,1 @@\n-  if (Atomic::load(&_num_cards) <= Atomic::load(&_mutator_refinement_threshold)) {\n+  if (AtomicAccess::load(&_num_cards) <= AtomicAccess::load(&_mutator_refinement_threshold)) {\n@@ -517,1 +517,1 @@\n-  if (Atomic::load(&_num_cards) <= stop_at) return false;\n+  if (AtomicAccess::load(&_num_cards) <= stop_at) return false;\n@@ -594,1 +594,1 @@\n-  return Atomic::load(&_mutator_refinement_threshold);\n+  return AtomicAccess::load(&_mutator_refinement_threshold);\n@@ -598,1 +598,1 @@\n-  Atomic::store(&_mutator_refinement_threshold, value);\n+  AtomicAccess::store(&_mutator_refinement_threshold, value);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.cpp","additions":25,"deletions":25,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -46,1 +46,1 @@\n-  Atomic::store(&_num_regions_evac_failed, 0u);\n+  AtomicAccess::store(&_num_regions_evac_failed, 0u);\n@@ -72,1 +72,1 @@\n-                                                     Atomic::load(&_num_regions_evac_failed),\n+                                                     AtomicAccess::load(&_num_regions_evac_failed),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureRegions.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -35,1 +35,1 @@\n-  return Atomic::load(&_num_regions_evac_failed);\n+  return AtomicAccess::load(&_num_regions_evac_failed);\n@@ -60,1 +60,1 @@\n-    size_t offset = Atomic::fetch_then_add(&_num_regions_evac_failed, 1u);\n+    size_t offset = AtomicAccess::fetch_then_add(&_num_regions_evac_failed, 1u);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacFailureRegions.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -33,1 +33,1 @@\n-  Atomic::add(&_direct_allocated, value, memory_order_relaxed);\n+  AtomicAccess::add(&_direct_allocated, value, memory_order_relaxed);\n@@ -37,1 +37,1 @@\n-  Atomic::add(&_num_plab_filled, value, memory_order_relaxed);\n+  AtomicAccess::add(&_num_plab_filled, value, memory_order_relaxed);\n@@ -41,1 +41,1 @@\n-  Atomic::add(&_num_direct_allocated, value, memory_order_relaxed);\n+  AtomicAccess::add(&_num_direct_allocated, value, memory_order_relaxed);\n@@ -45,2 +45,2 @@\n-  Atomic::add(&_region_end_waste, value, memory_order_relaxed);\n-  Atomic::inc(&_regions_filled, memory_order_relaxed);\n+  AtomicAccess::add(&_region_end_waste, value, memory_order_relaxed);\n+  AtomicAccess::inc(&_regions_filled, memory_order_relaxed);\n@@ -50,2 +50,2 @@\n-  Atomic::add(&_failure_used, used, memory_order_relaxed);\n-  Atomic::add(&_failure_waste, waste, memory_order_relaxed);\n+  AtomicAccess::add(&_failure_used, used, memory_order_relaxed);\n+  AtomicAccess::add(&_failure_waste, waste, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1EvacStats.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -77,1 +77,1 @@\n-  uintx old_head = Atomic::load(&_head);\n+  uintx old_head = AtomicAccess::load(&_head);\n@@ -83,1 +83,1 @@\n-    new_head = Atomic::cmpxchg(&_head, old_head, new_head);\n+    new_head = AtomicAccess::cmpxchg(&_head, old_head, new_head);\n@@ -95,1 +95,1 @@\n-  uintx old_head = Atomic::load(&_head);\n+  uintx old_head = AtomicAccess::load(&_head);\n@@ -99,1 +99,1 @@\n-    new_head = Atomic::cmpxchg(&_head, old_head, new_head);\n+    new_head = AtomicAccess::cmpxchg(&_head, old_head, new_head);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FreeIdSet.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -66,1 +66,1 @@\n-  Atomic::store(&_compaction_tops[r->hrm_index()], value);\n+  AtomicAccess::store(&_compaction_tops[r->hrm_index()], value);\n@@ -70,1 +70,1 @@\n-  return Atomic::load(&_compaction_tops[r->hrm_index()]);\n+  return AtomicAccess::load(&_compaction_tops[r->hrm_index()]);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -291,1 +291,1 @@\n-  Atomic::add(&_garbage_bytes, garbage_bytes, memory_order_relaxed);\n+  AtomicAccess::add(&_garbage_bytes, garbage_bytes, memory_order_relaxed);\n@@ -444,1 +444,1 @@\n-  st->print(\"|%3zu\", Atomic::load(&_pinned_object_count));\n+  st->print(\"|%3zu\", AtomicAccess::load(&_pinned_object_count));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -397,1 +397,1 @@\n-  size_t pinned_count() const { return Atomic::load(&_pinned_object_count); }\n+  size_t pinned_count() const { return AtomicAccess::load(&_pinned_object_count); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -197,1 +197,1 @@\n-      HeapWord* result = Atomic::cmpxchg(&_top, obj, new_top);\n+      HeapWord* result = AtomicAccess::cmpxchg(&_top, obj, new_top);\n@@ -261,1 +261,1 @@\n-  return Atomic::load_acquire(&_parsable_bottom);\n+  return AtomicAccess::load_acquire(&_parsable_bottom);\n@@ -265,1 +265,1 @@\n-  Atomic::release_store(&_parsable_bottom, bottom());\n+  AtomicAccess::release_store(&_parsable_bottom, bottom());\n@@ -514,1 +514,1 @@\n-  Atomic::add(&_pinned_object_count, value, memory_order_relaxed);\n+  AtomicAccess::add(&_pinned_object_count, value, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -729,1 +729,1 @@\n-  uint old_val = Atomic::cmpxchg(&_claims[region_index], Unclaimed, Claimed);\n+  uint old_val = AtomicAccess::cmpxchg(&_claims[region_index], Unclaimed, Claimed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegionManager.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegionRemSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegionRemSet.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegionRemSet.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -64,2 +64,2 @@\n-  Atomic::add(&_num_segments, num, memory_order_relaxed);\n-  Atomic::add(&_mem_size, mem_size, memory_order_relaxed);\n+  AtomicAccess::add(&_num_segments, num, memory_order_relaxed);\n+  AtomicAccess::add(&_mem_size, mem_size, memory_order_relaxed);\n@@ -70,1 +70,1 @@\n-                prefix, Atomic::load(&_num_segments), Atomic::load(&_mem_size));\n+                prefix, AtomicAccess::load(&_num_segments), AtomicAccess::load(&_mem_size));\n@@ -78,2 +78,2 @@\n-  num_segments = Atomic::load(&_num_segments);\n-  mem_size = Atomic::load(&_mem_size);\n+  num_segments = AtomicAccess::load(&_num_segments);\n+  mem_size = AtomicAccess::load(&_mem_size);\n@@ -82,2 +82,2 @@\n-    Atomic::sub(&_num_segments, num_segments, memory_order_relaxed);\n-    Atomic::sub(&_mem_size, mem_size, memory_order_relaxed);\n+    AtomicAccess::sub(&_num_segments, num_segments, memory_order_relaxed);\n+    AtomicAccess::sub(&_mem_size, mem_size, memory_order_relaxed);\n@@ -99,2 +99,2 @@\n-  Atomic::sub(&_num_segments, num_freed, memory_order_relaxed);\n-  Atomic::sub(&_mem_size, mem_size_freed, memory_order_relaxed);\n+  AtomicAccess::sub(&_num_segments, num_freed, memory_order_relaxed);\n+  AtomicAccess::sub(&_mem_size, mem_size_freed, memory_order_relaxed);\n@@ -118,1 +118,1 @@\n-  Segment* old = Atomic::cmpxchg(&_first, prev, next);\n+  Segment* old = AtomicAccess::cmpxchg(&_first, prev, next);\n@@ -129,3 +129,3 @@\n-    Atomic::inc(&_num_segments, memory_order_relaxed);\n-    Atomic::add(&_mem_size, next->mem_size(), memory_order_relaxed);\n-    Atomic::add(&_num_total_slots, next->num_slots(), memory_order_relaxed);\n+    AtomicAccess::inc(&_num_segments, memory_order_relaxed);\n+    AtomicAccess::add(&_mem_size, next->mem_size(), memory_order_relaxed);\n+    AtomicAccess::add(&_num_total_slots, next->num_slots(), memory_order_relaxed);\n@@ -158,1 +158,1 @@\n-  Segment* cur = Atomic::load_acquire(&_first);\n+  Segment* cur = AtomicAccess::load_acquire(&_first);\n@@ -196,1 +196,1 @@\n-  Segment* cur = Atomic::load_acquire(&_first);\n+  Segment* cur = AtomicAccess::load_acquire(&_first);\n@@ -204,1 +204,1 @@\n-      Atomic::inc(&_num_allocated_slots, memory_order_relaxed);\n+      AtomicAccess::inc(&_num_allocated_slots, memory_order_relaxed);\n@@ -216,1 +216,1 @@\n-  return Atomic::load(&_num_segments);\n+  return AtomicAccess::load(&_num_segments);\n@@ -241,1 +241,1 @@\n-  Segment* cur = Atomic::load_acquire(&_first);\n+  Segment* cur = AtomicAccess::load_acquire(&_first);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1MonotonicArena.cpp","additions":19,"deletions":19,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -84,1 +84,1 @@\n-  const Segment* first_segment() const { return Atomic::load(&_first); }\n+  const Segment* first_segment() const { return AtomicAccess::load(&_first); }\n@@ -86,1 +86,1 @@\n-  uint num_total_slots() const { return Atomic::load(&_num_total_slots); }\n+  uint num_total_slots() const { return AtomicAccess::load(&_num_total_slots); }\n@@ -88,1 +88,1 @@\n-    uint allocated = Atomic::load(&_num_allocated_slots);\n+    uint allocated = AtomicAccess::load(&_num_allocated_slots);\n@@ -217,2 +217,2 @@\n-  size_t num_segments() const { return Atomic::load(&_num_segments); }\n-  size_t mem_size() const { return Atomic::load(&_mem_size); }\n+  size_t num_segments() const { return AtomicAccess::load(&_num_segments); }\n+  size_t mem_size() const { return AtomicAccess::load(&_mem_size); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1MonotonicArena.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -38,1 +38,1 @@\n-  uint result = Atomic::fetch_then_add(&_next_allocate, 1u, memory_order_relaxed);\n+  uint result = AtomicAccess::fetch_then_add(&_next_allocate, 1u, memory_order_relaxed);\n@@ -51,2 +51,2 @@\n-    Atomic::dec(&_num_segments, memory_order_relaxed);\n-    Atomic::sub(&_mem_size, result->mem_size(), memory_order_relaxed);\n+    AtomicAccess::dec(&_num_segments, memory_order_relaxed);\n+    AtomicAccess::sub(&_mem_size, result->mem_size(), memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1MonotonicArena.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1PageBasedVirtualSpace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -38,1 +38,1 @@\n-  if (Atomic::load(&_cleaning_claimed)) {\n+  if (AtomicAccess::load(&_cleaning_claimed)) {\n@@ -42,1 +42,1 @@\n-  return !Atomic::cmpxchg(&_cleaning_claimed, false, true);\n+  return !AtomicAccess::cmpxchg(&_cleaning_claimed, false, true);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParallelCleaning.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -207,2 +207,2 @@\n-  Atomic::store(&_young_list_desired_length, new_young_list_desired_length);\n-  Atomic::store(&_young_list_target_length, new_young_list_target_length);\n+  AtomicAccess::store(&_young_list_desired_length, new_young_list_desired_length);\n+  AtomicAccess::store(&_young_list_target_length, new_young_list_target_length);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -355,2 +355,2 @@\n-  uint young_list_desired_length() const { return Atomic::load(&_young_list_desired_length); }\n-  uint young_list_target_length() const { return Atomic::load(&_young_list_target_length); }\n+  uint young_list_desired_length() const { return AtomicAccess::load(&_young_list_desired_length); }\n+  uint young_list_target_length() const { return AtomicAccess::load(&_young_list_target_length); }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -135,1 +135,1 @@\n-  Atomic::add(&_entry_count, node->size());\n+  AtomicAccess::add(&_entry_count, node->size());\n@@ -144,1 +144,1 @@\n-    Atomic::add(&_entry_count, buffers._entry_count);\n+    AtomicAccess::add(&_entry_count, buffers._entry_count);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RedirtyCardsQueue.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -50,1 +50,1 @@\n-    Atomic::add(&_target[cur->_region_idx]._live_words, cur->_stats._live_words);\n+    AtomicAccess::add(&_target[cur->_region_idx]._live_words, cur->_stats._live_words);\n@@ -54,1 +54,1 @@\n-    Atomic::add(&_target[cur->_region_idx]._incoming_refs, cur->_stats._incoming_refs);\n+    AtomicAccess::add(&_target[cur->_region_idx]._incoming_refs, cur->_stats._incoming_refs);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RegionMarkStatsCache.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -174,1 +174,1 @@\n-      bool marked_as_dirty = Atomic::cmpxchg(&_contains[region], false, true) == false;\n+      bool marked_as_dirty = AtomicAccess::cmpxchg(&_contains[region], false, true) == false;\n@@ -176,1 +176,1 @@\n-        uint allocated = Atomic::fetch_then_add(&_cur_idx, 1u);\n+        uint allocated = AtomicAccess::fetch_then_add(&_cur_idx, 1u);\n@@ -241,1 +241,1 @@\n-        uint next = Atomic::fetch_then_add(&_cur_dirty_regions, num_regions_per_worker);\n+        uint next = AtomicAccess::fetch_then_add(&_cur_dirty_regions, num_regions_per_worker);\n@@ -400,1 +400,1 @@\n-    return Atomic::fetch_then_add(&_card_table_scan_state[region], increment, memory_order_relaxed);\n+    return AtomicAccess::fetch_then_add(&_card_table_scan_state[region], increment, memory_order_relaxed);\n@@ -1337,1 +1337,1 @@\n-            !Atomic::cmpxchg(&_fast_reclaim_handled, false, true)) {\n+            !AtomicAccess::cmpxchg(&_fast_reclaim_handled, false, true)) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -441,1 +441,1 @@\n-    Atomic::add(&_humongous_candidates, candidates);\n+    AtomicAccess::add(&_humongous_candidates, candidates);\n@@ -445,1 +445,1 @@\n-    Atomic::add(&_humongous_total, total);\n+    AtomicAccess::add(&_humongous_total, total);\n@@ -682,1 +682,1 @@\n-      if (!Atomic::cmpxchg(&_pinned_regions_recorded, false, true)) {\n+      if (!AtomicAccess::cmpxchg(&_pinned_regions_recorded, false, true)) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -599,2 +599,2 @@\n-      BufferNode* next = Atomic::load(&_rdc_buffers[index]._head);\n-      BufferNode* tail = Atomic::load(&_rdc_buffers[index]._tail);\n+      BufferNode* next = AtomicAccess::load(&_rdc_buffers[index]._head);\n+      BufferNode* tail = AtomicAccess::load(&_rdc_buffers[index]._tail);\n@@ -604,1 +604,1 @@\n-        next = Atomic::cmpxchg(&_rdc_buffers[index]._head, node, (node != tail ) ? node->next() : nullptr);\n+        next = AtomicAccess::cmpxchg(&_rdc_buffers[index]._head, node, (node != tail ) ? node->next() : nullptr);\n@@ -872,1 +872,1 @@\n-    bool has_new_retained_regions = Atomic::load(&_num_retained_regions) != 0;\n+    bool has_new_retained_regions = AtomicAccess::load(&_num_retained_regions) != 0;\n@@ -907,1 +907,1 @@\n-    Atomic::add(&_num_retained_regions, cl.num_retained_regions(), memory_order_relaxed);\n+    AtomicAccess::add(&_num_retained_regions, cl.num_retained_regions(), memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -574,1 +574,1 @@\n-      if (Atomic::cmpxchg(top_addr(), cur_top, cur_chunk_top) == cur_top) {\n+      if (AtomicAccess::cmpxchg(top_addr(), cur_top, cur_chunk_top) == cur_top) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableNUMASpace.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -133,1 +133,1 @@\n-  Atomic::release_store(end_addr(), mr.end());\n+  AtomicAccess::release_store(end_addr(), mr.end());\n@@ -165,1 +165,1 @@\n-    HeapWord* obj = Atomic::load_acquire(top_addr());\n+    HeapWord* obj = AtomicAccess::load_acquire(top_addr());\n@@ -168,1 +168,1 @@\n-      HeapWord* result = Atomic::cmpxchg(top_addr(), obj, new_top);\n+      HeapWord* result = AtomicAccess::cmpxchg(top_addr(), obj, new_top);\n@@ -187,1 +187,1 @@\n-  return Atomic::cmpxchg(top_addr(), expected_top, obj) == expected_top;\n+  return AtomicAccess::cmpxchg(top_addr(), expected_top, obj) == expected_top;\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -522,1 +522,1 @@\n-    block_index = Atomic::fetch_then_add(&_claimed_index, 1u);\n+    block_index = AtomicAccess::fetch_then_add(&_claimed_index, 1u);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-  const uint local_GCTimeRatio = Atomic::load(&GCTimeRatio);\n+  const uint local_GCTimeRatio = AtomicAccess::load(&GCTimeRatio);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psAdaptiveSizePolicy.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -386,1 +386,1 @@\n-  Atomic::dec(&_preprocessing_active_workers);\n+  AtomicAccess::dec(&_preprocessing_active_workers);\n@@ -388,1 +388,1 @@\n-  while (Atomic::load_acquire(&_preprocessing_active_workers) > 0) {\n+  while (AtomicAccess::load_acquire(&_preprocessing_active_workers) > 0) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCardTable.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1311,1 +1311,1 @@\n-    uint counter = Atomic::fetch_then_add(claim_counter, num_regions_per_stripe);\n+    uint counter = AtomicAccess::fetch_then_add(claim_counter, num_regions_per_stripe);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -446,1 +446,1 @@\n-  Atomic::add(&_dc_and_los, dc_mask);\n+  AtomicAccess::add(&_dc_and_los, dc_mask);\n@@ -469,1 +469,1 @@\n-  Atomic::add(&_dc_and_los, static_cast<region_sz_t>(words));\n+  AtomicAccess::add(&_dc_and_los, static_cast<region_sz_t>(words));\n@@ -475,1 +475,1 @@\n-  const region_sz_t old = Atomic::cmpxchg(&_dc_and_los, los, dc_claimed | los);\n+  const region_sz_t old = AtomicAccess::cmpxchg(&_dc_and_los, los, dc_claimed | los);\n@@ -480,1 +480,1 @@\n-  return Atomic::cmpxchg(&_shadow_state, UnusedRegion, NormalRegion) == UnusedRegion;\n+  return AtomicAccess::cmpxchg(&_shadow_state, UnusedRegion, NormalRegion) == UnusedRegion;\n@@ -485,1 +485,1 @@\n-  return Atomic::cmpxchg(&_shadow_state, UnusedRegion, ShadowRegion) == UnusedRegion;\n+  return AtomicAccess::cmpxchg(&_shadow_state, UnusedRegion, ShadowRegion) == UnusedRegion;\n@@ -489,1 +489,1 @@\n-  int old = Atomic::cmpxchg(&_shadow_state, ShadowRegion, FilledShadow);\n+  int old = AtomicAccess::cmpxchg(&_shadow_state, ShadowRegion, FilledShadow);\n@@ -494,1 +494,1 @@\n-  return Atomic::cmpxchg(&_shadow_state, FilledShadow, CopiedShadow) == FilledShadow;\n+  return AtomicAccess::cmpxchg(&_shadow_state, FilledShadow, CopiedShadow) == FilledShadow;\n@@ -498,1 +498,1 @@\n-  int old = Atomic::cmpxchg(&_shadow_state, ShadowRegion, NormalRegion);\n+  int old = AtomicAccess::cmpxchg(&_shadow_state, ShadowRegion, NormalRegion);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/spaceCounters.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -211,1 +211,1 @@\n-      if (Atomic::add(&counter, 1u) % 10 == 0) {\n+      if (AtomicAccess::add(&counter, 1u) % 10 == 0) {\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -51,1 +51,1 @@\n-  Atomic::release_store(&_has_terminated, true);\n+  AtomicAccess::release_store(&_has_terminated, true);\n@@ -60,1 +60,1 @@\n-  Atomic::release_store_fence(&_should_terminate, true);\n+  AtomicAccess::release_store_fence(&_should_terminate, true);\n@@ -72,1 +72,1 @@\n-  return Atomic::load_acquire(&_should_terminate);\n+  return AtomicAccess::load_acquire(&_should_terminate);\n@@ -76,1 +76,1 @@\n-  return Atomic::load_acquire(&_has_terminated);\n+  return AtomicAccess::load_acquire(&_has_terminated);\n","filename":"src\/hotspot\/share\/gc\/shared\/concurrentGCThread.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  FreeNode* old_head = Atomic::xchg(&_head, node);\n+  FreeNode* old_head = AtomicAccess::xchg(&_head, node);\n@@ -51,1 +51,1 @@\n-  return Atomic::add(&_count, size_t(1));\n+  return AtomicAccess::add(&_count, size_t(1));\n@@ -55,2 +55,2 @@\n-  NodeList result{Atomic::load(&_head), _tail, Atomic::load(&_count)};\n-  Atomic::store(&_head, (FreeNode*)nullptr);\n+  NodeList result{AtomicAccess::load(&_head), _tail, AtomicAccess::load(&_count)};\n+  AtomicAccess::store(&_head, (FreeNode*)nullptr);\n@@ -58,1 +58,1 @@\n-  Atomic::store(&_count, size_t(0));\n+  AtomicAccess::store(&_count, size_t(0));\n@@ -63,1 +63,1 @@\n-  return  Atomic::load(&_count);\n+  return  AtomicAccess::load(&_count);\n@@ -88,1 +88,1 @@\n-  uint index = Atomic::load(&_active_pending_list);\n+  uint index = AtomicAccess::load(&_active_pending_list);\n@@ -96,1 +96,1 @@\n-  uint index = Atomic::load(&_active_pending_list);\n+  uint index = AtomicAccess::load(&_active_pending_list);\n@@ -103,1 +103,1 @@\n-  return Atomic::load(&_free_count);\n+  return AtomicAccess::load(&_free_count);\n@@ -107,1 +107,1 @@\n-  uint index = Atomic::load(&_active_pending_list);\n+  uint index = AtomicAccess::load(&_active_pending_list);\n@@ -127,1 +127,1 @@\n-    size_t count = Atomic::sub(&_free_count, 1u);\n+    size_t count = AtomicAccess::sub(&_free_count, 1u);\n@@ -152,1 +152,1 @@\n-    uint index = Atomic::load_acquire(&_active_pending_list);\n+    uint index = AtomicAccess::load_acquire(&_active_pending_list);\n@@ -167,2 +167,2 @@\n-  if (Atomic::load(&_transfer_lock) || \/\/ Skip CAS if likely to fail.\n-      Atomic::cmpxchg(&_transfer_lock, false, true)) {\n+  if (AtomicAccess::load(&_transfer_lock) || \/\/ Skip CAS if likely to fail.\n+      AtomicAccess::cmpxchg(&_transfer_lock, false, true)) {\n@@ -175,1 +175,1 @@\n-  uint index = Atomic::load(&_active_pending_list);\n+  uint index = AtomicAccess::load(&_active_pending_list);\n@@ -177,1 +177,1 @@\n-  Atomic::release_store(&_active_pending_list, new_active);\n+  AtomicAccess::release_store(&_active_pending_list, new_active);\n@@ -189,1 +189,1 @@\n-    Atomic::add(&_free_count, count);\n+    AtomicAccess::add(&_free_count, count);\n@@ -194,1 +194,1 @@\n-  Atomic::release_store(&_transfer_lock, false);\n+  AtomicAccess::release_store(&_transfer_lock, false);\n","filename":"src\/hotspot\/share\/gc\/shared\/freeListAllocator.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -69,1 +69,1 @@\n-    FreeNode* next() { return Atomic::load(&_next); }\n+    FreeNode* next() { return AtomicAccess::load(&_next); }\n@@ -73,1 +73,1 @@\n-    void set_next(FreeNode* next) { Atomic::store(&_next, next); }\n+    void set_next(FreeNode* next) { AtomicAccess::store(&_next, next); }\n","filename":"src\/hotspot\/share\/gc\/shared\/freeListAllocator.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -87,1 +87,1 @@\n-  assert(Atomic::load(&_is_gc_request_pending) == false, \"precondition\");\n+  assert(AtomicAccess::load(&_is_gc_request_pending) == false, \"precondition\");\n@@ -91,1 +91,1 @@\n-  Atomic::store(&_is_gc_request_pending, true);\n+  AtomicAccess::store(&_is_gc_request_pending, true);\n@@ -115,1 +115,1 @@\n-  assert(Atomic::load(&_verify_in_cr_count) == 0, \"inv\");\n+  assert(AtomicAccess::load(&_verify_in_cr_count) == 0, \"inv\");\n@@ -120,1 +120,1 @@\n-  assert(Atomic::load(&_is_gc_request_pending) == true, \"precondition\");\n+  assert(AtomicAccess::load(&_is_gc_request_pending) == true, \"precondition\");\n@@ -122,1 +122,1 @@\n-  Atomic::store(&_is_gc_request_pending, false);\n+  AtomicAccess::store(&_is_gc_request_pending, false);\n@@ -142,1 +142,1 @@\n-    if (!Atomic::load(&_is_gc_request_pending)) {\n+    if (!AtomicAccess::load(&_is_gc_request_pending)) {\n","filename":"src\/hotspot\/share\/gc\/shared\/gcLocker.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,1 @@\n-    if (Atomic::load(&_is_gc_request_pending)) {\n+    if (AtomicAccess::load(&_is_gc_request_pending)) {\n@@ -47,1 +47,1 @@\n-    DEBUG_ONLY(Atomic::add(&_verify_in_cr_count, (uint64_t)1);)\n+    DEBUG_ONLY(AtomicAccess::add(&_verify_in_cr_count, (uint64_t)1);)\n@@ -58,1 +58,1 @@\n-    Atomic::add(&_verify_in_cr_count, (uint64_t)-1);\n+    AtomicAccess::add(&_verify_in_cr_count, (uint64_t)-1);\n","filename":"src\/hotspot\/share\/gc\/shared\/gcLocker.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -151,1 +151,1 @@\n-  return Atomic::load_acquire(&_block_count);\n+  return AtomicAccess::load_acquire(&_block_count);\n@@ -155,1 +155,1 @@\n-  int new_value = Atomic::add(&_refcount, 1);\n+  int new_value = AtomicAccess::add(&_refcount, 1);\n@@ -160,1 +160,1 @@\n-  int new_value = Atomic::sub(&_refcount, 1);\n+  int new_value = AtomicAccess::sub(&_refcount, 1);\n@@ -172,1 +172,1 @@\n-    Atomic::release_store(&_block_count, index + 1);\n+    AtomicAccess::release_store(&_block_count, index + 1);\n@@ -275,2 +275,2 @@\n-  return (Atomic::load_acquire(&_release_refcount) == 0) &&\n-         (Atomic::load_acquire(&_deferred_updates_next) == nullptr);\n+  return (AtomicAccess::load_acquire(&_release_refcount) == 0) &&\n+         (AtomicAccess::load_acquire(&_deferred_updates_next) == nullptr);\n@@ -324,1 +324,1 @@\n-  uintx sum = Atomic::add(&_allocated_bitmask, add);\n+  uintx sum = AtomicAccess::add(&_allocated_bitmask, add);\n@@ -455,1 +455,1 @@\n-  Atomic::inc(&_allocation_count); \/\/ release updates outside lock.\n+  AtomicAccess::inc(&_allocation_count); \/\/ release updates outside lock.\n@@ -493,1 +493,1 @@\n-  Atomic::add(&_allocation_count, num_taken);\n+  AtomicAccess::add(&_allocation_count, num_taken);\n@@ -509,1 +509,1 @@\n-    Atomic::sub(&_allocation_count, num_taken - limit);\n+    AtomicAccess::sub(&_allocation_count, num_taken - limit);\n@@ -602,1 +602,1 @@\n-  Atomic::release_store(&_active_array, new_array);\n+  AtomicAccess::release_store(&_active_array, new_array);\n@@ -620,1 +620,1 @@\n-  ActiveArray* result = Atomic::load_acquire(&_active_array);\n+  ActiveArray* result = AtomicAccess::load_acquire(&_active_array);\n@@ -675,1 +675,1 @@\n-  Atomic::inc(&_release_refcount);\n+  AtomicAccess::inc(&_release_refcount);\n@@ -682,1 +682,1 @@\n-    uintx fetched = Atomic::cmpxchg(&_allocated_bitmask, old_allocated, new_value);\n+    uintx fetched = AtomicAccess::cmpxchg(&_allocated_bitmask, old_allocated, new_value);\n@@ -701,1 +701,1 @@\n-    if (Atomic::replace_if_null(&_deferred_updates_next, this)) {\n+    if (AtomicAccess::replace_if_null(&_deferred_updates_next, this)) {\n@@ -706,1 +706,1 @@\n-        Block* fetched = Atomic::cmpxchg(&owner->_deferred_updates, head, this);\n+        Block* fetched = AtomicAccess::cmpxchg(&owner->_deferred_updates, head, this);\n@@ -723,1 +723,1 @@\n-  Atomic::dec(&_release_refcount);\n+  AtomicAccess::dec(&_release_refcount);\n@@ -732,1 +732,1 @@\n-  Block* block = Atomic::load_acquire(&_deferred_updates);\n+  Block* block = AtomicAccess::load_acquire(&_deferred_updates);\n@@ -738,1 +738,1 @@\n-    Block* fetched = Atomic::cmpxchg(&_deferred_updates, block, tail);\n+    Block* fetched = AtomicAccess::cmpxchg(&_deferred_updates, block, tail);\n@@ -783,1 +783,1 @@\n-  Atomic::dec(&_allocation_count);\n+  AtomicAccess::dec(&_allocation_count);\n@@ -809,1 +809,1 @@\n-    Atomic::sub(&_allocation_count, count);\n+    AtomicAccess::sub(&_allocation_count, count);\n@@ -909,1 +909,1 @@\n-  if (Atomic::load_acquire(&needs_cleanup_requested) &&\n+  if (AtomicAccess::load_acquire(&needs_cleanup_requested) &&\n@@ -914,1 +914,1 @@\n-    Atomic::release_store(&needs_cleanup_requested, false);\n+    AtomicAccess::release_store(&needs_cleanup_requested, false);\n@@ -926,2 +926,2 @@\n-  Atomic::release_store(&_needs_cleanup, true);\n-  Atomic::release_store_fence(&needs_cleanup_requested, true);\n+  AtomicAccess::release_store(&_needs_cleanup, true);\n+  AtomicAccess::release_store_fence(&needs_cleanup_requested, true);\n@@ -933,2 +933,2 @@\n-  if (!Atomic::load_acquire(&_needs_cleanup) &&\n-      (Atomic::load_acquire(&_deferred_updates) == nullptr)) {\n+  if (!AtomicAccess::load_acquire(&_needs_cleanup) &&\n+      (AtomicAccess::load_acquire(&_deferred_updates) == nullptr)) {\n@@ -941,1 +941,1 @@\n-  Atomic::release_store_fence(&_needs_cleanup, false);\n+  AtomicAccess::release_store_fence(&_needs_cleanup, false);\n@@ -1087,1 +1087,1 @@\n-  size_t start = Atomic::load_acquire(&_next_block);\n+  size_t start = AtomicAccess::load_acquire(&_next_block);\n@@ -1100,1 +1100,1 @@\n-  \/\/ Atomic::add with possible overshoot.  This can perform better\n+  \/\/ AtomicAccess::add with possible overshoot.  This can perform better\n@@ -1104,1 +1104,1 @@\n-  size_t end = Atomic::add(&_next_block, step);\n+  size_t end = AtomicAccess::add(&_next_block, step);\n@@ -1131,1 +1131,1 @@\n-  return Atomic::load(&_num_dead);\n+  return AtomicAccess::load(&_num_dead);\n@@ -1135,1 +1135,1 @@\n-  Atomic::add(&_num_dead, num_dead);\n+  AtomicAccess::add(&_num_dead, num_dead);\n@@ -1139,1 +1139,1 @@\n-  _storage->report_num_dead(Atomic::load(&_num_dead));\n+  _storage->report_num_dead(AtomicAccess::load(&_num_dead));\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.cpp","additions":35,"deletions":35,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorageSetParState.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -71,1 +71,1 @@\n-  } while (Atomic::cmpxchg(&_claimed_nmethod, first, last.method()) != first);\n+  } while (AtomicAccess::cmpxchg(&_claimed_nmethod, first, last.method()) != first);\n@@ -107,1 +107,1 @@\n-  return !Atomic::cmpxchg(&_clean_klass_tree_claimed, false, true);\n+  return !AtomicAccess::cmpxchg(&_clean_klass_tree_claimed, false, true);\n","filename":"src\/hotspot\/share\/gc\/shared\/parallelCleaning.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -51,1 +51,1 @@\n-  size_t new_count = Atomic::add(&_refcount, count, memory_order_relaxed);\n+  size_t new_count = AtomicAccess::add(&_refcount, count, memory_order_relaxed);\n@@ -96,1 +96,1 @@\n-  size_t refcount = Atomic::sub(&state->_refcount, size_t(1), memory_order_release);\n+  size_t refcount = AtomicAccess::sub(&state->_refcount, size_t(1), memory_order_release);\n@@ -120,1 +120,1 @@\n-  uint idx = Atomic::fetch_then_add(&_registered_allocators, 1u, memory_order_relaxed);\n+  uint idx = AtomicAccess::fetch_then_add(&_registered_allocators, 1u, memory_order_relaxed);\n@@ -127,2 +127,2 @@\n-  uint old = Atomic::fetch_then_add(&_released_allocators, 1u, memory_order_relaxed);\n-  assert(old < Atomic::load(&_registered_allocators), \"too many releases\");\n+  uint old = AtomicAccess::fetch_then_add(&_released_allocators, 1u, memory_order_relaxed);\n+  assert(old < AtomicAccess::load(&_registered_allocators), \"too many releases\");\n@@ -133,2 +133,2 @@\n-  uint count = Atomic::load(&_registered_allocators);\n-  assert(count == Atomic::load(&_released_allocators),\n+  uint count = AtomicAccess::load(&_registered_allocators);\n+  assert(count == AtomicAccess::load(&_released_allocators),\n@@ -139,2 +139,2 @@\n-  Atomic::store(&_registered_allocators, 0u);\n-  DEBUG_ONLY(Atomic::store(&_released_allocators, 0u);)\n+  AtomicAccess::store(&_registered_allocators, 0u);\n+  DEBUG_ONLY(AtomicAccess::store(&_released_allocators, 0u);)\n","filename":"src\/hotspot\/share\/gc\/shared\/partialArrayState.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -55,3 +55,3 @@\n-  size_t start = Atomic::fetch_then_add(index_addr,\n-                                        _chunk_size,\n-                                        memory_order_relaxed);\n+  size_t start = AtomicAccess::fetch_then_add(index_addr,\n+                                              _chunk_size,\n+                                              memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shared\/partialArrayTaskStepper.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -35,1 +35,1 @@\n-  Atomic::add(&_allocated, v);\n+  AtomicAccess::add(&_allocated, v);\n@@ -39,1 +39,1 @@\n-  Atomic::add(&_unused, v);\n+  AtomicAccess::add(&_unused, v);\n@@ -43,1 +43,1 @@\n-  Atomic::add(&_wasted, v);\n+  AtomicAccess::add(&_wasted, v);\n@@ -47,1 +47,1 @@\n-  Atomic::add(&_undo_wasted, v);\n+  AtomicAccess::add(&_undo_wasted, v);\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -63,1 +63,1 @@\n-    Atomic::add(total_size_addr, stack_size);\n+    AtomicAccess::add(total_size_addr, stack_size);\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -55,1 +55,1 @@\n-    char* cur_start = Atomic::load(&_cur_addr);\n+    char* cur_start = AtomicAccess::load(&_cur_addr);\n@@ -59,1 +59,1 @@\n-    } else if (cur_start == Atomic::cmpxchg(&_cur_addr, cur_start, cur_end)) {\n+    } else if (cur_start == AtomicAccess::cmpxchg(&_cur_addr, cur_start, cur_end)) {\n","filename":"src\/hotspot\/share\/gc\/shared\/pretouchTask.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -217,1 +217,1 @@\n-  Atomic::add(&_ref_dropped[ref_type_2_index(ref_type)], count, memory_order_relaxed);\n+  AtomicAccess::add(&_ref_dropped[ref_type_2_index(ref_type)], count, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shared\/referenceProcessorPhaseTimes.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -90,1 +90,1 @@\n-  size_t value = Atomic::load(cfptr);\n+  size_t value = AtomicAccess::load(cfptr);\n@@ -96,1 +96,1 @@\n-    value = Atomic::cmpxchg(cfptr, old, value);\n+    value = AtomicAccess::cmpxchg(cfptr, old, value);\n@@ -103,1 +103,1 @@\n-  size_t value = Atomic::load(cfptr);\n+  size_t value = AtomicAccess::load(cfptr);\n@@ -109,1 +109,1 @@\n-    value = Atomic::cmpxchg(cfptr, old, value);\n+    value = AtomicAccess::cmpxchg(cfptr, old, value);\n@@ -335,1 +335,1 @@\n-  Atomic::store(&_count_and_process_flag, size_t(0));\n+  AtomicAccess::store(&_count_and_process_flag, size_t(0));\n","filename":"src\/hotspot\/share\/gc\/shared\/satbMarkQueue.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -129,1 +129,1 @@\n-      HeapWord* result = Atomic::cmpxchg(top_addr(), obj, new_top);\n+      HeapWord* result = AtomicAccess::cmpxchg(top_addr(), obj, new_top);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -78,1 +78,1 @@\n-    OopStorage* storage = Atomic::load(&_storage_for_requests)->storage();\n+    OopStorage* storage = AtomicAccess::load(&_storage_for_requests)->storage();\n@@ -86,1 +86,1 @@\n-  _storage_for_processing = Atomic::xchg(&_storage_for_requests, _storage_for_processing);\n+  _storage_for_processing = AtomicAccess::xchg(&_storage_for_requests, _storage_for_processing);\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupProcessor.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -37,1 +37,1 @@\n-  return Atomic::load_acquire(&_use_count) > 0;\n+  return AtomicAccess::load_acquire(&_use_count) > 0;\n@@ -43,2 +43,2 @@\n-  StorageUse* storage = Atomic::load(ptr);\n-  Atomic::inc(&storage->_use_count);\n+  StorageUse* storage = AtomicAccess::load(ptr);\n+  AtomicAccess::inc(&storage->_use_count);\n@@ -49,1 +49,1 @@\n-  size_t result = Atomic::sub(&_use_count, size_t(1));\n+  size_t result = AtomicAccess::sub(&_use_count, size_t(1));\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupStorageUse.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -246,1 +246,1 @@\n-  switch (Atomic::load(&_dead_state)) {\n+  switch (AtomicAccess::load(&_dead_state)) {\n@@ -248,1 +248,1 @@\n-    Atomic::store(&_dead_count, num_dead);\n+    AtomicAccess::store(&_dead_count, num_dead);\n@@ -254,2 +254,2 @@\n-    Atomic::store(&_dead_count, num_dead);\n-    Atomic::release_store(&_dead_state, DeadState::good);\n+    AtomicAccess::store(&_dead_count, num_dead);\n+    AtomicAccess::release_store(&_dead_state, DeadState::good);\n@@ -259,1 +259,1 @@\n-    Atomic::release_store(&_dead_state, DeadState::wait1);\n+    AtomicAccess::release_store(&_dead_state, DeadState::wait1);\n@@ -478,1 +478,1 @@\n-  return Atomic::load_acquire(&_dead_state) == DeadState::good;\n+  return AtomicAccess::load_acquire(&_dead_state) == DeadState::good;\n@@ -484,1 +484,1 @@\n-         ((_number_of_entries - Atomic::load(&_dead_count)) > _grow_threshold);\n+         ((_number_of_entries - AtomicAccess::load(&_dead_count)) > _grow_threshold);\n@@ -490,1 +490,1 @@\n-         Config::should_cleanup_table(_number_of_entries, Atomic::load(&_dead_count));\n+         Config::should_cleanup_table(_number_of_entries, AtomicAccess::load(&_dead_count));\n@@ -649,1 +649,1 @@\n-  size_t dead_count = Atomic::load(&_dead_count);\n+  size_t dead_count = AtomicAccess::load(&_dead_count);\n@@ -673,2 +673,2 @@\n-  Atomic::store(&_dead_count, size_t(0));\n-  Atomic::store(&_dead_state, DeadState::cleaning);\n+  AtomicAccess::store(&_dead_count, size_t(0));\n+  AtomicAccess::store(&_dead_state, DeadState::cleaning);\n@@ -708,1 +708,1 @@\n-  Atomic::store(&_dead_state, DeadState::wait2);\n+  AtomicAccess::store(&_dead_state, DeadState::wait2);\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupTable.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-    Atomic::store(&_suspend_all, true);\n+    AtomicAccess::store(&_suspend_all, true);\n@@ -130,1 +130,1 @@\n-  Atomic::store(&_suspend_all, false);\n+  AtomicAccess::store(&_suspend_all, false);\n","filename":"src\/hotspot\/share\/gc\/shared\/suspendibleThreadSet.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -62,1 +62,1 @@\n-  static bool should_yield() { return Atomic::load(&_suspend_all); }\n+  static bool should_yield() { return AtomicAccess::load(&_suspend_all); }\n","filename":"src\/hotspot\/share\/gc\/shared\/suspendibleThreadSet.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/taskqueue.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -143,1 +143,1 @@\n-    return Atomic::load(&_bottom);\n+    return AtomicAccess::load(&_bottom);\n@@ -147,1 +147,1 @@\n-    return Atomic::load_acquire(&_bottom);\n+    return AtomicAccess::load_acquire(&_bottom);\n@@ -151,1 +151,1 @@\n-    Atomic::store(&_bottom, new_bottom);\n+    AtomicAccess::store(&_bottom, new_bottom);\n@@ -155,1 +155,1 @@\n-    Atomic::release_store(&_bottom, new_bottom);\n+    AtomicAccess::release_store(&_bottom, new_bottom);\n@@ -159,1 +159,1 @@\n-    return Age(Atomic::load(&_age._data));\n+    return Age(AtomicAccess::load(&_age._data));\n@@ -163,1 +163,1 @@\n-    Atomic::store(&_age._data, new_age._data);\n+    AtomicAccess::store(&_age._data, new_age._data);\n@@ -167,1 +167,1 @@\n-    return Age(Atomic::cmpxchg(&_age._data, old_age._data, new_age._data));\n+    return Age(AtomicAccess::cmpxchg(&_age._data, old_age._data, new_age._data));\n@@ -172,1 +172,1 @@\n-    return Atomic::load(&_age._fields._top);\n+    return AtomicAccess::load(&_age._fields._top);\n","filename":"src\/hotspot\/share\/gc\/shared\/taskqueue.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/taskqueue.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -463,1 +463,1 @@\n-  return Atomic::load(&_start);\n+  return AtomicAccess::load(&_start);\n@@ -467,1 +467,1 @@\n-  return Atomic::load(&_top);\n+  return AtomicAccess::load(&_top);\n","filename":"src\/hotspot\/share\/gc\/shared\/threadLocalAllocBuffer.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -64,1 +64,1 @@\n-  const uint worker_id = Atomic::fetch_then_add(&_started, 1u);\n+  const uint worker_id = AtomicAccess::fetch_then_add(&_started, 1u);\n@@ -73,1 +73,1 @@\n-  const uint not_finished = Atomic::sub(&_not_finished, 1u);\n+  const uint not_finished = AtomicAccess::sub(&_not_finished, 1u);\n","filename":"src\/hotspot\/share\/gc\/shared\/workerThread.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -91,1 +91,1 @@\n-  if (Atomic::cmpxchg(&_verification_done, false, true)) {\n+  if (AtomicAccess::cmpxchg(&_verification_done, false, true)) {\n@@ -119,1 +119,1 @@\n-  return !_tasks[t] && !Atomic::cmpxchg(&_tasks[t], false, true);\n+  return !_tasks[t] && !AtomicAccess::cmpxchg(&_tasks[t], false, true);\n@@ -132,1 +132,1 @@\n-    t = Atomic::add(&_num_claimed, 1u) - 1;\n+    t = AtomicAccess::add(&_num_claimed, 1u) - 1;\n","filename":"src\/hotspot\/share\/gc\/shared\/workerUtils.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCodeRoots.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -152,1 +152,1 @@\n-  size_t old = Atomic::load(&_current_index);\n+  size_t old = AtomicAccess::load(&_current_index);\n@@ -156,1 +156,1 @@\n-      size_t cur = Atomic::cmpxchg(&_current_index, old, index + 1, memory_order_relaxed);\n+      size_t cur = AtomicAccess::cmpxchg(&_current_index, old, index + 1, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-  Atomic::inc(&_gc_id);\n+  AtomicAccess::inc(&_gc_id);\n@@ -38,1 +38,1 @@\n-  return Atomic::load(&_gc_id);\n+  return AtomicAccess::load(&_gc_id);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  Atomic::dec(&_bits);\n+  AtomicAccess::dec(&_bits);\n@@ -45,1 +45,1 @@\n-  Atomic::release_store_fence(&_bits, (jint)0);\n+  AtomicAccess::release_store_fence(&_bits, (jint)0);\n@@ -49,1 +49,1 @@\n-  jint threads_in_evac = Atomic::load_acquire(&_bits);\n+  jint threads_in_evac = AtomicAccess::load_acquire(&_bits);\n@@ -55,1 +55,1 @@\n-    jint other = Atomic::cmpxchg(&_bits, threads_in_evac, newval);\n+    jint other = AtomicAccess::cmpxchg(&_bits, threads_in_evac, newval);\n@@ -68,1 +68,1 @@\n-  jint threads_in_evac = Atomic::load_acquire(&_bits);\n+  jint threads_in_evac = AtomicAccess::load_acquire(&_bits);\n@@ -76,1 +76,1 @@\n-    jint other = Atomic::cmpxchg(&_bits, threads_in_evac, threads_in_evac + 1);\n+    jint other = AtomicAccess::cmpxchg(&_bits, threads_in_evac, threads_in_evac + 1);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacOOMHandler.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -35,1 +35,1 @@\n-  return Atomic::load_acquire(&_bits);\n+  return AtomicAccess::load_acquire(&_bits);\n@@ -39,1 +39,1 @@\n-  return Atomic::load_acquire(&_bits) & ~OOM_MARKER_MASK;\n+  return AtomicAccess::load_acquire(&_bits) & ~OOM_MARKER_MASK;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacOOMHandler.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -152,1 +152,1 @@\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n+  return AtomicAccess::load(&_bytes_allocated_since_gc_start);\n@@ -156,1 +156,1 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  AtomicAccess::store(&_bytes_allocated_since_gc_start, (size_t)0);\n@@ -160,1 +160,1 @@\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+  AtomicAccess::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n@@ -859,1 +859,1 @@\n-  return Atomic::add(&_affiliated_region_count, (size_t) 1);\n+  return AtomicAccess::add(&_affiliated_region_count, (size_t) 1);\n@@ -867,1 +867,1 @@\n-  auto affiliated_region_count = Atomic::sub(&_affiliated_region_count, (size_t) 1);\n+  auto affiliated_region_count = AtomicAccess::sub(&_affiliated_region_count, (size_t) 1);\n@@ -875,1 +875,1 @@\n-  return Atomic::sub(&_affiliated_region_count, (size_t) 1);\n+  return AtomicAccess::sub(&_affiliated_region_count, (size_t) 1);\n@@ -880,1 +880,1 @@\n-  return Atomic::add(&_affiliated_region_count, delta);\n+  return AtomicAccess::add(&_affiliated_region_count, delta);\n@@ -885,1 +885,1 @@\n-  assert(Atomic::load(&_affiliated_region_count) >= delta, \"Affiliated region count cannot be negative\");\n+  assert(AtomicAccess::load(&_affiliated_region_count) >= delta, \"Affiliated region count cannot be negative\");\n@@ -887,1 +887,1 @@\n-  auto const affiliated_region_count = Atomic::sub(&_affiliated_region_count, delta);\n+  auto const affiliated_region_count = AtomicAccess::sub(&_affiliated_region_count, delta);\n@@ -896,2 +896,2 @@\n-  Atomic::store(&_affiliated_region_count, num_regions);\n-  Atomic::store(&_used, num_bytes);\n+  AtomicAccess::store(&_affiliated_region_count, num_regions);\n+  AtomicAccess::store(&_used, num_bytes);\n@@ -902,1 +902,1 @@\n-  Atomic::add(&_used, bytes);\n+  AtomicAccess::add(&_used, bytes);\n@@ -907,1 +907,1 @@\n-    Atomic::add(&_humongous_waste, bytes);\n+    AtomicAccess::add(&_humongous_waste, bytes);\n@@ -915,1 +915,1 @@\n-    Atomic::sub(&_humongous_waste, bytes);\n+    AtomicAccess::sub(&_humongous_waste, bytes);\n@@ -922,1 +922,1 @@\n-  Atomic::sub(&_used, bytes);\n+  AtomicAccess::sub(&_used, bytes);\n@@ -926,1 +926,1 @@\n-  return Atomic::load(&_affiliated_region_count);\n+  return AtomicAccess::load(&_affiliated_region_count);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":16,"deletions":16,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -131,1 +131,1 @@\n-  size_t used() const override { return Atomic::load(&_used); }\n+  size_t used() const override { return AtomicAccess::load(&_used); }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -89,1 +89,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -701,1 +701,1 @@\n-  return Atomic::load(&_committed);\n+  return AtomicAccess::load(&_committed);\n@@ -792,1 +792,1 @@\n-  size_t v = Atomic::load(&_soft_max_size);\n+  size_t v = AtomicAccess::load(&_soft_max_size);\n@@ -803,1 +803,1 @@\n-  Atomic::store(&_soft_max_size, v);\n+  AtomicAccess::store(&_soft_max_size, v);\n@@ -855,1 +855,1 @@\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t new_soft_max = AtomicAccess::load(&SoftMaxHeapSize);\n@@ -1981,2 +1981,2 @@\n-    while (Atomic::load(&_index) < max) {\n-      size_t cur = Atomic::fetch_then_add(&_index, stride, memory_order_relaxed);\n+    while (AtomicAccess::load(&_index) < max) {\n+      size_t cur = AtomicAccess::fetch_then_add(&_index, stride, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -63,1 +63,1 @@\n-  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  size_t new_index = AtomicAccess::add(&_index, (size_t) 1, memory_order_relaxed);\n@@ -77,1 +77,1 @@\n-  Atomic::store(&_gc_no_progress_count, (size_t) 0);\n+  AtomicAccess::store(&_gc_no_progress_count, (size_t) 0);\n@@ -81,1 +81,1 @@\n-  Atomic::inc(&_gc_no_progress_count);\n+  AtomicAccess::inc(&_gc_no_progress_count);\n@@ -85,1 +85,1 @@\n-  return Atomic::load(&_gc_no_progress_count);\n+  return AtomicAccess::load(&_gc_no_progress_count);\n@@ -200,1 +200,1 @@\n-  Atomic::cmpxchg(addr, compare, update, memory_order_release);\n+  AtomicAccess::cmpxchg(addr, compare, update, memory_order_release);\n@@ -206,1 +206,1 @@\n-  Atomic::cmpxchg(addr, compare, u, memory_order_release);\n+  AtomicAccess::cmpxchg(addr, compare, u, memory_order_release);\n@@ -213,1 +213,1 @@\n-  Atomic::cmpxchg(addr, c, u, memory_order_release);\n+  AtomicAccess::cmpxchg(addr, c, u, memory_order_release);\n@@ -218,1 +218,1 @@\n-  return (oop) Atomic::cmpxchg(addr, compare, update, memory_order_release) == compare;\n+  return (oop) AtomicAccess::cmpxchg(addr, compare, update, memory_order_release) == compare;\n@@ -224,1 +224,1 @@\n-  return (narrowOop) Atomic::cmpxchg(addr, compare, u, memory_order_release) == compare;\n+  return (narrowOop) AtomicAccess::cmpxchg(addr, compare, u, memory_order_release) == compare;\n@@ -231,1 +231,1 @@\n-  return CompressedOops::decode(Atomic::cmpxchg(addr, c, u, memory_order_release)) == compare;\n+  return CompressedOops::decode(AtomicAccess::cmpxchg(addr, c, u, memory_order_release)) == compare;\n@@ -240,1 +240,1 @@\n-  Atomic::cmpxchg(addr, compare, oop(), memory_order_relaxed);\n+  AtomicAccess::cmpxchg(addr, compare, oop(), memory_order_relaxed);\n@@ -246,1 +246,1 @@\n-  Atomic::cmpxchg(addr, cmp, narrowOop(), memory_order_relaxed);\n+  AtomicAccess::cmpxchg(addr, cmp, narrowOop(), memory_order_relaxed);\n@@ -251,1 +251,1 @@\n-  Atomic::cmpxchg(addr, compare, narrowOop(), memory_order_relaxed);\n+  AtomicAccess::cmpxchg(addr, compare, narrowOop(), memory_order_relaxed);\n@@ -432,1 +432,1 @@\n-  Atomic::store(_affiliations + r->index(), (uint8_t) new_affiliation);\n+  AtomicAccess::store(_affiliations + r->index(), (uint8_t) new_affiliation);\n@@ -436,1 +436,1 @@\n-  return (ShenandoahAffiliation) Atomic::load(_affiliations + index);\n+  return (ShenandoahAffiliation) AtomicAccess::load(_affiliations + index);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":16,"deletions":16,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -837,1 +837,1 @@\n-  Atomic::store(&_state, to);\n+  AtomicAccess::store(&_state, to);\n@@ -841,1 +841,1 @@\n-  Atomic::add(&_critical_pins, (size_t)1);\n+  AtomicAccess::add(&_critical_pins, (size_t)1);\n@@ -846,1 +846,1 @@\n-  Atomic::sub(&_critical_pins, (size_t)1);\n+  AtomicAccess::sub(&_critical_pins, (size_t)1);\n@@ -850,1 +850,1 @@\n-  return Atomic::load(&_critical_pins);\n+  return AtomicAccess::load(&_critical_pins);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -219,1 +219,1 @@\n-  RegionState state()              const { return Atomic::load(&_state); }\n+  RegionState state()              const { return AtomicAccess::load(&_state); }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -141,1 +141,1 @@\n-  size_t new_live_data = Atomic::add(&_live_data, s, memory_order_relaxed);\n+  size_t new_live_data = AtomicAccess::add(&_live_data, s, memory_order_relaxed);\n@@ -145,1 +145,1 @@\n-  Atomic::store(&_live_data, (size_t)0);\n+  AtomicAccess::store(&_live_data, (size_t)0);\n@@ -149,1 +149,1 @@\n-  return Atomic::load(&_live_data);\n+  return AtomicAccess::load(&_live_data);\n@@ -181,1 +181,1 @@\n-  HeapWord* watermark = Atomic::load_acquire(&_update_watermark);\n+  HeapWord* watermark = AtomicAccess::load_acquire(&_update_watermark);\n@@ -188,1 +188,1 @@\n-  Atomic::release_store(&_update_watermark, w);\n+  AtomicAccess::release_store(&_update_watermark, w);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -110,1 +110,1 @@\n-    if (current - last > ShenandoahRegionSamplingRate && Atomic::cmpxchg(&_last_sample_millis, last, current) == last) {\n+    if (current - last > ShenandoahRegionSamplingRate && AtomicAccess::cmpxchg(&_last_sample_millis, last, current) == last) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -49,2 +49,2 @@\n-  while (Atomic::load(&_state) == locked ||\n-         Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+  while (AtomicAccess::load(&_state) == locked ||\n+         AtomicAccess::cmpxchg(&_state, unlocked, locked) != unlocked) {\n@@ -116,1 +116,1 @@\n-  Thread* const owner = Atomic::load(&_owner);\n+  Thread* const owner = AtomicAccess::load(&_owner);\n@@ -120,1 +120,1 @@\n-    Atomic::store(&_owner, thread);\n+    AtomicAccess::store(&_owner, thread);\n@@ -133,1 +133,1 @@\n-    Atomic::store(&_owner, (Thread*)nullptr);\n+    AtomicAccess::store(&_owner, (Thread*)nullptr);\n@@ -140,1 +140,1 @@\n-  Thread* const owner = Atomic::load(&_owner);\n+  Thread* const owner = AtomicAccess::load(&_owner);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    assert(Atomic::load(&_owner) != Thread::current(), \"reentrant locking attempt, would deadlock\");\n+    assert(AtomicAccess::load(&_owner) != Thread::current(), \"reentrant locking attempt, would deadlock\");\n@@ -54,1 +54,1 @@\n-        (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked)) {\n+        (AtomicAccess::cmpxchg(&_state, unlocked, locked) != unlocked)) {\n@@ -61,3 +61,3 @@\n-    assert(Atomic::load(&_state) == locked, \"must be locked\");\n-    assert(Atomic::load(&_owner) == nullptr, \"must not be owned\");\n-    DEBUG_ONLY(Atomic::store(&_owner, Thread::current());)\n+    assert(AtomicAccess::load(&_state) == locked, \"must be locked\");\n+    assert(AtomicAccess::load(&_owner) == nullptr, \"must not be owned\");\n+    DEBUG_ONLY(AtomicAccess::store(&_owner, Thread::current());)\n@@ -67,2 +67,2 @@\n-    assert(Atomic::load(&_owner) == Thread::current(), \"sanity\");\n-    DEBUG_ONLY(Atomic::store(&_owner, (Thread*)nullptr);)\n+    assert(AtomicAccess::load(&_owner) == Thread::current(), \"sanity\");\n+    DEBUG_ONLY(AtomicAccess::store(&_owner, (Thread*)nullptr);)\n@@ -70,1 +70,1 @@\n-    Atomic::store(&_state, unlocked);\n+    AtomicAccess::store(&_state, unlocked);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -50,1 +50,1 @@\n-  bm_word_t old_val = Atomic::load(addr);\n+  bm_word_t old_val = AtomicAccess::load(addr);\n@@ -58,1 +58,1 @@\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order_relaxed);\n+    const bm_word_t cur_val = AtomicAccess::cmpxchg(addr, old_val, new_val, memory_order_relaxed);\n@@ -75,1 +75,1 @@\n-  bm_word_t old_val = Atomic::load(addr);\n+  bm_word_t old_val = AtomicAccess::load(addr);\n@@ -85,1 +85,1 @@\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order_relaxed);\n+    const bm_word_t cur_val = AtomicAccess::cmpxchg(addr, old_val, new_val, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -437,1 +437,1 @@\n-    size_t cur = Atomic::fetch_then_add(&_claimed, stride, memory_order_relaxed);\n+    size_t cur = AtomicAccess::fetch_then_add(&_claimed, stride, memory_order_relaxed);\n@@ -461,1 +461,1 @@\n-    size_t cur = Atomic::fetch_then_add(&_claimed, stride, memory_order_relaxed);\n+    size_t cur = AtomicAccess::fetch_then_add(&_claimed, stride, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -205,1 +205,1 @@\n-  Atomic::add(&_sum, val);\n+  AtomicAccess::add(&_sum, val);\n@@ -220,1 +220,1 @@\n-  Atomic::add(&_mags[mag], (size_t)1);\n+  AtomicAccess::add(&_mags[mag], (size_t)1);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNumberSeq.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-    Atomic::add(&_trashed_oops, processor.trashed_oops());\n+    AtomicAccess::add(&_trashed_oops, processor.trashed_oops());\n@@ -152,1 +152,1 @@\n-    Atomic::add(&_trashed_oops, processor.trashed_oops());\n+    AtomicAccess::add(&_trashed_oops, processor.trashed_oops());\n@@ -186,1 +186,1 @@\n-        Atomic::store(&_is_preempted, true);\n+        AtomicAccess::store(&_is_preempted, true);\n@@ -194,1 +194,1 @@\n-    return !Atomic::load(&_is_preempted);\n+    return !AtomicAccess::load(&_is_preempted);\n@@ -243,1 +243,1 @@\n-  Atomic::store(&_promoted_expended, (size_t) 0);\n+  AtomicAccess::store(&_promoted_expended, (size_t) 0);\n@@ -249,1 +249,1 @@\n-  return Atomic::add(&_promoted_expended, increment);\n+  return AtomicAccess::add(&_promoted_expended, increment);\n@@ -253,1 +253,1 @@\n-  return Atomic::sub(&_promoted_expended, decrement);\n+  return AtomicAccess::sub(&_promoted_expended, decrement);\n@@ -257,1 +257,1 @@\n-  return Atomic::load(&_promoted_expended);\n+  return AtomicAccess::load(&_promoted_expended);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -124,1 +124,1 @@\n-  T raw_oop = Atomic::load(reference_referent_addr<T>(reference));\n+  T raw_oop = AtomicAccess::load(reference_referent_addr<T>(reference));\n@@ -508,1 +508,1 @@\n-    oop prev = Atomic::xchg(&_pending_list, head);\n+    oop prev = AtomicAccess::xchg(&_pending_list, head);\n@@ -523,1 +523,1 @@\n-  uint worker_id = Atomic::add(&_iterate_discovered_list_id, 1U, memory_order_relaxed) - 1;\n+  uint worker_id = AtomicAccess::add(&_iterate_discovered_list_id, 1U, memory_order_relaxed) - 1;\n@@ -530,1 +530,1 @@\n-    worker_id = Atomic::add(&_iterate_discovered_list_id, 1U, memory_order_relaxed) - 1;\n+    worker_id = AtomicAccess::add(&_iterate_discovered_list_id, 1U, memory_order_relaxed) - 1;\n@@ -563,1 +563,1 @@\n-  Atomic::release_store_fence(&_iterate_discovered_list_id, 0U);\n+  AtomicAccess::release_store_fence(&_iterate_discovered_list_id, 0U);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-  return Atomic::fetch_then_add(&_claimed, _stride, memory_order_relaxed);\n+  return AtomicAccess::fetch_then_add(&_claimed, _stride, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootProcessor.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -372,1 +372,1 @@\n-  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  size_t new_index = AtomicAccess::add(&_index, (size_t) 1, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -52,1 +52,1 @@\n-    Atomic::release_store_fence(&value, (ShenandoahSharedValue)SET);\n+    AtomicAccess::release_store_fence(&value, (ShenandoahSharedValue)SET);\n@@ -56,1 +56,1 @@\n-    Atomic::release_store_fence(&value, (ShenandoahSharedValue)UNSET);\n+    AtomicAccess::release_store_fence(&value, (ShenandoahSharedValue)UNSET);\n@@ -60,1 +60,1 @@\n-    return Atomic::load_acquire(&value) == SET;\n+    return AtomicAccess::load_acquire(&value) == SET;\n@@ -64,1 +64,1 @@\n-    return Atomic::load_acquire(&value) == UNSET;\n+    return AtomicAccess::load_acquire(&value) == UNSET;\n@@ -79,1 +79,1 @@\n-    ShenandoahSharedValue old = Atomic::cmpxchg(&value, (ShenandoahSharedValue)UNSET, (ShenandoahSharedValue)SET);\n+    ShenandoahSharedValue old = AtomicAccess::cmpxchg(&value, (ShenandoahSharedValue)UNSET, (ShenandoahSharedValue)SET);\n@@ -87,1 +87,1 @@\n-    ShenandoahSharedValue old = Atomic::cmpxchg(&value, (ShenandoahSharedValue)SET, (ShenandoahSharedValue)UNSET);\n+    ShenandoahSharedValue old = AtomicAccess::cmpxchg(&value, (ShenandoahSharedValue)SET, (ShenandoahSharedValue)UNSET);\n@@ -123,1 +123,1 @@\n-      ShenandoahSharedValue ov = Atomic::load_acquire(&value);\n+      ShenandoahSharedValue ov = AtomicAccess::load_acquire(&value);\n@@ -131,1 +131,1 @@\n-      if (Atomic::cmpxchg(&value, ov, nv) == ov) {\n+      if (AtomicAccess::cmpxchg(&value, ov, nv) == ov) {\n@@ -142,1 +142,1 @@\n-      ShenandoahSharedValue ov = Atomic::load_acquire(&value);\n+      ShenandoahSharedValue ov = AtomicAccess::load_acquire(&value);\n@@ -149,1 +149,1 @@\n-      if (Atomic::cmpxchg(&value, ov, nv) == ov) {\n+      if (AtomicAccess::cmpxchg(&value, ov, nv) == ov) {\n@@ -157,1 +157,1 @@\n-    Atomic::release_store_fence(&value, (ShenandoahSharedValue)0);\n+    AtomicAccess::release_store_fence(&value, (ShenandoahSharedValue)0);\n@@ -168,1 +168,1 @@\n-    uint uvalue = Atomic::load_acquire(&value);\n+    uint uvalue = AtomicAccess::load_acquire(&value);\n@@ -175,1 +175,1 @@\n-    return (Atomic::load_acquire(&value) & (ShenandoahSharedValue) mask) == 0;\n+    return (AtomicAccess::load_acquire(&value) & (ShenandoahSharedValue) mask) == 0;\n@@ -179,1 +179,1 @@\n-    return (Atomic::load_acquire(&value)) == 0;\n+    return (AtomicAccess::load_acquire(&value)) == 0;\n@@ -227,1 +227,1 @@\n-    Atomic::release_store_fence(&value, (EnumValueType)v);\n+    AtomicAccess::release_store_fence(&value, (EnumValueType)v);\n@@ -231,1 +231,1 @@\n-    return (T)Atomic::load_acquire(&value);\n+    return (T)AtomicAccess::load_acquire(&value);\n@@ -237,1 +237,1 @@\n-    return (T)Atomic::cmpxchg(&value, (EnumValueType)expected, (EnumValueType)new_value);\n+    return (T)AtomicAccess::cmpxchg(&value, (EnumValueType)expected, (EnumValueType)new_value);\n@@ -243,1 +243,1 @@\n-    return (T)Atomic::xchg(&value, (EnumValueType)new_value);\n+    return (T)AtomicAccess::xchg(&value, (EnumValueType)new_value);\n@@ -276,1 +276,1 @@\n-    Atomic::release_store_fence(&value, (ShenandoahSharedValue)tokens);\n+    AtomicAccess::release_store_fence(&value, (ShenandoahSharedValue)tokens);\n@@ -281,1 +281,1 @@\n-      ShenandoahSharedValue ov = Atomic::load_acquire(&value);\n+      ShenandoahSharedValue ov = AtomicAccess::load_acquire(&value);\n@@ -286,1 +286,1 @@\n-      if (Atomic::cmpxchg(&value, ov, nv) == ov) {\n+      if (AtomicAccess::cmpxchg(&value, ov, nv) == ov) {\n@@ -294,1 +294,1 @@\n-    Atomic::release_store_fence(&value, (ShenandoahSharedValue)0);\n+    AtomicAccess::release_store_fence(&value, (ShenandoahSharedValue)0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSharedVariables.hpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -343,1 +343,1 @@\n-  jint index = Atomic::add(&_claimed_index, 1, memory_order_relaxed);\n+  jint index = AtomicAccess::add(&_claimed_index, 1, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahTaskqueue.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -196,1 +196,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahForwarding::size(obj), memory_order_relaxed);\n+          AtomicAccess::add(&_ld[obj_reg->index()], (uint) ShenandoahForwarding::size(obj), memory_order_relaxed);\n@@ -625,1 +625,1 @@\n-    Atomic::add(&_processed, processed, memory_order_relaxed);\n+    AtomicAccess::add(&_processed, processed, memory_order_relaxed);\n@@ -672,1 +672,1 @@\n-    return Atomic::load(&_processed);\n+    return AtomicAccess::load(&_processed);\n@@ -687,1 +687,1 @@\n-      size_t v = Atomic::fetch_then_add(&_claimed, 1u, memory_order_relaxed);\n+      size_t v = AtomicAccess::fetch_then_add(&_claimed, 1u, memory_order_relaxed);\n@@ -715,1 +715,1 @@\n-    Atomic::add(&_processed, processed, memory_order_relaxed);\n+    AtomicAccess::add(&_processed, processed, memory_order_relaxed);\n@@ -748,1 +748,1 @@\n-    Atomic::add(&_processed, processed, memory_order_relaxed);\n+    AtomicAccess::add(&_processed, processed, memory_order_relaxed);\n@@ -1026,1 +1026,1 @@\n-        juint start_live = Atomic::load(&ld[r->humongous_start_region()->index()]);\n+        juint start_live = AtomicAccess::load(&ld[r->humongous_start_region()->index()]);\n@@ -1031,1 +1031,1 @@\n-        verf_live = Atomic::load(&ld[r->index()]);\n+        verf_live = AtomicAccess::load(&ld[r->index()]);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -30,1 +30,1 @@\n-  Atomic::store(&_should_abort, true);\n+  AtomicAccess::store(&_should_abort, true);\n","filename":"src\/hotspot\/share\/gc\/z\/zAbort.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -32,1 +32,1 @@\n-  return Atomic::load(&_should_abort);\n+  return AtomicAccess::load(&_should_abort);\n","filename":"src\/hotspot\/share\/gc\/z\/zAbort.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -478,1 +478,1 @@\n-    (void)Atomic::load((int*)(uintptr_t)addr);\n+    (void)AtomicAccess::load((int*)(uintptr_t)addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zAddress.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zArray.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -133,1 +133,1 @@\n-  const size_t claimed_index = Atomic::fetch_then_add(&_next, 1u, memory_order_relaxed);\n+  const size_t claimed_index = AtomicAccess::fetch_then_add(&_next, 1u, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/z\/zArray.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -96,1 +96,1 @@\n-    const zpointer prev_ptr = Atomic::cmpxchg(p, ptr, heal_ptr, memory_order_relaxed);\n+    const zpointer prev_ptr = AtomicAccess::cmpxchg(p, ptr, heal_ptr, memory_order_relaxed);\n@@ -368,1 +368,1 @@\n-  const zpointer ptr = Atomic::load(p);\n+  const zpointer ptr = AtomicAccess::load(p);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -166,1 +166,1 @@\n-    Atomic::store(dst, ZAddress::store_good(elem));\n+    AtomicAccess::store(dst, ZAddress::store_good(elem));\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -335,1 +335,1 @@\n-  Atomic::store(dst, ZAddress::store_good(obj));\n+  AtomicAccess::store(dst, ZAddress::store_good(obj));\n@@ -347,1 +347,1 @@\n-  Atomic::store(dst, ZAddress::store_good(obj));\n+  AtomicAccess::store(dst, ZAddress::store_good(obj));\n@@ -412,1 +412,1 @@\n-    Atomic::store(p, ZAddress::store_good(addr));\n+    AtomicAccess::store(p, ZAddress::store_good(addr));\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -73,1 +73,1 @@\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val);\n+    const bm_word_t cur_val = AtomicAccess::cmpxchg(addr, old_val, new_val);\n","filename":"src\/hotspot\/share\/gc\/z\/zBitMap.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -49,1 +49,1 @@\n-  const uintptr_t value = Atomic::load(value_addr);\n+  const uintptr_t value = AtomicAccess::load(value_addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zContinuation.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -53,1 +53,1 @@\n-  return Atomic::cmpxchg(&_claimed, false, true) == false;\n+  return AtomicAccess::cmpxchg(&_claimed, false, true) == false;\n@@ -63,1 +63,1 @@\n-  Atomic::store(&_in_place_thread, Thread::current());\n+  AtomicAccess::store(&_in_place_thread, Thread::current());\n@@ -79,1 +79,1 @@\n-  Atomic::store(&_in_place_thread, (Thread*)nullptr);\n+  AtomicAccess::store(&_in_place_thread, (Thread*)nullptr);\n@@ -84,1 +84,1 @@\n-  return Atomic::load(&_in_place_thread) == Thread::current() && offset < _in_place_top_at_start;\n+  return AtomicAccess::load(&_in_place_thread) == Thread::current() && offset < _in_place_top_at_start;\n@@ -89,1 +89,1 @@\n-    const int32_t ref_count = Atomic::load_acquire(&_ref_count);\n+    const int32_t ref_count = AtomicAccess::load_acquire(&_ref_count);\n@@ -104,1 +104,1 @@\n-    if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) == ref_count) {\n+    if (AtomicAccess::cmpxchg(&_ref_count, ref_count, ref_count + 1) == ref_count) {\n@@ -113,1 +113,1 @@\n-    const int32_t ref_count = Atomic::load(&_ref_count);\n+    const int32_t ref_count = AtomicAccess::load(&_ref_count);\n@@ -117,1 +117,1 @@\n-    if (Atomic::cmpxchg(&_ref_count, ref_count, -ref_count) != ref_count) {\n+    if (AtomicAccess::cmpxchg(&_ref_count, ref_count, -ref_count) != ref_count) {\n@@ -125,1 +125,1 @@\n-      while (Atomic::load_acquire(&_ref_count) != -1) {\n+      while (AtomicAccess::load_acquire(&_ref_count) != -1) {\n@@ -137,1 +137,1 @@\n-    const int32_t ref_count = Atomic::load(&_ref_count);\n+    const int32_t ref_count = AtomicAccess::load(&_ref_count);\n@@ -142,1 +142,1 @@\n-      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count - 1) != ref_count) {\n+      if (AtomicAccess::cmpxchg(&_ref_count, ref_count, ref_count - 1) != ref_count) {\n@@ -155,1 +155,1 @@\n-      if (Atomic::cmpxchg(&_ref_count, ref_count, ref_count + 1) != ref_count) {\n+      if (AtomicAccess::cmpxchg(&_ref_count, ref_count, ref_count + 1) != ref_count) {\n@@ -174,1 +174,1 @@\n-  if (Atomic::load_acquire(&_ref_count) != 0) {\n+  if (AtomicAccess::load_acquire(&_ref_count) != 0) {\n@@ -176,1 +176,1 @@\n-    while (Atomic::load_acquire(&_ref_count) != 0) {\n+    while (AtomicAccess::load_acquire(&_ref_count) != 0) {\n@@ -185,1 +185,1 @@\n-  assert(Atomic::load(&_ref_count) != 0, \"The page has been released\/detached\");\n+  assert(AtomicAccess::load(&_ref_count) != 0, \"The page has been released\/detached\");\n@@ -190,1 +190,1 @@\n-  Atomic::store(&_done, true);\n+  AtomicAccess::store(&_done, true);\n@@ -194,1 +194,1 @@\n-  return Atomic::load(&_done);\n+  return AtomicAccess::load(&_done);\n@@ -291,1 +291,1 @@\n-  const ZPublishState res = Atomic::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::none, ZPublishState::published);\n+  const ZPublishState res = AtomicAccess::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::none, ZPublishState::published);\n@@ -322,1 +322,1 @@\n-  const ZPublishState res = Atomic::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::none, ZPublishState::reject);\n+  const ZPublishState res = AtomicAccess::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::none, ZPublishState::reject);\n@@ -343,1 +343,1 @@\n-    const ZPublishState res2 = Atomic::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::published, ZPublishState::reject);\n+    const ZPublishState res2 = AtomicAccess::cmpxchg(&_relocated_remembered_fields_state, ZPublishState::published, ZPublishState::reject);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.cpp","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -199,1 +199,1 @@\n-  assert(Atomic::load(&_ref_count) != 0, \"The page has been released\/detached\");\n+  assert(AtomicAccess::load(&_ref_count) != 0, \"The page has been released\/detached\");\n@@ -210,1 +210,1 @@\n-  return Atomic::load_acquire(entries() + *cursor);\n+  return AtomicAccess::load_acquire(entries() + *cursor);\n@@ -276,1 +276,1 @@\n-    const ZForwardingEntry prev_entry = Atomic::cmpxchg(entries() + *cursor, old_entry, new_entry, memory_order_relaxed);\n+    const ZForwardingEntry prev_entry = AtomicAccess::cmpxchg(entries() + *cursor, old_entry, new_entry, memory_order_relaxed);\n@@ -310,1 +310,1 @@\n-  const ZPublishState res = Atomic::load(&_relocated_remembered_fields_state);\n+  const ZPublishState res = AtomicAccess::load(&_relocated_remembered_fields_state);\n@@ -330,1 +330,1 @@\n-  return Atomic::load(&_relocated_remembered_fields_state) == ZPublishState::reject;\n+  return AtomicAccess::load(&_relocated_remembered_fields_state) == ZPublishState::reject;\n@@ -338,1 +338,1 @@\n-  const ZPublishState res = Atomic::load_acquire(&_relocated_remembered_fields_state);\n+  const ZPublishState res = AtomicAccess::load_acquire(&_relocated_remembered_fields_state);\n@@ -366,1 +366,1 @@\n-    Atomic::store(&_relocated_remembered_fields_state, ZPublishState::reject);\n+    AtomicAccess::store(&_relocated_remembered_fields_state, ZPublishState::reject);\n@@ -373,1 +373,1 @@\n-    Atomic::store(&_relocated_remembered_fields_state, ZPublishState::accept);\n+    AtomicAccess::store(&_relocated_remembered_fields_state, ZPublishState::accept);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwarding.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -41,1 +41,1 @@\n-  char* const addr = Atomic::fetch_then_add(&_top, size);\n+  char* const addr = AtomicAccess::fetch_then_add(&_top, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zForwardingAllocator.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -290,1 +290,1 @@\n-  Atomic::add(&_freed, size, memory_order_relaxed);\n+  AtomicAccess::add(&_freed, size, memory_order_relaxed);\n@@ -298,1 +298,1 @@\n-  Atomic::add(&_promoted, size, memory_order_relaxed);\n+  AtomicAccess::add(&_promoted, size, memory_order_relaxed);\n@@ -306,1 +306,1 @@\n-  Atomic::add(&_compacted, size, memory_order_relaxed);\n+  AtomicAccess::add(&_compacted, size, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/z\/zGeneration.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -60,1 +60,1 @@\n-  return Atomic::load(_map + index);\n+  return AtomicAccess::load(_map + index);\n@@ -72,1 +72,1 @@\n-  Atomic::store(_map + index, value);\n+  AtomicAccess::store(_map + index, value);\n@@ -82,1 +82,1 @@\n-    Atomic::store(_map + index, value);\n+    AtomicAccess::store(_map + index, value);\n@@ -89,1 +89,1 @@\n-  return Atomic::load_acquire(_map + index);\n+  return AtomicAccess::load_acquire(_map + index);\n@@ -95,1 +95,1 @@\n-  Atomic::release_store(_map + index, value);\n+  AtomicAccess::release_store(_map + index, value);\n","filename":"src\/hotspot\/share\/gc\/z\/zGranuleMap.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n-    const oop o = Atomic::load(p);\n+    const oop o = AtomicAccess::load(p);\n","filename":"src\/hotspot\/share\/gc\/z\/zHeapIterator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -48,1 +48,1 @@\n-    return Atomic::fetch_then_add(&_claim_stripe, 1, memory_order_relaxed);\n+    return AtomicAccess::fetch_then_add(&_claim_stripe, 1, memory_order_relaxed);\n@@ -69,1 +69,1 @@\n-      for (int index; (index = Atomic::fetch_then_add(claim_addr(i), 1, memory_order_relaxed)) < stripe_max;) {\n+      for (int index; (index = AtomicAccess::fetch_then_add(claim_addr(i), 1, memory_order_relaxed)) < stripe_max;) {\n@@ -78,1 +78,1 @@\n-      for (int index; (index = Atomic::fetch_then_add(claim_addr(i), 1, memory_order_relaxed)) < stripe_max;) {\n+      for (int index; (index = AtomicAccess::fetch_then_add(claim_addr(i), 1, memory_order_relaxed)) < stripe_max;) {\n@@ -175,1 +175,1 @@\n-    return Atomic::fetch_then_add(&_claim_array[index], 1, memory_order_relaxed);\n+    return AtomicAccess::fetch_then_add(&_claim_array[index], 1, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/z\/zIndexDistributor.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -59,1 +59,1 @@\n-    const int64_t count = Atomic::load_acquire(&_count);\n+    const int64_t count = AtomicAccess::load_acquire(&_count);\n@@ -64,1 +64,1 @@\n-      while (Atomic::load_acquire(&_count) < 0) {\n+      while (AtomicAccess::load_acquire(&_count) < 0) {\n@@ -73,1 +73,1 @@\n-    if (Atomic::cmpxchg(&_count, count, -(count + 1)) != count) {\n+    if (AtomicAccess::cmpxchg(&_count, count, -(count + 1)) != count) {\n@@ -83,1 +83,1 @@\n-      while (Atomic::load_acquire(&_count) != -1) {\n+      while (AtomicAccess::load_acquire(&_count) != -1) {\n@@ -94,1 +94,1 @@\n-  const int64_t count = Atomic::load_acquire(&_count);\n+  const int64_t count = AtomicAccess::load_acquire(&_count);\n@@ -99,1 +99,1 @@\n-  Atomic::release_store(&_count, (int64_t)0);\n+  AtomicAccess::release_store(&_count, (int64_t)0);\n@@ -105,1 +105,1 @@\n-    const int64_t count = Atomic::load_acquire(&_count);\n+    const int64_t count = AtomicAccess::load_acquire(&_count);\n@@ -115,1 +115,1 @@\n-      while (Atomic::load_acquire(&_count) < 0) {\n+      while (AtomicAccess::load_acquire(&_count) < 0) {\n@@ -124,1 +124,1 @@\n-    if (Atomic::cmpxchg(&_count, count, count + 1) != count) {\n+    if (AtomicAccess::cmpxchg(&_count, count, count + 1) != count) {\n@@ -145,1 +145,1 @@\n-    const int64_t count = Atomic::load_acquire(&_count);\n+    const int64_t count = AtomicAccess::load_acquire(&_count);\n@@ -150,1 +150,1 @@\n-      if (Atomic::cmpxchg(&_count, count, count - 1) != count) {\n+      if (AtomicAccess::cmpxchg(&_count, count, count - 1) != count) {\n@@ -155,1 +155,1 @@\n-      if (Atomic::cmpxchg(&_count, count, count + 1) != count) {\n+      if (AtomicAccess::cmpxchg(&_count, count, count + 1) != count) {\n","filename":"src\/hotspot\/share\/gc\/z\/zJNICritical.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -63,1 +63,1 @@\n-  for (uint32_t seqnum = Atomic::load_acquire(&_seqnum);\n+  for (uint32_t seqnum = AtomicAccess::load_acquire(&_seqnum);\n@@ -65,1 +65,1 @@\n-       seqnum = Atomic::load_acquire(&_seqnum)) {\n+       seqnum = AtomicAccess::load_acquire(&_seqnum)) {\n@@ -69,1 +69,1 @@\n-      if (Atomic::cmpxchg(&_seqnum, seqnum, seqnum_initializing) == seqnum) {\n+      if (AtomicAccess::cmpxchg(&_seqnum, seqnum, seqnum_initializing) == seqnum) {\n@@ -90,1 +90,1 @@\n-        Atomic::release_store(&_seqnum, generation->seqnum());\n+        AtomicAccess::release_store(&_seqnum, generation->seqnum());\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -43,1 +43,1 @@\n-  return Atomic::load_acquire(&_seqnum) == ZGeneration::generation(id)->seqnum();\n+  return AtomicAccess::load_acquire(&_seqnum) == ZGeneration::generation(id)->seqnum();\n@@ -119,2 +119,2 @@\n-  Atomic::add(&_live_objects, objects);\n-  Atomic::add(&_live_bytes, bytes);\n+  AtomicAccess::add(&_live_objects, objects);\n+  AtomicAccess::add(&_live_bytes, bytes);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -53,1 +53,1 @@\n-  Thread* const owner = Atomic::load(&_owner);\n+  Thread* const owner = AtomicAccess::load(&_owner);\n@@ -57,1 +57,1 @@\n-    Atomic::store(&_owner, thread);\n+    AtomicAccess::store(&_owner, thread);\n@@ -70,1 +70,1 @@\n-    Atomic::store(&_owner, (Thread*)nullptr);\n+    AtomicAccess::store(&_owner, (Thread*)nullptr);\n@@ -77,1 +77,1 @@\n-  Thread* const owner = Atomic::load(&_owner);\n+  Thread* const owner = AtomicAccess::load(&_owner);\n","filename":"src\/hotspot\/share\/gc\/z\/zLock.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -219,1 +219,1 @@\n-  return Atomic::load(&_num_nodes);\n+  return AtomicAccess::load(&_num_nodes);\n","filename":"src\/hotspot\/share\/gc\/z\/zMappedCache.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -598,1 +598,1 @@\n-  Atomic::inc(&_work_nterminateflush);\n+  AtomicAccess::inc(&_work_nterminateflush);\n@@ -614,1 +614,1 @@\n-  if (Atomic::load(&_work_nproactiveflush) == ZMarkProactiveFlushMax) {\n+  if (AtomicAccess::load(&_work_nproactiveflush) == ZMarkProactiveFlushMax) {\n@@ -619,1 +619,1 @@\n-  Atomic::inc(&_work_nproactiveflush);\n+  AtomicAccess::inc(&_work_nproactiveflush);\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -75,1 +75,1 @@\n-  return Atomic::load(&_head) == nullptr;\n+  return AtomicAccess::load(&_head) == nullptr;\n@@ -80,1 +80,1 @@\n-  ZMarkStackListNode* head = Atomic::load(&_head);\n+  ZMarkStackListNode* head = AtomicAccess::load(&_head);\n@@ -90,1 +90,1 @@\n-    ZMarkStackListNode* prev = Atomic::cmpxchg(&_head, head, node, memory_order_release);\n+    ZMarkStackListNode* prev = AtomicAccess::cmpxchg(&_head, head, node, memory_order_release);\n@@ -96,1 +96,1 @@\n-      Atomic::inc(&_length, memory_order_relaxed);\n+      AtomicAccess::inc(&_length, memory_order_relaxed);\n@@ -108,1 +108,1 @@\n-  ZMarkStackListNode* head = Atomic::load(&_head);\n+  ZMarkStackListNode* head = AtomicAccess::load(&_head);\n@@ -118,1 +118,1 @@\n-    Atomic::store(hazard_ptr, head);\n+    AtomicAccess::store(hazard_ptr, head);\n@@ -130,1 +130,1 @@\n-    ZMarkStackListNode* const head_after_publish = Atomic::load_acquire(&_head);\n+    ZMarkStackListNode* const head_after_publish = AtomicAccess::load_acquire(&_head);\n@@ -144,1 +144,1 @@\n-    ZMarkStackListNode* const prev = Atomic::cmpxchg(&_head, head, next, memory_order_relaxed);\n+    ZMarkStackListNode* const prev = AtomicAccess::cmpxchg(&_head, head, next, memory_order_relaxed);\n@@ -152,1 +152,1 @@\n-      Atomic::release_store(hazard_ptr, (ZMarkStackListNode*)nullptr);\n+      AtomicAccess::release_store(hazard_ptr, (ZMarkStackListNode*)nullptr);\n@@ -155,1 +155,1 @@\n-      Atomic::dec(&_length, memory_order_relaxed);\n+      AtomicAccess::dec(&_length, memory_order_relaxed);\n@@ -170,1 +170,1 @@\n-  const ssize_t result = Atomic::load(&_length);\n+  const ssize_t result = AtomicAccess::load(&_length);\n@@ -224,1 +224,1 @@\n-  if (Atomic::cmpxchg(&_nstripes_mask, old_nstripes_mask, new_nstripes_mask) == old_nstripes_mask) {\n+  if (AtomicAccess::cmpxchg(&_nstripes_mask, old_nstripes_mask, new_nstripes_mask) == old_nstripes_mask) {\n@@ -233,1 +233,1 @@\n-  return Atomic::load(&_nstripes_mask) + 1;\n+  return AtomicAccess::load(&_nstripes_mask) + 1;\n@@ -261,1 +261,1 @@\n-  const size_t mask = Atomic::load(&_nstripes_mask);\n+  const size_t mask = AtomicAccess::load(&_nstripes_mask);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStack.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -95,1 +95,1 @@\n-  const size_t index = (addr >> ZMarkStripeShift) & Atomic::load(&_nstripes_mask);\n+  const size_t index = (addr >> ZMarkStripeShift) & AtomicAccess::load(&_nstripes_mask);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStack.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -45,2 +45,2 @@\n-  Atomic::store(&_nworkers, nworkers);\n-  Atomic::store(&_nworking, nworkers);\n+  AtomicAccess::store(&_nworkers, nworkers);\n+  AtomicAccess::store(&_nworking, nworkers);\n@@ -54,1 +54,1 @@\n-  Atomic::store(&_nworking, _nworking - 1);\n+  AtomicAccess::store(&_nworking, _nworking - 1);\n@@ -72,1 +72,1 @@\n-  Atomic::store(&_nworking, _nworking - 1);\n+  AtomicAccess::store(&_nworking, _nworking - 1);\n@@ -87,1 +87,1 @@\n-    Atomic::store(&_nawakening, _nawakening - 1);\n+    AtomicAccess::store(&_nawakening, _nawakening - 1);\n@@ -95,1 +95,1 @@\n-  Atomic::store(&_nworking, _nworking + 1);\n+  AtomicAccess::store(&_nworking, _nworking + 1);\n@@ -101,3 +101,3 @@\n-  uint nworking = Atomic::load(&_nworking);\n-  uint nawakening = Atomic::load(&_nawakening);\n-  if (nworking + nawakening == Atomic::load(&_nworkers)) {\n+  uint nworking = AtomicAccess::load(&_nworking);\n+  uint nawakening = AtomicAccess::load(&_nawakening);\n+  if (nworking + nawakening == AtomicAccess::load(&_nworkers)) {\n@@ -116,1 +116,1 @@\n-    Atomic::store(&_nawakening, _nawakening + 1);\n+    AtomicAccess::store(&_nawakening, _nawakening + 1);\n@@ -122,2 +122,2 @@\n-  uint nworking = Atomic::load(&_nworking);\n-  uint nawakening = Atomic::load(&_nawakening);\n+  uint nworking = AtomicAccess::load(&_nworking);\n+  uint nawakening = AtomicAccess::load(&_nawakening);\n@@ -125,1 +125,1 @@\n-  return nworking + nawakening == Atomic::load(&_nworkers);\n+  return nworking + nawakening == AtomicAccess::load(&_nworkers);\n@@ -131,1 +131,1 @@\n-    Atomic::store(&_resurrected, value);\n+    AtomicAccess::store(&_resurrected, value);\n@@ -141,1 +141,1 @@\n-  return Atomic::load(&_resurrected);\n+  return AtomicAccess::load(&_resurrected);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkTerminate.inline.hpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -73,1 +73,1 @@\n-    ZMarkStackListNode* const hazard = Atomic::load(&remote_state->_hazard_ptr);\n+    ZMarkStackListNode* const hazard = AtomicAccess::load(&remote_state->_hazard_ptr);\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkingSMR.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -149,1 +149,1 @@\n-      const oop o = Atomic::load(p); \/\/ C1 PatchingStub may replace it concurrently.\n+      const oop o = AtomicAccess::load(p); \/\/ C1 PatchingStub may replace it concurrently.\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -60,1 +60,1 @@\n-    const size_t partition_start = MIN2(Atomic::fetch_then_add(&_claimed, partition_size), _size);\n+    const size_t partition_start = MIN2(AtomicAccess::fetch_then_add(&_claimed, partition_size), _size);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethodTableIteration.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -69,1 +69,1 @@\n-  ZPage* page = Atomic::load_acquire(shared_page);\n+  ZPage* page = AtomicAccess::load_acquire(shared_page);\n@@ -84,1 +84,1 @@\n-      ZPage* const prev_page = Atomic::cmpxchg(shared_page, page, new_page);\n+      ZPage* const prev_page = AtomicAccess::cmpxchg(shared_page, page, new_page);\n@@ -116,1 +116,1 @@\n-  ZPage* page = Atomic::load_acquire(shared_medium_page);\n+  ZPage* page = AtomicAccess::load_acquire(shared_medium_page);\n@@ -230,1 +230,1 @@\n-  const ZPage* const page = Atomic::load_acquire(shared_addr);\n+  const ZPage* const page = AtomicAccess::load_acquire(shared_addr);\n","filename":"src\/hotspot\/share\/gc\/z\/zObjectAllocator.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -82,2 +82,2 @@\n-  Atomic::store(&_seqnum, generation()->seqnum());\n-  Atomic::store(&_seqnum_other, ZGeneration::generation(_generation_id == ZGenerationId::young ? ZGenerationId::old : ZGenerationId::young)->seqnum());\n+  AtomicAccess::store(&_seqnum, generation()->seqnum());\n+  AtomicAccess::store(&_seqnum_other, ZGeneration::generation(_generation_id == ZGenerationId::young ? ZGenerationId::old : ZGenerationId::young)->seqnum());\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -471,1 +471,1 @@\n-    const zoffset_end prev_top = Atomic::cmpxchg(&_top, addr, new_top);\n+    const zoffset_end prev_top = AtomicAccess::cmpxchg(&_top, addr, new_top);\n@@ -515,1 +515,1 @@\n-    const zoffset_end prev_top = Atomic::cmpxchg(&_top, old_top, new_top);\n+    const zoffset_end prev_top = AtomicAccess::cmpxchg(&_top, old_top, new_top);\n","filename":"src\/hotspot\/share\/gc\/z\/zPage.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -653,1 +653,1 @@\n-    Atomic::add(&_capacity, increased);\n+    AtomicAccess::add(&_capacity, increased);\n@@ -663,1 +663,1 @@\n-  Atomic::sub(&_capacity, size);\n+  AtomicAccess::sub(&_capacity, size);\n@@ -668,1 +668,1 @@\n-    Atomic::store(&_current_max_capacity, _capacity);\n+    AtomicAccess::store(&_current_max_capacity, _capacity);\n@@ -938,1 +938,1 @@\n-      const uintptr_t claimed = Atomic::fetch_then_add(&_current, size);\n+      const uintptr_t claimed = AtomicAccess::fetch_then_add(&_current, size);\n@@ -1283,1 +1283,1 @@\n-  const size_t soft_max_heapsize = Atomic::load(&SoftMaxHeapSize);\n+  const size_t soft_max_heapsize = AtomicAccess::load(&SoftMaxHeapSize);\n@@ -1292,1 +1292,1 @@\n-    current_max_capacity += Atomic::load(&partition->_current_max_capacity);\n+    current_max_capacity += AtomicAccess::load(&partition->_current_max_capacity);\n@@ -1303,1 +1303,1 @@\n-    capacity += Atomic::load(&partition->_capacity);\n+    capacity += AtomicAccess::load(&partition->_capacity);\n@@ -1310,1 +1310,1 @@\n-  return Atomic::load(&_used);\n+  return AtomicAccess::load(&_used);\n@@ -1314,1 +1314,1 @@\n-  return Atomic::load(&_used_generations[(int)id]);\n+  return AtomicAccess::load(&_used_generations[(int)id]);\n@@ -1324,2 +1324,2 @@\n-    capacity += (ssize_t)Atomic::load(&partition->_capacity);\n-    claimed += (ssize_t)Atomic::load(&partition->_claimed);\n+    capacity += (ssize_t)AtomicAccess::load(&partition->_capacity);\n+    claimed += (ssize_t)AtomicAccess::load(&partition->_claimed);\n@@ -1379,1 +1379,1 @@\n-  Atomic::add(&_used_generations[(int)id], size, memory_order_relaxed);\n+  AtomicAccess::add(&_used_generations[(int)id], size, memory_order_relaxed);\n@@ -1384,1 +1384,1 @@\n-  Atomic::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n+  AtomicAccess::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n@@ -2232,1 +2232,1 @@\n-  const size_t used = Atomic::add(&_used, size);\n+  const size_t used = AtomicAccess::add(&_used, size);\n@@ -2244,1 +2244,1 @@\n-  const size_t used = Atomic::sub(&_used, size);\n+  const size_t used = AtomicAccess::sub(&_used, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -319,1 +319,1 @@\n-    const zaddress old_pending_list = Atomic::xchg(_pending_list.addr(), keep_head);\n+    const zaddress old_pending_list = AtomicAccess::xchg(_pending_list.addr(), keep_head);\n@@ -338,1 +338,1 @@\n-    const zaddress discovered_list = Atomic::xchg(start, zaddress::null);\n+    const zaddress discovered_list = AtomicAccess::xchg(start, zaddress::null);\n","filename":"src\/hotspot\/share\/gc\/z\/zReferenceProcessor.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -67,1 +67,1 @@\n-  return Atomic::load(&_needs_attention) != 0;\n+  return AtomicAccess::load(&_needs_attention) != 0;\n@@ -71,1 +71,1 @@\n-  const int needs_attention = Atomic::add(&_needs_attention, 1);\n+  const int needs_attention = AtomicAccess::add(&_needs_attention, 1);\n@@ -76,1 +76,1 @@\n-  const int needs_attention = Atomic::sub(&_needs_attention, 1);\n+  const int needs_attention = AtomicAccess::sub(&_needs_attention, 1);\n@@ -86,1 +86,1 @@\n-  Atomic::store(&_is_active, false);\n+  AtomicAccess::store(&_is_active, false);\n@@ -91,1 +91,1 @@\n-  return Atomic::load(&_is_active);\n+  return AtomicAccess::load(&_is_active);\n@@ -466,1 +466,1 @@\n-      Atomic::inc(&_in_place_count);\n+      AtomicAccess::inc(&_in_place_count);\n@@ -542,1 +542,1 @@\n-        Atomic::inc(&_in_place_count);\n+        AtomicAccess::inc(&_in_place_count);\n@@ -744,1 +744,1 @@\n-    const zpointer ptr = Atomic::load(p);\n+    const zpointer ptr = AtomicAccess::load(p);\n@@ -1214,1 +1214,1 @@\n-      Atomic::add(&_numa_local_forwardings, numa_local_forwardings_worker, memory_order_relaxed);\n+      AtomicAccess::add(&_numa_local_forwardings, numa_local_forwardings_worker, memory_order_relaxed);\n@@ -1226,1 +1226,1 @@\n-  const zpointer ptr = Atomic::load(p);\n+  const zpointer ptr = AtomicAccess::load(p);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocationSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -406,1 +406,1 @@\n-  BitMap::idx_t prev = Atomic::load(&_claimed);\n+  BitMap::idx_t prev = AtomicAccess::load(&_claimed);\n@@ -415,1 +415,1 @@\n-      Atomic::cmpxchg(&_claimed, prev, page_index, memory_order_relaxed);\n+      AtomicAccess::cmpxchg(&_claimed, prev, page_index, memory_order_relaxed);\n@@ -419,1 +419,1 @@\n-    const BitMap::idx_t res = Atomic::cmpxchg(&_claimed, prev, page_index + 1, memory_order_relaxed);\n+    const BitMap::idx_t res = AtomicAccess::cmpxchg(&_claimed, prev, page_index + 1, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/z\/zRemembered.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -40,1 +40,1 @@\n-  Atomic::store(&_blocked, false);\n+  AtomicAccess::store(&_blocked, false);\n","filename":"src\/hotspot\/share\/gc\/z\/zResurrection.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -32,1 +32,1 @@\n-  return Atomic::load(&_blocked);\n+  return AtomicAccess::load(&_blocked);\n","filename":"src\/hotspot\/share\/gc\/z\/zResurrection.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -94,1 +94,1 @@\n-  if (!Atomic::load(&_completed)) {\n+  if (!AtomicAccess::load(&_completed)) {\n@@ -96,2 +96,2 @@\n-    if (!Atomic::load(&_completed)) {\n-      Atomic::store(&_completed, true);\n+    if (!AtomicAccess::load(&_completed)) {\n+      AtomicAccess::store(&_completed, true);\n@@ -123,1 +123,1 @@\n-  return Atomic::fetch_then_add(&_claimed, 1u);\n+  return AtomicAccess::fetch_then_add(&_claimed, 1u);\n","filename":"src\/hotspot\/share\/gc\/z\/zRootsIterator.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -448,3 +448,3 @@\n-      const uint64_t nsamples = Atomic::xchg(&cpu_data->_nsamples, (uint64_t)0);\n-      const uint64_t sum = Atomic::xchg(&cpu_data->_sum, (uint64_t)0);\n-      const uint64_t max = Atomic::xchg(&cpu_data->_max, (uint64_t)0);\n+      const uint64_t nsamples = AtomicAccess::xchg(&cpu_data->_nsamples, (uint64_t)0);\n+      const uint64_t sum = AtomicAccess::xchg(&cpu_data->_sum, (uint64_t)0);\n+      const uint64_t max = AtomicAccess::xchg(&cpu_data->_max, (uint64_t)0);\n@@ -483,1 +483,1 @@\n-    counter += Atomic::xchg(&cpu_data->_counter, (uint64_t)0);\n+    counter += AtomicAccess::xchg(&cpu_data->_counter, (uint64_t)0);\n@@ -505,1 +505,1 @@\n-    all._counter += Atomic::xchg(&cpu_data->_counter, (uint64_t)0);\n+    all._counter += AtomicAccess::xchg(&cpu_data->_counter, (uint64_t)0);\n@@ -895,2 +895,2 @@\n-  Atomic::add(&cpu_data->_nsamples, 1u);\n-  Atomic::add(&cpu_data->_sum, value);\n+  AtomicAccess::add(&cpu_data->_nsamples, 1u);\n+  AtomicAccess::add(&cpu_data->_sum, value);\n@@ -906,1 +906,1 @@\n-    const uint64_t prev_max = Atomic::cmpxchg(&cpu_data->_max, max, new_max);\n+    const uint64_t prev_max = AtomicAccess::cmpxchg(&cpu_data->_max, max, new_max);\n@@ -925,1 +925,1 @@\n-  const uint64_t value = Atomic::add(&cpu_data->_counter, increment);\n+  const uint64_t value = AtomicAccess::add(&cpu_data->_counter, increment);\n@@ -932,1 +932,1 @@\n-  Atomic::add(&cpu_data->_counter, increment);\n+  AtomicAccess::add(&cpu_data->_counter, increment);\n@@ -959,1 +959,1 @@\n-  const size_t allocated = Atomic::add(&_allocated_since_sample, allocation_bytes);\n+  const size_t allocated = AtomicAccess::add(&_allocated_since_sample, allocation_bytes);\n@@ -961,1 +961,1 @@\n-  if (allocated < Atomic::load(&_sampling_granule)) {\n+  if (allocated < AtomicAccess::load(&_sampling_granule)) {\n@@ -971,1 +971,1 @@\n-  const size_t allocated_sample = Atomic::load(&_allocated_since_sample);\n+  const size_t allocated_sample = AtomicAccess::load(&_allocated_since_sample);\n@@ -988,1 +988,1 @@\n-  Atomic::sub(&_allocated_since_sample, allocated_sample);\n+  AtomicAccess::sub(&_allocated_since_sample, allocated_sample);\n","filename":"src\/hotspot\/share\/gc\/z\/zStat.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -33,1 +33,1 @@\n-  Atomic::add(&_used, size, memory_order_relaxed);\n+  AtomicAccess::add(&_used, size, memory_order_relaxed);\n@@ -39,1 +39,1 @@\n-  Atomic::sub(&_used, size, memory_order_relaxed);\n+  AtomicAccess::sub(&_used, size, memory_order_relaxed);\n@@ -43,1 +43,1 @@\n-  const size_t used = Atomic::xchg(&_used, (size_t) 0);\n+  const size_t used = AtomicAccess::xchg(&_used, (size_t) 0);\n","filename":"src\/hotspot\/share\/gc\/z\/zTLABUsage.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-  const zaddress_unsafe addr = Atomic::load(p);\n+  const zaddress_unsafe addr = AtomicAccess::load(p);\n","filename":"src\/hotspot\/share\/gc\/z\/zUncoloredRoot.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -403,1 +403,1 @@\n-    Atomic::add(&_partition->_claimed, flushed);\n+    AtomicAccess::add(&_partition->_claimed, flushed);\n@@ -419,1 +419,1 @@\n-    Atomic::sub(&_partition->_claimed, flushed);\n+    AtomicAccess::sub(&_partition->_claimed, flushed);\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-    zaddress_unsafe addr = Atomic::load(ZUncoloredRoot::cast(p));\n+    zaddress_unsafe addr = AtomicAccess::load(ZUncoloredRoot::cast(p));\n","filename":"src\/hotspot\/share\/gc\/z\/zUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -447,1 +447,1 @@\n-    _visited_ptr_pre_loaded = Atomic::load(_visited_p);\n+    _visited_ptr_pre_loaded = AtomicAccess::load(_visited_p);\n@@ -655,1 +655,1 @@\n-    const zpointer ptr = Atomic::load(p);\n+    const zpointer ptr = AtomicAccess::load(p);\n@@ -698,1 +698,1 @@\n-    if (Atomic::load(p) != ptr) {\n+    if (AtomicAccess::load(p) != ptr) {\n","filename":"src\/hotspot\/share\/gc\/z\/zVerify.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/gc\/z\/zWeakRootsProcessor.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -32,1 +32,1 @@\n-  return Atomic::load(&_requested_nworkers) != 0;\n+  return AtomicAccess::load(&_requested_nworkers) != 0;\n","filename":"src\/hotspot\/share\/gc\/z\/zWorkers.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -187,1 +187,1 @@\n-    BytecodePrinter printer(Atomic::load_acquire(&_method_currently_being_printed));\n+    BytecodePrinter printer(AtomicAccess::load_acquire(&_method_currently_being_printed));\n@@ -190,1 +190,1 @@\n-    Atomic::release_store(&_method_currently_being_printed, method());\n+    AtomicAccess::release_store(&_method_currently_being_printed, method());\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeTracer.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -355,1 +355,1 @@\n-  Atomic::inc(&Exceptions::_stack_overflow_errors);\n+  AtomicAccess::inc(&Exceptions::_stack_overflow_errors);\n@@ -369,1 +369,1 @@\n-  Atomic::inc(&Exceptions::_stack_overflow_errors);\n+  AtomicAccess::inc(&Exceptions::_stack_overflow_errors);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -451,1 +451,1 @@\n-  return Atomic::load_acquire(&(_array[i % size]));\n+  return AtomicAccess::load_acquire(&(_array[i % size]));\n@@ -455,1 +455,1 @@\n-  return Atomic::cmpxchg(&_array[i % size], old, entry) == old;\n+  return AtomicAccess::cmpxchg(&_array[i % size], old, entry) == old;\n@@ -565,1 +565,1 @@\n-    OopMapCacheEntry* head = Atomic::load(&_old_entries);\n+    OopMapCacheEntry* head = AtomicAccess::load(&_old_entries);\n@@ -567,1 +567,1 @@\n-    if (Atomic::cmpxchg(&_old_entries, head, entry) == head) {\n+    if (AtomicAccess::cmpxchg(&_old_entries, head, entry) == head) {\n@@ -581,1 +581,1 @@\n-  return Atomic::load(&_old_entries) != nullptr;\n+  return AtomicAccess::load(&_old_entries) != nullptr;\n@@ -595,1 +595,1 @@\n-  OopMapCacheEntry* entry = Atomic::xchg(&_old_entries, (OopMapCacheEntry*)nullptr);\n+  OopMapCacheEntry* entry = AtomicAccess::xchg(&_old_entries, (OopMapCacheEntry*)nullptr);\n","filename":"src\/hotspot\/share\/interpreter\/oopMapCache.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -91,2 +91,2 @@\n-    Atomic::store(&_dead_samples, true);\n-    Atomic::store(&_last_sweep, (int64_t)JfrTicks::now().value());\n+    AtomicAccess::store(&_dead_samples, true);\n+    AtomicAccess::store(&_last_sweep, (int64_t)JfrTicks::now().value());\n@@ -116,2 +116,2 @@\n-  Atomic::store(&_dead_samples, false);\n-  Atomic::store(&_last_sweep, (int64_t)JfrTicks::now().value());\n+  AtomicAccess::store(&_dead_samples, false);\n+  AtomicAccess::store(&_last_sweep, (int64_t)JfrTicks::now().value());\n@@ -159,1 +159,1 @@\n-  while (Atomic::cmpxchg(&_lock, 0, 1) == 1) {}\n+  while (AtomicAccess::cmpxchg(&_lock, 0, 1) == 1) {}\n@@ -243,1 +243,1 @@\n-  if (Atomic::load(&_dead_samples)) {\n+  if (AtomicAccess::load(&_dead_samples)) {\n@@ -246,1 +246,1 @@\n-    Atomic::store(&_dead_samples, false);\n+    AtomicAccess::store(&_dead_samples, false);\n@@ -363,1 +363,1 @@\n-  return Atomic::load(&_last_sweep);\n+  return AtomicAccess::load(&_last_sweep);\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/sampling\/objectSampler.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -88,1 +88,1 @@\n-    elementIndex = Atomic::load_acquire(&_head);\n+    elementIndex = AtomicAccess::load_acquire(&_head);\n@@ -92,1 +92,1 @@\n-  } while (Atomic::cmpxchg(&_head, elementIndex, elementIndex + 1) != elementIndex);\n+  } while (AtomicAccess::cmpxchg(&_head, elementIndex, elementIndex + 1) != elementIndex);\n@@ -105,1 +105,1 @@\n-  return Atomic::load_acquire(&_head);\n+  return AtomicAccess::load_acquire(&_head);\n@@ -109,1 +109,1 @@\n-  Atomic::release_store(&_head, size);\n+  AtomicAccess::release_store(&_head, size);\n@@ -131,1 +131,1 @@\n-  return Atomic::load_acquire(&_head) == 0;\n+  return AtomicAccess::load_acquire(&_head) == 0;\n@@ -135,1 +135,1 @@\n-  return Atomic::load(&_lost_samples);\n+  return AtomicAccess::load(&_lost_samples);\n@@ -139,2 +139,2 @@\n-  Atomic::inc(&_lost_samples_sum);\n-  Atomic::inc(&_lost_samples);\n+  AtomicAccess::inc(&_lost_samples_sum);\n+  AtomicAccess::inc(&_lost_samples);\n@@ -144,1 +144,1 @@\n-  return Atomic::xchg(&_lost_samples, (u4)0);\n+  return AtomicAccess::xchg(&_lost_samples, (u4)0);\n@@ -162,1 +162,1 @@\n-  Atomic::release_store(&_head, (u4)0);\n+  AtomicAccess::release_store(&_head, (u4)0);\n@@ -222,1 +222,1 @@\n-  int64_t get_sampling_period() const { return Atomic::load(&_current_sampling_period_ns); };\n+  int64_t get_sampling_period() const { return AtomicAccess::load(&_current_sampling_period_ns); };\n@@ -268,1 +268,1 @@\n-  Atomic::release_store(&_is_async_processing_of_cpu_time_jfr_requests_triggered, true);\n+  AtomicAccess::release_store(&_is_async_processing_of_cpu_time_jfr_requests_triggered, true);\n@@ -273,1 +273,1 @@\n-      !Atomic::load_acquire(&_signal_handler_installed)) {\n+      !AtomicAccess::load_acquire(&_signal_handler_installed)) {\n@@ -283,1 +283,1 @@\n-    if (!Atomic::or_then_fetch(&_warned_about_timer_creation_failure, true)) {\n+    if (!AtomicAccess::or_then_fetch(&_warned_about_timer_creation_failure, true)) {\n@@ -314,2 +314,2 @@\n-  if (Atomic::cmpxchg(&_disenrolled, true, false)) {\n-    Atomic::store(&_warned_about_timer_creation_failure, false);\n+  if (AtomicAccess::cmpxchg(&_disenrolled, true, false)) {\n+    AtomicAccess::store(&_warned_about_timer_creation_failure, false);\n@@ -329,1 +329,1 @@\n-  if (!Atomic::cmpxchg(&_disenrolled, false, true)) {\n+  if (!AtomicAccess::cmpxchg(&_disenrolled, false, true)) {\n@@ -331,1 +331,1 @@\n-    if (Atomic::load_acquire(&_signal_handler_installed)) {\n+    if (AtomicAccess::load_acquire(&_signal_handler_installed)) {\n@@ -356,1 +356,1 @@\n-    if (Atomic::cmpxchg(&_is_async_processing_of_cpu_time_jfr_requests_triggered, true, false)) {\n+    if (AtomicAccess::cmpxchg(&_is_async_processing_of_cpu_time_jfr_requests_triggered, true, false)) {\n@@ -412,1 +412,1 @@\n-  Atomic::inc(&count);\n+  AtomicAccess::inc(&count);\n@@ -414,1 +414,1 @@\n-    Atomic::inc(&biased_count);\n+    AtomicAccess::inc(&biased_count);\n@@ -416,2 +416,2 @@\n-  if (Atomic::load(&count) % 1000 == 0) {\n-    log_debug(jfr)(\"CPU thread sampler sent %zu events, lost %d, biased %zu\\n\", Atomic::load(&count), Atomic::load(&_lost_samples_sum), Atomic::load(&biased_count));\n+  if (AtomicAccess::load(&count) % 1000 == 0) {\n+    log_debug(jfr)(\"CPU thread sampler sent %zu events, lost %d, biased %zu\\n\", AtomicAccess::load(&count), AtomicAccess::load(&_lost_samples_sum), AtomicAccess::load(&biased_count));\n@@ -649,2 +649,2 @@\n-  Atomic::or_then_fetch(&_active_signal_handlers, STOP_SIGNAL_BIT, memory_order_acq_rel);\n-  while (Atomic::load_acquire(&_active_signal_handlers) > STOP_SIGNAL_BIT) {\n+  AtomicAccess::or_then_fetch(&_active_signal_handlers, STOP_SIGNAL_BIT, memory_order_acq_rel);\n+  while (AtomicAccess::load_acquire(&_active_signal_handlers) > STOP_SIGNAL_BIT) {\n@@ -659,1 +659,1 @@\n-  u4 old_value = Atomic::fetch_then_add(&_active_signal_handlers, (u4)1, memory_order_acq_rel);\n+  u4 old_value = AtomicAccess::fetch_then_add(&_active_signal_handlers, (u4)1, memory_order_acq_rel);\n@@ -662,1 +662,1 @@\n-    Atomic::dec(&_active_signal_handlers, memory_order_acq_rel);\n+    AtomicAccess::dec(&_active_signal_handlers, memory_order_acq_rel);\n@@ -669,1 +669,1 @@\n-  Atomic::dec(&_active_signal_handlers, memory_order_acq_rel);\n+  AtomicAccess::dec(&_active_signal_handlers, memory_order_acq_rel);\n@@ -673,1 +673,1 @@\n-  Atomic::release_store(&_active_signal_handlers, (u4)0);\n+  AtomicAccess::release_store(&_active_signal_handlers, (u4)0);\n@@ -700,1 +700,1 @@\n-  Atomic::release_store(&_signal_handler_installed, true);\n+  AtomicAccess::release_store(&_signal_handler_installed, true);\n@@ -738,1 +738,1 @@\n-    Atomic::store(&_current_sampling_period_ns, period);\n+    AtomicAccess::store(&_current_sampling_period_ns, period);\n@@ -745,1 +745,1 @@\n-  if (_throttle.enabled() && Atomic::load_acquire(&_disenrolled) == false) {\n+  if (_throttle.enabled() && AtomicAccess::load_acquire(&_disenrolled) == false) {\n@@ -748,1 +748,1 @@\n-    Atomic::store(&_current_sampling_period_ns, _throttle.compute_sampling_period());\n+    AtomicAccess::store(&_current_sampling_period_ns, _throttle.compute_sampling_period());\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrCPUTimeThreadSampler.cpp","additions":34,"deletions":34,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -86,2 +86,2 @@\n-  int64_t java_period() const { return Atomic::load(&_java_period_millis); };\n-  int64_t native_period() const { return Atomic::load(&_native_period_millis); };\n+  int64_t java_period() const { return AtomicAccess::load(&_java_period_millis); };\n+  int64_t native_period() const { return AtomicAccess::load(&_native_period_millis); };\n@@ -379,1 +379,1 @@\n-  Atomic::store(&_java_period_millis, period_millis);\n+  AtomicAccess::store(&_java_period_millis, period_millis);\n@@ -384,1 +384,1 @@\n-  Atomic::store(&_native_period_millis, period_millis);\n+  AtomicAccess::store(&_native_period_millis, period_millis);\n","filename":"src\/hotspot\/share\/jfr\/periodic\/sampling\/jfrThreadSampler.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/jfr\/recorder\/checkpoint\/jfrCheckpointManager.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -46,1 +46,1 @@\n-  } while (Atomic::cmpxchg(dest, compare_value, exchange_value) != compare_value);\n+  } while (AtomicAccess::cmpxchg(dest, compare_value, exchange_value) != compare_value);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/checkpoint\/types\/traceid\/jfrTraceId.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -109,1 +109,1 @@\n-    if (current == new_value || Atomic::cmpxchg(dest, current, new_value) == current) {\n+    if (current == new_value || AtomicAccess::cmpxchg(dest, current, new_value) == current) {\n","filename":"src\/hotspot\/share\/jfr\/recorder\/checkpoint\/types\/traceid\/jfrTraceIdBits.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -57,1 +57,1 @@\n-  Atomic::release_store(&_method_tracer_state, true);\n+  AtomicAccess::release_store(&_method_tracer_state, true);\n@@ -62,1 +62,1 @@\n-  Atomic::release_store(&_method_tracer_state, false);\n+  AtomicAccess::release_store(&_method_tracer_state, false);\n@@ -66,1 +66,1 @@\n-  return Atomic::load_acquire(&_method_tracer_state);\n+  return AtomicAccess::load_acquire(&_method_tracer_state);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/checkpoint\/types\/traceid\/jfrTraceIdEpoch.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -526,1 +526,1 @@\n-  return Atomic::cmpxchg(&jfr_shutdown_lock, 0, 1) == 0;\n+  return AtomicAccess::cmpxchg(&jfr_shutdown_lock, 0, 1) == 0;\n","filename":"src\/hotspot\/share\/jfr\/recorder\/repository\/jfrEmergencyDump.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -88,1 +88,1 @@\n-    const int current_msgs = Atomic::load(&_messages);\n+    const int current_msgs = AtomicAccess::load(&_messages);\n@@ -91,1 +91,1 @@\n-    const int result = Atomic::cmpxchg(&_messages, current_msgs, exchange_value);\n+    const int result = AtomicAccess::cmpxchg(&_messages, current_msgs, exchange_value);\n@@ -119,1 +119,1 @@\n-  const uintptr_t serial_id = Atomic::load(&_msg_read_serial) + 1;\n+  const uintptr_t serial_id = AtomicAccess::load(&_msg_read_serial) + 1;\n@@ -134,1 +134,1 @@\n-  return serial_id <= Atomic::load(&_msg_handled_serial);\n+  return serial_id <= AtomicAccess::load(&_msg_handled_serial);\n@@ -139,1 +139,1 @@\n-  return Atomic::load(&_messages) == 0;\n+  return AtomicAccess::load(&_messages) == 0;\n@@ -144,1 +144,1 @@\n-  const int messages = Atomic::xchg(&_messages, 0);\n+  const int messages = AtomicAccess::xchg(&_messages, 0);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/service\/jfrPostBox.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -70,1 +70,1 @@\n-    if (Atomic::cmpxchg(&_lock, 0, 1) == 0) {\n+    if (AtomicAccess::cmpxchg(&_lock, 0, 1) == 0) {\n","filename":"src\/hotspot\/share\/jfr\/recorder\/service\/jfrRecorderService.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-  return Atomic::load_acquire(&_top);\n+  return AtomicAccess::load_acquire(&_top);\n@@ -76,1 +76,1 @@\n-  Atomic::release_store(&_top, new_top);\n+  AtomicAccess::release_store(&_top, new_top);\n@@ -83,1 +83,1 @@\n-    if (Atomic::cmpxchg(&_top, current_top, TOP_CRITICAL_SECTION) == current_top) {\n+    if (AtomicAccess::cmpxchg(&_top, current_top, TOP_CRITICAL_SECTION) == current_top) {\n@@ -108,1 +108,1 @@\n-  } while (current_id != nullptr || Atomic::cmpxchg(&_identity, current_id, id) != current_id);\n+  } while (current_id != nullptr || AtomicAccess::cmpxchg(&_identity, current_id, id) != current_id);\n@@ -114,1 +114,1 @@\n-  return current_id == nullptr && Atomic::cmpxchg(&_identity, current_id, id) == current_id;\n+  return current_id == nullptr && AtomicAccess::cmpxchg(&_identity, current_id, id) == current_id;\n@@ -126,1 +126,1 @@\n-  Atomic::release_store(&_identity, (const void*)nullptr);\n+  AtomicAccess::release_store(&_identity, (const void*)nullptr);\n@@ -181,1 +181,1 @@\n-  return Atomic::load_acquire(dest);\n+  return AtomicAccess::load_acquire(dest);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrBuffer.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -115,1 +115,1 @@\n-    Atomic::release_store(&_pos, new_pos);\n+    AtomicAccess::release_store(&_pos, new_pos);\n@@ -138,1 +138,1 @@\n-    return end() - Atomic::load_acquire(&_pos);\n+    return end() - AtomicAccess::load_acquire(&_pos);\n@@ -144,1 +144,1 @@\n-    return Atomic::load_acquire(&_pos) == start();\n+    return AtomicAccess::load_acquire(&_pos) == start();\n@@ -148,1 +148,1 @@\n-    return Atomic::load_acquire(&_identity);\n+    return AtomicAccess::load_acquire(&_identity);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrBuffer.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -270,1 +270,1 @@\n-    Atomic::inc(&_free_list_cache_count);\n+    AtomicAccess::inc(&_free_list_cache_count);\n@@ -283,1 +283,1 @@\n-    Atomic::dec(&_free_list_cache_count);\n+    AtomicAccess::dec(&_free_list_cache_count);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrMemorySpace.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -51,1 +51,1 @@\n-  const size_t result = Atomic::add(&_full_count, (size_t)1);\n+  const size_t result = AtomicAccess::add(&_full_count, (size_t)1);\n@@ -62,1 +62,1 @@\n-  } while (Atomic::cmpxchg(&_full_count, current, exchange) != current);\n+  } while (AtomicAccess::cmpxchg(&_full_count, current, exchange) != current);\n@@ -67,1 +67,1 @@\n-  Atomic::store(&_full_count, (size_t)0);\n+  AtomicAccess::store(&_full_count, (size_t)0);\n@@ -79,1 +79,1 @@\n-  return Atomic::load(&_global_lease_count);\n+  return AtomicAccess::load(&_global_lease_count);\n@@ -83,1 +83,1 @@\n-  return Atomic::add(&_global_lease_count, (size_t)1);\n+  return AtomicAccess::add(&_global_lease_count, (size_t)1);\n@@ -92,1 +92,1 @@\n-  } while (Atomic::cmpxchg(&_global_lease_count, current, exchange) != current);\n+  } while (AtomicAccess::cmpxchg(&_global_lease_count, current, exchange) != current);\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrStorageControl.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -52,1 +52,1 @@\n-  return Atomic::load_acquire(t->pos_address()) - top;\n+  return AtomicAccess::load_acquire(t->pos_address()) - top;\n@@ -155,1 +155,1 @@\n-  const size_t unflushed_size = Atomic::load_acquire(t->pos_address()) - current_top;\n+  const size_t unflushed_size = AtomicAccess::load_acquire(t->pos_address()) - current_top;\n","filename":"src\/hotspot\/share\/jfr\/recorder\/storage\/jfrStorageUtils.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/jfr\/recorder\/stringpool\/jfrStringPool.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -92,1 +92,1 @@\n-  return Atomic::load_acquire(&_active_window);\n+  return AtomicAccess::load_acquire(&_active_window);\n@@ -100,1 +100,1 @@\n-  const int64_t end_ticks = Atomic::load(&_end_ticks);\n+  const int64_t end_ticks = AtomicAccess::load(&_end_ticks);\n@@ -111,1 +111,1 @@\n-  const size_t ordinal = Atomic::add(&_measured_population_size, static_cast<size_t>(1));\n+  const size_t ordinal = AtomicAccess::add(&_measured_population_size, static_cast<size_t>(1));\n@@ -142,1 +142,1 @@\n-  Atomic::release_store(&_active_window, next);\n+  AtomicAccess::release_store(&_active_window, next);\n@@ -200,1 +200,1 @@\n-    Atomic::store(&_end_ticks, static_cast<int64_t>(0));\n+    AtomicAccess::store(&_end_ticks, static_cast<int64_t>(0));\n@@ -203,1 +203,1 @@\n-  Atomic::store(&_measured_population_size, static_cast<size_t>(0));\n+  AtomicAccess::store(&_measured_population_size, static_cast<size_t>(0));\n@@ -205,1 +205,1 @@\n-  Atomic::store(&_end_ticks, end_ticks);\n+  AtomicAccess::store(&_end_ticks, end_ticks);\n@@ -282,1 +282,1 @@\n-  return Atomic::load(&_measured_population_size);\n+  return AtomicAccess::load(&_measured_population_size);\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrAdaptiveSampler.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -154,1 +154,1 @@\n-    compare_value = Atomic::load(&num_edges);\n+    compare_value = AtomicAccess::load(&num_edges);\n@@ -158,1 +158,1 @@\n-  } while (compare_value != Atomic::cmpxchg(&num_edges, compare_value, compare_value + 1));\n+  } while (compare_value != AtomicAccess::cmpxchg(&num_edges, compare_value, compare_value + 1));\n@@ -307,1 +307,1 @@\n-  return Atomic::load(&_pending_head);\n+  return AtomicAccess::load(&_pending_head);\n@@ -320,1 +320,1 @@\n-  Atomic::store(&_pending_head, head);\n+  AtomicAccess::store(&_pending_head, head);\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrDeprecationManager.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -325,1 +325,1 @@\n-  return Atomic::load(&_vthread_excluded);\n+  return AtomicAccess::load(&_vthread_excluded);\n@@ -340,1 +340,1 @@\n-  Atomic::store(&tl->_vthread_epoch, static_cast<u2>(0));\n+  AtomicAccess::store(&tl->_vthread_epoch, static_cast<u2>(0));\n@@ -360,1 +360,1 @@\n-  return Atomic::load_acquire(&_vthread) ? is_vthread_excluded(): _jvm_thread_excluded;\n+  return AtomicAccess::load_acquire(&_vthread) ? is_vthread_excluded(): _jvm_thread_excluded;\n@@ -404,1 +404,1 @@\n-  Atomic::store(&jt->jfr_thread_local()->_vthread_epoch, epoch);\n+  AtomicAccess::store(&jt->jfr_thread_local()->_vthread_epoch, epoch);\n@@ -430,1 +430,1 @@\n-  return Atomic::load(&t->jfr_thread_local()->_vthread_id);\n+  return AtomicAccess::load(&t->jfr_thread_local()->_vthread_id);\n@@ -448,1 +448,1 @@\n-  return Atomic::load(&jt->jfr_thread_local()->_vthread_epoch);\n+  return AtomicAccess::load(&jt->jfr_thread_local()->_vthread_epoch);\n@@ -453,2 +453,2 @@\n-  if (Atomic::load(&_generation) != current_generation) {\n-    Atomic::store(&_generation, current_generation);\n+  if (AtomicAccess::load(&_generation) != current_generation) {\n+    AtomicAccess::store(&_generation, current_generation);\n@@ -507,1 +507,1 @@\n-      Atomic::store(&tl->_vthread_id, tid);\n+      AtomicAccess::store(&tl->_vthread_id, tid);\n@@ -528,1 +528,1 @@\n-  return Atomic::load_acquire(&jt->jfr_thread_local()->_vthread) && jt->last_continuation() != nullptr;\n+  return AtomicAccess::load_acquire(&jt->jfr_thread_local()->_vthread) && jt->last_continuation() != nullptr;\n@@ -561,1 +561,1 @@\n-    Atomic::release_store(&tl->_vthread, false);\n+    AtomicAccess::release_store(&tl->_vthread, false);\n@@ -565,1 +565,1 @@\n-  Atomic::store(&tl->_vthread_id, AccessThreadTraceId::id(thread));\n+  AtomicAccess::store(&tl->_vthread_id, AccessThreadTraceId::id(thread));\n@@ -568,1 +568,1 @@\n-  Atomic::store(&tl->_vthread_excluded, excluded);\n+  AtomicAccess::store(&tl->_vthread_excluded, excluded);\n@@ -570,1 +570,1 @@\n-    Atomic::store(&tl->_vthread_epoch, static_cast<u2>(epoch_raw & epoch_mask));\n+    AtomicAccess::store(&tl->_vthread_epoch, static_cast<u2>(epoch_raw & epoch_mask));\n@@ -572,1 +572,1 @@\n-  Atomic::release_store(&tl->_vthread, true);\n+  AtomicAccess::release_store(&tl->_vthread, true);\n@@ -610,1 +610,1 @@\n-  return Atomic::load_acquire(&_cpu_time_jfr_locked) == ENQUEUE;\n+  return AtomicAccess::load_acquire(&_cpu_time_jfr_locked) == ENQUEUE;\n@@ -614,1 +614,1 @@\n-  return Atomic::load_acquire(&_cpu_time_jfr_locked) == DEQUEUE;\n+  return AtomicAccess::load_acquire(&_cpu_time_jfr_locked) == DEQUEUE;\n@@ -618,1 +618,1 @@\n-  return Atomic::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, ENQUEUE) == UNLOCKED;\n+  return AtomicAccess::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, ENQUEUE) == UNLOCKED;\n@@ -624,1 +624,1 @@\n-    CPUTimeLockState got = Atomic::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, DEQUEUE);\n+    CPUTimeLockState got = AtomicAccess::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, DEQUEUE);\n@@ -637,1 +637,1 @@\n-  while (Atomic::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, DEQUEUE) != UNLOCKED) {\n+  while (AtomicAccess::cmpxchg(&_cpu_time_jfr_locked, UNLOCKED, DEQUEUE) != UNLOCKED) {\n@@ -643,1 +643,1 @@\n-  Atomic::release_store(&_cpu_time_jfr_locked, UNLOCKED);\n+  AtomicAccess::release_store(&_cpu_time_jfr_locked, UNLOCKED);\n@@ -647,1 +647,1 @@\n-  Atomic::release_store(&_has_cpu_time_jfr_requests, has_requests);\n+  AtomicAccess::release_store(&_has_cpu_time_jfr_requests, has_requests);\n@@ -651,1 +651,1 @@\n-  return Atomic::load_acquire(&_has_cpu_time_jfr_requests);\n+  return AtomicAccess::load_acquire(&_has_cpu_time_jfr_requests);\n@@ -663,1 +663,1 @@\n-  Atomic::release_store(&_do_async_processing_of_cpu_time_jfr_requests, wants);\n+  AtomicAccess::release_store(&_do_async_processing_of_cpu_time_jfr_requests, wants);\n@@ -667,1 +667,1 @@\n-  return Atomic::load_acquire(&_do_async_processing_of_cpu_time_jfr_requests);\n+  return AtomicAccess::load_acquire(&_do_async_processing_of_cpu_time_jfr_requests);\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrThreadLocal.cpp","additions":26,"deletions":26,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -172,1 +172,1 @@\n-    return Atomic::load_acquire(&_sample_state);\n+    return AtomicAccess::load_acquire(&_sample_state);\n@@ -176,1 +176,1 @@\n-    Atomic::release_store(&_sample_state, state);\n+    AtomicAccess::release_store(&_sample_state, state);\n@@ -212,1 +212,1 @@\n-    return Atomic::load_acquire(&_enqueued_requests);\n+    return AtomicAccess::load_acquire(&_enqueued_requests);\n@@ -219,1 +219,1 @@\n-      Atomic::release_store(&_enqueued_requests, true);\n+      AtomicAccess::release_store(&_enqueued_requests, true);\n@@ -229,1 +229,1 @@\n-    Atomic::release_store(&_enqueued_requests, false);\n+    AtomicAccess::release_store(&_enqueued_requests, false);\n","filename":"src\/hotspot\/share\/jfr\/support\/jfrThreadLocal.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-  return Atomic::load_acquire(&_current);\n+  return AtomicAccess::load_acquire(&_current);\n@@ -86,1 +86,1 @@\n-  add_previous_filter(Atomic::xchg(&_current, new_filter));\n+  add_previous_filter(AtomicAccess::xchg(&_current, new_filter));\n","filename":"src\/hotspot\/share\/jfr\/support\/methodtracer\/jfrFilterManager.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -42,1 +42,1 @@\n-  } while (Atomic::cmpxchg(dest, compare_value, exchange_value) != compare_value);\n+  } while (AtomicAccess::cmpxchg(dest, compare_value, exchange_value) != compare_value);\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrAllocation.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -70,1 +70,1 @@\n-    Node* next = Atomic::load_acquire(&current->_next);\n+    Node* next = AtomicAccess::load_acquire(&current->_next);\n@@ -161,1 +161,1 @@\n-  if (Atomic::load_acquire(&last->_next) == predecessor) {\n+  if (AtomicAccess::load_acquire(&last->_next) == predecessor) {\n@@ -228,1 +228,1 @@\n-  if (last != nullptr && Atomic::load_acquire(&last->_next) == successor) {\n+  if (last != nullptr && AtomicAccess::load_acquire(&last->_next) == successor) {\n@@ -252,1 +252,1 @@\n-  const Node* next = Atomic::load_acquire(&current->_next);\n+  const Node* next = AtomicAccess::load_acquire(&current->_next);\n@@ -277,1 +277,1 @@\n-  NodePtr next = Atomic::load_acquire(&current->_next);\n+  NodePtr next = AtomicAccess::load_acquire(&current->_next);\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrConcurrentLinkedListHost.inline.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,1 @@\n-  return Atomic::load_acquire(&_head._next) == &_tail;\n+  return AtomicAccess::load_acquire(&_head._next) == &_tail;\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrConcurrentQueue.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -62,1 +62,1 @@\n-    return (TableEntry*)Atomic::load_acquire(&_entry);\n+    return (TableEntry*)AtomicAccess::load_acquire(&_entry);\n@@ -64,1 +64,1 @@\n-  void set_entry(TableEntry* entry) { Atomic::release_store(&_entry, entry);}\n+  void set_entry(TableEntry* entry) { AtomicAccess::release_store(&_entry, entry);}\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrHashtable.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -42,1 +42,1 @@\n-  return (NodeType*)Atomic::load_acquire(&_head);\n+  return (NodeType*)AtomicAccess::load_acquire(&_head);\n@@ -62,1 +62,1 @@\n-  } while (Atomic::cmpxchg(&_head, next, node) != next);\n+  } while (AtomicAccess::cmpxchg(&_head, next, node) != next);\n@@ -73,1 +73,1 @@\n-  } while (Atomic::cmpxchg(&_head, node, next) != node);\n+  } while (AtomicAccess::cmpxchg(&_head, node, next) != node);\n@@ -94,1 +94,1 @@\n-    prev = Atomic::cmpxchg(&_head, node, next);\n+    prev = AtomicAccess::cmpxchg(&_head, node, next);\n@@ -126,1 +126,1 @@\n-  } while (Atomic::cmpxchg(&_head, node, (NodeType*)nullptr) != node);\n+  } while (AtomicAccess::cmpxchg(&_head, node, (NodeType*)nullptr) != node);\n@@ -139,1 +139,1 @@\n-  Atomic::store(&_head, first);\n+  AtomicAccess::store(&_head, first);\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrLinkedList.inline.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -38,1 +38,1 @@\n-  return Atomic::cmpxchg(address, current, exchange) == current;\n+  return AtomicAccess::cmpxchg(address, current, exchange) == current;\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrNode.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -115,1 +115,1 @@\n-    Atomic::inc(&_refs, memory_order_relaxed);\n+    AtomicAccess::inc(&_refs, memory_order_relaxed);\n@@ -119,1 +119,1 @@\n-    if (0 == Atomic::sub(&_refs, 1, memory_order_release)) {\n+    if (0 == AtomicAccess::sub(&_refs, 1, memory_order_release)) {\n@@ -127,1 +127,1 @@\n-    return Atomic::load(&_refs);\n+    return AtomicAccess::load(&_refs);\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrRefCountPointer.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -37,1 +37,1 @@\n-    Atomic::release_store(&_signaled, true);\n+    AtomicAccess::release_store(&_signaled, true);\n@@ -41,1 +41,1 @@\n-    Atomic::release_store(&_signaled, false);\n+    AtomicAccess::release_store(&_signaled, false);\n@@ -45,1 +45,1 @@\n-    return Atomic::load_acquire(&_signaled);\n+    return AtomicAccess::load_acquire(&_signaled);\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrSignal.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -39,1 +39,1 @@\n-  JfrTryLock(volatile int* lock) : _lock(lock), _acquired(Atomic::cmpxchg(lock, 0, 1) == 0) {}\n+  JfrTryLock(volatile int* lock) : _lock(lock), _acquired(AtomicAccess::cmpxchg(lock, 0, 1) == 0) {}\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrTryLock.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -53,1 +53,1 @@\n-  return Atomic::load(&_tip._value);\n+  return AtomicAccess::load(&_tip._value);\n@@ -62,1 +62,1 @@\n-  } while (Atomic::cmpxchg(&_tip._value, cmp, xchg) != cmp);\n+  } while (AtomicAccess::cmpxchg(&_tip._value, cmp, xchg) != cmp);\n@@ -70,1 +70,1 @@\n-    if (node->_live || Atomic::cmpxchg(&node->_live, false, true)) {\n+    if (node->_live || AtomicAccess::cmpxchg(&node->_live, false, true)) {\n@@ -83,1 +83,1 @@\n-  } while (Atomic::cmpxchg(&_head, next, node) != next);\n+  } while (AtomicAccess::cmpxchg(&_head, next, node) != next);\n@@ -99,1 +99,1 @@\n-  Atomic::release_store_fence(&_version, version);\n+  AtomicAccess::release_store_fence(&_version, version);\n@@ -133,1 +133,1 @@\n-    const Type checkedout = Atomic::load_acquire(&node->_version);\n+    const Type checkedout = AtomicAccess::load_acquire(&node->_version);\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrVersionSystem.inline.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -364,1 +364,1 @@\n-  if (_first_error_tid == invalid_id && Atomic::cmpxchg(&_first_error_tid, invalid_id, current_thread_id) == invalid_id) {\n+  if (_first_error_tid == invalid_id && AtomicAccess::cmpxchg(&_first_error_tid, invalid_id, current_thread_id) == invalid_id) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmci.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -218,3 +218,3 @@\n-  Atomic::inc(&_count);\n-  Atomic::add(&_codeBlobs_size, cb->size());\n-  Atomic::add(&_codeBlobs_code_size, cb->code_size());\n+  AtomicAccess::inc(&_count);\n+  AtomicAccess::add(&_codeBlobs_size, cb->size());\n+  AtomicAccess::add(&_codeBlobs_code_size, cb->code_size());\n@@ -224,2 +224,2 @@\n-  Atomic::inc(&_methods_compiled);\n-  Atomic::inc(&_global_compilation_ticks);\n+  AtomicAccess::inc(&_methods_compiled);\n+  AtomicAccess::inc(&_global_compilation_ticks);\n@@ -231,1 +231,1 @@\n-    Atomic::inc(&_err_upcalls);\n+    AtomicAccess::inc(&_err_upcalls);\n@@ -260,1 +260,1 @@\n-    Atomic::inc(&_ok_upcalls);\n+    AtomicAccess::inc(&_ok_upcalls);\n@@ -265,1 +265,1 @@\n-  Atomic::inc(&_global_compilation_ticks);\n+  AtomicAccess::inc(&_global_compilation_ticks);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1625,1 +1625,1 @@\n-  if (!report_error && Atomic::cmpxchg(&report_error, 0, 1) == 0) {\n+  if (!report_error && AtomicAccess::cmpxchg(&report_error, 0, 1) == 0) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -157,1 +157,1 @@\n-          jlong old_value = Atomic::cmpxchg((jlong*)handle, (jlong) value, (jlong) (ptr_tag));\n+          jlong old_value = AtomicAccess::cmpxchg((jlong*)handle, (jlong) value, (jlong) (ptr_tag));\n","filename":"src\/hotspot\/share\/jvmci\/metadataHandles.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -321,1 +321,1 @@\n-    Atomic::release_store_fence(&AsyncLogWriter::_instance, self);\n+    AtomicAccess::release_store_fence(&AsyncLogWriter::_instance, self);\n","filename":"src\/hotspot\/share\/logging\/logAsyncWriter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -36,1 +36,1 @@\n-  const char* host_name = Atomic::load_acquire(&_host_name);\n+  const char* host_name = AtomicAccess::load_acquire(&_host_name);\n@@ -41,1 +41,1 @@\n-      const char* old_value = Atomic::cmpxchg(&_host_name, (const char*)nullptr, host_name);\n+      const char* old_value = AtomicAccess::cmpxchg(&_host_name, (const char*)nullptr, host_name);\n","filename":"src\/hotspot\/share\/logging\/logDecorations.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -32,1 +32,1 @@\n-  jint result = Atomic::add(&_active_readers, 1);\n+  jint result = AtomicAccess::add(&_active_readers, 1);\n@@ -38,1 +38,1 @@\n-  jint result = Atomic::add(&_active_readers, -1);\n+  jint result = AtomicAccess::add(&_active_readers, -1);\n@@ -45,1 +45,1 @@\n-  while (Atomic::load(&_active_readers) != 0) {\n+  while (AtomicAccess::load(&_active_readers) != 0) {\n@@ -136,1 +136,1 @@\n-    LogOutputNode* lnode = Atomic::load(&_level_start[l]);\n+    LogOutputNode* lnode = AtomicAccess::load(&_level_start[l]);\n@@ -138,1 +138,1 @@\n-      Atomic::store(&_level_start[l], node);\n+      AtomicAccess::store(&_level_start[l], node);\n@@ -143,3 +143,3 @@\n-  for (LogOutputNode* cur = Atomic::load(&_level_start[LogLevel::Last]); cur != nullptr; cur = Atomic::load(&cur->_next)) {\n-    if (cur != node && Atomic::load(&cur->_next) == node->_next) {\n-      Atomic::store(&cur->_next, node);\n+  for (LogOutputNode* cur = AtomicAccess::load(&_level_start[LogLevel::Last]); cur != nullptr; cur = AtomicAccess::load(&cur->_next)) {\n+    if (cur != node && AtomicAccess::load(&cur->_next) == node->_next) {\n+      AtomicAccess::store(&cur->_next, node);\n","filename":"src\/hotspot\/share\/logging\/logOutputList.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -130,1 +130,1 @@\n-      _current = Atomic::load_acquire(&_current->_next);\n+      _current = AtomicAccess::load_acquire(&_current->_next);\n@@ -146,1 +146,1 @@\n-    return Iterator(this, Atomic::load_acquire(&_level_start[level]));\n+    return Iterator(this, AtomicAccess::load_acquire(&_level_start[level]));\n","filename":"src\/hotspot\/share\/logging\/logOutputList.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  \/\/ the implied memory order of Atomic::add().\n+  \/\/ the implied memory order of AtomicAccess::add().\n","filename":"src\/hotspot\/share\/logging\/logTagSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -44,2 +44,2 @@\n-  julong value = Atomic::load(dest);\n-  Atomic::store(dest, value + add_value);\n+  julong value = AtomicAccess::load(dest);\n+  AtomicAccess::store(dest, value + add_value);\n","filename":"src\/hotspot\/share\/memory\/allocation.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -216,1 +216,1 @@\n-  int         report_full()                      { return Atomic::add(&_full_count, 1); }\n+  int         report_full()                      { return AtomicAccess::add(&_full_count, 1); }\n","filename":"src\/hotspot\/share\/memory\/heap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -542,1 +542,1 @@\n-  if (!Atomic::load(&_success)) {\n+  if (!AtomicAccess::load(&_success)) {\n@@ -550,1 +550,1 @@\n-    Atomic::store(&_success, false);\n+    AtomicAccess::store(&_success, false);\n@@ -561,1 +561,1 @@\n-    Atomic::add(&_missed_count, missed_count);\n+    AtomicAccess::add(&_missed_count, missed_count);\n@@ -563,1 +563,1 @@\n-    Atomic::store(&_success, false);\n+    AtomicAccess::store(&_success, false);\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -322,1 +322,1 @@\n-  size_t value = Atomic::load_acquire(&_capacity_until_GC);\n+  size_t value = AtomicAccess::load_acquire(&_capacity_until_GC);\n@@ -356,1 +356,1 @@\n-  size_t prev_value = Atomic::cmpxchg(&_capacity_until_GC, old_capacity_until_GC, new_value);\n+  size_t prev_value = AtomicAccess::cmpxchg(&_capacity_until_GC, old_capacity_until_GC, new_value);\n@@ -374,1 +374,1 @@\n-  return Atomic::sub(&_capacity_until_GC, v);\n+  return AtomicAccess::sub(&_capacity_until_GC, v);\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -96,1 +96,1 @@\n-  T get() const               { return Atomic::load(&_c); }\n+  T get() const               { return AtomicAccess::load(&_c); }\n@@ -99,1 +99,1 @@\n-    Atomic::inc(&_c, memory_order_relaxed);\n+    AtomicAccess::inc(&_c, memory_order_relaxed);\n@@ -103,1 +103,1 @@\n-    Atomic::dec(&_c, memory_order_relaxed);\n+    AtomicAccess::dec(&_c, memory_order_relaxed);\n@@ -107,1 +107,1 @@\n-    Atomic::add(&_c, v, memory_order_relaxed);\n+    AtomicAccess::add(&_c, v, memory_order_relaxed);\n@@ -111,1 +111,1 @@\n-    Atomic::sub(&_c, v, memory_order_relaxed);\n+    AtomicAccess::sub(&_c, v, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/counters.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -111,1 +111,1 @@\n-#define INCREMENTOR_ATOMIC(name)    static void inc_##name() { Atomic::inc(&_##name); }\n+#define INCREMENTOR_ATOMIC(name)    static void inc_##name() { AtomicAccess::inc(&_##name); }\n","filename":"src\/hotspot\/share\/memory\/metaspace\/internalStats.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -101,1 +101,1 @@\n-  Atomic::release_store(&_first_node, vsn);\n+  AtomicAccess::release_store(&_first_node, vsn);\n@@ -192,1 +192,1 @@\n-  const VirtualSpaceNode* vsn = Atomic::load_acquire(&_first_node);\n+  const VirtualSpaceNode* vsn = AtomicAccess::load_acquire(&_first_node);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceList.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-  Atomic::store(&_has_critical_allocation, true);\n+  AtomicAccess::store(&_has_critical_allocation, true);\n@@ -182,1 +182,1 @@\n-  if (Atomic::load(&_has_critical_allocation)) {\n+  if (AtomicAccess::load(&_has_critical_allocation)) {\n@@ -208,1 +208,1 @@\n-    Atomic::store(&_has_critical_allocation, false);\n+    AtomicAccess::store(&_has_critical_allocation, false);\n","filename":"src\/hotspot\/share\/memory\/metaspaceCriticalAllocation.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -52,2 +52,2 @@\n-    if (!Atomic::load(&reported)) {\n-      if (!Atomic::cmpxchg(&reported, false, true)) {\n+    if (!AtomicAccess::load(&reported)) {\n+      if (!AtomicAccess::cmpxchg(&reported, false, true)) {\n","filename":"src\/hotspot\/share\/memory\/resourceArea.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -741,1 +741,1 @@\n-    next = (int)Atomic::add(&_preallocated_out_of_memory_error_avail_count, -1);\n+    next = (int)AtomicAccess::add(&_preallocated_out_of_memory_error_avail_count, -1);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -126,1 +126,1 @@\n-    if (Atomic::replace_if_null(&_table[index], entry)) {\n+    if (AtomicAccess::replace_if_null(&_table[index], entry)) {\n@@ -253,1 +253,1 @@\n-  return Atomic::replace_if_null(&_next, entry);\n+  return AtomicAccess::replace_if_null(&_next, entry);\n","filename":"src\/hotspot\/share\/nmt\/mallocSiteTable.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/mallocSiteTable.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -53,1 +53,1 @@\n-    size_t old_sz = Atomic::cmpxchg(&_peak_size, peak_sz, size, memory_order_relaxed);\n+    size_t old_sz = AtomicAccess::cmpxchg(&_peak_size, peak_sz, size, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/nmt\/mallocTracker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -64,1 +64,1 @@\n-    size_t cnt = Atomic::add(&_count, size_t(1), memory_order_relaxed);\n+    size_t cnt = AtomicAccess::add(&_count, size_t(1), memory_order_relaxed);\n@@ -66,1 +66,1 @@\n-      size_t sum = Atomic::add(&_size, sz, memory_order_relaxed);\n+      size_t sum = AtomicAccess::add(&_size, sz, memory_order_relaxed);\n@@ -74,1 +74,1 @@\n-    Atomic::dec(&_count, memory_order_relaxed);\n+    AtomicAccess::dec(&_count, memory_order_relaxed);\n@@ -76,1 +76,1 @@\n-      Atomic::sub(&_size, sz, memory_order_relaxed);\n+      AtomicAccess::sub(&_size, sz, memory_order_relaxed);\n@@ -83,1 +83,1 @@\n-      size_t sum = Atomic::add(&_size, size_t(sz), memory_order_relaxed);\n+      size_t sum = AtomicAccess::add(&_size, size_t(sz), memory_order_relaxed);\n@@ -88,2 +88,2 @@\n-  inline size_t count() const { return Atomic::load(&_count); }\n-  inline size_t size()  const { return Atomic::load(&_size);  }\n+  inline size_t count() const { return AtomicAccess::load(&_count); }\n+  inline size_t size()  const { return AtomicAccess::load(&_size);  }\n@@ -92,1 +92,1 @@\n-    return Atomic::load(&_peak_count);\n+    return AtomicAccess::load(&_peak_count);\n@@ -96,1 +96,1 @@\n-    return Atomic::load(&_peak_size);\n+    return AtomicAccess::load(&_peak_size);\n","filename":"src\/hotspot\/share\/nmt\/mallocTracker.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -119,1 +119,1 @@\n-  if (enabled() && Atomic::cmpxchg(&g_final_report_did_run, false, true) == false) {\n+  if (enabled() && AtomicAccess::cmpxchg(&g_final_report_did_run, false, true) == false) {\n","filename":"src\/hotspot\/share\/nmt\/memTracker.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/nmt\/nmtPreInit.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-    size_t old_sz = Atomic::cmpxchg(&_peak_size, peak_sz, size, memory_order_relaxed);\n+    size_t old_sz = AtomicAccess::cmpxchg(&_peak_size, peak_sz, size, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -85,1 +85,1 @@\n-    return Atomic::load(&_peak_size);\n+    return AtomicAccess::load(&_peak_size);\n","filename":"src\/hotspot\/share\/nmt\/virtualMemoryTracker.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -143,1 +143,1 @@\n-  return Atomic::load_acquire(reinterpret_cast<const volatile T*>(addr));\n+  return AtomicAccess::load_acquire(reinterpret_cast<const volatile T*>(addr));\n@@ -151,1 +151,1 @@\n-  return Atomic::load_acquire(reinterpret_cast<const volatile T*>(addr));\n+  return AtomicAccess::load_acquire(reinterpret_cast<const volatile T*>(addr));\n@@ -159,1 +159,1 @@\n-  return Atomic::load(reinterpret_cast<const volatile T*>(addr));\n+  return AtomicAccess::load(reinterpret_cast<const volatile T*>(addr));\n@@ -167,1 +167,1 @@\n-  Atomic::release_store_fence(reinterpret_cast<volatile T*>(addr), value);\n+  AtomicAccess::release_store_fence(reinterpret_cast<volatile T*>(addr), value);\n@@ -175,1 +175,1 @@\n-  Atomic::release_store(reinterpret_cast<volatile T*>(addr), value);\n+  AtomicAccess::release_store(reinterpret_cast<volatile T*>(addr), value);\n@@ -183,1 +183,1 @@\n-  Atomic::store(reinterpret_cast<volatile T*>(addr), value);\n+  AtomicAccess::store(reinterpret_cast<volatile T*>(addr), value);\n@@ -191,4 +191,4 @@\n-  return Atomic::cmpxchg(reinterpret_cast<volatile T*>(addr),\n-                         compare_value,\n-                         new_value,\n-                         memory_order_relaxed);\n+  return AtomicAccess::cmpxchg(reinterpret_cast<volatile T*>(addr),\n+                               compare_value,\n+                               new_value,\n+                               memory_order_relaxed);\n@@ -202,4 +202,4 @@\n-  return Atomic::cmpxchg(reinterpret_cast<volatile T*>(addr),\n-                         compare_value,\n-                         new_value,\n-                         memory_order_conservative);\n+  return AtomicAccess::cmpxchg(reinterpret_cast<volatile T*>(addr),\n+                               compare_value,\n+                               new_value,\n+                               memory_order_conservative);\n@@ -213,2 +213,2 @@\n-  return Atomic::xchg(reinterpret_cast<volatile T*>(addr),\n-                      new_value);\n+  return AtomicAccess::xchg(reinterpret_cast<volatile T*>(addr),\n+                            new_value);\n","filename":"src\/hotspot\/share\/oops\/accessBackend.inline.hpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -136,2 +136,2 @@\n-  T at_acquire(const int i)            { return Atomic::load_acquire(adr_at(i)); }\n-  void release_at_put(int i, T x)      { Atomic::release_store(adr_at(i), x); }\n+  T at_acquire(const int i)            { return AtomicAccess::load_acquire(adr_at(i)); }\n+  void release_at_put(int i, T x)      { AtomicAccess::release_store(adr_at(i), x); }\n","filename":"src\/hotspot\/share\/oops\/array.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -33,1 +33,1 @@\n-  return Atomic::load_acquire(&_higher_dimension);\n+  return AtomicAccess::load_acquire(&_higher_dimension);\n@@ -37,1 +37,1 @@\n-  Atomic::release_store(&_higher_dimension, k);\n+  AtomicAccess::release_store(&_higher_dimension, k);\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/oops\/constMethodFlags.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -280,1 +280,1 @@\n-  Atomic::release_store(adr, k);\n+  AtomicAccess::release_store(adr, k);\n@@ -697,1 +697,1 @@\n-  Atomic::release_store(adr, k);\n+  AtomicAccess::release_store(adr, k);\n@@ -699,3 +699,3 @@\n-  jbyte old_tag = Atomic::cmpxchg((jbyte*)this_cp->tag_addr_at(cp_index),\n-                                  (jbyte)JVM_CONSTANT_UnresolvedClass,\n-                                  (jbyte)JVM_CONSTANT_Class);\n+  jbyte old_tag = AtomicAccess::cmpxchg((jbyte*)this_cp->tag_addr_at(cp_index),\n+                                        (jbyte)JVM_CONSTANT_UnresolvedClass,\n+                                        (jbyte)JVM_CONSTANT_Class);\n@@ -706,1 +706,1 @@\n-    Atomic::store(adr, (Klass*)nullptr);\n+    AtomicAccess::store(adr, (Klass*)nullptr);\n@@ -1038,3 +1038,3 @@\n-    jbyte old_tag = Atomic::cmpxchg((jbyte*)this_cp->tag_addr_at(cp_index),\n-                                    (jbyte)tag.value(),\n-                                    (jbyte)error_tag);\n+    jbyte old_tag = AtomicAccess::cmpxchg((jbyte*)this_cp->tag_addr_at(cp_index),\n+                                          (jbyte)tag.value(),\n+                                          (jbyte)error_tag);\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -44,1 +44,1 @@\n-  return Atomic::load_acquire(adr);\n+  return AtomicAccess::load_acquire(adr);\n","filename":"src\/hotspot\/share\/oops\/constantPool.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/oops\/cpCache.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -176,1 +176,1 @@\n-  Atomic::fetch_then_or(&flags, mask);\n+  AtomicAccess::fetch_then_or(&flags, mask);\n@@ -180,1 +180,1 @@\n-  Atomic::fetch_then_and(&flags, (u1)(~mask));\n+  AtomicAccess::fetch_then_and(&flags, (u1)(~mask));\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1416,1 +1416,1 @@\n-    InstanceKlass* ikls = Atomic::load_acquire(ik);\n+    InstanceKlass* ikls = AtomicAccess::load_acquire(ik);\n@@ -1432,1 +1432,1 @@\n-    Atomic::release_store(addr, ik);\n+    AtomicAccess::release_store(addr, ik);\n@@ -1762,1 +1762,1 @@\n-  OopMapCache* oop_map_cache = Atomic::load_acquire(&_oop_map_cache);\n+  OopMapCache* oop_map_cache = AtomicAccess::load_acquire(&_oop_map_cache);\n@@ -1766,1 +1766,1 @@\n-    OopMapCache* other = Atomic::cmpxchg(&_oop_map_cache, (OopMapCache*)nullptr, oop_map_cache);\n+    OopMapCache* other = AtomicAccess::cmpxchg(&_oop_map_cache, (OopMapCache*)nullptr, oop_map_cache);\n@@ -2393,1 +2393,1 @@\n-  Atomic::release_store(&jmeths[idnum + 1], new_id);\n+  AtomicAccess::release_store(&jmeths[idnum + 1], new_id);\n@@ -2408,1 +2408,1 @@\n-  return Atomic::load_acquire(&_methods_jmethod_ids);\n+  return AtomicAccess::load_acquire(&_methods_jmethod_ids);\n@@ -2412,1 +2412,1 @@\n-  Atomic::release_store(&_methods_jmethod_ids, jmeths);\n+  AtomicAccess::release_store(&_methods_jmethod_ids, jmeths);\n@@ -2451,1 +2451,1 @@\n-  jmethodID id = Atomic::load_acquire(&jmeths[idnum + 1]);\n+  jmethodID id = AtomicAccess::load_acquire(&jmeths[idnum + 1]);\n@@ -2500,1 +2500,1 @@\n-    jmethodID id = Atomic::load_acquire(&jmeths[idnum + 1]);\n+    jmethodID id = AtomicAccess::load_acquire(&jmeths[idnum + 1]);\n@@ -2504,1 +2504,1 @@\n-      Atomic::release_store(&jmeths[idnum + 1], id);\n+      AtomicAccess::release_store(&jmeths[idnum + 1], id);\n@@ -2557,1 +2557,1 @@\n-      InstanceKlass* impl = Atomic::load_acquire(iklass);\n+      InstanceKlass* impl = AtomicAccess::load_acquire(iklass);\n@@ -2560,1 +2560,1 @@\n-        if (Atomic::cmpxchg(iklass, impl, (InstanceKlass*)nullptr) == impl) {\n+        if (AtomicAccess::cmpxchg(iklass, impl, (InstanceKlass*)nullptr) == impl) {\n@@ -4262,1 +4262,1 @@\n-  Atomic::release_store(&_init_state, state);\n+  AtomicAccess::release_store(&_init_state, state);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -509,1 +509,1 @@\n-  JavaThread* init_thread()  { return Atomic::load(&_init_thread); }\n+  JavaThread* init_thread()  { return AtomicAccess::load(&_init_thread); }\n@@ -523,1 +523,1 @@\n-  ClassState  init_state() const           { return Atomic::load_acquire(&_init_state); }\n+  ClassState  init_state() const           { return AtomicAccess::load_acquire(&_init_state); }\n@@ -1065,1 +1065,1 @@\n-    Atomic::store(&_init_thread, thread);\n+    AtomicAccess::store(&_init_thread, thread);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -67,1 +67,1 @@\n-  return Atomic::load_acquire(&_array_klasses);\n+  return AtomicAccess::load_acquire(&_array_klasses);\n@@ -71,1 +71,1 @@\n-  Atomic::release_store(&_array_klasses, k);\n+  AtomicAccess::release_store(&_array_klasses, k);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -125,2 +125,2 @@\n-  void atomic_set_bits(u1 bits)   { Atomic::fetch_then_or(&_status, bits); }\n-  void atomic_clear_bits(u1 bits) { Atomic::fetch_then_and(&_status, (u1)(~bits)); }\n+  void atomic_set_bits(u1 bits)   { AtomicAccess::fetch_then_or(&_status, bits); }\n+  void atomic_clear_bits(u1 bits) { AtomicAccess::fetch_then_and(&_status, (u1)(~bits)); }\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -620,1 +620,1 @@\n-  for (Klass* chain = Atomic::load_acquire(&_subklass);\n+  for (Klass* chain = AtomicAccess::load_acquire(&_subklass);\n@@ -624,1 +624,1 @@\n-       chain = Atomic::load(&chain->_next_sibling))\n+       chain = AtomicAccess::load(&chain->_next_sibling))\n@@ -641,1 +641,1 @@\n-  for (Klass* chain = Atomic::load(&_next_sibling);\n+  for (Klass* chain = AtomicAccess::load(&_next_sibling);\n@@ -643,1 +643,1 @@\n-       chain = Atomic::load(&chain->_next_sibling)) {\n+       chain = AtomicAccess::load(&chain->_next_sibling)) {\n@@ -660,1 +660,1 @@\n-  Atomic::release_store(&_subklass, s);\n+  AtomicAccess::release_store(&_subklass, s);\n@@ -668,1 +668,1 @@\n-  Atomic::store(&_next_sibling, s);\n+  AtomicAccess::store(&_next_sibling, s);\n@@ -687,1 +687,1 @@\n-    Klass* prev_first_subklass = Atomic::load_acquire(&_super->_subklass);\n+    Klass* prev_first_subklass = AtomicAccess::load_acquire(&_super->_subklass);\n@@ -696,1 +696,1 @@\n-    if (Atomic::cmpxchg(&super->_subklass, prev_first_subklass, this) == prev_first_subklass) {\n+    if (AtomicAccess::cmpxchg(&super->_subklass, prev_first_subklass, this) == prev_first_subklass) {\n@@ -706,1 +706,1 @@\n-    Klass* subklass = Atomic::load_acquire(&_subklass);\n+    Klass* subklass = AtomicAccess::load_acquire(&_subklass);\n@@ -711,1 +711,1 @@\n-    Atomic::cmpxchg(&_subklass, subklass, subklass->next_sibling());\n+    AtomicAccess::cmpxchg(&_subklass, subklass, subklass->next_sibling());\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -68,1 +68,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -636,1 +636,1 @@\n-    Atomic::replace_if_null(&method->_method_data, mtd->final_profile());\n+    AtomicAccess::replace_if_null(&method->_method_data, mtd->final_profile());\n@@ -663,1 +663,1 @@\n-  if (!Atomic::replace_if_null(&method->_method_data, method_data)) {\n+  if (!AtomicAccess::replace_if_null(&method->_method_data, method_data)) {\n@@ -714,1 +714,1 @@\n-  return Atomic::replace_if_null(&_method_counters, counters);\n+  return AtomicAccess::replace_if_null(&_method_counters, counters);\n@@ -1352,1 +1352,1 @@\n-  nmethod *code = Atomic::load_acquire(&_code);\n+  nmethod *code = AtomicAccess::load_acquire(&_code);\n@@ -1392,1 +1392,1 @@\n-    Atomic::release_store(&mh->_from_interpreted_entry , mh->get_i2c_entry());\n+    AtomicAccess::release_store(&mh->_from_interpreted_entry , mh->get_i2c_entry());\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -37,1 +37,1 @@\n-  return Atomic::load_acquire(&_from_compiled_entry);\n+  return AtomicAccess::load_acquire(&_from_compiled_entry);\n@@ -41,1 +41,1 @@\n-  return Atomic::load_acquire(&_from_interpreted_entry);\n+  return AtomicAccess::load_acquire(&_from_interpreted_entry);\n@@ -46,1 +46,1 @@\n-  return Atomic::load_acquire(&_code);\n+  return AtomicAccess::load_acquire(&_code);\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -163,1 +163,1 @@\n-      return Atomic::cmpxchg(reinterpret_cast<MethodTrainingData**>(&_method_training_data), cur, td) == cur;\n+      return AtomicAccess::cmpxchg(reinterpret_cast<MethodTrainingData**>(&_method_training_data), cur, td) == cur;\n","filename":"src\/hotspot\/share\/oops\/methodCounters.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -916,1 +916,1 @@\n-      FailedSpeculation* old_fs = Atomic::cmpxchg(cursor, (FailedSpeculation*) nullptr, fs);\n+      FailedSpeculation* old_fs = AtomicAccess::cmpxchg(cursor, (FailedSpeculation*) nullptr, fs);\n@@ -1515,1 +1515,1 @@\n-    \/\/ No need for \"Atomic::load_acquire\" ops,\n+    \/\/ No need for \"AtomicAccess::load_acquire\" ops,\n@@ -1642,1 +1642,1 @@\n-    \/\/ No need for \"Atomic::load_acquire\" ops,\n+    \/\/ No need for \"AtomicAccess::load_acquire\" ops,\n@@ -1863,1 +1863,1 @@\n-  Mutex* lock = Atomic::load_acquire(&_extra_data_lock);\n+  Mutex* lock = AtomicAccess::load_acquire(&_extra_data_lock);\n@@ -1867,1 +1867,1 @@\n-    Mutex* old = Atomic::cmpxchg(&_extra_data_lock, (Mutex*)nullptr, lock);\n+    Mutex* old = AtomicAccess::cmpxchg(&_extra_data_lock, (Mutex*)nullptr, lock);\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -184,1 +184,1 @@\n-    return Atomic::load_acquire(&_header._struct._flags);\n+    return AtomicAccess::load_acquire(&_header._struct._flags);\n@@ -217,1 +217,1 @@\n-    } while (compare_value != Atomic::cmpxchg(&_header._struct._flags, compare_value, static_cast<u1>(compare_value | bit)));\n+    } while (compare_value != AtomicAccess::cmpxchg(&_header._struct._flags, compare_value, static_cast<u1>(compare_value | bit)));\n@@ -232,1 +232,1 @@\n-    } while (compare_value != Atomic::cmpxchg(&_header._struct._flags, compare_value, exchange_value));\n+    } while (compare_value != AtomicAccess::cmpxchg(&_header._struct._flags, compare_value, exchange_value));\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -34,1 +34,1 @@\n-  Atomic::release_store(&_cells[index], value);\n+  AtomicAccess::release_store(&_cells[index], value);\n","filename":"src\/hotspot\/share\/oops\/methodData.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -90,2 +90,2 @@\n-  void atomic_set_bits(u4 bits)   { Atomic::fetch_then_or(&_status, bits); }\n-  void atomic_clear_bits(u4 bits) { Atomic::fetch_then_and(&_status, ~bits); }\n+  void atomic_set_bits(u4 bits)   { AtomicAccess::fetch_then_or(&_status, bits); }\n+  void atomic_clear_bits(u4 bits) { AtomicAccess::fetch_then_and(&_status, ~bits); }\n","filename":"src\/hotspot\/share\/oops\/methodFlags.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -181,1 +181,1 @@\n-address oopDesc::address_field_acquire(int offset) const              { return Atomic::load_acquire(field_addr<address>(offset)); }\n+address oopDesc::address_field_acquire(int offset) const              { return AtomicAccess::load_acquire(field_addr<address>(offset)); }\n@@ -184,1 +184,1 @@\n-void oopDesc::release_address_field_put(int offset, address value)    { Atomic::release_store(field_addr<address>(offset), value); }\n+void oopDesc::release_address_field_put(int offset, address value)    { AtomicAccess::release_store(field_addr<address>(offset), value); }\n@@ -189,2 +189,2 @@\n-Metadata* oopDesc::metadata_field_acquire(int offset) const           { return Atomic::load_acquire(field_addr<Metadata*>(offset)); }\n-void oopDesc::release_metadata_field_put(int offset, Metadata* value) { Atomic::release_store(field_addr<Metadata*>(offset), value); }\n+Metadata* oopDesc::metadata_field_acquire(int offset) const           { return AtomicAccess::load_acquire(field_addr<Metadata*>(offset)); }\n+void oopDesc::release_metadata_field_put(int offset, Metadata* value) { AtomicAccess::release_store(field_addr<Metadata*>(offset), value); }\n@@ -192,2 +192,2 @@\n-jbyte oopDesc::byte_field_acquire(int offset) const                   { return Atomic::load_acquire(field_addr<jbyte>(offset)); }\n-void oopDesc::release_byte_field_put(int offset, jbyte value)         { Atomic::release_store(field_addr<jbyte>(offset), value); }\n+jbyte oopDesc::byte_field_acquire(int offset) const                   { return AtomicAccess::load_acquire(field_addr<jbyte>(offset)); }\n+void oopDesc::release_byte_field_put(int offset, jbyte value)         { AtomicAccess::release_store(field_addr<jbyte>(offset), value); }\n@@ -195,2 +195,2 @@\n-jchar oopDesc::char_field_acquire(int offset) const                   { return Atomic::load_acquire(field_addr<jchar>(offset)); }\n-void oopDesc::release_char_field_put(int offset, jchar value)         { Atomic::release_store(field_addr<jchar>(offset), value); }\n+jchar oopDesc::char_field_acquire(int offset) const                   { return AtomicAccess::load_acquire(field_addr<jchar>(offset)); }\n+void oopDesc::release_char_field_put(int offset, jchar value)         { AtomicAccess::release_store(field_addr<jchar>(offset), value); }\n@@ -198,2 +198,2 @@\n-jboolean oopDesc::bool_field_acquire(int offset) const                { return Atomic::load_acquire(field_addr<jboolean>(offset)); }\n-void oopDesc::release_bool_field_put(int offset, jboolean value)      { Atomic::release_store(field_addr<jboolean>(offset), jboolean(value & 1)); }\n+jboolean oopDesc::bool_field_acquire(int offset) const                { return AtomicAccess::load_acquire(field_addr<jboolean>(offset)); }\n+void oopDesc::release_bool_field_put(int offset, jboolean value)      { AtomicAccess::release_store(field_addr<jboolean>(offset), jboolean(value & 1)); }\n@@ -201,2 +201,2 @@\n-jint oopDesc::int_field_acquire(int offset) const                     { return Atomic::load_acquire(field_addr<jint>(offset)); }\n-void oopDesc::release_int_field_put(int offset, jint value)           { Atomic::release_store(field_addr<jint>(offset), value); }\n+jint oopDesc::int_field_acquire(int offset) const                     { return AtomicAccess::load_acquire(field_addr<jint>(offset)); }\n+void oopDesc::release_int_field_put(int offset, jint value)           { AtomicAccess::release_store(field_addr<jint>(offset), value); }\n@@ -204,2 +204,2 @@\n-jshort oopDesc::short_field_acquire(int offset) const                 { return Atomic::load_acquire(field_addr<jshort>(offset)); }\n-void oopDesc::release_short_field_put(int offset, jshort value)       { Atomic::release_store(field_addr<jshort>(offset), value); }\n+jshort oopDesc::short_field_acquire(int offset) const                 { return AtomicAccess::load_acquire(field_addr<jshort>(offset)); }\n+void oopDesc::release_short_field_put(int offset, jshort value)       { AtomicAccess::release_store(field_addr<jshort>(offset), value); }\n@@ -207,2 +207,2 @@\n-jlong oopDesc::long_field_acquire(int offset) const                   { return Atomic::load_acquire(field_addr<jlong>(offset)); }\n-void oopDesc::release_long_field_put(int offset, jlong value)         { Atomic::release_store(field_addr<jlong>(offset), value); }\n+jlong oopDesc::long_field_acquire(int offset) const                   { return AtomicAccess::load_acquire(field_addr<jlong>(offset)); }\n+void oopDesc::release_long_field_put(int offset, jlong value)         { AtomicAccess::release_store(field_addr<jlong>(offset), value); }\n@@ -210,2 +210,2 @@\n-jfloat oopDesc::float_field_acquire(int offset) const                 { return Atomic::load_acquire(field_addr<jfloat>(offset)); }\n-void oopDesc::release_float_field_put(int offset, jfloat value)       { Atomic::release_store(field_addr<jfloat>(offset), value); }\n+jfloat oopDesc::float_field_acquire(int offset) const                 { return AtomicAccess::load_acquire(field_addr<jfloat>(offset)); }\n+void oopDesc::release_float_field_put(int offset, jfloat value)       { AtomicAccess::release_store(field_addr<jfloat>(offset), value); }\n@@ -213,2 +213,2 @@\n-jdouble oopDesc::double_field_acquire(int offset) const               { return Atomic::load_acquire(field_addr<jdouble>(offset)); }\n-void oopDesc::release_double_field_put(int offset, jdouble value)     { Atomic::release_store(field_addr<jdouble>(offset), value); }\n+jdouble oopDesc::double_field_acquire(int offset) const               { return AtomicAccess::load_acquire(field_addr<jdouble>(offset)); }\n+void oopDesc::release_double_field_put(int offset, jdouble value)     { AtomicAccess::release_store(field_addr<jdouble>(offset), value); }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -54,1 +54,1 @@\n-  return Atomic::load(&_mark);\n+  return AtomicAccess::load(&_mark);\n@@ -58,1 +58,1 @@\n-  return Atomic::load_acquire(&_mark);\n+  return AtomicAccess::load_acquire(&_mark);\n@@ -62,1 +62,1 @@\n-  Atomic::store(&_mark, m);\n+  AtomicAccess::store(&_mark, m);\n@@ -70,1 +70,1 @@\n-  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+  AtomicAccess::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n@@ -74,1 +74,1 @@\n-  Atomic::release_store(&_mark, m);\n+  AtomicAccess::release_store(&_mark, m);\n@@ -78,1 +78,1 @@\n-  return Atomic::cmpxchg(&_mark, old_mark, new_mark);\n+  return AtomicAccess::cmpxchg(&_mark, old_mark, new_mark);\n@@ -82,1 +82,1 @@\n-  return Atomic::cmpxchg(&_mark, old_mark, new_mark, order);\n+  return AtomicAccess::cmpxchg(&_mark, old_mark, new_mark, order);\n@@ -124,1 +124,1 @@\n-      narrowKlass narrow_klass = Atomic::load_acquire(&_metadata._compressed_klass);\n+      narrowKlass narrow_klass = AtomicAccess::load_acquire(&_metadata._compressed_klass);\n@@ -128,1 +128,1 @@\n-      return Atomic::load_acquire(&_metadata._klass);\n+      return AtomicAccess::load_acquire(&_metadata._klass);\n@@ -169,1 +169,1 @@\n-    Atomic::release_store((narrowKlass*)raw_mem,\n+    AtomicAccess::release_store((narrowKlass*)raw_mem,\n@@ -172,1 +172,1 @@\n-    Atomic::release_store((Klass**)raw_mem, k);\n+    AtomicAccess::release_store((Klass**)raw_mem, k);\n@@ -274,2 +274,2 @@\n-inline jint oopDesc::int_field_relaxed(int offset) const            { return Atomic::load(field_addr<jint>(offset)); }\n-inline void oopDesc::int_field_put_relaxed(int offset, jint value)  { Atomic::store(field_addr<jint>(offset), value); }\n+inline jint oopDesc::int_field_relaxed(int offset) const            { return AtomicAccess::load(field_addr<jint>(offset)); }\n+inline void oopDesc::int_field_put_relaxed(int offset, jint value)  { AtomicAccess::store(field_addr<jint>(offset), value); }\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -133,2 +133,2 @@\n-  u1 get_code()                 const { return Atomic::load_acquire(&_get_code);      }\n-  u1 put_code()                 const { return Atomic::load_acquire(&_put_code);      }\n+  u1 get_code()                 const { return AtomicAccess::load_acquire(&_get_code);      }\n+  u1 put_code()                 const { return AtomicAccess::load_acquire(&_put_code);      }\n@@ -167,1 +167,1 @@\n-    Atomic::release_store(code, new_code);\n+    AtomicAccess::release_store(code, new_code);\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-  Method* method()               const { return Atomic::load_acquire(&_method); }\n+  Method* method()               const { return AtomicAccess::load_acquire(&_method); }\n@@ -104,1 +104,1 @@\n-    Atomic::store(&_number_of_parameters, (u2)value);\n+    AtomicAccess::store(&_number_of_parameters, (u2)value);\n@@ -116,1 +116,1 @@\n-    Atomic::release_store(&_method, m);\n+    AtomicAccess::release_store(&_method, m);\n","filename":"src\/hotspot\/share\/oops\/resolvedIndyEntry.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -148,1 +148,1 @@\n-  Method* method() const { return Atomic::load_acquire(&_method); }\n+  Method* method() const { return AtomicAccess::load_acquire(&_method); }\n@@ -167,2 +167,2 @@\n-  u1 bytecode1() const { return Atomic::load_acquire(&_bytecode1); }\n-  u1 bytecode2() const { return Atomic::load_acquire(&_bytecode2); }\n+  u1 bytecode1() const { return AtomicAccess::load_acquire(&_bytecode1); }\n+  u1 bytecode2() const { return AtomicAccess::load_acquire(&_bytecode2); }\n@@ -203,1 +203,1 @@\n-    Atomic::release_store(code, new_code);\n+    AtomicAccess::release_store(code, new_code);\n@@ -215,1 +215,1 @@\n-    Atomic::release_store(&_method, m);\n+    AtomicAccess::release_store(&_method, m);\n","filename":"src\/hotspot\/share\/oops\/resolvedMethodEntry.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -298,1 +298,1 @@\n-      found = Atomic::cmpxchg(&_hash_and_refcount, old_value, old_value + 1);\n+      found = AtomicAccess::cmpxchg(&_hash_and_refcount, old_value, old_value + 1);\n@@ -317,1 +317,1 @@\n-    NOT_PRODUCT(Atomic::inc(&_total_count);)\n+    NOT_PRODUCT(AtomicAccess::inc(&_total_count);)\n@@ -337,1 +337,1 @@\n-      found = Atomic::cmpxchg(&_hash_and_refcount, old_value, old_value - 1);\n+      found = AtomicAccess::cmpxchg(&_hash_and_refcount, old_value, old_value - 1);\n@@ -359,1 +359,1 @@\n-      found = Atomic::cmpxchg(&_hash_and_refcount, old_value, pack_hash_and_refcount(hash, PERM_REFCOUNT));\n+      found = AtomicAccess::cmpxchg(&_hash_and_refcount, old_value, pack_hash_and_refcount(hash, PERM_REFCOUNT));\n","filename":"src\/hotspot\/share\/oops\/symbol.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -36,2 +36,2 @@\n-  uint i = Atomic::add(&_index, 1u) % QueueSize;\n-  Symbol* old = Atomic::xchg(&_queue[i], sym);\n+  uint i = AtomicAccess::add(&_index, 1u) % QueueSize;\n+  Symbol* old = AtomicAccess::xchg(&_queue[i], sym);\n@@ -43,1 +43,1 @@\n-    Symbol* sym = Atomic::xchg(&_queue[i], (Symbol*) nullptr);\n+    Symbol* sym = AtomicAccess::xchg(&_queue[i], (Symbol*) nullptr);\n","filename":"src\/hotspot\/share\/oops\/symbolHandle.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -253,1 +253,1 @@\n-  uint init_deps_left1 = Atomic::sub(&_init_deps_left, 1);\n+  uint init_deps_left1 = AtomicAccess::sub(&_init_deps_left, 1);\n","filename":"src\/hotspot\/share\/oops\/trainingData.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -678,1 +678,1 @@\n-    return Atomic::load_acquire(&_init_deps_left);\n+    return AtomicAccess::load_acquire(&_init_deps_left);\n","filename":"src\/hotspot\/share\/oops\/trainingData.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -159,1 +159,1 @@\n-  return Atomic::load_acquire(byte_at_addr(which));\n+  return AtomicAccess::load_acquire(byte_at_addr(which));\n@@ -162,1 +162,1 @@\n-  Atomic::release_store(byte_at_addr(which), contents);\n+  AtomicAccess::release_store(byte_at_addr(which), contents);\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -5070,1 +5070,1 @@\n-  tty->print_cr(\"No escape = %d, Arg escape = %d, Global escape = %d\", Atomic::load(&_no_escape_counter), Atomic::load(&_arg_escape_counter), Atomic::load(&_global_escape_counter));\n+  tty->print_cr(\"No escape = %d, Arg escape = %d, Global escape = %d\", AtomicAccess::load(&_no_escape_counter), AtomicAccess::load(&_arg_escape_counter), AtomicAccess::load(&_global_escape_counter));\n@@ -5081,1 +5081,1 @@\n-        Atomic::inc(&ConnectionGraph::_no_escape_counter);\n+        AtomicAccess::inc(&ConnectionGraph::_no_escape_counter);\n@@ -5083,1 +5083,1 @@\n-        Atomic::inc(&ConnectionGraph::_arg_escape_counter);\n+        AtomicAccess::inc(&ConnectionGraph::_arg_escape_counter);\n@@ -5085,1 +5085,1 @@\n-        Atomic::inc(&ConnectionGraph::_global_escape_counter);\n+        AtomicAccess::inc(&ConnectionGraph::_global_escape_counter);\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3641,2 +3641,2 @@\n- *   Atomic::store(&tl->_contextual_tid, java_lang_Thread::tid(thread));\n- *   Atomic::store(&tl->_contextual_thread_excluded, is_excluded);\n+ *   AtomicAccess::store(&tl->_contextual_tid, java_lang_Thread::tid(thread));\n+ *   AtomicAccess::store(&tl->_contextual_thread_excluded, is_excluded);\n@@ -3645,1 +3645,1 @@\n- *     Atomic::store(&tl->_vthread_epoch, vthread_epoch);\n+ *     AtomicAccess::store(&tl->_vthread_epoch, vthread_epoch);\n@@ -3647,1 +3647,1 @@\n- *   Atomic::release_store(&tl->_vthread, true);\n+ *   AtomicAccess::release_store(&tl->_vthread, true);\n@@ -3650,1 +3650,1 @@\n- * Atomic::release_store(&tl->_vthread, false);\n+ * AtomicAccess::release_store(&tl->_vthread, false);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -833,1 +833,1 @@\n-    Atomic::inc(&_long_loop_candidates);\n+    AtomicAccess::inc(&_long_loop_candidates);\n@@ -1151,1 +1151,1 @@\n-    Atomic::inc(&_long_loop_nests);\n+    AtomicAccess::inc(&_long_loop_nests);\n@@ -2588,1 +2588,1 @@\n-    Atomic::inc(&_long_loop_counted_loops);\n+    AtomicAccess::inc(&_long_loop_counted_loops);\n","filename":"src\/hotspot\/share\/opto\/loopnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -150,1 +150,1 @@\n-    Atomic::inc(&PhaseMacroExpand::_GC_barriers_removed_counter);\n+    AtomicAccess::inc(&PhaseMacroExpand::_GC_barriers_removed_counter);\n@@ -2394,1 +2394,1 @@\n-          Atomic::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n@@ -2419,1 +2419,1 @@\n-          Atomic::inc(&PhaseMacroExpand::_objs_scalar_replaced_counter);\n+          AtomicAccess::inc(&PhaseMacroExpand::_objs_scalar_replaced_counter);\n@@ -2459,1 +2459,1 @@\n-    Atomic::add(&PhaseMacroExpand::_memory_barriers_removed_counter, membar_before - membar_after);\n+    AtomicAccess::add(&PhaseMacroExpand::_memory_barriers_removed_counter, membar_before - membar_after);\n@@ -2684,4 +2684,4 @@\n-  tty->print(\"Objects scalar replaced = %d, \", Atomic::load(&_objs_scalar_replaced_counter));\n-  tty->print(\"Monitor objects removed = %d, \", Atomic::load(&_monitor_objects_removed_counter));\n-  tty->print(\"GC barriers removed = %d, \", Atomic::load(&_GC_barriers_removed_counter));\n-  tty->print_cr(\"Memory barriers removed = %d\", Atomic::load(&_memory_barriers_removed_counter));\n+  tty->print(\"Objects scalar replaced = %d, \", AtomicAccess::load(&_objs_scalar_replaced_counter));\n+  tty->print(\"Monitor objects removed = %d, \", AtomicAccess::load(&_monitor_objects_removed_counter));\n+  tty->print(\"GC barriers removed = %d, \", AtomicAccess::load(&_GC_barriers_removed_counter));\n+  tty->print_cr(\"Memory barriers removed = %d\", AtomicAccess::load(&_memory_barriers_removed_counter));\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -2209,1 +2209,1 @@\n-  } while (Atomic::cmpxchg(&_named_counters, head, c) != head);\n+  } while (AtomicAccess::cmpxchg(&_named_counters, head, c) != head);\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -412,1 +412,1 @@\n-  Atomic::add(&_stropts_total, encountered);\n+  AtomicAccess::add(&_stropts_total, encountered);\n@@ -685,1 +685,1 @@\n-              Atomic::inc(&_stropts_merged);\n+              AtomicAccess::inc(&_stropts_merged);\n@@ -2044,1 +2044,1 @@\n-  Atomic::inc(&_stropts_replaced);\n+  AtomicAccess::inc(&_stropts_replaced);\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -2951,1 +2951,1 @@\n-  if (Atomic::cmpxchg(&directBufferSupportInitializeStarted, 0, 1) == 0) {\n+  if (AtomicAccess::cmpxchg(&directBufferSupportInitializeStarted, 0, 1) == 0) {\n@@ -3421,1 +3421,1 @@\n-    Atomic::store(a++, *b++);\n+    AtomicAccess::store(a++, *b++);\n@@ -3537,1 +3537,1 @@\n-  \/\/ We're about to use Atomic::xchg for synchronization.  Some Zero\n+  \/\/ We're about to use AtomicAccess::xchg for synchronization.  Some Zero\n@@ -3544,1 +3544,1 @@\n-    jint b = Atomic::xchg(&a, (jint) 0xdeadbeef);\n+    jint b = AtomicAccess::xchg(&a, (jint) 0xdeadbeef);\n@@ -3546,3 +3546,3 @@\n-    void *d = Atomic::xchg(&c, &b);\n-    assert(a == (jint) 0xdeadbeef && b == (jint) 0xcafebabe, \"Atomic::xchg() works\");\n-    assert(c == &b && d == &a, \"Atomic::xchg() works\");\n+    void *d = AtomicAccess::xchg(&c, &b);\n+    assert(a == (jint) 0xdeadbeef && b == (jint) 0xcafebabe, \"AtomicAccess::xchg() works\");\n+    assert(c == &b && d == &a, \"AtomicAccess::xchg() works\");\n@@ -3559,1 +3559,1 @@\n-  \/\/ We use Atomic::xchg rather than Atomic::add\/dec since on some platforms\n+  \/\/ We use AtomicAccess::xchg rather than AtomicAccess::add\/dec since on some platforms\n@@ -3561,2 +3561,2 @@\n-  \/\/ on a multiprocessor Atomic::xchg does not have this problem.\n-  if (Atomic::xchg(&vm_created, IN_PROGRESS) != NOT_CREATED) {\n+  \/\/ on a multiprocessor AtomicAccess::xchg does not have this problem.\n+  if (AtomicAccess::xchg(&vm_created, IN_PROGRESS) != NOT_CREATED) {\n@@ -3571,1 +3571,1 @@\n-  if (Atomic::xchg(&safe_to_recreate_vm, 0) == 0) {\n+  if (AtomicAccess::xchg(&safe_to_recreate_vm, 0) == 0) {\n@@ -3595,1 +3595,1 @@\n-    Atomic::release_store(&vm_created, COMPLETE);\n+    AtomicAccess::release_store(&vm_created, COMPLETE);\n@@ -3661,1 +3661,1 @@\n-    Atomic::release_store(&vm_created, NOT_CREATED);\n+    AtomicAccess::release_store(&vm_created, NOT_CREATED);\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":14,"deletions":14,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -74,1 +74,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -86,1 +86,1 @@\n-  return Atomic::load_acquire(&_next);\n+  return AtomicAccess::load_acquire(&_next);\n","filename":"src\/hotspot\/share\/prims\/jvmtiAgent.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -105,1 +105,1 @@\n-    JvmtiAgent* next = Atomic::load(tail_ptr);\n+    JvmtiAgent* next = AtomicAccess::load(tail_ptr);\n@@ -108,1 +108,1 @@\n-      if (Atomic::cmpxchg(tail_ptr, (JvmtiAgent*)nullptr, agent) != nullptr) {\n+      if (AtomicAccess::cmpxchg(tail_ptr, (JvmtiAgent*)nullptr, agent) != nullptr) {\n@@ -138,1 +138,1 @@\n-  return Atomic::load_acquire(&_head);\n+  return AtomicAccess::load_acquire(&_head);\n","filename":"src\/hotspot\/share\/prims\/jvmtiAgentList.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -331,1 +331,1 @@\n-    return Atomic::load_acquire(&_tag_map);\n+    return AtomicAccess::load_acquire(&_tag_map);\n@@ -335,1 +335,1 @@\n-    Atomic::release_store(&_tag_map, tag_map);\n+    AtomicAccess::release_store(&_tag_map, tag_map);\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnvBase.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -303,1 +303,1 @@\n-    if (!Atomic::replace_if_null(&_jvmti_breakpoints, breakpoints)) {\n+    if (!AtomicAccess::replace_if_null(&_jvmti_breakpoints, breakpoints)) {\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -125,1 +125,1 @@\n-    if (Atomic::replace_if_null(&_owner, self)) {\n+    if (AtomicAccess::replace_if_null(&_owner, self)) {\n@@ -140,1 +140,1 @@\n-    if (_owner == nullptr && Atomic::replace_if_null(&_owner, self)) {\n+    if (_owner == nullptr && AtomicAccess::replace_if_null(&_owner, self)) {\n@@ -157,1 +157,1 @@\n-  Atomic::release_store(&_owner, (Thread*)nullptr);\n+  AtomicAccess::release_store(&_owner, (Thread*)nullptr);\n@@ -327,1 +327,1 @@\n-  \/\/ TODO Atomic::load on _owner field\n+  \/\/ TODO AtomicAccess::load on _owner field\n","filename":"src\/hotspot\/share\/prims\/jvmtiRawMonitor.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -4540,1 +4540,1 @@\n-    u8 result = Atomic::cmpxchg(&_id_counter, id, next_id);\n+    u8 result = AtomicAccess::cmpxchg(&_id_counter, id, next_id);\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -310,1 +310,1 @@\n-      Atomic::store(&_sync_protocol_enabled_permanently, true);\n+      AtomicAccess::store(&_sync_protocol_enabled_permanently, true);\n@@ -350,1 +350,1 @@\n-  Atomic::inc(&_VTMS_transition_disable_for_one_count);\n+  AtomicAccess::inc(&_VTMS_transition_disable_for_one_count);\n@@ -380,1 +380,1 @@\n-    Atomic::inc(&_VTMS_transition_disable_for_all_count);\n+    AtomicAccess::inc(&_VTMS_transition_disable_for_all_count);\n@@ -418,1 +418,1 @@\n-  Atomic::dec(&_VTMS_transition_disable_for_one_count);\n+  AtomicAccess::dec(&_VTMS_transition_disable_for_one_count);\n@@ -438,1 +438,1 @@\n-    Atomic::dec(&_VTMS_transition_disable_for_all_count);\n+    AtomicAccess::dec(&_VTMS_transition_disable_for_all_count);\n","filename":"src\/hotspot\/share\/prims\/jvmtiThreadState.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -104,4 +104,4 @@\n-  static void inc_sync_protocol_enabled_count()      { Atomic::inc(&_sync_protocol_enabled_count); }\n-  static void dec_sync_protocol_enabled_count()      { Atomic::dec(&_sync_protocol_enabled_count); }\n-  static int  sync_protocol_enabled_count()          { return Atomic::load(&_sync_protocol_enabled_count); }\n-  static bool sync_protocol_enabled_permanently()    { return Atomic::load(&_sync_protocol_enabled_permanently); }\n+  static void inc_sync_protocol_enabled_count()      { AtomicAccess::inc(&_sync_protocol_enabled_count); }\n+  static void dec_sync_protocol_enabled_count()      { AtomicAccess::dec(&_sync_protocol_enabled_count); }\n+  static int  sync_protocol_enabled_count()          { return AtomicAccess::load(&_sync_protocol_enabled_count); }\n+  static bool sync_protocol_enabled_permanently()    { return AtomicAccess::load(&_sync_protocol_enabled_permanently); }\n","filename":"src\/hotspot\/share\/prims\/jvmtiThreadState.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -218,1 +218,1 @@\n-  Atomic::inc(&_items_count);\n+  AtomicAccess::inc(&_items_count);\n@@ -222,1 +222,1 @@\n-  Atomic::dec(&_items_count);\n+  AtomicAccess::dec(&_items_count);\n@@ -261,1 +261,1 @@\n-  Atomic::store(&_has_work, true);\n+  AtomicAccess::store(&_has_work, true);\n@@ -266,1 +266,1 @@\n-  return Atomic::load_acquire(&_has_work);\n+  return AtomicAccess::load_acquire(&_has_work);\n@@ -278,1 +278,1 @@\n-  Atomic::release_store(&_has_work, false);\n+  AtomicAccess::release_store(&_has_work, false);\n","filename":"src\/hotspot\/share\/prims\/resolvedMethodTable.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -740,1 +740,1 @@\n-  return Atomic::cmpxchg(addr, e, x);\n+  return AtomicAccess::cmpxchg(addr, e, x);\n@@ -746,1 +746,1 @@\n-  return Atomic::cmpxchg(addr, e, x);\n+  return AtomicAccess::cmpxchg(addr, e, x);\n@@ -761,1 +761,1 @@\n-  return Atomic::cmpxchg(addr, e, x) == e;\n+  return AtomicAccess::cmpxchg(addr, e, x) == e;\n@@ -767,1 +767,1 @@\n-  return Atomic::cmpxchg(addr, e, x) == e;\n+  return AtomicAccess::cmpxchg(addr, e, x) == e;\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -76,1 +76,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -2306,1 +2306,1 @@\n-      Atomic::inc(&_num_threads_completed);\n+      AtomicAccess::inc(&_num_threads_completed);\n@@ -2373,1 +2373,1 @@\n-    while (Atomic::cmpxchg(&_emulated_lock, 0, 1) != 0) {}\n+    while (AtomicAccess::cmpxchg(&_emulated_lock, 0, 1) != 0) {}\n@@ -2380,1 +2380,1 @@\n-  Atomic::store(&_emulated_lock, 0);\n+  AtomicAccess::store(&_emulated_lock, 0);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1,1235 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_RUNTIME_ATOMIC_HPP\n-#define SHARE_RUNTIME_ATOMIC_HPP\n-\n-#include \"memory\/allocation.hpp\"\n-#include \"metaprogramming\/enableIf.hpp\"\n-#include \"metaprogramming\/primitiveConversions.hpp\"\n-#include \"runtime\/orderAccess.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"utilities\/bytes.hpp\"\n-#include \"utilities\/checkedCast.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-#include <type_traits>\n-\n-enum atomic_memory_order {\n-  \/\/ The modes that align with C++11 are intended to\n-  \/\/ follow the same semantics.\n-  memory_order_relaxed = 0,\n-  memory_order_acquire = 2,\n-  memory_order_release = 3,\n-  memory_order_acq_rel = 4,\n-  memory_order_seq_cst = 5,\n-  \/\/ Strong two-way memory barrier.\n-  memory_order_conservative = 8\n-};\n-\n-enum ScopedFenceType {\n-    X_ACQUIRE\n-  , RELEASE_X\n-  , RELEASE_X_FENCE\n-};\n-\n-class Atomic : AllStatic {\n-public:\n-  \/\/ Atomic operations on int64 types are required to be available on\n-  \/\/ all platforms. At a minimum a 64-bit cmpxchg must be available\n-  \/\/ from which other atomic operations can be constructed if needed.\n-  \/\/ The legacy `Abstract_VMVersion::supports_cx8()` function used to\n-  \/\/ indicate if this support existed, allowing for alternative lock-\n-  \/\/ based mechanism to be used. But today this function is required\n-  \/\/ to return true and in the future will be removed entirely.\n-\n-  \/\/ The memory operations that are mentioned with each of the atomic\n-  \/\/ function families come from src\/share\/vm\/runtime\/orderAccess.hpp,\n-  \/\/ e.g., <fence> is described in that file and is implemented by the\n-  \/\/ OrderAccess::fence() function. See that file for the gory details\n-  \/\/ on the Memory Access Ordering Model.\n-\n-  \/\/ All of the atomic operations that imply a read-modify-write action\n-  \/\/ guarantee a two-way memory barrier across that operation. Historically\n-  \/\/ these semantics reflect the strength of atomic operations that are\n-  \/\/ provided on SPARC\/X86. We assume that strength is necessary unless\n-  \/\/ we can prove that a weaker form is sufficiently safe.\n-\n-  \/\/ Atomically store to a location\n-  \/\/ The type T must be either a pointer type convertible to or equal\n-  \/\/ to D, an integral\/enum type equal to D, or a type equal to D that\n-  \/\/ is primitive convertible using PrimitiveConversions.\n-  template<typename D, typename T>\n-  inline static void store(volatile D* dest, T store_value);\n-\n-  template <typename D, typename T>\n-  inline static void release_store(volatile D* dest, T store_value);\n-\n-  template <typename D, typename T>\n-  inline static void release_store_fence(volatile D* dest, T store_value);\n-\n-  \/\/ Atomically load from a location\n-  \/\/ The type T must be either a pointer type, an integral\/enum type,\n-  \/\/ or a type that is primitive convertible using PrimitiveConversions.\n-  template<typename T>\n-  inline static T load(const volatile T* dest);\n-\n-  template <typename T>\n-  inline static T load_acquire(const volatile T* dest);\n-\n-  \/\/ Atomically add to a location. *add*() provide:\n-  \/\/ <fence> add-value-to-dest <membar StoreLoad|StoreStore>\n-\n-  \/\/ Returns updated value.\n-  template<typename D, typename I>\n-  inline static D add(D volatile* dest, I add_value,\n-                      atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Returns previous value.\n-  template<typename D, typename I>\n-  inline static D fetch_then_add(D volatile* dest, I add_value,\n-                                 atomic_memory_order order = memory_order_conservative);\n-\n-  template<typename D, typename I>\n-  inline static D sub(D volatile* dest, I sub_value,\n-                      atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Atomically increment location. inc() provide:\n-  \/\/ <fence> increment-dest <membar StoreLoad|StoreStore>\n-  \/\/ The type D may be either a pointer type, or an integral\n-  \/\/ type. If it is a pointer type, then the increment is\n-  \/\/ scaled to the size of the type pointed to by the pointer.\n-  template<typename D>\n-  inline static void inc(D volatile* dest,\n-                         atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Atomically decrement a location. dec() provide:\n-  \/\/ <fence> decrement-dest <membar StoreLoad|StoreStore>\n-  \/\/ The type D may be either a pointer type, or an integral\n-  \/\/ type. If it is a pointer type, then the decrement is\n-  \/\/ scaled to the size of the type pointed to by the pointer.\n-  template<typename D>\n-  inline static void dec(D volatile* dest,\n-                         atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Performs atomic exchange of *dest with exchange_value. Returns old\n-  \/\/ prior value of *dest. xchg*() provide:\n-  \/\/ <fence> exchange-value-with-dest <membar StoreLoad|StoreStore>\n-  \/\/ The type T must be either a pointer type convertible to or equal\n-  \/\/ to D, an integral\/enum type equal to D, or a type equal to D that\n-  \/\/ is primitive convertible using PrimitiveConversions.\n-  template<typename D, typename T>\n-  inline static D xchg(volatile D* dest, T exchange_value,\n-                       atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Performs atomic compare of *dest and compare_value, and exchanges\n-  \/\/ *dest with exchange_value if the comparison succeeded. Returns prior\n-  \/\/ value of *dest. cmpxchg*() provide:\n-  \/\/ <fence> compare-and-exchange <membar StoreLoad|StoreStore>\n-\n-  template<typename D, typename U, typename T>\n-  inline static D cmpxchg(D volatile* dest,\n-                          U compare_value,\n-                          T exchange_value,\n-                          atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Performs atomic compare of *dest and nullptr, and replaces *dest\n-  \/\/ with exchange_value if the comparison succeeded.  Returns true if\n-  \/\/ the comparison succeeded and the exchange occurred.  This is\n-  \/\/ often used as part of lazy initialization, as a lock-free\n-  \/\/ alternative to the Double-Checked Locking Pattern.\n-  template<typename D, typename T>\n-  inline static bool replace_if_null(D* volatile* dest, T* value,\n-                                     atomic_memory_order order = memory_order_conservative);\n-\n-  \/\/ Bitwise logical operations (and, or, xor)\n-  \/\/\n-  \/\/ All operations apply the corresponding operation to the value in dest and\n-  \/\/ bits, storing the result in dest. They return either the old value\n-  \/\/ (fetch_then_BITOP) or the newly updated value (BITOP_then_fetch).\n-  \/\/\n-  \/\/ Requirements:\n-  \/\/ - T is an integral type\n-  \/\/ - sizeof(T) == 1 || sizeof(T) == sizeof(int) || sizeof(T) == sizeof(void*)\n-\n-  \/\/ Performs atomic bitwise-and of *dest and bits, storing the result in\n-  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n-  \/\/ this sequence of operations:\n-  \/\/ { tmp = *dest; *dest &= bits; return tmp; }\n-  template<typename T>\n-  static T fetch_then_and(volatile T* dest, T bits,\n-                          atomic_memory_order order = memory_order_conservative) {\n-    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n-    return PlatformBitops<sizeof(T)>().fetch_then_and(dest, bits, order);\n-  }\n-\n-  \/\/ Performs atomic bitwise-or of *dest and bits, storing the result in\n-  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n-  \/\/ this sequence of operations:\n-  \/\/ { tmp = *dest; *dest |= bits; return tmp; }\n-  template<typename T>\n-  static T fetch_then_or(volatile T* dest, T bits,\n-                         atomic_memory_order order = memory_order_conservative) {\n-    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n-    return PlatformBitops<sizeof(T)>().fetch_then_or(dest, bits, order);\n-  }\n-\n-  \/\/ Performs atomic bitwise-xor of *dest and bits, storing the result in\n-  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n-  \/\/ this sequence of operations:\n-  \/\/ { tmp = *dest; *dest ^= bits; return tmp; }\n-  template<typename T>\n-  static T fetch_then_xor(volatile T* dest, T bits,\n-                          atomic_memory_order order = memory_order_conservative) {\n-    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n-    return PlatformBitops<sizeof(T)>().fetch_then_xor(dest, bits, order);\n-  }\n-\n-  \/\/ Performs atomic bitwise-and of *dest and bits, storing the result in\n-  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n-  \/\/ this operation:\n-  \/\/ { return *dest &= bits; }\n-  template<typename T>\n-  static T and_then_fetch(volatile T* dest, T bits,\n-                          atomic_memory_order order = memory_order_conservative) {\n-    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n-    return PlatformBitops<sizeof(T)>().and_then_fetch(dest, bits, order);\n-  }\n-\n-  \/\/ Performs atomic bitwise-or of *dest and bits, storing the result in\n-  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n-  \/\/ this operation:\n-  \/\/ { return *dest |= bits; }\n-  template<typename T>\n-  static T or_then_fetch(volatile T* dest, T bits,\n-                         atomic_memory_order order = memory_order_conservative) {\n-    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n-    return PlatformBitops<sizeof(T)>().or_then_fetch(dest, bits, order);\n-  }\n-\n-  \/\/ Performs atomic bitwise-xor of *dest and bits, storing the result in\n-  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n-  \/\/ this operation:\n-  \/\/ { return *dest ^= bits; }\n-  template<typename T>\n-  static T xor_then_fetch(volatile T* dest, T bits,\n-                          atomic_memory_order order = memory_order_conservative) {\n-    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n-    return PlatformBitops<sizeof(T)>().xor_then_fetch(dest, bits, order);\n-  }\n-\n-private:\n-  \/\/ Test whether From is implicitly convertible to To.\n-  \/\/ From and To must be pointer types.\n-  \/\/ Note: Provides the limited subset of C++11 std::is_convertible\n-  \/\/ that is needed here.\n-  template<typename From, typename To> struct IsPointerConvertible;\n-\n-protected:\n-  \/\/ Dispatch handler for store.  Provides type-based validity\n-  \/\/ checking and limited conversions around calls to the platform-\n-  \/\/ specific implementation layer provided by PlatformOp.\n-  template<typename D, typename T, typename PlatformOp, typename Enable = void>\n-  struct StoreImpl;\n-\n-  \/\/ Platform-specific implementation of store.  Support for sizes\n-  \/\/ of 1, 2, 4, and (if different) pointer size bytes are required.\n-  \/\/ The class is a function object that must be default constructable,\n-  \/\/ with these requirements:\n-  \/\/\n-  \/\/ either:\n-  \/\/ - dest is of type D*, an integral, enum or pointer type.\n-  \/\/ - new_value are of type T, an integral, enum or pointer type D or\n-  \/\/   pointer type convertible to D.\n-  \/\/ or:\n-  \/\/ - T and D are the same and are primitive convertible using PrimitiveConversions\n-  \/\/ and either way:\n-  \/\/ - platform_store is an object of type PlatformStore<sizeof(T)>.\n-  \/\/\n-  \/\/ Then\n-  \/\/   platform_store(new_value, dest)\n-  \/\/ must be a valid expression.\n-  \/\/\n-  \/\/ The default implementation is a volatile store. If a platform\n-  \/\/ requires more for e.g. 64 bit stores, a specialization is required\n-  template<size_t byte_size> struct PlatformStore;\n-\n-  \/\/ Dispatch handler for load.  Provides type-based validity\n-  \/\/ checking and limited conversions around calls to the platform-\n-  \/\/ specific implementation layer provided by PlatformOp.\n-  template<typename T, typename PlatformOp, typename Enable = void>\n-  struct LoadImpl;\n-\n-  \/\/ Platform-specific implementation of load. Support for sizes of\n-  \/\/ 1, 2, 4 bytes and (if different) pointer size bytes are required.\n-  \/\/ The class is a function object that must be default\n-  \/\/ constructable, with these requirements:\n-  \/\/\n-  \/\/ - dest is of type T*, an integral, enum or pointer type, or\n-  \/\/   T is convertible to a primitive type using PrimitiveConversions\n-  \/\/ - platform_load is an object of type PlatformLoad<sizeof(T)>.\n-  \/\/\n-  \/\/ Then\n-  \/\/   platform_load(src)\n-  \/\/ must be a valid expression, returning a result convertible to T.\n-  \/\/\n-  \/\/ The default implementation is a volatile load. If a platform\n-  \/\/ requires more for e.g. 64 bit loads, a specialization is required\n-  template<size_t byte_size> struct PlatformLoad;\n-\n-  \/\/ Give platforms a variation point to specialize.\n-  template<size_t byte_size, ScopedFenceType type> struct PlatformOrderedStore;\n-  template<size_t byte_size, ScopedFenceType type> struct PlatformOrderedLoad;\n-\n-private:\n-  \/\/ Dispatch handler for add.  Provides type-based validity checking\n-  \/\/ and limited conversions around calls to the platform-specific\n-  \/\/ implementation layer provided by PlatformAdd.\n-  template<typename D, typename I, typename Enable = void>\n-  struct AddImpl;\n-\n-  \/\/ Platform-specific implementation of add.  Support for sizes of 4\n-  \/\/ bytes and (if different) pointer size bytes are required.  The\n-  \/\/ class must be default constructable, with these requirements:\n-  \/\/\n-  \/\/ - dest is of type D*, where D is an integral or pointer type.\n-  \/\/ - add_value is of type I, an integral type.\n-  \/\/ - sizeof(I) == sizeof(D).\n-  \/\/ - if D is an integral type, I == D.\n-  \/\/ - if D is a pointer type P*, sizeof(P) == 1.\n-  \/\/ - order is of type atomic_memory_order.\n-  \/\/ - platform_add is an object of type PlatformAdd<sizeof(D)>.\n-  \/\/\n-  \/\/ Then both\n-  \/\/   platform_add.add_then_fetch(dest, add_value, order)\n-  \/\/   platform_add.fetch_then_add(dest, add_value, order)\n-  \/\/ must be valid expressions returning a result convertible to D.\n-  \/\/\n-  \/\/ add_then_fetch atomically adds add_value to the value of dest,\n-  \/\/ returning the new value.\n-  \/\/\n-  \/\/ fetch_then_add atomically adds add_value to the value of dest,\n-  \/\/ returning the old value.\n-  \/\/\n-  \/\/ When the destination type D of the Atomic operation is a pointer type P*,\n-  \/\/ the addition must scale the add_value by sizeof(P) to add that many bytes\n-  \/\/ to the destination value.  Rather than requiring each platform deal with\n-  \/\/ this, the shared part of the implementation performs some adjustments\n-  \/\/ before and after calling the platform operation.  It ensures the pointee\n-  \/\/ type of the destination value passed to the platform operation has size\n-  \/\/ 1, casting if needed.  It also scales add_value by sizeof(P).  The result\n-  \/\/ of the platform operation is cast back to P*.  This means the platform\n-  \/\/ operation does not need to account for the scaling.  It also makes it\n-  \/\/ easy for the platform to implement one of add_then_fetch or fetch_then_add\n-  \/\/ in terms of the other (which is a common approach).\n-  \/\/\n-  \/\/ No definition is provided; all platforms must explicitly define\n-  \/\/ this class and any needed specializations.\n-  template<size_t byte_size> struct PlatformAdd;\n-\n-  \/\/ Support for platforms that implement some variants of add using a\n-  \/\/ (typically out of line) non-template helper function.  The\n-  \/\/ generic arguments passed to PlatformAdd need to be translated to\n-  \/\/ the appropriate type for the helper function, the helper function\n-  \/\/ invoked on the translated arguments, and the result translated\n-  \/\/ back.  Type is the parameter \/ return type of the helper\n-  \/\/ function.  No scaling of add_value is performed when D is a pointer\n-  \/\/ type, so this function can be used to implement the support function\n-  \/\/ required by AddAndFetch.\n-  template<typename Type, typename Fn, typename D, typename I>\n-  static D add_using_helper(Fn fn, D volatile* dest, I add_value);\n-\n-  \/\/ Dispatch handler for cmpxchg.  Provides type-based validity\n-  \/\/ checking and limited conversions around calls to the\n-  \/\/ platform-specific implementation layer provided by\n-  \/\/ PlatformCmpxchg.\n-  template<typename D, typename U, typename T, typename Enable = void>\n-  struct CmpxchgImpl;\n-\n-  \/\/ Platform-specific implementation of cmpxchg.  Support for sizes\n-  \/\/ of 1, 4, and 8 are required.  The class is a function object that\n-  \/\/ must be default constructable, with these requirements:\n-  \/\/\n-  \/\/ - dest is of type T*.\n-  \/\/ - exchange_value and compare_value are of type T.\n-  \/\/ - order is of type atomic_memory_order.\n-  \/\/ - platform_cmpxchg is an object of type PlatformCmpxchg<sizeof(T)>.\n-  \/\/\n-  \/\/ Then\n-  \/\/   platform_cmpxchg(dest, compare_value, exchange_value, order)\n-  \/\/ must be a valid expression, returning a result convertible to T.\n-  \/\/\n-  \/\/ A default definition is provided, which declares a function template\n-  \/\/   T operator()(T volatile*, T, T, atomic_memory_order) const\n-  \/\/\n-  \/\/ For each required size, a platform must either provide an\n-  \/\/ appropriate definition of that function, or must entirely\n-  \/\/ specialize the class template for that size.\n-  template<size_t byte_size> struct PlatformCmpxchg;\n-\n-  \/\/ Support for platforms that implement some variants of cmpxchg\n-  \/\/ using a (typically out of line) non-template helper function.\n-  \/\/ The generic arguments passed to PlatformCmpxchg need to be\n-  \/\/ translated to the appropriate type for the helper function, the\n-  \/\/ helper invoked on the translated arguments, and the result\n-  \/\/ translated back.  Type is the parameter \/ return type of the\n-  \/\/ helper function.\n-  template<typename Type, typename Fn, typename T>\n-  static T cmpxchg_using_helper(Fn fn,\n-                                T volatile* dest,\n-                                T compare_value,\n-                                T exchange_value);\n-\n-  \/\/ Support platforms that do not provide Read-Modify-Write atomic\n-  \/\/ accesses for 1-byte and 8-byte widths. To use, derive PlatformCmpxchg<1>,\n-  \/\/ PlatformAdd<S>, PlatformXchg<S> from these classes.\n-public: \/\/ Temporary, can't be private: C++03 11.4\/2. Fixed by C++11.\n-  struct CmpxchgByteUsingInt;\n-  template<size_t byte_size>\n-  struct XchgUsingCmpxchg;\n-  template<size_t byte_size>\n-  class AddUsingCmpxchg;\n-private:\n-\n-  \/\/ Dispatch handler for xchg.  Provides type-based validity\n-  \/\/ checking and limited conversions around calls to the\n-  \/\/ platform-specific implementation layer provided by\n-  \/\/ PlatformXchg.\n-  template<typename D, typename T, typename Enable = void>\n-  struct XchgImpl;\n-\n-  \/\/ Platform-specific implementation of xchg.  Support for sizes\n-  \/\/ of 4, and sizeof(intptr_t) are required.  The class is a function\n-  \/\/ object that must be default constructable, with these requirements:\n-  \/\/\n-  \/\/ - dest is of type T*.\n-  \/\/ - exchange_value is of type T.\n-  \/\/ - platform_xchg is an object of type PlatformXchg<sizeof(T)>.\n-  \/\/\n-  \/\/ Then\n-  \/\/   platform_xchg(dest, exchange_value)\n-  \/\/ must be a valid expression, returning a result convertible to T.\n-  \/\/\n-  \/\/ A default definition is provided, which declares a function template\n-  \/\/   T operator()(T volatile*, T, atomic_memory_order) const\n-  \/\/\n-  \/\/ For each required size, a platform must either provide an\n-  \/\/ appropriate definition of that function, or must entirely\n-  \/\/ specialize the class template for that size.\n-  template<size_t byte_size> struct PlatformXchg;\n-\n-  \/\/ Support for platforms that implement some variants of xchg\n-  \/\/ using a (typically out of line) non-template helper function.\n-  \/\/ The generic arguments passed to PlatformXchg need to be\n-  \/\/ translated to the appropriate type for the helper function, the\n-  \/\/ helper invoked on the translated arguments, and the result\n-  \/\/ translated back.  Type is the parameter \/ return type of the\n-  \/\/ helper function.\n-  template<typename Type, typename Fn, typename T>\n-  static T xchg_using_helper(Fn fn,\n-                             T volatile* dest,\n-                             T exchange_value);\n-\n-  \/\/ Platform-specific implementation of the bitops (and, or, xor).  Support\n-  \/\/ for sizes of 4 bytes and (if different) pointer size bytes are required.\n-  \/\/ The class is a function object that must be default constructable, with\n-  \/\/ these requirements:\n-  \/\/\n-  \/\/ - T is an integral type.\n-  \/\/ - dest is of type T*.\n-  \/\/ - bits is of type T.\n-  \/\/ - order is of type atomic_memory_order.\n-  \/\/ - platform_bitops is an object of type PlatformBitops<sizeof(T)>.\n-  \/\/\n-  \/\/ Then\n-  \/\/  platform_bitops.fetch_then_and(dest, bits, order)\n-  \/\/  platform_bitops.fetch_then_or(dest, bits, order)\n-  \/\/  platform_bitops.fetch_then_xor(dest, bits, order)\n-  \/\/  platform_bitops.and_then_fetch(dest, bits, order)\n-  \/\/  platform_bitops.or_then_fetch(dest, bits, order)\n-  \/\/  platform_bitops.xor_then_fetch(dest, bits, order)\n-  \/\/ must all be valid expressions, returning a result convertible to T.\n-  \/\/\n-  \/\/ A default definition is provided, which implements all of the operations\n-  \/\/ using cmpxchg.\n-  \/\/\n-  \/\/ For each required size, a platform must either use the default or\n-  \/\/ entirely specialize the class for that size by providing all of the\n-  \/\/ required operations.\n-  \/\/\n-  \/\/ The second (bool) template parameter allows platforms to provide a\n-  \/\/ partial specialization with a parameterized size, and is otherwise\n-  \/\/ unused.  The default value for that bool parameter means specializations\n-  \/\/ don't need to mention it.\n-  template<size_t size, bool = true> class PlatformBitops;\n-\n-  \/\/ Helper base classes that may be used to implement PlatformBitops.\n-  class PrefetchBitopsUsingCmpxchg;\n-  class PostfetchBitopsUsingCmpxchg;\n-  class PostfetchBitopsUsingPrefetch;\n-};\n-\n-template<typename From, typename To>\n-struct Atomic::IsPointerConvertible<From*, To*> : AllStatic {\n-  \/\/ Determine whether From* is implicitly convertible to To*, using\n-  \/\/ the \"sizeof trick\".\n-  typedef char yes;\n-  typedef char (&no)[2];\n-\n-  static yes test(To*);\n-  static no test(...);\n-  static From* test_value;\n-\n-  static const bool value = (sizeof(yes) == sizeof(test(test_value)));\n-};\n-\n-\/\/ Handle load for pointer and integral types.\n-template<typename T, typename PlatformOp>\n-struct Atomic::LoadImpl<\n-  T,\n-  PlatformOp,\n-  typename EnableIf<std::is_integral<T>::value || std::is_pointer<T>::value>::type>\n-{\n-  T operator()(T const volatile* dest) const {\n-    \/\/ Forward to the platform handler for the size of T.\n-    return PlatformOp()(dest);\n-  }\n-};\n-\n-\/\/ Handle load for types that have a translator.\n-\/\/\n-\/\/ All the involved types must be identical.\n-\/\/\n-\/\/ This translates the original call into a call on the decayed\n-\/\/ arguments, and returns the recovered result of that translated\n-\/\/ call.\n-template<typename T, typename PlatformOp>\n-struct Atomic::LoadImpl<\n-  T,\n-  PlatformOp,\n-  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n-{\n-  T operator()(T const volatile* dest) const {\n-    typedef PrimitiveConversions::Translate<T> Translator;\n-    typedef typename Translator::Decayed Decayed;\n-    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n-    Decayed result = PlatformOp()(reinterpret_cast<Decayed const volatile*>(dest));\n-    return Translator::recover(result);\n-  }\n-};\n-\n-\/\/ Default implementation of atomic load if a specific platform\n-\/\/ does not provide a specialization for a certain size class.\n-\/\/ For increased safety, the default implementation only allows\n-\/\/ load types that are pointer sized or smaller. If a platform still\n-\/\/ supports wide atomics, then it has to use specialization\n-\/\/ of Atomic::PlatformLoad for that wider size class.\n-template<size_t byte_size>\n-struct Atomic::PlatformLoad {\n-  template<typename T>\n-  T operator()(T const volatile* dest) const {\n-    STATIC_ASSERT(sizeof(T) <= sizeof(void*)); \/\/ wide atomics need specialization\n-    return *dest;\n-  }\n-};\n-\n-\/\/ Handle store for integral types.\n-\/\/\n-\/\/ All the involved types must be identical.\n-template<typename T, typename PlatformOp>\n-struct Atomic::StoreImpl<\n-  T, T,\n-  PlatformOp,\n-  typename EnableIf<std::is_integral<T>::value>::type>\n-{\n-  void operator()(T volatile* dest, T new_value) const {\n-    \/\/ Forward to the platform handler for the size of T.\n-    PlatformOp()(dest, new_value);\n-  }\n-};\n-\n-\/\/ Handle store for pointer types.\n-\/\/\n-\/\/ The new_value must be implicitly convertible to the\n-\/\/ destination's type; it must be type-correct to store the\n-\/\/ new_value in the destination.\n-template<typename D, typename T, typename PlatformOp>\n-struct Atomic::StoreImpl<\n-  D*, T*,\n-  PlatformOp,\n-  typename EnableIf<Atomic::IsPointerConvertible<T*, D*>::value>::type>\n-{\n-  void operator()(D* volatile* dest, T* new_value) const {\n-    \/\/ Allow derived to base conversion, and adding cv-qualifiers.\n-    D* value = new_value;\n-    PlatformOp()(dest, value);\n-  }\n-};\n-\n-\/\/ Handle store for types that have a translator.\n-\/\/\n-\/\/ All the involved types must be identical.\n-\/\/\n-\/\/ This translates the original call into a call on the decayed\n-\/\/ arguments.\n-template<typename T, typename PlatformOp>\n-struct Atomic::StoreImpl<\n-  T, T,\n-  PlatformOp,\n-  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n-{\n-  void operator()(T volatile* dest, T new_value) const {\n-    typedef PrimitiveConversions::Translate<T> Translator;\n-    typedef typename Translator::Decayed Decayed;\n-    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n-    PlatformOp()(reinterpret_cast<Decayed volatile*>(dest),\n-                 Translator::decay(new_value));\n-  }\n-};\n-\n-\/\/ Default implementation of atomic store if a specific platform\n-\/\/ does not provide a specialization for a certain size class.\n-\/\/ For increased safety, the default implementation only allows\n-\/\/ storing types that are pointer sized or smaller. If a platform still\n-\/\/ supports wide atomics, then it has to use specialization\n-\/\/ of Atomic::PlatformStore for that wider size class.\n-template<size_t byte_size>\n-struct Atomic::PlatformStore {\n-  template<typename T>\n-  void operator()(T volatile* dest,\n-                  T new_value) const {\n-    STATIC_ASSERT(sizeof(T) <= sizeof(void*)); \/\/ wide atomics need specialization\n-    (void)const_cast<T&>(*dest = new_value);\n-  }\n-};\n-\n-template<typename D>\n-inline void Atomic::inc(D volatile* dest, atomic_memory_order order) {\n-  STATIC_ASSERT(std::is_pointer<D>::value || std::is_integral<D>::value);\n-  using I = std::conditional_t<std::is_pointer<D>::value, ptrdiff_t, D>;\n-  Atomic::add(dest, I(1), order);\n-}\n-\n-template<typename D>\n-inline void Atomic::dec(D volatile* dest, atomic_memory_order order) {\n-  STATIC_ASSERT(std::is_pointer<D>::value || std::is_integral<D>::value);\n-  using I = std::conditional_t<std::is_pointer<D>::value, ptrdiff_t, D>;\n-  \/\/ Assumes two's complement integer representation.\n-  #pragma warning(suppress: 4146)\n-  Atomic::add(dest, I(-1), order);\n-}\n-\n-template<typename D, typename I>\n-inline D Atomic::sub(D volatile* dest, I sub_value, atomic_memory_order order) {\n-  STATIC_ASSERT(std::is_pointer<D>::value || std::is_integral<D>::value);\n-  STATIC_ASSERT(std::is_integral<I>::value);\n-  \/\/ If D is a pointer type, use [u]intptr_t as the addend type,\n-  \/\/ matching signedness of I.  Otherwise, use D as the addend type.\n-  using PI = std::conditional_t<std::is_signed<I>::value, intptr_t, uintptr_t>;\n-  using AddendType = std::conditional_t<std::is_pointer<D>::value, PI, D>;\n-  \/\/ Only allow conversions that can't change the value.\n-  STATIC_ASSERT(std::is_signed<I>::value == std::is_signed<AddendType>::value);\n-  STATIC_ASSERT(sizeof(I) <= sizeof(AddendType));\n-  AddendType addend = sub_value;\n-  \/\/ Assumes two's complement integer representation.\n-  #pragma warning(suppress: 4146) \/\/ In case AddendType is not signed.\n-  return Atomic::add(dest, -addend, order);\n-}\n-\n-\/\/ Define the class before including platform file, which may specialize\n-\/\/ the operator definition.  No generic definition of specializations\n-\/\/ of the operator template are provided, nor are there any generic\n-\/\/ specializations of the class.  The platform file is responsible for\n-\/\/ providing those.\n-template<size_t byte_size>\n-struct Atomic::PlatformCmpxchg {\n-  template<typename T>\n-  T operator()(T volatile* dest,\n-               T compare_value,\n-               T exchange_value,\n-               atomic_memory_order order) const;\n-};\n-\n-\/\/ Define the class before including platform file, which may use this\n-\/\/ as a base class, requiring it be complete.  The definition is later\n-\/\/ in this file, near the other definitions related to cmpxchg.\n-struct Atomic::CmpxchgByteUsingInt {\n-  static uint8_t get_byte_in_int(uint32_t n, uint32_t idx);\n-  static uint32_t set_byte_in_int(uint32_t n, uint8_t b, uint32_t idx);\n-  template<typename T>\n-  T operator()(T volatile* dest,\n-               T compare_value,\n-               T exchange_value,\n-               atomic_memory_order order) const;\n-};\n-\n-\/\/ Define the class before including platform file, which may use this\n-\/\/ as a base class, requiring it be complete.  The definition is later\n-\/\/ in this file, near the other definitions related to xchg.\n-template<size_t byte_size>\n-struct Atomic::XchgUsingCmpxchg {\n-  template<typename T>\n-  T operator()(T volatile* dest,\n-               T exchange_value,\n-               atomic_memory_order order) const;\n-};\n-\n-\/\/ Define the class before including platform file, which may use this\n-\/\/ as a base class, requiring it be complete.\n-template<size_t byte_size>\n-class Atomic::AddUsingCmpxchg {\n-public:\n-  template<typename D, typename I>\n-  static inline D add_then_fetch(D volatile* dest,\n-                                 I add_value,\n-                                 atomic_memory_order order) {\n-    D addend = add_value;\n-    return fetch_then_add(dest, add_value, order) + add_value;\n-  }\n-\n-  template<typename D, typename I>\n-  static inline D fetch_then_add(D volatile* dest,\n-                          I add_value,\n-                          atomic_memory_order order) {\n-    STATIC_ASSERT(byte_size == sizeof(I));\n-    STATIC_ASSERT(byte_size == sizeof(D));\n-\n-    D old_value;\n-    D new_value;\n-    do {\n-      old_value = Atomic::load(dest);\n-      new_value = old_value + add_value;\n-    } while (old_value != Atomic::cmpxchg(dest, old_value, new_value, order));\n-    return old_value;\n-  }\n-};\n-\n-\/\/ Define the class before including platform file, which may specialize\n-\/\/ the operator definition.  No generic definition of specializations\n-\/\/ of the operator template are provided, nor are there any generic\n-\/\/ specializations of the class.  The platform file is responsible for\n-\/\/ providing those.\n-template<size_t byte_size>\n-struct Atomic::PlatformXchg {\n-  template<typename T>\n-  T operator()(T volatile* dest,\n-               T exchange_value,\n-               atomic_memory_order order) const;\n-};\n-\n-\/\/ Implement fetch_then_bitop operations using a CAS loop.\n-class Atomic::PrefetchBitopsUsingCmpxchg {\n-  template<typename T, typename Op>\n-  T bitop(T volatile* dest, atomic_memory_order order, Op operation) const {\n-    T old_value;\n-    T new_value;\n-    T fetched_value = Atomic::load(dest);\n-    do {\n-      old_value = fetched_value;\n-      new_value = operation(old_value);\n-      fetched_value = Atomic::cmpxchg(dest, old_value, new_value, order);\n-    } while (old_value != fetched_value);\n-    return fetched_value;\n-  }\n-\n-public:\n-  template<typename T>\n-  T fetch_then_and(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bitop(dest, order, [&](T value) -> T { return value & bits; });\n-  }\n-\n-  template<typename T>\n-  T fetch_then_or(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bitop(dest, order, [&](T value) -> T { return value | bits; });\n-  }\n-\n-  template<typename T>\n-  T fetch_then_xor(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bitop(dest, order, [&](T value) -> T { return value ^ bits; });\n-  }\n-};\n-\n-\/\/ Implement bitop_then_fetch operations using a CAS loop.\n-class Atomic::PostfetchBitopsUsingCmpxchg {\n-  template<typename T, typename Op>\n-  T bitop(T volatile* dest, atomic_memory_order order, Op operation) const {\n-    T old_value;\n-    T new_value;\n-    T fetched_value = Atomic::load(dest);\n-    do {\n-      old_value = fetched_value;\n-      new_value = operation(old_value);\n-      fetched_value = Atomic::cmpxchg(dest, old_value, new_value, order);\n-    } while (old_value != fetched_value);\n-    return new_value;\n-  }\n-\n-public:\n-  template<typename T>\n-  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bitop(dest, order, [&](T value) -> T { return value & bits; });\n-  }\n-\n-  template<typename T>\n-  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bitop(dest, order, [&](T value) -> T { return value | bits; });\n-  }\n-\n-  template<typename T>\n-  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bitop(dest, order, [&](T value) -> T { return value ^ bits; });\n-  }\n-};\n-\n-\/\/ Implement bitop_then_fetch operations by calling fetch_then_bitop and\n-\/\/ applying the operation to the result and the bits argument.\n-class Atomic::PostfetchBitopsUsingPrefetch {\n-public:\n-  template<typename T>\n-  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bits & Atomic::fetch_then_and(dest, bits, order);\n-  }\n-\n-  template<typename T>\n-  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bits | Atomic::fetch_then_or(dest, bits, order);\n-  }\n-\n-  template<typename T>\n-  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n-    return bits ^ Atomic::fetch_then_xor(dest, bits, order);\n-  }\n-};\n-\n-\/\/ The default definition uses cmpxchg.  Platforms can override by defining a\n-\/\/ partial specialization providing size, either as a template parameter or as\n-\/\/ a specific value.\n-template<size_t size, bool>\n-class Atomic::PlatformBitops\n-  : public PrefetchBitopsUsingCmpxchg,\n-    public PostfetchBitopsUsingCmpxchg\n-{};\n-\n-template <ScopedFenceType T>\n-class ScopedFenceGeneral: public StackObj {\n- public:\n-  void prefix() {}\n-  void postfix() {}\n-};\n-\n-\/\/ The following methods can be specialized using simple template specialization\n-\/\/ in the platform specific files for optimization purposes. Otherwise the\n-\/\/ generalized variant is used.\n-\n-template<> inline void ScopedFenceGeneral<X_ACQUIRE>::postfix()       { OrderAccess::acquire(); }\n-template<> inline void ScopedFenceGeneral<RELEASE_X>::prefix()        { OrderAccess::release(); }\n-template<> inline void ScopedFenceGeneral<RELEASE_X_FENCE>::prefix()  { OrderAccess::release(); }\n-template<> inline void ScopedFenceGeneral<RELEASE_X_FENCE>::postfix() { OrderAccess::fence();   }\n-\n-template <ScopedFenceType T>\n-class ScopedFence : public ScopedFenceGeneral<T> {\n-  void *const _field;\n- public:\n-  ScopedFence(void *const field) : _field(field) { prefix(); }\n-  ~ScopedFence() { postfix(); }\n-  void prefix() { ScopedFenceGeneral<T>::prefix(); }\n-  void postfix() { ScopedFenceGeneral<T>::postfix(); }\n-};\n-\n-\/\/ platform specific in-line definitions - must come before shared definitions\n-\n-#include OS_CPU_HEADER(atomic)\n-\n-\/\/ shared in-line definitions\n-\n-\/\/ size_t casts...\n-#if (SIZE_MAX != UINTPTR_MAX)\n-#error size_t is not WORD_SIZE, interesting platform, but missing implementation here\n-#endif\n-\n-template<typename T>\n-inline T Atomic::load(const volatile T* dest) {\n-  return LoadImpl<T, PlatformLoad<sizeof(T)> >()(dest);\n-}\n-\n-template<size_t byte_size, ScopedFenceType type>\n-struct Atomic::PlatformOrderedLoad {\n-  template <typename T>\n-  T operator()(const volatile T* p) const {\n-    ScopedFence<type> f((void*)p);\n-    return Atomic::load(p);\n-  }\n-};\n-\n-template <typename T>\n-inline T Atomic::load_acquire(const volatile T* p) {\n-  return LoadImpl<T, PlatformOrderedLoad<sizeof(T), X_ACQUIRE> >()(p);\n-}\n-\n-template<typename D, typename T>\n-inline void Atomic::store(volatile D* dest, T store_value) {\n-  StoreImpl<D, T, PlatformStore<sizeof(D)> >()(dest, store_value);\n-}\n-\n-template<size_t byte_size, ScopedFenceType type>\n-struct Atomic::PlatformOrderedStore {\n-  template <typename T>\n-  void operator()(volatile T* p, T v) const {\n-    ScopedFence<type> f((void*)p);\n-    Atomic::store(p, v);\n-  }\n-};\n-\n-template <typename D, typename T>\n-inline void Atomic::release_store(volatile D* p, T v) {\n-  StoreImpl<D, T, PlatformOrderedStore<sizeof(D), RELEASE_X> >()(p, v);\n-}\n-\n-template <typename D, typename T>\n-inline void Atomic::release_store_fence(volatile D* p, T v) {\n-  StoreImpl<D, T, PlatformOrderedStore<sizeof(D), RELEASE_X_FENCE> >()(p, v);\n-}\n-\n-template<typename D, typename I>\n-inline D Atomic::add(D volatile* dest, I add_value,\n-                     atomic_memory_order order) {\n-  return AddImpl<D, I>::add_then_fetch(dest, add_value, order);\n-}\n-\n-template<typename D, typename I>\n-inline D Atomic::fetch_then_add(D volatile* dest, I add_value,\n-                                atomic_memory_order order) {\n-  return AddImpl<D, I>::fetch_then_add(dest, add_value, order);\n-}\n-\n-template<typename D, typename I>\n-struct Atomic::AddImpl<\n-  D, I,\n-  typename EnableIf<std::is_integral<I>::value &&\n-                    std::is_integral<D>::value &&\n-                    (sizeof(I) <= sizeof(D)) &&\n-                    (std::is_signed<I>::value == std::is_signed<D>::value)>::type>\n-{\n-  static D add_then_fetch(D volatile* dest, I add_value, atomic_memory_order order) {\n-    D addend = add_value;\n-    return PlatformAdd<sizeof(D)>().add_then_fetch(dest, addend, order);\n-  }\n-  static D fetch_then_add(D volatile* dest, I add_value, atomic_memory_order order) {\n-    D addend = add_value;\n-    return PlatformAdd<sizeof(D)>().fetch_then_add(dest, addend, order);\n-  }\n-};\n-\n-template<typename P, typename I>\n-struct Atomic::AddImpl<\n-  P*, I,\n-  typename EnableIf<std::is_integral<I>::value && (sizeof(I) <= sizeof(P*))>::type>\n-{\n-  STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));\n-  STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));\n-\n-  \/\/ Type of the scaled addend.  An integral type of the same size as a\n-  \/\/ pointer, and the same signedness as I.\n-  using SI = std::conditional_t<std::is_signed<I>::value, intptr_t, uintptr_t>;\n-\n-  \/\/ Type of the unscaled destination.  A pointer type with pointee size == 1.\n-  using UP = const char*;\n-\n-  \/\/ Scale add_value by the size of the pointee.\n-  static SI scale_addend(SI add_value) {\n-    return add_value * SI(sizeof(P));\n-  }\n-\n-  \/\/ Casting between P* and UP* here intentionally uses C-style casts,\n-  \/\/ because reinterpret_cast can't cast away cv qualifiers.  Using copy_cv\n-  \/\/ would be an alternative if it existed.\n-\n-  \/\/ Unscale dest to a char* pointee for consistency with scaled addend.\n-  static UP volatile* unscale_dest(P* volatile* dest) {\n-    return (UP volatile*) dest;\n-  }\n-\n-  \/\/ Convert the unscaled char* result to a P*.\n-  static P* scale_result(UP result) {\n-    return (P*) result;\n-  }\n-\n-  static P* add_then_fetch(P* volatile* dest, I addend, atomic_memory_order order) {\n-    return scale_result(PlatformAdd<sizeof(P*)>().add_then_fetch(unscale_dest(dest),\n-                                                                scale_addend(addend),\n-                                                                order));\n-  }\n-\n-  static P* fetch_then_add(P* volatile* dest, I addend, atomic_memory_order order) {\n-    return scale_result(PlatformAdd<sizeof(P*)>().fetch_then_add(unscale_dest(dest),\n-                                                                scale_addend(addend),\n-                                                                order));\n-  }\n-};\n-\n-template<typename Type, typename Fn, typename D, typename I>\n-inline D Atomic::add_using_helper(Fn fn, D volatile* dest, I add_value) {\n-  return PrimitiveConversions::cast<D>(\n-    fn(PrimitiveConversions::cast<Type>(add_value),\n-       reinterpret_cast<Type volatile*>(dest)));\n-}\n-\n-template<typename D, typename U, typename T>\n-inline D Atomic::cmpxchg(D volatile* dest,\n-                         U compare_value,\n-                         T exchange_value,\n-                         atomic_memory_order order) {\n-  return CmpxchgImpl<D, U, T>()(dest, compare_value, exchange_value, order);\n-}\n-\n-template<typename D, typename T>\n-inline bool Atomic::replace_if_null(D* volatile* dest, T* value,\n-                                    atomic_memory_order order) {\n-  \/\/ Presently using a trivial implementation in terms of cmpxchg.\n-  \/\/ Consider adding platform support, to permit the use of compiler\n-  \/\/ intrinsics like gcc's __sync_bool_compare_and_swap.\n-  D* expected_null = nullptr;\n-  return expected_null == cmpxchg(dest, expected_null, value, order);\n-}\n-\n-\/\/ Handle cmpxchg for integral types.\n-\/\/\n-\/\/ All the involved types must be identical.\n-template<typename T>\n-struct Atomic::CmpxchgImpl<\n-  T, T, T,\n-  typename EnableIf<std::is_integral<T>::value>::type>\n-{\n-  T operator()(T volatile* dest, T compare_value, T exchange_value,\n-               atomic_memory_order order) const {\n-    \/\/ Forward to the platform handler for the size of T.\n-    return PlatformCmpxchg<sizeof(T)>()(dest,\n-                                        compare_value,\n-                                        exchange_value,\n-                                        order);\n-  }\n-};\n-\n-\/\/ Handle cmpxchg for pointer types.\n-\/\/\n-\/\/ The destination's type and the compare_value type must be the same,\n-\/\/ ignoring cv-qualifiers; we don't care about the cv-qualifiers of\n-\/\/ the compare_value.\n-\/\/\n-\/\/ The exchange_value must be implicitly convertible to the\n-\/\/ destination's type; it must be type-correct to store the\n-\/\/ exchange_value in the destination.\n-template<typename D, typename U, typename T>\n-struct Atomic::CmpxchgImpl<\n-  D*, U*, T*,\n-  typename EnableIf<Atomic::IsPointerConvertible<T*, D*>::value &&\n-                    std::is_same<std::remove_cv_t<D>,\n-                                 std::remove_cv_t<U>>::value>::type>\n-{\n-  D* operator()(D* volatile* dest, U* compare_value, T* exchange_value,\n-               atomic_memory_order order) const {\n-    \/\/ Allow derived to base conversion, and adding cv-qualifiers.\n-    D* new_value = exchange_value;\n-    \/\/ Don't care what the CV qualifiers for compare_value are,\n-    \/\/ but we need to match D* when calling platform support.\n-    D* old_value = const_cast<D*>(compare_value);\n-    return PlatformCmpxchg<sizeof(D*)>()(dest, old_value, new_value, order);\n-  }\n-};\n-\n-\/\/ Handle cmpxchg for types that have a translator.\n-\/\/\n-\/\/ All the involved types must be identical.\n-\/\/\n-\/\/ This translates the original call into a call on the decayed\n-\/\/ arguments, and returns the recovered result of that translated\n-\/\/ call.\n-template<typename T>\n-struct Atomic::CmpxchgImpl<\n-  T, T, T,\n-  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n-{\n-  T operator()(T volatile* dest, T compare_value, T exchange_value,\n-               atomic_memory_order order) const {\n-    typedef PrimitiveConversions::Translate<T> Translator;\n-    typedef typename Translator::Decayed Decayed;\n-    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n-    return Translator::recover(\n-      cmpxchg(reinterpret_cast<Decayed volatile*>(dest),\n-              Translator::decay(compare_value),\n-              Translator::decay(exchange_value),\n-              order));\n-  }\n-};\n-\n-template<typename Type, typename Fn, typename T>\n-inline T Atomic::cmpxchg_using_helper(Fn fn,\n-                                      T volatile* dest,\n-                                      T compare_value,\n-                                      T exchange_value) {\n-  STATIC_ASSERT(sizeof(Type) == sizeof(T));\n-  return PrimitiveConversions::cast<T>(\n-    fn(PrimitiveConversions::cast<Type>(exchange_value),\n-       reinterpret_cast<Type volatile*>(dest),\n-       PrimitiveConversions::cast<Type>(compare_value)));\n-}\n-\n-inline uint32_t Atomic::CmpxchgByteUsingInt::set_byte_in_int(uint32_t n,\n-                                                             uint8_t b,\n-                                                             uint32_t idx) {\n-  uint32_t bitsIdx = BitsPerByte * idx;\n-  return (n & ~(static_cast<uint32_t>(0xff) << bitsIdx))\n-          | (static_cast<uint32_t>(b) << bitsIdx);\n-}\n-\n-inline uint8_t Atomic::CmpxchgByteUsingInt::get_byte_in_int(uint32_t n,\n-                                                            uint32_t idx) {\n-  uint32_t bitsIdx = BitsPerByte * idx;\n-  return (uint8_t)(n >> bitsIdx);\n-}\n-\n-template<typename T>\n-inline T Atomic::CmpxchgByteUsingInt::operator()(T volatile* dest,\n-                                                 T compare_value,\n-                                                 T exchange_value,\n-                                                 atomic_memory_order order) const {\n-  STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));\n-  uint8_t canon_exchange_value = exchange_value;\n-  uint8_t canon_compare_value = compare_value;\n-  volatile uint32_t* aligned_dest\n-    = reinterpret_cast<volatile uint32_t*>(align_down(dest, sizeof(uint32_t)));\n-  uint32_t offset = checked_cast<uint32_t>(pointer_delta(dest, aligned_dest, 1));\n-\n-  uint32_t idx = (Endian::NATIVE == Endian::BIG)\n-                   ? (sizeof(uint32_t) - 1 - offset)\n-                   : offset;\n-\n-  \/\/ current value may not be what we are looking for, so force it\n-  \/\/ to that value so the initial cmpxchg will fail if it is different\n-  uint32_t cur = set_byte_in_int(Atomic::load(aligned_dest), canon_compare_value, idx);\n-\n-  \/\/ always execute a real cmpxchg so that we get the required memory\n-  \/\/ barriers even on initial failure\n-  do {\n-    \/\/ value to swap in matches current value\n-    \/\/ except for the one byte we want to update\n-    uint32_t new_value = set_byte_in_int(cur, canon_exchange_value, idx);\n-\n-    uint32_t res = cmpxchg(aligned_dest, cur, new_value, order);\n-    if (res == cur) break;      \/\/ success\n-\n-    \/\/ at least one byte in the int changed value, so update\n-    \/\/ our view of the current int\n-    cur = res;\n-    \/\/ if our byte is still as cur we loop and try again\n-  } while (get_byte_in_int(cur, idx) == canon_compare_value);\n-\n-  return PrimitiveConversions::cast<T>(get_byte_in_int(cur, idx));\n-}\n-\n-\/\/ Handle xchg for integral types.\n-\/\/\n-\/\/ All the involved types must be identical.\n-template<typename T>\n-struct Atomic::XchgImpl<\n-  T, T,\n-  typename EnableIf<std::is_integral<T>::value>::type>\n-{\n-  T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {\n-    \/\/ Forward to the platform handler for the size of T.\n-    return PlatformXchg<sizeof(T)>()(dest, exchange_value, order);\n-  }\n-};\n-\n-\/\/ Handle xchg for pointer types.\n-\/\/\n-\/\/ The exchange_value must be implicitly convertible to the\n-\/\/ destination's type; it must be type-correct to store the\n-\/\/ exchange_value in the destination.\n-template<typename D, typename T>\n-struct Atomic::XchgImpl<\n-  D*, T*,\n-  typename EnableIf<Atomic::IsPointerConvertible<T*, D*>::value>::type>\n-{\n-  D* operator()(D* volatile* dest, T* exchange_value, atomic_memory_order order) const {\n-    \/\/ Allow derived to base conversion, and adding cv-qualifiers.\n-    D* new_value = exchange_value;\n-    return PlatformXchg<sizeof(D*)>()(dest, new_value, order);\n-  }\n-};\n-\n-\/\/ Handle xchg for types that have a translator.\n-\/\/\n-\/\/ All the involved types must be identical.\n-\/\/\n-\/\/ This translates the original call into a call on the decayed\n-\/\/ arguments, and returns the recovered result of that translated\n-\/\/ call.\n-template<typename T>\n-struct Atomic::XchgImpl<\n-  T, T,\n-  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n-{\n-  T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {\n-    typedef PrimitiveConversions::Translate<T> Translator;\n-    typedef typename Translator::Decayed Decayed;\n-    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n-    return Translator::recover(\n-      xchg(reinterpret_cast<Decayed volatile*>(dest),\n-           Translator::decay(exchange_value),\n-           order));\n-  }\n-};\n-\n-template<typename Type, typename Fn, typename T>\n-inline T Atomic::xchg_using_helper(Fn fn,\n-                                   T volatile* dest,\n-                                   T exchange_value) {\n-  STATIC_ASSERT(sizeof(Type) == sizeof(T));\n-  \/\/ Notice the swapped order of arguments. Change when\/if stubs are rewritten.\n-  return PrimitiveConversions::cast<T>(\n-    fn(PrimitiveConversions::cast<Type>(exchange_value),\n-       reinterpret_cast<Type volatile*>(dest)));\n-}\n-\n-template<typename D, typename T>\n-inline D Atomic::xchg(volatile D* dest, T exchange_value, atomic_memory_order order) {\n-  return XchgImpl<D, T>()(dest, exchange_value, order);\n-}\n-\n-template<size_t byte_size>\n-template<typename T>\n-inline T Atomic::XchgUsingCmpxchg<byte_size>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n-  STATIC_ASSERT(byte_size == sizeof(T));\n-\n-  T old_value;\n-  do {\n-    old_value = Atomic::load(dest);\n-  } while (old_value != Atomic::cmpxchg(dest, old_value, exchange_value, order));\n-  return old_value;\n-}\n-\n-#endif \/\/ SHARE_RUNTIME_ATOMIC_HPP\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":0,"deletions":1235,"binary":false,"changes":1235,"status":"deleted"},{"patch":"@@ -0,0 +1,1235 @@\n+\/*\n+ * Copyright (c) 1999, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_ATOMICACCESS_HPP\n+#define SHARE_RUNTIME_ATOMICACCESS_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/bytes.hpp\"\n+#include \"utilities\/checkedCast.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include <type_traits>\n+\n+enum atomic_memory_order {\n+  \/\/ The modes that align with C++11 are intended to\n+  \/\/ follow the same semantics.\n+  memory_order_relaxed = 0,\n+  memory_order_acquire = 2,\n+  memory_order_release = 3,\n+  memory_order_acq_rel = 4,\n+  memory_order_seq_cst = 5,\n+  \/\/ Strong two-way memory barrier.\n+  memory_order_conservative = 8\n+};\n+\n+enum ScopedFenceType {\n+    X_ACQUIRE\n+  , RELEASE_X\n+  , RELEASE_X_FENCE\n+};\n+\n+class AtomicAccess : AllStatic {\n+public:\n+  \/\/ Atomic operations on int64 types are required to be available on\n+  \/\/ all platforms. At a minimum a 64-bit cmpxchg must be available\n+  \/\/ from which other atomic operations can be constructed if needed.\n+  \/\/ The legacy `Abstract_VMVersion::supports_cx8()` function used to\n+  \/\/ indicate if this support existed, allowing for alternative lock-\n+  \/\/ based mechanism to be used. But today this function is required\n+  \/\/ to return true and in the future will be removed entirely.\n+\n+  \/\/ The memory operations that are mentioned with each of the atomic\n+  \/\/ function families come from src\/share\/vm\/runtime\/orderAccess.hpp,\n+  \/\/ e.g., <fence> is described in that file and is implemented by the\n+  \/\/ OrderAccess::fence() function. See that file for the gory details\n+  \/\/ on the Memory Access Ordering Model.\n+\n+  \/\/ All of the atomic operations that imply a read-modify-write action\n+  \/\/ guarantee a two-way memory barrier across that operation. Historically\n+  \/\/ these semantics reflect the strength of atomic operations that are\n+  \/\/ provided on SPARC\/X86. We assume that strength is necessary unless\n+  \/\/ we can prove that a weaker form is sufficiently safe.\n+\n+  \/\/ Atomically store to a location\n+  \/\/ The type T must be either a pointer type convertible to or equal\n+  \/\/ to D, an integral\/enum type equal to D, or a type equal to D that\n+  \/\/ is primitive convertible using PrimitiveConversions.\n+  template<typename D, typename T>\n+  inline static void store(volatile D* dest, T store_value);\n+\n+  template <typename D, typename T>\n+  inline static void release_store(volatile D* dest, T store_value);\n+\n+  template <typename D, typename T>\n+  inline static void release_store_fence(volatile D* dest, T store_value);\n+\n+  \/\/ Atomically load from a location\n+  \/\/ The type T must be either a pointer type, an integral\/enum type,\n+  \/\/ or a type that is primitive convertible using PrimitiveConversions.\n+  template<typename T>\n+  inline static T load(const volatile T* dest);\n+\n+  template <typename T>\n+  inline static T load_acquire(const volatile T* dest);\n+\n+  \/\/ Atomically add to a location. *add*() provide:\n+  \/\/ <fence> add-value-to-dest <membar StoreLoad|StoreStore>\n+\n+  \/\/ Returns updated value.\n+  template<typename D, typename I>\n+  inline static D add(D volatile* dest, I add_value,\n+                      atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Returns previous value.\n+  template<typename D, typename I>\n+  inline static D fetch_then_add(D volatile* dest, I add_value,\n+                                 atomic_memory_order order = memory_order_conservative);\n+\n+  template<typename D, typename I>\n+  inline static D sub(D volatile* dest, I sub_value,\n+                      atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Atomically increment location. inc() provide:\n+  \/\/ <fence> increment-dest <membar StoreLoad|StoreStore>\n+  \/\/ The type D may be either a pointer type, or an integral\n+  \/\/ type. If it is a pointer type, then the increment is\n+  \/\/ scaled to the size of the type pointed to by the pointer.\n+  template<typename D>\n+  inline static void inc(D volatile* dest,\n+                         atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Atomically decrement a location. dec() provide:\n+  \/\/ <fence> decrement-dest <membar StoreLoad|StoreStore>\n+  \/\/ The type D may be either a pointer type, or an integral\n+  \/\/ type. If it is a pointer type, then the decrement is\n+  \/\/ scaled to the size of the type pointed to by the pointer.\n+  template<typename D>\n+  inline static void dec(D volatile* dest,\n+                         atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Performs atomic exchange of *dest with exchange_value. Returns old\n+  \/\/ prior value of *dest. xchg*() provide:\n+  \/\/ <fence> exchange-value-with-dest <membar StoreLoad|StoreStore>\n+  \/\/ The type T must be either a pointer type convertible to or equal\n+  \/\/ to D, an integral\/enum type equal to D, or a type equal to D that\n+  \/\/ is primitive convertible using PrimitiveConversions.\n+  template<typename D, typename T>\n+  inline static D xchg(volatile D* dest, T exchange_value,\n+                       atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Performs atomic compare of *dest and compare_value, and exchanges\n+  \/\/ *dest with exchange_value if the comparison succeeded. Returns prior\n+  \/\/ value of *dest. cmpxchg*() provide:\n+  \/\/ <fence> compare-and-exchange <membar StoreLoad|StoreStore>\n+\n+  template<typename D, typename U, typename T>\n+  inline static D cmpxchg(D volatile* dest,\n+                          U compare_value,\n+                          T exchange_value,\n+                          atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Performs atomic compare of *dest and nullptr, and replaces *dest\n+  \/\/ with exchange_value if the comparison succeeded.  Returns true if\n+  \/\/ the comparison succeeded and the exchange occurred.  This is\n+  \/\/ often used as part of lazy initialization, as a lock-free\n+  \/\/ alternative to the Double-Checked Locking Pattern.\n+  template<typename D, typename T>\n+  inline static bool replace_if_null(D* volatile* dest, T* value,\n+                                     atomic_memory_order order = memory_order_conservative);\n+\n+  \/\/ Bitwise logical operations (and, or, xor)\n+  \/\/\n+  \/\/ All operations apply the corresponding operation to the value in dest and\n+  \/\/ bits, storing the result in dest. They return either the old value\n+  \/\/ (fetch_then_BITOP) or the newly updated value (BITOP_then_fetch).\n+  \/\/\n+  \/\/ Requirements:\n+  \/\/ - T is an integral type\n+  \/\/ - sizeof(T) == 1 || sizeof(T) == sizeof(int) || sizeof(T) == sizeof(void*)\n+\n+  \/\/ Performs atomic bitwise-and of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n+  \/\/ this sequence of operations:\n+  \/\/ { tmp = *dest; *dest &= bits; return tmp; }\n+  template<typename T>\n+  static T fetch_then_and(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().fetch_then_and(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-or of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n+  \/\/ this sequence of operations:\n+  \/\/ { tmp = *dest; *dest |= bits; return tmp; }\n+  template<typename T>\n+  static T fetch_then_or(volatile T* dest, T bits,\n+                         atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().fetch_then_or(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-xor of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the prior value of *dest.  That is, atomically performs\n+  \/\/ this sequence of operations:\n+  \/\/ { tmp = *dest; *dest ^= bits; return tmp; }\n+  template<typename T>\n+  static T fetch_then_xor(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().fetch_then_xor(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-and of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n+  \/\/ this operation:\n+  \/\/ { return *dest &= bits; }\n+  template<typename T>\n+  static T and_then_fetch(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().and_then_fetch(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-or of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n+  \/\/ this operation:\n+  \/\/ { return *dest |= bits; }\n+  template<typename T>\n+  static T or_then_fetch(volatile T* dest, T bits,\n+                         atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().or_then_fetch(dest, bits, order);\n+  }\n+\n+  \/\/ Performs atomic bitwise-xor of *dest and bits, storing the result in\n+  \/\/ *dest.  Returns the new value of *dest.  That is, atomically performs\n+  \/\/ this operation:\n+  \/\/ { return *dest ^= bits; }\n+  template<typename T>\n+  static T xor_then_fetch(volatile T* dest, T bits,\n+                          atomic_memory_order order = memory_order_conservative) {\n+    static_assert(std::is_integral<T>::value, \"bitop with non-integral type\");\n+    return PlatformBitops<sizeof(T)>().xor_then_fetch(dest, bits, order);\n+  }\n+\n+private:\n+  \/\/ Test whether From is implicitly convertible to To.\n+  \/\/ From and To must be pointer types.\n+  \/\/ Note: Provides the limited subset of C++11 std::is_convertible\n+  \/\/ that is needed here.\n+  template<typename From, typename To> struct IsPointerConvertible;\n+\n+protected:\n+  \/\/ Dispatch handler for store.  Provides type-based validity\n+  \/\/ checking and limited conversions around calls to the platform-\n+  \/\/ specific implementation layer provided by PlatformOp.\n+  template<typename D, typename T, typename PlatformOp, typename Enable = void>\n+  struct StoreImpl;\n+\n+  \/\/ Platform-specific implementation of store.  Support for sizes\n+  \/\/ of 1, 2, 4, and (if different) pointer size bytes are required.\n+  \/\/ The class is a function object that must be default constructable,\n+  \/\/ with these requirements:\n+  \/\/\n+  \/\/ either:\n+  \/\/ - dest is of type D*, an integral, enum or pointer type.\n+  \/\/ - new_value are of type T, an integral, enum or pointer type D or\n+  \/\/   pointer type convertible to D.\n+  \/\/ or:\n+  \/\/ - T and D are the same and are primitive convertible using PrimitiveConversions\n+  \/\/ and either way:\n+  \/\/ - platform_store is an object of type PlatformStore<sizeof(T)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/   platform_store(new_value, dest)\n+  \/\/ must be a valid expression.\n+  \/\/\n+  \/\/ The default implementation is a volatile store. If a platform\n+  \/\/ requires more for e.g. 64 bit stores, a specialization is required\n+  template<size_t byte_size> struct PlatformStore;\n+\n+  \/\/ Dispatch handler for load.  Provides type-based validity\n+  \/\/ checking and limited conversions around calls to the platform-\n+  \/\/ specific implementation layer provided by PlatformOp.\n+  template<typename T, typename PlatformOp, typename Enable = void>\n+  struct LoadImpl;\n+\n+  \/\/ Platform-specific implementation of load. Support for sizes of\n+  \/\/ 1, 2, 4 bytes and (if different) pointer size bytes are required.\n+  \/\/ The class is a function object that must be default\n+  \/\/ constructable, with these requirements:\n+  \/\/\n+  \/\/ - dest is of type T*, an integral, enum or pointer type, or\n+  \/\/   T is convertible to a primitive type using PrimitiveConversions\n+  \/\/ - platform_load is an object of type PlatformLoad<sizeof(T)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/   platform_load(src)\n+  \/\/ must be a valid expression, returning a result convertible to T.\n+  \/\/\n+  \/\/ The default implementation is a volatile load. If a platform\n+  \/\/ requires more for e.g. 64 bit loads, a specialization is required\n+  template<size_t byte_size> struct PlatformLoad;\n+\n+  \/\/ Give platforms a variation point to specialize.\n+  template<size_t byte_size, ScopedFenceType type> struct PlatformOrderedStore;\n+  template<size_t byte_size, ScopedFenceType type> struct PlatformOrderedLoad;\n+\n+private:\n+  \/\/ Dispatch handler for add.  Provides type-based validity checking\n+  \/\/ and limited conversions around calls to the platform-specific\n+  \/\/ implementation layer provided by PlatformAdd.\n+  template<typename D, typename I, typename Enable = void>\n+  struct AddImpl;\n+\n+  \/\/ Platform-specific implementation of add.  Support for sizes of 4\n+  \/\/ bytes and (if different) pointer size bytes are required.  The\n+  \/\/ class must be default constructable, with these requirements:\n+  \/\/\n+  \/\/ - dest is of type D*, where D is an integral or pointer type.\n+  \/\/ - add_value is of type I, an integral type.\n+  \/\/ - sizeof(I) == sizeof(D).\n+  \/\/ - if D is an integral type, I == D.\n+  \/\/ - if D is a pointer type P*, sizeof(P) == 1.\n+  \/\/ - order is of type atomic_memory_order.\n+  \/\/ - platform_add is an object of type PlatformAdd<sizeof(D)>.\n+  \/\/\n+  \/\/ Then both\n+  \/\/   platform_add.add_then_fetch(dest, add_value, order)\n+  \/\/   platform_add.fetch_then_add(dest, add_value, order)\n+  \/\/ must be valid expressions returning a result convertible to D.\n+  \/\/\n+  \/\/ add_then_fetch atomically adds add_value to the value of dest,\n+  \/\/ returning the new value.\n+  \/\/\n+  \/\/ fetch_then_add atomically adds add_value to the value of dest,\n+  \/\/ returning the old value.\n+  \/\/\n+  \/\/ When the destination type D of the Atomic operation is a pointer type P*,\n+  \/\/ the addition must scale the add_value by sizeof(P) to add that many bytes\n+  \/\/ to the destination value.  Rather than requiring each platform deal with\n+  \/\/ this, the shared part of the implementation performs some adjustments\n+  \/\/ before and after calling the platform operation.  It ensures the pointee\n+  \/\/ type of the destination value passed to the platform operation has size\n+  \/\/ 1, casting if needed.  It also scales add_value by sizeof(P).  The result\n+  \/\/ of the platform operation is cast back to P*.  This means the platform\n+  \/\/ operation does not need to account for the scaling.  It also makes it\n+  \/\/ easy for the platform to implement one of add_then_fetch or fetch_then_add\n+  \/\/ in terms of the other (which is a common approach).\n+  \/\/\n+  \/\/ No definition is provided; all platforms must explicitly define\n+  \/\/ this class and any needed specializations.\n+  template<size_t byte_size> struct PlatformAdd;\n+\n+  \/\/ Support for platforms that implement some variants of add using a\n+  \/\/ (typically out of line) non-template helper function.  The\n+  \/\/ generic arguments passed to PlatformAdd need to be translated to\n+  \/\/ the appropriate type for the helper function, the helper function\n+  \/\/ invoked on the translated arguments, and the result translated\n+  \/\/ back.  Type is the parameter \/ return type of the helper\n+  \/\/ function.  No scaling of add_value is performed when D is a pointer\n+  \/\/ type, so this function can be used to implement the support function\n+  \/\/ required by AddAndFetch.\n+  template<typename Type, typename Fn, typename D, typename I>\n+  static D add_using_helper(Fn fn, D volatile* dest, I add_value);\n+\n+  \/\/ Dispatch handler for cmpxchg.  Provides type-based validity\n+  \/\/ checking and limited conversions around calls to the\n+  \/\/ platform-specific implementation layer provided by\n+  \/\/ PlatformCmpxchg.\n+  template<typename D, typename U, typename T, typename Enable = void>\n+  struct CmpxchgImpl;\n+\n+  \/\/ Platform-specific implementation of cmpxchg.  Support for sizes\n+  \/\/ of 1, 4, and 8 are required.  The class is a function object that\n+  \/\/ must be default constructable, with these requirements:\n+  \/\/\n+  \/\/ - dest is of type T*.\n+  \/\/ - exchange_value and compare_value are of type T.\n+  \/\/ - order is of type atomic_memory_order.\n+  \/\/ - platform_cmpxchg is an object of type PlatformCmpxchg<sizeof(T)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/   platform_cmpxchg(dest, compare_value, exchange_value, order)\n+  \/\/ must be a valid expression, returning a result convertible to T.\n+  \/\/\n+  \/\/ A default definition is provided, which declares a function template\n+  \/\/   T operator()(T volatile*, T, T, atomic_memory_order) const\n+  \/\/\n+  \/\/ For each required size, a platform must either provide an\n+  \/\/ appropriate definition of that function, or must entirely\n+  \/\/ specialize the class template for that size.\n+  template<size_t byte_size> struct PlatformCmpxchg;\n+\n+  \/\/ Support for platforms that implement some variants of cmpxchg\n+  \/\/ using a (typically out of line) non-template helper function.\n+  \/\/ The generic arguments passed to PlatformCmpxchg need to be\n+  \/\/ translated to the appropriate type for the helper function, the\n+  \/\/ helper invoked on the translated arguments, and the result\n+  \/\/ translated back.  Type is the parameter \/ return type of the\n+  \/\/ helper function.\n+  template<typename Type, typename Fn, typename T>\n+  static T cmpxchg_using_helper(Fn fn,\n+                                T volatile* dest,\n+                                T compare_value,\n+                                T exchange_value);\n+\n+  \/\/ Support platforms that do not provide Read-Modify-Write atomic\n+  \/\/ accesses for 1-byte and 8-byte widths. To use, derive PlatformCmpxchg<1>,\n+  \/\/ PlatformAdd<S>, PlatformXchg<S> from these classes.\n+public: \/\/ Temporary, can't be private: C++03 11.4\/2. Fixed by C++11.\n+  struct CmpxchgByteUsingInt;\n+  template<size_t byte_size>\n+  struct XchgUsingCmpxchg;\n+  template<size_t byte_size>\n+  class AddUsingCmpxchg;\n+private:\n+\n+  \/\/ Dispatch handler for xchg.  Provides type-based validity\n+  \/\/ checking and limited conversions around calls to the\n+  \/\/ platform-specific implementation layer provided by\n+  \/\/ PlatformXchg.\n+  template<typename D, typename T, typename Enable = void>\n+  struct XchgImpl;\n+\n+  \/\/ Platform-specific implementation of xchg.  Support for sizes\n+  \/\/ of 4, and sizeof(intptr_t) are required.  The class is a function\n+  \/\/ object that must be default constructable, with these requirements:\n+  \/\/\n+  \/\/ - dest is of type T*.\n+  \/\/ - exchange_value is of type T.\n+  \/\/ - platform_xchg is an object of type PlatformXchg<sizeof(T)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/   platform_xchg(dest, exchange_value)\n+  \/\/ must be a valid expression, returning a result convertible to T.\n+  \/\/\n+  \/\/ A default definition is provided, which declares a function template\n+  \/\/   T operator()(T volatile*, T, atomic_memory_order) const\n+  \/\/\n+  \/\/ For each required size, a platform must either provide an\n+  \/\/ appropriate definition of that function, or must entirely\n+  \/\/ specialize the class template for that size.\n+  template<size_t byte_size> struct PlatformXchg;\n+\n+  \/\/ Support for platforms that implement some variants of xchg\n+  \/\/ using a (typically out of line) non-template helper function.\n+  \/\/ The generic arguments passed to PlatformXchg need to be\n+  \/\/ translated to the appropriate type for the helper function, the\n+  \/\/ helper invoked on the translated arguments, and the result\n+  \/\/ translated back.  Type is the parameter \/ return type of the\n+  \/\/ helper function.\n+  template<typename Type, typename Fn, typename T>\n+  static T xchg_using_helper(Fn fn,\n+                             T volatile* dest,\n+                             T exchange_value);\n+\n+  \/\/ Platform-specific implementation of the bitops (and, or, xor).  Support\n+  \/\/ for sizes of 4 bytes and (if different) pointer size bytes are required.\n+  \/\/ The class is a function object that must be default constructable, with\n+  \/\/ these requirements:\n+  \/\/\n+  \/\/ - T is an integral type.\n+  \/\/ - dest is of type T*.\n+  \/\/ - bits is of type T.\n+  \/\/ - order is of type atomic_memory_order.\n+  \/\/ - platform_bitops is an object of type PlatformBitops<sizeof(T)>.\n+  \/\/\n+  \/\/ Then\n+  \/\/  platform_bitops.fetch_then_and(dest, bits, order)\n+  \/\/  platform_bitops.fetch_then_or(dest, bits, order)\n+  \/\/  platform_bitops.fetch_then_xor(dest, bits, order)\n+  \/\/  platform_bitops.and_then_fetch(dest, bits, order)\n+  \/\/  platform_bitops.or_then_fetch(dest, bits, order)\n+  \/\/  platform_bitops.xor_then_fetch(dest, bits, order)\n+  \/\/ must all be valid expressions, returning a result convertible to T.\n+  \/\/\n+  \/\/ A default definition is provided, which implements all of the operations\n+  \/\/ using cmpxchg.\n+  \/\/\n+  \/\/ For each required size, a platform must either use the default or\n+  \/\/ entirely specialize the class for that size by providing all of the\n+  \/\/ required operations.\n+  \/\/\n+  \/\/ The second (bool) template parameter allows platforms to provide a\n+  \/\/ partial specialization with a parameterized size, and is otherwise\n+  \/\/ unused.  The default value for that bool parameter means specializations\n+  \/\/ don't need to mention it.\n+  template<size_t size, bool = true> class PlatformBitops;\n+\n+  \/\/ Helper base classes that may be used to implement PlatformBitops.\n+  class PrefetchBitopsUsingCmpxchg;\n+  class PostfetchBitopsUsingCmpxchg;\n+  class PostfetchBitopsUsingPrefetch;\n+};\n+\n+template<typename From, typename To>\n+struct AtomicAccess::IsPointerConvertible<From*, To*> : AllStatic {\n+  \/\/ Determine whether From* is implicitly convertible to To*, using\n+  \/\/ the \"sizeof trick\".\n+  typedef char yes;\n+  typedef char (&no)[2];\n+\n+  static yes test(To*);\n+  static no test(...);\n+  static From* test_value;\n+\n+  static const bool value = (sizeof(yes) == sizeof(test(test_value)));\n+};\n+\n+\/\/ Handle load for pointer and integral types.\n+template<typename T, typename PlatformOp>\n+struct AtomicAccess::LoadImpl<\n+  T,\n+  PlatformOp,\n+  typename EnableIf<std::is_integral<T>::value || std::is_pointer<T>::value>::type>\n+{\n+  T operator()(T const volatile* dest) const {\n+    \/\/ Forward to the platform handler for the size of T.\n+    return PlatformOp()(dest);\n+  }\n+};\n+\n+\/\/ Handle load for types that have a translator.\n+\/\/\n+\/\/ All the involved types must be identical.\n+\/\/\n+\/\/ This translates the original call into a call on the decayed\n+\/\/ arguments, and returns the recovered result of that translated\n+\/\/ call.\n+template<typename T, typename PlatformOp>\n+struct AtomicAccess::LoadImpl<\n+  T,\n+  PlatformOp,\n+  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n+{\n+  T operator()(T const volatile* dest) const {\n+    typedef PrimitiveConversions::Translate<T> Translator;\n+    typedef typename Translator::Decayed Decayed;\n+    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n+    Decayed result = PlatformOp()(reinterpret_cast<Decayed const volatile*>(dest));\n+    return Translator::recover(result);\n+  }\n+};\n+\n+\/\/ Default implementation of atomic load if a specific platform\n+\/\/ does not provide a specialization for a certain size class.\n+\/\/ For increased safety, the default implementation only allows\n+\/\/ load types that are pointer sized or smaller. If a platform still\n+\/\/ supports wide atomics, then it has to use specialization\n+\/\/ of AtomicAccess::PlatformLoad for that wider size class.\n+template<size_t byte_size>\n+struct AtomicAccess::PlatformLoad {\n+  template<typename T>\n+  T operator()(T const volatile* dest) const {\n+    STATIC_ASSERT(sizeof(T) <= sizeof(void*)); \/\/ wide atomics need specialization\n+    return *dest;\n+  }\n+};\n+\n+\/\/ Handle store for integral types.\n+\/\/\n+\/\/ All the involved types must be identical.\n+template<typename T, typename PlatformOp>\n+struct AtomicAccess::StoreImpl<\n+  T, T,\n+  PlatformOp,\n+  typename EnableIf<std::is_integral<T>::value>::type>\n+{\n+  void operator()(T volatile* dest, T new_value) const {\n+    \/\/ Forward to the platform handler for the size of T.\n+    PlatformOp()(dest, new_value);\n+  }\n+};\n+\n+\/\/ Handle store for pointer types.\n+\/\/\n+\/\/ The new_value must be implicitly convertible to the\n+\/\/ destination's type; it must be type-correct to store the\n+\/\/ new_value in the destination.\n+template<typename D, typename T, typename PlatformOp>\n+struct AtomicAccess::StoreImpl<\n+  D*, T*,\n+  PlatformOp,\n+  typename EnableIf<AtomicAccess::IsPointerConvertible<T*, D*>::value>::type>\n+{\n+  void operator()(D* volatile* dest, T* new_value) const {\n+    \/\/ Allow derived to base conversion, and adding cv-qualifiers.\n+    D* value = new_value;\n+    PlatformOp()(dest, value);\n+  }\n+};\n+\n+\/\/ Handle store for types that have a translator.\n+\/\/\n+\/\/ All the involved types must be identical.\n+\/\/\n+\/\/ This translates the original call into a call on the decayed\n+\/\/ arguments.\n+template<typename T, typename PlatformOp>\n+struct AtomicAccess::StoreImpl<\n+  T, T,\n+  PlatformOp,\n+  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n+{\n+  void operator()(T volatile* dest, T new_value) const {\n+    typedef PrimitiveConversions::Translate<T> Translator;\n+    typedef typename Translator::Decayed Decayed;\n+    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n+    PlatformOp()(reinterpret_cast<Decayed volatile*>(dest),\n+                 Translator::decay(new_value));\n+  }\n+};\n+\n+\/\/ Default implementation of atomic store if a specific platform\n+\/\/ does not provide a specialization for a certain size class.\n+\/\/ For increased safety, the default implementation only allows\n+\/\/ storing types that are pointer sized or smaller. If a platform still\n+\/\/ supports wide atomics, then it has to use specialization\n+\/\/ of AtomicAccess::PlatformStore for that wider size class.\n+template<size_t byte_size>\n+struct AtomicAccess::PlatformStore {\n+  template<typename T>\n+  void operator()(T volatile* dest,\n+                  T new_value) const {\n+    STATIC_ASSERT(sizeof(T) <= sizeof(void*)); \/\/ wide atomics need specialization\n+    (void)const_cast<T&>(*dest = new_value);\n+  }\n+};\n+\n+template<typename D>\n+inline void AtomicAccess::inc(D volatile* dest, atomic_memory_order order) {\n+  STATIC_ASSERT(std::is_pointer<D>::value || std::is_integral<D>::value);\n+  using I = std::conditional_t<std::is_pointer<D>::value, ptrdiff_t, D>;\n+  AtomicAccess::add(dest, I(1), order);\n+}\n+\n+template<typename D>\n+inline void AtomicAccess::dec(D volatile* dest, atomic_memory_order order) {\n+  STATIC_ASSERT(std::is_pointer<D>::value || std::is_integral<D>::value);\n+  using I = std::conditional_t<std::is_pointer<D>::value, ptrdiff_t, D>;\n+  \/\/ Assumes two's complement integer representation.\n+  #pragma warning(suppress: 4146)\n+  AtomicAccess::add(dest, I(-1), order);\n+}\n+\n+template<typename D, typename I>\n+inline D AtomicAccess::sub(D volatile* dest, I sub_value, atomic_memory_order order) {\n+  STATIC_ASSERT(std::is_pointer<D>::value || std::is_integral<D>::value);\n+  STATIC_ASSERT(std::is_integral<I>::value);\n+  \/\/ If D is a pointer type, use [u]intptr_t as the addend type,\n+  \/\/ matching signedness of I.  Otherwise, use D as the addend type.\n+  using PI = std::conditional_t<std::is_signed<I>::value, intptr_t, uintptr_t>;\n+  using AddendType = std::conditional_t<std::is_pointer<D>::value, PI, D>;\n+  \/\/ Only allow conversions that can't change the value.\n+  STATIC_ASSERT(std::is_signed<I>::value == std::is_signed<AddendType>::value);\n+  STATIC_ASSERT(sizeof(I) <= sizeof(AddendType));\n+  AddendType addend = sub_value;\n+  \/\/ Assumes two's complement integer representation.\n+  #pragma warning(suppress: 4146) \/\/ In case AddendType is not signed.\n+  return AtomicAccess::add(dest, -addend, order);\n+}\n+\n+\/\/ Define the class before including platform file, which may specialize\n+\/\/ the operator definition.  No generic definition of specializations\n+\/\/ of the operator template are provided, nor are there any generic\n+\/\/ specializations of the class.  The platform file is responsible for\n+\/\/ providing those.\n+template<size_t byte_size>\n+struct AtomicAccess::PlatformCmpxchg {\n+  template<typename T>\n+  T operator()(T volatile* dest,\n+               T compare_value,\n+               T exchange_value,\n+               atomic_memory_order order) const;\n+};\n+\n+\/\/ Define the class before including platform file, which may use this\n+\/\/ as a base class, requiring it be complete.  The definition is later\n+\/\/ in this file, near the other definitions related to cmpxchg.\n+struct AtomicAccess::CmpxchgByteUsingInt {\n+  static uint8_t get_byte_in_int(uint32_t n, uint32_t idx);\n+  static uint32_t set_byte_in_int(uint32_t n, uint8_t b, uint32_t idx);\n+  template<typename T>\n+  T operator()(T volatile* dest,\n+               T compare_value,\n+               T exchange_value,\n+               atomic_memory_order order) const;\n+};\n+\n+\/\/ Define the class before including platform file, which may use this\n+\/\/ as a base class, requiring it be complete.  The definition is later\n+\/\/ in this file, near the other definitions related to xchg.\n+template<size_t byte_size>\n+struct AtomicAccess::XchgUsingCmpxchg {\n+  template<typename T>\n+  T operator()(T volatile* dest,\n+               T exchange_value,\n+               atomic_memory_order order) const;\n+};\n+\n+\/\/ Define the class before including platform file, which may use this\n+\/\/ as a base class, requiring it be complete.\n+template<size_t byte_size>\n+class AtomicAccess::AddUsingCmpxchg {\n+public:\n+  template<typename D, typename I>\n+  static inline D add_then_fetch(D volatile* dest,\n+                                 I add_value,\n+                                 atomic_memory_order order) {\n+    D addend = add_value;\n+    return fetch_then_add(dest, add_value, order) + add_value;\n+  }\n+\n+  template<typename D, typename I>\n+  static inline D fetch_then_add(D volatile* dest,\n+                          I add_value,\n+                          atomic_memory_order order) {\n+    STATIC_ASSERT(byte_size == sizeof(I));\n+    STATIC_ASSERT(byte_size == sizeof(D));\n+\n+    D old_value;\n+    D new_value;\n+    do {\n+      old_value = AtomicAccess::load(dest);\n+      new_value = old_value + add_value;\n+    } while (old_value != AtomicAccess::cmpxchg(dest, old_value, new_value, order));\n+    return old_value;\n+  }\n+};\n+\n+\/\/ Define the class before including platform file, which may specialize\n+\/\/ the operator definition.  No generic definition of specializations\n+\/\/ of the operator template are provided, nor are there any generic\n+\/\/ specializations of the class.  The platform file is responsible for\n+\/\/ providing those.\n+template<size_t byte_size>\n+struct AtomicAccess::PlatformXchg {\n+  template<typename T>\n+  T operator()(T volatile* dest,\n+               T exchange_value,\n+               atomic_memory_order order) const;\n+};\n+\n+\/\/ Implement fetch_then_bitop operations using a CAS loop.\n+class AtomicAccess::PrefetchBitopsUsingCmpxchg {\n+  template<typename T, typename Op>\n+  T bitop(T volatile* dest, atomic_memory_order order, Op operation) const {\n+    T old_value;\n+    T new_value;\n+    T fetched_value = AtomicAccess::load(dest);\n+    do {\n+      old_value = fetched_value;\n+      new_value = operation(old_value);\n+      fetched_value = AtomicAccess::cmpxchg(dest, old_value, new_value, order);\n+    } while (old_value != fetched_value);\n+    return fetched_value;\n+  }\n+\n+public:\n+  template<typename T>\n+  T fetch_then_and(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value & bits; });\n+  }\n+\n+  template<typename T>\n+  T fetch_then_or(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value | bits; });\n+  }\n+\n+  template<typename T>\n+  T fetch_then_xor(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value ^ bits; });\n+  }\n+};\n+\n+\/\/ Implement bitop_then_fetch operations using a CAS loop.\n+class AtomicAccess::PostfetchBitopsUsingCmpxchg {\n+  template<typename T, typename Op>\n+  T bitop(T volatile* dest, atomic_memory_order order, Op operation) const {\n+    T old_value;\n+    T new_value;\n+    T fetched_value = AtomicAccess::load(dest);\n+    do {\n+      old_value = fetched_value;\n+      new_value = operation(old_value);\n+      fetched_value = AtomicAccess::cmpxchg(dest, old_value, new_value, order);\n+    } while (old_value != fetched_value);\n+    return new_value;\n+  }\n+\n+public:\n+  template<typename T>\n+  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value & bits; });\n+  }\n+\n+  template<typename T>\n+  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value | bits; });\n+  }\n+\n+  template<typename T>\n+  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bitop(dest, order, [&](T value) -> T { return value ^ bits; });\n+  }\n+};\n+\n+\/\/ Implement bitop_then_fetch operations by calling fetch_then_bitop and\n+\/\/ applying the operation to the result and the bits argument.\n+class AtomicAccess::PostfetchBitopsUsingPrefetch {\n+public:\n+  template<typename T>\n+  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bits & AtomicAccess::fetch_then_and(dest, bits, order);\n+  }\n+\n+  template<typename T>\n+  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bits | AtomicAccess::fetch_then_or(dest, bits, order);\n+  }\n+\n+  template<typename T>\n+  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    return bits ^ AtomicAccess::fetch_then_xor(dest, bits, order);\n+  }\n+};\n+\n+\/\/ The default definition uses cmpxchg.  Platforms can override by defining a\n+\/\/ partial specialization providing size, either as a template parameter or as\n+\/\/ a specific value.\n+template<size_t size, bool>\n+class AtomicAccess::PlatformBitops\n+  : public PrefetchBitopsUsingCmpxchg,\n+    public PostfetchBitopsUsingCmpxchg\n+{};\n+\n+template <ScopedFenceType T>\n+class ScopedFenceGeneral: public StackObj {\n+ public:\n+  void prefix() {}\n+  void postfix() {}\n+};\n+\n+\/\/ The following methods can be specialized using simple template specialization\n+\/\/ in the platform specific files for optimization purposes. Otherwise the\n+\/\/ generalized variant is used.\n+\n+template<> inline void ScopedFenceGeneral<X_ACQUIRE>::postfix()       { OrderAccess::acquire(); }\n+template<> inline void ScopedFenceGeneral<RELEASE_X>::prefix()        { OrderAccess::release(); }\n+template<> inline void ScopedFenceGeneral<RELEASE_X_FENCE>::prefix()  { OrderAccess::release(); }\n+template<> inline void ScopedFenceGeneral<RELEASE_X_FENCE>::postfix() { OrderAccess::fence();   }\n+\n+template <ScopedFenceType T>\n+class ScopedFence : public ScopedFenceGeneral<T> {\n+  void *const _field;\n+ public:\n+  ScopedFence(void *const field) : _field(field) { prefix(); }\n+  ~ScopedFence() { postfix(); }\n+  void prefix() { ScopedFenceGeneral<T>::prefix(); }\n+  void postfix() { ScopedFenceGeneral<T>::postfix(); }\n+};\n+\n+\/\/ platform specific in-line definitions - must come before shared definitions\n+\n+#include OS_CPU_HEADER(atomic)\n+\n+\/\/ shared in-line definitions\n+\n+\/\/ size_t casts...\n+#if (SIZE_MAX != UINTPTR_MAX)\n+#error size_t is not WORD_SIZE, interesting platform, but missing implementation here\n+#endif\n+\n+template<typename T>\n+inline T AtomicAccess::load(const volatile T* dest) {\n+  return LoadImpl<T, PlatformLoad<sizeof(T)> >()(dest);\n+}\n+\n+template<size_t byte_size, ScopedFenceType type>\n+struct AtomicAccess::PlatformOrderedLoad {\n+  template <typename T>\n+  T operator()(const volatile T* p) const {\n+    ScopedFence<type> f((void*)p);\n+    return AtomicAccess::load(p);\n+  }\n+};\n+\n+template <typename T>\n+inline T AtomicAccess::load_acquire(const volatile T* p) {\n+  return LoadImpl<T, PlatformOrderedLoad<sizeof(T), X_ACQUIRE> >()(p);\n+}\n+\n+template<typename D, typename T>\n+inline void AtomicAccess::store(volatile D* dest, T store_value) {\n+  StoreImpl<D, T, PlatformStore<sizeof(D)> >()(dest, store_value);\n+}\n+\n+template<size_t byte_size, ScopedFenceType type>\n+struct AtomicAccess::PlatformOrderedStore {\n+  template <typename T>\n+  void operator()(volatile T* p, T v) const {\n+    ScopedFence<type> f((void*)p);\n+    AtomicAccess::store(p, v);\n+  }\n+};\n+\n+template <typename D, typename T>\n+inline void AtomicAccess::release_store(volatile D* p, T v) {\n+  StoreImpl<D, T, PlatformOrderedStore<sizeof(D), RELEASE_X> >()(p, v);\n+}\n+\n+template <typename D, typename T>\n+inline void AtomicAccess::release_store_fence(volatile D* p, T v) {\n+  StoreImpl<D, T, PlatformOrderedStore<sizeof(D), RELEASE_X_FENCE> >()(p, v);\n+}\n+\n+template<typename D, typename I>\n+inline D AtomicAccess::add(D volatile* dest, I add_value,\n+                           atomic_memory_order order) {\n+  return AddImpl<D, I>::add_then_fetch(dest, add_value, order);\n+}\n+\n+template<typename D, typename I>\n+inline D AtomicAccess::fetch_then_add(D volatile* dest, I add_value,\n+                                      atomic_memory_order order) {\n+  return AddImpl<D, I>::fetch_then_add(dest, add_value, order);\n+}\n+\n+template<typename D, typename I>\n+struct AtomicAccess::AddImpl<\n+  D, I,\n+  typename EnableIf<std::is_integral<I>::value &&\n+                    std::is_integral<D>::value &&\n+                    (sizeof(I) <= sizeof(D)) &&\n+                    (std::is_signed<I>::value == std::is_signed<D>::value)>::type>\n+{\n+  static D add_then_fetch(D volatile* dest, I add_value, atomic_memory_order order) {\n+    D addend = add_value;\n+    return PlatformAdd<sizeof(D)>().add_then_fetch(dest, addend, order);\n+  }\n+  static D fetch_then_add(D volatile* dest, I add_value, atomic_memory_order order) {\n+    D addend = add_value;\n+    return PlatformAdd<sizeof(D)>().fetch_then_add(dest, addend, order);\n+  }\n+};\n+\n+template<typename P, typename I>\n+struct AtomicAccess::AddImpl<\n+  P*, I,\n+  typename EnableIf<std::is_integral<I>::value && (sizeof(I) <= sizeof(P*))>::type>\n+{\n+  STATIC_ASSERT(sizeof(intptr_t) == sizeof(P*));\n+  STATIC_ASSERT(sizeof(uintptr_t) == sizeof(P*));\n+\n+  \/\/ Type of the scaled addend.  An integral type of the same size as a\n+  \/\/ pointer, and the same signedness as I.\n+  using SI = std::conditional_t<std::is_signed<I>::value, intptr_t, uintptr_t>;\n+\n+  \/\/ Type of the unscaled destination.  A pointer type with pointee size == 1.\n+  using UP = const char*;\n+\n+  \/\/ Scale add_value by the size of the pointee.\n+  static SI scale_addend(SI add_value) {\n+    return add_value * SI(sizeof(P));\n+  }\n+\n+  \/\/ Casting between P* and UP* here intentionally uses C-style casts,\n+  \/\/ because reinterpret_cast can't cast away cv qualifiers.  Using copy_cv\n+  \/\/ would be an alternative if it existed.\n+\n+  \/\/ Unscale dest to a char* pointee for consistency with scaled addend.\n+  static UP volatile* unscale_dest(P* volatile* dest) {\n+    return (UP volatile*) dest;\n+  }\n+\n+  \/\/ Convert the unscaled char* result to a P*.\n+  static P* scale_result(UP result) {\n+    return (P*) result;\n+  }\n+\n+  static P* add_then_fetch(P* volatile* dest, I addend, atomic_memory_order order) {\n+    return scale_result(PlatformAdd<sizeof(P*)>().add_then_fetch(unscale_dest(dest),\n+                                                                scale_addend(addend),\n+                                                                order));\n+  }\n+\n+  static P* fetch_then_add(P* volatile* dest, I addend, atomic_memory_order order) {\n+    return scale_result(PlatformAdd<sizeof(P*)>().fetch_then_add(unscale_dest(dest),\n+                                                                scale_addend(addend),\n+                                                                order));\n+  }\n+};\n+\n+template<typename Type, typename Fn, typename D, typename I>\n+inline D AtomicAccess::add_using_helper(Fn fn, D volatile* dest, I add_value) {\n+  return PrimitiveConversions::cast<D>(\n+    fn(PrimitiveConversions::cast<Type>(add_value),\n+       reinterpret_cast<Type volatile*>(dest)));\n+}\n+\n+template<typename D, typename U, typename T>\n+inline D AtomicAccess::cmpxchg(D volatile* dest,\n+                               U compare_value,\n+                               T exchange_value,\n+                               atomic_memory_order order) {\n+  return CmpxchgImpl<D, U, T>()(dest, compare_value, exchange_value, order);\n+}\n+\n+template<typename D, typename T>\n+inline bool AtomicAccess::replace_if_null(D* volatile* dest, T* value,\n+                                          atomic_memory_order order) {\n+  \/\/ Presently using a trivial implementation in terms of cmpxchg.\n+  \/\/ Consider adding platform support, to permit the use of compiler\n+  \/\/ intrinsics like gcc's __sync_bool_compare_and_swap.\n+  D* expected_null = nullptr;\n+  return expected_null == cmpxchg(dest, expected_null, value, order);\n+}\n+\n+\/\/ Handle cmpxchg for integral types.\n+\/\/\n+\/\/ All the involved types must be identical.\n+template<typename T>\n+struct AtomicAccess::CmpxchgImpl<\n+  T, T, T,\n+  typename EnableIf<std::is_integral<T>::value>::type>\n+{\n+  T operator()(T volatile* dest, T compare_value, T exchange_value,\n+               atomic_memory_order order) const {\n+    \/\/ Forward to the platform handler for the size of T.\n+    return PlatformCmpxchg<sizeof(T)>()(dest,\n+                                        compare_value,\n+                                        exchange_value,\n+                                        order);\n+  }\n+};\n+\n+\/\/ Handle cmpxchg for pointer types.\n+\/\/\n+\/\/ The destination's type and the compare_value type must be the same,\n+\/\/ ignoring cv-qualifiers; we don't care about the cv-qualifiers of\n+\/\/ the compare_value.\n+\/\/\n+\/\/ The exchange_value must be implicitly convertible to the\n+\/\/ destination's type; it must be type-correct to store the\n+\/\/ exchange_value in the destination.\n+template<typename D, typename U, typename T>\n+struct AtomicAccess::CmpxchgImpl<\n+  D*, U*, T*,\n+  typename EnableIf<AtomicAccess::IsPointerConvertible<T*, D*>::value &&\n+                    std::is_same<std::remove_cv_t<D>,\n+                                 std::remove_cv_t<U>>::value>::type>\n+{\n+  D* operator()(D* volatile* dest, U* compare_value, T* exchange_value,\n+               atomic_memory_order order) const {\n+    \/\/ Allow derived to base conversion, and adding cv-qualifiers.\n+    D* new_value = exchange_value;\n+    \/\/ Don't care what the CV qualifiers for compare_value are,\n+    \/\/ but we need to match D* when calling platform support.\n+    D* old_value = const_cast<D*>(compare_value);\n+    return PlatformCmpxchg<sizeof(D*)>()(dest, old_value, new_value, order);\n+  }\n+};\n+\n+\/\/ Handle cmpxchg for types that have a translator.\n+\/\/\n+\/\/ All the involved types must be identical.\n+\/\/\n+\/\/ This translates the original call into a call on the decayed\n+\/\/ arguments, and returns the recovered result of that translated\n+\/\/ call.\n+template<typename T>\n+struct AtomicAccess::CmpxchgImpl<\n+  T, T, T,\n+  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n+{\n+  T operator()(T volatile* dest, T compare_value, T exchange_value,\n+               atomic_memory_order order) const {\n+    typedef PrimitiveConversions::Translate<T> Translator;\n+    typedef typename Translator::Decayed Decayed;\n+    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n+    return Translator::recover(\n+      cmpxchg(reinterpret_cast<Decayed volatile*>(dest),\n+              Translator::decay(compare_value),\n+              Translator::decay(exchange_value),\n+              order));\n+  }\n+};\n+\n+template<typename Type, typename Fn, typename T>\n+inline T AtomicAccess::cmpxchg_using_helper(Fn fn,\n+                                            T volatile* dest,\n+                                            T compare_value,\n+                                            T exchange_value) {\n+  STATIC_ASSERT(sizeof(Type) == sizeof(T));\n+  return PrimitiveConversions::cast<T>(\n+    fn(PrimitiveConversions::cast<Type>(exchange_value),\n+       reinterpret_cast<Type volatile*>(dest),\n+       PrimitiveConversions::cast<Type>(compare_value)));\n+}\n+\n+inline uint32_t AtomicAccess::CmpxchgByteUsingInt::set_byte_in_int(uint32_t n,\n+                                                                   uint8_t b,\n+                                                                   uint32_t idx) {\n+  uint32_t bitsIdx = BitsPerByte * idx;\n+  return (n & ~(static_cast<uint32_t>(0xff) << bitsIdx))\n+          | (static_cast<uint32_t>(b) << bitsIdx);\n+}\n+\n+inline uint8_t AtomicAccess::CmpxchgByteUsingInt::get_byte_in_int(uint32_t n,\n+                                                                  uint32_t idx) {\n+  uint32_t bitsIdx = BitsPerByte * idx;\n+  return (uint8_t)(n >> bitsIdx);\n+}\n+\n+template<typename T>\n+inline T AtomicAccess::CmpxchgByteUsingInt::operator()(T volatile* dest,\n+                                                       T compare_value,\n+                                                       T exchange_value,\n+                                                       atomic_memory_order order) const {\n+  STATIC_ASSERT(sizeof(T) == sizeof(uint8_t));\n+  uint8_t canon_exchange_value = exchange_value;\n+  uint8_t canon_compare_value = compare_value;\n+  volatile uint32_t* aligned_dest\n+    = reinterpret_cast<volatile uint32_t*>(align_down(dest, sizeof(uint32_t)));\n+  uint32_t offset = checked_cast<uint32_t>(pointer_delta(dest, aligned_dest, 1));\n+\n+  uint32_t idx = (Endian::NATIVE == Endian::BIG)\n+                   ? (sizeof(uint32_t) - 1 - offset)\n+                   : offset;\n+\n+  \/\/ current value may not be what we are looking for, so force it\n+  \/\/ to that value so the initial cmpxchg will fail if it is different\n+  uint32_t cur = set_byte_in_int(AtomicAccess::load(aligned_dest), canon_compare_value, idx);\n+\n+  \/\/ always execute a real cmpxchg so that we get the required memory\n+  \/\/ barriers even on initial failure\n+  do {\n+    \/\/ value to swap in matches current value\n+    \/\/ except for the one byte we want to update\n+    uint32_t new_value = set_byte_in_int(cur, canon_exchange_value, idx);\n+\n+    uint32_t res = cmpxchg(aligned_dest, cur, new_value, order);\n+    if (res == cur) break;      \/\/ success\n+\n+    \/\/ at least one byte in the int changed value, so update\n+    \/\/ our view of the current int\n+    cur = res;\n+    \/\/ if our byte is still as cur we loop and try again\n+  } while (get_byte_in_int(cur, idx) == canon_compare_value);\n+\n+  return PrimitiveConversions::cast<T>(get_byte_in_int(cur, idx));\n+}\n+\n+\/\/ Handle xchg for integral types.\n+\/\/\n+\/\/ All the involved types must be identical.\n+template<typename T>\n+struct AtomicAccess::XchgImpl<\n+  T, T,\n+  typename EnableIf<std::is_integral<T>::value>::type>\n+{\n+  T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {\n+    \/\/ Forward to the platform handler for the size of T.\n+    return PlatformXchg<sizeof(T)>()(dest, exchange_value, order);\n+  }\n+};\n+\n+\/\/ Handle xchg for pointer types.\n+\/\/\n+\/\/ The exchange_value must be implicitly convertible to the\n+\/\/ destination's type; it must be type-correct to store the\n+\/\/ exchange_value in the destination.\n+template<typename D, typename T>\n+struct AtomicAccess::XchgImpl<\n+  D*, T*,\n+  typename EnableIf<AtomicAccess::IsPointerConvertible<T*, D*>::value>::type>\n+{\n+  D* operator()(D* volatile* dest, T* exchange_value, atomic_memory_order order) const {\n+    \/\/ Allow derived to base conversion, and adding cv-qualifiers.\n+    D* new_value = exchange_value;\n+    return PlatformXchg<sizeof(D*)>()(dest, new_value, order);\n+  }\n+};\n+\n+\/\/ Handle xchg for types that have a translator.\n+\/\/\n+\/\/ All the involved types must be identical.\n+\/\/\n+\/\/ This translates the original call into a call on the decayed\n+\/\/ arguments, and returns the recovered result of that translated\n+\/\/ call.\n+template<typename T>\n+struct AtomicAccess::XchgImpl<\n+  T, T,\n+  typename EnableIf<PrimitiveConversions::Translate<T>::value>::type>\n+{\n+  T operator()(T volatile* dest, T exchange_value, atomic_memory_order order) const {\n+    typedef PrimitiveConversions::Translate<T> Translator;\n+    typedef typename Translator::Decayed Decayed;\n+    STATIC_ASSERT(sizeof(T) == sizeof(Decayed));\n+    return Translator::recover(\n+      xchg(reinterpret_cast<Decayed volatile*>(dest),\n+           Translator::decay(exchange_value),\n+           order));\n+  }\n+};\n+\n+template<typename Type, typename Fn, typename T>\n+inline T AtomicAccess::xchg_using_helper(Fn fn,\n+                                         T volatile* dest,\n+                                         T exchange_value) {\n+  STATIC_ASSERT(sizeof(Type) == sizeof(T));\n+  \/\/ Notice the swapped order of arguments. Change when\/if stubs are rewritten.\n+  return PrimitiveConversions::cast<T>(\n+    fn(PrimitiveConversions::cast<Type>(exchange_value),\n+       reinterpret_cast<Type volatile*>(dest)));\n+}\n+\n+template<typename D, typename T>\n+inline D AtomicAccess::xchg(volatile D* dest, T exchange_value, atomic_memory_order order) {\n+  return XchgImpl<D, T>()(dest, exchange_value, order);\n+}\n+\n+template<size_t byte_size>\n+template<typename T>\n+inline T AtomicAccess::XchgUsingCmpxchg<byte_size>::operator()(T volatile* dest,\n+                                                               T exchange_value,\n+                                                               atomic_memory_order order) const {\n+  STATIC_ASSERT(byte_size == sizeof(T));\n+\n+  T old_value;\n+  do {\n+    old_value = AtomicAccess::load(dest);\n+  } while (old_value != AtomicAccess::cmpxchg(dest, old_value, exchange_value, order));\n+  return old_value;\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_ATOMICACCESS_HPP\n","filename":"src\/hotspot\/share\/runtime\/atomicAccess.hpp","additions":1235,"deletions":0,"binary":false,"changes":1235,"status":"added"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -48,2 +48,2 @@\n-  uintptr_t get_metadata() const { return Atomic::load(&_metadata); }\n-  void set_metadata(uintptr_t value) { Atomic::store(&_metadata, value); }\n+  uintptr_t get_metadata() const { return AtomicAccess::load(&_metadata); }\n+  void set_metadata(uintptr_t value) { AtomicAccess::store(&_metadata, value); }\n","filename":"src\/hotspot\/share\/runtime\/basicLock.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  Atomic::store(&_metadata, header.value());\n+  AtomicAccess::store(&_metadata, header.value());\n","filename":"src\/hotspot\/share\/runtime\/basicLock.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -157,1 +157,1 @@\n-  return Atomic::load(chunk->field_addr<uint8_t>(_flags_offset));\n+  return AtomicAccess::load(chunk->field_addr<uint8_t>(_flags_offset));\n@@ -161,1 +161,1 @@\n-  Atomic::store(chunk->field_addr<uint8_t>(_flags_offset), value);\n+  AtomicAccess::store(chunk->field_addr<uint8_t>(_flags_offset), value);\n@@ -165,1 +165,1 @@\n-  return Atomic::load_acquire(chunk->field_addr<uint8_t>(_flags_offset));\n+  return AtomicAccess::load_acquire(chunk->field_addr<uint8_t>(_flags_offset));\n@@ -169,1 +169,1 @@\n-  Atomic::release_store(chunk->field_addr<uint8_t>(_flags_offset), value);\n+  AtomicAccess::release_store(chunk->field_addr<uint8_t>(_flags_offset), value);\n@@ -173,1 +173,1 @@\n-  return Atomic::cmpxchg(chunk->field_addr<uint8_t>(_flags_offset), expected_value, new_value) == expected_value;\n+  return AtomicAccess::cmpxchg(chunk->field_addr<uint8_t>(_flags_offset), expected_value, new_value) == expected_value;\n@@ -190,1 +190,1 @@\n-  return Atomic::load(chunk->field_addr<uint8_t>(_lockStackSize_offset));\n+  return AtomicAccess::load(chunk->field_addr<uint8_t>(_lockStackSize_offset));\n@@ -194,1 +194,1 @@\n-  Atomic::store(chunk->field_addr<uint8_t>(_lockStackSize_offset), value);\n+  AtomicAccess::store(chunk->field_addr<uint8_t>(_lockStackSize_offset), value);\n","filename":"src\/hotspot\/share\/runtime\/continuationJavaClasses.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -73,1 +73,1 @@\n-  Atomic::add(&(instance->_gc_total_cpu_time_diff), diff);\n+  AtomicAccess::add(&(instance->_gc_total_cpu_time_diff), diff);\n@@ -80,1 +80,1 @@\n-  jlong fetched_value = Atomic::xchg(&(instance->_gc_total_cpu_time_diff), new_value);\n+  jlong fetched_value = AtomicAccess::xchg(&(instance->_gc_total_cpu_time_diff), new_value);\n","filename":"src\/hotspot\/share\/runtime\/cpuTimeCounters.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,1 +67,1 @@\n-  \/\/ It is incremented using Atomic::add() to prevent race conditions, and\n+  \/\/ It is incremented using AtomicAccess::add() to prevent race conditions, and\n","filename":"src\/hotspot\/share\/runtime\/cpuTimeCounters.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -138,1 +138,1 @@\n-  Atomic::store(&nm->_deoptimization_status, status);\n+  AtomicAccess::store(&nm->_deoptimization_status, status);\n@@ -1122,1 +1122,1 @@\n-      if (!Atomic::replace_if_null(&_singleton, s)) {\n+      if (!AtomicAccess::replace_if_null(&_singleton, s)) {\n@@ -1185,1 +1185,1 @@\n-      if (!Atomic::replace_if_null(&_singleton, s)) {\n+      if (!AtomicAccess::replace_if_null(&_singleton, s)) {\n@@ -1988,1 +1988,1 @@\n-  if (1 == critical_section || Atomic::cmpxchg(&critical_section, 0, 1) == 1) {\n+  if (1 == critical_section || AtomicAccess::cmpxchg(&critical_section, 0, 1) == 1) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -77,1 +77,1 @@\n-    int32_t val = Atomic::load(&_pending_threads);\n+    int32_t val = AtomicAccess::load(&_pending_threads);\n@@ -81,2 +81,2 @@\n-  void add_target_count(int count) { Atomic::add(&_pending_threads, count); }\n-  int32_t pending_threads()        { return Atomic::load(&_pending_threads); }\n+  void add_target_count(int count) { AtomicAccess::add(&_pending_threads, count); }\n+  int32_t pending_threads()        { return AtomicAccess::load(&_pending_threads); }\n@@ -351,1 +351,1 @@\n-  Atomic::dec(&_pending_threads);\n+  AtomicAccess::dec(&_pending_threads);\n","filename":"src\/hotspot\/share\/runtime\/handshake.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-  void set_active_handshaker(Thread* thread) { Atomic::store(&_active_handshaker, thread); }\n+  void set_active_handshaker(Thread* thread) { AtomicAccess::store(&_active_handshaker, thread); }\n@@ -150,1 +150,1 @@\n-  Thread* active_handshaker() const { return Atomic::load(&_active_handshaker); }\n+  Thread* active_handshaker() const { return AtomicAccess::load(&_active_handshaker); }\n","filename":"src\/hotspot\/share\/runtime\/handshake.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -241,1 +241,1 @@\n-  return Atomic::load_acquire(&_init_completed);\n+  return AtomicAccess::load_acquire(&_init_completed);\n@@ -254,1 +254,1 @@\n-  Atomic::release_store(&_init_completed, true);\n+  AtomicAccess::release_store(&_init_completed, true);\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/interfaceSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1072,1 +1072,1 @@\n-  return Atomic::load(&_exception_oop);\n+  return AtomicAccess::load(&_exception_oop);\n@@ -1076,1 +1076,1 @@\n-  Atomic::store(&_exception_oop, o);\n+  AtomicAccess::store(&_exception_oop, o);\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -185,1 +185,1 @@\n-    \/\/ Use Atomic::load() to prevent data race between concurrent modification and\n+    \/\/ Use AtomicAccess::load() to prevent data race between concurrent modification and\n@@ -188,1 +188,1 @@\n-    return Atomic::load(&_current_pending_monitor);\n+    return AtomicAccess::load(&_current_pending_monitor);\n@@ -191,1 +191,1 @@\n-    Atomic::store(&_current_pending_monitor, monitor);\n+    AtomicAccess::store(&_current_pending_monitor, monitor);\n@@ -201,1 +201,1 @@\n-    return Atomic::load(&_current_waiting_monitor);\n+    return AtomicAccess::load(&_current_waiting_monitor);\n@@ -204,1 +204,1 @@\n-    Atomic::store(&_current_waiting_monitor, monitor);\n+    AtomicAccess::store(&_current_waiting_monitor, monitor);\n@@ -718,1 +718,1 @@\n-    return Atomic::load(&_carrier_thread_suspended);\n+    return AtomicAccess::load(&_carrier_thread_suspended);\n@@ -730,2 +730,2 @@\n-  bool VTMS_transition_mark() const              { return Atomic::load(&_VTMS_transition_mark); }\n-  void set_VTMS_transition_mark(bool val)        { Atomic::store(&_VTMS_transition_mark, val); }\n+  bool VTMS_transition_mark() const              { return AtomicAccess::load(&_VTMS_transition_mark); }\n+  void set_VTMS_transition_mark(bool val)        { AtomicAccess::store(&_VTMS_transition_mark, val); }\n@@ -949,1 +949,1 @@\n-  bool in_critical_atomic() { return Atomic::load(&_jni_active_critical) > 0; }\n+  bool in_critical_atomic() { return AtomicAccess::load(&_jni_active_critical) > 0; }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -50,1 +50,1 @@\n-  while (Atomic::cmpxchg(&_suspend_flags, flags, (flags | f)) != flags);\n+  while (AtomicAccess::cmpxchg(&_suspend_flags, flags, (flags | f)) != flags);\n@@ -57,1 +57,1 @@\n-  while (Atomic::cmpxchg(&_suspend_flags, flags, (flags & ~f)) != flags);\n+  while (AtomicAccess::cmpxchg(&_suspend_flags, flags, (flags & ~f)) != flags);\n@@ -69,1 +69,1 @@\n-  return Atomic::cmpxchg(&_carrier_thread_suspended, false, true) == false;\n+  return AtomicAccess::cmpxchg(&_carrier_thread_suspended, false, true) == false;\n@@ -72,1 +72,1 @@\n-  return Atomic::cmpxchg(&_carrier_thread_suspended, true, false) == true;\n+  return AtomicAccess::cmpxchg(&_carrier_thread_suspended, true, false) == true;\n@@ -140,1 +140,1 @@\n-  return Atomic::load_acquire(&_thread_state);\n+  return AtomicAccess::load_acquire(&_thread_state);\n@@ -142,1 +142,1 @@\n-  return Atomic::load(&_thread_state);\n+  return AtomicAccess::load(&_thread_state);\n@@ -152,1 +152,1 @@\n-  Atomic::release_store(&_thread_state, s);\n+  AtomicAccess::release_store(&_thread_state, s);\n@@ -154,1 +154,1 @@\n-  Atomic::store(&_thread_state, s);\n+  AtomicAccess::store(&_thread_state, s);\n@@ -201,1 +201,1 @@\n-  TerminatedTypes l_terminated = Atomic::load_acquire(&_terminated);\n+  TerminatedTypes l_terminated = AtomicAccess::load_acquire(&_terminated);\n@@ -208,1 +208,1 @@\n-  TerminatedTypes l_terminated = Atomic::load_acquire(&_terminated);\n+  TerminatedTypes l_terminated = AtomicAccess::load_acquire(&_terminated);\n@@ -214,1 +214,1 @@\n-  TerminatedTypes l_terminated = Atomic::load_acquire(&_terminated);\n+  TerminatedTypes l_terminated = AtomicAccess::load_acquire(&_terminated);\n@@ -219,1 +219,1 @@\n-  Atomic::release_store(&_terminated, t);\n+  AtomicAccess::release_store(&_terminated, t);\n","filename":"src\/hotspot\/share\/runtime\/javaThread.inline.hpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -338,1 +338,1 @@\n-    Atomic::inc(&_blocks_allocated);\n+    AtomicAccess::inc(&_blocks_allocated);\n@@ -375,1 +375,1 @@\n-      Atomic::dec(&_blocks_allocated);\n+      AtomicAccess::dec(&_blocks_allocated);\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -118,1 +118,1 @@\n-    Atomic::inc(&_items_count, memory_order_relaxed);\n+    AtomicAccess::inc(&_items_count, memory_order_relaxed);\n@@ -122,1 +122,1 @@\n-    Atomic::dec(&_items_count, memory_order_relaxed);\n+    AtomicAccess::dec(&_items_count, memory_order_relaxed);\n@@ -126,1 +126,1 @@\n-    size_t count = Atomic::load(&_items_count);\n+    size_t count = AtomicAccess::load(&_items_count);\n@@ -198,2 +198,2 @@\n-    if (!_table->is_max_size_reached() && !Atomic::load(&_resize)) {\n-      Atomic::store(&_resize, true);\n+    if (!_table->is_max_size_reached() && !AtomicAccess::load(&_resize)) {\n+      AtomicAccess::store(&_resize, true);\n@@ -219,1 +219,1 @@\n-    return should_grow() || should_shrink() || Atomic::load(&_resize);\n+    return should_grow() || should_shrink() || AtomicAccess::load(&_resize);\n@@ -268,1 +268,1 @@\n-      if (!_table->is_max_size_reached() && Atomic::load(&_resize)) {\n+      if (!_table->is_max_size_reached() && AtomicAccess::load(&_resize)) {\n@@ -275,1 +275,1 @@\n-    Atomic::store(&_resize, false);\n+    AtomicAccess::store(&_resize, false);\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -107,1 +107,1 @@\n-  void raw_set_owner(Thread* new_owner) { Atomic::store(&_owner, new_owner); }\n+  void raw_set_owner(Thread* new_owner) { AtomicAccess::store(&_owner, new_owner); }\n@@ -205,1 +205,1 @@\n-  Thread* owner() const         { return Atomic::load(&_owner); }\n+  Thread* owner() const         { return AtomicAccess::load(&_owner); }\n","filename":"src\/hotspot\/share\/runtime\/mutex.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -57,1 +57,1 @@\n-  _current(Atomic::load_acquire(&_the_list._head))\n+  _current(AtomicAccess::load_acquire(&_the_list._head))\n@@ -66,1 +66,1 @@\n-  _current = Atomic::load_acquire(&_current->_next);\n+  _current = AtomicAccess::load_acquire(&_current->_next);\n@@ -79,2 +79,2 @@\n-  Atomic::release_store(&_next, _the_list._head);\n-  Atomic::release_store(&_the_list._head, this);\n+  AtomicAccess::release_store(&_next, _the_list._head);\n+  AtomicAccess::release_store(&_the_list._head, this);\n","filename":"src\/hotspot\/share\/runtime\/nonJavaThread.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -699,1 +699,1 @@\n-    ObjectWaiter* head = Atomic::load(&_entry_list);\n+    ObjectWaiter* head = AtomicAccess::load(&_entry_list);\n@@ -701,1 +701,1 @@\n-    if (Atomic::cmpxchg(&_entry_list, head, node) == head) {\n+    if (AtomicAccess::cmpxchg(&_entry_list, head, node) == head) {\n@@ -718,1 +718,1 @@\n-    ObjectWaiter* head = Atomic::load(&_entry_list);\n+    ObjectWaiter* head = AtomicAccess::load(&_entry_list);\n@@ -720,1 +720,1 @@\n-    if (Atomic::cmpxchg(&_entry_list, head, node) == head) {\n+    if (AtomicAccess::cmpxchg(&_entry_list, head, node) == head) {\n@@ -809,1 +809,1 @@\n-    if (Atomic::cmpxchg(&_contentions, 0, INT_MIN) != 0) {\n+    if (AtomicAccess::cmpxchg(&_contentions, 0, INT_MIN) != 0) {\n@@ -826,1 +826,1 @@\n-  ObjectWaiter* w = Atomic::load(&_entry_list);\n+  ObjectWaiter* w = AtomicAccess::load(&_entry_list);\n@@ -1273,1 +1273,1 @@\n-  ObjectWaiter* w = Atomic::load_acquire(&_entry_list);\n+  ObjectWaiter* w = AtomicAccess::load_acquire(&_entry_list);\n@@ -1342,1 +1342,1 @@\n-    ObjectWaiter* w = Atomic::load(&_entry_list);\n+    ObjectWaiter* w = AtomicAccess::load(&_entry_list);\n@@ -1345,1 +1345,1 @@\n-      if (Atomic::cmpxchg(&_entry_list, w, (ObjectWaiter*)nullptr) == w) {\n+      if (AtomicAccess::cmpxchg(&_entry_list, w, (ObjectWaiter*)nullptr) == w) {\n@@ -1382,1 +1382,1 @@\n-    ObjectWaiter* w = Atomic::load(&_entry_list);\n+    ObjectWaiter* w = AtomicAccess::load(&_entry_list);\n@@ -1388,1 +1388,1 @@\n-      if (Atomic::cmpxchg(&_entry_list, w, next) == w) {\n+      if (AtomicAccess::cmpxchg(&_entry_list, w, next) == w) {\n@@ -1515,1 +1515,1 @@\n-      ObjectWaiter* w = Atomic::load(&_entry_list);\n+      ObjectWaiter* w = AtomicAccess::load(&_entry_list);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -287,1 +287,1 @@\n-  \/\/ old_value, using Atomic::cmpxchg(). Otherwise, does not change the\n+  \/\/ old_value, using AtomicAccess::cmpxchg(). Otherwise, does not change the\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -67,1 +67,1 @@\n-  return Atomic::load(&_metadata);\n+  return AtomicAccess::load(&_metadata);\n@@ -71,1 +71,1 @@\n-  Atomic::store(&_metadata, value);\n+  AtomicAccess::store(&_metadata, value);\n@@ -116,1 +116,1 @@\n-  return Atomic::load(&_owner);\n+  return AtomicAccess::load(&_owner);\n@@ -120,1 +120,1 @@\n-  return Atomic::load(&_stack_locker);\n+  return AtomicAccess::load(&_stack_locker);\n@@ -124,1 +124,1 @@\n-  Atomic::store(&_stack_locker, locker);\n+  AtomicAccess::store(&_stack_locker, locker);\n@@ -139,1 +139,1 @@\n-  return Atomic::load(&_contentions);\n+  return AtomicAccess::load(&_contentions);\n@@ -144,1 +144,1 @@\n-  Atomic::add(&_contentions, value);\n+  AtomicAccess::add(&_contentions, value);\n@@ -162,1 +162,1 @@\n-  int64_t prev = Atomic::load(&_owner);\n+  int64_t prev = AtomicAccess::load(&_owner);\n@@ -166,1 +166,1 @@\n-  Atomic::release_store(&_owner, NO_OWNER);\n+  AtomicAccess::release_store(&_owner, NO_OWNER);\n@@ -176,1 +176,1 @@\n-  int64_t prev = Atomic::load(&_owner);\n+  int64_t prev = AtomicAccess::load(&_owner);\n@@ -181,1 +181,1 @@\n-  Atomic::store(&_owner, new_value);\n+  AtomicAccess::store(&_owner, new_value);\n@@ -197,1 +197,1 @@\n-  int64_t prev = Atomic::cmpxchg(&_owner, old_value, new_value);\n+  int64_t prev = AtomicAccess::cmpxchg(&_owner, old_value, new_value);\n@@ -212,1 +212,1 @@\n-  return Atomic::load(&_succ) != NO_OWNER;\n+  return AtomicAccess::load(&_succ) != NO_OWNER;\n@@ -216,1 +216,1 @@\n-  return owner_id_from(thread) == Atomic::load(&_succ);\n+  return owner_id_from(thread) == AtomicAccess::load(&_succ);\n@@ -220,1 +220,1 @@\n-  Atomic::store(&_succ, owner_id_from(thread));\n+  AtomicAccess::store(&_succ, owner_id_from(thread));\n@@ -224,1 +224,1 @@\n-  Atomic::store(&_succ, java_lang_Thread::thread_id(vthread));\n+  AtomicAccess::store(&_succ, java_lang_Thread::thread_id(vthread));\n@@ -228,1 +228,1 @@\n-  Atomic::store(&_succ, NO_OWNER);\n+  AtomicAccess::store(&_succ, NO_OWNER);\n@@ -232,1 +232,1 @@\n-  return Atomic::load(&_succ);\n+  return AtomicAccess::load(&_succ);\n@@ -241,1 +241,1 @@\n-  return Atomic::load(&_next_om);\n+  return AtomicAccess::load(&_next_om);\n@@ -246,1 +246,1 @@\n-  Atomic::store(&_next_om, new_value);\n+  AtomicAccess::store(&_next_om, new_value);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -830,1 +830,1 @@\n-    if (Atomic::cmpxchg(&_rand_seed, seed, rand, memory_order_relaxed) == seed) {\n+    if (AtomicAccess::cmpxchg(&_rand_seed, seed, rand, memory_order_relaxed) == seed) {\n@@ -1579,1 +1579,1 @@\n-  Atomic::release_store(&_image_release_file_content, tmp);\n+  AtomicAccess::release_store(&_image_release_file_content, tmp);\n@@ -1584,1 +1584,1 @@\n-  char* ifrc = Atomic::load_acquire(&_image_release_file_content);\n+  char* ifrc = AtomicAccess::load_acquire(&_image_release_file_content);\n@@ -2349,1 +2349,1 @@\n-        Atomic::add(reinterpret_cast<int*>(cur), 0, memory_order_relaxed);\n+        AtomicAccess::add(reinterpret_cast<int*>(cur), 0, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1109,1 +1109,1 @@\n-\/\/ so arguably we should provide Atomic::SpinPause() instead\n+\/\/ so arguably we should provide AtomicAccess::SpinPause() instead\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -253,1 +253,1 @@\n-  Atomic::store(&_has_PerfData, false);\n+  AtomicAccess::store(&_has_PerfData, false);\n@@ -279,1 +279,1 @@\n-    Atomic::release_store(&_has_PerfData, true);\n+    AtomicAccess::release_store(&_has_PerfData, true);\n","filename":"src\/hotspot\/share\/runtime\/perfData.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -700,1 +700,1 @@\n-    static bool has_PerfData() { return Atomic::load_acquire(&_has_PerfData); }\n+    static bool has_PerfData() { return AtomicAccess::load_acquire(&_has_PerfData); }\n","filename":"src\/hotspot\/share\/runtime\/perfData.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -157,1 +157,1 @@\n-  Atomic::release_store(&_initialized, 1);\n+  AtomicAccess::release_store(&_initialized, 1);\n@@ -270,1 +270,1 @@\n-  return Atomic::load_acquire(&_initialized) != 0;\n+  return AtomicAccess::load_acquire(&_initialized) != 0;\n","filename":"src\/hotspot\/share\/runtime\/perfMemory.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -309,1 +309,1 @@\n-  Atomic::release_store(&_safepoint_counter, _safepoint_counter + 1);\n+  AtomicAccess::release_store(&_safepoint_counter, _safepoint_counter + 1);\n@@ -444,1 +444,1 @@\n-    Atomic::release_store(&_safepoint_counter, _safepoint_counter + 1);\n+    AtomicAccess::release_store(&_safepoint_counter, _safepoint_counter + 1);\n@@ -612,1 +612,1 @@\n-    Atomic::inc(&_nof_threads_hit_polling_page);\n+    AtomicAccess::inc(&_nof_threads_hit_polling_page);\n@@ -688,1 +688,1 @@\n-  return Atomic::load_acquire(&_safepoint_id);\n+  return AtomicAccess::load_acquire(&_safepoint_id);\n@@ -692,1 +692,1 @@\n-  Atomic::release_store(&_safepoint_id, SafepointSynchronize::InactiveSafepointCounter);\n+  AtomicAccess::release_store(&_safepoint_id, SafepointSynchronize::InactiveSafepointCounter);\n@@ -696,1 +696,1 @@\n-  Atomic::release_store(&_safepoint_id, safepoint_id);\n+  AtomicAccess::release_store(&_safepoint_id, safepoint_id);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -40,1 +40,1 @@\n-  Atomic::store(&_polling_page, poll_value);\n+  AtomicAccess::store(&_polling_page, poll_value);\n@@ -45,1 +45,1 @@\n-  Atomic::store(&_polling_word, poll_value);\n+  AtomicAccess::store(&_polling_word, poll_value);\n@@ -51,1 +51,1 @@\n-  return Atomic::load_acquire(&_polling_word);\n+  return AtomicAccess::load_acquire(&_polling_word);\n","filename":"src\/hotspot\/share\/runtime\/safepointMechanism.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -899,1 +899,1 @@\n-  Atomic::inc(&Exceptions::_stack_overflow_errors);\n+  AtomicAccess::inc(&Exceptions::_stack_overflow_errors);\n@@ -1379,1 +1379,1 @@\n-  Atomic::inc(addr);\n+  AtomicAccess::inc(addr);\n@@ -1605,1 +1605,1 @@\n-  Atomic::inc(&_ic_miss_ctr);\n+  AtomicAccess::inc(&_ic_miss_ctr);\n@@ -1731,1 +1731,1 @@\n-  Atomic::inc(&_wrong_method_ctr);\n+  AtomicAccess::inc(&_wrong_method_ctr);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -189,1 +189,1 @@\n-  uint32_t state = Atomic::load(&_state);\n+  uint32_t state = AtomicAccess::load(&_state);\n@@ -234,2 +234,2 @@\n-    Atomic::release_store(&_watermark, _iterator->callee());\n-    Atomic::release_store(&_state, StackWatermarkState::create(epoch_id(), false \/* is_done *\/)); \/\/ release watermark w.r.t. epoch\n+    AtomicAccess::release_store(&_watermark, _iterator->callee());\n+    AtomicAccess::release_store(&_state, StackWatermarkState::create(epoch_id(), false \/* is_done *\/)); \/\/ release watermark w.r.t. epoch\n@@ -237,2 +237,2 @@\n-    Atomic::release_store(&_watermark, uintptr_t(0)); \/\/ Release stack data modifications w.r.t. watermark\n-    Atomic::release_store(&_state, StackWatermarkState::create(epoch_id(), true \/* is_done *\/)); \/\/ release watermark w.r.t. epoch\n+    AtomicAccess::release_store(&_watermark, uintptr_t(0)); \/\/ Release stack data modifications w.r.t. watermark\n+    AtomicAccess::release_store(&_state, StackWatermarkState::create(epoch_id(), true \/* is_done *\/)); \/\/ release watermark w.r.t. epoch\n@@ -266,1 +266,1 @@\n-  return Atomic::load_acquire(&_watermark);\n+  return AtomicAccess::load_acquire(&_watermark);\n@@ -287,1 +287,1 @@\n-  return processing_started(Atomic::load(&_state));\n+  return processing_started(AtomicAccess::load(&_state));\n@@ -291,1 +291,1 @@\n-  return processing_started(Atomic::load_acquire(&_state));\n+  return processing_started(AtomicAccess::load_acquire(&_state));\n@@ -295,1 +295,1 @@\n-  return processing_completed(Atomic::load(&_state));\n+  return processing_completed(AtomicAccess::load(&_state));\n@@ -299,1 +299,1 @@\n-  return processing_completed(Atomic::load_acquire(&_state));\n+  return processing_completed(AtomicAccess::load_acquire(&_state));\n","filename":"src\/hotspot\/share\/runtime\/stackWatermark.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/stackWatermarkSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -81,1 +81,1 @@\n-  Atomic::store(&_suspended, is_suspend);\n+  AtomicAccess::store(&_suspended, is_suspend);\n","filename":"src\/hotspot\/share\/runtime\/suspendResumeManager.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-    return Atomic::load(&_suspended);\n+    return AtomicAccess::load(&_suspended);\n","filename":"src\/hotspot\/share\/runtime\/suspendResumeManager.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -73,1 +73,1 @@\n-    head = Atomic::load(&_head);\n+    head = AtomicAccess::load(&_head);\n@@ -75,1 +75,1 @@\n-  } while (Atomic::cmpxchg(&_head, head, m) != head);\n+  } while (AtomicAccess::cmpxchg(&_head, head, m) != head);\n@@ -77,1 +77,1 @@\n-  size_t count = Atomic::add(&_count, 1u, memory_order_relaxed);\n+  size_t count = AtomicAccess::add(&_count, 1u, memory_order_relaxed);\n@@ -80,1 +80,1 @@\n-    old_max = Atomic::load(&_max);\n+    old_max = AtomicAccess::load(&_max);\n@@ -84,1 +84,1 @@\n-  } while (Atomic::cmpxchg(&_max, old_max, count, memory_order_relaxed) != old_max);\n+  } while (AtomicAccess::cmpxchg(&_max, old_max, count, memory_order_relaxed) != old_max);\n@@ -88,1 +88,1 @@\n-  return Atomic::load(&_count);\n+  return AtomicAccess::load(&_count);\n@@ -92,1 +92,1 @@\n-  return Atomic::load(&_max);\n+  return AtomicAccess::load(&_max);\n@@ -113,1 +113,1 @@\n-  ObjectMonitor* m = Atomic::load_acquire(&_head);\n+  ObjectMonitor* m = AtomicAccess::load_acquire(&_head);\n@@ -134,1 +134,1 @@\n-        if (prev == nullptr && Atomic::load(&_head) != m) {\n+        if (prev == nullptr && AtomicAccess::load(&_head) != m) {\n@@ -146,1 +146,1 @@\n-        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, m, next);\n+        ObjectMonitor* prev_head = AtomicAccess::cmpxchg(&_head, m, next);\n@@ -158,1 +158,1 @@\n-        assert(Atomic::load(&_head) != m, \"Sanity\");\n+        assert(AtomicAccess::load(&_head) != m, \"Sanity\");\n@@ -183,1 +183,1 @@\n-    ObjectMonitor* m = Atomic::load_acquire(&_head);\n+    ObjectMonitor* m = AtomicAccess::load_acquire(&_head);\n@@ -191,1 +191,1 @@\n-  Atomic::sub(&_count, unlinked_count);\n+  AtomicAccess::sub(&_count, unlinked_count);\n@@ -196,1 +196,1 @@\n-  return Iterator(Atomic::load_acquire(&_head));\n+  return Iterator(AtomicAccess::load_acquire(&_head));\n@@ -1087,1 +1087,1 @@\n-      uintptr_t v = Atomic::cmpxchg(monitor->metadata_addr(), mark.value(), temp.value());\n+      uintptr_t v = AtomicAccess::cmpxchg(monitor->metadata_addr(), mark.value(), temp.value());\n@@ -1320,1 +1320,1 @@\n-  Atomic::sub(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n+  AtomicAccess::sub(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n@@ -1324,1 +1324,1 @@\n-  Atomic::add(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n+  AtomicAccess::add(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -293,1 +293,1 @@\n-  Atomic::store(&_ParkEvent, (ParkEvent*)nullptr);\n+  AtomicAccess::store(&_ParkEvent, (ParkEvent*)nullptr);\n@@ -418,1 +418,1 @@\n-    uintx res = Atomic::cmpxchg(&_threads_do_token, token, claim_token);\n+    uintx res = AtomicAccess::cmpxchg(&_threads_do_token, token, claim_token);\n@@ -578,1 +578,1 @@\n-  if (Atomic::cmpxchg(adr, 0, 1) == 0) {\n+  if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) {\n@@ -599,1 +599,1 @@\n-    if (Atomic::cmpxchg(adr, 0, 1) == 0) return;\n+    if (AtomicAccess::cmpxchg(adr, 0, 1) == 0) return;\n@@ -614,1 +614,1 @@\n-  Atomic::release_store(adr, 0);\n+  AtomicAccess::release_store(adr, 0);\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -601,1 +601,1 @@\n-  bool has_terminated()                       { return Atomic::load(&_ParkEvent) == nullptr; };\n+  bool has_terminated()                       { return AtomicAccess::load(&_ParkEvent) == nullptr; };\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -39,1 +39,1 @@\n-  jlong allocated_bytes = Atomic::load_acquire(&_allocated_bytes);\n+  jlong allocated_bytes = AtomicAccess::load_acquire(&_allocated_bytes);\n@@ -62,1 +62,1 @@\n-  return (ThreadsList*)Atomic::cmpxchg(&_threads_hazard_ptr, compare_value, exchange_value);\n+  return (ThreadsList*)AtomicAccess::cmpxchg(&_threads_hazard_ptr, compare_value, exchange_value);\n@@ -66,1 +66,1 @@\n-  return (ThreadsList*)Atomic::load_acquire(&_threads_hazard_ptr);\n+  return (ThreadsList*)AtomicAccess::load_acquire(&_threads_hazard_ptr);\n@@ -70,1 +70,1 @@\n-  Atomic::release_store_fence(&_threads_hazard_ptr, new_list);\n+  AtomicAccess::release_store_fence(&_threads_hazard_ptr, new_list);\n","filename":"src\/hotspot\/share\/runtime\/thread.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -443,1 +443,1 @@\n-  return Atomic::load_acquire(&_sampling_interval);\n+  return AtomicAccess::load_acquire(&_sampling_interval);\n@@ -447,1 +447,1 @@\n-  Atomic::release_store(&_sampling_interval, sampling_interval);\n+  AtomicAccess::release_store(&_sampling_interval, sampling_interval);\n","filename":"src\/hotspot\/share\/runtime\/threadHeapSampler.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -41,1 +41,1 @@\n-  return Atomic::load(&next_thread_id);\n+  return AtomicAccess::load(&next_thread_id);\n@@ -47,2 +47,2 @@\n-    next_tid = Atomic::load(&next_thread_id);\n-  } while (Atomic::cmpxchg(&next_thread_id, next_tid, next_tid + 1) != next_tid);\n+    next_tid = AtomicAccess::load(&next_thread_id);\n+  } while (AtomicAccess::cmpxchg(&next_thread_id, next_tid, next_tid + 1) != next_tid);\n","filename":"src\/hotspot\/share\/runtime\/threadIdentifier.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -71,1 +71,1 @@\n-\/\/ unsigned since this is a time value. Set via Atomic::cmpxchg() in a\n+\/\/ unsigned since this is a time value. Set via AtomicAccess::cmpxchg() in a\n@@ -119,1 +119,1 @@\n-\/\/ unsigned since this is a time value. Set via Atomic::cmpxchg() in a\n+\/\/ unsigned since this is a time value. Set via AtomicAccess::cmpxchg() in a\n@@ -143,1 +143,1 @@\n-  Atomic::add(&_deleted_thread_times, add_value);\n+  AtomicAccess::add(&_deleted_thread_times, add_value);\n@@ -147,1 +147,1 @@\n-  Atomic::inc(&_deleted_thread_cnt);\n+  AtomicAccess::inc(&_deleted_thread_cnt);\n@@ -165,1 +165,1 @@\n-    if (Atomic::cmpxchg(&_deleted_thread_time_max, cur_value, new_value) == cur_value) {\n+    if (AtomicAccess::cmpxchg(&_deleted_thread_time_max, cur_value, new_value) == cur_value) {\n@@ -179,1 +179,1 @@\n-  return (ThreadsList*)Atomic::xchg(&_java_thread_list, new_list);\n+  return (ThreadsList*)AtomicAccess::xchg(&_java_thread_list, new_list);\n@@ -693,1 +693,1 @@\n-  Atomic::dec(&_nested_handle_cnt);\n+  AtomicAccess::dec(&_nested_handle_cnt);\n@@ -736,1 +736,1 @@\n-  Atomic::inc(&_nested_handle_cnt);\n+  AtomicAccess::inc(&_nested_handle_cnt);\n@@ -898,1 +898,1 @@\n-  Atomic::dec(&_delete_notify);\n+  AtomicAccess::dec(&_delete_notify);\n@@ -904,1 +904,1 @@\n-  return (Atomic::load_acquire(&_delete_notify) != 0);\n+  return (AtomicAccess::load_acquire(&_delete_notify) != 0);\n@@ -1057,1 +1057,1 @@\n-  Atomic::inc(&_delete_notify);\n+  AtomicAccess::inc(&_delete_notify);\n","filename":"src\/hotspot\/share\/runtime\/threadSMR.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -117,1 +117,1 @@\n-  Atomic::add(&_tlh_times, add_value);\n+  AtomicAccess::add(&_tlh_times, add_value);\n@@ -121,1 +121,1 @@\n-  Atomic::inc(&_tlh_cnt);\n+  AtomicAccess::inc(&_tlh_cnt);\n@@ -131,1 +131,1 @@\n-    if (Atomic::cmpxchg(&_tlh_time_max, cur_value, new_value) == cur_value) {\n+    if (AtomicAccess::cmpxchg(&_tlh_time_max, cur_value, new_value) == cur_value) {\n@@ -139,1 +139,1 @@\n-  return (ThreadsList*)Atomic::load_acquire(&_java_thread_list);\n+  return (ThreadsList*)AtomicAccess::load_acquire(&_java_thread_list);\n","filename":"src\/hotspot\/share\/runtime\/threadSMR.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -71,1 +71,1 @@\n-  return Atomic::load_acquire(&_armed) != 0;\n+  return AtomicAccess::load_acquire(&_armed) != 0;\n@@ -77,1 +77,1 @@\n-  Atomic::release_store_fence(&_armed, 1);\n+  AtomicAccess::release_store_fence(&_armed, 1);\n@@ -81,1 +81,1 @@\n-  Atomic::release_store_fence(&_armed, 0);\n+  AtomicAccess::release_store_fence(&_armed, 0);\n@@ -160,1 +160,1 @@\n-  Atomic::store(&_is_running, true);\n+  AtomicAccess::store(&_is_running, true);\n","filename":"src\/hotspot\/share\/runtime\/vmThread.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -91,1 +91,1 @@\n-  bool is_running() const { return Atomic::load(&_is_running); }\n+  bool is_running() const { return AtomicAccess::load(&_is_running); }\n","filename":"src\/hotspot\/share\/runtime\/vmThread.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -126,1 +126,1 @@\n-    Atomic::store(&_state, new_state);\n+    AtomicAccess::store(&_state, new_state);\n@@ -130,1 +130,1 @@\n-    return Atomic::load(&_state);\n+    return AtomicAccess::load(&_state);\n@@ -135,1 +135,1 @@\n-    return Atomic::cmpxchg(&_state, cmp_state, new_state);\n+    return AtomicAccess::cmpxchg(&_state, cmp_state, new_state);\n@@ -139,1 +139,1 @@\n-    return Atomic::load(&_state) == AL_INITIALIZED;\n+    return AtomicAccess::load(&_state) == AL_INITIALIZED;\n@@ -143,1 +143,1 @@\n-    Atomic::store(&_state, AL_INITIALIZED);\n+    AtomicAccess::store(&_state, AL_INITIALIZED);\n","filename":"src\/hotspot\/share\/services\/attachListener.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  return Atomic::load(&_has_error);\n+  return AtomicAccess::load(&_has_error);\n@@ -83,1 +83,1 @@\n-  Atomic::store(&_has_error, true);\n+  AtomicAccess::store(&_has_error, true);\n","filename":"src\/hotspot\/share\/services\/cpuTimeUsage.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -108,1 +108,1 @@\n-  return Atomic::load(&_objects_on_heap);\n+  return AtomicAccess::load(&_objects_on_heap);\n@@ -112,1 +112,1 @@\n-  return Atomic::load(&_total_finalizers_run);\n+  return AtomicAccess::load(&_total_finalizers_run);\n@@ -116,1 +116,1 @@\n-  Atomic::inc(&_objects_on_heap, memory_order_relaxed);\n+  AtomicAccess::inc(&_objects_on_heap, memory_order_relaxed);\n@@ -120,2 +120,2 @@\n-  Atomic::inc(&_total_finalizers_run, memory_order_relaxed);\n-  Atomic::dec(&_objects_on_heap, memory_order_relaxed);\n+  AtomicAccess::inc(&_total_finalizers_run, memory_order_relaxed);\n+  AtomicAccess::dec(&_objects_on_heap, memory_order_relaxed);\n@@ -196,1 +196,1 @@\n-  Atomic::store(&_has_work, value);\n+  AtomicAccess::store(&_has_work, value);\n@@ -200,1 +200,1 @@\n-  return Atomic::load(&_has_work);\n+  return AtomicAccess::load(&_has_work);\n","filename":"src\/hotspot\/share\/services\/finalizerService.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1723,2 +1723,2 @@\n-    _thread_serial_num = Atomic::fetch_then_add(thread_counter, 1);\n-    _start_frame_serial_num = Atomic::fetch_then_add(frame_counter, frame_count());\n+    _thread_serial_num = AtomicAccess::fetch_then_add(thread_counter, 1);\n+    _start_frame_serial_num = AtomicAccess::fetch_then_add(frame_counter, frame_count());\n@@ -2255,1 +2255,1 @@\n-    return Atomic::fetch_then_add(&_dump_seq, 1);\n+    return AtomicAccess::fetch_then_add(&_dump_seq, 1);\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/services\/lowMemoryDetector.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -63,1 +63,1 @@\n-  if (Atomic::load_acquire(&_memory_mgr_obj_initialized)) {\n+  if (AtomicAccess::load_acquire(&_memory_mgr_obj_initialized)) {\n@@ -82,1 +82,1 @@\n-  if (!Atomic::load_acquire(&_memory_mgr_obj_initialized)) {\n+  if (!AtomicAccess::load_acquire(&_memory_mgr_obj_initialized)) {\n@@ -139,1 +139,1 @@\n-    if (Atomic::load(&_memory_mgr_obj_initialized)) {\n+    if (AtomicAccess::load(&_memory_mgr_obj_initialized)) {\n@@ -150,1 +150,1 @@\n-      Atomic::release_store(&_memory_mgr_obj_initialized, true);\n+      AtomicAccess::release_store(&_memory_mgr_obj_initialized, true);\n","filename":"src\/hotspot\/share\/services\/memoryManager.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -70,1 +70,1 @@\n-  if (Atomic::load_acquire(&_memory_pool_obj_initialized)) {\n+  if (AtomicAccess::load_acquire(&_memory_pool_obj_initialized)) {\n@@ -93,1 +93,1 @@\n-  if (!Atomic::load_acquire(&_memory_pool_obj_initialized)) {\n+  if (!AtomicAccess::load_acquire(&_memory_pool_obj_initialized)) {\n@@ -134,1 +134,1 @@\n-    if (Atomic::load(&_memory_pool_obj_initialized)) {\n+    if (AtomicAccess::load(&_memory_pool_obj_initialized)) {\n@@ -145,1 +145,1 @@\n-      Atomic::release_store(&_memory_pool_obj_initialized, true);\n+      AtomicAccess::release_store(&_memory_pool_obj_initialized, true);\n","filename":"src\/hotspot\/share\/services\/memoryPool.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -123,1 +123,1 @@\n-  Atomic::inc(&_items_count);\n+  AtomicAccess::inc(&_items_count);\n@@ -128,1 +128,1 @@\n-  Atomic::dec(&_items_count);\n+  AtomicAccess::dec(&_items_count);\n","filename":"src\/hotspot\/share\/services\/threadIdTable.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -143,1 +143,1 @@\n-  Atomic::inc(&_atomic_threads_count);\n+  AtomicAccess::inc(&_atomic_threads_count);\n@@ -152,1 +152,1 @@\n-    Atomic::inc(&_atomic_daemon_threads_count);\n+    AtomicAccess::inc(&_atomic_daemon_threads_count);\n@@ -157,1 +157,1 @@\n-  Atomic::dec(&_atomic_threads_count);\n+  AtomicAccess::dec(&_atomic_threads_count);\n@@ -160,1 +160,1 @@\n-    Atomic::dec(&_atomic_daemon_threads_count);\n+    AtomicAccess::dec(&_atomic_daemon_threads_count);\n","filename":"src\/hotspot\/share\/services\/threadService.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -109,1 +109,1 @@\n-  static jlong exited_allocated_bytes()       { return Atomic::load(&_exited_allocated_bytes); }\n+  static jlong exited_allocated_bytes()       { return AtomicAccess::load(&_exited_allocated_bytes); }\n@@ -114,1 +114,1 @@\n-    Atomic::store(&_exited_allocated_bytes, _exited_allocated_bytes + size);\n+    AtomicAccess::store(&_exited_allocated_bytes, _exited_allocated_bytes + size);\n","filename":"src\/hotspot\/share\/services\/threadService.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -246,1 +246,1 @@\n-    bm_word_t w = Atomic::load(pw);\n+    bm_word_t w = AtomicAccess::load(pw);\n@@ -250,1 +250,1 @@\n-      bm_word_t res = Atomic::cmpxchg(pw, w, nw);\n+      bm_word_t res = AtomicAccess::cmpxchg(pw, w, nw);\n","filename":"src\/hotspot\/share\/utilities\/bitMap.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/bitMap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -47,1 +47,1 @@\n-    return Atomic::load(addr);\n+    return AtomicAccess::load(addr);\n@@ -53,1 +53,1 @@\n-    return Atomic::load_acquire(addr);\n+    return AtomicAccess::load_acquire(addr);\n@@ -77,1 +77,1 @@\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order);\n+    const bm_word_t cur_val = AtomicAccess::cmpxchg(addr, old_val, new_val, memory_order);\n@@ -96,1 +96,1 @@\n-    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order);\n+    const bm_word_t cur_val = AtomicAccess::cmpxchg(addr, old_val, new_val, memory_order);\n","filename":"src\/hotspot\/share\/utilities\/bitMap.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -66,1 +66,1 @@\n-  return Atomic::load_acquire(&_next);\n+  return AtomicAccess::load_acquire(&_next);\n@@ -75,1 +75,1 @@\n-  return Atomic::load_acquire(&_first);\n+  return AtomicAccess::load_acquire(&_first);\n@@ -87,1 +87,1 @@\n-  Atomic::release_store(tmp, clear_set_state(node, *dst));\n+  AtomicAccess::release_store(tmp, clear_set_state(node, *dst));\n@@ -96,1 +96,1 @@\n-  return clear_state(Atomic::load_acquire(&_first));\n+  return clear_state(AtomicAccess::load_acquire(&_first));\n@@ -153,1 +153,1 @@\n-  if (Atomic::cmpxchg(&_first, expect, node) == expect) {\n+  if (AtomicAccess::cmpxchg(&_first, expect, node) == expect) {\n@@ -168,1 +168,1 @@\n-  if (Atomic::cmpxchg(&_first, tmp, set_state(tmp, STATE_LOCK_BIT)) == tmp) {\n+  if (AtomicAccess::cmpxchg(&_first, tmp, set_state(tmp, STATE_LOCK_BIT)) == tmp) {\n@@ -181,1 +181,1 @@\n-  Atomic::release_store(&_first, clear_state(first()));\n+  AtomicAccess::release_store(&_first, clear_state(first()));\n@@ -189,1 +189,1 @@\n-  Atomic::release_store(&_first, set_state(_first, STATE_REDIRECT_BIT));\n+  AtomicAccess::release_store(&_first, set_state(_first, STATE_REDIRECT_BIT));\n@@ -225,2 +225,2 @@\n-  if (Atomic::load_acquire(&_cht->_invisible_epoch) != nullptr) {\n-    Atomic::release_store_fence(&_cht->_invisible_epoch, (Thread*)nullptr);\n+  if (AtomicAccess::load_acquire(&_cht->_invisible_epoch) != nullptr) {\n+    AtomicAccess::release_store_fence(&_cht->_invisible_epoch, (Thread*)nullptr);\n@@ -297,1 +297,1 @@\n-  if (Atomic::load_acquire(&_invisible_epoch) == thread) {\n+  if (AtomicAccess::load_acquire(&_invisible_epoch) == thread) {\n@@ -303,1 +303,1 @@\n-  Atomic::release_store(&_invisible_epoch, thread);\n+  AtomicAccess::release_store(&_invisible_epoch, thread);\n@@ -382,1 +382,1 @@\n-  return Atomic::load_acquire(&_table);\n+  return AtomicAccess::load_acquire(&_table);\n@@ -390,1 +390,1 @@\n-  return Atomic::load_acquire(&_new_table);\n+  return AtomicAccess::load_acquire(&_new_table);\n@@ -400,1 +400,1 @@\n-  Atomic::release_store(&_table, _new_table);\n+  AtomicAccess::release_store(&_table, _new_table);\n@@ -800,1 +800,1 @@\n-  Atomic::release_store(&_table, table);\n+  AtomicAccess::release_store(&_table, table);\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTable.inline.hpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -65,2 +65,2 @@\n-      if (Atomic::load(&_next) < _limit) {\n-        size_t claimed = Atomic::fetch_then_add(&_next, _size);\n+      if (AtomicAccess::load(&_next) < _limit) {\n+        size_t claimed = AtomicAccess::fetch_then_add(&_next, _size);\n@@ -81,1 +81,1 @@\n-      return Atomic::load_acquire(&_next) >= _limit;\n+      return AtomicAccess::load_acquire(&_next) >= _limit;\n","filename":"src\/hotspot\/share\/utilities\/concurrentHashTableTasks.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -306,8 +306,8 @@\n-    case 8:  Atomic::store(&to[7], Atomic::load(&from[7]));\n-    case 7:  Atomic::store(&to[6], Atomic::load(&from[6]));\n-    case 6:  Atomic::store(&to[5], Atomic::load(&from[5]));\n-    case 5:  Atomic::store(&to[4], Atomic::load(&from[4]));\n-    case 4:  Atomic::store(&to[3], Atomic::load(&from[3]));\n-    case 3:  Atomic::store(&to[2], Atomic::load(&from[2]));\n-    case 2:  Atomic::store(&to[1], Atomic::load(&from[1]));\n-    case 1:  Atomic::store(&to[0], Atomic::load(&from[0]));\n+    case 8:  AtomicAccess::store(&to[7], AtomicAccess::load(&from[7]));\n+    case 7:  AtomicAccess::store(&to[6], AtomicAccess::load(&from[6]));\n+    case 6:  AtomicAccess::store(&to[5], AtomicAccess::load(&from[5]));\n+    case 5:  AtomicAccess::store(&to[4], AtomicAccess::load(&from[4]));\n+    case 4:  AtomicAccess::store(&to[3], AtomicAccess::load(&from[3]));\n+    case 3:  AtomicAccess::store(&to[2], AtomicAccess::load(&from[2]));\n+    case 2:  AtomicAccess::store(&to[1], AtomicAccess::load(&from[1]));\n+    case 1:  AtomicAccess::store(&to[0], AtomicAccess::load(&from[0]));\n@@ -317,1 +317,1 @@\n-        Atomic::store(to++, Atomic::load(from++));\n+        AtomicAccess::store(to++, AtomicAccess::load(from++));\n","filename":"src\/hotspot\/share\/utilities\/copy.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -268,1 +268,1 @@\n-  if (Atomic::cmpxchg(&out_of_memory_reported, 0, 1) == 0) {\n+  if (AtomicAccess::cmpxchg(&out_of_memory_reported, 0, 1) == 0) {\n@@ -747,1 +747,1 @@\n-    if (Atomic::cmpxchg(&g_asserting_thread, (intx)0, my_tid) == 0) {\n+    if (AtomicAccess::cmpxchg(&g_asserting_thread, (intx)0, my_tid) == 0) {\n","filename":"src\/hotspot\/share\/utilities\/debug.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -54,1 +54,1 @@\n-    old_head = Atomic::load(&Events::_logs);\n+    old_head = AtomicAccess::load(&Events::_logs);\n@@ -56,1 +56,1 @@\n-  } while (Atomic::cmpxchg(&Events::_logs, old_head, this) != old_head);\n+  } while (AtomicAccess::cmpxchg(&Events::_logs, old_head, this) != old_head);\n@@ -62,1 +62,1 @@\n-  EventLog* log = Atomic::load(&Events::_logs);\n+  EventLog* log = AtomicAccess::load(&Events::_logs);\n@@ -71,1 +71,1 @@\n-  EventLog* log = Atomic::load(&Events::_logs);\n+  EventLog* log = AtomicAccess::load(&Events::_logs);\n@@ -84,1 +84,1 @@\n-    EventLog* log = Atomic::load(&Events::_logs);\n+    EventLog* log = AtomicAccess::load(&Events::_logs);\n","filename":"src\/hotspot\/share\/utilities\/events.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -180,1 +180,1 @@\n-    Atomic::inc(&_linkage_errors, memory_order_relaxed);\n+    AtomicAccess::inc(&_linkage_errors, memory_order_relaxed);\n@@ -256,1 +256,1 @@\n-    Atomic::inc(&Exceptions::_stack_overflow_errors, memory_order_relaxed);\n+    AtomicAccess::inc(&Exceptions::_stack_overflow_errors, memory_order_relaxed);\n@@ -498,1 +498,1 @@\n-     Atomic::inc(&_out_of_memory_error_metaspace_errors, memory_order_relaxed);\n+     AtomicAccess::inc(&_out_of_memory_error_metaspace_errors, memory_order_relaxed);\n@@ -500,1 +500,1 @@\n-     Atomic::inc(&_out_of_memory_error_class_metaspace_errors, memory_order_relaxed);\n+     AtomicAccess::inc(&_out_of_memory_error_class_metaspace_errors, memory_order_relaxed);\n@@ -503,1 +503,1 @@\n-     Atomic::inc(&_out_of_memory_error_java_heap_errors, memory_order_relaxed);\n+     AtomicAccess::inc(&_out_of_memory_error_java_heap_errors, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/utilities\/exceptions.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -47,1 +47,1 @@\n-    return Atomic::load_acquire(&_first);\n+    return AtomicAccess::load_acquire(&_first);\n","filename":"src\/hotspot\/share\/utilities\/filterQueue.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-    if (Atomic::cmpxchg(&_first, head, insnode) == head) {\n+    if (AtomicAccess::cmpxchg(&_first, head, insnode) == head) {\n@@ -94,1 +94,1 @@\n-      if (Atomic::cmpxchg(&_first, match, match->_next) == match) {\n+      if (AtomicAccess::cmpxchg(&_first, match, match->_next) == match) {\n","filename":"src\/hotspot\/share\/utilities\/filterQueue.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -44,1 +44,1 @@\n-      uintx cnt = Atomic::load_acquire(thread->get_rcu_counter());\n+      uintx cnt = AtomicAccess::load_acquire(thread->get_rcu_counter());\n@@ -61,2 +61,2 @@\n-  \/\/ Atomic::add must provide fence since we have storeload dependency.\n-  uintx gbl_cnt = Atomic::add(&_global_counter._counter, COUNTER_INCREMENT);\n+  \/\/ AtomicAccess::add must provide fence since we have storeload dependency.\n+  uintx gbl_cnt = AtomicAccess::add(&_global_counter._counter, COUNTER_INCREMENT);\n","filename":"src\/hotspot\/share\/utilities\/globalCounter.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -36,1 +36,1 @@\n-  uintx old_cnt = Atomic::load(thread->get_rcu_counter());\n+  uintx old_cnt = AtomicAccess::load(thread->get_rcu_counter());\n@@ -41,1 +41,1 @@\n-    new_cnt = Atomic::load(&_global_counter._counter) | COUNTER_ACTIVE;\n+    new_cnt = AtomicAccess::load(&_global_counter._counter) | COUNTER_ACTIVE;\n@@ -43,1 +43,1 @@\n-  Atomic::release_store_fence(thread->get_rcu_counter(), new_cnt);\n+  AtomicAccess::release_store_fence(thread->get_rcu_counter(), new_cnt);\n@@ -52,2 +52,2 @@\n-  Atomic::release_store(thread->get_rcu_counter(),\n-                        static_cast<uintx>(context));\n+  AtomicAccess::release_store(thread->get_rcu_counter(),\n+                              static_cast<uintx>(context));\n","filename":"src\/hotspot\/share\/utilities\/globalCounter.inline.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -68,1 +68,1 @@\n-      cur = Atomic::cmpxchg(&_top, cur, first);\n+      cur = AtomicAccess::cmpxchg(&_top, cur, first);\n@@ -92,1 +92,1 @@\n-      result = Atomic::cmpxchg(&_top, result, new_top);\n+      result = AtomicAccess::cmpxchg(&_top, result, new_top);\n@@ -104,1 +104,1 @@\n-    return Atomic::xchg(&_top, (T*)nullptr);\n+    return AtomicAccess::xchg(&_top, (T*)nullptr);\n@@ -148,1 +148,1 @@\n-  T* top() const { return Atomic::load(&_top); }\n+  T* top() const { return AtomicAccess::load(&_top); }\n@@ -163,1 +163,1 @@\n-    return Atomic::load(next_ptr(const_cast<T&>(value)));\n+    return AtomicAccess::load(next_ptr(const_cast<T&>(value)));\n@@ -171,1 +171,1 @@\n-    Atomic::store(next_ptr(value), new_next);\n+    AtomicAccess::store(next_ptr(value), new_next);\n","filename":"src\/hotspot\/share\/utilities\/lockFreeStack.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -34,1 +34,1 @@\n-  return Atomic::load(next_ptr(const_cast<T&>(node)));\n+  return AtomicAccess::load(next_ptr(const_cast<T&>(node)));\n@@ -39,1 +39,1 @@\n-  Atomic::store(next_ptr(node), new_next);\n+  AtomicAccess::store(next_ptr(node), new_next);\n@@ -63,1 +63,1 @@\n-  T* head = Atomic::load(&_head);\n+  T* head = AtomicAccess::load(&_head);\n@@ -74,1 +74,1 @@\n-  return Atomic::load(&_head) == nullptr;\n+  return AtomicAccess::load(&_head) == nullptr;\n@@ -108,1 +108,1 @@\n-  T* old_tail = Atomic::xchg(&_tail, &last);\n+  T* old_tail = AtomicAccess::xchg(&_tail, &last);\n@@ -113,1 +113,1 @@\n-    assert(Atomic::load(&_head) == nullptr, \"invariant\");\n+    assert(AtomicAccess::load(&_head) == nullptr, \"invariant\");\n@@ -115,1 +115,1 @@\n-  } else if (is_end(Atomic::cmpxchg(next_ptr(*old_tail), end_marker(), &first))) {\n+  } else if (is_end(AtomicAccess::cmpxchg(next_ptr(*old_tail), end_marker(), &first))) {\n@@ -131,1 +131,1 @@\n-    DEBUG_ONLY(T* old_head = Atomic::load(&_head);)\n+    DEBUG_ONLY(T* old_head = AtomicAccess::load(&_head);)\n@@ -137,1 +137,1 @@\n-  Atomic::store(&_head, &first);\n+  AtomicAccess::store(&_head, &first);\n@@ -144,1 +144,1 @@\n-  T* old_head = Atomic::load_acquire(&_head);\n+  T* old_head = AtomicAccess::load_acquire(&_head);\n@@ -150,1 +150,1 @@\n-  T* next_node = Atomic::load_acquire(next_ptr(*old_head));\n+  T* next_node = AtomicAccess::load_acquire(next_ptr(*old_head));\n@@ -163,1 +163,1 @@\n-    if (old_head != Atomic::cmpxchg(&_head, old_head, next_node)) {\n+    if (old_head != AtomicAccess::cmpxchg(&_head, old_head, next_node)) {\n@@ -191,1 +191,1 @@\n-  } else if (is_end(Atomic::cmpxchg(next_ptr(*old_head), next_node, (T*)nullptr))) {\n+  } else if (is_end(AtomicAccess::cmpxchg(next_ptr(*old_head), next_node, (T*)nullptr))) {\n@@ -206,1 +206,1 @@\n-    Atomic::cmpxchg(&_head, old_head, (T*)nullptr);\n+    AtomicAccess::cmpxchg(&_head, old_head, (T*)nullptr);\n@@ -212,1 +212,1 @@\n-    Atomic::cmpxchg(&_tail, old_head, (T*)nullptr);\n+    AtomicAccess::cmpxchg(&_tail, old_head, (T*)nullptr);\n@@ -240,1 +240,1 @@\n-  T* tail = Atomic::load(&_tail);\n+  T* tail = AtomicAccess::load(&_tail);\n@@ -242,3 +242,3 @@\n-  Pair<T*, T*> result(Atomic::load(&_head), tail);\n-  Atomic::store(&_head, (T*)nullptr);\n-  Atomic::store(&_tail, (T*)nullptr);\n+  Pair<T*, T*> result(AtomicAccess::load(&_head), tail);\n+  AtomicAccess::store(&_head, (T*)nullptr);\n+  AtomicAccess::store(&_tail, (T*)nullptr);\n","filename":"src\/hotspot\/share\/utilities\/nonblockingQueue.inline.hpp","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -46,1 +46,1 @@\n-  assert(Atomic::add(&_writers, 1u) == 1u, \"multiple writers\");\n+  assert(AtomicAccess::add(&_writers, 1u) == 1u, \"multiple writers\");\n@@ -66,1 +66,1 @@\n-    value = Atomic::cmpxchg(&_enter, old, value);\n+    value = AtomicAccess::cmpxchg(&_enter, old, value);\n@@ -87,1 +87,1 @@\n-  while (old != Atomic::load_acquire(old_ptr)) {\n+  while (old != AtomicAccess::load_acquire(old_ptr)) {\n@@ -98,1 +98,1 @@\n-  DEBUG_ONLY(Atomic::dec(&_writers);)\n+  DEBUG_ONLY(AtomicAccess::dec(&_writers);)\n","filename":"src\/hotspot\/share\/utilities\/singleWriterSynchronizer.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -90,1 +90,1 @@\n-  return Atomic::add(&_enter, 2u);\n+  return AtomicAccess::add(&_enter, 2u);\n@@ -94,1 +94,1 @@\n-  uint exit_value = Atomic::add(&_exit[enter_value & 1], 2u);\n+  uint exit_value = AtomicAccess::add(&_exit[enter_value & 1], 2u);\n","filename":"src\/hotspot\/share\/utilities\/singleWriterSynchronizer.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -45,1 +45,1 @@\n-    Atomic::inc(&_added_items);\n+    AtomicAccess::inc(&_added_items);\n@@ -53,1 +53,1 @@\n-    Atomic::inc(&_removed_items);\n+    AtomicAccess::inc(&_removed_items);\n","filename":"src\/hotspot\/share\/utilities\/tableStatistics.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -562,1 +562,1 @@\n-  Atomic::store(&_reporting_start_time, now);\n+  AtomicAccess::store(&_reporting_start_time, now);\n@@ -566,1 +566,1 @@\n-  return Atomic::load(&_reporting_start_time);\n+  return AtomicAccess::load(&_reporting_start_time);\n@@ -571,1 +571,1 @@\n-  Atomic::store(&_step_start_time, now);\n+  AtomicAccess::store(&_step_start_time, now);\n@@ -575,1 +575,1 @@\n-  return Atomic::load(&_step_start_time);\n+  return AtomicAccess::load(&_step_start_time);\n@@ -579,1 +579,1 @@\n-  return Atomic::store(&_step_start_time, (jlong)0);\n+  return AtomicAccess::store(&_step_start_time, (jlong)0);\n@@ -1344,1 +1344,1 @@\n-  Atomic::replace_if_null(&_handshake_timed_out_thread, thread);\n+  AtomicAccess::replace_if_null(&_handshake_timed_out_thread, thread);\n@@ -1350,1 +1350,1 @@\n-  Atomic::replace_if_null(&_safepoint_timed_out_thread, thread);\n+  AtomicAccess::replace_if_null(&_safepoint_timed_out_thread, thread);\n@@ -1354,1 +1354,1 @@\n-  return Atomic::load(&_handshake_timed_out_thread);\n+  return AtomicAccess::load(&_handshake_timed_out_thread);\n@@ -1358,1 +1358,1 @@\n-  return Atomic::load(&_safepoint_timed_out_thread);\n+  return AtomicAccess::load(&_safepoint_timed_out_thread);\n@@ -1694,1 +1694,1 @@\n-      Atomic::cmpxchg(&_first_error_tid, (intptr_t)-1, mytid) == -1) {\n+      AtomicAccess::cmpxchg(&_first_error_tid, (intptr_t)-1, mytid) == -1) {\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -82,1 +82,1 @@\n-  assert(Atomic::load(&_barrier_tag) == 0,\n+  assert(AtomicAccess::load(&_barrier_tag) == 0,\n@@ -84,2 +84,2 @@\n-         Atomic::load(&_barrier_tag));\n-  Atomic::release_store(&_barrier_tag, barrier_tag);\n+         AtomicAccess::load(&_barrier_tag));\n+  AtomicAccess::release_store(&_barrier_tag, barrier_tag);\n@@ -95,1 +95,1 @@\n-  int barrier_tag = Atomic::load_acquire(&_barrier_tag);\n+  int barrier_tag = AtomicAccess::load_acquire(&_barrier_tag);\n@@ -97,1 +97,1 @@\n-  Atomic::release_store(&_barrier_tag, 0);\n+  AtomicAccess::release_store(&_barrier_tag, 0);\n@@ -124,1 +124,1 @@\n-    state = Atomic::load_acquire(&_state);\n+    state = AtomicAccess::load_acquire(&_state);\n@@ -137,1 +137,1 @@\n-  int64_t prev_state = Atomic::cmpxchg(&_state, state, new_state);\n+  int64_t prev_state = AtomicAccess::cmpxchg(&_state, state, new_state);\n@@ -148,1 +148,1 @@\n-    int cur = Atomic::load_acquire(&_outstanding_wakeups);\n+    int cur = AtomicAccess::load_acquire(&_outstanding_wakeups);\n@@ -155,1 +155,1 @@\n-    int prev = Atomic::cmpxchg(&_outstanding_wakeups, cur, cur - 1);\n+    int prev = AtomicAccess::cmpxchg(&_outstanding_wakeups, cur, cur - 1);\n@@ -175,1 +175,1 @@\n-    int64_t state = Atomic::load_acquire(&_state);\n+    int64_t state = AtomicAccess::load_acquire(&_state);\n@@ -185,1 +185,1 @@\n-    if (Atomic::cmpxchg(&_state, state, new_state) == state) {\n+    if (AtomicAccess::cmpxchg(&_state, state, new_state) == state) {\n@@ -194,1 +194,1 @@\n-    Atomic::release_store(&_outstanding_wakeups, waiters);\n+    AtomicAccess::release_store(&_outstanding_wakeups, waiters);\n@@ -200,1 +200,1 @@\n-  assert(Atomic::load(&_outstanding_wakeups) == 0, \"Post disarm: Should not have outstanding wakeups\");\n+  assert(AtomicAccess::load(&_outstanding_wakeups) == 0, \"Post disarm: Should not have outstanding wakeups\");\n@@ -206,1 +206,1 @@\n-    int64_t state = Atomic::load_acquire(&_state);\n+    int64_t state = AtomicAccess::load_acquire(&_state);\n@@ -222,1 +222,1 @@\n-    if (Atomic::cmpxchg(&_state, state, new_state) == state) {\n+    if (AtomicAccess::cmpxchg(&_state, state, new_state) == state) {\n@@ -241,1 +241,1 @@\n-    int64_t state = Atomic::load_acquire(&_state);\n+    int64_t state = AtomicAccess::load_acquire(&_state);\n@@ -251,1 +251,1 @@\n-    if (Atomic::cmpxchg(&_state, state, new_state) == state) {\n+    if (AtomicAccess::cmpxchg(&_state, state, new_state) == state) {\n","filename":"src\/hotspot\/share\/utilities\/waitBarrier_generic.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-  return Atomic::load_acquire(&_loaded);\n+  return AtomicAccess::load_acquire(&_loaded);\n@@ -114,1 +114,1 @@\n-  Atomic::release_store(&_loaded, true);\n+  AtomicAccess::release_store(&_loaded, true);\n","filename":"src\/hotspot\/share\/utilities\/zipLibrary.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,2 +35,2 @@\n-    Atomic::add(&_sum, chunk);\n-    Atomic::store(&_max, max_chunks);\n+    AtomicAccess::add(&_sum, chunk);\n+    AtomicAccess::store(&_max, max_chunks);\n@@ -38,2 +38,2 @@\n-  int sum() { return Atomic::load(&_sum); }\n-  int max() { return Atomic::load(&_max); }\n+  int sum() { return AtomicAccess::load(&_sum); }\n+  int max() { return AtomicAccess::load(&_max); }\n","filename":"test\/hotspot\/gtest\/cds\/test_archiveWorkers.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -65,2 +65,2 @@\n-    Atomic::inc(&_num_do_work);\n-    bool orig_value = Atomic::cmpxchg(&_do_work_called_by[worker_id], false, true);\n+    AtomicAccess::inc(&_num_do_work);\n+    bool orig_value = AtomicAccess::cmpxchg(&_do_work_called_by[worker_id], false, true);\n@@ -71,1 +71,1 @@\n-    ASSERT_EQ(Atomic::load(&_num_do_work), num_workers);\n+    ASSERT_EQ(AtomicAccess::load(&_num_do_work), num_workers);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_g1BatchedGangTask.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -416,2 +416,2 @@\n-    Atomic::add(&_added, added);\n-    Atomic::add(&_found, found);\n+    AtomicAccess::add(&_added, added);\n+    AtomicAccess::add(&_found, found);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_g1CardSet.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -46,1 +46,1 @@\n-  static uintx head(const G1FreeIdSet& set) { return Atomic::load(&set._head); }\n+  static uintx head(const G1FreeIdSet& set) { return AtomicAccess::load(&set._head); }\n@@ -109,1 +109,1 @@\n-    while (Atomic::load_acquire(_continue_running)) {\n+    while (AtomicAccess::load_acquire(_continue_running)) {\n@@ -116,1 +116,1 @@\n-    Atomic::add(_total_allocations, _allocations);\n+    AtomicAccess::add(_total_allocations, _allocations);\n@@ -148,1 +148,1 @@\n-  Atomic::release_store(&continue_running, false);\n+  AtomicAccess::release_store(&continue_running, false);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_g1FreeIdSet.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -62,1 +62,1 @@\n-    uint index = Atomic::fetch_then_add(&_claim_id, 1u);\n+    uint index = AtomicAccess::fetch_then_add(&_claim_id, 1u);\n","filename":"test\/hotspot\/gtest\/gc\/g1\/test_stressCommitUncommit.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -138,1 +138,1 @@\n-    while (Atomic::load_acquire(_continue_running)) {\n+    while (AtomicAccess::load_acquire(_continue_running)) {\n@@ -145,1 +145,1 @@\n-    Atomic::add(_total_allocations, _allocations);\n+    AtomicAccess::add(_total_allocations, _allocations);\n@@ -175,1 +175,1 @@\n-      } else if (!Atomic::load_acquire(_continue_running)) {\n+      } else if (!AtomicAccess::load_acquire(_continue_running)) {\n@@ -225,1 +225,1 @@\n-  Atomic::release_store(&allocator_running, false);\n+  AtomicAccess::release_store(&allocator_running, false);\n@@ -230,1 +230,1 @@\n-  Atomic::release_store(&processor_running, false);\n+  AtomicAccess::release_store(&processor_running, false);\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_bufferNodeAllocator.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"test\/hotspot\/gtest\/jfr\/test_adaptiveSampler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -39,2 +39,2 @@\n-    Atomic::store(&_test_value, zero);\n-    T value = Atomic::add(&_test_value, five);\n+    AtomicAccess::store(&_test_value, zero);\n+    T value = AtomicAccess::add(&_test_value, five);\n@@ -42,1 +42,1 @@\n-    EXPECT_EQ(five, Atomic::load(&_test_value));\n+    EXPECT_EQ(five, AtomicAccess::load(&_test_value));\n@@ -48,2 +48,2 @@\n-    Atomic::store(&_test_value, zero);\n-    T value = Atomic::fetch_then_add(&_test_value, five);\n+    AtomicAccess::store(&_test_value, zero);\n+    T value = AtomicAccess::fetch_then_add(&_test_value, five);\n@@ -51,1 +51,1 @@\n-    EXPECT_EQ(five, Atomic::load(&_test_value));\n+    EXPECT_EQ(five, AtomicAccess::load(&_test_value));\n@@ -75,2 +75,2 @@\n-  Atomic::store(&_test_value, zero);\n-  uint* value = Atomic::add(&_test_value, 5);\n+  AtomicAccess::store(&_test_value, zero);\n+  uint* value = AtomicAccess::add(&_test_value, 5);\n@@ -78,1 +78,1 @@\n-  EXPECT_EQ(five, Atomic::load(&_test_value));\n+  EXPECT_EQ(five, AtomicAccess::load(&_test_value));\n@@ -80,2 +80,2 @@\n-  Atomic::store(&_test_value, zero);\n-  value = Atomic::fetch_then_add(&_test_value, 6);\n+  AtomicAccess::store(&_test_value, zero);\n+  value = AtomicAccess::fetch_then_add(&_test_value, 6);\n@@ -83,1 +83,1 @@\n-  EXPECT_EQ(six, Atomic::load(&_test_value));\n+  EXPECT_EQ(six, AtomicAccess::load(&_test_value));\n@@ -95,2 +95,2 @@\n-    Atomic::store(&_test_value, zero);\n-    T res = Atomic::xchg(&_test_value, five);\n+    AtomicAccess::store(&_test_value, zero);\n+    T res = AtomicAccess::xchg(&_test_value, five);\n@@ -98,1 +98,1 @@\n-    EXPECT_EQ(five, Atomic::load(&_test_value));\n+    EXPECT_EQ(five, AtomicAccess::load(&_test_value));\n@@ -122,2 +122,2 @@\n-    Atomic::store(&_test_value, zero);\n-    T res = Atomic::cmpxchg(&_test_value, five, ten);\n+    AtomicAccess::store(&_test_value, zero);\n+    T res = AtomicAccess::cmpxchg(&_test_value, five, ten);\n@@ -125,2 +125,2 @@\n-    EXPECT_EQ(zero, Atomic::load(&_test_value));\n-    res = Atomic::cmpxchg(&_test_value, zero, ten);\n+    EXPECT_EQ(zero, AtomicAccess::load(&_test_value));\n+    res = AtomicAccess::cmpxchg(&_test_value, zero, ten);\n@@ -128,1 +128,1 @@\n-    EXPECT_EQ(ten, Atomic::load(&_test_value));\n+    EXPECT_EQ(ten, AtomicAccess::load(&_test_value));\n@@ -170,1 +170,1 @@\n-    Atomic::cmpxchg(&_array[index], _default_val, one);\n+    AtomicAccess::cmpxchg(&_array[index], _default_val, one);\n@@ -173,1 +173,1 @@\n-    Atomic::cmpxchg(&_array[index], one, _default_val);\n+    AtomicAccess::cmpxchg(&_array[index], one, _default_val);\n@@ -197,3 +197,3 @@\n-    EXPECT_NE(value, Atomic::load(&_test_value));\n-    Atomic::store(&_test_value, value);\n-    EXPECT_EQ(value, Atomic::load(&_test_value));\n+    EXPECT_NE(value, AtomicAccess::load(&_test_value));\n+    AtomicAccess::store(&_test_value, value);\n+    EXPECT_EQ(value, AtomicAccess::load(&_test_value));\n@@ -203,6 +203,6 @@\n-    EXPECT_NE(value1, Atomic::load(&_test_value));\n-    Atomic::store(&_test_value, value1);\n-    EXPECT_EQ(value1, Atomic::cmpxchg(&_test_value, value2, value2));\n-    EXPECT_EQ(value1, Atomic::load(&_test_value));\n-    EXPECT_EQ(value1, Atomic::cmpxchg(&_test_value, value1, value2));\n-    EXPECT_EQ(value2, Atomic::load(&_test_value));\n+    EXPECT_NE(value1, AtomicAccess::load(&_test_value));\n+    AtomicAccess::store(&_test_value, value1);\n+    EXPECT_EQ(value1, AtomicAccess::cmpxchg(&_test_value, value2, value2));\n+    EXPECT_EQ(value1, AtomicAccess::load(&_test_value));\n+    EXPECT_EQ(value1, AtomicAccess::cmpxchg(&_test_value, value1, value2));\n+    EXPECT_EQ(value2, AtomicAccess::load(&_test_value));\n@@ -212,4 +212,4 @@\n-    EXPECT_NE(value1, Atomic::load(&_test_value));\n-    Atomic::store(&_test_value, value1);\n-    EXPECT_EQ(value1, Atomic::xchg(&_test_value, value2));\n-    EXPECT_EQ(value2, Atomic::load(&_test_value));\n+    EXPECT_NE(value1, AtomicAccess::load(&_test_value));\n+    AtomicAccess::store(&_test_value, value1);\n+    EXPECT_EQ(value1, AtomicAccess::xchg(&_test_value, value2));\n+    EXPECT_EQ(value2, AtomicAccess::load(&_test_value));\n@@ -255,1 +255,1 @@\n-    Atomic::store(&_test_value, _old_value);\n+    AtomicAccess::store(&_test_value, _old_value);\n@@ -258,1 +258,1 @@\n-    T result = Atomic::fetch_then_and(&_test_value, _change_value);\n+    T result = AtomicAccess::fetch_then_and(&_test_value, _change_value);\n@@ -260,1 +260,1 @@\n-    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+    EXPECT_EQ(expected, AtomicAccess::load(&_test_value));\n@@ -264,1 +264,1 @@\n-    Atomic::store(&_test_value, _old_value);\n+    AtomicAccess::store(&_test_value, _old_value);\n@@ -267,1 +267,1 @@\n-    T result = Atomic::fetch_then_or(&_test_value, _change_value);\n+    T result = AtomicAccess::fetch_then_or(&_test_value, _change_value);\n@@ -269,1 +269,1 @@\n-    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+    EXPECT_EQ(expected, AtomicAccess::load(&_test_value));\n@@ -273,1 +273,1 @@\n-    Atomic::store(&_test_value, _old_value);\n+    AtomicAccess::store(&_test_value, _old_value);\n@@ -276,1 +276,1 @@\n-    T result = Atomic::fetch_then_xor(&_test_value, _change_value);\n+    T result = AtomicAccess::fetch_then_xor(&_test_value, _change_value);\n@@ -278,1 +278,1 @@\n-    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+    EXPECT_EQ(expected, AtomicAccess::load(&_test_value));\n@@ -282,1 +282,1 @@\n-    Atomic::store(&_test_value, _old_value);\n+    AtomicAccess::store(&_test_value, _old_value);\n@@ -285,1 +285,1 @@\n-    T result = Atomic::and_then_fetch(&_test_value, _change_value);\n+    T result = AtomicAccess::and_then_fetch(&_test_value, _change_value);\n@@ -287,1 +287,1 @@\n-    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+    EXPECT_EQ(expected, AtomicAccess::load(&_test_value));\n@@ -291,1 +291,1 @@\n-    Atomic::store(&_test_value, _old_value);\n+    AtomicAccess::store(&_test_value, _old_value);\n@@ -294,1 +294,1 @@\n-    T result = Atomic::or_then_fetch(&_test_value, _change_value);\n+    T result = AtomicAccess::or_then_fetch(&_test_value, _change_value);\n@@ -296,1 +296,1 @@\n-    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+    EXPECT_EQ(expected, AtomicAccess::load(&_test_value));\n@@ -300,1 +300,1 @@\n-    Atomic::store(&_test_value, _old_value);\n+    AtomicAccess::store(&_test_value, _old_value);\n@@ -303,1 +303,1 @@\n-    T result = Atomic::xor_then_fetch(&_test_value, _change_value);\n+    T result = AtomicAccess::xor_then_fetch(&_test_value, _change_value);\n@@ -305,1 +305,1 @@\n-    EXPECT_EQ(expected, Atomic::load(&_test_value));\n+    EXPECT_EQ(expected, AtomicAccess::load(&_test_value));\n","filename":"test\/hotspot\/gtest\/runtime\/test_atomic.cpp","additions":54,"deletions":54,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -1200,1 +1200,1 @@\n-    Atomic::add(_total_scanned, par_scan._count);\n+    AtomicAccess::add(_total_scanned, par_scan._count);\n","filename":"test\/hotspot\/gtest\/utilities\/test_concurrentHashtable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -24,1 +24,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -47,2 +47,2 @@\n-      volatile TestData* read_test = Atomic::load_acquire(_test);\n-      long value = Atomic::load_acquire(&read_test->test_value);\n+      volatile TestData* read_test = AtomicAccess::load_acquire(_test);\n+      long value = AtomicAccess::load_acquire(&read_test->test_value);\n@@ -53,2 +53,2 @@\n-        volatile TestData* test = Atomic::load_acquire(_test);\n-        long value = Atomic::load_acquire(&test->test_value);\n+        volatile TestData* test = AtomicAccess::load_acquire(_test);\n+        long value = AtomicAccess::load_acquire(&test->test_value);\n@@ -64,1 +64,1 @@\n-  Atomic::release_store(&test, tmp);\n+  AtomicAccess::release_store(&test, tmp);\n@@ -77,1 +77,1 @@\n-    Atomic::release_store(&test, tmp);\n+    AtomicAccess::release_store(&test, tmp);\n","filename":"test\/hotspot\/gtest\/utilities\/test_globalCounter.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -24,1 +24,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -55,1 +55,1 @@\n-    Atomic::release_store(&_state, new_state);\n+    AtomicAccess::release_store(&_state, new_state);\n@@ -60,2 +60,2 @@\n-    Atomic::release_store(&_state, new_state);\n-    while (!Atomic::load_acquire(&_proceed)) {\n+    AtomicAccess::release_store(&_state, new_state);\n+    while (!AtomicAccess::load_acquire(&_proceed)) {\n@@ -64,1 +64,1 @@\n-    Atomic::release_store(&_proceed, false);\n+    AtomicAccess::release_store(&_proceed, false);\n@@ -69,1 +69,1 @@\n-    return Atomic::load_acquire(&_state);\n+    return AtomicAccess::load_acquire(&_state);\n@@ -80,1 +80,1 @@\n-    Atomic::release_store(&_proceed, true);\n+    AtomicAccess::release_store(&_proceed, true);\n","filename":"test\/hotspot\/gtest\/utilities\/test_globalCounter_nested.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -228,1 +228,1 @@\n-    Atomic::release_store_fence(&_ready, true);\n+    AtomicAccess::release_store_fence(&_ready, true);\n@@ -233,1 +233,1 @@\n-        Atomic::inc(_processed);\n+        AtomicAccess::inc(_processed);\n@@ -235,1 +235,1 @@\n-      } else if (Atomic::load_acquire(_processed) == _process_limit) {\n+      } else if (AtomicAccess::load_acquire(_processed) == _process_limit) {\n@@ -242,1 +242,1 @@\n-  bool ready() const { return Atomic::load_acquire(&_ready); }\n+  bool ready() const { return AtomicAccess::load_acquire(&_ready); }\n","filename":"test\/hotspot\/gtest\/utilities\/test_lockFreeStack.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -205,1 +205,1 @@\n-    Atomic::release_store_fence(&_ready, true);\n+    AtomicAccess::release_store_fence(&_ready, true);\n@@ -210,1 +210,1 @@\n-        Atomic::inc(_processed);\n+        AtomicAccess::inc(_processed);\n@@ -212,1 +212,1 @@\n-      } else if (Atomic::load_acquire(_processed) == _process_limit) {\n+      } else if (AtomicAccess::load_acquire(_processed) == _process_limit) {\n@@ -219,1 +219,1 @@\n-  bool ready() const { return Atomic::load_acquire(&_ready); }\n+  bool ready() const { return AtomicAccess::load_acquire(&_ready); }\n","filename":"test\/hotspot\/gtest\/utilities\/test_nonblockingQueue.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -58,1 +58,1 @@\n-    while (Atomic::load_acquire(_continue_running) != 0) {\n+    while (AtomicAccess::load_acquire(_continue_running) != 0) {\n@@ -62,1 +62,1 @@\n-      uintx value = Atomic::load_acquire(_synchronized_value);\n+      uintx value = AtomicAccess::load_acquire(_synchronized_value);\n@@ -65,1 +65,1 @@\n-        new_value = Atomic::load_acquire(_synchronized_value);\n+        new_value = AtomicAccess::load_acquire(_synchronized_value);\n@@ -99,1 +99,1 @@\n-    while (Atomic::load_acquire(_continue_running) != 0) {\n+    while (AtomicAccess::load_acquire(_continue_running) != 0) {\n","filename":"test\/hotspot\/gtest\/utilities\/test_singleWriterSynchronizer.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -24,1 +24,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -51,1 +51,1 @@\n-      tag = Atomic::load_acquire(&wait_tag);\n+      tag = AtomicAccess::load_acquire(&wait_tag);\n@@ -53,1 +53,1 @@\n-      Atomic::release_store(&_on_barrier, tag);\n+      AtomicAccess::release_store(&_on_barrier, tag);\n@@ -62,1 +62,1 @@\n-      vv = Atomic::load_acquire(&valid_value);\n+      vv = AtomicAccess::load_acquire(&valid_value);\n@@ -64,1 +64,1 @@\n-      Atomic::release_store(&_on_barrier, 0);\n+      AtomicAccess::release_store(&_on_barrier, 0);\n@@ -106,1 +106,1 @@\n-      Atomic::release_store_fence(&wait_tag, next_tag);\n+      AtomicAccess::release_store_fence(&wait_tag, next_tag);\n@@ -117,1 +117,1 @@\n-      Atomic::release_store(&valid_value, valid_value + 1); \/\/ odd\n+      AtomicAccess::release_store(&valid_value, valid_value + 1); \/\/ odd\n@@ -120,1 +120,1 @@\n-      Atomic::release_store(&valid_value, valid_value + 1); \/\/ even\n+      AtomicAccess::release_store(&valid_value, valid_value + 1); \/\/ even\n@@ -122,1 +122,1 @@\n-      Atomic::release_store_fence(&wait_tag, 0); \/\/ Stores in WB must not float up.\n+      AtomicAccess::release_store_fence(&wait_tag, 0); \/\/ Stores in WB must not float up.\n","filename":"test\/hotspot\/gtest\/utilities\/test_waitBarrier.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"}]}