{"files":[{"patch":"@@ -39,5 +39,5 @@\n-STATIC_ASSERT(is_aligned((int)Chunk::tiny_size, ARENA_AMALLOC_ALIGNMENT));\n-STATIC_ASSERT(is_aligned((int)Chunk::init_size, ARENA_AMALLOC_ALIGNMENT));\n-STATIC_ASSERT(is_aligned((int)Chunk::medium_size, ARENA_AMALLOC_ALIGNMENT));\n-STATIC_ASSERT(is_aligned((int)Chunk::size, ARENA_AMALLOC_ALIGNMENT));\n-STATIC_ASSERT(is_aligned((int)Chunk::non_pool_size, ARENA_AMALLOC_ALIGNMENT));\n+STATIC_ASSERT(is_aligned(Chunk::tiny_size, ARENA_AMALLOC_ALIGNMENT));\n+STATIC_ASSERT(is_aligned(Chunk::init_size, ARENA_AMALLOC_ALIGNMENT));\n+STATIC_ASSERT(is_aligned(Chunk::medium_size, ARENA_AMALLOC_ALIGNMENT));\n+STATIC_ASSERT(is_aligned(Chunk::size, ARENA_AMALLOC_ALIGNMENT));\n+STATIC_ASSERT(is_aligned(Chunk::non_pool_size, ARENA_AMALLOC_ALIGNMENT));\n@@ -54,3 +54,2 @@\n-  \/\/ Our four static pools\n-  static const int _num_pools = 4;\n-  static ChunkPool _pools[_num_pools];\n+  \/\/ Our four static pools.\n+  static ChunkPool _pools[4];\n@@ -59,1 +58,1 @@\n-  ChunkPool(size_t size) : _first(nullptr), _size(size) {}\n+  constexpr ChunkPool(size_t size) : _first(nullptr), _size(size) {}\n@@ -68,0 +67,3 @@\n+    if (c != nullptr) {\n+      c->set_next(nullptr);\n+    }\n@@ -73,1 +75,2 @@\n-    assert(chunk->length() == _size, \"wrong pool for this chunk\");\n+    assert(chunk->pool_size() == Chunk::pool_size(_size), \"wrong pool for this chunk\");\n+    assert(chunk->length() >= _size, \"wrong pool for this chunk\");\n@@ -88,1 +91,1 @@\n-      os::free(cur);\n+      cur->deallocate();\n@@ -95,1 +98,1 @@\n-    for (int i = 0; i < _num_pools; i++) {\n+    for (size_t i = 0; i < ARRAY_SIZE(_pools); i++) {\n@@ -102,1 +105,1 @@\n-    for (int i = 0; i < _num_pools; i++) {\n+    for (size_t i = 0; i < ARRAY_SIZE(_pools); i++) {\n@@ -110,0 +113,17 @@\n+  static ChunkPool* get_pool_for_chunk(Chunk* c) {\n+    ChunkPool* pool;\n+    const ChunkPoolSize pool_size = c->pool_size();\n+    const size_t pool_index = static_cast<size_t>(pool_size) - 1;  \/\/ Underflow is fine.\n+    if (pool_index < ARRAY_SIZE(_pools)) {\n+      pool = &_pools[pool_index];\n+      assert(pool_size == Chunk::pool_size(pool->_size), \"unexpected chunk pool\");\n+      assert(c->length() >= pool->_size,\n+             \"chunk too small, expected at least: \" SIZE_FORMAT \" got: \" SIZE_FORMAT,\n+             pool->_size, c->length());\n+    } else {\n+      assert(pool_size == ChunkPoolSize::NONE, \"unexpected chunk pool\");\n+      pool = nullptr;\n+    }\n+    return pool;\n+  }\n+\n@@ -112,1 +132,2 @@\n-ChunkPool ChunkPool::_pools[] = { Chunk::size, Chunk::medium_size, Chunk::init_size, Chunk::tiny_size };\n+\/\/ Must be in the same order as ChunkPoolSize. The index is ChunkPoolSize minus 1.\n+ChunkPool ChunkPool::_pools[] = { Chunk::tiny_size, Chunk::small_size, Chunk::medium_size, Chunk::large_size };\n@@ -123,1 +144,2 @@\n-   void task() {\n+\n+   void task() override {\n@@ -131,2 +153,25 @@\n-void* Chunk::operator new (size_t sizeofChunk, AllocFailType alloc_failmode, size_t length) throw() {\n-  \/\/ - requested_size = sizeof(Chunk)\n+Chunk* Chunk::allocate(size_t length, AllocFailType alloc_failmode) {\n+  assert(is_aligned(length, ARENA_AMALLOC_ALIGNMENT),\n+         \"chunk payload length misaligned: \" SIZE_FORMAT, length);\n+  const size_t overhead = aligned_overhead_size();\n+  size_t bytes = length + overhead;\n+  size_t actual_bytes;\n+  void* p = os::malloc(bytes, mtChunk, CALLER_PC, &actual_bytes);\n+  if (p == nullptr) {\n+    if (alloc_failmode == AllocFailStrategy::EXIT_OOM) {\n+      vm_exit_out_of_memory(bytes, OOM_MALLOC_ERROR, \"Chunk::allocate\");\n+    }\n+    return nullptr;\n+  }\n+  assert(is_aligned(p, ARENA_AMALLOC_ALIGNMENT), \"chunk start address misaligned\");\n+  actual_bytes -= overhead;\n+  return ::new(p) Chunk(actual_bytes, pool_size(length));\n+}\n+\n+void Chunk::deallocate() {\n+  size_t size = raw_length() + aligned_overhead_size();\n+  this->~Chunk();\n+  os::free_sized(this, size);\n+}\n+\n+Chunk* Chunk::acquire(size_t length, AllocFailType alloc_failmode) {\n@@ -149,6 +194,2 @@\n-\n-  assert(sizeofChunk == sizeof(Chunk), \"weird request size\");\n-  assert(is_aligned(length, ARENA_AMALLOC_ALIGNMENT), \"chunk payload length misaligned: \"\n-         SIZE_FORMAT \".\", length);\n-  \/\/ Try to reuse a freed chunk from the pool\n-  ChunkPool* pool = ChunkPool::get_pool_for_size(length);\n+  const size_t aligned_length = ARENA_ALIGN(length);\n+  ChunkPool* pool = ChunkPool::get_pool_for_size(aligned_length);\n@@ -158,1 +199,3 @@\n-      assert(c->length() == length, \"wrong length?\");\n+      assert(c->length() >= aligned_length,\n+             \"chunk too small, expected at least: \" SIZE_FORMAT \" got: \" SIZE_FORMAT,\n+             aligned_length, c->length());\n@@ -162,9 +205,1 @@\n-  \/\/ Either the pool was empty, or this is a non-standard length. Allocate a new Chunk from C-heap.\n-  size_t bytes = ARENA_ALIGN(sizeofChunk) + length;\n-  void* p = os::malloc(bytes, mtChunk, CALLER_PC);\n-  if (p == nullptr && alloc_failmode == AllocFailStrategy::EXIT_OOM) {\n-    vm_exit_out_of_memory(bytes, OOM_MALLOC_ERROR, \"Chunk::new\");\n-  }\n-  \/\/ We rely on arena alignment <= malloc alignment.\n-  assert(is_aligned(p, ARENA_AMALLOC_ALIGNMENT), \"Chunk start address misaligned.\");\n-  return p;\n+  return allocate(aligned_length, alloc_failmode);\n@@ -173,4 +208,2 @@\n-void Chunk::operator delete(void* p) {\n-  \/\/ If this is a standard-sized chunk, return it to its pool; otherwise free it.\n-  Chunk* c = (Chunk*)p;\n-  ChunkPool* pool = ChunkPool::get_pool_for_size(c->length());\n+void Chunk::release() {\n+  ChunkPool* pool = ChunkPool::get_pool_for_chunk(this);\n@@ -178,1 +211,1 @@\n-    pool->free(c);\n+    pool->free(this);\n@@ -181,1 +214,1 @@\n-    os::free(c);\n+    deallocate();\n@@ -185,2 +218,7 @@\n-Chunk::Chunk(size_t length) : _len(length) {\n-  _next = nullptr;         \/\/ Chain on the linked list\n+Chunk::Chunk(size_t length, ChunkPoolSize pool_size)\n+    : _next(static_cast<uintptr_t>(pool_size)), _len(length) {\n+#if defined(ASSERT)\n+  if (is_aligned(length, aligned_overhead_size())) {\n+    assert(this->length() == length, \"size mismatch\");\n+  }\n+#endif\n@@ -190,3 +228,3 @@\n-  Chunk *k = this;\n-  while( k ) {\n-    Chunk *tmp = k->next();\n+  Chunk* c = this;\n+  while (c != nullptr) {\n+    Chunk* next = c->next();\n@@ -194,3 +232,3 @@\n-    if (ZapResourceArea) memset(k->bottom(), badResourceValue, k->length());\n-    delete k;                   \/\/ Free chunk (was malloc'd)\n-    k = tmp;\n+    if (ZapResourceArea) memset(c->bottom(), badResourceValue, c->length());\n+    c->release();\n+    c = next;\n@@ -201,2 +239,2 @@\n-  _next->chop();\n-  _next = nullptr;\n+  next()->chop();\n+  set_next(nullptr);\n@@ -218,2 +256,1 @@\n-  init_size = ARENA_ALIGN(init_size);\n-  _first = _chunk = new (AllocFailStrategy::EXIT_OOM, init_size) Chunk(init_size);\n+  _first = _chunk = Chunk::acquire(init_size, AllocFailStrategy::EXIT_OOM);\n@@ -223,1 +260,1 @@\n-  set_size_in_bytes(init_size);\n+  set_size_in_bytes(_chunk->length());\n@@ -226,7 +263,1 @@\n-Arena::Arena(MEMFLAGS flag) : _flags(flag), _size_in_bytes(0) {\n-  _first = _chunk = new (AllocFailStrategy::EXIT_OOM, Chunk::init_size) Chunk(Chunk::init_size);\n-  _hwm = _chunk->bottom();      \/\/ Save the cached hwm, max\n-  _max = _chunk->top();\n-  MemTracker::record_new_arena(flag);\n-  set_size_in_bytes(Chunk::init_size);\n-}\n+Arena::Arena(MEMFLAGS flag) : Arena(flag, Chunk::small_size) {}\n@@ -290,4 +321,0 @@\n-  \/\/ Get minimal required size.  Either real big, or even bigger for giant objs\n-  \/\/ (Note: all chunk sizes have to be 64-bit aligned)\n-  size_t len = MAX2(ARENA_ALIGN(x), (size_t) Chunk::size);\n-\n@@ -299,1 +326,3 @@\n-  _chunk = new (alloc_failmode, len) Chunk(len);\n+  \/\/ Get minimal required size.  Either real big, or even bigger for giant objs\n+  \/\/ (Note: all chunk sizes have to be 64-bit aligned)\n+  _chunk = Chunk::acquire(MAX2(ARENA_ALIGN(x), Chunk::size), alloc_failmode);\n@@ -309,1 +338,1 @@\n-  set_size_in_bytes(size_in_bytes() + len);\n+  set_size_in_bytes(size_in_bytes() + _chunk->length());\n","filename":"src\/hotspot\/share\/memory\/arena.cpp","additions":94,"deletions":65,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -34,0 +35,1 @@\n+#include <cstddef>\n@@ -37,2 +39,16 @@\n-#define ARENA_AMALLOC_ALIGNMENT BytesPerLong\n-#define ARENA_ALIGN(x) (align_up((x), ARENA_AMALLOC_ALIGNMENT))\n+#define ARENA_AMALLOC_ALIGNMENT ((size_t) BytesPerLong)\n+#define ARENA_ALIGN(x) (align_up((size_t) (x), ARENA_AMALLOC_ALIGNMENT))\n+\n+enum class ChunkPoolSize : uintptr_t {\n+  NONE = 0,\n+\n+  TINY = 1,\n+  SMALL = 2,\n+  MEDIUM = 3,\n+  LARGE = 4,\n+};\n+\n+constexpr uintptr_t chunk_pool_size_bits = uintptr_t{8} - 1;\n+constexpr uintptr_t chunk_pool_size_mask = ~chunk_pool_size_bits;\n+\n+class ChunkPool;\n@@ -42,2 +58,1 @@\n-class Chunk: CHeapObj<mtChunk> {\n-\n+class Chunk final {\n@@ -45,11 +60,23 @@\n-  Chunk*       _next;     \/\/ Next Chunk in list\n-  const size_t _len;      \/\/ Size of this Chunk\n- public:\n-  void* operator new(size_t size, AllocFailType alloc_failmode, size_t length) throw();\n-  void  operator delete(void* p);\n-  Chunk(size_t length);\n-\n-  enum {\n-    \/\/ default sizes; make them slightly smaller than 2**k to guard against\n-    \/\/ buddy-system style malloc implementations\n-    \/\/ Note: please keep these constants 64-bit aligned.\n+  \/\/ Ensure natural malloc alignment has enough bits for us to use in _next.\n+  STATIC_ASSERT(alignof(std::max_align_t) >= 8);\n+\n+  friend class ChunkPool;\n+\n+  uintptr_t _next;    \/\/ Next Chunk in list. The lower 3 bits encode ChunkPoolSize.\n+  const size_t _len;  \/\/ Unaligned size of this Chunk, not including Chunk itself.\n+\n+  uintptr_t raw_pool_size() const {\n+    return _next & chunk_pool_size_bits;\n+  }\n+\n+  uintptr_t raw_next() const {\n+    return _next & chunk_pool_size_mask;\n+  }\n+\n+  size_t raw_length() const { return _len; }\n+\n+  static Chunk* allocate(size_t length, AllocFailType alloc_failmode);\n+\n+  void deallocate();\n+\n+  static constexpr size_t slack =\n@@ -57,2 +84,1 @@\n-    slack      = 40,            \/\/ [RGV] Not sure if this is right, but make it\n-                                \/\/       a multiple of 8.\n+    40  \/\/ [RGV] Not sure if this is right, but make it a multiple of 8.\n@@ -60,1 +86,1 @@\n-    slack      = 24,            \/\/ suspected sizeof(Chunk) + internal malloc headers\n+    24  \/\/ suspected sizeof(Chunk) + internal malloc headers\n@@ -62,0 +88,1 @@\n+    ;\n@@ -63,6 +90,43 @@\n-    tiny_size  =  256  - slack, \/\/ Size of first chunk (tiny)\n-    init_size  =  1*K  - slack, \/\/ Size of first chunk (normal aka small)\n-    medium_size= 10*K  - slack, \/\/ Size of medium-sized chunk\n-    size       = 32*K  - slack, \/\/ Default size of an Arena chunk (following the first)\n-    non_pool_size = init_size + 32 \/\/ An initial size which is not one of above\n-  };\n+  Chunk(size_t length, ChunkPoolSize pool_size);\n+\n+  NONCOPYABLE(Chunk);\n+\n+ public:\n+  static Chunk* acquire(size_t length, AllocFailType alloc_failmode);\n+\n+  void release();\n+\n+  \/\/ default sizes; make them slightly smaller than 2**k to guard against\n+  \/\/ buddy-system style malloc implementations\n+  \/\/ Note: please keep these constants 64-bit aligned.\n+  static constexpr size_t tiny_size = 256 - slack;        \/\/ Size of first chunk (tiny)\n+  static constexpr size_t small_size = 1*K - slack;       \/\/ Size of first chunk (normal aka small)\n+  static constexpr size_t init_size = small_size;\n+  static constexpr size_t medium_size = 10*K - slack;     \/\/ Size of medium-sized chunk\n+  static constexpr size_t large_size = 32*K - slack;      \/\/ Large size of an Arena chunk (following the first)\n+  static constexpr size_t size = large_size;\n+  static constexpr size_t non_pool_size = init_size + 32; \/\/ An initial size which is not one of above\n+\n+  ChunkPoolSize pool_size() const {\n+    return static_cast<ChunkPoolSize>(raw_pool_size());\n+  }\n+  static ChunkPoolSize pool_size(size_t length) {\n+    switch (length) {\n+      case tiny_size:\n+        return ChunkPoolSize::TINY;\n+      case small_size:\n+        return ChunkPoolSize::SMALL;\n+      case medium_size:\n+        return ChunkPoolSize::MEDIUM;\n+      case large_size:\n+        return ChunkPoolSize::LARGE;\n+      default:\n+        return ChunkPoolSize::NONE;\n+    }\n+  }\n+\n+  \/\/ Release this chunk and all subsequent chunks.\n+  void chop();\n+\n+  \/\/ Release all subsequent chunks.\n+  void next_chop();\n@@ -70,3 +134,0 @@\n-  void chop();                  \/\/ Chop this chunk\n-  void next_chop();             \/\/ Chop next chunk\n-  static size_t aligned_overhead_size(void) { return ARENA_ALIGN(sizeof(Chunk)); }\n@@ -74,0 +135,10 @@\n+  static size_t aligned_overhead_size() { return aligned_overhead_size(sizeof(Chunk)); }\n+\n+  size_t length() const { return align_down(raw_length(), ARENA_AMALLOC_ALIGNMENT); }\n+\n+  Chunk* next() const { return reinterpret_cast<Chunk*>(raw_next()); }\n+\n+  void set_next(Chunk* next) {\n+    assert(next == nullptr || is_aligned(next, alignof(std::max_align_t)), \"chunk misaligned\");\n+    _next = reinterpret_cast<uintptr_t>(next) | raw_pool_size();\n+  }\n@@ -75,7 +146,3 @@\n-  size_t length() const         { return _len;  }\n-  Chunk* next() const           { return _next;  }\n-  void set_next(Chunk* n)       { _next = n;  }\n-  \/\/ Boundaries of data area (possibly unused)\n-  char* bottom() const          { return ((char*) this) + aligned_overhead_size();  }\n-  char* top()    const          { return bottom() + _len; }\n-  bool contains(char* p) const  { return bottom() <= p && p <= top(); }\n+  char* bottom() const {\n+    return reinterpret_cast<char*>(reinterpret_cast<uintptr_t>(this) + aligned_overhead_size());\n+  }\n@@ -83,1 +150,7 @@\n-  \/\/ Start the chunk_pool cleaner task\n+  char* top() const { return bottom() + length(); }\n+\n+  bool contains(const void* p) const {\n+    return bottom() <= static_cast<const char*>(p) && static_cast<const char*>(p) <= top();\n+  }\n+\n+  \/\/ Start the chunk_pool cleaner task. Must only be called once.\n@@ -85,0 +158,8 @@\n+\n+  \/\/ Chunks must be acquired and released using acquire() and release().\n+  static void* operator new(size_t size) = delete;\n+  static void* operator new[](size_t size) = delete;\n+  static void operator delete(void* p) = delete;\n+  static void operator delete[](void* p) = delete;\n+  static void operator delete(void* p, size_t size) = delete;\n+  static void operator delete[](void* p, size_t size) = delete;\n","filename":"src\/hotspot\/share\/memory\/arena.hpp","additions":116,"deletions":35,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -77,0 +77,4 @@\n+#if defined(ADDRESS_SANITIZER)\n+#include <sanitizer\/allocator_interface.h>\n+#endif\n+\n@@ -81,0 +85,12 @@\n+#ifdef LINUX\n+#include <malloc.h>\n+#endif\n+\n+#ifdef __APPLE__\n+#include <malloc\/malloc.h>\n+#endif\n+\n+#ifdef __FreeBSD__\n+#include <malloc_np.h>\n+#endif\n+\n@@ -621,2 +637,2 @@\n-void* os::malloc(size_t size, MEMFLAGS flags) {\n-  return os::malloc(size, flags, CALLER_PC);\n+void* os::malloc(size_t size, MEMFLAGS flags, size_t* actual_size) {\n+  return os::malloc(size, flags, CALLER_PC, actual_size);\n@@ -625,1 +641,1 @@\n-void* os::malloc(size_t size, MEMFLAGS memflags, const NativeCallStack& stack) {\n+void* os::malloc(size_t size, MEMFLAGS memflags, const NativeCallStack& stack, size_t* actual_size) {\n@@ -629,1 +645,1 @@\n-  if (NMTPreInit::handle_malloc(&rc, size)) {\n+  if (NMTPreInit::handle_malloc(&rc, size, actual_size)) {\n@@ -647,1 +663,3 @@\n-  const size_t outer_size = size + MemTracker::overhead_per_malloc();\n+  const size_t inner_size = size;  \/\/ Original requested size, adjusted to not be 0.\n+  const size_t overhead = MemTracker::overhead_per_malloc();  \/\/ Extra NMT overhead, if enabled.\n+  const size_t outer_size = inner_size + overhead;  \/\/ Adjusted size.\n@@ -650,1 +668,1 @@\n-  if (outer_size < size) {\n+  if (outer_size < inner_size) {\n@@ -659,0 +677,24 @@\n+  \/\/ Malloc size feedback, only when requested.\n+  if (actual_size != nullptr) {\n+    size_t usable_size;\n+#if defined(ADDRESS_SANITIZER)\n+    \/\/ ASan, TSan, and MSan are their own allocators, call them directly. They override `malloc()`\n+    \/\/ and friends.\n+    usable_size = __sanitizer_get_allocated_size(outer_ptr);\n+#elif defined(LINUX) || defined(__FreeBSD__)\n+    \/\/ Linux and FreeBSD provide `malloc_usable_size()` to query the usable size of the memory\n+    \/\/ returned by `malloc()`.\n+    usable_size = ::malloc_usable_size(outer_ptr);\n+#elif defined(__APPLE__)\n+    \/\/ Apple has `malloc_size()` which is exactly the same as `malloc_usable_size()`.\n+    usable_size = ::malloc_size(outer_ptr);\n+#else\n+    \/\/ All other platforms either lack the ability to query the usable size or are known to always\n+    \/\/ return the original size given to `malloc()` (e.g. Windows).\n+    usable_size = outer_size;\n+#endif\n+    usable_size = MAX2(usable_size, outer_size);\n+    size = *actual_size = usable_size - overhead;\n+    assert(size >= inner_size, \"malloc usable size cannot be less than requested\");\n+  }\n+\n@@ -761,1 +803,1 @@\n-void  os::free(void *memblock) {\n+void os::free(void *memblock) {\n@@ -780,0 +822,22 @@\n+void os::free_sized(void *memblock, size_t size) {\n+\n+  \/\/ Special handling for NMT preinit phase before arguments are parsed\n+  if (NMTPreInit::handle_free_sized(memblock, size)) {\n+    return;\n+  }\n+\n+  if (memblock == nullptr) {\n+    assert(size == 0, \"size mismatch\");\n+    return;\n+  }\n+\n+  DEBUG_ONLY(break_if_ptr_caught(memblock);)\n+\n+  \/\/ When NMT is enabled this checks for heap overwrites, then deaccounts the old block.\n+  void* const old_outer_ptr = MemTracker::record_free(memblock);\n+\n+  \/\/ C23 introduced free_sized, but no libc implementations have added it yet at the time of\n+  \/\/ writing.\n+  ALLOW_C_FUNCTION(::free, ::free(old_outer_ptr);)\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":71,"deletions":7,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -863,5 +863,12 @@\n-  \/\/ General allocation (must be MT-safe)\n-  static void* malloc  (size_t size, MEMFLAGS flags, const NativeCallStack& stack);\n-  static void* malloc  (size_t size, MEMFLAGS flags);\n-  static void* realloc (void *memblock, size_t size, MEMFLAGS flag, const NativeCallStack& stack);\n-  static void* realloc (void *memblock, size_t size, MEMFLAGS flag);\n+  \/\/ Memory allocation. If `actual_size` is not nullptr, the actual usable size of the underlying\n+  \/\/ memory allocation is returned. This will be at least as large as `size`. This should only be\n+  \/\/ specified in cases where extra usable memory is useful, for example in an arena allocator, as\n+  \/\/ it may require more overhead to fetch the usable size. You *should* call os::free_sized instead\n+  \/\/ of os::free if `actual_size` was specified. The size passed to os::free_sized *must* be the\n+  \/\/ same as was returned in `actual_size`.\n+  static void* malloc(size_t size, MEMFLAGS flags, const NativeCallStack& stack, size_t* actual_size = nullptr);\n+  static void* malloc(size_t size, MEMFLAGS flags, size_t* actual_size = nullptr);\n+\n+  \/\/ Memory reallocation.\n+  static void* realloc(void *memblock, size_t size, MEMFLAGS flag, const NativeCallStack& stack);\n+  static void* realloc(void *memblock, size_t size, MEMFLAGS flag);\n@@ -870,1 +877,2 @@\n-  static void  free    (void *memblock);\n+  static void  free(void *memblock);\n+  static void  free_sized(void *memblock, size_t size);\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -266,1 +266,1 @@\n-  static bool handle_malloc(void** rc, size_t size) {\n+  static bool handle_malloc(void** rc, size_t size, size_t* actual_size = nullptr) {\n@@ -272,0 +272,3 @@\n+      if (actual_size != nullptr) {\n+        *actual_size = size;\n+      }\n@@ -372,0 +375,42 @@\n+  static bool handle_free_sized(void* p, size_t size) {\n+    if (p == nullptr) { \/\/ free(null)\n+      assert(size == 0, \"size mismatch\");\n+      return true;\n+    }\n+    switch (MemTracker::tracking_level()) {\n+      case NMT_unknown: {\n+        \/\/ pre-NMT-init:\n+        \/\/ - the allocation must be in the hash map, since all allocations went through\n+        \/\/   NMTPreInit::handle_malloc()\n+        \/\/ - find the old entry, unhang from map, free it\n+        NMTPreInitAllocation* a = find_and_remove_in_map(p);\n+        assert(a->size == size, \"size mismatch\");\n+        NMTPreInitAllocation::do_free(a);\n+        _num_frees_pre++;\n+        return true;\n+      }\n+      break;\n+      case NMT_off: {\n+        \/\/ post-NMT-init, NMT *disabled*:\n+        \/\/ Neither pre- nor post-init-allocation use malloc headers, therefore we can just\n+        \/\/ relegate the realloc to os::realloc.\n+        return false;\n+      }\n+      break;\n+      default: {\n+        \/\/ post-NMT-init, NMT *enabled*:\n+        \/\/ - look up (but don't remove! lu table is read-only here.) the entry\n+        \/\/ - if found, we do nothing: the lu table is readonly, so we keep the old address\n+        \/\/   in the table. We leave the block allocated to prevent the libc from returning\n+        \/\/   the same address and confusing us.\n+        \/\/ - if not found, we let regular os::free() handle this pointer\n+        const NMTPreInitAllocation* a = find_in_map(p);\n+        if (a != nullptr) {\n+          assert(a->size == size, \"size mismatch\");\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/hotspot\/share\/services\/nmtPreInit.hpp","additions":46,"deletions":1,"binary":false,"changes":47,"status":"modified"}]}