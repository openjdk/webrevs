{"files":[{"patch":"@@ -69,0 +69,1 @@\n+define_pd_global(uint, SuperWordStoreToLoadForwardingFailureDetection, 8);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_globals_aarch64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+define_pd_global(uint, SuperWordStoreToLoadForwardingFailureDetection, 16);\n","filename":"src\/hotspot\/cpu\/arm\/c2_globals_arm.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+define_pd_global(uint, SuperWordStoreToLoadForwardingFailureDetection, 16);\n","filename":"src\/hotspot\/cpu\/ppc\/c2_globals_ppc.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+define_pd_global(uint, SuperWordStoreToLoadForwardingFailureDetection, 16);\n","filename":"src\/hotspot\/cpu\/riscv\/c2_globals_riscv.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+define_pd_global(uint, SuperWordStoreToLoadForwardingFailureDetection, 16);\n","filename":"src\/hotspot\/cpu\/s390\/c2_globals_s390.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+define_pd_global(uint, SuperWordStoreToLoadForwardingFailureDetection, 16);\n","filename":"src\/hotspot\/cpu\/x86\/c2_globals_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -358,0 +358,6 @@\n+  product_pd(uint, SuperWordStoreToLoadForwardingFailureDetection, DIAGNOSTIC, \\\n+          \"if >0, auto-vectorization detects possible store-to-load \"       \\\n+          \"forwarding failures. The number specifies over how many \"        \\\n+          \"loop iterations this detection spans.\")                          \\\n+          range(0, 4096)                                                    \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1871,0 +1871,1 @@\n+  if (vtransform.has_store_to_load_forwarding_failure()) { return false; }\n","filename":"src\/hotspot\/share\/opto\/superword.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-static void print_con_or_idx(const Node* n) {\n+void VPointer::print_con_or_idx(const Node* n) {\n@@ -1372,1 +1372,1 @@\n-  print_con_or_idx(_base);\n+  VPointer::print_con_or_idx(_base);\n@@ -1377,1 +1377,1 @@\n-  print_con_or_idx(_invar);\n+  VPointer::print_con_or_idx(_invar);\n@@ -2171,1 +2171,1 @@\n-    print_con_or_idx(_init_node);\n+    VPointer::print_con_or_idx(_init_node);\n@@ -2177,1 +2177,1 @@\n-    print_con_or_idx(_base);\n+    VPointer::print_con_or_idx(_base);\n@@ -2179,1 +2179,1 @@\n-    print_con_or_idx(_invar);\n+    VPointer::print_con_or_idx(_invar);\n","filename":"src\/hotspot\/share\/opto\/vectorization.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -873,0 +873,1 @@\n+  NOT_PRODUCT( static void print_con_or_idx(const Node* n); )\n","filename":"src\/hotspot\/share\/opto\/vectorization.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -147,0 +147,225 @@\n+\/\/ We use two comparisons, because a subtraction could underflow.\n+#define RETURN_CMP_VALUE_IF_NOT_EQUAL(a, b) \\\n+  if (a < b) { return -1; }                 \\\n+  if (a > b) { return  1; }\n+\n+\/\/ Helper-class for VTransformGraph::has_store_to_load_forwarding_failure.\n+\/\/ It represents a memory region: [ptr, ptr + memory_size)\n+class VMemoryRegion : public StackObj {\n+private:\n+  Node* _base;        \/\/ ptr = base + offset + invar + scale * iv\n+  int _scale;\n+  Node* _invar;\n+  int _offset;\n+  uint _memory_size;\n+  bool _is_load;      \/\/ load or store?\n+  uint _schedule_order;\n+\n+public:\n+  VMemoryRegion() {} \/\/ empty constructor for GrowableArray\n+  VMemoryRegion(const VPointer& vpointer, int iv_offset, int vector_length, uint schedule_order) :\n+    _base(vpointer.base()),\n+    _scale(vpointer.scale_in_bytes()),\n+    _invar(vpointer.invar()),\n+    _offset(vpointer.offset_in_bytes() + _scale * iv_offset),\n+    _memory_size(vpointer.memory_size() * vector_length),\n+    _is_load(vpointer.mem()->is_Load()),\n+    _schedule_order(schedule_order) {}\n+\n+    Node* base()          const { return _base; }\n+    int scale()           const { return _scale; }\n+    Node* invar()         const { return _invar; }\n+    int offset()          const { return _offset; }\n+    uint memory_size()    const { return _memory_size; }\n+    bool is_load()        const { return _is_load; }\n+    uint schedule_order() const { return _schedule_order; }\n+\n+    static int cmp_for_sort_by_group(VMemoryRegion* r1, VMemoryRegion* r2) {\n+      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1->base()->_idx, r2->base()->_idx);\n+      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1->scale(),      r2->scale());\n+      int r1_invar_idx = r1->invar() == nullptr ? 0 : r1->invar()->_idx;\n+      int r2_invar_idx = r2->invar() == nullptr ? 0 : r2->invar()->_idx;\n+      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1_invar_idx,      r2_invar_idx);\n+      return 0; \/\/ equal\n+    }\n+\n+    static int cmp_for_sort(VMemoryRegion* r1, VMemoryRegion* r2) {\n+      int cmp_group = cmp_for_sort_by_group(r1, r2);\n+      if (cmp_group != 0) { return cmp_group; }\n+\n+      RETURN_CMP_VALUE_IF_NOT_EQUAL(r1->offset(),     r2->offset());\n+      return 0; \/\/ equal\n+    }\n+\n+    enum Aliasing { DIFFERENT_GROUP, BEFORE, EXACT_OVERLAP, PARTIAL_OVERLAP, AFTER };\n+\n+    Aliasing aliasing(VMemoryRegion& other) {\n+      VMemoryRegion* p1 = this;\n+      VMemoryRegion* p2 = &other;\n+      if (cmp_for_sort_by_group(p1, p2) != 0) { return DIFFERENT_GROUP; }\n+\n+      jlong offset1 = p1->offset();\n+      jlong offset2 = p2->offset();\n+      jlong memory_size1 = p1->memory_size();\n+      jlong memory_size2 = p2->memory_size();\n+\n+      if (offset1 >= offset2 + memory_size2) { return AFTER; }\n+      if (offset2 >= offset1 + memory_size1) { return BEFORE; }\n+      if (offset1 == offset2 && memory_size1 == memory_size2) { return EXACT_OVERLAP; }\n+      return PARTIAL_OVERLAP;\n+    }\n+\n+#ifndef PRODUCT\n+  void print() const {\n+    tty->print(\"VMemoryRegion[%s %dbytes, schedule_order(%4d), base\",\n+               _is_load ? \"load \" : \"store\", _memory_size, _schedule_order);\n+    VPointer::print_con_or_idx(_base);\n+    tty->print(\" + offset(%4d)\", _offset);\n+    tty->print(\" + invar\");\n+    VPointer::print_con_or_idx(_invar);\n+    tty->print_cr(\" + scale(%4d) * iv]\", _scale);\n+  }\n+#endif\n+};\n+\n+\/\/ Store-to-load-forwarding is a CPU memory optimization, where a load can directly fetch\n+\/\/ its value from the store-buffer, rather than from the L1 cache. This is many CPU cycles\n+\/\/ faster. However, this optimization comes with some restrictions, depending on the CPU.\n+\/\/ Generally, store-to-load-forwarding works if the load and store memory regions match\n+\/\/ exactly (same start and width). Generally problematic are partial overlaps - though\n+\/\/ some CPU's can handle even some subsets of these cases. We conservatively assume that\n+\/\/ all such partial overlaps lead to a store-to-load-forwarding failures, which means the\n+\/\/ load has to stall until the store goes from the store-buffer into the L1 cache, incurring\n+\/\/ a penalty of many CPU cycles.\n+\/\/\n+\/\/ Unfortunately, vectorization can introduce such store-to-load-forwarding failures.\n+\/\/ Example (with \"iteration distance\" 3):\n+\/\/   for (int i = 10; i < SIZE; i++) {\n+\/\/       aI[i] = aI[i - 3] + 1;\n+\/\/   }\n+\/\/\n+\/\/ Assume we have 2-element vectors (2*4 = 8 bytes). This gives us this machine code:\n+\/\/   load_8_bytes( ptr + -12)\n+\/\/   store_8_bytes(ptr + 0)\n+\/\/   load_8_bytes( ptr + -4)\n+\/\/   store_8_bytes(ptr + 8)\n+\/\/   load_8_bytes( ptr + 4)\n+\/\/   store_8_bytes(ptr + 16)\n+\/\/   ...\n+\/\/\n+\/\/ We see that eventually all loads are dependent on earlier stores, but the values cannot\n+\/\/ be forwarded because there is some partial overlap.\n+\/\/\n+\/\/ Preferably, we would have some latency-based cost-model that accounts for such forwarding\n+\/\/ failures, and decide if vectorization with forwarding failures is still profitable. For\n+\/\/ now we go with a simpler heuristic: we simply forbid vectorization if we can PROVE that\n+\/\/ there will be a forwarding failure. This approach has at least 2 possible weaknesses:\n+\/\/\n+\/\/  (1) There may be forwarding failures in cases where we cannot prove it.\n+\/\/      Example:\n+\/\/        for (int i = 10; i < SIZE; i++) {\n+\/\/            bI[i] = aI[i - 3] + 1;\n+\/\/        }\n+\/\/\n+\/\/      We do not know if aI and bI refer to the same array or not. However, it is reasonable\n+\/\/      to assume that if we have two different array references, that they most likely refer\n+\/\/      to different arrays (i.e. no aliasing), where we would have no forwarding failures.\n+\/\/  (2) There could be some loops where vectorization introduces forwarding failures, and thus\n+\/\/      the latency of the loop body is high, but this does not matter because it is dominated\n+\/\/      by other latency\/throughput based costs in the loop body.\n+\/\/\n+\/\/ Performance measurements with the JMH benchmark StoreToLoadForwarding.java have indicated\n+\/\/ that there is some iteration threshold: if the failure happens between a store and load that\n+\/\/ have an iteration distance below this threshold, the latency is the limiting factor, and we\n+\/\/ should not vectorize to avoid the latency penalty of store-to-load-forwarding failures. If\n+\/\/ the iteration distance is larger than this threshold, the throughput is the limiting factor,\n+\/\/ and we should vectorize in these cases to improve throughput.\n+\/\/\n+bool VTransformGraph::has_store_to_load_forwarding_failure(const VLoopAnalyzer& vloop_analyzer) const {\n+  if (SuperWordStoreToLoadForwardingFailureDetection == 0) { return false; }\n+\n+  \/\/ Collect all pointers for scalar and vector loads\/stores.\n+  ResourceMark rm;\n+  GrowableArray<VMemoryRegion> memory_regions;\n+\n+  \/\/ To detect store-to-load-forwarding failures at the iteration threshold or below, we\n+  \/\/ simulate a super-unrolling to reach SuperWordStoreToLoadForwardingFailureDetection\n+  \/\/ iterations at least. This is a heuristic, and we are not trying to be very precise\n+  \/\/ with the iteration distance. If we have already unrolled more than the iteration\n+  \/\/ threshold, i.e. if \"SuperWordStoreToLoadForwardingFailureDetection < unrolled_count\",\n+  \/\/ then we simply check if there are any store-to-load-forwarding failures in the unrolled\n+  \/\/ loop body, which may be at larger distance than the desired threshold. We cannot do any\n+  \/\/ more fine-grained analysis, because the unrolling has lost the information about the\n+  \/\/ iteration distance.\n+  int simulated_unrolling_count = SuperWordStoreToLoadForwardingFailureDetection;\n+  int unrolled_count = vloop_analyzer.vloop().cl()->unrolled_count();\n+  uint simulated_super_unrolling_count = MAX2(1, simulated_unrolling_count \/ unrolled_count);\n+  int iv_stride = vloop_analyzer.vloop().iv_stride();\n+  int schedule_order = 0;\n+  for (uint k = 0; k < simulated_super_unrolling_count; k++) {\n+    int iv_offset = k * iv_stride; \/\/ virtual super-unrolling\n+    for (int i = 0; i < _schedule.length(); i++) {\n+      VTransformNode* vtn = _schedule.at(i);\n+      if (vtn->is_load_or_store_in_loop()) {\n+        const VPointer& p = vtn->vpointer(vloop_analyzer);\n+        if (p.valid()) {\n+          VTransformVectorNode* vector = vtn->isa_Vector();\n+          uint vector_length = vector != nullptr ? vector->nodes().length() : 1;\n+          memory_regions.push(VMemoryRegion(p, iv_offset, vector_length, schedule_order++));\n+        }\n+      }\n+    }\n+  }\n+\n+  \/\/ Sort the pointers by group (same base, invar and stride), and then by offset.\n+  memory_regions.sort(VMemoryRegion::cmp_for_sort);\n+\n+#ifndef PRODUCT\n+  if (_trace._verbose) {\n+    tty->print_cr(\"VTransformGraph::has_store_to_load_forwarding_failure:\");\n+    tty->print_cr(\"  simulated_unrolling_count = %d\", simulated_unrolling_count);\n+    tty->print_cr(\"  simulated_super_unrolling_count = %d\", simulated_super_unrolling_count);\n+    for (int i = 0; i < memory_regions.length(); i++) {\n+      VMemoryRegion& region = memory_regions.at(i);\n+      region.print();\n+    }\n+  }\n+#endif\n+\n+  \/\/ For all pairs of pointers in the same group, check if they have a partial overlap.\n+  for (int i = 0; i < memory_regions.length(); i++) {\n+    VMemoryRegion& region1 = memory_regions.at(i);\n+\n+    for (int j = i + 1; j < memory_regions.length(); j++) {\n+      VMemoryRegion& region2 = memory_regions.at(j);\n+\n+      const VMemoryRegion::Aliasing aliasing = region1.aliasing(region2);\n+      if (aliasing == VMemoryRegion::Aliasing::DIFFERENT_GROUP ||\n+          aliasing == VMemoryRegion::Aliasing::BEFORE) {\n+        break; \/\/ We have reached the next group or pointers that are always after.\n+      } else if (aliasing == VMemoryRegion::Aliasing::EXACT_OVERLAP) {\n+        continue;\n+      } else {\n+        assert(aliasing == VMemoryRegion::Aliasing::PARTIAL_OVERLAP, \"no other case can happen\");\n+        if ((region1.is_load() && !region2.is_load() && region1.schedule_order() > region2.schedule_order()) ||\n+            (!region1.is_load() && region2.is_load() && region1.schedule_order() < region2.schedule_order())) {\n+          \/\/ We predict that this leads to a store-to-load-forwarding failure penalty.\n+#ifndef PRODUCT\n+          if (_trace._rejections) {\n+            tty->print_cr(\"VTransformGraph::has_store_to_load_forwarding_failure:\");\n+            tty->print_cr(\"  Partial overlap of store->load. We predict that this leads to\");\n+            tty->print_cr(\"  a store-to-load-forwarding failure penalty which makes\");\n+            tty->print_cr(\"  vectorization unprofitable. These are the two pointers:\");\n+            region1.print();\n+            region2.print();\n+          }\n+#endif\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/vtransform.cpp","additions":225,"deletions":0,"binary":false,"changes":225,"status":"modified"},{"patch":"@@ -69,0 +69,2 @@\n+class VTransformLoadVectorNode;\n+class VTransformStoreVectorNode;\n@@ -160,0 +162,1 @@\n+  bool has_store_to_load_forwarding_failure(const VLoopAnalyzer& vloop_analyzer) const;\n@@ -224,0 +227,1 @@\n+  bool has_store_to_load_forwarding_failure() const { return _graph.has_store_to_load_forwarding_failure(_vloop_analyzer); }\n@@ -313,0 +317,5 @@\n+  virtual VTransformLoadVectorNode* isa_LoadVector() { return nullptr; }\n+  virtual VTransformStoreVectorNode* isa_StoreVector() { return nullptr; }\n+\n+  virtual bool is_load_or_store_in_loop() const { return false; }\n+  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const { ShouldNotReachHere(); }\n@@ -336,0 +345,2 @@\n+  virtual bool is_load_or_store_in_loop() const override { return _node->is_Load() || _node->is_Store(); }\n+  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const override { return vloop_analyzer.vpointers().vpointer(node()->as_Mem()); }\n@@ -350,0 +361,1 @@\n+  virtual bool is_load_or_store_in_loop() const override { return false; }\n@@ -475,0 +487,3 @@\n+  virtual VTransformLoadVectorNode* isa_LoadVector() override { return this; }\n+  virtual bool is_load_or_store_in_loop() const override { return true; }\n+  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const override { return vloop_analyzer.vpointers().vpointer(nodes().at(0)->as_Mem()); }\n@@ -485,0 +500,3 @@\n+  virtual VTransformStoreVectorNode* isa_StoreVector() override { return this; }\n+  virtual bool is_load_or_store_in_loop() const override { return true; }\n+  virtual const VPointer& vpointer(const VLoopAnalyzer& vloop_analyzer) const override { return vloop_analyzer.vpointers().vpointer(nodes().at(0)->as_Mem()); }\n","filename":"src\/hotspot\/share\/opto\/vtransform.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -171,0 +171,3 @@\n+        tests.put(\"test14dB\",    () -> { return test14dB(aB.clone()); });\n+        tests.put(\"test14eB\",    () -> { return test14eB(aB.clone()); });\n+        tests.put(\"test14fB\",    () -> { return test14fB(aB.clone()); });\n@@ -242,0 +245,3 @@\n+                 \"test14dB\",\n+                 \"test14eB\",\n+                 \"test14fB\",\n@@ -1131,3 +1137,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n-                  IRNode.ADD_VB, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.ADD_VB, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\"},\n@@ -1146,0 +1152,3 @@\n+            \/\/ Since the stride is shorter than the vector length, there will be always\n+            \/\/ partial overlap of loads with previous stores, this leads to failure in\n+            \/\/ store-to-load-forwarding -> vectorization not profitable.\n@@ -1167,3 +1176,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n-                  IRNode.ADD_VB, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.ADD_VB, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\"},\n@@ -1182,0 +1191,3 @@\n+            \/\/ Since the stride is shorter than the vector length, there will be always\n+            \/\/ partial overlap of loads with previous stores, this leads to failure in\n+            \/\/ store-to-load-forwarding -> vectorization not profitable.\n@@ -1203,3 +1215,3 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n-                  IRNode.ADD_VB, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.ADD_VB, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\"},\n@@ -1218,0 +1230,3 @@\n+            \/\/ Since the stride is shorter than the vector length, there will be always\n+            \/\/ partial overlap of loads with previous stores, this leads to failure in\n+            \/\/ store-to-load-forwarding -> vectorization not profitable.\n@@ -1238,0 +1253,84 @@\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, IRNode.VECTOR_SIZE + \"min(max_byte, 8)\", \"> 0\",\n+                  IRNode.ADD_VB,        IRNode.VECTOR_SIZE + \"min(max_byte, 8)\", \"> 0\",\n+                  IRNode.STORE_VECTOR,                                           \"> 0\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"})\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.ADD_VB, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"true\"})\n+    static Object[] test14dB(byte[] a) {\n+        \/\/ non-power-of-2 stride\n+        for (int i = 0; i < RANGE-20; i+=9) {\n+            a[i+0]++;\n+            a[i+1]++;\n+            a[i+2]++;\n+            a[i+3]++;\n+            a[i+4]++;\n+            a[i+5]++;\n+            a[i+6]++;\n+            a[i+7]++;\n+        }\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, IRNode.VECTOR_SIZE + \"min(max_byte, 8)\", \"> 0\",\n+                  IRNode.ADD_VB,        IRNode.VECTOR_SIZE + \"min(max_byte, 8)\", \"> 0\",\n+                  IRNode.STORE_VECTOR,                                           \"> 0\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"})\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.ADD_VB, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"true\"})\n+    static Object[] test14eB(byte[] a) {\n+        \/\/ non-power-of-2 stride\n+        for (int i = 0; i < RANGE-32; i+=11) {\n+            a[i+0]++;\n+            a[i+1]++;\n+            a[i+2]++;\n+            a[i+3]++;\n+            a[i+4]++;\n+            a[i+5]++;\n+            a[i+6]++;\n+            a[i+7]++;\n+        }\n+        return new Object[]{ a };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, IRNode.VECTOR_SIZE + \"min(max_byte, 8)\", \"> 0\",\n+                  IRNode.ADD_VB,        IRNode.VECTOR_SIZE + \"min(max_byte, 8)\", \"> 0\",\n+                  IRNode.STORE_VECTOR,                                           \"> 0\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"})\n+    @IR(counts = {IRNode.LOAD_VECTOR_B, \"= 0\",\n+                  IRNode.ADD_VB, \"= 0\",\n+                  IRNode.STORE_VECTOR, \"= 0\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIf = {\"AlignVector\", \"true\"})\n+    static Object[] test14fB(byte[] a) {\n+        \/\/ non-power-of-2 stride\n+        for (int i = 0; i < RANGE-40; i+=12) {\n+            a[i+0]++;\n+            a[i+1]++;\n+            a[i+2]++;\n+            a[i+3]++;\n+            a[i+4]++;\n+            a[i+5]++;\n+            a[i+6]++;\n+            a[i+7]++;\n+        }\n+        return new Object[]{ a };\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":108,"deletions":9,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @bug 8298935\n+ * @bug 8298935 8334431\n@@ -58,4 +58,14 @@\n-    int[] goldI7 = new int[RANGE];\n-    float[] goldF7 = new float[RANGE];\n-    int[] goldI8 = new int[RANGE];\n-    float[] goldF8 = new float[RANGE];\n+    int[] goldI7a = new int[RANGE];\n+    float[] goldF7a = new float[RANGE];\n+    int[] goldI7b = new int[RANGE];\n+    float[] goldF7b = new float[RANGE];\n+    float[] goldF7b_2 = new float[RANGE];\n+    int[] goldI7c = new int[RANGE];\n+    float[] goldF7c = new float[RANGE];\n+    int[] goldI8a = new int[RANGE];\n+    float[] goldF8a = new float[RANGE];\n+    int[] goldI8b = new int[RANGE];\n+    int[] goldI8b_2 = new int[RANGE];\n+    float[] goldF8b = new float[RANGE];\n+    int[] goldI8c = new int[RANGE];\n+    float[] goldF8c = new float[RANGE];\n@@ -66,1 +76,6 @@\n-        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\");\n+        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n+                                   \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-AlignVector\", \"-XX:-VerifyAlignVector\");\n+        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n+                                   \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:+AlignVector\", \"-XX:-VerifyAlignVector\");\n+        TestFramework.runWithFlags(\"-XX:CompileCommand=compileonly,TestCyclicDependency::test*\",\n+                                   \"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:+AlignVector\", \"-XX:+VerifyAlignVector\");\n@@ -98,6 +113,18 @@\n-        \/\/ test7\n-        init(goldI7, goldF7);\n-        test7(goldI7, goldF7);\n-        \/\/ test8\n-        init(goldI8, goldF8);\n-        test8(goldI8, goldF8);\n+        \/\/ test7a\n+        init(goldI7a, goldF7a);\n+        test7a(goldI7a, goldF7a);\n+        \/\/ test7b\n+        init(goldI7b, goldF7b, goldF7b_2);\n+        test7b(goldI7b, goldF7b, goldF7b_2);\n+        \/\/ test7c\n+        init(goldI7c, goldF7c);\n+        test7c(goldI7c, goldF7c, goldF7c);\n+        \/\/ test8a\n+        init(goldI8a, goldF8a);\n+        test8a(goldI8a, goldF8a);\n+        \/\/ test8b\n+        init(goldI8b, goldI8b_2, goldF8b);\n+        test8b(goldI8b, goldI8b_2, goldF8b);\n+        \/\/ test8c\n+        init(goldI8c, goldF8c);\n+        test8c(goldI8c, goldI8c, goldF8c);\n@@ -208,1 +235,1 @@\n-    @Run(test = \"test7\")\n+    @Run(test = \"test7a\")\n@@ -210,1 +237,1 @@\n-    public void runTest7() {\n+    public void runTest7a() {\n@@ -214,3 +241,3 @@\n-        test7(dataI, dataF);\n-        verifyI(\"test7\", dataI, goldI7);\n-        verifyF(\"test7\", dataF, goldF7);\n+        test7a(dataI, dataF);\n+        verifyI(\"test7a\", dataI, goldI7a);\n+        verifyF(\"test7a\", dataF, goldF7a);\n@@ -219,1 +246,1 @@\n-    @Run(test = \"test8\")\n+    @Run(test = \"test7b\")\n@@ -221,1 +248,49 @@\n-    public void runTest8() {\n+    public void runTest7b() {\n+        int[] dataI = new int[RANGE];\n+        float[] dataF = new float[RANGE];\n+        float[] dataF_2 = new float[RANGE];\n+        init(dataI, dataF, dataF_2);\n+        test7b(dataI, dataF, dataF_2);\n+        verifyI(\"test7b\", dataI, goldI7b);\n+        verifyF(\"test7b\", dataF, goldF7b);\n+        verifyF(\"test7b\", dataF_2, goldF7b_2);\n+    }\n+\n+    @Run(test = \"test7c\")\n+    @Warmup(100)\n+    public void runTest7c() {\n+        int[] dataI = new int[RANGE];\n+        float[] dataF = new float[RANGE];\n+        init(dataI, dataF);\n+        test7c(dataI, dataF, dataF);\n+        verifyI(\"test7c\", dataI, goldI7c);\n+        verifyF(\"test7c\", dataF, goldF7c);\n+    }\n+\n+    @Run(test = \"test8a\")\n+    @Warmup(100)\n+    public void runTest8a() {\n+        int[] dataI = new int[RANGE];\n+        float[] dataF = new float[RANGE];\n+        init(dataI, dataF);\n+        test8a(dataI, dataF);\n+        verifyI(\"test8a\", dataI, goldI8a);\n+        verifyF(\"test8a\", dataF, goldF8a);\n+    }\n+\n+    @Run(test = \"test8b\")\n+    @Warmup(100)\n+    public void runTest8b() {\n+        int[] dataI = new int[RANGE];\n+        int[] dataI_2 = new int[RANGE];\n+        float[] dataF = new float[RANGE];\n+        init(dataI, dataI_2, dataF);\n+        test8b(dataI, dataI_2, dataF);\n+        verifyI(\"test8b\", dataI, goldI8b);\n+        verifyI(\"test8b\", dataI_2, goldI8b_2);\n+        verifyF(\"test8b\", dataF, goldF8b);\n+    }\n+\n+    @Run(test = \"test8c\")\n+    @Warmup(100)\n+    public void runTest8c() {\n@@ -225,3 +300,3 @@\n-        test8(dataI, dataF);\n-        verifyI(\"test8\", dataI, goldI8);\n-        verifyF(\"test8\", dataF, goldF8);\n+        test8c(dataI, dataI, dataF);\n+        verifyI(\"test8c\", dataI, goldI8c);\n+        verifyF(\"test8c\", dataF, goldF8c);\n@@ -331,1 +406,2 @@\n-    @IR(counts = {IRNode.ADD_VI, \"> 0\"},\n+    @IR(counts = {IRNode.ADD_VI, \"= 0\",\n+                  IRNode.ADD_VF, \"= 0\"},\n@@ -334,0 +410,4 @@\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, \"= 0\"},\n+        applyIf = {\"AlignVector\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n@@ -335,1 +415,1 @@\n-    static void test7(int[] dataI, float[] dataF) {\n+    static void test7a(int[] dataI, float[] dataF) {\n@@ -338,2 +418,0 @@\n-            \/\/ write forward 3 -> cannot vectorize\n-            \/\/ separate types should make decision separately if they vectorize or not\n@@ -342,0 +420,6 @@\n+            \/\/ write forward 3:\n+            \/\/   AlignVector=true -> cannot vectorize because load and store cannot be both aligned\n+            \/\/   AlignVector=false -> could vectorize, but would get 2-element vectors where\n+            \/\/                        store-to-load-forwarding fails, because we have store-load\n+            \/\/                        dependencies that have partial overlap.\n+            \/\/                        -> all vectorization cancled.\n@@ -348,1 +432,28 @@\n-    @IR(counts = {IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, \"= 0\"},\n+        applyIf = {\"AlignVector\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Some aarch64 machines have AlignVector == true, like ThunderX2\n+    static void test7b(int[] dataI, float[] dataF, float[] dataF_2) {\n+        for (int i = 0; i < RANGE - 32; i++) {\n+            \/\/ write forward 32 -> more than vector size -> can vectorize\n+            int v = dataI[i];\n+            dataI[i + 32] = v + 5;\n+            \/\/ write forward 3 to different array reference:\n+            \/\/   AlignVector=true -> cannot vectorize because load and store cannot be both aligned\n+            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n+            \/\/                        failure. But we can only have 2-element vectors in case\n+            \/\/                        the two float-arrays reference the same array.\n+            \/\/                        Note: at runtime the float-arrays are always different.\n+            float f = dataF[i];\n+            dataF_2[i + 3] = f + 3.5f;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"2\", \"> 0\"},\n@@ -351,0 +462,4 @@\n+    @IR(counts = {IRNode.ADD_VI, \"> 0\",\n+                  IRNode.ADD_VF, \"= 0\"},\n+        applyIf = {\"AlignVector\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n@@ -352,1 +467,1 @@\n-    static void test8(int[] dataI, float[] dataF) {\n+    static void test7c(int[] dataI, float[] dataF, float[] dataF_2) {\n@@ -355,2 +470,31 @@\n-            \/\/ write forward 3 -> cannot vectorize\n-            \/\/ separate types should make decision separately if they vectorize or not\n+            int v = dataI[i];\n+            dataI[i + 32] = v + 5;\n+            \/\/ write forward 3 to different array reference:\n+            \/\/   AlignVector=true -> cannot vectorize because load and store cannot be both aligned\n+            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n+            \/\/                        failure. But we can only have 2-element vectors in case\n+            \/\/                        the two float-arrays reference the same array.\n+            \/\/                        Note: at runtime the float-arrays are always the same.\n+            float f = dataF[i];\n+            dataF_2[i + 3] = f + 3.5f;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ADD_VI, \"= 0\",\n+                  IRNode.ADD_VF, \"= 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"= 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+        applyIf = {\"AlignVector\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Some aarch64 machines have AlignVector == true, like ThunderX2\n+    static void test8a(int[] dataI, float[] dataF) {\n+        for (int i = 0; i < RANGE - 32; i++) {\n+            \/\/ write forward 3:\n+            \/\/   AlignVector=true -> cannot vectorize because load and store cannot be both aligned\n+            \/\/   AlignVector=false -> could vectorize, but would get 2-element vectors where\n+            \/\/                        store-to-load-forwarding fails, because we have store-load\n+            \/\/                        dependencies that have partial overlap.\n+            \/\/                        -> all vectorization cancled.\n@@ -359,0 +503,53 @@\n+            \/\/ write forward 32 -> more than vector size -> can vectorize\n+            float f = dataF[i];\n+            dataF[i + 32] = f + 3.5f;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ADD_VI, IRNode.VECTOR_SIZE + \"2\", \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"= 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+        applyIf = {\"AlignVector\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Some aarch64 machines have AlignVector == true, like ThunderX2\n+    static void test8b(int[] dataI, int[] dataI_2, float[] dataF) {\n+        for (int i = 0; i < RANGE - 32; i++) {\n+            \/\/ write forward 3 to different array reference:\n+            \/\/   AlignVector=true -> cannot vectorize because load and store cannot be both aligned\n+            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n+            \/\/                        failure. But we can only have 2-element vectors in case\n+            \/\/                        the two float-arrays reference the same array.\n+            \/\/                        Note: at runtime the float-arrays are always different.\n+            int v = dataI[i];\n+            dataI_2[i + 3] = v + 5;\n+            \/\/ write forward 32 -> more than vector size -> can vectorize\n+            float f = dataF[i];\n+            dataF[i + 32] = f + 3.5f;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.ADD_VI, IRNode.VECTOR_SIZE + \"2\", \"> 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    @IR(counts = {IRNode.ADD_VI, \"= 0\",\n+                  IRNode.ADD_VF, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \"> 0\"},\n+        applyIf = {\"AlignVector\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Some aarch64 machines have AlignVector == true, like ThunderX2\n+    static void test8c(int[] dataI, int[] dataI_2, float[] dataF) {\n+        for (int i = 0; i < RANGE - 32; i++) {\n+            \/\/ write forward 3 to different array reference:\n+            \/\/   AlignVector=true -> cannot vectorize because load and store cannot be both aligned\n+            \/\/   AlignVector=false -> vectorizes because we cannot prove store-to-load forwarding\n+            \/\/                        failure. But we can only have 2-element vectors in case\n+            \/\/                        the two float-arrays reference the same array.\n+            \/\/                        Note: at runtime the float-arrays are always the same.\n+            int v = dataI[i];\n+            dataI_2[i + 3] = v + 5;\n+            \/\/ write forward 32 -> more than vector size -> can vectorize\n@@ -383,0 +580,16 @@\n+    public static void init(int[] dataI, float[] dataF, float[] dataF_2) {\n+        for (int j = 0; j < RANGE; j++) {\n+            dataI[j] = j;\n+            dataF[j] = j * 0.5f;\n+            dataF_2[j] = j * 0.3f;\n+        }\n+    }\n+\n+    public static void init(int[] dataI, int[] dataI_2, float[] dataF) {\n+        for (int j = 0; j < RANGE; j++) {\n+            dataI[j] = j;\n+            dataI_2[j] = 3*j - 42;\n+            dataF[j] = j * 0.5f;\n+        }\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestCyclicDependency.java","additions":243,"deletions":30,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -646,0 +646,6 @@\n+    enum ExpectVectorization {\n+        ALWAYS,    \/\/ -> positive \"count\" IR rule\n+        UNKNOWN,   \/\/ -> disable IR rule\n+        NEVER      \/\/ -> negative \"failOn\" IR rule\n+    };\n+\n@@ -659,0 +665,1 @@\n+            boolean isSingleArray;\n@@ -661,0 +668,1 @@\n+                isSingleArray = true;\n@@ -666,0 +674,1 @@\n+                isSingleArray = false;\n@@ -671,0 +680,1 @@\n+                isSingleArray = false;\n@@ -715,1 +725,1 @@\n-                   generateIRRules(),\n+                   generateIRRules(isSingleArray),\n@@ -729,1 +739,1 @@\n-        String generateIRRules() {\n+        String generateIRRules(boolean isSingleArray) {\n@@ -747,0 +757,2 @@\n+                int log2 = 31 - Integer.numberOfLeadingZeros(offset);\n+                int floorPow2Offset = 1 << log2;\n@@ -748,4 +760,2 @@\n-                    int log2 = 31 - Integer.numberOfLeadingZeros(offset);\n-                    int floorPow2 = 1 << log2;\n-                    maxVectorWidth = Math.min(maxVectorWidth, floorPow2 * type.size);\n-                    builder.append(\"    \/\/ Vectors must have at most \" + floorPow2 +\n+                    maxVectorWidth = Math.min(maxVectorWidth, floorPow2Offset * type.size);\n+                    builder.append(\"    \/\/ Vectors must have at most \" + floorPow2Offset +\n@@ -756,0 +766,43 @@\n+                ExpectVectorization expectVectorization = ExpectVectorization.ALWAYS;\n+                if (isSingleArray && 0 < offset && offset < 64) {\n+                    \/\/ In a store-forward case at iteration distances below a certain threshold, and not there\n+                    \/\/ is some partial overlap between the expected vector store and some vector load in a later\n+                    \/\/ iteration, we avoid vectorization to avoid the latency penalties of store-to-load\n+                    \/\/ forwarding failure. We only detect these failures in single-array cases.\n+                    \/\/\n+                    \/\/ Note: we currently never detect store-to-load-forwarding failures beyond 64 iterations,\n+                    \/\/       And so if the offset >= 64, we always expect vectorization.\n+                    \/\/\n+                    \/\/ The condition for partial overlap:\n+                    \/\/   offset % #elements != 0\n+                    \/\/\n+                    \/\/ But we do not know #elements exactly, only a range from min\/maxVectorWidth.\n+\n+                    int maxElements = maxVectorWidth \/ type.size;\n+                    int minElements = minVectorWidth \/ type.size;\n+                    boolean sometimesPartialOverlap = offset % maxElements != 0;\n+                    \/\/ If offset % minElements != 0, then it does also not hold for any larger vector.\n+                    boolean alwaysPartialOverlap = offset % minElements != 0;\n+\n+                    if (alwaysPartialOverlap) {\n+                        \/\/ It is a little tricky to know the exact threshold. On all platforms and in all\n+                        \/\/ unrolling cases, it is between 8 and 64. Hence, we have these 3 cases:\n+                        if (offset <= 8) {\n+                            builder.append(\"    \/\/ We always detect store-to-load-forwarding failures -> never vectorize.\\n\");\n+                            expectVectorization = ExpectVectorization.NEVER;\n+                        } else if (offset <= 64) {\n+                            builder.append(\"    \/\/ Unknown if detect store-to-load-forwarding failures -> maybe disable IR rules.\\n\");\n+                            expectVectorization = ExpectVectorization.UNKNOWN;\n+                        } else {\n+                            \/\/ offset > 64  -> offset too large, expect no store-to-load-failure detection\n+                            throw new RuntimeException(\"impossible\");\n+                        }\n+                    } else if (sometimesPartialOverlap && !alwaysPartialOverlap) {\n+                        builder.append(\"    \/\/ Partial overlap condition true: sometimes but not always -> maybe disable IR rules.\\n\");\n+                        expectVectorization = ExpectVectorization.UNKNOWN;\n+                    } else {\n+                        builder.append(\"    \/\/ Partial overlap never happens -> expect vectorization.\\n\");\n+                        expectVectorization = ExpectVectorization.ALWAYS;\n+                    }\n+                }\n+\n@@ -757,0 +810,1 @@\n+                ExpectVectorization expectVectorization1 = expectVectorization;\n@@ -763,1 +817,1 @@\n-                    r1.setNegative();\n+                    expectVectorization1 = ExpectVectorization.NEVER;\n@@ -767,0 +821,1 @@\n+                r1.setExpectVectVectorization(expectVectorization1);\n@@ -770,0 +825,1 @@\n+                ExpectVectorization expectVectorization2 = expectVectorization;\n@@ -794,1 +850,1 @@\n-                    r2.setNegative();\n+                    expectVectorization2 = ExpectVectorization.NEVER;\n@@ -796,2 +852,6 @@\n-                    builder.append(\"    \/\/ Alignment unknown -> disable IR rule.\\n\");\n-                    r2.disable();\n+                    if (expectVectorization2 != ExpectVectorization.NEVER) {\n+                        builder.append(\"    \/\/ Alignment unknown -> disable IR rule.\\n\");\n+                        expectVectorization2 = ExpectVectorization.UNKNOWN;\n+                    } else {\n+                        builder.append(\"    \/\/ Alignment unknown -> but already proved no vectorization above.\\n\");\n+                    }\n@@ -802,1 +862,1 @@\n-                    r2.setNegative();\n+                    expectVectorization2 = ExpectVectorization.NEVER;\n@@ -806,0 +866,1 @@\n+                r2.setExpectVectVectorization(expectVectorization2);\n@@ -849,6 +910,6 @@\n-        void setNegative() {\n-            this.isPositiveRule = false;\n-        }\n-\n-        void disable() {\n-            this.isEnabled = false;\n+        void setExpectVectVectorization(ExpectVectorization expectVectorization) {\n+            switch(expectVectorization) {\n+                case ExpectVectorization.NEVER   -> { this.isPositiveRule = false; }\n+                case ExpectVectorization.UNKNOWN -> { this.isEnabled = false; }\n+                case ExpectVectorization.ALWAYS  -> {}\n+            }\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestDependencyOffsets.java","additions":78,"deletions":17,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -141,1 +141,1 @@\n-    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse2\", \"true\"},\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"sse4.1\", \"true\"},\n@@ -143,0 +143,3 @@\n+    \/\/ With sse2, the MulI does not vectorize. This means we have vectorized stores\n+    \/\/ to res1, but scalar loads from res1. The store-to-load-forwarding failure\n+    \/\/ detection catches this and rejects vectorization.\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/LoopCombinedOpTest.java","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,142 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package org.openjdk.bench.vm.compiler;\n+\n+import org.openjdk.jmh.annotations.*;\n+import org.openjdk.jmh.infra.*;\n+\n+import java.lang.invoke.*;\n+\n+import java.util.concurrent.TimeUnit;\n+import java.util.Random;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@State(Scope.Thread)\n+@Warmup(iterations = 2, time = 1, timeUnit = TimeUnit.SECONDS)\n+@Measurement(iterations = 3, time = 1, timeUnit = TimeUnit.SECONDS)\n+@Fork(value = 1)\n+public abstract class VectorStoreToLoadForwarding {\n+    @Param({\"10000\"})\n+    public int SIZE;\n+\n+    @Param({  \"0\",   \"1\",   \"2\",   \"3\",   \"4\",   \"5\",   \"6\",   \"7\",   \"8\",   \"9\",\n+             \"10\",  \"11\",  \"12\",  \"13\",  \"14\",  \"15\",  \"16\",  \"17\",  \"18\",  \"19\",\n+             \"20\",  \"21\",  \"22\",  \"23\",  \"24\",  \"25\",  \"26\",  \"27\",  \"28\",  \"29\",\n+             \"30\",  \"31\",  \"32\",  \"33\",  \"34\",  \"35\",  \"36\",  \"37\",  \"38\",  \"39\",\n+             \"40\",  \"41\",  \"42\",  \"43\",  \"44\",  \"45\",  \"46\",  \"47\",  \"48\",  \"49\",\n+             \"50\",  \"51\",  \"52\",  \"53\",  \"54\",  \"55\",  \"56\",  \"57\",  \"58\",  \"59\",\n+             \"60\",  \"61\",  \"62\",  \"63\",  \"64\",  \"65\",  \"66\",  \"67\",  \"68\",  \"69\",\n+             \"70\",  \"71\",  \"72\",  \"73\",  \"74\",  \"75\",  \"76\",  \"77\",  \"78\",  \"79\",\n+             \"80\",  \"81\",  \"82\",  \"83\",  \"84\",  \"85\",  \"86\",  \"87\",  \"88\",  \"89\",\n+             \"90\",  \"91\",  \"92\",  \"93\",  \"94\",  \"95\",  \"96\",  \"97\",  \"98\",  \"99\",\n+            \"100\", \"101\", \"102\", \"103\", \"104\", \"105\", \"106\", \"107\", \"108\", \"109\",\n+            \"110\", \"111\", \"112\", \"113\", \"114\", \"115\", \"116\", \"117\", \"118\", \"119\",\n+            \"120\", \"121\", \"122\", \"123\", \"124\", \"125\", \"126\", \"127\", \"128\", \"129\"})\n+    public int OFFSET;\n+\n+    \/\/ To get compile-time constants for OFFSET\n+    static final MutableCallSite MUTABLE_CONSTANT = new MutableCallSite(MethodType.methodType(int.class));\n+    static final MethodHandle MUTABLE_CONSTANT_HANDLE = MUTABLE_CONSTANT.dynamicInvoker();\n+\n+    public int START = 1000;\n+\n+    private byte[] aB;\n+    private short[] aS;\n+    private int[] aI;\n+    private long[] aL;\n+\n+    @Param(\"0\")\n+    private int seed;\n+    private Random r = new Random(seed);\n+\n+    @Setup\n+    public void init() throws Throwable {\n+        aB = new byte[SIZE];\n+        aS = new short[SIZE];\n+        aI = new int[SIZE];\n+        aL = new long[SIZE];\n+\n+        for (int i = START; i < SIZE; i++) {\n+            aB[i] = (byte)r.nextInt();\n+            aS[i] = (short)r.nextInt();\n+            aI[i] = r.nextInt();\n+            aL[i] = r.nextLong();\n+        }\n+\n+        MethodHandle constant = MethodHandles.constant(int.class, OFFSET);\n+        MUTABLE_CONSTANT.setTarget(constant);\n+    }\n+\n+    @CompilerControl(CompilerControl.Mode.DONT_INLINE)\n+    private int offset_con() throws Throwable {\n+        return (int) MUTABLE_CONSTANT_HANDLE.invokeExact();\n+    }\n+\n+    @Benchmark\n+    public void bytes() throws Throwable {\n+        int offset = offset_con();\n+        for (int i = START; i < SIZE; i++) {\n+            aB[i] = (byte)(aB[i - offset] + 1);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void shorts() throws Throwable {\n+        int offset = offset_con();\n+        for (int i = START; i < SIZE; i++) {\n+            aS[i] = (short)(aS[i - offset] + 1);\n+        }\n+    }\n+\n+    @Benchmark\n+    public void ints() throws Throwable {\n+        int offset = offset_con();\n+        for (int i = START; i < SIZE; i++) {\n+            aI[i] = aI[i - offset] + 1;\n+        }\n+    }\n+\n+    @Benchmark\n+    public void longs() throws Throwable {\n+        int offset = offset_con();\n+        for (int i = START; i < SIZE; i++) {\n+            aL[i] = (long)(aL[i - offset] + 1);\n+        }\n+    }\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:+UseSuperWord\"\n+    })\n+    public static class Default extends VectorStoreToLoadForwarding {}\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:-UseSuperWord\"\n+    })\n+    public static class NoVectorization extends VectorStoreToLoadForwarding {}\n+\n+    @Fork(value = 1, jvmArgs = {\n+        \"-XX:+UseSuperWord\", \"-XX:+UnlockDiagnosticVMOptions\", \"-XX:SuperWordStoreToLoadForwardingFailureDetection=0\"\n+    })\n+    public static class NoStoreToLoadForwardFailureDetection extends VectorStoreToLoadForwarding {}\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/VectorStoreToLoadForwarding.java","additions":142,"deletions":0,"binary":false,"changes":142,"status":"added"}]}