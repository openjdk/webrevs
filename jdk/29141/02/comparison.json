{"files":[{"patch":"@@ -6084,1 +6084,5 @@\n-  \/\/ (parsedLength or (parsedLength - 48) must be divisible by 64.)\n+  \/\/ we assume that parsed and condensed are allocated such that for\n+  \/\/ n = (parsedLength + 63) \/ 64\n+  \/\/ n blocks of 96 bytes of input can be processed, i.e.\n+  \/\/ index + n * 96 <= condensed.length and\n+  \/\/ n * 64 <= parsed.length\n@@ -6088,2 +6092,2 @@\n-  \/\/ parsed (short[112 or 256]) = c_rarg2\n-  \/\/ parsedLength (112 or 256) = c_rarg3\n+  \/\/ parsed (short[]) = c_rarg2\n+  \/\/ parsedLength = c_rarg3\n@@ -6091,1 +6095,1 @@\n-    Label L_F00, L_loop, L_end;\n+    Label L_F00, L_loop;\n@@ -6212,69 +6216,2 @@\n-    __ cmp(parsedLength, (u1)64);\n-    __ br(Assembler::GE, L_loop);\n-    __ cbz(parsedLength, L_end);\n-\n-    \/\/ if anything is left it should be a final 72 bytes of input\n-    \/\/ i.e. a final 48 12-bit values. so we handle this by loading\n-    \/\/ 48 bytes into all 16B lanes of front(vin) and only 24\n-    \/\/ bytes into the lower 8B lane of back(vin)\n-    vs_ld3_post(vs_front(vin), __ T16B, condensed);\n-    vs_ld3(vs_back(vin), __ T8B, condensed);\n-\n-    \/\/ Expand vin[0] into va[0:1], and vin[1] into va[2:3] and va[4:5]\n-    \/\/ n.b. target elements 2 and 3 of va duplicate elements 4 and\n-    \/\/ 5 and target element 2 of vb duplicates element 4.\n-    __ ushll(va[0], __ T8H, vin[0], __ T8B, 0);\n-    __ ushll2(va[1], __ T8H, vin[0], __ T16B, 0);\n-    __ ushll(va[2], __ T8H, vin[1], __ T8B, 0);\n-    __ ushll2(va[3], __ T8H, vin[1], __ T16B, 0);\n-    __ ushll(va[4], __ T8H, vin[1], __ T8B, 0);\n-    __ ushll2(va[5], __ T8H, vin[1], __ T16B, 0);\n-\n-    \/\/ This time expand just the lower 8 lanes\n-    __ ushll(vb[0], __ T8H, vin[3], __ T8B, 0);\n-    __ ushll(vb[2], __ T8H, vin[4], __ T8B, 0);\n-    __ ushll(vb[4], __ T8H, vin[4], __ T8B, 0);\n-\n-    \/\/ shift lo byte of copy 1 of the middle stripe into the high byte\n-    __ shl(va[2], __ T8H, va[2], 8);\n-    __ shl(va[3], __ T8H, va[3], 8);\n-    __ shl(vb[2], __ T8H, vb[2], 8);\n-\n-    \/\/ expand vin[2] into va[6:7] and lower 8 lanes of vin[5] into\n-    \/\/ vb[6] pre-shifted by 4 to ensure top bits of the input 12-bit\n-    \/\/ int are in bit positions [4..11].\n-    __ ushll(va[6], __ T8H, vin[2], __ T8B, 4);\n-    __ ushll2(va[7], __ T8H, vin[2], __ T16B, 4);\n-    __ ushll(vb[6], __ T8H, vin[5], __ T8B, 4);\n-\n-    \/\/ mask hi 4 bits of each 1st 12-bit int in pair from copy1 and\n-    \/\/ shift lo 4 bits of each 2nd 12-bit int in pair to bottom of\n-    \/\/ copy2\n-    __ andr(va[2], __ T16B, va[2], v31);\n-    __ andr(va[3], __ T16B, va[3], v31);\n-    __ ushr(va[4], __ T8H, va[4], 4);\n-    __ ushr(va[5], __ T8H, va[5], 4);\n-    __ andr(vb[2], __ T16B, vb[2], v31);\n-    __ ushr(vb[4], __ T8H, vb[4], 4);\n-\n-\n-\n-    \/\/ sum hi 4 bits and lo 8 bits of each 1st 12-bit int in pair and\n-    \/\/ hi 8 bits plus lo 4 bits of each 2nd 12-bit int in pair\n-\n-    \/\/ n.b. ordering ensures: i) inputs are consumed before they are\n-    \/\/ overwritten ii) order of 16-bit results across succsessive\n-    \/\/ pairs of vectors in va and then lower half of vb reflects order\n-    \/\/ of corresponding 12-bit inputs\n-    __ addv(va[0], __ T8H, va[0], va[2]);\n-    __ addv(va[2], __ T8H, va[1], va[3]);\n-    __ addv(va[1], __ T8H, va[4], va[6]);\n-    __ addv(va[3], __ T8H, va[5], va[7]);\n-    __ addv(vb[0], __ T8H, vb[0], vb[2]);\n-    __ addv(vb[1], __ T8H, vb[4], vb[6]);\n-\n-    \/\/ store 48 results interleaved as shorts\n-    vs_st2_post(vs_front(va), __ T8H, parsed);\n-    vs_st2_post(vs_front(vs_front(vb)), __ T8H, parsed);\n-\n-    __ BIND(L_end);\n+    __ cmp(parsedLength, (u1)0);\n+    __ br(Assembler::GT, L_loop);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":10,"deletions":73,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -1356,7 +1356,4 @@\n-    \/\/ The intrinsic implementations assume that the input and output buffers\n-    \/\/ are such that condensed can be read in 96-byte chunks and\n-    \/\/ parsed can be written in 64 shorts chunks except for the last chunk\n-    \/\/ that can be either 48 or 64 shorts. In other words,\n-    \/\/ if (i - 1) * 64 < parsedLengths <= i * 64 then\n-    \/\/ parsed.length should be either i * 64 or (i-1) * 64 + 48 and\n-    \/\/ condensed.length should be at least index + i * 96.\n+    \/\/ An intrinsic implementation assumes that the input and output buffers\n+    \/\/ are such that condensed can be read in chunks of 192 bytes and\n+    \/\/ parsed can be written in chunks of 128 shorts, so callers should allocate\n+    \/\/ the condensed and parsed arrays accordingly, see the assert()\n@@ -1365,7 +1362,4 @@\n-        int i = parsedLength \/ 64;\n-        int remainder = parsedLength - i * 64;\n-        if (remainder != 0) {\n-            i++;\n-        }\n-        assert ((remainder == 0) || (remainder == 48)) &&\n-                (index + i * 96 <= condensed.length);\n+        int n = (parsedLength + 127) \/ 128;\n+        assert ((parsed.length >= n * 128) &&\n+                (condensed.length >= index + n * 192));\n+\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/ML_KEM.java","additions":8,"deletions":14,"binary":false,"changes":22,"status":"modified"}]}