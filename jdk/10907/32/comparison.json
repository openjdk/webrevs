{"files":[{"patch":"@@ -3821,1 +3821,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3839,27 +3839,40 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, rscratch1, slow);\n+\n+        \/\/ Indicate success on completion.\n+        __ cmp(oop, oop); \/\/ Force ZF=1 to indicate success.\n+        __ b(count);\n+        __ bind(slow);\n+        __ tst(oop, oop); \/\/ Force ZF=0 to indicate failure and take slow-path. We know that oop != null.\n+        __ b(no_count);\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+        __ br(Assembler::EQ, cont);\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ mov(rscratch1, sp);\n+        __ sub(disp_hdr, disp_hdr, rscratch1);\n+        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+        \/\/ If condition is true we are cont and hence we can store 0 as the\n+        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ b(cont);\n+      }\n@@ -3868,0 +3881,1 @@\n+      __ b(cont);\n@@ -3869,1 +3883,0 @@\n-    __ b(cont);\n@@ -3882,7 +3895,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3903,0 +3917,1 @@\n+    __ bind(count);\n@@ -3916,1 +3931,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3920,1 +3935,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -3934,3 +3949,10 @@\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+\n+        \/\/ Indicate success on completion.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -3938,2 +3960,4 @@\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                   \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ b(cont);\n+      }\n@@ -3942,0 +3966,1 @@\n+      __ b(cont);\n@@ -3943,1 +3968,0 @@\n-    __ b(cont);\n@@ -3951,0 +3975,14 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -3977,0 +4015,1 @@\n+    __ bind(count);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":83,"deletions":44,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -66,2 +66,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -86,33 +85,38 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+     orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -130,5 +134,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -138,8 +145,4 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+\n+  if (UseFastLocking) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n@@ -147,1 +150,13 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -149,2 +164,0 @@\n-  \/\/ done\n-  bind(done);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":64,"deletions":51,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -66,0 +67,24 @@\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 20;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ str(rthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ ldrw(t, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+  __ subw(t, t, oopSize);\n+  __ strw(t, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -761,50 +761,55 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n-\n+    if (UseFastLocking) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -816,1 +821,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -853,3 +858,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -863,3 +870,2 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n@@ -867,2 +873,17 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = header_reg;\n+      ldrw(tmp, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+      subw(tmp, tmp, oopSize);\n+      ldr(tmp, Address(rthread, tmp));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n+\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n@@ -870,2 +891,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n@@ -873,0 +894,3 @@\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":85,"deletions":61,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -6214,0 +6214,52 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+  subw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_offset_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -1584,0 +1584,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1778,28 +1778,33 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg %r0\n+        __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ orr(swap_reg, rscratch1, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest iff dest == r0 else r0 <- dest\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+\n+        __ sub(swap_reg, sp, swap_reg);\n+        __ neg(swap_reg, swap_reg);\n+        __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ br(Assembler::NE, slow_path_lock);\n+      }\n@@ -1916,1 +1921,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1932,9 +1937,14 @@\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ldr(old_hdr, Address(r0, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":48,"deletions":38,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -55,2 +55,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -70,1 +69,1 @@\n-    bnez(t0, slow_case, true \/* is_far *\/);\n+    bnez(t0, slow_case, \/* is_far *\/ true);\n@@ -75,33 +74,39 @@\n-  \/\/ and mark it as unlocked\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  la(t1, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr -sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  sub(hdr, hdr, sp);\n-  mv(t0, aligned_mask - (int)os::vm_page_size());\n-  andr(hdr, hdr, t0);\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  bnez(hdr, slow_case, \/* is_far *\/ true);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, t0, t1, slow_case);\n+  } else {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    ori(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    la(t1, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr -sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    sub(hdr, hdr, sp);\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n+    andr(hdr, hdr, t0);\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    bnez(hdr, slow_case, \/* is_far *\/ true);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -118,5 +123,8 @@\n-  \/\/ load displaced header\n-  ld(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  beqz(hdr, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    ld(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    beqz(hdr, done);\n+  }\n+\n@@ -126,8 +134,4 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    la(t0, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+\n+  if (UseFastLocking) {\n+    ld(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, t0, t1, slow_case);\n@@ -135,1 +139,13 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      la(t0, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -137,2 +153,0 @@\n-  \/\/ done\n-  bind(done);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":66,"deletions":52,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,24 @@\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0 without using RISC-V compressed\n+  \/\/ instruction-set extension, in which case C2CodeStubList::emit() will throw an assertion\n+  \/\/ and report the actual size that is needed.\n+  return 20;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  Register t = tmp();\n+  assert(t != noreg, \"need tmp register\");\n+\n+  \/\/ Fix owner to be the current thread.\n+  __ sd(xthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+\n+  \/\/ Pop owner object from lock-stack.\n+  __ lwu(t, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+  __ subw(t, t, oopSize);\n+  __ sw(t, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+\n+  __ j(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/c2_CodeStubs_riscv.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (UseFastLocking) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -846,1 +852,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -884,3 +890,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -894,3 +902,11 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = header_reg;\n+      lwu(tmp, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+      subw(tmp, tmp, oopSize);\n+      add(tmp, xthread, tmp);\n+      ld(tmp, Address(tmp, 0));\n+      bne(tmp, obj_reg, slow_case);\n@@ -898,2 +914,3 @@\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n@@ -901,2 +918,12 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+      bind(slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":66,"deletions":39,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -2423,1 +2423,1 @@\n-    bgtu(in_nmethod ? sp : fp, t0, slow_path, true \/* is_far *\/);\n+    bgtu(in_nmethod ? sp : fp, t0, slow_path, \/* is_far *\/ true);\n@@ -2426,1 +2426,1 @@\n-    bnez(t0, slow_path, true \/* is_far *\/);\n+    bnez(t0, slow_path, \/* is_far *\/ true);\n@@ -4485,0 +4485,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+  mv(tmp2, (unsigned)LockStack::end_offset());\n+  bge(tmp1, tmp2, slow, \/* is_far *\/ true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+  add(tmp2, xthread, tmp1);\n+  sd(obj, Address(tmp2, 0));\n+  addw(tmp1, tmp1, oopSize);\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  mv(tmp1, ~markWord::lock_mask_in_place);\n+  andr(hdr, hdr, tmp1);\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+  subw(tmp1, tmp1, oopSize);\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_offset_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":57,"deletions":2,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -1386,0 +1386,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2387,1 +2387,1 @@\n-  \/\/ using the cr register as the bool result: 0 for success; others failed.\n+  \/\/ Use cr register to indicate the fast_lock result: zero for success; non-zero for failure.\n@@ -2397,1 +2397,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -2405,4 +2405,4 @@\n-      __ load_klass(flag, oop);\n-      __ lwu(flag, Address(flag, Klass::access_flags_offset()));\n-      __ andi(flag, flag, JVM_ACC_IS_VALUE_BASED_CLASS, tmp \/* tmp *\/);\n-      __ bnez(flag, cont, true \/* is_far *\/);\n+      __ load_klass(tmp, oop);\n+      __ lwu(tmp, Address(tmp, Klass::access_flags_offset()));\n+      __ andi(flag, tmp, JVM_ACC_IS_VALUE_BASED_CLASS);\n+      __ bnez(flag, cont, \/* is_far *\/ true);\n@@ -2416,30 +2416,43 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+\n+        \/\/ Indicate success on completion.\n+        __ mv(flag, zr);\n+        __ j(count);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+        __ j(no_count);\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ ori(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+                   Assembler::rl, \/*result*\/disp_hdr);\n+        __ mv(flag, zr);\n+        __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+        \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ sub(disp_hdr, disp_hdr, sp);\n+        __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n+        \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+        \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+        \/\/ recursive lock.\n+        __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+        __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      }\n+      __ j(cont);\n@@ -2448,0 +2461,1 @@\n+      __ j(cont);\n@@ -2450,2 +2464,0 @@\n-    __ j(cont);\n-\n@@ -2462,6 +2474,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2478,1 +2492,2 @@\n-\n+    \/\/ zero flag indicates success\n+    \/\/ non-zero flag indicates failure\n@@ -2481,0 +2496,1 @@\n+    __ bind(count);\n@@ -2486,1 +2502,1 @@\n-  \/\/ using cr flag to indicate the fast_unlock result: 0 for success; others failed.\n+  \/\/ Use cr register to indicate the fast_unlock result: zero for success; non-zero for failure.\n@@ -2496,1 +2512,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -2500,1 +2516,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2515,7 +2531,20 @@\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n-\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+        \/\/ Indicate success on completion.\n+        __ mv(flag, zr);\n+        __ j(count);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+        __ j(no_count);\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n+\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+                   Assembler::rl, \/*result*\/tmp);\n+        __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+        __ j(cont);\n+      }\n@@ -2524,0 +2553,1 @@\n+      __ j(cont);\n@@ -2525,1 +2555,0 @@\n-    __ j(cont);\n@@ -2533,0 +2562,12 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ld(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ andi(t0, tmp2, (int64_t)(intptr_t)ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ bnez(t0, stub->entry(), \/* is_far *\/ true);\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -2555,1 +2596,2 @@\n-\n+    \/\/ zero flag indicates success\n+    \/\/ non-zero flag indicates failure\n@@ -2558,0 +2600,1 @@\n+    __ bind(count);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":100,"deletions":57,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -1675,25 +1675,30 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg % x10\n-      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ ori(swap_reg, t0, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest if dest == x10 else x10 <- dest\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-      __ sub(swap_reg, swap_reg, sp);\n-      __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ bnez(swap_reg, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg % x10\n+        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ ori(swap_reg, t0, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest if dest == x10 else x10 <- dest\n+        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+        __ sub(swap_reg, swap_reg, sp);\n+        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ bnez(swap_reg, slow_path_lock);\n+      }\n@@ -1794,1 +1799,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1810,9 +1815,14 @@\n-      \/\/ get address of the stack lock\n-      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ld(old_hdr, Address(x10, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ld(old_hdr, Address(x10, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -3511,0 +3511,1 @@\n+    Register tmp = UseFastLocking ? op->scratch_opr()->as_register() : noreg;\n@@ -3512,1 +3513,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -322,1 +322,2 @@\n-  monitor_enter(obj.result(), lock, syncTempOpr(), LIR_OprFact::illegalOpr,\n+  LIR_Opr tmp = UseFastLocking ? new_register(T_INT) : LIR_OprFact::illegalOpr;\n+  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -45,2 +45,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -65,33 +64,45 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case);\n+  } else {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -111,6 +122,9 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n+\n@@ -119,1 +133,0 @@\n-\n@@ -121,10 +134,0 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -132,0 +135,16 @@\n+  if (UseFastLocking) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":72,"deletions":53,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  int lock_object  (Register swap, Register obj, Register disp_hdr, Label& slow_case);\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Register tmp, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,17 @@\n+#ifdef _LP64\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 18;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n+  __ subl(Address(r15_thread, JavaThread::lock_stack_offset_offset()), oopSize);\n+  __ jmp(continuation());\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -551,1 +551,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -602,1 +602,1 @@\n-  jccb(Assembler::notZero, IsInflated);\n+  jcc(Assembler::notZero, IsInflated);\n@@ -605,14 +605,23 @@\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n-    movptr(Address(boxReg, 0), tmpReg);\n+    if (UseFastLocking) {\n+      Label slow;\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n+      jmp(COUNT);\n+      bind(slow);\n+      testptr(objReg, objReg); \/\/ force ZF=0 to indicate failure\n+      jmp(NO_COUNT);\n+    } else {\n+      \/\/ Attempt stack-locking ...\n+      orptr (tmpReg, markWord::unlocked_value);\n+      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+      lock();\n+      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+      jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+      \/\/ Recursive locking.\n+      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+      \/\/ Locked by current thread if difference with current SP is less than one page.\n+      subptr(tmpReg, rsp);\n+      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+      movptr(Address(boxReg, 0), tmpReg);\n+    }\n@@ -650,2 +659,1 @@\n-  \/\/ Optimistic form: consider XORL tmpReg,tmpReg\n-  movptr(tmpReg, NULL_WORD);\n+  xorptr(tmpReg, tmpReg);\n@@ -653,7 +661,1 @@\n-  \/\/ Appears unlocked - try to swing _owner from null to non-null.\n-  \/\/ Ideally, I'd manifest \"Self\" with get_thread and then attempt\n-  \/\/ to CAS the register containing Self into m->Owner.\n-  \/\/ But we don't have enough registers, so instead we can either try to CAS\n-  \/\/ rsp or the address of the box (in scr) into &m->owner.  If the CAS succeeds\n-  \/\/ we later store \"Self\" into m->Owner.  Transiently storing a stack address\n-  \/\/ (rsp or the address of the box) into  m->owner is harmless.\n+  \/\/ Appears unlocked - try to swing _owner from null to current thread.\n@@ -662,1 +664,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -664,7 +666,0 @@\n-  \/\/ If we weren't able to swing _owner from NULL to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n@@ -707,6 +702,1 @@\n-#ifndef _LP64\n-  get_thread(tmpReg);\n-  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n-#else \/\/ _LP64\n-  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-#endif\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n@@ -776,1 +766,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -783,1 +773,15 @@\n-    jccb   (Assembler::zero, Stacked);\n+    jcc(Assembler::zero, Stacked);\n+    if (UseFastLocking) {\n+      \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n+      testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) (intptr_t) ANONYMOUS_OWNER);\n+#ifdef _LP64\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg);\n+      Compile::current()->output()->add_stub(stub);\n+      jcc(Assembler::notEqual, stub->entry());\n+      bind(stub->continuation());\n+#else\n+      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n+      \/\/ Call the slow-path instead.\n+      jcc(Assembler::notEqual, NO_COUNT);\n+#endif\n+    }\n@@ -795,1 +799,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -909,3 +913,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (UseFastLocking) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":55,"deletions":45,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1226,53 +1226,65 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n+\n+      bind(count_locking);\n+    }\n@@ -1287,1 +1299,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -1321,3 +1333,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1331,3 +1345,20 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movl(tmp, Address(thread, JavaThread::lock_stack_offset_offset()));\n+      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlocked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1335,2 +1366,2 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1338,2 +1369,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n@@ -1341,3 +1372,3 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1345,2 +1376,2 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1348,1 +1379,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":102,"deletions":70,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -9679,0 +9679,45 @@\n+\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  cmpl(Address(thread, JavaThread::lock_stack_offset_offset()), LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movl(tmp, Address(thread, JavaThread::lock_stack_offset_offset()));\n+  movptr(Address(thread, tmp, Address::times_1), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_offset_offset()), tmp);\n+}\n+\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_offset_offset()), oopSize);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -152,0 +152,2 @@\n+  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n+  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n@@ -2011,0 +2013,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1680,30 +1680,36 @@\n-      \/\/ Load immediate 1 into swap_reg %rax,\n-      __ movptr(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax,\n+        __ movptr(swap_reg, 1);\n+\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+       __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -1833,1 +1839,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1849,12 +1855,18 @@\n-      \/\/  get old displaced header\n-      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/  get old displaced header\n+        __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":55,"deletions":43,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2148,0 +2148,7 @@\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax\n+        __ movl(swap_reg, 1);\n@@ -2149,5 +2156,2 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2155,2 +2159,2 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2158,4 +2162,4 @@\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n+        \/\/ src -> dest iff dest == rax else rax <- dest\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n@@ -2163,1 +2167,1 @@\n-      \/\/ Hmm should this move to the slow path code area???\n+        \/\/ Hmm should this move to the slow path code area???\n@@ -2165,8 +2169,8 @@\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2174,2 +2178,2 @@\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n@@ -2177,3 +2181,4 @@\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -2293,1 +2298,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2309,9 +2314,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ movptr(old_hdr, Address(rax, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        __ lock();\n+        __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":46,"deletions":35,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -13805,1 +13805,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13808,1 +13808,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13812,0 +13812,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13813,1 +13814,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13821,1 +13822,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13824,1 +13825,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13828,0 +13829,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13829,1 +13831,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -13364,1 +13364,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13380,1 +13380,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -760,2 +760,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(UseFastLocking || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, UseFastLocking ? nullptr : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -737,1 +737,17 @@\n-JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter(JavaThread* current, BasicObjectLock* elem))\n+JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter(JavaThread* current, void* elem))\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    \/\/ TODO: We accept elem as void* to workaround a limitation of registers in x86_32. Interpreter\n+    \/\/ code is really sending an oopDesc* here.\n+    \/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n+    \/\/ but we are using that register to hold the thread. We don't have enough registers to\n+    \/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n+    \/\/ the object. See also InterpreterMacroAssembler::lock_object().\n+    \/\/ As soon as traditional stack-locking goes away we can change elem to be oopDesc*\n+    \/\/ (also in monitorexit below).\n+    Handle h_obj(current, oop((reinterpret_cast<oopDesc*>(elem))));\n+    assert(Universe::heap()->is_in_or_null(h_obj()),\n+           \"must be null or an object\");\n+    ObjectSynchronizer::enter(h_obj, nullptr, current);\n+    return;\n+  }\n+  BasicObjectLock* lock = reinterpret_cast<BasicObjectLock*>(elem);\n@@ -739,1 +755,1 @@\n-  current->last_frame().interpreter_frame_verify_monitor(elem);\n+  current->last_frame().interpreter_frame_verify_monitor(lock);\n@@ -741,1 +757,1 @@\n-  Handle h_obj(current, elem->obj());\n+  Handle h_obj(current, lock->obj());\n@@ -744,2 +760,2 @@\n-  ObjectSynchronizer::enter(h_obj, elem->lock(), current);\n-  assert(Universe::heap()->is_in_or_null(elem->obj()),\n+  ObjectSynchronizer::enter(h_obj, lock->lock(), current);\n+  assert(Universe::heap()->is_in_or_null(lock->obj()),\n@@ -748,1 +764,1 @@\n-  current->last_frame().interpreter_frame_verify_monitor(elem);\n+  current->last_frame().interpreter_frame_verify_monitor(lock);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-  static void    monitorenter(JavaThread* current, BasicObjectLock* elem);\n+  static void    monitorenter(JavaThread* current, void* elem);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-\/\/    [ptr             | 00]  locked             ptr points to real header on stack\n+\/\/    [ptr             | 00]  locked             ptr points to real header on stack (stack-locking in use)\n+\/\/    [header          | 00]  locked             locked regular object header (fast-locking in use)\n@@ -55,1 +56,1 @@\n-\/\/    [ptr             | 10]  monitor            inflated lock (header is wapped out)\n+\/\/    [ptr             | 10]  monitor            inflated lock (header is swapped out)\n@@ -57,1 +58,1 @@\n-\/\/    [0 ............ 0| 00]  inflating          inflation in progress\n+\/\/    [0 ............ 0| 00]  inflating          inflation in progress (stack-locking in use)\n@@ -159,0 +160,1 @@\n+  \/\/ Fast-locking does not use INFLATING.\n@@ -173,1 +175,1 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    return !UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n@@ -179,0 +181,9 @@\n+\n+  bool is_fast_locked() const {\n+    return UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+  }\n+  markWord set_fast_locked() const {\n+    \/\/ Clear the lock_mask_in_place bits to set locked_value:\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -188,1 +199,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return UseFastLocking ? lockbits == monitor_value   \/\/ monitor?\n+                    : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":18,"deletions":5,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -122,1 +122,2 @@\n-  \/\/ at a safepoint, it must not be zero.\n+  \/\/ at a safepoint, it must not be zero. fast-locking does allow the\n+  \/\/ mark to be zero at a safepoint.\n@@ -131,1 +132,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return  UseFastLocking || !SafepointSynchronize::is_at_safepoint();\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -89,0 +89,15 @@\n+#ifdef _LP64\n+class C2HandleAnonOMOwnerStub : public C2CodeStub {\n+private:\n+  Register _monitor;\n+  Register _tmp;\n+public:\n+  C2HandleAnonOMOwnerStub(Register monitor, Register tmp = noreg) : C2CodeStub(),\n+    _monitor(monitor), _tmp(tmp) {}\n+  Register monitor() { return _monitor; }\n+  Register tmp() { return _tmp; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+#endif\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1996,1 +1996,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1981,0 +1981,3 @@\n+                                                                            \\\n+  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n+                \"Use fast-locking instead of stack-locking\")                \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -490,2 +491,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack() {\n@@ -988,0 +990,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n@@ -1381,0 +1384,4 @@\n+\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1143,0 +1144,10 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_offset_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::offset_offset(); }\n+  static ByteSize lock_stack_base_offset()      { return byte_offset_of(JavaThread, _lock_stack) + LockStack::base_offset(); }\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/lockStack.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+LockStack::LockStack() :\n+  _offset(in_bytes(JavaThread::lock_stack_base_offset())) {\n+}\n+\n+int LockStack::end_offset() {\n+  return in_bytes(JavaThread::lock_stack_base_offset()) + CAPACITY * oopSize;\n+}\n+\n+#ifndef PRODUCT\n+void LockStack::validate(const char* msg) const {\n+  assert(UseFastLocking && !UseHeavyMonitors, \"never use lock-stack when fast-locking is disabled\");\n+  int end = to_index(_offset);\n+  for (int i = 0; i < end; i++) {\n+    for (int j = i + 1; j < end; j++) {\n+      assert(_base[i] != _base[j], \"entries must be unique: %s\", msg);\n+    }\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class Thread;\n+class OopClosure;\n+\n+class LockStack {\n+  friend class VMStructs;\n+private:\n+  static const int CAPACITY = 8;\n+  \/\/ The offset of the next element, in bytes, relative to the JavaThread structure.\n+  \/\/ We do this instead of a simple index into the array because this allows for\n+  \/\/ efficient addressing in generated code.\n+  int _offset;\n+  oop _base[CAPACITY];\n+\n+  void validate(const char* msg) const PRODUCT_RETURN;\n+\n+  static inline int to_index(int offset);\n+\n+public:\n+  static ByteSize offset_offset() { return byte_offset_of(LockStack, _offset); }\n+  static ByteSize base_offset()   { return byte_offset_of(LockStack, _base); }\n+\n+  LockStack();\n+\n+  static int end_offset();\n+  inline bool can_push() const;\n+  inline void push(oop o);\n+  inline oop pop();\n+  inline void remove(oop o);\n+\n+  inline bool contains(oop o) const;\n+\n+  \/\/ GC support\n+  inline void oops_do(OopClosure* cl);\n+\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+\n+inline int LockStack::to_index(int offset) {\n+  return (offset - in_bytes(JavaThread::lock_stack_base_offset())) \/ oopSize;\n+}\n+\n+inline bool LockStack::can_push() const {\n+  return to_index(_offset) < CAPACITY;\n+}\n+\n+inline void LockStack::push(oop o) {\n+  validate(\"pre-push\");\n+  assert(oopDesc::is_oop(o), \"must be\");\n+  assert(!contains(o), \"entries must be unique\");\n+  assert(can_push(), \"must have room\");\n+  _base[to_index(_offset)] = o;\n+  _offset += oopSize;\n+  validate(\"post-push\");\n+}\n+\n+inline oop LockStack::pop() {\n+  validate(\"pre-pop\");\n+  assert(to_index(_offset) > 0, \"underflow, probably unbalanced push\/pop\");\n+  _offset -= oopSize;\n+  oop o = _base[to_index(_offset)];\n+  assert(!contains(o), \"entries must be unique\");\n+  validate(\"post-pop\");\n+  return o;\n+}\n+\n+inline void LockStack::remove(oop o) {\n+  validate(\"pre-remove\");\n+  assert(contains(o), \"entry must be present\");\n+  int end = to_index(_offset);\n+  for (int i = 0; i < end; i++) {\n+    if (_base[i] == o) {\n+      int last = end - 1;\n+      for (; i < last; i++) {\n+        _base[i] = _base[i + 1];\n+      }\n+      _offset -= oopSize;\n+      break;\n+    }\n+  }\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  validate(\"post-remove\");\n+}\n+\n+inline bool LockStack::contains(oop o) const {\n+  validate(\"pre-contains\");\n+  int end = to_index(_offset);\n+  for (int i = end - 1; i >= 0; i--) {\n+    if (_base[i] == o) {\n+      validate(\"post-contains\");\n+      return true;\n+    }\n+  }\n+  validate(\"post-contains\");\n+  return false;\n+}\n+\n+inline void LockStack::oops_do(OopClosure* cl) {\n+  validate(\"pre-oops-do\");\n+  int end = to_index(_offset);\n+  for (int i = 0; i < end; i++) {\n+    cl->do_oop(&_base[i]);\n+  }\n+  validate(\"post-oops-do\");\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1138,1 +1138,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1358,1 +1358,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1407,0 +1407,1 @@\n+  assert(cur != ANONYMOUS_OWNER, \"no anon owner here\");\n@@ -1410,1 +1411,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -147,2 +147,10 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits.\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better.\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  #define ANONYMOUS_OWNER reinterpret_cast<void*>(1)\n@@ -266,0 +274,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(nullptr, ANONYMOUS_OWNER);\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == ANONYMOUS_OWNER;\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(ANONYMOUS_OWNER, owner);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -36,3 +37,11 @@\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+  if (UseFastLocking) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object()) ? 1 : 0;\n+    } else {\n+      return current == owner_raw() ? 1 : 0;\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return 1;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -314,1 +315,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(oop(obj))) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -316,1 +318,1 @@\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+    \/\/ fast-locked or stack-locked by caller so by definition the implied waitset is empty.\n@@ -388,10 +390,12 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-null so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ stack-locking in the object's header, the second check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (!UseFastLocking) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-null so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ stack-locking in the object's header, the second check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -486,6 +490,40 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (UseFastLocking) {\n+      \/\/ fast-locking does not use the 'lock' parameter.\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        while (true) {\n+          if (mark.is_neutral()) {\n+            assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+            \/\/ Try to swing into 'fast-locked' state without inflating.\n+            markWord locked_mark = mark.set_fast_locked();\n+            markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+            if (old_mark == mark) {\n+              \/\/ Successfully fast-locked, push object to lock-stack and return.\n+              lock_stack.push(obj());\n+              return;\n+            }\n+            \/\/ Otherwise retry.\n+            mark = old_mark;\n+          } else {\n+            \/\/ Fall-through to inflate-enter.\n+            break;\n+          }\n+        }\n+      }\n+      \/\/ No room on the lock_stack so fall-through to inflate-enter.\n+    } else {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -494,8 +532,0 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n@@ -503,5 +533,6 @@\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -509,1 +540,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee(!obj->mark().has_locker() && !obj->mark().is_fast_locked(), \"must not be stack-locked or fast-locked\");\n@@ -528,25 +559,15 @@\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (UseFastLocking) {\n+      \/\/ fast-locking does not use the 'lock' parameter.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -554,0 +575,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -555,0 +579,27 @@\n+    } else {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -556,8 +607,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -566,0 +609,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -575,0 +627,7 @@\n+  if (UseFastLocking && monitor->is_owner_anonymous()) {\n+    \/\/ It must be owned by us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -701,3 +760,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (UseFastLocking) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -716,3 +782,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (UseFastLocking) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -744,1 +817,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || UseFastLocking) {\n+    \/\/ fast-locking does not use the markWord::INFLATING() protocol.\n@@ -859,0 +933,5 @@\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -868,1 +947,1 @@\n-      guarantee(!mark.has_locker(), \"must not be stack locked\");\n+      guarantee(!mark.has_locker(), \"must not be stack-locked\");\n@@ -913,2 +992,9 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n-      \/\/ This is a stack lock owned by the calling thread so fetch the\n+    } else if (mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast-lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ This is a stack-lock owned by the calling thread so fetch the\n@@ -925,1 +1011,1 @@\n-      \/\/ So we have to inflate the stack lock into an ObjectMonitor\n+      \/\/ So we have to inflate the stack-lock into an ObjectMonitor\n@@ -978,1 +1064,0 @@\n-  \/\/ Uncontended case, header points to stack\n@@ -980,0 +1065,1 @@\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -982,1 +1068,6 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n+\n+  if (mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -984,0 +1075,1 @@\n+    \/\/ inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -996,2 +1088,0 @@\n-  address owner = nullptr;\n-\n@@ -1000,1 +1090,0 @@\n-  \/\/ Uncontended case, header points to stack\n@@ -1002,1 +1091,3 @@\n-    owner = (address) mark.locker();\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n@@ -1005,2 +1096,8 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n-  else if (mark.has_monitor()) {\n+  if (mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n+  }\n+\n+  if (mark.has_monitor()) {\n+    \/\/ inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -1011,6 +1108,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1019,5 +1112,0 @@\n-  \/\/ Unlocked case, header in place\n-  \/\/ Cannot have assertion since this object may have been\n-  \/\/ locked by another thread when reaching here.\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n-\n@@ -1030,1 +1118,1 @@\n-\/\/ ObjectMonitors where owner is set to a stack lock address in thread.\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n@@ -1040,1 +1128,1 @@\n-      \/\/ is set to a stack lock address in the target thread.\n+      \/\/ is set to a stack-lock address in the target thread.\n@@ -1066,1 +1154,1 @@\n-    \/\/ Owner set to a stack lock address in thread should never be seen here:\n+    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n@@ -1211,4 +1299,11 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n-    \/\/ *  INFLATING    - busy wait for conversion to complete\n-    \/\/ *  Neutral      - aggressively inflate the object.\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n+    \/\/ *  INFLATING    - Busy wait for conversion from stack-locked to\n+    \/\/                   inflated.\n+    \/\/ *  neutral      - Aggressively inflate the object.\n@@ -1221,0 +1316,5 @@\n+      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1224,9 +1324,66 @@\n-    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-    \/\/ Some other thread is converting from stack-locked to inflated.\n-    \/\/ Only that thread can complete inflation -- other threads must wait.\n-    \/\/ The INFLATING value is transient.\n-    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (!UseFastLocking) {\n+      \/\/ Fast-locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (mark.is_fast_locked()) {\n+      assert(UseFastLocking, \"can only happen with fast-locking\");\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be: checked in is_lock_owned()\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1236,1 +1393,1 @@\n-    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    \/\/ Could be stack-locked either by current or by some other thread.\n@@ -1243,4 +1400,3 @@\n-    \/\/ the odds of inflation contention.\n-\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n+    \/\/ the odds of inflation contention. If we lose the race to set INFLATING,\n+    \/\/ then we just delete the ObjectMonitor and loop around again.\n+    \/\/\n@@ -1248,0 +1404,1 @@\n+      assert(!UseFastLocking, \"cannot happen with fast-locking\");\n@@ -1409,2 +1566,2 @@\n-\/\/ is set to a stack lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack lock. All of the current consumers of\n+\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n+\/\/ that holds that stack-lock. All of the current consumers of\n@@ -1412,1 +1569,1 @@\n-\/\/ those do not have the owner set to a stack lock address.\n+\/\/ those do not have the owner set to a stack-lock address.\n@@ -1429,1 +1586,1 @@\n-      \/\/ not include when owner is set to a stack lock address in thread.\n+      \/\/ not include when owner is set to a stack-lock address in thread.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":277,"deletions":120,"binary":false,"changes":397,"status":"modified"},{"patch":"@@ -536,0 +536,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -1391,0 +1392,1 @@\n+  assert(!UseFastLocking, \"only with stack-locking\");\n@@ -1420,0 +1422,10 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(UseFastLocking, \"Only with fast-locking\");\n+  for (JavaThread* q : *t_list) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -1421,2 +1433,13 @@\n-  address owner = (address)monitor->owner();\n-  return owning_thread_from_monitor_owner(t_list, owner);\n+  assert(SafepointSynchronize::is_at_safepoint(), \"not safe outside of safepoint\");\n+  if (UseFastLocking) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == nullptr || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n","filename":"src\/hotspot\/share\/runtime\/threads.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -702,0 +702,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _offset,                                       int)                                   \\\n+  nonstatic_field(LockStack,                   _base[0],                                      oop)                                   \\\n@@ -1315,0 +1318,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1124,1 +1124,1 @@\n-  if (maxDepth == 0) {\n+  if (!UseFastLocking && maxDepth == 0) {\n","filename":"src\/hotspot\/share\/services\/management.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  private static long          lockStackOffsetOffset;\n+  private static long          lockStackBaseOffset;\n@@ -56,0 +58,1 @@\n+  private static long oopPtrSize;\n@@ -88,0 +91,1 @@\n+    Type typeLockStack = db.lookupType(\"LockStack\");\n@@ -101,0 +105,4 @@\n+    lockStackOffsetOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_offset\").getOffset();\n+    lockStackBaseOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_base[0]\").getOffset();\n+    oopPtrSize = VM.getVM().getAddressSize();\n+\n@@ -397,0 +405,17 @@\n+  public boolean isLockOwned(OopHandle obj) {\n+    long current = lockStackBaseOffset;\n+    long end = addr.getJIntAt(lockStackOffsetOffset);\n+    if (Assert.ASSERTS_ENABLED) {\n+      Assert.that(current <= end, \"current stack offset must be above base offset\");\n+    }\n+\n+    while (current < end) {\n+      Address oop = addr.getAddressAt(current);\n+      if (oop.equals(obj)) {\n+        return true;\n+      }\n+      current += oopPtrSize;\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+          mark.monitor().isOwnedAnonymous() ||\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaVFrame.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -82,0 +82,4 @@\n+  public boolean isOwnedAnonymous() {\n+    return addr.getAddressAt(ownerFieldOffset).asLongValue() == 1;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -213,0 +213,1 @@\n+        assert(!VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\"));\n@@ -230,1 +231,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\")) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"}]}