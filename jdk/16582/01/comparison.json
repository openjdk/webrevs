{"files":[{"patch":"@@ -2115,5 +2115,0 @@\n-bool os::can_execute_large_page_memory() {\n-  \/\/ Does not matter, we do not support huge pages.\n-  return false;\n-}\n-\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1812,5 +1812,0 @@\n-bool os::can_execute_large_page_memory() {\n-  \/\/ Does not matter, we do not support huge pages.\n-  return false;\n-}\n-\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4018,4 +4018,0 @@\n-bool os::can_execute_large_page_memory() {\n-  return UseTransparentHugePages;\n-}\n-\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3438,4 +3438,0 @@\n-bool os::can_execute_large_page_memory() {\n-  return true;\n-}\n-\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -199,1 +199,13 @@\n-void CodeCache::initialize_heaps() {\n+static uint non_nmethod_heap_min_size() {\n+  return CodeCacheMinimumUseSpace DEBUG_ONLY(* 3);\n+}\n+\n+void CodeCache::initialize_heaps_sizes(size_t preferred_page_size) {\n+  if (!SegmentedCodeCache) {\n+    \/\/ Use a single code heap\n+    FLAG_SET_ERGO(NonNMethodCodeHeapSize, (uintx)os::vm_page_size());\n+    FLAG_SET_ERGO(ProfiledCodeHeapSize, 0);\n+    FLAG_SET_ERGO(NonProfiledCodeHeapSize, 0);\n+    return;\n+  }\n+\n@@ -203,2 +215,1 @@\n-  const size_t ps           = page_size(false, 8);\n-  const size_t min_size     = MAX2(os::vm_allocation_granularity(), ps);\n+  const size_t min_size     = MAX2(os::vm_allocation_granularity(), preferred_page_size);\n@@ -302,3 +313,1 @@\n-  \/\/ Make sure we have enough space for VM internal code\n-  uint min_code_cache_size = CodeCacheMinimumUseSpace DEBUG_ONLY(* 3);\n-  if (non_nmethod_size < min_code_cache_size) {\n+  if (non_nmethod_size < non_nmethod_heap_min_size()) {\n@@ -307,1 +316,1 @@\n-        non_nmethod_size\/K, min_code_cache_size\/K));\n+        non_nmethod_size\/K, non_nmethod_heap_min_size()\/K));\n@@ -315,0 +324,1 @@\n+}\n@@ -316,8 +326,5 @@\n-  \/\/ Print warning if using large pages but not able to use the size given\n-  if (UseLargePages) {\n-    const size_t lg_ps = page_size(false, 1);\n-    if (ps < lg_ps) {\n-      log_warning(codecache)(\"Code cache size too small for \" PROPERFMT \" pages. \"\n-                             \"Reverting to smaller page size (\" PROPERFMT \").\",\n-                             PROPERFMTARGS(lg_ps), PROPERFMTARGS(ps));\n-    }\n+void CodeCache::create_heaps(const ReservedCodeSpace& rs) {\n+  if (!SegmentedCodeCache) {\n+    \/\/ Use a single code heap\n+    add_heap(rs, \"CodeCache\", CodeBlobType::All);\n+    return;\n@@ -326,5 +333,17 @@\n-  \/\/ Note: if large page support is enabled, min_size is at least the large\n-  \/\/ page size. This ensures that the code cache is covered by large pages.\n-  non_nmethod_size = align_up(non_nmethod_size, min_size);\n-  profiled_size    = align_down(profiled_size, min_size);\n-  non_profiled_size = align_down(non_profiled_size, min_size);\n+  assert(heap_available(CodeBlobType::NonNMethod) &&\n+         (heap_available(CodeBlobType::MethodProfiled) || heap_available(CodeBlobType::MethodNonProfiled)),\n+         \"Invalid segmented CodeCache configuration\");\n+\n+  const size_t size_alignment = rs.page_size();\n+  assert(is_aligned(rs.size(), rs.page_size()), \"The size of the reserved CodeCache memory must be page aligned\");\n+\n+  \/\/ TODO: NonNMethodCodeHeapSize is usually small: ~5MB-8M. Should we align down\n+  \/\/       if large pages are used.\n+  size_t non_nmethod_size = align_up(NonNMethodCodeHeapSize, size_alignment);\n+  assert(non_nmethod_size >= non_nmethod_heap_min_size(), \"Not enough space in non-nmethod code heap\");\n+\n+  size_t profiled_size = 0;\n+  if (heap_available(CodeBlobType::MethodProfiled)) {\n+    profiled_size = heap_available(CodeBlobType::MethodNonProfiled) ? align_down(ProfiledCodeHeapSize, size_alignment)\n+                                                                    : rs.size() - non_nmethod_size;\n+  }\n@@ -339,1 +358,0 @@\n-  ReservedCodeSpace rs = reserve_heap_memory(cache_size, ps);\n@@ -345,3 +363,0 @@\n-  \/\/ Register CodeHeaps with LSan as we sometimes embed pointers to malloc memory.\n-  LSAN_REGISTER_ROOT_REGION(rs.base(), rs.size());\n-\n@@ -356,18 +371,3 @@\n-size_t CodeCache::page_size(bool aligned, size_t min_pages) {\n-  if (os::can_execute_large_page_memory()) {\n-    if (InitialCodeCacheSize < ReservedCodeCacheSize) {\n-      \/\/ Make sure that the page size allows for an incremental commit of the reserved space\n-      min_pages = MAX2(min_pages, (size_t)8);\n-    }\n-    return aligned ? os::page_size_for_region_aligned(ReservedCodeCacheSize, min_pages) :\n-                     os::page_size_for_region_unaligned(ReservedCodeCacheSize, min_pages);\n-  } else {\n-    return os::vm_page_size();\n-  }\n-}\n-\n-ReservedCodeSpace CodeCache::reserve_heap_memory(size_t size, size_t rs_ps) {\n-  \/\/ Align and reserve space for code cache\n-  const size_t rs_align = MAX2(rs_ps, os::vm_allocation_granularity());\n-  const size_t rs_size = align_up(size, rs_align);\n-  ReservedCodeSpace rs(rs_size, rs_align, rs_ps);\n+ReservedCodeSpace CodeCache::reserve_memory(size_t size, size_t preferred_page_size) {\n+  assert(is_aligned(size, preferred_page_size), \"The size of the reserved CodeCache memory must be page aligned\");\n+  ReservedCodeSpace rs(size, preferred_page_size);\n@@ -376,1 +376,1 @@\n-                                          rs_size\/K));\n+                                          size\/K));\n@@ -445,1 +445,1 @@\n-void CodeCache::add_heap(ReservedSpace rs, const char* name, CodeBlobType code_blob_type) {\n+void CodeCache::add_heap(const ReservedSpace& rs, const char* name, CodeBlobType code_blob_type) {\n@@ -457,1 +457,7 @@\n-  size_initial = align_up(size_initial, os::vm_page_size());\n+  if (rs.special()) {\n+    \/\/ Special Reserve Space is fully committed.\n+    size_initial = rs.size();\n+  }\n+  size_initial = align_up(size_initial, rs.page_size());\n+  assert(size_initial <= rs.size(), \"Initial size must not exceed reserved size\");\n+\n@@ -1183,0 +1189,68 @@\n+static void try_enable_segmented_code_cache(size_t preferred_page_size) {\n+  if (CompilerConfig::is_interpreter_only() && SegmentedCodeCache) {\n+    log_warning(codecache)(\"SegmentedCodeCache has no meaningful effect with -Xint\");\n+    FLAG_SET_DEFAULT(SegmentedCodeCache, false);\n+  }\n+  if (CompilerConfig::is_tiered() && FLAG_IS_DEFAULT(SegmentedCodeCache)\n+      && ReservedCodeCacheSize >= 240*M\n+      && NonNMethodCodeHeapSize > preferred_page_size) {\n+    FLAG_SET_ERGO(SegmentedCodeCache, true);\n+  }\n+}\n+\n+\/\/ The minimum number of memory pages we want CodeCaceh to have.\n+\/\/ It's some magic number to reduce memory fragmentation.\n+constexpr size_t min_pages = 8;\n+\n+static size_t preferred_page_size_for_heaps(size_t page_size) {\n+  if (!SegmentedCodeCache) {\n+    \/\/ There is only one heap for the whole CodeCache.\n+    \/\/ No need to seach for a page size suitable for all heaps.\n+    return page_size;\n+  }\n+\n+  assert(CodeCache::heap_available(CodeBlobType::NonNMethod) &&\n+         (CodeCache::heap_available(CodeBlobType::MethodProfiled) ||\n+          CodeCache::heap_available(CodeBlobType::MethodNonProfiled)),\n+         \"Invalid segmented CodeCache configuration\");\n+\n+  size_t min_pages_for_non_nmethods = 2;\n+  size_t min_pages_for_profiled = (min_pages - min_pages_for_non_nmethods) \/ 2;\n+  size_t min_pages_for_non_profiled = min_pages - min_pages_for_non_nmethods - min_pages_for_profiled;\n+  if (UseLargePages && FLAG_IS_CMDLINE(NonNMethodCodeHeapSize) &&\n+      FLAG_IS_CMDLINE(ProfiledCodeHeapSize) &&\n+      FLAG_IS_CMDLINE(NonProfiledCodeHeapSize)) {\n+    \/\/ As a user uses large pages and specifies heap sizes,\n+    \/\/ their intent is likely to use the lagest page.\n+    min_pages_for_non_nmethods = 1;\n+    min_pages_for_profiled = 1;\n+    min_pages_for_non_profiled = 1;\n+  }\n+\n+  page_size = MIN2(page_size, os::page_size_for_region_unaligned(NonNMethodCodeHeapSize, min_pages_for_non_nmethods));\n+\n+  if (CodeCache::heap_available(CodeBlobType::MethodProfiled)) {\n+    if (!CodeCache::heap_available(CodeBlobType::MethodNonProfiled)) {\n+      min_pages_for_profiled += min_pages_for_non_profiled;\n+    }\n+    page_size = MIN2(page_size, os::page_size_for_region_unaligned(ProfiledCodeHeapSize, min_pages_for_profiled));\n+  }\n+  if (CodeCache::heap_available(CodeBlobType::MethodNonProfiled)) {\n+    if (!CodeCache::heap_available(CodeBlobType::MethodProfiled)) {\n+      min_pages_for_non_profiled += min_pages_for_profiled;\n+    }\n+    page_size = MIN2(page_size, os::page_size_for_region_unaligned(NonProfiledCodeHeapSize, min_pages_for_non_profiled));\n+  }\n+  return page_size;\n+}\n+\n+static size_t preferred_page_size() {\n+  if (UseLargePages && FLAG_IS_CMDLINE(ReservedCodeCacheSize)) {\n+    \/\/ As a user wants large pages and specifies the CodeCache reserved size,\n+    \/\/ their intent is likely to use the lagest page size.\n+    return os::page_size_for_region_aligned(ReservedCodeCacheSize, 1);\n+  }\n+\n+  return os::page_size_for_region_aligned(ReservedCodeCacheSize, min_pages);\n+}\n+\n@@ -1194,12 +1268,15 @@\n-  if (SegmentedCodeCache) {\n-    \/\/ Use multiple code heaps\n-    initialize_heaps();\n-  } else {\n-    \/\/ Use a single code heap\n-    FLAG_SET_ERGO(NonNMethodCodeHeapSize, (uintx)os::vm_page_size());\n-    FLAG_SET_ERGO(ProfiledCodeHeapSize, 0);\n-    FLAG_SET_ERGO(NonProfiledCodeHeapSize, 0);\n-    ReservedCodeSpace rs = reserve_heap_memory(ReservedCodeCacheSize, page_size(false, 8));\n-    \/\/ Register CodeHeaps with LSan as we sometimes embed pointers to malloc memory.\n-    LSAN_REGISTER_ROOT_REGION(rs.base(), rs.size());\n-    add_heap(rs, \"CodeCache\", CodeBlobType::All);\n+  size_t preferred_ps = preferred_page_size();\n+  try_enable_segmented_code_cache(preferred_ps);\n+  initialize_heaps_sizes(preferred_ps);\n+  preferred_ps = preferred_page_size_for_heaps(preferred_ps);\n+  const size_t size = align_up(ReservedCodeCacheSize, preferred_ps);\n+  if (size > ReservedCodeCacheSize) {\n+    log_warning(codecache)(\"Code cache size was increased to \" PROPERFMT \" bytes to be \"\n+                           \"preferred page size aligned (\" PROPERFMT \").\",\n+                           PROPERFMTARGS(size), PROPERFMTARGS(preferred_ps));\n+  }\n+  ReservedCodeSpace rs = reserve_memory(size, preferred_ps);\n+  if (rs.page_size() < preferred_ps) {\n+    log_warning(codecache)(\"Code cache size too small for \" PROPERFMT \" pages. \"\n+                           \"Reverting to smaller page size (\" PROPERFMT \").\",\n+                           PROPERFMTARGS(preferred_ps), PROPERFMTARGS(rs.page_size()));\n@@ -1208,0 +1285,4 @@\n+  \/\/ Register CodeHeaps with LSan as we sometimes embed pointers to malloc memory.\n+  LSAN_REGISTER_ROOT_REGION(rs.base(), rs.size());\n+  create_heaps(rs);\n+\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":138,"deletions":57,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-  static void initialize_heaps();                             \/\/ Initializes the CodeHeaps\n+  static void initialize_heaps_sizes(size_t preferred_page_size);\n@@ -117,0 +117,2 @@\n+  static void create_heaps(const ReservedCodeSpace& rs);\n+  static void add_heap(CodeHeap* heap);\n@@ -118,1 +120,1 @@\n-  static void add_heap(ReservedSpace rs, const char* name, CodeBlobType code_blob_type);\n+  static void add_heap(const ReservedSpace& rs, const char* name, CodeBlobType code_blob_type);\n@@ -124,1 +126,1 @@\n-  static ReservedCodeSpace reserve_heap_memory(size_t size, size_t rs_ps); \/\/ Reserves one continuous chunk of memory for the CodeHeaps\n+  static ReservedCodeSpace reserve_memory(size_t size, size_t preferred_page_size); \/\/ Reserves one continuous chunk of memory for the CodeHeaps\n@@ -142,1 +144,0 @@\n-  static size_t page_size(bool aligned = true, size_t min_pages = 1); \/\/ Returns the page size used by the CodeCache\n@@ -146,1 +147,0 @@\n-  static void add_heap(CodeHeap* heap);\n","filename":"src\/hotspot\/share\/code\/codeCache.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -319,6 +319,0 @@\n-    \/\/ Enable SegmentedCodeCache if tiered compilation is enabled, ReservedCodeCacheSize >= 240M\n-    \/\/ and the code cache contains at least 8 pages (segmentation disables advantage of huge pages).\n-    if (FLAG_IS_DEFAULT(SegmentedCodeCache) && ReservedCodeCacheSize >= 240*M &&\n-        8 * CodeCache::page_size() <= ReservedCodeCacheSize) {\n-      FLAG_SET_ERGO(SegmentedCodeCache, true);\n-    }\n@@ -540,4 +534,0 @@\n-    if (SegmentedCodeCache) {\n-      warning(\"SegmentedCodeCache has no meaningful effect with -Xint\");\n-      FLAG_SET_DEFAULT(SegmentedCodeCache, false);\n-    }\n","filename":"src\/hotspot\/share\/compiler\/compilerDefinitions.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-ReservedSpace ReservedSpace::first_part(size_t partition_size, size_t alignment) {\n+ReservedSpace ReservedSpace::first_part(size_t partition_size, size_t alignment) const {\n@@ -319,1 +319,1 @@\n-ReservedSpace::last_part(size_t partition_size, size_t alignment) {\n+ReservedSpace::last_part(size_t partition_size, size_t alignment) const {\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -96,2 +96,2 @@\n-  ReservedSpace first_part(size_t partition_size, size_t alignment);\n-  ReservedSpace last_part (size_t partition_size, size_t alignment);\n+  ReservedSpace first_part(size_t partition_size, size_t alignment) const;\n+  ReservedSpace last_part (size_t partition_size, size_t alignment) const;\n@@ -100,2 +100,2 @@\n-  inline ReservedSpace first_part(size_t partition_size);\n-  inline ReservedSpace last_part (size_t partition_size);\n+  inline ReservedSpace first_part(size_t partition_size) const;\n+  inline ReservedSpace last_part (size_t partition_size) const;\n@@ -117,1 +117,1 @@\n-ReservedSpace::first_part(size_t partition_size)\n+ReservedSpace::first_part(size_t partition_size) const\n@@ -122,1 +122,1 @@\n-ReservedSpace ReservedSpace::last_part(size_t partition_size)\n+ReservedSpace ReservedSpace::last_part(size_t partition_size) const\n@@ -154,0 +154,1 @@\n+  ReservedCodeSpace(size_t r_size, size_t page_size) : ReservedCodeSpace(r_size, MAX2(page_size, os::vm_allocation_granularity()), page_size) {}\n","filename":"src\/hotspot\/share\/memory\/virtualspace.hpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -534,1 +534,0 @@\n-  static bool   can_execute_large_page_memory();\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}