{"files":[{"patch":"@@ -1768,4 +1768,0 @@\n-  \/\/ insert a nop at the start of the prolog so we can patch in a\n-  \/\/ branch if we need to invalidate the method later\n-  __ nop();\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1689,6 +1689,1 @@\n-  if (method_is_frameless) {\n-    \/\/ Add nop at beginning of all frameless methods to prevent any\n-    \/\/ oop instructions from getting overwritten by make_not_entrant\n-    \/\/ (patching attempt would fail).\n-    __ nop();\n-  } else {\n+  if (!method_is_frameless) {\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1371,8 +1371,0 @@\n-  \/\/ insert a nop at the start of the prolog so we can patch in a\n-  \/\/ branch if we need to invalidate the method later\n-  {\n-    Assembler::IncompressibleScope scope(masm); \/\/ keep the nop as 4 bytes for patching.\n-    MacroAssembler::assert_alignment(__ pc());\n-    __ nop();  \/\/ 4 bytes\n-  }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -328,10 +328,0 @@\n-  if (breakAtEntry) {\n-    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n-    \/\/ patching by patch_verified_entry().\n-    \/\/\n-    \/\/ Breakpoint has one byte first instruction.\n-    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n-    \/\/ code is not generated (see build_frame() above).\n-    \/\/ For all these cases generate long instruction first.\n-    fat_nop();\n-  }\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -53,7 +53,0 @@\n-\n-  \/\/ WARNING: Initial instruction MUST be 5 bytes or longer so that\n-  \/\/ NativeJump::patch_verified_entry will be able to patch out the entry\n-  \/\/ code safely. The push to verify stack depth is ok at 5 bytes,\n-  \/\/ the frame allocation can be either 3 or 6 bytes. So if we don't do\n-  \/\/ stack bang then we must use the 6 byte frame allocation even if\n-  \/\/ we have no frame. :-(\n@@ -90,2 +83,1 @@\n-    \/\/ Create frame (force generation of a 4 byte immediate value)\n-    subptr_imm32(rsp, framesize);\n+    subptr(rsp, framesize);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1624,13 +1624,0 @@\n-\/\/ A 5 byte nop that is safe for patching (see patch_verified_entry)\n-void MacroAssembler::fat_nop() {\n-  if (UseAddressNop) {\n-    addr_nop_5();\n-  } else {\n-    emit_int8((uint8_t)0x26); \/\/ es:\n-    emit_int8((uint8_t)0x2e); \/\/ cs:\n-    emit_int8((uint8_t)0x64); \/\/ fs:\n-    emit_int8((uint8_t)0x65); \/\/ gs:\n-    emit_int8((uint8_t)0x90);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -212,2 +212,0 @@\n-  \/\/ A 5 byte nop that is safe for patching (see patch_verified_entry)\n-  void fat_nop();\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -339,49 +339,0 @@\n-void NativeJump::check_verified_entry_alignment(address entry, address verified_entry) {\n-  \/\/ Patching to not_entrant can happen while activations of the method are\n-  \/\/ in use. The patching in that instance must happen only when certain\n-  \/\/ alignment restrictions are true. These guarantees check those\n-  \/\/ conditions.\n-  const int linesize = 64;\n-\n-  \/\/ Must be wordSize aligned\n-  guarantee(((uintptr_t) verified_entry & (wordSize -1)) == 0,\n-            \"illegal address for code patching 2\");\n-  \/\/ First 5 bytes must be within the same cache line - 4827828\n-  guarantee((uintptr_t) verified_entry \/ linesize ==\n-            ((uintptr_t) verified_entry + 4) \/ linesize,\n-            \"illegal address for code patching 3\");\n-}\n-\n-\n-\/\/ MT safe inserting of a jump over an unknown instruction sequence (used by nmethod::make_not_entrant)\n-\/\/ The problem: jmp <dest> is a 5-byte instruction. Atomic write can be only with 4 bytes.\n-\/\/ First patches the first word atomically to be a jump to itself.\n-\/\/ Then patches the last byte  and then atomically patches the first word (4-bytes),\n-\/\/ thus inserting the desired jump\n-\/\/ This code is mt-safe with the following conditions: entry point is 4 byte aligned,\n-\/\/ entry point is in same cache line as unverified entry point, and the instruction being\n-\/\/ patched is >= 5 byte (size of patch).\n-\/\/\n-\/\/ In C2 the 5+ byte sized instruction is enforced by code in MachPrologNode::emit.\n-\/\/ In C1 the restriction is enforced by CodeEmitter::method_entry\n-\/\/ In JVMCI, the restriction is enforced by HotSpotFrameContext.enter(...)\n-\/\/\n-void NativeJump::patch_verified_entry(address entry, address verified_entry, address dest) {\n-  \/\/ complete jump instruction (to be inserted) is in code_buffer;\n-  union {\n-    jlong cb_long;\n-    unsigned char code_buffer[8];\n-  } u;\n-\n-  u.cb_long = *(jlong *)verified_entry;\n-\n-  intptr_t disp = (intptr_t)dest - ((intptr_t)verified_entry + 1 + 4);\n-  guarantee(disp == (intptr_t)(int32_t)disp, \"must be 32-bit offset\");\n-\n-  u.code_buffer[0] = instruction_code;\n-  *(int32_t*)(u.code_buffer + 1) = (int32_t)disp;\n-\n-  Atomic::store((jlong *) verified_entry, u.cb_long);\n-  ICache::invalidate_range(verified_entry, 8);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.cpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -448,3 +448,0 @@\n-  \/\/ MT-safe insertion of native jump at verified method entry\n-  static void check_verified_entry_alignment(address entry, address verified_entry);\n-  static void patch_verified_entry(address entry, address verified_entry, address dest);\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"code\/relocInfo.hpp\"\n@@ -695,7 +694,0 @@\n-  if (!is_in_use()) {\n-    low_boundary += NativeJump::instruction_size;\n-    \/\/ %%% Note:  On SPARC we patch only a 4-byte trap, not a full NativeJump.\n-    \/\/ This means that the low_boundary is going to be a little too high.\n-    \/\/ This shouldn't matter, since oops of non-entrant methods are never used.\n-    \/\/ In fact, why are we bothering to look at oops in a non-entrant method??\n-  }\n@@ -1656,4 +1648,0 @@\n-  \/\/ Enter a critical section to prevent a race with deopts that patch code and updates the relocation info.\n-  \/\/ Unfortunately, we have to lock the NMethodState_lock before the tty lock due to the deadlock rules and\n-  \/\/ cannot lock in a more finely grained manner.\n-  ConditionalMutexLocker ml(NMethodState_lock, !NMethodState_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n@@ -2043,13 +2031,1 @@\n-      NativeJump::patch_verified_entry(entry_point(), verified_entry_point(),\n-                                       SharedRuntime::get_handle_wrong_method_stub());\n-\n-      \/\/ Update the relocation info for the patched entry.\n-      \/\/ First, get the old relocation info...\n-      RelocIterator iter(this, verified_entry_point(), verified_entry_point() + 8);\n-      if (iter.next() && iter.addr() == verified_entry_point()) {\n-        Relocation* old_reloc = iter.reloc();\n-        \/\/ ...then reset the iterator to update it.\n-        RelocIterator iter(this, verified_entry_point(), verified_entry_point() + 8);\n-        relocInfo::change_reloc_info_for_address(&iter, verified_entry_point(), old_reloc->type(),\n-                                                 relocInfo::relocType::runtime_call_type);\n-      }\n+      BarrierSet::barrier_set()->barrier_set_nmethod()->make_not_entrant(this);\n@@ -2945,3 +2921,0 @@\n-  \/\/ Make sure all the entry points are correctly aligned for patching.\n-  NativeJump::check_verified_entry_alignment(entry_point(), verified_entry_point());\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":1,"deletions":28,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -75,1 +75,12 @@\n-  set_guard_value(nm, disarmed_guard_value());\n+  arm_with(nm, disarmed_guard_value());\n+}\n+\n+void BarrierSetNMethod::arm_with(nmethod* nm, int value) {\n+  assert((value & not_entrant) == 0, \"not_entrant bit is reserved\");\n+  \/\/ Enter critical section.  Does not block for safepoint.\n+  ConditionalMutexLocker ml(NMethodEntryBarrier_lock, !NMethodEntryBarrier_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  \/\/ Do not undo sticky bit\n+  if (is_not_entrant(nm)) {\n+    value |= not_entrant;\n+  }\n+  set_guard_value(nm, value);\n@@ -79,1 +90,1 @@\n-  return guard_value(nm) != disarmed_guard_value();\n+  return (guard_value(nm) & ~not_entrant) != disarmed_guard_value();\n@@ -155,1 +166,1 @@\n-    _current_phase = 1;\n+    _current_phase = initial;\n@@ -181,1 +192,1 @@\n-  bool may_enter = bs_nm->nmethod_entry_barrier(nm);\n+  bool may_enter = !bs_nm->is_not_entrant(nm) && bs_nm->nmethod_entry_barrier(nm);\n@@ -184,14 +195,16 @@\n-  \/\/ In case a concurrent thread disarmed the nmethod, we need to ensure the new instructions\n-  \/\/ are made visible, by using a cross modify fence. Note that this is synchronous cross modifying\n-  \/\/ code, where the existence of new instructions is communicated via data (the guard value).\n-  \/\/ This cross modify fence is only needed when the nmethod entry barrier modifies the\n-  \/\/ instructions. Not all platforms currently do that, so if this check becomes expensive,\n-  \/\/ it can be made conditional on the nmethod_patching_type.\n-  OrderAccess::cross_modify_fence();\n-\n-  \/\/ Diagnostic option to force deoptimization 1 in 10 times. It is otherwise\n-  \/\/ a very rare event.\n-  if (DeoptimizeNMethodBarriersALot && !nm->is_osr_method()) {\n-    static volatile uint32_t counter=0;\n-    if (Atomic::add(&counter, 1u) % 10 == 0) {\n-      may_enter = false;\n+  if (may_enter) {\n+    \/\/ In case a concurrent thread disarmed the nmethod, we need to ensure the new instructions\n+    \/\/ are made visible, by using a cross modify fence. Note that this is synchronous cross modifying\n+    \/\/ code, where the existence of new instructions is communicated via data (the guard value).\n+    \/\/ This cross modify fence is only needed when the nmethod entry barrier modifies the\n+    \/\/ instructions. Not all platforms currently do that, so if this check becomes expensive,\n+    \/\/ it can be made conditional on the nmethod_patching_type.\n+    OrderAccess::cross_modify_fence();\n+\n+    \/\/ Diagnostic option to force deoptimization 1 in 10 times. It is otherwise\n+    \/\/ a very rare event.\n+    if (DeoptimizeNMethodBarriersALot && !nm->is_osr_method()) {\n+      static volatile uint32_t counter=0;\n+      if (Atomic::add(&counter, 1u) % 10 == 0) {\n+        may_enter = false;\n+      }\n@@ -223,0 +236,16 @@\n+\n+\/\/ Make the nmethod permanently not-entrant, so that nmethod_stub_entry_barrier() will call\n+\/\/ deoptimize() to redirect the caller to SharedRuntime::get_handle_wrong_method_stub().\n+\/\/ A sticky armed bit is set and other bits are preserved.  As a result, a call to\n+\/\/ nmethod_stub_entry_barrier() may appear to be spurious, because is_armed() still returns\n+\/\/ false and nmethod_entry_barrier() is not called.\n+void BarrierSetNMethod::make_not_entrant(nmethod* nm) {\n+  \/\/ Enter critical section.  Does not block for safepoint.\n+  ConditionalMutexLocker ml(NMethodEntryBarrier_lock, !NMethodEntryBarrier_lock->owned_by_self(), Mutex::_no_safepoint_check_flag);\n+  int value = guard_value(nm) | not_entrant;\n+  set_guard_value(nm, value);\n+}\n+\n+bool BarrierSetNMethod::is_not_entrant(nmethod* nm) {\n+  return (guard_value(nm) & not_entrant) != 0;\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":47,"deletions":18,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -39,0 +39,7 @@\n+  enum {\n+    \/\/ negative values are permanent\n+    not_entrant = 1 << 31, \/\/ armed sticky bit, see make_not_entrant\n+    armed = 0,\n+    initial = 1,\n+  };\n+\n@@ -40,0 +47,6 @@\n+  bool is_armed_permanently(nmethod* nm) { return guard_value(nm) < 0; }\n+  void arm(nmethod* nm) { arm_with(nm, armed); }\n+\n+protected:\n+  virtual int guard_value(nmethod* nm);\n+  void set_guard_value(nmethod* nm, int value);\n@@ -42,1 +55,1 @@\n-  BarrierSetNMethod() : _current_phase(1) {}\n+  BarrierSetNMethod() : _current_phase(initial) {}\n@@ -53,1 +66,1 @@\n-  bool is_armed(nmethod* nm);\n+  virtual bool is_armed(nmethod* nm);\n@@ -55,0 +68,2 @@\n+  virtual void make_not_entrant(nmethod* nm);\n+  virtual bool is_not_entrant(nmethod* nm);\n@@ -56,2 +71,1 @@\n-  int guard_value(nmethod* nm);\n-  void set_guard_value(nmethod* nm, int value);\n+  virtual void arm_with(nmethod* nm, int value);\n@@ -59,1 +73,1 @@\n-  void arm_all_nmethods();\n+  virtual void arm_all_nmethods();\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.hpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -153,1 +153,1 @@\n-        _bs->set_guard_value(nm, 0);\n+        _bs->arm(nm);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCodeRoots.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -108,0 +108,30 @@\n+\n+void ZBarrierSetNMethod::arm_with(nmethod* nm, int value) {\n+  assert((value & not_entrant) == 0, \"not_entrant bit is reserved\");\n+  ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+  \/\/ Preserve the sticky bit\n+  if (is_not_entrant(nm)) {\n+    value |= not_entrant;\n+  }\n+  set_guard_value(nm, value);\n+}\n+\n+bool ZBarrierSetNMethod::is_armed(nmethod* nm) {\n+  int value = guard_value(nm) & ~not_entrant;\n+  return value != disarmed_guard_value();\n+}\n+\n+void ZBarrierSetNMethod::make_not_entrant(nmethod* nm) {\n+  ZLocker<ZReentrantLock> locker(ZNMethod::lock_for_nmethod(nm));\n+  int value = guard_value(nm) | not_entrant; \/\/ permanent sticky value\n+  set_guard_value(nm, value);\n+}\n+\n+bool ZBarrierSetNMethod::is_not_entrant(nmethod* nm) {\n+  return (guard_value(nm) & not_entrant) != 0;\n+}\n+\n+uintptr_t ZBarrierSetNMethod::color(nmethod* nm) {\n+  \/\/ color is stored at low order bits of int; conversion to uintptr_t is fine\n+  return uintptr_t(guard_value(nm) & ~not_entrant);\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetNMethod.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -33,0 +33,4 @@\n+  enum : int {\n+    not_entrant = 1 << 31, \/\/ armed sticky bit, see make_not_entrant\n+  };\n+\n@@ -37,0 +41,2 @@\n+  uintptr_t color(nmethod* nm);\n+\n@@ -42,0 +48,6 @@\n+\n+  virtual void make_not_entrant(nmethod* nm);\n+  virtual bool is_not_entrant(nmethod* nm);\n+  virtual void arm_with(nmethod* nm, int value);\n+  virtual bool is_armed(nmethod* nm);\n+  virtual void arm_all_nmethods() { ShouldNotCallThis(); }\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSetNMethod.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -772,1 +772,1 @@\n-      _bs_nm->set_guard_value(nm, (int)untype(new_disarm_value_ptr));\n+      _bs_nm->arm_with(nm, (int)untype(new_disarm_value_ptr));\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -244,1 +244,1 @@\n-  bs->set_guard_value(nm, value);\n+  bs->arm_with(nm, value);\n@@ -303,3 +303,2 @@\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  \/\/ color is stored at low order bits of int; conversion to uintptr_t is fine\n-  return (uintptr_t)bs_nm->guard_value(nm);\n+  ZBarrierSetNMethod* bs_nm = static_cast<ZBarrierSetNMethod*>(BarrierSet::barrier_set()->barrier_set_nmethod());\n+  return bs_nm->color(nm);\n","filename":"src\/hotspot\/share\/gc\/z\/zNMethod.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+Mutex*   NMethodEntryBarrier_lock     = nullptr;\n@@ -208,0 +209,2 @@\n+  MUTEX_DEFN(NMethodEntryBarrier_lock        , PaddedMutex  , service-1);\n+\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+extern Mutex*   NMethodEntryBarrier_lock;        \/\/ protects nmethod entry barrier\n","filename":"src\/hotspot\/share\/runtime\/mutexLocker.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}