{"files":[{"patch":"@@ -78,0 +78,1 @@\n+    _shenandoah_humongous_allocation_failure,\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -623,0 +623,15 @@\n+bool ShenandoahOldHeuristics::resume_old_cycle() {\n+  \/\/ If we are preparing to mark old, or if we are already marking old, then try to continue that work.\n+  if (_old_generation->is_concurrent_mark_in_progress()) {\n+    log_trigger(\"Resume marking old\");\n+    return true;\n+  }\n+\n+  if (_old_generation->is_preparing_for_mark()) {\n+    log_trigger(\"Resume preparing to mark old\");\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n@@ -624,6 +639,8 @@\n-  \/\/ Cannot start a new old-gen GC until previous one has finished.\n-  \/\/\n-  \/\/ Future refinement: under certain circumstances, we might be more sophisticated about this choice.\n-  \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations.\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (!_old_generation->can_start_gc() || heap->collection_set()->has_old_regions()) {\n+\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (_old_generation->is_doing_mixed_evacuations()) {\n+    \/\/ Do not try to start an old cycle if we are waiting for old regions to be evacuated (we need\n+    \/\/ a young cycle for this). Note that the young heuristic has a feature to expedite old evacuations.\n+    \/\/ Future refinement: under certain circumstances, we might be more sophisticated about this choice.\n+    \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations.\n+    log_debug(gc)(\"Not starting an old cycle because we are waiting for mixed evacuations\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -188,0 +188,1 @@\n+  bool resume_old_cycle();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-bool ShenandoahCollectorPolicy::is_at_shutdown() {\n+bool ShenandoahCollectorPolicy::is_at_shutdown() const {\n@@ -130,1 +130,1 @@\n-bool is_explicit_gc(GCCause::Cause cause) {\n+bool ShenandoahCollectorPolicy::is_explicit_gc(GCCause::Cause cause) {\n@@ -132,1 +132,3 @@\n-      || GCCause::is_serviceability_requested_gc(cause);\n+      || GCCause::is_serviceability_requested_gc(cause)\n+      || cause == GCCause::_wb_full_gc\n+      || cause == GCCause::_wb_young_gc;\n@@ -139,1 +141,1 @@\n-      && !is_explicit_gc(cause);\n+      && !ShenandoahCollectorPolicy::is_explicit_gc(cause);\n@@ -144,1 +146,2 @@\n-  return is_explicit_gc(cause)\n+  return ShenandoahCollectorPolicy::is_explicit_gc(cause)\n+      || ShenandoahCollectorPolicy::is_shenandoah_gc(cause)\n@@ -156,0 +159,9 @@\n+bool ShenandoahCollectorPolicy::is_shenandoah_gc(GCCause::Cause cause) {\n+  return cause == GCCause::_allocation_failure\n+      || cause == GCCause::_shenandoah_stop_vm\n+      || cause == GCCause::_shenandoah_allocation_failure_evac\n+      || cause == GCCause::_shenandoah_humongous_allocation_failure\n+      || cause == GCCause::_shenandoah_concurrent_gc\n+      || cause == GCCause::_shenandoah_upgrade_to_full_gc;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":17,"deletions":5,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-  bool is_at_shutdown();\n+  bool is_at_shutdown() const;\n@@ -97,0 +97,7 @@\n+  static bool is_allocation_failure(GCCause::Cause cause) {\n+    return cause == GCCause::_allocation_failure\n+        || cause == GCCause::_shenandoah_allocation_failure_evac\n+        || cause == GCCause::_shenandoah_humongous_allocation_failure;\n+  }\n+\n+  static bool is_shenandoah_gc(GCCause::Cause cause);\n@@ -98,0 +105,1 @@\n+  static bool is_explicit_gc(GCCause::Cause cause);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -53,1 +53,0 @@\n-\n@@ -62,1 +61,6 @@\n-  while (!in_graceful_shutdown() && !should_terminate()) {\n+  while (!should_terminate() && !policy->is_at_shutdown()) {\n+    const GCCause::Cause cancelled_cause = heap->cancelled_cause();\n+    if (cancelled_cause == GCCause::_shenandoah_stop_vm) {\n+      break;\n+    }\n+\n@@ -64,1 +68,1 @@\n-    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n+    const bool alloc_failure_pending = ShenandoahCollectorPolicy::is_allocation_failure(cancelled_cause);\n@@ -255,5 +259,0 @@\n-\n-  \/\/ Wait for the actual stop(), can't leave run_service() earlier.\n-  while (!should_terminate()) {\n-    os::naked_short_sleep(ShenandoahControlIntervalMin);\n-  }\n@@ -323,2 +322,5 @@\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n+    if (heap->cancelled_cause() == GCCause::_shenandoah_stop_vm) {\n+      return true;\n+    }\n+\n+    if (ShenandoahCollectorPolicy::is_allocation_failure(heap->cancelled_cause())) {\n@@ -328,0 +330,1 @@\n+      return true;\n@@ -329,1 +332,2 @@\n-    return true;\n+\n+    fatal(\"Unexpected reason for cancellation: %s\", GCCause::to_string(heap->cancelled_cause()));\n@@ -335,1 +339,1 @@\n-  \/\/ Nothing to do here.\n+  ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_stop_vm);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":16,"deletions":12,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+\n+#include \"shenandoahCollectorPolicy.hpp\"\n@@ -40,8 +42,0 @@\n-void ShenandoahController::prepare_for_graceful_shutdown() {\n-  _graceful_shutdown.set();\n-}\n-\n-bool ShenandoahController::in_graceful_shutdown() {\n-  return _graceful_shutdown.is_set();\n-}\n-\n@@ -56,2 +50,2 @@\n-void ShenandoahController::handle_alloc_failure(ShenandoahAllocRequest& req, bool block) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+void ShenandoahController::handle_alloc_failure(const ShenandoahAllocRequest& req, bool block) {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -60,7 +54,2 @@\n-  bool is_humongous = ShenandoahHeapRegion::requires_humongous(req.size());\n-\n-  if (try_set_alloc_failure_gc(is_humongous)) {\n-    \/\/ Only report the first allocation failure\n-    log_info(gc)(\"Failed to allocate %s, %zu%s\",\n-                 req.type_string(),\n-                 byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));\n+  const bool is_humongous = ShenandoahHeapRegion::requires_humongous(req.size());\n+  const GCCause::Cause cause = is_humongous ? GCCause::_shenandoah_humongous_allocation_failure : GCCause::_allocation_failure;\n@@ -68,2 +57,3 @@\n-    \/\/ Now that alloc failure GC is scheduled, we can abort everything else\n-    heap->cancel_gc(GCCause::_allocation_failure);\n+  if (heap->cancel_gc(cause)) {\n+    log_info(gc)(\"Failed to allocate %s, \" PROPERFMT, req.type_string(), PROPERFMTARGS(req.size() * HeapWordSize));\n+    request_gc(cause);\n@@ -72,1 +62,0 @@\n-\n@@ -75,1 +64,1 @@\n-    while (is_alloc_failure_gc()) {\n+    while (ShenandoahCollectorPolicy::is_allocation_failure(heap->cancelled_cause())) {\n@@ -82,2 +71,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  bool is_humongous = ShenandoahHeapRegion::requires_humongous(words);\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  const bool is_humongous = ShenandoahHeapRegion::requires_humongous(words);\n+  const GCCause::Cause cause = is_humongous ? GCCause::_shenandoah_humongous_allocation_failure : GCCause::_shenandoah_allocation_failure_evac;\n@@ -85,4 +75,2 @@\n-  if (try_set_alloc_failure_gc(is_humongous)) {\n-    \/\/ Only report the first allocation failure\n-    log_info(gc)(\"Failed to allocate %zu%s for evacuation\",\n-                 byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));\n+  if (heap->cancel_gc(cause)) {\n+    log_info(gc)(\"Failed to allocate \" PROPERFMT \" for evacuation\", PROPERFMTARGS(words * HeapWordSize));\n@@ -90,3 +78,0 @@\n-\n-  \/\/ Forcefully report allocation failure\n-  heap->cancel_gc(GCCause::_shenandoah_allocation_failure_evac);\n@@ -96,2 +81,0 @@\n-  _alloc_failure_gc.unset();\n-  _humongous_alloc_failure_gc.unset();\n@@ -101,11 +84,0 @@\n-\n-bool ShenandoahController::try_set_alloc_failure_gc(bool is_humongous) {\n-  if (is_humongous) {\n-    _humongous_alloc_failure_gc.try_set();\n-  }\n-  return _alloc_failure_gc.try_set();\n-}\n-\n-bool ShenandoahController::is_alloc_failure_gc() {\n-  return _alloc_failure_gc.is_set();\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.cpp","additions":15,"deletions":43,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -39,2 +39,0 @@\n-  ShenandoahSharedFlag _graceful_shutdown;\n-\n@@ -48,2 +46,0 @@\n-  ShenandoahSharedFlag _alloc_failure_gc;\n-  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n@@ -59,1 +55,0 @@\n-    ConcurrentGCThread(),\n@@ -72,1 +67,1 @@\n-  void handle_alloc_failure(ShenandoahAllocRequest& req, bool block);\n+  void handle_alloc_failure(const ShenandoahAllocRequest& req, bool block);\n@@ -84,3 +79,0 @@\n-  \/\/ True if allocation failure flag has been set.\n-  bool is_alloc_failure_gc();\n-\n@@ -93,7 +85,0 @@\n-  \/\/ These essentially allows to cancel a collection cycle for the\n-  \/\/ purpose of shutting down the JVM, without trying to start a degenerated\n-  \/\/ cycle.\n-  void prepare_for_graceful_shutdown();\n-  bool in_graceful_shutdown();\n-\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.hpp","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -50,3 +49,1 @@\n-  ShenandoahController(),\n-  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n-  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahGCRequest_lock\", true),\n@@ -54,3 +51,4 @@\n-  _requested_generation(GLOBAL),\n-  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n-  _degen_generation(nullptr),\n+  _requested_generation(nullptr),\n+  _degen_point(ShenandoahGC::_degenerated_unset),\n+  _heap(ShenandoahGenerationalHeap::heap()),\n+  _age_period(0),\n@@ -64,1 +62,0 @@\n-  ShenandoahGenerationalHeap* const heap = ShenandoahGenerationalHeap::heap();\n@@ -66,4 +63,3 @@\n-  const GCMode default_mode = concurrent_normal;\n-  ShenandoahGenerationType generation = GLOBAL;\n-\n-  uint age_period = 0;\n+  const int64_t wait_ms = ShenandoahPacing ? ShenandoahControlIntervalMin : 0;\n+  ShenandoahGCRequest request;\n+  while (!should_terminate()) {\n@@ -71,1 +67,2 @@\n-  ShenandoahCollectorPolicy* const policy = heap->shenandoah_policy();\n+    \/\/ This control loop iteration has seen this much allocation.\n+    const size_t allocs_seen = reset_allocs_seen();\n@@ -73,6 +70,0 @@\n-  \/\/ Heuristics are notified of allocation failures here and other outcomes\n-  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n-  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n-  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n-  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n-  while (!in_graceful_shutdown() && !should_terminate()) {\n@@ -80,2 +71,1 @@\n-    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n-    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+    check_for_request(request);\n@@ -83,1 +73,3 @@\n-    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n+    if (request.cause == GCCause::_shenandoah_stop_vm) {\n+      break;\n+    }\n@@ -85,1 +77,8 @@\n-    const bool is_gc_requested = ShenandoahCollectorPolicy::is_requested_gc(cause);\n+    if (request.cause != GCCause::_no_gc) {\n+      run_gc_cycle(request);\n+    } else {\n+      \/\/ Report to pacer that we have seen this many words allocated\n+      if (ShenandoahPacing && (allocs_seen > 0)) {\n+        _heap->pacer()->report_alloc(allocs_seen);\n+      }\n+    }\n@@ -87,2 +86,10 @@\n-    \/\/ This control loop iteration has seen this much allocation.\n-    const size_t allocs_seen = reset_allocs_seen();\n+    \/\/ If the cycle was cancelled, continue the next iteration to deal with it. Otherwise,\n+    \/\/ if there was no other cycle requested, cleanup and wait for the next request.\n+    if (!_heap->cancelled_gc()) {\n+      MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      if (_requested_gc_cause == GCCause::_no_gc) {\n+        set_gc_mode(ml, none);\n+        ml.wait(wait_ms);\n+      }\n+    }\n+  }\n@@ -90,2 +97,2 @@\n-    \/\/ Check if we have seen a new target for soft max heap size.\n-    const bool soft_max_changed = heap->check_soft_max_changed();\n+  set_gc_mode(stopped);\n+}\n@@ -93,3 +100,9 @@\n-    \/\/ Choose which GC mode to run in. The block below should select a single mode.\n-    set_gc_mode(none);\n-    ShenandoahGC::ShenandoahDegenPoint degen_point = ShenandoahGC::_degenerated_unset;\n+void ShenandoahGenerationalControlThread::stop_service() {\n+  log_debug(gc, thread)(\"Stopping control thread\");\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _heap->cancel_gc(GCCause::_shenandoah_stop_vm);\n+  _requested_gc_cause = GCCause::_shenandoah_stop_vm;\n+  notify_cancellation(ml, GCCause::_shenandoah_stop_vm);\n+  \/\/ We can't wait here because it may interfere with the active cycle's ability\n+  \/\/ to reach a safepoint (this runs on a java thread).\n+}\n@@ -97,3 +110,16 @@\n-    if (alloc_failure_pending) {\n-      \/\/ Allocation failure takes precedence: we have to deal with it first thing\n-      cause = GCCause::_allocation_failure;\n+void ShenandoahGenerationalControlThread::check_for_request(ShenandoahGCRequest& request) {\n+  \/\/ Hold the lock while we read request cause and generation\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  if (_heap->cancelled_gc()) {\n+    \/\/ The previous request was cancelled. Either it was cancelled for an allocation\n+    \/\/ failure (degenerated cycle), or old marking was cancelled to run a young collection.\n+    \/\/ In either case, the correct generation for the next cycle can be determined by\n+    \/\/ the cancellation cause.\n+    request.cause = _heap->cancelled_cause();\n+    if (request.cause == GCCause::_shenandoah_concurrent_gc) {\n+      request.generation = _heap->young_generation();\n+      _heap->clear_cancelled_gc(false);\n+    }\n+  } else {\n+    request.cause = _requested_gc_cause;\n+    request.generation = _requested_generation;\n@@ -101,3 +127,5 @@\n-      \/\/ Consume the degen point, and seed it with default value\n-      degen_point = _degen_point;\n-      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+    \/\/ Only clear these if we made a request from them. In the case of a cancelled gc,\n+    \/\/ we do not want to inadvertently lose this pending request.\n+    _requested_gc_cause = GCCause::_no_gc;\n+    _requested_generation = nullptr;\n+  }\n@@ -105,5 +133,3 @@\n-      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n-        _degen_generation = heap->young_generation();\n-      } else {\n-        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n-      }\n+  if (request.cause == GCCause::_no_gc || request.cause == GCCause::_shenandoah_stop_vm) {\n+    return;\n+  }\n@@ -111,3 +137,10 @@\n-      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n-      generation = _degen_generation->type();\n-      bool old_gen_evacuation_failed = heap->old_generation()->clear_failed_evacuation();\n+  GCMode mode;\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(request.cause)) {\n+    mode = prepare_for_allocation_failure_request(request);\n+  } else if (ShenandoahCollectorPolicy::is_explicit_gc(request.cause)) {\n+    mode = prepare_for_explicit_gc_request(request);\n+  } else {\n+    mode = prepare_for_concurrent_gc_request(request);\n+  }\n+  set_gc_mode(ml, mode);\n+}\n@@ -115,1 +148,1 @@\n-      heuristics->log_trigger(\"Handle Allocation Failure\");\n+ShenandoahGenerationalControlThread::GCMode ShenandoahGenerationalControlThread::prepare_for_allocation_failure_request(ShenandoahGCRequest &request) {\n@@ -117,16 +150,8 @@\n-      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n-          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n-        heuristics->record_allocation_failure_gc();\n-        policy->record_alloc_failure_to_degenerated(degen_point);\n-        set_gc_mode(stw_degenerated);\n-      } else {\n-        heuristics->record_allocation_failure_gc();\n-        policy->record_alloc_failure_to_full();\n-        generation = GLOBAL;\n-        set_gc_mode(stw_full);\n-      }\n-    } else if (is_gc_requested) {\n-      generation = GLOBAL;\n-      global_heuristics->log_trigger(\"GC request (%s)\", GCCause::to_string(cause));\n-      global_heuristics->record_requested_gc();\n+  if (_degen_point == ShenandoahGC::_degenerated_unset) {\n+    _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+    request.generation = _heap->young_generation();\n+  } else if (request.generation->is_old()) {\n+    \/\/ This means we degenerated during the young bootstrap for the old generation\n+    \/\/ cycle. The following degenerated cycle should therefore also be young.\n+    request.generation = _heap->young_generation();\n+  }\n@@ -134,20 +159,2 @@\n-      if (ShenandoahCollectorPolicy::should_run_full_gc(cause)) {\n-        set_gc_mode(stw_full);\n-      } else {\n-        set_gc_mode(default_mode);\n-        \/\/ Unload and clean up everything\n-        heap->set_unload_classes(global_heuristics->can_unload_classes());\n-      }\n-    } else {\n-      \/\/ We should only be here if the regulator requested a cycle or if\n-      \/\/ there is an old generation mark in progress.\n-      if (cause == GCCause::_shenandoah_concurrent_gc) {\n-        if (_requested_generation == OLD && heap->old_generation()->is_doing_mixed_evacuations()) {\n-          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n-          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n-          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n-          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n-          generation = YOUNG;\n-        } else {\n-          generation = _requested_generation;\n-        }\n+  ShenandoahHeuristics* heuristics = request.generation->heuristics();\n+  bool old_gen_evacuation_failed = _heap->old_generation()->clear_failed_evacuation();\n@@ -155,2 +162,1 @@\n-        \/\/ preemption was requested or this is a regular cycle\n-        set_gc_mode(default_mode);\n+  heuristics->log_trigger(\"Handle Allocation Failure\");\n@@ -158,4 +164,13 @@\n-        \/\/ Don't start a new old marking if there is one already in progress\n-        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n-          set_gc_mode(servicing_old);\n-        }\n+  \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n+  if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n+      !old_gen_evacuation_failed && request.cause != GCCause::_shenandoah_humongous_allocation_failure) {\n+    heuristics->record_allocation_failure_gc();\n+    _heap->shenandoah_policy()->record_alloc_failure_to_degenerated(_degen_point);\n+    return stw_degenerated;\n+  } else {\n+    heuristics->record_allocation_failure_gc();\n+    _heap->shenandoah_policy()->record_alloc_failure_to_full();\n+    request.generation = _heap->global_generation();\n+    return stw_full;\n+  }\n+}\n@@ -163,18 +178,5 @@\n-        if (generation == GLOBAL) {\n-          heap->set_unload_classes(global_heuristics->should_unload_classes());\n-        } else {\n-          heap->set_unload_classes(false);\n-        }\n-      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n-        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n-        \/\/ mixed evacuation in progress, so resume working on that.\n-        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n-                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n-                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n-\n-        cause = GCCause::_shenandoah_concurrent_gc;\n-        generation = OLD;\n-        set_gc_mode(servicing_old);\n-        heap->set_unload_classes(false);\n-      }\n-    }\n+ShenandoahGenerationalControlThread::GCMode ShenandoahGenerationalControlThread::prepare_for_explicit_gc_request(ShenandoahGCRequest &request) {\n+  ShenandoahHeuristics* global_heuristics = _heap->global_generation()->heuristics();\n+  request.generation = _heap->global_generation();\n+  global_heuristics->log_trigger(\"GC request (%s)\", GCCause::to_string(request.cause));\n+  global_heuristics->record_requested_gc();\n@@ -182,2 +184,9 @@\n-    const bool gc_requested = (gc_mode() != none);\n-    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n+  if (ShenandoahCollectorPolicy::should_run_full_gc(request.cause)) {\n+    return stw_full;;\n+  } else {\n+    \/\/ Unload and clean up everything. Note that this is an _explicit_ request and so does not use\n+    \/\/ the same `should_unload_classes` call as the regulator's concurrent gc request.\n+    _heap->set_unload_classes(global_heuristics->can_unload_classes());\n+    return concurrent_normal;\n+  }\n+}\n@@ -185,3 +194,3 @@\n-    if (gc_requested) {\n-      \/\/ Cannot uncommit bitmap slices during concurrent reset\n-      ShenandoahNoUncommitMark forbid_region_uncommit(heap);\n+ShenandoahGenerationalControlThread::GCMode ShenandoahGenerationalControlThread::prepare_for_concurrent_gc_request(ShenandoahGCRequest &request) {\n+  assert(!(request.generation->is_old() && _heap->old_generation()->is_doing_mixed_evacuations()),\n+             \"Old heuristic should not request cycles while it waits for mixed evacuations\");\n@@ -189,5 +198,6 @@\n-      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n-      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n-      if (generation == GLOBAL && (alloc_failure_pending || is_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n-        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n-      }\n+  if (request.generation->is_global()) {\n+    ShenandoahHeuristics* global_heuristics = _heap->global_generation()->heuristics();\n+    _heap->set_unload_classes(global_heuristics->should_unload_classes());\n+  } else {\n+    _heap->set_unload_classes(false);\n+  }\n@@ -195,54 +205,3 @@\n-      \/\/ GC is starting, bump the internal ID\n-      update_gc_id();\n-\n-      heap->reset_bytes_allocated_since_gc_start();\n-\n-      MetaspaceCombinedStats meta_sizes = MetaspaceUtils::get_combined_statistics();\n-\n-      \/\/ If GC was requested, we are sampling the counters even without actual triggers\n-      \/\/ from allocation machinery. This captures GC phases more accurately.\n-      heap->set_forced_counters_update(true);\n-\n-      \/\/ If GC was requested, we better dump freeset data for performance debugging\n-      heap->free_set()->log_status_under_lock();\n-\n-      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n-      const bool was_aging_cycle = heap->is_aging_cycle();\n-      heap->set_aging_cycle(false);\n-\n-      switch (gc_mode()) {\n-        case concurrent_normal: {\n-          \/\/ At this point:\n-          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n-          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n-          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n-          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n-          if (age_period-- == 0) {\n-             heap->set_aging_cycle(true);\n-             age_period = ShenandoahAgingCyclePeriod - 1;\n-          }\n-          service_concurrent_normal_cycle(heap, generation, cause);\n-          break;\n-        }\n-        case stw_degenerated: {\n-          heap->set_aging_cycle(was_aging_cycle);\n-          service_stw_degenerated_cycle(cause, degen_point);\n-          break;\n-        }\n-        case stw_full: {\n-          if (age_period-- == 0) {\n-            heap->set_aging_cycle(true);\n-            age_period = ShenandoahAgingCyclePeriod - 1;\n-          }\n-          service_stw_full_cycle(cause);\n-          break;\n-        }\n-        case servicing_old: {\n-          assert(generation == OLD, \"Expected old generation here\");\n-          GCIdMark gc_id_mark;\n-          service_concurrent_old_cycle(heap, cause);\n-          break;\n-        }\n-        default:\n-          ShouldNotReachHere();\n-      }\n+  \/\/ preemption was requested or this is a regular cycle\n+  return request.generation->is_old() ? servicing_old : concurrent_normal;\n+}\n@@ -250,4 +209,8 @@\n-      \/\/ If this was the requested GC cycle, notify waiters about it\n-      if (is_gc_requested) {\n-        notify_gc_waiters();\n-      }\n+void ShenandoahGenerationalControlThread::maybe_set_aging_cycle() {\n+  if (_age_period-- == 0) {\n+    _heap->set_aging_cycle(true);\n+    _age_period = ShenandoahAgingCyclePeriod - 1;\n+  } else {\n+    _heap->set_aging_cycle(false);\n+  }\n+}\n@@ -255,4 +218,1 @@\n-      \/\/ If this was the allocation failure GC cycle, notify waiters about it\n-      if (alloc_failure_pending) {\n-        notify_alloc_failure_waiters();\n-      }\n+void ShenandoahGenerationalControlThread::run_gc_cycle(ShenandoahGCRequest request) {\n@@ -260,3 +220,2 @@\n-      \/\/ Report current free set state at the end of cycle, whether\n-      \/\/ it is a normal completion, or the abort.\n-      heap->free_set()->log_status_under_lock();\n+  log_debug(gc, thread)(\"Starting GC (%s): %s, %s\", gc_mode_name(gc_mode()), GCCause::to_string(request.cause), request.generation->name());\n+  assert(gc_mode() != none, \"GC mode cannot be none here\");\n@@ -264,4 +223,5 @@\n-      \/\/ Notify Universe about new heap usage. This has implications for\n-      \/\/ global soft refs policy, and we better report it every time heap\n-      \/\/ usage goes down.\n-      heap->update_capacity_and_used_at_gc();\n+  \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n+  \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+  if (request.generation->is_global() && (ShenandoahCollectorPolicy::is_allocation_failure(request.cause) || ShenandoahCollectorPolicy::is_explicit_gc(request.cause) || ShenandoahAlwaysClearSoftRefs)) {\n+    _heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+  }\n@@ -269,2 +229,2 @@\n-      \/\/ Signal that we have completed a visit to all live objects.\n-      heap->record_whole_heap_examined_timestamp();\n+  \/\/ GC is starting, bump the internal ID\n+  update_gc_id();\n@@ -272,4 +232,1 @@\n-      \/\/ Disable forced counters update, and update counters one more time\n-      \/\/ to capture the state at the end of GC session.\n-      heap->handle_force_counters_update();\n-      heap->set_forced_counters_update(false);\n+  _heap->reset_bytes_allocated_since_gc_start();\n@@ -277,2 +234,1 @@\n-      \/\/ Retract forceful part of soft refs policy\n-      heap->soft_ref_policy()->set_should_clear_all_soft_refs(false);\n+  MetaspaceCombinedStats meta_sizes = MetaspaceUtils::get_combined_statistics();\n@@ -280,4 +236,3 @@\n-      \/\/ Clear metaspace oom flag, if current cycle unloaded classes\n-      if (heap->unload_classes()) {\n-        global_heuristics->clear_metaspace_oom();\n-      }\n+  \/\/ If GC was requested, we are sampling the counters even without actual triggers\n+  \/\/ from allocation machinery. This captures GC phases more accurately.\n+  _heap->set_forced_counters_update(true);\n@@ -285,1 +240,2 @@\n-      process_phase_timings(heap);\n+  \/\/ If GC was requested, we better dump freeset data for performance debugging\n+  _heap->free_set()->log_status_under_lock();\n@@ -287,2 +243,3 @@\n-      \/\/ Print Metaspace change following GC (if logging is enabled).\n-      MetaspaceUtils::print_metaspace_change(meta_sizes);\n+  {\n+    \/\/ Cannot uncommit bitmap slices during concurrent reset\n+    ShenandoahNoUncommitMark forbid_region_uncommit(_heap);\n@@ -290,3 +247,4 @@\n-      \/\/ GC is over, we are at idle now\n-      if (ShenandoahPacing) {\n-        heap->pacer()->setup_for_idle();\n+    switch (gc_mode()) {\n+      case concurrent_normal: {\n+        service_concurrent_normal_cycle(request);\n+        break;\n@@ -294,4 +252,3 @@\n-    } else {\n-      \/\/ Report to pacer that we have seen this many words allocated\n-      if (ShenandoahPacing && (allocs_seen > 0)) {\n-        heap->pacer()->report_alloc(allocs_seen);\n+      case stw_degenerated: {\n+        service_stw_degenerated_cycle(request);\n+        break;\n@@ -299,9 +256,3 @@\n-    }\n-\n-    \/\/ Check if we have seen a new target for soft max heap size or if a gc was requested.\n-    \/\/ Either of these conditions will attempt to uncommit regions.\n-    if (ShenandoahUncommit) {\n-      if (heap->check_soft_max_changed()) {\n-        heap->notify_soft_max_changed();\n-      } else if (is_gc_requested) {\n-        heap->notify_explicit_gc_requested();\n+      case stw_full: {\n+        service_stw_full_cycle(request.cause);\n+        break;\n@@ -309,0 +260,8 @@\n+      case servicing_old: {\n+        assert(request.generation->is_old(), \"Expected old generation here\");\n+        GCIdMark gc_id_mark;\n+        service_concurrent_old_cycle(request);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n@@ -310,0 +269,1 @@\n+  }\n@@ -311,7 +271,3 @@\n-    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n-    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n-      \/\/ The timed wait is necessary because this thread has a responsibility to send\n-      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n-      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n-      lock.wait(ShenandoahControlIntervalMax);\n-    }\n+  \/\/ If this was the requested GC cycle, notify waiters about it\n+  if (ShenandoahCollectorPolicy::is_explicit_gc(request.cause)) {\n+    notify_gc_waiters();\n@@ -320,1 +276,4 @@\n-  set_gc_mode(stopped);\n+  \/\/ If this was an allocation failure GC cycle, notify waiters about it\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(request.cause)) {\n+    notify_alloc_failure_waiters();\n+  }\n@@ -322,3 +281,33 @@\n-  \/\/ Wait for the actual stop(), can't leave run_service() earlier.\n-  while (!should_terminate()) {\n-    os::naked_short_sleep(ShenandoahControlIntervalMin);\n+  \/\/ Report current free set state at the end of cycle, whether\n+  \/\/ it is a normal completion, or the abort.\n+  _heap->free_set()->log_status_under_lock();\n+\n+  \/\/ Notify Universe about new heap usage. This has implications for\n+  \/\/ global soft refs policy, and we better report it every time heap\n+  \/\/ usage goes down.\n+  _heap->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  _heap->record_whole_heap_examined_timestamp();\n+\n+  \/\/ Disable forced counters update, and update counters one more time\n+  \/\/ to capture the state at the end of GC session.\n+  _heap->handle_force_counters_update();\n+  _heap->set_forced_counters_update(false);\n+\n+  \/\/ Retract forceful part of soft refs policy\n+  _heap->soft_ref_policy()->set_should_clear_all_soft_refs(false);\n+\n+  \/\/ Clear metaspace oom flag, if current cycle unloaded classes\n+  if (_heap->unload_classes()) {\n+    _heap->global_generation()->heuristics()->clear_metaspace_oom();\n+  }\n+\n+  process_phase_timings();\n+\n+  \/\/ Print Metaspace change following GC (if logging is enabled).\n+  MetaspaceUtils::print_metaspace_change(meta_sizes);\n+\n+  \/\/ GC is over, we are at idle now\n+  if (ShenandoahPacing) {\n+    _heap->pacer()->setup_for_idle();\n@@ -326,0 +315,13 @@\n+\n+  \/\/ Check if we have seen a new target for soft max heap size or if a gc was requested.\n+  \/\/ Either of these conditions will attempt to uncommit regions.\n+  if (ShenandoahUncommit) {\n+    if (_heap->check_soft_max_changed()) {\n+      _heap->notify_soft_max_changed();\n+    } else if (ShenandoahCollectorPolicy::is_explicit_gc(request.cause)) {\n+      _heap->notify_explicit_gc_requested();\n+    }\n+  }\n+\n+  log_debug(gc, thread)(\"Completed GC (%s): %s, %s, cancelled: %s\",\n+    gc_mode_name(gc_mode()), GCCause::to_string(request.cause), request.generation->name(), GCCause::to_string(_heap->cancelled_cause()));\n@@ -328,1 +330,1 @@\n-void ShenandoahGenerationalControlThread::process_phase_timings(const ShenandoahGenerationalHeap* heap) {\n+void ShenandoahGenerationalControlThread::process_phase_timings() {\n@@ -330,1 +332,1 @@\n-  heap->phase_timings()->flush_par_workers_to_cycle();\n+  _heap->phase_timings()->flush_par_workers_to_cycle();\n@@ -332,1 +334,1 @@\n-    heap->pacer()->flush_stats_to_cycle();\n+    _heap->pacer()->flush_stats_to_cycle();\n@@ -335,1 +337,1 @@\n-  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n+  ShenandoahEvacuationTracker* evac_tracker = _heap->evac_tracker();\n@@ -344,1 +346,1 @@\n-      heap->phase_timings()->print_cycle_on(&ls);\n+      _heap->phase_timings()->print_cycle_on(&ls);\n@@ -348,1 +350,1 @@\n-        heap->pacer()->print_cycle_on(&ls);\n+        _heap->pacer()->print_cycle_on(&ls);\n@@ -354,1 +356,1 @@\n-  heap->phase_timings()->flush_cycle_to_global();\n+  _heap->phase_timings()->flush_cycle_to_global();\n@@ -382,3 +384,1 @@\n-void ShenandoahGenerationalControlThread::service_concurrent_normal_cycle(ShenandoahGenerationalHeap* heap,\n-                                                                          const ShenandoahGenerationType generation,\n-                                                                          GCCause::Cause cause) {\n+void ShenandoahGenerationalControlThread::service_concurrent_normal_cycle(ShenandoahGCRequest request) {\n@@ -386,23 +386,5 @@\n-  switch (generation) {\n-    case YOUNG: {\n-      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n-      \/\/ concurrent mark in the old generation. We need to think about promotions\n-      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n-      \/\/ they end up in, but we have to be sure we don't promote into any regions\n-      \/\/ that are in the cset.\n-      log_info(gc, ergo)(\"Start GC cycle (Young)\");\n-      service_concurrent_cycle(heap->young_generation(), cause, false);\n-      break;\n-    }\n-    case OLD: {\n-      log_info(gc, ergo)(\"Start GC cycle (Old)\");\n-      service_concurrent_old_cycle(heap, cause);\n-      break;\n-    }\n-    case GLOBAL: {\n-      log_info(gc, ergo)(\"Start GC cycle (Global)\");\n-      service_concurrent_cycle(heap->global_generation(), cause, false);\n-      break;\n-    }\n-    default:\n-      ShouldNotReachHere();\n+  log_info(gc, ergo)(\"Start GC cycle (%s)\", request.generation->name());\n+  if (request.generation->is_old()) {\n+    service_concurrent_old_cycle(request);\n+  } else {\n+    service_concurrent_cycle(request.generation, request.cause, false);\n@@ -412,3 +394,3 @@\n-void ShenandoahGenerationalControlThread::service_concurrent_old_cycle(ShenandoahGenerationalHeap* heap, GCCause::Cause &cause) {\n-  ShenandoahOldGeneration* old_generation = heap->old_generation();\n-  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+void ShenandoahGenerationalControlThread::service_concurrent_old_cycle(ShenandoahGCRequest request) {\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n@@ -417,1 +399,1 @@\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  TraceCollectorStats tcs(_heap->monitoring_support()->concurrent_collection_counters());\n@@ -421,1 +403,2 @@\n-      ShenandoahGCSession session(cause, old_generation);\n+      ShenandoahGCSession session(request.cause, old_generation);\n+      assert(gc_mode() == servicing_old, \"Filling should be servicing old\");\n@@ -433,1 +416,1 @@\n-        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n+        log_info(gc, thread)(\"Preparation for old generation cycle was cancelled\");\n@@ -451,4 +434,3 @@\n-      ShenandoahGCSession session(cause, young_generation);\n-      service_concurrent_cycle(heap, young_generation, cause, true);\n-      process_phase_timings(heap);\n-      if (heap->cancelled_gc()) {\n+      service_concurrent_cycle(young_generation, request.cause, true);\n+      process_phase_timings();\n+      if (_heap->cancelled_gc()) {\n@@ -461,4 +443,1 @@\n-      \/\/ Reset the degenerated point. Normally this would happen at the top\n-      \/\/ of the control loop, but here we have just completed a young cycle\n-      \/\/ which has bootstrapped the old concurrent marking.\n-      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+      assert(_degen_point == ShenandoahGC::_degenerated_unset, \"Degen point should not be set if gc wasn't cancelled\");\n@@ -473,2 +452,2 @@\n-      ShenandoahGCSession session(cause, old_generation);\n-      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      ShenandoahGCSession session(request.cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, request.cause);\n@@ -478,2 +457,2 @@\n-          heap->mmu_tracker()->record_old_marking_increment(true);\n-          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+          _heap->mmu_tracker()->record_old_marking_increment(true);\n+          _heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n@@ -482,2 +461,2 @@\n-        heap->mmu_tracker()->record_old_marking_increment(false);\n-        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+        _heap->mmu_tracker()->record_old_marking_increment(false);\n+        _heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n@@ -493,1 +472,1 @@\n-  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  assert(_heap->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n@@ -496,2 +475,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n@@ -503,1 +480,1 @@\n-    heap->notify_gc_progress();\n+    _heap->notify_gc_progress();\n@@ -507,1 +484,1 @@\n-  if (heap->cancelled_gc()) {\n+  if (_heap->cancelled_gc()) {\n@@ -518,2 +495,2 @@\n-    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n-      heap->shenandoah_policy()->record_interrupted_old();\n+    if (cause == GCCause::_shenandoah_concurrent_gc) {\n+      _heap->shenandoah_policy()->record_interrupted_old();\n@@ -526,37 +503,38 @@\n-void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n-  \/\/ Normal cycle goes via all concurrent phases. If allocation failure (af) happens during\n-  \/\/ any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.\n-  \/\/ If second allocation failure happens during Degenerated GC cycle (for example, when GC\n-  \/\/ tries to evac something and no memory is available), cycle degrades to Full GC.\n-  \/\/\n-  \/\/ There are also a shortcut through the normal cycle: immediate garbage shortcut, when\n-  \/\/ heuristics says there are no regions to compact, and all the collection comes from immediately\n-  \/\/ reclaimable regions.\n-  \/\/\n-  \/\/ ................................................................................................\n-  \/\/\n-  \/\/                                    (immediate garbage shortcut)                Concurrent GC\n-  \/\/                             \/-------------------------------------------\\\n-  \/\/                             |                                           |\n-  \/\/                             |                                           |\n-  \/\/                             |                                           |\n-  \/\/                             |                                           v\n-  \/\/ [START] ----> Conc Mark ----o----> Conc Evac --o--> Conc Update-Refs ---o----> [END]\n-  \/\/                   |                    |                 |              ^\n-  \/\/                   | (af)               | (af)            | (af)         |\n-  \/\/ ..................|....................|.................|..............|.......................\n-  \/\/                   |                    |                 |              |\n-  \/\/                   |                    |                 |              |      Degenerated GC\n-  \/\/                   v                    v                 v              |\n-  \/\/               STW Mark ----------> STW Evac ----> STW Update-Refs ----->o\n-  \/\/                   |                    |                 |              ^\n-  \/\/                   | (af)               | (af)            | (af)         |\n-  \/\/ ..................|....................|.................|..............|.......................\n-  \/\/                   |                    |                 |              |\n-  \/\/                   |                    v                 |              |      Full GC\n-  \/\/                   \\------------------->o<----------------\/              |\n-  \/\/                                        |                                |\n-  \/\/                                        v                                |\n-  \/\/                                      Full GC  --------------------------\/\n-  \/\/\n-  if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) return;\n+\/\/ Normal cycle goes via all concurrent phases. If allocation failure (af) happens during\n+\/\/ any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.\n+\/\/ If second allocation failure happens during Degenerated GC cycle (for example, when GC\n+\/\/ tries to evac something and no memory is available), cycle degrades to Full GC.\n+\/\/\n+\/\/ There are also a shortcut through the normal cycle: immediate garbage shortcut, when\n+\/\/ heuristics says there are no regions to compact, and all the collection comes from immediately\n+\/\/ reclaimable regions.\n+\/\/\n+\/\/ ................................................................................................\n+\/\/\n+\/\/                                    (immediate garbage shortcut)                Concurrent GC\n+\/\/                             \/-------------------------------------------\\\n+\/\/                             |                                           |\n+\/\/                             |                                           |\n+\/\/                             |                                           |\n+\/\/                             |                                           v\n+\/\/ [START] ----> Conc Mark ----o----> Conc Evac --o--> Conc Update-Refs ---o----> [END]\n+\/\/                   |                    |                 |              ^\n+\/\/                   | (af)               | (af)            | (af)         |\n+\/\/ ..................|....................|.................|..............|.......................\n+\/\/                   |                    |                 |              |\n+\/\/                   |                    |                 |              |      Degenerated GC\n+\/\/                   v                    v                 v              |\n+\/\/               STW Mark ----------> STW Evac ----> STW Update-Refs ----->o\n+\/\/                   |                    |                 |              ^\n+\/\/                   | (af)               | (af)            | (af)         |\n+\/\/ ..................|....................|.................|..............|.......................\n+\/\/                   |                    |                 |              |\n+\/\/                   |                    v                 |              |      Full GC\n+\/\/                   \\------------------->o<----------------\/              |\n+\/\/                                        |                                |\n+\/\/                                        v                                |\n+\/\/                                      Full GC  --------------------------\/\n+\/\/\n+void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahGeneration* generation,\n+                                                                   GCCause::Cause cause,\n+                                                                   bool do_old_gc_bootstrap) {\n@@ -564,3 +542,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahGCSession session(cause, generation);\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n+    return;\n+  }\n@@ -568,2 +546,8 @@\n-  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n-}\n+  \/\/ At this point:\n+  \/\/  if (generation == YOUNG), this is a normal young cycle or a bootstrap cycle\n+  \/\/  if (generation == GLOBAL), this is a GLOBAL cycle\n+  \/\/ In either case, we want to age old objects if this is an aging cycle\n+  maybe_set_aging_cycle();\n+\n+  ShenandoahGCSession session(cause, generation);\n+  TraceCollectorStats tcs(_heap->monitoring_support()->concurrent_collection_counters());\n@@ -571,4 +555,0 @@\n-void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n-                                                       ShenandoahGeneration* generation,\n-                                                       GCCause::Cause& cause,\n-                                                       bool do_old_gc_bootstrap) {\n@@ -580,1 +560,1 @@\n-    heap->notify_gc_progress();\n+    _heap->notify_gc_progress();\n@@ -583,1 +563,1 @@\n-    assert(heap->cancelled_gc(), \"Must have been cancelled\");\n+    assert(_heap->cancelled_gc(), \"Must have been cancelled\");\n@@ -585,4 +565,0 @@\n-\n-    \/\/ Concurrent young-gen collection degenerates to young\n-    \/\/ collection.  Same for global collections.\n-    _degen_generation = generation;\n@@ -590,0 +566,1 @@\n+\n@@ -591,1 +568,1 @@\n-  ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+  ShenandoahMmuTracker* mmu_tracker = _heap->mmu_tracker();\n@@ -593,1 +570,1 @@\n-    if (heap->cancelled_gc()) {\n+    if (_heap->cancelled_gc()) {\n@@ -600,1 +577,1 @@\n-      if (heap->collection_set()->has_old_regions()) {\n+      if (_heap->collection_set()->has_old_regions()) {\n@@ -611,1 +588,1 @@\n-    if (heap->cancelled_gc()) {\n+    if (_heap->cancelled_gc()) {\n@@ -619,1 +596,1 @@\n-  heap->log_heap_status(msg);\n+  _heap->log_heap_status(msg);\n@@ -623,2 +600,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (!heap->cancelled_gc()) {\n+  if (!_heap->cancelled_gc()) {\n@@ -628,10 +604,3 @@\n-  if (in_graceful_shutdown()) {\n-    return true;\n-  }\n-\n-  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-\n-  if (is_alloc_failure_gc()) {\n-    _degen_point = point;\n-    _preemption_requested.unset();\n+  if (_heap->cancelled_cause() == GCCause::_shenandoah_stop_vm\n+    || _heap->cancelled_cause() == GCCause::_shenandoah_concurrent_gc) {\n+    log_debug(gc, thread)(\"Cancellation detected, reason: %s\", GCCause::to_string(_heap->cancelled_cause()));\n@@ -641,9 +610,3 @@\n-  if (_preemption_requested.is_set()) {\n-    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n-    _preemption_requested.unset();\n-\n-    \/\/ Old generation marking is only cancellable during concurrent marking.\n-    \/\/ Once final mark is complete, the code does not check again for cancellation.\n-    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n-    \/\/ make it to this case. The calling code is responsible for forcing a\n-    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(_heap->cancelled_cause())) {\n+    assert(_degen_point == ShenandoahGC::_degenerated_unset,\n+           \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n@@ -651,1 +614,3 @@\n-    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+    log_debug(gc, thread)(\"Cancellation detected:, reason: %s, degen point: %s\",\n+                          GCCause::to_string(_heap->cancelled_cause()),\n+                          ShenandoahGC::degen_point_to_string(_degen_point));\n@@ -659,4 +624,0 @@\n-void ShenandoahGenerationalControlThread::stop_service() {\n-  \/\/ Nothing to do here.\n-}\n-\n@@ -664,2 +625,0 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-\n@@ -667,2 +626,2 @@\n-  ShenandoahGCSession session(cause, heap->global_generation());\n-\n+  ShenandoahGCSession session(cause, _heap->global_generation());\n+  maybe_set_aging_cycle();\n@@ -671,0 +630,1 @@\n+  _degen_point = ShenandoahGC::_degenerated_unset;\n@@ -673,4 +633,2 @@\n-void ShenandoahGenerationalControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n-                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n-  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+void ShenandoahGenerationalControlThread::service_stw_degenerated_cycle(ShenandoahGCRequest request) {\n+  assert(_degen_point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n@@ -679,1 +637,1 @@\n-  ShenandoahGCSession session(cause, _degen_generation);\n+  ShenandoahGCSession session(request.cause, request.generation);\n@@ -681,2 +639,3 @@\n-  ShenandoahDegenGC gc(point, _degen_generation);\n-  gc.collect(cause);\n+  ShenandoahDegenGC gc(_degen_point, request.generation);\n+  gc.collect(request.cause);\n+  _degen_point = ShenandoahGC::_degenerated_unset;\n@@ -684,4 +643,4 @@\n-  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n-  if (_degen_generation->is_global()) {\n-    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n-    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  assert(_heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (request.generation->is_global()) {\n+    assert(_heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(_heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n@@ -689,2 +648,2 @@\n-    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n-    ShenandoahOldGeneration* old = heap->old_generation();\n+    assert(request.generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old = _heap->old_generation();\n@@ -698,1 +657,5 @@\n-  if (ShenandoahCollectorPolicy::should_handle_requested_gc(cause)) {\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(cause)) {\n+    \/\/ GC should already be cancelled. Here we are just notifying the control thread to\n+    \/\/ wake up and handle the cancellation request, so we don't need to set _requested_gc_cause.\n+    notify_cancellation(cause);\n+  } else if (ShenandoahCollectorPolicy::should_handle_requested_gc(cause)) {\n@@ -703,2 +666,2 @@\n-bool ShenandoahGenerationalControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n-  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n+bool ShenandoahGenerationalControlThread::request_concurrent_gc(ShenandoahGeneration* generation) {\n+  if (_heap->cancelled_gc()) {\n@@ -706,2 +669,1 @@\n-    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n-                          BOOL_TO_STR(_preemption_requested.is_set()),\n+    log_debug(gc, thread)(\"Reject request for concurrent gc: gc_requested: %s, gc_cancelled: %s\",\n@@ -709,1 +671,1 @@\n-                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n+                          BOOL_TO_STR(_heap->cancelled_gc()));\n@@ -713,12 +675,9 @@\n-  if (gc_mode() == none) {\n-    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n-      return false;\n-    }\n-\n-    _requested_generation = generation;\n-    notify_control_thread();\n-\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    while (gc_mode() == none) {\n+  if (preempt_old_marking(generation)) {\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC\", generation->name());\n+\n+    \/\/ Cancel the old GC and wait for the control thread to start servicing the new request.\n+    \/\/ We are using the fact old is only preemptible when the control thread mode is servicing_old\n+    MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() != concurrent_normal) {\n+      ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+      notify_cancellation(ml, GCCause::_shenandoah_concurrent_gc);\n@@ -727,0 +686,1 @@\n+\n@@ -730,13 +690,7 @@\n-  if (preempt_old_marking(generation)) {\n-    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n-    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n-      return false;\n-    }\n-\n-    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n-    _requested_generation = generation;\n-    _preemption_requested.set();\n-    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n-    notify_control_thread();\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  if (gc_mode() == none) {\n+    while (gc_mode() == none) {\n+      if (_requested_gc_cause != GCCause::_no_gc) {\n+        log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(_requested_gc_cause));\n+        return false;\n+      }\n@@ -744,2 +698,1 @@\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    while (gc_mode() == servicing_old) {\n+      notify_control_thread(ml, GCCause::_shenandoah_concurrent_gc, generation);\n@@ -751,0 +704,1 @@\n+\n@@ -757,3 +711,22 @@\n-void ShenandoahGenerationalControlThread::notify_control_thread() {\n-  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n-  _control_lock.notify();\n+void ShenandoahGenerationalControlThread::notify_control_thread(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  notify_control_thread(ml, cause, generation);\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_control_thread(MonitorLocker& ml, GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  assert(_control_lock.is_locked(), \"Request lock must be held here\");\n+  log_debug(gc, thread)(\"Notify control (%s): %s, %s\", gc_mode_name(gc_mode()), GCCause::to_string(cause), generation->name());\n+  _requested_gc_cause = cause;\n+  _requested_generation = generation;\n+  ml.notify();\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_cancellation(GCCause::Cause cause) {\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  notify_cancellation(ml, cause);\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_cancellation(MonitorLocker& ml, GCCause::Cause cause) {\n+  assert(_heap->cancelled_gc(), \"GC should already be cancelled\");\n+  log_debug(gc,thread)(\"Notify control (%s): %s\", gc_mode_name(gc_mode()), GCCause::to_string(cause));\n+  ml.notify();\n@@ -762,2 +735,2 @@\n-bool ShenandoahGenerationalControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n-  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n+bool ShenandoahGenerationalControlThread::preempt_old_marking(ShenandoahGeneration* generation) {\n+  return generation->is_young() && _allow_old_preemption.try_unset();\n@@ -772,2 +745,1 @@\n-    Atomic::xchg(&_requested_gc_cause, cause);\n-    notify_control_thread();\n+    notify_control_thread(cause, ShenandoahHeap::heap()->global_generation());\n@@ -793,6 +765,1 @@\n-    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n-    }\n-\n-    notify_control_thread();\n+    notify_control_thread(cause, ShenandoahHeap::heap()->global_generation());\n@@ -809,1 +776,1 @@\n-const char* ShenandoahGenerationalControlThread::gc_mode_name(ShenandoahGenerationalControlThread::GCMode mode) {\n+const char* ShenandoahGenerationalControlThread::gc_mode_name(GCMode mode) {\n@@ -822,1 +789,6 @@\n-void ShenandoahGenerationalControlThread::set_gc_mode(ShenandoahGenerationalControlThread::GCMode new_mode) {\n+void ShenandoahGenerationalControlThread::set_gc_mode(GCMode new_mode) {\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  set_gc_mode(ml, new_mode);\n+}\n+\n+void ShenandoahGenerationalControlThread::set_gc_mode(MonitorLocker& ml, GCMode new_mode) {\n@@ -824,2 +796,1 @@\n-    log_debug(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    log_debug(gc, thread)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":407,"deletions":436,"binary":false,"changes":843,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include <runtime\/mutexLocker.hpp>\n+\n@@ -31,1 +33,0 @@\n-#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -55,0 +56,7 @@\n+  class ShenandoahGCRequest {\n+  public:\n+    ShenandoahGCRequest() : generation(nullptr), cause(GCCause::_no_gc) {}\n+    ShenandoahGeneration* generation;\n+    GCCause::Cause cause;\n+  };\n+\n@@ -56,0 +64,2 @@\n+  \/\/ This lock is used to coordinate setting the _requested_gc_cause, _requested generation\n+  \/\/ and _gc_mode. It is important that these be changed together and have a consistent view.\n@@ -57,1 +67,0 @@\n-  Monitor _regulator_lock;\n@@ -59,0 +68,2 @@\n+  \/\/ This is true when the old generation cycle is in an interruptible phase (i.e., marking or\n+  \/\/ preparing for mark).\n@@ -60,1 +71,0 @@\n-  ShenandoahSharedFlag _preemption_requested;\n@@ -62,0 +72,2 @@\n+  \/\/ Represents a normal (non cancellation) gc request. This can be set by mutators (System.gc,\n+  \/\/ whitebox gc, etc.) or by the regulator thread when the heuristics want to start a cycle.\n@@ -63,1 +75,6 @@\n-  volatile ShenandoahGenerationType _requested_generation;\n+\n+  \/\/ This is the generation the request should operate on.\n+  ShenandoahGeneration* _requested_generation;\n+\n+  \/\/ Only the control thread knows the correct degeneration point. This is used to have the\n+  \/\/ control thread resume a STW cycle from the point where the concurrent cycle was cancelled.\n@@ -65,1 +82,0 @@\n-  ShenandoahGeneration* _degen_generation;\n@@ -67,0 +83,7 @@\n+  \/\/ A reference to the heap\n+  ShenandoahGenerationalHeap* _heap;\n+\n+  \/\/ This is used to keep track of whether to age objects during the current cycle.\n+  uint _age_period;\n+\n+  \/\/ The mode is read frequently by requesting threads and only ever written by the control thread.\n@@ -80,1 +103,1 @@\n-  bool request_concurrent_gc(ShenandoahGenerationType generation);\n+  bool request_concurrent_gc(ShenandoahGeneration* generation);\n@@ -82,0 +105,1 @@\n+  \/\/ Returns the current state of the control thread\n@@ -86,1 +110,0 @@\n-\n@@ -90,0 +113,3 @@\n+  \/\/ Executes one GC cycle\n+  void run_gc_cycle(ShenandoahGCRequest request);\n+\n@@ -94,1 +120,3 @@\n-  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n+  void service_stw_degenerated_cycle(ShenandoahGCRequest request);\n+  void service_concurrent_normal_cycle(ShenandoahGCRequest request);\n+  void service_concurrent_old_cycle(ShenandoahGCRequest cause);\n@@ -98,2 +126,1 @@\n-  \/\/ Handle GC request.\n-  \/\/ Blocks until GC is over.\n+  \/\/ Blocks until at least one global GC cycle is complete.\n@@ -102,3 +129,0 @@\n-  bool is_explicit_gc(GCCause::Cause cause) const;\n-  bool is_implicit_gc(GCCause::Cause cause) const;\n-\n@@ -106,1 +130,1 @@\n-  bool preempt_old_marking(ShenandoahGenerationType generation);\n+  bool preempt_old_marking(ShenandoahGeneration* generation);\n@@ -108,1 +132,1 @@\n-  void process_phase_timings(const ShenandoahGenerationalHeap* heap);\n+  void process_phase_timings();\n@@ -110,3 +134,3 @@\n-  void service_concurrent_normal_cycle(ShenandoahGenerationalHeap* heap,\n-                                       ShenandoahGenerationType generation,\n-                                       GCCause::Cause cause);\n+  void set_gc_mode(GCMode new_mode);\n+  void set_gc_mode(MonitorLocker& ml, GCMode new_mode);\n+  static const char* gc_mode_name(GCMode mode);\n@@ -114,2 +138,3 @@\n-  void service_concurrent_old_cycle(ShenandoahGenerationalHeap* heap,\n-                                    GCCause::Cause &cause);\n+  \/\/ Takes the request lock and updates the requested cause and generation, then notifies the control thread.\n+  void notify_control_thread(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void notify_control_thread(MonitorLocker& ml, GCCause::Cause cause, ShenandoahGeneration* generation);\n@@ -117,1 +142,3 @@\n-  void set_gc_mode(GCMode new_mode);\n+  \/\/ Notifies the control thread, but does not update the requested cause or generation.\n+  void notify_cancellation(GCCause::Cause cause);\n+  void notify_cancellation(MonitorLocker& ml, GCCause::Cause cause);\n@@ -119,1 +146,2 @@\n-  static const char* gc_mode_name(GCMode mode);\n+  void maybe_set_aging_cycle();\n+  void check_for_request(ShenandoahGCRequest& request);\n@@ -121,1 +149,1 @@\n-  void notify_control_thread();\n+  GCMode prepare_for_allocation_failure_request(ShenandoahGCRequest &request);\n@@ -123,4 +151,1 @@\n-  void service_concurrent_cycle(ShenandoahHeap* heap,\n-                                ShenandoahGeneration* generation,\n-                                GCCause::Cause &cause,\n-                                bool do_old_gc_bootstrap);\n+  GCMode prepare_for_explicit_gc_request(ShenandoahGCRequest &request);\n@@ -128,0 +153,1 @@\n+  GCMode prepare_for_concurrent_gc_request(ShenandoahGCRequest &request);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.hpp","additions":53,"deletions":27,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -586,0 +586,1 @@\n+  _cancelled_gc.set(GCCause::_no_gc);\n@@ -2110,3 +2111,3 @@\n-bool ShenandoahHeap::try_cancel_gc() {\n-  jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);\n-  return prev == CANCELLABLE;\n+bool ShenandoahHeap::try_cancel_gc(GCCause::Cause cause) {\n+  jbyte prev = _cancelled_gc.cmpxchg(cause, GCCause::_no_gc);\n+  return prev == GCCause::_no_gc;\n@@ -2126,2 +2127,2 @@\n-void ShenandoahHeap::cancel_gc(GCCause::Cause cause) {\n-  if (try_cancel_gc()) {\n+bool ShenandoahHeap::cancel_gc(GCCause::Cause cause) {\n+  if (try_cancel_gc(cause)) {\n@@ -2129,1 +2130,1 @@\n-    log_info(gc)(\"%s\", msg.buffer());\n+    log_info(gc,thread)(\"%s\", msg.buffer());\n@@ -2132,0 +2133,1 @@\n+    return true;\n@@ -2133,0 +2135,1 @@\n+  return false;\n@@ -2145,1 +2148,1 @@\n-  \/\/ Step 0a. Stop reporting on gc thread cpu utilization\n+  \/\/ Step 1. Stop reporting on gc thread cpu utilization\n@@ -2148,9 +2151,1 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n-  \/\/ Note that we cannot do that with stop(), because stop() is blocking and waits for the actual shutdown.\n-  \/\/ Doing stop() here would wait for the normal GC cycle to complete, never falling through to cancel below.\n-  control_thread()->prepare_for_graceful_shutdown();\n-\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n-  cancel_gc(GCCause::_shenandoah_stop_vm);\n-\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 2. Wait until GC worker exits normally (this will cancel any ongoing GC).\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":11,"deletions":16,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -433,10 +433,0 @@\n-  enum CancelState {\n-    \/\/ Normal state. GC has not been cancelled and is open for cancellation.\n-    \/\/ Worker threads can suspend for safepoint.\n-    CANCELLABLE,\n-\n-    \/\/ GC has been cancelled. Worker threads can not suspend for\n-    \/\/ safepoint but must finish their work as soon as possible.\n-    CANCELLED\n-  };\n-\n@@ -444,1 +434,1 @@\n-  ShenandoahSharedEnumFlag<CancelState> _cancelled_gc;\n+  ShenandoahSharedEnumFlag<GCCause::Cause> _cancelled_gc;\n@@ -450,1 +440,1 @@\n-  bool try_cancel_gc();\n+  bool try_cancel_gc(GCCause::Cause cause);\n@@ -455,0 +445,1 @@\n+  inline GCCause::Cause cancelled_cause() const;\n@@ -459,1 +450,1 @@\n-  void cancel_gc(GCCause::Cause cause);\n+  bool cancel_gc(GCCause::Cause cause);\n@@ -461,1 +452,0 @@\n-public:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":14,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -255,1 +255,1 @@\n-  return _cancelled_gc.get() == CANCELLED;\n+  return _cancelled_gc.get() != GCCause::_no_gc;\n@@ -267,0 +267,4 @@\n+inline GCCause::Cause ShenandoahHeap::cancelled_cause() const {\n+  return _cancelled_gc.get();\n+}\n+\n@@ -268,1 +272,1 @@\n-  _cancelled_gc.set(CANCELLABLE);\n+  _cancelled_gc.set(GCCause::_no_gc);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -498,1 +498,1 @@\n-    log_debug(gc)(\"Old generation transition from %s to %s\", state_name(_state), state_name(new_state));\n+    log_debug(gc, thread)(\"Old generation transition from %s to %s\", state_name(_state), state_name(new_state));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  ConcurrentGCThread(),\n+  _heap(ShenandoahHeap::heap()),\n@@ -41,4 +41,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  _old_heuristics = heap->old_generation()->heuristics();\n-  _young_heuristics = heap->young_generation()->heuristics();\n-  _global_heuristics = heap->global_generation()->heuristics();\n+  _old_heuristics = _heap->old_generation()->heuristics();\n+  _young_heuristics = _heap->young_generation()->heuristics();\n+  _global_heuristics = _heap->global_generation()->heuristics();\n@@ -65,1 +64,1 @@\n-        if (request_concurrent_gc(GLOBAL)) {\n+        if (request_concurrent_gc(_heap->global_generation())) {\n@@ -75,1 +74,1 @@\n-          } else if (request_concurrent_gc(YOUNG)) {\n+          } else if (request_concurrent_gc(_heap->young_generation())) {\n@@ -78,0 +77,4 @@\n+        } else if (_old_heuristics->resume_old_cycle() || _old_heuristics->should_start_gc()) {\n+          if (request_concurrent_gc(_heap->old_generation())) {\n+            log_debug(gc)(\"Heuristics request to resume old collection accepted\");\n+          }\n@@ -128,2 +131,2 @@\n-bool ShenandoahRegulatorThread::start_old_cycle() {\n-  return _old_heuristics->should_start_gc() && request_concurrent_gc(OLD);\n+bool ShenandoahRegulatorThread::start_old_cycle() const {\n+  return _old_heuristics->should_start_gc() && request_concurrent_gc(_heap->old_generation());\n@@ -132,2 +135,2 @@\n-bool ShenandoahRegulatorThread::start_young_cycle() {\n-  return _young_heuristics->should_start_gc() && request_concurrent_gc(YOUNG);\n+bool ShenandoahRegulatorThread::start_young_cycle() const {\n+  return _young_heuristics->should_start_gc() && request_concurrent_gc(_heap->young_generation());\n@@ -136,2 +139,2 @@\n-bool ShenandoahRegulatorThread::start_global_cycle() {\n-  return _global_heuristics->should_start_gc() && request_concurrent_gc(GLOBAL);\n+bool ShenandoahRegulatorThread::start_global_cycle() const {\n+  return _global_heuristics->should_start_gc() && request_concurrent_gc(_heap->global_generation());\n@@ -140,1 +143,1 @@\n-bool ShenandoahRegulatorThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n+bool ShenandoahRegulatorThread::request_concurrent_gc(ShenandoahGeneration* generation) const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":17,"deletions":14,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+class ShenandoahHeap;\n@@ -30,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -31,0 +33,1 @@\n+class ShenandoahOldHeuristics;\n@@ -61,3 +64,4 @@\n-  bool start_old_cycle();\n-  bool start_young_cycle();\n-  bool start_global_cycle();\n+  bool start_old_cycle() const;\n+  bool start_young_cycle() const;\n+  bool start_global_cycle() const;\n+  bool resume_old_cycle();\n@@ -73,1 +77,1 @@\n-  bool request_concurrent_gc(ShenandoahGenerationType generation);\n+  bool request_concurrent_gc(ShenandoahGeneration* generation) const;\n@@ -75,0 +79,1 @@\n+  ShenandoahHeap* _heap;\n@@ -77,1 +82,1 @@\n-  ShenandoahHeuristics* _old_heuristics;\n+  ShenandoahOldHeuristics* _old_heuristics;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -478,1 +478,1 @@\n-  log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+  log_debug(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n@@ -656,1 +656,1 @@\n-  log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+  log_debug(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}