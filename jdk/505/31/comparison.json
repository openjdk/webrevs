{"files":[{"patch":"@@ -112,1 +112,1 @@\n-    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), false);\n+    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), ShenandoahBarrierSet::AccessKind::NORMAL);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -228,1 +228,1 @@\n-void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address load_addr, bool weak) {\n+void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address load_addr, ShenandoahBarrierSet::AccessKind kind) {\n@@ -255,1 +255,1 @@\n-  if (!weak) {\n+  if (kind == ShenandoahBarrierSet::AccessKind::NORMAL) {\n@@ -263,8 +263,20 @@\n-  if (weak) {\n-    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n-  } else {\n-    if (UseCompressedOops) {\n-      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n-    } else {\n-      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n-    }\n+  switch (kind) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      if (UseCompressedOops) {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n+      } else {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      if (UseCompressedOops) {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow));\n+      } else {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -329,2 +341,2 @@\n-    bool weak = ShenandoahBarrierSet::use_load_reference_barrier_weak(decorators, type);\n-    load_reference_barrier(masm, dst, src, weak);\n+    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n+    load_reference_barrier(masm, dst, src, kind);\n@@ -644,4 +656,12 @@\n-  if (stub->is_weak()) {\n-    __ far_call(RuntimeAddress(bs->load_reference_barrier_weak_rt_code_blob()->code_begin()));\n-  } else {\n-    __ far_call(RuntimeAddress(bs->load_reference_barrier_rt_code_blob()->code_begin()));\n+  switch (stub->kind()) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      __ far_call(RuntimeAddress(bs->load_reference_barrier_normal_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      __ far_call(RuntimeAddress(bs->load_reference_barrier_weak_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ far_call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -703,1 +723,1 @@\n-void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_weak) {\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::AccessKind kind) {\n@@ -710,6 +730,20 @@\n-  if (is_weak) {\n-    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n-  } else if (UseCompressedOops) {\n-    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n-  } else {\n-    __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+  switch (kind) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      if (UseCompressedOops) {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow));\n+      } else {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier));\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      if (UseCompressedOops) {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow));\n+      } else {\n+        __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ mov(lr, CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n+      break;\n+   default:\n+      ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":57,"deletions":23,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -58,1 +59,1 @@\n-  void load_reference_barrier(MacroAssembler* masm, Register dst, Address load_addr, bool weak);\n+  void load_reference_barrier(MacroAssembler* masm, Register dst, Address load_addr, ShenandoahBarrierSet::AccessKind kind);\n@@ -68,1 +69,1 @@\n-  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_weak);\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::AccessKind kind);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), false);\n+    result = load_reference_barrier(access.gen(), result, LIR_OprFact::addressConst(0), ShenandoahBarrierSet::AccessKind::NORMAL);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -271,1 +271,1 @@\n-void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address src, bool weak) {\n+void ShenandoahBarrierSetAssembler::load_reference_barrier(MacroAssembler* masm, Register dst, Address src, ShenandoahBarrierSet::AccessKind kind) {\n@@ -295,1 +295,1 @@\n-  if (!weak) {\n+  if (kind == ShenandoahBarrierSet::AccessKind::NORMAL) {\n@@ -341,8 +341,20 @@\n-  if (weak) {\n-    __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), dst, tmp2);\n-  } else {\n-    if (UseCompressedOops) {\n-      __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), dst, tmp2);\n-    } else {\n-      __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), dst, tmp2);\n-    }\n+  switch (kind) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      if (UseCompressedOops) {\n+        __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), dst, tmp2);\n+      } else {\n+        __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), dst, tmp2);\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      if (UseCompressedOops) {\n+        __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow), dst, tmp2);\n+      } else {\n+        __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), dst, tmp2);\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), dst, tmp2);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -373,1 +385,1 @@\n-  if  (!weak) {\n+  if  (kind == ShenandoahBarrierSet::AccessKind::NORMAL) {\n@@ -470,2 +482,2 @@\n-    bool weak = ShenandoahBarrierSet::use_load_reference_barrier_weak(decorators, type);\n-    load_reference_barrier(masm, dst, src, weak);\n+    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n+    load_reference_barrier(masm, dst, src, kind);\n@@ -821,4 +833,12 @@\n-  if (stub->is_weak()) {\n-    __ call(RuntimeAddress(bs->load_reference_barrier_weak_rt_code_blob()->code_begin()));\n-  } else {\n-    __ call(RuntimeAddress(bs->load_reference_barrier_rt_code_blob()->code_begin()));\n+  switch (stub->kind()) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      __ call(RuntimeAddress(bs->load_reference_barrier_normal_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      __ call(RuntimeAddress(bs->load_reference_barrier_weak_rt_code_blob()->code_begin()));\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ call(RuntimeAddress(bs->load_reference_barrier_native_rt_code_blob()->code_begin()));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -889,1 +909,1 @@\n-void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_weak) {\n+void ShenandoahBarrierSetAssembler::generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::AccessKind kind) {\n@@ -898,6 +918,20 @@\n-  if (is_weak) {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), c_rarg0, c_rarg1);\n-  } else if (UseCompressedOops) {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), c_rarg0, c_rarg1);\n-  } else {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), c_rarg0, c_rarg1);\n+  switch (kind) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      if (UseCompressedOops) {\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow), c_rarg0, c_rarg1);\n+      } else {\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), c_rarg0, c_rarg1);\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      if (UseCompressedOops) {\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow), c_rarg0, c_rarg1);\n+      } else {\n+        __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), c_rarg0, c_rarg1);\n+      }\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), c_rarg0, c_rarg1);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n@@ -908,4 +942,10 @@\n-  if (is_weak) {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), rax, rbx);\n-  } else {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), rax, rbx);\n+  switch (kind) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier), rax, rbx);\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), rax, rbx);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":68,"deletions":28,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n+\n@@ -65,1 +67,1 @@\n-  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, bool is_weak);\n+  void generate_c1_load_reference_barrier_runtime_stub(StubAssembler* sasm, ShenandoahBarrierSet::AccessKind kind);\n@@ -68,1 +70,1 @@\n-  void load_reference_barrier(MacroAssembler* masm, Register dst, Address src, bool weak);\n+  void load_reference_barrier(MacroAssembler* masm, Register dst, Address src, ShenandoahBarrierSet::AccessKind kind);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -54,1 +54,3 @@\n-  _load_reference_barrier_rt_code_blob(NULL) {}\n+  _load_reference_barrier_normal_rt_code_blob(NULL),\n+  _load_reference_barrier_native_rt_code_blob(NULL),\n+  _load_reference_barrier_weak_rt_code_blob(NULL) {}\n@@ -110,1 +112,1 @@\n-LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native) {\n+LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::AccessKind kind) {\n@@ -112,1 +114,1 @@\n-    return load_reference_barrier_impl(gen, obj, addr, is_native);\n+    return load_reference_barrier_impl(gen, obj, addr, kind);\n@@ -118,1 +120,1 @@\n-LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native) {\n+LIR_Opr ShenandoahBarrierSetC1::load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::AccessKind kind) {\n@@ -151,1 +153,1 @@\n-  CodeStub* slow = new ShenandoahLoadReferenceBarrierStub(obj, addr, result, tmp1, tmp2, is_native);\n+  CodeStub* slow = new ShenandoahLoadReferenceBarrierStub(obj, addr, result, tmp1, tmp2, kind);\n@@ -214,2 +216,2 @@\n-    bool is_weak = ShenandoahBarrierSet::use_load_reference_barrier_weak(decorators, type);\n-    tmp = load_reference_barrier(gen, tmp, access.resolved_addr(), is_weak);\n+    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n+    tmp = load_reference_barrier(gen, tmp, access.resolved_addr(), kind);\n@@ -254,1 +256,1 @@\n-  const bool _is_weak;\n+  const ShenandoahBarrierSet::AccessKind _kind;\n@@ -257,1 +259,1 @@\n-  C1ShenandoahLoadReferenceBarrierCodeGenClosure(bool is_weak) : _is_weak(is_weak) {}\n+  C1ShenandoahLoadReferenceBarrierCodeGenClosure(ShenandoahBarrierSet::AccessKind kind) : _kind(kind) {}\n@@ -261,1 +263,1 @@\n-    bs->generate_c1_load_reference_barrier_runtime_stub(sasm, _is_weak);\n+    bs->generate_c1_load_reference_barrier_runtime_stub(sasm, _kind);\n@@ -272,2 +274,2 @@\n-    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_code_gen_cl(false);\n-    _load_reference_barrier_rt_code_blob = Runtime1::generate_blob(buffer_blob, -1,\n+    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_code_gen_cl(ShenandoahBarrierSet::AccessKind::NORMAL);\n+    _load_reference_barrier_normal_rt_code_blob = Runtime1::generate_blob(buffer_blob, -1,\n@@ -277,1 +279,6 @@\n-    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_weak_code_gen_cl(true);\n+    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_native_code_gen_cl(ShenandoahBarrierSet::AccessKind::NATIVE);\n+    _load_reference_barrier_native_rt_code_blob = Runtime1::generate_blob(buffer_blob, -1,\n+                                                                          \"shenandoah_load_reference_barrier_native_slow\",\n+                                                                          false, &lrb_native_code_gen_cl);\n+\n+    C1ShenandoahLoadReferenceBarrierCodeGenClosure lrb_weak_code_gen_cl(ShenandoahBarrierSet::AccessKind::WEAK);\n@@ -279,2 +286,2 @@\n-                                                                   \"shenandoah_load_reference_barrier_weak_slow\",\n-                                                                   false, &lrb_weak_code_gen_cl);\n+                                                                           \"shenandoah_load_reference_barrier_weak_slow\",\n+                                                                           false, &lrb_weak_code_gen_cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.cpp","additions":22,"deletions":15,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  bool _is_weak;\n+  ShenandoahBarrierSet::AccessKind _kind;\n@@ -99,2 +99,2 @@\n-  ShenandoahLoadReferenceBarrierStub(LIR_Opr obj, LIR_Opr addr, LIR_Opr result, LIR_Opr tmp1, LIR_Opr tmp2, bool is_weak) :\n-          _obj(obj), _addr(addr), _result(result), _tmp1(tmp1), _tmp2(tmp2), _is_weak(is_weak)\n+  ShenandoahLoadReferenceBarrierStub(LIR_Opr obj, LIR_Opr addr, LIR_Opr result, LIR_Opr tmp1, LIR_Opr tmp2, ShenandoahBarrierSet::AccessKind kind) :\n+          _obj(obj), _addr(addr), _result(result), _tmp1(tmp1), _tmp2(tmp2), _kind(kind)\n@@ -114,1 +114,1 @@\n-  bool is_weak() const { return _is_weak; }\n+  ShenandoahBarrierSet::AccessKind kind() const { return _kind; }\n@@ -193,1 +193,2 @@\n-  CodeBlob* _load_reference_barrier_rt_code_blob;\n+  CodeBlob* _load_reference_barrier_normal_rt_code_blob;\n+  CodeBlob* _load_reference_barrier_native_rt_code_blob;\n@@ -198,1 +199,1 @@\n-  LIR_Opr load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native);\n+  LIR_Opr load_reference_barrier(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::AccessKind kind);\n@@ -201,1 +202,1 @@\n-  LIR_Opr load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, bool is_native);\n+  LIR_Opr load_reference_barrier_impl(LIRGenerator* gen, LIR_Opr obj, LIR_Opr addr, ShenandoahBarrierSet::AccessKind kind);\n@@ -213,3 +214,8 @@\n-  CodeBlob* load_reference_barrier_rt_code_blob() {\n-    assert(_load_reference_barrier_rt_code_blob != NULL, \"\");\n-    return _load_reference_barrier_rt_code_blob;\n+  CodeBlob* load_reference_barrier_normal_rt_code_blob() {\n+    assert(_load_reference_barrier_normal_rt_code_blob != NULL, \"\");\n+    return _load_reference_barrier_normal_rt_code_blob;\n+  }\n+\n+  CodeBlob* load_reference_barrier_native_rt_code_blob() {\n+    assert(_load_reference_barrier_native_rt_code_blob != NULL, \"\");\n+    return _load_reference_barrier_native_rt_code_blob;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp","additions":16,"deletions":10,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -308,1 +308,2 @@\n-         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak));\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak)) ||\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow));\n@@ -548,3 +549,2 @@\n-    load = new ShenandoahLoadReferenceBarrierNode(NULL,\n-                                                  load,\n-                                                  ShenandoahBarrierSet::use_load_reference_barrier_weak(decorators, type));\n+    ShenandoahBarrierSet::AccessKind kind = ShenandoahBarrierSet::access_kind(decorators, type);\n+    load = new ShenandoahLoadReferenceBarrierNode(NULL, load, kind);\n@@ -647,1 +647,1 @@\n-    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, false));\n+    load_store = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, load_store, ShenandoahBarrierSet::AccessKind::NORMAL));\n@@ -715,1 +715,1 @@\n-    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, false));\n+    result = kit->gvn().transform(new ShenandoahLoadReferenceBarrierNode(NULL, result, ShenandoahBarrierSet::AccessKind::NORMAL));\n@@ -1063,1 +1063,1 @@\n-    \/\/ If one input is NULL, then step over the barriers (except LRB native) on the other input\n+    \/\/ If one input is NULL, then step over the barriers normal LRB barriers on the other input\n@@ -1066,1 +1066,1 @@\n-          ((ShenandoahLoadReferenceBarrierNode*)in2)->is_weak())) {\n+          ((ShenandoahLoadReferenceBarrierNode*)in2)->kind() != ShenandoahBarrierSet::AccessKind::NORMAL)) {\n@@ -1071,1 +1071,1 @@\n-          ((ShenandoahLoadReferenceBarrierNode*)in1)->is_weak())) {\n+          ((ShenandoahLoadReferenceBarrierNode*)in1)->kind() != ShenandoahBarrierSet::AccessKind::NORMAL)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -959,1 +959,2 @@\n-void ShenandoahBarrierC2Support::call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem, bool is_weak, PhaseIdealLoop* phase) {\n+void ShenandoahBarrierC2Support::call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem,\n+                                               ShenandoahBarrierSet::AccessKind kind, PhaseIdealLoop* phase) {\n@@ -970,7 +971,22 @@\n-  address target = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n-          CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow) :\n-          CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier);\n-\n-  address calladdr = is_weak ? CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak)\n-                             : target;\n-  const char* name = is_weak ? \"load_reference_barrier_native\" : \"load_reference_barrier\";\n+  address calladdr = NULL;\n+  const char* name = NULL;\n+  switch (kind) {\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      calladdr = CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n+      name = \"load_reference_barrier_native\";\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      calladdr = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak_narrow) :\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak);\n+      name = \"load_reference_barrier_weak\";\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      calladdr = LP64_ONLY(UseCompressedOops) NOT_LP64(false) ?\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_narrow) :\n+                 CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier);\n+      name = \"load_reference_barrier\";\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -1341,1 +1357,1 @@\n-    if (!lrb->is_weak()) {\n+    if (lrb->kind() == ShenandoahBarrierSet::AccessKind::NORMAL) {\n@@ -1392,1 +1408,1 @@\n-    call_lrb_stub(ctrl, val, addr, result_mem, raw_mem, lrb->is_weak(), phase);\n+    call_lrb_stub(ctrl, val, addr, result_mem, raw_mem, lrb->kind(), phase);\n@@ -2888,2 +2904,2 @@\n-ShenandoahLoadReferenceBarrierNode::ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* obj, bool weak)\n-: Node(ctrl, obj), _weak(weak) {\n+ShenandoahLoadReferenceBarrierNode::ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* obj, ShenandoahBarrierSet::AccessKind kind)\n+: Node(ctrl, obj), _kind(kind) {\n@@ -2893,2 +2909,2 @@\n-bool ShenandoahLoadReferenceBarrierNode::is_weak() const {\n-  return _weak;\n+ShenandoahBarrierSet::AccessKind ShenandoahLoadReferenceBarrierNode::kind() const {\n+  return _kind;\n@@ -2902,1 +2918,15 @@\n-  return Node::hash() + (_weak ? 1 : 0);\n+  uint hash = Node::hash();\n+  switch (_kind) {\n+    case ShenandoahBarrierSet::AccessKind::NORMAL:\n+      hash += 0;\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::WEAK:\n+      hash += 1;\n+      break;\n+    case ShenandoahBarrierSet::AccessKind::NATIVE:\n+      hash += 2;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  return hash;\n@@ -2907,1 +2937,1 @@\n-         _weak == ((const ShenandoahLoadReferenceBarrierNode&)n)._weak;\n+         _kind == ((const ShenandoahLoadReferenceBarrierNode&)n)._kind;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":46,"deletions":16,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -63,1 +64,2 @@\n-  static void call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem, bool is_weak, PhaseIdealLoop* phase);\n+  static void call_lrb_stub(Node*& ctrl, Node*& val, Node* load_addr, Node*& result_mem, Node* raw_mem,\n+                            ShenandoahBarrierSet::AccessKind kind, PhaseIdealLoop* phase);\n@@ -232,1 +234,1 @@\n-  bool _weak;\n+  ShenandoahBarrierSet::AccessKind _kind;\n@@ -235,1 +237,1 @@\n-  ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* val, bool native);\n+  ShenandoahLoadReferenceBarrierNode(Node* ctrl, Node* val, ShenandoahBarrierSet::AccessKind kind);\n@@ -237,1 +239,1 @@\n-  bool is_weak() const;\n+  ShenandoahBarrierSet::AccessKind kind() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -64,6 +64,0 @@\n-bool ShenandoahAggressiveHeuristics::should_process_references() {\n-  if (!can_process_references()) return false;\n-  \/\/ Randomly process refs with 50% chance.\n-  return (os::random() & 1) == 1;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -40,2 +40,0 @@\n-  virtual bool should_process_references();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -260,12 +260,0 @@\n-bool ShenandoahHeuristics::can_process_references() {\n-  if (ShenandoahRefProcFrequency == 0) return false;\n-  return true;\n-}\n-\n-bool ShenandoahHeuristics::should_process_references() {\n-  if (!can_process_references()) return false;\n-  size_t cycle = ShenandoahHeap::heap()->shenandoah_policy()->cycle_counter();\n-  \/\/ Process references every Nth GC cycle.\n-  return cycle % ShenandoahRefProcFrequency == 0;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -123,3 +123,0 @@\n-  virtual bool can_process_references();\n-  virtual bool should_process_references();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,5 +39,0 @@\n-bool ShenandoahPassiveHeuristics::should_process_references() {\n-  \/\/ Always process references, if we can.\n-  return can_process_references();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-  virtual bool should_process_references();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,1 +71,2 @@\n-  msg.append(\"    %3s marked \\n\",                    ctx->is_marked(obj) ? \"\" : \"not\");\n+  msg.append(\"    %3s marked strong\\n\",              ctx->is_marked_strong(obj) ? \"\" : \"not\");\n+  msg.append(\"    %3s marked weak\\n\",                ctx->is_marked_weak(obj) ? \"\" : \"not\");\n@@ -356,18 +357,0 @@\n-void ShenandoahAsserts::assert_rp_isalive_not_installed(const char *file, int line) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ReferenceProcessor* rp = heap->ref_processor();\n-  if (rp->is_alive_non_header() != NULL) {\n-    print_rp_failure(\"Shenandoah assert_rp_isalive_not_installed failed\", rp->is_alive_non_header(),\n-                     file, line);\n-  }\n-}\n-\n-void ShenandoahAsserts::assert_rp_isalive_installed(const char *file, int line) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ReferenceProcessor* rp = heap->ref_processor();\n-  if (rp->is_alive_non_header() == NULL) {\n-    print_rp_failure(\"Shenandoah assert_rp_isalive_installed failed\", rp->is_alive_non_header(),\n-                     file, line);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":2,"deletions":19,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -68,3 +68,0 @@\n-  static void assert_rp_isalive_not_installed(const char *file, int line);\n-  static void assert_rp_isalive_installed(const char *file, int line);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -90,11 +90,0 @@\n-bool ShenandoahBarrierSet::use_load_reference_barrier_weak(DecoratorSet decorators, BasicType type) {\n-  assert(need_load_reference_barrier(decorators, type), \"Should be subset of LRB\");\n-  assert(is_reference_type(type), \"Why we here?\");\n-  \/\/ Native load reference barrier is only needed for concurrent root processing\n-  if (!ShenandoahConcurrentRoots::can_do_concurrent_roots()) {\n-    return false;\n-  }\n-\n-  return ((decorators & IN_NATIVE) != 0) && ((decorators & ON_STRONG_OOP_REF) == 0);\n-}\n-\n@@ -112,0 +101,10 @@\n+ShenandoahBarrierSet::AccessKind ShenandoahBarrierSet::access_kind(DecoratorSet decorators, BasicType type) {\n+  if ((decorators & IN_NATIVE) != 0) {\n+    return AccessKind::NATIVE;\n+  } else if ((decorators & (ON_WEAK_OOP_REF | ON_PHANTOM_OOP_REF | ON_UNKNOWN_OOP_REF)) != 0) {\n+    return AccessKind::WEAK;\n+  } else {\n+    return AccessKind::NORMAL;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":10,"deletions":11,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -36,1 +36,7 @@\n-private:\n+public:\n+  enum class AccessKind {\n+    \/\/ Regular in-heap access on reference fields\n+    NORMAL,\n+\n+    \/\/ Off-heap reference access\n+    NATIVE,\n@@ -38,0 +44,5 @@\n+    \/\/ In-heap reference access on referent fields of j.l.r.Reference objects\n+    WEAK\n+  };\n+\n+private:\n@@ -56,1 +67,0 @@\n-  static bool use_load_reference_barrier_weak(DecoratorSet decorators, BasicType type);\n@@ -58,0 +68,1 @@\n+  static AccessKind access_kind(DecoratorSet decorators, BasicType type);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -201,1 +201,1 @@\n-  value = bs->load_reference_barrier(value);\n+  value = bs->load_reference_barrier<decorators, T>(value, addr);\n@@ -210,3 +210,3 @@\n-  value = bs->load_reference_barrier(value);\n-  bs->keep_alive_if_weak(AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset),\n-                         value);\n+  DecoratorSet resolved_decorators = AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+  value = bs->load_reference_barrier<decorators>(value, AccessInternal::oop_field_addr<decorators>(base, offset));\n+  bs->keep_alive_if_weak(resolved_decorators, value);\n@@ -220,0 +220,1 @@\n+  shenandoah_assert_not_in_cset_if(addr, value, value != NULL && !ShenandoahHeap::heap()->cancelled_gc());\n@@ -342,1 +343,1 @@\n-      if (ENQUEUE && !ctx->is_marked(obj)) {\n+      if (ENQUEUE && !ctx->is_marked_strong(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-#include \"gc\/shared\/referenceProcessor.hpp\"\n-#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n@@ -43,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -64,1 +63,1 @@\n-    ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, NO_DEDUP>(p, _heap, _queue, _mark_context);\n+    ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, NO_DEDUP>(p, _heap, _queue, _mark_context, false);\n@@ -77,1 +76,1 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -81,1 +80,2 @@\n-  _mark_context(_heap->marking_context())\n+  _mark_context(_heap->marking_context()),\n+  _weak(false)\n@@ -156,8 +156,2 @@\n-    ReferenceProcessor* rp;\n-    if (heap->process_references()) {\n-      rp = heap->ref_processor();\n-      shenandoah_assert_rp_isalive_installed();\n-    } else {\n-      rp = NULL;\n-    }\n-\n+    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    assert(rp != NULL, \"need reference processor\");\n@@ -209,1 +203,1 @@\n-  ReferenceProcessor*             _rp;\n+  ShenandoahReferenceProcessor*   _rp;\n@@ -225,6 +219,1 @@\n-  _rp(NULL) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (heap->process_references()) {\n-    _rp = heap->ref_processor();\n-    shenandoah_assert_rp_isalive_installed();\n-  }\n+  _rp(ShenandoahHeap::heap()->ref_processor()) {\n@@ -256,7 +245,1 @@\n-    ReferenceProcessor* rp;\n-    if (heap->process_references()) {\n-      rp = heap->ref_processor();\n-      shenandoah_assert_rp_isalive_installed();\n-    } else {\n-      rp = NULL;\n-    }\n+    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -306,1 +289,0 @@\n-\n@@ -309,0 +291,4 @@\n+  ShenandoahReferenceProcessor* ref_processor = heap->ref_processor();\n+  ref_processor->reset_thread_locals();\n+  ref_processor->set_soft_reference_policy(_heap->soft_ref_policy()->should_clear_all_soft_refs());\n+\n@@ -410,1 +396,1 @@\n-  SuspendibleThreadSetJoiner         _sts_joiner;\n+  SuspendibleThreadSetJoiner          _sts_joiner;\n@@ -412,2 +398,2 @@\n-  ShenandoahObjToScanQueueSet* const _queue_set;\n-  ReferenceProcessor* const          _rp;\n+  ShenandoahObjToScanQueueSet* const  _queue_set;\n+  ShenandoahReferenceProcessor* const _rp;\n@@ -417,1 +403,1 @@\n-                                    ReferenceProcessor* rp,\n+                                    ShenandoahReferenceProcessor* rp,\n@@ -424,1 +410,1 @@\n-                                                                     ReferenceProcessor* rp,\n+                                                                     ShenandoahReferenceProcessor* rp,\n@@ -445,13 +431,1 @@\n-  ReferenceProcessor* rp = NULL;\n-  if (_heap->process_references()) {\n-    rp = _heap->ref_processor();\n-    rp->set_active_mt_degree(nworkers);\n-\n-    \/\/ enable (\"weak\") refs discovery\n-    rp->enable_discovery(true \/*verify_no_refs*\/);\n-    rp->setup_policy(_heap->soft_ref_policy()->should_clear_all_soft_refs());\n-  }\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-  ShenandoahIsAliveSelector is_alive;\n-  ReferenceProcessorIsAliveMutator fix_isalive(_heap->ref_processor(), is_alive.is_alive_closure());\n+  ShenandoahReferenceProcessor* rp = _heap->ref_processor();\n@@ -483,4 +457,0 @@\n-    shenandoah_assert_rp_isalive_not_installed();\n-    ShenandoahIsAliveSelector is_alive;\n-    ReferenceProcessorIsAliveMutator fix_isalive(_heap->ref_processor(), is_alive.is_alive_closure());\n-\n@@ -527,5 +497,0 @@\n-  \/\/ When we're done marking everything, we process weak references.\n-  if (_heap->process_references()) {\n-    weak_refs_work(full_gc);\n-  }\n-\n@@ -537,327 +502,0 @@\n-\/\/ Weak Reference Closures\n-class ShenandoahCMDrainMarkingStackClosure: public VoidClosure {\n-  uint _worker_id;\n-  TaskTerminator* _terminator;\n-  bool _reset_terminator;\n-\n-public:\n-  ShenandoahCMDrainMarkingStackClosure(uint worker_id, TaskTerminator* t, bool reset_terminator = false):\n-    _worker_id(worker_id),\n-    _terminator(t),\n-    _reset_terminator(reset_terminator) {\n-  }\n-\n-  void do_void() {\n-    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-\n-    ShenandoahHeap* sh = ShenandoahHeap::heap();\n-    ShenandoahConcurrentMark* scm = sh->concurrent_mark();\n-    assert(sh->process_references(), \"why else would we be here?\");\n-    ReferenceProcessor* rp = sh->ref_processor();\n-\n-    shenandoah_assert_rp_isalive_installed();\n-\n-    scm->mark_loop(_worker_id, _terminator, rp,\n-                   false,   \/\/ not cancellable\n-                   false);  \/\/ do not do strdedup\n-\n-    if (_reset_terminator) {\n-      _terminator->reset_for_reuse();\n-    }\n-  }\n-};\n-\n-class ShenandoahCMKeepAliveClosure : public OopClosure {\n-private:\n-  ShenandoahObjToScanQueue* _queue;\n-  ShenandoahHeap* _heap;\n-  ShenandoahMarkingContext* const _mark_context;\n-\n-  template <class T>\n-  inline void do_oop_work(T* p) {\n-    ShenandoahConcurrentMark::mark_through_ref<T, NONE, NO_DEDUP>(p, _heap, _queue, _mark_context);\n-  }\n-\n-public:\n-  ShenandoahCMKeepAliveClosure(ShenandoahObjToScanQueue* q) :\n-    _queue(q),\n-    _heap(ShenandoahHeap::heap()),\n-    _mark_context(_heap->marking_context()) {}\n-\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-};\n-\n-class ShenandoahCMKeepAliveUpdateClosure : public OopClosure {\n-private:\n-  ShenandoahObjToScanQueue* _queue;\n-  ShenandoahHeap* _heap;\n-  ShenandoahMarkingContext* const _mark_context;\n-\n-  template <class T>\n-  inline void do_oop_work(T* p) {\n-    ShenandoahConcurrentMark::mark_through_ref<T, SIMPLE, NO_DEDUP>(p, _heap, _queue, _mark_context);\n-  }\n-\n-public:\n-  ShenandoahCMKeepAliveUpdateClosure(ShenandoahObjToScanQueue* q) :\n-    _queue(q),\n-    _heap(ShenandoahHeap::heap()),\n-    _mark_context(_heap->marking_context()) {}\n-\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-};\n-\n-class ShenandoahWeakUpdateClosure : public OopClosure {\n-private:\n-  ShenandoahHeap* const _heap;\n-\n-  template <class T>\n-  inline void do_oop_work(T* p) {\n-    oop o = _heap->maybe_update_with_forwarded(p);\n-    shenandoah_assert_marked_except(p, o, o == NULL);\n-  }\n-\n-public:\n-  ShenandoahWeakUpdateClosure() : _heap(ShenandoahHeap::heap()) {}\n-\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-};\n-\n-class ShenandoahRefProcTaskProxy : public AbstractGangTask {\n-private:\n-  AbstractRefProcTaskExecutor::ProcessTask& _proc_task;\n-  TaskTerminator* _terminator;\n-\n-public:\n-  ShenandoahRefProcTaskProxy(AbstractRefProcTaskExecutor::ProcessTask& proc_task,\n-                             TaskTerminator* t) :\n-    AbstractGangTask(\"Shenandoah Process Weak References\"),\n-    _proc_task(proc_task),\n-    _terminator(t) {\n-  }\n-\n-  void work(uint worker_id) {\n-    Thread* current_thread = Thread::current();\n-    ResourceMark rm(current_thread);\n-    HandleMark hm(current_thread);\n-    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahCMDrainMarkingStackClosure complete_gc(worker_id, _terminator);\n-    if (heap->has_forwarded_objects()) {\n-      ShenandoahForwardedIsAliveClosure is_alive;\n-      ShenandoahCMKeepAliveUpdateClosure keep_alive(heap->concurrent_mark()->get_queue(worker_id));\n-      _proc_task.work(worker_id, is_alive, keep_alive, complete_gc);\n-    } else {\n-      ShenandoahIsAliveClosure is_alive;\n-      ShenandoahCMKeepAliveClosure keep_alive(heap->concurrent_mark()->get_queue(worker_id));\n-      _proc_task.work(worker_id, is_alive, keep_alive, complete_gc);\n-    }\n-  }\n-};\n-\n-class ShenandoahRefProcTaskExecutor : public AbstractRefProcTaskExecutor {\n-private:\n-  WorkGang* _workers;\n-\n-public:\n-  ShenandoahRefProcTaskExecutor(WorkGang* workers) :\n-    _workers(workers) {\n-  }\n-\n-  \/\/ Executes a task using worker threads.\n-  void execute(ProcessTask& task, uint ergo_workers) {\n-    assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at a safepoint\");\n-\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahConcurrentMark* cm = heap->concurrent_mark();\n-    ShenandoahPushWorkerQueuesScope scope(_workers, cm->task_queues(),\n-                                          ergo_workers,\n-                                          \/* do_check = *\/ false);\n-    uint nworkers = _workers->active_workers();\n-    cm->task_queues()->reserve(nworkers);\n-    TaskTerminator terminator(nworkers, cm->task_queues());\n-    ShenandoahRefProcTaskProxy proc_task_proxy(task, &terminator);\n-    _workers->run_task(&proc_task_proxy);\n-  }\n-};\n-\n-void ShenandoahConcurrentMark::weak_refs_work(bool full_gc) {\n-  assert(_heap->process_references(), \"sanity\");\n-\n-  ShenandoahPhaseTimings::Phase phase_root =\n-          full_gc ?\n-          ShenandoahPhaseTimings::full_gc_weakrefs :\n-          ShenandoahPhaseTimings::weakrefs;\n-\n-  ShenandoahGCPhase phase(phase_root);\n-\n-  ReferenceProcessor* rp = _heap->ref_processor();\n-\n-  \/\/ NOTE: We cannot shortcut on has_discovered_references() here, because\n-  \/\/ we will miss marking JNI Weak refs then, see implementation in\n-  \/\/ ReferenceProcessor::process_discovered_references.\n-  weak_refs_work_doit(full_gc);\n-\n-  rp->verify_no_references_recorded();\n-  assert(!rp->discovery_enabled(), \"Post condition\");\n-\n-}\n-\n-void ShenandoahConcurrentMark::weak_refs_work_doit(bool full_gc) {\n-  ReferenceProcessor* rp = _heap->ref_processor();\n-\n-  ShenandoahPhaseTimings::Phase phase_process =\n-          full_gc ?\n-          ShenandoahPhaseTimings::full_gc_weakrefs_process :\n-          ShenandoahPhaseTimings::weakrefs_process;\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-  ShenandoahIsAliveSelector is_alive;\n-  ReferenceProcessorIsAliveMutator fix_isalive(rp, is_alive.is_alive_closure());\n-\n-  WorkGang* workers = _heap->workers();\n-  uint nworkers = workers->active_workers();\n-\n-  rp->setup_policy(_heap->soft_ref_policy()->should_clear_all_soft_refs());\n-  rp->set_active_mt_degree(nworkers);\n-\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-\n-  \/\/ complete_gc and keep_alive closures instantiated here are only needed for\n-  \/\/ single-threaded path in RP. They share the queue 0 for tracking work, which\n-  \/\/ simplifies implementation. Since RP may decide to call complete_gc several\n-  \/\/ times, we need to be able to reuse the terminator.\n-  uint serial_worker_id = 0;\n-  TaskTerminator terminator(1, task_queues());\n-  ShenandoahCMDrainMarkingStackClosure complete_gc(serial_worker_id, &terminator, \/* reset_terminator = *\/ true);\n-\n-  ShenandoahRefProcTaskExecutor executor(workers);\n-\n-  ReferenceProcessorPhaseTimes pt(_heap->gc_timer(), rp->num_queues());\n-\n-  {\n-    \/\/ Note: Don't emit JFR event for this phase, to avoid overflow nesting phase level.\n-    \/\/ Reference Processor emits 2 levels JFR event, that can get us over the JFR\n-    \/\/ event nesting level limits, in case of degenerated GC gets upgraded to\n-    \/\/ full GC.\n-    ShenandoahTimingsTracker phase_timing(phase_process);\n-\n-    if (_heap->has_forwarded_objects()) {\n-      ShenandoahCMKeepAliveUpdateClosure keep_alive(get_queue(serial_worker_id));\n-      const ReferenceProcessorStats& stats =\n-        rp->process_discovered_references(is_alive.is_alive_closure(), &keep_alive,\n-                                          &complete_gc, &executor,\n-                                          &pt);\n-       _heap->tracer()->report_gc_reference_stats(stats);\n-    } else {\n-      ShenandoahCMKeepAliveClosure keep_alive(get_queue(serial_worker_id));\n-      const ReferenceProcessorStats& stats =\n-        rp->process_discovered_references(is_alive.is_alive_closure(), &keep_alive,\n-                                          &complete_gc, &executor,\n-                                          &pt);\n-      _heap->tracer()->report_gc_reference_stats(stats);\n-    }\n-\n-    pt.print_all_references();\n-\n-    assert(task_queues()->is_empty(), \"Should be empty\");\n-  }\n-}\n-\n-class ShenandoahCancelledGCYieldClosure : public YieldClosure {\n-private:\n-  ShenandoahHeap* const _heap;\n-public:\n-  ShenandoahCancelledGCYieldClosure() : _heap(ShenandoahHeap::heap()) {};\n-  virtual bool should_return() { return _heap->cancelled_gc(); }\n-};\n-\n-class ShenandoahPrecleanCompleteGCClosure : public VoidClosure {\n-public:\n-  void do_void() {\n-    ShenandoahHeap* sh = ShenandoahHeap::heap();\n-    ShenandoahConcurrentMark* scm = sh->concurrent_mark();\n-    assert(sh->process_references(), \"why else would we be here?\");\n-    TaskTerminator terminator(1, scm->task_queues());\n-\n-    ReferenceProcessor* rp = sh->ref_processor();\n-    shenandoah_assert_rp_isalive_installed();\n-\n-    scm->mark_loop(0, &terminator, rp,\n-                   false, \/\/ not cancellable\n-                   false); \/\/ do not do strdedup\n-  }\n-};\n-\n-class ShenandoahPrecleanTask : public AbstractGangTask {\n-private:\n-  ReferenceProcessor* _rp;\n-\n-public:\n-  ShenandoahPrecleanTask(ReferenceProcessor* rp) :\n-          AbstractGangTask(\"Shenandoah Precleaning\"),\n-          _rp(rp) {}\n-\n-  void work(uint worker_id) {\n-    assert(worker_id == 0, \"The code below is single-threaded, only one worker is expected\");\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-\n-    ShenandoahHeap* sh = ShenandoahHeap::heap();\n-    assert(!sh->has_forwarded_objects(), \"No forwarded objects expected here\");\n-\n-    ShenandoahObjToScanQueue* q = sh->concurrent_mark()->get_queue(worker_id);\n-\n-    ShenandoahCancelledGCYieldClosure yield;\n-    ShenandoahPrecleanCompleteGCClosure complete_gc;\n-\n-    ShenandoahIsAliveClosure is_alive;\n-    ShenandoahCMKeepAliveClosure keep_alive(q);\n-    ResourceMark rm;\n-    _rp->preclean_discovered_references(&is_alive, &keep_alive,\n-                                        &complete_gc, &yield,\n-                                        NULL);\n-  }\n-};\n-\n-void ShenandoahConcurrentMark::preclean_weak_refs() {\n-  \/\/ Pre-cleaning weak references before diving into STW makes sense at the\n-  \/\/ end of concurrent mark. This will filter out the references which referents\n-  \/\/ are alive. Note that ReferenceProcessor already filters out these on reference\n-  \/\/ discovery, and the bulk of work is done here. This phase processes leftovers\n-  \/\/ that missed the initial filtering, i.e. when referent was marked alive after\n-  \/\/ reference was discovered by RP.\n-\n-  assert(_heap->process_references(), \"sanity\");\n-\n-  \/\/ Shortcut if no references were discovered to avoid winding up threads.\n-  ReferenceProcessor* rp = _heap->ref_processor();\n-  if (!rp->has_discovered_references()) {\n-    return;\n-  }\n-\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-\n-  ReferenceProcessorMTDiscoveryMutator fix_mt_discovery(rp, false);\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-  ShenandoahIsAliveSelector is_alive;\n-  ReferenceProcessorIsAliveMutator fix_isalive(rp, is_alive.is_alive_closure());\n-\n-  \/\/ Execute precleaning in the worker thread: it will give us GCLABs, String dedup\n-  \/\/ queues and other goodies. When upstream ReferenceProcessor starts supporting\n-  \/\/ parallel precleans, we can extend this to more threads.\n-  WorkGang* workers = _heap->workers();\n-  uint nworkers = workers->active_workers();\n-  assert(nworkers == 1, \"This code uses only a single worker\");\n-  task_queues()->reserve(nworkers);\n-\n-  ShenandoahPrecleanTask task(rp);\n-  workers->run_task(&task);\n-\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n-\n@@ -879,1 +517,1 @@\n-void ShenandoahConcurrentMark::mark_loop_prework(uint w, TaskTerminator *t, ReferenceProcessor *rp,\n+void ShenandoahConcurrentMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor* rp,\n@@ -937,0 +575,2 @@\n+  _heap->ref_processor()->set_mark_closure(worker_id, cl);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":23,"deletions":383,"binary":false,"changes":406,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class ShenandoahReferenceProcessor;\n@@ -52,1 +53,1 @@\n-  inline void do_chunked_array_start(ShenandoahObjToScanQueue* q, T* cl, oop array);\n+  inline void do_chunked_array_start(ShenandoahObjToScanQueue* q, T* cl, oop array, bool weak);\n@@ -55,1 +56,1 @@\n-  inline void do_chunked_array(ShenandoahObjToScanQueue* q, T* cl, oop array, int chunk, int pow);\n+  inline void do_chunked_array(ShenandoahObjToScanQueue* q, T* cl, oop array, int chunk, int pow, bool weak);\n@@ -63,1 +64,1 @@\n-  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ReferenceProcessor *rp, bool strdedup);\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor* rp, bool strdedup);\n@@ -66,1 +67,1 @@\n-  void mark_loop(uint worker_id, TaskTerminator* terminator, ReferenceProcessor *rp,\n+  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor* rp,\n@@ -76,1 +77,1 @@\n-  static inline void mark_through_ref(T* p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context);\n+  static inline void mark_through_ref(T* p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak);\n@@ -85,9 +86,0 @@\n-\/\/ ---------- Weak references\n-\/\/\n-private:\n-  void weak_refs_work(bool full_gc);\n-  void weak_refs_work_doit(bool full_gc);\n-\n-public:\n-  void preclean_weak_refs();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":6,"deletions":14,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2015, 2020, Red Hat, Inc. All rights reserved.\n@@ -48,0 +48,4 @@\n+  \/\/ Are we in weak subgraph scan?\n+  bool weak = task->is_weak();\n+  cl->set_weak(weak);\n+\n@@ -55,1 +59,1 @@\n-      do_chunked_array_start<T>(q, cl, obj);\n+      do_chunked_array_start<T>(q, cl, obj, weak);\n@@ -64,1 +68,5 @@\n-    count_liveness(live_data, obj);\n+    \/\/ Avoid double-counting objects that are visited twice due to upgrade\n+    \/\/ from final- to strong mark.\n+    if (task->count_liveness()) {\n+      count_liveness(live_data, obj);\n+    }\n@@ -67,1 +75,1 @@\n-    do_chunked_array<T>(q, cl, obj, task->chunk(), task->pow());\n+    do_chunked_array<T>(q, cl, obj, task->chunk(), task->pow(), weak);\n@@ -101,1 +109,1 @@\n-inline void ShenandoahConcurrentMark::do_chunked_array_start(ShenandoahObjToScanQueue* q, T* cl, oop obj) {\n+inline void ShenandoahConcurrentMark::do_chunked_array_start(ShenandoahObjToScanQueue* q, T* cl, oop obj, bool weak) {\n@@ -132,1 +140,1 @@\n-      bool pushed = q->push(ShenandoahMarkTask(array, 1, pow));\n+      bool pushed = q->push(ShenandoahMarkTask(array, true, weak, 1, pow));\n@@ -145,1 +153,1 @@\n-        bool pushed = q->push(ShenandoahMarkTask(array, left_chunk, pow));\n+        bool pushed = q->push(ShenandoahMarkTask(array, true, weak, left_chunk, pow));\n@@ -163,1 +171,1 @@\n-inline void ShenandoahConcurrentMark::do_chunked_array(ShenandoahObjToScanQueue* q, T* cl, oop obj, int chunk, int pow) {\n+inline void ShenandoahConcurrentMark::do_chunked_array(ShenandoahObjToScanQueue* q, T* cl, oop obj, int chunk, int pow, bool weak) {\n@@ -174,1 +182,1 @@\n-    bool pushed = q->push(ShenandoahMarkTask(array, chunk - 1, pow));\n+    bool pushed = q->push(ShenandoahMarkTask(array, true, weak, chunk - 1, pow));\n@@ -218,1 +226,1 @@\n-      ShenandoahConcurrentMark::mark_through_ref<oop, NONE, STRING_DEDUP>(p, _heap, _queue, _mark_context);\n+      ShenandoahConcurrentMark::mark_through_ref<oop, NONE, STRING_DEDUP>(p, _heap, _queue, _mark_context, false);\n@@ -224,1 +232,1 @@\n-inline void ShenandoahConcurrentMark::mark_through_ref(T *p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context) {\n+inline void ShenandoahConcurrentMark::mark_through_ref(T *p, ShenandoahHeap* heap, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak) {\n@@ -255,2 +263,9 @@\n-      if (mark_context->mark(obj)) {\n-        bool pushed = q->push(ShenandoahMarkTask(obj));\n+      bool skip_live = false;\n+      bool marked;\n+      if (weak) {\n+        marked = mark_context->mark_weak(obj);\n+      } else {\n+        marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n+      }\n+      if (marked) {\n+        bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.inline.hpp","additions":28,"deletions":13,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -144,1 +144,0 @@\n-        heap->set_process_references(heuristics->can_process_references());\n@@ -161,1 +160,0 @@\n-        heap->set_process_references(heuristics->can_process_references());\n@@ -175,1 +173,0 @@\n-      heap->set_process_references(heuristics->should_process_references());\n@@ -407,3 +404,0 @@\n-  \/\/ If not cancelled, can try to concurrently pre-clean\n-  heap->entry_preclean();\n-\n@@ -415,0 +409,1 @@\n+    heap->entry_weak_refs();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -209,1 +210,1 @@\n-  _bitmap_size = MarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n@@ -212,1 +213,1 @@\n-  size_t bitmap_bytes_per_region = reg_size_bytes \/ MarkBitMap::heap_map_factor();\n+  size_t bitmap_bytes_per_region = reg_size_bytes \/ ShenandoahMarkBitMap::heap_map_factor();\n@@ -396,3 +397,0 @@\n-  _ref_proc_mt_processing = ParallelRefProcEnabled && (ParallelGCThreads > 1);\n-  _ref_proc_mt_discovery = _max_workers > 1;\n-\n@@ -478,1 +476,1 @@\n-  _ref_processor(NULL),\n+  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -618,2 +616,0 @@\n-  ref_processing_init();\n-\n@@ -1794,7 +1790,3 @@\n-    if (process_references()) {\n-      \/\/ Abandon reference processing right away: pre-cleaning must have failed.\n-      ReferenceProcessor *rp = ref_processor();\n-      rp->disable_discovery();\n-      rp->abandon_partial_discovery();\n-      rp->verify_no_references_recorded();\n-    }\n+    \/\/ Abandon reference processing right away: pre-cleaning must have failed.\n+    ShenandoahReferenceProcessor* rp = ref_processor();\n+    rp->abandon_partial_discovery();\n@@ -2007,0 +1999,9 @@\n+void ShenandoahHeap::op_weak_refs() {\n+  \/\/ Concurrent weak refs processing\n+  {\n+    ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n+    ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n+    ref_processor()->process_references(workers(), true \/* concurrent *\/);\n+  }\n+}\n+\n@@ -2080,7 +2081,0 @@\n-void ShenandoahHeap::op_preclean() {\n-  if (ShenandoahPacing) {\n-    pacer()->setup_for_preclean();\n-  }\n-  concurrent_mark()->preclean_weak_refs();\n-}\n-\n@@ -2128,1 +2122,0 @@\n-      set_process_references(heuristics()->can_process_references());\n@@ -2153,0 +2146,6 @@\n+      {\n+        ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n+        ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n+        ref_processor()->process_references(workers(), false \/* concurrent *\/);\n+      }\n+\n@@ -2313,16 +2312,0 @@\n-void ShenandoahHeap::ref_processing_init() {\n-  assert(_max_workers > 0, \"Sanity\");\n-\n-  _ref_processor =\n-    new ReferenceProcessor(&_subject_to_discovery,  \/\/ is_subject_to_discovery\n-                           _ref_proc_mt_processing, \/\/ MT processing\n-                           _max_workers,            \/\/ Degree of MT processing\n-                           _ref_proc_mt_discovery,  \/\/ MT discovery\n-                           _max_workers,            \/\/ Degree of MT discovery\n-                           false,                   \/\/ Reference discovery is not atomic\n-                           NULL,                    \/\/ No closure, should be installed before use\n-                           true);                   \/\/ Scale worker threads\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-}\n-\n@@ -2464,4 +2447,0 @@\n-void ShenandoahHeap::set_process_references(bool pr) {\n-  _process_references.set_cond(pr);\n-}\n-\n@@ -2472,4 +2451,0 @@\n-bool ShenandoahHeap::process_references() const {\n-  return _process_references.is_set();\n-}\n-\n@@ -3070,0 +3045,13 @@\n+void ShenandoahHeap::entry_weak_refs() {\n+  static const char* msg = \"Concurrent weak references\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_refs_processing(),\n+                              \"concurrent weak references\");\n+\n+  try_inject_alloc_failure();\n+  op_weak_refs();\n+}\n+\n@@ -3156,16 +3144,0 @@\n-void ShenandoahHeap::entry_preclean() {\n-  if (ShenandoahPreclean && process_references()) {\n-    static const char* msg = \"Concurrent precleaning\";\n-    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_preclean);\n-    EventMark em(\"%s\", msg);\n-\n-    ShenandoahWorkerScope scope(workers(),\n-                                ShenandoahWorkerPolicy::calc_workers_for_conc_preclean(),\n-                                \"concurrent preclean\",\n-                                \/* check_workers = *\/ false);\n-\n-    try_inject_alloc_failure();\n-    op_preclean();\n-  }\n-}\n-\n@@ -3248,1 +3220,0 @@\n-  bool proc_refs = process_references();\n@@ -3251,5 +3222,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Pause Init Mark (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Pause Init Mark (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n@@ -3265,1 +3232,0 @@\n-  bool proc_refs = process_references();\n@@ -3268,5 +3234,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Pause Final Mark (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Pause Final Mark (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n@@ -3282,1 +3244,0 @@\n-  bool proc_refs = process_references();\n@@ -3285,5 +3246,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Concurrent marking (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Concurrent marking (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":38,"deletions":81,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-class ReferenceProcessor;\n@@ -63,1 +62,1 @@\n-class ShenandoahObjToScanQueueSet;\n+class ShenandoahReferenceProcessor;\n@@ -393,1 +392,1 @@\n-  void entry_preclean();\n+  void entry_weak_refs();\n@@ -418,1 +417,1 @@\n-  void op_preclean();\n+  void op_weak_refs();\n@@ -497,7 +496,1 @@\n-  AlwaysTrueClosure    _subject_to_discovery;\n-  ReferenceProcessor*  _ref_processor;\n-  ShenandoahSharedFlag _process_references;\n-  bool                 _ref_proc_mt_discovery;\n-  bool                 _ref_proc_mt_processing;\n-\n-  void ref_processing_init();\n+  ShenandoahReferenceProcessor* const _ref_processor;\n@@ -506,5 +499,1 @@\n-  ReferenceProcessor* ref_processor() { return _ref_processor; }\n-  bool ref_processor_mt_discovery()   { return _ref_proc_mt_discovery;  }\n-  bool ref_processor_mt_processing()  { return _ref_proc_mt_processing; }\n-  void set_process_references(bool pr);\n-  bool process_references() const;\n+  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":5,"deletions":16,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -404,1 +404,0 @@\n-  MarkBitMap* mark_bit_map = ctx->mark_bit_map();\n@@ -416,1 +415,1 @@\n-  HeapWord* cb = mark_bit_map->get_next_marked_addr(start, end);\n+  HeapWord* cb = ctx->get_next_marked_addr(start, end);\n@@ -443,1 +442,1 @@\n-          cb = mark_bit_map->get_next_marked_addr(cb, limit_bitmap);\n+          cb = ctx->get_next_marked_addr(cb, limit_bitmap);\n@@ -466,1 +465,1 @@\n-        cb = mark_bit_map->get_next_marked_addr(cb, limit_bitmap);\n+        cb = ctx->get_next_marked_addr(cb, limit_bitmap);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -60,4 +60,0 @@\n-\n-  log_info(gc, init)(\"Reference Processing: %s discovery, %s processing\",\n-                     heap->ref_processor_mt_discovery() ? \"Parallel\" : \"Serial\",\n-                     heap->ref_processor_mt_processing() ? \"Parallel\" : \"Serial\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,146 @@\n+\/*\n+ * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+ShenandoahMarkBitMap::ShenandoahMarkBitMap(MemRegion heap, MemRegion storage) :\n+  _shift(LogMinObjAlignment),\n+  _covered(heap),\n+  _map((BitMap::bm_word_t*) storage.start()),\n+  _size((heap.word_size() * 2) >> _shift) {\n+}\n+\n+size_t ShenandoahMarkBitMap::compute_size(size_t heap_size) {\n+  return ReservedSpace::allocation_align_size_up(heap_size \/ mark_distance());\n+}\n+\n+size_t ShenandoahMarkBitMap::mark_distance() {\n+  return MinObjAlignmentInBytes * BitsPerByte \/ 2;\n+}\n+\n+HeapWord* ShenandoahMarkBitMap::get_next_marked_addr(const HeapWord* addr,\n+                                                     const HeapWord* limit) const {\n+  assert(limit != NULL, \"limit must not be NULL\");\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  size_t const addr_offset = address_to_index(align_up(addr, HeapWordSize << LogMinObjAlignment));\n+  size_t const limit_offset = address_to_index(limit);\n+  size_t const nextOffset = get_next_one_offset(addr_offset, limit_offset);\n+  return index_to_address(nextOffset);\n+}\n+\n+void ShenandoahMarkBitMap::clear_range_within_word(idx_t beg, idx_t end) {\n+  \/\/ With a valid range (beg <= end), this test ensures that end != 0, as\n+  \/\/ required by inverted_bit_mask_for_range.  Also avoids an unnecessary write.\n+  if (beg != end) {\n+    bm_word_t mask = inverted_bit_mask_for_range(beg, end);\n+    *word_addr(beg) &= mask;\n+  }\n+}\n+\n+void ShenandoahMarkBitMap::clear_range(idx_t beg, idx_t end) {\n+  verify_range(beg, end);\n+\n+  idx_t beg_full_word = to_words_align_up(beg);\n+  idx_t end_full_word = to_words_align_down(end);\n+\n+  if (beg_full_word < end_full_word) {\n+    \/\/ The range includes at least one full word.\n+    clear_range_within_word(beg, bit_index(beg_full_word));\n+    clear_range_of_words(beg_full_word, end_full_word);\n+    clear_range_within_word(bit_index(end_full_word), end);\n+  } else {\n+    \/\/ The range spans at most 2 partial words.\n+    idx_t boundary = MIN2(bit_index(beg_full_word), end);\n+    clear_range_within_word(beg, boundary);\n+    clear_range_within_word(boundary, end);\n+  }\n+}\n+\n+bool ShenandoahMarkBitMap::is_small_range_of_words(idx_t beg_full_word, idx_t end_full_word) {\n+  \/\/ There is little point to call large version on small ranges.\n+  \/\/ Need to check carefully, keeping potential idx_t over\/underflow in mind,\n+  \/\/ because beg_full_word > end_full_word can occur when beg and end are in\n+  \/\/ the same word.\n+  \/\/ The threshold should be at least one word.\n+  STATIC_ASSERT(small_range_words >= 1);\n+  return beg_full_word + small_range_words >= end_full_word;\n+}\n+\n+\n+void ShenandoahMarkBitMap::clear_large_range(idx_t beg, idx_t end) {\n+  verify_range(beg, end);\n+\n+  idx_t beg_full_word = to_words_align_up(beg);\n+  idx_t end_full_word = to_words_align_down(end);\n+\n+  if (is_small_range_of_words(beg_full_word, end_full_word)) {\n+    clear_range(beg, end);\n+    return;\n+  }\n+\n+  \/\/ The range includes at least one full word.\n+  clear_range_within_word(beg, bit_index(beg_full_word));\n+  clear_large_range_of_words(beg_full_word, end_full_word);\n+  clear_range_within_word(bit_index(end_full_word), end);\n+}\n+\n+void ShenandoahMarkBitMap::clear_range_large(MemRegion mr) {\n+  MemRegion intersection = mr.intersection(_covered);\n+  assert(!intersection.is_empty(),\n+         \"Given range from \" PTR_FORMAT \" to \" PTR_FORMAT \" is completely outside the heap\",\n+          p2i(mr.start()), p2i(mr.end()));\n+  \/\/ convert address range into offset range\n+  size_t beg = address_to_index(intersection.start());\n+  size_t end = address_to_index(intersection.end());\n+  clear_large_range(beg, end);\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahMarkBitMap::check_mark(HeapWord* addr) const {\n+  assert(ShenandoahHeap::heap()->is_in(addr),\n+         \"Trying to access bitmap \" PTR_FORMAT \" for address \" PTR_FORMAT \" not in the heap.\",\n+         p2i(this), p2i(addr));\n+}\n+\n+void ShenandoahMarkBitMap::verify_index(idx_t bit) const {\n+  assert(bit < _size,\n+         \"BitMap index out of bounds: \" SIZE_FORMAT \" >= \" SIZE_FORMAT,\n+         bit, _size);\n+}\n+\n+void ShenandoahMarkBitMap::verify_limit(idx_t bit) const {\n+  assert(bit <= _size,\n+         \"BitMap limit out of bounds: \" SIZE_FORMAT \" > \" SIZE_FORMAT,\n+         bit, _size);\n+}\n+\n+void ShenandoahMarkBitMap::verify_range(idx_t beg, idx_t end) const {\n+  assert(beg <= end,\n+         \"BitMap range error: \" SIZE_FORMAT \" > \" SIZE_FORMAT, beg, end);\n+  verify_limit(end);\n+}\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":146,"deletions":0,"binary":false,"changes":146,"status":"added"},{"patch":"@@ -0,0 +1,180 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+\n+#include \"memory\/memRegion.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahMarkBitMap {\n+public:\n+  typedef size_t idx_t;         \/\/ Type used for bit and word indices.\n+  typedef uintptr_t bm_word_t;  \/\/ Element type of array that represents the\n+                                \/\/ bitmap, with BitsPerWord bits per element.\n+\n+private:\n+  \/\/ Values for get_next_bit_impl flip parameter.\n+  static const bm_word_t find_ones_flip = 0;\n+  static const bm_word_t find_zeros_flip = ~(bm_word_t)0;\n+\n+  int const _shift;\n+  MemRegion _covered;\n+\n+  bm_word_t* _map;     \/\/ First word in bitmap\n+  idx_t      _size;    \/\/ Size of bitmap (in bits)\n+\n+  \/\/ Threshold for performing small range operation, even when large range\n+  \/\/ operation was requested. Measured in words.\n+  static const size_t small_range_words = 32;\n+\n+  static bool is_small_range_of_words(idx_t beg_full_word, idx_t end_full_word);\n+\n+  inline size_t address_to_index(const HeapWord* addr) const;\n+  inline HeapWord* index_to_address(size_t offset) const;\n+\n+  void check_mark(HeapWord* addr) const NOT_DEBUG_RETURN;\n+\n+  \/\/ Return a mask that will select the specified bit, when applied to the word\n+  \/\/ containing the bit.\n+  static bm_word_t bit_mask(idx_t bit) { return (bm_word_t)1 << bit_in_word(bit); }\n+\n+  \/\/ Return the bit number of the first bit in the specified word.\n+  static idx_t bit_index(idx_t word)  { return word << LogBitsPerWord; }\n+\n+  \/\/ Return the position of bit within the word that contains it (e.g., if\n+  \/\/ bitmap words are 32 bits, return a number 0 <= n <= 31).\n+  static idx_t bit_in_word(idx_t bit) { return bit & (BitsPerWord - 1); }\n+\n+  bm_word_t* map()                 { return _map; }\n+  const bm_word_t* map() const     { return _map; }\n+  bm_word_t map(idx_t word) const { return _map[word]; }\n+\n+  \/\/ Return a pointer to the word containing the specified bit.\n+  bm_word_t* word_addr(idx_t bit) {\n+    return map() + to_words_align_down(bit);\n+  }\n+\n+  const bm_word_t* word_addr(idx_t bit) const {\n+    return map() + to_words_align_down(bit);\n+  }\n+\n+  static inline const bm_word_t load_word_ordered(const volatile bm_word_t* const addr, atomic_memory_order memory_order);\n+\n+  bool at(idx_t index) const {\n+    verify_index(index);\n+    return (*word_addr(index) & bit_mask(index)) != 0;\n+  }\n+\n+  \/\/ Assumes relevant validity checking for bit has already been done.\n+  static idx_t raw_to_words_align_up(idx_t bit) {\n+    return raw_to_words_align_down(bit + (BitsPerWord - 1));\n+  }\n+\n+  \/\/ Assumes relevant validity checking for bit has already been done.\n+  static idx_t raw_to_words_align_down(idx_t bit) {\n+    return bit >> LogBitsPerWord;\n+  }\n+\n+  \/\/ Word-aligns bit and converts it to a word offset.\n+  \/\/ precondition: bit <= size()\n+  idx_t to_words_align_up(idx_t bit) const {\n+    verify_limit(bit);\n+    return raw_to_words_align_up(bit);\n+  }\n+\n+  \/\/ Word-aligns bit and converts it to a word offset.\n+  \/\/ precondition: bit <= size()\n+  inline idx_t to_words_align_down(idx_t bit) const {\n+    verify_limit(bit);\n+    return raw_to_words_align_down(bit);\n+  }\n+\n+  \/\/ Helper for get_next_{zero,one}_bit variants.\n+  \/\/ - flip designates whether searching for 1s or 0s.  Must be one of\n+  \/\/   find_{zeros,ones}_flip.\n+  \/\/ - aligned_right is true if r_index is a priori on a bm_word_t boundary.\n+  template<bm_word_t flip, bool aligned_right>\n+  inline idx_t get_next_bit_impl(idx_t l_index, idx_t r_index) const;\n+\n+  inline idx_t get_next_one_offset (idx_t l_index, idx_t r_index) const;\n+\n+  void clear_large_range (idx_t beg, idx_t end);\n+\n+  \/\/ Verify bit is less than size().\n+  void verify_index(idx_t bit) const NOT_DEBUG_RETURN;\n+  \/\/ Verify bit is not greater than size().\n+  void verify_limit(idx_t bit) const NOT_DEBUG_RETURN;\n+  \/\/ Verify [beg,end) is a valid range, e.g. beg <= end <= size().\n+  void verify_range(idx_t beg, idx_t end) const NOT_DEBUG_RETURN;\n+\n+public:\n+  static size_t compute_size(size_t heap_size);\n+  \/\/ Returns the amount of bytes on the heap between two marks in the bitmap.\n+  static size_t mark_distance();\n+  \/\/ Returns how many bytes (or bits) of the heap a single byte (or bit) of the\n+  \/\/ mark bitmap corresponds to. This is the same as the mark distance above.\n+  static size_t heap_map_factor() {\n+    return mark_distance();\n+  }\n+\n+  ShenandoahMarkBitMap(MemRegion heap, MemRegion storage);\n+\n+  \/\/ Mark word as 'strong' if it hasn't been marked strong yet.\n+  \/\/ Return true if the word has been marked strong, false if it has already been\n+  \/\/ marked strong or if another thread has beat us by marking it\n+  \/\/ strong.\n+  \/\/ Words that have been marked final before or by a concurrent thread will be\n+  \/\/ upgraded to strong. In this case, this method also returns true.\n+  inline bool mark_strong(HeapWord* w, bool& was_upgraded);\n+\n+  \/\/ Mark word as 'weak' if it hasn't been marked weak or strong yet.\n+  \/\/ Return true if the word has been marked weak, false if it has already been\n+  \/\/ marked strong or weak or if another thread has beat us by marking it\n+  \/\/ strong or weak.\n+  inline bool mark_weak(HeapWord* heap_addr);\n+\n+  inline bool is_marked(HeapWord* addr) const;\n+  inline bool is_marked_strong(HeapWord* w)  const;\n+  inline bool is_marked_weak(HeapWord* addr) const;\n+\n+  \/\/ Return the address corresponding to the next marked bit at or after\n+  \/\/ \"addr\", and before \"limit\", if \"limit\" is non-NULL.  If there is no\n+  \/\/ such bit, returns \"limit\" if that is non-NULL, or else \"endWord()\".\n+  HeapWord* get_next_marked_addr(const HeapWord* addr,\n+                                 const HeapWord* limit) const;\n+\n+  bm_word_t inverted_bit_mask_for_range(idx_t beg, idx_t end) const;\n+  void  clear_range_within_word    (idx_t beg, idx_t end);\n+  void clear_range (idx_t beg, idx_t end);\n+  void clear_range_large(MemRegion mr);\n+\n+  void clear_range_of_words(idx_t beg, idx_t end);\n+  void clear_large_range_of_words(idx_t beg, idx_t end);\n+  static void clear_range_of_words(bm_word_t* map, idx_t beg, idx_t end);\n+\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":180,"deletions":0,"binary":false,"changes":180,"status":"added"},{"patch":"@@ -0,0 +1,218 @@\n+\/*\n+ * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/count_trailing_zeros.hpp\"\n+\n+inline size_t ShenandoahMarkBitMap::address_to_index(const HeapWord* addr) const {\n+  return (pointer_delta(addr, _covered.start()) << 1) >> _shift;\n+}\n+\n+inline HeapWord* ShenandoahMarkBitMap::index_to_address(size_t offset) const {\n+  return _covered.start() + ((offset >> 1) << _shift);\n+}\n+\n+inline bool ShenandoahMarkBitMap::mark_strong(HeapWord* heap_addr, bool& was_upgraded) {\n+  check_mark(heap_addr);\n+\n+  idx_t bit = address_to_index(heap_addr);\n+  verify_index(bit);\n+  volatile bm_word_t* const addr = word_addr(bit);\n+  const bm_word_t mask = bit_mask(bit);\n+  const bm_word_t mask_weak = (bm_word_t)1 << (bit_in_word(bit) + 1);\n+  bm_word_t old_val = load_word_ordered(addr, memory_order_conservative);\n+\n+  do {\n+    const bm_word_t new_val = old_val | mask;\n+    if (new_val == old_val) {\n+      assert(!was_upgraded, \"Should be false already\");\n+      return false;     \/\/ Someone else beat us to it.\n+    }\n+    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order_conservative);\n+    if (cur_val == old_val) {\n+      was_upgraded = (cur_val & mask_weak) != 0;\n+      return true;      \/\/ Success.\n+    }\n+    old_val = cur_val;  \/\/ The value changed, try again.\n+  } while (true);\n+}\n+\n+inline bool ShenandoahMarkBitMap::mark_weak(HeapWord* heap_addr) {\n+  check_mark(heap_addr);\n+\n+  idx_t bit = address_to_index(heap_addr);\n+  verify_index(bit);\n+  volatile bm_word_t* const addr = word_addr(bit);\n+  const bm_word_t mask_weak = (bm_word_t)1 << (bit_in_word(bit) + 1);\n+  const bm_word_t mask_strong = (bm_word_t)1 << bit_in_word(bit);\n+  bm_word_t old_val = load_word_ordered(addr, memory_order_conservative);\n+\n+  do {\n+    if ((old_val & mask_strong) != 0) {\n+      return false;     \/\/ Already marked strong\n+    }\n+    const bm_word_t new_val = old_val | mask_weak;\n+    if (new_val == old_val) {\n+      return false;     \/\/ Someone else beat us to it.\n+    }\n+    const bm_word_t cur_val = Atomic::cmpxchg(addr, old_val, new_val, memory_order_conservative);\n+    if (cur_val == old_val) {\n+      return true;      \/\/ Success.\n+    }\n+    old_val = cur_val;  \/\/ The value changed, try again.\n+  } while (true);\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked_strong(HeapWord* addr)  const {\n+  check_mark(addr);\n+  return at(address_to_index(addr));\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked_weak(HeapWord* addr) const {\n+  check_mark(addr);\n+  return at(address_to_index(addr) + 1);\n+}\n+\n+inline bool ShenandoahMarkBitMap::is_marked(HeapWord* addr) const {\n+  check_mark(addr);\n+  idx_t index = address_to_index(addr);\n+  verify_index(index);\n+  bm_word_t mask = (bm_word_t)3 << bit_in_word(index);\n+  return (*word_addr(index) & mask) != 0;\n+}\n+\n+inline const ShenandoahMarkBitMap::bm_word_t ShenandoahMarkBitMap::load_word_ordered(const volatile bm_word_t* const addr, atomic_memory_order memory_order) {\n+  if (memory_order == memory_order_relaxed || memory_order == memory_order_release) {\n+    return Atomic::load(addr);\n+  } else {\n+    assert(memory_order == memory_order_acq_rel ||\n+           memory_order == memory_order_acquire ||\n+           memory_order == memory_order_conservative,\n+           \"unexpected memory ordering\");\n+    return Atomic::load_acquire(addr);\n+  }\n+}\n+\n+template<ShenandoahMarkBitMap::bm_word_t flip, bool aligned_right>\n+inline ShenandoahMarkBitMap::idx_t ShenandoahMarkBitMap::get_next_bit_impl(idx_t l_index, idx_t r_index) const {\n+  STATIC_ASSERT(flip == find_ones_flip || flip == find_zeros_flip);\n+  verify_range(l_index, r_index);\n+  assert(!aligned_right || is_aligned(r_index, BitsPerWord), \"r_index not aligned\");\n+\n+  \/\/ The first word often contains an interesting bit, either due to\n+  \/\/ density or because of features of the calling algorithm.  So it's\n+  \/\/ important to examine that first word with a minimum of fuss,\n+  \/\/ minimizing setup time for later words that will be wasted if the\n+  \/\/ first word is indeed interesting.\n+\n+  \/\/ The benefit from aligned_right being true is relatively small.\n+  \/\/ It saves an operation in the setup for the word search loop.\n+  \/\/ It also eliminates the range check on the final result.\n+  \/\/ However, callers often have a comparison with r_index, and\n+  \/\/ inlining often allows the two comparisons to be combined; it is\n+  \/\/ important when !aligned_right that return paths either return\n+  \/\/ r_index or a value dominated by a comparison with r_index.\n+  \/\/ aligned_right is still helpful when the caller doesn't have a\n+  \/\/ range check because features of the calling algorithm guarantee\n+  \/\/ an interesting bit will be present.\n+\n+  if (l_index < r_index) {\n+    \/\/ Get the word containing l_index, and shift out low bits.\n+    idx_t index = to_words_align_down(l_index);\n+    bm_word_t cword = (map(index) ^ flip) >> bit_in_word(l_index);\n+    if ((cword & 1) != 0) {\n+      \/\/ The first bit is similarly often interesting. When it matters\n+      \/\/ (density or features of the calling algorithm make it likely\n+      \/\/ the first bit is set), going straight to the next clause compares\n+      \/\/ poorly with doing this check first; count_trailing_zeros can be\n+      \/\/ relatively expensive, plus there is the additional range check.\n+      \/\/ But when the first bit isn't set, the cost of having tested for\n+      \/\/ it is relatively small compared to the rest of the search.\n+      return l_index;\n+    } else if (cword != 0) {\n+      \/\/ Flipped and shifted first word is non-zero.\n+      idx_t result = l_index + count_trailing_zeros(cword);\n+      if (aligned_right || (result < r_index)) return result;\n+      \/\/ Result is beyond range bound; return r_index.\n+    } else {\n+      \/\/ Flipped and shifted first word is zero.  Word search through\n+      \/\/ aligned up r_index for a non-zero flipped word.\n+      idx_t limit = aligned_right\n+                    ? to_words_align_down(r_index) \/\/ Miniscule savings when aligned.\n+                    : to_words_align_up(r_index);\n+      while (++index < limit) {\n+        cword = map(index) ^ flip;\n+        if (cword != 0) {\n+          idx_t result = bit_index(index) + count_trailing_zeros(cword);\n+          if (aligned_right || (result < r_index)) return result;\n+          \/\/ Result is beyond range bound; return r_index.\n+          assert((index + 1) == limit, \"invariant\");\n+          break;\n+        }\n+      }\n+      \/\/ No bits in range; return r_index.\n+    }\n+  }\n+  return r_index;\n+}\n+\n+inline ShenandoahMarkBitMap::idx_t ShenandoahMarkBitMap::get_next_one_offset(idx_t l_offset, idx_t r_offset) const {\n+  return get_next_bit_impl<find_ones_flip, false>(l_offset, r_offset);\n+}\n+\n+\/\/ Returns a bit mask for a range of bits [beg, end) within a single word.  Each\n+\/\/ bit in the mask is 0 if the bit is in the range, 1 if not in the range.  The\n+\/\/ returned mask can be used directly to clear the range, or inverted to set the\n+\/\/ range.  Note:  end must not be 0.\n+inline ShenandoahMarkBitMap::bm_word_t\n+ShenandoahMarkBitMap::inverted_bit_mask_for_range(idx_t beg, idx_t end) const {\n+  assert(end != 0, \"does not work when end == 0\");\n+  assert(beg == end || to_words_align_down(beg) == to_words_align_down(end - 1),\n+         \"must be a single-word range\");\n+  bm_word_t mask = bit_mask(beg) - 1;   \/\/ low (right) bits\n+  if (bit_in_word(end) != 0) {\n+    mask |= ~(bit_mask(end) - 1);       \/\/ high (left) bits\n+  }\n+  return mask;\n+}\n+\n+inline void ShenandoahMarkBitMap::clear_range_of_words(bm_word_t* map, idx_t beg, idx_t end) {\n+  for (idx_t i = beg; i < end; ++i) map[i] = 0;\n+}\n+\n+inline void ShenandoahMarkBitMap::clear_large_range_of_words(idx_t beg, idx_t end) {\n+  assert(beg <= end, \"underflow\");\n+  memset(_map + beg, 0, (end - beg) * sizeof(bm_word_t));\n+}\n+\n+inline void ShenandoahMarkBitMap::clear_range_of_words(idx_t beg, idx_t end) {\n+  clear_range_of_words(_map, beg, end);\n+}\n+\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp","additions":218,"deletions":0,"binary":false,"changes":218,"status":"added"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -132,2 +133,1 @@\n-    ReferenceProcessor* rp = heap->ref_processor();\n-    rp->disable_discovery();\n+    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -135,1 +135,0 @@\n-    rp->verify_no_references_recorded();\n@@ -244,1 +243,0 @@\n-  heap->set_process_references(heap->heuristics()->can_process_references());\n@@ -247,1 +245,1 @@\n-  ReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -249,3 +247,1 @@\n-  rp->enable_discovery(true \/*verify_no_refs*\/);\n-  rp->setup_policy(true); \/\/ forcefully purge all soft references\n-  rp->set_active_mt_degree(heap->workers()->active_workers());\n+  rp->set_soft_reference_policy(true); \/\/ forcefully purge all soft references\n@@ -256,0 +252,1 @@\n+  rp->process_references(heap->workers(), false \/* concurrent *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkCompact.cpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+  _mark_bit_map(heap_region, bitmap_region),\n@@ -36,1 +37,0 @@\n-  _mark_bit_map.initialize(heap_region, bitmap_region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"gc\/shared\/markBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -38,1 +39,1 @@\n-  MarkBitMap _mark_bit_map;\n+  ShenandoahMarkBitMap _mark_bit_map;\n@@ -54,1 +55,2 @@\n-  inline bool mark(oop obj);\n+  inline bool mark_strong(oop obj, bool& was_upgraded);\n+  inline bool mark_weak(oop obj);\n@@ -56,1 +58,6 @@\n-  inline bool is_marked(oop obj) const;\n+  \/\/ Simple versions of marking accessors, to be used outside of marking (e.g. no possible concurrent updates)\n+  inline bool is_marked(oop) const;\n+  inline bool is_marked_strong(oop obj) const;\n+  inline bool is_marked_weak(oop obj) const;\n+\n+  inline HeapWord* get_next_marked_addr(HeapWord* addr, HeapWord* limit) const;\n@@ -61,2 +68,0 @@\n-  inline MarkBitMap* mark_bit_map();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp\"\n@@ -30,2 +31,3 @@\n-inline MarkBitMap* ShenandoahMarkingContext::mark_bit_map() {\n-  return &_mark_bit_map;\n+inline bool ShenandoahMarkingContext::mark_strong(oop obj, bool& was_upgraded) {\n+  shenandoah_assert_not_forwarded(NULL, obj);\n+  return (! allocated_after_mark_start(obj)) && _mark_bit_map.mark_strong(cast_from_oop<HeapWord*>(obj), was_upgraded);\n@@ -34,1 +36,1 @@\n-inline bool ShenandoahMarkingContext::mark(oop obj) {\n+inline bool ShenandoahMarkingContext::mark_weak(oop obj) {\n@@ -36,1 +38,1 @@\n-  return (! allocated_after_mark_start(obj)) && _mark_bit_map.par_mark(obj);\n+  return (! allocated_after_mark_start(obj)) && _mark_bit_map.mark_weak(cast_from_oop<HeapWord *>(obj));\n@@ -40,1 +42,13 @@\n-  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked(obj);\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked(cast_from_oop<HeapWord *>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked_strong(cast_from_oop<HeapWord*>(obj));\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_weak(oop obj) const {\n+  return allocated_after_mark_start(obj) || _mark_bit_map.is_marked_weak(cast_from_oop<HeapWord *>(obj));\n+}\n+\n+inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(HeapWord* start, HeapWord* limit) const {\n+  return _mark_bit_map.get_next_marked_addr(start, limit);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+  bool _weak;\n@@ -58,1 +59,9 @@\n-  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp);\n+  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp);\n+\n+  bool is_weak() const {\n+    return _weak;\n+  }\n+\n+  void set_weak(bool weak) {\n+    _weak = weak;\n+  }\n@@ -67,1 +76,1 @@\n-  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -81,1 +90,1 @@\n-  ShenandoahMarkUpdateRefsDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -95,1 +104,1 @@\n-  ShenandoahMarkUpdateRefsMetadataClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -109,1 +118,1 @@\n-  ShenandoahMarkUpdateRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkUpdateRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -123,1 +132,1 @@\n-  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -137,1 +146,1 @@\n-  ShenandoahMarkRefsDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -151,1 +160,1 @@\n-  ShenandoahMarkResolveRefsClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkResolveRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -165,1 +174,1 @@\n-  ShenandoahMarkRefsMetadataClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n@@ -179,1 +188,1 @@\n-  ShenandoahMarkRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ReferenceProcessor* rp) :\n+  ShenandoahMarkRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":19,"deletions":10,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, STRING_DEDUP>(p, _heap, _queue, _mark_context);\n+  ShenandoahConcurrentMark::mark_through_ref<T, UPDATE_REFS, STRING_DEDUP>(p, _heap, _queue, _mark_context, _weak);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -156,10 +156,0 @@\n-void ShenandoahPacer::setup_for_preclean() {\n-  assert(ShenandoahPacing, \"Only be here when pacing is enabled\");\n-\n-  size_t initial = _heap->max_capacity();\n-  restart_with(initial, 1.0);\n-\n-  log_info(gc, ergo)(\"Pacer for Precleaning. Non-Taxable: \" SIZE_FORMAT \"%s\",\n-                     byte_size_in_proper_unit(initial), proper_unit_for_byte_size(initial));\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -82,1 +82,0 @@\n-  void setup_for_preclean();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+    case conc_weak_refs_work:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -63,2 +63,0 @@\n-  f(conc_preclean,                                  \"Concurrent Precleaning\")          \\\n-                                                                                       \\\n@@ -85,0 +83,3 @@\n+  f(conc_weak_refs,                                 \"Concurrent Weak References\")      \\\n+  f(conc_weak_refs_work,                            \"  Process\")                       \\\n+  SHENANDOAH_PAR_PHASE_DO(conc_weak_refs_work_,     \"    CWRF: \", f)                   \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,592 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+static ReferenceType reference_type(oop reference) {\n+  return InstanceKlass::cast(reference->klass())->reference_type();\n+}\n+\n+static const char* reference_type_name(ReferenceType type) {\n+  switch (type) {\n+    case REF_SOFT:\n+      return \"Soft\";\n+\n+    case REF_WEAK:\n+      return \"Weak\";\n+\n+    case REF_FINAL:\n+      return \"Final\";\n+\n+    case REF_PHANTOM:\n+      return \"Phantom\";\n+\n+    default:\n+      ShouldNotReachHere();\n+      return NULL;\n+  }\n+}\n+\n+template <typename T>\n+static void set_oop_field(T* field, oop value);\n+\n+template <>\n+void set_oop_field<oop>(oop* field, oop value) {\n+  *field = value;\n+}\n+\n+template <>\n+void set_oop_field<narrowOop>(narrowOop* field, oop value) {\n+  *field = CompressedOops::encode(value);\n+}\n+\n+static oop lrb(oop obj) {\n+  if (obj != NULL && ShenandoahHeap::heap()->marking_context()->is_marked(obj)) {\n+    return ShenandoahBarrierSet::barrier_set()->load_reference_barrier(obj);\n+  } else {\n+    return obj;\n+  }\n+}\n+\n+template <typename T>\n+static volatile T* reference_referent_addr(oop reference) {\n+  return (volatile T*)java_lang_ref_Reference::referent_addr_raw(reference);\n+}\n+\n+template <typename T>\n+static oop reference_referent(oop reference) {\n+  T heap_oop = Atomic::load(reference_referent_addr<T>(reference));\n+  return CompressedOops::decode(heap_oop);\n+}\n+\n+static void reference_set_referent(oop reference, oop referent) {\n+  java_lang_ref_Reference::set_referent_raw(reference, referent);\n+}\n+\n+template <typename T>\n+static T* reference_discovered_addr(oop reference) {\n+  return reinterpret_cast<T*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+}\n+\n+template <typename T>\n+static oop reference_discovered(oop reference) {\n+  T heap_oop = *reference_discovered_addr<T>(reference);\n+  return lrb(CompressedOops::decode(heap_oop));\n+}\n+\n+template <typename T>\n+static void reference_set_discovered(oop reference, oop discovered);\n+\n+template <>\n+void reference_set_discovered<oop>(oop reference, oop discovered) {\n+  *reference_discovered_addr<oop>(reference) = discovered;\n+}\n+\n+template <>\n+void reference_set_discovered<narrowOop>(oop reference, oop discovered) {\n+  *reference_discovered_addr<narrowOop>(reference) = CompressedOops::encode(discovered);\n+}\n+\n+template<typename T>\n+static bool reference_cas_discovered(oop reference, oop discovered);\n+\n+template<>\n+bool reference_cas_discovered<narrowOop>(oop reference, oop discovered) {\n+  volatile narrowOop* addr = reinterpret_cast<volatile narrowOop*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+  narrowOop compare = CompressedOops::encode(NULL);\n+  narrowOop exchange = CompressedOops::encode(discovered);\n+  return Atomic::cmpxchg(addr, compare, exchange) == compare;\n+}\n+\n+template<>\n+bool reference_cas_discovered<oop>(oop reference, oop discovered) {\n+  volatile oop* addr = reinterpret_cast<volatile oop*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+  return Atomic::cmpxchg(addr, oop(NULL), discovered) == NULL;\n+}\n+\n+template <typename T>\n+static T* reference_next_addr(oop reference) {\n+  return reinterpret_cast<T*>(java_lang_ref_Reference::next_addr_raw(reference));\n+}\n+\n+template <typename T>\n+static oop reference_next(oop reference) {\n+  T heap_oop = RawAccess<>::oop_load(reference_next_addr<T>(reference));\n+  return lrb(CompressedOops::decode(heap_oop));\n+}\n+\n+static void reference_set_next(oop reference, oop next) {\n+  java_lang_ref_Reference::set_next_raw(reference, next);\n+}\n+\n+static void soft_reference_update_clock() {\n+  const jlong now = os::javaTimeNanos() \/ NANOSECS_PER_MILLISEC;\n+  java_lang_ref_SoftReference::set_clock(now);\n+}\n+\n+ShenandoahRefProcThreadLocal::ShenandoahRefProcThreadLocal() :\n+  _discovered_list(NULL),\n+  _encountered_count(),\n+  _discovered_count(),\n+  _enqueued_count() {\n+}\n+\n+void ShenandoahRefProcThreadLocal::reset() {\n+  _discovered_list = NULL;\n+  _mark_closure = NULL;\n+  for (uint i = 0; i < reference_type_count; i++) {\n+    _encountered_count[i] = 0;\n+    _discovered_count[i] = 0;\n+    _enqueued_count[i] = 0;\n+  }\n+}\n+\n+template <typename T>\n+T* ShenandoahRefProcThreadLocal::discovered_list_addr() {\n+  return reinterpret_cast<T*>(&_discovered_list);\n+}\n+\n+template <>\n+oop ShenandoahRefProcThreadLocal::discovered_list_head<oop>() const {\n+  return *reinterpret_cast<const oop*>(&_discovered_list);\n+}\n+\n+template <>\n+oop ShenandoahRefProcThreadLocal::discovered_list_head<narrowOop>() const {\n+  return CompressedOops::decode(*reinterpret_cast<const narrowOop*>(&_discovered_list));\n+}\n+\n+template <>\n+void ShenandoahRefProcThreadLocal::set_discovered_list_head<narrowOop>(oop head) {\n+  *discovered_list_addr<narrowOop>() = CompressedOops::encode(head);\n+}\n+\n+template <>\n+void ShenandoahRefProcThreadLocal::set_discovered_list_head<oop>(oop head) {\n+  *discovered_list_addr<oop>() = head;\n+}\n+\n+ShenandoahReferenceProcessor::ShenandoahReferenceProcessor(uint max_workers) :\n+  _soft_reference_policy(NULL),\n+  _ref_proc_thread_locals(NEW_C_HEAP_ARRAY(ShenandoahRefProcThreadLocal, max_workers, mtGC)),\n+  _pending_list(NULL),\n+  _pending_list_tail(&_pending_list),\n+  _iterate_discovered_list_id(0U) {\n+  for (size_t i = 0; i < max_workers; i++) {\n+    _ref_proc_thread_locals[i].reset();\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::reset_thread_locals() {\n+  uint max_workers = ShenandoahHeap::heap()->max_workers();\n+  for (uint i = 0; i < max_workers; i++) {\n+    _ref_proc_thread_locals[i].reset();\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::set_mark_closure(uint worker_id, ShenandoahMarkRefsSuperClosure* mark_closure) {\n+  _ref_proc_thread_locals[worker_id].set_mark_closure(mark_closure);\n+}\n+\n+void ShenandoahReferenceProcessor::set_soft_reference_policy(bool clear) {\n+  static AlwaysClearPolicy always_clear_policy;\n+  static LRUMaxHeapPolicy lru_max_heap_policy;\n+\n+  if (clear) {\n+    log_info(gc, ref)(\"Clearing All SoftReferences\");\n+    _soft_reference_policy = &always_clear_policy;\n+  } else {\n+    _soft_reference_policy = &lru_max_heap_policy;\n+  }\n+\n+  _soft_reference_policy->setup();\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::is_inactive(oop reference, oop referent, ReferenceType type) const {\n+  if (type == REF_FINAL) {\n+    \/\/ A FinalReference is inactive if its next field is non-null. An application can't\n+    \/\/ call enqueue() or clear() on a FinalReference.\n+    return reference_next<T>(reference) != NULL;\n+  } else {\n+    \/\/ A non-FinalReference is inactive if the referent is null. The referent can only\n+    \/\/ be null if the application called Reference.enqueue() or Reference.clear().\n+    return referent == NULL;\n+  }\n+}\n+\n+bool ShenandoahReferenceProcessor::is_strongly_live(oop referent) const {\n+  return ShenandoahHeap::heap()->marking_context()->is_marked_strong(referent);\n+}\n+\n+bool ShenandoahReferenceProcessor::is_softly_live(oop reference, ReferenceType type) const {\n+  if (type != REF_SOFT) {\n+    \/\/ Not a SoftReference\n+    return false;\n+  }\n+\n+  \/\/ Ask SoftReference policy\n+  const jlong clock = java_lang_ref_SoftReference::clock();\n+  assert(clock != 0, \"Clock not initialized\");\n+  assert(_soft_reference_policy != NULL, \"Policy not initialized\");\n+  return !_soft_reference_policy->should_clear_reference(reference, clock);\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::should_discover(oop reference, ReferenceType type) const {\n+  T* referent_addr = (T*) java_lang_ref_Reference::referent_addr_raw(reference);\n+  T heap_oop = RawAccess<>::oop_load(referent_addr);\n+  oop referent = CompressedOops::decode_not_null(heap_oop);\n+\n+  if (is_inactive<T>(reference, referent, type)) {\n+    log_trace(gc,ref)(\"Reference inactive: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+\n+  if (is_strongly_live(referent)) {\n+    log_trace(gc,ref)(\"Reference strongly live: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+\n+  if (is_softly_live(reference, type)) {\n+    log_trace(gc,ref)(\"Reference softly live: \" PTR_FORMAT, p2i(reference));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::should_drop(oop reference, ReferenceType type) const {\n+  const oop referent = reference_referent<T>(reference);\n+  if (referent == NULL) {\n+    \/\/ Reference has been cleared, by a call to Reference.enqueue()\n+    \/\/ or Reference.clear() from the application, which means we\n+    \/\/ should drop the reference.\n+    return true;\n+  }\n+\n+  \/\/ Check if the referent is still alive, in which case we should\n+  \/\/ drop the reference.\n+  if (type == REF_PHANTOM) {\n+    return ShenandoahHeap::heap()->complete_marking_context()->is_marked(referent);\n+  } else {\n+    return ShenandoahHeap::heap()->complete_marking_context()->is_marked_strong(referent);\n+  }\n+}\n+\n+template <typename T>\n+void ShenandoahReferenceProcessor::make_inactive(oop reference, ReferenceType type) const {\n+  if (type == REF_FINAL) {\n+    \/\/ Don't clear referent. It is needed by the Finalizer thread to make the call\n+    \/\/ to finalize(). A FinalReference is instead made inactive by self-looping the\n+    \/\/ next field. An application can't call FinalReference.enqueue(), so there is\n+    \/\/ no race to worry about when setting the next field.\n+    assert(reference_next<T>(reference) == NULL, \"Already inactive\");\n+    assert(ShenandoahHeap::heap()->marking_context()->is_marked(reference_referent<T>(reference)), \"only make inactive final refs with alive referents\");\n+    reference_set_next(reference, reference);\n+  } else {\n+    \/\/ Clear referent\n+    reference_set_referent(reference, NULL);\n+  }\n+}\n+\n+template <typename T>\n+bool ShenandoahReferenceProcessor::discover(oop reference, ReferenceType type, uint worker_id) {\n+  if (!should_discover<T>(reference, type)) {\n+    \/\/ Not discovered\n+    return false;\n+  }\n+\n+  if (reference_discovered<T>(reference) != NULL) {\n+    \/\/ Already discovered. This can happen if the reference is marked finalizable first, and then strong,\n+    \/\/ in which case it will be seen 2x by marking.\n+    log_trace(gc,ref)(\"Reference already discovered: \" PTR_FORMAT, p2i(reference));\n+    return true;\n+  }\n+\n+  if (type == REF_FINAL) {\n+    ShenandoahMarkRefsSuperClosure* cl = _ref_proc_thread_locals[worker_id].mark_closure();\n+    bool weak = cl->is_weak();\n+    cl->set_weak(true);\n+    if (UseCompressedOops) {\n+      cl->do_oop(reinterpret_cast<narrowOop*>(java_lang_ref_Reference::referent_addr_raw(reference)));\n+    } else {\n+      cl->do_oop(reinterpret_cast<oop*>(java_lang_ref_Reference::referent_addr_raw(reference)));\n+    }\n+    cl->set_weak(weak);\n+  }\n+\n+  \/\/ Add reference to discovered list\n+  assert(worker_id != ShenandoahThreadLocalData::INVALID_WORKER_ID, \"need valid worker ID\");\n+  ShenandoahRefProcThreadLocal& refproc_data = _ref_proc_thread_locals[worker_id];\n+  oop discovered_head = refproc_data.discovered_list_head<T>();\n+  if (discovered_head == NULL) {\n+    \/\/ Self-loop tail of list. We distinguish discovered from not-discovered references by looking at their\n+    \/\/ discovered field: if it is NULL, then it is not-yet discovered, otherwise it is discovered\n+    discovered_head = reference;\n+  }\n+  if (reference_cas_discovered<T>(reference, discovered_head)) {\n+    refproc_data.set_discovered_list_head<T>(reference);\n+    assert(refproc_data.discovered_list_head<T>() == reference, \"reference must be new discovered head\");\n+    log_trace(gc, ref)(\"Discovered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+    _ref_proc_thread_locals[worker_id].inc_discovered(type);\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahReferenceProcessor::discover_reference(oop reference, ReferenceType type) {\n+  if (!RegisterReferences) {\n+    \/\/ Reference processing disabled\n+    return false;\n+  }\n+\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  uint worker_id = ShenandoahThreadLocalData::worker_id(Thread::current());\n+  _ref_proc_thread_locals->inc_encountered(type);\n+\n+  if (UseCompressedOops) {\n+    return discover<narrowOop>(reference, type, worker_id);\n+  } else {\n+    return discover<oop>(reference, type, worker_id);\n+  }\n+}\n+\n+template <typename T>\n+oop ShenandoahReferenceProcessor::drop(oop reference, ReferenceType type) {\n+  log_trace(gc, ref)(\"Dropped Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  assert(reference_referent<T>(reference) == NULL ||\n+         ShenandoahHeap::heap()->marking_context()->is_marked(reference_referent<T>(reference)), \"only drop references with alive referents\");\n+\n+  \/\/ Unlink and return next in list\n+  oop next = reference_discovered<T>(reference);\n+  reference_set_discovered<T>(reference, NULL);\n+  return next;\n+}\n+\n+template <typename T>\n+T* ShenandoahReferenceProcessor::keep(oop reference, ReferenceType type, uint worker_id) {\n+  log_trace(gc, ref)(\"Enqueued Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+\n+  \/\/ Update statistics\n+  _ref_proc_thread_locals[worker_id].inc_enqueued(type);\n+\n+  \/\/ Make reference inactive\n+  make_inactive<T>(reference, type);\n+\n+  \/\/ Return next in list\n+  return reference_discovered_addr<T>(reference);\n+}\n+\n+template <typename T>\n+void ShenandoahReferenceProcessor::process_references(ShenandoahRefProcThreadLocal& refproc_data, uint worker_id) {;\n+  log_trace(gc, ref)(\"Processing discovered list #%u : \" PTR_FORMAT, worker_id, p2i(refproc_data.discovered_list_head<T>()));\n+  T* list = refproc_data.discovered_list_addr<T>();\n+  \/\/ The list head is basically a GC root, we need to resolve and update it,\n+  \/\/ otherwise we will later swap a from-space ref into Universe::pending_list().\n+  if (!CompressedOops::is_null(*list)) {\n+    oop first_resolved = lrb(CompressedOops::decode_not_null(*list));\n+    set_oop_field(list, first_resolved);\n+  }\n+  T* p = list;\n+  while (true) {\n+    const oop reference = lrb(CompressedOops::decode(*p));\n+    if (reference == NULL) {\n+      break;\n+    }\n+    log_trace(gc, ref)(\"Processing reference: \" PTR_FORMAT, p2i(reference));\n+    const ReferenceType type = reference_type(reference);\n+\n+    if (should_drop<T>(reference, type)) {\n+      set_oop_field(p, drop<T>(reference, type));\n+    } else {\n+      p = keep<T>(reference, type, worker_id);\n+    }\n+\n+    const oop discovered = lrb(reference_discovered<T>(reference));\n+    if (reference == discovered) {\n+      \/\/ Reset terminating self-loop to NULL\n+      reference_set_discovered<T>(reference, oop(NULL));\n+      break;\n+    }\n+  }\n+\n+  \/\/ Prepend discovered references to internal pending list\n+  if (!CompressedOops::is_null(*list)) {\n+    oop head = lrb(CompressedOops::decode_not_null(*list));\n+    shenandoah_assert_not_in_cset_except(&head, head, ShenandoahHeap::heap()->cancelled_gc() || !ShenandoahLoadRefBarrier);\n+    oop prev = Atomic::xchg(&_pending_list, head);\n+    RawAccess<>::oop_store(p, prev);\n+    if (prev == NULL) {\n+      \/\/ First to prepend to list, record tail\n+      _pending_list_tail = reinterpret_cast<void*>(p);\n+    }\n+\n+    \/\/ Clear discovered list\n+    set_oop_field(list, oop(NULL));\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::work() {\n+  \/\/ Process discovered references\n+  uint max_workers = ShenandoahHeap::heap()->max_workers();\n+  uint worker_id = Atomic::add(&_iterate_discovered_list_id, 1U) - 1;\n+  while (worker_id < max_workers) {\n+    if (UseCompressedOops) {\n+      process_references<narrowOop>(_ref_proc_thread_locals[worker_id], worker_id);\n+    } else {\n+      process_references<oop>(_ref_proc_thread_locals[worker_id], worker_id);\n+    }\n+    worker_id = Atomic::add(&_iterate_discovered_list_id, 1U) - 1;\n+  }\n+}\n+\n+class ShenandoahReferenceProcessorTask : public AbstractGangTask {\n+private:\n+  ShenandoahReferenceProcessor* const _reference_processor;\n+\n+public:\n+  ShenandoahReferenceProcessorTask(ShenandoahReferenceProcessor* reference_processor) :\n+    AbstractGangTask(\"ShenandoahReferenceProcessorTask\"),\n+    _reference_processor(reference_processor) {\n+  }\n+\n+  virtual void work(uint worker_id) {\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    _reference_processor->work();\n+  }\n+};\n+\n+void ShenandoahReferenceProcessor::process_references(WorkGang* workers, bool concurrent) {\n+\n+  Atomic::release_store_fence(&_iterate_discovered_list_id, 0U);\n+\n+  \/\/ Process discovered lists\n+  ShenandoahReferenceProcessorTask task(this);\n+  workers->run_task(&task);\n+\n+  \/\/ Update SoftReference clock\n+  soft_reference_update_clock();\n+\n+  \/\/ Collect, log and trace statistics\n+  collect_statistics();\n+\n+  enqueue_references(concurrent);\n+}\n+\n+void ShenandoahReferenceProcessor::enqueue_references_locked() {\n+  \/\/ Prepend internal pending list to external pending list\n+  shenandoah_assert_not_in_cset_except(&_pending_list, _pending_list, ShenandoahHeap::heap()->cancelled_gc() || !ShenandoahLoadRefBarrier);\n+  if (UseCompressedOops) {\n+    *reinterpret_cast<narrowOop*>(_pending_list_tail) = CompressedOops::encode(Universe::swap_reference_pending_list(_pending_list));\n+  } else {\n+    *reinterpret_cast<oop*>(_pending_list_tail) = Universe::swap_reference_pending_list(_pending_list);\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::enqueue_references(bool concurrent) {\n+  if (_pending_list == NULL) {\n+    \/\/ Nothing to enqueue\n+    return;\n+  }\n+\n+  if (!concurrent) {\n+    \/\/ When called from mark-compact or degen-GC, the locking is done by the VMOperation,\n+    enqueue_references_locked();\n+  } else {\n+    \/\/ Heap_lock protects external pending list\n+    MonitorLocker ml(Heap_lock, Mutex::_no_safepoint_check_flag);\n+\n+    enqueue_references_locked();\n+\n+    \/\/ Notify ReferenceHandler thread\n+    ml.notify_all();\n+  }\n+\n+  \/\/ Reset internal pending list\n+  _pending_list = NULL;\n+  _pending_list_tail = &_pending_list;\n+}\n+\n+template<typename T>\n+void ShenandoahReferenceProcessor::clean_discovered_list(T* list) {\n+  T discovered = *list;\n+  while (!CompressedOops::is_null(discovered)) {\n+    oop discovered_ref = CompressedOops::decode_not_null(discovered);\n+    set_oop_field<T>(list, oop(NULL));\n+    list = reference_discovered_addr<T>(discovered_ref);\n+    discovered = *list;\n+  }\n+}\n+\n+void ShenandoahReferenceProcessor::abandon_partial_discovery() {\n+  uint max_workers = ShenandoahHeap::heap()->max_workers();\n+  for (uint index = 0; index < max_workers; index++) {\n+    if (UseCompressedOops) {\n+      clean_discovered_list<narrowOop>(_ref_proc_thread_locals[index].discovered_list_addr<narrowOop>());\n+    } else {\n+      clean_discovered_list<oop>(_ref_proc_thread_locals[index].discovered_list_addr<oop>());\n+    }\n+  }\n+  if (_pending_list != NULL) {\n+    oop pending = _pending_list;\n+    _pending_list = NULL;\n+    if (UseCompressedOops) {\n+      narrowOop* list = reference_discovered_addr<narrowOop>(pending);\n+      clean_discovered_list<narrowOop>(list);\n+    } else {\n+      oop* list = reference_discovered_addr<oop>(pending);\n+      clean_discovered_list<oop>(list);\n+    }\n+  }\n+  _pending_list_tail = &_pending_list;\n+}\n+\n+void ShenandoahReferenceProcessor::collect_statistics() {\n+  Counters encountered = {};\n+  Counters discovered = {};\n+  Counters enqueued = {};\n+  uint max_workers = ShenandoahHeap::heap()->max_workers();\n+  for (uint i = 0; i < max_workers; i++) {\n+    for (size_t type = 0; type < reference_type_count; type++) {\n+      encountered[type] += _ref_proc_thread_locals[i].encountered((ReferenceType)type);\n+      discovered[type] += _ref_proc_thread_locals[i].discovered((ReferenceType)type);\n+      enqueued[type] += _ref_proc_thread_locals[i].enqueued((ReferenceType)type);\n+    }\n+  }\n+  log_info(gc,ref)(\"Encountered references: Soft: \" SIZE_FORMAT \", Weak: \" SIZE_FORMAT \", Final: \" SIZE_FORMAT \", Phantom: \" SIZE_FORMAT,\n+                   encountered[REF_SOFT], encountered[REF_WEAK], encountered[REF_FINAL], encountered[REF_PHANTOM]);\n+  log_info(gc,ref)(\"Discovered  references: Soft: \" SIZE_FORMAT \", Weak: \" SIZE_FORMAT \", Final: \" SIZE_FORMAT \", Phantom: \" SIZE_FORMAT,\n+                   discovered[REF_SOFT], discovered[REF_WEAK], discovered[REF_FINAL], discovered[REF_PHANTOM]);\n+  log_info(gc,ref)(\"Enqueued    references: Soft: \" SIZE_FORMAT \", Weak: \" SIZE_FORMAT \", Final: \" SIZE_FORMAT \", Phantom: \" SIZE_FORMAT,\n+                   enqueued[REF_SOFT], enqueued[REF_WEAK], enqueued[REF_FINAL], enqueued[REF_PHANTOM]);\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":592,"deletions":0,"binary":false,"changes":592,"status":"added"},{"patch":"@@ -0,0 +1,185 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Red Hat, Inc. and\/or its affiliates.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+\n+#include \"gc\/shared\/referenceDiscoverer.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class ShenandoahMarkRefsSuperClosure;\n+class WorkGang;\n+\n+static const size_t reference_type_count = REF_PHANTOM + 1;\n+typedef size_t Counters[reference_type_count];\n+\n+\/*\n+ * Shenandoah concurrent reference processing\n+ *\n+ * Concurrent reference processing is made up of two main phases:\n+ * 1. Concurrent reference marking: Discover all j.l.r.Reference objects and determine reachability of all live objects.\n+ * 2. Concurrent reference processing: For all discoved j.l.r.References, determine whether to keep them alive or clean\n+ *    them. Also, clean and enqueue relevant references concurrently.\n+ *\n+ * Concurrent reference marking:\n+ * The goal here is to establish the kind of reachability for all objects on the heap. We distinguish two kinds of\n+ * reachability:\n+ * - An object is 'strongly reachable' if it can be found by searching transitively from GC roots.\n+ * - An object is 'finalizably reachable' if it is not strongly reachable, but can be found by searching\n+ *   from the referents of FinalReferences.\n+ *\n+ * These reachabilities are implemented in shenandoahMarkBitMap.*\n+ * Conceptually, marking starts with a strong wavefront at the GC roots. Whenever a Reference object is encountered,\n+ * it may be discovered by the ShenandoahReferenceProcessor. If it is discovered, it\n+ * gets added to the discovered list, and that wavefront stops there, except when it's a FinalReference, in which\n+ * case the wavefront switches to finalizable marking and marks through the referent. When a Reference is not\n+ * discovered, e.g. if it's a SoftReference that is not eligible for discovery, then marking continues as if the\n+ * Reference was a regular object. Whenever a strong wavefront encounters an object that is already marked\n+ * finalizable, then the object's reachability is upgraded to strong.\n+ *\n+ * Concurrent reference processing:\n+ * This happens after the concurrent marking phase and the final marking pause, when reachability for all objects\n+ * has been established.\n+ * The discovered list is scanned and for each reference is decided what to do:\n+ * - If the referent is reachable (finalizable for PhantomReference, strong for all others), then the Reference\n+ *   is dropped from the discovered list and otherwise ignored\n+ * - Otherwise its referent becomes cleared and the Reference added to the pending list, from which it will later\n+ *   be processed (e.g. enqueued in its ReferenceQueue) by the Java ReferenceHandler thread.\n+ *\n+ * In order to prevent resurrection by Java threads calling Reference.get() concurrently while we are clearing\n+ * referents, we employ a special barrier, the native LRB, which returns NULL when the referent is unreachable.\n+ *\/\n+\n+class ShenandoahRefProcThreadLocal : public CHeapObj<mtGC> {\n+private:\n+  void* _discovered_list;\n+  ShenandoahMarkRefsSuperClosure* _mark_closure;\n+  Counters _encountered_count;\n+  Counters _discovered_count;\n+  Counters _enqueued_count;\n+\n+public:\n+  ShenandoahRefProcThreadLocal();\n+\n+  ShenandoahRefProcThreadLocal(const ShenandoahRefProcThreadLocal&) = delete; \/\/ non construction-copyable\n+  ShenandoahRefProcThreadLocal& operator=(const ShenandoahRefProcThreadLocal&) = delete; \/\/ non copyable\n+\n+  void reset();\n+\n+  ShenandoahMarkRefsSuperClosure* mark_closure() const {\n+    return _mark_closure;\n+  }\n+\n+  void set_mark_closure(ShenandoahMarkRefsSuperClosure* mark_closure) {\n+    _mark_closure = mark_closure;\n+  }\n+\n+  template<typename T>\n+  T* discovered_list_addr();\n+  template<typename T>\n+  oop discovered_list_head() const;\n+  template<typename T>\n+  void set_discovered_list_head(oop head);\n+\n+  size_t encountered(ReferenceType type) const {\n+    return _encountered_count[type];\n+  }\n+  size_t discovered(ReferenceType type) const {\n+    return _discovered_count[type];\n+  }\n+  size_t enqueued(ReferenceType type) const {\n+    return _enqueued_count[type];\n+  }\n+\n+  void inc_encountered(ReferenceType type) {\n+    _encountered_count[type]++;\n+  }\n+  void inc_discovered(ReferenceType type) {\n+    _discovered_count[type]++;\n+  }\n+  void inc_enqueued(ReferenceType type) {\n+    _enqueued_count[type]++;\n+  }\n+};\n+\n+class ShenandoahReferenceProcessor : public ReferenceDiscoverer {\n+private:\n+  ReferencePolicy* _soft_reference_policy;\n+\n+  ShenandoahRefProcThreadLocal* _ref_proc_thread_locals;\n+\n+  oop _pending_list;\n+  void* _pending_list_tail; \/\/ T*\n+\n+  volatile uint _iterate_discovered_list_id;\n+\n+  template <typename T>\n+  bool is_inactive(oop reference, oop referent, ReferenceType type) const;\n+  bool is_strongly_live(oop referent) const;\n+  bool is_softly_live(oop reference, ReferenceType type) const;\n+\n+  template <typename T>\n+  bool should_discover(oop reference, ReferenceType type) const;\n+  template <typename T>\n+  bool should_drop(oop reference, ReferenceType type) const;\n+\n+  template <typename T>\n+  void make_inactive(oop reference, ReferenceType type) const;\n+\n+  template <typename T>\n+  bool discover(oop reference, ReferenceType type, uint worker_id);\n+\n+  template <typename T>\n+  oop drop(oop reference, ReferenceType type);\n+  template <typename T>\n+  T* keep(oop reference, ReferenceType type, uint worker_id);\n+\n+  template <typename T>\n+  void process_references(ShenandoahRefProcThreadLocal& refproc_data, uint worker_id);\n+  void enqueue_references_locked();\n+  void enqueue_references(bool concurrent);\n+\n+  void collect_statistics();\n+\n+  template<typename T>\n+  void clean_discovered_list(T* list);\n+\n+public:\n+  ShenandoahReferenceProcessor(uint max_workers);\n+\n+  void reset_thread_locals();\n+  void set_mark_closure(uint worker_id, ShenandoahMarkRefsSuperClosure* mark_closure);\n+\n+  void set_soft_reference_policy(bool clear);\n+\n+  bool discover_reference(oop obj, ReferenceType type) override;\n+\n+  void process_references(WorkGang* workers, bool concurrent);\n+\n+  void work();\n+\n+  void abandon_partial_discovery();\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.hpp","additions":185,"deletions":0,"binary":false,"changes":185,"status":"added"},{"patch":"@@ -72,0 +72,4 @@\n+\n+JRT_LEAF(oopDesc*, ShenandoahRuntime::load_reference_barrier_weak_narrow(oopDesc * src, narrowOop* load_addr))\n+  return (oopDesc*) ShenandoahBarrierSet::barrier_set()->load_reference_barrier<ON_UNKNOWN_OOP_REF, narrowOop>(oop(src), load_addr);\n+JRT_END\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRuntime.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+  static oopDesc* load_reference_barrier_weak_narrow(oopDesc* src, narrowOop* load_addr);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-\/\/    |---------oop---------|-pow-|--chunk---|\n+\/\/    |xx-------oop---------|-pow-|--chunk---|\n@@ -82,0 +82,4 @@\n+\/\/ Lower bits of oop are reserved to handle \"skip_live\" and \"strong\" properties. Since this encoding\n+\/\/ stores uncompressed oops, those bits are always available. These bits default to zero for \"skip_live\"\n+\/\/ and \"weak\". This aligns with their frequent values: strong\/counted-live references.\n+\/\/\n@@ -148,1 +152,3 @@\n-  static const uintptr_t oop_extract_mask       = right_n_bits(oop_bits);\n+  static const uintptr_t oop_extract_mask       = right_n_bits(oop_bits) - 3;\n+  static const uintptr_t skip_live_extract_mask = 1 << 0;\n+  static const uintptr_t weak_extract_mask      = 1 << 1;\n@@ -172,1 +178,9 @@\n-  inline uintptr_t encode_oop(oop obj) const {\n+  inline bool decode_weak(uintptr_t val) const {\n+    return (val & weak_extract_mask) != 0;\n+  }\n+\n+  inline bool decode_cnt_live(uintptr_t val) const {\n+    return (val & skip_live_extract_mask) == 0;\n+  }\n+\n+  inline uintptr_t encode_oop(oop obj, bool skip_live, bool weak) const {\n@@ -174,1 +188,8 @@\n-    return cast_from_oop<uintptr_t>(obj);\n+    uintptr_t encoded = cast_from_oop<uintptr_t>(obj);\n+    if (skip_live) {\n+      encoded |= skip_live_extract_mask;\n+    }\n+    if (weak) {\n+      encoded |= weak_extract_mask;\n+    }\n+    return encoded;\n@@ -186,4 +207,6 @@\n-  ShenandoahMarkTask(oop o = NULL) {\n-    uintptr_t enc = encode_oop(o);\n-    assert(decode_oop(enc) == o,    \"oop encoding should work: \" PTR_FORMAT, p2i(o));\n-    assert(decode_not_chunked(enc), \"task should not be chunked\");\n+  ShenandoahMarkTask(oop o = NULL, bool skip_live = false, bool weak = false) {\n+    uintptr_t enc = encode_oop(o, skip_live, weak);\n+    assert(decode_oop(enc) == o,     \"oop encoding should work: \" PTR_FORMAT, p2i(o));\n+    assert(decode_cnt_live(enc) == !skip_live, \"skip_live encoding should work\");\n+    assert(decode_weak(enc) == weak, \"weak encoding should work\");\n+    assert(decode_not_chunked(enc),  \"task should not be chunked\");\n@@ -193,2 +216,2 @@\n-  ShenandoahMarkTask(oop o, int chunk, int pow) {\n-    uintptr_t enc_oop = encode_oop(o);\n+  ShenandoahMarkTask(oop o, bool skip_live, bool weak, int chunk, int pow) {\n+    uintptr_t enc_oop = encode_oop(o, skip_live, weak);\n@@ -199,0 +222,2 @@\n+    assert(decode_cnt_live(enc) == !skip_live, \"skip_live should be true for chunked tasks\");\n+    assert(decode_weak(enc) == weak,   \"weak encoding should work\");\n@@ -213,0 +238,2 @@\n+  inline bool is_weak()        const { return decode_weak(_obj);        }\n+  inline bool count_liveness() const { return decode_cnt_live(_obj);    }\n@@ -235,0 +262,2 @@\n+  bool _skip_live;\n+  bool _weak;\n@@ -239,2 +268,2 @@\n-  ShenandoahMarkTask(oop o = NULL, int chunk = 0, int pow = 0):\n-    _obj(o), _chunk(chunk), _pow(pow) {\n+  ShenandoahMarkTask(oop o = NULL, bool skip_live = false, bool weak = false, int chunk = 0, int pow = 0):\n+    _obj(o), _skip_live(skip_live), _weak(weak), _chunk(chunk), _pow(pow) {\n@@ -251,0 +280,2 @@\n+  inline bool is_weak()        const { return _weak; }\n+  inline bool count_liveness() const { return !_skip_live; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahTaskqueue.hpp","additions":43,"deletions":12,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-class VM_ShenandoahFinalMarkStartEvac: public VM_ShenandoahReferenceOperation {\n+class VM_ShenandoahFinalMarkStartEvac: public VM_ShenandoahOperation {\n@@ -64,1 +64,1 @@\n-  VM_ShenandoahFinalMarkStartEvac() : VM_ShenandoahReferenceOperation() {};\n+  VM_ShenandoahFinalMarkStartEvac() : VM_ShenandoahOperation() {};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -50,0 +50,11 @@\n+static bool is_instance_ref_klass(Klass* k) {\n+  return k->is_instance_klass() && InstanceKlass::cast(k)->reference_type() != REF_NONE;\n+}\n+\n+class ShenandoahIgnoreReferenceDiscoverer : public ReferenceDiscoverer {\n+public:\n+  virtual bool discover_reference(oop obj, ReferenceType type) {\n+    return true;\n+  }\n+};\n+\n@@ -71,1 +82,6 @@\n-    _loc(NULL) { }\n+    _loc(NULL) {\n+    if (options._verify_marked == ShenandoahVerifier::_verify_marked_complete_except_references ||\n+        options._verify_marked == ShenandoahVerifier::_verify_marked_disable) {\n+      set_ref_discoverer_internal(new ShenandoahIgnoreReferenceDiscoverer());\n+    }\n+  }\n@@ -85,1 +101,3 @@\n-\n+      if (is_instance_ref_klass(obj->klass())) {\n+        obj = ShenandoahForwarding::get_forwardee(obj);\n+      }\n@@ -211,0 +229,4 @@\n+      case ShenandoahVerifier::_verify_marked_complete_except_references:\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+              \"Must be marked in complete bitmap, except j.l.r.Reference referents\");\n+        break;\n@@ -529,2 +551,2 @@\n-    MarkBitMap* mark_bit_map = _heap->complete_marking_context()->mark_bit_map();\n-    HeapWord* tams = _heap->complete_marking_context()->top_at_mark_start(r);\n+    ShenandoahMarkingContext* ctx = _heap->complete_marking_context();\n+    HeapWord* tams = ctx->top_at_mark_start(r);\n@@ -535,1 +557,1 @@\n-      HeapWord* addr = mark_bit_map->get_next_marked_addr(start, tams);\n+      HeapWord* addr = ctx->get_next_marked_addr(start, tams);\n@@ -541,1 +563,1 @@\n-          addr = mark_bit_map->get_next_marked_addr(addr, tams);\n+          addr = ctx->get_next_marked_addr(addr, tams);\n@@ -569,3 +591,4 @@\n-    cl.verify_oops_from(obj);\n-    (*processed)++;\n-\n+    if (!is_instance_ref_klass(obj->klass())) {\n+      cl.verify_oops_from(obj);\n+      (*processed)++;\n+    }\n@@ -721,1 +744,1 @@\n-  if (ShenandoahVerifyLevel >= 4 && marked == _verify_marked_complete) {\n+  if (ShenandoahVerifyLevel >= 4 && (marked == _verify_marked_complete || marked == _verify_marked_complete_except_references)) {\n@@ -796,1 +819,1 @@\n-          _verify_marked_complete,     \/\/ bitmaps as precise as we can get\n+          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n@@ -800,1 +823,1 @@\n-          _verify_gcstate_stable,       \/\/ mark should have stabilized the heap\n+          _verify_gcstate_stable,      \/\/ mark should have stabilized the heap\n@@ -813,6 +836,6 @@\n-          _verify_forwarded_none,    \/\/ no forwarded references\n-          _verify_marked_complete,   \/\/ walk over marked objects too\n-          _verify_cset_disable,      \/\/ non-forwarded references to cset expected\n-          _verify_liveness_complete, \/\/ liveness data must be complete here\n-          _verify_regions_disable,   \/\/ trash regions not yet recycled\n-          _verify_gcstate_stable,    \/\/ mark should have stabilized the heap\n+          _verify_forwarded_none,                    \/\/ no forwarded references\n+          _verify_marked_complete_except_references, \/\/ walk over marked objects too\n+          _verify_cset_disable,                      \/\/ non-forwarded references to cset expected\n+          _verify_liveness_complete,                 \/\/ liveness data must be complete here\n+          _verify_regions_disable,                   \/\/ trash regions not yet recycled\n+          _verify_gcstate_stable,                    \/\/ mark should have stabilized the heap\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":41,"deletions":18,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -68,1 +68,5 @@\n-    _verify_marked_complete\n+    _verify_marked_complete,\n+\n+    \/\/ Objects should be marked in \"complete\" bitmap, except j.l.r.Reference referents, which\n+    \/\/ may be dangling after marking but before conc-weakrefs-processing.\n+    _verify_marked_complete_except_references\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+uint ShenandoahWorkerPolicy::_prev_conc_refs_proc  = 0;\n@@ -66,0 +67,10 @@\n+\/\/ Calculate workers for concurrent refs processing\n+uint ShenandoahWorkerPolicy::calc_workers_for_conc_refs_processing() {\n+  uint active_workers = (_prev_conc_refs_proc == 0) ? ConcGCThreads : _prev_conc_refs_proc;\n+  _prev_conc_refs_proc =\n+    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n+                                           active_workers,\n+                                           Threads::number_of_non_daemon_threads());\n+  return _prev_conc_refs_proc;\n+}\n+\n@@ -70,3 +81,3 @@\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n+          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n+                                                 active_workers,\n+                                                 Threads::number_of_non_daemon_threads());\n@@ -126,5 +137,0 @@\n-uint ShenandoahWorkerPolicy::calc_workers_for_conc_preclean() {\n-  \/\/ Precleaning is single-threaded\n-  return 1;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.cpp","additions":14,"deletions":8,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+  static uint _prev_conc_refs_proc;\n@@ -56,0 +57,3 @@\n+  \/\/ Calculate workers for concurrent refs processing\n+  static uint calc_workers_for_conc_refs_processing();\n+\n@@ -71,3 +75,0 @@\n-  \/\/ Calculate workers for concurrent precleaning\n-  static uint calc_workers_for_conc_preclean();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -79,7 +79,0 @@\n-  product(uintx, ShenandoahRefProcFrequency, 5, EXPERIMENTAL,               \\\n-          \"Process process weak (soft, phantom, finalizers) references \"    \\\n-          \"every Nth cycle. Normally affects concurrent GC cycles only, \"   \\\n-          \"as degenerated and full GCs would try to process references \"    \\\n-          \"regardless. Set to zero to disable reference processing \"        \\\n-          \"completely.\")                                                    \\\n-                                                                            \\\n@@ -316,5 +309,0 @@\n-  product(bool, ShenandoahPreclean, true, DIAGNOSTIC,                       \\\n-          \"Do concurrent preclean phase before final mark: process \"        \\\n-          \"definitely alive references to avoid dealing with them during \"  \\\n-          \"pause.\")                                                         \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+ * @requires vm.gc != \"Shenandoah\"\n","filename":"test\/hotspot\/jtreg\/gc\/TestSoftReferencesBehaviorOnOOME.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}