{"files":[{"patch":"@@ -49,0 +49,4 @@\n+\/\/ This sets the OOM bit for a single counter.  If decrement is true, it also decrements the count of evacuating threads\n+\/\/ associated with this counter.  After all _num_counters OOM bits have been set, all threads newly attempting to enter_evacuation\n+\/\/ will be informed that they cannot allocate for evacation.  Threads that entered evacuation before the OOM bit was set may\n+\/\/ continue to allocate for evacuation until they exit_evacuation.\n@@ -58,1 +62,1 @@\n-      \/\/ Success: wait for other threads to get out of the protocol and return.\n+      \/\/ Success: return so we can wait for other threads to stop allocating.\n@@ -97,0 +101,3 @@\n+#ifdef ASSERT\n+  Atomic::store(&_evacuation_state, _evacuating);\n+#endif\n@@ -124,1 +131,2 @@\n-void ShenandoahEvacOOMHandler::wait_for_one_counter(ShenandoahEvacOOMCounter* ptr) {\n+\/\/ Wait until this counter's OOM bit is set and there are no more evacuating threads associated with the counter.\n+void ShenandoahEvacOOMHandler::wait_for_no_evac_threads_on_counter(ShenandoahEvacOOMCounter* counter) {\n@@ -128,1 +136,1 @@\n-  while (ptr->load_acquire() != ShenandoahEvacOOMCounter::OOM_MARKER_MASK) {\n+  while (counter->load_acquire() != ShenandoahEvacOOMCounter::OOM_MARKER_MASK) {\n@@ -133,0 +141,5 @@\n+\/\/ Wait until every counter's OOM bit is set and the number of evacuating threads associated with every counter is zero.\n+\/\/ This assures that this thread waits for some other thread's handle_out_of_memory_during_evacuation() call to finish\n+\/\/ setting the OOM bit in all counters before this thread proceeds.\n+\/\/\n+\/\/ Then disable further allocations by the current thread by setting its thread-local oom_during_evac flag to true.\n@@ -137,1 +150,1 @@\n-    wait_for_one_counter(&_threads_in_evac[i]);\n+    wait_for_no_evac_threads_on_counter(&_threads_in_evac[i]);\n@@ -143,0 +156,3 @@\n+#ifdef ASSERT\n+  Atomic::store(&_evacuation_state, _oom_not_evacuating);\n+#endif\n@@ -145,0 +161,10 @@\n+\/\/ Increment the count of evacuating threads if this thread is authorized to allocate and no other allocating thread\n+\/\/ has experienced out-of-memory when attempting an evacuation allocation.\n+\/\/\n+\/\/ Upon return:\n+\/\/\n+\/\/  1. The thread is authorized to allocate for evacuation and the count of allocating threads has been incremented to\n+\/\/     include this thread, or\n+\/\/  2. The thread is not authorized to allocate for evacuation and the count of allocating thread does not include this thread.\n+\/\/\n+\/\/ Thread-local flag is_oom_during_evac(thr) is false iff thread thr is authorized to allocate for evacuation.\n@@ -155,0 +181,11 @@\n+\/\/ Decrement the count of evacuating threads if this thread is still authorized to allocate for evacuation.\n+\/\/\n+\/\/ Upon return:\n+\/\/\n+\/\/  1. The thread is authorized to allocate for evacuation.\n+\/\/  2. The count of threads that are authorized to allocate for evacuations does not include this thread.\n+\/\/\n+\/\/ Note: Authorizing the thread to allocate for evacuation has \"no effect\".  This is simply the \"presumed\" default state\n+\/\/       of every thread.  When\/if this thread subsequently attempts to re-register, we will check whether further\n+\/\/       allocations are authorized by this thread and we will adjust the thread-local authorization flag (is_oom_during_evac)\n+\/\/       if necessary.  The thread will not attempt to allocate for evacuation without first re-registering.\n@@ -167,0 +204,21 @@\n+\/\/ The current thread failed to allocate memory required by evacuation.  Perform the following:\n+\/\/\n+\/\/ Upon entry:\n+\/\/\n+\/\/  1. The current thread is known to be authorized to allocate for evacuation.\n+\/\/\n+\/\/ Upon return:\n+\/\/\n+\/\/  1. The OOM bit is set for every counter.\n+\/\/  2. This thread's thread-local is_oom_during_evac flag is true, denoting that this thread is no longer authorized\n+\/\/     to perform evacuation allocations.\n+\/\/  3. The count of threads authorized to evacuate for allocation has been decremented, because this thread is no\n+\/\/     longer authorized.\n+\/\/  4. We have waited for all evacuating threads to stop allocating, after which it is safe for this thread to resolve\n+\/\/     remaining objects as either forwarded or not forwarded.  Hereafter, the status of these objects will not\n+\/\/     change until we STW to perform full GC.\n+\/\/\n+\/\/ Note: Multiple threads may handle_out_of_memory_during_evacuation() at the same time.  Setting the OOM bit on every\n+\/\/       counter is idempotent.  Any particular thread will execute handle_out_of_memory_during_evacuation() only once\n+\/\/       per GC cycle.\n+\/\/\n@@ -182,0 +240,2 @@\n+\/\/ This method resets the count of evacuating threads to zero and clears the OOM bit for each counter.\n+\/\/ We call this at the start of each GC cycle.\n@@ -187,0 +247,3 @@\n+#ifdef ASSERT\n+  Atomic::store(&_evacuation_state, _evacuating);\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacOOMHandler.cpp","additions":67,"deletions":4,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -62,26 +62,69 @@\n- * load-reference-barrier (i.e. it cannot copy the object to to-space), it does not\n- * necessarily follow we can return immediately from the LRB (and store to from-space).\n- *\n- * In very basic case, on such failure we may wait until the evacuation is over,\n- * and then resolve the forwarded copy, and to the store there. This is possible\n- * because other threads might still have space in their GCLABs, and successfully\n- * evacuate the object.\n- *\n- * But, there is a race due to non-atomic evac_in_progress transition. Consider\n- * thread A is stuck waiting for the evacuation to be over -- it cannot leave with\n- * from-space copy yet. Control thread drops evacuation_in_progress preparing for\n- * next STW phase that has to recover from OOME. Thread B misses that update, and\n- * successfully evacuates the object, does the write to to-copy. But, before\n- * Thread B is able to install the fwdptr, thread A discovers evac_in_progress is\n- * down, exits from here, reads the fwdptr, discovers old from-copy, and stores there.\n- * Thread B then wakes up and installs to-copy. This breaks to-space invariant, and\n- * silently corrupts the heap: we accepted two writes to separate copies of the object.\n- *\n- * The way it is solved here is to maintain a counter of threads inside the\n- * 'evacuation path'. The 'evacuation path' is the part of evacuation that does the actual\n- * allocation, copying and CASing of the copy object, and is protected by this\n- * OOM-during-evac-handler. The handler allows multiple threads to enter and exit\n- * evacuation path, but on OOME it requires all threads that experienced OOME to wait\n- * for current threads to leave, and blocks other threads from entering. The counter state\n- * is striped across multiple cache lines to reduce contention when many threads attempt\n- * to enter or leave the protocol at the same time.\n+ * load-reference-barrier (i.e. it cannot copy the object to to-space), a special\n+ * protocol is required to assure that all threads see the same version of every\n+ * object.\n+ *\n+ * This file and its accompanying .cpp and .inline.hpp files hold the implementation\n+ * of this protocol.  The general idea is as follows:\n+ *\n+ *  1. If we fail to evacuate the entirety of live memory from all cset regions,\n+ *     we will transition first to degenerated GC and if that fails, to full GC at\n+ *     the end of this cancelled concurrent evacuation phase.  To heal heap invariants\n+ *     that may be violated temporarily during the Evac-OOM protocol, degenerated GC\n+ *     overwrites the update_watermark field of every heap region with its current top\n+ *     (because a pointer to the cset may have been leaked through a failed LRB and\n+ *     written into the heap above the original value of update_watermark).  Furthermore,\n+ *     the degenerated GC restarts the evacuation effort, revisiting every region in\n+ *     the cset (because we have no assurance that cset regions already evacuated during\n+ *     the concurrent evacuation were fully evacuated; at least one object failed to\n+ *     properly evacuate).  Degen GC evacuation may succeed after concurrent evacuation\n+ *     failed in the case that degeneration is triggered by failure of a mutator thread's\n+ *     LRB if there is more memory available within the GCLABs of GC worker threads than\n+ *     was available in the failing mutator's GCLAB.  Note, however, that the failed\n+ *     mutator thread was unable to refresh its GCLAB, so it is now unlikely that any\n+ *     GC Worker threads will be able to further expand their GCLABs.  This combination\n+ *     of circumstances contributes to the likelihood that degenerated GC will find it\n+ *     necessary to upgrade to Full GC.  Full GC marks and compacts the entire heap,\n+ *     redundantly repeating the marking and evacuation work that had just been completed\n+ *     by the cancelled concurrent and degenerated GC efforts.  Full GC marking includes\n+ *     resolution of lingering forwarding pointers that may be scattered throughout\n+ *     the previously selected cset when evacuation is abandoned before it has been\n+ *     completed.\n+ *\n+ *  2. If any thread A fails to evacuate object X, it will wait to see if some\n+ *     other mutator or GC worker thread can successfully evacuate object X.  At the moment\n+ *     thread A fails to allocate, it launches the OOM-during-evacuation protocol.  There\n+ *     is no going back (even though some other thread may successfully evacuate object X).\n+ *\n+ *  3. The protocol consists of:\n+ *\n+ *     a) Thread A sets internal state to indicate that OOM-during-evac has been\n+ *        encountered.\n+ *     b) Thread A now waits for all other threads to finish any ongoing allocations\n+ *        for evacuation that might be in process.\n+ *     c) Other threads that subsequently announce intent to allocate for evacuation are\n+ *        informed that the OOM-during-evac protocol has been initated.  As with thread A,\n+ *        these threads also wait for all other threads to finish any ongoing allocations\n+ *        for evacuation that might be in process.\n+ *     d) After all threads have finished whatever allocations for evacuation they\n+ *        were in the process of performing, the evacution state of the heap is considered\n+ *        to be \"frozen\".  At this point, some cset objects may have been successfully\n+ *        evacuated and some cset objects may have failed to evacuate.  There will be\n+ *        no more evaucations until we STW and perform Full GC.\n+ *     e) Now, all of the threads that were waiting for evacuating threads to finish\n+ *        allocations that were in progress are allowed to run, but they are not allowed\n+ *        to allocate for evacuation.  Additional threads that newly announce intent to\n+ *        allocate for evacuation are immediately allowed to continue running, also without\n+ *        authorization to allocate.\n+ *     f) Threads that desire to allocate for evacuation but are not authorized to do so\n+ *        simply consult the head of each cset object.  If the header denotes that the\n+ *        object has been evacuated by a different thread, then this thread will replace\n+ *        its pointer to the object with a pointer to the new location.  If the header\n+ *        denotes that this object has not yet been copied, this thread will continue to\n+ *        use the original cset location as the official version of the object.  Since\n+ *        no threads are allowed to allocate for evacuation in this phase, all threads\n+ *        accessing this same object will agree to refer to this object at its original\n+ *        location within the cset.\n+ *     g) Evacuation is cancelled and all threads will eventually reach a Full GC\n+ *        safepoint.  Marking by Full GC will finish updating references that might\n+ *        be inconsistent within the heap, and will then compact all live memory within\n+ *        the heap.\n@@ -91,0 +134,2 @@\n+ * Maintain a count of how many threads are on an evac-path (which is allocating for evacuation)\n+ *\n@@ -97,1 +142,4 @@\n- *     counter drops to 0, then exit with resolving the ptr\n+ *     counter drops to 0, then continue without authorization to allocate.\n+ *     As such, the thread will treat unforwarded cset objects as residing\n+ *     permanently at their original location.\n+ *\n@@ -103,1 +151,4 @@\n- *   - success: busy-loop until counter drops to zero, then exit with resolve\n+ *   - success: busy-loop until counter drops to zero, Then continue\n+ *     to execute without authnorization to allocate.  Any unforwarded\n+ *     cset objects will be treated as residing permanently at their original\n+ *     location.\n@@ -107,1 +158,46 @@\n- *       zero, then exit with resolve\n+ *       zero, then continue to execute without autnorization to allocate,\n+ *       as above.\n+ *\/\n+\n+\/*\n+ * For most service workloads, OOM-during-evac will be very rare.  Most services are provisioned\n+ * with enough memory and CPU cores to avoid experiencing OOM during evac.  The typical cause for\n+ * OOM during evac is a spike in client requests, possibly related to a DOS attack.  In some cases,\n+ * OOM during evac can also occur because the heap becomes fragmented.  For example, it may not be\n+ * possible to find contiguous memory to evacuate an object that is 50% of the heap region size even\n+ * though there is an abundance of \"fragmented\" memory available to support evacuation of thousands of\n+ * smaller (more normal-sized) objects.  When OOM during evac does occur, there are opportunities to\n+ * make the protocol more efficient.\n+ *\n+ * TODO: make refinements to the OOM-during-evac protocol so that it is less disruptive and more efficient.\n+ *\n+ *  1. Allow a mutator or GC worker thread that fails to allocate for evacuation to mark a single\n+ *     cset object as frozen-in-from-space and then continue to evacuate other objects while other\n+ *     threads continue to evacuate other objects.  A draft solution is described here, along with discussion\n+ *     of prerequisites required for full implementation:  https:\/\/github.com\/openjdk\/jdk\/pull\/12881\n+ *     This allows all threads, including the one that failed to evacuate a single object, to fully utilize\n+ *     all of the memory available within their existing GCLABs and allows more of evacuation to be\n+ *     performed concurrently rather than requiring STW Full GC.\n+ *\n+ *  2. At the end of evacuation, if there were any failures to evacuate, fixup the cset before\n+ *     we go to update-refs.  This can be done concurrently.  Fixup consists of:\n+ *\n+ *     a. Take region out of cset if it contains objects that failed to evacuate.\n+ *\n+ *     b. For each such region, set top to be address following last object that failed to evacuate.\n+ *        Mangle memory above the new top as appropriate.\n+ *\n+ *     c. For each such region, make the garbage found below and between uncopied objects parseable:\n+ *        overwrite each run of garbage with array-of-integer object of appropriate size.  Generational\n+ *        Shenandoah calls this coalesce-and-fill.\n+ *\n+ *  3. Do not automatically degenerate or upgrade to Full GC.  Continue with concurrent GC as long as possible.\n+ *     The desired result is that the next concurrent GC will start a bit sooner (given that we failed to\n+ *     reclaim all of the garbage in this cycle's cset so we will have less allocation runway to idle before\n+ *     the next concurrent GC is triggered).  In the ideal, the next concurrent GC will finish cleaning up\n+ *     the cset regions that were abandoned in this cycle.  There is already a mechanism in place to escalate\n+ *     to Full GC if the mutator experiences out-of-memory and\/or if concurrent GC is not \"productive\".\n+ *     Transitions to Full GC are very costly because (i) this results in a very long STW pause during which\n+ *     mutator threads are unresponsive, and (ii) Full GC redundantly repeats work that was already successfully\n+ *     performed concurrently and\/or by degenerated GC.  When OOM-during-evac transitions to Full GC, we throw\n+ *     away and repeat all of the previously completed work of marking and evacuating.\n@@ -110,0 +206,8 @@\n+#ifdef ASSERT\n+public:\n+  enum ShenandoahEvacuationState {\n+    _evacuating,\n+    _oom_not_evacuating\n+  };\n+  volatile ShenandoahEvacuationState _evacuation_state;\n+#endif\n@@ -119,1 +223,1 @@\n-  void wait_for_one_counter(ShenandoahEvacOOMCounter* ptr);\n+  void wait_for_no_evac_threads_on_counter(ShenandoahEvacOOMCounter* counter);\n@@ -127,1 +231,10 @@\n-   * Attempt to enter the protected evacuation path.\n+   * Enter a protected evacuation path.\n+   *\n+   * Upon return:\n+   *\n+   *  1. Thread t has authorization to allocate for evacuation and the count of evacuating threads includes thread t, or\n+   *\n+   *  2. Thread t has no authorization to allocate for evacuation and the count of evacuating threads does not include\n+   *     thread t.\n+   *\n+   * This function may pause while it waits for coordination with other allocating threads.\n@@ -129,3 +242,5 @@\n-   * When this returns true, it is safe to continue with normal evacuation.\n-   * When this method returns false, evacuation must not be entered, and caller\n-   * may safely continue with a simple resolve (if Java thread).\n+   * Authority to allocate for evacuation is represented by thread-local flag is_oom_during_evac(t) equal to false.\n+   * If this thread is not authorized to allocate and it encounters an object residing within the cset, it uses\n+   * the most current location of the object, as represented by the object's header.  If the object was not previously\n+   * evacuated, the evac-OOM protocol assures that the object will not be subsequently evacuated during the remainder\n+   * of the concurrent evacuation phase.\n@@ -136,1 +251,1 @@\n-   * Leave evacuation path.\n+   * Leave a protected evacuation path.\n@@ -141,1 +256,1 @@\n-   * Signal out-of-memory during evacuation. It will prevent any other threads\n+   * Signal out-of-memory during evacuation. This will prevent any other threads\n@@ -143,1 +258,4 @@\n-   * evacuation path, and then return. It is then safe to continue with a simple resolve.\n+   * evacuation path, and then return. Following this, it is safe to assume that\n+   * any object residing in the cset and not previously forwarded will remain in\n+   * the cset throughout the remainder of the concurrent evacuation phase.  It will\n+   * not be subsequently evacuated.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacOOMHandler.hpp","additions":155,"deletions":37,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -42,0 +42,17 @@\n+\/\/ Announce the intent by thread thr to perform allocations for evacuation.\n+\/\/\n+\/\/ Upon return:\n+\/\/\n+\/\/  1. The count of nested allocate-for-evacuation scopes for this thread has been incremented.\n+\/\/  2. Thread thr is authorized to allocate for evacuation and the count of allocating threads represents this thread, or\n+\/\/  3. Thread thr is not authorized to allocate for evacuation and the count of allocating thread does not include this thread.\n+\/\/\n+\/\/ Thread-local flag is_oom_during_evac(thr) is false iff thread thr is authorized to allocate for evacuation.\n+\/\/\n+\/\/ Notes: If this thread subsequently encounters a \"need\" to allocate memory for evacuation but it is not authorized to\n+\/\/        allocate for evacuation, this thread will simply treat the relevant cset object as \"frozen within from-space\".\n+\/\/        If this thread is forbidden to allocate, then all threads are forbidden to allocate.  As soon as a first thread\n+\/\/        begins to execute within an \"evacuation region\" without authorization to allocate, the evac-OOM protocol requires\n+\/\/        that no additional objects be evacuated.  Normally, this phase of executing without authorization to evacuate is\n+\/\/        immediately followed by a Full GC which compacts all of heap memory in STW mode.\n+\n@@ -44,12 +61,21 @@\n- if (level == 0) {\n-   \/\/ Entering top level scope, register this thread.\n-   register_thread(thr);\n- } else if (!ShenandoahThreadLocalData::is_oom_during_evac(thr)) {\n-   ShenandoahEvacOOMCounter* counter = counter_for_thread(thr);\n-   jint threads_in_evac = counter->load_acquire();\n-   \/\/ If OOM is in progress, handle it.\n-   if ((threads_in_evac & ShenandoahEvacOOMCounter::OOM_MARKER_MASK) != 0) {\n-     counter->decrement();\n-     wait_for_no_evac_threads();\n-   }\n- }\n+  if (level == 0) {\n+    \/\/ Entering top level scope, register this thread.\n+    register_thread(thr);\n+  } else if (!ShenandoahThreadLocalData::is_oom_during_evac(thr)) {\n+    ShenandoahEvacOOMCounter* counter = counter_for_thread(thr);\n+    jint threads_in_evac = counter->load_acquire();\n+    \/\/ If OOM is in progress, handle it.\n+    if ((threads_in_evac & ShenandoahEvacOOMCounter::OOM_MARKER_MASK) != 0) {\n+      counter->decrement();\n+      wait_for_no_evac_threads();\n+    }\n+  }\n+#ifdef ASSERT\n+  ShenandoahEvacuationState state = Atomic::load(&_evacuation_state);\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thr)) {\n+    \/\/ This thread is not authorized to allocate\n+    assert(state == _oom_not_evacuating, \"Thread no longer evacuating implies no threads are evacuating\");\n+  } else {\n+    assert(state == _evacuating, \"Thread evacuating implies all threads are evacuating\");\n+  }\n+#endif\n@@ -58,0 +84,12 @@\n+\/\/ Announce intent to leave a control scope that performs allocation for evacuation.\n+\/\/\n+\/\/ Upon return:\n+\/\/\n+\/\/ 1. The thread-local count of nested allocation-for-evacuation scopes for this thread has been decremented.\n+\/\/ 2. If we have left the outer-most allocation-for-evacuation scope for this thread:\n+\/\/    a. The count of threads that are allocating for evacuation does not represent this thread\n+\/\/    b. This thread is authorized to allocate for evacuation.\n+\/\/\n+\/\/ Notes: A thread that has already entered evacuation and not left may make a nested re-entry into evacuation.  Each nested\n+\/\/        invocation of enter_evacuation should be matched by invocation of leave_evacuation.\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacOOMHandler.inline.hpp","additions":50,"deletions":12,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -294,0 +294,5 @@\n+#ifdef ASSERT\n+    ShenandoahEvacOOMHandler::ShenandoahEvacuationState state = Atomic::load(&(_oom_evac_handler._evacuation_state));\n+    assert(state == ShenandoahEvacOOMHandler::_oom_not_evacuating,\n+           \"all threads must be not evacuating if any thread is not evacuating\");\n+#endif\n@@ -296,0 +301,5 @@\n+#ifdef ASSERT\n+  ShenandoahEvacOOMHandler::ShenandoahEvacuationState state = Atomic::load(&(_oom_evac_handler._evacuation_state));\n+  assert(state == ShenandoahEvacOOMHandler::_evacuating,\n+         \"all threads must be evacuating if any thread is evacuating\");\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"}]}