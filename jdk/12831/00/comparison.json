{"files":[{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -161,0 +161,4 @@\n+        return primitiveLayoutForSize(size, useFloat).carrier();\n+    }\n+\n+    public static ValueLayout primitiveLayoutForSize(long size, boolean useFloat) {\n@@ -163,1 +167,1 @@\n-                return float.class;\n+                return JAVA_FLOAT;\n@@ -165,1 +169,1 @@\n-                return double.class;\n+                return JAVA_DOUBLE;\n@@ -169,1 +173,1 @@\n-                return byte.class;\n+                return JAVA_BYTE;\n@@ -171,1 +175,1 @@\n-                return short.class;\n+                return JAVA_SHORT;\n@@ -173,1 +177,1 @@\n-                return int.class;\n+                return JAVA_INT;\n@@ -175,1 +179,1 @@\n-                return long.class;\n+                return JAVA_LONG;\n@@ -179,1 +183,1 @@\n-        throw new IllegalArgumentException(\"No type for size: \" + size + \" isFloat=\" + useFloat);\n+        throw new IllegalArgumentException(\"No layout for size: \" + size + \" isFloat=\" + useFloat);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java","additions":12,"deletions":8,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,0 +47,1 @@\n+import java.lang.foreign.ValueLayout;\n@@ -68,0 +69,1 @@\n+    private static final int MAX_COPY_SIZE = 8;\n@@ -220,2 +222,2 @@\n-        void alignStack(long alignment) {\n-            stackOffset = Utils.alignUp(stackOffset, alignment);\n+        private boolean hasRegister(int type) {\n+            return hasEnoughRegisters(type, 1);\n@@ -224,11 +226,2 @@\n-        VMStorage stackAlloc(long size, long alignment) {\n-            assert forArguments : \"no stack returns\";\n-            long alignedStackOffset = Utils.alignUp(stackOffset, alignment);\n-\n-            short encodedSize = (short) size;\n-            assert (encodedSize & 0xFFFF) == size;\n-\n-            VMStorage storage =\n-                AArch64Architecture.stackStorage(encodedSize, (int)alignedStackOffset);\n-            stackOffset = alignedStackOffset + size;\n-            return storage;\n+        private boolean hasEnoughRegisters(int type, int count) {\n+            return nRegs[type] + count <= MAX_REGISTER_ARGUMENTS;\n@@ -237,22 +230,5 @@\n-        VMStorage stackAlloc(MemoryLayout layout) {\n-            long stackSlotAlignment = requiresSubSlotStackPacking() && !forVarArgs\n-                    ? layout.byteAlignment()\n-                    : Math.max(layout.byteAlignment(), STACK_SLOT_SIZE);\n-            return stackAlloc(layout.byteSize(), stackSlotAlignment);\n-        }\n-\n-        VMStorage[] regAlloc(int type, int count) {\n-            if (nRegs[type] + count <= MAX_REGISTER_ARGUMENTS) {\n-                ABIDescriptor abiDescriptor = abiDescriptor();\n-                VMStorage[] source =\n-                    (forArguments ? abiDescriptor.inputStorage : abiDescriptor.outputStorage)[type];\n-                VMStorage[] result = new VMStorage[count];\n-                for (int i = 0; i < count; i++) {\n-                    result[i] = source[nRegs[type]++];\n-                }\n-                return result;\n-            } else {\n-                \/\/ Any further allocations for this register type must\n-                \/\/ be from the stack.\n-                nRegs[type] = MAX_REGISTER_ARGUMENTS;\n-                return null;\n+        private static Class<?> adjustCarrierForStack(Class<?> carrier) {\n+            if (carrier == float.class) {\n+                carrier = int.class;\n+            } else if (carrier == double.class) {\n+                carrier = long.class;\n@@ -260,0 +236,1 @@\n+            return carrier;\n@@ -262,2 +239,38 @@\n-        VMStorage[] regAlloc(int type, MemoryLayout layout) {\n-            boolean spillRegistersPartially = forVariadicFunction && spillsVariadicStructsPartially();\n+        record StructStorage(long offset, Class<?> carrier, VMStorage storage) {}\n+\n+        \/*\n+        In the simplest case structs are copied in chunks. i.e. the fields don't matter, just the size.\n+        The struct is split into 8-byte chunks, and those chunks are either passed in registers and\/or on the stack.\n+\n+        Homogeneous float aggregates (HFAs) can be copied in a field-wise manner, i.e. the struct is split into it's\n+        fields and those fields are the chunks which are passed. For HFAs the rules are more complicated and ABI based:\n+\n+                        | enough registers | some registers, but not enough  | no registers\n+        ----------------+------------------+---------------------------------+-------------------------\n+        Linux           | FW in regs       | CW on the stack                 | CW on the stack\n+        MacOs, non-VA   | FW in regs       | FW on the stack                 | FW on the stack\n+        MacOs, VA       | FW in regs       | CW on the stack                 | CW on the stack\n+        Windows, non-VF | FW in regs       | CW on the stack                 | CW on the stack\n+        Windows, VF     | FW in regs       | CW split between regs and stack | CW on the stack\n+        (where FW = Field-wise copy, CW = Chunk-wise copy, VA is a variadic argument, and VF is a variadic function)\n+\n+        For regular structs, the rules are as follows:\n+\n+                        | enough registers | some registers, but not enough  | no registers\n+        ----------------+------------------+---------------------------------+-------------------------\n+        Linux           | CW in regs       | CW on the stack                 | CW on the stack\n+        MacOs           | CW in regs       | CW on the stack                 | CW on the stack\n+        Windows, non-VF | CW in regs       | CW on the stack                 | CW on the stack\n+        Windows, VF     | CW in regs       | CW split between regs and stack | CW on the stack\n+         *\/\n+        StructStorage[] structStorages(GroupLayout layout, boolean forHFA) {\n+            int numChunks = (int)Utils.alignUp(layout.byteSize(), MAX_COPY_SIZE) \/ MAX_COPY_SIZE;\n+\n+            int regType = StorageType.INTEGER;\n+            List<MemoryLayout> scalarLayouts = null;\n+            int requiredStorages = numChunks;\n+            if (forHFA) {\n+                regType = StorageType.VECTOR;\n+                scalarLayouts = TypeClass.scalarLayouts(layout);\n+                requiredStorages = scalarLayouts.size();\n+            }\n@@ -265,4 +278,1 @@\n-            return spillRegistersPartially ?\n-                regAllocPartial(type, layout) :\n-                regAlloc(type, requiredRegisters(layout));\n-        }\n+            boolean hasEnoughRegisters = hasEnoughRegisters(regType, requiredStorages);\n@@ -270,3 +280,9 @@\n-        int requiredRegisters(MemoryLayout layout) {\n-            return (int)Utils.alignUp(layout.byteSize(), 8) \/ 8;\n-        }\n+            \/\/ For the ABI variants that pack arguments spilled to the\n+            \/\/ stack, HFA arguments are spilled as if their individual\n+            \/\/ fields had been allocated separately rather than as if the\n+            \/\/ struct had been spilled as a whole.\n+            boolean useFieldWiseSpill = requiresSubSlotStackPacking() && !forVarArgs;\n+            boolean isFieldWise = forHFA && (hasEnoughRegisters || useFieldWiseSpill);\n+            if (!isFieldWise) {\n+                requiredStorages = numChunks;\n+            }\n@@ -274,4 +290,6 @@\n-        VMStorage[] regAllocPartial(int type, MemoryLayout layout) {\n-            int availableRegisters = MAX_REGISTER_ARGUMENTS - nRegs[type];\n-            if (availableRegisters <= 0) {\n-                return null;\n+            boolean spillPartially = forVariadicFunction && spillsVariadicStructsPartially();\n+            boolean furtherAllocationFromTheStack = !hasEnoughRegisters && !spillPartially;\n+            if (furtherAllocationFromTheStack) {\n+                \/\/ Any further allocations for this register type must\n+                \/\/ be from the stack.\n+                nRegs[regType] = MAX_REGISTER_ARGUMENTS;\n@@ -280,3 +298,5 @@\n-            int requestRegisters = Math.min(requiredRegisters(layout), availableRegisters);\n-            return regAlloc(type, requestRegisters);\n-        }\n+            if (requiresSubSlotStackPacking() && !isFieldWise) {\n+                \/\/ Pad to the next stack slot boundary instead of packing\n+                \/\/ additional arguments into the unused space.\n+                alignStack(STACK_SLOT_SIZE);\n+            }\n@@ -284,4 +304,13 @@\n-        VMStorage nextStorage(int type, MemoryLayout layout) {\n-            if (type == StorageType.VECTOR) {\n-                boolean forVariadicFunctionArgs = forArguments && forVariadicFunction;\n-                boolean useIntRegsForFloatingPointArgs = forVariadicFunctionArgs && useIntRegsForVariadicFloatingPointArgs();\n+            StructStorage[] structStorages = new StructStorage[requiredStorages];\n+            long offset = 0;\n+            for (int i = 0; i < structStorages.length; i++) {\n+                ValueLayout copyLayout;\n+                if (isFieldWise) {\n+                    \/\/ We should only get here for HFAs, which can't have padding\n+                    copyLayout = (ValueLayout) scalarLayouts.get(i);\n+                } else {\n+                    \/\/ chunk-wise copy\n+                    long copySize = Math.min(layout.byteSize() - offset, MAX_COPY_SIZE);\n+                    boolean useFloat = false; \/\/ never use float for chunk-wise copies\n+                    copyLayout = SharedUtils.primitiveLayoutForSize(copySize, useFloat);\n+                }\n@@ -289,2 +318,6 @@\n-                if (useIntRegsForFloatingPointArgs) {\n-                    type = StorageType.INTEGER;\n+                VMStorage storage = nextStorage(regType, copyLayout);\n+                Class<?> carrier = copyLayout.carrier();\n+                if (isFieldWise && storage.type() == StorageType.STACK) {\n+                    \/\/ copyLayout is a field of an HFA\n+                    \/\/ Don't use floats on the stack\n+                    carrier = adjustCarrierForStack(carrier);\n@@ -292,0 +325,2 @@\n+                structStorages[i] = new StructStorage(offset, carrier, storage);\n+                offset += copyLayout.byteSize();\n@@ -294,3 +329,4 @@\n-            VMStorage[] storage = regAlloc(type, 1);\n-            if (storage == null) {\n-                return stackAlloc(layout);\n+            if (requiresSubSlotStackPacking() && !isFieldWise) {\n+                \/\/ Pad to the next stack slot boundary instead of packing\n+                \/\/ additional arguments into the unused space.\n+                alignStack(STACK_SLOT_SIZE);\n@@ -299,1 +335,1 @@\n-            return storage[0];\n+            return structStorages;\n@@ -302,13 +338,3 @@\n-        VMStorage[] nextStorageForHFA(GroupLayout group) {\n-            final int nFields = group.memberLayouts().size();\n-            VMStorage[] regs = regAlloc(StorageType.VECTOR, nFields);\n-            if (regs == null && requiresSubSlotStackPacking() && !forVarArgs) {\n-                \/\/ For the ABI variants that pack arguments spilled to the\n-                \/\/ stack, HFA arguments are spilled as if their individual\n-                \/\/ fields had been allocated separately rather than as if the\n-                \/\/ struct had been spilled as a whole.\n-\n-                VMStorage[] slots = new VMStorage[nFields];\n-                for (int i = 0; i < nFields; i++) {\n-                    slots[i] = stackAlloc(group.memberLayouts().get(i));\n-                }\n+        private void alignStack(long alignment) {\n+            stackOffset = Utils.alignUp(stackOffset, alignment);\n+        }\n@@ -316,4 +342,24 @@\n-                return slots;\n-            } else {\n-                return regs;\n-            }\n+        \/\/ allocate a single ValueLayout, either in a register or on the stack\n+        VMStorage nextStorage(int type, ValueLayout layout) {\n+            return hasRegister(type) ? regAlloc(type) : stackAlloc(layout);\n+        }\n+\n+        private VMStorage regAlloc(int type) {\n+            ABIDescriptor abiDescriptor = abiDescriptor();\n+            VMStorage[] source = (forArguments ? abiDescriptor.inputStorage : abiDescriptor.outputStorage)[type];\n+            return source[nRegs[type]++];\n+        }\n+\n+        private VMStorage stackAlloc(ValueLayout layout) {\n+            assert forArguments : \"no stack returns\";\n+            long stackSlotAlignment = requiresSubSlotStackPacking() && !forVarArgs\n+                    ? layout.byteAlignment()\n+                    : Math.max(layout.byteAlignment(), STACK_SLOT_SIZE);\n+            long alignedStackOffset = Utils.alignUp(stackOffset, stackSlotAlignment);\n+\n+            short encodedSize = (short) layout.byteSize();\n+            assert (encodedSize & 0xFFFF) == layout.byteSize();\n+\n+            VMStorage storage = AArch64Architecture.stackStorage(encodedSize, (int)alignedStackOffset);\n+            stackOffset = alignedStackOffset + layout.byteSize();\n+            return storage;\n@@ -338,55 +384,0 @@\n-        protected void spillStructUnbox(Binding.Builder bindings, MemoryLayout layout) {\n-            \/\/ If a struct has been assigned register or HFA class but\n-            \/\/ there are not enough free registers to hold the entire\n-            \/\/ struct, it must be passed on the stack. I.e. not split\n-            \/\/ between registers and stack.\n-\n-            spillPartialStructUnbox(bindings, layout, 0);\n-        }\n-\n-        protected void spillPartialStructUnbox(Binding.Builder bindings, MemoryLayout layout, long offset) {\n-            while (offset < layout.byteSize()) {\n-                long copy = Math.min(layout.byteSize() - offset, STACK_SLOT_SIZE);\n-                VMStorage storage =\n-                    storageCalculator.stackAlloc(copy, STACK_SLOT_SIZE);\n-                if (offset + STACK_SLOT_SIZE < layout.byteSize()) {\n-                    bindings.dup();\n-                }\n-                Class<?> type = SharedUtils.primitiveCarrierForSize(copy, false);\n-                bindings.bufferLoad(offset, type)\n-                        .vmStore(storage, type);\n-                offset += STACK_SLOT_SIZE;\n-            }\n-\n-            if (requiresSubSlotStackPacking()) {\n-                \/\/ Pad to the next stack slot boundary instead of packing\n-                \/\/ additional arguments into the unused space.\n-                storageCalculator.alignStack(STACK_SLOT_SIZE);\n-            }\n-        }\n-\n-        protected void spillStructBox(Binding.Builder bindings, MemoryLayout layout) {\n-            \/\/ If a struct has been assigned register or HFA class but\n-            \/\/ there are not enough free registers to hold the entire\n-            \/\/ struct, it must be passed on the stack. I.e. not split\n-            \/\/ between registers and stack.\n-\n-            long offset = 0;\n-            while (offset < layout.byteSize()) {\n-                long copy = Math.min(layout.byteSize() - offset, STACK_SLOT_SIZE);\n-                VMStorage storage =\n-                    storageCalculator.stackAlloc(copy, STACK_SLOT_SIZE);\n-                Class<?> type = SharedUtils.primitiveCarrierForSize(copy, false);\n-                bindings.dup()\n-                        .vmLoad(storage, type)\n-                        .bufferStore(offset, type);\n-                offset += STACK_SLOT_SIZE;\n-            }\n-\n-            if (requiresSubSlotStackPacking()) {\n-                \/\/ Pad to the next stack slot boundary instead of packing\n-                \/\/ additional arguments into the unused space.\n-                storageCalculator.alignStack(STACK_SLOT_SIZE);\n-            }\n-        }\n-\n@@ -422,1 +413,1 @@\n-                case STRUCT_REGISTER: {\n+                case STRUCT_REGISTER, STRUCT_HFA -> {\n@@ -424,16 +415,3 @@\n-                    VMStorage[] regs = storageCalculator.regAlloc(StorageType.INTEGER, layout);\n-\n-                    if (regs != null) {\n-                        int regIndex = 0;\n-                        long offset = 0;\n-                        while (offset < layout.byteSize() && regIndex < regs.length) {\n-                            final long copy = Math.min(layout.byteSize() - offset, 8);\n-                            VMStorage storage = regs[regIndex++];\n-                            Class<?> type = SharedUtils.primitiveCarrierForSize(copy, false);\n-                            if (offset + copy < layout.byteSize()) {\n-                                bindings.dup();\n-                            }\n-                            bindings.bufferLoad(offset, type)\n-                                    .vmStore(storage, type);\n-                            offset += copy;\n-                        }\n+                    boolean forHFA = argumentClass == TypeClass.STRUCT_HFA;\n+                    StorageCalculator.StructStorage[] structStorages\n+                            = storageCalculator.structStorages((GroupLayout) layout, forHFA);\n@@ -441,3 +419,4 @@\n-                        final long bytesLeft = Math.min(layout.byteSize() - offset, 8);\n-                        if (bytesLeft > 0) {\n-                            spillPartialStructUnbox(bindings, layout, offset);\n+                    for (int i = 0; i < structStorages.length; i++) {\n+                        StorageCalculator.StructStorage structStorage = structStorages[i];\n+                        if (i < structStorages.length - 1) {\n+                            bindings.dup();\n@@ -445,2 +424,2 @@\n-                    } else {\n-                        spillStructUnbox(bindings, layout);\n+                        bindings.bufferLoad(structStorage.offset(), structStorage.carrier())\n+                                .vmStore(structStorage.storage(), structStorage.carrier());\n@@ -448,1 +427,0 @@\n-                    break;\n@@ -450,1 +428,1 @@\n-                case STRUCT_REFERENCE: {\n+                case STRUCT_REFERENCE -> {\n@@ -454,2 +432,1 @@\n-                    VMStorage storage = storageCalculator.nextStorage(\n-                        StorageType.INTEGER, AArch64.C_POINTER);\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, AArch64.C_POINTER);\n@@ -457,24 +434,0 @@\n-                    break;\n-                }\n-                case STRUCT_HFA: {\n-                    assert carrier == MemorySegment.class;\n-                    GroupLayout group = (GroupLayout)layout;\n-                    VMStorage[] regs = storageCalculator.nextStorageForHFA(group);\n-                    if (regs != null) {\n-                        long offset = 0;\n-                        for (int i = 0; i < group.memberLayouts().size(); i++) {\n-                            VMStorage storage = regs[i];\n-                            final long size = group.memberLayouts().get(i).byteSize();\n-                            boolean useFloat = storage.type() == StorageType.VECTOR;\n-                            Class<?> type = SharedUtils.primitiveCarrierForSize(size, useFloat);\n-                            if (i + 1 < group.memberLayouts().size()) {\n-                                bindings.dup();\n-                            }\n-                            bindings.bufferLoad(offset, type)\n-                                    .vmStore(storage, type);\n-                            offset += size;\n-                        }\n-                    } else {\n-                        spillStructUnbox(bindings, layout);\n-                    }\n-                    break;\n@@ -482,1 +435,1 @@\n-                case POINTER: {\n+                case POINTER -> {\n@@ -484,2 +437,1 @@\n-                    VMStorage storage =\n-                        storageCalculator.nextStorage(StorageType.INTEGER, layout);\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, (ValueLayout) layout);\n@@ -487,1 +439,0 @@\n-                    break;\n@@ -489,3 +440,2 @@\n-                case INTEGER: {\n-                    VMStorage storage =\n-                        storageCalculator.nextStorage(StorageType.INTEGER, layout);\n+                case INTEGER -> {\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, (ValueLayout) layout);\n@@ -493,1 +443,0 @@\n-                    break;\n@@ -495,3 +444,6 @@\n-                case FLOAT: {\n-                    VMStorage storage =\n-                        storageCalculator.nextStorage(StorageType.VECTOR, layout);\n+                case FLOAT -> {\n+                    boolean forVariadicFunctionArgs = forArguments && forVariadicFunction;\n+                    boolean useIntReg = forVariadicFunctionArgs && useIntRegsForVariadicFloatingPointArgs();\n+\n+                    int type = useIntReg ? StorageType.INTEGER : StorageType.VECTOR;\n+                    VMStorage storage = storageCalculator.nextStorage(type, (ValueLayout) layout);\n@@ -499,1 +451,0 @@\n-                    break;\n@@ -501,2 +452,1 @@\n-                default:\n-                    throw new UnsupportedOperationException(\"Unhandled class \" + argumentClass);\n+                default -> throw new UnsupportedOperationException(\"Unhandled class \" + argumentClass);\n@@ -526,1 +476,1 @@\n-                case STRUCT_REGISTER -> {\n+                case STRUCT_REGISTER, STRUCT_HFA -> {\n@@ -528,0 +478,1 @@\n+                    boolean forHFA = argumentClass == TypeClass.STRUCT_HFA;\n@@ -529,17 +480,7 @@\n-                    VMStorage[] regs = storageCalculator.regAlloc(\n-                            StorageType.INTEGER, layout);\n-                    if (regs != null) {\n-                        int regIndex = 0;\n-                        long offset = 0;\n-                        while (offset < layout.byteSize()) {\n-                            final long copy = Math.min(layout.byteSize() - offset, 8);\n-                            VMStorage storage = regs[regIndex++];\n-                            bindings.dup();\n-                            boolean useFloat = storage.type() == StorageType.VECTOR;\n-                            Class<?> type = SharedUtils.primitiveCarrierForSize(copy, useFloat);\n-                            bindings.vmLoad(storage, type)\n-                                    .bufferStore(offset, type);\n-                            offset += copy;\n-                        }\n-                    } else {\n-                        spillStructBox(bindings, layout);\n+                    StorageCalculator.StructStorage[] structStorages\n+                            = storageCalculator.structStorages((GroupLayout) layout, forHFA);\n+\n+                    for (StorageCalculator.StructStorage structStorage : structStorages) {\n+                        bindings.dup();\n+                        bindings.vmLoad(structStorage.storage(), structStorage.carrier())\n+                                .bufferStore(structStorage.offset(), structStorage.carrier());\n@@ -550,2 +491,1 @@\n-                    VMStorage storage = storageCalculator.nextStorage(\n-                            StorageType.INTEGER, AArch64.C_POINTER);\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, AArch64.C_POINTER);\n@@ -555,21 +495,0 @@\n-                case STRUCT_HFA -> {\n-                    assert carrier == MemorySegment.class;\n-                    bindings.allocate(layout);\n-                    GroupLayout group = (GroupLayout) layout;\n-                    VMStorage[] regs = storageCalculator.nextStorageForHFA(group);\n-                    if (regs != null) {\n-                        long offset = 0;\n-                        for (int i = 0; i < group.memberLayouts().size(); i++) {\n-                            VMStorage storage = regs[i];\n-                            final long size = group.memberLayouts().get(i).byteSize();\n-                            boolean useFloat = storage.type() == StorageType.VECTOR;\n-                            Class<?> type = SharedUtils.primitiveCarrierForSize(size, useFloat);\n-                            bindings.dup()\n-                                    .vmLoad(storage, type)\n-                                    .bufferStore(offset, type);\n-                            offset += size;\n-                        }\n-                    } else {\n-                        spillStructBox(bindings, layout);\n-                    }\n-                }\n@@ -577,2 +496,1 @@\n-                    VMStorage storage =\n-                            storageCalculator.nextStorage(StorageType.INTEGER, layout);\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, (ValueLayout) layout);\n@@ -583,2 +501,1 @@\n-                    VMStorage storage =\n-                            storageCalculator.nextStorage(StorageType.INTEGER, layout);\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, (ValueLayout) layout);\n@@ -588,2 +505,1 @@\n-                    VMStorage storage =\n-                            storageCalculator.nextStorage(StorageType.VECTOR, layout);\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.VECTOR, (ValueLayout) layout);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/CallArranger.java","additions":161,"deletions":245,"binary":false,"changes":406,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+import java.lang.foreign.SequenceLayout;\n@@ -32,0 +33,2 @@\n+import java.util.List;\n+import java.util.ArrayList;\n@@ -61,0 +64,21 @@\n+    static List<MemoryLayout> scalarLayouts(GroupLayout gl) {\n+        List<MemoryLayout> out = new ArrayList<>();\n+        scalarLayoutsInternal(out, gl);\n+        return out;\n+    }\n+\n+    private static void scalarLayoutsInternal(List<MemoryLayout> out, GroupLayout gl) {\n+        for (MemoryLayout member : gl.memberLayouts()) {\n+            if (member instanceof GroupLayout memberGl) {\n+                scalarLayoutsInternal(out, memberGl);\n+            } else if (member instanceof SequenceLayout memberSl) {\n+                for (long i = 0; i < memberSl.elementCount(); i++) {\n+                    out.add(memberSl.elementLayout());\n+                }\n+            } else {\n+                \/\/ padding or value layouts\n+                out.add(member);\n+            }\n+        }\n+    }\n+\n@@ -65,1 +89,3 @@\n-        final int numElements = groupLayout.memberLayouts().size();\n+        List<MemoryLayout> scalarLayouts = scalarLayouts(groupLayout);\n+\n+        final int numElements = scalarLayouts.size();\n@@ -69,1 +95,1 @@\n-        MemoryLayout baseType = groupLayout.memberLayouts().get(0);\n+        MemoryLayout baseType = scalarLayouts.get(0);\n@@ -78,1 +104,1 @@\n-        for (MemoryLayout elem : groupLayout.memberLayouts()) {\n+        for (MemoryLayout elem : scalarLayouts) {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/TypeClass.java","additions":30,"deletions":4,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- *  Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ *  Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -25,0 +25,1 @@\n+import java.lang.foreign.Arena;\n@@ -26,0 +27,1 @@\n+import java.lang.foreign.GroupLayout;\n@@ -29,0 +31,2 @@\n+import java.lang.foreign.PaddingLayout;\n+import java.lang.foreign.SegmentAllocator;\n@@ -30,0 +34,2 @@\n+import java.lang.foreign.SequenceLayout;\n+import java.lang.foreign.StructLayout;\n@@ -31,0 +37,1 @@\n+import java.lang.foreign.UnionLayout;\n@@ -35,0 +42,11 @@\n+import java.lang.invoke.MethodType;\n+import java.lang.invoke.VarHandle;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.function.UnaryOperator;\n+import java.util.random.RandomGenerator;\n+\n+import static java.lang.foreign.MemoryLayout.PathElement.groupElement;\n+import static java.lang.foreign.MemoryLayout.PathElement.sequenceElement;\n@@ -40,0 +58,11 @@\n+    private static final MethodHandle MH_SAVER;\n+\n+    static {\n+        try {\n+            MH_SAVER = MethodHandles.lookup().findStatic(NativeTestHelper.class, \"saver\",\n+                    MethodType.methodType(Object.class, Object[].class, List.class, AtomicReference.class, SegmentAllocator.class, int.class));\n+        } catch (ReflectiveOperationException e) {\n+            throw new ExceptionInInitializerError(e);\n+        }\n+    }\n+\n@@ -129,0 +158,123 @@\n+\n+    public record TestValue (Object value, Consumer<Object> check) {}\n+\n+    public static TestValue genTestValue(RandomGenerator random, MemoryLayout layout, SegmentAllocator allocator) {\n+        if (layout instanceof StructLayout struct) {\n+            MemorySegment segment = allocator.allocate(struct);\n+            List<Consumer<Object>> fieldChecks = new ArrayList<>();\n+            for (MemoryLayout fieldLayout : struct.memberLayouts()) {\n+                if (fieldLayout instanceof PaddingLayout) continue;\n+                MemoryLayout.PathElement fieldPath = groupElement(fieldLayout.name().orElseThrow());\n+                fieldChecks.add(initField(random, segment, struct, fieldLayout, fieldPath, allocator));\n+            }\n+            return new TestValue(segment, actual -> fieldChecks.forEach(check -> check.accept(actual)));\n+        } else if (layout instanceof UnionLayout union) {\n+            MemorySegment segment = allocator.allocate(union);\n+            List<MemoryLayout> filteredFields = union.memberLayouts().stream()\n+                                                                     .filter(l -> !(l instanceof PaddingLayout))\n+                                                                     .toList();\n+            int fieldIdx = random.nextInt(filteredFields.size());\n+            MemoryLayout fieldLayout = filteredFields.get(fieldIdx);\n+            MemoryLayout.PathElement fieldPath = groupElement(fieldLayout.name().orElseThrow());\n+            Consumer<Object> check = initField(random, segment, union, fieldLayout, fieldPath, allocator);\n+            return new TestValue(segment, check);\n+        } else if (layout instanceof SequenceLayout array) {\n+            MemorySegment segment = allocator.allocate(array);\n+            List<Consumer<Object>> elementChecks = new ArrayList<>();\n+            for (int i = 0; i < array.elementCount(); i++) {\n+                elementChecks.add(initField(random, segment, array, array.elementLayout(), sequenceElement(i), allocator));\n+            }\n+            return new TestValue(segment, actual -> elementChecks.forEach(check -> check.accept(actual)));\n+        } else if (layout instanceof ValueLayout.OfAddress) {\n+            MemorySegment value = MemorySegment.ofAddress(random.nextLong());\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        }else if (layout instanceof ValueLayout.OfByte) {\n+            byte value = (byte) random.nextInt();\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        } else if (layout instanceof ValueLayout.OfShort) {\n+            short value = (short) random.nextInt();\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        } else if (layout instanceof ValueLayout.OfInt) {\n+            int value = random.nextInt();\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        } else if (layout instanceof ValueLayout.OfLong) {\n+            long value = random.nextLong();\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        } else if (layout instanceof ValueLayout.OfFloat) {\n+            float value = random.nextFloat();\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        } else if (layout instanceof ValueLayout.OfDouble) {\n+            double value = random.nextDouble();\n+            return new TestValue(value, actual -> assertEquals(actual, value));\n+        }\n+\n+        throw new IllegalStateException(\"Unexpected layout: \" + layout);\n+    }\n+\n+    private static Consumer<Object> initField(RandomGenerator random, MemorySegment container, MemoryLayout containerLayout,\n+                                              MemoryLayout fieldLayout, MemoryLayout.PathElement fieldPath,\n+                                              SegmentAllocator allocator) {\n+        TestValue fieldValue = genTestValue(random, fieldLayout, allocator);\n+        Consumer<Object> fieldCheck = fieldValue.check();\n+        if (fieldLayout instanceof GroupLayout || fieldLayout instanceof SequenceLayout) {\n+            UnaryOperator<MemorySegment> slicer = slicer(containerLayout, fieldPath);\n+            MemorySegment slice = slicer.apply(container);\n+            slice.copyFrom((MemorySegment) fieldValue.value());\n+            return actual -> fieldCheck.accept(slicer.apply((MemorySegment) actual));\n+        } else {\n+            VarHandle accessor = containerLayout.varHandle(fieldPath);\n+            \/\/set value\n+            accessor.set(container, fieldValue.value());\n+            return actual -> fieldCheck.accept(accessor.get((MemorySegment) actual));\n+        }\n+    }\n+\n+    private static UnaryOperator<MemorySegment> slicer(MemoryLayout containerLayout, MemoryLayout.PathElement fieldPath) {\n+        MethodHandle slicer = containerLayout.sliceHandle(fieldPath);\n+        return container -> {\n+              try {\n+                return (MemorySegment) slicer.invokeExact(container);\n+            } catch (Throwable e) {\n+                throw new IllegalStateException(e);\n+            }\n+        };\n+    }\n+\n+    private static void assertEquals(Object actual, Object expected) {\n+        if (actual.getClass() != expected.getClass()) {\n+            throw new AssertionError(\"Type mismatch: \" + actual.getClass() + \" != \" + expected.getClass());\n+        }\n+        if (!actual.equals(expected)) {\n+            throw new AssertionError(\"Not equal: \" + actual + \" != \" + expected);\n+        }\n+    }\n+\n+    \/**\n+     * Make an upcall stub that saves its arguments into the given 'ref' array\n+     *\n+     * @param fd function descriptor for the upcall stub\n+     * @param capturedArgs box to save arguments in\n+     * @param arena allocator for making copies of by-value structs\n+     * @param retIdx the index of the argument to return\n+     * @return return the upcall stub\n+     *\/\n+    public static MemorySegment makeArgSaverCB(FunctionDescriptor fd, Arena arena,\n+                                               AtomicReference<Object[]> capturedArgs, int retIdx) {\n+        MethodHandle target = MethodHandles.insertArguments(MH_SAVER, 1, fd.argumentLayouts(), capturedArgs, arena, retIdx);\n+        target = target.asCollector(Object[].class, fd.argumentLayouts().size());\n+        target = target.asType(fd.toMethodType());\n+        return LINKER.upcallStub(target, fd, arena.scope());\n+    }\n+\n+    private static Object saver(Object[] o, List<MemoryLayout> argLayouts, AtomicReference<Object[]> ref, SegmentAllocator allocator, int retArg) {\n+        for (int i = 0; i < o.length; i++) {\n+            if (argLayouts.get(i) instanceof GroupLayout gl) {\n+                MemorySegment ms = (MemorySegment) o[i];\n+                MemorySegment copy = allocator.allocate(gl);\n+                copy.copyFrom(ms);\n+                o[i] = copy;\n+            }\n+        }\n+        ref.set(o);\n+        return retArg != -1 ? o[retArg] : null;\n+    }\n","filename":"test\/jdk\/java\/foreign\/NativeTestHelper.java","additions":153,"deletions":1,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -429,0 +429,50 @@\n+\n+    @Test\n+    public void testFloatArrayStruct() {\n+        \/\/ should be classified as HFA\n+        StructLayout S10 = MemoryLayout.structLayout(\n+                MemoryLayout.sequenceLayout(4, C_DOUBLE)\n+        );\n+        MethodType mt = MethodType.methodType(MemorySegment.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.of(S10, S10);\n+        FunctionDescriptor fdExpected = FunctionDescriptor.of(S10, ADDRESS, ADDRESS, S10); \/\/ uses return buffer\n+        CallArranger.Bindings bindings = CallArranger.LINUX.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n+\n+        \/\/ This is identical to the non-variadic calling sequence\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(RETURN_BUFFER_STORAGE, long.class) },\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { dup(),\n+                bufferLoad(0, double.class),\n+                vmStore(v0, double.class),\n+              dup(),\n+                bufferLoad(8, double.class),\n+                vmStore(v1, double.class),\n+              dup(),\n+                bufferLoad(16, double.class),\n+                vmStore(v2, double.class),\n+                bufferLoad(24, double.class),\n+                vmStore(v3, double.class) },\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{\n+            allocate(S10),\n+              dup(),\n+                 vmLoad(v0, double.class),\n+                 bufferStore(0, double.class),\n+              dup(),\n+                 vmLoad(v1, double.class),\n+                 bufferStore(8, double.class),\n+              dup(),\n+                 vmLoad(v2, double.class),\n+                 bufferStore(16, double.class),\n+              dup(),\n+                 vmLoad(v3, double.class),\n+                 bufferStore(24, double.class),\n+        });\n+    }\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestLinuxAArch64CallArranger.java","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+ * @requires sun.arch.data.model == \"64\"\n@@ -283,0 +284,109 @@\n+\n+    \/\/ structs that are passed field-wise should not have padding after them\n+    @Test\n+    public void testMacArgsOnStack5() {\n+        StructLayout struct = MemoryLayout.structLayout(\n+            C_FLOAT\n+        );\n+        MethodType mt = MethodType.methodType(void.class,\n+                long.class, long.class, long.class, long.class,\n+                long.class, long.class, long.class, long.class,\n+                double.class, double.class, double.class, double.class,\n+                double.class, double.class, double.class, double.class,\n+                MemorySegment.class, int.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(\n+                C_LONG_LONG, C_LONG_LONG, C_LONG_LONG, C_LONG_LONG,\n+                C_LONG_LONG, C_LONG_LONG, C_LONG_LONG, C_LONG_LONG,\n+                C_DOUBLE, C_DOUBLE, C_DOUBLE, C_DOUBLE,\n+                C_DOUBLE, C_DOUBLE, C_DOUBLE, C_DOUBLE,\n+                struct, C_INT, C_POINTER);\n+        CallArranger.Bindings bindings = CallArranger.MACOS.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(r0, long.class) },\n+            { vmStore(r1, long.class) },\n+            { vmStore(r2, long.class) },\n+            { vmStore(r3, long.class) },\n+            { vmStore(r4, long.class) },\n+            { vmStore(r5, long.class) },\n+            { vmStore(r6, long.class) },\n+            { vmStore(r7, long.class) },\n+            { vmStore(v0, double.class) },\n+            { vmStore(v1, double.class) },\n+            { vmStore(v2, double.class) },\n+            { vmStore(v3, double.class) },\n+            { vmStore(v4, double.class) },\n+            { vmStore(v5, double.class) },\n+            { vmStore(v6, double.class) },\n+            { vmStore(v7, double.class) },\n+            {\n+                bufferLoad(0, int.class),\n+                vmStore(stackStorage((short) 4, 0), int.class),\n+            },\n+            { vmStore(stackStorage((short) 4, 4), int.class) },\n+            { unboxAddress(), vmStore(stackStorage((short) 8, 8), long.class) },\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n+\n+    \/\/ structs that are passed chunk-wise should have padding before them, as well as after\n+    @Test\n+    public void testMacArgsOnStack6() {\n+        StructLayout struct = MemoryLayout.structLayout(\n+            C_INT\n+        );\n+        MethodType mt = MethodType.methodType(void.class,\n+                long.class, long.class, long.class, long.class,\n+                long.class, long.class, long.class, long.class,\n+                double.class, double.class, double.class, double.class,\n+                double.class, double.class, double.class, double.class,\n+                int.class, MemorySegment.class, double.class, MemorySegment.class);\n+        FunctionDescriptor fd = FunctionDescriptor.ofVoid(\n+                C_LONG_LONG, C_LONG_LONG, C_LONG_LONG, C_LONG_LONG,\n+                C_LONG_LONG, C_LONG_LONG, C_LONG_LONG, C_LONG_LONG,\n+                C_DOUBLE, C_DOUBLE, C_DOUBLE, C_DOUBLE,\n+                C_DOUBLE, C_DOUBLE, C_DOUBLE, C_DOUBLE,\n+                C_INT, struct, C_DOUBLE, C_POINTER);\n+        CallArranger.Bindings bindings = CallArranger.MACOS.getBindings(mt, fd, false);\n+\n+        assertFalse(bindings.isInMemoryReturn());\n+        CallingSequence callingSequence = bindings.callingSequence();\n+        assertEquals(callingSequence.callerMethodType(), mt.insertParameterTypes(0, MemorySegment.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(), vmStore(TARGET_ADDRESS_STORAGE, long.class) },\n+            { vmStore(r0, long.class) },\n+            { vmStore(r1, long.class) },\n+            { vmStore(r2, long.class) },\n+            { vmStore(r3, long.class) },\n+            { vmStore(r4, long.class) },\n+            { vmStore(r5, long.class) },\n+            { vmStore(r6, long.class) },\n+            { vmStore(r7, long.class) },\n+            { vmStore(v0, double.class) },\n+            { vmStore(v1, double.class) },\n+            { vmStore(v2, double.class) },\n+            { vmStore(v3, double.class) },\n+            { vmStore(v4, double.class) },\n+            { vmStore(v5, double.class) },\n+            { vmStore(v6, double.class) },\n+            { vmStore(v7, double.class) },\n+            { vmStore(stackStorage((short) 4, 0), int.class) },\n+            {\n+                bufferLoad(0, int.class),\n+                vmStore(stackStorage((short) 4, 8), int.class),\n+            },\n+            { vmStore(stackStorage((short) 8, 16), double.class) },\n+            { unboxAddress(), vmStore(stackStorage((short) 8, 24), long.class) },\n+        });\n+\n+        checkReturnBindings(callingSequence, new Binding[]{});\n+    }\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestMacOsAArch64CallArranger.java","additions":111,"deletions":1,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -0,0 +1,260 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @enablePreview\n+ * @library ..\/ \/test\/lib\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires vm.flavor != \"zero\"\n+ * @build NativeTestHelper\n+ * @run testng\/othervm --enable-native-access=ALL-UNNAMED TestNested\n+ *\/\n+\n+import org.testng.annotations.DataProvider;\n+import org.testng.annotations.Test;\n+\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SegmentAllocator;\n+import java.lang.foreign.StructLayout;\n+import java.lang.foreign.UnionLayout;\n+import java.lang.invoke.MethodHandle;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+public class TestNested extends NativeTestHelper {\n+\n+    static {\n+        System.loadLibrary(\"Nested\");\n+    }\n+\n+    @Test(dataProvider = \"nestedLayouts\")\n+    public void testNested(GroupLayout layout) throws Throwable {\n+        try (Arena arena = Arena.openConfined()) {\n+            Random random = new Random(0);\n+            TestValue testValue = genTestValue(random, layout, arena);\n+\n+            String funcName = \"test_\" + layout.name().orElseThrow();\n+            FunctionDescriptor downcallDesc = FunctionDescriptor.of(layout, layout, C_POINTER);\n+            FunctionDescriptor upcallDesc = FunctionDescriptor.of(layout, layout);\n+\n+            MethodHandle downcallHandle = downcallHandle(funcName, downcallDesc);\n+            AtomicReference<Object[]> returnBox = new AtomicReference<>();\n+            MemorySegment stub = makeArgSaverCB(upcallDesc, arena, returnBox, 0);\n+\n+            MemorySegment returned = (MemorySegment) downcallHandle.invokeExact(\n+                    (SegmentAllocator) arena, (MemorySegment) testValue.value(), stub);\n+\n+            testValue.check().accept(returnBox.get()[0]);\n+            testValue.check().accept(returned);\n+        }\n+    }\n+\n+    @DataProvider\n+    public static Object[][] nestedLayouts() {\n+        List<GroupLayout> layouts = List.of(\n+                S1, U1, U17, S2, S3, S4, S5, S6, U2, S7, U3, U4, U5, U6, U7, S8, S9, U8, U9, U10, S10,\n+                U11, S11, U12, S12, U13, U14, U15, S13, S14, U16, S15);\n+        return layouts.stream().map(l -> new Object[]{l}).toArray(Object[][]::new);\n+    }\n+\n+    static final StructLayout S1 = MemoryLayout.structLayout(\n+            C_DOUBLE.withName(\"f0\"),\n+            C_LONG_LONG.withName(\"f1\"),\n+            C_DOUBLE.withName(\"f2\"),\n+            C_INT.withName(\"f3\"),\n+            MemoryLayout.paddingLayout(32)\n+    ).withName(\"S1\");\n+    static final UnionLayout U1 = MemoryLayout.unionLayout(\n+            C_SHORT.withName(\"f0\"),\n+            C_LONG_LONG.withName(\"f1\"),\n+            C_SHORT.withName(\"f2\"),\n+            MemoryLayout.sequenceLayout(4, MemoryLayout.sequenceLayout(3, C_CHAR)).withName(\"f3\"),\n+            MemoryLayout.paddingLayout(128)\n+    ).withName(\"U1\");\n+    static final UnionLayout U17 = MemoryLayout.unionLayout(\n+            C_CHAR.withName(\"f0\"),\n+            C_CHAR.withName(\"f1\"),\n+            C_LONG_LONG.withName(\"f2\"),\n+            C_DOUBLE.withName(\"f3\")\n+    ).withName(\"U17\");\n+    static final StructLayout S2 = MemoryLayout.structLayout(\n+            U17.withName(\"f0\"),\n+            MemoryLayout.sequenceLayout(4, C_LONG_LONG).withName(\"f1\"),\n+            C_SHORT.withName(\"f2\"),\n+            MemoryLayout.paddingLayout(48)\n+    ).withName(\"S2\");\n+    static final StructLayout S3 = MemoryLayout.structLayout(\n+            C_FLOAT.withName(\"f0\"),\n+            C_INT.withName(\"f1\"),\n+            U1.withName(\"f2\"),\n+            S2.withName(\"f3\")\n+    ).withName(\"S3\");\n+    static final StructLayout S4 = MemoryLayout.structLayout(\n+            MemoryLayout.sequenceLayout(2, C_SHORT).withName(\"f0\"),\n+            MemoryLayout.paddingLayout(32),\n+            S1.withName(\"f1\")\n+    ).withName(\"S4\");\n+    static final StructLayout S5 = MemoryLayout.structLayout(\n+            C_FLOAT.withName(\"f0\"),\n+            MemoryLayout.paddingLayout(32),\n+            C_POINTER.withName(\"f1\"),\n+            S4.withName(\"f2\")\n+    ).withName(\"S5\");\n+    static final StructLayout S6 = MemoryLayout.structLayout(\n+            S5.withName(\"f0\")\n+    ).withName(\"S6\");\n+    static final UnionLayout U2 = MemoryLayout.unionLayout(\n+            C_FLOAT.withName(\"f0\"),\n+            C_SHORT.withName(\"f1\"),\n+            C_POINTER.withName(\"f2\"),\n+            C_FLOAT.withName(\"f3\")\n+    ).withName(\"U2\");\n+    static final StructLayout S7 = MemoryLayout.structLayout(\n+            C_DOUBLE.withName(\"f0\"),\n+            C_SHORT.withName(\"f1\"),\n+            C_SHORT.withName(\"f2\"),\n+            MemoryLayout.paddingLayout(32),\n+            C_LONG_LONG.withName(\"f3\")\n+    ).withName(\"S7\");\n+    static final UnionLayout U3 = MemoryLayout.unionLayout(\n+            C_POINTER.withName(\"f0\"),\n+            U2.withName(\"f1\"),\n+            C_LONG_LONG.withName(\"f2\"),\n+            S7.withName(\"f3\")\n+    ).withName(\"U3\");\n+    static final UnionLayout U4 = MemoryLayout.unionLayout(\n+            C_FLOAT.withName(\"f0\")\n+    ).withName(\"U4\");\n+    static final UnionLayout U5 = MemoryLayout.unionLayout(\n+            U3.withName(\"f0\"),\n+            MemoryLayout.sequenceLayout(3, C_LONG_LONG).withName(\"f1\"),\n+            U4.withName(\"f2\"),\n+            C_FLOAT.withName(\"f3\")\n+    ).withName(\"U5\");\n+    static final UnionLayout U6 = MemoryLayout.unionLayout(\n+            C_SHORT.withName(\"f0\"),\n+            C_FLOAT.withName(\"f1\"),\n+            U5.withName(\"f2\"),\n+            C_SHORT.withName(\"f3\")\n+    ).withName(\"U6\");\n+    static final UnionLayout U7 = MemoryLayout.unionLayout(\n+            C_SHORT.withName(\"f0\")\n+    ).withName(\"U7\");\n+    static final StructLayout S8 = MemoryLayout.structLayout(\n+            MemoryLayout.sequenceLayout(3, C_DOUBLE).withName(\"f0\"),\n+            U7.withName(\"f1\"),\n+            MemoryLayout.paddingLayout(48),\n+            C_POINTER.withName(\"f2\"),\n+            C_POINTER.withName(\"f3\")\n+    ).withName(\"S8\");\n+    static final StructLayout S9 = MemoryLayout.structLayout(\n+            C_CHAR.withName(\"f0\"),\n+            MemoryLayout.paddingLayout(56),\n+            MemoryLayout.sequenceLayout(2, C_DOUBLE).withName(\"f1\"),\n+            C_CHAR.withName(\"f2\"),\n+            MemoryLayout.paddingLayout(56),\n+            S8.withName(\"f3\")\n+    ).withName(\"S9\");\n+    static final UnionLayout U8 = MemoryLayout.unionLayout(\n+            C_LONG_LONG.withName(\"f0\"),\n+            C_POINTER.withName(\"f1\"),\n+            S9.withName(\"f2\")\n+    ).withName(\"U8\");\n+    static final UnionLayout U9 = MemoryLayout.unionLayout(\n+            C_INT.withName(\"f0\"),\n+            C_DOUBLE.withName(\"f1\"),\n+            MemoryLayout.sequenceLayout(2, C_SHORT).withName(\"f2\"),\n+            C_LONG_LONG.withName(\"f3\")\n+    ).withName(\"U9\");\n+    static final UnionLayout U10 = MemoryLayout.unionLayout(\n+            C_LONG_LONG.withName(\"f0\"),\n+            U9.withName(\"f1\"),\n+            C_CHAR.withName(\"f2\"),\n+            C_FLOAT.withName(\"f3\")\n+    ).withName(\"U10\");\n+    static final StructLayout S10 = MemoryLayout.structLayout(\n+            MemoryLayout.sequenceLayout(4, C_DOUBLE).withName(\"f0\")\n+    ).withName(\"S10\");\n+    static final UnionLayout U11 = MemoryLayout.unionLayout(\n+            MemoryLayout.sequenceLayout(3, S10).withName(\"f0\")\n+    ).withName(\"U11\");\n+    static final StructLayout S11 = MemoryLayout.structLayout(\n+            C_SHORT.withName(\"f0\"),\n+            C_CHAR.withName(\"f1\"),\n+            MemoryLayout.paddingLayout(8)\n+    ).withName(\"S11\");\n+    static final UnionLayout U12 = MemoryLayout.unionLayout(\n+            C_FLOAT.withName(\"f0\"),\n+            S11.withName(\"f1\"),\n+            C_CHAR.withName(\"f2\"),\n+            C_CHAR.withName(\"f3\")\n+    ).withName(\"U12\");\n+    static final StructLayout S12 = MemoryLayout.structLayout(\n+            U12.withName(\"f0\"),\n+            C_FLOAT.withName(\"f1\")\n+    ).withName(\"S12\");\n+    static final UnionLayout U13 = MemoryLayout.unionLayout(\n+            C_FLOAT.withName(\"f0\"),\n+            S12.withName(\"f1\")\n+    ).withName(\"U13\");\n+    static final UnionLayout U14 = MemoryLayout.unionLayout(\n+            C_INT.withName(\"f0\"),\n+            MemoryLayout.sequenceLayout(2, C_POINTER).withName(\"f1\"),\n+            MemoryLayout.sequenceLayout(2, MemoryLayout.sequenceLayout(3, C_FLOAT)).withName(\"f2\")\n+    ).withName(\"U14\");\n+    static final UnionLayout U15 = MemoryLayout.unionLayout(\n+            C_POINTER.withName(\"f0\"),\n+            C_LONG_LONG.withName(\"f1\"),\n+            MemoryLayout.sequenceLayout(1, C_DOUBLE).withName(\"f2\"),\n+            C_LONG_LONG.withName(\"f3\")\n+    ).withName(\"U15\");\n+    static final StructLayout S13 = MemoryLayout.structLayout(\n+            C_INT.withName(\"f0\"),\n+            C_CHAR.withName(\"f1\"),\n+            MemoryLayout.paddingLayout(24),\n+            C_POINTER.withName(\"f2\"),\n+            C_CHAR.withName(\"f3\"),\n+            MemoryLayout.paddingLayout(56)\n+    ).withName(\"S13\");\n+    static final StructLayout S14 = MemoryLayout.structLayout(\n+            C_LONG_LONG.withName(\"f0\")\n+    ).withName(\"S14\");\n+    static final UnionLayout U16 = MemoryLayout.unionLayout(\n+            MemoryLayout.sequenceLayout(4, C_SHORT).withName(\"f0\"),\n+            C_INT.withName(\"f1\"),\n+            S13.withName(\"f2\"),\n+            S14.withName(\"f3\")\n+    ).withName(\"U16\");\n+    static final StructLayout S15 = MemoryLayout.structLayout(\n+            U16.withName(\"f0\"),\n+            C_FLOAT.withName(\"f1\"),\n+            C_INT.withName(\"f2\"),\n+            C_LONG_LONG.withName(\"f3\")\n+    ).withName(\"S15\");\n+}\n","filename":"test\/jdk\/java\/foreign\/nested\/TestNested.java","additions":260,"deletions":0,"binary":false,"changes":260,"status":"added"},{"patch":"@@ -0,0 +1,94 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifdef _WIN64\n+#define EXPORT __declspec(dllexport)\n+#else\n+#define EXPORT\n+#endif\n+\n+struct S1{ double f0; long long f1; double f2; int f3; };\n+union U1{ short f0; long long f1; short f2; char f3[4][3]; };\n+union U17{ char f0; char f1; long long f2; double f3; };\n+struct S2{ union U17 f0; long long f1[4]; short f2; };\n+struct S3{ float f0; int f1; union U1 f2; struct S2 f3; };\n+struct S4{ short f0[2]; struct S1 f1; };\n+struct S5{ float f0; void* f1; struct S4 f2; };\n+struct S6{ struct S5 f0; };\n+union U2{ float f0; short f1; void* f2; float f3; };\n+struct S7{ double f0; short f1; short f2; long long f3; };\n+union U3{ void* f0; union U2 f1; long long f2; struct S7 f3; };\n+union U4{ float f0; };\n+union U5{ union U3 f0; long long f1[3]; union U4 f2; float f3; };\n+union U6{ short f0; float f1; union U5 f2; short f3; };\n+union U7{ short f0; };\n+struct S8{ double f0[3]; union U7 f1; void* f2; void* f3; };\n+struct S9{ char f0; double f1[2]; char f2; struct S8 f3; };\n+union U8{ long long f0; void* f1; struct S9 f2; };\n+union U9{ int f0; double f1; short f2[2]; long long f3; };\n+union U10{ long long f0; union U9 f1; char f2; float f3; };\n+struct S10{ double f0[4]; };\n+union U11{ struct S10 f0[3]; };\n+struct S11{ short f0; char f1; };\n+union U12{ float f0; struct S11 f1; char f2; char f3; };\n+struct S12{ union U12 f0; float f1; };\n+union U13{ float f0; struct S12 f1; };\n+union U14{ int f0; void* f1[2]; float f2[2][3]; };\n+union U15{ void* f0; long long f1; double f2[1]; long long f3; };\n+struct S13{ int f0; char f1; void* f2; char f3; };\n+struct S14{ long long f0; };\n+union U16{ short f0[4]; int f1; struct S13 f2; struct S14 f3; };\n+struct S15{ union U16 f0; float f1; int f2; long long f3; };\n+\n+EXPORT struct S1 test_S1(struct S1 arg, struct S1(*cb)(struct S1)) { return cb(arg); }\n+EXPORT union U1 test_U1(union U1 arg, union U1(*cb)(union U1)) { return cb(arg); }\n+EXPORT union U17 test_U17(union U17 arg, union U17(*cb)(union U17)) { return cb(arg); }\n+EXPORT struct S2 test_S2(struct S2 arg, struct S2(*cb)(struct S2)) { return cb(arg); }\n+EXPORT struct S3 test_S3(struct S3 arg, struct S3(*cb)(struct S3)) { return cb(arg); }\n+EXPORT struct S4 test_S4(struct S4 arg, struct S4(*cb)(struct S4)) { return cb(arg); }\n+EXPORT struct S5 test_S5(struct S5 arg, struct S5(*cb)(struct S5)) { return cb(arg); }\n+EXPORT struct S6 test_S6(struct S6 arg, struct S6(*cb)(struct S6)) { return cb(arg); }\n+EXPORT union U2 test_U2(union U2 arg, union U2(*cb)(union U2)) { return cb(arg); }\n+EXPORT struct S7 test_S7(struct S7 arg, struct S7(*cb)(struct S7)) { return cb(arg); }\n+EXPORT union U3 test_U3(union U3 arg, union U3(*cb)(union U3)) { return cb(arg); }\n+EXPORT union U4 test_U4(union U4 arg, union U4(*cb)(union U4)) { return cb(arg); }\n+EXPORT union U5 test_U5(union U5 arg, union U5(*cb)(union U5)) { return cb(arg); }\n+EXPORT union U6 test_U6(union U6 arg, union U6(*cb)(union U6)) { return cb(arg); }\n+EXPORT union U7 test_U7(union U7 arg, union U7(*cb)(union U7)) { return cb(arg); }\n+EXPORT struct S8 test_S8(struct S8 arg, struct S8(*cb)(struct S8)) { return cb(arg); }\n+EXPORT struct S9 test_S9(struct S9 arg, struct S9(*cb)(struct S9)) { return cb(arg); }\n+EXPORT union U8 test_U8(union U8 arg, union U8(*cb)(union U8)) { return cb(arg); }\n+EXPORT union U9 test_U9(union U9 arg, union U9(*cb)(union U9)) { return cb(arg); }\n+EXPORT union U10 test_U10(union U10 arg, union U10(*cb)(union U10)) { return cb(arg); }\n+EXPORT struct S10 test_S10(struct S10 arg, struct S10(*cb)(struct S10)) { return cb(arg); }\n+EXPORT union U11 test_U11(union U11 arg, union U11(*cb)(union U11)) { return cb(arg); }\n+EXPORT struct S11 test_S11(struct S11 arg, struct S11(*cb)(struct S11)) { return cb(arg); }\n+EXPORT union U12 test_U12(union U12 arg, union U12(*cb)(union U12)) { return cb(arg); }\n+EXPORT struct S12 test_S12(struct S12 arg, struct S12(*cb)(struct S12)) { return cb(arg); }\n+EXPORT union U13 test_U13(union U13 arg, union U13(*cb)(union U13)) { return cb(arg); }\n+EXPORT union U14 test_U14(union U14 arg, union U14(*cb)(union U14)) { return cb(arg); }\n+EXPORT union U15 test_U15(union U15 arg, union U15(*cb)(union U15)) { return cb(arg); }\n+EXPORT struct S13 test_S13(struct S13 arg, struct S13(*cb)(struct S13)) { return cb(arg); }\n+EXPORT struct S14 test_S14(struct S14 arg, struct S14(*cb)(struct S14)) { return cb(arg); }\n+EXPORT union U16 test_U16(union U16 arg, union U16(*cb)(union U16)) { return cb(arg); }\n+EXPORT struct S15 test_S15(struct S15 arg, struct S15(*cb)(struct S15)) { return cb(arg); }\n","filename":"test\/jdk\/java\/foreign\/nested\/libNested.c","additions":94,"deletions":0,"binary":false,"changes":94,"status":"added"}]}