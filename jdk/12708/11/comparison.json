{"files":[{"patch":"@@ -71,1 +71,1 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n","filename":"src\/hotspot\/cpu\/aarch64\/vmstorage_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n","filename":"src\/hotspot\/cpu\/arm\/vmstorage_arm.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020 SAP SE. All rights reserved.\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n@@ -26,0 +26,7 @@\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n@@ -27,1 +34,60 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+\n+#define __ _masm->\n+\n+class DowncallStubGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+  const ABIDescriptor& _abi;\n+\n+  const GrowableArray<VMStorage>& _input_registers;\n+  const GrowableArray<VMStorage>& _output_registers;\n+\n+  bool _needs_return_buffer;\n+  int _captured_state_mask;\n+\n+  int _frame_complete;\n+  int _frame_size_slots;\n+  OopMapSet* _oop_maps;\n+public:\n+  DowncallStubGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         const ABIDescriptor& abi,\n+                         const GrowableArray<VMStorage>& input_registers,\n+                         const GrowableArray<VMStorage>& output_registers,\n+                         bool needs_return_buffer,\n+                         int captured_state_mask)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _abi(abi),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _needs_return_buffer(needs_return_buffer),\n+     _captured_state_mask(captured_state_mask),\n+     _frame_complete(0),\n+     _frame_size_slots(0),\n+     _oop_maps(NULL) {\n+  }\n+\n+  void generate();\n+\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n+\n+  int framesize() const {\n+    return (_frame_size_slots >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n+\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+};\n+\n+static const int native_invoker_code_size = 1024;\n@@ -37,2 +103,240 @@\n-  Unimplemented();\n-  return nullptr;\n+  int locs_size = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  DowncallStubGenerator g(&code, signature, num_args, ret_bt, abi,\n+                          input_registers, output_registers,\n+                          needs_return_buffer, captured_state_mask);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n+\n+#ifndef PRODUCT\n+  LogTarget(Trace, foreign, downcall) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    stub->print_on(&ls);\n+  }\n+#endif\n+\n+  return stub;\n+}\n+\n+void DowncallStubGenerator::generate() {\n+  Register callerSP            = R2, \/\/ C\/C++ uses R2 as TOC, but we can reuse it here\n+           tmp                 = R11_scratch1, \/\/ same as shuffle_reg\n+           call_target_address = R12_scratch2; \/\/ same as _abi._scratch2 (ABIv2 requires this reg!)\n+  VMStorage shuffle_reg = _abi._scratch1;\n+  JavaCallingConvention in_conv;\n+  NativeCallingConvention out_conv(_input_registers);\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, shuffle_reg);\n+\n+#ifndef PRODUCT\n+  LogTarget(Trace, foreign, downcall) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ Stack frame size computation:\n+  \/\/ We use the number of input VMStorage elements because PPC64 requires slots for all arguments\n+  \/\/ (even if they are passed in registers), at least 8.\n+  \/\/ This may be a bit more than needed when HFA is used (see CallArranger.java).\n+  \/\/ (abi_reg_args is abi_minframe plus space for 8 argument register spill slots)\n+  assert(_abi._shadow_space_bytes == frame::abi_minframe_size, \"expected space according to ABI\");\n+  int allocated_frame_size = frame::abi_minframe_size + MAX2(_input_registers.length(), 8) * BytesPerWord;\n+\n+  bool should_save_return_value = !_needs_return_buffer;\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_offset = -1;\n+\n+  if (should_save_return_value) {\n+    spill_offset = frame::abi_reg_args_size;\n+    \/\/ Spill area can be shared with additional out args (>8),\n+    \/\/ since it is only used after the call.\n+    int frame_size_including_spill_area = frame::abi_reg_args_size + out_reg_spiller.spill_size_bytes();\n+    if (frame_size_including_spill_area > allocated_frame_size) {\n+      allocated_frame_size = frame_size_including_spill_area;\n+    }\n+  }\n+\n+  StubLocations locs;\n+  assert(as_Register(_abi._scratch2) == call_target_address, \"required by ABIv2\");\n+  locs.set(StubLocations::TARGET_ADDRESS, _abi._scratch2);\n+  if (_needs_return_buffer) {\n+    locs.set_frame_data(StubLocations::RETURN_BUFFER, allocated_frame_size);\n+    allocated_frame_size += BytesPerWord; \/\/ for address spill\n+  }\n+  if (_captured_state_mask != 0) {\n+    locs.set_frame_data(StubLocations::CAPTURED_STATE_BUFFER, allocated_frame_size);\n+    allocated_frame_size += BytesPerWord;\n+  }\n+\n+  allocated_frame_size = align_up(allocated_frame_size, StackAlignmentInBytes);\n+  _frame_size_slots = allocated_frame_size >> LogBytesPerInt;\n+\n+  _oop_maps  = new OopMapSet();\n+  address start = __ pc();\n+\n+  __ save_LR_CR(tmp); \/\/ Save in old frame.\n+  __ mr(callerSP, R1_SP); \/\/ preset (used to access caller frame argument slots)\n+  __ push_frame(allocated_frame_size, tmp);\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+  __ calculate_address_from_global_toc(tmp, the_pc, true, true, true, true);\n+  __ set_last_Java_frame(R1_SP, tmp);\n+  OopMap* map = new OopMap(_frame_size_slots, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ li(R0, _thread_in_native);\n+  __ release();\n+  __ stw(R0, in_bytes(JavaThread::thread_state_offset()), R16_thread);\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm, as_VMStorage(callerSP), frame::jit_out_preserve_size, frame::abi_minframe_size, locs);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ call_c(call_target_address);\n+\n+  if (_needs_return_buffer) {\n+    \/\/ Store return values as required by BoxBindingCalculator.\n+    __ ld(tmp, locs.data_offset(StubLocations::RETURN_BUFFER), R1_SP);\n+    int offset = 0;\n+    for (int i = 0; i < _output_registers.length(); i++) {\n+      VMStorage reg = _output_registers.at(i);\n+      if (reg.type() == StorageType::INTEGER) {\n+        \/\/ Store in matching size (not relevant for little endian).\n+        if (reg.segment_mask() == REG32_MASK) {\n+          __ stw(as_Register(reg), offset, tmp);\n+        } else {\n+          __ std(as_Register(reg), offset, tmp);\n+        }\n+        offset += 8;\n+      } else if (reg.type() == StorageType::FLOAT) {\n+        \/\/ Java code doesn't perform float-double format conversions. Do it here.\n+        if (reg.segment_mask() == REG32_MASK) {\n+          __ stfs(as_FloatRegister(reg), offset, tmp);\n+        } else {\n+          __ stfd(as_FloatRegister(reg), offset, tmp);\n+        }\n+        offset += 8;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  if (_captured_state_mask != 0) {\n+    __ block_comment(\"{ save thread local\");\n+\n+    if (should_save_return_value) {\n+      out_reg_spiller.generate_spill(_masm, spill_offset);\n+    }\n+\n+    __ load_const_optimized(call_target_address, CAST_FROM_FN_PTR(uint64_t, DowncallLinker::capture_state), R0);\n+    __ ld(R3_ARG1, locs.data_offset(StubLocations::CAPTURED_STATE_BUFFER), R1_SP);\n+    __ load_const_optimized(R4_ARG2, _captured_state_mask, R0);\n+    __ call_c(call_target_address);\n+\n+    if (should_save_return_value) {\n+      out_reg_spiller.generate_fill(_masm, spill_offset);\n+    }\n+\n+    __ block_comment(\"} save thread local\");\n+  }\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  \/\/ State transition\n+  __ li(tmp, _thread_in_native_trans);\n+  __ release();\n+  __ stw(tmp, in_bytes(JavaThread::thread_state_offset()), R16_thread);\n+  if (!UseSystemMemoryBarrier) {\n+    __ fence(); \/\/ Order state change wrt. safepoint poll.\n+  }\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, tmp, true \/* at_return *\/, false \/* in_nmethod *\/);\n+\n+  __ lwz(tmp, in_bytes(JavaThread::suspend_flags_offset()), R16_thread);\n+  __ cmpwi(CCR0, tmp, 0);\n+  __ bne(CCR0, L_safepoint_poll_slow_path);\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ li(tmp, _thread_in_Java);\n+  __ lwsync(); \/\/ Acquire safepoint and suspend state, release thread state.\n+  __ stw(tmp, in_bytes(JavaThread::thread_state_offset()), R16_thread);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ lwz(tmp, in_bytes(JavaThread::stack_guard_state_offset()), R16_thread);\n+  __ cmpwi(CCR0, tmp, StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ beq(CCR0, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame();\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(tmp);\n+  __ blr();\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+\n+  if (should_save_return_value) {\n+    \/\/ Need to save the native result registers around any runtime calls.\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n+\n+  __ load_const_optimized(call_target_address, CAST_FROM_FN_PTR(uint64_t, JavaThread::check_special_condition_for_native_trans), R0);\n+  __ mr(R3_ARG1, R16_thread);\n+  __ call_c(call_target_address);\n+\n+  if (should_save_return_value) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ b(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+\n+  if (should_save_return_value) {\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n+\n+  __ load_const_optimized(call_target_address, CAST_FROM_FN_PTR(uint64_t, SharedRuntime::reguard_yellow_pages), R0);\n+  __ call_c(call_target_address);\n+\n+  if (should_save_return_value) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ b(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n","filename":"src\/hotspot\/cpu\/ppc\/downcallLinker_ppc.cpp","additions":309,"deletions":5,"binary":false,"changes":314,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020 SAP SE. All rights reserved.\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n@@ -26,1 +26,6 @@\n-#include \"code\/vmreg.hpp\"\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -28,1 +33,3 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"prims\/foreignGlobals.inline.hpp\"\n+#include \"prims\/vmstorage.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -30,1 +37,11 @@\n-class MacroAssembler;\n+#define __ masm->\n+\n+bool ABIDescriptor::is_volatile_reg(Register reg) const {\n+  return _integer_argument_registers.contains(reg)\n+    || _integer_additional_volatile_registers.contains(reg);\n+}\n+\n+bool ABIDescriptor::is_volatile_reg(FloatRegister reg) const {\n+    return _float_argument_registers.contains(reg)\n+        || _float_additional_volatile_registers.contains(reg);\n+}\n@@ -32,1 +49,0 @@\n-\/\/ Stubbed out, implement later\n@@ -34,2 +50,22 @@\n-  Unimplemented();\n-  return {};\n+  oop abi_oop = JNIHandles::resolve_non_null(jabi);\n+  ABIDescriptor abi;\n+\n+  objArrayOop inputStorage = jdk_internal_foreign_abi_ABIDescriptor::inputStorage(abi_oop);\n+  parse_register_array(inputStorage, StorageType::INTEGER, abi._integer_argument_registers, as_Register);\n+  parse_register_array(inputStorage, StorageType::FLOAT, abi._float_argument_registers, as_FloatRegister);\n+\n+  objArrayOop outputStorage = jdk_internal_foreign_abi_ABIDescriptor::outputStorage(abi_oop);\n+  parse_register_array(outputStorage, StorageType::INTEGER, abi._integer_return_registers, as_Register);\n+  parse_register_array(outputStorage, StorageType::FLOAT, abi._float_return_registers, as_FloatRegister);\n+\n+  objArrayOop volatileStorage = jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(abi_oop);\n+  parse_register_array(volatileStorage, StorageType::INTEGER, abi._integer_additional_volatile_registers, as_Register);\n+  parse_register_array(volatileStorage, StorageType::FLOAT, abi._float_additional_volatile_registers, as_FloatRegister);\n+\n+  abi._stack_alignment_bytes = jdk_internal_foreign_abi_ABIDescriptor::stackAlignment(abi_oop);\n+  abi._shadow_space_bytes = jdk_internal_foreign_abi_ABIDescriptor::shadowSpace(abi_oop);\n+\n+  abi._scratch1 = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::scratch1(abi_oop));\n+  abi._scratch2 = parse_vmstorage(jdk_internal_foreign_abi_ABIDescriptor::scratch2(abi_oop));\n+\n+  return abi;\n@@ -39,2 +75,4 @@\n-  Unimplemented();\n-  return -1;\n+  if (reg.type() == StorageType::INTEGER || reg.type() == StorageType::FLOAT) {\n+    return 8;\n+  }\n+  return 0; \/\/ stack and BAD\n@@ -44,1 +82,7 @@\n-  Unimplemented();\n+  if (reg.type() == StorageType::INTEGER) {\n+    __ std(as_Register(reg), offset, R1_SP);\n+  } else if (reg.type() == StorageType::FLOAT) {\n+    __ stfd(as_FloatRegister(reg), offset, R1_SP);\n+  } else {\n+    \/\/ stack and BAD\n+  }\n@@ -48,1 +92,120 @@\n-  Unimplemented();\n+  if (reg.type() == StorageType::INTEGER) {\n+    __ ld(as_Register(reg), offset, R1_SP);\n+  } else if (reg.type() == StorageType::FLOAT) {\n+    __ lfd(as_FloatRegister(reg), offset, R1_SP);\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+static int reg2offset(VMStorage vms, int stk_bias) {\n+  assert(!vms.is_reg(), \"wrong usage\");\n+  return vms.index_or_offset() + stk_bias;\n+}\n+\n+static void move_reg64(MacroAssembler* masm, int out_stk_bias,\n+                       VMStorage from_reg, VMStorage to_reg) {\n+  int out_bias = 0;\n+  switch (to_reg.type()) {\n+    case StorageType::INTEGER:\n+      if (to_reg.segment_mask() == REG64_MASK && from_reg.segment_mask() == REG32_MASK) {\n+        \/\/ see CCallingConventionRequiresIntsAsLongs\n+        __ extsw(as_Register(to_reg), as_Register(from_reg));\n+      } else {\n+        __ mr_if_needed(as_Register(to_reg), as_Register(from_reg));\n+      }\n+      break;\n+    case StorageType::FLOAT:\n+      \/\/ FP arguments can get passed in GP reg!\n+      assert(from_reg.segment_mask() == to_reg.segment_mask(), \"sanity\");\n+      if (to_reg.segment_mask() == REG32_MASK) {\n+        __ stw(as_Register(from_reg), -8, R1_SP);\n+        __ lfs(as_FloatRegister(to_reg), -8, R1_SP);\n+      } else {\n+        __ std(as_Register(from_reg), -8, R1_SP);\n+        __ lfd(as_FloatRegister(to_reg), -8, R1_SP);\n+      }\n+      break;\n+    case StorageType::STACK:\n+      out_bias = out_stk_bias; \/\/ fallthrough\n+    case StorageType::FRAME_DATA:\n+      \/\/ Integer types always get a 64 bit slot in C.\n+      \/\/ Note: The case in which we'd have to store into a Java frame slot doesn't happen,\n+      \/\/ because we always have enough GP regs to hold all values passed in GP regs by C.\n+      if (from_reg.segment_mask() == REG32_MASK) {\n+        \/\/ see CCallingConventionRequiresIntsAsLongs\n+        __ extsw(R0, as_Register(from_reg));\n+        __ std(R0, reg2offset(to_reg, out_bias), R1_SP);\n+      } else {\n+        __ std(as_Register(from_reg), reg2offset(to_reg, out_bias), R1_SP);\n+      }\n+      break;\n+    default: ShouldNotReachHere();\n+  }\n+}\n+\n+static void move_float(MacroAssembler* masm, int out_stk_bias,\n+                       VMStorage from_reg, VMStorage to_reg) {\n+  switch (to_reg.type()) {\n+    case StorageType::INTEGER:\n+      \/\/ FP arguments can get passed in GP reg!\n+      assert(from_reg.segment_mask() == to_reg.segment_mask(), \"sanity\");\n+      if (from_reg.segment_mask() == REG32_MASK) {\n+        __ stfs(as_FloatRegister(from_reg), -8, R1_SP);\n+        __ lwa(as_Register(to_reg), -8, R1_SP);\n+      } else {\n+        __ stfd(as_FloatRegister(from_reg), -8, R1_SP);\n+        __ ld(as_Register(to_reg), -8, R1_SP);\n+      }\n+      break;\n+    case StorageType::FLOAT:\n+      __ fmr_if_needed(as_FloatRegister(to_reg), as_FloatRegister(from_reg));\n+      break;\n+    case StorageType::STACK:\n+      if (from_reg.segment_mask() == REG32_MASK) {\n+        assert(to_reg.stack_size() == 4, \"size should match\");\n+        \/\/ TODO: Check if AIX needs 4 Byte offset\n+        __ stfs(as_FloatRegister(from_reg), reg2offset(to_reg, out_stk_bias), R1_SP);\n+      } else {\n+        assert(to_reg.stack_size() == 8, \"size should match\");\n+        __ stfd(as_FloatRegister(from_reg), reg2offset(to_reg, out_stk_bias), R1_SP);\n+      }\n+      break;\n+    default: ShouldNotReachHere();\n+  }\n+}\n+\n+static void move_stack(MacroAssembler* masm, Register callerSP, int in_stk_bias, int out_stk_bias,\n+                       VMStorage from_reg, VMStorage to_reg) {\n+  int out_bias = 0;\n+  switch (to_reg.type()) {\n+    case StorageType::INTEGER:\n+      switch (from_reg.stack_size()) {\n+        case 8: __ ld( as_Register(to_reg), reg2offset(from_reg, in_stk_bias), callerSP); break;\n+        case 4: __ lwa(as_Register(to_reg), reg2offset(from_reg, in_stk_bias), callerSP); break;\n+        default: ShouldNotReachHere();\n+      }\n+      break;\n+    case StorageType::FLOAT:\n+      switch (from_reg.stack_size()) {\n+        case 8: __ lfd(as_FloatRegister(to_reg), reg2offset(from_reg, in_stk_bias), callerSP); break;\n+        case 4: __ lfs(as_FloatRegister(to_reg), reg2offset(from_reg, in_stk_bias), callerSP); break;\n+        default: ShouldNotReachHere();\n+      }\n+      break;\n+    case StorageType::STACK:\n+      out_bias = out_stk_bias; \/\/ fallthrough\n+    case StorageType::FRAME_DATA: {\n+      switch (from_reg.stack_size()) {\n+        case 8: __ ld( R0, reg2offset(from_reg, in_stk_bias), callerSP); break;\n+        case 4: __ lwa(R0, reg2offset(from_reg, in_stk_bias), callerSP); break;\n+        default: ShouldNotReachHere();\n+      }\n+      switch (to_reg.stack_size()) {\n+        case 8: __ std(R0, reg2offset(to_reg, out_bias), R1_SP); break;\n+        case 4: __ stw(R0, reg2offset(to_reg, out_bias), R1_SP); break;\n+        default: ShouldNotReachHere();\n+      }\n+    } break;\n+    default: ShouldNotReachHere();\n+  }\n@@ -52,1 +215,27 @@\n-  Unimplemented();\n+  Register callerSP = as_Register(tmp); \/\/ preset\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    VMStorage from_reg = move.from;\n+    VMStorage to_reg   = move.to;\n+\n+    \/\/ replace any placeholders\n+    if (from_reg.type() == StorageType::PLACEHOLDER) {\n+      from_reg = locs.get(from_reg);\n+    }\n+    if (to_reg.type() == StorageType::PLACEHOLDER) {\n+      to_reg = locs.get(to_reg);\n+    }\n+\n+    switch (from_reg.type()) {\n+      case StorageType::INTEGER:\n+        move_reg64(masm, out_stk_bias, from_reg, to_reg);\n+        break;\n+      case StorageType::FLOAT:\n+        move_float(masm, out_stk_bias, from_reg, to_reg);\n+        break;\n+      case StorageType::STACK:\n+        move_stack(masm, callerSP, in_stk_bias, out_stk_bias, from_reg, to_reg);\n+        break;\n+      default: ShouldNotReachHere();\n+    }\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.cpp","additions":202,"deletions":13,"binary":false,"changes":215,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020 SAP SE. All rights reserved.\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n@@ -28,1 +28,21 @@\n-class ABIDescriptor {};\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+struct ABIDescriptor {\n+  GrowableArray<Register> _integer_argument_registers;\n+  GrowableArray<Register> _integer_return_registers;\n+  GrowableArray<FloatRegister> _float_argument_registers;\n+  GrowableArray<FloatRegister> _float_return_registers;\n+\n+  GrowableArray<Register> _integer_additional_volatile_registers;\n+  GrowableArray<FloatRegister> _float_additional_volatile_registers;\n+\n+  int32_t _stack_alignment_bytes;\n+  int32_t _shadow_space_bytes;\n+\n+  VMStorage _scratch1;\n+  VMStorage _scratch2;\n+\n+  bool is_volatile_reg(Register reg) const;\n+  bool is_volatile_reg(FloatRegister reg) const;\n+};\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.hpp","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -209,2 +209,4 @@\n-  ShouldNotCallThis();\n-  return nullptr;\n+  assert(frame.is_upcall_stub_frame(), \"wrong frame\");\n+  \/\/ need unextended_sp here, since normal sp is wrong for interpreter callees\n+  return reinterpret_cast<UpcallStub::FrameData*>(\n+    reinterpret_cast<address>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n@@ -214,2 +216,19 @@\n-  ShouldNotCallThis();\n-  return false;\n+  assert(is_upcall_stub_frame(), \"must be optimzed entry frame\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  return jfa->last_Java_sp() == NULL;\n+}\n+\n+frame frame::sender_for_upcall_stub_frame(RegisterMap* map) const {\n+  assert(map != NULL, \"map must be set\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  assert(!upcall_stub_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  frame fr(jfa->last_Java_sp(), jfa->last_Java_pc());\n+\n+  return fr;\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.cpp","additions":23,"deletions":4,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -282,2 +282,3 @@\n-  if (is_entry_frame())           return sender_for_entry_frame(map);\n-  if (is_interpreted_frame())     return sender_for_interpreter_frame(map);\n+  if (is_entry_frame())       return sender_for_entry_frame(map);\n+  if (is_upcall_stub_frame()) return sender_for_upcall_stub_frame(map);\n+  if (is_interpreted_frame()) return sender_for_interpreter_frame(map);\n","filename":"src\/hotspot\/cpu\/ppc\/frame_ppc.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n@@ -311,1 +311,8 @@\n-  __ stop(\"Should not reach here\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ ld(temp_target, jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address_offset_in_bytes(), nep_reg);\n+  __ mtctr(temp_target);\n+  __ bctr();\n","filename":"src\/hotspot\/cpu\/ppc\/methodHandles_ppc.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020 SAP SE. All rights reserved.\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n@@ -26,0 +26,3 @@\n+#include \"asm\/macroAssembler.inline.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n@@ -27,1 +30,87 @@\n-#include \"utilities\/debug.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#define __ _masm->\n+\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (int i = 0; i < Register::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n+    if (reg == R1_SP || reg == R13) continue;\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  return size;\n+}\n+\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (int i = 0; i < Register::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n+    if (reg == R1_SP || reg == R13) continue;\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ std(reg, offset, R1_SP);\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ stfd(reg, offset, R1_SP);\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n+\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (int i = 0; i < Register::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    \/\/ R1 saved\/restored by prologue\/epilogue, R13 (system thread) won't get modified!\n+    if (reg == R1_SP || reg == R13) continue;\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ld(reg, offset, R1_SP);\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegister::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ lfd(reg, offset, R1_SP);\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n@@ -35,2 +124,234 @@\n-  ShouldNotCallThis();\n-  return nullptr;\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  CodeBuffer buffer(\"upcall_stub\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  Register callerSP            = R2, \/\/ C\/C++ uses R2 as TOC, but we can reuse it here\n+           tmp                 = R11_scratch1, \/\/ same as shuffle_reg\n+           call_target_address = R12_scratch2; \/\/ same as _abi._scratch2\n+  VMStorage shuffle_reg = abi._scratch1;\n+  JavaCallingConvention out_conv;\n+  NativeCallingConvention in_conv(call_regs._arg_regs);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg);\n+  \/\/ The Java call uses the JIT ABI, but we also call C.\n+  int out_arg_area = MAX2(frame::jit_out_preserve_size + arg_shuffle.out_arg_bytes(), (int)frame::abi_reg_args_size);\n+\n+#ifndef PRODUCT\n+  LogTarget(Trace, foreign, upcall) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spiller(call_regs._arg_regs);\n+  RegSpiller result_spiller(call_regs._ret_regs);\n+\n+  int res_save_area_offset   = out_arg_area;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spiller.spill_size_bytes();\n+  int frame_data_offset      = reg_save_area_offset   + reg_save_area_size;\n+  int frame_bottom_offset    = frame_data_offset      + sizeof(UpcallStub::FrameData);\n+\n+  StubLocations locs;\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n+    \/\/ use a free register for shuffling code to pick up return\n+    \/\/ buffer address from\n+    locs.set(StubLocations::RETURN_BUFFER, abi._scratch2);\n+  }\n+\n+  int frame_size = align_up(frame_bottom_offset, StackAlignmentInBytes);\n+\n+  \/\/ The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_are_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ function_entry(); \/\/ called by C\n+  __ save_LR_CR(R0);\n+  assert((abi._stack_alignment_bytes % 16) == 0, \"must be 16 byte aligned\");\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ push_frame(frame_size, tmp);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it).\n+  arg_spiller.generate_spill(_masm, arg_save_area_offset);\n+  \/\/ Java methods won't preserve them, so save them here:\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  \/\/ Java code uses TOC (pointer to code cache).\n+  __ load_const_optimized(R29_TOC, MacroAssembler::global_toc(), R0); \/\/ reinit\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ load_const_optimized(call_target_address, CAST_FROM_FN_PTR(uint64_t, UpcallLinker::on_entry), R0);\n+  __ addi(R3_ARG1, R1_SP, frame_data_offset);\n+  __ call_c(call_target_address);\n+  __ mr(R16_thread, R3_RET);\n+  __ block_comment(\"} on_entry\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spiller.generate_fill(_masm, arg_save_area_offset);\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ addi(as_Register(locs.get(StubLocations::RETURN_BUFFER)), R1_SP, ret_buf_offset);\n+  }\n+  __ ld(callerSP, _abi0(callers_sp), R1_SP); \/\/ preset (used to access caller frame argument slots)\n+  arg_shuffle.generate(_masm, as_VMStorage(callerSP), frame::abi_minframe_size, frame::jit_out_preserve_size, locs);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ load_const_optimized(R3_ARG1, (intptr_t)receiver, R0);\n+  __ resolve_jobject(R3_ARG1, tmp, R31, MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS); \/\/ kills R31\n+  __ block_comment(\"} receiver \");\n+\n+  __ load_const_optimized(R19_method, (intptr_t)entry);\n+  __ std(R19_method, in_bytes(JavaThread::callee_target_offset()), R16_thread);\n+\n+  __ ld(call_target_address, in_bytes(Method::from_compiled_offset()), R19_method);\n+  __ mtctr(call_target_address);\n+  __ bctrl();\n+\n+  \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+    \/\/ CallArranger can pick a return type that goes in the same reg for both CCs.\n+    if (call_regs._ret_regs.length() > 0) { \/\/ 0 or 1\n+      VMStorage ret_reg = call_regs._ret_regs.at(0);\n+      \/\/ Check if the return reg is as expected.\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+          __ extsw(R3_RET, R3_RET); \/\/ Clear garbage in high half.\n+          \/\/ fallthrough\n+        case T_LONG:\n+          assert(as_Register(ret_reg) == R3_RET, \"unexpected result register\");\n+          break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          assert(as_FloatRegister(ret_reg) == F1_RET, \"unexpected result register\");\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+    }\n+  } else {\n+    \/\/ Load return values as required by UnboxBindingCalculator.\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    int offset = ret_buf_offset;\n+    for (int i = 0; i < call_regs._ret_regs.length(); i++) {\n+      VMStorage reg = call_regs._ret_regs.at(i);\n+      if (reg.type() == StorageType::INTEGER) {\n+        \/\/ Load in matching size (not relevant for little endian).\n+        if (reg.segment_mask() == REG32_MASK) {\n+          __ lwa(as_Register(reg), offset, R1_SP);\n+        } else {\n+          __ ld(as_Register(reg), offset, R1_SP);\n+        }\n+        offset += 8;\n+      } else if (reg.type() == StorageType::FLOAT) {\n+        \/\/ Java code doesn't perform float-double format conversions. Do it here.\n+        if (reg.segment_mask() == REG32_MASK) {\n+          __ lfs(as_FloatRegister(reg), offset, R1_SP);\n+        } else {\n+          __ lfd(as_FloatRegister(reg), offset, R1_SP);\n+        }\n+        offset += 8;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ load_const_optimized(call_target_address, CAST_FROM_FN_PTR(uint64_t, UpcallLinker::on_exit), R0);\n+  __ addi(R3_ARG1, R1_SP, frame_data_offset);\n+  __ call_c(call_target_address);\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n+  __ pop_frame();\n+  __ restore_LR_CR(R0);\n+  __ blr();\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ exception handler\");\n+\n+  intptr_t exception_handler_offset = __ pc() - start;\n+\n+  \/\/ Native caller has no idea how to handle exceptions,\n+  \/\/ so we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(R3_ARG1);\n+  __ load_const_optimized(call_target_address, CAST_FROM_FN_PTR(uint64_t, UpcallLinker::handle_uncaught_exception), R0);\n+  __ call_c(call_target_address);\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n+\n+  _masm->flush();\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char* name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  UpcallStub* blob\n+    = UpcallStub::create(name,\n+                         &buffer,\n+                         exception_handler_offset,\n+                         receiver,\n+                         in_ByteSize(frame_data_offset));\n+#ifndef ABI_ELFv2\n+  \/\/ Need to patch the FunctionDescriptor after relocating.\n+  address fd_addr = blob->code_begin();\n+  FunctionDescriptor* fd = (FunctionDescriptor*)fd_addr;\n+  fd->set_entry(fd_addr + sizeof(FunctionDescriptor));\n+#endif\n+\n+#ifndef PRODUCT\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    blob->print_on(&ls);\n+  }\n+#endif\n+\n+  return blob->code_begin();\n","filename":"src\/hotspot\/cpu\/ppc\/upcallLinker_ppc.cpp","additions":326,"deletions":5,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -2,1 +2,2 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n@@ -31,0 +32,1 @@\n+\/\/ keep in sync with jdk\/internal\/foreign\/abi\/ppc64\/PPC64Architecture\n@@ -32,2 +34,4 @@\n-  STACK = 0,\n-  PLACEHOLDER = 1,\n+  INTEGER = 0,\n+  FLOAT = 1,\n+  STACK = 2,\n+  PLACEHOLDER = 3,\n@@ -35,1 +39,1 @@\n-  FRAME_DATA = PLACEHOLDER + 1,\n+  FRAME_DATA = 4,\n@@ -41,1 +45,1 @@\n-   return false;\n+   return type == StorageType::INTEGER || type == StorageType::FLOAT;\n@@ -47,1 +51,54 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+\/\/ Needs to be consistent with PPC64Architecture.java.\n+constexpr uint16_t REG32_MASK = 0b0000000000000001;\n+constexpr uint16_t REG64_MASK = 0b0000000000000011;\n+\n+inline Register as_Register(VMStorage vms) {\n+  assert(vms.type() == StorageType::INTEGER, \"not the right type\");\n+  return ::as_Register(vms.index());\n+}\n+\n+inline FloatRegister as_FloatRegister(VMStorage vms) {\n+  assert(vms.type() == StorageType::FLOAT, \"not the right type\");\n+  return ::as_FloatRegister(vms.index());\n+}\n+\n+constexpr inline VMStorage as_VMStorage(Register reg, uint16_t segment_mask = REG64_MASK) {\n+  return VMStorage::reg_storage(StorageType::INTEGER, segment_mask, reg.encoding());\n+}\n+\n+constexpr inline VMStorage as_VMStorage(FloatRegister reg, uint16_t segment_mask = REG64_MASK) {\n+  return VMStorage::reg_storage(StorageType::FLOAT, segment_mask, reg.encoding());\n+}\n+\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n+  if (reg->is_Register()) {\n+    uint16_t segment_mask = 0;\n+    switch (bt) {\n+      case T_BOOLEAN:\n+      case T_CHAR   :\n+      case T_BYTE   :\n+      case T_SHORT  :\n+      case T_INT    : segment_mask = REG32_MASK; break;\n+      default       : segment_mask = REG64_MASK; break;\n+    }\n+    return as_VMStorage(reg->as_Register(), segment_mask);\n+  } else if (reg->is_FloatRegister()) {\n+    \/\/ FP regs always use double format. However, we need the correct format for loads \/stores.\n+    return as_VMStorage(reg->as_FloatRegister(), (bt == T_FLOAT) ? REG32_MASK : REG64_MASK);\n+  } else if (reg->is_stack()) {\n+    uint16_t size = 0;\n+    switch (bt) {\n+      case T_BOOLEAN:\n+      case T_CHAR   :\n+      case T_BYTE   :\n+      case T_SHORT  :\n+      case T_INT    :\n+      case T_FLOAT  : size = 4; break;\n+      default       : size = 8; break;\n+    }\n+    return VMStorage(StorageType::STACK, size,\n+                     checked_cast<uint16_t>(reg->reg2stack() * VMRegImpl::stack_slot_size));\n+  } else if (!reg->is_valid()) {\n+    return VMStorage::invalid();\n+  }\n+\n@@ -52,1 +109,1 @@\n-#endif \/\/ CPU_PPC_VMSTORAGE_PPC_INLINE_HPP\n\\ No newline at end of file\n+#endif \/\/ CPU_PPC_VMSTORAGE_PPC_INLINE_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/vmstorage_ppc.hpp","additions":64,"deletions":7,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n","filename":"src\/hotspot\/cpu\/riscv\/vmstorage_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n","filename":"src\/hotspot\/cpu\/s390\/vmstorage_s390.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -85,1 +85,1 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n","filename":"src\/hotspot\/cpu\/x86\/vmstorage_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-inline VMStorage as_VMStorage(VMReg reg) {\n+inline VMStorage as_VMStorage(VMReg reg, BasicType bt) {\n","filename":"src\/hotspot\/cpu\/zero\/vmstorage_zero.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -179,1 +179,1 @@\n-    regs[i] = as_VMStorage(pair.first());\n+    regs[i] = as_VMStorage(pair.first(), sig_bt[i]);\n","filename":"src\/hotspot\/share\/prims\/foreignGlobals.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+    LINUX_PPC_64,\n@@ -65,0 +66,4 @@\n+        } else if (ARCH.equals(\"ppc64le\")) {\n+            \/\/ ppc64le only exists for Linux (which uses useABIv2 = true).\n+            \/\/ Big Endian linux \/ AIX implementation is not yet complete (useABIv2 = false).\n+            ABI = LINUX_PPC_64;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/CABI.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -214,0 +214,55 @@\n+    \/**\n+     * This class defines layout constants modelling standard primitive types supported by the PPC64 ABI.\n+     *\/\n+    public static final class PPC64 {\n+\n+        private PPC64() {\n+            \/\/just the one\n+        }\n+\n+        \/**\n+         * The {@code bool} native type.\n+         *\/\n+        public static final ValueLayout.OfBoolean C_BOOL = ValueLayout.JAVA_BOOLEAN;\n+\n+        \/**\n+         * The {@code char} native type.\n+         *\/\n+        public static final ValueLayout.OfByte C_CHAR = ValueLayout.JAVA_BYTE;\n+\n+        \/**\n+         * The {@code short} native type.\n+         *\/\n+        public static final ValueLayout.OfShort C_SHORT = ValueLayout.JAVA_SHORT.withBitAlignment(16);\n+\n+        \/**\n+         * The {@code int} native type.\n+         *\/\n+        public static final ValueLayout.OfInt C_INT = ValueLayout.JAVA_INT.withBitAlignment(32);\n+\n+        \/**\n+         * The {@code long} native type.\n+         *\/\n+        public static final ValueLayout.OfLong C_LONG = ValueLayout.JAVA_LONG.withBitAlignment(64);\n+\n+        \/**\n+         * The {@code long long} native type.\n+         *\/\n+        public static final ValueLayout.OfLong C_LONG_LONG = ValueLayout.JAVA_LONG.withBitAlignment(64);\n+\n+        \/**\n+         * The {@code float} native type.\n+         *\/\n+        public static final ValueLayout.OfFloat C_FLOAT = ValueLayout.JAVA_FLOAT.withBitAlignment(32);\n+\n+        \/**\n+         * The {@code double} native type.\n+         *\/\n+        public static final ValueLayout.OfDouble C_DOUBLE = ValueLayout.JAVA_DOUBLE.withBitAlignment(64);\n+\n+        \/**\n+         * The {@code T*} native type.\n+         *\/\n+        public static final ValueLayout.OfAddress C_POINTER = ValueLayout.ADDRESS.withBitAlignment(64).asUnbounded();\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/PlatformLayouts.java","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-                case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64, LINUX_RISCV_64 -> libLookup(libs -> libs.load(jdkLibraryPath(\"syslookup\")));\n+                case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64, LINUX_PPC_64, LINUX_RISCV_64 -> libLookup(libs -> libs.load(jdkLibraryPath(\"syslookup\")));\n@@ -122,1 +122,1 @@\n-            case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64, LINUX_RISCV_64 -> \"lib\";\n+            case SYS_V, LINUX_AARCH_64, MAC_OS_AARCH_64, LINUX_PPC_64, LINUX_RISCV_64 -> \"lib\";\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/SystemLookup.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import jdk.internal.foreign.abi.ppc64.linux.LinuxPPC64Linker;\n@@ -48,1 +49,1 @@\n-                                                                      SysVx64Linker, WindowsAArch64Linker, Windowsx64Linker, LinuxRISCV64Linker {\n+                                                                      SysVx64Linker, WindowsAArch64Linker, Windowsx64Linker, LinuxPPC64Linker, LinuxRISCV64Linker {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/AbstractLinker.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import jdk.internal.foreign.abi.ppc64.linux.LinuxPPC64Linker;\n@@ -194,0 +195,1 @@\n+            case LINUX_PPC_64 -> LinuxPPC64Linker.getInstance();\n@@ -308,0 +310,1 @@\n+            default -> throw new RuntimeException(\"Unsupported platform\");\n@@ -319,0 +322,1 @@\n+            default -> throw new RuntimeException(\"Unsupported platform\");\n@@ -330,0 +334,1 @@\n+            default -> throw new RuntimeException(\"Unsupported platform\");\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,469 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.ppc64;\n+\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n+import jdk.internal.foreign.abi.ABIDescriptor;\n+import jdk.internal.foreign.abi.Binding;\n+import jdk.internal.foreign.abi.CallingSequence;\n+import jdk.internal.foreign.abi.CallingSequenceBuilder;\n+import jdk.internal.foreign.abi.DowncallLinker;\n+import jdk.internal.foreign.abi.LinkerOptions;\n+import jdk.internal.foreign.abi.UpcallLinker;\n+import jdk.internal.foreign.abi.SharedUtils;\n+import jdk.internal.foreign.abi.VMStorage;\n+import jdk.internal.foreign.abi.ppc64.linux.LinuxPPC64CallArranger;\n+import jdk.internal.foreign.Utils;\n+\n+import java.lang.foreign.SegmentScope;\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodType;\n+import java.nio.ByteOrder;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static jdk.internal.foreign.PlatformLayouts.*;\n+import static jdk.internal.foreign.abi.ppc64.PPC64Architecture.*;\n+import static jdk.internal.foreign.abi.ppc64.PPC64Architecture.Regs.*;\n+\n+\/**\n+ * For the PPC64 C ABI specifically, this class uses CallingSequenceBuilder\n+ * to translate a C FunctionDescriptor into a CallingSequence, which can then be turned into a MethodHandle.\n+ *\n+ * This includes taking care of synthetic arguments like pointers to return buffers for 'in-memory' returns.\n+ *\n+ * There are minor differences between the ABIs implemented on Linux and AIX\n+ * which are handled in sub-classes. Clients should access these through the provided\n+ * public constants CallArranger.LINUX.\n+ *\/\n+public abstract class CallArranger {\n+    \/\/ Linux PPC64 Little Endian uses ABI v2.\n+    private static final boolean useABIv2 = ByteOrder.nativeOrder() == ByteOrder.LITTLE_ENDIAN;\n+    private static final int STACK_SLOT_SIZE = 8;\n+    public static final int MAX_REGISTER_ARGUMENTS = 8;\n+    public static final int MAX_FLOAT_REGISTER_ARGUMENTS = 13;\n+\n+    \/\/ This is derived from the 64-Bit ELF V2 ABI spec, restricted to what's\n+    \/\/ possible when calling to\/from C code.\n+    private static final ABIDescriptor C = abiFor(\n+        new VMStorage[] { r3, r4, r5, r6, r7, r8, r9, r10 }, \/\/ GP input\n+        new VMStorage[] { f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, f11, f12, f13 }, \/\/ FP intput\n+        new VMStorage[] { r3, r4 }, \/\/ GP output\n+        new VMStorage[] { f1, f2, f3, f4, f5, f6, f7, f8 }, \/\/ FP output\n+        new VMStorage[] { r0, r2, r11, r12 }, \/\/ volatile GP (excluding argument registers)\n+        new VMStorage[] { f0 }, \/\/ volatile FP (excluding argument registers)\n+        16, \/\/ Stack is always 16 byte aligned on PPC64\n+        useABIv2 ? 32 : 48, \/\/ ABI header (excluding argument register spill slots)\n+        r11, \/\/ scratch reg\n+        r12  \/\/ target addr reg, otherwise used as scratch reg\n+    );\n+\n+    public record Bindings(CallingSequence callingSequence,\n+                           boolean isInMemoryReturn) {\n+    }\n+\n+    private record HfaRegs(VMStorage[] first, VMStorage[] second) {}\n+\n+    public static final CallArranger LINUX = new LinuxPPC64CallArranger();\n+\n+    protected CallArranger() {}\n+\n+    public Bindings getBindings(MethodType mt, FunctionDescriptor cDesc, boolean forUpcall) {\n+        return getBindings(mt, cDesc, forUpcall, LinkerOptions.empty());\n+    }\n+\n+    public Bindings getBindings(MethodType mt, FunctionDescriptor cDesc, boolean forUpcall, LinkerOptions options) {\n+        CallingSequenceBuilder csb = new CallingSequenceBuilder(C, forUpcall, options);\n+\n+        BindingCalculator argCalc = forUpcall ? new BoxBindingCalculator(true) : new UnboxBindingCalculator(true);\n+        BindingCalculator retCalc = forUpcall ? new UnboxBindingCalculator(false) : new BoxBindingCalculator(false);\n+\n+        boolean returnInMemory = isInMemoryReturn(cDesc.returnLayout());\n+        if (returnInMemory) {\n+            Class<?> carrier = MemorySegment.class;\n+            MemoryLayout layout = PPC64.C_POINTER;\n+            csb.addArgumentBindings(carrier, layout, argCalc.getBindings(carrier, layout));\n+        } else if (cDesc.returnLayout().isPresent()) {\n+            Class<?> carrier = mt.returnType();\n+            MemoryLayout layout = cDesc.returnLayout().get();\n+            csb.setReturnBindings(carrier, layout, retCalc.getBindings(carrier, layout));\n+        }\n+\n+        for (int i = 0; i < mt.parameterCount(); i++) {\n+            Class<?> carrier = mt.parameterType(i);\n+            MemoryLayout layout = cDesc.argumentLayouts().get(i);\n+            if (options.isVarargsIndex(i)) {\n+                argCalc.storageCalculator.adjustForVarArgs();\n+            }\n+            csb.addArgumentBindings(carrier, layout, argCalc.getBindings(carrier, layout));\n+        }\n+\n+        return new Bindings(csb.build(), returnInMemory);\n+    }\n+\n+    public MethodHandle arrangeDowncall(MethodType mt, FunctionDescriptor cDesc, LinkerOptions options) {\n+        Bindings bindings = getBindings(mt, cDesc, false, options);\n+\n+        MethodHandle handle = new DowncallLinker(C, bindings.callingSequence).getBoundMethodHandle();\n+\n+        if (bindings.isInMemoryReturn) {\n+            handle = SharedUtils.adaptDowncallForIMR(handle, cDesc, bindings.callingSequence);\n+        }\n+\n+        return handle;\n+    }\n+\n+    public MemorySegment arrangeUpcall(MethodHandle target, MethodType mt, FunctionDescriptor cDesc, SegmentScope session) {\n+        Bindings bindings = getBindings(mt, cDesc, true);\n+\n+        if (bindings.isInMemoryReturn) {\n+            target = SharedUtils.adaptUpcallForIMR(target, true \/* drop return, since we don't have bindings for it *\/);\n+        }\n+\n+        return UpcallLinker.make(C, target, bindings.callingSequence, session);\n+    }\n+\n+    private static boolean isInMemoryReturn(Optional<MemoryLayout> returnLayout) {\n+        return returnLayout\n+            .filter(GroupLayout.class::isInstance)\n+            .filter(layout -> !TypeClass.isStructHFAorReturnRegisterAggregate(layout, useABIv2))\n+            .isPresent();\n+    }\n+\n+    class StorageCalculator {\n+        private final boolean forArguments;\n+        private boolean forVarArgs = false;\n+\n+        private final int[] nRegs = new int[] { 0, 0 };\n+        private long stackOffset = 0;\n+\n+        public StorageCalculator(boolean forArguments) {\n+            this.forArguments = forArguments;\n+        }\n+\n+        VMStorage stackAlloc(long size, long alignment) {\n+            long alignedStackOffset = Utils.alignUp(stackOffset, alignment);\n+\n+            short encodedSize = (short) size;\n+            assert (encodedSize & 0xFFFF) == size;\n+\n+            VMStorage storage = PPC64Architecture.stackStorage(encodedSize, (int)alignedStackOffset);\n+            stackOffset = alignedStackOffset + size;\n+            return storage;\n+        }\n+\n+        VMStorage regAlloc(int type) {\n+            \/\/ GP regs always need to get reserved even when float regs are used.\n+            int gpRegCnt = 1;\n+            int fpRegCnt = (type == StorageType.INTEGER) ? 0 : 1;\n+\n+            \/\/ Use stack if not enough registers available.\n+            if (type == StorageType.FLOAT && nRegs[StorageType.FLOAT] + fpRegCnt > MAX_FLOAT_REGISTER_ARGUMENTS) {\n+                type = StorageType.INTEGER; \/\/ Try gp reg.\n+            }\n+            if (type == StorageType.INTEGER && nRegs[StorageType.INTEGER] + gpRegCnt > MAX_REGISTER_ARGUMENTS) return null;\n+\n+            VMStorage[] source = (forArguments ? C.inputStorage : C.outputStorage)[type];\n+            VMStorage result = source[nRegs[type]];\n+\n+            nRegs[StorageType.INTEGER] += gpRegCnt;\n+            nRegs[StorageType.FLOAT] += fpRegCnt;\n+            return result;\n+        }\n+\n+        \/\/ Integers need size for int to long conversion (required by ABI).\n+        \/\/ FP loads and stores must use the correct IEEE 754 precision format (32\/64 bit).\n+        \/\/ Note: Can return a GP reg for a float!\n+        VMStorage nextStorage(int type, boolean is32Bit) {\n+            VMStorage reg = regAlloc(type);\n+            VMStorage stack;\n+            if (!useABIv2 && is32Bit) {\n+                stackAlloc(4, STACK_SLOT_SIZE); \/\/ Skip first half of stack slot.\n+                stack = stackAlloc(4, 4);\n+            } else {\n+                stack = stackAlloc(is32Bit ? 4 : 8, STACK_SLOT_SIZE);\n+            }\n+            if (reg == null) return stack;\n+            if (is32Bit) {\n+                reg = new VMStorage(reg.type(), PPC64Architecture.REG32_MASK, reg.indexOrOffset());\n+            }\n+            return reg;\n+        }\n+\n+        \/\/ Regular struct, no HFA.\n+        VMStorage[] structAlloc(MemoryLayout layout) {\n+            \/\/ TODO: Big Endian can't pass partially used slots correctly.\n+            if (!useABIv2 && layout.byteSize() % 8 != 0) throw new UnsupportedOperationException(\n+                \"Only MemoryLayouts with size multiple of 8 supported. This layout has size \" +\n+                layout.byteSize() + \".\");\n+\n+            \/\/ Allocate individual fields as gp slots (regs and stack).\n+            int nFields = (int) ((layout.byteSize() + 7) \/ 8);\n+            VMStorage[] result = new VMStorage[nFields];\n+            for (int i = 0; i < nFields; i++) {\n+                result[i] = nextStorage(StorageType.INTEGER, false);\n+            }\n+            return result;\n+        }\n+\n+        HfaRegs hfaAlloc(List<MemoryLayout> scalarLayouts) {\n+            \/\/ Determine count and type.\n+            int count = scalarLayouts.size();\n+            Class<?> elementCarrier = ((ValueLayout) (scalarLayouts.get(0))).carrier();\n+            int elementSize = (elementCarrier == float.class) ? 4 : 8;\n+\n+            \/\/ Allocate registers.\n+            int fpRegCnt = count;\n+            \/\/ Rest will get put into a struct. Compute number of 64 bit slots.\n+            int structSlots = 0;\n+            boolean needOverlapping = false; \/\/ See \"no partial DW rule\" below.\n+\n+            int availableFpRegs = MAX_FLOAT_REGISTER_ARGUMENTS - nRegs[StorageType.FLOAT];\n+            if (count > availableFpRegs) {\n+                fpRegCnt = availableFpRegs;\n+                int remainingElements = count - availableFpRegs;\n+                if (elementCarrier == float.class) {\n+                    if ((fpRegCnt & 1) != 0) {\n+                        needOverlapping = true;\n+                        remainingElements--; \/\/ After overlapped one.\n+                    }\n+                    structSlots = (remainingElements + 1) \/ 2;\n+                } else {\n+                    structSlots = remainingElements;\n+                }\n+            }\n+\n+            VMStorage[] source = (forArguments ? C.inputStorage : C.outputStorage)[StorageType.FLOAT];\n+            VMStorage[] result  = new VMStorage[fpRegCnt + structSlots],\n+                        result2 = new VMStorage[fpRegCnt + structSlots]; \/\/ For overlapping.\n+            if (elementCarrier == float.class) {\n+                \/\/ Mark elements as single precision (32 bit).\n+                for (int i = 0; i < fpRegCnt; i++) {\n+                    VMStorage sourceReg = source[nRegs[StorageType.FLOAT] + i];\n+                    result[i] = new VMStorage(StorageType.FLOAT, PPC64Architecture.REG32_MASK,\n+                                              sourceReg.indexOrOffset());\n+                }\n+            } else {\n+                for (int i = 0; i < fpRegCnt; i++) {\n+                    result[i] = source[nRegs[StorageType.FLOAT] + i];\n+                }\n+            }\n+\n+            nRegs[StorageType.FLOAT] += fpRegCnt;\n+            \/\/ Reserve GP regs and stack slots for the packed HFA (when using single precision).\n+            int gpRegCnt = (elementCarrier == float.class) ? ((fpRegCnt + 1) \/ 2)\n+                                                           : fpRegCnt;\n+            nRegs[StorageType.INTEGER] += gpRegCnt;\n+            stackAlloc(fpRegCnt * elementSize, STACK_SLOT_SIZE);\n+\n+            if (needOverlapping) {\n+                \/\/ \"no partial DW rule\": Put GP reg or stack slot into result2.\n+                \/\/ Note: Can only happen with forArguments = true.\n+                VMStorage overlappingReg;\n+                if (nRegs[StorageType.INTEGER] <= MAX_REGISTER_ARGUMENTS) {\n+                    VMStorage allocatedGpReg = C.inputStorage[StorageType.INTEGER][nRegs[StorageType.INTEGER] - 1];\n+                    overlappingReg = new VMStorage(StorageType.INTEGER,\n+                                                   PPC64Architecture.REG64_MASK, allocatedGpReg.indexOrOffset());\n+                } else {\n+                    overlappingReg = new VMStorage(StorageType.STACK,\n+                                                   (short) STACK_SLOT_SIZE, (int) stackOffset - 4);\n+                    stackOffset += 4; \/\/ We now have a 64 bit slot, but reserved only 32 bit before.\n+                }\n+                result2[fpRegCnt - 1] = overlappingReg;\n+            }\n+\n+            \/\/ Allocate rest as struct.\n+            for (int i = 0; i < structSlots; i++) {\n+                result[fpRegCnt + i] = nextStorage(StorageType.INTEGER, false);\n+            }\n+\n+            return new HfaRegs(result, result2);\n+        }\n+\n+        void adjustForVarArgs() {\n+            \/\/ PPC64 can pass VarArgs in GP regs. But we're not using FP regs.\n+            nRegs[StorageType.FLOAT] = MAX_FLOAT_REGISTER_ARGUMENTS;\n+            forVarArgs = true;\n+        }\n+    }\n+\n+    abstract class BindingCalculator {\n+        protected final StorageCalculator storageCalculator;\n+\n+        protected BindingCalculator(boolean forArguments) {\n+            this.storageCalculator = new StorageCalculator(forArguments);\n+        }\n+\n+        abstract List<Binding> getBindings(Class<?> carrier, MemoryLayout layout);\n+    }\n+\n+    \/\/ Compute recipe for transfering arguments \/ return values to C from Java.\n+    class UnboxBindingCalculator extends BindingCalculator {\n+        UnboxBindingCalculator(boolean forArguments) {\n+            super(forArguments);\n+        }\n+\n+        @Override\n+        List<Binding> getBindings(Class<?> carrier, MemoryLayout layout) {\n+            TypeClass argumentClass = TypeClass.classifyLayout(layout, useABIv2);\n+            Binding.Builder bindings = Binding.builder();\n+            switch (argumentClass) {\n+                case STRUCT_REGISTER -> {\n+                    assert carrier == MemorySegment.class;\n+                    VMStorage[] regs = storageCalculator.structAlloc(layout);\n+                    long offset = 0;\n+                    for (VMStorage storage : regs) {\n+                        \/\/ Last slot may be partly used.\n+                        final long size = Math.min(layout.byteSize() - offset, 8);\n+                        Class<?> type = SharedUtils.primitiveCarrierForSize(size, false);\n+                        if (offset + size < layout.byteSize()) {\n+                            bindings.dup();\n+                        }\n+                        bindings.bufferLoad(offset, type, (int) size)\n+                                .vmStore(storage, type);\n+                        offset += size;\n+                    }\n+                }\n+                case STRUCT_HFA -> {\n+                    assert carrier == MemorySegment.class;\n+                    List<MemoryLayout> scalarLayouts = TypeClass.scalarLayouts((GroupLayout) layout);\n+                    HfaRegs regs = storageCalculator.hfaAlloc(scalarLayouts);\n+                    final long baseSize = scalarLayouts.get(0).byteSize();\n+                    long offset = 0;\n+                    for (int index = 0; index < regs.first().length; index++) {\n+                        VMStorage storage = regs.first()[index];\n+                        \/\/ Floats are 4 Bytes, Double, GP reg and stack slots 8 Bytes (except maybe last slot).\n+                        long size = (baseSize == 4 &&\n+                                     (storage.type() == StorageType.FLOAT || layout.byteSize() - offset < 8)) ? 4 : 8;\n+                        Class<?> type = SharedUtils.primitiveCarrierForSize(size, storage.type() == StorageType.FLOAT);\n+                        if (offset + size < layout.byteSize()) {\n+                            bindings.dup();\n+                        }\n+                        bindings.bufferLoad(offset, type)\n+                                .vmStore(storage, type);\n+                        VMStorage storage2 = regs.second()[index];\n+                        if (storage2 != null) {\n+                            \/\/ We have a second slot to fill (always 64 bit GP reg or stack slot).\n+                            size = 8;\n+                            if (offset + size < layout.byteSize()) {\n+                                bindings.dup();\n+                            }\n+                            bindings.bufferLoad(offset, long.class)\n+                                    .vmStore(storage2, long.class);\n+                        }\n+                        offset += size;\n+                    }\n+                }\n+                case POINTER -> {\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, false);\n+                    bindings.unboxAddress()\n+                            .vmStore(storage, long.class);\n+                }\n+                case INTEGER -> {\n+                    \/\/ ABI requires all int types to get extended to 64 bit.\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, false);\n+                    bindings.vmStore(storage, carrier);\n+                }\n+                case FLOAT -> {\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.FLOAT, carrier == float.class);\n+                    bindings.vmStore(storage, carrier);\n+                }\n+                default -> throw new UnsupportedOperationException(\"Unhandled class \" + argumentClass);\n+            }\n+            return bindings.build();\n+        }\n+    }\n+\n+    \/\/ Compute recipe for transfering arguments \/ return values from C to Java.\n+    class BoxBindingCalculator extends BindingCalculator {\n+        BoxBindingCalculator(boolean forArguments) {\n+            super(forArguments);\n+        }\n+\n+        @Override\n+        List<Binding> getBindings(Class<?> carrier, MemoryLayout layout) {\n+            TypeClass argumentClass = TypeClass.classifyLayout(layout, useABIv2);\n+            Binding.Builder bindings = Binding.builder();\n+            switch (argumentClass) {\n+                case STRUCT_REGISTER -> {\n+                    assert carrier == MemorySegment.class;\n+                    bindings.allocate(layout);\n+                    VMStorage[] regs = storageCalculator.structAlloc(layout);\n+                    long offset = 0;\n+                    for (VMStorage storage : regs) {\n+                        \/\/ Last slot may be partly used.\n+                        final long size = Math.min(layout.byteSize() - offset, 8);\n+                        Class<?> type = SharedUtils.primitiveCarrierForSize(size, false);\n+                        bindings.dup()\n+                                .vmLoad(storage, type)\n+                                .bufferStore(offset, type, (int) size);\n+                        offset += size;\n+                    }\n+                }\n+                case STRUCT_HFA -> {\n+                    assert carrier == MemorySegment.class;\n+                    bindings.allocate(layout);\n+                    List<MemoryLayout> scalarLayouts = TypeClass.scalarLayouts((GroupLayout) layout);\n+                    HfaRegs regs = storageCalculator.hfaAlloc(scalarLayouts);\n+                    final long baseSize = scalarLayouts.get(0).byteSize();\n+                    long offset = 0;\n+                    for (int index = 0; index < regs.first().length; index++) {\n+                        \/\/ Use second if available since first one only contains one 32 bit value.\n+                        VMStorage storage = regs.second()[index] == null ? regs.first()[index] : regs.second()[index];\n+                        \/\/ Floats are 4 Bytes, Double, GP reg and stack slots 8 Bytes (except maybe last slot).\n+                        final long size = (baseSize == 4 &&\n+                                           (storage.type() == StorageType.FLOAT || layout.byteSize() - offset < 8)) ? 4 : 8;\n+                        Class<?> type = SharedUtils.primitiveCarrierForSize(size, storage.type() == StorageType.FLOAT);\n+                        bindings.dup()\n+                                .vmLoad(storage, type)\n+                                .bufferStore(offset, type);\n+                        offset += size;\n+                    }\n+                }\n+                case POINTER -> {\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, false);\n+                    bindings.vmLoad(storage, long.class)\n+                            .boxAddressRaw(Utils.pointeeSize(layout));\n+                }\n+                case INTEGER -> {\n+                    \/\/ We could use carrier != long.class for BoxBindingCalculator, but C always uses 64 bit slots.\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.INTEGER, false);\n+                    bindings.vmLoad(storage, carrier);\n+                }\n+                case FLOAT -> {\n+                    VMStorage storage = storageCalculator.nextStorage(StorageType.FLOAT, carrier == float.class);\n+                    bindings.vmLoad(storage, carrier);\n+                }\n+                default -> throw new UnsupportedOperationException(\"Unhandled class \" + argumentClass);\n+            }\n+            return bindings.build();\n+        }\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ppc64\/CallArranger.java","additions":469,"deletions":0,"binary":false,"changes":469,"status":"added"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.ppc64;\n+\n+import jdk.internal.foreign.abi.ABIDescriptor;\n+import jdk.internal.foreign.abi.Architecture;\n+import jdk.internal.foreign.abi.StubLocations;\n+import jdk.internal.foreign.abi.VMStorage;\n+\n+public class PPC64Architecture implements Architecture {\n+    public static final Architecture INSTANCE = new PPC64Architecture();\n+\n+    \/\/ Needs to be consistent with vmstorage_ppc.hpp.\n+    public static final short REG32_MASK = 0b0000_0000_0000_0001;\n+    public static final short REG64_MASK = 0b0000_0000_0000_0011;\n+\n+    private static final int INTEGER_REG_SIZE = 8;\n+    private static final int FLOAT_REG_SIZE = 8;\n+    private static final int STACK_SLOT_SIZE = 8;\n+\n+    @Override\n+    public boolean isStackType(int cls) {\n+        return cls == StorageType.STACK;\n+    }\n+\n+    @Override\n+    public int typeSize(int cls) {\n+        switch (cls) {\n+            case StorageType.INTEGER: return INTEGER_REG_SIZE;\n+            case StorageType.FLOAT: return FLOAT_REG_SIZE;\n+            \/\/ STACK is deliberately omitted\n+        }\n+\n+        throw new IllegalArgumentException(\"Invalid Storage Class: \" + cls);\n+    }\n+\n+    public interface StorageType {\n+        byte INTEGER = 0;\n+        byte FLOAT = 1;\n+        byte STACK = 2;\n+        byte PLACEHOLDER = 3;\n+    }\n+\n+    public static class Regs { \/\/ break circular dependency\n+        public static final VMStorage r0  = integerRegister(0);\n+        public static final VMStorage r1  = integerRegister(1);\n+        public static final VMStorage r2  = integerRegister(2);\n+        public static final VMStorage r3  = integerRegister(3);\n+        public static final VMStorage r4  = integerRegister(4);\n+        public static final VMStorage r5  = integerRegister(5);\n+        public static final VMStorage r6  = integerRegister(6);\n+        public static final VMStorage r7  = integerRegister(7);\n+        public static final VMStorage r8  = integerRegister(8);\n+        public static final VMStorage r9  = integerRegister(9);\n+        public static final VMStorage r10 = integerRegister(10);\n+        public static final VMStorage r11 = integerRegister(11);\n+        public static final VMStorage r12 = integerRegister(12);\n+        public static final VMStorage r13 = integerRegister(13);\n+        public static final VMStorage r14 = integerRegister(14);\n+        public static final VMStorage r15 = integerRegister(15);\n+        public static final VMStorage r16 = integerRegister(16);\n+        public static final VMStorage r17 = integerRegister(17);\n+        public static final VMStorage r18 = integerRegister(18);\n+        public static final VMStorage r19 = integerRegister(19);\n+        public static final VMStorage r20 = integerRegister(20);\n+        public static final VMStorage r21 = integerRegister(21);\n+        public static final VMStorage r22 = integerRegister(22);\n+        public static final VMStorage r23 = integerRegister(23);\n+        public static final VMStorage r24 = integerRegister(24);\n+        public static final VMStorage r25 = integerRegister(25);\n+        public static final VMStorage r26 = integerRegister(26);\n+        public static final VMStorage r27 = integerRegister(27);\n+        public static final VMStorage r28 = integerRegister(28);\n+        public static final VMStorage r29 = integerRegister(29);\n+        public static final VMStorage r30 = integerRegister(30);\n+        public static final VMStorage r31 = integerRegister(31);\n+\n+        public static final VMStorage f0  = floatRegister(0);\n+        public static final VMStorage f1  = floatRegister(1);\n+        public static final VMStorage f2  = floatRegister(2);\n+        public static final VMStorage f3  = floatRegister(3);\n+        public static final VMStorage f4  = floatRegister(4);\n+        public static final VMStorage f5  = floatRegister(5);\n+        public static final VMStorage f6  = floatRegister(6);\n+        public static final VMStorage f7  = floatRegister(7);\n+        public static final VMStorage f8  = floatRegister(8);\n+        public static final VMStorage f9  = floatRegister(9);\n+        public static final VMStorage f10 = floatRegister(10);\n+        public static final VMStorage f11 = floatRegister(11);\n+        public static final VMStorage f12 = floatRegister(12);\n+        public static final VMStorage f13 = floatRegister(13);\n+        public static final VMStorage f14 = floatRegister(14);\n+        public static final VMStorage f15 = floatRegister(15);\n+        public static final VMStorage f16 = floatRegister(16);\n+        public static final VMStorage f17 = floatRegister(17);\n+        public static final VMStorage f18 = floatRegister(18);\n+        public static final VMStorage f19 = floatRegister(19);\n+        public static final VMStorage f20 = floatRegister(20);\n+        public static final VMStorage f21 = floatRegister(21);\n+        public static final VMStorage f22 = floatRegister(22);\n+        public static final VMStorage f23 = floatRegister(23);\n+        public static final VMStorage f24 = floatRegister(24);\n+        public static final VMStorage f25 = floatRegister(25);\n+        public static final VMStorage f26 = floatRegister(26);\n+        public static final VMStorage f27 = floatRegister(27);\n+        public static final VMStorage f28 = floatRegister(28);\n+        public static final VMStorage f29 = floatRegister(29);\n+        public static final VMStorage f30 = floatRegister(30);\n+        public static final VMStorage f31 = floatRegister(31);\n+    }\n+\n+    private static VMStorage integerRegister(int index) {\n+        return new VMStorage(StorageType.INTEGER, REG64_MASK, index, \"r\" + index);\n+    }\n+\n+    private static VMStorage floatRegister(int index) {\n+        return new VMStorage(StorageType.FLOAT, REG64_MASK, index, \"v\" + index);\n+    }\n+\n+    public static VMStorage stackStorage(short size, int byteOffset) {\n+        return new VMStorage(StorageType.STACK, size, byteOffset);\n+    }\n+\n+    public static ABIDescriptor abiFor(VMStorage[] inputIntRegs,\n+                                       VMStorage[] inputFloatRegs,\n+                                       VMStorage[] outputIntRegs,\n+                                       VMStorage[] outputFloatRegs,\n+                                       VMStorage[] volatileIntRegs,\n+                                       VMStorage[] volatileFloatRegs,\n+                                       int stackAlignment,\n+                                       int shadowSpace,\n+                                       VMStorage scratch1, VMStorage scratch2) {\n+        return new ABIDescriptor(\n+            INSTANCE,\n+            new VMStorage[][] {\n+                inputIntRegs,\n+                inputFloatRegs,\n+            },\n+            new VMStorage[][] {\n+                outputIntRegs,\n+                outputFloatRegs,\n+            },\n+            new VMStorage[][] {\n+                volatileIntRegs,\n+                volatileFloatRegs,\n+            },\n+            stackAlignment,\n+            shadowSpace,\n+            scratch1, scratch2,\n+            StubLocations.TARGET_ADDRESS.storage(StorageType.PLACEHOLDER),\n+            StubLocations.RETURN_BUFFER.storage(StorageType.PLACEHOLDER),\n+            StubLocations.CAPTURED_STATE_BUFFER.storage(StorageType.PLACEHOLDER));\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ppc64\/PPC64Architecture.java","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,136 @@\n+\/*\n+ * Copyright (c) 2022, 2023 Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.ppc64;\n+\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.SequenceLayout;\n+import java.lang.foreign.ValueLayout;\n+import java.util.List;\n+import java.util.ArrayList;\n+\n+public enum TypeClass {\n+    STRUCT_REGISTER,\n+    STRUCT_HFA, \/\/ Homogeneous Float Aggregate\n+    POINTER,\n+    INTEGER,\n+    FLOAT;\n+\n+    private static final int MAX_RETURN_AGGREGATE_REGS_SIZE = 2;\n+\n+    private static TypeClass classifyValueType(ValueLayout type) {\n+        Class<?> carrier = type.carrier();\n+        if (carrier == boolean.class || carrier == byte.class || carrier == char.class ||\n+            carrier == short.class || carrier == int.class || carrier == long.class) {\n+            return INTEGER;\n+        } else if (carrier == float.class || carrier == double.class) {\n+            return FLOAT;\n+        } else if (carrier == MemorySegment.class) {\n+            return POINTER;\n+        } else {\n+            throw new IllegalStateException(\"Cannot get here: \" + carrier.getName());\n+        }\n+    }\n+\n+    static boolean isReturnRegisterAggregate(MemoryLayout type) {\n+        return type.bitSize() <= MAX_RETURN_AGGREGATE_REGS_SIZE * 64;\n+    }\n+\n+    static List<MemoryLayout> scalarLayouts(GroupLayout gl) {\n+        List<MemoryLayout> out = new ArrayList<>();\n+        scalarLayoutsInternal(out, gl);\n+        return out;\n+    }\n+\n+    private static void scalarLayoutsInternal(List<MemoryLayout> out, GroupLayout gl) {\n+        for (MemoryLayout member : gl.memberLayouts()) {\n+            if (member instanceof GroupLayout memberGl) {\n+                scalarLayoutsInternal(out, memberGl);\n+            } else if (member instanceof SequenceLayout memberSl) {\n+                for (long i = 0; i < memberSl.elementCount(); i++) {\n+                    out.add(memberSl.elementLayout());\n+                }\n+            } else {\n+                \/\/ padding or value layouts\n+                out.add(member);\n+            }\n+        }\n+    }\n+\n+    static boolean isHomogeneousFloatAggregate(MemoryLayout type, boolean useABIv2) {\n+        List<MemoryLayout> scalarLayouts = scalarLayouts((GroupLayout) type);\n+\n+        final int numElements = scalarLayouts.size();\n+        if (numElements > (useABIv2 ? 8 : 1) || numElements == 0)\n+            return false;\n+\n+        MemoryLayout baseType = scalarLayouts.get(0);\n+\n+        if (!(baseType instanceof ValueLayout))\n+            return false;\n+\n+        TypeClass baseArgClass = classifyValueType((ValueLayout) baseType);\n+        if (baseArgClass != FLOAT)\n+           return false;\n+\n+        for (MemoryLayout elem : scalarLayouts) {\n+            if (!(elem instanceof ValueLayout))\n+                return false;\n+\n+            TypeClass argClass = classifyValueType((ValueLayout) elem);\n+            if (elem.bitSize() != baseType.bitSize() ||\n+                    elem.bitAlignment() != baseType.bitAlignment() ||\n+                    baseArgClass != argClass) {\n+                return false;\n+            }\n+        }\n+\n+        return true;\n+    }\n+\n+    private static TypeClass classifyStructType(MemoryLayout layout, boolean useABIv2) {\n+        if (isHomogeneousFloatAggregate(layout, useABIv2)) {\n+            return TypeClass.STRUCT_HFA;\n+        }\n+        return TypeClass.STRUCT_REGISTER;\n+    }\n+\n+    static boolean isStructHFAorReturnRegisterAggregate(MemoryLayout layout, boolean useABIv2) {\n+        if (!(layout instanceof GroupLayout) || !useABIv2) return false;\n+        return isHomogeneousFloatAggregate(layout, true) || isReturnRegisterAggregate(layout);\n+    }\n+\n+    public static TypeClass classifyLayout(MemoryLayout type, boolean useABIv2) {\n+        if (type instanceof ValueLayout) {\n+            return classifyValueType((ValueLayout) type);\n+        } else if (type instanceof GroupLayout) {\n+            return classifyStructType(type, useABIv2);\n+        } else {\n+            throw new IllegalArgumentException(\"Unhandled type \" + type);\n+        }\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ppc64\/TypeClass.java","additions":136,"deletions":0,"binary":false,"changes":136,"status":"added"},{"patch":"@@ -0,0 +1,34 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.ppc64.linux;\n+\n+import jdk.internal.foreign.abi.ppc64.CallArranger;\n+\n+\/**\n+ * PPC64 CallArranger specialized for Linux ABI.\n+ *\/\n+public class LinuxPPC64CallArranger extends CallArranger {\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ppc64\/linux\/LinuxPPC64CallArranger.java","additions":34,"deletions":0,"binary":false,"changes":34,"status":"added"},{"patch":"@@ -0,0 +1,58 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.ppc64.linux;\n+\n+import jdk.internal.foreign.abi.AbstractLinker;\n+import jdk.internal.foreign.abi.LinkerOptions;\n+import jdk.internal.foreign.abi.ppc64.CallArranger;\n+\n+import java.lang.foreign.SegmentScope;\n+import java.lang.foreign.FunctionDescriptor;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodType;\n+import java.util.function.Consumer;\n+\n+public final class LinuxPPC64Linker extends AbstractLinker {\n+    private static LinuxPPC64Linker instance;\n+\n+    public static LinuxPPC64Linker getInstance() {\n+        if (instance == null) {\n+            instance = new LinuxPPC64Linker();\n+        }\n+        return instance;\n+    }\n+\n+    @Override\n+    protected MethodHandle arrangeDowncall(MethodType inferredMethodType, FunctionDescriptor function, LinkerOptions options) {\n+        return CallArranger.LINUX.arrangeDowncall(inferredMethodType, function, options);\n+    }\n+\n+    @Override\n+    protected MemorySegment arrangeUpcall(MethodHandle target, MethodType targetType, FunctionDescriptor function, SegmentScope scope) {\n+        return CallArranger.LINUX.arrangeUpcall(target, targetType, function, scope);\n+    }\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ppc64\/linux\/LinuxPPC64Linker.java","additions":58,"deletions":0,"binary":false,"changes":58,"status":"added"},{"patch":"@@ -31,1 +31,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\"\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/compiler\/TestLinkToNativeRBP.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/LibraryLookupTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestClassLoaderFindNative.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestDowncallScope.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestDowncallStack.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestFunctionDescriptor.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,190 @@\n+\/*\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @summary Test passing of Homogeneous Float Aggregates.\n+ * @enablePreview\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n+ * @requires !vm.musl\n+ *\n+ * @run testng\/othervm --enable-native-access=ALL-UNNAMED TestHFA\n+ *\/\n+\n+import java.lang.foreign.*;\n+import java.lang.invoke.MethodHandle;\n+import org.testng.annotations.Test;\n+\n+import static java.lang.foreign.ValueLayout.*;\n+\n+public class TestHFA {\n+\n+    static {\n+        System.loadLibrary(\"TestHFA\");\n+    }\n+\n+    final static Linker abi = Linker.nativeLinker();\n+    final static SymbolLookup lookup = SymbolLookup.loaderLookup();\n+\n+    static final OfFloat FLOAT = JAVA_FLOAT.withBitAlignment(32);\n+\n+    final static GroupLayout S_FFLayout = MemoryLayout.structLayout(\n+        FLOAT.withName(\"p0\"),\n+        FLOAT.withName(\"p1\")\n+    ).withName(\"S_FF\");\n+\n+    final static GroupLayout S_FFFFFFFLayout = MemoryLayout.structLayout(\n+        FLOAT.withName(\"p0\"),\n+        FLOAT.withName(\"p1\"),\n+        FLOAT.withName(\"p2\"),\n+        FLOAT.withName(\"p3\"),\n+        FLOAT.withName(\"p4\"),\n+        FLOAT.withName(\"p5\"),\n+        FLOAT.withName(\"p6\")\n+    ).withName(\"S_FFFF\");\n+\n+    static final FunctionDescriptor fdadd_floats_structs = FunctionDescriptor.of(S_FFFFFFFLayout, S_FFFFFFFLayout, S_FFFFFFFLayout);\n+    static final FunctionDescriptor fdadd_float_to_struct_after_floats = FunctionDescriptor.of(S_FFLayout,\n+        JAVA_FLOAT, JAVA_FLOAT, JAVA_FLOAT, JAVA_FLOAT, JAVA_FLOAT,\n+        JAVA_FLOAT, JAVA_FLOAT, JAVA_FLOAT, JAVA_FLOAT, JAVA_FLOAT,\n+        JAVA_FLOAT, JAVA_FLOAT, S_FFLayout, JAVA_FLOAT);\n+    static final FunctionDescriptor fdadd_float_to_struct_after_structs = FunctionDescriptor.of(S_FFLayout,\n+        S_FFLayout, S_FFLayout, S_FFLayout, S_FFLayout, S_FFLayout, S_FFLayout,\n+        S_FFLayout, JAVA_FLOAT);\n+    static final FunctionDescriptor fdadd_float_to_large_struct_after_structs = FunctionDescriptor.of(S_FFFFFFFLayout,\n+        S_FFLayout, S_FFLayout, S_FFLayout, S_FFLayout, S_FFLayout, S_FFLayout,\n+        S_FFFFFFFLayout, JAVA_FLOAT);\n+\n+    final static MethodHandle mhadd_float_structs = abi.downcallHandle(lookup.find(\"add_float_structs\").orElseThrow(),\n+        fdadd_floats_structs);\n+    final static MethodHandle mhadd_float_to_struct_after_floats = abi.downcallHandle(lookup.find(\"add_float_to_struct_after_floats\").orElseThrow(),\n+        fdadd_float_to_struct_after_floats);\n+    final static MethodHandle mhadd_float_to_struct_after_structs = abi.downcallHandle(lookup.find(\"add_float_to_struct_after_structs\").orElseThrow(),\n+        fdadd_float_to_struct_after_structs);\n+    final static MethodHandle mhadd_float_to_large_struct_after_structs = abi.downcallHandle(lookup.find(\"add_float_to_large_struct_after_structs\").orElseThrow(),\n+        fdadd_float_to_large_struct_after_structs);\n+\n+    @Test\n+    public static void testAddFloatStructs() {\n+        float p0 = 0.0f, p1 = 0.0f, p2 = 0.0f, p3 = 0.0f, p4 = 0.0f, p5 = 0.0f, p6 = 0.0f;\n+        try {\n+            Arena arena = Arena.openConfined();\n+            MemorySegment s = MemorySegment.allocateNative(S_FFFFFFFLayout, arena.scope());\n+            s.set(FLOAT, 0, 1.0f);\n+            s.set(FLOAT, 4, 2.0f);\n+            s.set(FLOAT, 8, 3.0f);\n+            s.set(FLOAT, 12, 4.0f);\n+            s.set(FLOAT, 16, 5.0f);\n+            s.set(FLOAT, 20, 6.0f);\n+            s.set(FLOAT, 24, 7.0f);\n+            s = (MemorySegment)mhadd_float_structs.invokeExact((SegmentAllocator)arena, s, s);\n+            p0 = s.get(FLOAT, 0);\n+            p1 = s.get(FLOAT, 4);\n+            p2 = s.get(FLOAT, 8);\n+            p3 = s.get(FLOAT, 12);\n+            p4 = s.get(FLOAT, 16);\n+            p5 = s.get(FLOAT, 20);\n+            p6 = s.get(FLOAT, 24);\n+            System.out.println(\"S_FFFFFFF(\" + p0 + \";\" + p1 + \";\" + p2 + \";\" + p3 + \";\" + p4 + \";\" + p5 + \";\" + p6 + \")\");\n+        } catch (Throwable t) {\n+            t.printStackTrace();\n+        }\n+        if (p0 != 2.0f || p1 != 4.0f || p2 != 6.0f || p3 != 8.0f || p4 != 10.0f || p5 != 12.0f || p6 != 14.0f)\n+            throw new RuntimeException(\"add_float_structs\");\n+    }\n+\n+    @Test\n+    public static void testAddFloatToStructAfterFloats() {\n+        float p0 = 0.0f, p1 = 0.0f;\n+        try {\n+            Arena arena = Arena.openConfined();\n+            MemorySegment s = MemorySegment.allocateNative(S_FFLayout, arena.scope());\n+            s.set(FLOAT, 0, 1.0f);\n+            s.set(FLOAT, 4, 1.0f);\n+            s = (MemorySegment)mhadd_float_to_struct_after_floats.invokeExact((SegmentAllocator)arena,\n+                1.0f, 2.0f, 3.0f, 4.0f, 5.0f,\n+                6.0f, 7.0f, 8.0f, 9.0f, 10.0f,\n+                11.0f, 12.0f, s, 1.0f);\n+            p0 = s.get(FLOAT, 0);\n+            p1 = s.get(FLOAT, 4);\n+            System.out.println(\"S_FF(\" + p0 + \";\" + p1 + \")\");\n+        } catch (Throwable t) {\n+            t.printStackTrace();\n+        }\n+        if (p0 != 2.0f || p1 != 1.0f) throw new RuntimeException(\"add_float_to_struct_after_floats error\");\n+    }\n+\n+    @Test\n+    public static void testAddFloatToStructAfterStructs() {\n+        float p0 = 0.0f, p1 = 0.0f;\n+        try {\n+            Arena arena = Arena.openConfined();\n+            MemorySegment s = MemorySegment.allocateNative(S_FFLayout, arena.scope());\n+            s.set(FLOAT, 0, 1.0f);\n+            s.set(FLOAT, 4, 1.0f);\n+            s = (MemorySegment)mhadd_float_to_struct_after_structs.invokeExact((SegmentAllocator)arena,\n+                 s, s, s, s, s, s,\n+                 s, 1.0f);\n+            p0 = s.get(FLOAT, 0);\n+            p1 = s.get(FLOAT, 4);\n+            System.out.println(\"S_FF(\" + p0 + \";\" + p1 + \")\");\n+        } catch (Throwable t) {\n+            t.printStackTrace();\n+        }\n+        if (p0 != 2.0f || p1 != 1.0f) throw new RuntimeException(\"add_float_to_struct_after_structs error\");\n+    }\n+\n+    @Test\n+    public static void testAddFloatToLargeStructAfterStructs() {\n+        float p0 = 0.0f, p1 = 0.0f, p2 = 0.0f, p3 = 0.0f, p4 = 0.0f, p5 = 0.0f, p6 = 0.0f;\n+        try {\n+            Arena arena = Arena.openConfined();\n+            MemorySegment s = MemorySegment.allocateNative(S_FFFFFFFLayout, arena.scope());\n+            s.set(FLOAT, 0, 1.0f);\n+            s.set(FLOAT, 4, 2.0f);\n+            s.set(FLOAT, 8, 3.0f);\n+            s.set(FLOAT, 12, 4.0f);\n+            s.set(FLOAT, 16, 5.0f);\n+            s.set(FLOAT, 20, 6.0f);\n+            s.set(FLOAT, 24, 7.0f);\n+            s = (MemorySegment)mhadd_float_to_large_struct_after_structs.invokeExact((SegmentAllocator)arena,\n+                 s, s, s, s, s, s,\n+                 s, 1.0f);\n+            p0 = s.get(FLOAT, 0);\n+            p1 = s.get(FLOAT, 4);\n+            p2 = s.get(FLOAT, 8);\n+            p3 = s.get(FLOAT, 12);\n+            p4 = s.get(FLOAT, 16);\n+            p5 = s.get(FLOAT, 20);\n+            p6 = s.get(FLOAT, 24);\n+            System.out.println(\"S_FFFFFFF(\" + p0 + \";\" + p1 + \";\" + p2 + \";\" + p3 + \";\" + p4 + \";\" + p5 + \";\" + p6 + \")\");\n+        } catch (Throwable t) {\n+            t.printStackTrace();\n+        }\n+        if (p0 != 2.0f || p1 != 2.0f || p2 != 3.0f || p3 != 4.0f || p4 != 5.0f || p5 != 6.0f || p6 != 7.0f)\n+            throw new RuntimeException(\"add_float_to_large_struct_after_structs error\");\n+    }\n+}\n","filename":"test\/jdk\/java\/foreign\/TestHFA.java","additions":190,"deletions":0,"binary":false,"changes":190,"status":"added"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestHeapAlignment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestIllegalLink.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -49,1 +49,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -62,1 +62,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -75,1 +75,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -88,1 +88,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -100,1 +100,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -112,1 +112,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -124,1 +124,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -136,1 +136,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -149,1 +149,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -162,1 +162,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -175,1 +175,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -188,1 +188,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -201,1 +201,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -214,1 +214,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -227,1 +227,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -240,1 +240,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -253,1 +253,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -266,1 +266,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -279,1 +279,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -293,1 +293,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestMatrix.java","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestNULLAddress.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestNative.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestSegments.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestStringEncoding.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallAsync.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallException.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallHighArity.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallScope.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallStack.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestUpcallStructScope.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/TestVarArgs.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/capturecallstate\/TestCaptureCallState.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/dontrelease\/TestDontRelease.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/enablenativeaccess\/TestEnableNativeAccess.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/enablenativeaccess\/TestEnableNativeAccessDynamic.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/handles\/Driver.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"shared.h\"\n+\n+struct S_FFFFFFF { float p0, p1, p2, p3, p4, p5, p6; };\n+\n+EXPORT struct S_FFFFFFF add_float_structs(struct S_FFFFFFF p0,\n+  struct S_FFFFFFF p1){\n+  p0.p0 += p1.p0;\n+  p0.p1 += p1.p1;\n+  p0.p2 += p1.p2;\n+  p0.p3 += p1.p3;\n+  p0.p4 += p1.p4;\n+  p0.p5 += p1.p5;\n+  p0.p6 += p1.p6;\n+  return p0;\n+}\n+\n+\/\/ Corner case on PPC64le: Pass struct S_FF partially in FP register and on stack.\n+\/\/ Pass additional float on stack.\n+EXPORT struct S_FF add_float_to_struct_after_floats(\n+  float f1, float f2, float f3, float f4, float f5,\n+  float f6, float f7, float f8, float f9, float f10,\n+  float f11, float f12, struct S_FF s, float f) {\n+  s.p0 += f;\n+  return s;\n+}\n+\n+\/\/ Corner case on PPC64le: Pass struct S_FF partially in FP register and in GP register.\n+\/\/ Pass additional float in GP register.\n+EXPORT struct S_FF add_float_to_struct_after_structs(\n+  struct S_FF s1, struct S_FF s2, struct S_FF s3, struct S_FF s4, struct S_FF s5, struct S_FF s6,\n+  struct S_FF s, float f) {\n+  s.p0 += f;\n+  return s;\n+}\n+\n+\/\/ Corner case on PPC64le: Pass struct S_FF partially in FP register and in GP register and on stack.\n+EXPORT struct S_FFFFFFF add_float_to_large_struct_after_structs(\n+  struct S_FF s1, struct S_FF s2, struct S_FF s3, struct S_FF s4, struct S_FF s5, struct S_FF s6,\n+  struct S_FFFFFFF s, float f) {\n+  s.p0 += f;\n+  return s;\n+}\n","filename":"test\/jdk\/java\/foreign\/libTestHFA.c","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/loaderLookup\/TestLoaderLookup.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/loaderLookup\/TestLoaderLookupJNI.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/nested\/TestNested.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/normalize\/TestNormalize.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/passheapsegment\/TestPassHeapSegment.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -45,1 +45,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\")\n@@ -64,1 +64,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\")\n","filename":"test\/jdk\/java\/foreign\/stackwalk\/TestAsyncStackWalk.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n@@ -45,1 +45,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\")\n@@ -64,1 +64,1 @@\n- * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\")\n+ * @requires (((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\")\n","filename":"test\/jdk\/java\/foreign\/stackwalk\/TestStackWalk.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"riscv64\"\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\" | os.arch == \"ppc64le\" | os.arch == \"riscv64\"\n","filename":"test\/jdk\/java\/foreign\/upcalldeopt\/TestUpcallDeopt.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}