{"files":[{"patch":"@@ -3501,0 +3501,24 @@\n+\/\/ Move Aligned 256bit Vector\n+void Assembler::vmovdqa(XMMRegister dst, Address src) {\n+  assert(UseAVX > 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::vmovdqa(Address dst, XMMRegister src) {\n+  assert(UseAVX > 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.reset_is_clear_context();\n+  \/\/ swap src<->dst for encoding\n+  assert(src != xnoreg, \"sanity\");\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x7F);\n+  emit_operand(src, dst, 0);\n+}\n+\n@@ -3763,0 +3787,21 @@\n+\/\/ Move Aligned 512bit Vector\n+void Assembler::evmovdqaq(XMMRegister dst, Address src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdqaq(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdqaq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1758,0 +1758,4 @@\n+  \/\/ Move Aligned 256bit Vector\n+  void vmovdqa(XMMRegister dst, Address src);\n+  void vmovdqa(Address dst, XMMRegister src);\n+\n@@ -1791,0 +1795,4 @@\n+  \/\/ Move Aligned 512bit Vector\n+  void evmovdqaq(XMMRegister dst, Address src, int vector_len);\n+  void evmovdqaq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2699,0 +2699,54 @@\n+void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqu(Address dst, XMMRegister src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqu(XMMRegister dst, Address src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqa(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    vmovdqa(dst, as_Address(src));\n+  }\n+  else {\n+    lea(rscratch, src);\n+    vmovdqa(dst, Address(rscratch, 0));\n+  }\n+}\n+\n+void MacroAssembler::vmovdqa(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (vector_len == AVX_512bit) {\n+    evmovdqaq(dst, src, AVX_512bit, rscratch);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqa(dst, src, rscratch);\n+  } else {\n+    movdqa(dst, src, rscratch);\n+  }\n+}\n+\n@@ -2823,0 +2877,23 @@\n+void MacroAssembler::evmovdqaq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evmovdqaq(dst, mask, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evmovdqaq(dst, mask, Address(rscratch, 0), merge, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evmovdqaq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evmovdqaq(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evmovdqaq(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1350,0 +1350,8 @@\n+  void vmovdqu(XMMRegister dst, XMMRegister    src, int vector_len);\n+  void vmovdqu(XMMRegister dst, Address        src, int vector_len);\n+  void vmovdqu(Address     dst, XMMRegister    src, int vector_len);\n+\n+  \/\/ AVX Aligned forms\n+  using Assembler::vmovdqa;\n+  void vmovdqa(XMMRegister dst, AddressLiteral src,                 Register rscratch = noreg);\n+  void vmovdqa(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1406,0 +1414,1 @@\n+  void evmovdqaq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1415,0 +1424,1 @@\n+  void evmovdqaq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2024, 2025, Intel Corporation. All rights reserved.\n@@ -31,1 +31,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t MODULUS_P256[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t MODULUS_P256[] = {\n@@ -37,2 +37,2 @@\n-static address modulus_p256() {\n-  return (address)MODULUS_P256;\n+static address modulus_p256(int index = 0) {\n+  return (address)&MODULUS_P256[index];\n@@ -41,1 +41,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t P256_MASK52[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t P256_MASK52[] = {\n@@ -51,1 +51,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t SHIFT1R[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t SHIFT1R[] = {\n@@ -61,1 +61,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t SHIFT1L[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t SHIFT1L[] = {\n@@ -71,0 +71,8 @@\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t MASKL5[] = {\n+  0xFFFFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFFFFULL,\n+  0xFFFFFFFFFFFFFFFFULL, 0x0000000000000000ULL,\n+};\n+static address mask_limb5() {\n+  return (address)MASKL5;\n+}\n+\n@@ -116,1 +124,0 @@\n- *          if (i == 4) break;\n@@ -127,7 +134,4 @@\n- *   \/\/ Last Carry round: Combine high\/low partial sums Acc1<high_bits> + Acc1 + Acc2\n- *   carry = Acc1 >> 52\n- *   Acc1 = Acc1 shift one q element >>\n- *   Acc1  = mask52(Acc1)\n- *   Acc2  += carry\n- *   Acc1 = Acc1 + Acc2\n- *   output to rLimbs\n+ *\n+ * At this point the result in Acc1 can overflow by 1 Modulus and needs carry\n+ * propagation. Subtract one modulus, carry-propagate both results and select\n+ * (constant-time) the positive number of the two\n@@ -148,1 +152,1 @@\n-  XMMRegister carry = xmm13;\n+  XMMRegister Carry = xmm13;\n@@ -151,9 +155,13 @@\n-  XMMRegister modulus = xmm20;\n-  XMMRegister shift1L = xmm21;\n-  XMMRegister shift1R = xmm22;\n-  XMMRegister mask52  = xmm23;\n-  KRegister limb0    = k1;\n-  KRegister allLimbs = k2;\n-\n-  __ mov64(t0, 0x1);\n-  __ kmovql(limb0, t0);\n+  XMMRegister modulus = xmm5;\n+  XMMRegister shift1L = xmm6;\n+  XMMRegister shift1R = xmm7;\n+  XMMRegister Mask52  = xmm8;\n+  KRegister allLimbs = k1;\n+  KRegister limb0    = k2;\n+  KRegister masks[] = {limb0, k3, k4, k5};\n+\n+  for (int i=0; i<4; i++) {\n+    __ mov64(t0, 1ULL<<i);\n+    __ kmovql(masks[i], t0);\n+  }\n+\n@@ -162,3 +170,3 @@\n-  __ evmovdquq(shift1L, allLimbs, ExternalAddress(shift_1L()), false, Assembler::AVX_512bit, rscratch);\n-  __ evmovdquq(shift1R, allLimbs, ExternalAddress(shift_1R()), false, Assembler::AVX_512bit, rscratch);\n-  __ evmovdquq(mask52, allLimbs, ExternalAddress(p256_mask52()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(shift1L, allLimbs, ExternalAddress(shift_1L()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(shift1R, allLimbs, ExternalAddress(shift_1R()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(Mask52, allLimbs, ExternalAddress(p256_mask52()), false, Assembler::AVX_512bit, rscratch);\n@@ -167,1 +175,1 @@\n-  __ evmovdquq(modulus, allLimbs, ExternalAddress(modulus_p256()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(modulus, allLimbs, ExternalAddress(modulus_p256()), false, Assembler::AVX_512bit, rscratch);\n@@ -199,2 +207,0 @@\n-      if (i == 4) break;\n-\n@@ -204,1 +210,1 @@\n-      __ evpsrlq(carry, limb0, Acc1, 52, true, Assembler::AVX_512bit);\n+      __ evpsrlq(Carry, limb0, Acc1, 52, true, Assembler::AVX_512bit);\n@@ -207,1 +213,1 @@\n-      __ evpaddq(Acc2, limb0, carry, Acc2, true, Assembler::AVX_512bit);\n+      __ evpaddq(Acc2, limb0, Carry, Acc2, true, Assembler::AVX_512bit);\n@@ -216,3 +222,4 @@\n-  \/\/ Last Carry round: Combine high\/low partial sums Acc1<high_bits> + Acc1 + Acc2\n-  \/\/ carry = Acc1 >> 52\n-  __ evpsrlq(carry, allLimbs, Acc1, 52, true, Assembler::AVX_512bit);\n+  \/\/ At this point the result in Acc1 needs carry propagation\n+  \/\/ It also can overflow by 1 Modulus. Subtract one modulus\n+  \/\/ then do carry propagation simultaneously on both results\n+  \/\/ Carry out from the last limb becomes the mask to select the correct result\n@@ -220,2 +227,3 @@\n-  \/\/ Acc1 = Acc1 shift one q element >>\n-  __ evpermq(Acc1, allLimbs, shift1R, Acc1, false, Assembler::AVX_512bit);\n+  XMMRegister Acc1L = A;\n+  XMMRegister Acc2L = B;\n+  __ vpsubq(Acc2, Acc1, modulus, Assembler::AVX_512bit);\n@@ -223,2 +231,5 @@\n-  \/\/ Acc1  = mask52(Acc1)\n-  __ evpandq(Acc1, Acc1, mask52, Assembler::AVX_512bit); \/\/ Clear top 12 bits\n+  \/\/ digit 0 (Output to Acc1L & Acc2L)\n+  __ evpsraq(Carry, limb0, Acc2, 52, false, Assembler::AVX_256bit);\n+  __ evpandq(Acc2L, limb0, Acc2, Mask52, false, Assembler::AVX_256bit);\n+  __ evpermq(Acc2, allLimbs, shift1R, Acc2, false, Assembler::AVX_512bit);\n+  __ vpaddq(Acc2, Acc2, Carry, Assembler::AVX_256bit);\n@@ -226,2 +237,23 @@\n-  \/\/ Acc2 += carry\n-  __ evpaddq(Acc2, allLimbs, carry, Acc2, true, Assembler::AVX_512bit);\n+  __ evpsraq(Carry, limb0, Acc1, 52, false, Assembler::AVX_256bit);\n+  __ evpandq(Acc1L, limb0, Acc1, Mask52, false, Assembler::AVX_256bit);\n+  __ evpermq(Acc1, allLimbs, shift1R, Acc1, false, Assembler::AVX_512bit);\n+  __ vpaddq(Acc1, Acc1, Carry, Assembler::AVX_256bit);\n+\n+  KRegister limb = limb0;\n+  for (int i = 1; i<4; i++) {\n+    __ evpsraq(Carry, masks[i-1], Acc2, 52, false, Assembler::AVX_256bit);\n+    if (i == 1 || i == 3) {\n+      __ vpalignr(Carry, Carry, Carry, 8, Assembler::AVX_256bit);\n+    } else {\n+      __ vpermq(Carry, Carry, 0b10010011, Assembler::AVX_256bit);\n+    }\n+    __ vpaddq(Acc2, Acc2, Carry, Assembler::AVX_256bit);\n+\n+    __ evpsraq(Carry, masks[i-1], Acc1, 52, false, Assembler::AVX_256bit);\n+    if (i == 1 || i == 3) {\n+      __ vpalignr(Carry, Carry, Carry, 8, Assembler::AVX_256bit);\n+    } else {\n+      __ vpermq(Carry, Carry, 0b10010011, Assembler::AVX_256bit); \/\/0b-2-1-0-3\n+    }\n+    __ vpaddq(Acc1, Acc1, Carry, Assembler::AVX_256bit);\n+  }\n@@ -229,2 +261,11 @@\n-  \/\/ Acc1 = Acc1 + Acc2\n-  __ vpaddq(Acc1, Acc1, Acc2, Assembler::AVX_512bit);\n+  \/\/ Mask\n+  __ evpsraq(Carry, Acc2, 64, Assembler::AVX_256bit);\n+  __ vpermq(Carry, Carry, 0b11111111, Assembler::AVX_256bit); \/\/0b-3-3-3-3\n+  __ evpandq(Acc1, Acc1, Mask52, Assembler::AVX_256bit);\n+  __ evpandq(Acc2, Acc2, Mask52, Assembler::AVX_256bit);\n+\n+  \/\/ Acc2 = (Acc1 & Mask) | (Acc2 & !Mask)\n+  __ vpandn(Acc2L, Carry, Acc2L, Assembler::AVX_256bit);\n+  __ vpternlogq(Acc2L, 0xF8, Carry, Acc1L, Assembler::AVX_256bit); \/\/ A | B&C orAandBC\n+  __ vpandn(Acc2, Carry, Acc2, Assembler::AVX_256bit);\n+  __ vpternlogq(Acc2, 0xF8, Carry, Acc1, Assembler::AVX_256bit);\n@@ -233,3 +274,240 @@\n-  __ movq(Address(rLimbs, 0), Acc1);\n-  __ evpermq(Acc1, k0, shift1R, Acc1, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rLimbs, 8), k0, Acc1, true, Assembler::AVX_256bit);\n+  __ movq(Address(rLimbs, 0), Acc2L);\n+  __ evmovdquq(Address(rLimbs, 8), Acc2, Assembler::AVX_256bit);\n+\n+  \/\/ Cleanup\n+  \/\/ Zero out zmm0-zmm15, higher registers not used by intrinsic.\n+  __ vzeroall();\n+}\n+\n+\/**\n+ * Unrolled Word-by-Word Montgomery Multiplication\n+ * r = a * b * 2^-260 (mod P)\n+ *\n+ * Use vpmadd52{l,h}uq multiply for upper four limbs and use\n+ * scalar mulq for the lowest limb.\n+ *\n+ * One has to be careful with mulq vs vpmadd52 'crossovers'; mulq high\/low\n+ * is split as 40:64 bits vs 52:52 in the vector version. Shifts are required\n+ * to line up values before addition (see following ascii art)\n+ *\n+ * Pseudocode:\n+ *\n+ *                                                     +--+--+--+--+  +--+\n+ *   M = load(*modulus_p256)                           |m5|m4|m3|m2|  |m1|\n+ *                                                     +--+--+--+--+  +--+\n+ *   A = load(*aLimbs)                                 |a5|a4|a3|a2|  |a1|\n+ *                                                     +--+--+--+--+  +--+\n+ *   Acc1 = 0                                          | 0| 0| 0| 0|  | 0|\n+ *                                                     +--+--+--+--+  +--+\n+ *      ---- for i = 0 to 4\n+ *                                                     +--+--+--+--+  +--+\n+ *          Acc2 = 0                                   | 0| 0| 0| 0|  | 0|\n+ *                                                     +--+--+--+--+  +--+\n+ *          B = replicate(bLimbs[i])                   |bi|bi|bi|bi|  |bi|\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |a5|a4|a3|a2|  |a1|\n+ *          Acc1 += A *  B                            *|bi|bi|bi|bi|  |bi|\n+ *                                               Acc1+=|c5|c4|c3|c2|  |c1|\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |a5|a4|a3|a2|  |a1|\n+ *          Acc2 += A *h B                           *h|bi|bi|bi|bi|  |bi|\n+ *                                               Acc2+=|d5|d4|d3|d2|  |d1|\n+ *                                                     +--+--+--+--+  +--+\n+ *          N = replicate(Acc1[0])                     |n0|n0|n0|n0|  |n0|\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |m5|m4|m3|m2|  |m1|\n+ *          Acc1 += M *  N                            *|n0|n0|n0|n0|  |n0|\n+ *                                               Acc1+=|c5|c4|c3|c2|  |c1| Note: 52 low bits of c1 == 0 due to Montgomery!\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |m5|m4|m3|m2|  |m1|\n+ *          Acc2 += M *h N                           *h|n0|n0|n0|n0|  |n0|\n+ *                                               Acc2+=|d5|d4|d3|d2|  |d1|\n+ *                                                     +--+--+--+--+  +--+\n+ *          \/\/ Combine high\/low partial sums Acc1 + Acc2\n+ *                                                                    +--+\n+ *          carry = Acc1[0] >> 52                                     |c1|\n+ *                                                                    +--+\n+ *          Acc2[0] += carry                                          |d1|\n+ *                                                                    +--+\n+ *                                                     +--+--+--+--+  +--+\n+ *          Acc1 = Acc1 shift one q element>>          | 0|c5|c4|c3|  |c2|\n+ *                                                    +|d5|d4|d3|d2|  |d1|\n+ *          Acc1 = Acc1 + Acc2                   Acc1+=|c5|c4|c3|c2|  |c1|\n+ *                                                     +--+--+--+--+  +--+\n+ *      ---- done\n+ *                                                     +--+--+--+--+  +--+\n+ *   Acc2 = Acc1 - M                                   |d5|d4|d3|d2|  |d1|\n+ *                                                     +--+--+--+--+  +--+\n+ *   Carry propagate Acc2\n+ *   Carry propagate Acc1\n+ *   Mask = sign(Acc2)\n+ *   Result = Mask\n+ * \n+ * Acc1 can overflow by one modulus (hence Acc2); Either Acc1 or Acc2 contain\n+ * the correct result. However, they both need carry propagation (i.e. normalize\n+ * limbs down to 52 bits each).\n+ * \n+ * Carry propagation would require relatively expensive vector lane operations,\n+ * so instead dump to memory and read as scalar registers\n+ * \n+ * Note: the order of reduce-then-propagate vs propagate-then-reduce is different\n+ * in Java\n+ *\/\n+void montgomeryMultiplyAVX2(const Register aLimbs, const Register bLimbs, const Register rLimbs,\n+  const Register tmp_rax, const Register tmp_rdx, const Register tmp1, const Register tmp2,\n+  const Register tmp3, const Register tmp4, const Register tmp5, const Register tmp6,\n+  const Register tmp7, MacroAssembler* _masm) {\n+  Register rscratch = tmp1;\n+\n+  \/\/ Inputs\n+  Register    a = tmp1;\n+  XMMRegister A = xmm0;\n+  XMMRegister B = xmm1;\n+\n+  \/\/ Intermediates\n+  Register    acc1  = tmp2;\n+  XMMRegister Acc1  = xmm3;\n+  Register    acc2  = tmp3;\n+  XMMRegister Acc2  = xmm4;\n+  XMMRegister N     = xmm5;\n+  XMMRegister Carry = xmm6;\n+\n+  \/\/ Constants\n+  Register    modulus   = tmp4;\n+  XMMRegister Modulus   = xmm7;\n+  Register    mask52    = tmp5;\n+  XMMRegister Mask52    = xmm8;\n+  XMMRegister MaskLimb5 = xmm9;\n+  XMMRegister Zero      = xmm10;\n+\n+  __ mov64(mask52, P256_MASK52[0]);\n+  __ movq(Mask52, mask52);\n+  __ vpbroadcastq(Mask52, Mask52, Assembler::AVX_256bit);\n+  __ vmovdqa(MaskLimb5, ExternalAddress(mask_limb5()), Assembler::AVX_256bit, rscratch);\n+  __ vpxorq(Zero, Zero, Zero, Assembler::AVX_256bit);\n+\n+  \/\/ M = load(*modulus_p256)\n+  __ movq(modulus, mask52);\n+  __ vmovdqu(Modulus, ExternalAddress(modulus_p256(1)), Assembler::AVX_256bit, rscratch);\n+\n+  \/\/ A = load(*aLimbs);\n+  __ movq(a, Address(aLimbs, 0));\n+  __ vmovdqu(A, Address(aLimbs, 8)); \/\/Assembler::AVX_256bit\n+\n+  \/\/ Acc1 = 0\n+  __ vpxorq(Acc1, Acc1, Acc1, Assembler::AVX_256bit);\n+  for (int i = 0; i< 5; i++) {\n+      \/\/ Acc2 = 0\n+      __ vpxorq(Acc2, Acc2, Acc2, Assembler::AVX_256bit);\n+\n+      \/\/ B = replicate(bLimbs[i])\n+      __ movq(tmp_rax, Address(bLimbs, i*8)); \/\/(b==rax)\n+      __ vpbroadcastq(B, Address(bLimbs, i*8), Assembler::AVX_256bit);\n+\n+      \/\/ Acc1 += A * B\n+      \/\/ Acc2 += A *h B\n+      __ mulq(a); \/\/ rdx:rax = a*rax\n+      if (i == 0) {\n+        __ movq(acc1, tmp_rax);\n+        __ movq(acc2, tmp_rdx);\n+      } else {\n+        \/\/ Careful with limb size\/carries; from mulq, tmp_rax uses full 64 bits\n+        __ xorq(acc2, acc2);\n+        __ addq(acc1, tmp_rax);\n+        __ adcq(acc2, tmp_rdx);\n+      }\n+      __ vpmadd52luq(Acc1, A, B, Assembler::AVX_256bit);\n+      __ vpmadd52huq(Acc2, A, B, Assembler::AVX_256bit);\n+\n+      \/\/ N = replicate(Acc1[0])\n+      if  (i != 0) {\n+        __ movq(tmp_rax, acc1); \/\/ (n==rax)\n+      }\n+      __ andq(tmp_rax, mask52);\n+      __ movq(N, acc1); \/\/ masking implicit in vpmadd52\n+      __ vpbroadcastq(N, N, Assembler::AVX_256bit);\n+\n+      \/\/ Acc1 += M *  N\n+      __ mulq(modulus); \/\/ rdx:rax = modulus*rax\n+      __ vpmadd52luq(Acc1, Modulus, N, Assembler::AVX_256bit);\n+      __ addq(acc1, tmp_rax); \/\/carry flag set!\n+\n+      \/\/ Acc2 += M *h N\n+      __ adcq(acc2, tmp_rdx);\n+      __ vpmadd52huq(Acc2, Modulus, N, Assembler::AVX_256bit);\n+\n+      \/\/ Combine high\/low partial sums Acc1 + Acc2\n+\n+      \/\/ carry = Acc1[0] >> 52\n+      __ shrq(acc1, 52); \/\/ low 52 of acc1 ignored, is zero, because Montgomery\n+\n+      \/\/ Acc2[0] += carry\n+      __ shlq(acc2, 12);\n+      __ addq(acc2, acc1);\n+\n+      \/\/ Acc1 = Acc1 shift one q element >>\n+      __ movq(acc1, Acc1);\n+      __ vpermq(Acc1, Acc1, 0b11111001, Assembler::AVX_256bit);\n+      __ vpand(Acc1, Acc1, MaskLimb5, Assembler::AVX_256bit);\n+\n+      \/\/ Acc1 = Acc1 + Acc2\n+      __ addq(acc1, acc2);\n+      __ vpaddq(Acc1, Acc1, Acc2, Assembler::AVX_256bit);\n+  }\n+\n+  __ movq(acc2, acc1);\n+  __ subq(acc2, modulus);\n+  __ vpsubq(Acc2, Acc1, Modulus, Assembler::AVX_256bit);\n+  __ vmovdqa(Address(rsp, 0), Acc2); \/\/Assembler::AVX_256bit\n+\n+  \/\/ Carry propagate the subtraction result Acc2 first (since the last carry is \n+  \/\/ used to select result). Careful, following registers overlap:\n+  \/\/ acc1  = tmp2; acc2  = tmp3; mask52 = tmp5\n+  \/\/ Note that Acc2 limbs are signed (i.e. result of a subtract with modulus)\n+  \/\/ i.e. using signed shift is needed for correctness\n+  Register limb[] = {acc2, tmp1, tmp4, tmp_rdx, tmp6};\n+  Register carry = tmp_rax;\n+  for (int i = 0; i<5; i++) {\n+    if (i > 0) {\n+      __ movq(limb[i], Address(rsp, -8+i*8));\n+      __ addq(limb[i], carry);\n+    }\n+    __ movq(carry, limb[i]);\n+    if (i==4) break;\n+    __ sarq(carry, 52);\n+  }\n+  __ sarq(carry, 63);\n+  __ notq(carry); \/\/select\n+  Register select = carry;\n+  carry = tmp7;\n+\n+  \/\/ Now carry propagate the multiply result and (constant-time) select correct\n+  \/\/ output digit\n+  Register digit = acc1;\n+  __ vmovdqa(Address(rsp, 0), Acc1); \/\/Assembler::AVX_256bit\n+\n+  for (int i = 0; i<5; i++) {\n+    if (i>0) {\n+      __ movq(digit, Address(rsp, -8+i*8));\n+      __ addq(digit, carry);\n+    }\n+    __ movq(carry, digit);\n+    __ sarq(carry, 52);\n+\n+    \/\/ long dummyLimbs = maskValue & (a[i] ^ b[i]);\n+    \/\/ a[i] = dummyLimbs ^ a[i];\n+    __ xorq(limb[i], digit);\n+    __ andq(limb[i], select);\n+    __ xorq(digit, limb[i]);\n+\n+    __ andq(digit, mask52);\n+    __ movq(Address(rLimbs, i*8), digit);\n+  }\n+\n+  \/\/ Cleanup\n+  \/\/ Zero out ymm0-ymm15.\n+  __ vzeroall();\n+  __ vpxorq(Acc1, Acc1, Acc1, Assembler::AVX_256bit);\n+  __ vmovdqa(Address(rsp, 0), Acc1); \/\/Assembler::AVX_256bit\n@@ -244,7 +522,52 @@\n-  \/\/ Register Map\n-  const Register aLimbs  = c_rarg0; \/\/ rdi | rcx\n-  const Register bLimbs  = c_rarg1; \/\/ rsi | rdx\n-  const Register rLimbs  = c_rarg2; \/\/ rdx | r8\n-  const Register tmp     = r9;\n-\n-  montgomeryMultiply(aLimbs, bLimbs, rLimbs, tmp, _masm);\n+  if (EnableX86ECoreOpts && UseAVX > 1) {\n+    __ push(r12);\n+    __ push(r13);\n+    __ push(r14);\n+    #ifdef _WIN64\n+    __ push(rsi);\n+    __ push(rdi);\n+    #endif\n+    __ push(rbp);\n+    __ movq(rbp, rsp);\n+  __ andq(rsp, -32);\n+  __ subptr(rsp, 32);\n+\n+    \/\/ Register Map\n+    const Register aLimbs  = c_rarg0; \/\/ c_rarg0: rdi | rcx\n+    const Register bLimbs  = rsi;     \/\/ c_rarg1: rsi | rdx\n+    const Register rLimbs  = r8;      \/\/ c_rarg2: rdx | r8\n+    const Register tmp1    = r9;\n+    const Register tmp2    = r10;\n+    const Register tmp3    = r11;\n+    const Register tmp4    = r12;\n+    const Register tmp5    = r13;\n+    const Register tmp6    = r14;\n+    #ifdef _WIN64\n+    const Register tmp7    = rdi;\n+    __ movq(bLimbs, c_rarg1); \/\/ free-up rdx\n+    #else\n+    const Register tmp7    = rcx;\n+    __ movq(rLimbs, c_rarg2); \/\/ free-up rdx\n+    #endif\n+\n+    montgomeryMultiplyAVX2(aLimbs, bLimbs, rLimbs, rax, rdx,\n+                           tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, _masm);\n+\n+    __ movq(rsp, rbp);\n+    __ pop(rbp);\n+    #ifdef _WIN64\n+    __ pop(rdi);\n+    __ pop(rsi);\n+    #endif\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n+  } else {\n+    \/\/ Register Map\n+    const Register aLimbs  = c_rarg0; \/\/ rdi | rcx\n+    const Register bLimbs  = c_rarg1; \/\/ rsi | rdx\n+    const Register rLimbs  = c_rarg2; \/\/ rdx | r8\n+    const Register tmp     = r9;\n+\n+    montgomeryMultiply(aLimbs, bLimbs, rLimbs, tmp, _masm);\n+  }\n@@ -261,5 +584,19 @@\n-void assign_avx(XMMRegister A, Address aAddr, XMMRegister B, Address bAddr, KRegister select, int vector_len, MacroAssembler* _masm) {\n-  __ evmovdquq(A, aAddr, vector_len);\n-  __ evmovdquq(B, bAddr, vector_len);\n-  __ evmovdquq(A, select, B, true, vector_len);\n-  __ evmovdquq(aAddr, A, vector_len);\n+void assign_avx(Register aBase, Register bBase, int offset, XMMRegister select, XMMRegister tmp, XMMRegister aTmp, int vector_len, MacroAssembler* _masm) {\n+  if (vector_len == Assembler::AVX_512bit && UseAVX < 3) {\n+    assign_avx(aBase, bBase, offset,      select, tmp, aTmp, Assembler::AVX_256bit, _masm);\n+    assign_avx(aBase, bBase, offset + 32, select, tmp, aTmp, Assembler::AVX_256bit, _masm);\n+    return;\n+  }\n+\n+  Address aAddr = Address(aBase, offset);\n+  Address bAddr = Address(bBase, offset);\n+\n+  \/\/ Original java:\n+  \/\/ long dummyLimbs = maskValue & (a[i] ^ b[i]);\n+  \/\/ a[i] = dummyLimbs ^ a[i];\n+  __ vmovdqu(tmp, aAddr, vector_len);\n+  __ vmovdqu(aTmp, tmp, vector_len);\n+  __ vpxor(tmp, tmp, bAddr, vector_len);\n+  __ vpand(tmp, tmp, select, vector_len);\n+  __ vpxor(tmp, tmp, aTmp, vector_len);\n+  __ vmovdqu(aAddr, tmp, vector_len);\n@@ -268,1 +605,1 @@\n-void assign_scalar(Address aAddr, Address bAddr, Register select, Register tmp, MacroAssembler* _masm) {\n+void assign_scalar(Register aBase, Register bBase, int offset, Register select, Register tmp, MacroAssembler* _masm) {\n@@ -273,0 +610,3 @@\n+  Address aAddr = Address(aBase, offset);\n+  Address bAddr = Address(bBase, offset);\n+\n@@ -309,0 +649,1 @@\n+  XMMRegister select = xmm2;\n@@ -311,1 +652,0 @@\n-  KRegister select = k1;\n@@ -315,1 +655,6 @@\n-  __ kmovql(select, set);\n+  if (UseAVX > 2) {\n+    __ evpbroadcastq(select, set, Assembler::AVX_512bit);\n+  } else {\n+    __ movq(select, set);\n+    __ vpbroadcastq(select, select, Assembler::AVX_256bit);\n+  }\n@@ -335,1 +680,1 @@\n-  assign_scalar(Address(aLimbs, 0), Address(bLimbs, 0), set, tmp, _masm);\n+  assign_scalar(aLimbs, bLimbs, 0, set, tmp, _masm);\n@@ -344,2 +689,2 @@\n-  assign_scalar(Address(aLimbs, 0), Address(bLimbs, 0), set, tmp, _masm);\n-  assign_avx(A, Address(aLimbs, 8), B, Address(bLimbs, 8), select, Assembler::AVX_256bit, _masm);\n+  assign_scalar(aLimbs, bLimbs, 0, set, tmp, _masm);\n+  assign_avx   (aLimbs, bLimbs, 8, select, A, B, Assembler::AVX_256bit, _masm);\n@@ -349,2 +694,2 @@\n-  assign_avx(A, Address(aLimbs, 0),  B, Address(bLimbs, 0),  select, Assembler::AVX_128bit, _masm);\n-  assign_avx(A, Address(aLimbs, 16), B, Address(bLimbs, 16), select, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  0, select, A, B, Assembler::AVX_128bit, _masm);\n+  assign_avx(aLimbs, bLimbs, 16, select, A, B, Assembler::AVX_512bit, _masm);\n@@ -354,3 +699,3 @@\n-  assign_avx(A, Address(aLimbs, 0),  B, Address(bLimbs, 0),  select, Assembler::AVX_128bit, _masm);\n-  assign_avx(A, Address(aLimbs, 16), B, Address(bLimbs, 16), select, Assembler::AVX_256bit, _masm);\n-  assign_avx(A, Address(aLimbs, 48), B, Address(bLimbs, 48), select, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,   0, select, A, B, Assembler::AVX_128bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  16, select, A, B, Assembler::AVX_256bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  48, select, A, B, Assembler::AVX_512bit, _masm);\n@@ -360,2 +705,2 @@\n-  assign_avx(A, Address(aLimbs, 0),  B, Address(bLimbs, 0),  select, Assembler::AVX_512bit, _masm);\n-  assign_avx(A, Address(aLimbs, 64), B, Address(bLimbs, 64), select, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,   0, select, A, B, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  64, select, A, B, Assembler::AVX_512bit, _masm);\n@@ -365,4 +710,4 @@\n-  assign_scalar(Address(aLimbs, 0), Address(bLimbs, 0), set, tmp, _masm);\n-  assign_avx(A, Address(aLimbs, 8),  B, Address(bLimbs, 8),  select, Assembler::AVX_128bit, _masm);\n-  assign_avx(A, Address(aLimbs, 24), B, Address(bLimbs, 24), select, Assembler::AVX_512bit, _masm);\n-  assign_avx(A, Address(aLimbs, 88), B, Address(bLimbs, 88), select, Assembler::AVX_512bit, _masm);\n+  assign_scalar(aLimbs, bLimbs,  0, set, tmp, _masm);\n+  assign_avx   (aLimbs, bLimbs,  8, select, A, B, Assembler::AVX_128bit, _masm);\n+  assign_avx   (aLimbs, bLimbs, 24, select, A, B, Assembler::AVX_512bit, _masm);\n+  assign_avx   (aLimbs, bLimbs, 88, select, A, B, Assembler::AVX_512bit, _masm);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_poly_mont.cpp","additions":421,"deletions":76,"binary":false,"changes":497,"status":"modified"},{"patch":"@@ -1406,1 +1406,1 @@\n-  if (supports_avx512ifma() && supports_avx512vlbw()) {\n+  if ((supports_avx512ifma() && supports_avx512vlbw()) || supports_avxifma()) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -535,1 +535,1 @@\n-  do_name(intPolyMult_name, \"multImpl\")                                                                                     \\\n+  do_name(intPolyMult_name, \"mult\")                                                                                     \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025 Oracle and\/or its affiliates. All rights reserved.\n@@ -162,6 +162,0 @@\n-    protected void mult(long[] a, long[] b, long[] r) {\n-        multImpl(a, b, r);\n-        reducePositive(r);\n-    }\n-\n-    @ForceInline\n@@ -169,1 +163,1 @@\n-    private void multImpl(long[] a, long[] b, long[] r) {\n+    protected void mult(long[] a, long[] b, long[] r) {\n@@ -401,0 +395,1 @@\n+        \/\/ Final carry propagate\n@@ -402,10 +397,36 @@\n-        c6 += d2 + dd1;\n-        c7 += d3 + dd2;\n-        c8 += d4 + dd3;\n-        c9 = dd4;\n-\n-        r[0] = c5;\n-        r[1] = c6;\n-        r[2] = c7;\n-        r[3] = c8;\n-        r[4] = c9;\n+        c6 += d2 + dd1 + (c5 >>> BITS_PER_LIMB);\n+        c7 += d3 + dd2 + (c6 >>> BITS_PER_LIMB);\n+        c8 += d4 + dd3 + (c7 >>> BITS_PER_LIMB);\n+        c9 = dd4 + (c8 >>> BITS_PER_LIMB);\n+\n+        c5 &= LIMB_MASK;\n+        c6 &= LIMB_MASK;\n+        c7 &= LIMB_MASK;\n+        c8 &= LIMB_MASK;\n+\n+        \/\/ At this point, the result {c5, c6, c7, c8, c9} could overflow by\n+        \/\/ one modulus. Subtract one modulus (with carry propagation), into\n+        \/\/ {c0, c1, c2, c3, c4}. Note that in this calculation, limbs are\n+        \/\/ signed\n+        c0 = c5 - modulus[0];\n+        c1 = c6 - modulus[1] + (c0 >> BITS_PER_LIMB);\n+        c0 &= LIMB_MASK;\n+        c2 = c7 - modulus[2] + (c1 >> BITS_PER_LIMB);\n+        c1 &= LIMB_MASK;\n+        c3 = c8 - modulus[3] + (c2 >> BITS_PER_LIMB);\n+        c2 &= LIMB_MASK;\n+        c4 = c9 - modulus[4] + (c3 >> BITS_PER_LIMB);\n+        c3 &= LIMB_MASK;\n+\n+        \/\/ We now must select a result that is in range of [0,modulus). i.e.\n+        \/\/ either {c0-4} or {c5-9}. `If statements` are not allowed here, so use\n+        \/\/ boolean algebra (i.e. a mask). If statement would had been `if {c0-4}\n+        \/\/ is negative`, which essentially means 'what is the sign bit of c4'\n+        \/\/ A signed shift is the easiest way to broadcast c4-sign-bit into a\n+        \/\/ mask\n+        long mask = c4 >> BITS_PER_LIMB;\n+        r[0] = ((c5 & mask) | (c0 & ~mask));\n+        r[1] = ((c6 & mask) | (c1 & ~mask));\n+        r[2] = ((c7 & mask) | (c2 & ~mask));\n+        r[3] = ((c8 & mask) | (c3 & ~mask));\n+        r[4] = ((c9 & mask) | (c4 & ~mask));\n","filename":"src\/java.base\/share\/classes\/sun\/security\/util\/math\/intpoly\/MontgomeryIntegerPolynomialP256.java","additions":39,"deletions":18,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2024, 2025, Intel Corporation. All rights reserved.\n@@ -26,2 +26,0 @@\n-import sun.security.util.math.IntegerMontgomeryFieldModuloP;\n-import sun.security.util.math.ImmutableIntegerModuloP;\n@@ -29,0 +27,1 @@\n+import sun.security.util.math.*;\n@@ -63,0 +62,13 @@\n+    private static void checkOverflow(ImmutableIntegerModuloP testValue, long seed) {\n+        long limbs[] = testValue.getLimbs();\n+        BigInteger mod = MontgomeryIntegerPolynomialP256.ONE.MODULUS;\n+        BigInteger ref = BigInteger.ZERO;\n+        for (int i = 0; i<limbs.length; i++) {\n+            ref.add(BigInteger.valueOf(limbs[i]).shiftLeft(i*52));\n+        }\n+        if (ref.compareTo(mod)!=-1) {\n+            throw new RuntimeException(\"SEED[\" + seed + \"]: \" +\n+            ref.toString(16) + \" != \" + mod.toString(16));\n+        }\n+    }\n+\n@@ -65,2 +77,4 @@\n-        if (!reference.equals(testValue.asBigInteger())) {\n-            throw new RuntimeException(\"SEED: \" + seed);\n+        BigInteger test = testValue.asBigInteger();\n+        if (!reference.equals(test)) {\n+            throw new RuntimeException(\"SEED[\" + seed + \"]: \" +\n+                reference.toString(16) + \" != \" + test.toString(16));\n@@ -80,0 +94,4 @@\n+        BigInteger bRef = (new BigInteger(P.bitLength(), rnd)).mod(P);\n+        SmallValue two = montField.getSmallValue(2);\n+        SmallValue three = montField.getSmallValue(3);\n+        SmallValue four = montField.getSmallValue(4);\n@@ -85,0 +103,6 @@\n+        checkOverflow(a, seed);\n+\n+        ImmutableIntegerModuloP b = montField.getElement(bRef);\n+        bRef = bRef.multiply(r).mod(P);\n+        check(bRef, b, seed);\n+        checkOverflow(b, seed);\n@@ -90,0 +114,1 @@\n+            checkOverflow(a, seed);\n@@ -97,0 +122,33 @@\n+\n+        if (rnd.nextBoolean()) {\n+            aRef = aRef.subtract(bRef).mod(P);\n+            a = a.mutable().setDifference(b).fixed();\n+            check(aRef, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            aRef = aRef.multiply(bRef).multiply(rInv).mod(P);\n+            a = a.multiply(b);\n+            check(aRef, a, seed);\n+            checkOverflow(a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            aRef = aRef.multiply(BigInteger.valueOf(2)).mod(P);\n+            a = a.mutable().setProduct(two).fixed();\n+            check(aRef, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            aRef = aRef.multiply(BigInteger.valueOf(3)).mod(P);\n+            a = a.mutable().setProduct(three).fixed();\n+            check(aRef, a, seed);\n+            checkOverflow(a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            aRef = aRef.multiply(BigInteger.valueOf(4)).mod(P);\n+            a = a.mutable().setProduct(four).fixed();\n+            check(aRef, a, seed);\n+            checkOverflow(a, seed);\n+        }\n","filename":"test\/jdk\/com\/sun\/security\/util\/math\/intpoly\/MontgomeryPolynomialFuzzTest.java","additions":63,"deletions":5,"binary":false,"changes":68,"status":"modified"}]}