{"files":[{"patch":"@@ -3501,0 +3501,24 @@\n+\/\/ Move Aligned 256bit Vector\n+void Assembler::vmovdqa(XMMRegister dst, Address src) {\n+  assert(UseAVX > 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src, 0);\n+}\n+\n+void Assembler::vmovdqa(Address dst, XMMRegister src) {\n+  assert(UseAVX > 0, \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_256bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.reset_is_clear_context();\n+  \/\/ swap src<->dst for encoding\n+  assert(src != xnoreg, \"sanity\");\n+  vex_prefix(dst, 0, src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x7F);\n+  emit_operand(src, dst, 0);\n+}\n+\n@@ -3763,0 +3787,21 @@\n+\/\/ Move Aligned 512bit Vector\n+void Assembler::evmovdqaq(XMMRegister dst, Address src, int vector_len) {\n+  \/\/ Unmasked instruction\n+  evmovdqaq(dst, k0, src, \/*merge*\/ false, vector_len);\n+}\n+\n+void Assembler::evmovdqaq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ false, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_FVM, \/* input_size_in_bits *\/ EVEX_NObit);\n+  attributes.set_embedded_opmask_register_specifier(mask);\n+  attributes.set_is_evex_instruction();\n+  if (merge) {\n+    attributes.reset_is_clear_context();\n+  }\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x6F);\n+  emit_operand(dst, src, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1758,0 +1758,4 @@\n+  \/\/ Move Aligned 256bit Vector\n+  void vmovdqa(XMMRegister dst, Address src);\n+  void vmovdqa(Address dst, XMMRegister src);\n+\n@@ -1791,0 +1795,4 @@\n+  \/\/ Move Aligned 512bit Vector\n+  void evmovdqaq(XMMRegister dst, Address src, int vector_len);\n+  void evmovdqaq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len);\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2699,0 +2699,54 @@\n+void MacroAssembler::vmovdqu(XMMRegister dst, XMMRegister src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqu(Address dst, XMMRegister src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqu(XMMRegister dst, Address src, int vector_len) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqu(dst, src);\n+  } else {\n+    movdqu(dst, src);\n+  }\n+}\n+\n+void MacroAssembler::vmovdqa(XMMRegister dst, AddressLiteral src, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    vmovdqa(dst, as_Address(src));\n+  }\n+  else {\n+    lea(rscratch, src);\n+    vmovdqa(dst, Address(rscratch, 0));\n+  }\n+}\n+\n+void MacroAssembler::vmovdqa(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (vector_len == AVX_512bit) {\n+    evmovdqaq(dst, src, AVX_512bit, rscratch);\n+  } else if (vector_len == AVX_256bit) {\n+    vmovdqa(dst, src, rscratch);\n+  } else {\n+    movdqa(dst, src, rscratch);\n+  }\n+}\n+\n@@ -2823,0 +2877,23 @@\n+void MacroAssembler::evmovdqaq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evmovdqaq(dst, mask, as_Address(src), merge, vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evmovdqaq(dst, mask, Address(rscratch, 0), merge, vector_len);\n+  }\n+}\n+\n+void MacroAssembler::evmovdqaq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    Assembler::evmovdqaq(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::evmovdqaq(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1350,0 +1350,8 @@\n+  void vmovdqu(XMMRegister dst, XMMRegister    src, int vector_len);\n+  void vmovdqu(XMMRegister dst, Address        src, int vector_len);\n+  void vmovdqu(Address     dst, XMMRegister    src, int vector_len);\n+\n+  \/\/ AVX Aligned forms\n+  using Assembler::vmovdqa;\n+  void vmovdqa(XMMRegister dst, AddressLiteral src,                 Register rscratch = noreg);\n+  void vmovdqa(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1406,0 +1414,1 @@\n+  void evmovdqaq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = noreg);\n@@ -1415,0 +1424,1 @@\n+  void evmovdqaq(XMMRegister dst, KRegister mask, AddressLiteral src, bool merge, int vector_len, Register rscratch = noreg);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2024, 2025, Intel Corporation. All rights reserved.\n@@ -31,1 +31,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t MODULUS_P256[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t MODULUS_P256[] = {\n@@ -37,2 +37,2 @@\n-static address modulus_p256() {\n-  return (address)MODULUS_P256;\n+static address modulus_p256(int index = 0) {\n+  return (address)&MODULUS_P256[index];\n@@ -41,1 +41,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t P256_MASK52[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t P256_MASK52[] = {\n@@ -51,1 +51,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t SHIFT1R[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t SHIFT1R[] = {\n@@ -61,1 +61,1 @@\n-ATTRIBUTE_ALIGNED(64) uint64_t SHIFT1L[] = {\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t SHIFT1L[] = {\n@@ -71,0 +71,8 @@\n+ATTRIBUTE_ALIGNED(64) constexpr uint64_t MASKL5[] = {\n+  0xFFFFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFFFFULL,\n+  0xFFFFFFFFFFFFFFFFULL, 0x0000000000000000ULL,\n+};\n+static address mask_limb5() {\n+  return (address)MASKL5;\n+}\n+\n@@ -97,0 +105,2 @@\n+ *                                                     | 0| 0| 0|a5|a4|a3|a2|a1|\n+ *          Acc1 += A *  B                            *|bi|bi|bi|bi|bi|bi|bi|bi|\n@@ -98,2 +108,0 @@\n- *                                                    *| 0| 0| 0|a5|a4|a3|a2|a1|\n- *          Acc1 += A *  B                             |bi|bi|bi|bi|bi|bi|bi|bi|\n@@ -101,3 +109,3 @@\n- *                                               Acc2+=| 0| 0| 0| 0| 0| 0| 0| 0|\n- *                                                   *h| 0| 0| 0|a5|a4|a3|a2|a1|\n- *          Acc2 += A *h B                             |bi|bi|bi|bi|bi|bi|bi|bi|\n+ *                                                     | 0| 0| 0|a5|a4|a3|a2|a1|\n+ *          Acc2 += A *h B                           *h|bi|bi|bi|bi|bi|bi|bi|bi|\n+ *                                               Acc2+=| 0| 0| 0| d5|d4|d3|d2|d1|\n@@ -108,3 +116,3 @@\n- *                                               Acc1+=| 0| 0| 0|c5|c4|c3|c2|c1|\n- *                                                    *| 0| 0| 0|m5|m4|m3|m2|m1|\n- *          Acc1 += M *  N                             |n0|n0|n0|n0|n0|n0|n0|n0| Note: 52 low bits of Acc1[0] == 0 due to Montgomery!\n+ *                                                     | 0| 0| 0|m5|m4|m3|m2|m1|\n+ *          Acc1 += M *  N                            *|n0|n0|n0|n0|n0|n0|n0|n0|\n+ *                                               Acc1+=| 0| 0| 0|c5|c4|c3|c2|c1| Note: 52 low bits of c1 == 0 due to Montgomery!\n@@ -112,0 +120,2 @@\n+ *                                                     | 0| 0| 0|m5|m4|m3|m2|m1|\n+ *          Acc2 += M *h N                           *h|n0|n0|n0|n0|n0|n0|n0|n0|\n@@ -113,2 +123,0 @@\n- *                                                   *h| 0| 0| 0|m5|m4|m3|m2|m1|\n- *          Acc2 += M *h N                             |n0|n0|n0|n0|n0|n0|n0|n0|\n@@ -116,1 +124,0 @@\n- *          if (i == 4) break;\n@@ -127,7 +134,29 @@\n- *   \/\/ Last Carry round: Combine high\/low partial sums Acc1<high_bits> + Acc1 + Acc2\n- *   carry = Acc1 >> 52\n- *   Acc1 = Acc1 shift one q element >>\n- *   Acc1  = mask52(Acc1)\n- *   Acc2  += carry\n- *   Acc1 = Acc1 + Acc2\n- *   output to rLimbs\n+ *\n+ * At this point the result in Acc1 can overflow by 1 Modulus and needs carry\n+ * propagation. Subtract one modulus, carry-propagate both results and select\n+ * (constant-time) the positive number of the two\n+ *\n+ * Carry = Acc1[0] >> 52\n+ * Acc1L = Acc1[0] & mask52\n+ * Acc1  = Acc1 shift one q element>>\n+ * Acc1 += Carry\n+ *\n+ * Carry = Acc2[0] >> 52\n+ * Acc2L = Acc2[0] & mask52\n+ * Acc2  = Acc2 shift one q element>>\n+ * Acc2 += Carry\n+ *\n+ * for col:=1 to 4\n+ *   Carry = Acc2[col]>>52\n+ *   Carry = Carry shift one q element<<\n+ *   Acc2 += Carry\n+ *\n+ *   Carry = Acc1[col]>>52\n+ *   Carry = Carry shift one q element<<\n+ *   Acc1 += Carry\n+ * done\n+ *\n+ * Acc1 &= mask52\n+ * Acc2 &= mask52\n+ * Mask = sign(Acc2)\n+ * Result = select(Mask ? Acc1 or Acc2)\n@@ -148,1 +177,1 @@\n-  XMMRegister carry = xmm13;\n+  XMMRegister Carry = xmm13;\n@@ -151,9 +180,13 @@\n-  XMMRegister modulus = xmm20;\n-  XMMRegister shift1L = xmm21;\n-  XMMRegister shift1R = xmm22;\n-  XMMRegister mask52  = xmm23;\n-  KRegister limb0    = k1;\n-  KRegister allLimbs = k2;\n-\n-  __ mov64(t0, 0x1);\n-  __ kmovql(limb0, t0);\n+  XMMRegister modulus = xmm5;\n+  XMMRegister shift1L = xmm6;\n+  XMMRegister shift1R = xmm7;\n+  XMMRegister Mask52  = xmm8;\n+  KRegister allLimbs = k1;\n+  KRegister limb0    = k2;\n+  KRegister masks[] = {limb0, k3, k4, k5};\n+\n+  for (int i=0; i<4; i++) {\n+    __ mov64(t0, 1ULL<<i);\n+    __ kmovql(masks[i], t0);\n+  }\n+\n@@ -162,3 +195,3 @@\n-  __ evmovdquq(shift1L, allLimbs, ExternalAddress(shift_1L()), false, Assembler::AVX_512bit, rscratch);\n-  __ evmovdquq(shift1R, allLimbs, ExternalAddress(shift_1R()), false, Assembler::AVX_512bit, rscratch);\n-  __ evmovdquq(mask52, allLimbs, ExternalAddress(p256_mask52()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(shift1L, allLimbs, ExternalAddress(shift_1L()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(shift1R, allLimbs, ExternalAddress(shift_1R()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(Mask52, allLimbs, ExternalAddress(p256_mask52()), false, Assembler::AVX_512bit, rscratch);\n@@ -167,1 +200,1 @@\n-  __ evmovdquq(modulus, allLimbs, ExternalAddress(modulus_p256()), false, Assembler::AVX_512bit, rscratch);\n+  __ evmovdqaq(modulus, allLimbs, ExternalAddress(modulus_p256()), false, Assembler::AVX_512bit, rscratch);\n@@ -199,2 +232,0 @@\n-      if (i == 4) break;\n-\n@@ -204,1 +235,1 @@\n-      __ evpsrlq(carry, limb0, Acc1, 52, true, Assembler::AVX_512bit);\n+      __ evpsrlq(Carry, limb0, Acc1, 52, true, Assembler::AVX_512bit);\n@@ -207,1 +238,1 @@\n-      __ evpaddq(Acc2, limb0, carry, Acc2, true, Assembler::AVX_512bit);\n+      __ evpaddq(Acc2, limb0, Carry, Acc2, true, Assembler::AVX_512bit);\n@@ -216,5 +247,19 @@\n-  \/\/ Last Carry round: Combine high\/low partial sums Acc1<high_bits> + Acc1 + Acc2\n-  \/\/ carry = Acc1 >> 52\n-  __ evpsrlq(carry, allLimbs, Acc1, 52, true, Assembler::AVX_512bit);\n-\n-  \/\/ Acc1 = Acc1 shift one q element >>\n+  \/\/ At this point the result is in Acc1, but needs to be normailized to 52bit\n+  \/\/ limbs (i.e. needs carry propagation) It can also overflow by 1 modulus.\n+  \/\/ Subtract one modulus from Acc1 into Acc2 then carry propagate both\n+  \/\/ simultaneously\n+\n+  XMMRegister Acc1L = A;\n+  XMMRegister Acc2L = B;\n+  __ vpsubq(Acc2, Acc1, modulus, Assembler::AVX_512bit);\n+\n+  \/\/ digit 0 carry out\n+  \/\/ Also split Acc1 and Acc2 into two 256-bit vectors each {Acc1, Acc1L} and\n+  \/\/ {Acc2, Acc2L} to use 256bit operations\n+  __ evpsraq(Carry, limb0, Acc2, 52, false, Assembler::AVX_256bit);\n+  __ evpandq(Acc2L, limb0, Acc2, Mask52, false, Assembler::AVX_256bit);\n+  __ evpermq(Acc2, allLimbs, shift1R, Acc2, false, Assembler::AVX_512bit);\n+  __ vpaddq(Acc2, Acc2, Carry, Assembler::AVX_256bit);\n+\n+  __ evpsraq(Carry, limb0, Acc1, 52, false, Assembler::AVX_256bit);\n+  __ evpandq(Acc1L, limb0, Acc1, Mask52, false, Assembler::AVX_256bit);\n@@ -222,0 +267,35 @@\n+  __ vpaddq(Acc1, Acc1, Carry, Assembler::AVX_256bit);\n+\n+ \/* remaining digits carry\n+  * Note1: Carry register contains just the carry for the particular\n+  * column (zero-mask the rest) and gets progressively shifted left\n+  * Note2: 'element shift' with vpermq is more expensive, so using vpalignr when\n+  * possible. vpalignr shifts 'right' not left, so place the carry appropiately\n+  *                               +--+--+--+--+    +--+--+--+--+         +--+--+\n+  * vpalignr(X, X, X, 8):         |x4|x3|x2|x1| >> |x2|x1|x2|x1|         |x1|x2|\n+  *                               +--+--+--+--+    +--+--+--+--+ >>      +--+--+\n+  *                                     |          +--+--+--+--+   +--+--+\n+  *                                     |          |x4|x3|x4|x3|   |x3|x4|\n+  *                                     |          +--+--+--+--+   +--+--+\n+  *                                     |                                vv\n+  *                                     |                          +--+--+--+--+\n+  *  (x3 and x1 is effectively shifted  +------------------------> |x3|x4|x1|x2|\n+  *   left; zero-mask everything but one column of interest)       +--+--+--+--+\n+  *\/\n+  for (int i = 1; i<4; i++) {\n+    __ evpsraq(Carry, masks[i-1], Acc2, 52, false, Assembler::AVX_256bit);\n+    if (i == 1 || i == 3) {\n+      __ vpalignr(Carry, Carry, Carry, 8, Assembler::AVX_256bit);\n+    } else {\n+      __ vpermq(Carry, Carry, 0b10010011, Assembler::AVX_256bit);\n+    }\n+    __ vpaddq(Acc2, Acc2, Carry, Assembler::AVX_256bit);\n+\n+    __ evpsraq(Carry, masks[i-1], Acc1, 52, false, Assembler::AVX_256bit);\n+    if (i == 1 || i == 3) {\n+      __ vpalignr(Carry, Carry, Carry, 8, Assembler::AVX_256bit);\n+    } else {\n+      __ vpermq(Carry, Carry, 0b10010011, Assembler::AVX_256bit); \/\/0b-2-1-0-3\n+    }\n+    __ vpaddq(Acc1, Acc1, Carry, Assembler::AVX_256bit);\n+  }\n@@ -223,5 +303,7 @@\n-  \/\/ Acc1  = mask52(Acc1)\n-  __ evpandq(Acc1, Acc1, mask52, Assembler::AVX_512bit); \/\/ Clear top 12 bits\n-\n-  \/\/ Acc2 += carry\n-  __ evpaddq(Acc2, allLimbs, carry, Acc2, true, Assembler::AVX_512bit);\n+  \/\/ Iff Acc2 is negative, then Acc1 contains the result.\n+  \/\/ if Acc2 is negative, upper 12 bits will be set; arithmetic shift by 64 bits\n+  \/\/ generates a mask from Acc2 sign bit\n+  __ evpsraq(Carry, Acc2, 64, Assembler::AVX_256bit);\n+  __ vpermq(Carry, Carry, 0b11111111, Assembler::AVX_256bit); \/\/0b-3-3-3-3\n+  __ evpandq(Acc1, Acc1, Mask52, Assembler::AVX_256bit);\n+  __ evpandq(Acc2, Acc2, Mask52, Assembler::AVX_256bit);\n@@ -229,2 +311,5 @@\n-  \/\/ Acc1 = Acc1 + Acc2\n-  __ vpaddq(Acc1, Acc1, Acc2, Assembler::AVX_512bit);\n+  \/\/ Acc2 = (Acc1 & Mask) | (Acc2 & !Mask)\n+  __ vpandn(Acc2L, Carry, Acc2L, Assembler::AVX_256bit);\n+  __ vpternlogq(Acc2L, 0xF8, Carry, Acc1L, Assembler::AVX_256bit); \/\/ A | B&C orAandBC\n+  __ vpandn(Acc2, Carry, Acc2, Assembler::AVX_256bit);\n+  __ vpternlogq(Acc2, 0xF8, Carry, Acc1, Assembler::AVX_256bit);\n@@ -233,3 +318,240 @@\n-  __ movq(Address(rLimbs, 0), Acc1);\n-  __ evpermq(Acc1, k0, shift1R, Acc1, true, Assembler::AVX_512bit);\n-  __ evmovdquq(Address(rLimbs, 8), k0, Acc1, true, Assembler::AVX_256bit);\n+  __ movq(Address(rLimbs, 0), Acc2L);\n+  __ evmovdquq(Address(rLimbs, 8), Acc2, Assembler::AVX_256bit);\n+\n+  \/\/ Cleanup\n+  \/\/ Zero out zmm0-zmm15, higher registers not used by intrinsic.\n+  __ vzeroall();\n+}\n+\n+\/**\n+ * Unrolled Word-by-Word Montgomery Multiplication\n+ * r = a * b * 2^-260 (mod P)\n+ *\n+ * Use vpmadd52{l,h}uq multiply for upper four limbs and use\n+ * scalar mulq for the lowest limb.\n+ *\n+ * One has to be careful with mulq vs vpmadd52 'crossovers'; mulq high\/low\n+ * is split as 40:64 bits vs 52:52 in the vector version. Shifts are required\n+ * to line up values before addition (see following ascii art)\n+ *\n+ * Pseudocode:\n+ *\n+ *                                                     +--+--+--+--+  +--+\n+ *   M = load(*modulus_p256)                           |m5|m4|m3|m2|  |m1|\n+ *                                                     +--+--+--+--+  +--+\n+ *   A = load(*aLimbs)                                 |a5|a4|a3|a2|  |a1|\n+ *                                                     +--+--+--+--+  +--+\n+ *   Acc1 = 0                                          | 0| 0| 0| 0|  | 0|\n+ *                                                     +--+--+--+--+  +--+\n+ *      ---- for i = 0 to 4\n+ *                                                     +--+--+--+--+  +--+\n+ *          Acc2 = 0                                   | 0| 0| 0| 0|  | 0|\n+ *                                                     +--+--+--+--+  +--+\n+ *          B = replicate(bLimbs[i])                   |bi|bi|bi|bi|  |bi|\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |a5|a4|a3|a2|  |a1|\n+ *          Acc1 += A *  B                            *|bi|bi|bi|bi|  |bi|\n+ *                                               Acc1+=|c5|c4|c3|c2|  |c1|\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |a5|a4|a3|a2|  |a1|\n+ *          Acc2 += A *h B                           *h|bi|bi|bi|bi|  |bi|\n+ *                                               Acc2+=|d5|d4|d3|d2|  |d1|\n+ *                                                     +--+--+--+--+  +--+\n+ *          N = replicate(Acc1[0])                     |n0|n0|n0|n0|  |n0|\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |m5|m4|m3|m2|  |m1|\n+ *          Acc1 += M *  N                            *|n0|n0|n0|n0|  |n0|\n+ *                                               Acc1+=|c5|c4|c3|c2|  |c1| Note: 52 low bits of c1 == 0 due to Montgomery!\n+ *                                                     +--+--+--+--+  +--+\n+ *                                                     |m5|m4|m3|m2|  |m1|\n+ *          Acc2 += M *h N                           *h|n0|n0|n0|n0|  |n0|\n+ *                                               Acc2+=|d5|d4|d3|d2|  |d1|\n+ *                                                     +--+--+--+--+  +--+\n+ *          \/\/ Combine high\/low partial sums Acc1 + Acc2\n+ *                                                                    +--+\n+ *          carry = Acc1[0] >> 52                                     |c1|\n+ *                                                                    +--+\n+ *          Acc2[0] += carry                                          |d1|\n+ *                                                                    +--+\n+ *                                                     +--+--+--+--+  +--+\n+ *          Acc1 = Acc1 shift one q element>>          | 0|c5|c4|c3|  |c2|\n+ *                                                    +|d5|d4|d3|d2|  |d1|\n+ *          Acc1 = Acc1 + Acc2                   Acc1+=|c5|c4|c3|c2|  |c1|\n+ *                                                     +--+--+--+--+  +--+\n+ *      ---- done\n+ *                                                     +--+--+--+--+  +--+\n+ *   Acc2 = Acc1 - M                                   |d5|d4|d3|d2|  |d1|\n+ *                                                     +--+--+--+--+  +--+\n+ *   Carry propagate Acc2\n+ *   Carry propagate Acc1\n+ *   Mask = sign(Acc2)\n+ *   Result = select(Mask ? Acc1 or Acc2)\n+ *\n+ * Acc1 can overflow by one modulus (hence Acc2); Either Acc1 or Acc2 contain\n+ * the correct result. However, they both need carry propagation (i.e. normalize\n+ * limbs down to 52 bits each).\n+ *\n+ * Carry propagation would require relatively expensive vector lane operations,\n+ * so instead dump to memory and read as scalar registers\n+ *\n+ * Note: the order of reduce-then-propagate vs propagate-then-reduce is different\n+ * in Java\n+ *\/\n+void montgomeryMultiplyAVX2(const Register aLimbs, const Register bLimbs, const Register rLimbs,\n+  const Register tmp_rax, const Register tmp_rdx, const Register tmp1, const Register tmp2,\n+  const Register tmp3, const Register tmp4, const Register tmp5, const Register tmp6,\n+  const Register tmp7, MacroAssembler* _masm) {\n+  Register rscratch = tmp1;\n+\n+  \/\/ Inputs\n+  Register    a = tmp1;\n+  XMMRegister A = xmm0;\n+  XMMRegister B = xmm1;\n+\n+  \/\/ Intermediates\n+  Register    acc1  = tmp2;\n+  XMMRegister Acc1  = xmm3;\n+  Register    acc2  = tmp3;\n+  XMMRegister Acc2  = xmm4;\n+  XMMRegister N     = xmm5;\n+  XMMRegister Carry = xmm6;\n+\n+  \/\/ Constants\n+  Register    modulus   = tmp4;\n+  XMMRegister Modulus   = xmm7;\n+  Register    mask52    = tmp5;\n+  XMMRegister Mask52    = xmm8;\n+  XMMRegister MaskLimb5 = xmm9;\n+  XMMRegister Zero      = xmm10;\n+\n+  __ mov64(mask52, P256_MASK52[0]);\n+  __ movq(Mask52, mask52);\n+  __ vpbroadcastq(Mask52, Mask52, Assembler::AVX_256bit);\n+  __ vmovdqa(MaskLimb5, ExternalAddress(mask_limb5()), Assembler::AVX_256bit, rscratch);\n+  __ vpxor(Zero, Zero, Zero, Assembler::AVX_256bit);\n+\n+  \/\/ M = load(*modulus_p256)\n+  __ movq(modulus, mask52);\n+  __ vmovdqu(Modulus, ExternalAddress(modulus_p256(1)), Assembler::AVX_256bit, rscratch);\n+\n+  \/\/ A = load(*aLimbs);\n+  __ movq(a, Address(aLimbs, 0));\n+  __ vmovdqu(A, Address(aLimbs, 8)); \/\/Assembler::AVX_256bit\n+\n+  \/\/ Acc1 = 0\n+  __ vpxor(Acc1, Acc1, Acc1, Assembler::AVX_256bit);\n+  for (int i = 0; i< 5; i++) {\n+      \/\/ Acc2 = 0\n+      __ vpxor(Acc2, Acc2, Acc2, Assembler::AVX_256bit);\n+\n+      \/\/ B = replicate(bLimbs[i])\n+      __ movq(tmp_rax, Address(bLimbs, i*8)); \/\/(b==rax)\n+      __ vpbroadcastq(B, Address(bLimbs, i*8), Assembler::AVX_256bit);\n+\n+      \/\/ Acc1 += A * B\n+      \/\/ Acc2 += A *h B\n+      __ mulq(a); \/\/ rdx:rax = a*rax\n+      if (i == 0) {\n+        __ movq(acc1, tmp_rax);\n+        __ movq(acc2, tmp_rdx);\n+      } else {\n+        \/\/ Careful with limb size\/carries; from mulq, tmp_rax uses full 64 bits\n+        __ xorq(acc2, acc2);\n+        __ addq(acc1, tmp_rax);\n+        __ adcq(acc2, tmp_rdx);\n+      }\n+      __ vpmadd52luq(Acc1, A, B, Assembler::AVX_256bit);\n+      __ vpmadd52huq(Acc2, A, B, Assembler::AVX_256bit);\n+\n+      \/\/ N = replicate(Acc1[0])\n+      if  (i != 0) {\n+        __ movq(tmp_rax, acc1); \/\/ (n==rax)\n+      }\n+      __ andq(tmp_rax, mask52);\n+      __ movq(N, acc1); \/\/ masking implicit in vpmadd52\n+      __ vpbroadcastq(N, N, Assembler::AVX_256bit);\n+\n+      \/\/ Acc1 += M *  N\n+      __ mulq(modulus); \/\/ rdx:rax = modulus*rax\n+      __ vpmadd52luq(Acc1, Modulus, N, Assembler::AVX_256bit);\n+      __ addq(acc1, tmp_rax); \/\/carry flag set!\n+\n+      \/\/ Acc2 += M *h N\n+      __ adcq(acc2, tmp_rdx);\n+      __ vpmadd52huq(Acc2, Modulus, N, Assembler::AVX_256bit);\n+\n+      \/\/ Combine high\/low partial sums Acc1 + Acc2\n+\n+      \/\/ carry = Acc1[0] >> 52\n+      __ shrq(acc1, 52); \/\/ low 52 of acc1 ignored, is zero, because Montgomery\n+\n+      \/\/ Acc2[0] += carry\n+      __ shlq(acc2, 12);\n+      __ addq(acc2, acc1);\n+\n+      \/\/ Acc1 = Acc1 shift one q element >>\n+      __ movq(acc1, Acc1);\n+      __ vpermq(Acc1, Acc1, 0b11111001, Assembler::AVX_256bit);\n+      __ vpand(Acc1, Acc1, MaskLimb5, Assembler::AVX_256bit);\n+\n+      \/\/ Acc1 = Acc1 + Acc2\n+      __ addq(acc1, acc2);\n+      __ vpaddq(Acc1, Acc1, Acc2, Assembler::AVX_256bit);\n+  }\n+\n+  __ movq(acc2, acc1);\n+  __ subq(acc2, modulus);\n+  __ vpsubq(Acc2, Acc1, Modulus, Assembler::AVX_256bit);\n+  __ vmovdqa(Address(rsp, 0), Acc2); \/\/Assembler::AVX_256bit\n+\n+  \/\/ Carry propagate the subtraction result Acc2 first (since the last carry is\n+  \/\/ used to select result). Careful, following registers overlap:\n+  \/\/ acc1  = tmp2; acc2  = tmp3; mask52 = tmp5\n+  \/\/ Note that Acc2 limbs are signed (i.e. result of a subtract with modulus)\n+  \/\/ i.e. using signed shift is needed for correctness\n+  Register limb[] = {acc2, tmp1, tmp4, tmp_rdx, tmp6};\n+  Register carry = tmp_rax;\n+  for (int i = 0; i<5; i++) {\n+    if (i > 0) {\n+      __ movq(limb[i], Address(rsp, -8+i*8));\n+      __ addq(limb[i], carry);\n+    }\n+    __ movq(carry, limb[i]);\n+    if (i==4) break;\n+    __ sarq(carry, 52);\n+  }\n+  __ sarq(carry, 63);\n+  __ notq(carry); \/\/select\n+  Register select = carry;\n+  carry = tmp7;\n+\n+  \/\/ Now carry propagate the multiply result and (constant-time) select correct\n+  \/\/ output digit\n+  Register digit = acc1;\n+  __ vmovdqa(Address(rsp, 0), Acc1); \/\/Assembler::AVX_256bit\n+\n+  for (int i = 0; i<5; i++) {\n+    if (i>0) {\n+      __ movq(digit, Address(rsp, -8+i*8));\n+      __ addq(digit, carry);\n+    }\n+    __ movq(carry, digit);\n+    __ sarq(carry, 52);\n+\n+    \/\/ long dummyLimbs = maskValue & (a[i] ^ b[i]);\n+    \/\/ a[i] = dummyLimbs ^ a[i];\n+    __ xorq(limb[i], digit);\n+    __ andq(limb[i], select);\n+    __ xorq(digit, limb[i]);\n+\n+    __ andq(digit, mask52);\n+    __ movq(Address(rLimbs, i*8), digit);\n+  }\n+\n+  \/\/ Cleanup\n+  \/\/ Zero out ymm0-ymm15.\n+  __ vzeroall();\n+  __ vpxor(Acc1, Acc1, Acc1, Assembler::AVX_256bit);\n+  __ vmovdqa(Address(rsp, 0), Acc1); \/\/Assembler::AVX_256bit\n@@ -244,7 +566,52 @@\n-  \/\/ Register Map\n-  const Register aLimbs  = c_rarg0; \/\/ rdi | rcx\n-  const Register bLimbs  = c_rarg1; \/\/ rsi | rdx\n-  const Register rLimbs  = c_rarg2; \/\/ rdx | r8\n-  const Register tmp     = r9;\n-\n-  montgomeryMultiply(aLimbs, bLimbs, rLimbs, tmp, _masm);\n+  if (EnableX86ECoreOpts && UseAVX > 1) {\n+    __ push(r12);\n+    __ push(r13);\n+    __ push(r14);\n+    #ifdef _WIN64\n+    __ push(rsi);\n+    __ push(rdi);\n+    #endif\n+    __ push(rbp);\n+    __ movq(rbp, rsp);\n+  __ andq(rsp, -32);\n+  __ subptr(rsp, 32);\n+\n+    \/\/ Register Map\n+    const Register aLimbs  = c_rarg0; \/\/ c_rarg0: rdi | rcx\n+    const Register bLimbs  = rsi;     \/\/ c_rarg1: rsi | rdx\n+    const Register rLimbs  = r8;      \/\/ c_rarg2: rdx | r8\n+    const Register tmp1    = r9;\n+    const Register tmp2    = r10;\n+    const Register tmp3    = r11;\n+    const Register tmp4    = r12;\n+    const Register tmp5    = r13;\n+    const Register tmp6    = r14;\n+    #ifdef _WIN64\n+    const Register tmp7    = rdi;\n+    __ movq(bLimbs, c_rarg1); \/\/ free-up rdx\n+    #else\n+    const Register tmp7    = rcx;\n+    __ movq(rLimbs, c_rarg2); \/\/ free-up rdx\n+    #endif\n+\n+    montgomeryMultiplyAVX2(aLimbs, bLimbs, rLimbs, rax, rdx,\n+                           tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, _masm);\n+\n+    __ movq(rsp, rbp);\n+    __ pop(rbp);\n+    #ifdef _WIN64\n+    __ pop(rdi);\n+    __ pop(rsi);\n+    #endif\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n+  } else {\n+    \/\/ Register Map\n+    const Register aLimbs  = c_rarg0; \/\/ rdi | rcx\n+    const Register bLimbs  = c_rarg1; \/\/ rsi | rdx\n+    const Register rLimbs  = c_rarg2; \/\/ rdx | r8\n+    const Register tmp     = r9;\n+\n+    montgomeryMultiply(aLimbs, bLimbs, rLimbs, tmp, _masm);\n+  }\n@@ -261,5 +628,19 @@\n-void assign_avx(XMMRegister A, Address aAddr, XMMRegister B, Address bAddr, KRegister select, int vector_len, MacroAssembler* _masm) {\n-  __ evmovdquq(A, aAddr, vector_len);\n-  __ evmovdquq(B, bAddr, vector_len);\n-  __ evmovdquq(A, select, B, true, vector_len);\n-  __ evmovdquq(aAddr, A, vector_len);\n+void assign_avx(Register aBase, Register bBase, int offset, XMMRegister select, XMMRegister tmp, XMMRegister aTmp, int vector_len, MacroAssembler* _masm) {\n+  if (vector_len == Assembler::AVX_512bit && UseAVX < 3) {\n+    assign_avx(aBase, bBase, offset,      select, tmp, aTmp, Assembler::AVX_256bit, _masm);\n+    assign_avx(aBase, bBase, offset + 32, select, tmp, aTmp, Assembler::AVX_256bit, _masm);\n+    return;\n+  }\n+\n+  Address aAddr = Address(aBase, offset);\n+  Address bAddr = Address(bBase, offset);\n+\n+  \/\/ Original java:\n+  \/\/ long dummyLimbs = maskValue & (a[i] ^ b[i]);\n+  \/\/ a[i] = dummyLimbs ^ a[i];\n+  __ vmovdqu(tmp, aAddr, vector_len);\n+  __ vmovdqu(aTmp, tmp, vector_len);\n+  __ vpxor(tmp, tmp, bAddr, vector_len);\n+  __ vpand(tmp, tmp, select, vector_len);\n+  __ vpxor(tmp, tmp, aTmp, vector_len);\n+  __ vmovdqu(aAddr, tmp, vector_len);\n@@ -268,1 +649,1 @@\n-void assign_scalar(Address aAddr, Address bAddr, Register select, Register tmp, MacroAssembler* _masm) {\n+void assign_scalar(Register aBase, Register bBase, int offset, Register select, Register tmp, MacroAssembler* _masm) {\n@@ -273,0 +654,3 @@\n+  Address aAddr = Address(aBase, offset);\n+  Address bAddr = Address(bBase, offset);\n+\n@@ -309,0 +693,1 @@\n+  XMMRegister select = xmm2;\n@@ -311,1 +696,0 @@\n-  KRegister select = k1;\n@@ -315,1 +699,6 @@\n-  __ kmovql(select, set);\n+  if (UseAVX > 2) {\n+    __ evpbroadcastq(select, set, Assembler::AVX_512bit);\n+  } else {\n+    __ movq(select, set);\n+    __ vpbroadcastq(select, select, Assembler::AVX_256bit);\n+  }\n@@ -335,1 +724,1 @@\n-  assign_scalar(Address(aLimbs, 0), Address(bLimbs, 0), set, tmp, _masm);\n+  assign_scalar(aLimbs, bLimbs, 0, set, tmp, _masm);\n@@ -344,2 +733,2 @@\n-  assign_scalar(Address(aLimbs, 0), Address(bLimbs, 0), set, tmp, _masm);\n-  assign_avx(A, Address(aLimbs, 8), B, Address(bLimbs, 8), select, Assembler::AVX_256bit, _masm);\n+  assign_scalar(aLimbs, bLimbs, 0, set, tmp, _masm);\n+  assign_avx   (aLimbs, bLimbs, 8, select, A, B, Assembler::AVX_256bit, _masm);\n@@ -349,2 +738,2 @@\n-  assign_avx(A, Address(aLimbs, 0),  B, Address(bLimbs, 0),  select, Assembler::AVX_128bit, _masm);\n-  assign_avx(A, Address(aLimbs, 16), B, Address(bLimbs, 16), select, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  0, select, A, B, Assembler::AVX_128bit, _masm);\n+  assign_avx(aLimbs, bLimbs, 16, select, A, B, Assembler::AVX_512bit, _masm);\n@@ -354,3 +743,3 @@\n-  assign_avx(A, Address(aLimbs, 0),  B, Address(bLimbs, 0),  select, Assembler::AVX_128bit, _masm);\n-  assign_avx(A, Address(aLimbs, 16), B, Address(bLimbs, 16), select, Assembler::AVX_256bit, _masm);\n-  assign_avx(A, Address(aLimbs, 48), B, Address(bLimbs, 48), select, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,   0, select, A, B, Assembler::AVX_128bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  16, select, A, B, Assembler::AVX_256bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  48, select, A, B, Assembler::AVX_512bit, _masm);\n@@ -360,2 +749,2 @@\n-  assign_avx(A, Address(aLimbs, 0),  B, Address(bLimbs, 0),  select, Assembler::AVX_512bit, _masm);\n-  assign_avx(A, Address(aLimbs, 64), B, Address(bLimbs, 64), select, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,   0, select, A, B, Assembler::AVX_512bit, _masm);\n+  assign_avx(aLimbs, bLimbs,  64, select, A, B, Assembler::AVX_512bit, _masm);\n@@ -365,4 +754,4 @@\n-  assign_scalar(Address(aLimbs, 0), Address(bLimbs, 0), set, tmp, _masm);\n-  assign_avx(A, Address(aLimbs, 8),  B, Address(bLimbs, 8),  select, Assembler::AVX_128bit, _masm);\n-  assign_avx(A, Address(aLimbs, 24), B, Address(bLimbs, 24), select, Assembler::AVX_512bit, _masm);\n-  assign_avx(A, Address(aLimbs, 88), B, Address(bLimbs, 88), select, Assembler::AVX_512bit, _masm);\n+  assign_scalar(aLimbs, bLimbs,  0, set, tmp, _masm);\n+  assign_avx   (aLimbs, bLimbs,  8, select, A, B, Assembler::AVX_128bit, _masm);\n+  assign_avx   (aLimbs, bLimbs, 24, select, A, B, Assembler::AVX_512bit, _masm);\n+  assign_avx   (aLimbs, bLimbs, 88, select, A, B, Assembler::AVX_512bit, _masm);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_poly_mont.cpp","additions":476,"deletions":87,"binary":false,"changes":563,"status":"modified"},{"patch":"@@ -1406,1 +1406,1 @@\n-  if (supports_avx512ifma() && supports_avx512vlbw()) {\n+  if ((supports_avx512ifma() && supports_avx512vlbw()) || supports_avxifma()) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -535,1 +535,1 @@\n-  do_name(intPolyMult_name, \"multImpl\")                                                                                     \\\n+  do_name(intPolyMult_name, \"mult\")                                                                                     \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025 Oracle and\/or its affiliates. All rights reserved.\n@@ -162,6 +162,0 @@\n-    protected void mult(long[] a, long[] b, long[] r) {\n-        multImpl(a, b, r);\n-        reducePositive(r);\n-    }\n-\n-    @ForceInline\n@@ -169,1 +163,1 @@\n-    private void multImpl(long[] a, long[] b, long[] r) {\n+    protected void mult(long[] a, long[] b, long[] r) {\n@@ -401,0 +395,1 @@\n+        \/\/ Final carry propagate\n@@ -402,10 +397,35 @@\n-        c6 += d2 + dd1;\n-        c7 += d3 + dd2;\n-        c8 += d4 + dd3;\n-        c9 = dd4;\n-\n-        r[0] = c5;\n-        r[1] = c6;\n-        r[2] = c7;\n-        r[3] = c8;\n-        r[4] = c9;\n+        c6 += d2 + dd1 + (c5 >>> BITS_PER_LIMB);\n+        c7 += d3 + dd2 + (c6 >>> BITS_PER_LIMB);\n+        c8 += d4 + dd3 + (c7 >>> BITS_PER_LIMB);\n+        c9 = dd4 + (c8 >>> BITS_PER_LIMB);\n+\n+        c5 &= LIMB_MASK;\n+        c6 &= LIMB_MASK;\n+        c7 &= LIMB_MASK;\n+        c8 &= LIMB_MASK;\n+\n+        \/\/ At this point, the result {c5, c6, c7, c8, c9} could overflow by\n+        \/\/ one modulus. Subtract one modulus (with carry propagation), into\n+        \/\/ {c0, c1, c2, c3, c4}. Note that in this calculation, limbs are\n+        \/\/ signed\n+        c0 = c5 - modulus[0];\n+        c1 = c6 - modulus[1] + (c0 >> BITS_PER_LIMB);\n+        c0 &= LIMB_MASK;\n+        c2 = c7 - modulus[2] + (c1 >> BITS_PER_LIMB);\n+        c1 &= LIMB_MASK;\n+        c3 = c8 - modulus[3] + (c2 >> BITS_PER_LIMB);\n+        c2 &= LIMB_MASK;\n+        c4 = c9 - modulus[4] + (c3 >> BITS_PER_LIMB);\n+        c3 &= LIMB_MASK;\n+\n+        \/\/ We now must select a result that is in range of [0,modulus). i.e.\n+        \/\/ either {c0-4} or {c5-9}. Iff {c0-4} is negative, then {c5-9} contains\n+        \/\/ the result. (After carry propagation) IF c4 is negative, {c0-4} is\n+        \/\/ negative. Arithmetic shift by 64 bits generates a mask from c4 that\n+        \/\/ can be used to select 'constant time' either {c0-4} or {c5-9}.\n+        long mask = c4 >> 63;\n+        r[0] = ((c5 & mask) | (c0 & ~mask));\n+        r[1] = ((c6 & mask) | (c1 & ~mask));\n+        r[2] = ((c7 & mask) | (c2 & ~mask));\n+        r[3] = ((c8 & mask) | (c3 & ~mask));\n+        r[4] = ((c9 & mask) | (c4 & ~mask));\n","filename":"src\/java.base\/share\/classes\/sun\/security\/util\/math\/intpoly\/MontgomeryIntegerPolynomialP256.java","additions":38,"deletions":18,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Intel Corporation. All rights reserved.\n+ * Copyright (c) 2024, 2025, Intel Corporation. All rights reserved.\n@@ -26,2 +26,0 @@\n-import sun.security.util.math.IntegerMontgomeryFieldModuloP;\n-import sun.security.util.math.ImmutableIntegerModuloP;\n@@ -29,0 +27,1 @@\n+import sun.security.util.math.*;\n@@ -38,1 +37,1 @@\n- * @summary Unit test MontgomeryPolynomialFuzzTest.\n+ * @summary Unit test MontgomeryPolynomialFuzzTest without intrinsic, plain java\n@@ -48,1 +47,1 @@\n- * @summary Unit test MontgomeryPolynomialFuzzTest.\n+ * @summary Unit test MontgomeryPolynomialFuzzTest with intrinsic enabled\n@@ -51,1 +50,2 @@\n-\/\/ This test case is NOT entirely deterministic, it uses a random seed for pseudo-random number generator\n+\/\/ This test case is NOT entirely deterministic, it uses a random seed for\n+\/\/ pseudo-random number generator\n@@ -63,1 +63,1 @@\n-    private static void check(BigInteger reference,\n+    private static void checkOverflow(String opMsg,\n@@ -65,2 +65,23 @@\n-        if (!reference.equals(testValue.asBigInteger())) {\n-            throw new RuntimeException(\"SEED: \" + seed);\n+        long limbs[] = testValue.getLimbs();\n+        BigInteger mod = MontgomeryIntegerPolynomialP256.ONE.MODULUS;\n+        BigInteger ref = BigInteger.ZERO;\n+        for (int i = 0; i<limbs.length; i++) {\n+            ref.add(BigInteger.valueOf(limbs[i]).shiftLeft(i*52));\n+        }\n+        if (ref.compareTo(mod)!=-1) {\n+            String msg = \"Error while \" + opMsg + System.lineSeparator()\n+                + ref.toString(16) + \" != \" + mod.toString(16) + System.lineSeparator()\n+                + \"To reproduce, set SEED to [\" + seed + \"L]: \";\n+            throw new RuntimeException(msg);\n+        }\n+    }\n+\n+    private static void check(String opMsg, BigInteger reference,\n+            ImmutableIntegerModuloP testValue, long seed) {\n+        BigInteger test = testValue.asBigInteger();\n+        if (!reference.equals(test)) {\n+            String msg = \"Error while \" + opMsg + System.lineSeparator()\n+                + reference.toString(16) + \" != \" + test.toString(16)\n+                + System.lineSeparator()+ \"To reproduce, set SEED to [\"\n+                + seed + \"L]: \";\n+            throw new RuntimeException(msg);\n@@ -72,0 +93,2 @@\n+        \/\/ To reproduce an error, fix the value of the seed to the value from\n+        \/\/ the failure\n@@ -80,0 +103,4 @@\n+        BigInteger bRef = (new BigInteger(P.bitLength(), rnd)).mod(P);\n+        SmallValue two = montField.getSmallValue(2);\n+        SmallValue three = montField.getSmallValue(3);\n+        SmallValue four = montField.getSmallValue(4);\n@@ -83,0 +110,1 @@\n+        String msg = \"converting \"+aRef.toString(16) + \" to montgomery domain\";\n@@ -84,1 +112,8 @@\n-        check(aRef, a, seed);\n+        check(msg, aRef, a, seed);\n+        checkOverflow(msg, a, seed);\n+\n+        ImmutableIntegerModuloP b = montField.getElement(bRef);\n+        msg = \"converting \"+aRef.toString(16) + \" to montgomery domain\";\n+        bRef = bRef.multiply(r).mod(P);\n+        check(msg, bRef, b, seed);\n+        checkOverflow(msg, b, seed);\n@@ -87,0 +122,1 @@\n+            msg = \"squaring \"+aRef.toString(16);\n@@ -89,1 +125,2 @@\n-            check(aRef, a, seed);\n+            check(msg, aRef, a, seed);\n+            checkOverflow(msg, a, seed);\n@@ -93,0 +130,1 @@\n+            msg = \"doubling \"+aRef.toString(16);\n@@ -95,1 +133,39 @@\n-            check(aRef, a, seed);\n+            check(msg, aRef, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            msg = \"subtracting \"+bRef.toString(16)+\" from \"+aRef.toString(16);\n+            aRef = aRef.subtract(bRef).mod(P);\n+            a = a.mutable().setDifference(b).fixed();\n+            check(msg, aRef, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            msg = \"multiplying \"+bRef.toString(16)+\" with \"+aRef.toString(16);\n+            aRef = aRef.multiply(bRef).multiply(rInv).mod(P);\n+            a = a.multiply(b);\n+            check(msg, aRef, a, seed);\n+            checkOverflow(msg, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            msg = \"multiplying \"+aRef.toString(16)+\" with constant 2\";\n+            aRef = aRef.multiply(BigInteger.valueOf(2)).mod(P);\n+            a = a.mutable().setProduct(two).fixed();\n+            check(msg, aRef, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            msg = \"multiplying \"+aRef.toString(16)+\" with constant 3\";\n+            aRef = aRef.multiply(BigInteger.valueOf(3)).mod(P);\n+            a = a.mutable().setProduct(three).fixed();\n+            check(msg, aRef, a, seed);\n+            checkOverflow(msg, a, seed);\n+        }\n+\n+        if (rnd.nextBoolean()) {\n+            msg = \"multiplying \"+aRef.toString(16)+\" with constant 4\";\n+            aRef = aRef.multiply(BigInteger.valueOf(4)).mod(P);\n+            a = a.mutable().setProduct(four).fixed();\n+            check(msg, aRef, a, seed);\n+            checkOverflow(msg, a, seed);\n","filename":"test\/jdk\/com\/sun\/security\/util\/math\/intpoly\/MontgomeryPolynomialFuzzTest.java","additions":88,"deletions":12,"binary":false,"changes":100,"status":"modified"}]}