{"files":[{"patch":"@@ -318,0 +318,12 @@\n+    \/**\n+     * Submits the given task to the given executor. If the scheduler is a\n+     * ForkJoinPool then the task is first adapted to a ForkJoinTask.\n+     *\/\n+    private void submit(Executor executor, Runnable task) {\n+        if (executor instanceof ForkJoinPool pool) {\n+            pool.submit(ForkJoinTask.adapt(task));\n+        } else {\n+            executor.execute(task);\n+        }\n+    }\n+\n@@ -338,1 +350,1 @@\n-                        scheduler.execute(runContinuation);\n+                        submit(scheduler, runContinuation);\n@@ -343,1 +355,1 @@\n-                    scheduler.execute(runContinuation);\n+                    submit(scheduler, runContinuation);\n@@ -1539,1 +1551,1 @@\n-}\n\\ No newline at end of file\n+}\n","filename":"src\/java.base\/share\/classes\/java\/lang\/VirtualThread.java","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -563,52 +563,17 @@\n-     * workers to scan for tasks.  Method signalWork and its callers\n-     * try to approximate the unattainable goal of having the right\n-     * number of workers activated for the tasks at hand, but must err\n-     * on the side of too many workers vs too few to avoid stalls:\n-     *\n-     *  * If computations are purely tree structured, it suffices for\n-     *    every worker to activate another when it pushes a task into\n-     *    an empty queue, resulting in O(log(#threads)) steps to full\n-     *    activation. Emptiness must be conservatively approximated,\n-     *    which may result in unnecessary signals.  Also, to reduce\n-     *    resource usages in some cases, at the expense of slower\n-     *    startup in others, activation of an idle thread is preferred\n-     *    over creating a new one, here and elsewhere.\n-     *\n-     *  * At the other extreme, if \"flat\" tasks (those that do not in\n-     *    turn generate others) come in serially from only a single\n-     *    producer, each worker taking a task from a queue should\n-     *    propagate a signal if there are more tasks in that\n-     *    queue. This is equivalent to, but generally faster than,\n-     *    arranging the stealer take multiple tasks, re-pushing one or\n-     *    more on its own queue, and signalling (because its queue is\n-     *    empty), also resulting in logarithmic full activation\n-     *    time. If tasks do not not engage in unbounded loops based on\n-     *    the actions of other workers with unknown dependencies loop,\n-     *    this form of proagation can be limited to one signal per\n-     *    activation (phase change). We distinguish the cases by\n-     *    further signalling only if the task is an InterruptibleTask\n-     *    (see below), which are the only supported forms of task that\n-     *    may do so.\n-     *\n-     * * Because we don't know about usage patterns (or most commonly,\n-     *    mixtures), we use both approaches, which present even more\n-     *    opportunities to over-signal. (Failure to distinguish these\n-     *    cases in terms of submission methods was arguably an early\n-     *    design mistake.)  Note that in either of these contexts,\n-     *    signals may be (and often are) unnecessary because active\n-     *    workers continue scanning after running tasks without the\n-     *    need to be signalled (which is one reason work stealing is\n-     *    often faster than alternatives), so additional workers\n-     *    aren't needed.\n-     *\n-     * * For rapidly branching tasks that require full pool resources,\n-     *   oversignalling is OK, because signalWork will soon have no\n-     *   more workers to create or reactivate. But for others (mainly\n-     *   externally submitted tasks), overprovisioning may cause very\n-     *   noticeable slowdowns due to contention and resource\n-     *   wastage. We reduce impact by deactivating workers when\n-     *   queues don't have accessible tasks, but reactivating and\n-     *   rescanning if other tasks remain.\n-     *\n-     * * Despite these, signal contention and overhead effects still\n-     *   occur during ramp-up and ramp-down of small computations.\n+     * workers to scan for tasks.  SignalWork is invoked in two cases:\n+     * (1) When a task is pushed onto an empty queue, and (2) When a\n+     * worker takes a top-level task from a queue that has additional\n+     * tasks. Together, these suffice in O(log(#threads)) steps to\n+     * fully activate with at least enough workers, and ideally no\n+     * more than required.  This ideal is unobtainable: Callers do not\n+     * know whether another worker will finish its current task and\n+     * poll for others without need of a signal (which is otherwise an\n+     * advantage of work-stealing vs other schemes), and also must\n+     * conservatively estimate the triggering conditions of emptiness\n+     * or non-emptiness; all of which usually cause more activations\n+     * than necessary (see below). (Method signalWork is also used as\n+     * failsafe in case of Thread failures in deregisterWorker, to\n+     * activate or create a new worker to replace them).\n+     *\n+     * Top-Level scheduling\n+     * ====================\n@@ -621,2 +586,2 @@\n-     * permutation on each invocation.  The pseudorandom generator\n-     * need not have high-quality statistical properties in the long\n+     * permutation on each rescan.  The pseudorandom generator need\n+     * not have high-quality statistical properties in the long\n@@ -624,12 +589,1 @@\n-     * from ThreadLocalRandom probes, which are cheap and\n-     * suffice. Each queue's polling attempts to avoid becoming stuck\n-     * when other scanners\/pollers stall.  Scans do not otherwise\n-     * explicitly take into account core affinities, loads, cache\n-     * localities, etc, However, they do exploit temporal locality\n-     * (which usually approximates these) by preferring to re-poll\n-     * from the same queue after a successful poll before trying\n-     * others, which also reduces bookkeeping, cache traffic, and\n-     * scanning overhead. But it also reduces fairness, which is\n-     * partially counteracted by giving up on detected interference\n-     * (which also reduces contention when too many workers try to\n-     * take small tasks from the same queue).\n+     * from ThreadLocalRandom probes, which are cheap and suffice.\n@@ -638,8 +592,35 @@\n-     * it tries to deactivate()), giving up (and rescanning) on \"ctl\"\n-     * contention. To avoid missed signals during deactivation, the\n-     * method rescans and reactivates if there may have been a missed\n-     * signal during deactivation. To reduce false-alarm reactivations\n-     * while doing so, we scan multiple times (analogously to method\n-     * quiescent()) before trying to reactivate.  Because idle workers\n-     * are often not yet blocked (parked), we use a WorkQueue field to\n-     * advertise that a waiter actually needs unparking upon signal.\n+     * it invokes deactivate, that first deactivates (to an IDLE\n+     * phase).  Avoiding missed signals during deactivation requires a\n+     * (conservative) rescan, reactivating if there may be tasks to\n+     * poll. Because idle workers are often not yet blocked (parked),\n+     * we use a WorkQueue field to advertise that a waiter actually\n+     * needs unparking upon signal.\n+     *\n+     * When tasks are constructed as (recursive) DAGs, top-level\n+     * scanning is usually infrequent, and doesn't encounter most\n+     * of the following problems addressed by runWorker and awaitWork:\n+     *\n+     * Locality. Polls are organized into \"runs\", continuing until\n+     * empty or contended, while also minimizing interference by\n+     * postponing bookeeping to ends of runs. This may reduce\n+     * fairness.\n+     *\n+     * Contention. When many workers try to poll few queues, they\n+     * often collide, generating CAS failures and disrupting locality\n+     * of workers already running their tasks. This also leads to\n+     * stalls when tasks cannot be taken because other workers have\n+     * not finished poll operations, which is detected by reading\n+     * ahead in queue arrays. In both cases, workers restart scans in a\n+     * way that approximates randomized backoff.\n+     *\n+     * Oversignalling. When many short top-level tasks are present in\n+     * a small number of queues, the above signalling strategy may\n+     * activate many more workers than needed, worsening locality and\n+     * contention problems, while also generating more global\n+     * contention (field ctl is CASed on every activation and\n+     * deactivation). We filter out (both in runWorker and\n+     * signalWork) attempted signals that are surely not needed\n+     * because the signalled tasks are already taken.\n+     *\n+     * Shutdown and Quiescence\n+     * =======================\n@@ -895,3 +876,1 @@\n-     * paths. The inability to rely on caller-runs may also require\n-     * extra signalling (resulting in scanning and contention) so is\n-     * done only conditionally in methods push and runworker.\n+     * paths.\n@@ -964,3 +943,7 @@\n-     * embedded @Contended region segregates fields most heavily\n-     * updated by owners from those most commonly read by stealers or\n-     * other management.\n+     * embedded @Contended isolates the very busy top index, and\n+     * another segregates status and bookkeeping fields written\n+     * (mostly) by owners, that otherwise interfere with reading\n+     * array, top, and base fields. There are other variables commonly\n+     * contributing to false-sharing-related performance issues\n+     * (including fields of class Thread), but we can't do much about\n+     * this except try to minimize access.\n@@ -975,7 +958,5 @@\n-     * Currently, arrays for workers are initialized to be just large\n-     * enough to avoid resizing in most tree-structured tasks, but\n-     * larger for external queues where both false-sharing problems\n-     * and the need for resizing are more common. (Maintenance note:\n-     * any changes in fields, queues, or their uses, or JVM layout\n-     * policies, must be accompanied by re-evaluation of these\n-     * placement and sizing decisions.)\n+     * Currently, arrays are initialized to be just large enough to\n+     * avoid resizing in most tree-structured tasks, but grow rapidly\n+     * until large.  (Maintenance note: any changes in fields, queues,\n+     * or their uses, or JVM layout policies, must be accompanied by\n+     * re-evaluation of these placement and sizing decisions.)\n@@ -1064,1 +1045,1 @@\n-     * Initial capacity of work-stealing queue array for workers.\n+     * Initial capacity of work-stealing queue array.\n@@ -1069,6 +1050,0 @@\n-    \/**\n-     * Initial capacity of work-stealing queue array for external queues.\n-     * Must be a power of two, at least 2. See above.\n-     *\/\n-    static final int INITIAL_EXTERNAL_QUEUE_CAPACITY = 1 << 9;\n-\n@@ -1206,2 +1181,1 @@\n-        \/\/ fields otherwise causing more unnecessary false-sharing cache misses\n-        @jdk.internal.vm.annotation.Contended(\"w\")\n+        @jdk.internal.vm.annotation.Contended(\"t\") \/\/ segregate\n@@ -1209,0 +1183,2 @@\n+\n+        \/\/ fields otherwise causing more unnecessary false-sharing cache misses\n@@ -1214,0 +1190,2 @@\n+        volatile int parking;      \/\/ nonzero if parked in awaitWork\n+        @jdk.internal.vm.annotation.Contended(\"w\")\n@@ -1217,2 +1195,0 @@\n-        @jdk.internal.vm.annotation.Contended(\"w\")\n-        volatile int parking;      \/\/ nonzero if parked in awaitWork\n@@ -1251,4 +1227,0 @@\n-            array = new ForkJoinTask<?>[owner == null ?\n-                                        INITIAL_EXTERNAL_QUEUE_CAPACITY :\n-                                        INITIAL_QUEUE_CAPACITY];\n-            this.owner = owner;\n@@ -1256,0 +1228,4 @@\n+            if ((this.owner = owner) == null) {\n+                array = new ForkJoinTask<?>[INITIAL_QUEUE_CAPACITY];\n+                phase = id | IDLE;\n+            }\n@@ -1282,5 +1258,4 @@\n-            int s = top, b = base, m, cap, room; ForkJoinTask<?>[] a;\n-            if ((a = array) != null && (cap = a.length) > 0 && \/\/ else disabled\n-                task != null) {\n-                int pk = task.noUserHelp() + 1;             \/\/ prev slot offset\n-                if ((room = (m = cap - 1) - (s - b)) >= 0) {\n+            int s = top, b = base, m, cap, room; ForkJoinTask<?>[] a, na;\n+            if ((a = array) != null && (cap = a.length) > 0) { \/\/ else disabled\n+                int k = (m = cap - 1) & s;\n+                if ((room = m - (s - b)) >= 0) {\n@@ -1288,1 +1263,1 @@\n-                    long pos = slotOffset(m & s);\n+                    long pos = slotOffset(k);\n@@ -1293,2 +1268,2 @@\n-                    if (room == 0)                          \/\/ resize\n-                        growArray(a, cap, s);\n+                    if (room == 0 && (na = growArray(a, cap, s)) != null)\n+                        k = ((a = na).length - 1) & s;      \/\/ resize\n@@ -1300,3 +1275,4 @@\n-                if ((room == 0 || a[m & (s - pk)] == null) &&\n-                    pool != null)\n-                    pool.signalWork();   \/\/ may have appeared empty\n+                if (pool != null &&\n+                    (room == 0 ||\n+                     U.getReferenceAcquire(a, slotOffset(m & (s - 1))) == null))\n+                    pool.signalWork(a, k);    \/\/ may have appeared empty\n@@ -1311,0 +1287,1 @@\n+         * @return new array, or null on failure\n@@ -1312,2 +1289,3 @@\n-        private void growArray(ForkJoinTask<?>[] a, int cap, int s) {\n-            int newCap = cap << 1;\n+        private ForkJoinTask<?>[] growArray(ForkJoinTask<?>[] a, int cap, int s) {\n+            int newCap = (cap >= 1 << 16) ? cap << 1 : cap << 2;\n+            ForkJoinTask<?>[] newArray = null;\n@@ -1315,1 +1293,0 @@\n-                ForkJoinTask<?>[] newArray = null;\n@@ -1332,0 +1309,1 @@\n+            return newArray;\n@@ -1335,3 +1313,1 @@\n-         * Takes next task, if one exists, in order specified by mode,\n-         * so acts as either local-pop or local-poll. Called only by owner.\n-         * @param fifo nonzero if FIFO mode\n+         * Takes next task, if one exists, in lifo order.\n@@ -1339,1 +1315,1 @@\n-        private ForkJoinTask<?> nextLocalTask(int fifo) {\n+        private ForkJoinTask<?> localPop() {\n@@ -1341,9 +1317,23 @@\n-            ForkJoinTask<?>[] a = array;\n-            int b = base, p = top, cap;\n-            if (p - b > 0 && a != null && (cap = a.length) > 0) {\n-                for (int m = cap - 1, s, nb;;) {\n-                    if (fifo == 0 || (nb = b + 1) == p) {\n-                        if ((t = (ForkJoinTask<?>)U.getAndSetReference(\n-                                 a, slotOffset(m & (s = p - 1)), null)) != null)\n-                            updateTop(s);       \/\/ else lost race for only task\n-                        break;\n+            int s = top - 1, cap; long k; ForkJoinTask<?>[] a;\n+            if ((a = array) != null && (cap = a.length) > 0 &&\n+                U.getReference(a, k = slotOffset((cap - 1) & s)) != null &&\n+                (t = (ForkJoinTask<?>)U.getAndSetReference(a, k, null)) != null)\n+                updateTop(s);\n+            return t;\n+        }\n+\n+        \/**\n+         * Takes next task, if one exists, in fifo order.\n+         *\/\n+        private ForkJoinTask<?> localPoll() {\n+            ForkJoinTask<?> t = null;\n+            int p = top, cap; ForkJoinTask<?>[] a;\n+            if ((a = array) != null && (cap = a.length) > 0) {\n+                for (int b = base; p - b > 0; ) {\n+                    int nb = b + 1;\n+                    long k = slotOffset((cap - 1) & b);\n+                    if (U.getReference(a, k) == null) {\n+                        if (nb == p)\n+                            break;          \/\/ else base is lagging\n+                        while (b == (b = U.getIntAcquire(this, BASE)))\n+                            Thread.onSpinWait(); \/\/ spin to reduce memory traffic\n@@ -1351,2 +1341,2 @@\n-                    if ((t = (ForkJoinTask<?>)U.getAndSetReference(\n-                             a, slotOffset(m & b), null)) != null) {\n+                    else if ((t = (ForkJoinTask<?>)\n+                              U.getAndSetReference(a, k, null)) != null) {\n@@ -1356,4 +1346,2 @@\n-                    while (b == (b = U.getIntAcquire(this, BASE)))\n-                        Thread.onSpinWait();    \/\/ spin to reduce memory traffic\n-                    if (p - b <= 0)\n-                        break;\n+                    else\n+                        b = base;\n@@ -1367,1 +1355,0 @@\n-         * (Always internal, never called for Common pool.)\n@@ -1370,1 +1357,1 @@\n-            return nextLocalTask(config & FIFO);\n+            return (config & FIFO) == 0 ? localPop() : localPoll();\n@@ -1446,1 +1433,2 @@\n-         * Runs the given task, as well as remaining local tasks.\n+         * Runs the given task, as well as remaining local tasks, and\n+         * those from the given queue that can be polled without interference.\n@@ -1448,4 +1436,50 @@\n-        final void topLevelExec(ForkJoinTask<?> task, int fifo) {\n-            while (task != null) {\n-                task.doExec();\n-                task = nextLocalTask(fifo);\n+        final void topLevelExec(ForkJoinTask<?> task, WorkQueue q, int fifo) {\n+            if (task != null && q != null) {\n+                int stolen = 1;\n+                for (;;) {\n+                    task.doExec();\n+                    task = null;\n+                    int p = top, cap; ForkJoinTask<?>[] a;\n+                    if ((a = array) == null || (cap = a.length) <= 0)\n+                        break;\n+                    if (fifo == 0) {  \/\/ specialized localPop\n+                        int s = p - 1;\n+                        long k = slotOffset((cap - 1) & s);\n+                        if (U.getReference(a, k) != null &&\n+                            (task = (ForkJoinTask<?>)\n+                             U.getAndSetReference(a, k, null)) != null)\n+                            top = s;\n+                    } else {         \/\/ specialized localPoll\n+                        for (int b = base; p - b > 0; ) {\n+                            int nb = b + 1;\n+                            long k = slotOffset((cap - 1) & b);\n+                            if (U.getReference(a, k) != null &&\n+                                (task = (ForkJoinTask<?>)\n+                                 U.getAndSetReference(a, k, null)) != null) {\n+                                base = nb;\n+                                break;\n+                            }\n+                            if (nb == p)\n+                                break;\n+                            while (b == (b = U.getIntAcquire(this, BASE)))\n+                                Thread.onSpinWait();\n+                        }\n+                    }\n+                    if (task == null) { \/\/ one-shot steal attempt\n+                        int qb = q.base, qcap; ForkJoinTask<?>[] qa; long bp;\n+                        if ((qa = q.array) != null && (qcap = qa.length) > 0 &&\n+                            (task = (ForkJoinTask<?>)U.getReferenceAcquire(\n+                                qa, bp = slotOffset((qcap - 1) & qb))) != null &&\n+                            q.base == qb &&\n+                            U.compareAndSetReference(qa, bp, task, null)) {\n+                            q.base = qb + 1;\n+                            ++stolen;\n+                        }\n+                        else\n+                            break;\n+                    }\n+                }\n+                nsteals += stolen;\n+                ForkJoinWorkerThread o;\n+                if ((config & CLEAR_TLS) != 0 && (o = owner) != null)\n+                    o.resetThreadLocals();\n@@ -1581,1 +1615,1 @@\n-            for (ForkJoinTask<?> t; (t = nextLocalTask(0)) != null; ) {\n+            for (ForkJoinTask<?> t; (t = localPop()) != null; ) {\n@@ -1783,1 +1817,2 @@\n-        if (w != null && (runState & STOP) == 0L) {\n+        if (w != null) {\n+            w.array = new ForkJoinTask<?>[INITIAL_QUEUE_CAPACITY];\n@@ -1861,1 +1896,0 @@\n-            signalWork();                  \/\/ possibly replace\n@@ -1863,0 +1897,1 @@\n+            signalWork(null, 0);           \/\/ possibly replace\n@@ -1869,1 +1904,2 @@\n-     * Releases an idle worker, or creates one if not enough exist.\n+     * Releases an idle worker, or creates one if not enough exist,\n+     * giving up if array a is nonnull and task at a[k] already taken.\n@@ -1871,1 +1907,1 @@\n-    final void signalWork() {\n+    final void signalWork(ForkJoinTask<?>[] a, int k) {\n@@ -1887,1 +1923,1 @@\n-                nc = ((c + TC_UNIT) & TC_MASK);\n+                nc = ((c + TC_UNIT) & TC_MASK) | ac;\n@@ -1892,2 +1928,4 @@\n-                nc = (v.stackPred & LMASK) | (c & TC_MASK);\n-            if (c == (c = compareAndExchangeCtl(c, nc | ac))) {\n+                nc = (v.stackPred & LMASK) | (c & TC_MASK) | ac;\n+            if (a != null && k < a.length && k >= 0 && a[k] == null)\n+                break;\n+            if (c == (c = ctl) && c == (c = compareAndExchangeCtl(c, nc))) {\n@@ -1977,4 +2015,6 @@\n-            int phase = w.phase, r = w.stackPred;     \/\/ seed from registerWorker\n-            int fifo = w.config & FIFO, nsteals = 0, src = -1;\n-            for (;;) {\n-                WorkQueue[] qs;\n+            WorkQueue[] qs;\n+            int phase = w.phase, r = w.stackPred;         \/\/ seed from registerWorker\n+            int fifo = (int)config & FIFO, rescans = 0, n;\n+            while ((runState & STOP) == 0L && (qs = queues) != null &&\n+                   (n = qs.length) > 0) {\n+                int i = r, step = (r >>> 16) | 1;\n@@ -1982,10 +2022,6 @@\n-                if ((runState & STOP) != 0L || (qs = queues) == null)\n-                    break;\n-                int n = qs.length, i = r, step = (r >>> 16) | 1;\n-                boolean rescan = false;\n-                scan: for (int l = n; l > 0; --l, i += step) {  \/\/ scan queues\n-                    int j, cap; WorkQueue q; ForkJoinTask<?>[] a;\n-                    if ((q = qs[j = i & (n - 1)]) != null &&\n-                        (a = q.array) != null && (cap = a.length) > 0) {\n-                        for (int m = cap - 1, pb = -1, b = q.base;;) {\n-                            ForkJoinTask<?> t; long k;\n+                scan: for (int j = n; j != 0; --j, i += step) {\n+                    WorkQueue q; int qid;\n+                    if ((q = qs[qid = i & (n - 1)]) != null) {\n+                        ForkJoinTask<?>[] a; int cap;     \/\/ poll queue\n+                        while ((a = q.array) != null && (cap = a.length) > 0) {\n+                            int b, nb, nk; long bp; ForkJoinTask<?> t;\n@@ -1993,14 +2029,13 @@\n-                                a, k = slotOffset(m & b));\n-                            if (b != (b = q.base) || t == null ||\n-                                !U.compareAndSetReference(a, k, t, null)) {\n-                                if (a[b & m] == null) {\n-                                    if (rescan)           \/\/ end of run\n-                                        break scan;\n-                                    if (a[(b + 1) & m] == null &&\n-                                        a[(b + 2) & m] == null) {\n-                                        break;            \/\/ probably empty\n-                                    }\n-                                    if (pb == (pb = b)) { \/\/ track progress\n-                                        rescan = true;    \/\/ stalled; reorder scan\n-                                        break scan;\n-                                    }\n+                                a, bp = slotOffset((cap - 1) & (b = q.base)));\n+                            if (q.base != b)\n+                                continue;                 \/\/ inconsistent\n+                            long np = slotOffset(nk = (nb = b + 1) & (cap - 1));\n+                            if (t == null) {\n+                                if (q.array != a)         \/\/ resized\n+                                    continue;\n+                                if (rescans > 0)          \/\/ ran or stalled\n+                                    break scan;\n+                                if (U.getReference(a, np) != null ||\n+                                    (rescans < 0 && q.top - b > 0)) {\n+                                    rescans = 1;          \/\/ may be stalled\n+                                    continue;\n@@ -2008,0 +2043,3 @@\n+                                if (U.getReference(a, bp) != null)\n+                                    continue;             \/\/ stale\n+                                break;                    \/\/ probably empty\n@@ -2009,13 +2047,15 @@\n-                            else {\n-                                boolean propagate;\n-                                int nb = q.base = b + 1, prevSrc = src;\n-                                w.nsteals = ++nsteals;\n-                                w.source = src = j;       \/\/ volatile\n-                                rescan = true;\n-                                int nh = t.noUserHelp();\n-                                if (propagate =\n-                                    (prevSrc != src || nh != 0) && a[nb & m] != null)\n-                                    signalWork();\n-                                w.topLevelExec(t, fifo);\n-                                if ((b = q.base) != nb && !propagate)\n-                                    break scan;          \/\/ reduce interference\n+                            if ((phase & IDLE) != 0 &&\n+                                ((phase = tryReactivate(w, phase)) & IDLE) != 0) {\n+                                rescans = 1;              \/\/ can't take yet\n+                                break scan;\n+                            }\n+                            if (U.getReference(a, bp) == t &&\n+                                U.compareAndSetReference(a, bp, t, null)) {\n+                                q.base = nb;\n+                                Object nt = U.getReferenceAcquire(a, np);\n+                                w.source = qid;\n+                                rescans = 1;\n+                                if (nt != null &&         \/\/ confirm a[nk]\n+                                    U.getReferenceAcquire(a, np) == nt)\n+                                    signalWork(a, nk);    \/\/ propagate\n+                                w.topLevelExec(t, q, fifo);\n@@ -2026,5 +2066,7 @@\n-                if (!rescan) {\n-                    if (((phase = deactivate(w, phase)) & IDLE) != 0)\n-                        break;\n-                    src = -1;                            \/\/ re-enable propagation\n-                }\n+                int prev;\n+                if (rescans >= 0)\n+                    --rescans;\n+                else if ((phase = deactivate(w, prev = phase)) == 0)\n+                    break;\n+                else if (phase != prev)\n+                    rescans = 0;\n@@ -2036,1 +2078,2 @@\n-     * Deactivates and if necessary awaits signal or termination.\n+     * If active, tries to deactivate worker, keeping active on\n+     * contention; else awaits signal or termination\n@@ -2038,3 +2081,3 @@\n-     * @param w the worker\n-     * @param phase current phase\n-     * @return current phase, with IDLE set if worker should exit\n+     * @param w the work queue\n+     * @param phase w's currently known phase\n+     * @return current phase or 0 on exit\n@@ -2043,28 +2086,27 @@\n-        if (w == null)                        \/\/ currently impossible\n-            return IDLE;\n-        int p = phase | IDLE, activePhase = phase + (IDLE << 1);\n-        long pc = ctl, qc = (activePhase & LMASK) | ((pc - RC_UNIT) & UMASK);\n-        int sp = w.stackPred = (int)pc;       \/\/ set ctl stack link\n-        w.phase = p;\n-        if (!compareAndSetCtl(pc, qc))        \/\/ try to enqueue\n-            return w.phase = phase;           \/\/ back out on possible signal\n-        int ac = (short)(qc >>> RC_SHIFT), n; long e; WorkQueue[] qs;\n-        if (((e = runState) & STOP) != 0L ||\n-            ((e & SHUTDOWN) != 0L && ac == 0 && quiescent() > 0) ||\n-            (qs = queues) == null || (n = qs.length) <= 0)\n-            return IDLE;                      \/\/ terminating\n-\n-        for (int prechecks = Math.min(ac, 2), \/\/ reactivation threshold\n-             k = Math.max(n + (n << 1), SPIN_WAITS << 1);;) {\n-            WorkQueue q; int cap; ForkJoinTask<?>[] a; long c;\n-            if (w.phase == activePhase)\n-                return activePhase;\n-            if (--k < 0)\n-                return awaitWork(w, p);       \/\/ block, drop, or exit\n-            if ((q = qs[k & (n - 1)]) == null)\n-                Thread.onSpinWait();\n-            else if ((a = q.array) != null && (cap = a.length) > 0 &&\n-                     a[q.base & (cap - 1)] != null && --prechecks < 0 &&\n-                     (int)(c = ctl) == activePhase &&\n-                     compareAndSetCtl(c, (sp & LMASK) | ((c + RC_UNIT) & UMASK)))\n-                return w.phase = activePhase; \/\/ reactivate\n+        if (w != null) {                          \/\/ always true; hoist checks\n+            if ((phase & IDLE) == 0) {\n+                int idlePhase = phase | IDLE;\n+                long pc = ctl, e;\n+                long qc = ((phase + (IDLE << 1)) & LMASK) | ((pc - RC_UNIT) & UMASK);\n+                w.stackPred = (int)pc;            \/\/ set ctl stack link\n+                w.phase = idlePhase;              \/\/ try to enqueue\n+                if (!compareAndSetCtl(pc, qc))\n+                    w.phase = phase;              \/\/ back out on contention\n+                else {\n+                    phase = idlePhase;\n+                    if ((qc & RC_MASK) <= 0L && ((e = runState) & SHUTDOWN) != 0L &&\n+                        (e & STOP) == 0L)\n+                        quiescent();              \/\/ may trigger quiescent termination\n+                }\n+            }\n+            else if ((runState & STOP) != 0L)\n+                phase = 0;\n+            else {                                \/\/ spin before blocking\n+                int activePhase = phase + IDLE;\n+                int noise = activePhase | (activePhase >>> 16);\n+                int spins = (SPIN_WAITS << 1) | (noise & (SPIN_WAITS - 1));\n+                while ((phase = w.phase) != activePhase && --spins != 0)\n+                    Thread.onSpinWait();\n+                if (spins == 0 && awaitWork(w, phase = activePhase) != 0)\n+                    phase = 0;\n+            }\n@@ -2072,0 +2114,17 @@\n+        return phase;\n+    }\n+\n+    \/**\n+     * Reactivates worker w if it is currently top of ctl stack\n+     *\n+     * @param w the work queue\n+     * @param phase w's currently known (idle) phase\n+     * @return currently known phase on exit\n+     *\/\n+    private int tryReactivate(WorkQueue w, int phase) {\n+        int activePhase = phase + IDLE; long c;\n+        if (w != null && (phase = w.phase) != activePhase &&\n+            (int)(c = ctl) == activePhase &&\n+            compareAndSetCtl(c, (w.stackPred & LMASK) | ((c + RC_UNIT) & UMASK)))\n+            phase = w.phase = activePhase;\n+        return phase;\n@@ -2078,2 +2137,2 @@\n-     * @param p current phase (known to be idle)\n-     * @return current phase, with IDLE set if worker should exit\n+     * @param activePhase w's next active phase\n+     * @return 0 if now active\n@@ -2081,22 +2140,19 @@\n-    private int awaitWork(WorkQueue w, int p) {\n-        if (w != null) {\n-            ForkJoinWorkerThread t; long deadline;\n-            if ((w.config & CLEAR_TLS) != 0 && (t = w.owner) != null)\n-                t.resetThreadLocals();          \/\/ clear before reactivate\n-            if ((ctl & RC_MASK) > 0L)\n-                deadline = 0L;\n-            else if ((deadline =\n-                      (((w.source != INVALID_ID) ? keepAlive : TIMEOUT_SLOP)) +\n-                      System.currentTimeMillis()) == 0L)\n-                deadline = 1L;                 \/\/ avoid zero\n-            int activePhase = p + IDLE;\n-            if ((p = w.phase) != activePhase && (runState & STOP) == 0L) {\n-                LockSupport.setCurrentBlocker(this);\n-                w.parking = 1;                 \/\/ enable unpark\n-                while ((p = w.phase) != activePhase) {\n-                    boolean trimmable = false; int trim;\n-                    Thread.interrupted();      \/\/ clear status\n-                    if ((runState & STOP) != 0L)\n-                        break;\n-                    if (deadline != 0L) {\n-                        if ((trim = tryTrim(w, p, deadline)) > 0)\n+    private int awaitWork(WorkQueue w, int activePhase) {\n+        int idle = 1;\n+        if (w != null) {                      \/\/ always true; hoist checks\n+            long waitTime = (w.source == INVALID_ID) ? 0L : keepAlive;\n+            LockSupport.setCurrentBlocker(this);\n+            for (long deadline = 0L;;) {\n+                Thread.interrupted();         \/\/ clear status\n+                if ((runState & STOP) != 0L)\n+                    break;\n+                if ((idle = w.phase - activePhase) == 0)\n+                    break;\n+                boolean trimmable = false;    \/\/ use timed wait if trimmable\n+                long d = 0L, c;\n+                if (((c = ctl) & RC_MASK) == 0L && (int)c == activePhase) {\n+                    long now = System.currentTimeMillis();\n+                    if (deadline == 0L)\n+                        deadline = waitTime + now;\n+                    if (deadline - now <= TIMEOUT_SLOP) {\n+                        if (tryTrim(w, c, activePhase))\n@@ -2104,4 +2160,1 @@\n-                        else if (trim < 0)\n-                            deadline = 0L;\n-                        else\n-                            trimmable = true;\n+                        continue;             \/\/ lost race to trim\n@@ -2109,1 +2162,2 @@\n-                    U.park(trimmable, deadline);\n+                    d = deadline;\n+                    trimmable = true;\n@@ -2111,2 +2165,6 @@\n-                w.parking = 0;\n-                LockSupport.setCurrentBlocker(null);\n+                w.parking = 1;                \/\/ enable unpark and recheck\n+                if ((idle = w.phase - activePhase) != 0)\n+                    U.park(trimmable, d);\n+                w.parking = 0;                \/\/ close unpark window\n+                if (idle == 0 || (idle = w.phase - activePhase) == 0)\n+                    break;\n@@ -2114,0 +2172,1 @@\n+            LockSupport.setCurrentBlocker(null);\n@@ -2115,1 +2174,1 @@\n-        return p;\n+        return idle;\n@@ -2120,2 +2179,1 @@\n-     * another to do the same.\n-     * @return > 0: trimmed, < 0 : not trimmable, else 0\n+     * another to do the same unless new tasks are found.\n@@ -2123,22 +2181,18 @@\n-    private int tryTrim(WorkQueue w, int phase, long deadline) {\n-        long c, nc; int stat, activePhase, vp, i; WorkQueue[] vs; WorkQueue v;\n-        if ((activePhase = phase + IDLE) != (int)(c = ctl) || w == null)\n-            stat = -1;                      \/\/ no longer ctl top\n-        else if (deadline - System.currentTimeMillis() >= TIMEOUT_SLOP)\n-            stat = 0;                       \/\/ spurious wakeup\n-        else if (!compareAndSetCtl(\n-                     c, nc = ((w.stackPred & LMASK) | (RC_MASK & c) |\n-                               (TC_MASK & (c - TC_UNIT)))))\n-            stat = -1;                      \/\/ lost race to signaller\n-        else {\n-            stat = 1;\n-            w.source = DROPPED;\n-            w.phase = activePhase;\n-            if ((vp = (int)nc) != 0 && (vs = queues) != null &&\n-                vs.length > (i = vp & SMASK) && (v = vs[i]) != null &&\n-                compareAndSetCtl(           \/\/ try to wake up next waiter\n-                    nc, ((UMASK & (nc + RC_UNIT)) |\n-                         (nc & TC_MASK) | (v.stackPred & LMASK)))) {\n-                v.source = INVALID_ID;      \/\/ enable cascaded timeouts\n-                v.phase = vp;\n-                U.unpark(v.owner);\n+    private boolean tryTrim(WorkQueue w, long c, int activePhase) {\n+        if (w != null) {\n+            int vp, i; WorkQueue[] vs; WorkQueue v;\n+            long nc = ((w.stackPred & LMASK) |\n+                       ((RC_MASK & c) | (TC_MASK & (c - TC_UNIT))));\n+            if (compareAndSetCtl(c, nc)) {\n+                w.source = DROPPED;\n+                w.phase = activePhase;\n+                if ((vp = (int)nc) != 0 && (vs = queues) != null &&\n+                    vs.length > (i = vp & SMASK) && (v = vs[i]) != null &&\n+                    compareAndSetCtl(           \/\/ try to wake up next waiter\n+                        nc, ((v.stackPred & LMASK) |\n+                             ((UMASK & (nc + RC_UNIT)) | (nc & TC_MASK))))) {\n+                    v.source = INVALID_ID;      \/\/ enable cascaded timeouts\n+                    v.phase = vp;\n+                    U.unpark(v.owner);\n+                }\n+                return true;\n@@ -2147,1 +2201,1 @@\n-        return stat;\n+        return false;\n@@ -2564,2 +2618,1 @@\n-     * throws RejectedExecutionException if shutdown or terminating.\n-     * @param r current ThreadLocalRandom.getProbe() value\n+     * throws RejectedExecutionException if shutdown\n@@ -2567,1 +2620,1 @@\n-     *        should be thrown when shutdown (else only if terminating)\n+     *        should be thrown when shutdown\n@@ -2569,4 +2622,4 @@\n-    private WorkQueue submissionQueue(int r, boolean rejectOnShutdown) {\n-        int reuse;                                   \/\/ nonzero if prefer create\n-        if ((reuse = r) == 0) {\n-            ThreadLocalRandom.localInit();           \/\/ initialize caller's probe\n+    final WorkQueue externalSubmissionQueue(boolean rejectOnShutdown) {\n+        int r;\n+        if ((r = ThreadLocalRandom.getProbe()) == 0) {\n+            ThreadLocalRandom.localInit();   \/\/ initialize caller's probe\n@@ -2575,5 +2628,3 @@\n-        for (int probes = 0; ; ++probes) {\n-            int n, i, id; WorkQueue[] qs; WorkQueue q;\n-            if ((qs = queues) == null)\n-                break;\n-            if ((n = qs.length) <= 0)\n+        for (;;) {\n+            WorkQueue q; WorkQueue[] qs; int n, id, i;\n+            if ((qs = queues) == null || (n = qs.length) <= 0)\n@@ -2582,6 +2633,4 @@\n-                WorkQueue w = new WorkQueue(null, id, 0, false);\n-                w.phase = id;\n-                boolean reject = ((lockRunState() & SHUTDOWN) != 0 &&\n-                                  rejectOnShutdown);\n-                if (!reject && queues == qs && qs[i] == null)\n-                    q = qs[i] = w;                   \/\/ else lost race to install\n+                WorkQueue newq = new WorkQueue(null, id, 0, false);\n+                lockRunState();\n+                if (qs[i] == null && queues == qs)\n+                    q = qs[i] = newq;         \/\/ else lost race to install\n@@ -2589,5 +2638,0 @@\n-                if (q != null)\n-                    return q;\n-                if (reject)\n-                    break;\n-                reuse = 0;\n@@ -2595,4 +2639,4 @@\n-            if (reuse == 0 || !q.tryLockPhase()) {   \/\/ move index\n-                if (reuse == 0) {\n-                    if (probes >= n >> 1)\n-                        reuse = r;                   \/\/ stop prefering free slot\n+            if (q != null && q.tryLockPhase()) {\n+                if (rejectOnShutdown && (runState & SHUTDOWN) != 0L) {\n+                    q.unlockPhase();          \/\/ check while q lock held\n+                    break;\n@@ -2600,9 +2644,0 @@\n-                else if (q != null)\n-                    reuse = 0;                       \/\/ probe on collision\n-                r = ThreadLocalRandom.advanceProbe(r);\n-            }\n-            else if (rejectOnShutdown && (runState & SHUTDOWN) != 0L) {\n-                q.unlockPhase();                     \/\/ check while q lock held\n-                break;\n-            }\n-            else\n@@ -2610,0 +2645,2 @@\n+            }\n+            r = ThreadLocalRandom.advanceProbe(r); \/\/ move\n@@ -2623,1 +2660,1 @@\n-            q = submissionQueue(ThreadLocalRandom.getProbe(), true);\n+            q = externalSubmissionQueue(true);\n@@ -2629,12 +2666,0 @@\n-    \/**\n-     * Returns queue for an external submission, bypassing call to\n-     * submissionQueue if already established and unlocked.\n-     *\/\n-    final WorkQueue externalSubmissionQueue(boolean rejectOnShutdown) {\n-        WorkQueue[] qs; WorkQueue q; int n;\n-        int r = ThreadLocalRandom.getProbe();\n-        return (((qs = queues) != null && (n = qs.length) > 0 &&\n-                 (q = qs[r & EXTERNAL_ID_MASK & (n - 1)]) != null && r != 0 &&\n-                 q.tryLockPhase()) ? q : submissionQueue(r, rejectOnShutdown));\n-    }\n-\n","filename":"src\/java.base\/share\/classes\/java\/util\/concurrent\/ForkJoinPool.java","additions":359,"deletions":334,"binary":false,"changes":693,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import java.util.concurrent.ForkJoinTask;\n@@ -45,1 +46,1 @@\n-    public static void main(String[] args) throws Exception {\n+    static void testSubmitExternalCallable() throws Exception {\n@@ -56,0 +57,17 @@\n+\n+    static void testSubmitAdaptedCallable() throws Exception {\n+        try (var pool = new ForkJoinPool(2)) {\n+            for (int i = 0; i < 100_000; i++) {\n+                var future1 = pool.submit(new AwaitCount(i));\n+                var future2 = pool.submit(ForkJoinTask.adapt(noop));\n+                future2.get();\n+                count.set(i + 1);\n+                future1.get();\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        testSubmitExternalCallable();\n+        testSubmitAdaptedCallable();\n+    }\n","filename":"test\/jdk\/java\/util\/concurrent\/forkjoin\/Starvation.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"}]}