{"files":[{"patch":"@@ -35,1 +35,1 @@\n-void ZVirtualMemoryManager::pd_initialize_after_reserve() {\n+void ZVirtualMemoryManager::pd_register_callbacks(ZMemoryManager* manager) {\n","filename":"src\/hotspot\/os\/posix\/gc\/z\/zVirtualMemory_posix.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  virtual void initialize_after_reserve(ZMemoryManager* manager) {}\n+  virtual void register_callbacks(ZMemoryManager* manager) {}\n@@ -50,1 +50,1 @@\n-  public:\n+  private:\n@@ -82,8 +82,1 @@\n-    \/\/ Called when a memory area is returned to the memory manager but can't\n-    \/\/ be merged with an already existing area. Make sure this area is covered\n-    \/\/ by a single placeholder.\n-    static void create_callback(const ZMemory* area) {\n-      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n-\n-      coalesce_into_one_placeholder(area->start(), area->size());\n-    }\n+    \/\/ Callback implementations\n@@ -91,4 +84,5 @@\n-    \/\/ Called when a complete memory area in the memory manager is allocated.\n-    \/\/ Create granule sized placeholders for the entire area.\n-    static void destroy_callback(const ZMemory* area) {\n-      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n+    \/\/ Called when a memory area is going to be handed out to be used.\n+    \/\/\n+    \/\/ Splits the memory area into granule-sized placeholders.\n+    static void prepare_for_hand_out_callback(const ZMemory& area) {\n+      assert(is_aligned(area.size(), ZGranuleSize), \"Must be granule aligned\");\n@@ -96,1 +90,1 @@\n-      split_into_granule_sized_placeholders(area->start(), area->size());\n+      split_into_granule_sized_placeholders(area.start(), area.size());\n@@ -99,5 +93,5 @@\n-    \/\/ Called when a memory area is allocated at the front of an exising memory area.\n-    \/\/ Turn the first part of the memory area into granule sized placeholders.\n-    static void shrink_from_front_callback(const ZMemory* area, size_t size) {\n-      assert(area->size() > size, \"Must be larger than what we try to split out\");\n-      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n+    \/\/ Called when a memory area is handed back to the memory manager.\n+    \/\/\n+    \/\/ Combines the granule-sized placeholders into one placeholder.\n+    static void prepare_for_hand_back_callback(const ZMemory& area) {\n+      assert(is_aligned(area.size(), ZGranuleSize), \"Must be granule aligned\");\n@@ -105,5 +99,1 @@\n-      \/\/ Split the area into two placeholders\n-      split_placeholder(area->start(), size);\n-\n-      \/\/ Split the first part into granule sized placeholders\n-      split_into_granule_sized_placeholders(area->start(), size);\n+      coalesce_into_one_placeholder(area.start(), area.size());\n@@ -112,21 +102,11 @@\n-    \/\/ Called when a memory area is allocated at the end of an existing memory area.\n-    \/\/ Turn the second part of the memory area into granule sized placeholders.\n-    static void shrink_from_back_callback(const ZMemory* area, size_t size) {\n-      assert(area->size() > size, \"Must be larger than what we try to split out\");\n-      assert(is_aligned(size, ZGranuleSize), \"Must be granule aligned\");\n-\n-      \/\/ Split the area into two placeholders\n-      const zoffset start = to_zoffset(area->end() - size);\n-      split_placeholder(start, size);\n-\n-      \/\/ Split the second part into granule sized placeholders\n-      split_into_granule_sized_placeholders(start, size);\n-    }\n-\n-    \/\/ Called when freeing a memory area and it can be merged at the start of an\n-    \/\/ existing area. Coalesce the underlying placeholders into one.\n-    static void grow_from_front_callback(const ZMemory* area, size_t size) {\n-      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n-\n-      const zoffset start = area->start() - size;\n-      coalesce_into_one_placeholder(start, area->size() + size);\n+    \/\/ Called when inserting a memory area and it can be merged with an\n+    \/\/ existing, adjacent memory area.\n+    \/\/\n+    \/\/ Coalesces the underlying placeholders into one.\n+    static void grow_callback(const ZMemory& from, const ZMemory& to) {\n+      assert(is_aligned(from.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(is_aligned(to.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(from != to, \"Must have grown\");\n+      assert(to.contains(from), \"Must be within\");\n+\n+      coalesce_into_one_placeholder(to.start(), to.size());\n@@ -135,4 +115,11 @@\n-    \/\/ Called when freeing a memory area and it can be merged at the end of an\n-    \/\/ existing area. Coalesce the underlying placeholders into one.\n-    static void grow_from_back_callback(const ZMemory* area, size_t size) {\n-      assert(is_aligned(area->size(), ZGranuleSize), \"Must be granule aligned\");\n+    \/\/ Called when a memory area is removed from the front or back of an existing\n+    \/\/ memory area.\n+    \/\/\n+    \/\/ Splits the memory into two placeholders.\n+    static void shrink_callback(const ZMemory& from, const ZMemory& to) {\n+      assert(is_aligned(from.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(is_aligned(to.size(), ZGranuleSize), \"Must be granule aligned\");\n+      assert(from != to, \"Must have shrunk\");\n+      assert(from.contains(to), \"Must be larger than what we try to split out\");\n+      assert(from.start() == to.start() || from.end() == to.end(),\n+             \"Only verified to work if we split a placeholder into two placeholders\");\n@@ -140,1 +127,2 @@\n-      coalesce_into_one_placeholder(area->start(), area->size() + size);\n+      \/\/ Split the area into two placeholders\n+      split_placeholder(to.start(), to.size());\n@@ -143,1 +131,2 @@\n-    static void register_with(ZMemoryManager* manager) {\n+  public:\n+    static ZMemoryManager::Callbacks callbacks() {\n@@ -149,3 +138,10 @@\n-      \/\/ The create and grow callbacks are called when virtual memory is\n-      \/\/ returned to the memory manager. The new memory area is then covered\n-      \/\/ by a new single placeholder.\n+      \/\/ The prepare_for_hand_out callback is called when virtual memory is\n+      \/\/ handed out to callers. The memory area is split into granule-sized\n+      \/\/ placeholders.\n+      \/\/\n+      \/\/ The prepare_for_hand_back callback is called when previously handed\n+      \/\/ out virtual memory is handed back  to the memory manager. The\n+      \/\/ returned memory area is then covered by a new single placeholder.\n+      \/\/\n+      \/\/ The grow callback is called when a virtual memory area grows. The\n+      \/\/ resulting memory area is then covered by a single placeholder.\n@@ -153,3 +149,3 @@\n-      \/\/ The destroy and shrink callbacks are called when virtual memory is\n-      \/\/ allocated from the memory manager. The memory area is then is split\n-      \/\/ into granule-sized placeholders.\n+      \/\/ The shrink callback is called when a virtual memory area is split into\n+      \/\/ two parts. The two resulting memory areas are then covered by two\n+      \/\/ separate placeholders.\n@@ -162,6 +158,4 @@\n-      callbacks._create = &create_callback;\n-      callbacks._destroy = &destroy_callback;\n-      callbacks._shrink_from_front = &shrink_from_front_callback;\n-      callbacks._shrink_from_back = &shrink_from_back_callback;\n-      callbacks._grow_from_front = &grow_from_front_callback;\n-      callbacks._grow_from_back = &grow_from_back_callback;\n+      callbacks._prepare_for_hand_out = &prepare_for_hand_out_callback;\n+      callbacks._prepare_for_hand_back = &prepare_for_hand_back_callback;\n+      callbacks._grow = &grow_callback;\n+      callbacks._shrink = &shrink_callback;\n@@ -169,1 +163,1 @@\n-      manager->register_callbacks(callbacks);\n+      return callbacks;\n@@ -173,2 +167,2 @@\n-  virtual void initialize_after_reserve(ZMemoryManager* manager) {\n-    PlaceholderCallbacks::register_with(manager);\n+  virtual void register_callbacks(ZMemoryManager* manager) {\n+    manager->register_callbacks(PlaceholderCallbacks::callbacks());\n@@ -223,2 +217,2 @@\n-void ZVirtualMemoryManager::pd_initialize_after_reserve() {\n-  _impl->initialize_after_reserve(&_manager);\n+void ZVirtualMemoryManager::pd_register_callbacks(ZMemoryManager* manager) {\n+  _impl->register_callbacks(manager);\n","filename":"src\/hotspot\/os\/windows\/gc\/z\/zVirtualMemory_windows.cpp","additions":63,"deletions":69,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+  friend class ZTest;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zArguments.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,0 +40,2 @@\n+  friend class ZTest;\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zInitialize.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,15 +28,0 @@\n-ZMemory* ZMemoryManager::create(zoffset start, size_t size) {\n-  ZMemory* const area = new ZMemory(start, size);\n-  if (_callbacks._create != nullptr) {\n-    _callbacks._create(area);\n-  }\n-  return area;\n-}\n-\n-void ZMemoryManager::destroy(ZMemory* area) {\n-  if (_callbacks._destroy != nullptr) {\n-    _callbacks._destroy(area);\n-  }\n-  delete area;\n-}\n-\n@@ -44,2 +29,4 @@\n-  if (_callbacks._shrink_from_front != nullptr) {\n-    _callbacks._shrink_from_front(area, size);\n+  if (_callbacks._shrink != nullptr) {\n+    const ZMemory* from = area;\n+    const ZMemory to(area->start() + size, area->size() - size);\n+    _callbacks._shrink(*from, to);\n@@ -51,2 +38,4 @@\n-  if (_callbacks._shrink_from_back != nullptr) {\n-    _callbacks._shrink_from_back(area, size);\n+  if (_callbacks._shrink != nullptr) {\n+    const ZMemory* from = area;\n+    const ZMemory to(area->start(), area->size() - size);\n+    _callbacks._shrink(*from, to);\n@@ -58,2 +47,4 @@\n-  if (_callbacks._grow_from_front != nullptr) {\n-    _callbacks._grow_from_front(area, size);\n+  if (_callbacks._grow != nullptr) {\n+    const ZMemory* from = area;\n+    const ZMemory to(area->start() - size, area->size() + size);\n+    _callbacks._grow(*from, to);\n@@ -65,2 +56,4 @@\n-  if (_callbacks._grow_from_back != nullptr) {\n-    _callbacks._grow_from_back(area, size);\n+  if (_callbacks._grow != nullptr) {\n+    const ZMemory* from = area;\n+    const ZMemory to(area->start(), area->size() + size);\n+    _callbacks._grow(*from, to);\n@@ -72,6 +65,4 @@\n-  : _create(nullptr),\n-    _destroy(nullptr),\n-    _shrink_from_front(nullptr),\n-    _shrink_from_back(nullptr),\n-    _grow_from_front(nullptr),\n-    _grow_from_back(nullptr) {}\n+  : _prepare_for_hand_out(nullptr),\n+    _prepare_for_hand_back(nullptr),\n+    _grow(nullptr),\n+    _shrink(nullptr) {}\n@@ -121,0 +112,2 @@\n+      zoffset start;\n+\n@@ -123,1 +116,1 @@\n-        const zoffset start = area->start();\n+        start = area->start();\n@@ -125,2 +118,1 @@\n-        destroy(area);\n-        return start;\n+        delete area;\n@@ -129,1 +121,1 @@\n-        const zoffset start = area->start();\n+        start = area->start();\n@@ -131,1 +123,0 @@\n-        return start;\n@@ -133,0 +124,6 @@\n+\n+      if (_callbacks._prepare_for_hand_out != nullptr) {\n+        _callbacks._prepare_for_hand_out(ZMemory(start, size));\n+      }\n+\n+      return start;\n@@ -145,0 +142,2 @@\n+    const zoffset start = area->start();\n+\n@@ -147,2 +146,0 @@\n-      const zoffset start = area->start();\n-      *allocated = area->size();\n@@ -150,2 +147,2 @@\n-      destroy(area);\n-      return start;\n+      *allocated = area->size();\n+      delete area;\n@@ -154,1 +151,0 @@\n-      const zoffset start = area->start();\n@@ -157,1 +153,0 @@\n-      return start;\n@@ -159,0 +154,6 @@\n+\n+    if (_callbacks._prepare_for_hand_out != nullptr) {\n+      _callbacks._prepare_for_hand_out(ZMemory(start, *allocated));\n+    }\n+\n+    return start;\n@@ -172,0 +173,2 @@\n+      zoffset start;\n+\n@@ -174,1 +177,1 @@\n-        const zoffset start = area->start();\n+        start = area->start();\n@@ -176,2 +179,1 @@\n-        destroy(area);\n-        return start;\n+        delete area;\n@@ -181,1 +183,5 @@\n-        return to_zoffset(area->end());\n+        start = to_zoffset(area->end());\n+      }\n+\n+      if (_callbacks._prepare_for_hand_out != nullptr) {\n+        _callbacks._prepare_for_hand_out(ZMemory(start, size));\n@@ -183,0 +189,2 @@\n+\n+      return start;\n@@ -190,1 +198,1 @@\n-void ZMemoryManager::free(zoffset start, size_t size) {\n+void ZMemoryManager::move_into(zoffset start, size_t size) {\n@@ -194,2 +202,0 @@\n-  ZLocker<ZLock> locker(&_lock);\n-\n@@ -216,1 +222,1 @@\n-        ZMemory* const new_area = create(start, size);\n+        ZMemory* const new_area = new ZMemory(start, size);\n@@ -232,1 +238,1 @@\n-    ZMemory* const new_area = create(start, size);\n+    ZMemory* const new_area = new ZMemory(start, size);\n@@ -236,0 +242,43 @@\n+\n+void ZMemoryManager::free(zoffset start, size_t size) {\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  if (_callbacks._prepare_for_hand_back != nullptr) {\n+    _callbacks._prepare_for_hand_back(ZMemory(start, size));\n+  }\n+\n+  move_into(start, size);\n+}\n+\n+void ZMemoryManager::register_range(zoffset start, size_t size) {\n+  \/\/ Note that there's no need to call the _prepare_for_hand_back when memory\n+  \/\/ is added the first time. We don't have to undo the effects of a previous\n+  \/\/ _prepare_for_hand_out callback.\n+\n+  \/\/ No need to lock during initialization.\n+\n+  move_into(start, size);\n+}\n+\n+bool ZMemoryManager::unregister_first(zoffset* start_out, size_t* size_out) {\n+  \/\/ Note that this doesn't hand out memory to be used, so we don't call the\n+  \/\/ _prepare_for_hand_out callback.\n+\n+  ZLocker<ZLock> locker(&_lock);\n+\n+  if (_freelist.is_empty()) {\n+    return false;\n+  }\n+\n+  \/\/ Don't invoke the _prepare_for_hand_out callback\n+\n+  ZMemory* const area = _freelist.remove_first();\n+\n+  \/\/ Return the range\n+  *start_out = area->start();\n+  *size_out  = area->size();\n+\n+  delete area;\n+\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.cpp","additions":98,"deletions":49,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -47,0 +47,5 @@\n+  bool operator==(const ZMemory& other) const;\n+  bool operator!=(const ZMemory& other) const;\n+\n+  bool contains(const ZMemory& other) const;\n+\n@@ -54,0 +59,2 @@\n+  friend class ZVirtualMemoryManagerTest;\n+\n@@ -55,2 +62,2 @@\n-  typedef void (*CreateDestroyCallback)(const ZMemory* area);\n-  typedef void (*ResizeCallback)(const ZMemory* area, size_t size);\n+  typedef void (*CallbackPrepare)(const ZMemory& area);\n+  typedef void (*CallbackResize)(const ZMemory& from, const ZMemory& to);\n@@ -59,6 +66,4 @@\n-    CreateDestroyCallback _create;\n-    CreateDestroyCallback _destroy;\n-    ResizeCallback        _shrink_from_front;\n-    ResizeCallback        _shrink_from_back;\n-    ResizeCallback        _grow_from_front;\n-    ResizeCallback        _grow_from_back;\n+    CallbackPrepare _prepare_for_hand_out;\n+    CallbackPrepare _prepare_for_hand_back;\n+    CallbackResize  _grow;\n+    CallbackResize  _shrink;\n@@ -74,2 +79,0 @@\n-  ZMemory* create(zoffset start, size_t size);\n-  void destroy(ZMemory* area);\n@@ -81,0 +84,2 @@\n+  void move_into(zoffset start, size_t size);\n+\n@@ -95,0 +100,2 @@\n+  void register_range(zoffset start, size_t size);\n+  bool unregister_first(zoffset* start_out, size_t* size_out);\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.hpp","additions":17,"deletions":10,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -49,0 +49,12 @@\n+inline bool ZMemory::operator==(const ZMemory& other) const {\n+  return _start == other._start && _end == other._end;\n+}\n+\n+inline bool ZMemory::operator!=(const ZMemory& other) const {\n+  return !operator==(other);\n+}\n+\n+inline bool ZMemory::contains(const ZMemory& other) const {\n+  return _start <= other._start && other.end() <= end();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zMemory.inline.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -43,0 +43,20 @@\n+void ZNMT::unreserve(zaddress_unsafe start, size_t size) {\n+  precond(is_aligned(untype(start), ZGranuleSize));\n+  precond(is_aligned(size, ZGranuleSize));\n+\n+  if (MemTracker::enabled()) {\n+    \/\/ We are the owner of the reserved memory, and any failure to unreserve\n+    \/\/ are fatal, so so we don't need to hold a lock while unreserving memory.\n+\n+    MemTracker::NmtVirtualMemoryLocker nvml;\n+\n+    \/\/ The current NMT implementation does not support unreserving a memory\n+    \/\/ region that was built up from smaller memory reservations. Workaround\n+    \/\/ this problem by splitting the work up into granule-sized chunks, which\n+    \/\/ is the smallest unit we ever reserve.\n+    for (size_t i = 0; i < size; i += ZGranuleSize) {\n+      MemTracker::record_virtual_memory_release((address)untype(start + i), ZGranuleSize);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -45,0 +45,2 @@\n+  static void unreserve(zaddress_unsafe start, size_t size);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zNMT.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -241,1 +241,1 @@\n-  _manager.free(zoffset(0), max_capacity);\n+  _manager.register_range(zoffset(0), max_capacity);\n","filename":"src\/hotspot\/share\/gc\/z\/zPhysicalMemory.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  \/\/ Register the Windows callbacks\n+  pd_register_callbacks(&_manager);\n+\n@@ -54,3 +57,0 @@\n-  \/\/ Initialize platform specific parts after reserving address space\n-  pd_initialize_after_reserve();\n-\n@@ -157,1 +157,1 @@\n-  _manager.free(start, size);\n+  _manager.register_range(start, size);\n@@ -214,0 +214,19 @@\n+void ZVirtualMemoryManager::unreserve(zoffset start, size_t size) {\n+  const zaddress_unsafe addr = ZOffset::address_unsafe(start);\n+\n+  \/\/ Unregister the reserved memory from NMT\n+  ZNMT::unreserve(addr, size);\n+\n+  \/\/ Unreserve address space\n+  pd_unreserve(addr, size);\n+}\n+\n+void ZVirtualMemoryManager::unreserve_all() {\n+  zoffset start;\n+  size_t size;\n+\n+  while (_manager.unregister_first(&start, &size)) {\n+    unreserve(start, size);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.cpp","additions":23,"deletions":4,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  friend class ZVirtualMemoryManagerTest;\n@@ -61,1 +62,1 @@\n-  void pd_initialize_after_reserve();\n+  void pd_register_callbacks(ZMemoryManager* manager);\n@@ -71,0 +72,2 @@\n+  void unreserve(zoffset start, size_t size);\n+\n@@ -84,0 +87,2 @@\n+\n+  void unreserve_all();\n","filename":"src\/hotspot\/share\/gc\/z\/zVirtualMemory.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n@@ -34,1 +34,1 @@\n-#include \"unittest.hpp\"\n+#include \"zunittest.hpp\"\n@@ -38,3 +38,1 @@\n-#define EXPECT_ALLOC_OK(offset) EXPECT_NE(offset, zoffset(UINTPTR_MAX))\n-\n-class ZMapperTest : public Test {\n+class ZMapperTest : public ZTest {\n@@ -42,6 +40,1 @@\n-  static constexpr size_t ZMapperTestReservationSize = 32 * M;\n-\n-  static bool            _initialized;\n-  static ZMemoryManager* _va;\n-\n-  static ZVirtualMemoryManager* _vmm;\n+  static constexpr size_t ReservationSize = 32 * M;\n@@ -49,1 +42,2 @@\n-  static bool _has_unreserved;\n+  ZVirtualMemoryManager* _vmm;\n+  ZMemoryManager*        _va;\n@@ -52,18 +46,0 @@\n-  bool reserve_for_test() {\n-    \/\/ Initialize platform specific parts before reserving address space\n-    _vmm->pd_initialize_before_reserve();\n-\n-    \/\/ Reserve address space\n-    if (!_vmm->pd_reserve(ZOffset::address_unsafe(zoffset(0)), ZMapperTestReservationSize)) {\n-      return false;\n-    }\n-\n-    \/\/ Make the address range free before setting up callbacks below\n-    _va->free(zoffset(0), ZMapperTestReservationSize);\n-\n-    \/\/ Initialize platform specific parts after reserving address space\n-    _vmm->pd_initialize_after_reserve();\n-\n-    return true;\n-  }\n-\n@@ -72,1 +48,1 @@\n-    if (!ZSyscall::is_supported()) {\n+    if (!is_os_supported()) {\n@@ -74,1 +50,0 @@\n-      return;\n@@ -77,3 +52,0 @@\n-    ZSyscall::initialize();\n-    ZGlobalsPointers::initialize();\n-\n@@ -82,0 +54,1 @@\n+    _vmm = ::new (_vmm) ZVirtualMemoryManager(ReservationSize);\n@@ -87,1 +60,1 @@\n-    if (!reserve_for_test()) {\n+    if (_vmm->reserved() != ReservationSize) {\n@@ -89,1 +62,0 @@\n-      return;\n@@ -91,3 +63,0 @@\n-\n-    _initialized = true;\n-    _has_unreserved = false;\n@@ -97,1 +66,1 @@\n-    if (!ZSyscall::is_supported()) {\n+    if (!is_os_supported()) {\n@@ -102,3 +71,3 @@\n-    if (_initialized && !_has_unreserved) {\n-      _vmm->pd_unreserve(ZOffset::address_unsafe(zoffset(0)), 0);\n-    }\n+    \/\/ Best-effort cleanup\n+    _vmm->unreserve_all();\n+    _vmm->~ZVirtualMemoryManager();\n@@ -108,1 +77,1 @@\n-  static void test_unreserve() {\n+  void test_unreserve() {\n@@ -110,1 +79,6 @@\n-    zoffset top    = _va->alloc_high_address(ZGranuleSize);\n+    zoffset middle = _va->alloc_low_address(ZGranuleSize);\n+    zoffset top    = _va->alloc_low_address(ZGranuleSize);\n+\n+    ASSERT_EQ(bottom, zoffset(0));\n+    ASSERT_EQ(middle, bottom + 1 * ZGranuleSize);\n+    ASSERT_EQ(top,    bottom + 2 * ZGranuleSize);\n@@ -113,1 +87,1 @@\n-    ZMapper::unreserve(ZOffset::address_unsafe(bottom + ZGranuleSize), ZGranuleSize);\n+    ZMapper::unreserve(ZOffset::address_unsafe(middle), ZGranuleSize);\n@@ -118,68 +92,0 @@\n-\n-    _has_unreserved = true;\n-  }\n-\n-  static void test_alloc_low_address() {\n-    \/\/ Verify that we get placeholder for first granule\n-    zoffset bottom = _va->alloc_low_address(ZGranuleSize);\n-    EXPECT_ALLOC_OK(bottom);\n-\n-    _va->free(bottom, ZGranuleSize);\n-\n-    \/\/ Alloc something larger than a granule and free it\n-    bottom = _va->alloc_low_address(ZGranuleSize * 3);\n-    EXPECT_ALLOC_OK(bottom);\n-\n-    _va->free(bottom, ZGranuleSize * 3);\n-\n-    \/\/ Free with more memory allocated\n-    bottom = _va->alloc_low_address(ZGranuleSize);\n-    EXPECT_ALLOC_OK(bottom);\n-\n-    zoffset next = _va->alloc_low_address(ZGranuleSize);\n-    EXPECT_ALLOC_OK(next);\n-\n-    _va->free(bottom, ZGranuleSize);\n-    _va->free(next, ZGranuleSize);\n-  }\n-\n-  static void test_alloc_high_address() {\n-    \/\/ Verify that we get placeholder for last granule\n-    zoffset high = _va->alloc_high_address(ZGranuleSize);\n-    EXPECT_ALLOC_OK(high);\n-\n-    zoffset prev = _va->alloc_high_address(ZGranuleSize);\n-    EXPECT_ALLOC_OK(prev);\n-\n-    _va->free(high, ZGranuleSize);\n-    _va->free(prev, ZGranuleSize);\n-\n-    \/\/ Alloc something larger than a granule and return it\n-    high = _va->alloc_high_address(ZGranuleSize * 2);\n-    EXPECT_ALLOC_OK(high);\n-\n-    _va->free(high, ZGranuleSize * 2);\n-  }\n-\n-  static void test_alloc_whole_area() {\n-    \/\/ Alloc the whole reservation\n-    zoffset bottom = _va->alloc_low_address(ZMapperTestReservationSize);\n-    EXPECT_ALLOC_OK(bottom);\n-\n-    \/\/ Free two chunks and then allocate them again\n-    _va->free(bottom, ZGranuleSize * 4);\n-    _va->free(bottom + ZGranuleSize * 6, ZGranuleSize * 6);\n-\n-    zoffset offset = _va->alloc_low_address(ZGranuleSize * 4);\n-    EXPECT_ALLOC_OK(offset);\n-\n-    offset = _va->alloc_low_address(ZGranuleSize * 6);\n-    EXPECT_ALLOC_OK(offset);\n-\n-    \/\/ Now free it all, and verify it can be re-allocated\n-    _va->free(bottom, ZMapperTestReservationSize);\n-\n-    bottom = _va->alloc_low_address(ZMapperTestReservationSize);\n-    EXPECT_ALLOC_OK(bottom);\n-\n-    _va->free(bottom, ZMapperTestReservationSize);\n@@ -189,5 +95,0 @@\n-bool ZMapperTest::_initialized   = false;\n-ZMemoryManager* ZMapperTest::_va = nullptr;\n-ZVirtualMemoryManager* ZMapperTest::_vmm = nullptr;\n-bool ZMapperTest::_has_unreserved;\n-\n@@ -198,12 +99,0 @@\n-TEST_VM_F(ZMapperTest, test_alloc_low_address) {\n-  test_alloc_low_address();\n-}\n-\n-TEST_VM_F(ZMapperTest, test_alloc_high_address) {\n-  test_alloc_high_address();\n-}\n-\n-TEST_VM_F(ZMapperTest, test_alloc_whole_area) {\n-  test_alloc_whole_area();\n-}\n-\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zMapper_windows.cpp","additions":21,"deletions":132,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -26,19 +26,1 @@\n-#include \"unittest.hpp\"\n-\n-class ZAddressOffsetMaxSetter {\n-private:\n-  const size_t _old_max;\n-  const size_t _old_mask;\n-\n-public:\n-  ZAddressOffsetMaxSetter()\n-    : _old_max(ZAddressOffsetMax),\n-      _old_mask(ZAddressOffsetMask) {\n-    ZAddressOffsetMax = size_t(16) * G * 1024;\n-    ZAddressOffsetMask = ZAddressOffsetMax - 1;\n-  }\n-  ~ZAddressOffsetMaxSetter() {\n-    ZAddressOffsetMax = _old_max;\n-    ZAddressOffsetMask = _old_mask;\n-  }\n-};\n+#include \"zunittest.hpp\"\n@@ -47,1 +29,1 @@\n-  ZAddressOffsetMaxSetter setter;\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n@@ -77,1 +59,1 @@\n-  ZAddressOffsetMaxSetter setter;\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zMemory.cpp","additions":3,"deletions":21,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"unittest.hpp\"\n+#include \"zunittest.hpp\"\n@@ -28,0 +28,2 @@\n+  ZAddressOffsetMaxSetter setter(size_t(16) * G * 1024);\n+\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zVirtualMemory.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,269 @@\n+\/*\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#include \"gc\/z\/zArguments.hpp\"\n+#include \"gc\/z\/zGlobals.hpp\"\n+#include \"gc\/z\/zInitialize.hpp\"\n+#include \"gc\/z\/zList.inline.hpp\"\n+#include \"gc\/z\/zMemory.inline.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"gc\/z\/zValue.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemory.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"zunittest.hpp\"\n+\n+using namespace testing;\n+\n+#define ASSERT_ALLOC_OK(offset) ASSERT_NE(offset, zoffset(UINTPTR_MAX))\n+\n+class ZCallbacksResetter {\n+private:\n+  ZMemoryManager::Callbacks* _callbacks;\n+  ZMemoryManager::Callbacks  _saved;\n+\n+public:\n+  ZCallbacksResetter(ZMemoryManager::Callbacks* callbacks)\n+    : _callbacks(callbacks),\n+      _saved(*callbacks) {\n+    *_callbacks = {};\n+  }\n+  ~ZCallbacksResetter() {\n+    *_callbacks = _saved;\n+  }\n+};\n+\n+class ZVirtualMemoryManagerTest : public ZTest {\n+private:\n+  static constexpr size_t ReservationSize = 32 * M;\n+\n+  ZMemoryManager*        _va;\n+  ZVirtualMemoryManager* _vmm;\n+\n+public:\n+  virtual void SetUp() {\n+    \/\/ Only run test on supported Windows versions\n+    if (!is_os_supported()) {\n+      GTEST_SKIP() << \"OS not supported\";\n+    }\n+\n+    void* vmr_mem = os::malloc(sizeof(ZVirtualMemoryManager), mtTest);\n+    _vmm = ::new (vmr_mem) ZVirtualMemoryManager(ReservationSize);\n+    _va = &_vmm->_manager;\n+  }\n+\n+  virtual void TearDown() {\n+    if (!is_os_supported()) {\n+      \/\/ Test skipped, nothing to cleanup\n+      return;\n+    }\n+\n+    \/\/ Best-effort cleanup\n+    _vmm->unreserve_all();\n+    _vmm->~ZVirtualMemoryManager();\n+    os::free(_vmm);\n+  }\n+\n+  void test_reserve_discontiguous_and_coalesce() {\n+    \/\/ Start by ensuring that we have 3 unreserved granules, and then let the\n+    \/\/ fourth granule be pre-reserved and therefore blocking subsequent requests\n+    \/\/ to reserve memory.\n+    \/\/\n+    \/\/ +----+----+----+----+\n+    \/\/                -----  pre-reserved - to block contiguous reservation\n+    \/\/ ---------------       unreserved   - to allow reservation of 3 granules\n+    \/\/\n+    \/\/ If we then asks for 4 granules starting at the first granule above,\n+    \/\/ then we won't be able to allocate 4 consecutive granules and the code\n+    \/\/ reverts into the discontiguous mode. This mode uses interval halving\n+    \/\/ to find the limits of memory areas that have already been reserved.\n+    \/\/ This will lead to the first 2 granules being reserved, then the third\n+    \/\/ granule will be reserved.\n+    \/\/\n+    \/\/ The problem we had with this is that this would yield two separate\n+    \/\/ placeholder reservations, even though they are adjacent. The callbacks\n+    \/\/ are supposed to fix that by coalescing the placeholders, *but* the\n+    \/\/ callbacks used to be only turned on *after* the reservation call. So,\n+    \/\/ we end up with one 3 granule large memory area in the manager, which\n+    \/\/ unexpectedly was covered by two placeholders (instead of the expected\n+    \/\/ one placeholder).\n+    \/\/\n+    \/\/ Later when the callbacks had been installed and we tried to fetch memory\n+    \/\/ from the manager, the callbacks would try to split off the placeholder\n+    \/\/ to separate the fetched memory from the memory left in the manager. This\n+    \/\/ used to fail because the memory was already split into two placeholders.\n+\n+    if (_vmm->reserved() < 4 * ZGranuleSize || !_va->free_is_contiguous()) {\n+      GTEST_SKIP() << \"Fixture failed to reserve adequate memory, reserved \"\n+          << (_vmm->reserved() >> ZGranuleSizeShift) << \" * ZGranuleSize\";\n+    }\n+\n+    \/\/ Start at the offset we reserved.\n+    const zoffset base_offset = _vmm->lowest_available_address();\n+\n+    \/\/ Empty the reserved memory in preparation for the rest of the test.\n+    _vmm->unreserve_all();\n+\n+    const zaddress_unsafe base = ZOffset::address_unsafe(base_offset);\n+    const zaddress_unsafe blocked = base + 3 * ZGranuleSize;\n+\n+    \/\/ Reserve the memory that is acting as a blocking reservation.\n+    {\n+      char* const result = os::attempt_reserve_memory_at((char*)untype(blocked), ZGranuleSize, !ExecMem, mtTest);\n+      if (uintptr_t(result) != untype(blocked)) {\n+        GTEST_SKIP() << \"Failed to reserve requested memory at \" << untype(blocked);\n+      }\n+    }\n+\n+    {\n+      \/\/ This ends up reserving 2 granules and then 1 granule adjacent to the\n+      \/\/ first. In previous implementations this resulted in two separate\n+      \/\/ placeholders (4MB and 2MB). This was a bug, because the manager is\n+      \/\/ designed to have one placeholder per memory area. This in turn would\n+      \/\/ lead to a subsequent failure when _vmm->alloc tried to split off the\n+      \/\/ 4MB that is already covered by its own placeholder. You can't place\n+      \/\/ a placeholder over an already existing placeholder.\n+\n+      \/\/ To reproduce this, the test needed to mimic the initializing memory\n+      \/\/ reservation code which had the placeholders turned off. This was done\n+      \/\/ with this helper:\n+      \/\/\n+      \/\/ ZCallbacksResetter resetter(&_va->_callbacks);\n+      \/\/\n+      \/\/ After the fix, we always have the callbacks turned on, so we don't\n+      \/\/ need this to mimic the initializing memory reservation.\n+\n+      const size_t reserved = _vmm->reserve_discontiguous(base_offset, 4 * ZGranuleSize, ZGranuleSize);\n+      ASSERT_LE(reserved, 3 * ZGranuleSize);\n+      if (reserved < 3 * ZGranuleSize) {\n+        GTEST_SKIP() << \"Failed reserve_discontiguous\"\n+            \", expected 3 * ZGranuleSize, got \" << (reserved >> ZGranuleSizeShift)\n+            << \" * ZGranuleSize\";\n+      }\n+    }\n+\n+    {\n+      \/\/ The test used to crash here because the 3 granule memory area was\n+      \/\/ inadvertently covered by two place holders (2 granules + 1 granule).\n+      const ZVirtualMemory vmem = _vmm->alloc(2 * ZGranuleSize, true);\n+      ASSERT_EQ(vmem.start(), base_offset);\n+      ASSERT_EQ(vmem.size(), 2 * ZGranuleSize);\n+\n+      \/\/ Cleanup - Must happen in granule-sizes because of how Windows hands\n+      \/\/ out memory in granule-sized placeholder reservations.\n+      _vmm->unreserve(base_offset, ZGranuleSize);\n+      _vmm->unreserve(base_offset + ZGranuleSize, ZGranuleSize);\n+    }\n+\n+    \/\/ Final cleanup\n+    const ZVirtualMemory vmem = _vmm->alloc(ZGranuleSize, true);\n+    ASSERT_EQ(vmem.start(), base_offset + 2 * ZGranuleSize);\n+    ASSERT_EQ(vmem.size(), ZGranuleSize);\n+    _vmm->unreserve(vmem.start(), vmem.size());\n+\n+    const bool released = os::release_memory((char*)untype(blocked), ZGranuleSize);\n+    ASSERT_TRUE(released);\n+  }\n+\n+  void test_alloc_low_address() {\n+    \/\/ Verify that we get a placeholder for the first granule\n+    zoffset bottom = _va->alloc_low_address(ZGranuleSize);\n+    ASSERT_ALLOC_OK(bottom);\n+\n+    _va->free(bottom, ZGranuleSize);\n+\n+    \/\/ Alloc something larger than a granule and free it\n+    bottom = _va->alloc_low_address(ZGranuleSize * 3);\n+    ASSERT_ALLOC_OK(bottom);\n+\n+    _va->free(bottom, ZGranuleSize * 3);\n+\n+    \/\/ Free with more memory allocated\n+    bottom = _va->alloc_low_address(ZGranuleSize);\n+    ASSERT_ALLOC_OK(bottom);\n+\n+    zoffset next = _va->alloc_low_address(ZGranuleSize);\n+    ASSERT_ALLOC_OK(next);\n+\n+    _va->free(bottom, ZGranuleSize);\n+    _va->free(next, ZGranuleSize);\n+  }\n+\n+  void test_alloc_high_address() {\n+    \/\/ Verify that we get a placeholder for the last granule\n+    zoffset high = _va->alloc_high_address(ZGranuleSize);\n+    ASSERT_ALLOC_OK(high);\n+\n+    zoffset prev = _va->alloc_high_address(ZGranuleSize);\n+    ASSERT_ALLOC_OK(prev);\n+\n+    _va->free(high, ZGranuleSize);\n+    _va->free(prev, ZGranuleSize);\n+\n+    \/\/ Alloc something larger than a granule and return it\n+    high = _va->alloc_high_address(ZGranuleSize * 2);\n+    ASSERT_ALLOC_OK(high);\n+\n+    _va->free(high, ZGranuleSize * 2);\n+  }\n+\n+  void test_alloc_whole_area() {\n+    \/\/ Alloc the whole reservation\n+    zoffset bottom = _va->alloc_low_address(ReservationSize);\n+    ASSERT_ALLOC_OK(bottom);\n+\n+    \/\/ Free two chunks and then allocate them again\n+    _va->free(bottom, ZGranuleSize * 4);\n+    _va->free(bottom + ZGranuleSize * 6, ZGranuleSize * 6);\n+\n+    zoffset offset = _va->alloc_low_address(ZGranuleSize * 4);\n+    ASSERT_ALLOC_OK(offset);\n+\n+    offset = _va->alloc_low_address(ZGranuleSize * 6);\n+    ASSERT_ALLOC_OK(offset);\n+\n+    \/\/ Now free it all, and verify it can be re-allocated\n+    _va->free(bottom, ReservationSize);\n+\n+    bottom = _va->alloc_low_address(ReservationSize);\n+    ASSERT_ALLOC_OK(bottom);\n+\n+    _va->free(bottom, ReservationSize);\n+  }\n+};\n+\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_reserve_discontiguous_and_coalesce) {\n+  test_reserve_discontiguous_and_coalesce();\n+}\n+\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_alloc_low_address) {\n+  test_alloc_low_address();\n+}\n+\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_alloc_high_address) {\n+  test_alloc_high_address();\n+}\n+\n+TEST_VM_F(ZVirtualMemoryManagerTest, test_alloc_whole_area) {\n+  test_alloc_whole_area();\n+}\n","filename":"test\/hotspot\/gtest\/gc\/z\/test_zVirtualMemoryManager.cpp","additions":269,"deletions":0,"binary":false,"changes":269,"status":"added"},{"patch":"@@ -0,0 +1,82 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef ZUNITTEST_HPP\n+#define ZUNITTEST_HPP\n+\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zArguments.hpp\"\n+#include \"gc\/z\/zInitialize.hpp\"\n+#include \"unittest.hpp\"\n+\n+class ZAddressOffsetMaxSetter {\n+  friend class ZTest;\n+\n+private:\n+  size_t _old_max;\n+  size_t _old_mask;\n+\n+public:\n+  ZAddressOffsetMaxSetter(size_t zaddress_offset_max)\n+    : _old_max(ZAddressOffsetMax),\n+      _old_mask(ZAddressOffsetMask) {\n+    ZAddressOffsetMax = zaddress_offset_max;\n+    ZAddressOffsetMask = ZAddressOffsetMax - 1;\n+  }\n+  ~ZAddressOffsetMaxSetter() {\n+    ZAddressOffsetMax = _old_max;\n+    ZAddressOffsetMask = _old_mask;\n+  }\n+};\n+\n+class ZTest : public testing::Test {\n+private:\n+  ZAddressOffsetMaxSetter _zaddress_offset_max_setter;\n+\n+protected:\n+  ZTest()\n+    : _zaddress_offset_max_setter(ZAddressOffsetMax) {\n+    if (!is_os_supported()) {\n+      \/\/ If the OS does not support ZGC do not run initialization, as it may crash the VM.\n+      return;\n+    }\n+\n+    \/\/ Initialize ZGC subsystems for gtests, may only be called once per process.\n+    static bool runs_once = [&]() {\n+      ZInitialize::pd_initialize();\n+      ZGlobalsPointers::initialize();\n+\n+      \/\/ ZGlobalsPointers::initialize() sets ZAddressOffsetMax, make sure the\n+      \/\/ first test fixture invocation has a correct ZAddressOffsetMaxSetter.\n+      _zaddress_offset_max_setter._old_max = ZAddressOffsetMax;\n+      _zaddress_offset_max_setter._old_mask = ZAddressOffsetMask;\n+      return true;\n+    }();\n+  }\n+\n+  bool is_os_supported() {\n+    return ZArguments::is_os_supported();\n+  }\n+};\n+\n+#endif \/\/ ZUNITTEST_HPP\n","filename":"test\/hotspot\/gtest\/gc\/z\/zunittest.hpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"added"}]}