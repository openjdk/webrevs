{"files":[{"patch":"@@ -271,0 +271,8 @@\n+  \/\/ Generate 'unsafe' set memory stub\n+  \/\/ Though just as safe as the other stubs, it takes an unscaled\n+  \/\/ size_t argument instead of an element count.\n+  \/\/\n+  \/\/ Examines the alignment of the operands and dispatches\n+  \/\/ to an int, short, or byte copy loop.\n+  address generate_unsafe_setmemory(const char *name, address byte_fill_entry);\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -155,0 +155,3 @@\n+  StubRoutines::_unsafe_setmemory = generate_unsafe_setmemory(\n+      \"unsafe_setmemory\", StubRoutines::_arrayof_jbyte_fill);\n+\n@@ -2479,0 +2482,269 @@\n+\/\/  Generate 'unsafe' set memory stub\n+\/\/  Though just as safe as the other stubs, it takes an unscaled\n+\/\/  size_t argument instead of an element count.\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - destination array address\n+\/\/    c_rarg1   - byte count (size_t)\n+\/\/    c_rarg2   - byte value\n+\/\/\n+\/\/ Examines the alignment of the operands and dispatches\n+\/\/ to an int, short, or byte fill loop.\n+\/\/\n+address StubGenerator::generate_unsafe_setmemory(const char *name,\n+                                                 address byte_fill_entry) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_unsafe_set_memory_ctr, rscratch1);\n+\n+  {\n+    Label L_exit, L_fillQuadwords, L_fillDwords, L_fillBytes;\n+\n+    setup_arg_regs(3);\n+#undef dest\n+#define dest rdi\n+#undef size\n+#define size rsi\n+#undef wide_value\n+#define wide_value rax\n+#undef rScratch1\n+#define rScratch1 rcx\n+#undef byteVal\n+#define byteVal rdx\n+#undef rScratch3\n+#define rScratch3 r8\n+#undef rScratch4\n+#define rScratch4 r11\n+\n+    \/\/     fill_to_memory_atomic(unsigned char*, unsigned long, unsigned char)\n+\n+    __ testq(size, size);\n+    __ jcc(Assembler::zero, L_exit);\n+\n+    \/\/ Propagate byte to full register\n+    __ movq(rScratch1, dest);\n+    __ orq(rScratch1, size);\n+    __ movzbl(rScratch3, byteVal);\n+    __ mov64(wide_value, 0x0101010101010101);\n+    __ imulq(wide_value, rScratch3);\n+\n+#undef byteVal\n+#define rScratch2 rdx\n+    __ testb(rScratch1, 7);\n+    __ jcc(Assembler::equal, L_fillQuadwords);\n+\n+    __ testb(rScratch1, 3);\n+    __ jcc(Assembler::equal, L_fillDwords);\n+\n+    __ testb(rScratch1, 1);\n+    __ jcc(Assembler::notEqual, L_fillBytes);\n+\n+    \/\/ Fill words\n+    {\n+      Label L_wordsTail, L_wordsLoop, L_wordsTailLoop;\n+      \/\/\/\/\/\/  Set words\n+      __ leaq(rScratch2, Address(size, 1));\n+      __ movq(rScratch1, rScratch2);\n+      __ shrq(rScratch1, 4);\n+      __ cmpq(rScratch2, 16);\n+      __ jccb(Assembler::below, L_wordsTail);\n+      __ leaq(rScratch3, Address(dest, 14));\n+      __ movq(rScratch4, rScratch1);\n+\n+      __ BIND(L_wordsLoop);\n+\n+      \/\/ Unroll 8 word stores\n+      for (int i = 7; i >= 0; i--) {\n+        __ movw(Address(rScratch3, -(2 * i)), wide_value);\n+      }\n+\n+      __ addq(rScratch3, 16);\n+      __ decrementq(rScratch4);\n+      __ jccb(Assembler::notEqual, L_wordsLoop);\n+\n+      __ BIND(L_wordsTail);\n+\n+      \/\/ Handle leftovers\n+      __ shlq(rScratch1, 3);\n+      __ shrq(rScratch2, 1);\n+      __ cmpq(rScratch1, rScratch2);\n+      __ jcc(Assembler::aboveEqual, L_exit);\n+      __ decrementq(size);\n+      __ shrq(size, 1);\n+      __ incrementq(size);\n+\n+      __ BIND(L_wordsTailLoop);\n+\n+      __ movw(Address(dest, rScratch1, Address::times_2), wide_value);\n+      __ incrementq(rScratch1);\n+      __ cmpq(size, rScratch1);\n+      __ jccb(Assembler::notEqual, L_wordsTailLoop);\n+      __ jmp(L_exit);\n+    }\n+\n+    __ BIND(L_fillQuadwords);\n+\n+    \/\/ Fill QUADWORDs\n+    {\n+      Label L_qwordLoop, L_qwordsTail, L_qwordsTailLoop;\n+\n+      __ leaq(rScratch2, Address(size, 7));\n+      __ movq(rScratch1, rScratch2);\n+      __ shrq(rScratch1, 6);\n+      __ cmpq(rScratch2, 64);\n+      __ jccb(Assembler::below, L_qwordsTail);\n+      __ leaq(rScratch3, Address(dest, 56));\n+      __ movq(rScratch4, rScratch1);\n+\n+      __ BIND(L_qwordLoop);\n+\n+      \/\/ Unroll 8 qword stores\n+      for (int i = 7; i >= 0; i--) {\n+        __ movq(Address(rScratch3, -(8 * i)), wide_value);\n+      }\n+      __ addq(rScratch3, 64);\n+      __ decrementq(rScratch4);\n+      __ jccb(Assembler::notZero, L_qwordLoop);\n+\n+      __ BIND(L_qwordsTail);\n+\n+      \/\/ Handle leftovers\n+      __ shlq(rScratch1, 3);\n+      __ shrq(rScratch2, 3);\n+      __ cmpq(rScratch1, rScratch2);\n+      __ jcc(Assembler::aboveEqual, L_exit);\n+      __ decrementq(size);\n+      __ shrq(size, 3);\n+      __ incrementq(size);\n+\n+      __ BIND(L_qwordsTailLoop);\n+\n+      __ movq(Address(dest, rScratch1, Address::times_8), wide_value);\n+      __ incrementq(rScratch1);\n+      __ cmpq(size, rScratch1);\n+      __ jccb(Assembler::notEqual, L_qwordsTailLoop);\n+      __ jmp(L_exit);\n+    }\n+\n+    __ BIND(L_fillDwords);\n+\n+    \/\/ Fill DWORDs\n+    {\n+      Label L_dwordLoop, L_dwordsTail, L_dwordsTailLoop;\n+\n+      __ leaq(rScratch2, Address(size, 3));\n+      __ movq(rScratch1, rScratch2);\n+      __ shrq(rScratch1, 5);\n+      __ cmpq(rScratch2, 32);\n+      __ jccb(Assembler::below, L_dwordsTail);\n+      __ leaq(rScratch3, Address(dest, 28));\n+      __ movq(rScratch4, rScratch1);\n+\n+      __ BIND(L_dwordLoop);\n+\n+      \/\/ Unroll 8 dword stores\n+      for (int i = 7; i >= 0; i--) {\n+        __ movl(Address(rScratch3, -(i * 4)), wide_value);\n+      }\n+      __ addq(rScratch3, 32);\n+      __ decrementq(rScratch4);\n+      __ jccb(Assembler::notZero, L_dwordLoop);\n+\n+      __ BIND(L_dwordsTail);\n+\n+#undef rScratch3\n+#undef rScratch4\n+\n+      \/\/ Handle leftovers\n+      __ shlq(rScratch1, 3);\n+      __ shrq(rScratch2, 2);\n+      __ cmpq(rScratch1, rScratch2);\n+      __ jccb(Assembler::aboveEqual, L_exit);\n+      __ decrementq(size);\n+      __ shrq(size, 2);\n+      __ incrementq(size);\n+\n+      __ BIND(L_dwordsTailLoop);\n+\n+      __ movl(Address(dest, rScratch1, Address::times_4), wide_value);\n+      __ incrementq(rScratch1);\n+      __ cmpq(size, rScratch1);\n+      __ jccb(Assembler::notEqual, L_dwordsTailLoop);\n+      __ jmpb(L_exit);\n+    }\n+\n+    __ BIND(L_fillBytes);\n+#ifdef MUSL_LIBC\n+    {\n+      Label L_byteLoop, L_longByteLoop, L_byteTail, L_byteTailLoop;\n+\n+#undef wide_value\n+#define savedSize rax\n+#undef rScratch2\n+#define byteVal rdx\n+\n+      __ movq(savedSize, size);\n+      __ andq(savedSize, 7);\n+      __ cmpq(size, 8);\n+      __ jccb(Assembler::aboveEqual, L_byteLoop);\n+      __ xorl(rScratch1, rScratch1);\n+      __ jmpb(L_byteTail);\n+\n+      __ BIND(L_byteLoop);\n+\n+      __ andq(size, -8);\n+      __ xorl(rScratch1, rScratch1);\n+\n+      __ BIND(L_longByteLoop);\n+\n+      \/\/ Unroll 8 byte stores\n+      for (int i = 0; i < 8; i++) {\n+        __ movb(Address(dest, rScratch1, Address::times_1, i), byteVal);\n+      }\n+\n+      __ addq(rScratch1, 8);\n+      __ cmpq(size, rScratch1);\n+      __ jccb(Assembler::notEqual, L_longByteLoop);\n+\n+      __ BIND(L_byteTail);\n+\n+      __ testq(savedSize, savedSize);\n+      __ jccb(Assembler::zero, L_exit);\n+      __ addq(dest, rScratch1);\n+      __ xorl(rScratch1, rScratch1);\n+\n+      __ BIND(L_byteTailLoop);\n+\n+      __ movb(Address(dest, rScratch1, Address::times_1), byteVal);\n+      __ incrementq(rScratch1);\n+      __ cmpq(savedSize, rScratch1);\n+      __ jccb(Assembler::notEqual, L_byteTailLoop);\n+    }\n+#else  \/\/ MUSL_LIBC\n+#define byteVal rdx\n+    {\n+      \/\/ rax has expanded byte value\n+      __ movq(byteVal, rax);\n+\n+      __ xchgq(size, byteVal);\n+      __ jump_cc(Assembler::notZero, RuntimeAddress(byte_fill_entry));\n+    }\n+#endif  \/\/ MUSL_LIBC\n+    __ BIND(L_exit);\n+\n+    restore_arg_regs();\n+    __ ret(0);\n+\n+#undef dest\n+#undef size\n+#undef savedSize\n+#undef rScratch1\n+#undef byteVal\n+  }\n+\n+  return start;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_arraycopy.cpp","additions":272,"deletions":0,"binary":false,"changes":272,"status":"modified"},{"patch":"@@ -151,0 +151,3 @@\n+    \/\/ Shared code tests for \"null\" to discover the stub is not generated.\n+    StubRoutines::_unsafe_setmemory          = nullptr;\n+\n","filename":"src\/hotspot\/cpu\/zero\/stubGenerator_zero.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -509,0 +509,3 @@\n+  case vmIntrinsics::_setMemory:\n+    if (!InlineUnsafeOps) return true;\n+    break;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -623,0 +623,3 @@\n+  do_intrinsic(_setMemory,                jdk_internal_misc_Unsafe,     setMemory_name,  setMemory_signature,          F_RN)     \\\n+   do_name(     setMemory_name,                                         \"setMemory0\")                                            \\\n+   do_signature(setMemory_signature,                                    \"(Ljava\/lang\/Object;JJB)V\")                              \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -345,0 +345,1 @@\n+  static_field(StubRoutines,                _unsafe_setmemory,                                address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -260,0 +260,3 @@\n+  case vmIntrinsics::_setMemory:\n+    if (StubRoutines::unsafe_setmemory() == nullptr) return false;\n+    break;\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -510,0 +510,1 @@\n+  case vmIntrinsics::_setMemory:                return inline_unsafe_setMemory();\n@@ -4944,0 +4945,51 @@\n+\n+  return true;\n+}\n+\n+bool LibraryCallKit::inline_unsafe_setMemory() {\n+  if (callee()->is_static())  return false;  \/\/ caller must have the capability!\n+  null_check_receiver();  \/\/ null-check receiver\n+  if (stopped())  return true;\n+\n+  C->set_has_unsafe_access(true);  \/\/ Mark eventual nmethod as \"unsafe\".\n+\n+  \/\/ printf(\"In inline_unsafe_setMemory\\n\");\n+\n+  Node* dst_base =         argument(1);  \/\/ type: oop\n+  Node* dst_off  = ConvL2X(argument(2)); \/\/ type: long\n+  Node* size     = ConvL2X(argument(4)); \/\/ type: long\n+  Node* byte     =         argument(6);  \/\/ type: byte\n+\n+  assert(Unsafe_field_offset_to_byte_offset(11) == 11,\n+         \"fieldOffset must be byte-scaled\");\n+\n+  Node* dst_addr = make_unsafe_address(dst_base, dst_off);\n+\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* doing_unsafe_access_addr = basic_plus_adr(top(), thread, in_bytes(JavaThread::doing_unsafe_access_offset()));\n+  BasicType doing_unsafe_access_bt = T_BYTE;\n+  assert((sizeof(bool) * CHAR_BIT) == 8, \"not implemented\");\n+\n+  \/\/ update volatile field\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(1), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n+  int flags = RC_LEAF | RC_NO_FP;\n+\n+  const TypePtr* dst_type = TypePtr::BOTTOM;\n+\n+  \/\/ Adjust memory effects of the runtime call based on input values.\n+  if (!has_wide_mem(_gvn, dst_addr, dst_base)) {\n+    dst_type = _gvn.type(dst_addr)->is_ptr(); \/\/ narrow out memory\n+\n+    flags |= RC_NARROW_MEM; \/\/ narrow in memory\n+  }\n+\n+  \/\/ Call it.  Note that the length argument is not scaled.\n+  make_runtime_call(flags,\n+                    OptoRuntime::make_setmemory_Type(),\n+                    StubRoutines::unsafe_setmemory(),\n+                    \"unsafe_setmemory\",\n+                    dst_type,\n+                    dst_addr, size XTOP, byte);\n+\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(0), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -234,0 +234,1 @@\n+  bool inline_unsafe_setMemory();\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -775,0 +775,20 @@\n+const TypeFunc* OptoRuntime::make_setmemory_Type() {\n+  \/\/ create input type (domain)\n+  int num_args      = 4;\n+  int argcnt = num_args;\n+  const Type** fields = TypeTuple::fields(argcnt);\n+  int argp = TypeFunc::Parms;\n+  fields[argp++] = TypePtr::NOTNULL;    \/\/ dest\n+  fields[argp++] = TypeLong::LONG;      \/\/ size\n+  fields[argp++] = Type::HALF;          \/\/ size\n+  fields[argp++] = TypeInt::INT;        \/\/ bytevalue\n+  assert(argp == TypeFunc::Parms+argcnt, \"correct decoding\");\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+argcnt, fields);\n+\n+  \/\/ no result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+  return TypeFunc::make(domain, range);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -269,0 +269,2 @@\n+  static const TypeFunc* make_setmemory_Type();\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -396,1 +396,6 @@\n-    Copy::fill_to_memory_atomic(p, sz, value);\n+    if (StubRoutines::unsafe_setmemory() != nullptr) {\n+      MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXExec, thread));\n+      StubRoutines::UnsafeSetMemory_stub()(p, sz, value);\n+    } else {\n+      Copy::fill_to_memory_atomic(p, sz, value);\n+    }\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -180,0 +180,2 @@\n+uint SharedRuntime::_unsafe_set_memory_ctr=0;\n+\n@@ -1992,0 +1994,2 @@\n+  if (_unsafe_set_memory_ctr) tty->print_cr(\"%5u unsafe set memorys\", _unsafe_set_memory_ctr);\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -545,0 +545,2 @@\n+  static uint _unsafe_set_memory_ctr;      \/\/ Slow-path includes alignment checks\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -112,0 +112,2 @@\n+address StubRoutines::_unsafe_setmemory                  = nullptr;\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -196,0 +196,2 @@\n+  static address _unsafe_setmemory;\n+\n@@ -384,0 +386,5 @@\n+  static address unsafe_setmemory()     { return _unsafe_setmemory; }\n+\n+  typedef void (*UnsafeSetMemoryStub)(const void* src, size_t count, char byte);\n+  static UnsafeSetMemoryStub UnsafeSetMemory_stub()         { return CAST_TO_FN_PTR(UnsafeSetMemoryStub,  _unsafe_setmemory); }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -211,48 +211,0 @@\n-\n-\/\/ Fill bytes; larger units are filled atomically if everything is aligned.\n-void Copy::fill_to_memory_atomic(void* to, size_t size, jubyte value) {\n-  address dst = (address) to;\n-  uintptr_t bits = (uintptr_t) to | (uintptr_t) size;\n-  if (bits % sizeof(jlong) == 0) {\n-    jlong fill = (julong)( (jubyte)value ); \/\/ zero-extend\n-    if (fill != 0) {\n-      fill += fill << 8;\n-      fill += fill << 16;\n-      fill += fill << 32;\n-    }\n-    \/\/Copy::fill_to_jlongs_atomic((jlong*) dst, size \/ sizeof(jlong));\n-    for (uintptr_t off = 0; off < size; off += sizeof(jlong)) {\n-      *(jlong*)(dst + off) = fill;\n-    }\n-  } else if (bits % sizeof(jint) == 0) {\n-    jint fill = (juint)( (jubyte)value ); \/\/ zero-extend\n-    if (fill != 0) {\n-      fill += fill << 8;\n-      fill += fill << 16;\n-    }\n-    \/\/Copy::fill_to_jints_atomic((jint*) dst, size \/ sizeof(jint));\n-    for (uintptr_t off = 0; off < size; off += sizeof(jint)) {\n-      *(jint*)(dst + off) = fill;\n-    }\n-  } else if (bits % sizeof(jshort) == 0) {\n-    jshort fill = (jushort)( (jubyte)value ); \/\/ zero-extend\n-    fill += (jshort)(fill << 8);\n-    \/\/Copy::fill_to_jshorts_atomic((jshort*) dst, size \/ sizeof(jshort));\n-    for (uintptr_t off = 0; off < size; off += sizeof(jshort)) {\n-      *(jshort*)(dst + off) = fill;\n-    }\n-  } else {\n-    \/\/ Not aligned, so no need to be atomic.\n-#ifdef MUSL_LIBC\n-    \/\/ This code is used by Unsafe and may hit the next page after truncation of mapped memory.\n-    \/\/ Therefore, we use volatile to prevent compilers from replacing the loop by memset which\n-    \/\/ may not trigger SIGBUS as needed (observed on Alpine Linux x86_64)\n-    jbyte fill = value;\n-    for (uintptr_t off = 0; off < size; off += sizeof(jbyte)) {\n-      *(volatile jbyte*)(dst + off) = fill;\n-    }\n-#else\n-    Copy::fill_to_bytes(dst, size, value);\n-#endif\n-  }\n-}\n","filename":"src\/hotspot\/share\/utilities\/copy.cpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -285,1 +285,50 @@\n-  static void fill_to_memory_atomic(void* to, size_t size, jubyte value = 0);\n+\n+  \/\/ Fill bytes; larger units are filled atomically if everything is aligned.\n+  inline static void fill_to_memory_atomic(void* to, size_t size,\n+                                           jubyte value) {\n+    address dst = (address)to;\n+    uintptr_t bits = (uintptr_t)to | (uintptr_t)size;\n+    if (bits % sizeof(jlong) == 0) {\n+      jlong fill = (julong)((jubyte)value);  \/\/ zero-extend\n+      if (fill != 0) {\n+        fill += fill << 8;\n+        fill += fill << 16;\n+        fill += fill << 32;\n+      }\n+      \/\/ Copy::fill_to_jlongs_atomic((jlong*) dst, size \/ sizeof(jlong));\n+      for (uintptr_t off = 0; off < size; off += sizeof(jlong)) {\n+        *(jlong*)(dst + off) = fill;\n+      }\n+    } else if (bits % sizeof(jint) == 0) {\n+      jint fill = (juint)((jubyte)value);  \/\/ zero-extend\n+      if (fill != 0) {\n+        fill += fill << 8;\n+        fill += fill << 16;\n+      }\n+      \/\/ Copy::fill_to_jints_atomic((jint*) dst, size \/ sizeof(jint));\n+      for (uintptr_t off = 0; off < size; off += sizeof(jint)) {\n+        *(jint*)(dst + off) = fill;\n+      }\n+    } else if (bits % sizeof(jshort) == 0) {\n+      jshort fill = (jushort)((jubyte)value);  \/\/ zero-extend\n+      fill += (jshort)(fill << 8);\n+      \/\/ Copy::fill_to_jshorts_atomic((jshort*) dst, size \/ sizeof(jshort));\n+      for (uintptr_t off = 0; off < size; off += sizeof(jshort)) {\n+        *(jshort*)(dst + off) = fill;\n+      }\n+    } else {\n+      \/\/ Not aligned, so no need to be atomic.\n+#ifdef MUSL_LIBC\n+      \/\/ This code is used by Unsafe and may hit the next page after truncation\n+      \/\/ of mapped memory. Therefore, we use volatile to prevent compilers from\n+      \/\/ replacing the loop by memset which may not trigger SIGBUS as needed\n+      \/\/ (observed on Alpine Linux x86_64)\n+      jbyte fill = value;\n+      for (uintptr_t off = 0; off < size; off += sizeof(jbyte)) {\n+        *(volatile jbyte*)(dst + off) = fill;\n+      }\n+#else\n+      Copy::fill_to_bytes(dst, size, value);\n+#endif\n+    }\n+  }\n@@ -303,1 +352,0 @@\n-\n","filename":"src\/hotspot\/share\/utilities\/copy.hpp","additions":50,"deletions":2,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -3827,0 +3827,1 @@\n+    @IntrinsicCandidate\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/misc\/Unsafe.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -210,0 +210,5 @@\n+        \/\/ long seed = 6742745864802755133L;\n+        long seed = random.nextLong();\n+        random.setSeed(seed);\n+        System.out.println(\"Seed set to \"+ seed);\n+\n","filename":"test\/jdk\/sun\/misc\/CopyMemory.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,70 @@\n+package org.openjdk.bench.java.lang.foreign;\n+\n+import sun.misc.Unsafe;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Setup;\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.MemorySegment;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+@BenchmarkMode(Mode.AverageTime)\n+@Warmup(iterations = 5, time = 500, timeUnit = TimeUnit.MILLISECONDS)\n+@Measurement(iterations = 10, time = 500, timeUnit = TimeUnit.MILLISECONDS)\n+@State(org.openjdk.jmh.annotations.Scope.Thread)\n+@OutputTimeUnit(TimeUnit.NANOSECONDS)\n+@Fork(value = 3, jvmArgsAppend = {\"--enable-native-access=ALL-UNNAMED\"})\n+public class MemorySegmentZeroUnsafe {\n+\n+    static final Unsafe UNSAFE = Utils.unsafe;\n+    long src;\n+\n+    @Param({\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"15\", \"16\", \"63\", \"64\", \"255\", \"256\"})\n+    public int size;\n+\n+    @Param({\"true\", \"false\"})\n+    public boolean aligned;\n+\n+    private MemorySegment segment;\n+    private long address;\n+\n+    @Setup\n+    public void setup() throws Throwable {\n+        Arena arena = Arena.global();\n+        long alignment = 1;\n+        \/\/ this complex logic is to ensure that if in the future we decide to batch writes with different\n+        \/\/ batches based on alignment, we would spot it here\n+        if (size == 2 || size == 3) {\n+            alignment = 2;\n+        } else if (size >= 4 && size <= 7) {\n+            alignment = 4;\n+        } else {\n+            alignment = 8;\n+        }\n+        if (aligned) {\n+            segment = arena.allocate(size, alignment);\n+        } else {\n+            \/\/ forcibly misaligned in both address AND size, given that would be the worst case\n+            segment = arena.allocate(size + 1, alignment).asSlice(1);\n+        }\n+        address = segment.address();\n+    }\n+\n+    @Benchmark\n+    public void panama() {\n+        segment.fill((byte) 0);\n+    }\n+\n+    @Benchmark\n+    public void unsafe() {\n+        UNSAFE.setMemory(address, size, (byte) 0);\n+    }\n+}\n","filename":"test\/micro\/org\/openjdk\/bench\/java\/lang\/foreign\/MemorySegmentZeroUnsafe.java","additions":70,"deletions":0,"binary":false,"changes":70,"status":"added"}]}