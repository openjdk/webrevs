{"files":[{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/serial\/markSweep.inline.hpp\"\n@@ -51,1 +52,1 @@\n-#include \"gc\/shared\/space.hpp\"\n+#include \"gc\/shared\/space.inline.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"runtime\/prefetch.inline.hpp\"\n@@ -69,2 +71,4 @@\n-void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+class DeadSpacer : StackObj {\n+  size_t _allowed_deadspace_words;\n+  bool _active;\n+  ContiguousSpace* _space;\n@@ -72,4 +76,16 @@\n-  SerialHeap* gch = SerialHeap::heap();\n-#ifdef ASSERT\n-  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n-    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+public:\n+  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n+    size_t ratio = _space->allowed_dead_ratio();\n+    _active = ratio > 0;\n+\n+    if (_active) {\n+      \/\/ We allow some amount of garbage towards the bottom of the space, so\n+      \/\/ we don't start compacting before there is a significant gain to be made.\n+      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n+      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n+      if ((MarkSweep::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n+        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n+      } else {\n+        _active = false;\n+      }\n+    }\n@@ -77,1 +93,0 @@\n-#endif\n@@ -79,4 +94,4 @@\n-  gch->trace_heap_before_gc(_gc_tracer);\n-\n-  \/\/ Increment the invocation count\n-  _total_invocations++;\n+  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n+    if (!_active) {\n+      return false;\n+    }\n@@ -84,4 +99,18 @@\n-  \/\/ Capture used regions for each generation that will be\n-  \/\/ subject to collection, so that card table adjustments can\n-  \/\/ be made intelligently (see clear \/ invalidate further below).\n-  gch->save_used_regions();\n+    size_t dead_length = pointer_delta(dead_end, dead_start);\n+    if (_allowed_deadspace_words >= dead_length) {\n+      _allowed_deadspace_words -= dead_length;\n+      CollectedHeap::fill_with_object(dead_start, dead_length);\n+      oop obj = cast_to_oop(dead_start);\n+      \/\/ obj->set_mark(obj->mark().set_marked());\n+\n+      assert(dead_length == obj->size(), \"bad filler object size\");\n+      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n+                                        p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n+\n+      return true;\n+    } else {\n+      _active = false;\n+      return false;\n+    }\n+  }\n+};\n@@ -89,1 +118,20 @@\n-  allocate_stacks();\n+\/\/ Implement the \"compaction\" part of the mark-compact GC algorithm.\n+class Compacter {\n+  \/\/ There are four spaces in total, but only the first three can be used after\n+  \/\/ compact. IOW, old and eden\/from must be enough for all live objs\n+  static constexpr uint max_num_spaces = 4;\n+\n+  struct CompactionSpace {\n+    ContiguousSpace* _space;\n+    \/\/ Will be the new top after compaction is complete.\n+    HeapWord* _compaction_top;\n+    \/\/ The first dead word in this contiguous space. It's an optimization to\n+    \/\/ skip large chunk of live objects at the beginning.\n+    HeapWord* _first_dead;\n+\n+    void init(ContiguousSpace* space) {\n+      _space = space;\n+      _compaction_top = space->bottom();\n+      _first_dead = nullptr;\n+    }\n+  };\n@@ -91,1 +139,3 @@\n-  mark_sweep_phase1(clear_all_softrefs);\n+  CompactionSpace _spaces[max_num_spaces];\n+  \/\/ The num of spaces to be compacted, i.e. containing live objs.\n+  uint _num_spaces;\n@@ -93,1 +143,1 @@\n-  mark_sweep_phase2();\n+  uint _index;\n@@ -95,5 +145,3 @@\n-  \/\/ Don't add any more derived pointers during phase3\n-#if COMPILER2_OR_JVMCI\n-  assert(DerivedPointerTable::is_active(), \"Sanity\");\n-  DerivedPointerTable::set_active(false);\n-#endif\n+  HeapWord* get_compaction_top(uint index) const {\n+    return _spaces[index]._compaction_top;\n+  }\n@@ -101,1 +149,3 @@\n-  mark_sweep_phase3();\n+  HeapWord* get_first_dead(uint index) const {\n+    return _spaces[index]._first_dead;\n+  }\n@@ -103,1 +153,3 @@\n-  mark_sweep_phase4();\n+  ContiguousSpace* get_space(uint index) const {\n+    return _spaces[index]._space;\n+  }\n@@ -105,1 +157,4 @@\n-  restore_marks();\n+  void record_first_dead(uint index, HeapWord* first_dead) {\n+    assert(_spaces[index]._first_dead == nullptr, \"should write only once\");\n+    _spaces[index]._first_dead = first_dead;\n+  }\n@@ -107,3 +162,18 @@\n-  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n-  \/\/ (Should this be in general part?)\n-  gch->save_marks();\n+  HeapWord* alloc(size_t words) {\n+    while (true) {\n+      if (words <= pointer_delta(_spaces[_index]._space->end(),\n+                                 _spaces[_index]._compaction_top)) {\n+        HeapWord* result = _spaces[_index]._compaction_top;\n+        _spaces[_index]._compaction_top += words;\n+        if (_index == 0) {\n+          \/\/ old-gen requires BOT update\n+          static_cast<TenuredSpace*>(_spaces[0]._space)->update_for_block(result, result + words);\n+        }\n+        return result;\n+      }\n+\n+      \/\/ out-of-memory in this space\n+      _index++;\n+      assert(_index < max_num_spaces - 1, \"the last space should not be used\");\n+    }\n+  }\n@@ -111,1 +181,5 @@\n-  deallocate_stacks();\n+  static void prefetch_read_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::read(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n@@ -113,1 +187,5 @@\n-  MarkSweep::_string_dedup_requests->flush();\n+  static void prefetch_write_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n@@ -115,2 +193,5 @@\n-  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n-  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+  static void prefetch_write_copy(void* p) {\n+    if (PrefetchCopyIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchCopyIntervalInBytes);\n+    }\n+  }\n@@ -118,1 +199,10 @@\n-  gch->prune_scavengable_nmethods();\n+  static void forward_obj(oop obj, HeapWord* new_addr) {\n+    prefetch_write_scan(obj);\n+    if (cast_from_oop<HeapWord*>(obj) != new_addr) {\n+      obj->forward_to(cast_to_oop(new_addr));\n+    } else {\n+      assert(obj->is_gc_marked(), \"inv\");\n+      \/\/ This obj will stay in-place. Fix the markword.\n+      obj->init_mark();\n+    }\n+  }\n@@ -120,3 +210,11 @@\n-  \/\/ Update heap occupancy information which is used as\n-  \/\/ input to soft ref clearing policy at the next gc.\n-  Universe::heap()->update_capacity_and_used_at_gc();\n+  static HeapWord* find_next_live_addr(HeapWord* start, HeapWord* end) {\n+    for (HeapWord* i_addr = start; i_addr < end; \/* empty *\/) {\n+      prefetch_read_scan(i_addr);\n+      oop obj = cast_to_oop(i_addr);\n+      if (obj->is_gc_marked()) {\n+        return i_addr;\n+      }\n+      i_addr += obj->size();\n+    }\n+    return end;\n+  };\n@@ -124,2 +222,3 @@\n-  \/\/ Signal that we have completed a visit to all live objects.\n-  Universe::heap()->record_whole_heap_examined_timestamp();\n+  static size_t relocate(HeapWord* addr) {\n+    \/\/ Prefetch source and destination\n+    prefetch_read_scan(addr);\n@@ -127,2 +226,5 @@\n-  gch->trace_heap_after_gc(_gc_tracer);\n-}\n+    oop obj = cast_to_oop(addr);\n+    oop new_obj = obj->forwardee();\n+    HeapWord* new_addr = cast_from_oop<HeapWord*>(new_obj);\n+    assert(addr != new_addr, \"inv\");\n+    prefetch_write_copy(new_addr);\n@@ -130,5 +232,3 @@\n-void GenMarkSweep::allocate_stacks() {\n-  void* scratch = nullptr;\n-  size_t num_words;\n-  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-  young_gen->contribute_scratch(scratch, num_words);\n+    size_t obj_size = obj->size();\n+    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    new_obj->init_mark();\n@@ -136,4 +236,1 @@\n-  if (scratch != nullptr) {\n-    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n-  } else {\n-    _preserved_count_max = 0;\n+    return obj_size;\n@@ -142,5 +239,16 @@\n-  _preserved_marks = (PreservedMark*)scratch;\n-  _preserved_count = 0;\n-\n-  _preserved_overflow_stack_set.init(1);\n-}\n+public:\n+  explicit Compacter(SerialHeap* heap) {\n+    \/\/ In this order so that heap is compacted towards old-gen.\n+    _spaces[0].init(heap->old_gen()->space());\n+    _spaces[1].init(heap->young_gen()->eden());\n+    _spaces[2].init(heap->young_gen()->from());\n+\n+    bool is_promotion_failed = (heap->young_gen()->from()->next_compaction_space() != nullptr);\n+    if (is_promotion_failed) {\n+      _spaces[3].init(heap->young_gen()->to());\n+      _num_spaces = 4;\n+    } else {\n+      _num_spaces = 3;\n+    }\n+    _index = 0;\n+  }\n@@ -148,0 +256,39 @@\n+  void phase2_calculate_new_addr() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      bool record_first_dead_done = false;\n+\n+      DeadSpacer dead_spacer(space);\n+\n+      while (cur_addr < top) {\n+        oop obj = cast_to_oop(cur_addr);\n+        size_t obj_size = obj->size();\n+        if (obj->is_gc_marked()) {\n+          HeapWord* new_addr = alloc(obj_size);\n+          forward_obj(obj, new_addr);\n+          cur_addr += obj_size;\n+        } else {\n+          \/\/ Skipping the current known-unmarked obj\n+          HeapWord* next_live_addr = find_next_live_addr(cur_addr + obj_size, top);\n+          if (dead_spacer.insert_deadspace(cur_addr, next_live_addr)) {\n+            \/\/ Register space for the filler obj\n+            alloc(pointer_delta(next_live_addr, cur_addr));\n+          } else {\n+            if (!record_first_dead_done) {\n+              record_first_dead(i, cur_addr);\n+              record_first_dead_done = true;\n+            }\n+            *(HeapWord**)cur_addr = next_live_addr;\n+          }\n+          cur_addr = next_live_addr;\n+        }\n+      }\n+\n+      if (!record_first_dead_done) {\n+        record_first_dead(i, top);\n+      }\n+    }\n+  }\n@@ -149,4 +296,18 @@\n-void GenMarkSweep::deallocate_stacks() {\n-  if (_preserved_count_max != 0) {\n-    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-    young_gen->reset_scratch();\n+  void phase3_adjust_pointers() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* const top = space->top();\n+      HeapWord* const first_dead = get_first_dead(i);\n+\n+      while (cur_addr < top) {\n+        prefetch_write_scan(cur_addr);\n+        if (cur_addr < first_dead || cast_to_oop(cur_addr)->is_gc_marked()) {\n+          size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_addr));\n+          cur_addr += size;\n+        } else {\n+          assert(*(HeapWord**)cur_addr > cur_addr, \"forward progress\");\n+          cur_addr = *(HeapWord**)cur_addr;\n+        }\n+      }\n+    }\n@@ -155,4 +316,28 @@\n-  _preserved_overflow_stack_set.reclaim();\n-  _marking_stack.clear();\n-  _objarray_stack.clear(true);\n-}\n+  void phase4_compact() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      \/\/ Check if the first obj inside this space is forwarded.\n+      if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+        \/\/ Jump over consecutive (in-place) live-objs-chunk\n+        cur_addr = get_first_dead(i);\n+      }\n+\n+      while (cur_addr < top) {\n+        if (!cast_to_oop(cur_addr)->is_forwarded()) {\n+          cur_addr = *(HeapWord**) cur_addr;\n+          continue;\n+        }\n+        cur_addr += relocate(cur_addr);\n+      }\n+\n+      \/\/ Reset top and unused memory\n+      space->set_top(get_compaction_top(i));\n+      if (ZapUnusedHeapArea) {\n+        space->mangle_unused_area();\n+      }\n+    }\n+  }\n+};\n@@ -160,1 +345,1 @@\n-void GenMarkSweep::mark_sweep_phase1(bool clear_all_softrefs) {\n+void GenMarkSweep::phase1_mark(bool clear_all_softrefs) {\n@@ -244,0 +429,2 @@\n+void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n@@ -245,3 +432,6 @@\n-void GenMarkSweep::mark_sweep_phase2() {\n-  \/\/ Now all live objects are marked, compute the new object addresses.\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n+  SerialHeap* gch = SerialHeap::heap();\n+#ifdef ASSERT\n+  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n+    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+  }\n+#endif\n@@ -249,2 +439,1 @@\n-  SerialHeap::heap()->prepare_for_compaction();\n-}\n+  gch->trace_heap_before_gc(_gc_tracer);\n@@ -252,4 +441,19 @@\n-class GenAdjustPointersClosure: public SerialHeap::GenClosure {\n-public:\n-  void do_generation(Generation* gen) {\n-    gen->adjust_pointers();\n+  \/\/ Increment the invocation count\n+  _total_invocations++;\n+\n+  \/\/ Capture used regions for each generation that will be\n+  \/\/ subject to collection, so that card table adjustments can\n+  \/\/ be made intelligently (see clear \/ invalidate further below).\n+  gch->save_used_regions();\n+\n+  allocate_stacks();\n+\n+  phase1_mark(clear_all_softrefs);\n+\n+  Compacter compacter{gch};\n+\n+  {\n+    \/\/ Now all live objects are marked, compute the new object addresses.\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n+\n+    compacter.phase2_calculate_new_addr();\n@@ -257,1 +461,0 @@\n-};\n@@ -259,2 +462,18 @@\n-void GenMarkSweep::mark_sweep_phase3() {\n-  SerialHeap* gch = SerialHeap::heap();\n+  \/\/ Don't add any more derived pointers during phase3\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_active(), \"Sanity\");\n+  DerivedPointerTable::set_active(false);\n+#endif\n+\n+  {\n+    \/\/ Adjust the pointers to reflect the new locations\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n@@ -262,2 +481,1 @@\n-  \/\/ Adjust the pointers to reflect the new locations\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n+    WeakProcessor::oops_do(&adjust_pointer_closure);\n@@ -265,1 +483,3 @@\n-  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    adjust_marks();\n+    compacter.phase3_adjust_pointers();\n+  }\n@@ -267,6 +487,3 @@\n-  CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-  gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                     &adjust_pointer_closure,\n-                     &adjust_cld_closure,\n-                     &adjust_cld_closure,\n-                     &code_closure);\n+  {\n+    \/\/ All pointers are now adjusted, move objects accordingly\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n@@ -274,1 +491,2 @@\n-  gch->gen_process_weak_roots(&adjust_pointer_closure);\n+    compacter.phase4_compact();\n+  }\n@@ -276,3 +494,23 @@\n-  adjust_marks();\n-  GenAdjustPointersClosure blk;\n-  gch->generation_iterate(&blk, true);\n+  restore_marks();\n+\n+  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n+  \/\/ (Should this be in general part?)\n+  gch->save_marks();\n+\n+  deallocate_stacks();\n+\n+  MarkSweep::_string_dedup_requests->flush();\n+\n+  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n+  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+\n+  gch->prune_scavengable_nmethods();\n+\n+  \/\/ Update heap occupancy information which is used as\n+  \/\/ input to soft ref clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+\n+  gch->trace_heap_after_gc(_gc_tracer);\n@@ -281,4 +519,10 @@\n-class GenCompactClosure: public SerialHeap::GenClosure {\n-public:\n-  void do_generation(Generation* gen) {\n-    gen->compact();\n+void GenMarkSweep::allocate_stacks() {\n+  void* scratch = nullptr;\n+  size_t num_words;\n+  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+  young_gen->contribute_scratch(scratch, num_words);\n+\n+  if (scratch != nullptr) {\n+    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n+  } else {\n+    _preserved_count_max = 0;\n@@ -286,1 +530,0 @@\n-};\n@@ -288,3 +531,5 @@\n-void GenMarkSweep::mark_sweep_phase4() {\n-  \/\/ All pointers are now adjusted, move objects accordingly\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n+  _preserved_marks = (PreservedMark*)scratch;\n+  _preserved_count = 0;\n+\n+  _preserved_overflow_stack_set.init(1);\n+}\n@@ -292,2 +537,9 @@\n-  GenCompactClosure blk;\n-  SerialHeap::heap()->generation_iterate(&blk, true);\n+void GenMarkSweep::deallocate_stacks() {\n+  if (_preserved_count_max != 0) {\n+    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+    young_gen->reset_scratch();\n+  }\n+\n+  _preserved_overflow_stack_set.reclaim();\n+  _marking_stack.clear();\n+  _objarray_stack.clear(true);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":352,"deletions":100,"binary":false,"changes":452,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-\n@@ -37,7 +36,1 @@\n-  static void mark_sweep_phase1(bool clear_all_softrefs);\n-  \/\/ Calculate new addresses\n-  static void mark_sweep_phase2();\n-  \/\/ Update pointers\n-  static void mark_sweep_phase3();\n-  \/\/ Move objects to new positions\n-  static void mark_sweep_phase4();\n+  static void phase1_mark(bool clear_all_softrefs);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -221,31 +221,0 @@\n-\n-void Generation::prepare_for_compaction(CompactPoint* cp) {\n-  \/\/ Generic implementation, can be specialized\n-  ContiguousSpace* space = first_compaction_space();\n-  while (space != nullptr) {\n-    space->prepare_for_compaction(cp);\n-    space = space->next_compaction_space();\n-  }\n-}\n-\n-class AdjustPointersClosure: public SpaceClosure {\n- public:\n-  void do_space(Space* sp) {\n-    sp->adjust_pointers();\n-  }\n-};\n-\n-void Generation::adjust_pointers() {\n-  \/\/ Note that this is done over all spaces, not just the compactible\n-  \/\/ ones.\n-  AdjustPointersClosure blk;\n-  space_iterate(&blk, true);\n-}\n-\n-void Generation::compact() {\n-  ContiguousSpace* sp = first_compaction_space();\n-  while (sp != nullptr) {\n-    sp->compact();\n-    sp = sp->next_compaction_space();\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/serial\/generation.cpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-class CompactPoint;\n+\n@@ -289,7 +289,0 @@\n-  \/\/ Mark sweep support phase2\n-  virtual void prepare_for_compaction(CompactPoint* cp);\n-  \/\/ Mark sweep support phase3\n-  virtual void adjust_pointers();\n-  \/\/ Mark sweep support phase4\n-  virtual void compact();\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/generation.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -73,2 +73,0 @@\n-  \/\/ Accessing spaces\n-  TenuredSpace* space() const { return _the_space; }\n@@ -88,0 +86,2 @@\n+  TenuredSpace* space() const { return _the_space; }\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -716,4 +716,0 @@\n-void GenCollectedHeap::gen_process_weak_roots(OopClosure* root_closure) {\n-  WeakProcessor::oops_do(root_closure);\n-}\n-\n@@ -914,9 +910,0 @@\n-#if INCLUDE_SERIALGC\n-void GenCollectedHeap::prepare_for_compaction() {\n-  \/\/ Start by compacting into same gen.\n-  CompactPoint cp(_old_gen);\n-  _old_gen->prepare_for_compaction(&cp);\n-  _young_gen->prepare_for_compaction(&cp);\n-}\n-#endif \/\/ INCLUDE_SERIALGC\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -295,5 +295,0 @@\n-  \/\/ Apply \"root_closure\" to all the weak roots of the system.\n-  \/\/ These include JNI weak roots, string table,\n-  \/\/ and referents of reachable weak refs.\n-  void gen_process_weak_roots(OopClosure* root_closure);\n-\n@@ -343,7 +338,0 @@\n-#if INCLUDE_SERIALGC\n-  \/\/ For use by mark-sweep.  As implemented, mark-sweep-compact is global\n-  \/\/ in an essential way: compaction is performed across generations, by\n-  \/\/ iterating over spaces.\n-  void prepare_for_compaction();\n-#endif\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-#include \"runtime\/prefetch.inline.hpp\"\n@@ -44,4 +43,0 @@\n-#if INCLUDE_SERIALGC\n-#include \"gc\/serial\/serialBlockOffsetTable.inline.hpp\"\n-#include \"gc\/serial\/defNewGeneration.hpp\"\n-#endif\n@@ -50,1 +45,0 @@\n-  _compaction_top(nullptr),\n@@ -62,2 +56,1 @@\n-                                 bool mangle_space)\n-{\n+                                 bool mangle_space) {\n@@ -73,1 +66,0 @@\n-  set_compaction_top(bottom);\n@@ -83,1 +75,0 @@\n-  _compaction_top = bottom();\n@@ -118,224 +109,0 @@\n-HeapWord* ContiguousSpace::forward(oop q, size_t size,\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n-  \/\/ q is alive\n-  \/\/ First check if we should switch compaction space\n-  assert(this == cp->space, \"'this' should be current compaction space.\");\n-  size_t compaction_max_size = pointer_delta(end(), compact_top);\n-  while (size > compaction_max_size) {\n-    \/\/ switch to next compaction space\n-    cp->space->set_compaction_top(compact_top);\n-    cp->space = cp->space->next_compaction_space();\n-    if (cp->space == nullptr) {\n-      cp->gen = GenCollectedHeap::heap()->young_gen();\n-      assert(cp->gen != nullptr, \"compaction must succeed\");\n-      cp->space = cp->gen->first_compaction_space();\n-      assert(cp->space != nullptr, \"generation must have a first compaction space\");\n-    }\n-    compact_top = cp->space->bottom();\n-    cp->space->set_compaction_top(compact_top);\n-    compaction_max_size = pointer_delta(cp->space->end(), compact_top);\n-  }\n-\n-  \/\/ store the forwarding pointer into the mark word\n-  if (cast_from_oop<HeapWord*>(q) != compact_top) {\n-    q->forward_to(cast_to_oop(compact_top));\n-    assert(q->is_gc_marked(), \"encoding the pointer should preserve the mark\");\n-  } else {\n-    \/\/ if the object isn't moving we can just set the mark to the default\n-    \/\/ mark and handle it specially later on.\n-    q->init_mark();\n-    assert(!q->is_forwarded(), \"should not be forwarded\");\n-  }\n-\n-  compact_top += size;\n-\n-  \/\/ We need to update the offset table so that the beginnings of objects can be\n-  \/\/ found during scavenge.  Note that we are updating the offset table based on\n-  \/\/ where the object will be once the compaction phase finishes.\n-  cp->space->update_for_block(compact_top - size, compact_top);\n-  return compact_top;\n-}\n-\n-#if INCLUDE_SERIALGC\n-\n-void ContiguousSpace::prepare_for_compaction(CompactPoint* cp) {\n-  \/\/ Compute the new addresses for the live objects and store it in the mark\n-  \/\/ Used by universe::mark_sweep_phase2()\n-\n-  \/\/ We're sure to be here before any objects are compacted into this\n-  \/\/ space, so this is a good time to initialize this:\n-  set_compaction_top(bottom());\n-\n-  if (cp->space == nullptr) {\n-    assert(cp->gen != nullptr, \"need a generation\");\n-    assert(cp->gen->first_compaction_space() == this, \"just checking\");\n-    cp->space = cp->gen->first_compaction_space();\n-    cp->space->set_compaction_top(cp->space->bottom());\n-  }\n-\n-  HeapWord* compact_top = cp->space->compaction_top(); \/\/ This is where we are currently compacting to.\n-\n-  DeadSpacer dead_spacer(this);\n-\n-  HeapWord*  end_of_live = bottom();  \/\/ One byte beyond the last byte of the last live object.\n-  HeapWord*  first_dead = nullptr; \/\/ The first dead object.\n-\n-  const intx interval = PrefetchScanIntervalInBytes;\n-\n-  HeapWord* cur_obj = bottom();\n-  HeapWord* scan_limit = top();\n-\n-  while (cur_obj < scan_limit) {\n-    if (cast_to_oop(cur_obj)->is_gc_marked()) {\n-      \/\/ prefetch beyond cur_obj\n-      Prefetch::write(cur_obj, interval);\n-      size_t size = cast_to_oop(cur_obj)->size();\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n-      cur_obj += size;\n-      end_of_live = cur_obj;\n-    } else {\n-      \/\/ run over all the contiguous dead objects\n-      HeapWord* end = cur_obj;\n-      do {\n-        \/\/ prefetch beyond end\n-        Prefetch::write(end, interval);\n-        end += cast_to_oop(end)->size();\n-      } while (end < scan_limit && !cast_to_oop(end)->is_gc_marked());\n-\n-      \/\/ see if we might want to pretend this object is alive so that\n-      \/\/ we don't have to compact quite as often.\n-      if (cur_obj == compact_top && dead_spacer.insert_deadspace(cur_obj, end)) {\n-        oop obj = cast_to_oop(cur_obj);\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n-        end_of_live = end;\n-      } else {\n-        \/\/ otherwise, it really is a free region.\n-\n-        \/\/ cur_obj is a pointer to a dead object. Use this dead memory to store a pointer to the next live object.\n-        *(HeapWord**)cur_obj = end;\n-\n-        \/\/ see if this is the first dead region.\n-        if (first_dead == nullptr) {\n-          first_dead = cur_obj;\n-        }\n-      }\n-\n-      \/\/ move on to the next object\n-      cur_obj = end;\n-    }\n-  }\n-\n-  assert(cur_obj == scan_limit, \"just checking\");\n-  _end_of_live = end_of_live;\n-  if (first_dead != nullptr) {\n-    _first_dead = first_dead;\n-  } else {\n-    _first_dead = end_of_live;\n-  }\n-\n-  \/\/ save the compaction_top of the compaction space.\n-  cp->space->set_compaction_top(compact_top);\n-}\n-\n-void ContiguousSpace::adjust_pointers() {\n-  \/\/ Check first is there is any work to do.\n-  if (used() == 0) {\n-    return;   \/\/ Nothing to do.\n-  }\n-\n-  \/\/ adjust all the interior pointers to point at the new locations of objects\n-  \/\/ Used by MarkSweep::mark_sweep_phase3()\n-\n-  HeapWord* cur_obj = bottom();\n-  HeapWord* const end_of_live = _end_of_live;  \/\/ Established by prepare_for_compaction().\n-  HeapWord* const first_dead = _first_dead;    \/\/ Established by prepare_for_compaction().\n-\n-  assert(first_dead <= end_of_live, \"Stands to reason, no?\");\n-\n-  const intx interval = PrefetchScanIntervalInBytes;\n-\n-  debug_only(HeapWord* prev_obj = nullptr);\n-  while (cur_obj < end_of_live) {\n-    Prefetch::write(cur_obj, interval);\n-    if (cur_obj < first_dead || cast_to_oop(cur_obj)->is_gc_marked()) {\n-      \/\/ cur_obj is alive\n-      \/\/ point all the oops to the new location\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n-      debug_only(prev_obj = cur_obj);\n-      cur_obj += size;\n-    } else {\n-      debug_only(prev_obj = cur_obj);\n-      \/\/ cur_obj is not a live object, instead it points at the next live object\n-      cur_obj = *(HeapWord**)cur_obj;\n-      assert(cur_obj > prev_obj, \"we should be moving forward through memory, cur_obj: \" PTR_FORMAT \", prev_obj: \" PTR_FORMAT, p2i(cur_obj), p2i(prev_obj));\n-    }\n-  }\n-\n-  assert(cur_obj == end_of_live, \"just checking\");\n-}\n-\n-void ContiguousSpace::compact() {\n-  \/\/ Copy all live objects to their new location\n-  \/\/ Used by MarkSweep::mark_sweep_phase4()\n-\n-  verify_up_to_first_dead(this);\n-\n-  HeapWord* const start = bottom();\n-  HeapWord* const end_of_live = _end_of_live;\n-\n-  assert(_first_dead <= end_of_live, \"Invariant. _first_dead: \" PTR_FORMAT \" <= end_of_live: \" PTR_FORMAT, p2i(_first_dead), p2i(end_of_live));\n-  if (_first_dead == end_of_live && (start == end_of_live || !cast_to_oop(start)->is_gc_marked())) {\n-    \/\/ Nothing to compact. The space is either empty or all live object should be left in place.\n-    clear_empty_region(this);\n-    return;\n-  }\n-\n-  const intx scan_interval = PrefetchScanIntervalInBytes;\n-  const intx copy_interval = PrefetchCopyIntervalInBytes;\n-\n-  assert(start < end_of_live, \"bottom: \" PTR_FORMAT \" should be < end_of_live: \" PTR_FORMAT, p2i(start), p2i(end_of_live));\n-  HeapWord* cur_obj = start;\n-  if (_first_dead > cur_obj && !cast_to_oop(cur_obj)->is_gc_marked()) {\n-    \/\/ All object before _first_dead can be skipped. They should not be moved.\n-    \/\/ A pointer to the first live object is stored at the memory location for _first_dead.\n-    cur_obj = *(HeapWord**)(_first_dead);\n-  }\n-\n-  debug_only(HeapWord* prev_obj = nullptr);\n-  while (cur_obj < end_of_live) {\n-    if (!cast_to_oop(cur_obj)->is_forwarded()) {\n-      debug_only(prev_obj = cur_obj);\n-      \/\/ The first word of the dead object contains a pointer to the next live object or end of space.\n-      cur_obj = *(HeapWord**)cur_obj;\n-      assert(cur_obj > prev_obj, \"we should be moving forward through memory\");\n-    } else {\n-      \/\/ prefetch beyond q\n-      Prefetch::read(cur_obj, scan_interval);\n-\n-      \/\/ size and destination\n-      size_t size = cast_to_oop(cur_obj)->size();\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n-\n-      \/\/ prefetch beyond compaction_top\n-      Prefetch::write(compaction_top, copy_interval);\n-\n-      \/\/ copy object and reinit its mark\n-      assert(cur_obj != compaction_top, \"everything in this pass should be moving\");\n-      Copy::aligned_conjoint_words(cur_obj, compaction_top, size);\n-      oop new_obj = cast_to_oop(compaction_top);\n-\n-      ContinuationGCSupport::transform_stack_chunk(new_obj);\n-\n-      new_obj->init_mark();\n-      assert(new_obj->klass() != nullptr, \"should have a class\");\n-\n-      debug_only(prev_obj = cur_obj);\n-      cur_obj += size;\n-    }\n-  }\n-\n-  clear_empty_region(this);\n-}\n-\n-#endif \/\/ INCLUDE_SERIALGC\n-\n@@ -484,4 +251,0 @@\n-void TenuredSpace::update_for_block(HeapWord* start, HeapWord* end) {\n-  _offsets.update_for_block(start, end);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":1,"deletions":238,"binary":false,"changes":239,"status":"modified"},{"patch":"@@ -187,6 +187,0 @@\n-#if INCLUDE_SERIALGC\n-  \/\/ Mark-sweep-compact support: all spaces can update pointers to objects\n-  \/\/ moving as a part of compaction.\n-  virtual void adjust_pointers() = 0;\n-#endif\n-\n@@ -199,11 +193,0 @@\n-\/\/ A structure to represent a point at which objects are being copied\n-\/\/ during compaction.\n-class CompactPoint : public StackObj {\n-public:\n-  Generation* gen;\n-  ContiguousSpace* space;\n-\n-  CompactPoint(Generation* g = nullptr) :\n-    gen(g), space(nullptr) {}\n-};\n-\n@@ -218,1 +201,0 @@\n-  HeapWord* _compaction_top;\n@@ -221,5 +203,1 @@\n-  static inline void verify_up_to_first_dead(ContiguousSpace* space) NOT_DEBUG_RETURN;\n-\n-  static inline void clear_empty_region(ContiguousSpace* space);\n-\n- protected:\n+protected:\n@@ -230,8 +208,0 @@\n-  \/\/ Used during compaction.\n-  HeapWord* _first_dead;\n-  HeapWord* _end_of_live;\n-\n-  \/\/ This the function to invoke when an allocation of an object covering\n-  \/\/ \"start\" to \"end\" occurs to update other internal data structures.\n-  virtual void update_for_block(HeapWord* start, HeapWord* the_end) { }\n-\n@@ -257,11 +227,1 @@\n-  virtual void clear(bool mangle_space);\n-\n-  \/\/ Used temporarily during a compaction phase to hold the value\n-  \/\/ top should have when compaction is complete.\n-  HeapWord* compaction_top() const { return _compaction_top;    }\n-\n-  void set_compaction_top(HeapWord* value) {\n-    assert(value == nullptr || (value >= bottom() && value <= end()),\n-      \"should point inside space\");\n-    _compaction_top = value;\n-  }\n+  void clear(bool mangle_space);\n@@ -273,1 +233,1 @@\n-  virtual ContiguousSpace* next_compaction_space() const {\n+  ContiguousSpace* next_compaction_space() const {\n@@ -281,18 +241,0 @@\n-#if INCLUDE_SERIALGC\n-  \/\/ MarkSweep support phase2\n-\n-  \/\/ Start the process of compaction of the current space: compute\n-  \/\/ post-compaction addresses, and insert forwarding pointers.  The fields\n-  \/\/ \"cp->gen\" and \"cp->compaction_space\" are the generation and space into\n-  \/\/ which we are currently compacting.  This call updates \"cp\" as necessary,\n-  \/\/ and leaves the \"compaction_top\" of the final value of\n-  \/\/ \"cp->compaction_space\" up-to-date.  Offset tables may be updated in\n-  \/\/ this phase as if the final copy had occurred; if so, \"cp->threshold\"\n-  \/\/ indicates when the next such action should be taken.\n-  void prepare_for_compaction(CompactPoint* cp);\n-  \/\/ MarkSweep support phase3\n-  void adjust_pointers() override;\n-  \/\/ MarkSweep support phase4\n-  virtual void compact();\n-#endif \/\/ INCLUDE_SERIALGC\n-\n@@ -303,14 +245,0 @@\n-  \/\/ \"q\" is an object of the given \"size\" that should be forwarded;\n-  \/\/ \"cp\" names the generation (\"gen\") and containing \"this\" (which must\n-  \/\/ also equal \"cp->space\").  \"compact_top\" is where in \"this\" the\n-  \/\/ next object should be forwarded to.  If there is room in \"this\" for\n-  \/\/ the object, insert an appropriate forwarding pointer in \"q\".\n-  \/\/ If not, go to the next compaction space (there must\n-  \/\/ be one, since compaction must succeed -- we go to the first space of\n-  \/\/ the previous generation if necessary, updating \"cp\"), reset compact_top\n-  \/\/ and then forward.  In either case, returns the new value of \"compact_top\".\n-  \/\/ Invokes the \"update_for_block\" function of the then-current compaction\n-  \/\/ space.\n-  virtual HeapWord* forward(oop q, size_t size, CompactPoint* cp,\n-                    HeapWord* compact_top);\n-\n@@ -362,6 +290,0 @@\n-  \/\/ Compaction support\n-  void reset_after_compaction() {\n-    assert(compaction_top() >= bottom() && compaction_top() <= end(), \"should point inside space\");\n-    set_top(compaction_top());\n-  }\n-\n@@ -422,2 +344,1 @@\n-  \/\/ MarkSweep support phase3\n-  void update_for_block(HeapWord* start, HeapWord* end) override;\n+  inline void update_for_block(HeapWord* start, HeapWord* end);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":4,"deletions":83,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/serial\/generation.hpp\"\n@@ -37,4 +36,0 @@\n-#if INCLUDE_SERIALGC\n-#include \"gc\/serial\/serialBlockOffsetTable.inline.hpp\"\n-#include \"gc\/serial\/markSweep.inline.hpp\"\n-#endif\n@@ -63,84 +58,2 @@\n-class DeadSpacer : StackObj {\n-  size_t _allowed_deadspace_words;\n-  bool _active;\n-  ContiguousSpace* _space;\n-\n-public:\n-  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n-    size_t ratio = _space->allowed_dead_ratio();\n-    _active = ratio > 0;\n-\n-    if (_active) {\n-      assert(!UseG1GC, \"G1 should not be using dead space\");\n-\n-      \/\/ We allow some amount of garbage towards the bottom of the space, so\n-      \/\/ we don't start compacting before there is a significant gain to be made.\n-      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n-      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n-      if ((MarkSweep::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n-        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n-      } else {\n-        _active = false;\n-      }\n-    }\n-  }\n-\n-  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n-    if (!_active) {\n-      return false;\n-    }\n-\n-    size_t dead_length = pointer_delta(dead_end, dead_start);\n-    if (_allowed_deadspace_words >= dead_length) {\n-      _allowed_deadspace_words -= dead_length;\n-      CollectedHeap::fill_with_object(dead_start, dead_length);\n-      oop obj = cast_to_oop(dead_start);\n-      obj->set_mark(obj->mark().set_marked());\n-\n-      assert(dead_length == obj->size(), \"bad filler object size\");\n-      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n-          p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n-\n-      return true;\n-    } else {\n-      _active = false;\n-      return false;\n-    }\n-  }\n-};\n-\n-#ifdef ASSERT\n-inline void ContiguousSpace::verify_up_to_first_dead(ContiguousSpace* space) {\n-  HeapWord* cur_obj = space->bottom();\n-\n-  if (cur_obj < space->_end_of_live && space->_first_dead > cur_obj && !cast_to_oop(cur_obj)->is_gc_marked()) {\n-     \/\/ we have a chunk of the space which hasn't moved and we've reinitialized\n-     \/\/ the mark word during the previous pass, so we can't use is_gc_marked for\n-     \/\/ the traversal.\n-     HeapWord* prev_obj = nullptr;\n-\n-     while (cur_obj < space->_first_dead) {\n-       size_t size = cast_to_oop(cur_obj)->size();\n-       assert(!cast_to_oop(cur_obj)->is_gc_marked(), \"should be unmarked (special dense prefix handling)\");\n-       prev_obj = cur_obj;\n-       cur_obj += size;\n-     }\n-  }\n-}\n-#endif\n-\n-inline void ContiguousSpace::clear_empty_region(ContiguousSpace* space) {\n-  \/\/ Let's remember if we were empty before we did the compaction.\n-  bool was_empty = space->used_region().is_empty();\n-  \/\/ Reset space after compaction is complete\n-  space->reset_after_compaction();\n-  \/\/ We do this clear, below, since it has overloaded meanings for some\n-  \/\/ space subtypes.  For example, TenuredSpace's that were\n-  \/\/ compacted into will have had their offset table thresholds updated\n-  \/\/ continuously, but those that weren't need to have their thresholds\n-  \/\/ re-initialized.  Also mangles unused area for debugging.\n-  if (space->used_region().is_empty()) {\n-    if (!was_empty) space->clear(SpaceDecorator::Mangle);\n-  } else {\n-    if (ZapUnusedHeapArea) space->mangle_unused_area();\n-  }\n+inline void TenuredSpace::update_for_block(HeapWord* start, HeapWord* end) {\n+  _offsets.update_for_block(start, end);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":2,"deletions":89,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -102,4 +102,0 @@\n-  nonstatic_field(ContiguousSpace,            _compaction_top,                               HeapWord*)                             \\\n-  nonstatic_field(ContiguousSpace,            _first_dead,                                   HeapWord*)                             \\\n-  nonstatic_field(ContiguousSpace,            _end_of_live,                                  HeapWord*)                             \\\n-                                                                                                                                     \\\n","filename":"src\/hotspot\/share\/gc\/shared\/vmStructs_gc.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"}]}