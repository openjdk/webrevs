{"files":[{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -47,1 +47,1 @@\n-    __ la_patchable(t0, safepoint_pc.target(), offset);\n+    __ la(t0, safepoint_pc.target(), offset);\n@@ -95,6 +95,3 @@\n-  RuntimeAddress target(Runtime1::entry_for(stub_id));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(ra, target, offset);\n-    __ jalr(ra, ra, offset);\n-  });\n+  \/\/ t0 and t1 are used as args in generate_exception_throwï¼Œ\n+  \/\/ so use ra as the tmp register for rt_call.\n+  __ rt_call(Runtime1::entry_for(stub_id), ra);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_CodeStubs_riscv.cpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1428,1 +1428,1 @@\n-    __ la_patchable(exceptionPC->as_register(), pc_for_athrow, offset);\n+    __ la(exceptionPC->as_register(), pc_for_athrow.target(), offset);\n@@ -1871,1 +1871,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ movptr(t0, target.target(), offset);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -70,6 +70,1 @@\n-  RuntimeAddress target(entry);\n-  relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    la_patchable(t0, target, offset);\n-    jalr(x1, t0, offset);\n-  });\n+  rt_call(entry);\n@@ -581,6 +576,1 @@\n-  RuntimeAddress addr(target);\n-  __ relocate(addr.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, addr, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(target);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Runtime1_riscv.cpp","additions":3,"deletions":13,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -49,1 +49,1 @@\n-    __ la_patchable(t0, safepoint_pc.target(), offset);\n+    __ la(t0, safepoint_pc.target(), offset);\n@@ -63,6 +63,1 @@\n-  RuntimeAddress target(StubRoutines::method_entry_barrier());\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(ra, t0, offset);\n-  });\n+  __ rt_call(StubRoutines::method_entry_barrier());\n","filename":"src\/hotspot\/cpu\/riscv\/c2_CodeStubs_riscv.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -190,1 +190,0 @@\n-  CardTable* ct = ctbs->card_table();\n@@ -207,1 +206,0 @@\n-  ExternalAddress cardtable((address) ct->byte_map_base());\n@@ -413,1 +411,0 @@\n-  CardTable* ct = ctbs->card_table();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -311,6 +311,1 @@\n-    RuntimeAddress target(StubRoutines::method_entry_barrier());\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(ra, t0, offset);\n-    });\n+    __ rt_call(StubRoutines::method_entry_barrier());\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.cpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -342,6 +342,2 @@\n-    Address target(stub->slow_path());\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ mv(t0, stub->slow_path());\n+    __ jalr(t0);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/x\/xBarrierSetAssembler_riscv.cpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -39,2 +39,1 @@\n-  \/\/ 5: auipc + ld + j + address(2 * instruction_size)\n-  return (MacroAssembler::far_branches() ? 6 : 5) * NativeInstruction::instruction_size;\n+  return 6 * NativeInstruction::instruction_size;\n","filename":"src\/hotspot\/cpu\/riscv\/icBuffer_riscv.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -197,1 +197,1 @@\n-    la_patchable(xdispatch, target, offset);\n+    la(xdispatch, target.target(), offset);\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -79,1 +79,1 @@\n-    __ la_patchable(rcounter_addr, target, offset);\n+    __ la(rcounter_addr, target.target(), offset);\n@@ -99,1 +99,1 @@\n-      __ la_patchable(result, target, offset);\n+      __ la(result, target.target(), offset);\n@@ -179,1 +179,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n","filename":"src\/hotspot\/cpu\/riscv\/jniFastGetField_riscv.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -343,1 +343,1 @@\n-      la_patchable(t0, target, offset);\n+      la(t0, target.target(), offset);\n@@ -424,1 +424,1 @@\n-    la_patchable(t1, target, offset);\n+    la(t1, target.target(), offset);\n@@ -469,1 +469,1 @@\n-    la_patchable(t1, target, offset);\n+    la(t1, target.target(), offset);\n@@ -720,2 +720,2 @@\n-void MacroAssembler::la(Register Rd, const address dest) {\n-  int64_t offset = dest - pc();\n+void MacroAssembler::la(Register Rd, const address addr) {\n+  int64_t offset = addr - pc();\n@@ -726,1 +726,23 @@\n-    movptr(Rd, dest);\n+    movptr(Rd, addr);\n+  }\n+}\n+\n+void MacroAssembler::la(Register Rd, const address addr, int32_t &offset) {\n+  assert((uintptr_t)addr < (1ull << 48), \"bad address\");\n+\n+  unsigned long target_address = (uintptr_t)addr;\n+  unsigned long low_address = (uintptr_t)CodeCache::low_bound();\n+  unsigned long high_address = (uintptr_t)CodeCache::high_bound();\n+  long offset_low = target_address - low_address;\n+  long offset_high = target_address - high_address;\n+\n+  \/\/ RISC-V doesn't compute a page-aligned address, in order to partially\n+  \/\/ compensate for the use of *signed* offsets in its base+disp12\n+  \/\/ addressing mode (RISC-V's PC-relative reach remains asymmetric\n+  \/\/ [-(2G + 2K), 2G - 2K).\n+  if (offset_high >= -((1L << 31) + (1L << 11)) && offset_low < (1L << 31) - (1L << 11)) {\n+    int64_t distance = addr - pc();\n+    auipc(Rd, (int32_t)distance + 0x800);\n+    offset = ((int32_t)distance << 20) >> 20;\n+  } else {\n+    movptr(Rd, addr, offset);\n@@ -1567,1 +1589,1 @@\n-        la_patchable(xheapbase, target, offset);\n+        la(xheapbase, target.target(), offset);\n@@ -2122,1 +2144,1 @@\n-    _masm->la_patchable(t0, target, offset);\n+    _masm->la(t0, target.target(), offset);\n@@ -2999,1 +3021,1 @@\n-void MacroAssembler::far_jump(Address entry, Register tmp) {\n+void MacroAssembler::far_jump(const Address &entry, Register tmp) {\n@@ -3006,12 +3028,6 @@\n-  IncompressibleRegion ir(this);  \/\/ Fixed length: see MacroAssembler::far_branch_size()\n-  if (far_branches()) {\n-    \/\/ We can use auipc + jalr here because we know that the total size of\n-    \/\/ the code cache cannot exceed 2Gb.\n-    relocate(entry.rspec(), [&] {\n-      int32_t offset;\n-      la_patchable(tmp, entry, offset);\n-      jalr(x0, tmp, offset);\n-    });\n-  } else {\n-    j(entry);\n-  }\n+  \/\/ Fixed length: see MacroAssembler::far_branch_size()\n+  relocate(entry.rspec(), [&] {\n+    int32_t offset;\n+    la(tmp, entry.target(), offset);\n+    jalr(x0, tmp, offset);\n+  });\n@@ -3020,1 +3036,1 @@\n-void MacroAssembler::far_call(Address entry, Register tmp) {\n+void MacroAssembler::far_call(const Address &entry, Register tmp) {\n@@ -3027,12 +3043,8 @@\n-  IncompressibleRegion ir(this);  \/\/ Fixed length: see MacroAssembler::far_branch_size()\n-  if (far_branches()) {\n-    \/\/ We can use auipc + jalr here because we know that the total size of\n-    \/\/ the code cache cannot exceed 2Gb.\n-    relocate(entry.rspec(), [&] {\n-      int32_t offset;\n-      la_patchable(tmp, entry, offset);\n-      jalr(x1, tmp, offset); \/\/ link\n-    });\n-  } else {\n-    jal(entry); \/\/ link\n-  }\n+  \/\/ Fixed length: see MacroAssembler::far_branch_size()\n+  \/\/ We can use auipc + jalr here because we know that the total size of\n+  \/\/ the code cache cannot exceed 2Gb.\n+  relocate(entry.rspec(), [&] {\n+    int32_t offset;\n+    la(tmp, entry.target(), offset);\n+    jalr(x1, tmp, offset); \/\/ link\n+  });\n@@ -3261,23 +3273,0 @@\n-void MacroAssembler::la_patchable(Register reg1, const Address &dest, int32_t &offset) {\n-  unsigned long low_address = (uintptr_t)CodeCache::low_bound();\n-  unsigned long high_address = (uintptr_t)CodeCache::high_bound();\n-  unsigned long dest_address = (uintptr_t)dest.target();\n-  long offset_low = dest_address - low_address;\n-  long offset_high = dest_address - high_address;\n-\n-  assert(dest.getMode() == Address::literal, \"la_patchable must be applied to a literal address\");\n-  assert((uintptr_t)dest.target() < (1ull << 48), \"bad address\");\n-\n-  \/\/ RISC-V doesn't compute a page-aligned address, in order to partially\n-  \/\/ compensate for the use of *signed* offsets in its base+disp12\n-  \/\/ addressing mode (RISC-V's PC-relative reach remains asymmetric\n-  \/\/ [-(2G + 2K), 2G - 2K).\n-  if (offset_high >= -((1L << 31) + (1L << 11)) && offset_low < (1L << 31) - (1L << 11)) {\n-    int64_t distance = dest.target() - pc();\n-    auipc(reg1, (int32_t)distance + 0x800);\n-    offset = ((int32_t)distance << 20) >> 20;\n-  } else {\n-    movptr(reg1, dest.target(), offset);\n-  }\n-}\n-\n@@ -3310,6 +3299,1 @@\n-    RuntimeAddress target(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone));\n-    relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      la_patchable(t0, target, offset);\n-      jalr(x1, t0, offset);\n-    });\n+    rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone));\n@@ -3321,1 +3305,1 @@\n-    target = RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry());\n+    RuntimeAddress target(StubRoutines::throw_delayed_StackOverflowError_entry());\n@@ -3324,1 +3308,1 @@\n-      la_patchable(t0, target, offset);\n+      movptr(t0, target.target(), offset);\n@@ -3386,11 +3370,9 @@\n-  if (far_branches()) {\n-    if (!in_scratch_emit_size()) {\n-      if (entry.rspec().type() == relocInfo::runtime_call_type) {\n-        assert(CodeBuffer::supports_shared_stubs(), \"must support shared stubs\");\n-        code()->share_trampoline_for(entry.target(), offset());\n-      } else {\n-        address stub = emit_trampoline_stub(offset(), target);\n-        if (stub == nullptr) {\n-          postcond(pc() == badAddress);\n-          return nullptr; \/\/ CodeCache is full\n-        }\n+  if (!in_scratch_emit_size()) {\n+    if (entry.rspec().type() == relocInfo::runtime_call_type) {\n+      assert(CodeBuffer::supports_shared_stubs(), \"must support shared stubs\");\n+      code()->share_trampoline_for(entry.target(), offset());\n+    } else {\n+      address stub = emit_trampoline_stub(offset(), target);\n+      if (stub == nullptr) {\n+        postcond(pc() == badAddress);\n+        return nullptr; \/\/ CodeCache is full\n@@ -3399,1 +3381,0 @@\n-    target = pc();\n@@ -3401,0 +3382,1 @@\n+  target = pc();\n@@ -3548,1 +3530,1 @@\n-    la_patchable(t0, src2, offset);\n+    la(t0, src2.target(), offset);\n@@ -4196,1 +4178,1 @@\n-    RuntimeAddress zero_blocks = RuntimeAddress(StubRoutines::riscv::zero_blocks());\n+    RuntimeAddress zero_blocks(StubRoutines::riscv::zero_blocks());\n@@ -4791,1 +4773,1 @@\n-    far_call(target);\n+    far_call(target, tmp);\n@@ -4795,1 +4777,1 @@\n-      la_patchable(tmp, target, offset);\n+      movptr(tmp, target.target(), offset);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":63,"deletions":81,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -721,1 +721,2 @@\n-  void la(Register Rd, const address dest);\n+  void la(Register Rd, const address addr);\n+  void la(Register Rd, const address addr, int32_t &offset);\n@@ -1065,6 +1066,2 @@\n-  static bool far_branches() {\n-    return ReservedCodeCacheSize > branch_range;\n-  }\n-\n-  \/\/ Emit a direct call\/jump if the entry address will always be in range,\n-  \/\/ otherwise a far call\/jump.\n+  \/\/ Emit a far call\/jump. Only invalidates the tmp register which\n+  \/\/ is used to keep the entry address for jalr.\n@@ -1076,4 +1073,2 @@\n-  \/\/ In the case of a far call\/jump, the entry address is put in the tmp register.\n-  \/\/ The tmp register is invalidated.\n-  void far_call(Address entry, Register tmp = t0);\n-  void far_jump(Address entry, Register tmp = t0);\n+  void far_call(const Address &entry, Register tmp = t0);\n+  void far_jump(const Address &entry, Register tmp = t0);\n@@ -1082,1 +1077,0 @@\n-    if (far_branches()) {\n@@ -1084,3 +1078,0 @@\n-    } else {\n-      return 4;\n-    }\n@@ -1098,2 +1089,0 @@\n-  void la_patchable(Register reg1, const Address &dest, int32_t &offset);\n-\n@@ -1433,0 +1422,2 @@\n+  \/\/ Emit a runtime call. Only invalidates the tmp register which\n+  \/\/ is used to keep the entry address for jalr\/movptr.\n@@ -1472,1 +1463,1 @@\n-        la_patchable(dest, target, offset);\n+        la(dest, target.target(), offset);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":9,"deletions":18,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1851,1 +1851,1 @@\n-  \/\/ la_patchable t0, #exception_blob_entry_point\n+  \/\/ auipc t0, #exception_blob_entry_point\n@@ -1853,2 +1853,0 @@\n-  \/\/ or\n-  \/\/ j #exception_blob_entry_point\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -347,6 +347,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));\n@@ -1625,1 +1620,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -1849,1 +1844,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -1982,6 +1977,1 @@\n-    RuntimeAddress target(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ rt_call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));\n@@ -2159,6 +2149,1 @@\n-    RuntimeAddress target(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n@@ -2256,6 +2241,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info));\n@@ -2403,6 +2383,1 @@\n-  target = RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n@@ -2498,6 +2473,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n@@ -2625,6 +2595,1 @@\n-  target = RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n@@ -2699,6 +2664,1 @@\n-  RuntimeAddress target(call_ptr);\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(call_ptr);\n@@ -2812,6 +2772,1 @@\n-    RuntimeAddress target(destination);\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ rt_call(destination);\n@@ -2946,7 +2901,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n-\n+  __ rt_call(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C));\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":12,"deletions":63,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -2474,1 +2474,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -2685,1 +2685,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -2978,1 +2978,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -3114,1 +3114,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"}]}