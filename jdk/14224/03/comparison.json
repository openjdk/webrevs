{"files":[{"patch":"@@ -3558,0 +3558,8 @@\n+void Assembler::vmovsd(XMMRegister dst, XMMRegister src, XMMRegister src2) {\n+  assert(UseAVX > 0, \"Requires some form of AVX\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(AVX_128bit, \/* rex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(src2->encoding(), src->encoding(), dst->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x11, (0xC0 | encode));\n+}\n+\n@@ -6549,0 +6557,23 @@\n+void Assembler::evfnmadd213sd(XMMRegister dst, XMMRegister src1, XMMRegister src2, EvexRoundPrefix rmode) { \/\/ Need to add rmode for rounding mode support\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(rmode, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_extended_context();\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xAD, (0xC0 | encode));\n+}\n+\n+void Assembler::vfnmadd213sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {\n+  assert(VM_Version::supports_fma(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xAD, (0xC0 | encode));\n+}\n+\n+void Assembler::vfnmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2) {\n+  assert(VM_Version::supports_fma(), \"\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int16((unsigned char)0xBD, (0xC0 | encode));\n+}\n+\n@@ -6910,0 +6941,16 @@\n+void Assembler::vroundsd(XMMRegister dst, XMMRegister src, XMMRegister src2, int32_t rmode) {\n+  assert(VM_Version::supports_avx(), \"\");\n+  assert(rmode <= 0x0f, \"rmode 0x%x\", rmode);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x0B, (0xC0 | encode), (rmode));\n+}\n+\n+void Assembler::vrndscalesd(XMMRegister dst,  XMMRegister src1, XMMRegister src2, int32_t rmode) {\n+  assert(VM_Version::supports_evex(), \"requires EVEX support\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  emit_int24(0x0B, (0xC0 | encode), (rmode));\n+}\n+\n@@ -8875,0 +8922,13 @@\n+void Assembler::extractps(Register dst, XMMRegister src, uint8_t imm8) {\n+  assert(VM_Version::supports_sse4_1(), \"\");\n+  assert(imm8 <= 0x03, \"imm8: %u\", imm8);\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = simd_prefix_and_encode(src, xnoreg, as_XMMRegister(dst->encoding()), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+  \/\/ imm8:\n+  \/\/ 0x00 - extract from bits 31:0\n+  \/\/ 0x01 - extract from bits 63:32\n+  \/\/ 0x02 - extract from bits 95:64\n+  \/\/ 0x03 - extract from bits 127:96\n+  emit_int24(0x17, (0xC0 | encode), imm8 & 0x03);\n+}\n+\n@@ -9549,0 +9609,9 @@\n+void Assembler::evdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode) {\n+  assert(VM_Version::supports_evex(), \"\");\n+  InstructionAttr attributes(rmode, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  attributes.set_extended_context();\n+  attributes.set_is_evex_instruction();\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_F2, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0x5E, (0xC0 | encode));\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -531,0 +531,7 @@\n+  enum EvexRoundPrefix {\n+    EVEX_RNE = 0x0,\n+    EVEX_RD  = 0x1,\n+    EVEX_RU  = 0x2,\n+    EVEX_RZ  = 0x3\n+  };\n+\n@@ -889,0 +896,2 @@\n+  void vmovsd(XMMRegister dst, XMMRegister src, XMMRegister src2);\n+\n@@ -2251,0 +2260,1 @@\n+  void evdivsd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode);\n@@ -2254,0 +2264,3 @@\n+  void vfnmadd213sd(XMMRegister dst, XMMRegister nds, XMMRegister src);\n+  void evfnmadd213sd(XMMRegister dst, XMMRegister nds, XMMRegister src, EvexRoundPrefix rmode);\n+  void vfnmadd231sd(XMMRegister dst, XMMRegister src1, XMMRegister src2);\n@@ -2343,0 +2356,1 @@\n+  void vrndscalesd(XMMRegister dst,  XMMRegister src1,  XMMRegister src2, int32_t rmode);\n@@ -2345,0 +2359,2 @@\n+  void vroundsd(XMMRegister dst, XMMRegister src, XMMRegister src2, int32_t rmode);\n+  void vroundsd(XMMRegister dst, XMMRegister src, Address src2, int32_t rmode);\n@@ -2728,0 +2744,2 @@\n+  void extractps(Register dst, XMMRegister src, uint8_t imm8);\n+\n@@ -2961,0 +2979,2 @@\n+  void set_extended_context(void) { _is_extended_context = true; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -971,1 +971,1 @@\n-      __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n+        __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -100,0 +102,8 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+    jdouble dx = (jdouble) x;\n+    jdouble dy = (jdouble) y;\n+\n+    retval = (jfloat) (*addr)(dx, dy);\n+  }\n@@ -105,0 +115,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -115,0 +127,6 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+\n+    retval = (*addr)(x, y);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -3940,0 +3940,4 @@\n+\n+  if ((UseAVX >= 1) && (VM_Version::supports_avx512vlbwdq() || VM_Version::supports_fma())) {\n+    StubRoutines::_fmod = generate_libmFmod(); \/\/ from stubGenerator_x86_64_fmod.cpp\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -489,0 +489,1 @@\n+  address generate_libmFmod();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,545 @@\n+\/*\n+ * Copyright (c) 2023, Intel Corporation. All rights reserved.\n+ * Intel Math Library (LIBM) Source Code\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+\/******************************************************************************\/\n+\/\/                     ALGORITHM DESCRIPTION - FMOD()\n+\/\/                     ---------------------\n+\/\/\n+\/\/ If either value1 or value2 is NaN, the result is NaN.\n+\/\/\n+\/\/ If neither value1 nor value2 is NaN, the sign of the result equals the sign of the dividend.\n+\/\/\n+\/\/ If the dividend is an infinity or the divisor is a zero or both, the result is NaN.\n+\/\/\n+\/\/ If the dividend is finite and the divisor is an infinity, the result equals the dividend.\n+\/\/\n+\/\/ If the dividend is a zero and the divisor is finite, the result equals the dividend.\n+\/\/\n+\/\/ In the remaining cases, where neither operand is an infinity, a zero, or NaN, the floating-point\n+\/\/ remainder result from a dividend value1 and a divisor value2 is defined by the mathematical\n+\/\/ relation result = value1 - (value2 * q), where q is an integer that is negative only if\n+\/\/ value1 \/ value2 is negative, and positive only if value1 \/ value2 is positive, and whose magnitude\n+\/\/ is as large as possible without exceeding the magnitude of the true mathematical quotient of value1 and value2.\n+\/\/\n+\/******************************************************************************\/\n+\n+#define __ _masm->\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_NaN[] = {\n+    0x7FFFFFFFFFFFFFFFULL, 0x7FFFFFFFFFFFFFFFULL   \/\/ NaN vector\n+};\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_1p260[] = {\n+    0x5030000000000000ULL,    \/\/ 0x1p+260\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_MAX[] = {\n+    0x7FEFFFFFFFFFFFFFULL,    \/\/ Max\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_INF[] = {\n+    0x7FF0000000000000ULL,    \/\/ Inf\n+};\n+\n+ATTRIBUTE_ALIGNED(32) static const uint64_t CONST_e307[] = {\n+    0x7FE0000000000000ULL\n+};\n+\n+address StubGenerator::generate_libmFmod() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"libmFmod\");\n+  address start = __ pc();\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_avx512vlbwdq()) {     \/\/ AVX512 version\n+\n+\/\/ Source used to generate the AVX512 fmod assembly below:\n+\/\/\n+\/\/ #include <ia32intrin.h>\n+\/\/ #include <emmintrin.h>\n+\/\/ #pragma float_control(precise, on)\n+\/\/\n+\/\/ #define UINT32 unsigned int\n+\/\/ #define SINT32 int\n+\/\/ #define UINT64 unsigned __int64\n+\/\/ #define SINT64 __int64\n+\/\/\n+\/\/ #define DP_FMA(a, b, c)    __fence(_mm_cvtsd_f64(_mm_fmadd_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c))))\n+\/\/ #define DP_FMA_RN(a, b, c)    _mm_cvtsd_f64(_mm_fmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC)))\n+\/\/ #define DP_FMA_RZ(a, b, c) __fence(_mm_cvtsd_f64(_mm_fmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+\/\/\n+\/\/ #define DP_ROUND_RZ(a)   _mm_cvtsd_f64(_mm_roundscale_sd(_mm_setzero_pd(), _mm_set_sd(a), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC)))\n+\/\/\n+\/\/ #define DP_CONST(C)    _castu64_f64(0x##C##ull)\n+\/\/ #define DP_AND(X, Y)   _mm_cvtsd_f64(_mm_and_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+\/\/ #define DP_XOR(X, Y)   _mm_cvtsd_f64(_mm_xor_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+\/\/ #define DP_OR(X, Y)    _mm_cvtsd_f64(_mm_or_pd(_mm_set_sd(X), _mm_set_sd(Y)))\n+\/\/ #define DP_DIV_RZ(a, b) __fence(_mm_cvtsd_f64(_mm_div_round_sd(_mm_set_sd(a), _mm_set_sd(b), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+\/\/ #define DP_FNMA(a, b, c)    __fence(_mm_cvtsd_f64(_mm_fnmadd_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c))))\n+\/\/ #define DP_FNMA_RZ(a, b, c) __fence(_mm_cvtsd_f64(_mm_fnmadd_round_sd(_mm_set_sd(a), _mm_set_sd(b), _mm_set_sd(c), (_MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC))))\n+\/\/\n+\/\/ #define D2L(x)  _mm_castpd_si128(x)\n+\/\/ \/\/ transfer highest 32 bits (of low 64b) to GPR\n+\/\/ #define TRANSFER_HIGH_INT32(X)   _mm_extract_epi32(D2L(_mm_set_sd(X)), 1)\n+\/\/\n+\/\/ double fmod(double x, double y)\n+\/\/ {\n+\/\/ double a, b, sgn_a, q, bs, bs2;\n+\/\/ unsigned eq;\n+\/\/\n+\/\/     \/\/ |x|, |y|\n+\/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+\/\/     b = DP_AND(y, DP_CONST(7fffffffffffffff));\n+\/\/     \/\/ sign(x)\n+\/\/     sgn_a = DP_XOR(x, a);\n+\/\/     q = DP_DIV_RZ(a, b);\n+\/\/     q = DP_ROUND_RZ(q);\n+\/\/     eq = TRANSFER_HIGH_INT32(q);\n+\/\/     if (!eq)  return x + sgn_a;\n+\/\/     if (eq >= 0x7fefffffu) goto SPECIAL_FMOD;\n+\/\/     a = DP_FNMA_RZ(b, q, a);\n+\/\/\n+\/\/ FMOD_CONT:\n+\/\/     while (b <= a)\n+\/\/     {\n+\/\/         q = DP_DIV_RZ(a, b);\n+\/\/         q = DP_ROUND_RZ(q);\n+\/\/         a = DP_FNMA_RZ(b, q, a);\n+\/\/     }\n+\/\/\n+\/\/     a = DP_XOR(a, sgn_a);\n+\/\/     return a;\n+\/\/\n+\/\/ SPECIAL_FMOD:\n+\/\/\n+\/\/     \/\/ y==0 or x==Inf?\n+\/\/     if ((b == 0.0) || (!(a <= DP_CONST(7fefffffffffffff))))\n+\/\/         return DP_FNMA(b, q, a);    \/\/ NaN\n+\/\/     \/\/ y is NaN?\n+\/\/     if (!(b <= DP_CONST(7ff0000000000000))) return y + y;\n+\/\/     \/\/ b* 2*1023\n+\/\/     bs = b * DP_CONST(7fe0000000000000);\n+\/\/     q = DP_DIV_RZ(a, bs);\n+\/\/     q = DP_ROUND_RZ(q);\n+\/\/     eq = TRANSFER_HIGH_INT32(q);\n+\/\/     if (eq >= 0x7fefffffu)\n+\/\/     {\n+\/\/         \/\/ b* 2*1023 * 2^1023\n+\/\/         bs2 = bs * DP_CONST(7fe0000000000000);\n+\/\/         while (bs2 <= a)\n+\/\/         {\n+\/\/             q = DP_DIV_RZ(a, bs2);\n+\/\/             q = DP_ROUND_RZ(q);\n+\/\/             a = DP_FNMA_RZ(bs2, q, a);\n+\/\/         }\n+\/\/     }\n+\/\/     else\n+\/\/     a = DP_FNMA_RZ(bs, q, a);\n+\/\/\n+\/\/     while (bs <= a)\n+\/\/     {\n+\/\/         q = DP_DIV_RZ(a, bs);\n+\/\/         q = DP_ROUND_RZ(q);\n+\/\/         a = DP_FNMA_RZ(bs, q, a);\n+\/\/     }\n+\/\/\n+\/\/     goto FMOD_CONT;\n+\/\/ }\n+\n+\n+  Label L_5280, L_52a0, L_5256, L_5300, L_5320, L_52c0, L_52d0, L_5360, L_5380, L_53b0, L_5390;\n+  Label L_53c0, L_52a6, L_53d0, L_exit;\n+\n+  __ movdqa(xmm2, xmm0);\n+  __ movq(xmm0, xmm0);\n+  __ mov64(rax, 0x7FFFFFFFFFFFFFFF);\n+  __ evpbroadcastq(xmm3, rax, Assembler::AVX_128bit);\n+\n+  __ vpand(xmm6, xmm0, xmm3, Assembler::AVX_128bit);\n+  __ vpand(xmm4, xmm1, xmm3, Assembler::AVX_128bit);\n+  __ vpxor(xmm3, xmm6, xmm0, Assembler::AVX_128bit);\n+  __ movq(xmm5, xmm4);\n+  __ evdivsd(xmm0, xmm6, xmm5, Assembler::EVEX_RZ);\n+  __ movq(xmm0, xmm0);\n+  __ vxorpd(xmm7, xmm7, xmm7, Assembler::AVX_128bit);\n+  __ vroundsd(xmm0, xmm7, xmm0, 0xb);\n+  __ extractps(rax, xmm0, 1);\n+  __ testl(rax, rax);\n+  __ jcc(Assembler::equal, L_5280);\n+\n+  __ cmpl(rax, 0x7feffffe);\n+  __ jcc(Assembler::belowEqual, L_52a0);\n+  __ vpxor(xmm2, xmm2, xmm2, Assembler::AVX_128bit);\n+  __ ucomisd(xmm4, xmm2);\n+  __ jcc(Assembler::notEqual, L_5256);\n+  __ jcc(Assembler::noParity, L_5300);\n+\n+  __ bind(L_5256);\n+  __ movsd(xmm2, ExternalAddress((address)CONST_MAX), rax);\n+  __ ucomisd(xmm2, xmm6);\n+  __ jcc(Assembler::below, L_5300);\n+  __ movsd(xmm0, ExternalAddress((address)CONST_INF), rax);\n+  __ ucomisd(xmm0, xmm4);\n+  __ jcc(Assembler::aboveEqual, L_5320);\n+\n+  __ vaddsd(xmm0, xmm1, xmm1);\n+  __ jmp(L_exit);\n+\n+  __ align32();\n+  __ bind(L_5280);\n+  __ vaddsd(xmm0, xmm3, xmm2);\n+  __ jmp(L_exit);\n+\n+  __ align(8);\n+  __ bind(L_52a0);\n+  __ evfnmadd213sd(xmm0, xmm4, xmm6, Assembler::EVEX_RZ);\n+  __ bind(L_52a6);\n+  __ ucomisd(xmm0, xmm4);\n+  __ jcc(Assembler::aboveEqual, L_52c0);\n+  __ vpxor(xmm0, xmm3, xmm0, Assembler::AVX_128bit);\n+  __ jmp(L_exit);\n+\n+  __ bind(L_52c0);\n+\n+  __ movq(xmm6, xmm0);\n+  __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+  __ align32();\n+  __ bind(L_52d0);\n+\n+  __ evdivsd(xmm2, xmm6, xmm5, Assembler::EVEX_RZ);\n+  __ movq(xmm2, xmm2);\n+  __ vroundsd(xmm2, xmm1, xmm2, 0xb);\n+  __ evfnmadd213sd(xmm2, xmm4, xmm0, Assembler::EVEX_RZ);\n+  __ ucomisd(xmm2, xmm4);\n+  __ movq(xmm6, xmm2);\n+  __ movapd(xmm0, xmm2);\n+  __ jcc(Assembler::aboveEqual, L_52d0);\n+\n+  __ vpxor(xmm0, xmm3, xmm2, Assembler::AVX_128bit);\n+  __ jmp(L_exit);\n+\n+  __ bind(L_5300);\n+  __ vfnmadd213sd(xmm0, xmm4, xmm6);\n+  __ jmp(L_exit);\n+\n+  __ bind(L_5320);\n+  __ vmulsd(xmm1, xmm4, ExternalAddress((address)CONST_e307), rax);\n+  __ movq(xmm2, xmm1);\n+  __ evdivsd(xmm0, xmm6, xmm2, Assembler::EVEX_RZ);\n+  __ movq(xmm0, xmm0);\n+  __ vroundsd(xmm7, xmm7, xmm0, 0xb);\n+  __ extractps(rax, xmm7, 1);\n+  __ cmpl(rax, 0x7fefffff);\n+  __ jcc(Assembler::below, L_5360);\n+  __ vmulsd(xmm0, xmm1, ExternalAddress((address)CONST_e307), rax);\n+  __ ucomisd(xmm6, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_5380);\n+  __ movapd(xmm7, xmm6);\n+  __ jmp(L_53b0);\n+\n+  __ bind(L_5360);\n+  __ evfnmadd213sd(xmm7, xmm1, xmm6, Assembler::EVEX_RZ);\n+  __ jmp(L_53b0);\n+\n+  __ bind(L_5380);\n+  __ vxorpd(xmm8, xmm8, xmm8, Assembler::AVX_128bit);\n+\n+  __ align32();\n+  __ bind(L_5390);\n+  __ evdivsd(xmm7, xmm6, xmm0, Assembler::EVEX_RZ);\n+  __ movq(xmm7, xmm7);\n+  __ vroundsd(xmm7, xmm8, xmm7, 0xb);\n+  __ evfnmadd213sd(xmm7, xmm0, xmm6, Assembler::EVEX_RZ);\n+  __ ucomisd(xmm7, xmm0);\n+  __ movapd(xmm6, xmm7);\n+  __ jcc(Assembler::aboveEqual, L_5390);\n+  __ bind(L_53b0);\n+  __ ucomisd(xmm7, xmm1);\n+  __ jcc(Assembler::aboveEqual, L_53c0);\n+  __ movapd(xmm0, xmm7);\n+  __ jmp(L_52a6);\n+\n+  __ bind(L_53c0);\n+  __ vxorpd(xmm6, xmm6, xmm6, Assembler::AVX_128bit);\n+  __ align32();\n+  __ bind(L_53d0);\n+  __ evdivsd(xmm0, xmm7, xmm2, Assembler::EVEX_RZ);\n+  __ movq(xmm0, xmm0);\n+  __ vroundsd(xmm0, xmm6, xmm0, 0xb);\n+  __ evfnmadd213sd(xmm0, xmm1, xmm7, Assembler::EVEX_RZ);\n+  __ ucomisd(xmm0, xmm1);\n+  __ movapd(xmm7, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_53d0);\n+  __ jmp(L_52a6);\n+\n+  __ bind(L_exit);\n+\n+  } else if (VM_Version::supports_fma()) {       \/\/ AVX2 version\n+\n+    Label L_104a, L_11bd, L_10c1, L_1090, L_11b9, L_10e7, L_11af, L_111c, L_10f3, L_116e, L_112a;\n+    Label L_1173, L_1157, L_117f, L_11a0;\n+\n+\/\/   double fmod(double x, double y)\n+\/\/ {\n+\/\/ double a, b, sgn_a, q, bs, bs2, corr, res;\n+\/\/ unsigned eq;\n+\/\/ unsigned mxcsr, mxcsr_rz;\n+\n+\/\/   __asm { stmxcsr DWORD PTR[mxcsr] }\n+\/\/   mxcsr_rz = 0x7f80 | mxcsr;\n+  __ push(rax);\n+  __ stmxcsr(Address(rsp, 0));\n+  __ movl(rax, Address(rsp, 0));\n+  __ movl(rcx, rax);\n+  __ orl(rcx, 0x7f80);\n+  __ movl(Address(rsp, 0x04), rcx);\n+\n+\/\/     \/\/ |x|, |y|\n+\/\/     a = DP_AND(x, DP_CONST(7fffffffffffffff));\n+  __ movq(xmm2, xmm0);\n+  __ vmovdqu(xmm3, ExternalAddress((address)CONST_NaN), rcx);\n+  __ vpand(xmm4, xmm2, xmm3, Assembler::AVX_128bit);\n+\/\/     b = DP_AND(y, DP_CONST(7fffffffffffffff));\n+  __ vpand(xmm3, xmm1, xmm3, Assembler::AVX_128bit);\n+\/\/   \/\/ sign(x)\n+\/\/   sgn_a = DP_XOR(x, a);\n+  __ mov64(rcx, 0x8000000000000000);\n+  __ movq(xmm5, rcx);\n+  __ vpand(xmm2, xmm2, xmm5, Assembler::AVX_128bit);\n+\n+\/\/   if (a < b)  return x + sgn_a;\n+  __ ucomisd(xmm3, xmm4);\n+  __ jcc(Assembler::belowEqual, L_104a);\n+  __ vaddsd(xmm0, xmm2, xmm0);\n+  __ jmp(L_11bd);\n+\n+\/\/   if (((mxcsr & 0x6000)!=0x2000) && (a < b * 0x1p+260))\n+  __ bind(L_104a);\n+  __ andl(rax, 0x6000);\n+  __ cmpl(rax, 0x2000);\n+  __ jcc(Assembler::equal, L_10c1);\n+  __ vmulsd(xmm0, xmm3, ExternalAddress((address)CONST_1p260), rax);\n+  __ ucomisd(xmm0, xmm4);\n+  __ jcc(Assembler::belowEqual, L_10c1);\n+\/\/   {\n+\/\/     q = DP_DIV(a, b);\n+  __ vdivpd(xmm0, xmm4, xmm3, Assembler::AVX_128bit);\n+\/\/     corr = DP_SHR(DP_FNMA(b, q, a), 63);\n+  __ movapd(xmm1, xmm0);\n+  __ vfnmadd213sd(xmm1, xmm3, xmm4);\n+  __ movq(xmm5, xmm1);\n+  __ vpxor(xmm1, xmm1, xmm1, Assembler::AVX_128bit);\n+  __ vpcmpgtq(xmm5, xmm1, xmm5, Assembler::AVX_128bit);\n+\/\/     q = DP_PSUBQ(q, corr);\n+  __ vpaddq(xmm0, xmm5, xmm0, Assembler::AVX_128bit);\n+\/\/     q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\/\/     a = DP_FNMA(b, q, a);\n+  __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+  __ align32();\n+\/\/     while (b <= a)\n+  __ bind(L_1090);\n+  __ ucomisd(xmm0, xmm3);\n+  __ jcc(Assembler::below, L_11b9);\n+\/\/     {\n+\/\/       q = DP_DIV(a, b);\n+  __ vdivsd(xmm4, xmm0, xmm3);\n+\/\/       corr = DP_SHR(DP_FNMA(b, q, a), 63);\n+  __ movapd(xmm5, xmm4);\n+  __ vfnmadd213sd(xmm5, xmm3, xmm0);\n+  __ movq(xmm5, xmm5);\n+  __ vpcmpgtq(xmm5, xmm1, xmm5, Assembler::AVX_128bit);\n+\/\/       q = DP_PSUBQ(q, corr);\n+  __ vpaddq(xmm4, xmm5, xmm4, Assembler::AVX_128bit);\n+\/\/       q = DP_TRUNC(q);\n+  __ vroundsd(xmm4, xmm4, xmm4, 3);\n+\/\/       a = DP_FNMA(b, q, a);\n+  __ vfnmadd231sd(xmm0, xmm3, xmm4);\n+  __ jmp(L_1090);\n+\/\/     }\n+\/\/     return DP_XOR(a, sgn_a);\n+\/\/   }\n+\n+\/\/   __asm { ldmxcsr DWORD PTR [mxcsr_rz] }\n+  __ bind(L_10c1);\n+  __ ldmxcsr(Address(rsp, 0x04));\n+\n+\/\/   q = DP_DIV(a, b);\n+  __ vdivpd(xmm0, xmm4, xmm3, Assembler::AVX_128bit);\n+\/\/   q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\n+\/\/   eq = TRANSFER_HIGH_INT32(q);\n+  __ extractps(rax, xmm0, 1);\n+\n+\/\/   if (__builtin_expect((eq >= 0x7fefffffu), (0==1))) goto SPECIAL_FMOD;\n+  __ cmpl(rax, 0x7feffffe);\n+  __ jcc(Assembler::above, L_10e7);\n+\n+\/\/   a = DP_FNMA(b, q, a);\n+  __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+  __ jmp(L_11af);\n+\n+\/\/ SPECIAL_FMOD:\n+\n+\/\/   \/\/ y==0 or x==Inf?\n+\/\/   if ((b == 0.0) || (!(a <= DP_CONST(7fefffffffffffff))))\n+  __ bind(L_10e7);\n+  __ vpxor(xmm5, xmm5, xmm5, Assembler::AVX_128bit);\n+  __ ucomisd(xmm3, xmm5);\n+  __ jcc(Assembler::notEqual, L_10f3);\n+  __ jcc(Assembler::noParity, L_111c);\n+\n+  __ bind(L_10f3);\n+  __ movsd(xmm5, ExternalAddress((address)CONST_MAX), rax);\n+  __ ucomisd(xmm5, xmm4);\n+  __ jcc(Assembler::below, L_111c);\n+\/\/     return res;\n+\/\/   }\n+\/\/   \/\/ y is NaN?\n+\/\/   if (!(b <= DP_CONST(7ff0000000000000))) {\n+  __ movsd(xmm0, ExternalAddress((address)CONST_INF), rax);\n+  __ ucomisd(xmm0, xmm3);\n+  __ jcc(Assembler::aboveEqual, L_112a);\n+\/\/     res = y + y;\n+  __ vaddsd(xmm0, xmm0, xmm1);\n+\/\/     __asm { ldmxcsr DWORD PTR[mxcsr] }\n+  __ ldmxcsr(Address(rsp, 0));\n+  __ jmp(L_11bd);\n+\/\/   {\n+\/\/     res = DP_FNMA(b, q, a);    \/\/ NaN\n+  __ bind(L_111c);\n+  __ vfnmadd213sd(xmm0, xmm3, xmm4);\n+\/\/     __asm { ldmxcsr DWORD PTR[mxcsr] }\n+  __ ldmxcsr(Address(rsp, 0));\n+  __ jmp(L_11bd);\n+\/\/     return res;\n+\/\/   }\n+\n+\/\/   \/\/ b* 2*1023\n+\/\/   bs = b * DP_CONST(7fe0000000000000);\n+  __ bind(L_112a);\n+  __ vmulsd(xmm1, xmm3, ExternalAddress((address)CONST_e307), rax);\n+\n+\/\/   q = DP_DIV(a, bs);\n+  __ vdivsd(xmm0, xmm4, xmm1);\n+\/\/   q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\n+\/\/   eq = TRANSFER_HIGH_INT32(q);\n+  __ extractps(rax, xmm0, 1);\n+\n+\/\/   if (eq >= 0x7fefffffu)\n+  __ cmpl(rax, 0x7fefffff);\n+  __ jcc(Assembler::below, L_116e);\n+\/\/   {\n+\/\/     \/\/ b* 2*1023 * 2^1023\n+\/\/     bs2 = bs * DP_CONST(7fe0000000000000);\n+  __ vmulsd(xmm0, xmm1, ExternalAddress((address)CONST_e307), rax);\n+\/\/     while (bs2 <= a)\n+  __ ucomisd(xmm4, xmm0);\n+  __ jcc(Assembler::below, L_1173);\n+\/\/     {\n+\/\/       q = DP_DIV(a, bs2);\n+  __ bind(L_1157);\n+  __ vdivsd(xmm5, xmm4, xmm0);\n+\/\/       q = DP_TRUNC(q);\n+  __ vroundsd(xmm5, xmm5, xmm5, 3);\n+\/\/       a = DP_FNMA(bs2, q, a);\n+  __ vfnmadd231sd(xmm4, xmm0, xmm5);\n+\/\/     while (bs2 <= a)\n+  __ ucomisd(xmm4, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_1157);\n+  __ jmp(L_1173);\n+\/\/     }\n+\/\/   }\n+\/\/   else\n+\/\/   a = DP_FNMA(bs, q, a);\n+  __ bind(L_116e);\n+  __ vfnmadd231sd(xmm4, xmm1, xmm0);\n+\n+\/\/   while (bs <= a)\n+  __ bind(L_1173);\n+  __ ucomisd(xmm4, xmm1);\n+  __ jcc(Assembler::aboveEqual, L_117f);\n+  __ movapd(xmm0, xmm4);\n+  __ jmp(L_11af);\n+\/\/   {\n+\/\/     q = DP_DIV(a, bs);\n+  __ bind(L_117f);\n+  __ vdivsd(xmm0, xmm4, xmm1);\n+\/\/     q = DP_TRUNC(q);\n+  __ vroundsd(xmm0, xmm0, xmm0, 3);\n+\/\/     a = DP_FNMA(bs, q, a);\n+  __ vfnmadd213sd(xmm0, xmm1, xmm4);\n+\n+\/\/   while (bs <= a)\n+  __ ucomisd(xmm0, xmm1);\n+  __ movapd(xmm4, xmm0);\n+  __ jcc(Assembler::aboveEqual, L_117f);\n+  __ jmp(L_11af);\n+  __ align32();\n+\/\/   {\n+\/\/     q = DP_DIV(a, b);\n+  __ bind(L_11a0);\n+  __ vdivsd(xmm1, xmm0, xmm3);\n+\/\/     q = DP_TRUNC(q);\n+  __ vroundsd(xmm1, xmm1, xmm1, 3);\n+\/\/     a = DP_FNMA(b, q, a);\n+  __ vfnmadd231sd(xmm0, xmm3, xmm1);\n+\n+\/\/ FMOD_CONT:\n+\/\/   while (b <= a)\n+  __ bind(L_11af);\n+  __ ucomisd(xmm0, xmm3);\n+  __ jcc(Assembler::aboveEqual, L_11a0);\n+\/\/   }\n+\n+\/\/   __asm { ldmxcsr DWORD PTR[mxcsr] }\n+  __ ldmxcsr(Address(rsp, 0));\n+  __ bind(L_11b9);\n+  __ vpxor(xmm0, xmm2, xmm0, Assembler::AVX_128bit);\n+\/\/   }\n+\n+\/\/   goto FMOD_CONT;\n+\n+\/\/ }\n+  __ bind(L_11bd);\n+  __ pop(rax);\n+\n+  } else {                                       \/\/ SSE version\n+    assert(false, \"SSE not implemented\");\n+  }\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_fmod.cpp","additions":545,"deletions":0,"binary":false,"changes":545,"status":"added"},{"patch":"@@ -164,0 +164,1 @@\n+address StubRoutines::_fmod = nullptr;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -252,0 +252,1 @@\n+  static address _fmod;\n@@ -428,0 +429,1 @@\n+  static address fmod()                { return _fmod; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -559,0 +559,1 @@\n+     static_field(StubRoutines,                _fmod,                                         address)                               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}