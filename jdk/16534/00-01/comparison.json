{"files":[{"patch":"@@ -862,1 +862,1 @@\n-    (_model == 0x97 || _model == 0xAC || _model == 0xAF)) {\n+    (_model == 0x97 || _model == 0xAA || _model == 0xAC || _model == 0xAF)) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5390,0 +5390,4 @@\n+    \/\/ Disable the intrinsic for 64-bit types with AVX2\n+    if ((bt == T_LONG || bt == T_DOUBLE) && UseAVX == 2) {\n+      return false;\n+    }\n@@ -5443,0 +5447,4 @@\n+  \/\/ Disable the intrinsic for 64-bit types with AVX2\n+  if ((bt == T_LONG || bt == T_DOUBLE) && UseAVX == 2) {\n+    return false;\n+  }\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1,341 +0,0 @@\n-\/*\n- * Copyright (c) 2021, 2023, Intel Corporation. All rights reserved.\n- * Copyright (c) 2021 Serge Sans Paille. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/\/ This implementation is based on x86-simd-sort(https:\/\/github.com\/intel\/x86-simd-sort)\n-\n-#ifndef AVX2_QSORT_64BIT\n-#define AVX2_QSORT_64BIT\n-\n-#include \"avx2-emu-funcs.hpp\"\n-#include \"xss-common-qsort.h\"\n-\n-\/*\n- * Constants used in sorting 8 elements in a ymm registers. Based on Bitonic\n- * sorting network (see\n- * https:\/\/en.wikipedia.org\/wiki\/Bitonic_sorter#\/media\/File:BitonicSort.svg)\n- *\/\n-\/\/ ymm                  3, 2, 1, 0\n-#define NETWORK_64BIT_R 0, 1, 2, 3\n-#define NETWORK_64BIT_1 1, 0, 3, 2\n-\n-\/*\n- * Assumes ymm is random and performs a full sorting network defined in\n- * https:\/\/en.wikipedia.org\/wiki\/Bitonic_sorter#\/media\/File:BitonicSort.svg\n- *\/\n-template <typename vtype, typename reg_t = typename vtype::reg_t>\n-X86_SIMD_SORT_INLINE reg_t sort_ymm_64bit(reg_t ymm) {\n-    const typename vtype::opmask_t oxAA =\n-        _mm256_set_epi64x(0xFFFFFFFFFFFFFFFF, 0, 0xFFFFFFFFFFFFFFFF, 0);\n-    const typename vtype::opmask_t oxCC =\n-        _mm256_set_epi64x(0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF, 0, 0);\n-    ymm = cmp_merge<vtype>(\n-        ymm, vtype::template permutexvar<SHUFFLE_MASK(2, 3, 0, 1)>(ymm), oxAA);\n-    ymm = cmp_merge<vtype>(\n-        ymm, vtype::template permutexvar<SHUFFLE_MASK(0, 1, 2, 3)>(ymm), oxCC);\n-    ymm = cmp_merge<vtype>(\n-        ymm, vtype::template permutexvar<SHUFFLE_MASK(2, 3, 0, 1)>(ymm), oxAA);\n-    return ymm;\n-}\n-\n-struct avx2_64bit_swizzle_ops;\n-\n-template <>\n-struct avx2_vector<int64_t> {\n-    using type_t = int64_t;\n-    using reg_t = __m256i;\n-    using ymmi_t = __m256i;\n-    using opmask_t = __m256i;\n-    static const uint8_t numlanes = 4;\n-#ifdef XSS_MINIMAL_NETWORK_SORT\n-    static constexpr int network_sort_threshold = numlanes;\n-#else\n-    static constexpr int network_sort_threshold = 64;\n-#endif\n-    static constexpr int partition_unroll_factor = 8;\n-\n-    using swizzle_ops = avx2_64bit_swizzle_ops;\n-\n-    static type_t type_max() { return X86_SIMD_SORT_MAX_INT64; }\n-    static type_t type_min() { return X86_SIMD_SORT_MIN_INT64; }\n-    static reg_t zmm_max() {\n-        return _mm256_set1_epi64x(type_max());\n-    }  \/\/ TODO: this should broadcast bits as is?\n-    static opmask_t get_partial_loadmask(uint64_t num_to_read) {\n-        auto mask = ((0x1ull << num_to_read) - 0x1ull);\n-        return convert_int_to_avx2_mask_64bit(mask);\n-    }\n-    static ymmi_t seti(int v1, int v2, int v3, int v4) {\n-        return _mm256_set_epi64x(v1, v2, v3, v4);\n-    }\n-    static opmask_t kxor_opmask(opmask_t x, opmask_t y) {\n-        return _mm256_xor_si256(x, y);\n-    }\n-    static opmask_t gt(reg_t x, reg_t y) { return _mm256_cmpgt_epi64(x, y); }\n-    static opmask_t ge(reg_t x, reg_t y) {\n-        opmask_t equal = eq(x, y);\n-        opmask_t greater = _mm256_cmpgt_epi64(x, y);\n-        return _mm256_or_si256(equal, greater);\n-    }\n-    static opmask_t eq(reg_t x, reg_t y) { return _mm256_cmpeq_epi64(x, y); }\n-    template <int scale>\n-    static reg_t mask_i64gather(reg_t src, opmask_t mask, __m256i index,\n-                                void const *base) {\n-        return _mm256_mask_i64gather_epi64(src, base, index, mask, scale);\n-    }\n-    template <int scale>\n-    static reg_t i64gather(__m256i index, void const *base) {\n-        return _mm256_i64gather_epi64((int64_t const *)base, index, scale);\n-    }\n-    static reg_t loadu(void const *mem) {\n-        return _mm256_loadu_si256((reg_t const *)mem);\n-    }\n-    static reg_t max(reg_t x, reg_t y) { return avx2_emu_max<type_t>(x, y); }\n-    static void mask_compressstoreu(void *mem, opmask_t mask, reg_t x) {\n-        return avx2_emu_mask_compressstoreu64<type_t>(mem, mask, x);\n-    }\n-    static int32_t double_compressstore(void *left_addr, void *right_addr,\n-                                        opmask_t k, reg_t reg) {\n-        return avx2_double_compressstore64<type_t>(left_addr, right_addr, k,\n-                                                   reg);\n-    }\n-    static reg_t maskz_loadu(opmask_t mask, void const *mem) {\n-        return _mm256_maskload_epi64((const long long int *)mem, mask);\n-    }\n-    static reg_t mask_loadu(reg_t x, opmask_t mask, void const *mem) {\n-        reg_t dst = _mm256_maskload_epi64((long long int *)mem, mask);\n-        return mask_mov(x, mask, dst);\n-    }\n-    static reg_t mask_mov(reg_t x, opmask_t mask, reg_t y) {\n-        return _mm256_castpd_si256(_mm256_blendv_pd(_mm256_castsi256_pd(x),\n-                                                    _mm256_castsi256_pd(y),\n-                                                    _mm256_castsi256_pd(mask)));\n-    }\n-    static void mask_storeu(void *mem, opmask_t mask, reg_t x) {\n-        return _mm256_maskstore_epi64((long long int *)mem, mask, x);\n-    }\n-    static reg_t min(reg_t x, reg_t y) { return avx2_emu_min<type_t>(x, y); }\n-    template <int32_t idx>\n-    static reg_t permutexvar(reg_t ymm) {\n-        return _mm256_permute4x64_epi64(ymm, idx);\n-    }\n-    template <int32_t idx>\n-    static reg_t permutevar(reg_t ymm) {\n-        return _mm256_permute4x64_epi64(ymm, idx);\n-    }\n-    static reg_t reverse(reg_t ymm) {\n-        const int32_t rev_index = SHUFFLE_MASK(0, 1, 2, 3);\n-        return permutexvar<rev_index>(ymm);\n-    }\n-    static type_t reducemax(reg_t v) {\n-        return avx2_emu_reduce_max64<type_t>(v);\n-    }\n-    static type_t reducemin(reg_t v) {\n-        return avx2_emu_reduce_min64<type_t>(v);\n-    }\n-    static reg_t set1(type_t v) { return _mm256_set1_epi64x(v); }\n-    template <uint8_t mask>\n-    static reg_t shuffle(reg_t ymm) {\n-        return _mm256_castpd_si256(\n-            _mm256_permute_pd(_mm256_castsi256_pd(ymm), mask));\n-    }\n-    static void storeu(void *mem, reg_t x) {\n-        _mm256_storeu_si256((__m256i *)mem, x);\n-    }\n-    static reg_t sort_vec(reg_t x) {\n-        return sort_ymm_64bit<avx2_vector<type_t>>(x);\n-    }\n-    static reg_t cast_from(__m256i v) { return v; }\n-    static __m256i cast_to(reg_t v) { return v; }\n-};\n-\n-template <>\n-struct avx2_vector<double> {\n-    using type_t = double;\n-    using reg_t = __m256d;\n-    using ymmi_t = __m256i;\n-    using opmask_t = __m256i;\n-    static const uint8_t numlanes = 4;\n-#ifdef XSS_MINIMAL_NETWORK_SORT\n-    static constexpr int network_sort_threshold = numlanes;\n-#else\n-    static constexpr int network_sort_threshold = 64;\n-#endif\n-    static constexpr int partition_unroll_factor = 8;\n-\n-    using swizzle_ops = avx2_64bit_swizzle_ops;\n-\n-    static type_t type_max() { return X86_SIMD_SORT_INFINITY; }\n-    static type_t type_min() { return -X86_SIMD_SORT_INFINITY; }\n-    static reg_t zmm_max() { return _mm256_set1_pd(type_max()); }\n-    static opmask_t get_partial_loadmask(uint64_t num_to_read) {\n-        auto mask = ((0x1ull << num_to_read) - 0x1ull);\n-        return convert_int_to_avx2_mask_64bit(mask);\n-    }\n-    static int32_t convert_mask_to_int(opmask_t mask) {\n-        return convert_avx2_mask_to_int_64bit(mask);\n-    }\n-    template <int type>\n-    static opmask_t fpclass(reg_t x) {\n-        if constexpr (type == (0x01 | 0x80)) {\n-            return _mm256_castpd_si256(_mm256_cmp_pd(x, x, _CMP_UNORD_Q));\n-        } else {\n-            static_assert(type == (0x01 | 0x80), \"should not reach here\");\n-        }\n-    }\n-    static ymmi_t seti(int v1, int v2, int v3, int v4) {\n-        return _mm256_set_epi64x(v1, v2, v3, v4);\n-    }\n-\n-    static reg_t maskz_loadu(opmask_t mask, void const *mem) {\n-        return _mm256_maskload_pd((const double *)mem, mask);\n-    }\n-    static opmask_t ge(reg_t x, reg_t y) {\n-        return _mm256_castpd_si256(_mm256_cmp_pd(x, y, _CMP_GE_OQ));\n-    }\n-    static opmask_t gt(reg_t x, reg_t y) {\n-        return _mm256_castpd_si256(_mm256_cmp_pd(x, y, _CMP_GT_OQ));\n-    }\n-    static opmask_t eq(reg_t x, reg_t y) {\n-        return _mm256_castpd_si256(_mm256_cmp_pd(x, y, _CMP_EQ_OQ));\n-    }\n-    template <int scale>\n-    static reg_t mask_i64gather(reg_t src, opmask_t mask, __m256i index,\n-                                void const *base) {\n-        return _mm256_mask_i64gather_pd(src, base, index,\n-                                        _mm256_castsi256_pd(mask), scale);\n-        ;\n-    }\n-    template <int scale>\n-    static reg_t i64gather(__m256i index, void const *base) {\n-        return _mm256_i64gather_pd((double *)base, index, scale);\n-    }\n-    static reg_t loadu(void const *mem) {\n-        return _mm256_loadu_pd((double const *)mem);\n-    }\n-    static reg_t max(reg_t x, reg_t y) { return _mm256_max_pd(x, y); }\n-    static void mask_compressstoreu(void *mem, opmask_t mask, reg_t x) {\n-        return avx2_emu_mask_compressstoreu64<type_t>(mem, mask, x);\n-    }\n-    static int32_t double_compressstore(void *left_addr, void *right_addr,\n-                                        opmask_t k, reg_t reg) {\n-        return avx2_double_compressstore64<type_t>(left_addr, right_addr, k,\n-                                                   reg);\n-    }\n-    static reg_t mask_loadu(reg_t x, opmask_t mask, void const *mem) {\n-        reg_t dst = _mm256_maskload_pd((type_t *)mem, mask);\n-        return mask_mov(x, mask, dst);\n-    }\n-    static reg_t mask_mov(reg_t x, opmask_t mask, reg_t y) {\n-        return _mm256_blendv_pd(x, y, _mm256_castsi256_pd(mask));\n-    }\n-    static void mask_storeu(void *mem, opmask_t mask, reg_t x) {\n-        return _mm256_maskstore_pd((type_t *)mem, mask, x);\n-    }\n-    static reg_t min(reg_t x, reg_t y) { return _mm256_min_pd(x, y); }\n-    template <int32_t idx>\n-    static reg_t permutexvar(reg_t ymm) {\n-        return _mm256_permute4x64_pd(ymm, idx);\n-    }\n-    template <int32_t idx>\n-    static reg_t permutevar(reg_t ymm) {\n-        return _mm256_permute4x64_pd(ymm, idx);\n-    }\n-    static reg_t reverse(reg_t ymm) {\n-        const int32_t rev_index = SHUFFLE_MASK(0, 1, 2, 3);\n-        return permutexvar<rev_index>(ymm);\n-    }\n-    static type_t reducemax(reg_t v) {\n-        return avx2_emu_reduce_max64<type_t>(v);\n-    }\n-    static type_t reducemin(reg_t v) {\n-        return avx2_emu_reduce_min64<type_t>(v);\n-    }\n-    static reg_t set1(type_t v) { return _mm256_set1_pd(v); }\n-    template <uint8_t mask>\n-    static reg_t shuffle(reg_t ymm) {\n-        return _mm256_permute_pd(ymm, mask);\n-    }\n-    static void storeu(void *mem, reg_t x) {\n-        _mm256_storeu_pd((double *)mem, x);\n-    }\n-    static reg_t sort_vec(reg_t x) {\n-        return sort_ymm_64bit<avx2_vector<type_t>>(x);\n-    }\n-    static reg_t cast_from(__m256i v) { return _mm256_castsi256_pd(v); }\n-    static __m256i cast_to(reg_t v) { return _mm256_castpd_si256(v); }\n-};\n-\n-struct avx2_64bit_swizzle_ops {\n-    template <typename vtype, int scale>\n-    X86_SIMD_SORT_INLINE typename vtype::reg_t swap_n(\n-        typename vtype::reg_t reg) {\n-        __m256i v = vtype::cast_to(reg);\n-\n-        if constexpr (scale == 2) {\n-            v = _mm256_permute4x64_epi64(v, 0b10110001);\n-        } else if constexpr (scale == 4) {\n-            v = _mm256_permute4x64_epi64(v, 0b01001110);\n-        } else {\n-            static_assert(scale == -1, \"should not be reached\");\n-        }\n-\n-        return vtype::cast_from(v);\n-    }\n-\n-    template <typename vtype, int scale>\n-    X86_SIMD_SORT_INLINE typename vtype::reg_t reverse_n(\n-        typename vtype::reg_t reg) {\n-        __m256i v = vtype::cast_to(reg);\n-\n-        if constexpr (scale == 2) {\n-            return swap_n<vtype, 2>(reg);\n-        } else if constexpr (scale == 4) {\n-            return vtype::reverse(reg);\n-        } else {\n-            static_assert(scale == -1, \"should not be reached\");\n-        }\n-\n-        return vtype::cast_from(v);\n-    }\n-\n-    template <typename vtype, int scale>\n-    X86_SIMD_SORT_INLINE typename vtype::reg_t merge_n(\n-        typename vtype::reg_t reg, typename vtype::reg_t other) {\n-        __m256d v1 = _mm256_castsi256_pd(vtype::cast_to(reg));\n-        __m256d v2 = _mm256_castsi256_pd(vtype::cast_to(other));\n-\n-        if constexpr (scale == 2) {\n-            v1 = _mm256_blend_pd(v1, v2, 0b0101);\n-        } else if constexpr (scale == 4) {\n-            v1 = _mm256_blend_pd(v1, v2, 0b0011);\n-        } else {\n-            static_assert(scale == -1, \"should not be reached\");\n-        }\n-\n-        return vtype::cast_from(_mm256_castpd_si256(v1));\n-    }\n-};\n-\n-#endif  \/\/ AVX2_QSORT_32BIT\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/avx2-64bit-qsort.hpp","additions":0,"deletions":341,"binary":false,"changes":341,"status":"deleted"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"avx2-64bit-qsort.hpp\"\n@@ -33,1 +32,0 @@\n-#define INSERTION_SORT_THRESHOLD_64BIT 20\n@@ -42,3 +40,0 @@\n-            case JVM_T_LONG:\n-                avx2_fast_sort((int64_t*)array, from_index, to_index, INSERTION_SORT_THRESHOLD_64BIT);\n-                break;\n@@ -48,3 +43,0 @@\n-            case JVM_T_DOUBLE:\n-                avx2_fast_sort((double*)array, from_index, to_index, INSERTION_SORT_THRESHOLD_64BIT);\n-                break;\n@@ -59,3 +51,0 @@\n-            case JVM_T_LONG:\n-                avx2_fast_partition((int64_t*)array, from_index, to_index, pivot_indices, index_pivot1, index_pivot2);\n-                break;\n@@ -65,3 +54,0 @@\n-            case JVM_T_DOUBLE:\n-                avx2_fast_partition((double*)array, from_index, to_index, pivot_indices, index_pivot1, index_pivot2);\n-                break;\n","filename":"src\/java.base\/linux\/native\/libsimdsort\/avx2-linux-qsort.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"}]}