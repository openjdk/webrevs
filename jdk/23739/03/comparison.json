{"files":[{"patch":"@@ -91,7 +91,32 @@\n-  __ push(saved_regs, sp);\n-  assert_different_registers(start, count, scratch);\n-  assert_different_registers(c_rarg0, count);\n-  __ mov(c_rarg0, start);\n-  __ mov(c_rarg1, count);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_post_entry), 2);\n-  __ pop(saved_regs, sp);\n+\n+  Label done;\n+  Label loop;\n+  Label next;\n+  const Register end = count;\n+\n+  __ cbz(count, done);\n+\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop))); \/\/ end = start + count << LogBytesPerHeapOop\n+  __ sub(end, end, BytesPerHeapOop);                                    \/\/ last element address to make inclusive\n+\n+  __ lsr(start, start, CardTable::card_shift());\n+  __ lsr(end, end, CardTable::card_shift());\n+  __ sub(count, end, start);                                            \/\/ Number of bytes to mark\n+\n+  __ ldr(scratch, Address(rthread, in_bytes(G1ThreadLocalData::card_table_base_offset())));\n+  __ add(start, start, scratch);\n+\n+  __ bind(loop);\n+  if (UseCondCardMark) {\n+    __ ldrb(scratch, Address(start, count));\n+    \/\/ Instead of loading clean_card_val and comparing, we exploit the fact that\n+    \/\/ the LSB of non-clean cards is always 0, and the LSB of clean cards 1.\n+    __ tbz(scratch, 0, next);\n+  }\n+  static_assert(G1CardTable::dirty_card_val() == 0, \"must be to use zr\");\n+  __ strb(zr, Address(start, count));\n+  __ bind(next);\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, loop);\n+\n+  __ bind(done);\n@@ -205,0 +230,1 @@\n+                                            const Register thread,\n@@ -209,0 +235,3 @@\n+  assert(thread == rthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, noreg, rscratch1);\n+\n@@ -219,5 +248,0 @@\n-  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n-  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n-  __ ldrb(tmp2, Address(tmp1));                          \/\/ tmp2 := card\n-  __ cmpw(tmp2, (int)G1CardTable::g1_young_card_val());  \/\/ tmp2 := card == young_card_val?\n-}\n@@ -225,19 +249,10 @@\n-static void generate_post_barrier_slow_path(MacroAssembler* masm,\n-                                            const Register thread,\n-                                            const Register tmp1,\n-                                            const Register tmp2,\n-                                            Label& done,\n-                                            Label& runtime) {\n-  __ membar(Assembler::StoreLoad);  \/\/ StoreLoad membar\n-  __ ldrb(tmp2, Address(tmp1));     \/\/ tmp2 := card\n-  __ cbzw(tmp2, done);\n-  \/\/ Storing a region crossing, non-null oop, card is clean.\n-  \/\/ Dirty card and log.\n-  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n-  __ strb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n-  generate_queue_test_and_insertion(masm,\n-                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                                    runtime,\n-                                    thread, tmp1, tmp2, rscratch1);\n-  __ b(done);\n+  Address card_table_addr(thread, in_bytes(G1ThreadLocalData::card_table_base_offset()));\n+  __ ldr(tmp2, card_table_addr);                         \/\/ tmp2 := card table base address\n+  if (UseCondCardMark) {\n+    __ ldrb(rscratch1, Address(tmp1, tmp2));             \/\/ rscratch1 := card\n+    \/\/ Instead of loading clean_card_val and comparing, we exploit the fact that\n+    \/\/ the LSB of non-clean cards is always 0, and the LSB of clean cards 1.\n+    __ tbz(rscratch1, 0, done);\n+  }\n+  static_assert(G1CardTable::dirty_card_val() == 0, \"must be to use zr\");\n+  __ strb(zr, Address(tmp1, tmp2));                      \/\/ *(card address) := dirty_card_val\n@@ -252,6 +267,0 @@\n-  assert(thread == rthread, \"must be\");\n-  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2,\n-                             rscratch1);\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg\n-         && tmp2 != noreg, \"expecting a register\");\n-\n@@ -259,14 +268,1 @@\n-  Label runtime;\n-\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n-  \/\/ If card is young, jump to done\n-  __ br(Assembler::EQ, done);\n-  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n-\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n-  __ pop(saved, sp);\n-\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, false \/* new_val_may_be_null *\/);\n@@ -332,32 +328,4 @@\n-                                                     G1PostBarrierStubC2* stub) {\n-  assert(thread == rthread, \"must be\");\n-  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2,\n-                             rscratch1);\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg\n-         && tmp2 != noreg, \"expecting a register\");\n-\n-  stub->initialize_registers(thread, tmp1, tmp2);\n-\n-  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n-  \/\/ If card is not young, jump to stub (slow path)\n-  __ br(Assembler::NE, *stub->entry());\n-\n-  __ bind(*stub->continuation());\n-}\n-\n-void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                                          G1PostBarrierStubC2* stub) const {\n-  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n-  Label runtime;\n-  Register thread = stub->thread();\n-  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n-  Register tmp2 = stub->tmp2();\n-  assert(stub->tmp3() == noreg, \"not needed in this platform\");\n-\n-  __ bind(*stub->entry());\n-  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n-\n-  __ bind(runtime);\n-  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n-  __ b(*stub->continuation());\n+                                                     bool new_val_may_be_null) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, new_val_may_be_null);\n+  __ bind(done);\n@@ -459,12 +427,0 @@\n-void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n-  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n-  __ bind(*stub->entry());\n-  assert(stub->addr()->is_register(), \"Precondition.\");\n-  assert(stub->new_val()->is_register(), \"Precondition.\");\n-  Register new_val_reg = stub->new_val()->as_register();\n-  __ cbz(new_val_reg, *stub->continuation());\n-  ce->store_parameter(stub->addr()->as_pointer_register(), 0);\n-  __ far_call(RuntimeAddress(bs->post_barrier_c1_runtime_code_blob()->code_begin()));\n-  __ b(*stub->continuation());\n-}\n-\n@@ -473,0 +429,11 @@\n+void G1BarrierSetAssembler::g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  masm->bind(done);\n+}\n+\n@@ -524,68 +491,0 @@\n-void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n-  __ prologue(\"g1_post_barrier\", false);\n-\n-  \/\/ arg0: store_address\n-  Address store_addr(rfp, 2*BytesPerWord);\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-  CardTable* ct = ctbs->card_table();\n-\n-  Label done;\n-  Label runtime;\n-\n-  \/\/ At this point we know new_value is non-null and the new_value crosses regions.\n-  \/\/ Must check to see if card is already dirty\n-\n-  const Register thread = rthread;\n-\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  const Register card_offset = rscratch2;\n-  \/\/ LR is free here, so we can use it to hold the byte_map_base.\n-  const Register byte_map_base = lr;\n-\n-  assert_different_registers(card_offset, byte_map_base, rscratch1);\n-\n-  __ load_parameter(0, card_offset);\n-  __ lsr(card_offset, card_offset, CardTable::card_shift());\n-  __ load_byte_map_base(byte_map_base);\n-  __ ldrb(rscratch1, Address(byte_map_base, card_offset));\n-  __ cmpw(rscratch1, (int)G1CardTable::g1_young_card_val());\n-  __ br(Assembler::EQ, done);\n-\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n-\n-  __ membar(Assembler::StoreLoad);\n-  __ ldrb(rscratch1, Address(byte_map_base, card_offset));\n-  __ cbzw(rscratch1, done);\n-\n-  \/\/ storing region crossing non-null, card is clean.\n-  \/\/ dirty card and log.\n-  __ strb(zr, Address(byte_map_base, card_offset));\n-\n-  \/\/ Convert card offset into an address in card_addr\n-  Register card_addr = card_offset;\n-  __ add(card_addr, byte_map_base, card_addr);\n-\n-  __ ldr(rscratch1, queue_index);\n-  __ cbz(rscratch1, runtime);\n-  __ sub(rscratch1, rscratch1, wordSize);\n-  __ str(rscratch1, queue_index);\n-\n-  \/\/ Reuse LR to hold buffer_addr\n-  const Register buffer_addr = lr;\n-\n-  __ ldr(buffer_addr, buffer);\n-  __ str(card_addr, Address(buffer_addr, rscratch1));\n-  __ b(done);\n-\n-  __ bind(runtime);\n-  __ push_call_clobbered_registers();\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_call_clobbered_registers();\n-  __ bind(done);\n-  __ epilogue();\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":62,"deletions":163,"binary":false,"changes":225,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,0 @@\n-class G1PostBarrierStub;\n@@ -37,1 +36,0 @@\n-class G1PostBarrierStubC2;\n@@ -68,1 +66,0 @@\n-  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n@@ -71,1 +68,7 @@\n-  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2);\n@@ -90,3 +93,1 @@\n-                                G1PostBarrierStubC2* c2_stub);\n-  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                     G1PostBarrierStubC2* stub) const;\n+                                bool new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.hpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,1 +65,1 @@\n-  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+  if (!G1BarrierStubC2::needs_post_barrier(node)) {\n@@ -70,2 +70,2 @@\n-  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, rthread, tmp1, tmp2, stub);\n+  bool new_val_may_be_null = G1BarrierStubC2::post_new_val_may_be_null(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, rthread, tmp1, tmp2, new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1_aarch64.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -205,0 +205,1 @@\n+                                            const Register thread,\n@@ -209,1 +210,2 @@\n-  \/\/ Does store cross heap regions?\n+  assert(thread == Rthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, noreg);\n@@ -211,0 +213,1 @@\n+  \/\/ Does store cross heap regions?\n@@ -219,2 +222,0 @@\n-  \/\/ storing region crossing non-null, is card already dirty?\n-  const Register card_addr = tmp1;\n@@ -222,3 +223,4 @@\n-  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-  __ mov_address(tmp2, (address)ct->card_table()->byte_map_base());\n-  __ add(card_addr, tmp2, AsmOperand(store_addr, lsr, CardTable::card_shift()));\n+  \/\/ storing region crossing non-null, is card already non-clean?\n+  Address card_table_addr(thread, in_bytes(G1ThreadLocalData::card_table_base_offset()));\n+  __ ldr(tmp2, card_table_addr);\n+  __ add(tmp1, tmp2, AsmOperand(store_addr, lsr, CardTable::card_shift()));\n@@ -226,3 +228,6 @@\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cmp(tmp2, (int)G1CardTable::g1_young_card_val());\n-}\n+  if (UseCondCardMark) {\n+    __ ldrb(tmp2, Address(tmp1));\n+    \/\/ Instead of loading clean_card_val and comparing, we exploit the fact that\n+    \/\/ the LSB of non-clean cards is always 0, and the LSB of clean cards 1.\n+    __ tbz(tmp2, 0, done);\n+  }\n@@ -230,24 +235,3 @@\n-static void generate_post_barrier_slow_path(MacroAssembler* masm,\n-                                            const Register thread,\n-                                            const Register tmp1,\n-                                            const Register tmp2,\n-                                            const Register tmp3,\n-                                            Label& done,\n-                                            Label& runtime) {\n-  __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad), tmp2);\n-  assert(CardTable::dirty_card_val() == 0, \"adjust this code\");\n-  \/\/ card_addr is loaded by generate_post_barrier_fast_path\n-  const Register card_addr = tmp1;\n-  __ ldrb(tmp2, Address(card_addr));\n-  __ cbz(tmp2, done);\n-\n-  \/\/ storing a region crossing, non-null oop, card is clean.\n-  \/\/ dirty card and log.\n-\n-  __ strb(__ zero_register(tmp2), Address(card_addr));\n-  generate_queue_test_and_insertion(masm,\n-                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                                    runtime,\n-                                    thread, card_addr, tmp2, tmp3);\n-  __ b(done);\n+  static_assert(G1CardTable::dirty_card_val() == 0, \"must be to use zero_register()\");\n+  __ zero_register(tmp2);\n+  __ strb(tmp2, Address(tmp1));                   \/\/ *(card address) := dirty_card_val\n@@ -256,1 +240,0 @@\n-\n@@ -260,5 +243,5 @@\n-                                           Register store_addr,\n-                                           Register new_val,\n-                                           Register tmp1,\n-                                           Register tmp2,\n-                                           Register tmp3) {\n+                                                  Register store_addr,\n+                                                  Register new_val,\n+                                                  Register tmp1,\n+                                                  Register tmp2,\n+                                                  Register tmp3) {\n@@ -266,23 +249,1 @@\n-  Label runtime;\n-\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n-  \/\/ If card is young, jump to done\n-  \/\/ card_addr and card are loaded by generate_post_barrier_fast_path\n-  const Register card      = tmp2;\n-  const Register card_addr = tmp1;\n-   __ b(done, eq);\n-  generate_post_barrier_slow_path(masm, Rthread, card_addr, tmp2, tmp3, done, runtime);\n-\n-  __ bind(runtime);\n-\n-  RegisterSet set = RegisterSet(store_addr) | RegisterSet(R0, R3) | RegisterSet(R12);\n-  __ push(set);\n-\n-  if (card_addr != R0) {\n-    __ mov(R0, card_addr);\n-  }\n-  __ mov(R1, Rthread);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), R0, R1);\n-\n-  __ pop(set);\n-\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, Rthread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n@@ -348,29 +309,4 @@\n-                                                     G1PostBarrierStubC2* stub) {\n-  assert(thread == Rthread, \"must be\");\n-  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, noreg);\n-\n-  stub->initialize_registers(thread, tmp1, tmp2, tmp3);\n-\n-  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n-  \/\/ If card is not young, jump to stub (slow path)\n-  __ b(*stub->entry(), ne);\n-\n-  __ bind(*stub->continuation());\n-}\n-\n-void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                                          G1PostBarrierStubC2* stub) const {\n-  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n-  Label runtime;\n-  Register thread = stub->thread();\n-  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n-  Register tmp2 = stub->tmp2();\n-  Register tmp3 = stub->tmp3();\n-\n-  __ bind(*stub->entry());\n-  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, tmp3,  *stub->continuation(), runtime);\n-\n-  __ bind(runtime);\n-  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp2);\n-  __ b(*stub->continuation());\n+                                                     bool new_val_may_be_null) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, new_val_may_be_null);\n+  __ bind(done);\n@@ -467,11 +403,11 @@\n-void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n-  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n-  __ bind(*stub->entry());\n-  assert(stub->addr()->is_register(), \"Precondition.\");\n-  assert(stub->new_val()->is_register(), \"Precondition.\");\n-  Register new_val_reg = stub->new_val()->as_register();\n-  __ cbz(new_val_reg, *stub->continuation());\n-  ce->verify_reserved_argument_area_size(1);\n-  __ str(stub->addr()->as_pointer_register(), Address(SP));\n-  __ call(bs->post_barrier_c1_runtime_code_blob()->code_begin(), relocInfo::runtime_call_type);\n-  __ b(*stub->continuation());\n+#undef __\n+\n+void G1BarrierSetAssembler::g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  masm->bind(done);\n@@ -480,1 +416,0 @@\n-#undef __\n@@ -540,96 +475,0 @@\n-void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n-  \/\/ Input:\n-  \/\/ - store_addr, pushed on the stack\n-\n-  __ set_info(\"g1_post_barrier_slow_id\", false);\n-\n-  Label done;\n-  Label recheck;\n-  Label runtime;\n-\n-  Address queue_index(Rthread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(Rthread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  AddressLiteral cardtable(ci_card_table_address_as<address>(), relocInfo::none);\n-\n-  \/\/ save at least the registers that need saving if the runtime is called\n-  const RegisterSet saved_regs = RegisterSet(R0,R3) | RegisterSet(R12) | RegisterSet(LR);\n-  const int nb_saved_regs = 6;\n-  assert(nb_saved_regs == saved_regs.size(), \"fix nb_saved_regs\");\n-  __ push(saved_regs);\n-\n-  const Register r_card_addr_0 = R0; \/\/ must be R0 for the slow case\n-  const Register r_obj_0 = R0;\n-  const Register r_card_base_1 = R1;\n-  const Register r_tmp2 = R2;\n-  const Register r_index_2 = R2;\n-  const Register r_buffer_3 = R3;\n-  const Register tmp1 = Rtemp;\n-\n-  __ ldr(r_obj_0, Address(SP, nb_saved_regs*wordSize));\n-  \/\/ Note: there is a comment in x86 code about not using\n-  \/\/ ExternalAddress \/ lea, due to relocation not working\n-  \/\/ properly for that address. Should be OK for arm, where we\n-  \/\/ explicitly specify that 'cardtable' has a relocInfo::none\n-  \/\/ type.\n-  __ lea(r_card_base_1, cardtable);\n-  __ add(r_card_addr_0, r_card_base_1, AsmOperand(r_obj_0, lsr, CardTable::card_shift()));\n-\n-  \/\/ first quick check without barrier\n-  __ ldrb(r_tmp2, Address(r_card_addr_0));\n-\n-  __ cmp(r_tmp2, (int)G1CardTable::g1_young_card_val());\n-  __ b(recheck, ne);\n-\n-  __ bind(done);\n-\n-  __ pop(saved_regs);\n-\n-  __ ret();\n-\n-  __ bind(recheck);\n-\n-  __ membar(MacroAssembler::Membar_mask_bits(MacroAssembler::StoreLoad), tmp1);\n-\n-  \/\/ reload card state after the barrier that ensures the stored oop was visible\n-  __ ldrb(r_tmp2, Address(r_card_addr_0));\n-\n-  assert(CardTable::dirty_card_val() == 0, \"adjust this code\");\n-  __ cbz(r_tmp2, done);\n-\n-  \/\/ storing region crossing non-null, card is clean.\n-  \/\/ dirty card and log.\n-\n-  assert(0 == (int)CardTable::dirty_card_val(), \"adjust this code\");\n-  if ((ci_card_table_address_as<intptr_t>() & 0xff) == 0) {\n-    \/\/ Card table is aligned so the lowest byte of the table address base is zero.\n-    __ strb(r_card_base_1, Address(r_card_addr_0));\n-  } else {\n-    __ strb(__ zero_register(r_tmp2), Address(r_card_addr_0));\n-  }\n-\n-  __ ldr(r_index_2, queue_index);\n-  __ ldr(r_buffer_3, buffer);\n-\n-  __ subs(r_index_2, r_index_2, wordSize);\n-  __ b(runtime, lt); \/\/ go to runtime if now negative\n-\n-  __ str(r_index_2, queue_index);\n-\n-  __ str(r_card_addr_0, Address(r_buffer_3, r_index_2));\n-\n-  __ b(done);\n-\n-  __ bind(runtime);\n-\n-  __ save_live_registers();\n-\n-  assert(r_card_addr_0 == c_rarg0, \"card_addr should be in R0\");\n-  __ mov(c_rarg1, Rthread);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), c_rarg0, c_rarg1);\n-\n-  __ restore_live_registers_without_return();\n-\n-  __ b(done);\n-}\n-\n","filename":"src\/hotspot\/cpu\/arm\/gc\/g1\/g1BarrierSetAssembler_arm.cpp","additions":38,"deletions":199,"binary":false,"changes":237,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,0 @@\n-class G1PostBarrierStub;\n@@ -37,1 +36,0 @@\n-class G1PostBarrierStubC2;\n@@ -69,1 +67,0 @@\n-  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n@@ -72,1 +69,7 @@\n-  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2);\n@@ -92,3 +95,1 @@\n-                                G1PostBarrierStubC2* c2_stub);\n-  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                     G1PostBarrierStubC2* stub) const;\n+                                bool new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/arm\/gc\/g1\/g1BarrierSetAssembler_arm.hpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -66,1 +66,1 @@\n-  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+  if (!G1BarrierStubC2::needs_post_barrier(node)) {\n@@ -71,2 +71,2 @@\n-  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Rthread, tmp1, tmp2, tmp3, stub);\n+  bool new_val_may_be_null = G1BarrierStubC2::post_new_val_may_be_null(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Rthread, tmp1, tmp2, tmp3, new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/arm\/gc\/g1\/g1_arm.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -233,30 +232,10 @@\n-static void generate_region_crossing_test(MacroAssembler* masm, const Register store_addr, const Register new_val) {\n-  __ xorr(R0, store_addr, new_val);                  \/\/ tmp1 := store address ^ new value\n-  __ srdi_(R0, R0, G1HeapRegion::LogOfHRGrainBytes); \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n-}\n-\n-static Address generate_card_young_test(MacroAssembler* masm, const Register store_addr, const Register tmp1, const Register tmp2) {\n-  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-  __ load_const_optimized(tmp1, (address)(ct->card_table()->byte_map_base()), tmp2);\n-  __ srdi(tmp2, store_addr, CardTable::card_shift());        \/\/ tmp1 := card address relative to card table base\n-  __ lbzx(R0, tmp1, tmp2);                                   \/\/ tmp1 := card address\n-  __ cmpwi(CR0, R0, (int)G1CardTable::g1_young_card_val());\n-  return Address(tmp1, tmp2); \/\/ return card address\n-}\n-\n-static void generate_card_dirty_test(MacroAssembler* masm, Address card_addr) {\n-  __ membar(Assembler::StoreLoad);                        \/\/ Must reload after StoreLoad membar due to concurrent refinement\n-  __ lbzx(R0, card_addr.base(), card_addr.index());       \/\/ tmp2 := card\n-  __ cmpwi(CR0, R0, (int)G1CardTable::dirty_card_val()); \/\/ tmp2 := card == dirty_card_val?\n-}\n-\n-void G1BarrierSetAssembler::g1_write_barrier_post(MacroAssembler* masm, DecoratorSet decorators,\n-                                                  Register store_addr, Register new_val,\n-                                                  Register tmp1, Register tmp2, Register tmp3,\n-                                                  MacroAssembler::PreservationLevel preservation_level) {\n-  bool not_null = (decorators & IS_NOT_NULL) != 0;\n-\n-  Label runtime, filtered;\n-  assert_different_registers(store_addr, new_val, tmp1, tmp2);\n-\n-  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n+  assert_different_registers(store_addr, new_val, tmp1, R0);\n+  assert_different_registers(store_addr, tmp1, tmp2, R0);\n@@ -264,2 +243,3 @@\n-  generate_region_crossing_test(masm, store_addr, new_val);\n-  __ beq(CR0, filtered);\n+  __ xorr(R0, store_addr, new_val);                          \/\/ R0 := store address ^ new value\n+  __ srdi_(R0, R0, G1HeapRegion::LogOfHRGrainBytes);         \/\/ R0 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ beq(CR0, done);\n@@ -268,1 +248,1 @@\n-  if (not_null) {\n+  if (!new_val_may_be_null) {\n@@ -271,1 +251,1 @@\n-    __ asm_assert_ne(\"null oop not allowed (G1 post)\"); \/\/ Checked by caller.\n+    __ asm_assert_ne(\"null oop not allowed (G1 post)\");      \/\/ Checked by caller.\n@@ -275,1 +255,1 @@\n-    __ beq(CR0, filtered);\n+    __ beq(CR0, done);\n@@ -278,19 +258,7 @@\n-  Address card_addr = generate_card_young_test(masm, store_addr, tmp1, tmp2);\n-  __ beq(CR0, filtered);\n-\n-  generate_card_dirty_test(masm, card_addr);\n-  __ beq(CR0, filtered);\n-\n-  __ li(R0, (int)G1CardTable::dirty_card_val());\n-  __ stbx(R0, card_addr.base(), card_addr.index()); \/\/ *(card address) := dirty_card_val\n-\n-  Register Rcard_addr = tmp3;\n-  __ add(Rcard_addr, card_addr.base(), card_addr.index()); \/\/ This is the address which needs to get enqueued.\n-\n-  generate_queue_insertion(masm,\n-                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                           runtime, Rcard_addr, tmp1);\n-  __ b(filtered);\n-\n-  __ bind(runtime);\n+  __ ld(tmp1, G1ThreadLocalData::card_table_base_offset(), thread);\n+  __ srdi(tmp2, store_addr, CardTable::card_shift());        \/\/ tmp2 := card address relative to card table base\n+  if (UseCondCardMark) {\n+    __ lbzx(R0, tmp1, tmp2);\n+    __ cmpwi(CR0, R0, (int)G1CardTable::clean_card_val());\n+    __ bne(CR0, done);\n+  }\n@@ -298,2 +266,3 @@\n-  assert(preservation_level == MacroAssembler::PRESERVATION_NONE,\n-         \"g1_write_barrier_post doesn't support preservation levels higher than PRESERVATION_NONE\");\n+  __ li(R0, G1CardTable::dirty_card_val());\n+  __ stbx(R0, tmp1, tmp2);\n+}\n@@ -301,2 +270,4 @@\n-  \/\/ Save the live input values.\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), Rcard_addr, R16_thread);\n+void G1BarrierSetAssembler::g1_write_barrier_post(MacroAssembler* masm, DecoratorSet decorators,\n+                                                  Register store_addr, Register new_val,\n+                                                  Register tmp1, Register tmp2) {\n+  bool not_null = (decorators & IS_NOT_NULL) != 0;\n@@ -304,1 +275,3 @@\n-  __ bind(filtered);\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, R16_thread, tmp1, tmp2, done, !not_null);\n+  __ bind(done);\n@@ -336,2 +309,1 @@\n-                          tmp1, tmp2, tmp3,\n-                          preservation_level);\n+                          tmp1, tmp2);\n@@ -460,1 +432,1 @@\n-                                                     G1PostBarrierStubC2* stub,\n+                                                     bool new_val_may_be_null,\n@@ -465,1 +437,1 @@\n-  stub->initialize_registers(R16_thread, tmp1, tmp2);\n+  Label done;\n@@ -467,1 +439,0 @@\n-  bool null_check_required = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n@@ -472,1 +443,1 @@\n-    if (null_check_required && CompressedOops::base() != nullptr) {\n+    if (new_val_may_be_null && CompressedOops::base() != nullptr) {\n@@ -476,2 +447,2 @@\n-      __ beq(CR0, *stub->continuation());\n-      null_check_required = false;\n+      __ beq(CR0, done);\n+      new_val_may_be_null = false;\n@@ -482,42 +453,2 @@\n-  generate_region_crossing_test(masm, store_addr, new_val_decoded);\n-  __ beq(CR0, *stub->continuation());\n-\n-  \/\/ crosses regions, storing null?\n-  if (null_check_required) {\n-    __ cmpdi(CR0, new_val_decoded, 0);\n-    __ beq(CR0, *stub->continuation());\n-  }\n-\n-  Address card_addr = generate_card_young_test(masm, store_addr, tmp1, tmp2);\n-  assert(card_addr.base() == tmp1 && card_addr.index() == tmp2, \"needed by post barrier stub\");\n-  __ bc_far_optimized(Assembler::bcondCRbiIs0, __ bi0(CR0, Assembler::equal), *stub->entry());\n-\n-  __ bind(*stub->continuation());\n-}\n-\n-void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                                          G1PostBarrierStubC2* stub) const {\n-  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n-  Label runtime;\n-  Address card_addr(stub->tmp1(), stub->tmp2()); \/\/ See above.\n-\n-  __ bind(*stub->entry());\n-\n-  generate_card_dirty_test(masm, card_addr);\n-  __ bc_far_optimized(Assembler::bcondCRbiIs1, __ bi0(CR0, Assembler::equal), *stub->continuation());\n-\n-  __ li(R0, (int)G1CardTable::dirty_card_val());\n-  __ stbx(R0, card_addr.base(), card_addr.index()); \/\/ *(card address) := dirty_card_val\n-\n-  Register Rcard_addr = stub->tmp1();\n-  __ add(Rcard_addr, card_addr.base(), card_addr.index()); \/\/ This is the address which needs to get enqueued.\n-\n-  generate_queue_insertion(masm,\n-                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                           runtime, Rcard_addr, stub->tmp2());\n-  __ b(*stub->continuation());\n-\n-  __ bind(runtime);\n-  generate_c2_barrier_runtime_call(masm, stub, Rcard_addr, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n-  __ b(*stub->continuation());\n+  generate_post_barrier_fast_path(masm, store_addr, new_val_decoded, R16_thread, tmp1, tmp2, done, new_val_may_be_null);\n+  __ bind(done);\n@@ -561,11 +492,1 @@\n-void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n-  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n-  __ bind(*stub->entry());\n-\n-  assert(stub->addr()->is_register(), \"Precondition.\");\n-  assert(stub->new_val()->is_register(), \"Precondition.\");\n-  Register addr_reg = stub->addr()->as_pointer_register();\n-  Register new_val_reg = stub->new_val()->as_register();\n-\n-  __ cmpdi(CR0, new_val_reg, 0);\n-  __ bc_far_optimized(Assembler::bcondCRbiIs1, __ bi0(CR0, Assembler::equal), *stub->continuation());\n+#undef __\n@@ -573,7 +494,9 @@\n-  address c_code = bs->post_barrier_c1_runtime_code_blob()->code_begin();\n-  \/\/__ load_const_optimized(R0, c_code);\n-  __ add_const_optimized(R0, R29_TOC, MacroAssembler::offset_to_global_toc(c_code));\n-  __ mtctr(R0);\n-  __ mr(R0, addr_reg); \/\/ Pass addr in R0.\n-  __ bctrl();\n-  __ b(*stub->continuation());\n+void G1BarrierSetAssembler::g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  masm->bind(done);\n@@ -582,1 +505,0 @@\n-#undef __\n@@ -645,80 +567,0 @@\n-void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n-  G1BarrierSet* bs = barrier_set_cast<G1BarrierSet>(BarrierSet::barrier_set());\n-\n-  __ set_info(\"g1_post_barrier_slow_id\", false);\n-\n-  \/\/ Using stack slots: spill addr, spill tmp2\n-  const int stack_slots = 2;\n-  Register tmp = R0;\n-  Register addr = R14;\n-  Register tmp2 = R15;\n-  CardTable::CardValue* byte_map_base = bs->card_table()->byte_map_base();\n-\n-  Label restart, refill, ret;\n-\n-  \/\/ Spill\n-  __ std(addr, -8, R1_SP);\n-  __ std(tmp2, -16, R1_SP);\n-\n-  __ srdi(addr, R0, CardTable::card_shift()); \/\/ Addr is passed in R0.\n-  __ load_const_optimized(\/*cardtable*\/ tmp2, byte_map_base, tmp);\n-  __ add(addr, tmp2, addr);\n-  __ lbz(tmp, 0, addr); \/\/ tmp := [addr + cardtable]\n-\n-  \/\/ Return if young card.\n-  __ cmpwi(CR0, tmp, G1CardTable::g1_young_card_val());\n-  __ beq(CR0, ret);\n-\n-  \/\/ Return if sequential consistent value is already dirty.\n-  __ membar(Assembler::StoreLoad);\n-  __ lbz(tmp, 0, addr); \/\/ tmp := [addr + cardtable]\n-\n-  __ cmpwi(CR0, tmp, G1CardTable::dirty_card_val());\n-  __ beq(CR0, ret);\n-\n-  \/\/ Not dirty.\n-\n-  \/\/ First, dirty it.\n-  __ li(tmp, G1CardTable::dirty_card_val());\n-  __ stb(tmp, 0, addr);\n-\n-  int dirty_card_q_index_byte_offset = in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset());\n-  int dirty_card_q_buf_byte_offset = in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset());\n-\n-  __ bind(restart);\n-\n-  \/\/ Get the index into the update buffer. G1DirtyCardQueue::_index is\n-  \/\/ a size_t so ld_ptr is appropriate here.\n-  __ ld(tmp2, dirty_card_q_index_byte_offset, R16_thread);\n-\n-  \/\/ index == 0?\n-  __ cmpdi(CR0, tmp2, 0);\n-  __ beq(CR0, refill);\n-\n-  __ ld(tmp, dirty_card_q_buf_byte_offset, R16_thread);\n-  __ addi(tmp2, tmp2, -oopSize);\n-\n-  __ std(tmp2, dirty_card_q_index_byte_offset, R16_thread);\n-  __ add(tmp2, tmp, tmp2);\n-  __ std(addr, 0, tmp2); \/\/ [_buf + index] := <address_of_card>\n-\n-  \/\/ Restore temp registers and return-from-leaf.\n-  __ bind(ret);\n-  __ ld(tmp2, -16, R1_SP);\n-  __ ld(addr, -8, R1_SP);\n-  __ blr();\n-\n-  __ bind(refill);\n-  const int nbytes_save = (MacroAssembler::num_volatile_regs + stack_slots) * BytesPerWord;\n-  __ save_volatile_gprs(R1_SP, -nbytes_save); \/\/ except R0\n-  __ mflr(R0);\n-  __ std(R0, _abi0(lr), R1_SP);\n-  __ push_frame_reg_args(nbytes_save, R0); \/\/ dummy frame for C call\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1DirtyCardQueueSet::handle_zero_index_for_thread), R16_thread);\n-  __ pop_frame();\n-  __ ld(R0, _abi0(lr), R1_SP);\n-  __ mtlr(R0);\n-  __ restore_volatile_gprs(R1_SP, -nbytes_save); \/\/ except R0\n-  __ b(restart);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1BarrierSetAssembler_ppc.cpp","additions":51,"deletions":209,"binary":false,"changes":260,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,0 @@\n-class G1PostBarrierStub;\n@@ -42,1 +41,0 @@\n-class G1PostBarrierStubC2;\n@@ -59,2 +57,1 @@\n-                             Register tmp1, Register tmp2, Register tmp3,\n-                             MacroAssembler::PreservationLevel preservation_level);\n+                             Register tmp1, Register tmp2);\n@@ -82,1 +79,1 @@\n-                                G1PostBarrierStubC2* c2_stub,\n+                                bool new_val_may_be_null,\n@@ -84,2 +81,0 @@\n-  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                     G1PostBarrierStubC2* stub) const;\n@@ -89,1 +84,0 @@\n-  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n@@ -92,1 +86,8 @@\n-  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1BarrierSetAssembler_ppc.hpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,1 +67,1 @@\n-  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+  if (!G1BarrierStubC2::needs_post_barrier(node)) {\n@@ -72,2 +72,2 @@\n-  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, tmp1, tmp2, stub, decode_new_val);\n+  bool new_val_may_be_null = G1BarrierStubC2::post_new_val_may_be_null(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, tmp1, tmp2, new_val_may_be_null, decode_new_val);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/g1\/g1_ppc.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -195,0 +195,1 @@\n+                                            const Register thread,\n@@ -199,0 +200,2 @@\n+  assert(thread == xthread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, noreg);\n@@ -203,0 +206,1 @@\n+\n@@ -207,1 +211,1 @@\n-  \/\/ Storing region crossing non-null, is card young?\n+  \/\/ Storing region crossing non-null, is card clean?\n@@ -209,4 +213,0 @@\n-  __ load_byte_map_base(tmp2);                           \/\/ tmp2 := card table base address\n-  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n-  __ lbu(tmp2, Address(tmp1));                           \/\/ tmp2 := card\n-}\n@@ -214,19 +214,12 @@\n-static void generate_post_barrier_slow_path(MacroAssembler* masm,\n-                                            const Register thread,\n-                                            const Register tmp1,\n-                                            const Register tmp2,\n-                                            Label& done,\n-                                            Label& runtime) {\n-  __ membar(MacroAssembler::StoreLoad);  \/\/ StoreLoad membar\n-  __ lbu(tmp2, Address(tmp1));           \/\/ tmp2 := card\n-  __ beqz(tmp2, done, true);\n-  \/\/ Storing a region crossing, non-null oop, card is clean.\n-  \/\/ Dirty card and log.\n-  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n-  __ sb(zr, Address(tmp1));       \/\/ *(card address) := dirty_card_val\n-  generate_queue_test_and_insertion(masm,\n-                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                                    runtime,\n-                                    thread, tmp1, tmp2, t0);\n-  __ j(done);\n+  Address card_table_address(xthread, G1ThreadLocalData::card_table_base_offset());\n+  __ ld(tmp2, card_table_address);                       \/\/ tmp2 := card table base address\n+  __ add(tmp1, tmp1, tmp2);                              \/\/ tmp1 := card address\n+  if (UseCondCardMark) {\n+    static_assert((uint)G1CardTable::clean_card_val() == 0xff, \"must be\");\n+    __ lbu(tmp2, Address(tmp1, 0));                      \/\/ tmp2 := card\n+    __ sub(tmp2, tmp2, G1CardTable::clean_card_val());   \/\/ Convert to clean_card_value() to a comparison\n+                                                         \/\/ against zero to avoid use of an extra temp.\n+    __ bnez(tmp2, done);\n+  }\n+  static_assert((uint)G1CardTable::dirty_card_val() == 0, \"must be to use zr\");\n+  __ sb(zr, Address(tmp1, 0));\n@@ -241,5 +234,0 @@\n-  assert(thread == xthread, \"must be\");\n-  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, t0);\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg,\n-         \"expecting a register\");\n-\n@@ -247,15 +235,1 @@\n-  Label runtime;\n-\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n-  \/\/ If card is young, jump to done (tmp2 holds the card value)\n-  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n-  __ beq(tmp2, t0, done);   \/\/ card == young_card_val?\n-  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, done, runtime);\n-\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr);\n-  __ push_reg(saved, sp);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp1, thread);\n-  __ pop_reg(saved, sp);\n-\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n@@ -321,31 +295,4 @@\n-                                                     G1PostBarrierStubC2* stub) {\n-  assert(thread == xthread, \"must be\");\n-  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, t0);\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg,\n-         \"expecting a register\");\n-\n-  stub->initialize_registers(thread, tmp1, tmp2);\n-\n-  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp1, tmp2, *stub->continuation(), new_val_may_be_null);\n-  \/\/ If card is not young, jump to stub (slow path) (tmp2 holds the card value)\n-  __ mv(t0, (int)G1CardTable::g1_young_card_val());\n-  __ bne(tmp2, t0, *stub->entry(), true);\n-\n-  __ bind(*stub->continuation());\n-}\n-\n-void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                                          G1PostBarrierStubC2* stub) const {\n-  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n-  Label runtime;\n-  Register thread = stub->thread();\n-  Register tmp1 = stub->tmp1(); \/\/ tmp1 holds the card address.\n-  Register tmp2 = stub->tmp2();\n-\n-  __ bind(*stub->entry());\n-  generate_post_barrier_slow_path(masm, thread, tmp1, tmp2, *stub->continuation(), runtime);\n-\n-  __ bind(runtime);\n-  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n-  __ j(*stub->continuation());\n+                                                     bool new_val_may_be_null) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, new_val_may_be_null);\n+  __ bind(done);\n@@ -446,12 +393,0 @@\n-void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n-  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n-  __ bind(*stub->entry());\n-  assert(stub->addr()->is_register(), \"Precondition\");\n-  assert(stub->new_val()->is_register(), \"Precondition\");\n-  Register new_val_reg = stub->new_val()->as_register();\n-  __ beqz(new_val_reg, *stub->continuation(), \/* is_far *\/ true);\n-  ce->store_parameter(stub->addr()->as_pointer_register(), 0);\n-  __ far_call(RuntimeAddress(bs->post_barrier_c1_runtime_code_blob()->code_begin()));\n-  __ j(*stub->continuation());\n-}\n-\n@@ -460,0 +395,11 @@\n+void G1BarrierSetAssembler::g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  masm->bind(done);\n+}\n+\n@@ -510,68 +456,0 @@\n-void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n-  __ prologue(\"g1_post_barrier\", false);\n-\n-  \/\/ arg0 : store_address\n-  Address store_addr(fp, 2 * BytesPerWord); \/\/ 2 BytesPerWord from fp\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-\n-  Label done;\n-  Label runtime;\n-\n-  \/\/ At this point we know new_value is non-null and the new_value crosses regions.\n-  \/\/ Must check to see if card is already dirty\n-  const Register thread = xthread;\n-\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  const Register card_offset = t1;\n-  \/\/ RA is free here, so we can use it to hold the byte_map_base.\n-  const Register byte_map_base = ra;\n-\n-  assert_different_registers(card_offset, byte_map_base, t0);\n-\n-  __ load_parameter(0, card_offset);\n-  __ srli(card_offset, card_offset, CardTable::card_shift());\n-  __ load_byte_map_base(byte_map_base);\n-\n-  \/\/ Convert card offset into an address in card_addr\n-  Register card_addr = card_offset;\n-  __ add(card_addr, byte_map_base, card_addr);\n-\n-  __ lbu(t0, Address(card_addr, 0));\n-  __ sub(t0, t0, (int)G1CardTable::g1_young_card_val());\n-  __ beqz(t0, done);\n-\n-  assert((int)CardTable::dirty_card_val() == 0, \"must be 0\");\n-\n-  __ membar(MacroAssembler::StoreLoad);\n-  __ lbu(t0, Address(card_addr, 0));\n-  __ beqz(t0, done);\n-\n-  \/\/ storing region crossing non-null, card is clean.\n-  \/\/ dirty card and log.\n-  __ sb(zr, Address(card_addr, 0));\n-\n-  __ ld(t0, queue_index);\n-  __ beqz(t0, runtime);\n-  __ subi(t0, t0, wordSize);\n-  __ sd(t0, queue_index);\n-\n-  \/\/ Reuse RA to hold buffer_addr\n-  const Register buffer_addr = ra;\n-\n-  __ ld(buffer_addr, buffer);\n-  __ add(t0, buffer_addr, t0);\n-  __ sd(card_addr, Address(t0, 0));\n-  __ j(done);\n-\n-  __ bind(runtime);\n-  __ push_call_clobbered_registers();\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-  __ pop_call_clobbered_registers();\n-  __ bind(done);\n-  __ epilogue();\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.cpp","additions":33,"deletions":155,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,0 @@\n-class G1PostBarrierStub;\n@@ -40,1 +39,0 @@\n-class G1PostBarrierStubC2;\n@@ -71,1 +69,0 @@\n-  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n@@ -74,1 +71,8 @@\n-  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2);\n+\n@@ -93,3 +97,1 @@\n-                                G1PostBarrierStubC2* c2_stub);\n-  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                     G1PostBarrierStubC2* stub) const;\n+                                bool new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1BarrierSetAssembler_riscv.hpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -66,1 +66,1 @@\n-  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+  if (!G1BarrierStubC2::needs_post_barrier(node)) {\n@@ -71,2 +71,2 @@\n-  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, xthread, tmp1, tmp2, stub);\n+  bool new_val_may_be_null = G1BarrierStubC2::post_new_val_may_be_null(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, xthread, tmp1, tmp2, new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/g1\/g1_riscv.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -208,11 +207,8 @@\n-void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n-                                                     Register store_addr,\n-                                                     Register new_val,\n-                                                     Register thread,\n-                                                     Register tmp1,\n-                                                     Register tmp2,\n-                                                     G1PostBarrierStubC2* stub) {\n-  BLOCK_COMMENT(\"g1_write_barrier_post_c2 {\");\n-\n-  assert(thread == Z_thread, \"must be\");\n-  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, Z_R1_scratch);\n+static void generate_post_barrier_fast_path(MacroAssembler* masm,\n+                                            const Register store_addr,\n+                                            const Register new_val,\n+                                            const Register thread,\n+                                            const Register tmp1,\n+                                            const Register tmp2,\n+                                            Label& done,\n+                                            bool new_val_may_be_null) {\n@@ -220,1 +216,1 @@\n-  assert(store_addr != noreg && new_val != noreg && tmp1 != noreg && tmp2 != noreg, \"expecting a register\");\n+  __ block_comment(\"generate_post_barrier_fast_path {\");\n@@ -222,1 +218,2 @@\n-  stub->initialize_registers(thread, tmp1, tmp2);\n+  assert(thread == Z_thread, \"must be\");\n+  assert_different_registers(store_addr, new_val, thread, tmp1, tmp2, noreg);\n@@ -224,1 +221,1 @@\n-  BLOCK_COMMENT(\"generate_region_crossing_test {\");\n+  \/\/ Does store cross heap regions?\n@@ -226,1 +223,1 @@\n-    __ z_xgrk(tmp1, store_addr, new_val);\n+    __ z_xgrk(tmp1, store_addr, new_val);    \/\/ tmp1 := store address ^ new value\n@@ -231,3 +228,2 @@\n-  __ z_srag(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n-  BLOCK_COMMENT(\"} generate_region_crossing_test\");\n+  __ z_srag(tmp1, tmp1, G1HeapRegion::LogOfHRGrainBytes); \/\/ tmp1 := ((store address ^ new value) >> LogOfHRGrainBytes)\n+  __ branch_optimized(Assembler::bcondEqual, done);\n@@ -235,2 +231,2 @@\n-  \/\/ crosses regions, storing null?\n-  if ((stub->barrier_data() & G1C2BarrierPostNotNull) == 0) {\n+  \/\/ Crosses regions, storing null?\n+  if (new_val_may_be_null) {\n@@ -238,1 +234,6 @@\n-    __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n+    __ z_bre(done);\n+  } else {\n+#ifdef ASSERT\n+    __ z_ltgr(new_val, new_val);\n+    __ asm_assert(Assembler::bcondNotZero, \"null oop not allowed (G1 post)\", 0x322); \/\/ Checked by caller.\n+#endif\n@@ -241,9 +242,1 @@\n-  BLOCK_COMMENT(\"generate_card_young_test {\");\n-  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-  \/\/ calculate address of card\n-  __ load_const_optimized(tmp2, (address)ct->card_table()->byte_map_base());      \/\/ Card table base.\n-  __ z_srlg(tmp1, store_addr, CardTable::card_shift());         \/\/ Index into card table.\n-  __ z_algr(tmp1, tmp2);                                      \/\/ Explicit calculation needed for cli.\n-\n-  \/\/ Filter young.\n-  __ z_cli(0, tmp1, G1CardTable::g1_young_card_val());\n+  __ z_srag(tmp1, store_addr, CardTable::card_shift());\n@@ -251,1 +244,2 @@\n-  BLOCK_COMMENT(\"} generate_card_young_test\");\n+  Address card_table_addr(thread, in_bytes(G1ThreadLocalData::card_table_base_offset()));\n+  __ z_alg(tmp1, card_table_addr);     \/\/ tmp1 := card address\n@@ -253,2 +247,4 @@\n-  \/\/ From here on, tmp1 holds the card address.\n-  __ branch_optimized(Assembler::bcondNotEqual, *stub->entry());\n+  if(UseCondCardMark) {\n+    __ z_cli(0, tmp1, G1CardTable::clean_card_val());\n+    __ branch_optimized(Assembler::bcondNotEqual, done);\n+  }\n@@ -256,1 +252,2 @@\n-  __ bind(*stub->continuation());\n+  static_assert(G1CardTable::dirty_card_val() == 0, \"must be to use z_mvi\");\n+  __ z_mvi(0, tmp1, G1CardTable::dirty_card_val()); \/\/ *(card address) := dirty_card_val\n@@ -258,1 +255,1 @@\n-  BLOCK_COMMENT(\"} g1_write_barrier_post_c2\");\n+  __ block_comment(\"} generate_post_barrier_fast_path\");\n@@ -261,43 +258,12 @@\n-void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                                          G1PostBarrierStubC2* stub) const {\n-\n-  BLOCK_COMMENT(\"generate_c2_post_barrier_stub {\");\n-\n-  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n-  Label runtime;\n-\n-  Register thread     = stub->thread();\n-  Register tmp1       = stub->tmp1(); \/\/ tmp1 holds the card address.\n-  Register tmp2       = stub->tmp2();\n-  Register Rcard_addr = tmp1;\n-\n-  __ bind(*stub->entry());\n-\n-  BLOCK_COMMENT(\"generate_card_clean_test {\");\n-  __ z_sync(); \/\/ Required to support concurrent cleaning.\n-  __ z_cli(0, Rcard_addr, 0); \/\/ Reload after membar.\n-  __ branch_optimized(Assembler::bcondEqual, *stub->continuation());\n-  BLOCK_COMMENT(\"} generate_card_clean_test\");\n-\n-  BLOCK_COMMENT(\"generate_dirty_card {\");\n-  \/\/ Storing a region crossing, non-null oop, card is clean.\n-  \/\/ Dirty card and log.\n-  STATIC_ASSERT(CardTable::dirty_card_val() == 0);\n-  __ z_mvi(0, Rcard_addr, CardTable::dirty_card_val());\n-  BLOCK_COMMENT(\"} generate_dirty_card\");\n-\n-  generate_queue_test_and_insertion(masm,\n-                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                                    runtime,\n-                                    Z_thread, tmp1, tmp2);\n-\n-  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n-\n-  __ bind(runtime);\n-\n-  generate_c2_barrier_runtime_call(masm, stub, tmp1, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n-\n-  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n-\n-  BLOCK_COMMENT(\"} generate_c2_post_barrier_stub\");\n+void G1BarrierSetAssembler::g1_write_barrier_post_c2(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2,\n+                                                     bool new_val_may_be_null) {\n+  BLOCK_COMMENT(\"g1_write_barrier_post_c2 {\");\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, new_val_may_be_null);\n+  __ bind(done);\n+  BLOCK_COMMENT(\"} g1_write_barrier_post_c2\");\n@@ -454,93 +420,3 @@\n-  assert_different_registers(Rstore_addr, Rnew_val, Rtmp1, Rtmp2); \/\/ Most probably, Rnew_val == Rtmp3.\n-\n-  Label callRuntime, filtered;\n-\n-  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-\n-  BLOCK_COMMENT(\"g1_write_barrier_post {\");\n-\n-  \/\/ Does store cross heap regions?\n-  \/\/ It does if the two addresses specify different grain addresses.\n-  if (VM_Version::has_DistinctOpnds()) {\n-    __ z_xgrk(Rtmp1, Rstore_addr, Rnew_val);\n-  } else {\n-    __ z_lgr(Rtmp1, Rstore_addr);\n-    __ z_xgr(Rtmp1, Rnew_val);\n-  }\n-  __ z_srag(Rtmp1, Rtmp1, G1HeapRegion::LogOfHRGrainBytes);\n-  __ z_bre(filtered);\n-\n-  \/\/ Crosses regions, storing null?\n-  if (not_null) {\n-#ifdef ASSERT\n-    __ z_ltgr(Rnew_val, Rnew_val);\n-    __ asm_assert(Assembler::bcondNotZero, \"null oop not allowed (G1 post)\", 0x322); \/\/ Checked by caller.\n-#endif\n-  } else {\n-    __ z_ltgr(Rnew_val, Rnew_val);\n-    __ z_bre(filtered);\n-  }\n-\n-  Rnew_val = noreg; \/\/ end of lifetime\n-\n-  \/\/ Storing region crossing non-null, is card already dirty?\n-  assert_different_registers(Rtmp1, Rtmp2, Rtmp3);\n-  \/\/ Make sure not to use Z_R0 for any of these registers.\n-  Register Rcard_addr = (Rtmp1 != Z_R0_scratch) ? Rtmp1 : Rtmp3;\n-  Register Rbase      = (Rtmp2 != Z_R0_scratch) ? Rtmp2 : Rtmp3;\n-\n-  \/\/ calculate address of card\n-  __ load_const_optimized(Rbase, (address)ct->card_table()->byte_map_base());      \/\/ Card table base.\n-  __ z_srlg(Rcard_addr, Rstore_addr, CardTable::card_shift());         \/\/ Index into card table.\n-  __ z_algr(Rcard_addr, Rbase);                                      \/\/ Explicit calculation needed for cli.\n-  Rbase = noreg; \/\/ end of lifetime\n-\n-  \/\/ Filter young.\n-  __ z_cli(0, Rcard_addr, G1CardTable::g1_young_card_val());\n-  __ z_bre(filtered);\n-\n-  \/\/ Check the card value. If dirty, we're done.\n-  \/\/ This also avoids false sharing of the (already dirty) card.\n-  __ z_sync(); \/\/ Required to support concurrent cleaning.\n-  __ z_cli(0, Rcard_addr, G1CardTable::dirty_card_val()); \/\/ Reload after membar.\n-  __ z_bre(filtered);\n-\n-  \/\/ Storing a region crossing, non-null oop, card is clean.\n-  \/\/ Dirty card and log.\n-  __ z_mvi(0, Rcard_addr, G1CardTable::dirty_card_val());\n-\n-  Register Rcard_addr_x = Rcard_addr;\n-  Register Rqueue_index = (Rtmp2 != Z_R0_scratch) ? Rtmp2 : Rtmp1;\n-  if (Rcard_addr == Rqueue_index) {\n-    Rcard_addr_x = Z_R0_scratch;  \/\/ Register shortage. We have to use Z_R0.\n-  }\n-  __ lgr_if_needed(Rcard_addr_x, Rcard_addr);\n-\n-  generate_queue_test_and_insertion(masm,\n-                                    G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                                    G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                                    callRuntime,\n-                                    Z_thread, Rcard_addr_x, Rqueue_index);\n-  __ z_bru(filtered);\n-\n-  __ bind(callRuntime);\n-\n-  \/\/ TODO: do we need a frame? Introduced to be on the safe side.\n-  bool needs_frame = true;\n-  __ lgr_if_needed(Rcard_addr, Rcard_addr_x); \/\/ copy back asap. push_frame will destroy Z_R0_scratch!\n-\n-  \/\/ VM call need frame to access(write) O register.\n-  if (needs_frame) {\n-    __ save_return_pc();\n-    __ push_frame_abi160(0); \/\/ Will use Z_R0 as tmp on old CPUs.\n-  }\n-\n-  \/\/ Save the live input values.\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), Rcard_addr, Z_thread);\n-\n-  if (needs_frame) {\n-    __ pop_frame();\n-    __ restore_return_pc();\n-  }\n-\n-  __ bind(filtered);\n+  Label done;\n+  generate_post_barrier_fast_path(masm, Rstore_addr, Rnew_val, Z_thread, Rtmp1, Rtmp2, done, !not_null);\n+  __ bind(done);\n@@ -618,14 +494,0 @@\n-void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n-  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n-  __ bind(*stub->entry());\n-  ce->check_reserved_argument_area(16); \/\/ RT stub needs 2 spill slots.\n-  assert(stub->addr()->is_register(), \"Precondition.\");\n-  assert(stub->new_val()->is_register(), \"Precondition.\");\n-  Register new_val_reg = stub->new_val()->as_register();\n-  __ z_ltgr(new_val_reg, new_val_reg);\n-  __ branch_optimized(Assembler::bcondZero, *stub->continuation());\n-  __ z_lgr(Z_R1_scratch, stub->addr()->as_pointer_register());\n-  ce->emit_call_c(bs->post_barrier_c1_runtime_code_blob()->code_begin());\n-  __ branch_optimized(Assembler::bcondAlways, *stub->continuation());\n-}\n-\n@@ -634,0 +496,11 @@\n+void G1BarrierSetAssembler::g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2) {\n+   Label done;\n+   generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+   masm->bind(done);\n+}\n+\n@@ -708,86 +581,0 @@\n-void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n-  \/\/ Z_R1_scratch: oop address, address of updated memory slot\n-\n-  BarrierSet* bs = BarrierSet::barrier_set();\n-  __ set_info(\"g1_post_barrier_slow_id\", false);\n-\n-  Register addr_oop  = Z_R1_scratch;\n-  Register addr_card = Z_R1_scratch;\n-  Register r1        = Z_R6; \/\/ Must be saved\/restored.\n-  Register r2        = Z_R7; \/\/ Must be saved\/restored.\n-  Register cardtable = r1;   \/\/ Must be non-volatile, because it is used to save addr_card.\n-  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n-  CardTable* ct = ctbs->card_table();\n-  CardTable::CardValue* byte_map_base = ct->byte_map_base();\n-\n-  \/\/ Save registers used below (see assertion in G1PreBarrierStub::emit_code()).\n-  __ z_stg(r1, 0*BytesPerWord + FrameMap::first_available_sp_in_frame, Z_SP);\n-\n-  Label not_already_dirty, restart, refill, young_card;\n-\n-  \/\/ Calculate address of card corresponding to the updated oop slot.\n-  AddressLiteral rs(byte_map_base);\n-  __ z_srlg(addr_card, addr_oop, CardTable::card_shift());\n-  addr_oop = noreg; \/\/ dead now\n-  __ load_const_optimized(cardtable, rs); \/\/ cardtable := <card table base>\n-  __ z_agr(addr_card, cardtable); \/\/ addr_card := addr_oop>>card_shift + cardtable\n-\n-  __ z_cli(0, addr_card, (int)G1CardTable::g1_young_card_val());\n-  __ z_bre(young_card);\n-\n-  __ z_sync(); \/\/ Required to support concurrent cleaning.\n-\n-  __ z_cli(0, addr_card, (int)CardTable::dirty_card_val());\n-  __ z_brne(not_already_dirty);\n-\n-  __ bind(young_card);\n-  \/\/ We didn't take the branch, so we're already dirty: restore\n-  \/\/ used registers and return.\n-  __ z_lg(r1, 0*BytesPerWord + FrameMap::first_available_sp_in_frame, Z_SP);\n-  __ z_br(Z_R14);\n-\n-  \/\/ Not dirty.\n-  __ bind(not_already_dirty);\n-\n-  \/\/ First, dirty it: [addr_card] := 0\n-  __ z_mvi(0, addr_card, CardTable::dirty_card_val());\n-\n-  Register idx = cardtable; \/\/ Must be non-volatile, because it is used to save addr_card.\n-  Register buf = r2;\n-  cardtable = noreg; \/\/ now dead\n-\n-  \/\/ Save registers used below (see assertion in G1PreBarrierStub::emit_code()).\n-  __ z_stg(r2, 1*BytesPerWord + FrameMap::first_available_sp_in_frame, Z_SP);\n-\n-  ByteSize dirty_card_q_index_byte_offset = G1ThreadLocalData::dirty_card_queue_index_offset();\n-  ByteSize dirty_card_q_buf_byte_offset = G1ThreadLocalData::dirty_card_queue_buffer_offset();\n-\n-  __ bind(restart);\n-\n-  \/\/ Get the index into the update buffer. G1DirtyCardQueue::_index is\n-  \/\/ a size_t so z_ltg is appropriate here.\n-  __ z_ltg(idx, Address(Z_thread, dirty_card_q_index_byte_offset));\n-\n-  \/\/ index == 0?\n-  __ z_brz(refill);\n-\n-  __ z_lg(buf, Address(Z_thread, dirty_card_q_buf_byte_offset));\n-  __ add2reg(idx, -oopSize);\n-\n-  __ z_stg(addr_card, 0, idx, buf); \/\/ [_buf + index] := <address_of_card>\n-  __ z_stg(idx, Address(Z_thread, dirty_card_q_index_byte_offset));\n-  \/\/ Restore killed registers and return.\n-  __ z_lg(r1, 0*BytesPerWord + FrameMap::first_available_sp_in_frame, Z_SP);\n-  __ z_lg(r2, 1*BytesPerWord + FrameMap::first_available_sp_in_frame, Z_SP);\n-  __ z_br(Z_R14);\n-\n-  __ bind(refill);\n-  save_volatile_registers(sasm);\n-  __ z_lgr(idx, addr_card); \/\/ Save addr_card, tmp3 must be non-volatile.\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1DirtyCardQueueSet::handle_zero_index_for_thread),\n-                                   Z_thread);\n-  __ z_lgr(addr_card, idx);\n-  restore_volatile_registers(sasm); \/\/ Restore addr_card.\n-  __ z_bru(restart);\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.cpp","additions":59,"deletions":272,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,1 +36,0 @@\n-class G1PostBarrierStub;\n@@ -38,1 +37,0 @@\n-class G1PostBarrierStubC2;\n@@ -63,1 +61,0 @@\n-  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n@@ -66,1 +63,8 @@\n-  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n+\n+  void g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2);\n+\n@@ -84,3 +88,1 @@\n-                                G1PostBarrierStubC2* c2_stub);\n-  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                     G1PostBarrierStubC2* stub) const;\n+                                bool new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1BarrierSetAssembler_s390.hpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,1 +65,1 @@\n-  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+  if (!G1BarrierStubC2::needs_post_barrier(node)) {\n@@ -70,2 +70,2 @@\n-  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Z_thread, tmp1, tmp2, stub);\n+  bool new_val_may_be_null = G1BarrierStubC2::post_new_val_may_be_null(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, Z_thread, tmp1, tmp2, new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/s390\/gc\/g1\/g1_s390.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -103,15 +103,15 @@\n-  __ push_call_clobbered_registers(false \/* save_fpu *\/);\n-#ifdef _LP64\n-  if (c_rarg0 == count) { \/\/ On win64 c_rarg0 == rcx\n-    assert_different_registers(c_rarg1, addr);\n-    __ mov(c_rarg1, count);\n-    __ mov(c_rarg0, addr);\n-  } else {\n-    assert_different_registers(c_rarg0, count);\n-    __ mov(c_rarg0, addr);\n-    __ mov(c_rarg1, count);\n-  }\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_post_entry), 2);\n-#else\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_post_entry),\n-                  addr, count);\n+  Label done;\n+\n+  __ testptr(count, count);\n+  __ jcc(Assembler::equal, done);\n+\n+  \/\/ Calculate end address in \"count\".\n+  Address::ScaleFactor scale = UseCompressedOops ? Address::times_4 : Address::times_8;\n+  __ leaq(count, Address(addr, count, scale));\n+\n+  \/\/ Calculate start card address in \"addr\".\n+  __ shrptr(addr, CardTable::card_shift());\n+\n+  Register thread = LP64_ONLY(r15_thread) NOT_LP64(tmp);\n+#ifndef __LP64\n+  __ get_thread(thread);\n@@ -119,1 +119,29 @@\n-  __ pop_call_clobbered_registers(false \/* save_fpu *\/);\n+  __ movptr(tmp, Address(thread, in_bytes(G1ThreadLocalData::card_table_base_offset())));\n+  __ addptr(addr, tmp);\n+\n+  \/\/ Calculate address of card of last word in the array.\n+  __ subptr(count, 1);\n+  __ shrptr(count, CardTable::card_shift());\n+  __ addptr(count, tmp);\n+\n+  Label loop;\n+  \/\/ Iterate from start card to end card (inclusive).\n+  __ bind(loop);\n+\n+  Label is_clean_card;\n+  __ cmpb(Address(addr, 0), G1CardTable::clean_card_val());\n+  __ jcc(Assembler::equal, is_clean_card);\n+\n+  Label next_card;\n+  __ bind(next_card);\n+  __ addptr(addr, sizeof(CardTable::CardValue));\n+  __ cmpptr(addr, count);\n+  __ jcc(Assembler::belowEqual, loop);\n+  __ jmp(done);\n+\n+  __ bind(is_clean_card);\n+  \/\/ Card was not clean. Dirty card and go to next..\n+  __ movb(Address(addr, 0), G1CardTable::dirty_card_val());\n+  __ jmp(next_card);\n+\n+  __ bind(done);\n@@ -225,1 +253,0 @@\n-\n@@ -288,1 +315,2 @@\n-                                            const Register tmp,\n+                                            const Register thread,\n+                                            const Register tmp1,\n@@ -292,1 +320,5 @@\n-  CardTableBarrierSet* ct = barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n+#ifdef _LP64\n+  assert(thread == r15_thread, \"must be\");\n+#endif \/\/ _LP64\n+  assert_different_registers(store_addr, new_val, thread, tmp1 \/*, tmp2 unused *\/, noreg);\n+\n@@ -294,3 +326,3 @@\n-  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n-  __ xorptr(tmp, new_val);                                       \/\/ tmp := store address ^ new value\n-  __ shrptr(tmp, G1HeapRegion::LogOfHRGrainBytes);               \/\/ ((store address ^ new value) >> LogOfHRGrainBytes) == 0?\n+  __ movptr(tmp1, store_addr);                                    \/\/ tmp1 := store address\n+  __ xorptr(tmp1, new_val);                                       \/\/ tmp1 := store address ^ new value\n+  __ shrptr(tmp1, G1HeapRegion::LogOfHRGrainBytes);               \/\/ ((store address ^ new value) >> LogOfHRGrainBytes) == 0?\n@@ -298,0 +330,1 @@\n+\n@@ -300,1 +333,1 @@\n-    __ cmpptr(new_val, NULL_WORD);                               \/\/ new value == null?\n+    __ cmpptr(new_val, NULL_WORD);                                \/\/ new value == null?\n@@ -303,9 +336,0 @@\n-  \/\/ Storing region crossing non-null, is card young?\n-  __ movptr(tmp, store_addr);                                    \/\/ tmp := store address\n-  __ shrptr(tmp, CardTable::card_shift());                       \/\/ tmp := card address relative to card table base\n-  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n-  \/\/ a valid address and therefore is not properly handled by the relocation code.\n-  __ movptr(tmp2, (intptr_t)ct->card_table()->byte_map_base());  \/\/ tmp2 := card table base address\n-  __ addptr(tmp, tmp2);                                          \/\/ tmp := card address\n-  __ cmpb(Address(tmp, 0), G1CardTable::g1_young_card_val());    \/\/ *(card address) == young_card_val?\n-}\n@@ -313,9 +337,9 @@\n-static void generate_post_barrier_slow_path(MacroAssembler* masm,\n-                                            const Register thread,\n-                                            const Register tmp,\n-                                            const Register tmp2,\n-                                            Label& done,\n-                                            Label& runtime) {\n-  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));  \/\/ StoreLoad membar\n-  __ cmpb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) == dirty_card_val?\n-  __ jcc(Assembler::equal, done);\n+  __ movptr(tmp1, store_addr);                                    \/\/ tmp1 := store address\n+  __ shrptr(tmp1, CardTable::card_shift());                       \/\/ tmp1 := card address relative to card table base\n+\n+  Address card_table_addr(thread, in_bytes(G1ThreadLocalData::card_table_base_offset()));\n+  __ addptr(tmp1, card_table_addr);                               \/\/ tmp1 := card address\n+  if (UseCondCardMark) {\n+    __ cmpb(Address(tmp1, 0), G1CardTable::clean_card_val());     \/\/ *(card address) == clean_card_val?\n+    __ jcc(Assembler::notEqual, done);\n+  }\n@@ -323,8 +347,2 @@\n-  \/\/ Dirty card and log.\n-  __ movb(Address(tmp, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) := dirty_card_val\n-  generate_queue_insertion(masm,\n-                           G1ThreadLocalData::dirty_card_queue_index_offset(),\n-                           G1ThreadLocalData::dirty_card_queue_buffer_offset(),\n-                           runtime,\n-                           thread, tmp, tmp2);\n-  __ jmp(done);\n+  \/\/ Dirty card.\n+  __ movb(Address(tmp1, 0), G1CardTable::dirty_card_val());       \/\/ *(card address) := dirty_card_val\n@@ -339,4 +357,0 @@\n-#ifdef _LP64\n-  assert(thread == r15_thread, \"must be\");\n-#endif \/\/ _LP64\n-\n@@ -344,14 +358,1 @@\n-  Label runtime;\n-\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, done, true \/* new_val_may_be_null *\/);\n-  \/\/ If card is young, jump to done\n-  __ jcc(Assembler::equal, done);\n-  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, done, runtime);\n-\n-  __ bind(runtime);\n-  \/\/ save the live input values\n-  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n-  __ push_set(saved);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), tmp, thread);\n-  __ pop_set(saved);\n-\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp, tmp2, done, true \/* new_val_may_be_null *\/);\n@@ -428,30 +429,4 @@\n-                                                     G1PostBarrierStubC2* stub) {\n-#ifdef _LP64\n-  assert(thread == r15_thread, \"must be\");\n-#endif \/\/ _LP64\n-\n-  stub->initialize_registers(thread, tmp, tmp2);\n-\n-  bool new_val_may_be_null = (stub->barrier_data() & G1C2BarrierPostNotNull) == 0;\n-  generate_post_barrier_fast_path(masm, store_addr, new_val, tmp, tmp2, *stub->continuation(), new_val_may_be_null);\n-  \/\/ If card is not young, jump to stub (slow path)\n-  __ jcc(Assembler::notEqual, *stub->entry());\n-\n-  __ bind(*stub->continuation());\n-}\n-\n-void G1BarrierSetAssembler::generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                                          G1PostBarrierStubC2* stub) const {\n-  Assembler::InlineSkippedInstructionsCounter skip_counter(masm);\n-  Label runtime;\n-  Register thread = stub->thread();\n-  Register tmp = stub->tmp1(); \/\/ tmp holds the card address.\n-  Register tmp2 = stub->tmp2();\n-  assert(stub->tmp3() == noreg, \"not needed in this platform\");\n-\n-  __ bind(*stub->entry());\n-  generate_post_barrier_slow_path(masm, thread, tmp, tmp2, *stub->continuation(), runtime);\n-\n-  __ bind(runtime);\n-  generate_c2_barrier_runtime_call(masm, stub, tmp, CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry));\n-  __ jmp(*stub->continuation());\n+                                                     bool new_val_may_be_null) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp, tmp2, done, new_val_may_be_null);\n+  __ bind(done);\n@@ -550,13 +525,0 @@\n-void G1BarrierSetAssembler::gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub) {\n-  G1BarrierSetC1* bs = (G1BarrierSetC1*)BarrierSet::barrier_set()->barrier_set_c1();\n-  __ bind(*stub->entry());\n-  assert(stub->addr()->is_register(), \"Precondition.\");\n-  assert(stub->new_val()->is_register(), \"Precondition.\");\n-  Register new_val_reg = stub->new_val()->as_register();\n-  __ cmpptr(new_val_reg, NULL_WORD);\n-  __ jcc(Assembler::equal, *stub->continuation());\n-  ce->store_parameter(stub->addr()->as_pointer_register(), 0);\n-  __ call(RuntimeAddress(bs->post_barrier_c1_runtime_code_blob()->code_begin()));\n-  __ jmp(*stub->continuation());\n-}\n-\n@@ -565,0 +527,11 @@\n+void G1BarrierSetAssembler::g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                                     Register store_addr,\n+                                                     Register new_val,\n+                                                     Register thread,\n+                                                     Register tmp1,\n+                                                     Register tmp2) {\n+  Label done;\n+  generate_post_barrier_fast_path(masm, store_addr, new_val, thread, tmp1, tmp2, done, true \/* new_val_may_be_null *\/);\n+  masm->bind(done);\n+}\n+\n@@ -631,74 +604,0 @@\n-void G1BarrierSetAssembler::generate_c1_post_barrier_runtime_stub(StubAssembler* sasm) {\n-  __ prologue(\"g1_post_barrier\", false);\n-\n-  CardTableBarrierSet* ct =\n-    barrier_set_cast<CardTableBarrierSet>(BarrierSet::barrier_set());\n-\n-  Label done;\n-  Label enqueued;\n-  Label runtime;\n-\n-  \/\/ At this point we know new_value is non-null and the new_value crosses regions.\n-  \/\/ Must check to see if card is already dirty\n-\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-\n-  Address queue_index(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset()));\n-  Address buffer(thread, in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()));\n-\n-  __ push(rax);\n-  __ push(rcx);\n-\n-  const Register cardtable = rax;\n-  const Register card_addr = rcx;\n-\n-  __ load_parameter(0, card_addr);\n-  __ shrptr(card_addr, CardTable::card_shift());\n-  \/\/ Do not use ExternalAddress to load 'byte_map_base', since 'byte_map_base' is NOT\n-  \/\/ a valid address and therefore is not properly handled by the relocation code.\n-  __ movptr(cardtable, (intptr_t)ct->card_table()->byte_map_base());\n-  __ addptr(card_addr, cardtable);\n-\n-  NOT_LP64(__ get_thread(thread);)\n-\n-  __ cmpb(Address(card_addr, 0), G1CardTable::g1_young_card_val());\n-  __ jcc(Assembler::equal, done);\n-\n-  __ membar(Assembler::Membar_mask_bits(Assembler::StoreLoad));\n-  __ cmpb(Address(card_addr, 0), CardTable::dirty_card_val());\n-  __ jcc(Assembler::equal, done);\n-\n-  \/\/ storing region crossing non-null, card is clean.\n-  \/\/ dirty card and log.\n-\n-  __ movb(Address(card_addr, 0), CardTable::dirty_card_val());\n-\n-  const Register tmp = rdx;\n-  __ push(rdx);\n-\n-  __ movptr(tmp, queue_index);\n-  __ testptr(tmp, tmp);\n-  __ jcc(Assembler::zero, runtime);\n-  __ subptr(tmp, wordSize);\n-  __ movptr(queue_index, tmp);\n-  __ addptr(tmp, buffer);\n-  __ movptr(Address(tmp, 0), card_addr);\n-  __ jmp(enqueued);\n-\n-  __ bind(runtime);\n-  __ push_call_clobbered_registers();\n-\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_field_post_entry), card_addr, thread);\n-\n-  __ pop_call_clobbered_registers();\n-\n-  __ bind(enqueued);\n-  __ pop(rdx);\n-\n-  __ bind(done);\n-  __ pop(rcx);\n-  __ pop(rax);\n-\n-  __ epilogue();\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":83,"deletions":184,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,0 @@\n-class G1PostBarrierStub;\n@@ -37,1 +36,0 @@\n-class G1PostBarrierStubC2;\n@@ -63,0 +61,4 @@\n+  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                       Register dst, Address src, Register tmp1, Register tmp_thread);\n+\n+#ifdef COMPILER1\n@@ -64,1 +66,0 @@\n-  void gen_post_barrier_stub(LIR_Assembler* ce, G1PostBarrierStub* stub);\n@@ -67,1 +68,0 @@\n-  void generate_c1_post_barrier_runtime_stub(StubAssembler* sasm);\n@@ -69,2 +69,7 @@\n-  virtual void load_at(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n-                       Register dst, Address src, Register tmp1, Register tmp_thread);\n+  void g1_write_barrier_post_c1(MacroAssembler* masm,\n+                                Register store_addr,\n+                                Register new_val,\n+                                Register thread,\n+                                Register tmp1,\n+                                Register tmp2);\n+#endif\n@@ -87,3 +92,1 @@\n-                                G1PostBarrierStubC2* c2_stub);\n-  void generate_c2_post_barrier_stub(MacroAssembler* masm,\n-                                     G1PostBarrierStubC2* stub) const;\n+                                bool new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.hpp","additions":13,"deletions":10,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -64,1 +64,1 @@\n-  if (!G1PostBarrierStubC2::needs_barrier(node)) {\n+  if (!G1BarrierStubC2::needs_post_barrier(node)) {\n@@ -69,2 +69,2 @@\n-  G1PostBarrierStubC2* const stub = G1PostBarrierStubC2::create(node);\n-  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, r15_thread, tmp1, tmp2, stub);\n+  bool new_val_may_be_null = G1BarrierStubC2::post_new_val_may_be_null(node);\n+  g1_asm->g1_write_barrier_post_c2(masm, store_addr, new_val, r15_thread, tmp1, tmp2, new_val_may_be_null);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1_x86_64.ad","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -33,0 +33,5 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_LIR.hpp\"\n+#include \"c1\/c1_LIRAssembler.hpp\"\n+#include \"c1\/c1_MacroAssembler.hpp\"\n+#endif \/\/ COMPILER1\n@@ -45,5 +50,0 @@\n-void G1PostBarrierStub::emit_code(LIR_Assembler* ce) {\n-  G1BarrierSetAssembler* bs = (G1BarrierSetAssembler*)BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->gen_post_barrier_stub(ce, this);\n-}\n-\n@@ -117,0 +117,81 @@\n+class LIR_OpG1PostBarrier : public LIR_Op {\n+ friend class LIR_OpVisitState;\n+\n+private:\n+  LIR_Opr       _addr;\n+  LIR_Opr       _new_val;\n+  LIR_Opr       _thread;\n+  LIR_Opr       _tmp1;\n+  LIR_Opr       _tmp2;\n+\n+public:\n+  LIR_OpG1PostBarrier(LIR_Opr addr,\n+                      LIR_Opr new_val,\n+                      LIR_Opr thread,\n+                      LIR_Opr tmp1,\n+                      LIR_Opr tmp2)\n+    : LIR_Op(lir_none, lir_none, nullptr),\n+      _addr(addr),\n+      _new_val(new_val),\n+      _thread(thread),\n+      _tmp1(tmp1),\n+      _tmp2(tmp2)\n+    {}\n+\n+  virtual void visit(LIR_OpVisitState* state) {\n+    state->do_input(_addr);\n+    state->do_input(_new_val);\n+    state->do_input(_thread);\n+\n+    \/\/ Use temp registers to ensure these they use different registers.\n+    state->do_temp(_addr);\n+    state->do_temp(_new_val);\n+    state->do_temp(_thread);\n+    state->do_temp(_tmp1);\n+    state->do_temp(_tmp2);\n+\n+    if (_info != nullptr) {\n+      state->do_info(_info);\n+    }\n+  }\n+\n+  virtual void emit_code(LIR_Assembler* ce) {\n+    if (_info != nullptr) {\n+      ce->add_debug_info_for_null_check_here(_info);\n+    }\n+\n+    Register addr = _addr->as_pointer_register();\n+    Register new_val = _new_val->as_pointer_register();\n+    Register thread = _thread->as_pointer_register();\n+    Register tmp1 = _tmp1->as_pointer_register();\n+    Register tmp2 = _tmp2->as_pointer_register();\n+\n+    \/\/ This may happen for a store of x.a = x - we do not need a post barrier for those\n+    \/\/ as the cross-region test will always exit early anyway.\n+    \/\/ The post barrier implementations can assume that addr and new_val are different\n+    \/\/ then.\n+    if (addr == new_val) {\n+      ce->masm()->block_comment(err_msg(\"same addr\/new_val due to self-referential store with imprecise card mark %s\", addr->name()));\n+      return;\n+    }\n+\n+    G1BarrierSetAssembler* bs_asm = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n+    bs_asm->g1_write_barrier_post_c1(ce->masm(), addr, new_val, thread, tmp1, tmp2);\n+  }\n+\n+  virtual void print_instr(outputStream* out) const {\n+    _addr->print(out);     out->print(\" \");\n+    _new_val->print(out);  out->print(\" \");\n+    _thread->print(out);   out->print(\" \");\n+    _tmp1->print(out);     out->print(\" \");\n+    _tmp2->print(out);     out->print(\" \");\n+    out->cr();\n+  }\n+\n+#ifndef PRODUCT\n+  virtual const char* name() const  {\n+    return \"lir_g1_post_barrier\";\n+  }\n+#endif \/\/ PRODUCT\n+};\n+\n@@ -153,23 +234,5 @@\n-  LIR_Opr xor_res = gen->new_pointer_register();\n-  LIR_Opr xor_shift_res = gen->new_pointer_register();\n-  if (two_operand_lir_form) {\n-    __ move(addr, xor_res);\n-    __ logical_xor(xor_res, new_val, xor_res);\n-    __ move(xor_res, xor_shift_res);\n-    __ unsigned_shift_right(xor_shift_res,\n-                            LIR_OprFact::intConst(checked_cast<jint>(G1HeapRegion::LogOfHRGrainBytes)),\n-                            xor_shift_res,\n-                            LIR_Opr::illegalOpr());\n-  } else {\n-    __ logical_xor(addr, new_val, xor_res);\n-    __ unsigned_shift_right(xor_res,\n-                            LIR_OprFact::intConst(checked_cast<jint>(G1HeapRegion::LogOfHRGrainBytes)),\n-                            xor_shift_res,\n-                            LIR_Opr::illegalOpr());\n-  }\n-\n-  __ cmp(lir_cond_notEqual, xor_shift_res, LIR_OprFact::intptrConst(NULL_WORD));\n-\n-  CodeStub* slow = new G1PostBarrierStub(addr, new_val);\n-  __ branch(lir_cond_notEqual, slow);\n-  __ branch_destination(slow->continuation());\n+  __ append(new LIR_OpG1PostBarrier(addr,\n+                                    new_val,\n+                                    gen->getThreadPointer() \/* thread *\/,\n+                                    gen->new_pointer_register() \/* tmp1 *\/,\n+                                    gen->new_pointer_register() \/* tmp2 *\/));\n@@ -210,8 +273,0 @@\n-class C1G1PostBarrierCodeGenClosure : public StubAssemblerCodeGenClosure {\n-  virtual OopMapSet* generate_code(StubAssembler* sasm) {\n-    G1BarrierSetAssembler* bs = (G1BarrierSetAssembler*)BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->generate_c1_post_barrier_runtime_stub(sasm);\n-    return nullptr;\n-  }\n-};\n-\n@@ -220,1 +275,0 @@\n-  C1G1PostBarrierCodeGenClosure post_code_gen_cl;\n@@ -223,2 +277,0 @@\n-  _post_barrier_c1_runtime_code_blob = Runtime1::generate_blob(buffer_blob, C1StubId::NO_STUBID, \"g1_post_barrier_slow\",\n-                                                               false, &post_code_gen_cl);\n","filename":"src\/hotspot\/share\/gc\/g1\/c1\/g1BarrierSetC1.cpp","additions":91,"deletions":39,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -94,28 +94,0 @@\n-class G1PostBarrierStub: public CodeStub {\n-  friend class G1BarrierSetC1;\n- private:\n-  LIR_Opr _addr;\n-  LIR_Opr _new_val;\n-\n- public:\n-  \/\/ addr (the address of the object head) and new_val must be registers.\n-  G1PostBarrierStub(LIR_Opr addr, LIR_Opr new_val): _addr(addr), _new_val(new_val) {\n-    FrameMap* f = Compilation::current()->frame_map();\n-    f->update_reserved_argument_area_size(2 * BytesPerWord);\n-  }\n-\n-  LIR_Opr addr() const { return _addr; }\n-  LIR_Opr new_val() const { return _new_val; }\n-\n-  virtual void emit_code(LIR_Assembler* e);\n-  virtual void visit(LIR_OpVisitState* visitor) {\n-    \/\/ don't pass in the code emit info since it's processed in the fast path\n-    visitor->do_slow_case();\n-    visitor->do_input(_addr);\n-    visitor->do_input(_new_val);\n-  }\n-#ifndef PRODUCT\n-  virtual void print_name(outputStream* out) const { out->print(\"G1PostBarrierStub\"); }\n-#endif \/\/ PRODUCT\n-};\n-\n@@ -127,1 +99,0 @@\n-  CodeBlob* _post_barrier_c1_runtime_code_blob;\n@@ -137,2 +108,1 @@\n-    : _pre_barrier_c1_runtime_code_blob(nullptr),\n-      _post_barrier_c1_runtime_code_blob(nullptr) {}\n+    : _pre_barrier_c1_runtime_code_blob(nullptr) {}\n@@ -141,1 +111,0 @@\n-  CodeBlob* post_barrier_c1_runtime_code_blob() { return _post_barrier_c1_runtime_code_blob; }\n","filename":"src\/hotspot\/share\/gc\/g1\/c1\/g1BarrierSetC1.hpp","additions":2,"deletions":33,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -301,1 +301,7 @@\n-    nodes += 60;\n+    \/\/ Approximate the number of nodes needed; an if costs 4 nodes (Cmp, Bool,\n+    \/\/ If, If projection), any other (Assembly) instruction is approximated with\n+    \/\/ a cost of 1.\n+    nodes +=   4  \/\/ base cost for the card write containing getting base offset, address calculation and the card write;\n+             + 6  \/\/ same region check: Uncompress (new_val) oop, xor, shr, (cmp), jmp\n+             + 4  \/\/ new_val is null check\n+             + 4; \/\/ card not clean check.\n@@ -389,2 +395,2 @@\n-    return G1PreBarrierStubC2::needs_barrier(mach) ||\n-           G1PostBarrierStubC2::needs_barrier(mach);\n+    return G1BarrierStubC2::needs_pre_barrier(mach) ||\n+           G1BarrierStubC2::needs_post_barrier(mach);\n@@ -404,0 +410,12 @@\n+bool G1BarrierStubC2::needs_pre_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPre) != 0;\n+}\n+\n+bool G1BarrierStubC2::needs_post_barrier(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPost) != 0;\n+}\n+\n+bool G1BarrierStubC2::post_new_val_may_be_null(const MachNode* node) {\n+  return (node->barrier_data() & G1C2BarrierPostNotNull) == 0;\n+}\n+\n@@ -407,1 +425,1 @@\n-  return (node->barrier_data() & G1C2BarrierPre) != 0;\n+  return needs_pre_barrier(node);\n@@ -451,42 +469,0 @@\n-G1PostBarrierStubC2::G1PostBarrierStubC2(const MachNode* node) : G1BarrierStubC2(node) {}\n-\n-bool G1PostBarrierStubC2::needs_barrier(const MachNode* node) {\n-  return (node->barrier_data() & G1C2BarrierPost) != 0;\n-}\n-\n-G1PostBarrierStubC2* G1PostBarrierStubC2::create(const MachNode* node) {\n-  G1PostBarrierStubC2* const stub = new (Compile::current()->comp_arena()) G1PostBarrierStubC2(node);\n-  if (!Compile::current()->output()->in_scratch_emit_size()) {\n-    barrier_set_state()->stubs()->append(stub);\n-  }\n-  return stub;\n-}\n-\n-void G1PostBarrierStubC2::initialize_registers(Register thread, Register tmp1, Register tmp2, Register tmp3) {\n-  _thread = thread;\n-  _tmp1 = tmp1;\n-  _tmp2 = tmp2;\n-  _tmp3 = tmp3;\n-}\n-\n-Register G1PostBarrierStubC2::thread() const {\n-  return _thread;\n-}\n-\n-Register G1PostBarrierStubC2::tmp1() const {\n-  return _tmp1;\n-}\n-\n-Register G1PostBarrierStubC2::tmp2() const {\n-  return _tmp2;\n-}\n-\n-Register G1PostBarrierStubC2::tmp3() const {\n-  return _tmp3;\n-}\n-\n-void G1PostBarrierStubC2::emit_code(MacroAssembler& masm) {\n-  G1BarrierSetAssembler* bs = static_cast<G1BarrierSetAssembler*>(BarrierSet::barrier_set()->barrier_set_assembler());\n-  bs->generate_c2_post_barrier_stub(&masm, this);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":22,"deletions":46,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -40,0 +40,4 @@\n+  static bool needs_pre_barrier(const MachNode* node);\n+  static bool needs_post_barrier(const MachNode* node);\n+  static bool post_new_val_may_be_null(const MachNode* node);\n+\n@@ -67,21 +71,0 @@\n-class G1PostBarrierStubC2 : public G1BarrierStubC2 {\n-private:\n-  Register _thread;\n-  Register _tmp1;\n-  Register _tmp2;\n-  Register _tmp3;\n-\n-protected:\n-  G1PostBarrierStubC2(const MachNode* node);\n-\n-public:\n-  static bool needs_barrier(const MachNode* node);\n-  static G1PostBarrierStubC2* create(const MachNode* node);\n-  void initialize_registers(Register thread, Register tmp1 = noreg, Register tmp2 = noreg, Register tmp3 = noreg);\n-  Register thread() const;\n-  Register tmp1() const;\n-  Register tmp2() const;\n-  Register tmp3() const;\n-  virtual void emit_code(MacroAssembler& masm);\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.hpp","additions":4,"deletions":21,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -265,3 +265,0 @@\n-  if (result != nullptr) {\n-    _g1h->dirty_young_block(result, *actual_word_size);\n-  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Allocator.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-static double young_card_scan_to_merge_ratio_defaults[] = {\n+static double young_card_merge_to_scan_ratio_defaults[] = {\n@@ -78,2 +78,1 @@\n-    _dirtied_cards_in_thread_buffers_seq(TruncatedSeqLength),\n-    _card_scan_to_merge_ratio_seq(TruncatedSeqLength),\n+    _card_merge_to_scan_ratio_seq(TruncatedSeqLength),\n@@ -87,0 +86,1 @@\n+    _merge_refinement_table_ms_seq(TruncatedSeqLength),\n@@ -105,1 +105,1 @@\n-  _card_scan_to_merge_ratio_seq.set_initial(young_card_scan_to_merge_ratio_defaults[index]);\n+  _card_merge_to_scan_ratio_seq.set_initial(young_card_merge_to_scan_ratio_defaults[index]);\n@@ -111,0 +111,1 @@\n+  _merge_refinement_table_ms_seq.add(0);\n@@ -179,4 +180,0 @@\n-void G1Analytics::report_dirtied_cards_in_thread_buffers(size_t cards) {\n-  _dirtied_cards_in_thread_buffers_seq.add(double(cards));\n-}\n-\n@@ -195,2 +192,2 @@\n-void G1Analytics::report_card_scan_to_merge_ratio(double merge_to_scan_ratio, bool for_young_only_phase) {\n-  _card_scan_to_merge_ratio_seq.add(merge_to_scan_ratio, for_young_only_phase);\n+void G1Analytics::report_card_merge_to_scan_ratio(double merge_to_scan_ratio, bool for_young_only_phase) {\n+  _card_merge_to_scan_ratio_seq.add(merge_to_scan_ratio, for_young_only_phase);\n@@ -211,0 +208,4 @@\n+void G1Analytics::report_merge_refinement_table_time_ms(double merge_refinement_table_time_ms) {\n+  _merge_refinement_table_ms_seq.add(merge_refinement_table_time_ms);\n+}\n+\n@@ -243,4 +244,0 @@\n-size_t G1Analytics::predict_dirtied_cards_in_thread_buffers() const {\n-  return predict_size(&_dirtied_cards_in_thread_buffers_seq);\n-}\n-\n@@ -248,1 +245,1 @@\n-  return card_rs_length * predict_in_unit_interval(&_card_scan_to_merge_ratio_seq, for_young_only_phase);\n+  return card_rs_length * predict_in_unit_interval(&_card_merge_to_scan_ratio_seq, for_young_only_phase);\n@@ -267,0 +264,4 @@\n+double G1Analytics::predict_merge_refinement_table_time_ms() const {\n+  return predict_zero_bounded(&_merge_refinement_table_ms_seq);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Analytics.cpp","additions":16,"deletions":15,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,4 +51,3 @@\n-  TruncatedSeq _dirtied_cards_in_thread_buffers_seq;\n-  \/\/ The ratio between the number of scanned cards and actually merged cards, for\n-  \/\/ young-only and mixed gcs.\n-  G1PhaseDependentSeq _card_scan_to_merge_ratio_seq;\n+  \/\/ The ratio between the number of merged cards to actually scanned cards for\n+  \/\/ card based remembered sets, for young-only and mixed gcs.\n+  G1PhaseDependentSeq _card_merge_to_scan_ratio_seq;\n@@ -58,1 +57,1 @@\n-  \/\/ The cost to merge a card during young-only and mixed gcs in ms.\n+  \/\/ The cost to merge a card from the remembered sets for non-young regions in ms.\n@@ -69,0 +68,2 @@\n+  \/\/ Prediction for merging the refinement table to the card table during GC.\n+  TruncatedSeq _merge_refinement_table_ms_seq;\n@@ -130,1 +131,0 @@\n-  void report_dirtied_cards_in_thread_buffers(size_t num_cards);\n@@ -134,1 +134,1 @@\n-  void report_card_scan_to_merge_ratio(double cards_per_entry_ratio, bool for_young_only_phase);\n+  void report_card_merge_to_scan_ratio(double merge_to_scan_ratio, bool for_young_only_phase);\n@@ -138,0 +138,1 @@\n+  void report_merge_refinement_table_time_ms(double pending_card_merge_time_ms);\n@@ -148,1 +149,0 @@\n-  size_t predict_dirtied_cards_in_thread_buffers() const;\n@@ -161,0 +161,1 @@\n+  double predict_merge_refinement_table_time_ms() const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Analytics.hpp","additions":10,"deletions":9,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -71,0 +71,6 @@\n+  \/\/ G1 prefers to use conditional card marking to avoid overwriting cards that\n+  \/\/ have already been found to contain a to-collection set reference. This reduces\n+  \/\/ refinement effort.\n+  if (FLAG_IS_DEFAULT(UseCondCardMark)) {\n+    FLAG_SET_ERGO(UseCondCardMark, true);\n+  }\n@@ -241,1 +247,1 @@\n-  uint max_parallel_refinement_threads = G1ConcRefinementThreads + G1DirtyCardQueueSet::num_par_ids();\n+  uint max_parallel_refinement_threads = G1ConcRefinementThreads;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"memory\/iterator.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -52,1 +54,2 @@\n-G1BarrierSet::G1BarrierSet(G1CardTable* card_table) :\n+G1BarrierSet::G1BarrierSet(G1CardTable* card_table,\n+                           G1CardTable* refinement_table) :\n@@ -59,1 +62,0 @@\n-  _dirty_card_queue_buffer_allocator(\"DC Buffer Allocator\", G1UpdateBufferSize),\n@@ -61,1 +63,1 @@\n-  _dirty_card_queue_set(&_dirty_card_queue_buffer_allocator)\n+  _refinement_table(refinement_table)\n@@ -64,0 +66,10 @@\n+G1BarrierSet::~G1BarrierSet() {\n+  delete _refinement_table;\n+}\n+\n+void G1BarrierSet::swap_global_card_table() {\n+  G1CardTable* temp = static_cast<G1CardTable*>(_card_table);\n+  _card_table = _refinement_table;\n+  _refinement_table = temp;\n+}\n+\n@@ -92,12 +104,0 @@\n-void G1BarrierSet::write_ref_field_post_slow(volatile CardValue* byte) {\n-  \/\/ In the slow path, we know a card is not young\n-  assert(*byte != G1CardTable::g1_young_card_val(), \"slow path invoked without filtering\");\n-  OrderAccess::storeload();\n-  if (*byte != G1CardTable::dirty_card_val()) {\n-    *byte = G1CardTable::dirty_card_val();\n-    Thread* thr = Thread::current();\n-    G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(thr);\n-    G1BarrierSet::dirty_card_queue_set().enqueue(queue, byte);\n-  }\n-}\n-\n@@ -108,2 +108,0 @@\n-  volatile CardValue* byte = _card_table->byte_for(mr.start());\n-  CardValue* last_byte = _card_table->byte_for(mr.last());\n@@ -111,3 +109,3 @@\n-  \/\/ skip young gen cards\n-  if (*byte == G1CardTable::g1_young_card_val()) {\n-    \/\/ MemRegion should not span multiple regions for the young gen.\n+  \/\/ Skip writes to young gen.\n+  if (G1CollectedHeap::heap()->heap_region_containing(mr.start())->is_young()) {\n+    \/\/ MemRegion should not span multiple regions for arrays in young gen.\n@@ -121,4 +119,15 @@\n-  OrderAccess::storeload();\n-  \/\/ Enqueue if necessary.\n-  G1DirtyCardQueueSet& qset = G1BarrierSet::dirty_card_queue_set();\n-  G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(thread);\n+  \/\/ We need to make sure that we get the start\/end byte information for the area\n+  \/\/ to mark from the same card table to avoid getting confused in the mark loop\n+  \/\/ further below - we might execute while the global card table is being switched.\n+  \/\/\n+  \/\/ It does not matter which card table we write to: at worst we may write to the\n+  \/\/ new card table (after the switching), which means that we will catch the\n+  \/\/ marks next time.\n+  \/\/ If we write to the old card table (after the switching, then the refinement\n+  \/\/ table) the oncoming handshake will do the memory synchronization.\n+  CardTable* card_table = Atomic::load(&_card_table);\n+\n+  volatile CardValue* byte = card_table->byte_for(mr.start());\n+  CardValue* last_byte = card_table->byte_for(mr.last());\n+\n+  \/\/ Dirty cards only if necessary.\n@@ -127,2 +136,1 @@\n-    assert(bv != G1CardTable::g1_young_card_val(), \"Invalid card\");\n-    if (bv != G1CardTable::dirty_card_val()) {\n+    if (bv == G1CardTable::clean_card_val()) {\n@@ -130,1 +138,0 @@\n-      qset.enqueue(queue, byte);\n@@ -151,4 +158,0 @@\n-  G1DirtyCardQueue& dirtyq = G1ThreadLocalData::dirty_card_queue(thread);\n-  assert(dirtyq.buffer() == nullptr, \"Dirty Card queue should not have a buffer\");\n-  assert(dirtyq.index() == 0, \"Dirty Card queue index should be zero\");\n-\n@@ -159,0 +162,4 @@\n+\n+  if (thread->is_Java_thread()) {\n+    G1ThreadLocalData::set_byte_map_base(thread, G1CollectedHeap::heap()->card_table_base());\n+  }\n@@ -168,6 +175,0 @@\n-  {\n-    G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(thread);\n-    G1DirtyCardQueueSet& qset = G1BarrierSet::dirty_card_queue_set();\n-    qset.flush_queue(queue);\n-    qset.record_detached_refinement_stats(queue.refinement_stats());\n-  }\n@@ -179,0 +180,5 @@\n+\n+void G1BarrierSet::print_on(outputStream* st) const {\n+  _card_table->print_on(st, \"Card\");\n+  _refinement_table->print_on(st, \"Refinement\");\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSet.cpp","additions":43,"deletions":37,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -36,3 +35,28 @@\n-\/\/ This barrier is specialized to use a logging barrier to support\n-\/\/ snapshot-at-the-beginning marking.\n-\n+\/\/ This barrier set is specialized to manage two card tables:\n+\/\/ * one the mutator is currently working on (\"card table\")\n+\/\/ * one the refinement threads or GC during pause are working on (\"refinement table\")\n+\/\/\n+\/\/ The card table acts like a regular card table where the mutator dirties cards\n+\/\/ containing potentially interesting references.\n+\/\/\n+\/\/ When the amount of dirty cards on the card table exceeds a threshold, G1 swaps\n+\/\/ the card tables and has the refinement threads reduce them by \"refining\"\n+\/\/ them.\n+\/\/ I.e. refinement looks at all dirty cards on the refinement table, and updates\n+\/\/ the remembered sets accordingly, clearing the cards on the refinement table.\n+\/\/\n+\/\/ Meanwhile the mutator continues dirtying the now empty card table.\n+\/\/\n+\/\/ This separation of data the mutator and refinement threads are working on\n+\/\/ removes the need for any fine-grained (per mutator write) synchronization between\n+\/\/ them, keeping the write barrier simple.\n+\/\/\n+\/\/ The refinement threads mark cards in the current collection set specially on the\n+\/\/ card table - this is fine wrt to synchronization with the mutator, because at\n+\/\/ most the mutator will overwrite it again if there is a race, as G1 will scan the\n+\/\/ entire card either way during the GC pause.\n+\/\/\n+\/\/ During garbage collection, if the refinement table is known to be non-empty, G1\n+\/\/ merges it back (and cleaning it) to the card table which is scanned for dirty\n+\/\/ cards.\n+\/\/\n@@ -43,1 +67,0 @@\n-  BufferNode::Allocator _dirty_card_queue_buffer_allocator;\n@@ -45,1 +68,6 @@\n-  G1DirtyCardQueueSet _dirty_card_queue_set;\n+\n+  G1CardTable* _refinement_table;\n+\n+ public:\n+  G1BarrierSet(G1CardTable* card_table, G1CardTable* refinement_table);\n+  virtual ~G1BarrierSet();\n@@ -51,3 +79,4 @@\n- public:\n-  G1BarrierSet(G1CardTable* table);\n-  ~G1BarrierSet() { }\n+  G1CardTable* refinement_table() const { return _refinement_table; }\n+\n+  \/\/ Swap the global card table references, without synchronization.\n+  void swap_global_card_table();\n@@ -77,1 +106,1 @@\n-  template <DecoratorSet decorators, typename T>\n+  template <DecoratorSet decorators = DECORATORS_NONE, typename T>\n@@ -79,1 +108,0 @@\n-  void write_ref_field_post_slow(volatile CardValue* byte);\n@@ -90,3 +118,1 @@\n-  static G1DirtyCardQueueSet& dirty_card_queue_set() {\n-    return g1_barrier_set()->_dirty_card_queue_set;\n-  }\n+  virtual void print_on(outputStream* st) const;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSet.hpp","additions":41,"deletions":15,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,3 +78,2 @@\n-  if (*byte != G1CardTable::g1_young_card_val()) {\n-    \/\/ Take a slow path for cards in old\n-    write_ref_field_post_slow(byte);\n+  if (*byte == G1CardTable::clean_card_val()) {\n+    *byte = G1CardTable::dirty_card_val();\n@@ -130,1 +129,1 @@\n-  G1BarrierSet *bs = barrier_set_cast<G1BarrierSet>(BarrierSet::barrier_set());\n+  G1BarrierSet *bs = g1_barrier_set();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSet.inline.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  G1BarrierSet *bs = barrier_set_cast<G1BarrierSet>(BarrierSet::barrier_set());\n+  G1BarrierSet *bs = G1BarrierSet::g1_barrier_set();\n@@ -37,1 +37,1 @@\n-  G1BarrierSet *bs = barrier_set_cast<G1BarrierSet>(BarrierSet::barrier_set());\n+  G1BarrierSet *bs = G1BarrierSet::g1_barrier_set();\n@@ -42,1 +42,1 @@\n-  G1BarrierSet *bs = barrier_set_cast<G1BarrierSet>(BarrierSet::barrier_set());\n+  G1BarrierSet *bs = G1BarrierSet::g1_barrier_set();\n@@ -56,8 +56,0 @@\n-\/\/ G1 post write barrier slowpath\n-JRT_LEAF(void, G1BarrierSetRuntime::write_ref_field_post_entry(volatile G1CardTable::CardValue* card_addr,\n-                                                               JavaThread* thread))\n-  assert(thread == JavaThread::current(), \"pre-condition\");\n-  G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(thread);\n-  G1BarrierSet::dirty_card_queue_set().enqueue(queue, card_addr);\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.cpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,1 +50,0 @@\n-  static void write_ref_field_post_entry(volatile CardValue* card_addr, JavaThread* thread);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1BarrierSetRuntime.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,10 +31,26 @@\n-void G1CardTable::g1_mark_as_young(const MemRegion& mr) {\n-  CardValue *const first = byte_for(mr.start());\n-  CardValue *const last = byte_after(mr.last());\n-\n-  memset_with_concurrent_readers(first, g1_young_gen, pointer_delta(last, first, sizeof(CardValue)));\n-}\n-\n-#ifndef PRODUCT\n-void G1CardTable::verify_g1_young_region(MemRegion mr) {\n-  verify_region(mr, g1_young_gen,  true);\n+void G1CardTable::verify_region(MemRegion mr, CardValue val, bool val_equals) {\n+  if (mr.is_empty()) {\n+    return;\n+  }\n+  CardValue* start    = byte_for(mr.start());\n+  CardValue* end      = byte_for(mr.last());\n+  bool failures = false;\n+  for (CardValue* curr = start; curr <= end; ++curr) {\n+    CardValue curr_val = *curr;\n+    bool failed = (val_equals) ? (curr_val != val) : (curr_val == val);\n+    if (failed) {\n+      if (!failures) {\n+        G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+        G1HeapRegion* r = g1h->heap_region_containing(mr.start());\n+        log_error(gc, verify)(\"== CT verification failed: [\" PTR_FORMAT \",\" PTR_FORMAT \"] r: %d (%s) %sexpecting value: %d\",\n+                              p2i(start), p2i(end), r->hrm_index(), r->get_short_type_str(),\n+                              (val_equals) ? \"\" : \"not \", val);\n+        failures = true;\n+      }\n+      log_error(gc, verify)(\"==   card \" PTR_FORMAT \" [\" PTR_FORMAT \",\" PTR_FORMAT \"], val: %d\",\n+                            p2i(curr), p2i(addr_for(curr)),\n+                            p2i((HeapWord*) (((size_t) addr_for(curr)) + _card_size)),\n+                            (int) curr_val);\n+    }\n+  }\n+  guarantee(!failures, \"there should not have been any failures\");\n@@ -42,1 +58,0 @@\n-#endif\n@@ -77,2 +92,1 @@\n-  volatile CardValue* card = byte_for(p);\n-  return *card == G1CardTable::g1_young_card_val();\n+  return G1CollectedHeap::heap()->heap_region_containing(p)->is_young();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTable.cpp","additions":27,"deletions":13,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,2 +55,0 @@\n-    g1_young_gen = CT_MR_BS_last_reserved << 1,\n-\n@@ -66,2 +64,2 @@\n-    \/\/ This means that the LSB determines what to do with the card during evacuation\n-    \/\/ given the following possible values:\n+    \/\/ This means that the LSB determines whether the card is clean or non-clean\n+    \/\/ (LSB is 1 -> clean, LSB is 0 -> non-clean) given the following possible values:\n@@ -70,1 +68,3 @@\n-    \/\/ 00000001 - already scanned, do not scan\n+    \/\/ xxxxxxx1 - clean, already scanned, do not scan again (during GC only).\n+    \/\/ 00000100 - dirty, needs to be scanned, dirty from remembered set (during GC only)\n+    \/\/ 00000010 - dirty, needs to be scanned, contains reference to collection set.\n@@ -73,1 +73,4 @@\n-    g1_card_already_scanned = 0x1\n+    g1_dirty_card = dirty_card,\n+    g1_card_already_scanned = 0x1,\n+    g1_to_cset_card = 0x2,\n+    g1_from_remset_card = 0x4\n@@ -78,0 +81,1 @@\n+  static const size_t WordAllFromRemset = (SIZE_MAX \/ 255) * g1_from_remset_card;\n@@ -86,1 +90,0 @@\n-  static CardValue g1_young_card_val() { return g1_young_gen; }\n@@ -89,2 +92,1 @@\n-  void verify_g1_young_region(MemRegion mr) PRODUCT_RETURN;\n-  void g1_mark_as_young(const MemRegion& mr);\n+  void verify_region(MemRegion mr, CardValue val, bool val_equals) override;\n@@ -96,1 +98,1 @@\n-  \/\/ Mark the given card as Dirty if it is Clean. Returns whether the card was\n+  \/\/ Mark the given card as From Remset if it is Clean. Returns whether the card was\n@@ -99,1 +101,1 @@\n-  inline bool mark_clean_as_dirty(CardValue* card);\n+  inline bool mark_clean_as_from_remset(CardValue* card);\n@@ -101,3 +103,4 @@\n-  \/\/ Change Clean cards in a (large) area on the card table as Dirty, preserving\n-  \/\/ already scanned cards. Assumes that most cards in that area are Clean.\n-  inline void mark_range_dirty(size_t start_card_index, size_t num_cards);\n+  \/\/ Change Clean cards in a (large) area on the card table as From_Remset, preserving\n+  \/\/ cards already marked otherwise. Assumes that most cards in that area are Clean.\n+  \/\/ Not atomic.\n+  inline size_t mark_clean_range_as_from_remset(size_t start_card_index, size_t num_cards);\n@@ -105,2 +108,3 @@\n-  \/\/ Change the given range of dirty cards to \"which\". All of these cards must be Dirty.\n-  inline void change_dirty_cards_to(CardValue* start_card, CardValue* end_card, CardValue which);\n+  \/\/ Change the given range of dirty cards to \"which\". All of these cards must be non-clean.\n+  \/\/ Returns the number of pending cards found.\n+  inline size_t change_dirty_cards_to(CardValue* start_card, CardValue* end_card, CardValue which);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTable.hpp","additions":21,"deletions":17,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,0 @@\n-\n@@ -31,0 +30,1 @@\n+#include \"utilities\/population_count.hpp\"\n@@ -37,1 +37,1 @@\n-inline bool G1CardTable::mark_clean_as_dirty(CardValue* card) {\n+inline bool G1CardTable::mark_clean_as_from_remset(CardValue* card) {\n@@ -40,1 +40,1 @@\n-    *card = dirty_card_val();\n+    *card = g1_from_remset_card;\n@@ -46,1 +46,12 @@\n-inline void G1CardTable::mark_range_dirty(size_t start_card_index, size_t num_cards) {\n+\/\/ Returns bits from a where mask is 0, and bits from b where mask is 1.\n+\/\/\n+\/\/ Example:\n+\/\/ a      = 0xAAAAAAAA\n+\/\/ b      = 0xBBBBBBBB\n+\/\/ mask   = 0xFF00FF00\n+\/\/ result = 0xBBAABBAA\n+inline size_t blend(size_t a, size_t b, size_t mask) {\n+  return a ^ ((a ^ b) & mask);\n+}\n+\n+inline size_t G1CardTable::mark_clean_range_as_from_remset(size_t start_card_index, size_t num_cards) {\n@@ -50,0 +61,2 @@\n+  size_t result = 0;\n+\n@@ -57,3 +70,4 @@\n-      *cur_word = WordAllDirty;\n-    } else if (value == WordAllDirty) {\n-      \/\/ do nothing.\n+      *cur_word = WordAllFromRemset;\n+      result += sizeof(size_t);\n+    } else if ((value & WordAlreadyScanned) == 0) {\n+      \/\/ Do nothing if there is no \"Clean\" card in it.\n@@ -61,9 +75,4 @@\n-      \/\/ There is a mix of cards in there. Tread slowly.\n-      CardValue* cur = (CardValue*)cur_word;\n-      for (size_t i = 0; i < sizeof(size_t); i++) {\n-        CardValue value = *cur;\n-        if (value == clean_card_val()) {\n-          *cur = dirty_card_val();\n-        }\n-        cur++;\n-      }\n+      \/\/ There is a mix of cards in there. Tread \"slowly\".\n+      size_t clean_card_mask = (value & WordAlreadyScanned) * 0xff; \/\/ All \"Clean\" cards have 0xff, all other places 0x00 now.\n+      result += population_count(clean_card_mask) \/ BitsPerByte;\n+      *cur_word = blend(value, WordAllFromRemset, clean_card_mask);\n@@ -73,0 +82,1 @@\n+  return result;\n@@ -75,1 +85,2 @@\n-inline void G1CardTable::change_dirty_cards_to(CardValue* start_card, CardValue* end_card, CardValue which) {\n+inline size_t G1CardTable::change_dirty_cards_to(CardValue* start_card, CardValue* end_card, CardValue which) {\n+  size_t result = 0;\n@@ -78,1 +89,1 @@\n-    assert(value == dirty_card_val(),\n+    assert((value & g1_card_already_scanned) == 0,\n@@ -80,0 +91,3 @@\n+    if (value == g1_dirty_card) {\n+      result++;\n+    }\n@@ -82,0 +96,1 @@\n+  return result;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTable.inline.hpp","additions":34,"deletions":19,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/g1\/g1CardTableClaimTable.inline.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.inline.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/checkedCast.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+G1CardTableClaimTable::G1CardTableClaimTable(uint chunks_per_region) :\n+  _max_reserved_regions(0),\n+  _card_claims(nullptr),\n+  _cards_per_chunk(checked_cast<uint>(G1HeapRegion::CardsPerRegion \/ chunks_per_region))\n+{\n+  guarantee(chunks_per_region > 0, \"%u chunks per region\", chunks_per_region);\n+}\n+\n+G1CardTableClaimTable::~G1CardTableClaimTable() {\n+  FREE_C_HEAP_ARRAY(uint, _card_claims);\n+}\n+\n+void G1CardTableClaimTable::initialize(uint max_reserved_regions) {\n+  assert(_card_claims == nullptr, \"Must not be initialized twice\");\n+  _card_claims = NEW_C_HEAP_ARRAY(uint, max_reserved_regions, mtGC);\n+  _max_reserved_regions = max_reserved_regions;\n+  reset_all_claims_to_unclaimed();\n+}\n+\n+void G1CardTableClaimTable::reset_all_claims_to_unclaimed() {\n+  for (size_t i = 0; i < _max_reserved_regions; i++) {\n+    _card_claims[i] = 0;\n+  }\n+}\n+\n+void G1CardTableClaimTable::reset_all_claims_to_claimed() {\n+  for (size_t i = 0; i < _max_reserved_regions; i++) {\n+    _card_claims[i] = (uint)G1HeapRegion::CardsPerRegion;\n+  }\n+}\n+\n+void G1CardTableClaimTable::heap_region_iterate_from_worker_offset(G1HeapRegionClosure* cl, uint worker_id, uint max_workers) {\n+  \/\/ Every worker will actually look at all regions, skipping over regions that\n+  \/\/ are completed.\n+  const size_t n_regions = _max_reserved_regions;\n+  const uint start_index = (uint)(worker_id * n_regions \/ max_workers);\n+\n+  for (uint count = 0; count < n_regions; count++) {\n+    const uint index = (start_index + count) % n_regions;\n+    assert(index < n_regions, \"sanity\");\n+    \/\/ Skip over fully processed regions\n+    if (!has_unclaimed_cards(index)) {\n+      continue;\n+    }\n+    G1HeapRegion* r = G1CollectedHeap::heap()->region_at(index);\n+    bool res = cl->do_heap_region(r);\n+    if (res) {\n+      return;\n+    }\n+  }\n+}\n+\n+G1CardTableChunkClaimer::G1CardTableChunkClaimer(G1CardTableClaimTable* scan_state, uint region_idx) :\n+  _claim_values(scan_state),\n+  _region_idx(region_idx),\n+  _cur_claim(0) {\n+  guarantee(size() <= G1HeapRegion::CardsPerRegion, \"Should not claim more space than possible.\");\n+}\n+\n+G1ChunkScanner::G1ChunkScanner(CardValue* const start_card, CardValue* const end_card) :\n+  _start_card(start_card),\n+  _end_card(end_card) {\n+    assert(is_word_aligned(start_card), \"precondition\");\n+    assert(is_word_aligned(end_card), \"precondition\");\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTableClaimTable.cpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,137 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1CARDTABLECLAIMTABLE_HPP\n+#define SHARE_GC_G1_G1CARDTABLECLAIMTABLE_HPP\n+\n+#include \"gc\/g1\/g1CardTable.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class G1HeapRegionClosure;\n+\n+\/\/ Helper class representing claim values for the cards in the card table corresponding\n+\/\/ to a region.\n+\/\/ I.e. for every region this class stores an atomic counter that represents the\n+\/\/ number of cards from 0 to the number of cards per region already claimed for\n+\/\/ this region.\n+\/\/ If the claimed value is >= the number of cards of a region, the region can be\n+\/\/ considered fully claimed.\n+\/\/\n+\/\/ Claiming works on full region (all cards in region) or a range of contiguous cards\n+\/\/ (chunk). Chunk size is given at construction time.\n+class G1CardTableClaimTable : public CHeapObj<mtGC> {\n+  uint _max_reserved_regions;\n+\n+  \/\/ Card table iteration claim values for every heap region, from 0 (completely unclaimed)\n+  \/\/ to (>=) G1HeapRegion::CardsPerRegion (completely claimed).\n+  uint volatile* _card_claims;\n+\n+  uint _cards_per_chunk;           \/\/ For conversion between card index and chunk index.\n+\n+  \/\/ Claim increment number of cards, returning the previous claim value.\n+  inline uint claim_cards(uint region, uint increment);\n+\n+public:\n+  G1CardTableClaimTable(uint chunks_per_region);\n+  ~G1CardTableClaimTable();\n+\n+  \/\/ Allocates the data structure and initializes the claims to unclaimed.\n+  void initialize(uint max_reserved_regions);\n+\n+  void reset_all_claims_to_unclaimed();\n+  void reset_all_claims_to_claimed();\n+\n+  inline bool has_unclaimed_cards(uint region);\n+  inline void reset_to_unclaimed(uint region);\n+\n+  \/\/ Claims all cards in that region, returning the previous claim value.\n+  inline uint claim_all_cards(uint region);\n+\n+  \/\/ Claim a single chunk in that region, returning the previous claim value.\n+  inline uint claim_chunk(uint region);\n+  inline uint cards_per_chunk() const;\n+\n+  size_t max_reserved_regions() { return _max_reserved_regions; }\n+\n+  void heap_region_iterate_from_worker_offset(G1HeapRegionClosure* cl, uint worker_id, uint max_workers);\n+};\n+\n+\/\/ Helper class to claim dirty chunks within the card table for a given region.\n+class G1CardTableChunkClaimer {\n+  G1CardTableClaimTable* _claim_values;\n+\n+  uint _region_idx;\n+  uint _cur_claim;\n+\n+public:\n+  G1CardTableChunkClaimer(G1CardTableClaimTable* claim_table, uint region_idx);\n+\n+  inline bool has_next();\n+\n+  inline uint value() const;\n+  inline uint size() const;\n+};\n+\n+\/\/ Helper class to locate consecutive dirty cards inside a range of cards.\n+class G1ChunkScanner {\n+  using Word = size_t;\n+  using CardValue = G1CardTable::CardValue;\n+\n+  CardValue* const _start_card;\n+  CardValue* const _end_card;\n+\n+  static const size_t ExpandedToScanMask = G1CardTable::WordAlreadyScanned;\n+  static const size_t ToScanMask = G1CardTable::g1_card_already_scanned;\n+\n+  inline bool is_card_dirty(const CardValue* const card) const;\n+\n+  inline bool is_word_aligned(const void* const addr) const;\n+\n+  inline CardValue* find_first_dirty_card(CardValue* i_card) const;\n+  inline CardValue* find_first_non_dirty_card(CardValue* i_card) const;\n+\n+public:\n+  G1ChunkScanner(CardValue* const start_card, CardValue* const end_card);\n+\n+  template<typename Func>\n+  void on_dirty_cards(Func&& f) {\n+    for (CardValue* cur_card = _start_card; cur_card < _end_card; \/* empty *\/) {\n+      CardValue* dirty_l = find_first_dirty_card(cur_card);\n+      CardValue* dirty_r = find_first_non_dirty_card(dirty_l);\n+\n+      assert(dirty_l <= dirty_r, \"inv\");\n+\n+      if (dirty_l == dirty_r) {\n+        assert(dirty_r == _end_card, \"finished the entire chunk\");\n+        return;\n+      }\n+\n+      f(dirty_l, dirty_r);\n+\n+      cur_card = dirty_r + 1;\n+    }\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_G1_G1CARDTABLECLAIMTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTableClaimTable.hpp","additions":137,"deletions":0,"binary":false,"changes":137,"status":"added"},{"patch":"@@ -0,0 +1,128 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1CARDTABLECLAIMTABLE_INLINE_HPP\n+#define SHARE_GC_G1_G1CARDTABLECLAIMTABLE_INLINE_HPP\n+\n+#include \"gc\/g1\/g1CardTableClaimTable.hpp\"\n+\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+bool G1CardTableClaimTable::has_unclaimed_cards(uint region) {\n+  assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n+  return Atomic::load(&_card_claims[region]) < G1HeapRegion::CardsPerRegion;\n+}\n+\n+void G1CardTableClaimTable::reset_to_unclaimed(uint region) {\n+  assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n+  Atomic::store(&_card_claims[region], 0u);\n+}\n+\n+uint G1CardTableClaimTable::claim_cards(uint region, uint increment) {\n+  assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n+  return Atomic::fetch_then_add(&_card_claims[region], increment, memory_order_relaxed);\n+}\n+\n+uint G1CardTableClaimTable::claim_chunk(uint region) {\n+  assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n+  return Atomic::fetch_then_add(&_card_claims[region], cards_per_chunk(), memory_order_relaxed);\n+}\n+\n+uint G1CardTableClaimTable::claim_all_cards(uint region) {\n+  return claim_cards(region, (uint)G1HeapRegion::CardsPerRegion);\n+}\n+\n+uint G1CardTableClaimTable::cards_per_chunk() const { return _cards_per_chunk; }\n+\n+bool G1CardTableChunkClaimer::has_next() {\n+  _cur_claim = _claim_values->claim_chunk(_region_idx);\n+  return (_cur_claim < G1HeapRegion::CardsPerRegion);\n+}\n+\n+uint G1CardTableChunkClaimer::value() const { return _cur_claim; }\n+uint G1CardTableChunkClaimer::size() const { return _claim_values->cards_per_chunk(); }\n+\n+bool G1ChunkScanner::is_card_dirty(const CardValue* const card) const {\n+  return (*card & ToScanMask) == 0;\n+}\n+\n+bool G1ChunkScanner::is_word_aligned(const void* const addr) const {\n+  return ((uintptr_t)addr) % sizeof(Word) == 0;\n+}\n+\n+G1CardTable::CardValue* G1ChunkScanner::find_first_dirty_card(CardValue* i_card) const {\n+  while (!is_word_aligned(i_card)) {\n+    if (is_card_dirty(i_card)) {\n+      return i_card;\n+    }\n+    i_card++;\n+  }\n+\n+  for (\/* empty *\/; i_card < _end_card; i_card += sizeof(Word)) {\n+    Word word_value = *reinterpret_cast<Word*>(i_card);\n+    bool has_dirty_cards_in_word = (~word_value & ExpandedToScanMask) != 0;\n+\n+    if (has_dirty_cards_in_word) {\n+      for (uint i = 0; i < sizeof(Word); ++i) {\n+        if (is_card_dirty(i_card)) {\n+          return i_card;\n+        }\n+        i_card++;\n+      }\n+      assert(false, \"should have early-returned\");\n+    }\n+  }\n+\n+  return _end_card;\n+}\n+\n+G1CardTable::CardValue* G1ChunkScanner::find_first_non_dirty_card(CardValue* i_card) const {\n+  while (!is_word_aligned(i_card)) {\n+    if (!is_card_dirty(i_card)) {\n+      return i_card;\n+    }\n+    i_card++;\n+  }\n+\n+  for (\/* empty *\/; i_card < _end_card; i_card += sizeof(Word)) {\n+    Word word_value = *reinterpret_cast<Word*>(i_card);\n+    bool all_cards_dirty = (word_value & ExpandedToScanMask) == 0;\n+\n+    if (!all_cards_dirty) {\n+      for (uint i = 0; i < sizeof(Word); ++i) {\n+        if (!is_card_dirty(i_card)) {\n+          return i_card;\n+        }\n+        i_card++;\n+      }\n+      assert(false, \"should have early-returned\");\n+    }\n+  }\n+\n+  return _end_card;\n+}\n+\n+#endif \/\/ SHARE_GC_G1_G1CARDTABLECLAIMTABLE_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CardTableClaimTable.inline.hpp","additions":128,"deletions":0,"binary":false,"changes":128,"status":"added"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -63,1 +62,0 @@\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n@@ -148,1 +146,1 @@\n-uint G1CollectedHeap::get_chunks_per_region() {\n+uint G1CollectedHeap::get_chunks_per_region_for_scan() {\n@@ -158,0 +156,12 @@\n+uint G1CollectedHeap::get_chunks_per_region_for_merge() {\n+  uint log_region_size = G1HeapRegion::LogOfHRGrainBytes;\n+  \/\/ Limit the expected input values to current known possible values of the\n+  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible\n+  \/\/ values for region size.\n+  assert(log_region_size >= 20 && log_region_size <= 29,\n+         \"expected value in [20,29], but got %u\", log_region_size);\n+\n+  uint half_log_region_size = (log_region_size + 1) \/ 2;\n+  return 1 << (half_log_region_size - 9);\n+}\n+\n@@ -600,1 +610,0 @@\n-    dirty_young_block(result, *actual_word_size);\n@@ -792,5 +801,7 @@\n-  \/\/ Discard all remembered set updates and reset refinement statistics.\n-  G1BarrierSet::dirty_card_queue_set().abandon_logs_and_stats();\n-  assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,\n-         \"DCQS should be empty\");\n-  concurrent_refine()->get_and_reset_refinement_stats();\n+  G1ConcurrentRefineWorkState& refine_state = concurrent_refine()->refine_state();\n+  if (refine_state.is_in_progress()) {\n+    \/\/ Record any available refinement statistics.\n+    policy()->record_refinement_stats(refine_state.stats());\n+    refine_state.complete(false \/* concurrent *\/, false \/* print_log *\/);\n+  }\n+  refine_state.reset_stats();\n@@ -1131,1 +1142,6 @@\n-  _card_table(nullptr),\n+  _refinement_epoch(0),\n+  _last_synchronized_start(0),\n+  _safepoint_duration(0),\n+  _last_refinement_epoch_start(0),\n+  _yield_duration_in_refinement_epoch(0),\n+  _last_safepoint_refinement_epoch(0),\n@@ -1250,1 +1266,1 @@\n-  _cr = G1ConcurrentRefine::create(policy(), &ecode);\n+  _cr = G1ConcurrentRefine::create(this, &ecode);\n@@ -1303,2 +1319,4 @@\n-  G1CardTable* ct = new G1CardTable(_reserved);\n-  G1BarrierSet* bs = new G1BarrierSet(ct);\n+  G1CardTable* card_table = new G1CardTable(_reserved);\n+  G1CardTable* refinement_table = new G1CardTable(_reserved);\n+\n+  G1BarrierSet* bs = new G1BarrierSet(card_table, refinement_table);\n@@ -1307,8 +1325,0 @@\n-  BarrierSet::set_barrier_set(bs);\n-  _card_table = ct;\n-\n-  {\n-    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();\n-    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);\n-    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);\n-  }\n@@ -1349,0 +1359,5 @@\n+  G1RegionToSpaceMapper* refinement_cards_storage =\n+    create_aux_memory_mapper(\"Refinement Card Table\",\n+                             G1CardTable::compute_size(heap_rs.size() \/ HeapWordSize),\n+                             G1CardTable::heap_map_factor());\n+\n@@ -1353,2 +1368,11 @@\n-  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage);\n-  _card_table->initialize(cardtable_storage);\n+  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage, refinement_cards_storage);\n+  card_table->initialize(cardtable_storage);\n+  refinement_table->initialize(refinement_cards_storage);\n+\n+  BarrierSet::set_barrier_set(bs);\n+\n+  {\n+    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();\n+    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);\n+    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);\n+  }\n@@ -1366,1 +1390,1 @@\n-  _rem_set = new G1RemSet(this, _card_table);\n+  _rem_set = new G1RemSet(this);\n@@ -1451,0 +1475,1 @@\n+  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_refine_control);\n@@ -1475,0 +1500,2 @@\n+\n+  _last_synchronized_start = os::elapsed_counter();\n@@ -1478,0 +1505,13 @@\n+  jlong now = os::elapsed_counter();\n+  jlong synchronize_duration = now - _last_synchronized_start;\n+\n+  _safepoint_duration += synchronize_duration;\n+\n+  if (_last_safepoint_refinement_epoch == refinement_epoch()) {\n+    _yield_duration_in_refinement_epoch += synchronize_duration;\n+  } else {\n+    _last_refinement_epoch_start = now;\n+    _last_safepoint_refinement_epoch = refinement_epoch();\n+    _yield_duration_in_refinement_epoch = 0;\n+  }\n+\n@@ -1481,0 +1521,10 @@\n+void G1CollectedHeap::set_last_refinement_epoch_start(jlong epoch_start, jlong last_yield_duration) {\n+  _last_refinement_epoch_start = epoch_start;\n+  guarantee(_yield_duration_in_refinement_epoch >= last_yield_duration, \"should be\");\n+  _yield_duration_in_refinement_epoch -= last_yield_duration;\n+}\n+\n+jlong G1CollectedHeap::yield_duration_in_refinement_epoch() {\n+  return _yield_duration_in_refinement_epoch;\n+}\n+\n@@ -2243,0 +2293,2 @@\n+\n+  _refinement_epoch++;\n@@ -2370,1 +2422,0 @@\n-  _verifier->verify_dirty_young_regions();\n@@ -2643,0 +2694,5 @@\n+  if (VerifyDuringGC) {\n+    \/\/ Card and refinement table must be clear for freed regions.\n+    card_table()->verify_region(MemRegion(hr->bottom(), hr->end()), G1CardTable::clean_card_val(), true);\n+    refinement_table()->verify_region(MemRegion(hr->bottom(), hr->end()), G1CardTable::clean_card_val(), true);\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":81,"deletions":25,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -177,1 +177,15 @@\n-  G1CardTable* _card_table;\n+\n+  \/\/ The current epoch for refinement, i.e. the number of times the card tables\n+  \/\/ have been swapped by a garbage collection.\n+  \/\/ Used for detecting whether concurrent refinement has been interrupted by a\n+  \/\/ garbage collection.\n+  size_t _refinement_epoch;\n+\n+  \/\/ The following members are for tracking safepoint durations between garbage\n+  \/\/ collections.\n+  jlong _last_synchronized_start;\n+  jlong _safepoint_duration;                       \/\/ Time spent in safepoints since start of VM.\n+\n+  jlong _last_refinement_epoch_start;\n+  jlong _yield_duration_in_refinement_epoch;       \/\/ Time spent in safepoints since beginning of last refinement epoch.\n+  size_t _last_safepoint_refinement_epoch;         \/\/ Refinement epoch before last safepoint.\n@@ -538,1 +552,1 @@\n-  \/\/ within a region to claim.\n+  \/\/ within a region to claim during card table scanning.\n@@ -543,1 +557,6 @@\n-  static uint get_chunks_per_region();\n+  static uint get_chunks_per_region_for_scan();\n+  \/\/ Return \"optimal\" number of chunks per region we want to use for claiming areas\n+  \/\/ within a region to claim during card table merging.\n+  \/\/ This is much smaller than for scanning as the merge work is much smaller.\n+  \/\/ Currently 1 for 1M regions, 2 for 2\/4M regions, 4 for 8\/16M regions and so on.\n+  static uint get_chunks_per_region_for_merge();\n@@ -683,5 +702,0 @@\n-  \/\/ It dirties the cards that cover the block so that the post\n-  \/\/ write barrier never queues anything when updating objects on this\n-  \/\/ block. It is assumed (and in fact we assert) that the block\n-  \/\/ belongs to a young region.\n-  inline void dirty_young_block(HeapWord* start, size_t word_size);\n@@ -902,0 +916,5 @@\n+  jlong synchronized_duration() const { return _safepoint_duration; }\n+  jlong last_refinement_epoch_start() const { return _last_refinement_epoch_start; }\n+  void set_last_refinement_epoch_start(jlong epoch_start, jlong last_yield_duration);\n+  jlong yield_duration_in_refinement_epoch();\n+\n@@ -1069,1 +1088,12 @@\n-    return _card_table;\n+    return static_cast<G1CardTable*>(G1BarrierSet::g1_barrier_set()->card_table());\n+  }\n+\n+  G1CardTable* refinement_table() const {\n+    return G1BarrierSet::g1_barrier_set()->refinement_table();\n+  }\n+\n+  size_t refinement_epoch() const { return _refinement_epoch; }\n+\n+  G1CardTable::CardValue* card_table_base() const {\n+    assert(card_table() != nullptr, \"must be\");\n+    return card_table()->byte_map_base();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":39,"deletions":9,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,24 +152,0 @@\n-\/\/ It dirties the cards that cover the block so that the post\n-\/\/ write barrier never queues anything when updating objects on this\n-\/\/ block. It is assumed (and in fact we assert) that the block\n-\/\/ belongs to a young region.\n-inline void\n-G1CollectedHeap::dirty_young_block(HeapWord* start, size_t word_size) {\n-  assert_heap_not_locked();\n-\n-  \/\/ Assign the containing region to containing_hr so that we don't\n-  \/\/ have to keep calling heap_region_containing() in the\n-  \/\/ asserts below.\n-  DEBUG_ONLY(G1HeapRegion* containing_hr = heap_region_containing(start);)\n-  assert(word_size > 0, \"pre-condition\");\n-  assert(containing_hr->is_in(start), \"it should contain start\");\n-  assert(containing_hr->is_young(), \"it should be young\");\n-  assert(!containing_hr->is_humongous(), \"it should not be humongous\");\n-\n-  HeapWord* end = start + word_size;\n-  assert(containing_hr->is_in(end - 1), \"it should also contain end - 1\");\n-\n-  MemRegion mr(start, end);\n-  card_table()->g1_mark_as_young(mr);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.inline.hpp","additions":1,"deletions":25,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -294,1 +294,2 @@\n-  size_t pending_cards = _policy->pending_cards_at_gc_start();\n+  bool in_young_only_phase = _policy->collector_state()->in_young_only_phase();\n+  size_t pending_cards = _policy->analytics()->predict_pending_cards(in_young_only_phase);\n@@ -309,1 +310,2 @@\n-  double predicted_base_time_ms = _policy->predict_base_time_ms(pending_cards, _g1h->young_regions_cardset()->occupied());\n+  size_t card_rs_length = _policy->analytics()->predict_card_rs_length(in_young_only_phase);\n+  double predicted_base_time_ms = _policy->predict_base_time_ms(pending_cards, card_rs_length);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectionSet.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/g1\/g1CardTableClaimTable.inline.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"gc\/g1\/g1ConcurrentRefine.hpp\"\n@@ -36,1 +38,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -484,1 +485,1 @@\n-  _worker_id_offset(G1DirtyCardQueueSet::num_par_ids() + G1ConcRefinementThreads),\n+  _worker_id_offset(G1ConcRefinementThreads), \/\/ The refinement control thread does not refine cards, so it's just the worker threads.\n@@ -1235,0 +1236,10 @@\n+    void reclaim_empty_region_common(G1HeapRegion* hr) {\n+      _freed_bytes += hr->used();\n+      hr->set_containing_set(nullptr);\n+      hr->clear_cardtable();\n+      hr->clear_refinement_table();\n+      _g1h->concurrent_mark()->clear_statistics(hr);\n+      G1HeapRegionPrinter::mark_reclaim(hr);\n+      _g1h->concurrent_refine()->notify_region_reclaimed(hr);\n+    }\n+\n@@ -1245,5 +1256,1 @@\n-        _freed_bytes += hr->used();\n-        hr->set_containing_set(nullptr);\n-        hr->clear_cardtable();\n-        _g1h->concurrent_mark()->clear_statistics(hr);\n-        G1HeapRegionPrinter::mark_reclaim(hr);\n+        reclaim_empty_region_common(hr);\n@@ -1262,5 +1269,1 @@\n-      _freed_bytes += hr->used();\n-      hr->set_containing_set(nullptr);\n-      hr->clear_cardtable();\n-      _g1h->concurrent_mark()->clear_statistics(hr);\n-      G1HeapRegionPrinter::mark_reclaim(hr);\n+      reclaim_empty_region_common(hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.cpp","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -581,0 +581,2 @@\n+  uint worker_id_offset() const { return _worker_id_offset; }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -249,1 +249,1 @@\n-      _rebuild_closure(G1CollectedHeap::heap(), worker_id),\n+      _rebuild_closure(G1CollectedHeap::heap(), worker_id + cm->worker_id_offset()),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRebuildAndScrub.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"gc\/g1\/g1Analytics.hpp\"\n@@ -26,0 +27,2 @@\n+#include \"gc\/g1\/g1CardTableClaimTable.inline.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n@@ -29,1 +32,1 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n+#include \"gc\/g1\/g1ConcurrentRefineWorkTask.hpp\"\n@@ -34,0 +37,1 @@\n+#include \"gc\/shared\/workerThread.hpp\"\n@@ -41,0 +45,1 @@\n+#include \"utilities\/ticks.hpp\"\n@@ -43,1 +48,1 @@\n-G1ConcurrentRefineThread* G1ConcurrentRefineThreadControl::create_refinement_thread(uint worker_id, bool initializing) {\n+G1ConcurrentRefineThread* G1ConcurrentRefineThreadControl::create_refinement_thread() {\n@@ -45,3 +50,1 @@\n-  if (initializing || !InjectGCWorkerCreationFailure) {\n-    result = G1ConcurrentRefineThread::create(_cr, worker_id);\n-  }\n+  result = G1ConcurrentRefineThread::create(_cr);\n@@ -49,2 +52,1 @@\n-    log_warning(gc)(\"Failed to create refinement thread %u, no more %s\",\n-                    worker_id,\n+    log_warning(gc)(\"Failed to create refinement control thread, no more %s\",\n@@ -62,1 +64,3 @@\n-  _threads(max_num_threads)\n+  _control_thread(nullptr),\n+  _workers(nullptr),\n+  _max_num_threads(max_num_threads)\n@@ -66,17 +70,2 @@\n-  while (_threads.is_nonempty()) {\n-    delete _threads.pop();\n-  }\n-}\n-\n-bool G1ConcurrentRefineThreadControl::ensure_threads_created(uint worker_id, bool initializing) {\n-  assert(worker_id < max_num_threads(), \"precondition\");\n-\n-  while ((uint)_threads.length() <= worker_id) {\n-    G1ConcurrentRefineThread* rt = create_refinement_thread(_threads.length(), initializing);\n-    if (rt == nullptr) {\n-      return false;\n-    }\n-    _threads.push(rt);\n-  }\n-\n-  return true;\n+  delete _control_thread;\n+  delete _workers;\n@@ -90,3 +79,3 @@\n-    _threads.push(create_refinement_thread(0, true));\n-    if (_threads.at(0) == nullptr) {\n-      vm_shutdown_during_initialization(\"Could not allocate primary refinement thread\");\n+    _control_thread = create_refinement_thread();\n+    if (_control_thread == nullptr) {\n+      vm_shutdown_during_initialization(\"Could not allocate refinement control thread\");\n@@ -95,7 +84,2 @@\n-\n-    if (!UseDynamicNumberOfGCThreads) {\n-      if (!ensure_threads_created(max_num_threads() - 1, true)) {\n-        vm_shutdown_during_initialization(\"Could not allocate refinement threads\");\n-        return JNI_ENOMEM;\n-      }\n-    }\n+    _workers = new WorkerThreads(\"G1 Refinement Workers\", max_num_threads());\n+    _workers->initialize_workers();\n@@ -103,1 +87,0 @@\n-\n@@ -108,2 +91,2 @@\n-void G1ConcurrentRefineThreadControl::assert_current_thread_is_primary_refinement_thread() const {\n-  assert(Thread::current() == _threads.at(0), \"Not primary thread\");\n+void G1ConcurrentRefineThreadControl::assert_current_thread_is_control_refinement_thread() const {\n+  assert(Thread::current() == _control_thread, \"Not refinement control thread\");\n@@ -113,5 +96,6 @@\n-bool G1ConcurrentRefineThreadControl::activate(uint worker_id) {\n-  if (ensure_threads_created(worker_id, false)) {\n-    _threads.at(worker_id)->activate();\n-    return true;\n-  }\n+void G1ConcurrentRefineThreadControl::activate() {\n+  _control_thread->activate();\n+}\n+\n+void G1ConcurrentRefineThreadControl::run_task(WorkerTask* task, uint num_workers) {\n+  assert(num_workers >= 1, \"must be\");\n@@ -119,1 +103,8 @@\n-  return false;\n+  WithActiveWorkers w(_workers, num_workers);\n+  _workers->run_task(task);\n+}\n+\n+void G1ConcurrentRefineThreadControl::control_thread_do(ThreadClosure* tc) {\n+  if (_control_thread != nullptr) {\n+    tc->do_thread(_control_thread);\n+  }\n@@ -123,2 +114,2 @@\n-  for (G1ConcurrentRefineThread* t : _threads) {\n-    tc->do_thread(t);\n+  if (_control_thread != nullptr) {\n+    _workers->threads_do(tc);\n@@ -129,2 +120,182 @@\n-  for (G1ConcurrentRefineThread* t : _threads) {\n-    t->stop();\n+  if (_control_thread != nullptr) {\n+    _control_thread->stop();\n+  }\n+}\n+\n+G1ConcurrentRefineWorkState::G1ConcurrentRefineWorkState(uint max_reserved_regions) :\n+  _state(State::Idle),\n+  _refine_work_epoch(0),\n+  _sweep_table(new G1CardTableClaimTable(G1CollectedHeap::get_chunks_per_region_for_merge())),\n+  _stats()\n+{\n+  _sweep_table->initialize(max_reserved_regions);\n+}\n+\n+G1ConcurrentRefineWorkState::~G1ConcurrentRefineWorkState() {\n+  delete _sweep_table;\n+}\n+\n+void G1ConcurrentRefineWorkState::set_state_start_time() {\n+  _state_start[static_cast<uint>(_state)] = Ticks::now();\n+}\n+\n+Tickspan G1ConcurrentRefineWorkState::get_duration(State start, State end) {\n+  return _state_start[static_cast<uint>(end)] - _state_start[static_cast<uint>(start)];\n+}\n+\n+void G1ConcurrentRefineWorkState::reset_stats() {\n+  stats()->reset();\n+}\n+\n+void G1ConcurrentRefineWorkState::add_yield_duration(jlong duration) {\n+  stats()->inc_yield_duration(duration);\n+}\n+\n+size_t G1ConcurrentRefineWorkState::refinement_epoch() {\n+  return G1CollectedHeap::heap()->refinement_epoch();\n+}\n+\n+bool G1ConcurrentRefineWorkState::advance_state(State next_state) {\n+  bool result = _refine_work_epoch == refinement_epoch();\n+  if (result) {\n+    _state = next_state;\n+  } else {\n+    _state = State::Idle;\n+  }\n+  return result;\n+}\n+\n+void G1ConcurrentRefineWorkState::assert_state(State expected) {\n+  assert(_state == expected, \"must be %s but is %s\", state_name(expected), state_name(_state));\n+}\n+\n+void G1ConcurrentRefineWorkState::start_refine_work() {\n+  assert_state(State::Idle);\n+\n+  set_state_start_time();\n+\n+  _refine_work_epoch = refinement_epoch();\n+\n+  _stats.reset();\n+\n+  advance_state(State::SwapGlobalCT);\n+}\n+\n+bool G1ConcurrentRefineWorkState::swap_global_card_table() {\n+  assert_state(State::SwapGlobalCT);\n+\n+  set_state_start_time();\n+\n+  {\n+    \/\/ We can't have any new threads being in the process of created while we\n+    \/\/ swap the card table because we read the current card table state during\n+    \/\/ initialization.\n+    \/\/ A safepoint may occur during that time, so leave the STS temporarily.\n+    SuspendibleThreadSetLeaver sts_leave;\n+\n+    MutexLocker mu(Threads_lock);\n+    G1BarrierSet::g1_barrier_set()->swap_global_card_table();\n+  }\n+\n+  return advance_state(State::SwapJavaThreadsCT);\n+}\n+\n+class G1SwapThreadCardTableClosure : public HandshakeClosure {\n+public:\n+  G1SwapThreadCardTableClosure() : HandshakeClosure(\"G1 Swap JT card table\") { }\n+\n+  virtual void do_thread(Thread* t) {\n+    G1BarrierSet* bs = G1BarrierSet::g1_barrier_set();\n+    G1ThreadLocalData::set_byte_map_base(t, bs->card_table()->byte_map_base());\n+  }\n+};\n+\n+bool G1ConcurrentRefineWorkState::swap_java_threads_ct() {\n+  assert_state(State::SwapJavaThreadsCT);\n+\n+  set_state_start_time();\n+\n+  {\n+    SuspendibleThreadSetLeaver sts_leave;\n+\n+    G1SwapThreadCardTableClosure cl;\n+    Handshake::execute(&cl);\n+  }\n+\n+  return advance_state(State::SwapGCThreadsCT);\n+}\n+\n+bool G1ConcurrentRefineWorkState::swap_gc_threads_ct() {\n+  assert_state(State::SwapGCThreadsCT);\n+\n+  set_state_start_time();\n+\n+  {\n+    class RendezvousGCThreads: public VM_Operation {\n+    public:\n+      VMOp_Type type() const { return VMOp_G1RendezvousGCThreads; }\n+\n+      virtual bool evaluate_at_safepoint() const {\n+        \/\/ We only care about synchronizing the GC threads.\n+        \/\/ Leave the Java threads running.\n+        return false;\n+      }\n+\n+      virtual bool skip_thread_oop_barriers() const {\n+        fatal(\"Concurrent VMOps should not call this\");\n+        return true;\n+      }\n+\n+      void doit() {\n+        \/\/ Light weight \"handshake\" of the GC threads\n+        SuspendibleThreadSet::synchronize();\n+\n+        G1SwapThreadCardTableClosure cl;\n+        G1CollectedHeap::heap()->gc_threads_do(&cl);\n+\n+        SuspendibleThreadSet::desynchronize();\n+      };\n+    } op;\n+\n+    SuspendibleThreadSetLeaver sts_leave;\n+    VMThread::execute(&op);\n+  }\n+\n+  return advance_state(State::SnapshotHeap);\n+}\n+\n+void G1ConcurrentRefineWorkState::snapshot_heap(bool concurrent) {\n+  if (concurrent) {\n+    assert_state(State::SnapshotHeap);\n+\n+    set_state_start_time();\n+  } else {\n+    assert_state(State::Idle);\n+  }\n+\n+  snapshot_heap_into(_sweep_table);\n+\n+  if (concurrent) {\n+    advance_state(State::SweepRT);\n+  }\n+}\n+\n+void G1ConcurrentRefineWorkState::sweep_rt_start() {\n+  assert_state(State::SweepRT);\n+\n+  set_state_start_time();\n+}\n+\n+bool G1ConcurrentRefineWorkState::sweep_rt_step() {\n+  assert_state(State::SweepRT);\n+\n+  G1ConcurrentRefine* cr = G1CollectedHeap::heap()->concurrent_refine();\n+\n+  G1ConcurrentRefineWorkTask task(_sweep_table, &_stats, cr->num_threads_wanted());\n+  cr->run_with_refinement_workers(&task);\n+\n+  if (task.sweep_completed()) {\n+    _state = State::CompleteRefineWork;\n+    return true;\n+  } else {\n+    return false;\n@@ -134,0 +305,70 @@\n+bool G1ConcurrentRefineWorkState::complete(bool concurrent, bool print_log) {\n+  if (concurrent) {\n+    assert_state(State::CompleteRefineWork);\n+  } else {\n+    \/\/ May have been forced to complete at any other time.\n+    assert(is_in_progress() && _state != State::CompleteRefineWork, \"must be but is %s\", state_name(_state));\n+  }\n+\n+  set_state_start_time();\n+\n+  if (print_log) {\n+    G1ConcurrentRefineStats* s = &_stats;\n+\n+    log_debug(gc, refine)(\"Refinement took %.2fms (pre-sweep %.2fms card refine %.2f) \"\n+                          \"(scanned %zu clean %zu (%.2f%%) not_clean %zu (%.2f%%) not_parsable %zu \"\n+                          \"refers_to_cset %zu (%.2f%%) still_refers_to_cset %zu (%.2f%%) clean_again %zu pending %zu)\",\n+                          get_duration(State::Idle, _state).seconds() * 1000.0,\n+                          get_duration(State::Idle, State::SweepRT).seconds() * 1000.0,\n+                          TimeHelper::counter_to_millis(s->refine_duration()),\n+                          s->cards_scanned(),\n+                          s->cards_clean(),\n+                          percent_of(s->cards_clean(), s->cards_scanned()),\n+                          s->cards_not_clean(),\n+                          percent_of(s->cards_not_clean(), s->cards_scanned()),\n+                          s->cards_not_parsable(),\n+                          s->cards_refer_to_cset(),\n+                          percent_of(s->cards_refer_to_cset(), s->cards_not_clean()),\n+                          s->cards_still_refer_to_cset(),\n+                          percent_of(s->cards_still_refer_to_cset(), s->cards_not_clean()),\n+                          s->cards_clean_again(),\n+                          s->cards_pending()\n+                         );\n+  }\n+\n+  bool has_sweep_rt_work = is_in_progress() && _state == State::SweepRT;\n+\n+  _state = State::Idle;\n+  return has_sweep_rt_work;\n+}\n+\n+void G1ConcurrentRefineWorkState::snapshot_heap_into(G1CardTableClaimTable* sweep_table) {\n+  \/\/ G1CollectedHeap::heap_region_iterate() will only visit committed regions. In the\n+  \/\/ state table \n+  sweep_table->reset_all_claims_to_claimed();\n+\n+  class SnapshotRegionsClosure : public G1HeapRegionClosure {\n+    G1CardTableClaimTable* _sweep_table;\n+\n+  public:\n+    SnapshotRegionsClosure(G1CardTableClaimTable* sweep_table) : G1HeapRegionClosure(), _sweep_table(sweep_table) { }\n+\n+    bool do_heap_region(G1HeapRegion* r) override {\n+      if (!r->is_free()) {\n+        \/\/ Need to scan all parts of non-free regions, so reset the claim.\n+        \/\/ No need for synchronization: we are only interested about regions\n+        \/\/ that were allocated before the handshake; the handshake makes such\n+        \/\/ regions' metadata visible to all threads, and we do not care about\n+        \/\/ humongous regions that were allocated afterwards.\n+        _sweep_table->reset_to_unclaimed(r->hrm_index());\n+      }\n+      return false;\n+    }\n+  } cl(sweep_table);\n+  G1CollectedHeap::heap()->heap_region_iterate(&cl);\n+}\n+\n+bool G1ConcurrentRefineWorkState::is_in_progress() const {\n+  return _state != State::Idle;\n+}\n+\n@@ -145,3 +386,3 @@\n-G1ConcurrentRefine::G1ConcurrentRefine(G1Policy* policy) :\n-  _policy(policy),\n-  _threads_wanted(0),\n+G1ConcurrentRefine::G1ConcurrentRefine(G1CollectedHeap* g1h) :\n+  _policy(g1h->policy()),\n+  _num_threads_wanted(0),\n@@ -151,1 +392,2 @@\n-  _threads_needed(policy, adjust_threads_period_ms()),\n+  _heap_was_locked(false),\n+  _threads_needed(g1h->policy(), adjust_threads_period_ms()),\n@@ -153,2 +395,2 @@\n-  _dcqs(G1BarrierSet::dirty_card_queue_set())\n-{}\n+  _refine_state(g1h->max_reserved_regions())\n+{ }\n@@ -160,2 +402,30 @@\n-G1ConcurrentRefine* G1ConcurrentRefine::create(G1Policy* policy, jint* ecode) {\n-  G1ConcurrentRefine* cr = new G1ConcurrentRefine(policy);\n+G1ConcurrentRefineWorkState& G1ConcurrentRefine::refine_state_for_merge() {\n+  bool has_sweep_claims = refine_state().complete(false);\n+  if (has_sweep_claims) {\n+    log_debug(gc, refine)(\"Continue existing work\");\n+  } else {\n+    \/\/ Refinement has been interrupted without having a snapshot. There may\n+    \/\/ be a mix of already swapped and not-swapped card tables assigned to threads,\n+    \/\/ so they might have already dirtied the swapped card tables.\n+    \/\/ Conservatively scan all (non-free, non-committed) region's card tables,\n+    \/\/ creating the snapshot right now.\n+    log_debug(gc, refine)(\"Create work from scratch\");\n+\n+    refine_state().snapshot_heap(false \/* concurrent *\/);\n+  }\n+  return refine_state();\n+}\n+\n+void G1ConcurrentRefine::run_with_refinement_workers(WorkerTask* task) {\n+  _thread_control.run_task(task, num_threads_wanted());\n+}\n+\n+void G1ConcurrentRefine::notify_region_reclaimed(G1HeapRegion* r) {\n+  assert_at_safepoint();\n+  if (_refine_state.is_in_progress()) {\n+    _refine_state.sweep_table()->claim_all_cards(r->hrm_index());\n+  }\n+}\n+\n+G1ConcurrentRefine* G1ConcurrentRefine::create(G1CollectedHeap* g1h, jint* ecode) {\n+  G1ConcurrentRefine* cr = new G1ConcurrentRefine(g1h);\n@@ -178,0 +448,5 @@\n+  worker_threads_do(tc);\n+  control_thread_do(tc);\n+}\n+\n+void G1ConcurrentRefine::worker_threads_do(ThreadClosure *tc) {\n@@ -181,0 +456,4 @@\n+void G1ConcurrentRefine::control_thread_do(ThreadClosure *tc) {\n+  _thread_control.control_thread_do(tc);\n+}\n+\n@@ -183,1 +462,0 @@\n-                                                     size_t predicted_thread_buffer_cards,\n@@ -187,2 +465,2 @@\n-    log_debug(gc, ergo, refine)(\"Unchanged pending cards target: %zu\",\n-                                _pending_cards_target);\n+    log_debug(gc, ergo, refine)(\"Unchanged pending cards target: %zu (processed %zu minimum %zu time %1.2f)\",\n+                                _pending_cards_target, processed_logged_cards, minimum, logged_cards_time_ms);\n@@ -194,3 +472,1 @@\n-  size_t budget = static_cast<size_t>(goal_ms * rate);\n-  \/\/ Deduct predicted cards in thread buffers to get target.\n-  size_t new_target = budget - MIN2(budget, predicted_thread_buffer_cards);\n+  size_t new_target = static_cast<size_t>(goal_ms * rate);\n@@ -209,1 +485,0 @@\n-                                         size_t predicted_thread_buffer_cards,\n@@ -211,1 +486,3 @@\n-  if (!G1UseConcRefinement) return;\n+  if (!G1UseConcRefinement) {\n+    return;\n+  }\n@@ -215,1 +492,0 @@\n-                              predicted_thread_buffer_cards,\n@@ -217,9 +493,1 @@\n-  if (_thread_control.max_num_threads() == 0) {\n-    \/\/ If no refinement threads then the mutator threshold is the target.\n-    _dcqs.set_mutator_refinement_threshold(_pending_cards_target);\n-  } else {\n-    \/\/ Provisionally make the mutator threshold unlimited, to be updated by\n-    \/\/ the next periodic adjustment.  Because card state may have changed\n-    \/\/ drastically, record that adjustment is needed and kick the primary\n-    \/\/ thread, in case it is waiting.\n-    _dcqs.set_mutator_refinement_threshold(SIZE_MAX);\n+  if (_thread_control.max_num_threads() != 0) {\n@@ -228,1 +496,1 @@\n-      _thread_control.activate(0);\n+      _thread_control.activate();\n@@ -233,1 +501,1 @@\n-\/\/ Wake up the primary thread less frequently when the time available until\n+\/\/ Wake up the control thread less frequently when the time available until\n@@ -235,1 +503,1 @@\n-\/\/ This reduces the number of primary thread wakeups that just immediately\n+\/\/ This reduces the number of control thread wakeups that just immediately\n@@ -242,1 +510,1 @@\n-  assert_current_thread_is_primary_refinement_thread();\n+  assert_current_thread_is_control_refinement_thread();\n@@ -244,0 +512,6 @@\n+    \/\/ Retry asap when the cause for not getting a prediction was that we temporarily\n+    \/\/ did not get the heap lock. Otherwise we might wait for too long until we get\n+    \/\/ back here.\n+    if (_heap_was_locked) {\n+      return 1;\n+    }\n@@ -271,1 +545,1 @@\n-\/\/ Adjust the target length (in regions) of the young gen, based on the the\n+\/\/ Adjust the target length (in regions) of the young gen, based on the\n@@ -290,4 +564,11 @@\n-    size_t card_rs_length = g1h->young_regions_cardset()->occupied();\n-\n-    size_t sampled_code_root_rs_length = cl.sampled_code_root_rs_length();\n-    _policy->revise_young_list_target_length(card_rs_length, sampled_code_root_rs_length);\n+    size_t pending_cards;\n+    size_t current_to_collection_set_cards;\n+    {\n+      MutexLocker x(G1RareEvent_lock, Mutex::_no_safepoint_check_flag);\n+      G1Policy* p = g1h->policy();\n+      pending_cards = p->current_pending_cards();\n+      current_to_collection_set_cards = p->current_to_collection_set_cards();\n+    }\n+    _policy->revise_young_list_target_length(pending_cards,\n+                                             current_to_collection_set_cards,\n+                                             cl.sampled_code_root_rs_length());\n@@ -297,2 +578,2 @@\n-bool G1ConcurrentRefine::adjust_threads_periodically() {\n-  assert_current_thread_is_primary_refinement_thread();\n+bool G1ConcurrentRefine::adjust_num_threads_periodically() {\n+  assert_current_thread_is_control_refinement_thread();\n@@ -300,1 +581,3 @@\n-  \/\/ Check whether it's time to do a periodic adjustment.\n+  _heap_was_locked = false;\n+  \/\/ Check whether it's time to do a periodic adjustment if there is no explicit\n+  \/\/ request pending. We might have spuriously woken up.\n@@ -303,2 +586,3 @@\n-    if (since_adjust.milliseconds() >= adjust_threads_period_ms()) {\n-      _needs_adjust = true;\n+    if (since_adjust.milliseconds() < adjust_threads_period_ms()) {\n+      _num_threads_wanted = 0;\n+      return false;\n@@ -308,19 +592,20 @@\n-  \/\/ If needed, try to adjust threads wanted.\n-  if (_needs_adjust) {\n-    \/\/ Getting used young bytes requires holding Heap_lock.  But we can't use\n-    \/\/ normal lock and block until available.  Blocking on the lock could\n-    \/\/ deadlock with a GC VMOp that is holding the lock and requesting a\n-    \/\/ safepoint.  Instead try to lock, and if fail then skip adjustment for\n-    \/\/ this iteration of the thread, do some refinement work, and retry the\n-    \/\/ adjustment later.\n-    if (Heap_lock->try_lock()) {\n-      size_t used_bytes = _policy->estimate_used_young_bytes_locked();\n-      Heap_lock->unlock();\n-      adjust_young_list_target_length();\n-      size_t young_bytes = _policy->young_list_target_length() * G1HeapRegion::GrainBytes;\n-      size_t available_bytes = young_bytes - MIN2(young_bytes, used_bytes);\n-      adjust_threads_wanted(available_bytes);\n-      _needs_adjust = false;\n-      _last_adjust = Ticks::now();\n-      return true;\n-    }\n+  \/\/ Reset pending request.\n+  _needs_adjust = false;\n+  \/\/ Getting used young bytes requires holding Heap_lock.  But we can't use\n+  \/\/ normal lock and block until available.  Blocking on the lock could\n+  \/\/ deadlock with a GC VMOp that is holding the lock and requesting a\n+  \/\/ safepoint.  Instead try to lock, and if fail then skip adjustment for\n+  \/\/ this iteration and retry the adjustment later.\n+  if (Heap_lock->try_lock()) {\n+    size_t used_bytes = _policy->estimate_used_young_bytes_locked();\n+    Heap_lock->unlock();\n+\n+    adjust_young_list_target_length();\n+    size_t young_bytes = _policy->young_list_target_length() * G1HeapRegion::GrainBytes;\n+    size_t available_bytes = young_bytes - MIN2(young_bytes, used_bytes);\n+    adjust_threads_wanted(available_bytes);\n+    _last_adjust = Ticks::now();\n+  } else {\n+    _heap_was_locked = true;\n+    \/\/ Defer adjustment to next time.\n+    _needs_adjust = true;\n@@ -329,5 +614,1 @@\n-  return false;\n-}\n-\n-bool G1ConcurrentRefine::is_in_last_adjustment_period() const {\n-  return _threads_needed.predicted_time_until_next_gc_ms() <= adjust_threads_period_ms();\n+  return (_num_threads_wanted > 0) && !wait_for_heap_lock();\n@@ -337,4 +618,6 @@\n-  assert_current_thread_is_primary_refinement_thread();\n-  size_t num_cards = _dcqs.num_cards();\n-  size_t mutator_threshold = SIZE_MAX;\n-  uint old_wanted = Atomic::load(&_threads_wanted);\n+  assert_current_thread_is_control_refinement_thread();\n+\n+  G1Policy* policy = G1CollectedHeap::heap()->policy();\n+  const G1Analytics* analytics = policy->analytics();\n+\n+  size_t num_cards = policy->current_pending_cards();\n@@ -342,1 +625,1 @@\n-  _threads_needed.update(old_wanted,\n+  _threads_needed.update(_num_threads_wanted,\n@@ -348,5 +631,1 @@\n-    \/\/ If running all the threads can't reach goal, turn on refinement by\n-    \/\/ mutator threads.  Using target as the threshold may be stronger\n-    \/\/ than required, but will do the most to get us under goal, and we'll\n-    \/\/ reevaluate with the next adjustment.\n-    mutator_threshold = _pending_cards_target;\n+    \/\/ Bound the wanted threads by maximum available.\n@@ -354,7 +633,0 @@\n-  } else if (is_in_last_adjustment_period()) {\n-    \/\/ If very little time remains until GC, enable mutator refinement.  If\n-    \/\/ the target has been reached, this keeps the number of pending cards on\n-    \/\/ target even if refinement threads deactivate in the meantime.  And if\n-    \/\/ the target hasn't been reached, this prevents things from getting\n-    \/\/ worse.\n-    mutator_threshold = _pending_cards_target;\n@@ -362,4 +634,4 @@\n-  Atomic::store(&_threads_wanted, new_wanted);\n-  _dcqs.set_mutator_refinement_threshold(mutator_threshold);\n-  log_debug(gc, refine)(\"Concurrent refinement: wanted %u, cards: %zu, \"\n-                        \"predicted: %zu, time: %1.2fms\",\n+  _num_threads_wanted = new_wanted;\n+\n+  log_debug(gc, refine)(\"Concurrent refinement: wanted %u, pending cards: %zu (pending-from-gc %zu), \"\n+                        \"predicted: %zu, goal %zu, time-until-next-gc: %1.2fms pred-refine-rate %1.2fc\/ms log-rate %1.2fc\/ms\",\n@@ -368,0 +640,1 @@\n+                        G1CollectedHeap::heap()->policy()->pending_cards_from_gc(),\n@@ -369,32 +642,5 @@\n-                        _threads_needed.predicted_time_until_next_gc_ms());\n-  \/\/ Activate newly wanted threads.  The current thread is the primary\n-  \/\/ refinement thread, so is already active.\n-  for (uint i = MAX2(old_wanted, 1u); i < new_wanted; ++i) {\n-    if (!_thread_control.activate(i)) {\n-      \/\/ Failed to allocate and activate thread.  Stop trying to activate, and\n-      \/\/ instead use mutator threads to make up the gap.\n-      Atomic::store(&_threads_wanted, i);\n-      _dcqs.set_mutator_refinement_threshold(_pending_cards_target);\n-      break;\n-    }\n-  }\n-}\n-\n-void G1ConcurrentRefine::reduce_threads_wanted() {\n-  assert_current_thread_is_primary_refinement_thread();\n-  if (!_needs_adjust) {         \/\/ Defer if adjustment request is active.\n-    uint wanted = Atomic::load(&_threads_wanted);\n-    if (wanted > 0) {\n-      Atomic::store(&_threads_wanted, --wanted);\n-    }\n-    \/\/ If very little time remains until GC, enable mutator refinement.  If\n-    \/\/ the target has been reached, this keeps the number of pending cards on\n-    \/\/ target even as refinement threads deactivate in the meantime.\n-    if (is_in_last_adjustment_period()) {\n-      _dcqs.set_mutator_refinement_threshold(_pending_cards_target);\n-    }\n-  }\n-}\n-\n-bool G1ConcurrentRefine::is_thread_wanted(uint worker_id) const {\n-  return worker_id < Atomic::load(&_threads_wanted);\n+                        _pending_cards_target,\n+                        _threads_needed.predicted_time_until_next_gc_ms(),\n+                        analytics->predict_concurrent_refine_rate_ms(),\n+                        analytics->predict_dirtied_cards_rate_ms()\n+                        );\n@@ -404,1 +650,1 @@\n-  assert_current_thread_is_primary_refinement_thread();\n+  assert_current_thread_is_control_refinement_thread();\n@@ -409,1 +655,1 @@\n-  assert_current_thread_is_primary_refinement_thread();\n+  assert_current_thread_is_control_refinement_thread();\n@@ -412,25 +658,0 @@\n-\n-G1ConcurrentRefineStats G1ConcurrentRefine::get_and_reset_refinement_stats() {\n-  struct CollectStats : public ThreadClosure {\n-    G1ConcurrentRefineStats _total_stats;\n-    virtual void do_thread(Thread* t) {\n-      G1ConcurrentRefineThread* crt = static_cast<G1ConcurrentRefineThread*>(t);\n-      G1ConcurrentRefineStats& stats = *crt->refinement_stats();\n-      _total_stats += stats;\n-      stats.reset();\n-    }\n-  } collector;\n-  threads_do(&collector);\n-  return collector._total_stats;\n-}\n-\n-uint G1ConcurrentRefine::worker_id_offset() {\n-  return G1DirtyCardQueueSet::num_par_ids();\n-}\n-\n-bool G1ConcurrentRefine::try_refinement_step(uint worker_id,\n-                                             size_t stop_at,\n-                                             G1ConcurrentRefineStats* stats) {\n-  uint adjusted_id = worker_id + worker_id_offset();\n-  return _dcqs.refine_completed_buffer_concurrently(adjusted_id, stop_at, stats);\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.cpp","additions":413,"deletions":192,"binary":false,"changes":605,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+class G1CollectedHeap;\n@@ -39,1 +40,1 @@\n-class G1DirtyCardQueueSet;\n+class G1HeapRegion;\n@@ -41,0 +42,1 @@\n+class G1CardTableClaimTable;\n@@ -42,0 +44,2 @@\n+class WorkerTask;\n+class WorkerThreads;\n@@ -47,1 +51,4 @@\n-  GrowableArrayCHeap<G1ConcurrentRefineThread*, mtGC> _threads;\n+  G1ConcurrentRefineThread* _control_thread;\n+\n+  WorkerThreads* _workers;\n+  uint _max_num_threads;\n@@ -51,3 +58,1 @@\n-  G1ConcurrentRefineThread* create_refinement_thread(uint worker_id, bool initializing);\n-\n-  bool ensure_threads_created(uint worker_id, bool initializing);\n+  G1ConcurrentRefineThread* create_refinement_thread();\n@@ -63,1 +68,1 @@\n-  void assert_current_thread_is_primary_refinement_thread() const NOT_DEBUG_RETURN;\n+  void assert_current_thread_is_control_refinement_thread() const NOT_DEBUG_RETURN;\n@@ -65,1 +70,1 @@\n-  uint max_num_threads() const { return _threads.capacity(); }\n+  uint max_num_threads() const { return _max_num_threads; }\n@@ -67,6 +72,2 @@\n-  \/\/ Activate the indicated thread.  If the thread has not yet been allocated,\n-  \/\/ allocate and then activate.  If allocation is needed and fails, return\n-  \/\/ false.  Otherwise return true.\n-  \/\/ precondition: worker_id < max_num_threads().\n-  \/\/ precondition: current thread is not the designated worker.\n-  bool activate(uint worker_id);\n+  \/\/ Activate the control thread.\n+  void activate();\n@@ -74,0 +75,3 @@\n+  void run_task(WorkerTask* task, uint num_workers);\n+\n+  void control_thread_do(ThreadClosure* tc);\n@@ -78,0 +82,78 @@\n+\/\/ Tracks the current refinement state from idle to completion (and reset back\n+\/\/ to idle).\n+class G1ConcurrentRefineWorkState {\n+\n+  enum class State : uint {\n+    Idle,                        \/\/ Refinement is doing nothing.\n+    SwapGlobalCT,                \/\/ Swap global card table.\n+    SwapJavaThreadsCT,           \/\/ Swap java thread's card tables.\n+    SwapGCThreadsCT,             \/\/ Swap GC thread's card tables.\n+    SnapshotHeap,                \/\/ Take a snapshot of the region's top() values.\n+    SweepRT,                     \/\/ Sweep the refinement table for pending (dirty) cards.\n+    CompleteRefineWork,          \/\/ Cleanup of refinement work, reset to idle.\n+    Last\n+  } _state;\n+\n+  static const char* state_name(State state) {\n+    static const char* _state_names[] = {\n+      \"Idle\",\n+      \"Swap Global CT\",\n+      \"Swap JavaThread CT\",\n+      \"Swap GC Thread CT\",\n+      \"Snapshot Heap\",\n+      \"Sweep RT\",\n+      \"Complete Refine Work\"\n+    };\n+\n+    return _state_names[static_cast<uint>(state)];\n+  }\n+\n+  \/\/ Current epoch the work has been started; used to determine if there has been\n+  \/\/ a forced card table swap due to a garbage collection while doing work.\n+  size_t _refine_work_epoch;\n+\n+  \/\/ Current heap snapshot.\n+  G1CardTableClaimTable* _sweep_table;\n+\n+  \/\/ Start times for all states.\n+  Ticks _state_start[static_cast<uint>(State::Last)];\n+\n+  void set_state_start_time();\n+  Tickspan get_duration(State start, State end);\n+\n+  G1ConcurrentRefineStats _stats;\n+\n+  static size_t refinement_epoch();\n+\n+  \/\/ Advances the state to next_state if not interrupted by a changed epoch. Returns\n+  \/\/ to Idle otherwise.\n+  bool advance_state(State next_state);\n+\n+  void assert_state(State expected);\n+\n+public:\n+  G1ConcurrentRefineWorkState(uint max_reserved_regions);\n+  ~G1ConcurrentRefineWorkState();\n+\n+  void start_refine_work();\n+\n+  bool swap_global_card_table();\n+  bool swap_java_threads_ct();\n+  bool swap_gc_threads_ct();\n+  void snapshot_heap(bool concurrent = true);\n+  void sweep_rt_start();\n+  bool sweep_rt_step();\n+\n+  bool complete(bool concurrent, bool print_log = true);\n+\n+  static void snapshot_heap_into(G1CardTableClaimTable* sweep_table);\n+\n+  G1CardTableClaimTable* sweep_table() { return _sweep_table; }\n+  G1ConcurrentRefineStats* stats() { return &_stats; }\n+  void reset_stats();\n+\n+  void add_yield_duration(jlong duration);\n+\n+  bool is_in_progress() const;\n+};\n+\n@@ -87,6 +169,3 @@\n-\/\/ Concurrent refinement is performed by a combination of dedicated threads\n-\/\/ and by mutator threads as they produce dirty cards.  If configured to not\n-\/\/ have any dedicated threads (-XX:G1ConcRefinementThreads=0) then all\n-\/\/ concurrent refinement work is performed by mutator threads.  When there are\n-\/\/ dedicated threads, they generally do most of the concurrent refinement\n-\/\/ work, to minimize throughput impact of refinement work on mutator threads.\n+\/\/ Concurrent refinement is performed by a set of dedicated threads.  If configured\n+\/\/ to not have any dedicated threads (-XX:G1ConcRefinementThreads=0) then no\n+\/\/ refinement work is performed at all.\n@@ -98,5 +177,8 @@\n-\/\/ There are two kinds of dedicated refinement threads, a single primary\n-\/\/ thread and some number of secondary threads.  When active, all refinement\n-\/\/ threads take buffers of dirty cards from the dirty card queue and process\n-\/\/ them.  Between buffers they query this owning object to find out whether\n-\/\/ they should continue running, deactivating themselves if not.\n+\/\/ There are two kinds of dedicated refinement threads, a single control\n+\/\/ thread and some number of refinement worker threads.\n+\/\/ The control thread determines whether there is need to do work, and then starts\n+\/\/ an appropriate number of refinement worker threads to get back to the target\n+\/\/ number of pending dirty cards.\n+\/\/\n+\/\/ The control wakes up periodically whether there is need to do refinement\n+\/\/ work, starting the refinement process as necessary.\n@@ -104,7 +186,0 @@\n-\/\/ The primary thread drives the control system that determines how many\n-\/\/ refinement threads should be active.  If inactive, it wakes up periodically\n-\/\/ to recalculate the number of active threads needed, and activates\n-\/\/ additional threads as necessary.  While active it also periodically\n-\/\/ recalculates the number wanted and activates more threads if needed.  It\n-\/\/ also reduces the number of wanted threads when the target has been reached,\n-\/\/ triggering deactivations.\n@@ -113,1 +188,1 @@\n-  volatile uint _threads_wanted;\n+  volatile uint _num_threads_wanted;\n@@ -118,0 +193,2 @@\n+  bool _heap_was_locked;                \/\/ The heap has been locked the last time we tried to adjust the number of refinement threads.\n+\n@@ -120,1 +197,0 @@\n-  G1DirtyCardQueueSet& _dcqs;\n@@ -122,1 +198,1 @@\n-  G1ConcurrentRefine(G1Policy* policy);\n+  G1ConcurrentRefineWorkState _refine_state;\n@@ -124,1 +200,1 @@\n-  static uint worker_id_offset();\n+  G1ConcurrentRefine(G1CollectedHeap* g1h);\n@@ -128,2 +204,2 @@\n-  void assert_current_thread_is_primary_refinement_thread() const {\n-    _thread_control.assert_current_thread_is_primary_refinement_thread();\n+  void assert_current_thread_is_control_refinement_thread() const {\n+    _thread_control.assert_current_thread_is_control_refinement_thread();\n@@ -143,1 +219,0 @@\n-                                   size_t predicted_thread_buffer_cards,\n@@ -147,1 +222,0 @@\n-  bool is_in_last_adjustment_period() const;\n@@ -159,0 +233,8 @@\n+  G1ConcurrentRefineWorkState& refine_state() { return _refine_state; }\n+\n+  G1ConcurrentRefineWorkState& refine_state_for_merge();\n+\n+  void run_with_refinement_workers(WorkerTask* task);\n+\n+  void notify_region_reclaimed(G1HeapRegion* r);\n+\n@@ -161,1 +243,1 @@\n-  static G1ConcurrentRefine* create(G1Policy* policy, jint* ecode);\n+  static G1ConcurrentRefine* create(G1CollectedHeap* g1h, jint* ecode);\n@@ -168,2 +250,2 @@\n-  \/\/ cards.  Updates the mutator refinement threshold.  Ensures the primary\n-  \/\/ refinement thread (if it exists) is active, so it will adjust the number\n+  \/\/ cards.  Updates the mutator refinement threshold.  Ensures the refinement\n+  \/\/ control thread (if it exists) is active, so it will adjust the number\n@@ -173,1 +255,0 @@\n-                       size_t predicted_thread_buffer_cards,\n@@ -179,7 +260,7 @@\n-  \/\/ May recalculate the number of refinement threads that should be active in\n-  \/\/ order to meet the pending cards target.  Returns true if adjustment was\n-  \/\/ performed, and clears any pending request.  Returns false if the\n-  \/\/ adjustment period has not expired, or because a timed or requested\n-  \/\/ adjustment could not be performed immediately and so was deferred.\n-  \/\/ precondition: current thread is the primary refinement thread.\n-  bool adjust_threads_periodically();\n+  \/\/ Recalculates the number of refinement threads that should be active in\n+  \/\/ order to meet the pending cards target.\n+  \/\/ Returns true if it could recalculate the number of threads and\n+  \/\/ refinement threads should be started.\n+  \/\/ Returns false if the adjustment period has not expired, or because a timed\n+  \/\/ or requested adjustment could not be performed immediately and so was deferred.\n+  bool adjust_num_threads_periodically();\n@@ -187,1 +268,1 @@\n-  \/\/ The amount of time (in ms) the primary refinement thread should sleep\n+  \/\/ The amount of time (in ms) the refinement control thread should sleep\n@@ -189,1 +270,1 @@\n-  \/\/ precondition: current thread is the primary refinement thread.\n+  \/\/ precondition: current thread is the refinement control thread.\n@@ -193,1 +274,1 @@\n-  \/\/ precondition: current thread is the primary refinement thread.\n+  \/\/ precondition: current thread is the refinement control thread.\n@@ -197,1 +278,1 @@\n-  \/\/ precondition: current thread is the primary refinement thread.\n+  \/\/ precondition: current thread is the refinement control thread.\n@@ -200,10 +281,3 @@\n-  \/\/ Reduce the number of active threads wanted.\n-  \/\/ precondition: current thread is the primary refinement thread.\n-  void reduce_threads_wanted();\n-\n-  \/\/ Test whether the thread designated by worker_id should be active.\n-  bool is_thread_wanted(uint worker_id) const;\n-\n-  \/\/ Return total of concurrent refinement stats for the\n-  \/\/ ConcurrentRefineThreads.  Also reset the stats for the threads.\n-  G1ConcurrentRefineStats get_and_reset_refinement_stats();\n+  \/\/ Indicate that last refinement adjustment had been deferred due to not\n+  \/\/ obtaining the heap lock.\n+  bool wait_for_heap_lock() const { return _heap_was_locked; }\n@@ -211,6 +285,1 @@\n-  \/\/ Perform a single refinement step; called by the refinement\n-  \/\/ threads.  Returns true if there was refinement work available.\n-  \/\/ Updates stats.\n-  bool try_refinement_step(uint worker_id,\n-                           size_t stop_at,\n-                           G1ConcurrentRefineStats* stats);\n+  uint num_threads_wanted() const { return _num_threads_wanted; }\n@@ -220,0 +289,3 @@\n+  \/\/ Iterate over specific refinement threads applying the given closure.\n+  void worker_threads_do(ThreadClosure *tc);\n+  void control_thread_do(ThreadClosure *tc);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefine.hpp","additions":143,"deletions":71,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/timer.hpp\"\n@@ -28,4 +30,9 @@\n-  _refinement_time(),\n-  _refined_cards(0),\n-  _precleaned_cards(0),\n-  _dirtied_cards(0)\n+  _sweep_duration(0),\n+  _yield_duration(0),\n+  _cards_scanned(0),\n+  _cards_clean(0),\n+  _cards_not_parsable(0),\n+  _cards_still_refer_to_cset(0),\n+  _cards_refer_to_cset(0),\n+  _cards_clean_again(0),\n+  _refine_duration(0)\n@@ -34,4 +41,12 @@\n-double G1ConcurrentRefineStats::refinement_rate_ms() const {\n-  \/\/ Report 0 when no time recorded because no refinement performed.\n-  double secs = refinement_time().seconds();\n-  return (secs > 0) ? (refined_cards() \/ (secs * MILLIUNITS)) : 0.0;\n+void G1ConcurrentRefineStats::add_atomic(G1ConcurrentRefineStats* other) {\n+  Atomic::add(&_sweep_duration, other->_sweep_duration, memory_order_relaxed);\n+  Atomic::add(&_yield_duration, other->_yield_duration, memory_order_relaxed);\n+\n+  Atomic::add(&_cards_scanned, other->_cards_scanned, memory_order_relaxed);\n+  Atomic::add(&_cards_clean, other->_cards_clean, memory_order_relaxed);\n+  Atomic::add(&_cards_not_parsable, other->_cards_not_parsable, memory_order_relaxed);\n+  Atomic::add(&_cards_still_refer_to_cset, other->_cards_still_refer_to_cset, memory_order_relaxed);\n+  Atomic::add(&_cards_refer_to_cset, other->_cards_refer_to_cset, memory_order_relaxed);\n+  Atomic::add(&_cards_clean_again, other->_cards_clean_again, memory_order_relaxed);\n+\n+  Atomic::add(&_refine_duration, other->_refine_duration, memory_order_relaxed);\n@@ -42,4 +57,11 @@\n-  _refinement_time += other._refinement_time;\n-  _refined_cards += other._refined_cards;\n-  _precleaned_cards += other._precleaned_cards;\n-  _dirtied_cards += other._dirtied_cards;\n+  _sweep_duration += other._sweep_duration;\n+  _yield_duration += other._yield_duration;\n+\n+  _cards_scanned += other._cards_scanned;\n+  _cards_clean += other._cards_clean;\n+  _cards_not_parsable += other._cards_not_parsable;\n+  _cards_still_refer_to_cset += other._cards_still_refer_to_cset;\n+  _cards_refer_to_cset += other._cards_refer_to_cset;\n+  _cards_clean_again += other._cards_clean_again;\n+\n+  _refine_duration += other._refine_duration;\n@@ -50,1 +72,1 @@\n-static T clipped_sub(T x, T y) {\n+static T saturated_sub(T x, T y) {\n@@ -56,4 +78,11 @@\n-  _refinement_time = clipped_sub(_refinement_time, other._refinement_time);\n-  _refined_cards = clipped_sub(_refined_cards, other._refined_cards);\n-  _precleaned_cards = clipped_sub(_precleaned_cards, other._precleaned_cards);\n-  _dirtied_cards = clipped_sub(_dirtied_cards, other._dirtied_cards);\n+  _sweep_duration = saturated_sub(_sweep_duration, other._sweep_duration);\n+  _yield_duration = saturated_sub(_yield_duration, other._yield_duration);\n+\n+  _cards_scanned = saturated_sub(_cards_scanned, other._cards_scanned);\n+  _cards_clean = saturated_sub(_cards_clean, other._cards_clean);\n+  _cards_not_parsable = saturated_sub(_cards_not_parsable, other._cards_not_parsable);\n+  _cards_still_refer_to_cset = saturated_sub(_cards_still_refer_to_cset, other._cards_still_refer_to_cset);\n+  _cards_refer_to_cset = saturated_sub(_cards_refer_to_cset, other._cards_refer_to_cset);\n+  _cards_clean_again = saturated_sub(_cards_clean_again, other._cards_clean_again);\n+\n+  _refine_duration = saturated_sub(_refine_duration, other._refine_duration);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineStats.cpp","additions":46,"deletions":17,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,4 +36,12 @@\n-  Tickspan _refinement_time;\n-  size_t _refined_cards;\n-  size_t _precleaned_cards;\n-  size_t _dirtied_cards;\n+  jlong _sweep_duration;              \/\/ Time spent sweeping the table finding non-clean cards\n+                                      \/\/ and refining them.\n+  jlong _yield_duration;              \/\/ Time spent yielding during the sweep (not doing the sweep).\n+\n+  size_t _cards_scanned;              \/\/ Total number of cards scanned.\n+  size_t _cards_clean;                \/\/ Number of cards found clean.\n+  size_t _cards_not_parsable;         \/\/ Number of cards we could not parse and left unrefined.\n+  size_t _cards_still_refer_to_cset;  \/\/ Number of cards marked still young.\n+  size_t _cards_refer_to_cset;        \/\/ Number of dirty cards that contain a to-cset reference.\n+  size_t _cards_clean_again;          \/\/ Dirtied cards that were cleaned.\n+\n+  jlong _refine_duration;             \/\/ Time spent during actual refinement.\n@@ -44,2 +52,5 @@\n-  \/\/ Time spent performing concurrent refinement.\n-  Tickspan refinement_time() const { return _refinement_time; }\n+  \/\/ Time spent performing sweeping the refinement table (includes actual refinement,\n+  \/\/ but not yield time).\n+  jlong sweep_duration() const { return _sweep_duration - _yield_duration; }\n+  jlong yield_duration() const { return _yield_duration; }\n+  jlong refine_duration() const { return _refine_duration; }\n@@ -48,1 +59,12 @@\n-  size_t refined_cards() const { return _refined_cards; }\n+  size_t refined_cards() const { return cards_not_clean(); }\n+\n+  size_t cards_scanned() const { return _cards_scanned; }\n+  size_t cards_clean() const { return _cards_clean; }\n+  size_t cards_not_clean() const { return _cards_scanned - _cards_clean; }\n+  size_t cards_not_parsable() const { return _cards_not_parsable; }\n+  size_t cards_still_refer_to_cset() const { return _cards_still_refer_to_cset; }\n+  size_t cards_refer_to_cset() const { return _cards_refer_to_cset; }\n+  size_t cards_clean_again() const { return _cards_clean_again; }\n+  \/\/ Number of cards that were marked dirty and in need of refinement. This includes cards recently\n+  \/\/ found to refer to the collection set as they originally were dirty.\n+  size_t cards_pending() const { return cards_not_clean() - _cards_still_refer_to_cset; }\n@@ -50,2 +72,1 @@\n-  \/\/ Refinement rate, in cards per ms.\n-  double refinement_rate_ms() const;\n+  size_t cards_to_cset() const { return _cards_still_refer_to_cset + _cards_refer_to_cset; }\n@@ -53,3 +74,3 @@\n-  \/\/ Number of cards for which refinement was skipped because some other\n-  \/\/ thread had already refined them.\n-  size_t precleaned_cards() const { return _precleaned_cards; }\n+  void inc_sweep_time(jlong t) { _sweep_duration += t; }\n+  void inc_yield_duration(jlong t) { _yield_duration += t; }\n+  void inc_refine_duration(jlong t) { _refine_duration += t; }\n@@ -57,2 +78,6 @@\n-  \/\/ Number of cards marked dirty and in need of refinement.\n-  size_t dirtied_cards() const { return _dirtied_cards; }\n+  void inc_cards_scanned(size_t increment = 1) { _cards_scanned += increment; }\n+  void inc_cards_clean(size_t increment = 1) { _cards_clean += increment; }\n+  void inc_cards_not_parsable() { _cards_not_parsable++; }\n+  void inc_cards_still_refer_to_cset() { _cards_still_refer_to_cset++; }\n+  void inc_cards_refer_to_cset() { _cards_refer_to_cset++; }\n+  void inc_cards_clean_again() { _cards_clean_again++; }\n@@ -60,4 +85,1 @@\n-  void inc_refinement_time(Tickspan t) { _refinement_time += t; }\n-  void inc_refined_cards(size_t cards) { _refined_cards += cards; }\n-  void inc_precleaned_cards(size_t cards) { _precleaned_cards += cards; }\n-  void inc_dirtied_cards(size_t cards) { _dirtied_cards += cards; }\n+  void add_atomic(G1ConcurrentRefineStats* other);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineStats.hpp","additions":41,"deletions":19,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/g1\/g1CardTableClaimTable.inline.hpp\"\n@@ -29,1 +30,2 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n+#include \"gc\/g1\/g1ConcurrentRefineWorkTask.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n@@ -41,1 +43,1 @@\n-G1ConcurrentRefineThread::G1ConcurrentRefineThread(G1ConcurrentRefine* cr, uint worker_id) :\n+G1ConcurrentRefineThread::G1ConcurrentRefineThread(G1ConcurrentRefine* cr) :\n@@ -45,1 +47,1 @@\n-  _notifier(Mutex::nosafepoint, FormatBuffer<>(\"G1 Refine#%d\", worker_id), true),\n+  _notifier(Mutex::nosafepoint, \"G1 Refine Control\", true),\n@@ -47,2 +49,0 @@\n-  _refinement_stats(),\n-  _worker_id(worker_id),\n@@ -51,2 +51,1 @@\n-  \/\/ set name\n-  set_name(\"G1 Refine#%d\", worker_id);\n+  set_name(\"G1 Refine Control\");\n@@ -58,1 +57,1 @@\n-  while (wait_for_completed_buffers()) {\n+  while (wait_for_work()) {\n@@ -60,1 +59,0 @@\n-    G1ConcurrentRefineStats active_stats_start = _refinement_stats;\n@@ -64,1 +62,1 @@\n-        report_inactive(\"Paused\", _refinement_stats - active_stats_start);\n+        report_inactive(\"Paused\");\n@@ -68,1 +66,0 @@\n-        active_stats_start = _refinement_stats;\n@@ -70,2 +67,10 @@\n-      } else if (maybe_deactivate()) {\n-        break;\n+      }\n+      \/\/ Look if we want to do refinement. If we don't then don't do any refinement\n+      \/\/ this. This thread may have just woken up but no threads are currently\n+      \/\/ needed, which is common.  In this case we want to just go back to\n+      \/\/ waiting, with a minimum of fuss; in particular, don't do any \"premature\"\n+      \/\/ refinement.  However, adjustment may be pending but temporarily\n+      \/\/ blocked. In that case we wait for adjustment to succeed.\n+      Ticks adjust_start = Ticks::now();\n+      if (cr()->adjust_num_threads_periodically()) {\n+        do_refinement();\n@@ -73,1 +78,8 @@\n-        do_refinement_step();\n+        log_debug(gc,refine)(\"Concurrent Refine Adjust Only (#threads wanted: %u adjustment_needed: %s wait_for_heap_lock: %s) %.2fms\",\n+                             cr()->num_threads_wanted(),\n+                             BOOL_TO_STR(cr()->is_thread_adjustment_needed()),\n+                             BOOL_TO_STR(cr()->wait_for_heap_lock()),\n+                             (Ticks::now() - adjust_start).seconds() * MILLIUNITS);\n+\n+        deactivate();\n+        break;\n@@ -76,1 +88,1 @@\n-    report_inactive(\"Deactivated\", _refinement_stats - active_stats_start);\n+    report_inactive(\"Deactivated\");\n@@ -80,1 +92,1 @@\n-  log_debug(gc, refine)(\"Stopping %d\", _worker_id);\n+  log_debug(gc, refine)(\"Stopping %s\", name());\n@@ -84,4 +96,1 @@\n-  log_trace(gc, refine)(\"%s worker %u, current: %zu\",\n-                        reason,\n-                        _worker_id,\n-                        G1BarrierSet::dirty_card_queue_set().num_cards());\n+  log_trace(gc, refine)(\"%s active (%s)\", name(), reason);\n@@ -90,9 +99,2 @@\n-void G1ConcurrentRefineThread::report_inactive(const char* reason,\n-                                               const G1ConcurrentRefineStats& stats) const {\n-  log_trace(gc, refine)\n-           (\"%s worker %u, cards: %zu, refined %zu, rate %1.2fc\/ms\",\n-            reason,\n-            _worker_id,\n-            G1BarrierSet::dirty_card_queue_set().num_cards(),\n-            stats.refined_cards(),\n-            stats.refinement_rate_ms());\n+void G1ConcurrentRefineThread::report_inactive(const char* reason) const {\n+  log_trace(gc, refine)(\"%s inactive (%s)\", name(), reason);\n@@ -110,13 +112,1 @@\n-bool G1ConcurrentRefineThread::maybe_deactivate() {\n-  assert(this == Thread::current(), \"precondition\");\n-  if (cr()->is_thread_wanted(_worker_id)) {\n-    return false;\n-  } else {\n-    MutexLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n-    bool requested = _requested_active;\n-    _requested_active = false;\n-    return !requested;  \/\/ Deactivate only if not recently requested active.\n-  }\n-}\n-\n-bool G1ConcurrentRefineThread::try_refinement_step(size_t stop_at) {\n+bool G1ConcurrentRefineThread::deactivate() {\n@@ -124,1 +114,4 @@\n-  return _cr->try_refinement_step(_worker_id, stop_at, &_refinement_stats);\n+  MutexLocker ml(&_notifier, Mutex::_no_safepoint_check_flag);\n+  bool requested = _requested_active;\n+  _requested_active = false;\n+  return !requested;  \/\/ Deactivate only if not recently requested active.\n@@ -131,16 +124,3 @@\n-\/\/ The (single) primary thread drives the controller for the refinement threads.\n-class G1PrimaryConcurrentRefineThread final : public G1ConcurrentRefineThread {\n-  bool wait_for_completed_buffers() override;\n-  bool maybe_deactivate() override;\n-  void do_refinement_step() override;\n-  void track_usage() override;\n-\n-public:\n-  G1PrimaryConcurrentRefineThread(G1ConcurrentRefine* cr) :\n-    G1ConcurrentRefineThread(cr, 0)\n-  {}\n-};\n-\n-\/\/ When inactive, the primary thread periodically wakes up and requests\n-\/\/ adjustment of the number of active refinement threads.\n-bool G1PrimaryConcurrentRefineThread::wait_for_completed_buffers() {\n+\/\/ When inactive, the control thread periodically wakes up to check if there is\n+\/\/ refinement work pending.\n+bool G1ConcurrentRefineThread::wait_for_work() {\n@@ -159,5 +139,2 @@\n-bool G1PrimaryConcurrentRefineThread::maybe_deactivate() {\n-  \/\/ Don't deactivate while needing to adjust the number of active threads.\n-  return !cr()->is_thread_adjustment_needed() &&\n-         G1ConcurrentRefineThread::maybe_deactivate();\n-}\n+void G1ConcurrentRefineThread::do_refinement() {\n+  G1ConcurrentRefineWorkState& state = _cr->refine_state();\n@@ -165,16 +142,2 @@\n-void G1PrimaryConcurrentRefineThread::do_refinement_step() {\n-  \/\/ Try adjustment first.  If it succeeds then don't do any refinement this\n-  \/\/ round.  This thread may have just woken up but no threads are currently\n-  \/\/ needed, which is common.  In this case we want to just go back to\n-  \/\/ waiting, with a minimum of fuss; in particular, don't do any \"premature\"\n-  \/\/ refinement.  However, adjustment may be pending but temporarily\n-  \/\/ blocked. In that case we *do* try refinement, rather than possibly\n-  \/\/ uselessly spinning while waiting for adjustment to succeed.\n-  if (!cr()->adjust_threads_periodically()) {\n-    \/\/ No adjustment, so try refinement, with the target as a cuttoff.\n-    if (!try_refinement_step(cr()->pending_cards_target())) {\n-      \/\/ Refinement was cut off, so proceed with fewer threads.\n-      cr()->reduce_threads_wanted();\n-    }\n-  }\n-}\n+  state.start_refine_work();\n+  log_debug(gc,refine)(\"Concurrent Refine Work Start (threads wanted: %u)\", _cr->num_threads_wanted());\n@@ -182,6 +145,6 @@\n-void G1PrimaryConcurrentRefineThread::track_usage() {\n-  G1ConcurrentRefineThread::track_usage();\n-  \/\/ The primary thread is responsible for updating the CPU time for all workers.\n-  if (UsePerfData && os::is_thread_cpu_time_supported()) {\n-    ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_conc_refine);\n-    cr()->threads_do(&tttc);\n+  \/\/ Swap card tables.\n+\n+  \/\/ 1. Global card table\n+  if (!state.swap_global_card_table()) {\n+    log_debug(gc, refine)(\"GC pause after Global Card Table Swap\");\n+    return;\n@@ -189,1 +152,1 @@\n-}\n+  log_debug(gc, refine)(\"Concurrent Refine Global Card Table Swap\");\n@@ -191,3 +154,6 @@\n-class G1SecondaryConcurrentRefineThread final : public G1ConcurrentRefineThread {\n-  bool wait_for_completed_buffers() override;\n-  void do_refinement_step() override;\n+  \/\/ 2. Java threads\n+  if (!state.swap_java_threads_ct()) {\n+    log_debug(gc, refine)(\"GC pause after Java Thread CT swap\");\n+    return;\n+  }\n+  log_debug(gc,refine)(\"Concurrent Refine Java Thread CT swap\");\n@@ -195,5 +161,4 @@\n-public:\n-  G1SecondaryConcurrentRefineThread(G1ConcurrentRefine* cr, uint worker_id) :\n-    G1ConcurrentRefineThread(cr, worker_id)\n-  {\n-    assert(worker_id > 0, \"precondition\");\n+  \/\/ 3. GC threads\n+  if (!state.swap_gc_threads_ct()) {\n+    log_debug(gc, refine)(\"GC pause after GC Thread CT swap\");\n+    return;\n@@ -201,1 +166,1 @@\n-};\n+  log_debug(gc,refine)(\"Concurrent Refine GC Thread CT swap\");\n@@ -203,5 +168,36 @@\n-bool G1SecondaryConcurrentRefineThread::wait_for_completed_buffers() {\n-  assert(this == Thread::current(), \"precondition\");\n-  MonitorLocker ml(notifier(), Mutex::_no_safepoint_check_flag);\n-  while (!requested_active() && !should_terminate()) {\n-    ml.wait();\n+  G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+  jlong epoch_yield_duration = g1h->yield_duration_in_refinement_epoch();\n+  jlong next_epoch_start = os::elapsed_counter();\n+\n+  jlong synchronize_duration_at_sweep_start = g1h->synchronized_duration();\n+\n+  \/\/ 4. Snapshot heap.\n+  state.snapshot_heap();\n+  log_debug(gc,refine)(\"Concurrent Refine Snapshot Heap\");\n+\n+  \/\/ 5. Sweep refinement table until done\n+  bool interrupted_by_gc = false;\n+\n+  state.sweep_rt_start();\n+  while (true) {\n+    bool completed = state.sweep_rt_step();\n+\n+    if (completed) {\n+      break;\n+    }\n+\n+    if (SuspendibleThreadSet::should_yield()) {\n+      jlong yield_start = os::elapsed_counter();\n+      SuspendibleThreadSet::yield();\n+\n+      \/\/ The yielding may have completed the task, check.\n+      if (!state.is_in_progress()) {\n+        log_debug(gc, refine)(\"GC completed sweeping, aborting concurrent operation\");\n+        interrupted_by_gc = true;\n+        break;\n+      } else {\n+        jlong yield_duration = os::elapsed_counter() - yield_start;\n+        log_debug(gc, refine)(\"Yielded from card table sweeping for %.2fms, no GC inbetween, continue\",\n+                              TimeHelper::counter_to_millis(yield_duration));\n+      }\n+    }\n@@ -209,2 +205,0 @@\n-  return !should_terminate();\n-}\n@@ -212,10 +206,24 @@\n-void G1SecondaryConcurrentRefineThread::do_refinement_step() {\n-  assert(this == Thread::current(), \"precondition\");\n-  \/\/ Secondary threads ignore the target and just drive the number of pending\n-  \/\/ dirty cards down.  The primary thread is responsible for noticing the\n-  \/\/ target has been reached and reducing the number of wanted threads.  This\n-  \/\/ makes the control of wanted threads all under the primary, while avoiding\n-  \/\/ useless spinning by secondary threads until the primary thread notices.\n-  \/\/ (Useless spinning is still possible if there are no pending cards, but\n-  \/\/ that should rarely happen.)\n-  try_refinement_step(0);\n+  if (!interrupted_by_gc) {\n+    state.add_yield_duration(G1CollectedHeap::heap()->synchronized_duration() - synchronize_duration_at_sweep_start);\n+\n+    state.complete(true);\n+\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+    G1Policy* policy = g1h->policy();\n+    G1ConcurrentRefineStats* stats = state.stats();\n+    policy->record_refinement_stats(stats);\n+\n+    {\n+      \/\/ The young gen revising mechanism reads the predictor and the values set\n+      \/\/ here. Avoid inconsistencies by locking.\n+      MutexLocker x(G1RareEvent_lock, Mutex::_no_safepoint_check_flag);\n+      policy->record_dirtying_stats(TimeHelper::counter_to_millis(G1CollectedHeap::heap()->last_refinement_epoch_start()),\n+                                    TimeHelper::counter_to_millis(next_epoch_start),\n+                                    stats->cards_pending(),\n+                                    TimeHelper::counter_to_millis(epoch_yield_duration),\n+                                    0 \/* pending_cards_from_gc *\/,\n+                                    stats->cards_to_cset());\n+      G1CollectedHeap::heap()->set_last_refinement_epoch_start(next_epoch_start, epoch_yield_duration);\n+    }\n+    stats->reset();\n+  }\n@@ -224,5 +232,3 @@\n-G1ConcurrentRefineThread*\n-G1ConcurrentRefineThread::create(G1ConcurrentRefine* cr, uint worker_id) {\n-  G1ConcurrentRefineThread* crt;\n-  if (worker_id == 0) {\n-    crt = new (std::nothrow) G1PrimaryConcurrentRefineThread(cr);\n+void G1ConcurrentRefineThread::track_usage() {\n+  if (os::supports_vtime()) {\n+    _vtime_accum = (os::elapsedVTime() - _vtime_start);\n@@ -230,1 +236,1 @@\n-    crt = new (std::nothrow) G1SecondaryConcurrentRefineThread(cr, worker_id);\n+    _vtime_accum = 0.0;\n@@ -232,0 +238,15 @@\n+  \/\/ The control thread is responsible for updating the CPU time for all workers.\n+  if (UsePerfData && os::is_thread_cpu_time_supported()) {\n+    {\n+      ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_conc_refine);\n+      cr()->worker_threads_do(&tttc);\n+    }\n+    {\n+      ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_conc_refine_control);\n+      cr()->control_thread_do(&tttc);\n+    }\n+  }\n+}\n+\n+G1ConcurrentRefineThread* G1ConcurrentRefineThread::create(G1ConcurrentRefine* cr) {\n+  G1ConcurrentRefineThread* crt = new (std::nothrow) G1ConcurrentRefineThread(cr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThread.cpp","additions":140,"deletions":119,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,2 +48,0 @@\n-  G1ConcurrentRefineStats _refinement_stats;\n-\n@@ -56,2 +54,1 @@\n-protected:\n-  G1ConcurrentRefineThread(G1ConcurrentRefine* cr, uint worker_id);\n+  G1ConcurrentRefineThread(G1ConcurrentRefine* cr);\n@@ -64,1 +61,1 @@\n-  virtual bool wait_for_completed_buffers() = 0;\n+  bool wait_for_work();\n@@ -68,1 +65,1 @@\n-  virtual bool maybe_deactivate();\n+  bool deactivate();\n@@ -72,1 +69,1 @@\n-  virtual void do_refinement_step() = 0;\n+  void do_refinement();\n@@ -75,14 +72,1 @@\n-  \/\/ If we are in Primary thread, we additionally update CPU time tracking.\n-  virtual void track_usage() {\n-    if (os::supports_vtime()) {\n-      _vtime_accum = (os::elapsedVTime() - _vtime_start);\n-    } else {\n-      _vtime_accum = 0.0;\n-    }\n-  };\n-\n-  \/\/ Helper for do_refinement_step implementations.  Try to perform some\n-  \/\/ refinement work, limited by stop_at.  Returns true if any refinement work\n-  \/\/ was performed, false if no work available per stop_at.\n-  \/\/ precondition: this is the current thread.\n-  bool try_refinement_step(size_t stop_at);\n+  void track_usage();\n@@ -91,1 +75,1 @@\n-  void report_inactive(const char* reason, const G1ConcurrentRefineStats& stats) const;\n+  void report_inactive(const char* reason) const;\n@@ -99,4 +83,1 @@\n-  static G1ConcurrentRefineThread* create(G1ConcurrentRefine* cr, uint worker_id);\n-  virtual ~G1ConcurrentRefineThread() = default;\n-\n-  uint worker_id() const { return _worker_id; }\n+  static G1ConcurrentRefineThread* create(G1ConcurrentRefine* cr);\n@@ -108,8 +89,0 @@\n-  G1ConcurrentRefineStats* refinement_stats() {\n-    return &_refinement_stats;\n-  }\n-\n-  const G1ConcurrentRefineStats* refinement_stats() const {\n-    return &_refinement_stats;\n-  }\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThread.hpp","additions":8,"deletions":35,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -48,4 +48,1 @@\n-\/\/ 2. Minimize the number of activations and deactivations for the\n-\/\/ refinement threads that run.\n-\/\/\n-\/\/ 3. Delay performing refinement work.  Having more dirty cards waiting to\n+\/\/ 2. Delay performing refinement work.  Having more dirty cards waiting to\n@@ -79,11 +76,6 @@\n-  \/\/ Estimate number of cards that need to be processed before next GC.  There\n-  \/\/ are no incoming cards when time is short, because in that case the\n-  \/\/ controller activates refinement by mutator threads to stay on target even\n-  \/\/ if threads deactivate in the meantime.  This also covers the case of not\n-  \/\/ having a real prediction of time until GC.\n-  size_t incoming_cards = 0;\n-  if (_predicted_time_until_next_gc_ms > _update_period_ms) {\n-    double incoming_rate = analytics->predict_dirtied_cards_rate_ms();\n-    double raw_cards = incoming_rate * _predicted_time_until_next_gc_ms;\n-    incoming_cards = static_cast<size_t>(raw_cards);\n-  }\n+  \/\/ Estimate number of cards that need to be processed before next GC.\n+\n+  double incoming_rate = analytics->predict_dirtied_cards_rate_ms();\n+  double raw_cards = incoming_rate * _predicted_time_until_next_gc_ms;\n+  size_t incoming_cards = static_cast<size_t>(raw_cards);\n+\n@@ -103,3 +95,2 @@\n-  \/\/ threads running, other than to treat the current thread as running.  That\n-  \/\/ might not be sufficient, but hopefully we were already reasonably close.\n-  \/\/ We won't accumulate more because mutator refinement will be activated.\n+  \/\/ threads needed.  That might not be sufficient, but hopefully we were\n+  \/\/ already reasonably close.\n@@ -136,0 +127,1 @@\n+  double rthreads = nthreads;\n@@ -137,1 +129,1 @@\n-    nthreads = ::ceil(nthreads);\n+    rthreads = ::ceil(nthreads);\n@@ -139,1 +131,1 @@\n-    nthreads = ::round(nthreads);\n+    rthreads = ::round(nthreads);\n@@ -142,1 +134,1 @@\n-  _threads_needed = static_cast<uint>(MIN2<size_t>(nthreads, UINT_MAX));\n+  _threads_needed = static_cast<uint>(MIN2<size_t>(rthreads, UINT_MAX));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineThreadsNeeded.cpp","additions":13,"deletions":21,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -0,0 +1,175 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/g1\/g1CardTableClaimTable.inline.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n+#include \"gc\/g1\/g1ConcurrentRefineWorkTask.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+class G1RefineRegionClosure : public G1HeapRegionClosure {\n+  using CardValue = G1CardTable::CardValue;\n+\n+  G1RemSet* _rem_set;\n+  G1CardTableClaimTable* _scan_state;\n+\n+  uint _worker_id;\n+\n+  size_t _num_collections_at_start;\n+\n+  bool has_work(G1HeapRegion* r) {\n+    return _scan_state->has_unclaimed_cards(r->hrm_index());\n+  }\n+\n+  void do_dirty_card(CardValue* source_card, CardValue* dest_card) {\n+#ifdef ASSERT\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+    G1HeapRegion* refinement_r = g1h->heap_region_containing(g1h->refinement_table()->addr_for(source_card));\n+    G1HeapRegion* card_r = g1h->heap_region_containing(g1h->card_table()->addr_for(dest_card));\n+    size_t refinement_i = g1h->refinement_table()->index_for_cardvalue(source_card);\n+    size_t card_i = g1h->card_table()->index_for_cardvalue(dest_card);\n+\n+    assert(refinement_r == card_r, \"not same region source %u (%zu) dest %u (%zu) \", refinement_r->hrm_index(), refinement_i, card_r->hrm_index(), card_i);\n+    assert(refinement_i == card_i, \"indexes are not same %zu %zu\", refinement_i, card_i);\n+#endif\n+    G1RemSet::RefineResult res = _rem_set->refine_card_concurrently(source_card, _worker_id);\n+    \/\/ Gather statistics.\n+    if (res == G1RemSet::CouldNotParse) {\n+      \/\/ Could not refine - redirty with the original value.\n+      *dest_card = *source_card;\n+      _refine_stats.inc_cards_not_parsable();\n+    } else if (res == G1RemSet::AlreadyToCSet) {\n+      *dest_card = G1CardTable::g1_to_cset_card;\n+      _refine_stats.inc_cards_still_refer_to_cset();\n+    } else if (res == G1RemSet::HasToCSetRef) {\n+      *dest_card = G1CardTable::g1_to_cset_card;\n+      _refine_stats.inc_cards_refer_to_cset();\n+    } else if (res == G1RemSet::NoInteresting) {\n+      _refine_stats.inc_cards_clean_again();\n+    }\n+    \/\/ Clean card on source card table.\n+    *source_card = G1CardTable::clean_card_val();\n+  }\n+\n+  size_t do_claimed_block(CardValue* dirty_l, CardValue* dirty_r, CardValue* dest_card) {\n+    for (CardValue* source = dirty_l; source < dirty_r; ++source, ++dest_card) {\n+      do_dirty_card(source, dest_card);\n+    }\n+    return pointer_delta(dirty_r, dirty_l, sizeof(CardValue));\n+  }\n+\n+public:\n+  bool _completed;\n+  G1ConcurrentRefineStats _refine_stats;\n+\n+  G1RefineRegionClosure(uint worker_id, G1CardTableClaimTable* scan_state) :\n+    G1HeapRegionClosure(),\n+    _rem_set(G1CollectedHeap::heap()->rem_set()),\n+    _scan_state(scan_state),\n+    _worker_id(worker_id),\n+    _completed(true),\n+    _refine_stats() { }\n+\n+  bool do_heap_region(G1HeapRegion* r) override {\n+\n+    if (!has_work(r)) {\n+      return false;\n+    }\n+\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+\n+    if (r->is_young()) {\n+      if (_scan_state->claim_all_cards(r->hrm_index()) == 0) {\n+        \/\/ Clear the pre-dirtying information.\n+        r->clear_refinement_table();\n+      }\n+      return false;\n+    }\n+\n+    G1CardTable* card_table = g1h->card_table();\n+    G1CardTable* refinement_table = g1h->refinement_table();\n+\n+    G1CardTableChunkClaimer claim(_scan_state, r->hrm_index());\n+\n+    size_t const region_card_base_idx = (size_t)r->hrm_index() << G1HeapRegion::LogCardsPerRegion;\n+\n+    while (claim.has_next()) {\n+      size_t const start_idx = region_card_base_idx + claim.value();\n+      CardValue* const start_card = refinement_table->byte_for_index(start_idx);\n+      CardValue* const end_card = start_card + claim.size();\n+\n+      CardValue* dest_card = card_table->byte_for_index(start_idx);\n+\n+      G1ChunkScanner scanner{start_card, end_card};\n+\n+      size_t scanned = 0;\n+      scanner.on_dirty_cards([&] (CardValue* dirty_l, CardValue* dirty_r) {\n+                               jlong refine_start = os::elapsed_counter();\n+                               scanned += do_claimed_block(dirty_l, dirty_r, dest_card + pointer_delta(dirty_l, start_card, sizeof(CardValue)));\n+                               _refine_stats.inc_refine_duration(os::elapsed_counter() - refine_start);\n+                             });\n+\n+      if (VerifyDuringGC) {\n+        for (CardValue* i = start_card; i < end_card; ++i) {\n+          guarantee(*i == G1CardTable::clean_card_val(), \"must be\");\n+        }\n+      }\n+\n+      _refine_stats.inc_cards_scanned(claim.size());\n+      _refine_stats.inc_cards_clean(claim.size() - scanned);\n+\n+      if (SuspendibleThreadSet::should_yield()) {\n+        _completed = false;\n+        break;\n+      }\n+    }\n+\n+    return !_completed;\n+  }\n+};\n+\n+G1ConcurrentRefineWorkTask::G1ConcurrentRefineWorkTask(G1CardTableClaimTable* scan_state,\n+                                                           G1ConcurrentRefineStats* stats,\n+                                                           uint max_workers) :\n+  WorkerTask(\"G1 Refine Task\"),\n+  _scan_state(scan_state),\n+  _stats(stats),\n+  _max_workers(max_workers),\n+  _sweep_completed(true)\n+{ }\n+\n+void G1ConcurrentRefineWorkTask::work(uint worker_id) {\n+  jlong start = os::elapsed_counter();\n+\n+  G1RefineRegionClosure sweep_cl(worker_id, _scan_state);\n+  _scan_state->heap_region_iterate_from_worker_offset(&sweep_cl, worker_id, _max_workers);\n+\n+  if (!sweep_cl._completed) {\n+    _sweep_completed = false;\n+  }\n+\n+  sweep_cl._refine_stats.inc_sweep_time(os::elapsed_counter() - start);\n+  _stats->add_atomic(&sweep_cl._refine_stats);\n+}\n+\n+bool G1ConcurrentRefineWorkTask::sweep_completed() const { return _sweep_completed; }\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineWorkTask.cpp","additions":175,"deletions":0,"binary":false,"changes":175,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_G1_G1CONCURRENTREFINEWORKTASK_HPP\n+#define SHARE_GC_G1_G1CONCURRENTREFINEWORKTASK_HPP\n+\n+#include \"gc\/g1\/g1ConcurrentRefineStats.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+\n+class G1CardTableClaimTable;\n+\n+class G1ConcurrentRefineWorkTask : public WorkerTask {\n+  G1CardTableClaimTable* _scan_state;\n+  G1ConcurrentRefineStats* _stats;\n+  uint _max_workers;\n+  bool _sweep_completed;\n+\n+public:\n+\n+  G1ConcurrentRefineWorkTask(G1CardTableClaimTable* scan_state, G1ConcurrentRefineStats* stats, uint max_workers);\n+\n+  void work(uint worker_id) override;\n+\n+  bool sweep_completed() const;\n+};\n+\n+#endif \/* SHARE_GC_G1_G1CONCURRENTREFINEWORKTASK_HPP *\/\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentRefineWorkTask.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -1,599 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"gc\/g1\/g1BarrierSet.inline.hpp\"\n-#include \"gc\/g1\/g1CardTableEntryClosure.hpp\"\n-#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n-#include \"gc\/g1\/g1ConcurrentRefineStats.hpp\"\n-#include \"gc\/g1\/g1ConcurrentRefineThread.hpp\"\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n-#include \"gc\/g1\/g1FreeIdSet.hpp\"\n-#include \"gc\/g1\/g1HeapRegionRemSet.inline.hpp\"\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n-#include \"gc\/g1\/g1RemSet.hpp\"\n-#include \"gc\/g1\/g1ThreadLocalData.hpp\"\n-#include \"gc\/shared\/bufferNode.hpp\"\n-#include \"gc\/shared\/bufferNodeList.hpp\"\n-#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/mutex.hpp\"\n-#include \"runtime\/mutexLocker.hpp\"\n-#include \"runtime\/os.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"runtime\/threads.hpp\"\n-#include \"runtime\/threadSMR.hpp\"\n-#include \"utilities\/globalCounter.inline.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#include \"utilities\/nonblockingQueue.inline.hpp\"\n-#include \"utilities\/pair.hpp\"\n-#include \"utilities\/quickSort.hpp\"\n-#include \"utilities\/ticks.hpp\"\n-\n-G1DirtyCardQueue::G1DirtyCardQueue(G1DirtyCardQueueSet* qset) :\n-  PtrQueue(qset),\n-  _refinement_stats(new G1ConcurrentRefineStats())\n-{ }\n-\n-G1DirtyCardQueue::~G1DirtyCardQueue() {\n-  delete _refinement_stats;\n-}\n-\n-\/\/ Assumed to be zero by concurrent threads.\n-static uint par_ids_start() { return 0; }\n-\n-G1DirtyCardQueueSet::G1DirtyCardQueueSet(BufferNode::Allocator* allocator) :\n-  PtrQueueSet(allocator),\n-  _num_cards(0),\n-  _mutator_refinement_threshold(SIZE_MAX),\n-  _completed(),\n-  _paused(),\n-  _free_ids(par_ids_start(), num_par_ids()),\n-  _detached_refinement_stats()\n-{}\n-\n-G1DirtyCardQueueSet::~G1DirtyCardQueueSet() {\n-  abandon_completed_buffers();\n-}\n-\n-\/\/ Determines how many mutator threads can process the buffers in parallel.\n-uint G1DirtyCardQueueSet::num_par_ids() {\n-  return (uint)os::initial_active_processor_count();\n-}\n-\n-void G1DirtyCardQueueSet::flush_queue(G1DirtyCardQueue& queue) {\n-  if (queue.buffer() != nullptr) {\n-    G1ConcurrentRefineStats* stats = queue.refinement_stats();\n-    stats->inc_dirtied_cards(queue.size());\n-  }\n-  PtrQueueSet::flush_queue(queue);\n-}\n-\n-void G1DirtyCardQueueSet::enqueue(G1DirtyCardQueue& queue,\n-                                  volatile CardValue* card_ptr) {\n-  CardValue* value = const_cast<CardValue*>(card_ptr);\n-  if (!try_enqueue(queue, value)) {\n-    handle_zero_index(queue);\n-    retry_enqueue(queue, value);\n-  }\n-}\n-\n-void G1DirtyCardQueueSet::handle_zero_index(G1DirtyCardQueue& queue) {\n-  assert(queue.index() == 0, \"precondition\");\n-  BufferNode* old_node = exchange_buffer_with_new(queue);\n-  if (old_node != nullptr) {\n-    assert(old_node->index() == 0, \"invariant\");\n-    G1ConcurrentRefineStats* stats = queue.refinement_stats();\n-    stats->inc_dirtied_cards(old_node->capacity());\n-    handle_completed_buffer(old_node, stats);\n-  }\n-}\n-\n-void G1DirtyCardQueueSet::handle_zero_index_for_thread(Thread* t) {\n-  G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(t);\n-  G1BarrierSet::dirty_card_queue_set().handle_zero_index(queue);\n-}\n-\n-size_t G1DirtyCardQueueSet::num_cards() const {\n-  return Atomic::load(&_num_cards);\n-}\n-\n-void G1DirtyCardQueueSet::enqueue_completed_buffer(BufferNode* cbn) {\n-  assert(cbn != nullptr, \"precondition\");\n-  \/\/ Increment _num_cards before adding to queue, so queue removal doesn't\n-  \/\/ need to deal with _num_cards possibly going negative.\n-  Atomic::add(&_num_cards, cbn->size());\n-  \/\/ Perform push in CS.  The old tail may be popped while the push is\n-  \/\/ observing it (attaching it to the new buffer).  We need to ensure it\n-  \/\/ can't be reused until the push completes, to avoid ABA problems.\n-  GlobalCounter::CriticalSection cs(Thread::current());\n-  _completed.push(*cbn);\n-}\n-\n-\/\/ Thread-safe attempt to remove and return the first buffer from\n-\/\/ the _completed queue, using the NonblockingQueue::try_pop() underneath.\n-\/\/ It has a limitation that it may return null when there are objects\n-\/\/ in the queue if there is a concurrent push\/append operation.\n-BufferNode* G1DirtyCardQueueSet::dequeue_completed_buffer() {\n-  Thread* current_thread = Thread::current();\n-  BufferNode* result = nullptr;\n-  while (true) {\n-    \/\/ Use GlobalCounter critical section to avoid ABA problem.\n-    \/\/ The release of a buffer to its allocator's free list uses\n-    \/\/ GlobalCounter::write_synchronize() to coordinate with this\n-    \/\/ dequeuing operation.\n-    \/\/ We use a CS per iteration, rather than over the whole loop,\n-    \/\/ because we're not guaranteed to make progress. Lingering in\n-    \/\/ one CS could defer releasing buffer to the free list for reuse,\n-    \/\/ leading to excessive allocations.\n-    GlobalCounter::CriticalSection cs(current_thread);\n-    if (_completed.try_pop(&result)) return result;\n-  }\n-}\n-\n-BufferNode* G1DirtyCardQueueSet::get_completed_buffer() {\n-  BufferNode* result = dequeue_completed_buffer();\n-  if (result == nullptr) {         \/\/ Unlikely if no paused buffers.\n-    enqueue_previous_paused_buffers();\n-    result = dequeue_completed_buffer();\n-    if (result == nullptr) return nullptr;\n-  }\n-  Atomic::sub(&_num_cards, result->size());\n-  return result;\n-}\n-\n-#ifdef ASSERT\n-void G1DirtyCardQueueSet::verify_num_cards() const {\n-  size_t actual = 0;\n-  for (BufferNode* cur = _completed.first();\n-       !_completed.is_end(cur);\n-       cur = cur->next()) {\n-    actual += cur->size();\n-  }\n-  assert(actual == Atomic::load(&_num_cards),\n-         \"Num entries in completed buffers should be %zu but are %zu\",\n-         Atomic::load(&_num_cards), actual);\n-}\n-#endif \/\/ ASSERT\n-\n-G1DirtyCardQueueSet::PausedBuffers::PausedList::PausedList() :\n-  _head(nullptr), _tail(nullptr),\n-  _safepoint_id(SafepointSynchronize::safepoint_id())\n-{}\n-\n-#ifdef ASSERT\n-G1DirtyCardQueueSet::PausedBuffers::PausedList::~PausedList() {\n-  assert(Atomic::load(&_head) == nullptr, \"precondition\");\n-  assert(_tail == nullptr, \"precondition\");\n-}\n-#endif \/\/ ASSERT\n-\n-bool G1DirtyCardQueueSet::PausedBuffers::PausedList::is_next() const {\n-  assert_not_at_safepoint();\n-  return _safepoint_id == SafepointSynchronize::safepoint_id();\n-}\n-\n-void G1DirtyCardQueueSet::PausedBuffers::PausedList::add(BufferNode* node) {\n-  assert_not_at_safepoint();\n-  assert(is_next(), \"precondition\");\n-  BufferNode* old_head = Atomic::xchg(&_head, node);\n-  if (old_head == nullptr) {\n-    assert(_tail == nullptr, \"invariant\");\n-    _tail = node;\n-  } else {\n-    node->set_next(old_head);\n-  }\n-}\n-\n-G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::PausedList::take() {\n-  BufferNode* head = Atomic::load(&_head);\n-  BufferNode* tail = _tail;\n-  Atomic::store(&_head, (BufferNode*)nullptr);\n-  _tail = nullptr;\n-  return HeadTail(head, tail);\n-}\n-\n-G1DirtyCardQueueSet::PausedBuffers::PausedBuffers() : _plist(nullptr) {}\n-\n-#ifdef ASSERT\n-G1DirtyCardQueueSet::PausedBuffers::~PausedBuffers() {\n-  assert(Atomic::load(&_plist) == nullptr, \"invariant\");\n-}\n-#endif \/\/ ASSERT\n-\n-void G1DirtyCardQueueSet::PausedBuffers::add(BufferNode* node) {\n-  assert_not_at_safepoint();\n-  PausedList* plist = Atomic::load_acquire(&_plist);\n-  if (plist == nullptr) {\n-    \/\/ Try to install a new next list.\n-    plist = new PausedList();\n-    PausedList* old_plist = Atomic::cmpxchg(&_plist, (PausedList*)nullptr, plist);\n-    if (old_plist != nullptr) {\n-      \/\/ Some other thread installed a new next list.  Use it instead.\n-      delete plist;\n-      plist = old_plist;\n-    }\n-  }\n-  assert(plist->is_next(), \"invariant\");\n-  plist->add(node);\n-}\n-\n-G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_previous() {\n-  assert_not_at_safepoint();\n-  PausedList* previous;\n-  {\n-    \/\/ Deal with plist in a critical section, to prevent it from being\n-    \/\/ deleted out from under us by a concurrent take_previous().\n-    GlobalCounter::CriticalSection cs(Thread::current());\n-    previous = Atomic::load_acquire(&_plist);\n-    if ((previous == nullptr) ||   \/\/ Nothing to take.\n-        previous->is_next() ||  \/\/ Not from a previous safepoint.\n-        \/\/ Some other thread stole it.\n-        (Atomic::cmpxchg(&_plist, previous, (PausedList*)nullptr) != previous)) {\n-      return HeadTail();\n-    }\n-  }\n-  \/\/ We now own previous.\n-  HeadTail result = previous->take();\n-  \/\/ There might be other threads examining previous (in concurrent\n-  \/\/ take_previous()).  Synchronize to wait until any such threads are\n-  \/\/ done with such examination before deleting.\n-  GlobalCounter::write_synchronize();\n-  delete previous;\n-  return result;\n-}\n-\n-G1DirtyCardQueueSet::HeadTail G1DirtyCardQueueSet::PausedBuffers::take_all() {\n-  assert_at_safepoint();\n-  HeadTail result;\n-  PausedList* plist = Atomic::load(&_plist);\n-  if (plist != nullptr) {\n-    Atomic::store(&_plist, (PausedList*)nullptr);\n-    result = plist->take();\n-    delete plist;\n-  }\n-  return result;\n-}\n-\n-void G1DirtyCardQueueSet::record_paused_buffer(BufferNode* node) {\n-  assert_not_at_safepoint();\n-  assert(node->next() == nullptr, \"precondition\");\n-  \/\/ Ensure there aren't any paused buffers from a previous safepoint.\n-  enqueue_previous_paused_buffers();\n-  \/\/ Cards for paused buffers are included in count, to contribute to\n-  \/\/ notification checking after the coming safepoint if it doesn't GC.\n-  \/\/ Note that this means the queue's _num_cards differs from the number\n-  \/\/ of cards in the queued buffers when there are paused buffers.\n-  Atomic::add(&_num_cards, node->size());\n-  _paused.add(node);\n-}\n-\n-void G1DirtyCardQueueSet::enqueue_paused_buffers_aux(const HeadTail& paused) {\n-  if (paused._head != nullptr) {\n-    assert(paused._tail != nullptr, \"invariant\");\n-    \/\/ Cards from paused buffers are already recorded in the queue count.\n-    _completed.append(*paused._head, *paused._tail);\n-  }\n-}\n-\n-void G1DirtyCardQueueSet::enqueue_previous_paused_buffers() {\n-  assert_not_at_safepoint();\n-  enqueue_paused_buffers_aux(_paused.take_previous());\n-}\n-\n-void G1DirtyCardQueueSet::enqueue_all_paused_buffers() {\n-  assert_at_safepoint();\n-  enqueue_paused_buffers_aux(_paused.take_all());\n-}\n-\n-void G1DirtyCardQueueSet::abandon_completed_buffers() {\n-  BufferNodeList list = take_all_completed_buffers();\n-  BufferNode* buffers_to_delete = list._head;\n-  while (buffers_to_delete != nullptr) {\n-    BufferNode* bn = buffers_to_delete;\n-    buffers_to_delete = bn->next();\n-    bn->set_next(nullptr);\n-    deallocate_buffer(bn);\n-  }\n-}\n-\n-\/\/ Merge lists of buffers. The source queue set is emptied as a\n-\/\/ result. The queue sets must share the same allocator.\n-void G1DirtyCardQueueSet::merge_bufferlists(G1RedirtyCardsQueueSet* src) {\n-  assert(allocator() == src->allocator(), \"precondition\");\n-  const BufferNodeList from = src->take_all_completed_buffers();\n-  if (from._head != nullptr) {\n-    Atomic::add(&_num_cards, from._entry_count);\n-    _completed.append(*from._head, *from._tail);\n-  }\n-}\n-\n-BufferNodeList G1DirtyCardQueueSet::take_all_completed_buffers() {\n-  enqueue_all_paused_buffers();\n-  verify_num_cards();\n-  Pair<BufferNode*, BufferNode*> pair = _completed.take_all();\n-  size_t num_cards = Atomic::load(&_num_cards);\n-  Atomic::store(&_num_cards, size_t(0));\n-  return BufferNodeList(pair.first, pair.second, num_cards);\n-}\n-\n-class G1RefineBufferedCards : public StackObj {\n-  BufferNode* const _node;\n-  CardTable::CardValue** const _node_buffer;\n-  const size_t _node_buffer_capacity;\n-  const uint _worker_id;\n-  G1ConcurrentRefineStats* _stats;\n-  G1RemSet* const _g1rs;\n-\n-  static inline ptrdiff_t compare_cards(const CardTable::CardValue* p1,\n-                                        const CardTable::CardValue* p2) {\n-    return p2 - p1;\n-  }\n-\n-  \/\/ Sorts the cards from start_index to _node_buffer_capacity in *decreasing*\n-  \/\/ address order. Tests showed that this order is preferable to not sorting\n-  \/\/ or increasing address order.\n-  void sort_cards(size_t start_index) {\n-    QuickSort::sort(&_node_buffer[start_index],\n-                    _node_buffer_capacity - start_index,\n-                    compare_cards);\n-  }\n-\n-  \/\/ Returns the index to the first clean card in the buffer.\n-  size_t clean_cards() {\n-    const size_t start = _node->index();\n-    assert(start <= _node_buffer_capacity, \"invariant\");\n-\n-    \/\/ Two-fingered compaction algorithm similar to the filtering mechanism in\n-    \/\/ SATBMarkQueue. The main difference is that clean_card_before_refine()\n-    \/\/ could change the buffer element in-place.\n-    \/\/ We don't check for SuspendibleThreadSet::should_yield(), because\n-    \/\/ cleaning and redirtying the cards is fast.\n-    CardTable::CardValue** src = &_node_buffer[start];\n-    CardTable::CardValue** dst = &_node_buffer[_node_buffer_capacity];\n-    assert(src <= dst, \"invariant\");\n-    for ( ; src < dst; ++src) {\n-      \/\/ Search low to high for a card to keep.\n-      if (_g1rs->clean_card_before_refine(src)) {\n-        \/\/ Found keeper.  Search high to low for a card to discard.\n-        while (src < --dst) {\n-          if (!_g1rs->clean_card_before_refine(dst)) {\n-            *dst = *src;         \/\/ Replace discard with keeper.\n-            break;\n-          }\n-        }\n-        \/\/ If discard search failed (src == dst), the outer loop will also end.\n-      }\n-    }\n-\n-    \/\/ dst points to the first retained clean card, or the end of the buffer\n-    \/\/ if all the cards were discarded.\n-    const size_t first_clean = dst - _node_buffer;\n-    assert(first_clean >= start && first_clean <= _node_buffer_capacity, \"invariant\");\n-    \/\/ Discarded cards are considered as refined.\n-    _stats->inc_refined_cards(first_clean - start);\n-    _stats->inc_precleaned_cards(first_clean - start);\n-    return first_clean;\n-  }\n-\n-  bool refine_cleaned_cards(size_t start_index) {\n-    bool result = true;\n-    size_t i = start_index;\n-    for ( ; i < _node_buffer_capacity; ++i) {\n-      if (SuspendibleThreadSet::should_yield()) {\n-        redirty_unrefined_cards(i);\n-        result = false;\n-        break;\n-      }\n-      _g1rs->refine_card_concurrently(_node_buffer[i], _worker_id);\n-    }\n-    _node->set_index(i);\n-    _stats->inc_refined_cards(i - start_index);\n-    return result;\n-  }\n-\n-  void redirty_unrefined_cards(size_t start) {\n-    for ( ; start < _node_buffer_capacity; ++start) {\n-      *_node_buffer[start] = G1CardTable::dirty_card_val();\n-    }\n-  }\n-\n-public:\n-  G1RefineBufferedCards(BufferNode* node,\n-                        uint worker_id,\n-                        G1ConcurrentRefineStats* stats) :\n-    _node(node),\n-    _node_buffer(reinterpret_cast<CardTable::CardValue**>(BufferNode::make_buffer_from_node(node))),\n-    _node_buffer_capacity(node->capacity()),\n-    _worker_id(worker_id),\n-    _stats(stats),\n-    _g1rs(G1CollectedHeap::heap()->rem_set()) {}\n-\n-  bool refine() {\n-    size_t first_clean_index = clean_cards();\n-    if (first_clean_index == _node_buffer_capacity) {\n-      _node->set_index(first_clean_index);\n-      return true;\n-    }\n-    \/\/ This fence serves two purposes. First, the cards must be cleaned\n-    \/\/ before processing the contents. Second, we can't proceed with\n-    \/\/ processing a region until after the read of the region's top in\n-    \/\/ collect_and_clean_cards(), for synchronization with possibly concurrent\n-    \/\/ humongous object allocation (see comment at the StoreStore fence before\n-    \/\/ setting the regions' tops in humongous allocation path).\n-    \/\/ It's okay that reading region's top and reading region's type were racy\n-    \/\/ wrto each other. We need both set, in any order, to proceed.\n-    OrderAccess::fence();\n-    sort_cards(first_clean_index);\n-    return refine_cleaned_cards(first_clean_index);\n-  }\n-};\n-\n-bool G1DirtyCardQueueSet::refine_buffer(BufferNode* node,\n-                                        uint worker_id,\n-                                        G1ConcurrentRefineStats* stats) {\n-  Ticks start_time = Ticks::now();\n-  G1RefineBufferedCards buffered_cards(node, worker_id, stats);\n-  bool result = buffered_cards.refine();\n-  stats->inc_refinement_time(Ticks::now() - start_time);\n-  return result;\n-}\n-\n-void G1DirtyCardQueueSet::handle_refined_buffer(BufferNode* node,\n-                                                bool fully_processed) {\n-  if (fully_processed) {\n-    assert(node->is_empty(), \"Buffer not fully consumed: index: %zu, size: %zu\",\n-           node->index(), node->capacity());\n-    deallocate_buffer(node);\n-  } else {\n-    assert(!node->is_empty(), \"Buffer fully consumed.\");\n-    \/\/ Buffer incompletely processed because there is a pending safepoint.\n-    \/\/ Record partially processed buffer, to be finished later.\n-    record_paused_buffer(node);\n-  }\n-}\n-\n-void G1DirtyCardQueueSet::handle_completed_buffer(BufferNode* new_node,\n-                                                  G1ConcurrentRefineStats* stats) {\n-  enqueue_completed_buffer(new_node);\n-\n-  \/\/ No need for mutator refinement if number of cards is below limit.\n-  if (Atomic::load(&_num_cards) <= Atomic::load(&_mutator_refinement_threshold)) {\n-    return;\n-  }\n-\n-  \/\/ Don't try to process a buffer that will just get immediately paused.\n-  \/\/ When going into a safepoint it's just a waste of effort.\n-  \/\/ When coming out of a safepoint, Java threads may be running before the\n-  \/\/ yield request (for non-Java threads) has been cleared.\n-  if (SuspendibleThreadSet::should_yield()) {\n-    return;\n-  }\n-\n-  \/\/ Only Java threads perform mutator refinement.\n-  if (!Thread::current()->is_Java_thread()) {\n-    return;\n-  }\n-\n-  BufferNode* node = get_completed_buffer();\n-  if (node == nullptr) return;     \/\/ Didn't get a buffer to process.\n-\n-  \/\/ Refine cards in buffer.\n-\n-  uint worker_id = _free_ids.claim_par_id(); \/\/ temporarily claim an id\n-  bool fully_processed = refine_buffer(node, worker_id, stats);\n-  _free_ids.release_par_id(worker_id); \/\/ release the id\n-\n-  \/\/ Deal with buffer after releasing id, to let another thread use id.\n-  handle_refined_buffer(node, fully_processed);\n-}\n-\n-bool G1DirtyCardQueueSet::refine_completed_buffer_concurrently(uint worker_id,\n-                                                               size_t stop_at,\n-                                                               G1ConcurrentRefineStats* stats) {\n-  \/\/ Not enough cards to trigger processing.\n-  if (Atomic::load(&_num_cards) <= stop_at) return false;\n-\n-  BufferNode* node = get_completed_buffer();\n-  if (node == nullptr) return false; \/\/ Didn't get a buffer to process.\n-\n-  bool fully_processed = refine_buffer(node, worker_id, stats);\n-  handle_refined_buffer(node, fully_processed);\n-  return true;\n-}\n-\n-void G1DirtyCardQueueSet::abandon_logs_and_stats() {\n-  assert_at_safepoint();\n-\n-  \/\/ Disable mutator refinement until concurrent refinement decides otherwise.\n-  set_mutator_refinement_threshold(SIZE_MAX);\n-\n-  \/\/ Iterate over all the threads, resetting per-thread queues and stats.\n-  struct AbandonThreadLogClosure : public ThreadClosure {\n-    G1DirtyCardQueueSet& _qset;\n-    AbandonThreadLogClosure(G1DirtyCardQueueSet& qset) : _qset(qset) {}\n-    virtual void do_thread(Thread* t) {\n-      G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(t);\n-      _qset.reset_queue(queue);\n-      queue.refinement_stats()->reset();\n-    }\n-  } closure(*this);\n-  Threads::threads_do(&closure);\n-\n-  enqueue_all_paused_buffers();\n-  abandon_completed_buffers();\n-\n-  \/\/ Reset stats from detached threads.\n-  MutexLocker ml(G1DetachedRefinementStats_lock, Mutex::_no_safepoint_check_flag);\n-  _detached_refinement_stats.reset();\n-}\n-\n-void G1DirtyCardQueueSet::update_refinement_stats(G1ConcurrentRefineStats& stats) {\n-  assert_at_safepoint();\n-\n-  _concatenated_refinement_stats = stats;\n-\n-  enqueue_all_paused_buffers();\n-  verify_num_cards();\n-\n-  \/\/ Collect and reset stats from detached threads.\n-  MutexLocker ml(G1DetachedRefinementStats_lock, Mutex::_no_safepoint_check_flag);\n-  _concatenated_refinement_stats += _detached_refinement_stats;\n-  _detached_refinement_stats.reset();\n-}\n-\n-G1ConcurrentRefineStats G1DirtyCardQueueSet::concatenate_log_and_stats(Thread* thread) {\n-  assert_at_safepoint();\n-\n-  G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(thread);\n-  \/\/ Flush the buffer if non-empty.  Flush before accumulating and\n-  \/\/ resetting stats, since flushing may modify the stats.\n-  if (!queue.is_empty()) {\n-    flush_queue(queue);\n-  }\n-\n-  G1ConcurrentRefineStats result = *queue.refinement_stats();\n-  queue.refinement_stats()->reset();\n-  return result;\n-}\n-\n-G1ConcurrentRefineStats G1DirtyCardQueueSet::concatenated_refinement_stats() const {\n-  assert_at_safepoint();\n-  return _concatenated_refinement_stats;\n-}\n-\n-void G1DirtyCardQueueSet::record_detached_refinement_stats(G1ConcurrentRefineStats* stats) {\n-  MutexLocker ml(G1DetachedRefinementStats_lock, Mutex::_no_safepoint_check_flag);\n-  _detached_refinement_stats += *stats;\n-  stats->reset();\n-}\n-\n-size_t G1DirtyCardQueueSet::mutator_refinement_threshold() const {\n-  return Atomic::load(&_mutator_refinement_threshold);\n-}\n-\n-void G1DirtyCardQueueSet::set_mutator_refinement_threshold(size_t value) {\n-  Atomic::store(&_mutator_refinement_threshold, value);\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.cpp","additions":0,"deletions":599,"binary":false,"changes":599,"status":"deleted"},{"patch":"@@ -1,302 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_G1_G1DIRTYCARDQUEUE_HPP\n-#define SHARE_GC_G1_G1DIRTYCARDQUEUE_HPP\n-\n-#include \"gc\/g1\/g1FreeIdSet.hpp\"\n-#include \"gc\/g1\/g1CardTable.hpp\"\n-#include \"gc\/g1\/g1ConcurrentRefineStats.hpp\"\n-#include \"gc\/shared\/bufferNode.hpp\"\n-#include \"gc\/shared\/bufferNodeList.hpp\"\n-#include \"gc\/shared\/ptrQueue.hpp\"\n-#include \"memory\/allocation.hpp\"\n-#include \"memory\/padded.hpp\"\n-#include \"utilities\/nonblockingQueue.hpp\"\n-\n-class G1PrimaryConcurrentRefineThread;\n-class G1DirtyCardQueueSet;\n-class G1RedirtyCardsQueueSet;\n-class Thread;\n-\n-\/\/ A ptrQueue whose elements are \"oops\", pointers to object heads.\n-class G1DirtyCardQueue: public PtrQueue {\n-  G1ConcurrentRefineStats* _refinement_stats;\n-\n-public:\n-  G1DirtyCardQueue(G1DirtyCardQueueSet* qset);\n-\n-  \/\/ Flush before destroying; queue may be used to capture pending work while\n-  \/\/ doing something else, with auto-flush on completion.\n-  ~G1DirtyCardQueue();\n-\n-  G1ConcurrentRefineStats* refinement_stats() const {\n-    return _refinement_stats;\n-  }\n-\n-  \/\/ Compiler support.\n-  static ByteSize byte_offset_of_index() {\n-    return PtrQueue::byte_offset_of_index<G1DirtyCardQueue>();\n-  }\n-  using PtrQueue::byte_width_of_index;\n-\n-  static ByteSize byte_offset_of_buf() {\n-    return PtrQueue::byte_offset_of_buf<G1DirtyCardQueue>();\n-  }\n-  using PtrQueue::byte_width_of_buf;\n-\n-};\n-\n-class G1DirtyCardQueueSet: public PtrQueueSet {\n-  \/\/ Head and tail of a list of BufferNodes, linked through their next()\n-  \/\/ fields.  Similar to BufferNodeList, but without the _entry_count.\n-  struct HeadTail {\n-    BufferNode* _head;\n-    BufferNode* _tail;\n-    HeadTail() : _head(nullptr), _tail(nullptr) {}\n-    HeadTail(BufferNode* head, BufferNode* tail) : _head(head), _tail(tail) {}\n-  };\n-\n-  \/\/ Concurrent refinement may stop processing in the middle of a buffer if\n-  \/\/ there is a pending safepoint, to avoid long delays to safepoint.  A\n-  \/\/ partially processed buffer needs to be recorded for processing by the\n-  \/\/ safepoint if it's a GC safepoint; otherwise it needs to be recorded for\n-  \/\/ further concurrent refinement work after the safepoint.  But if the\n-  \/\/ buffer was obtained from the completed buffer queue then it can't simply\n-  \/\/ be added back to the queue, as that would introduce a new source of ABA\n-  \/\/ for the queue.\n-  \/\/\n-  \/\/ The PausedBuffer object is used to record such buffers for the upcoming\n-  \/\/ safepoint, and provides access to the buffers recorded for previous\n-  \/\/ safepoints.  Before obtaining a buffer from the completed buffers queue,\n-  \/\/ we first transfer any buffers from previous safepoints to the queue.\n-  \/\/ This is ABA-safe because threads cannot be in the midst of a queue pop\n-  \/\/ across a safepoint.\n-  \/\/\n-  \/\/ The paused buffers are conceptually an extension of the completed buffers\n-  \/\/ queue, and operations which need to deal with all of the queued buffers\n-  \/\/ (such as concatenating or abandoning logs) also need to deal with any\n-  \/\/ paused buffers.  In general, if a safepoint performs a GC then the paused\n-  \/\/ buffers will be processed as part of it, and there won't be any paused\n-  \/\/ buffers after a GC safepoint.\n-  class PausedBuffers {\n-    class PausedList : public CHeapObj<mtGC> {\n-      BufferNode* volatile _head;\n-      BufferNode* _tail;\n-      size_t _safepoint_id;\n-\n-      NONCOPYABLE(PausedList);\n-\n-    public:\n-      PausedList();\n-      DEBUG_ONLY(~PausedList();)\n-\n-      \/\/ Return true if this list was created to hold buffers for the\n-      \/\/ next safepoint.\n-      \/\/ precondition: not at safepoint.\n-      bool is_next() const;\n-\n-      \/\/ Thread-safe add the buffer to the list.\n-      \/\/ precondition: not at safepoint.\n-      \/\/ precondition: is_next().\n-      void add(BufferNode* node);\n-\n-      \/\/ Take all the buffers from the list.  Not thread-safe.\n-      HeadTail take();\n-    };\n-\n-    \/\/ The most recently created list, which might be for either the next or\n-    \/\/ a previous safepoint, or might be null if the next list hasn't been\n-    \/\/ created yet.  We only need one list because of the requirement that\n-    \/\/ threads calling add() must first ensure there are no paused buffers\n-    \/\/ from a previous safepoint.  There might be many list instances existing\n-    \/\/ at the same time though; there can be many threads competing to create\n-    \/\/ and install the next list, and meanwhile there can be a thread dealing\n-    \/\/ with the previous list.\n-    PausedList* volatile _plist;\n-    DEFINE_PAD_MINUS_SIZE(1, DEFAULT_PADDING_SIZE, sizeof(PausedList*));\n-\n-    NONCOPYABLE(PausedBuffers);\n-\n-  public:\n-    PausedBuffers();\n-    DEBUG_ONLY(~PausedBuffers();)\n-\n-    \/\/ Thread-safe add the buffer to paused list for next safepoint.\n-    \/\/ precondition: not at safepoint.\n-    \/\/ precondition: does not have paused buffers from a previous safepoint.\n-    void add(BufferNode* node);\n-\n-    \/\/ Thread-safe take all paused buffers for previous safepoints.\n-    \/\/ precondition: not at safepoint.\n-    HeadTail take_previous();\n-\n-    \/\/ Take all the paused buffers.\n-    \/\/ precondition: at safepoint.\n-    HeadTail take_all();\n-  };\n-\n-  DEFINE_PAD_MINUS_SIZE(0, DEFAULT_PADDING_SIZE, 0);\n-  \/\/ Upper bound on the number of cards in the completed and paused buffers.\n-  volatile size_t _num_cards;\n-  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_PADDING_SIZE, sizeof(size_t));\n-  \/\/ If the queue contains more cards than configured here, the\n-  \/\/ mutator must start doing some of the concurrent refinement work.\n-  volatile size_t _mutator_refinement_threshold;\n-  DEFINE_PAD_MINUS_SIZE(2, DEFAULT_PADDING_SIZE, sizeof(size_t));\n-  \/\/ Buffers ready for refinement.\n-  \/\/ NonblockingQueue has inner padding of one cache line.\n-  NonblockingQueue<BufferNode, &BufferNode::next_ptr> _completed;\n-  \/\/ Add a trailer padding after NonblockingQueue.\n-  DEFINE_PAD_MINUS_SIZE(3, DEFAULT_PADDING_SIZE, sizeof(BufferNode*));\n-  \/\/ Buffers for which refinement is temporarily paused.\n-  \/\/ PausedBuffers has inner padding, including trailer.\n-  PausedBuffers _paused;\n-\n-  G1FreeIdSet _free_ids;\n-\n-  G1ConcurrentRefineStats _concatenated_refinement_stats;\n-  G1ConcurrentRefineStats _detached_refinement_stats;\n-\n-  \/\/ Verify _num_cards == sum of cards in the completed queue.\n-  void verify_num_cards() const NOT_DEBUG_RETURN;\n-\n-  \/\/ Thread-safe add a buffer to paused list for next safepoint.\n-  \/\/ precondition: not at safepoint.\n-  void record_paused_buffer(BufferNode* node);\n-  void enqueue_paused_buffers_aux(const HeadTail& paused);\n-  \/\/ Thread-safe transfer paused buffers for previous safepoints to the queue.\n-  \/\/ precondition: not at safepoint.\n-  void enqueue_previous_paused_buffers();\n-  \/\/ Transfer all paused buffers to the queue.\n-  \/\/ precondition: at safepoint.\n-  void enqueue_all_paused_buffers();\n-\n-  void abandon_completed_buffers();\n-\n-  \/\/ Refine the cards in \"node\" from its index to buffer_capacity.\n-  \/\/ Stops processing if SuspendibleThreadSet::should_yield() is true.\n-  \/\/ Returns true if the entire buffer was processed, false if there\n-  \/\/ is a pending yield request.  The node's index is updated to exclude\n-  \/\/ the processed elements, e.g. up to the element before processing\n-  \/\/ stopped, or one past the last element if the entire buffer was\n-  \/\/ processed. Updates stats.\n-  bool refine_buffer(BufferNode* node,\n-                     uint worker_id,\n-                     G1ConcurrentRefineStats* stats);\n-\n-  \/\/ Deal with buffer after a call to refine_buffer.  If fully processed,\n-  \/\/ deallocate the buffer.  Otherwise, record it as paused.\n-  void handle_refined_buffer(BufferNode* node, bool fully_processed);\n-\n-  \/\/ Thread-safe attempt to remove and return the first buffer from\n-  \/\/ the _completed queue.\n-  \/\/ Returns null if the queue is empty, or if a concurrent push\/append\n-  \/\/ interferes. It uses GlobalCounter critical section to avoid ABA problem.\n-  BufferNode* dequeue_completed_buffer();\n-  \/\/ Remove and return a completed buffer from the list, or return null\n-  \/\/ if none available.\n-  BufferNode* get_completed_buffer();\n-\n-  \/\/ Called when queue is full or has no buffer.\n-  void handle_zero_index(G1DirtyCardQueue& queue);\n-\n-  \/\/ Enqueue the buffer, and optionally perform refinement by the mutator.\n-  \/\/ Mutator refinement is only done by Java threads, and only if there\n-  \/\/ are more than mutator_refinement_threshold cards in the completed buffers.\n-  \/\/ Updates stats.\n-  \/\/\n-  \/\/ Mutator refinement, if performed, stops processing a buffer if\n-  \/\/ SuspendibleThreadSet::should_yield(), recording the incompletely\n-  \/\/ processed buffer for later processing of the remainder.\n-  void handle_completed_buffer(BufferNode* node, G1ConcurrentRefineStats* stats);\n-\n-public:\n-  G1DirtyCardQueueSet(BufferNode::Allocator* allocator);\n-  ~G1DirtyCardQueueSet();\n-\n-  \/\/ The number of parallel ids that can be claimed to allow collector or\n-  \/\/ mutator threads to do card-processing work.\n-  static uint num_par_ids();\n-\n-  static void handle_zero_index_for_thread(Thread* t);\n-\n-  virtual void enqueue_completed_buffer(BufferNode* node);\n-\n-  \/\/ Upper bound on the number of cards currently in this queue set.\n-  \/\/ Read without synchronization.  The value may be high because there\n-  \/\/ is a concurrent modification of the set of buffers.\n-  size_t num_cards() const;\n-\n-  void merge_bufferlists(G1RedirtyCardsQueueSet* src);\n-\n-  BufferNodeList take_all_completed_buffers();\n-\n-  void flush_queue(G1DirtyCardQueue& queue);\n-\n-  using CardValue = G1CardTable::CardValue;\n-  void enqueue(G1DirtyCardQueue& queue, volatile CardValue* card_ptr);\n-\n-  \/\/ If there are more than stop_at cards in the completed buffers, pop\n-  \/\/ a buffer, refine its contents, and return true.  Otherwise return\n-  \/\/ false.  Updates stats.\n-  \/\/\n-  \/\/ Stops processing a buffer if SuspendibleThreadSet::should_yield(),\n-  \/\/ recording the incompletely processed buffer for later processing of\n-  \/\/ the remainder.\n-  bool refine_completed_buffer_concurrently(uint worker_id,\n-                                            size_t stop_at,\n-                                            G1ConcurrentRefineStats* stats);\n-\n-  \/\/ If a full collection is happening, reset per-thread refinement stats and\n-  \/\/ partial logs, and release completed logs. The full collection will make\n-  \/\/ them all irrelevant.\n-  \/\/ precondition: at safepoint.\n-  void abandon_logs_and_stats();\n-\n-  \/\/ Update global refinement statistics with the ones given and the ones from\n-  \/\/ detached threads.\n-  \/\/ precondition: at safepoint.\n-  void update_refinement_stats(G1ConcurrentRefineStats& stats);\n-  \/\/ Add the given thread's partial logs to the global list and return and reset\n-  \/\/ its refinement stats.\n-  \/\/ precondition: at safepoint.\n-  G1ConcurrentRefineStats concatenate_log_and_stats(Thread* thread);\n-\n-  \/\/ Return the total of mutator refinement stats for all threads.\n-  \/\/ precondition: at safepoint.\n-  \/\/ precondition: only call after concatenate_logs_and_stats.\n-  G1ConcurrentRefineStats concatenated_refinement_stats() const;\n-\n-  \/\/ Accumulate refinement stats from threads that are detaching.\n-  void record_detached_refinement_stats(G1ConcurrentRefineStats* stats);\n-\n-  \/\/ Number of cards above which mutator threads should do refinement.\n-  size_t mutator_refinement_threshold() const;\n-\n-  \/\/ Set number of cards above which mutator threads should do refinement.\n-  void set_mutator_refinement_threshold(size_t value);\n-};\n-\n-#endif \/\/ SHARE_GC_G1_G1DIRTYCARDQUEUE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1DirtyCardQueue.hpp","additions":0,"deletions":302,"binary":false,"changes":302,"status":"deleted"},{"patch":"@@ -25,2 +25,0 @@\n-#include \"gc\/g1\/g1ConcurrentRefine.hpp\"\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -83,1 +81,1 @@\n-  return G1DirtyCardQueueSet::num_par_ids() + G1ConcRefinementThreads + MAX2(ConcGCThreads, ParallelGCThreads);\n+  return G1ConcRefinementThreads + ConcGCThreads;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FromCardCache.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -150,0 +150,5 @@\n+    if (VerifyDuringGC) {\n+      \/\/ Satisfy some asserts in free_..._region\n+      hr->clear_cardtable();\n+      hr->clear_refinement_table();\n+    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -38,0 +38,5 @@\n+  if (VerifyDuringGC) {\n+    \/\/ Satisfy some asserts in free_..._region.\n+    hr->clear_cardtable();\n+    hr->clear_refinement_table();\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+  hr->clear_refinement_table();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCResetMetadataTask.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,2 +53,1 @@\n-  _gc_par_phases[RetireTLABsAndFlushLogs] = new WorkerDataArray<double>(\"RetireTLABsAndFlushLogs\", \"JT Retire TLABs And Flush Logs (ms):\", max_gc_threads);\n-  _gc_par_phases[NonJavaThreadFlushLogs] = new WorkerDataArray<double>(\"NonJavaThreadFlushLogs\", \"Non-JT Flush Logs (ms):\", max_gc_threads);\n+  _gc_par_phases[RetireTLABs] = new WorkerDataArray<double>(\"RetireTLABs\", \"JavaThread Retire TLABs (ms):\", max_gc_threads);\n@@ -86,1 +85,1 @@\n-  _gc_par_phases[MergeLB] = new WorkerDataArray<double>(\"MergeLB\", \"Log Buffers (ms):\", max_gc_threads);\n+  _gc_par_phases[SweepRT] = new WorkerDataArray<double>(\"SweepRT\", \"Sweep (ms):\", max_gc_threads);\n@@ -110,0 +109,2 @@\n+  _gc_par_phases[ScanHR]->create_thread_work_items(\"Pending Cards:\", ScanHRPendingCards);\n+  _gc_par_phases[ScanHR]->create_thread_work_items(\"Scanned Empty:\", ScanHRScannedEmptyCards);\n@@ -115,0 +116,2 @@\n+  _gc_par_phases[OptScanHR]->create_thread_work_items(\"Pending Cards:\", ScanHRPendingCards);\n+  _gc_par_phases[OptScanHR]->create_thread_work_items(\"Scanned Empty:\", ScanHRScannedEmptyCards);\n@@ -122,3 +125,0 @@\n-  _gc_par_phases[MergeLB]->create_thread_work_items(\"Dirty Cards:\", MergeLBDirtyCards);\n-  _gc_par_phases[MergeLB]->create_thread_work_items(\"Skipped Cards:\", MergeLBSkippedCards);\n-\n@@ -132,1 +132,4 @@\n-  _gc_par_phases[MergePSS]->create_thread_work_items(\"Evac Fail Extra Cards:\", MergePSSEvacFailExtra);\n+  _gc_par_phases[MergePSS]->create_thread_work_items(\"Pending Cards:\", MergePSSPendingCards);\n+  _gc_par_phases[MergePSS]->create_thread_work_items(\"To-Young-Gen Cards:\", MergePSSToYoungGenCards);\n+  _gc_par_phases[MergePSS]->create_thread_work_items(\"Evac-Fail Cards:\", MergePSSEvacFail);\n+  _gc_par_phases[MergePSS]->create_thread_work_items(\"Marked Cards:\", MergePSSMarked);\n@@ -153,3 +156,0 @@\n-  _gc_par_phases[RedirtyCards] = new WorkerDataArray<double>(\"RedirtyCards\", \"Redirty Logged Cards (ms):\", max_gc_threads);\n-  _gc_par_phases[RedirtyCards]->create_thread_work_items(\"Redirtied Cards:\");\n-\n@@ -174,0 +174,1 @@\n+  _cur_merge_refinement_table_time_ms = 0.0;\n@@ -176,1 +177,0 @@\n-  _cur_distribute_log_buffers_time_ms = 0.0;\n@@ -252,1 +252,1 @@\n-      ASSERT_PHASE_UNINITIALIZED(MergeLB);\n+      ASSERT_PHASE_UNINITIALIZED(SweepRT);\n@@ -429,2 +429,1 @@\n-  debug_phase(_gc_par_phases[RetireTLABsAndFlushLogs], 1);\n-  debug_phase(_gc_par_phases[NonJavaThreadFlushLogs], 1);\n+  debug_phase(_gc_par_phases[RetireTLABs], 1);\n@@ -462,2 +461,2 @@\n-  debug_time(\"Distribute Log Buffers\", _cur_distribute_log_buffers_time_ms);\n-  debug_phase(_gc_par_phases[MergeLB]);\n+  debug_time(\"Merge Refinement Table\", _cur_merge_refinement_table_time_ms);\n+  debug_phase(_gc_par_phases[SweepRT], 1);\n@@ -525,1 +524,0 @@\n-  debug_phase(_gc_par_phases[RedirtyCards], 1);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.cpp","additions":15,"deletions":17,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,2 +49,1 @@\n-    RetireTLABsAndFlushLogs,\n-    NonJavaThreadFlushLogs,\n+    RetireTLABs,\n@@ -62,1 +61,1 @@\n-    MergeLB,\n+    SweepRT,\n@@ -74,1 +73,0 @@\n-    RedirtyCards,\n@@ -114,1 +112,2 @@\n-    MergeRSCards,\n+    MergeRSFromRemSetCards,\n+    MergeRSTotalCards,\n@@ -121,1 +120,1 @@\n-      \"Merged Cards:\" };\n+      \"Merged From RS Cards:\", \"Total Cards:\" };\n@@ -124,0 +123,2 @@\n+    ScanHRPendingCards,\n+    ScanHRScannedEmptyCards,\n@@ -132,5 +133,0 @@\n-  enum GCMergeLBWorkItems {\n-    MergeLBDirtyCards,\n-    MergeLBSkippedCards\n-  };\n-\n@@ -146,1 +142,4 @@\n-    MergePSSEvacFailExtra\n+    MergePSSPendingCards,      \/\/ To be scanned cards generated by GC (from cross-references and evacuation failure).\n+    MergePSSToYoungGenCards,   \/\/ To-young-gen cards generated by GC.\n+    MergePSSEvacFail,          \/\/ Evacuation failure generated dirty cards by GC.\n+    MergePSSMarked,            \/\/ Total newly marked cards.\n@@ -179,0 +178,2 @@\n+  \/\/ Merge refinement table time. Note that this time is included in _cur_merge_heap_roots_time_ms.\n+  double _cur_merge_refinement_table_time_ms;\n@@ -301,0 +302,4 @@\n+  void record_merge_refinement_table_time(double ms) {\n+    _cur_merge_refinement_table_time_ms = ms;\n+  }\n+\n@@ -309,4 +314,0 @@\n-  void record_distribute_log_buffers_time_ms(double ms) {\n-    _cur_distribute_log_buffers_time_ms += ms;\n-  }\n-\n@@ -385,4 +386,0 @@\n-  double cur_distribute_log_buffers_time_ms() {\n-    return _cur_distribute_log_buffers_time_ms;\n-  }\n-\n@@ -396,0 +393,4 @@\n+  double cur_merge_refinement_table_time() const {\n+    return _cur_merge_refinement_table_time_ms;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1GCPhaseTimes.hpp","additions":22,"deletions":21,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"memory\/memRegion.hpp\"\n@@ -140,0 +141,5 @@\n+void G1HeapRegion::clear_refinement_table() {\n+  G1CardTable* ct = G1CollectedHeap::heap()->refinement_table();\n+  ct->clear_MemRegion(MemRegion(bottom(), end()));\n+}\n+\n@@ -587,1 +593,2 @@\n-    CardValue _cv_obj;\n+\n+    CardValue _cv_obj;    \/\/ In card table\n@@ -590,0 +597,3 @@\n+    CardValue _cv_obj2;   \/\/ In refinement card table\n+    CardValue _cv_field2;\n+\n@@ -598,0 +608,4 @@\n+\n+      ct = this->_g1h->refinement_table();\n+      _cv_obj2 = *ct->byte_for_const(this->_containing_obj);\n+      _cv_field2 = *ct->byte_for_const(p);\n@@ -604,1 +618,1 @@\n-        const CardValue dirty = G1CardTable::dirty_card_val();\n+        const CardValue clean = G1CardTable::clean_card_val();\n@@ -607,2 +621,2 @@\n-                  _cv_field == dirty :\n-                  _cv_obj == dirty || _cv_field == dirty));\n+                  (_cv_field != clean || _cv_field2 != clean) :\n+                  (_cv_obj != clean || _cv_field != clean || _cv_obj2 != clean || _cv_field2 != clean)));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.cpp","additions":18,"deletions":4,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,0 +41,1 @@\n+class G1CardTable;\n@@ -475,0 +476,1 @@\n+  void clear_refinement_table();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegion.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -66,1 +66,2 @@\n-  _cardtable_mapper(nullptr),\n+  _card_table_mapper(nullptr),\n+  _refinement_table_mapper(nullptr),\n@@ -77,1 +78,2 @@\n-                                     G1RegionToSpaceMapper* cardtable) {\n+                                     G1RegionToSpaceMapper* card_table,\n+                                     G1RegionToSpaceMapper* refinement_table) {\n@@ -85,1 +87,2 @@\n-  _cardtable_mapper = cardtable;\n+  _card_table_mapper = card_table;\n+  _refinement_table_mapper = refinement_table;\n@@ -189,1 +192,2 @@\n-  _cardtable_mapper->commit_regions(index, num_regions, pretouch_workers);\n+  _card_table_mapper->commit_regions(index, num_regions, pretouch_workers);\n+  _refinement_table_mapper->commit_regions(index, num_regions, pretouch_workers);\n@@ -212,1 +216,2 @@\n-  _cardtable_mapper->uncommit_regions(start, num_regions);\n+  _card_table_mapper->uncommit_regions(start, num_regions);\n+  _refinement_table_mapper->uncommit_regions(start, num_regions);\n@@ -264,1 +269,3 @@\n-  _cardtable_mapper->signal_mapping_changed(start, num_regions);\n+  _card_table_mapper->signal_mapping_changed(start, num_regions);\n+  \/\/ Signal refinement table to clear the given regions.\n+  _refinement_table_mapper->signal_mapping_changed(start, num_regions);\n@@ -271,1 +278,2 @@\n-    _cardtable_mapper->committed_size();\n+    _card_table_mapper->committed_size() +\n+    _refinement_table_mapper->committed_size();\n@@ -276,1 +284,2 @@\n-    _cardtable_mapper->reserved_size();\n+    _card_table_mapper->reserved_size() +\n+    _refinement_table_mapper->reserved_size();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegionManager.cpp","additions":17,"deletions":8,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,1 +78,2 @@\n-  G1RegionToSpaceMapper* _cardtable_mapper;\n+  G1RegionToSpaceMapper* _card_table_mapper;\n+  G1RegionToSpaceMapper* _refinement_table_mapper;\n@@ -165,1 +166,2 @@\n-                  G1RegionToSpaceMapper* cardtable);\n+                  G1RegionToSpaceMapper* card_table,\n+                  G1RegionToSpaceMapper* refinement_table);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapRegionManager.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -558,1 +558,0 @@\n-#ifndef PRODUCT\n@@ -565,0 +564,1 @@\n+    _verifier->verify_ct_clean_region(r);\n@@ -566,1 +566,1 @@\n-      _verifier->verify_dirty_region(r);\n+      _verifier->verify_rt_clean_region(r);\n@@ -568,1 +568,1 @@\n-      _verifier->verify_not_dirty_region(r);\n+      _verifier->verify_rt_clean_from_top(r);\n@@ -581,5 +581,24 @@\n-void G1HeapVerifier::verify_not_dirty_region(G1HeapRegion* hr) {\n-  \/\/ All of the region should be clean.\n-  G1CardTable* ct = _g1h->card_table();\n-  MemRegion mr(hr->bottom(), hr->end());\n-  ct->verify_not_dirty_region(mr);\n+class G1VerifyRefinementTableClean: public G1HeapRegionClosure {\n+  G1HeapVerifier* _verifier;\n+\n+public:\n+  G1VerifyRefinementTableClean(G1HeapVerifier* verifier)\n+    : _verifier(verifier) { }\n+\n+  virtual bool do_heap_region(G1HeapRegion* r) {\n+    G1CardTable* ct = G1CollectedHeap::heap()->refinement_table();\n+    MemRegion mr(r->bottom(), r->end());\n+    ct->verify_region(mr, G1CardTable::clean_card_val(), true); \/\/ Must be all Clean from bottom -> end.\n+    return false;\n+  }\n+};\n+\n+void G1HeapVerifier::verify_refinement_table_clean() {\n+  G1VerifyRefinementTableClean cl(this);\n+  _g1h->heap_region_iterate(&cl);\n+}\n+\n+void G1HeapVerifier::verify_rt_clean_from_top(G1HeapRegion* hr) {\n+  G1CardTable* ct = _g1h->refinement_table();\n+  MemRegion mr(align_up(hr->top(), G1CardTable::card_size()), hr->end());\n+  ct->verify_region(mr, G1CardTable::clean_card_val(), true);\n@@ -588,1 +607,1 @@\n-void G1HeapVerifier::verify_dirty_region(G1HeapRegion* hr) {\n+void G1HeapVerifier::verify_rt_dirty_to_dummy_top(G1HeapRegion* hr) {\n@@ -596,1 +615,1 @@\n-  G1CardTable* ct = _g1h->card_table();\n+  G1CardTable* ct = _g1h->refinement_table();\n@@ -598,5 +617,1 @@\n-  if (hr->is_young()) {\n-    ct->verify_g1_young_region(mr);\n-  } else {\n-    ct->verify_dirty_region(mr);\n-  }\n+  ct->verify_dirty_region(mr);\n@@ -605,10 +620,5 @@\n-class G1VerifyDirtyYoungListClosure : public G1HeapRegionClosure {\n-private:\n-  G1HeapVerifier* _verifier;\n-public:\n-  G1VerifyDirtyYoungListClosure(G1HeapVerifier* verifier) : G1HeapRegionClosure(), _verifier(verifier) { }\n-  virtual bool do_heap_region(G1HeapRegion* r) {\n-    _verifier->verify_dirty_region(r);\n-    return false;\n-  }\n-};\n+void G1HeapVerifier::verify_ct_clean_region(G1HeapRegion* hr) {\n+  G1CardTable* ct = _g1h->card_table();\n+  MemRegion mr(hr->bottom(), hr->end());\n+  ct->verify_region(mr, G1CardTable::clean_card_val(), true);\n+}\n@@ -616,3 +626,4 @@\n-void G1HeapVerifier::verify_dirty_young_regions() {\n-  G1VerifyDirtyYoungListClosure cl(this);\n-  _g1h->collection_set()->iterate(&cl);\n+void G1HeapVerifier::verify_rt_clean_region(G1HeapRegion* hr) {\n+  G1CardTable* ct = _g1h->refinement_table();\n+  MemRegion mr(hr->bottom(), hr->end());\n+  ct->verify_region(mr, G1CardTable::clean_card_val(), true);\n@@ -621,0 +632,1 @@\n+#ifndef PRODUCT\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.cpp","additions":40,"deletions":28,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,2 @@\n-  void verify_card_table_cleanup() PRODUCT_RETURN;\n+  void verify_card_table_cleanup();\n+  void verify_refinement_table_clean();\n@@ -83,3 +84,4 @@\n-  void verify_not_dirty_region(G1HeapRegion* hr) PRODUCT_RETURN;\n-  void verify_dirty_region(G1HeapRegion* hr) PRODUCT_RETURN;\n-  void verify_dirty_young_regions() PRODUCT_RETURN;\n+  void verify_ct_clean_region(G1HeapRegion* hr);\n+  void verify_rt_dirty_to_dummy_top(G1HeapRegion* hr);\n+  void verify_rt_clean_from_top(G1HeapRegion* hr);\n+  void verify_rt_clean_region(G1HeapRegion* hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1HeapVerifier.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,1 +89,1 @@\n-  friend class G1SkipCardEnqueueSetter;\n+  friend class G1SkipCardMarkSetter;\n@@ -91,1 +91,1 @@\n-  enum SkipCardEnqueueTristate {\n+  enum SkipCardMarkTristate {\n@@ -97,1 +97,1 @@\n-  SkipCardEnqueueTristate _skip_card_enqueue;\n+  SkipCardMarkTristate _skip_card_mark;\n@@ -101,1 +101,1 @@\n-    G1ScanClosureBase(g1h, par_scan_state), _skip_card_enqueue(Uninitialized) { }\n+    G1ScanClosureBase(g1h, par_scan_state), _skip_card_mark(Uninitialized) { }\n@@ -112,2 +112,2 @@\n-\/\/ RAII object to properly set the _skip_card_enqueue field in G1ScanEvacuatedObjClosure.\n-class G1SkipCardEnqueueSetter : public StackObj {\n+\/\/ RAII object to properly set the _skip_card_mark field in G1ScanEvacuatedObjClosure.\n+class G1SkipCardMarkSetter : public StackObj {\n@@ -117,3 +117,3 @@\n-  G1SkipCardEnqueueSetter(G1ScanEvacuatedObjClosure* closure, bool skip_card_enqueue) : _closure(closure) {\n-    assert(_closure->_skip_card_enqueue == G1ScanEvacuatedObjClosure::Uninitialized, \"Must not be set\");\n-    _closure->_skip_card_enqueue = skip_card_enqueue ? G1ScanEvacuatedObjClosure::True : G1ScanEvacuatedObjClosure::False;\n+  G1SkipCardMarkSetter(G1ScanEvacuatedObjClosure* closure, bool skip_card_mark) : _closure(closure) {\n+    assert(_closure->_skip_card_mark == G1ScanEvacuatedObjClosure::Uninitialized, \"Must not be set\");\n+    _closure->_skip_card_mark = skip_card_mark ? G1ScanEvacuatedObjClosure::True : G1ScanEvacuatedObjClosure::False;\n@@ -122,2 +122,2 @@\n-  ~G1SkipCardEnqueueSetter() {\n-    DEBUG_ONLY(_closure->_skip_card_enqueue = G1ScanEvacuatedObjClosure::Uninitialized;)\n+  ~G1SkipCardMarkSetter() {\n+    DEBUG_ONLY(_closure->_skip_card_mark = G1ScanEvacuatedObjClosure::Uninitialized;)\n@@ -205,0 +205,2 @@\n+  bool _has_to_cset_ref;\n+  bool _has_to_old_ref;\n@@ -209,1 +211,3 @@\n-    _worker_id(worker_id) {\n+    _worker_id(worker_id),\n+    _has_to_cset_ref(false),\n+    _has_to_old_ref(false) {\n@@ -212,0 +216,3 @@\n+  bool has_to_cset_ref() const { return _has_to_cset_ref; }\n+  bool has_to_old_ref() const { return _has_to_old_ref; }\n+\n@@ -222,0 +229,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.hpp","additions":21,"deletions":13,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -93,2 +93,2 @@\n-    assert(_skip_card_enqueue != Uninitialized, \"Scan location has not been initialized.\");\n-    if (_skip_card_enqueue == True) {\n+    assert(_skip_card_mark != Uninitialized, \"Scan location has not been initialized.\");\n+    if (_skip_card_mark == True) {\n@@ -97,1 +97,1 @@\n-    _par_scan_state->enqueue_card_if_tracked(region_attr, p, obj);\n+    _par_scan_state->mark_card_if_tracked(region_attr, p, obj);\n@@ -130,0 +130,5 @@\n+  \/\/ Early out if we already found a to-young reference.\n+  if (_has_to_cset_ref) {\n+    return;\n+  }\n+\n@@ -149,1 +154,6 @@\n-  G1HeapRegionRemSet* to_rem_set = _g1h->heap_region_containing(obj)->rem_set();\n+  G1HeapRegion* to_region = _g1h->heap_region_containing(obj);\n+  _has_to_cset_ref = to_region->is_young();\n+  if (_has_to_cset_ref) {\n+    return;\n+  }\n+  G1HeapRegionRemSet* to_rem_set = to_region->rem_set();\n@@ -157,0 +167,1 @@\n+      _has_to_old_ref = true;\n@@ -183,1 +194,1 @@\n-    _par_scan_state->enqueue_card_if_tracked(region_attr, p, obj);\n+    _par_scan_state->mark_card_if_tracked(region_attr, p, obj);\n@@ -275,1 +286,4 @@\n-    G1HeapRegion* from = _g1h->heap_region_containing(p);\n+    if (to->is_young()) {\n+      G1BarrierSet::g1_barrier_set()->write_ref_field_post(p);\n+    } else {\n+      G1HeapRegion* from = _g1h->heap_region_containing(p);\n@@ -277,2 +291,3 @@\n-    if (from->rem_set()->cset_group() != rem_set->cset_group()) {\n-      rem_set->add_reference(p, _worker_id);\n+      if (from->rem_set()->cset_group() != rem_set->cset_group()) {\n+        rem_set->add_reference(p, _worker_id);\n+      }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-                                           G1RedirtyCardsQueueSet* rdcqs,\n@@ -67,2 +66,1 @@\n-    _rdc_local_qset(rdcqs),\n-    _ct(g1h->card_table()),\n+    _ct(g1h->refinement_table()),\n@@ -75,1 +73,2 @@\n-    _last_enqueued_card(SIZE_MAX),\n+    _num_marked_as_dirty_cards(0),\n+    _num_marked_as_into_cset_cards(0),\n@@ -91,1 +90,1 @@\n-    _evac_failure_enqueued_cards(0)\n+    _evac_failure_marked_cards(0)\n@@ -115,2 +114,1 @@\n-size_t G1ParScanThreadState::flush_stats(size_t* surviving_young_words, uint num_workers, BufferNodeList* rdc_buffers) {\n-  *rdc_buffers = _rdc_local_qset.flush();\n+size_t G1ParScanThreadState::flush_stats(size_t* surviving_young_words, uint num_workers) {\n@@ -150,2 +148,10 @@\n-size_t G1ParScanThreadState::evac_failure_enqueued_cards() const {\n-  return _evac_failure_enqueued_cards;\n+size_t G1ParScanThreadState::num_pending_cards() const {\n+  return _num_marked_as_dirty_cards + _evac_failure_marked_cards;\n+}\n+\n+size_t G1ParScanThreadState::num_marked_cards() const {\n+  return num_pending_cards() + _num_marked_as_into_cset_cards;\n+}\n+\n+size_t G1ParScanThreadState::evac_failure_marked_cards() const {\n+  return _evac_failure_marked_cards;\n@@ -233,1 +239,1 @@\n-  G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_new_survivor());\n+  G1SkipCardMarkSetter x(&_scanner, dest_attr.is_new_survivor());\n@@ -254,1 +260,1 @@\n-  \/\/ Skip the card enqueue iff the object (to_array) is in survivor region.\n+  \/\/ Skip the card mark iff the object (to_array) is in survivor region.\n@@ -259,1 +265,1 @@\n-  G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_young());\n+  G1SkipCardMarkSetter x(&_scanner, dest_attr.is_young());\n@@ -536,1 +542,1 @@\n-    \/\/ Skip the card enqueue iff the object (obj) is in survivor region.\n+    \/\/ Skip the card mark iff the object (obj) is in survivor region.\n@@ -541,1 +547,1 @@\n-    G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_young());\n+    G1SkipCardMarkSetter x(&_scanner, dest_attr.is_young());\n@@ -562,1 +568,1 @@\n-      new G1ParScanThreadState(_g1h, rdcqs(),\n+      new G1ParScanThreadState(_g1h,\n@@ -588,2 +594,5 @@\n-    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _num_workers, &_rdc_buffers[worker_id]) * HeapWordSize;\n-    size_t evac_fail_enqueued_cards = pss->evac_failure_enqueued_cards();\n+    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _num_workers) * HeapWordSize;\n+    size_t pending_cards = pss->num_pending_cards();\n+    size_t to_young_gen_cards = pss->num_marked_cards() - pss->num_pending_cards();\n+    size_t evac_fail_marked_cards = pss->evac_failure_marked_cards();\n+    size_t marked_cards = pss->num_marked_cards();\n@@ -594,1 +603,4 @@\n-    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, evac_fail_enqueued_cards, G1GCPhaseTimes::MergePSSEvacFailExtra);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, pending_cards, G1GCPhaseTimes::MergePSSPendingCards);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, to_young_gen_cards, G1GCPhaseTimes::MergePSSToYoungGenCards);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, evac_fail_marked_cards, G1GCPhaseTimes::MergePSSEvacFail);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, marked_cards, G1GCPhaseTimes::MergePSSMarked);\n@@ -600,4 +612,0 @@\n-  G1DirtyCardQueueSet& dcq = G1BarrierSet::dirty_card_queue_set();\n-  dcq.merge_bufferlists(rdcqs());\n-  rdcqs()->verify_empty();\n-\n@@ -642,1 +650,1 @@\n-    G1SkipCardEnqueueSetter x(&_scanner, false \/* skip_card_enqueue *\/);\n+    G1SkipCardMarkSetter x(&_scanner, false \/* skip_card_mark *\/);\n@@ -698,1 +706,0 @@\n-    _rdcqs(G1BarrierSet::dirty_card_queue_set().allocator()),\n@@ -700,1 +707,0 @@\n-    _rdc_buffers(NEW_C_HEAP_ARRAY(BufferNodeList, num_workers, mtGC)),\n@@ -708,1 +714,0 @@\n-    _rdc_buffers[i] = BufferNodeList();\n@@ -717,1 +722,0 @@\n-  FREE_C_HEAP_ARRAY(BufferNodeList, _rdc_buffers);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":31,"deletions":27,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,0 @@\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n@@ -55,1 +54,0 @@\n-  G1RedirtyCardsLocalQueueSet _rdc_local_qset;\n@@ -68,3 +66,2 @@\n-  \/\/ Remember the last enqueued card to avoid enqueuing the same card over and over;\n-  \/\/ since we only ever scan a card once, this is sufficient.\n-  size_t _last_enqueued_card;\n+  size_t _num_marked_as_dirty_cards;\n+  size_t _num_marked_as_into_cset_cards;\n@@ -107,4 +104,2 @@\n-  \/\/ Number of additional cards into evacuation failed regions enqueued into\n-  \/\/ the local DCQS. This is an approximation, as cards that would be added later\n-  \/\/ outside of evacuation failure will not be subtracted again.\n-  size_t _evac_failure_enqueued_cards;\n+  \/\/ Number of additional cards into evacuation failed regions.\n+  size_t _evac_failure_marked_cards;\n@@ -112,1 +107,1 @@\n-  \/\/ Enqueue the card if not already in the set; this is a best-effort attempt on\n+  \/\/ Mark the card if not already in the set; this is a best-effort attempt on\n@@ -114,3 +109,3 @@\n-  template <class T> bool enqueue_if_new(T* p);\n-  \/\/ Enqueue the card of p into the (evacuation failed) region.\n-  template <class T> void enqueue_card_into_evac_fail_region(T* p, oop obj);\n+  template <class T> bool mark_if_new(T* p, bool into_survivor);\n+  \/\/ Mark the card of p into the (evacuation failed) region.\n+  template <class T> void mark_card_into_evac_fail_region(T* p, oop obj);\n@@ -122,1 +117,0 @@\n-                       G1RedirtyCardsQueueSet* rdcqs,\n@@ -142,1 +136,1 @@\n-  \/\/ Apply the post barrier to the given reference field. Enqueues the card of p\n+  \/\/ Apply the post barrier to the given reference field. Marks the card of p\n@@ -148,1 +142,1 @@\n-  \/\/ Enqueue the card if the reference's target region's remembered set is tracked.\n+  \/\/ Mark the card if the reference's target region's remembered set is tracked.\n@@ -151,1 +145,1 @@\n-  template <class T> void enqueue_card_if_tracked(G1HeapRegionAttr region_attr, T* p, oop o);\n+  template <class T> void mark_card_if_tracked(G1HeapRegionAttr region_attr, T* p, oop o);\n@@ -159,1 +153,3 @@\n-  size_t evac_failure_enqueued_cards() const;\n+  size_t num_pending_cards() const;\n+  size_t evac_failure_marked_cards() const;\n+  size_t num_marked_cards() const;\n@@ -163,1 +159,1 @@\n-  size_t flush_stats(size_t* surviving_young_words, uint num_workers, BufferNodeList* buffer_log);\n+  size_t flush_stats(size_t* surviving_young_words, uint num_workers);\n@@ -245,1 +241,0 @@\n-  G1RedirtyCardsQueueSet _rdcqs;\n@@ -247,1 +242,0 @@\n-  BufferNodeList* _rdc_buffers;\n@@ -260,3 +254,0 @@\n-  G1RedirtyCardsQueueSet* rdcqs() { return &_rdcqs; }\n-  BufferNodeList* rdc_buffers() { return _rdc_buffers; }\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":16,"deletions":25,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -99,6 +99,5 @@\n-template <class T> bool G1ParScanThreadState::enqueue_if_new(T* p) {\n-  size_t card_index = ct()->index_for(p);\n-  \/\/ If the card hasn't been added to the buffer, do it.\n-  if (_last_enqueued_card != card_index) {\n-    _rdc_local_qset.enqueue(ct()->byte_for_index(card_index));\n-    _last_enqueued_card = card_index;\n+template <class T> bool G1ParScanThreadState::mark_if_new(T* p, bool into_new_survivor) {\n+  G1CardTable::CardValue* card = ct()->byte_for(p);\n+  G1CardTable::CardValue value = *card;\n+  if (value == G1CardTable::clean_card_val()) {\n+    *card = into_new_survivor ? G1CardTable::g1_to_cset_card : G1CardTable::g1_dirty_card;\n@@ -111,1 +110,1 @@\n-template <class T> void G1ParScanThreadState::enqueue_card_into_evac_fail_region(T* p, oop obj) {\n+template <class T> void G1ParScanThreadState::mark_card_into_evac_fail_region(T* p, oop obj) {\n@@ -116,2 +115,2 @@\n-  if (enqueue_if_new(p)) {\n-    _evac_failure_enqueued_cards++;\n+  if (mark_if_new(p, false \/* into_new_survivor *\/)) { \/\/ The reference is never into survivor regions.\n+    _evac_failure_marked_cards++;\n@@ -140,1 +139,1 @@\n-    enqueue_card_into_evac_fail_region(p, obj);\n+    mark_card_into_evac_fail_region(p, obj);\n@@ -143,1 +142,1 @@\n-  enqueue_card_if_tracked(dest_attr, p, obj);\n+  mark_card_if_tracked(dest_attr, p, obj);\n@@ -146,1 +145,1 @@\n-template <class T> void G1ParScanThreadState::enqueue_card_if_tracked(G1HeapRegionAttr region_attr, T* p, oop o) {\n+template <class T> void G1ParScanThreadState::mark_card_if_tracked(G1HeapRegionAttr region_attr, T* p, oop o) {\n@@ -151,1 +150,1 @@\n-  assert(!_g1h->heap_region_containing(o)->in_collection_set(), \"Should not try to enqueue reference into collection set region\");\n+  assert(!_g1h->heap_region_containing(o)->in_collection_set(), \"Should not try to mark reference into collection set region\");\n@@ -164,1 +163,8 @@\n-  enqueue_if_new(p);\n+  bool into_survivor = region_attr.is_new_survivor();\n+  if (mark_if_new(p, into_survivor)) {\n+    if (into_survivor) {\n+      _num_marked_as_into_cset_cards++;\n+    } else {\n+      _num_marked_as_dirty_cards++;\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.inline.hpp","additions":20,"deletions":14,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -71,2 +71,1 @@\n-  _card_rs_length(0),\n-  _pending_cards_at_gc_start(0),\n+  _pending_cards_from_gc(0),\n@@ -560,1 +559,1 @@\n-void G1Policy::revise_young_list_target_length(size_t card_rs_length, size_t code_root_rs_length) {\n+void G1Policy::revise_young_list_target_length(size_t pending_cards, size_t card_rs_length, size_t code_root_rs_length) {\n@@ -563,3 +562,0 @@\n-  size_t thread_buffer_cards = _analytics->predict_dirtied_cards_in_thread_buffers();\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-  size_t pending_cards = dcqs.num_cards() + thread_buffer_cards;\n@@ -575,1 +571,0 @@\n-  _pending_cards_at_gc_start = 0;\n@@ -606,1 +601,1 @@\n-static void log_refinement_stats(const char* kind, const G1ConcurrentRefineStats& stats) {\n+static void log_refinement_stats(const G1ConcurrentRefineStats& stats) {\n@@ -608,4 +603,3 @@\n-           (\"%s refinement: %.2fms, refined: %zu\"\n-            \", precleaned: %zu, dirtied: %zu\",\n-            kind,\n-            stats.refinement_time().seconds() * MILLIUNITS,\n+           (\"Refinement: sweep: %.2fms, yield: %.2fms refined: %zu, dirtied: %zu\",\n+            TimeHelper::counter_to_millis(stats.sweep_duration()),\n+            TimeHelper::counter_to_millis(stats.yield_duration()),\n@@ -613,2 +607,1 @@\n-            stats.precleaned_cards(),\n-            stats.dirtied_cards());\n+            stats.cards_pending());\n@@ -617,18 +610,2 @@\n-void G1Policy::record_concurrent_refinement_stats(size_t pending_cards,\n-                                                  size_t thread_buffer_cards) {\n-  _pending_cards_at_gc_start = pending_cards;\n-  _analytics->report_dirtied_cards_in_thread_buffers(thread_buffer_cards);\n-\n-  \/\/ Collect per-thread stats, mostly from mutator activity.\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-  G1ConcurrentRefineStats mut_stats = dcqs.concatenated_refinement_stats();\n-\n-  \/\/ Collect specialized concurrent refinement thread stats.\n-  G1ConcurrentRefine* cr = _g1h->concurrent_refine();\n-  G1ConcurrentRefineStats cr_stats = cr->get_and_reset_refinement_stats();\n-\n-  G1ConcurrentRefineStats total_stats = mut_stats + cr_stats;\n-\n-  log_refinement_stats(\"Mutator\", mut_stats);\n-  log_refinement_stats(\"Concurrent\", cr_stats);\n-  log_refinement_stats(\"Total\", total_stats);\n+void G1Policy::record_refinement_stats(G1ConcurrentRefineStats* refine_stats) {\n+  log_refinement_stats(*refine_stats);\n@@ -637,5 +614,6 @@\n-  \/\/ Don't update the rate if the current sample is empty or time is zero.\n-  Tickspan refinement_time = total_stats.refinement_time();\n-  size_t refined_cards = total_stats.refined_cards();\n-  if ((refined_cards > 0) && (refinement_time > Tickspan())) {\n-    double rate = refined_cards \/ (refinement_time.seconds() * MILLIUNITS);\n+  \/\/ Don't update the rate if the current sample is empty or time is zero (which is\n+  \/\/ the case during GC).\n+  double refinement_time = TimeHelper::counter_to_millis(refine_stats->sweep_duration());\n+  size_t refined_cards = refine_stats->refined_cards();\n+  if ((refined_cards > 0) && (refinement_time > 0)) {\n+    double rate = refined_cards \/ refinement_time;\n@@ -643,1 +621,1 @@\n-    log_debug(gc, refine, stats)(\"Concurrent refinement rate: %.2f cards\/ms\", rate);\n+    log_debug(gc, refine, stats)(\"Concurrent refinement rate: %.2f cards\/ms predicted: %.2f cards\/ms\", rate, _analytics->predict_concurrent_refine_rate_ms());\n@@ -645,0 +623,1 @@\n+}\n@@ -646,0 +625,14 @@\n+template<typename T>\n+static T saturated_sub(T x, T y) {\n+  return (x < y) ? T() : (x - y);\n+}\n+\n+void G1Policy::record_dirtying_stats(double last_mutator_start_dirty_ms,\n+                                     double last_mutator_end_dirty_ms,\n+                                     size_t pending_cards,\n+                                     double yield_duration_ms,\n+                                     size_t next_pending_cards_from_gc,\n+                                     size_t next_to_collection_set_cards) {\n+  assert(SafepointSynchronize::is_at_safepoint() || G1RareEvent_lock->is_locked(),\n+         \"must be (at safepoint %s locked %s)\",\n+         BOOL_TO_STR(SafepointSynchronize::is_at_safepoint()), BOOL_TO_STR(G1RareEvent_lock->is_locked()));\n@@ -647,3 +640,1 @@\n-  double mut_start_time = _analytics->prev_collection_pause_end_ms();\n-  double mut_end_time = phase_times()->cur_collection_start_sec() * MILLIUNITS;\n-  double mut_time = mut_end_time - mut_start_time;\n+\n@@ -654,2 +645,10 @@\n-  if (mut_time > 1.0) {   \/\/ Require > 1ms sample time.\n-    double dirtied_rate = total_stats.dirtied_cards() \/ mut_time;\n+  double const mutator_dirty_time_ms = (last_mutator_end_dirty_ms - last_mutator_start_dirty_ms) - yield_duration_ms;\n+  assert(mutator_dirty_time_ms >= 0.0,\n+         \"must be (start: %.2f end: %.2f yield: %.2f)\",\n+         last_mutator_start_dirty_ms, last_mutator_end_dirty_ms, yield_duration_ms);\n+\n+  if (mutator_dirty_time_ms > 1.0) {   \/\/ Require > 1ms sample time.\n+    \/\/ The subtractive term is pending_cards_from_gc() which includes both dirtied and dirty-as-young cards,\n+    \/\/ which can be larger than what is actually considered as \"pending\" (dirty cards only).\n+    size_t dirtied_cards = saturated_sub(pending_cards, pending_cards_from_gc());\n+    double dirtied_rate = dirtied_cards \/ mutator_dirty_time_ms;\n@@ -657,1 +656,5 @@\n-    log_debug(gc, refine, stats)(\"Generate dirty cards rate: %.2f cards\/ms\", dirtied_rate);\n+    log_debug(gc, refine, stats)(\"Generate dirty cards rate: %.2f cards\/ms dirtying time %.2f (start %.2f end %.2f yield %.2f) dirtied %zu (pending %zu during_gc %zu)\",\n+                                 dirtied_rate,\n+                                 mutator_dirty_time_ms,\n+                                 last_mutator_start_dirty_ms, last_mutator_end_dirty_ms, yield_duration_ms,\n+                                 dirtied_cards, pending_cards, pending_cards_from_gc());\n@@ -659,0 +662,3 @@\n+\n+  _pending_cards_from_gc = next_pending_cards_from_gc;\n+  _to_collection_set_cards = next_to_collection_set_cards;\n@@ -773,1 +779,2 @@\n-  size_t logged_dirty_cards = phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergeLB, G1GCPhaseTimes::MergeLBDirtyCards);\n+  size_t logged_dirty_cards = phase_times()->sum_thread_work_items(G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ScanHRPendingCards) +\n+                              phase_times()->sum_thread_work_items(G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::ScanHRPendingCards);\n@@ -777,2 +784,1 @@\n-  double merge_logged_cards_time = average_time_ms(G1GCPhaseTimes::MergeLB) +\n-                                   phase_times()->cur_distribute_log_buffers_time_ms();\n+  double merge_logged_cards_time = phase_times()->cur_merge_refinement_table_time();\n@@ -825,0 +831,16 @@\n+  size_t const total_cards_scanned = p->sum_thread_work_items(G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ScanHRScannedCards) +\n+                                     p->sum_thread_work_items(G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::ScanHRScannedCards);\n+\n+  \/\/ Number of scanned cards with \"Dirty\" value (and nothing else).\n+  size_t const pending_cards_from_refinement_table = p->sum_thread_work_items(G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ScanHRPendingCards) +\n+                                                     p->sum_thread_work_items(G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::ScanHRPendingCards);\n+  \/\/ Number of cards actually merged in the Merge RS phase. MergeRSCards below includes the cards from the Eager Reclaim phase.\n+  size_t const merged_cards_from_card_rs = p->sum_thread_work_items(G1GCPhaseTimes::MergeRS, G1GCPhaseTimes::MergeRSFromRemSetCards) +\n+                                           p->sum_thread_work_items(G1GCPhaseTimes::OptMergeRS, G1GCPhaseTimes::MergeRSFromRemSetCards);\n+  \/\/ Number of cards attempted to merge in the Merge RS phase.\n+  size_t const total_cards_from_rs = p->sum_thread_work_items(G1GCPhaseTimes::MergeRS, G1GCPhaseTimes::MergeRSTotalCards) +\n+                                     p->sum_thread_work_items(G1GCPhaseTimes::OptMergeRS, G1GCPhaseTimes::MergeRSTotalCards);\n+\n+  \/\/ Cards marked as being to collection set. May be inaccurate due to races.\n+  size_t const total_non_young_rs_cards = MIN2(pending_cards_from_refinement_table + merged_cards_from_card_rs, total_cards_scanned);\n+\n@@ -837,1 +859,0 @@\n-  }\n@@ -839,21 +860,3 @@\n-  record_pause(this_pause, start_time_sec, end_time_sec, allocation_failure);\n-\n-  if (G1GCPauseTypeHelper::is_last_young_pause(this_pause)) {\n-    assert(!G1GCPauseTypeHelper::is_concurrent_start_pause(this_pause),\n-           \"The young GC before mixed is not allowed to be concurrent start GC\");\n-    \/\/ This has been the young GC before we start doing mixed GCs. We already\n-    \/\/ decided to start mixed GCs much earlier, so there is nothing to do except\n-    \/\/ advancing the state.\n-    collector_state()->set_in_young_only_phase(false);\n-    collector_state()->set_in_young_gc_before_mixed(false);\n-  } else if (G1GCPauseTypeHelper::is_mixed_pause(this_pause)) {\n-    \/\/ This is a mixed GC. Here we decide whether to continue doing more\n-    \/\/ mixed GCs or not.\n-    if (!next_gc_should_be_mixed()) {\n-      log_debug(gc, ergo)(\"do not continue mixed GCs (candidate old regions not available)\");\n-      collector_state()->set_in_young_only_phase(true);\n-\n-      assert(!candidates()->has_more_marking_candidates(),\n-             \"only end mixed if all candidates from marking were processed\");\n-\n-      maybe_start_marking();\n+    double merge_refinement_table_time = p->cur_merge_refinement_table_time();\n+    if (merge_refinement_table_time != 0.0) {\n+      _analytics->report_merge_refinement_table_time_ms(merge_refinement_table_time);\n@@ -861,16 +864,1 @@\n-  } else {\n-    assert(is_young_only_pause, \"must be\");\n-  }\n-\n-  _eden_surv_rate_group->start_adding_regions();\n-\n-  if (update_stats) {\n-    \/\/ Update prediction for card merge.\n-    size_t const merged_cards_from_log_buffers = p->sum_thread_work_items(G1GCPhaseTimes::MergeLB, G1GCPhaseTimes::MergeLBDirtyCards);\n-    \/\/ MergeRSCards includes the cards from the Eager Reclaim phase.\n-    size_t const merged_cards_from_rs = p->sum_thread_work_items(G1GCPhaseTimes::MergeRS, G1GCPhaseTimes::MergeRSCards) +\n-                                        p->sum_thread_work_items(G1GCPhaseTimes::OptMergeRS, G1GCPhaseTimes::MergeRSCards);\n-    size_t const total_cards_merged = merged_cards_from_rs +\n-                                      merged_cards_from_log_buffers;\n-\n-    if (total_cards_merged >= G1NumCardsCostSampleThreshold) {\n+    if (merged_cards_from_card_rs >= G1NumCardsCostSampleThreshold) {\n@@ -879,2 +867,0 @@\n-                                    average_time_ms(G1GCPhaseTimes::MergeLB) +\n-                                    p->cur_distribute_log_buffers_time_ms() +\n@@ -882,1 +868,6 @@\n-      _analytics->report_cost_per_card_merge_ms(avg_time_merge_cards \/ total_cards_merged, is_young_only_pause);\n+      _analytics->report_cost_per_card_merge_ms(avg_time_merge_cards \/ merged_cards_from_card_rs, is_young_only_pause);\n+      log_debug(gc, ergo, cset)(\"cost per card merge (young %s): avg time %.2f merged cards %zu cost(1m) %.2f pred_cost(1m-yo) %.2f pred_cost(1m-old) %.2f\",\n+                                BOOL_TO_STR(is_young_only_pause),\n+                                avg_time_merge_cards, merged_cards_from_card_rs, 1e6 * avg_time_merge_cards \/ merged_cards_from_card_rs, _analytics->predict_card_merge_time_ms(1e6, true), _analytics->predict_card_merge_time_ms(1e6, false));\n+    } else {\n+      log_debug(gc, ergo, cset)(\"cost per card merge (young: %s): skipped, total cards %zu\", BOOL_TO_STR(is_young_only_pause), total_non_young_rs_cards);\n@@ -886,2 +877,0 @@\n-    size_t const total_cards_scanned = p->sum_thread_work_items(G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ScanHRScannedCards) +\n-                                       p->sum_thread_work_items(G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::ScanHRScannedCards);\n@@ -890,2 +879,2 @@\n-      double avg_time_dirty_card_scan = average_time_ms(G1GCPhaseTimes::ScanHR) +\n-                                        average_time_ms(G1GCPhaseTimes::OptScanHR);\n+      double avg_card_scan_time = average_time_ms(G1GCPhaseTimes::ScanHR) +\n+                                  average_time_ms(G1GCPhaseTimes::OptScanHR);\n@@ -893,1 +882,7 @@\n-      _analytics->report_cost_per_card_scan_ms(avg_time_dirty_card_scan \/ total_cards_scanned, is_young_only_pause);\n+      _analytics->report_cost_per_card_scan_ms(avg_card_scan_time \/ total_cards_scanned, is_young_only_pause);\n+\n+      log_debug(gc, ergo, cset)(\"cost per card scan (young: %s): avg time %.2f total cards %zu cost(1m) %.2f pred_cost(1m-yo) %.2f pred_cost(1m-old) %.2f\",\n+                                BOOL_TO_STR(is_young_only_pause),\n+                                avg_card_scan_time, total_cards_scanned, 1e6 * avg_card_scan_time \/ total_cards_scanned, _analytics->predict_card_scan_time_ms(1e6, true), _analytics->predict_card_scan_time_ms(1e6, false));\n+    } else {\n+      log_debug(gc, ergo, cset)(\"cost per card scan (young: %s): skipped, total cards %zu\", BOOL_TO_STR(is_young_only_pause), total_cards_scanned);\n@@ -896,8 +891,6 @@\n-    \/\/ Update prediction for the ratio between cards from the remembered\n-    \/\/ sets and actually scanned cards from the remembered sets.\n-    \/\/ Due to duplicates in the log buffers, the number of scanned cards\n-    \/\/ can be smaller than the cards in the log buffers.\n-    const size_t scanned_cards_from_rs = (total_cards_scanned > merged_cards_from_log_buffers) ? total_cards_scanned - merged_cards_from_log_buffers : 0;\n-    double scan_to_merge_ratio = 0.0;\n-    if (merged_cards_from_rs > 0) {\n-      scan_to_merge_ratio = (double)scanned_cards_from_rs \/ merged_cards_from_rs;\n+    \/\/ Update prediction for the ratio between cards actually merged onto the card\n+    \/\/ table from the remembered sets and the total number of cards attempted to\n+    \/\/ merge.\n+    double merge_to_scan_ratio = 1.0;\n+    if (total_cards_from_rs > 0) {\n+      merge_to_scan_ratio = (double)merged_cards_from_card_rs \/ total_cards_from_rs;\n@@ -905,1 +898,1 @@\n-    _analytics->report_card_scan_to_merge_ratio(scan_to_merge_ratio, is_young_only_pause);\n+    _analytics->report_card_merge_to_scan_ratio(merge_to_scan_ratio, is_young_only_pause);\n@@ -922,1 +915,2 @@\n-      double cost_per_byte_ms = (average_time_ms(G1GCPhaseTimes::ObjCopy) + average_time_ms(G1GCPhaseTimes::OptObjCopy)) \/ copied_bytes;\n+      double avg_copy_time = average_time_ms(G1GCPhaseTimes::ObjCopy) + average_time_ms(G1GCPhaseTimes::OptObjCopy);\n+      double cost_per_byte_ms = avg_copy_time \/ copied_bytes;\n@@ -938,2 +932,3 @@\n-    _analytics->report_pending_cards((double)pending_cards_at_gc_start(), is_young_only_pause);\n-    _analytics->report_card_rs_length((double)_card_rs_length, is_young_only_pause);\n+    _analytics->report_pending_cards(pending_cards_from_refinement_table, is_young_only_pause);\n+\n+    _analytics->report_card_rs_length(total_cards_scanned - total_non_young_rs_cards, is_young_only_pause);\n@@ -943,0 +938,43 @@\n+  {\n+    double mutator_end_time = phase_times()->cur_collection_start_sec() * MILLIUNITS;\n+    G1ConcurrentRefineStats* stats = _g1h->concurrent_refine()->refine_state().stats();\n+    \/\/ Record any available refinement statistics.\n+    record_refinement_stats(stats);\n+\n+    double yield_duration_ms = TimeHelper::counter_to_millis(_g1h->yield_duration_in_refinement_epoch());\n+    record_dirtying_stats(TimeHelper::counter_to_millis(_g1h->last_refinement_epoch_start()),\n+                          mutator_end_time,\n+                          pending_cards_from_refinement_table,\n+                          yield_duration_ms,\n+                          phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergePSS, G1GCPhaseTimes::MergePSSPendingCards),\n+                          phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergePSS, G1GCPhaseTimes::MergePSSToYoungGenCards));\n+  }\n+\n+  record_pause(this_pause, start_time_sec, end_time_sec, allocation_failure);\n+\n+  if (G1GCPauseTypeHelper::is_last_young_pause(this_pause)) {\n+    assert(!G1GCPauseTypeHelper::is_concurrent_start_pause(this_pause),\n+           \"The young GC before mixed is not allowed to be concurrent start GC\");\n+    \/\/ This has been the young GC before we start doing mixed GCs. We already\n+    \/\/ decided to start mixed GCs much earlier, so there is nothing to do except\n+    \/\/ advancing the state.\n+    collector_state()->set_in_young_only_phase(false);\n+    collector_state()->set_in_young_gc_before_mixed(false);\n+  } else if (G1GCPauseTypeHelper::is_mixed_pause(this_pause)) {\n+    \/\/ This is a mixed GC. Here we decide whether to continue doing more\n+    \/\/ mixed GCs or not.\n+    if (!next_gc_should_be_mixed()) {\n+      log_debug(gc, ergo)(\"do not continue mixed GCs (candidate old regions not available)\");\n+      collector_state()->set_in_young_only_phase(true);\n+\n+      assert(!candidates()->has_more_marking_candidates(),\n+             \"only end mixed if all candidates from marking were processed\");\n+\n+      maybe_start_marking();\n+    }\n+  } else {\n+    assert(is_young_only_pause, \"must be\");\n+  }\n+\n+  _eden_surv_rate_group->start_adding_regions();\n+\n@@ -975,3 +1013,3 @@\n-  size_t logged_cards =\n-    phase_times()->sum_thread_work_items(G1GCPhaseTimes::MergeLB,\n-                                         G1GCPhaseTimes::MergeLBDirtyCards);\n+  size_t logged_cards = phase_times()->sum_thread_work_items(G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ScanHRPendingCards) +\n+                        phase_times()->sum_thread_work_items(G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::ScanHRPendingCards);\n+\n@@ -979,1 +1017,0 @@\n-  size_t predicted_thread_buffer_cards = _analytics->predict_dirtied_cards_in_thread_buffers();\n@@ -983,1 +1020,1 @@\n-           (\"GC refinement: goal: %zu + %zu \/ %1.2fms, actual: %zu \/ %1.2fms, %s\",\n+           (\"GC refinement: goal: %zu \/ %1.2fms, actual: %zu \/ %1.2fms, %s\",\n@@ -985,1 +1022,0 @@\n-            predicted_thread_buffer_cards,\n@@ -993,1 +1029,0 @@\n-                      predicted_thread_buffer_cards,\n@@ -1066,4 +1101,3 @@\n-  size_t unique_cards_from_rs = _analytics->predict_scan_card_num(card_rs_length, in_young_only_phase);\n-  \/\/ Assume that all cards from the log buffers will be scanned, i.e. there are no\n-  \/\/ duplicates in that set.\n-  size_t effective_scanned_cards = unique_cards_from_rs + pending_cards;\n+  \/\/ Cards from the refinement table and the cards from the young gen remset are\n+  \/\/ unique to each other as they are located on the card table.\n+  size_t effective_scanned_cards = card_rs_length + pending_cards;\n@@ -1071,1 +1105,1 @@\n-  double card_merge_time = _analytics->predict_card_merge_time_ms(pending_cards + card_rs_length, in_young_only_phase);\n+  double refinement_table_merge_time = _analytics->predict_merge_refinement_table_time_ms();\n@@ -1077,1 +1111,1 @@\n-  double total_time = card_merge_time + card_scan_time + code_root_scan_time + constant_other_time + survivor_evac_time;\n+  double total_time = refinement_table_merge_time + card_scan_time + code_root_scan_time + constant_other_time + survivor_evac_time;\n@@ -1080,1 +1114,1 @@\n-                            \"card_merge_time %f card_scan_time %f code_root_rs_length %zu code_root_scan_time %f \"\n+                            \"refinement_table_merge_time %f card_scan_time %f code_root_rs_length %zu code_root_scan_time %f \"\n@@ -1083,1 +1117,1 @@\n-                            card_merge_time, card_scan_time, code_root_rs_length, code_root_scan_time,\n+                            refinement_table_merge_time, card_scan_time, code_root_rs_length, code_root_scan_time,\n@@ -1088,6 +1122,0 @@\n-double G1Policy::predict_base_time_ms(size_t pending_cards) const {\n-  bool for_young_only_phase = collector_state()->in_young_only_phase();\n-  size_t card_rs_length = _analytics->predict_card_rs_length(for_young_only_phase);\n-  return predict_base_time_ms(pending_cards, card_rs_length);\n-}\n-\n@@ -1415,0 +1443,16 @@\n+double G1Policy::last_mutator_dirty_start_time_ms() {\n+  return TimeHelper::counter_to_millis(_g1h->last_refinement_epoch_start());\n+}\n+\n+size_t G1Policy::current_pending_cards() {\n+  double now = os::elapsedTime() * MILLIUNITS;\n+  return _pending_cards_from_gc + _analytics->predict_dirtied_cards_rate_ms() * (now - last_mutator_dirty_start_time_ms());\n+}\n+\n+size_t G1Policy::current_to_collection_set_cards() {\n+  \/\/ The incremental part is covered by the dirtied_cards_rate, i.e. pending cards\n+  \/\/ cover both to collection set cards and other interesting cards because we do not\n+  \/\/ know which is which until we look.\n+  return _to_collection_set_cards;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.cpp","additions":170,"deletions":126,"binary":false,"changes":296,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,0 +51,1 @@\n+class G1ConcurrentRefineStats;\n@@ -104,3 +105,2 @@\n-  size_t _card_rs_length;\n-\n-  size_t _pending_cards_at_gc_start;\n+  size_t _pending_cards_from_gc;\n+  size_t _to_collection_set_cards;\n@@ -133,6 +133,0 @@\n-  void record_card_rs_length(size_t card_rs_length) {\n-    _card_rs_length = card_rs_length;\n-  }\n-\n-  double predict_base_time_ms(size_t pending_cards) const;\n-\n@@ -244,1 +238,7 @@\n-  size_t pending_cards_at_gc_start() const { return _pending_cards_at_gc_start; }\n+\n+  double last_mutator_dirty_start_time_ms();\n+  size_t pending_cards_from_gc() const { return _pending_cards_from_gc; }\n+\n+  size_t current_pending_cards();\n+\n+  size_t current_to_collection_set_cards();\n@@ -287,1 +287,1 @@\n-  void revise_young_list_target_length(size_t card_rs_length, size_t code_root_rs_length);\n+  void revise_young_list_target_length(size_t pending_cards, size_t card_rs_length, size_t code_root_rs_length);\n@@ -368,5 +368,9 @@\n-  \/\/ Record and log stats and pending cards before not-full collection.\n-  \/\/ thread_buffer_cards is the number of cards that were in per-thread\n-  \/\/ buffers.  pending_cards includes thread_buffer_cards.\n-  void record_concurrent_refinement_stats(size_t pending_cards,\n-                                          size_t thread_buffer_cards);\n+  \/\/ Record and log stats and pending cards to update predictors.\n+  void record_refinement_stats(G1ConcurrentRefineStats* stats);\n+\n+  void record_dirtying_stats(double last_mutator_start_dirty_ms,\n+                             double last_mutator_end_dirty_ms,\n+                             size_t pending_cards,\n+                             double yield_duration,\n+                             size_t next_pending_cards_from_gc,\n+                             size_t next_to_collection_set_cards);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Policy.hpp","additions":21,"deletions":17,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1,148 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n-#include \"gc\/shared\/bufferNode.hpp\"\n-#include \"runtime\/atomic.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-\/\/ G1RedirtyCardsLocalQueueSet\n-\n-G1RedirtyCardsLocalQueueSet::G1RedirtyCardsLocalQueueSet(G1RedirtyCardsQueueSet* shared_qset) :\n-  PtrQueueSet(shared_qset->allocator()),\n-  _shared_qset(shared_qset),\n-  _buffers(),\n-  _queue(this)\n-{}\n-\n-#ifdef ASSERT\n-G1RedirtyCardsLocalQueueSet::~G1RedirtyCardsLocalQueueSet() {\n-  assert(_buffers._head == nullptr, \"unflushed qset\");\n-  assert(_buffers._tail == nullptr, \"invariant\");\n-  assert(_buffers._entry_count == 0, \"invariant\");\n-}\n-#endif \/\/ ASSERT\n-\n-void G1RedirtyCardsLocalQueueSet::enqueue_completed_buffer(BufferNode* node) {\n-  _buffers._entry_count += node->size();\n-  node->set_next(_buffers._head);\n-  _buffers._head = node;\n-  if (_buffers._tail == nullptr) {\n-    _buffers._tail = node;\n-  }\n-}\n-\n-void G1RedirtyCardsLocalQueueSet::enqueue(void* value) {\n-  if (!try_enqueue(_queue, value)) {\n-    BufferNode* old_node = exchange_buffer_with_new(_queue);\n-    if (old_node != nullptr) {\n-      enqueue_completed_buffer(old_node);\n-    }\n-    retry_enqueue(_queue, value);\n-  }\n-}\n-\n-BufferNodeList G1RedirtyCardsLocalQueueSet::flush() {\n-  flush_queue(_queue);\n-  BufferNodeList cur_buffers = _buffers;\n-  _shared_qset->add_bufferlist(_buffers);\n-  _buffers = BufferNodeList();\n-  return cur_buffers;\n-}\n-\n-\/\/ G1RedirtyCardsLocalQueueSet::Queue\n-\n-G1RedirtyCardsLocalQueueSet::Queue::Queue(G1RedirtyCardsLocalQueueSet* qset) :\n-  PtrQueue(qset)\n-{}\n-\n-#ifdef ASSERT\n-G1RedirtyCardsLocalQueueSet::Queue::~Queue() {\n-  assert(buffer() == nullptr, \"unflushed queue\");\n-}\n-#endif \/\/ ASSERT\n-\n-\/\/ G1RedirtyCardsQueueSet\n-\n-G1RedirtyCardsQueueSet::G1RedirtyCardsQueueSet(BufferNode::Allocator* allocator) :\n-  PtrQueueSet(allocator),\n-  _list(),\n-  _entry_count(0),\n-  _tail(nullptr)\n-  DEBUG_ONLY(COMMA _collecting(true))\n-{}\n-\n-G1RedirtyCardsQueueSet::~G1RedirtyCardsQueueSet() {\n-  verify_empty();\n-}\n-\n-#ifdef ASSERT\n-void G1RedirtyCardsQueueSet::verify_empty() const {\n-  assert(_list.empty(), \"precondition\");\n-  assert(_tail == nullptr, \"invariant\");\n-  assert(_entry_count == 0, \"invariant\");\n-}\n-#endif \/\/ ASSERT\n-\n-BufferNode* G1RedirtyCardsQueueSet::all_completed_buffers() const {\n-  DEBUG_ONLY(_collecting = false;)\n-  return _list.top();\n-}\n-\n-BufferNodeList G1RedirtyCardsQueueSet::take_all_completed_buffers() {\n-  DEBUG_ONLY(_collecting = false;)\n-  BufferNodeList result(_list.pop_all(), _tail, _entry_count);\n-  _tail = nullptr;\n-  _entry_count = 0;\n-  DEBUG_ONLY(_collecting = true;)\n-  return result;\n-}\n-\n-void G1RedirtyCardsQueueSet::update_tail(BufferNode* node) {\n-  \/\/ Node is the tail of a (possibly single element) list just prepended to\n-  \/\/ _list.  If, after that prepend, node's follower is null, then node is\n-  \/\/ also the tail of _list, so record it as such.\n-  if (node->next() == nullptr) {\n-    assert(_tail == nullptr, \"invariant\");\n-    _tail = node;\n-  }\n-}\n-\n-void G1RedirtyCardsQueueSet::enqueue_completed_buffer(BufferNode* node) {\n-  assert(_collecting, \"precondition\");\n-  Atomic::add(&_entry_count, node->size());\n-  _list.push(*node);\n-  update_tail(node);\n-}\n-\n-void G1RedirtyCardsQueueSet::add_bufferlist(const BufferNodeList& buffers) {\n-  assert(_collecting, \"precondition\");\n-  if (buffers._head != nullptr) {\n-    assert(buffers._tail != nullptr, \"invariant\");\n-    Atomic::add(&_entry_count, buffers._entry_count);\n-    _list.prepend(*buffers._head, *buffers._tail);\n-    update_tail(buffers._tail);\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RedirtyCardsQueue.cpp","additions":0,"deletions":148,"binary":false,"changes":148,"status":"deleted"},{"patch":"@@ -1,98 +0,0 @@\n-\/*\n- * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_G1_G1REDIRTYCARDSQUEUE_HPP\n-#define SHARE_GC_G1_G1REDIRTYCARDSQUEUE_HPP\n-\n-#include \"gc\/shared\/bufferNode.hpp\"\n-#include \"gc\/shared\/bufferNodeList.hpp\"\n-#include \"gc\/shared\/ptrQueue.hpp\"\n-#include \"memory\/padded.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-class G1RedirtyCardsQueueSet;\n-\n-\/\/ A thread-local qset and queue.  It provides an uncontended staging\n-\/\/ area for completed buffers, to be flushed to the shared qset en masse.\n-class G1RedirtyCardsLocalQueueSet : private PtrQueueSet {\n-  class Queue : public PtrQueue {\n-  public:\n-    Queue(G1RedirtyCardsLocalQueueSet* qset);\n-    ~Queue() NOT_DEBUG(= default);\n-  };\n-\n-  G1RedirtyCardsQueueSet* _shared_qset;\n-  BufferNodeList _buffers;\n-  Queue _queue;\n-\n-  \/\/ Add the buffer to the local list.\n-  virtual void enqueue_completed_buffer(BufferNode* node);\n-\n-public:\n-  G1RedirtyCardsLocalQueueSet(G1RedirtyCardsQueueSet* shared_qset);\n-  ~G1RedirtyCardsLocalQueueSet() NOT_DEBUG(= default);\n-\n-  void enqueue(void* value);\n-\n-  \/\/ Transfer all completed buffers to the shared qset.\n-  \/\/ Returns the flushed BufferNodeList which is later used\n-  \/\/ as a shortcut into the shared qset.\n-  BufferNodeList flush();\n-};\n-\n-\/\/ Card table entries to be redirtied and the cards reprocessed later.\n-\/\/ Has two phases, collecting and processing.  During the collecting\n-\/\/ phase buffers are added to the set.  Once collecting is complete and\n-\/\/ processing starts, buffers can no longer be added.  Taking all the\n-\/\/ collected (and processed) buffers reverts back to collecting, allowing\n-\/\/ the set to be reused for another round of redirtying.\n-class G1RedirtyCardsQueueSet : public PtrQueueSet {\n-  DEFINE_PAD_MINUS_SIZE(1, DEFAULT_PADDING_SIZE, 0);\n-  BufferNode::Stack _list;\n-  DEFINE_PAD_MINUS_SIZE(2, DEFAULT_PADDING_SIZE, sizeof(size_t));\n-  volatile size_t _entry_count;\n-  DEFINE_PAD_MINUS_SIZE(3, DEFAULT_PADDING_SIZE, sizeof(BufferNode*));\n-  BufferNode* _tail;\n-  DEBUG_ONLY(mutable bool _collecting;)\n-\n-  void update_tail(BufferNode* node);\n-\n-public:\n-  G1RedirtyCardsQueueSet(BufferNode::Allocator* allocator);\n-  ~G1RedirtyCardsQueueSet();\n-\n-  void verify_empty() const NOT_DEBUG_RETURN;\n-\n-  \/\/ Collect buffers.  These functions are thread-safe.\n-  \/\/ precondition: Must not be concurrent with buffer processing.\n-  virtual void enqueue_completed_buffer(BufferNode* node);\n-  void add_bufferlist(const BufferNodeList& buffers);\n-\n-  \/\/ Processing phase operations.\n-  \/\/ precondition: Must not be concurrent with buffer collection.\n-  BufferNode* all_completed_buffers() const;\n-  BufferNodeList take_all_completed_buffers();\n-};\n-\n-#endif \/\/ SHARE_GC_G1_G1REDIRTYCARDSQUEUE_HPP\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RedirtyCardsQueue.hpp","additions":0,"deletions":98,"binary":false,"changes":98,"status":"deleted"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/g1\/g1CardTableClaimTable.inline.hpp\"\n@@ -34,1 +35,1 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n+#include \"gc\/g1\/g1ConcurrentRefineWorkTask.hpp\"\n@@ -92,24 +93,1 @@\n-  size_t _max_reserved_regions;\n-\n-  \/\/ Card table iteration claim for each heap region, from 0 (completely unscanned)\n-  \/\/ to (>=) G1HeapRegion::CardsPerRegion (completely scanned).\n-  uint volatile* _card_table_scan_state;\n-\n-  uint _scan_chunks_per_region;         \/\/ Number of chunks per region.\n-  uint8_t _log_scan_chunks_per_region;  \/\/ Log of number of chunks per region.\n-  bool* _region_scan_chunks;\n-  size_t _num_total_scan_chunks;        \/\/ Total number of elements in _region_scan_chunks.\n-  uint8_t _scan_chunks_shift;           \/\/ For conversion between card index and chunk index.\n-public:\n-  uint scan_chunk_size_in_cards() const { return (uint)1 << _scan_chunks_shift; }\n-\n-  \/\/ Returns whether the chunk corresponding to the given region\/card in region contain a\n-  \/\/ dirty card, i.e. actually needs scanning.\n-  bool chunk_needs_scan(uint const region_idx, uint const card_in_region) const {\n-    size_t const idx = ((size_t)region_idx << _log_scan_chunks_per_region) + (card_in_region >> _scan_chunks_shift);\n-    assert(idx < _num_total_scan_chunks, \"Index %zu out of bounds %zu\",\n-           idx, _num_total_scan_chunks);\n-    return _region_scan_chunks[idx];\n-  }\n-\n-private:\n+  G1CardTableClaimTable _card_claim_table;\n@@ -117,1 +95,1 @@\n-  \/\/ of GC because we scribbled over these card tables.\n+  \/\/ of GC because we scribbled over these card table entries.\n@@ -120,3 +98,2 @@\n-  \/\/ - they were part of the collection set: they may contain g1_young_card_val\n-  \/\/ or regular card marks that we never scan so we must always clear their card\n-  \/\/ table\n+  \/\/ - they were part of the collection set: they may contain regular card marks\n+  \/\/ that we never scan so we must always clear their card table.\n@@ -132,1 +109,1 @@\n-  \/\/ Set of (unique) regions that can be added to concurrently.\n+\/\/ Set of (unique) regions that can be added to concurrently.\n@@ -150,2 +127,0 @@\n-    static size_t chunk_size() { return M; }\n-\n@@ -200,1 +175,1 @@\n-  class G1ClearCardTableTask : public G1AbstractSubTask {\n+class G1ClearCardTableTask : public G1AbstractSubTask {\n@@ -232,3 +207,3 @@\n-#ifndef PRODUCT\n-      G1CollectedHeap::heap()->verifier()->verify_card_table_cleanup();\n-#endif\n+      if (VerifyDuringGC) {\n+        G1CollectedHeap::heap()->verifier()->verify_card_table_cleanup();\n+      }\n@@ -246,0 +221,4 @@\n+          \/\/ The card table contains \"dirty\" card marks. Clear unconditionally.\n+          \/\/\n+          \/\/ Humongous reclaim candidates are not in the dirty set. This is fine because\n+          \/\/ their card and refinement table should always be clear as they are typeArrays.\n@@ -247,0 +226,4 @@\n+          \/\/ There is no need to clear the refinement table here: at the start of the collection\n+          \/\/ we had to clear the refinement card table for collection set regions already, and any\n+          \/\/ old regions use it for old->collection set candidates, so they should not be cleared\n+          \/\/ either.\n@@ -254,7 +237,1 @@\n-    _max_reserved_regions(0),\n-    _card_table_scan_state(nullptr),\n-    _scan_chunks_per_region(G1CollectedHeap::get_chunks_per_region()),\n-    _log_scan_chunks_per_region(log2i(_scan_chunks_per_region)),\n-    _region_scan_chunks(nullptr),\n-    _num_total_scan_chunks(0),\n-    _scan_chunks_shift(0),\n+    _card_claim_table(G1CollectedHeap::get_chunks_per_region_for_scan()),\n@@ -263,2 +240,1 @@\n-    _scan_top(nullptr) {\n-  }\n+    _scan_top(nullptr) { }\n@@ -267,2 +243,0 @@\n-    FREE_C_HEAP_ARRAY(uint, _card_table_scan_state);\n-    FREE_C_HEAP_ARRAY(bool, _region_scan_chunks);\n@@ -272,8 +246,2 @@\n-  void initialize(size_t max_reserved_regions) {\n-    assert(_card_table_scan_state == nullptr, \"Must not be initialized twice\");\n-    _max_reserved_regions = max_reserved_regions;\n-    _card_table_scan_state = NEW_C_HEAP_ARRAY(uint, max_reserved_regions, mtGC);\n-    _num_total_scan_chunks = max_reserved_regions * _scan_chunks_per_region;\n-    _region_scan_chunks = NEW_C_HEAP_ARRAY(bool, _num_total_scan_chunks, mtGC);\n-\n-    _scan_chunks_shift = (uint8_t)log2i(G1HeapRegion::CardsPerRegion \/ _scan_chunks_per_region);\n+  void initialize(uint max_reserved_regions) {\n+    _card_claim_table.initialize(max_reserved_regions);\n@@ -283,0 +251,4 @@\n+  \/\/ Reset the claim and clear scan top for all regions, including\n+  \/\/ regions currently not available or free. Since regions might\n+  \/\/ become used during the collection these values must be valid\n+  \/\/ for those regions as well.\n@@ -284,5 +256,3 @@\n-    \/\/ Reset the claim and clear scan top for all regions, including\n-    \/\/ regions currently not available or free. Since regions might\n-    \/\/ become used during the collection these values must be valid\n-    \/\/ for those regions as well.\n-    for (size_t i = 0; i < _max_reserved_regions; i++) {\n+    size_t max_reserved_regions = _card_claim_table.max_reserved_regions();\n+\n+    for (size_t i = 0; i < max_reserved_regions; i++) {\n@@ -292,2 +262,2 @@\n-    _all_dirty_regions = new G1DirtyRegions(_max_reserved_regions);\n-    _next_dirty_regions = new G1DirtyRegions(_max_reserved_regions);\n+    _all_dirty_regions = new G1DirtyRegions(max_reserved_regions);\n+    _next_dirty_regions = new G1DirtyRegions(max_reserved_regions);\n@@ -297,1 +267,3 @@\n-    assert(_next_dirty_regions->size() == 0, \"next dirty regions must be empty\");\n+    \/\/ We populate the next dirty regions at the start of GC with all old\/humongous\n+    \/\/ regions.\n+    \/\/assert(_next_dirty_regions->size() == 0, \"next dirty regions must be empty\");\n@@ -299,5 +271,1 @@\n-    for (size_t i = 0; i < _max_reserved_regions; i++) {\n-      _card_table_scan_state[i] = 0;\n-    }\n-\n-    ::memset(_region_scan_chunks, false, _num_total_scan_chunks * sizeof(*_region_scan_chunks));\n+    _card_claim_table.reset_all_claims_to_unclaimed();\n@@ -324,10 +292,0 @@\n-  size_t num_visited_cards() const {\n-    size_t result = 0;\n-    for (uint i = 0; i < _num_total_scan_chunks; i++) {\n-      if (_region_scan_chunks[i]) {\n-        result++;\n-      }\n-    }\n-    return result * (G1HeapRegion::CardsPerRegion \/ _scan_chunks_per_region);\n-  }\n-\n@@ -338,18 +296,0 @@\n-  void set_chunk_range_dirty(size_t const region_card_idx, size_t const card_length) {\n-    size_t chunk_idx = region_card_idx >> _scan_chunks_shift;\n-    \/\/ Make sure that all chunks that contain the range are marked. Calculate the\n-    \/\/ chunk of the last card that is actually marked.\n-    size_t const end_chunk = (region_card_idx + card_length - 1) >> _scan_chunks_shift;\n-    for (; chunk_idx <= end_chunk; chunk_idx++) {\n-      _region_scan_chunks[chunk_idx] = true;\n-    }\n-  }\n-\n-  void set_chunk_dirty(size_t const card_idx) {\n-    assert((card_idx >> _scan_chunks_shift) < _num_total_scan_chunks,\n-           \"Trying to access index %zu out of bounds %zu\",\n-           card_idx >> _scan_chunks_shift, _num_total_scan_chunks);\n-    size_t const chunk_idx = card_idx >> _scan_chunks_shift;\n-    _region_scan_chunks[chunk_idx] = true;\n-  }\n-\n@@ -394,7 +334,1 @@\n-    assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n-    return _card_table_scan_state[region] < G1HeapRegion::CardsPerRegion;\n-  }\n-\n-  uint claim_cards_to_scan(uint region, uint increment) {\n-    assert(region < _max_reserved_regions, \"Tried to access invalid region %u\", region);\n-    return Atomic::fetch_then_add(&_card_table_scan_state[region], increment, memory_order_relaxed);\n+    return _card_claim_table.has_unclaimed_cards(region);\n@@ -404,1 +338,1 @@\n-#ifdef ASSERT\n+  #ifdef ASSERT\n@@ -409,1 +343,1 @@\n-#endif\n+  #endif\n@@ -434,0 +368,4 @@\n+\n+  G1CardTableChunkClaimer claimer(uint region_idx) {\n+    return G1CardTableChunkClaimer(&_card_claim_table, region_idx);\n+  }\n@@ -436,2 +374,1 @@\n-G1RemSet::G1RemSet(G1CollectedHeap* g1h,\n-                   G1CardTable* ct) :\n+G1RemSet::G1RemSet(G1CollectedHeap* g1h) :\n@@ -441,1 +378,0 @@\n-  _ct(ct),\n@@ -453,30 +389,0 @@\n-\/\/ Helper class to claim dirty chunks within the card table.\n-class G1CardTableChunkClaimer {\n-  G1RemSetScanState* _scan_state;\n-  uint _region_idx;\n-  uint _cur_claim;\n-\n-public:\n-  G1CardTableChunkClaimer(G1RemSetScanState* scan_state, uint region_idx) :\n-    _scan_state(scan_state),\n-    _region_idx(region_idx),\n-    _cur_claim(0) {\n-    guarantee(size() <= G1HeapRegion::CardsPerRegion, \"Should not claim more space than possible.\");\n-  }\n-\n-  bool has_next() {\n-    while (true) {\n-      _cur_claim = _scan_state->claim_cards_to_scan(_region_idx, size());\n-      if (_cur_claim >= G1HeapRegion::CardsPerRegion) {\n-        return false;\n-      }\n-      if (_scan_state->chunk_needs_scan(_region_idx, _cur_claim)) {\n-        return true;\n-      }\n-    }\n-  }\n-\n-  uint value() const { return _cur_claim; }\n-  uint size() const { return _scan_state->scan_chunk_size_in_cards(); }\n-};\n-\n@@ -498,0 +404,2 @@\n+  size_t _cards_pending;\n+  size_t _cards_empty;\n@@ -511,1 +419,1 @@\n-  HeapWord* scan_memregion(uint region_idx_for_card, MemRegion mr) {\n+  HeapWord* scan_memregion(uint region_idx_for_card, MemRegion mr, size_t &roots_found) {\n@@ -513,1 +421,1 @@\n-    G1ScanCardClosure card_cl(_g1h, _pss, _heap_roots_found);\n+    G1ScanCardClosure card_cl(_g1h, _pss, roots_found);\n@@ -523,2 +431,2 @@\n-  void do_claimed_block(uint const region_idx, CardValue* const dirty_l, CardValue* const dirty_r) {\n-    _ct->change_dirty_cards_to(dirty_l, dirty_r, _scanned_card_value);\n+  void do_claimed_block(uint const region_idx, CardValue* const dirty_l, CardValue* const dirty_r, size_t& pending_cards) {\n+    pending_cards += _ct->change_dirty_cards_to(dirty_l, dirty_r, _scanned_card_value);\n@@ -539,1 +447,2 @@\n-    _scanned_to = scan_memregion(region_idx, mr);\n+    size_t roots_found = 0;\n+    _scanned_to = scan_memregion(region_idx, mr, roots_found);\n@@ -541,0 +450,3 @@\n+    if (roots_found == 0) {\n+      _cards_empty += num_cards;\n+    }\n@@ -542,0 +454,1 @@\n+    _heap_roots_found += roots_found;\n@@ -544,98 +457,0 @@\n-  \/\/ To locate consecutive dirty cards inside a chunk.\n-  class ChunkScanner {\n-    using Word = size_t;\n-\n-    CardValue* const _start_card;\n-    CardValue* const _end_card;\n-\n-    static const size_t ExpandedToScanMask = G1CardTable::WordAlreadyScanned;\n-    static const size_t ToScanMask = G1CardTable::g1_card_already_scanned;\n-\n-    static bool is_card_dirty(const CardValue* const card) {\n-      return (*card & ToScanMask) == 0;\n-    }\n-\n-    static bool is_word_aligned(const void* const addr) {\n-      return ((uintptr_t)addr) % sizeof(Word) == 0;\n-    }\n-\n-    CardValue* find_first_dirty_card(CardValue* i_card) const {\n-      while (!is_word_aligned(i_card)) {\n-        if (is_card_dirty(i_card)) {\n-          return i_card;\n-        }\n-        i_card++;\n-      }\n-\n-      for (\/* empty *\/; i_card < _end_card; i_card += sizeof(Word)) {\n-        Word word_value = *reinterpret_cast<Word*>(i_card);\n-        bool has_dirty_cards_in_word = (~word_value & ExpandedToScanMask) != 0;\n-\n-        if (has_dirty_cards_in_word) {\n-          for (uint i = 0; i < sizeof(Word); ++i) {\n-            if (is_card_dirty(i_card)) {\n-              return i_card;\n-            }\n-            i_card++;\n-          }\n-          assert(false, \"should have early-returned\");\n-        }\n-      }\n-\n-      return _end_card;\n-    }\n-\n-    CardValue* find_first_non_dirty_card(CardValue* i_card) const {\n-      while (!is_word_aligned(i_card)) {\n-        if (!is_card_dirty(i_card)) {\n-          return i_card;\n-        }\n-        i_card++;\n-      }\n-\n-      for (\/* empty *\/; i_card < _end_card; i_card += sizeof(Word)) {\n-        Word word_value = *reinterpret_cast<Word*>(i_card);\n-        bool all_cards_dirty = (word_value == G1CardTable::WordAllDirty);\n-\n-        if (!all_cards_dirty) {\n-          for (uint i = 0; i < sizeof(Word); ++i) {\n-            if (!is_card_dirty(i_card)) {\n-              return i_card;\n-            }\n-            i_card++;\n-          }\n-          assert(false, \"should have early-returned\");\n-        }\n-      }\n-\n-      return _end_card;\n-    }\n-\n-  public:\n-    ChunkScanner(CardValue* const start_card, CardValue* const end_card) :\n-      _start_card(start_card),\n-      _end_card(end_card) {\n-        assert(is_word_aligned(start_card), \"precondition\");\n-        assert(is_word_aligned(end_card), \"precondition\");\n-      }\n-\n-    template<typename Func>\n-    void on_dirty_cards(Func&& f) {\n-      for (CardValue* cur_card = _start_card; cur_card < _end_card; \/* empty *\/) {\n-        CardValue* dirty_l = find_first_dirty_card(cur_card);\n-        CardValue* dirty_r = find_first_non_dirty_card(dirty_l);\n-\n-        assert(dirty_l <= dirty_r, \"inv\");\n-\n-        if (dirty_l == dirty_r) {\n-          assert(dirty_r == _end_card, \"finished the entire chunk\");\n-          return;\n-        }\n-\n-        f(dirty_l, dirty_r);\n-\n-        cur_card = dirty_r + 1;\n-      }\n-    }\n-  };\n-\n@@ -647,1 +462,1 @@\n-    G1CardTableChunkClaimer claim(_scan_state, region_idx);\n+    G1CardTableChunkClaimer claim = _scan_state->claimer(region_idx);\n@@ -655,0 +470,2 @@\n+    size_t pending_cards = 0;\n+\n@@ -663,1 +480,1 @@\n-      ChunkScanner chunk_scanner{start_card, end_card};\n+      G1ChunkScanner chunk_scanner{start_card, end_card};\n@@ -665,1 +482,1 @@\n-                                     do_claimed_block(region_idx, dirty_l, dirty_r);\n+                                     do_claimed_block(region_idx, dirty_l, dirty_r, pending_cards);\n@@ -668,0 +485,1 @@\n+    _cards_pending += pending_cards;\n@@ -682,0 +500,2 @@\n+    _cards_pending(0),\n+    _cards_empty(0),\n@@ -709,0 +529,2 @@\n+  size_t cards_pending() const { return _cards_pending; }\n+  size_t cards_scanned_empty() const { return _cards_empty; }\n@@ -731,0 +553,3 @@\n+\n+  p->record_or_add_thread_work_item(scan_phase, worker_id, cl.cards_pending(), G1GCPhaseTimes::ScanHRPendingCards);\n+  p->record_or_add_thread_work_item(scan_phase, worker_id, cl.cards_scanned_empty(), G1GCPhaseTimes::ScanHRScannedEmptyCards);\n@@ -904,0 +729,1 @@\n+    _scan_state->add_dirty_region(hrm_index);\n@@ -959,0 +785,84 @@\n+\/\/ Task to merge a non-dirty refinement table into the (primary) card table.\n+class MergeRefinementTableTask : public WorkerTask {\n+\n+  G1CardTableClaimTable* _scan_state;\n+  uint _max_workers;\n+\n+  class G1MergeRefinementTableRegionClosure : public G1HeapRegionClosure {\n+    G1CardTableClaimTable* _scan_state;\n+\n+    bool do_heap_region(G1HeapRegion* r) override {\n+      if (!_scan_state->has_unclaimed_cards(r->hrm_index())) {\n+        return false;\n+      }\n+\n+      \/\/ We can blindly clear all collection set region's refinement tables: these\n+      \/\/ regions will be evacuated and need their refinement table reset in case\n+      \/\/ of evacuation failure.\n+      \/\/ Young regions contain random marks, which are obvious to just clear. The\n+      \/\/ card marks of other collection set region's refinement tables are also\n+      \/\/ uninteresting.\n+      if (r->in_collection_set()) {\n+        uint claim = _scan_state->claim_all_cards(r->hrm_index());\n+        \/\/ Concurrent refinement may have started merging this region (we also\n+        \/\/ get here for non-young regions), the claim may be non-zero for those.\n+        \/\/ We could get away here with just clearing the area from the current\n+        \/\/ claim to the last card in the region, but for now just do it all.\n+        if (claim < G1HeapRegion::CardsPerRegion) {\n+          r->clear_refinement_table();\n+        }\n+        return false;\n+      }\n+\n+      assert(r->is_old_or_humongous(), \"must be\");\n+\n+      G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+      G1CardTable* card_table = g1h->card_table();\n+      G1CardTable* refinement_table = g1h->refinement_table();\n+\n+      size_t const region_card_base_idx = (size_t)r->hrm_index() << G1HeapRegion::LogCardsPerRegion;\n+\n+      G1CardTableChunkClaimer claim(_scan_state, r->hrm_index());\n+\n+      while (claim.has_next()) {\n+        size_t const start_idx = region_card_base_idx + claim.value();\n+\n+        size_t* card_cur_word = (size_t*)card_table->byte_for_index(start_idx);\n+\n+        size_t* refinement_cur_card = (size_t*)refinement_table->byte_for_index(start_idx);\n+        size_t* const refinement_end_card = refinement_cur_card + claim.size() \/ (sizeof(size_t) \/ sizeof(G1CardTable::CardValue));\n+\n+        for (; refinement_cur_card < refinement_end_card; ++refinement_cur_card, ++card_cur_word) {\n+          size_t value = *refinement_cur_card;\n+          *refinement_cur_card = G1CardTable::WordAllClean;\n+          \/\/ Dirty is \"0\", so we need to logically-and here. This is also safe\n+          \/\/ for all other possible values in the card table; at this point this\n+          \/\/ can be either g1_dirty_card or g1_to_cset_card which will both be\n+          \/\/ scanned.\n+          size_t new_value = *card_cur_word & value;\n+          *card_cur_word = new_value;\n+        }\n+      }\n+\n+      return false;\n+    }\n+\n+  public:\n+    G1MergeRefinementTableRegionClosure(G1CardTableClaimTable* scan_state) : G1HeapRegionClosure(), _scan_state(scan_state) {\n+    }\n+  };\n+\n+public:\n+  MergeRefinementTableTask(G1CardTableClaimTable* scan_state, uint max_workers) :\n+    WorkerTask(\"Merge Refinement Table\"), _scan_state(scan_state), _max_workers(max_workers) {     guarantee(_scan_state != nullptr, \"must be\");  }\n+\n+  void work(uint worker_id) override {\n+    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+\n+    G1GCParPhaseTimesTracker x(g1h->phase_times(), G1GCPhaseTimes::SweepRT, worker_id, false \/* allow multiple invocation *\/);\n+\n+    G1MergeRefinementTableRegionClosure cl(_scan_state);\n+    _scan_state->heap_region_iterate_from_worker_offset(&cl, worker_id, _max_workers);\n+  }\n+};\n+\n@@ -976,2 +886,6 @@\n-    void inc_remset_cards(size_t increment = 1) {\n-      _merged[G1GCPhaseTimes::MergeRSCards] += increment;\n+    void inc_merged_cards(size_t increment = 1) {\n+      _merged[G1GCPhaseTimes::MergeRSFromRemSetCards] += increment;\n+    }\n+\n+    void inc_total_cards(size_t increment = 1) {\n+      _merged[G1GCPhaseTimes::MergeRSTotalCards] += increment;\n@@ -981,1 +895,1 @@\n-      _merged[G1GCPhaseTimes::MergeRSCards] -= decrement;\n+      _merged[G1GCPhaseTimes::MergeRSTotalCards] -= decrement;\n@@ -1035,2 +949,2 @@\n-      if (_ct->mark_clean_as_dirty(value)) {\n-        _scan_state->set_chunk_dirty(_ct->index_for_cardvalue(value));\n+      if (_ct->mark_clean_as_from_remset(value)) {\n+        _stats.inc_merged_cards();\n@@ -1038,1 +952,1 @@\n-      _stats.inc_remset_cards();\n+      _stats.inc_total_cards();\n@@ -1059,1 +973,1 @@\n-      assert(tag < G1GCPhaseTimes::MergeRSCards, \"invalid tag %u\", tag);\n+      assert(tag < G1GCPhaseTimes::MergeRSFromRemSetCards, \"invalid tag %u\", tag);\n@@ -1069,3 +983,3 @@\n-      _ct->mark_range_dirty(_region_base_idx + start_card_idx, length);\n-      _stats.inc_remset_cards(length);\n-      _scan_state->set_chunk_range_dirty(_region_base_idx + start_card_idx, length);\n+      size_t cards_changed = _ct->mark_clean_range_as_from_remset(_region_base_idx + start_card_idx, length);\n+      _stats.inc_merged_cards(cards_changed);\n+      _stats.inc_total_cards(length);\n@@ -1092,3 +1006,3 @@\n-      if (!rem_set->is_empty()) {\n-        rem_set->iterate_for_merge(*this);\n-      }\n+      assert(!rem_set->is_empty(), \"should not be empty\");\n+\n+      rem_set->iterate_for_merge(*this);\n@@ -1097,1 +1011,1 @@\n-    virtual bool do_heap_region(G1HeapRegion* r) {\n+    bool do_heap_region(G1HeapRegion* r) override {\n@@ -1101,1 +1015,3 @@\n-      merge_card_set_for_region(r);\n+      if (!r->rem_set()->is_empty()) {\n+        merge_card_set_for_region(r);\n+      }\n@@ -1123,0 +1039,1 @@\n+    bool _initial_evacuation;\n@@ -1129,0 +1046,6 @@\n+    void assert_refinement_table_clear(G1HeapRegion* hr) {\n+#ifdef ASSERT\n+      _g1h->refinement_table()->verify_region(MemRegion(hr->bottom(), hr->end()), G1CardTable::clean_card_val(), true);\n+#endif\n+    }\n+\n@@ -1147,1 +1070,1 @@\n-    G1ClearBitmapClosure(G1CollectedHeap* g1h, G1RemSetScanState* scan_state) :\n+    G1ClearBitmapClosure(G1CollectedHeap* g1h, G1RemSetScanState* scan_state, bool initial_evacuation) :\n@@ -1149,1 +1072,2 @@\n-      _scan_state(scan_state)\n+      _scan_state(scan_state),\n+      _initial_evacuation(initial_evacuation)\n@@ -1155,0 +1079,16 @@\n+      \/\/ Collection set regions after the initial evacuation need their refinement\n+      \/\/ table cleared because\n+      \/\/ * we use the refinement table for recording references to other regions\n+      \/\/ during evacuation failure handling\n+      \/\/ * during previous passes we used the refinement table to contain marks for\n+      \/\/ cross-region references. Now that we evacuate the region, they need to be\n+      \/\/ cleared.\n+      \/\/\n+      \/\/ We do not need to do this extra work for initial evacuation because we\n+      \/\/ make sure the refinement table is clean for all regions either in\n+      \/\/ concurrent refinement or in the merge refinement table phase earlier.\n+      if (!_initial_evacuation) {\n+        hr->clear_refinement_table();\n+      } else {\n+        assert_refinement_table_clear(hr);\n+      }\n@@ -1215,53 +1155,0 @@\n-  \/\/ Visitor for the log buffer entries to merge them into the card table.\n-  class G1MergeLogBufferCardsClosure : public G1CardTableEntryClosure {\n-\n-    G1RemSetScanState* _scan_state;\n-    G1CardTable* _ct;\n-\n-    size_t _cards_dirty;\n-    size_t _cards_skipped;\n-\n-    void process_card(CardValue* card_ptr) {\n-      if (*card_ptr == G1CardTable::dirty_card_val()) {\n-        uint const region_idx = _ct->region_idx_for(card_ptr);\n-        _scan_state->add_dirty_region(region_idx);\n-        _scan_state->set_chunk_dirty(_ct->index_for_cardvalue(card_ptr));\n-        _cards_dirty++;\n-      }\n-    }\n-\n-  public:\n-    G1MergeLogBufferCardsClosure(G1CollectedHeap* g1h, G1RemSetScanState* scan_state) :\n-      _scan_state(scan_state),\n-      _ct(g1h->card_table()),\n-      _cards_dirty(0),\n-      _cards_skipped(0)\n-    {}\n-\n-    void do_card_ptr(CardValue* card_ptr) override {\n-      \/\/ The only time we care about recording cards that\n-      \/\/ contain references that point into the collection set\n-      \/\/ is during RSet updating within an evacuation pause.\n-      assert(SafepointSynchronize::is_at_safepoint(), \"not during an evacuation pause\");\n-\n-      uint const region_idx = _ct->region_idx_for(card_ptr);\n-\n-      \/\/ The second clause must come after - the log buffers might contain cards to uncommitted\n-      \/\/ regions.\n-      \/\/ This code may count duplicate entries in the log buffers (even if rare) multiple\n-      \/\/ times.\n-      if (_scan_state->contains_cards_to_process(region_idx)) {\n-        process_card(card_ptr);\n-      } else {\n-        \/\/ We may have had dirty cards in the (initial) collection set (or the\n-        \/\/ young regions which are always in the initial collection set). We do\n-        \/\/ not fix their cards here: we already added these regions to the set of\n-        \/\/ regions to clear the card table at the end during the prepare() phase.\n-        _cards_skipped++;\n-      }\n-    }\n-\n-    size_t cards_dirty() const { return _cards_dirty; }\n-    size_t cards_skipped() const { return _cards_skipped; }\n-  };\n-\n@@ -1272,8 +1159,0 @@\n-  \/\/ To mitigate contention due multiple threads accessing and popping BufferNodes from a shared\n-  \/\/ G1DirtyCardQueueSet, we implement a sequential distribution phase. Here, BufferNodes are\n-  \/\/ distributed to worker threads in a sequential manner utilizing the _dirty_card_buffers. By doing\n-  \/\/ so, we effectively alleviate the bottleneck encountered during pop operations on the\n-  \/\/ G1DirtyCardQueueSet. Importantly, this approach preserves the helping aspect among worker\n-  \/\/ threads, allowing them to assist one another in case of imbalances in work distribution.\n-  BufferNode::Stack* _dirty_card_buffers;\n-\n@@ -1284,11 +1163,0 @@\n-  void apply_closure_to_dirty_card_buffers(G1MergeLogBufferCardsClosure* cl, uint worker_id) {\n-    G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-    for (uint i = 0; i < _num_workers; i++) {\n-      uint index = (worker_id + i) % _num_workers;\n-      while (BufferNode* node = _dirty_card_buffers[index].pop()) {\n-        cl->apply_to_buffer(node, worker_id);\n-        dcqs.deallocate_buffer(node);\n-      }\n-    }\n-  }\n-\n@@ -1301,1 +1169,0 @@\n-    _dirty_card_buffers(nullptr),\n@@ -1304,50 +1171,1 @@\n-  {\n-    if (initial_evacuation) {\n-      Ticks start = Ticks::now();\n-\n-      _dirty_card_buffers = NEW_C_HEAP_ARRAY(BufferNode::Stack, num_workers, mtGC);\n-      for (uint i = 0; i < num_workers; i++) {\n-        new (&_dirty_card_buffers[i]) BufferNode::Stack();\n-      }\n-\n-      G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-      BufferNodeList buffers = dcqs.take_all_completed_buffers();\n-\n-      size_t entries_per_thread = ceil(buffers._entry_count \/ (double)num_workers);\n-\n-      BufferNode* head = buffers._head;\n-      BufferNode* tail = head;\n-\n-      uint worker = 0;\n-      while (tail != nullptr) {\n-        size_t count = tail->size();\n-        BufferNode* cur = tail->next();\n-\n-        while (count < entries_per_thread && cur != nullptr) {\n-          tail = cur;\n-          count += tail->size();\n-          cur = tail->next();\n-        }\n-\n-        tail->set_next(nullptr);\n-        _dirty_card_buffers[worker++ % num_workers].prepend(*head, *tail);\n-\n-        assert(cur != nullptr || tail == buffers._tail, \"Must be\");\n-        head = cur;\n-        tail = cur;\n-      }\n-\n-      Tickspan total = Ticks::now() - start;\n-      G1CollectedHeap::heap()->phase_times()->record_distribute_log_buffers_time_ms(total.seconds() * 1000.0);\n-    }\n-  }\n-\n-  ~G1MergeHeapRootsTask() {\n-    if (_dirty_card_buffers != nullptr) {\n-      using Stack = BufferNode::Stack;\n-      for (uint i = 0; i < _num_workers; i++) {\n-        _dirty_card_buffers[i].~Stack();\n-      }\n-      FREE_C_HEAP_ARRAY(Stack, _dirty_card_buffers);\n-    }\n-  }\n+  { }\n@@ -1406,1 +1224,1 @@\n-      G1ClearBitmapClosure clear(g1h, _scan_state);\n+      G1ClearBitmapClosure clear(g1h, _scan_state, _initial_evacuation);\n@@ -1409,12 +1227,0 @@\n-\n-    \/\/ Now apply the closure to all remaining log entries.\n-    if (_initial_evacuation) {\n-      assert(merge_remset_phase == G1GCPhaseTimes::MergeRS, \"Wrong merge phase\");\n-      G1GCParPhaseTimesTracker x(p, G1GCPhaseTimes::MergeLB, worker_id);\n-\n-      G1MergeLogBufferCardsClosure cl(g1h, _scan_state);\n-      apply_closure_to_dirty_card_buffers(&cl, worker_id);\n-\n-      p->record_thread_work_item(G1GCPhaseTimes::MergeLB, worker_id, cl.cards_dirty(), G1GCPhaseTimes::MergeLBDirtyCards);\n-      p->record_thread_work_item(G1GCPhaseTimes::MergeLB, worker_id, cl.cards_skipped(), G1GCPhaseTimes::MergeLBSkippedCards);\n-    }\n@@ -1424,6 +1230,2 @@\n-void G1RemSet::print_merge_heap_roots_stats() {\n-  LogTarget(Debug, gc, remset) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-\n-    size_t num_visited_cards = _scan_state->num_visited_cards();\n+static void merge_refinement_table() {\n+  G1CollectedHeap* g1h = G1CollectedHeap::heap();\n@@ -1431,1 +1233,2 @@\n-    size_t total_dirty_region_cards = _scan_state->num_cards_in_dirty_regions();\n+  G1ConcurrentRefineWorkState& state = g1h->concurrent_refine()->refine_state_for_merge();\n+  WorkerThreads* workers = g1h->workers();\n@@ -1433,11 +1236,3 @@\n-    G1CollectedHeap* g1h = G1CollectedHeap::heap();\n-    size_t total_old_region_cards =\n-      (g1h->num_regions() - (g1h->num_free_regions() - g1h->collection_set()->cur_length())) * G1HeapRegion::CardsPerRegion;\n-\n-    ls.print_cr(\"Visited cards %zu Total dirty %zu (%.2lf%%) Total old %zu (%.2lf%%)\",\n-                num_visited_cards,\n-                total_dirty_region_cards,\n-                percent_of(num_visited_cards, total_dirty_region_cards),\n-                total_old_region_cards,\n-                percent_of(num_visited_cards, total_old_region_cards));\n-  }\n+  MergeRefinementTableTask cl(state.sweep_table(), workers->active_workers());\n+  log_debug(gc, ergo)(\"Running %s using %u workers\", cl.name(), workers->active_workers());\n+  workers->run_task(&cl);\n@@ -1449,0 +1244,1 @@\n+  \/\/ 1. Prepare the merging process\n@@ -1462,2 +1258,5 @@\n-  WorkerThreads* workers = g1h->workers();\n-  size_t const increment_length = g1h->collection_set()->increment_length();\n+  \/\/ 2. (Optionally) Merge the refinement table into the card table (if needed).\n+\n+  G1ConcurrentRefineWorkState& state = g1h->concurrent_refine()->refine_state();\n+  if (initial_evacuation && state.is_in_progress()) {\n+    Ticks start = Ticks::now();\n@@ -1465,2 +1264,1 @@\n-  uint const num_workers = initial_evacuation ? workers->active_workers() :\n-                                                MIN2(workers->active_workers(), (uint)increment_length);\n+    merge_refinement_table();\n@@ -1468,0 +1266,4 @@\n+    g1h->phase_times()->record_merge_refinement_table_time((Ticks::now() - start).seconds() * MILLIUNITS);\n+  }\n+\n+  \/\/ 3. Merge other heap roots.\n@@ -1469,0 +1271,7 @@\n+    WorkerThreads* workers = g1h->workers();\n+\n+    size_t const increment_length = g1h->collection_set()->increment_length();\n+\n+    uint const num_workers = initial_evacuation ? workers->active_workers() :\n+                                                  MIN2(workers->active_workers(), (uint)increment_length);\n+\n@@ -1476,4 +1285,0 @@\n-    size_t young_rs_length = g1h->young_regions_cardset()->occupied();\n-    \/\/ We only use young_rs_length statistics to estimate young regions length.\n-    g1h->policy()->record_card_rs_length(young_rs_length);\n-\n@@ -1485,1 +1290,3 @@\n-  print_merge_heap_roots_stats();\n+  if (VerifyDuringGC && initial_evacuation) {\n+    g1h->verifier()->verify_refinement_table_clean();\n+  }\n@@ -1521,15 +1328,5 @@\n-bool G1RemSet::clean_card_before_refine(CardValue** const card_ptr_addr) {\n-  assert(!SafepointSynchronize::is_at_safepoint(), \"Only call concurrently\");\n-\n-  CardValue* card_ptr = *card_ptr_addr;\n-  \/\/ Find the start address represented by the card.\n-  HeapWord* start = _ct->addr_for(card_ptr);\n-  \/\/ And find the region containing it.\n-  G1HeapRegion* r = _g1h->heap_region_containing_or_null(start);\n-\n-  \/\/ If this is a (stale) card into an uncommitted region, exit.\n-  if (r == nullptr) {\n-    return false;\n-  }\n-\n-  check_card_ptr(card_ptr, _ct);\n+G1RemSet::RefineResult G1RemSet::refine_card_concurrently(CardValue* const card_ptr,\n+                                                          const uint worker_id) {\n+  assert(!_g1h->is_stw_gc_active(), \"Only call concurrently\");\n+  G1CardTable* ct = _g1h->refinement_table();\n+  check_card_ptr(card_ptr, ct);\n@@ -1537,5 +1334,4 @@\n-  \/\/ If the card is no longer dirty, nothing to do.\n-  \/\/ We cannot load the card value before the \"r == nullptr\" check above, because G1\n-  \/\/ could uncommit parts of the card table covering uncommitted regions.\n-  if (*card_ptr != G1CardTable::dirty_card_val()) {\n-    return false;\n+  \/\/ That card is already known to contain a reference to the collection set. Skip\n+  \/\/ further processing.\n+  if (*card_ptr == G1CardTable::g1_to_cset_card) {\n+    return AlreadyToCSet;\n@@ -1544,55 +1340,0 @@\n-  \/\/ This check is needed for some uncommon cases where we should\n-  \/\/ ignore the card.\n-  \/\/\n-  \/\/ The region could be young.  Cards for young regions are\n-  \/\/ distinctly marked (set to g1_young_gen), so the post-barrier will\n-  \/\/ filter them out.  However, that marking is performed\n-  \/\/ concurrently.  A write to a young object could occur before the\n-  \/\/ card has been marked young, slipping past the filter.\n-  \/\/\n-  \/\/ The card could be stale, because the region has been freed since\n-  \/\/ the card was recorded. In this case the region type could be\n-  \/\/ anything.  If (still) free or (reallocated) young, just ignore\n-  \/\/ it.  If (reallocated) old or humongous, the later card trimming\n-  \/\/ and additional checks in iteration may detect staleness.  At\n-  \/\/ worst, we end up processing a stale card unnecessarily.\n-  \/\/\n-  \/\/ In the normal (non-stale) case, the synchronization between the\n-  \/\/ enqueueing of the card and processing it here will have ensured\n-  \/\/ we see the up-to-date region type here.\n-  if (!r->is_old_or_humongous()) {\n-    return false;\n-  }\n-\n-  \/\/ Trim the region designated by the card to what's been allocated\n-  \/\/ in the region.  The card could be stale, or the card could cover\n-  \/\/ (part of) an object at the end of the allocated space and extend\n-  \/\/ beyond the end of allocation.\n-\n-  \/\/ Non-humongous objects are either allocated in the old regions during GC.\n-  \/\/ So if region is old then top is stable.\n-  \/\/ Humongous object allocation sets top last; if top has not yet been set,\n-  \/\/ this is a stale card and we'll end up with an empty intersection.\n-  \/\/ If this is not a stale card, the synchronization between the\n-  \/\/ enqueuing of the card and processing it here will have ensured\n-  \/\/ we see the up-to-date top here.\n-  HeapWord* scan_limit = r->top();\n-\n-  if (scan_limit <= start) {\n-    \/\/ If the trimmed region is empty, the card must be stale.\n-    return false;\n-  }\n-\n-  \/\/ Okay to clean and process the card now.  There are still some\n-  \/\/ stale card cases that may be detected by iteration and dealt with\n-  \/\/ as iteration failure.\n-  *const_cast<volatile CardValue*>(card_ptr) = G1CardTable::clean_card_val();\n-\n-  return true;\n-}\n-\n-void G1RemSet::refine_card_concurrently(CardValue* const card_ptr,\n-                                        const uint worker_id) {\n-  assert(!_g1h->is_stw_gc_active(), \"Only call concurrently\");\n-  check_card_ptr(card_ptr, _ct);\n-\n@@ -1600,1 +1341,1 @@\n-  HeapWord* start = _ct->addr_for(card_ptr);\n+  HeapWord* start = ct->addr_for(card_ptr);\n@@ -1610,1 +1351,1 @@\n-  assert(scan_limit > start, \"sanity\");\n+  assert(scan_limit > start, \"sanity region %u (%s) scan_limit \" PTR_FORMAT \" start \" PTR_FORMAT, r->hrm_index(), r->get_short_type_str(), p2i(scan_limit), p2i(start));\n@@ -1620,1 +1361,7 @@\n-    return;\n+    if (conc_refine_cl.has_to_cset_ref()) {\n+      return HasToCSetRef;\n+    } else if (conc_refine_cl.has_to_old_ref()) {\n+      return HasToOldRef;\n+    } else {\n+      return NoInteresting;\n+    }\n@@ -1622,1 +1369,0 @@\n-\n@@ -1627,1 +1373,1 @@\n-  \/\/ already marked the card cleaned, so taken responsibility for\n+  \/\/ already marked the card as cleaned, so taken responsibility for\n@@ -1629,28 +1375,1 @@\n-  \/\/\n-  \/\/ However, the card might have gotten re-dirtied and re-enqueued\n-  \/\/ while we worked.  (In fact, it's pretty likely.)\n-  if (*card_ptr == G1CardTable::dirty_card_val()) {\n-    return;\n-  }\n-\n-  enqueue_for_reprocessing(card_ptr);\n-}\n-\n-\/\/ Re-dirty and re-enqueue the card to retry refinement later.\n-\/\/ This is used to deal with a rare race condition in concurrent refinement.\n-void G1RemSet::enqueue_for_reprocessing(CardValue* card_ptr) {\n-  \/\/ We can't use the thread-local queue, because that might be the queue\n-  \/\/ that is being processed by us; we could be a Java thread conscripted to\n-  \/\/ perform refinement on our queue's current buffer.  This situation only\n-  \/\/ arises from rare race condition, so it's not worth any significant\n-  \/\/ development effort or clever lock-free queue implementation.  Instead\n-  \/\/ we use brute force, allocating and enqueuing an entire buffer for just\n-  \/\/ this card.  Since buffers are processed in FIFO order and we try to\n-  \/\/ keep some in the queue, it is likely that the racing state will have\n-  \/\/ resolved by the time this card comes up for reprocessing.\n-  *card_ptr = G1CardTable::dirty_card_val();\n-  G1DirtyCardQueueSet& dcqs = G1BarrierSet::dirty_card_queue_set();\n-  void** buffer = dcqs.allocate_buffer();\n-  size_t index = dcqs.buffer_capacity() - 1;\n-  buffer[index] = card_ptr;\n-  dcqs.enqueue_completed_buffer(BufferNode::make_node_from_buffer(buffer, index));\n+  return CouldNotParse;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.cpp","additions":251,"deletions":532,"binary":false,"changes":783,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/g1\/g1CardTableClaimTable.hpp\"\n@@ -43,0 +44,1 @@\n+class G1RemSetScanState;\n@@ -68,4 +70,1 @@\n-  G1CardTable*           _ct;\n-  G1Policy*              _g1p;\n-\n-  void print_merge_heap_roots_stats();\n+  G1Policy* _g1p;\n@@ -75,2 +74,0 @@\n-  void enqueue_for_reprocessing(CardValue* card_ptr);\n-\n@@ -81,1 +78,1 @@\n-  G1RemSet(G1CollectedHeap* g1h, G1CardTable* ct);\n+  G1RemSet(G1CollectedHeap* g1h);\n@@ -104,1 +101,1 @@\n-  \/\/ Creates a task for cleaining up temporary data structures and the\n+  \/\/ Creates a task for cleaning up temporary data structures and the\n@@ -125,5 +122,7 @@\n-  \/\/ Two methods for concurrent refinement support, executed concurrently to\n-  \/\/ the mutator:\n-  \/\/ Cleans the card at \"*card_ptr_addr\" before refinement, returns true iff the\n-  \/\/ card needs later refinement.\n-  bool clean_card_before_refine(CardValue** const card_ptr_addr);\n+  enum RefineResult {\n+      HasToCSetRef,          \/\/ The (dirty) card has a reference to the collection set.\n+      AlreadyToCSet,         \/\/ The card is already one marked as having a reference to the collection set.\n+      HasToOldRef,           \/\/ The dirty cards contains references to other old regions (not the collection set).\n+      NoInteresting,         \/\/ There is no interesting reference in the card.\n+      CouldNotParse          \/\/ The card is unparsable, need to retry later.\n+  };\n@@ -133,2 +132,2 @@\n-  void refine_card_concurrently(CardValue* const card_ptr,\n-                                const uint worker_id);\n+  RefineResult refine_card_concurrently(CardValue* const card_ptr,\n+                                        const uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSet.hpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -69,1 +68,4 @@\n-  _num_vtimes(G1ConcRefinementThreads),\n+   \/\/ worker threads plus refinement control thread; this is not completely correct if\n+   \/\/ G1ConcRefinementThreads == 0 but in that case we do not care about the extra\n+   \/\/ entry.\n+  _num_vtimes(G1ConcRefinementThreads + 1),\n","filename":"src\/hotspot\/share\/gc\/g1\/g1RemSetSummary.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-  for (size_t i = 0; i < _stats_arrays_length; ++i) {\n+  for (uint i = 0; i < _stats_arrays_length; ++i) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1SurvRateGroup.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,2 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n+#include \"gc\/g1\/g1CardTable.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n@@ -39,1 +40,1 @@\n-  G1DirtyCardQueue _dirty_card_queue;\n+  G1CardTable::CardValue* _byte_map_base;\n@@ -48,2 +49,2 @@\n-      _dirty_card_queue(&G1BarrierSet::dirty_card_queue_set()),\n-      _pin_cache() {}\n+      _byte_map_base(nullptr),\n+      _pin_cache() { }\n@@ -60,4 +61,0 @@\n-  static ByteSize dirty_card_queue_offset() {\n-    return Thread::gc_data_offset() + byte_offset_of(G1ThreadLocalData, _dirty_card_queue);\n-  }\n-\n@@ -77,4 +74,0 @@\n-  static G1DirtyCardQueue& dirty_card_queue(Thread* thread) {\n-    return data(thread)->_dirty_card_queue;\n-  }\n-\n@@ -93,2 +86,2 @@\n-  static ByteSize dirty_card_queue_index_offset() {\n-    return dirty_card_queue_offset() + G1DirtyCardQueue::byte_offset_of_index();\n+  static ByteSize card_table_base_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(G1ThreadLocalData, _byte_map_base);\n@@ -97,2 +90,2 @@\n-  static ByteSize dirty_card_queue_buffer_offset() {\n-    return dirty_card_queue_offset() + G1DirtyCardQueue::byte_offset_of_buf();\n+  static void set_byte_map_base(Thread* thread, G1CardTable::CardValue* new_byte_map_base) {\n+    data(thread)->_byte_map_base = new_byte_map_base;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ThreadLocalData.hpp","additions":10,"deletions":17,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n@@ -894,7 +893,2 @@\n-  \/\/ Special closure for enqueuing discovered fields: during enqueue the card table\n-  \/\/ may not be in shape to properly handle normal barrier calls (e.g. card marks\n-  \/\/ in regions that failed evacuation, scribbling of various values by card table\n-  \/\/ scan code). Additionally the regular barrier enqueues into the \"global\"\n-  \/\/ DCQS, but during GC we need these to-be-refined entries in the GC local queue\n-  \/\/ so that after clearing the card table, the redirty cards phase will properly\n-  \/\/ mark all dirty cards to be picked up by refinement.\n+  \/\/ G1 specific closure for marking discovered fields. Need to mark the card in the\n+  \/\/ refinement table as the card table is in use by garbage collection.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.cpp","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,0 @@\n-class G1RedirtyCardsQueueSet;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungCollector.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -290,1 +290,1 @@\n-    _num_chunks_per_region = G1CollectedHeap::get_chunks_per_region();\n+    _num_chunks_per_region = G1CollectedHeap::get_chunks_per_region_for_scan();\n@@ -303,1 +303,1 @@\n-    double workers_per_region = (double)G1CollectedHeap::get_chunks_per_region() \/ G1RestoreRetainedRegionChunksPerWorker;\n+    double workers_per_region = (double)G1CollectedHeap::get_chunks_per_region_for_scan() \/ G1RestoreRetainedRegionChunksPerWorker;\n@@ -483,37 +483,0 @@\n-class RedirtyLoggedCardTableEntryClosure : public G1CardTableEntryClosure {\n-  size_t _num_dirtied;\n-  G1CollectedHeap* _g1h;\n-  G1CardTable* _g1_ct;\n-  G1EvacFailureRegions* _evac_failure_regions;\n-\n-  G1HeapRegion* region_for_card(CardValue* card_ptr) const {\n-    return _g1h->heap_region_containing(_g1_ct->addr_for(card_ptr));\n-  }\n-\n-  bool will_become_free(G1HeapRegion* hr) const {\n-    \/\/ A region will be freed by during the FreeCollectionSet phase if the region is in the\n-    \/\/ collection set and has not had an evacuation failure.\n-    return _g1h->is_in_cset(hr) && !_evac_failure_regions->contains(hr->hrm_index());\n-  }\n-\n-public:\n-  RedirtyLoggedCardTableEntryClosure(G1CollectedHeap* g1h, G1EvacFailureRegions* evac_failure_regions) :\n-    G1CardTableEntryClosure(),\n-    _num_dirtied(0),\n-    _g1h(g1h),\n-    _g1_ct(g1h->card_table()),\n-    _evac_failure_regions(evac_failure_regions) { }\n-\n-  void do_card_ptr(CardValue* card_ptr) override {\n-    G1HeapRegion* hr = region_for_card(card_ptr);\n-\n-    \/\/ Should only dirty cards in regions that won't be freed.\n-    if (!will_become_free(hr)) {\n-      *card_ptr = G1CardTable::dirty_card_val();\n-      _num_dirtied++;\n-    }\n-  }\n-\n-  size_t num_dirtied()   const { return _num_dirtied; }\n-};\n-\n@@ -575,42 +538,0 @@\n-class G1PostEvacuateCollectionSetCleanupTask2::RedirtyLoggedCardsTask : public G1AbstractSubTask {\n-  BufferNodeList* _rdc_buffers;\n-  uint _num_buffer_lists;\n-  G1EvacFailureRegions* _evac_failure_regions;\n-\n-public:\n-  RedirtyLoggedCardsTask(G1EvacFailureRegions* evac_failure_regions, BufferNodeList* rdc_buffers, uint num_buffer_lists) :\n-    G1AbstractSubTask(G1GCPhaseTimes::RedirtyCards),\n-    _rdc_buffers(rdc_buffers),\n-    _num_buffer_lists(num_buffer_lists),\n-    _evac_failure_regions(evac_failure_regions) { }\n-\n-  double worker_cost() const override {\n-    \/\/ Needs more investigation.\n-    return G1CollectedHeap::heap()->workers()->active_workers();\n-  }\n-\n-  void do_work(uint worker_id) override {\n-    RedirtyLoggedCardTableEntryClosure cl(G1CollectedHeap::heap(), _evac_failure_regions);\n-\n-    uint start = worker_id;\n-    for (uint i = 0; i < _num_buffer_lists; i++) {\n-      uint index = (start + i) % _num_buffer_lists;\n-\n-      BufferNode* next = Atomic::load(&_rdc_buffers[index]._head);\n-      BufferNode* tail = Atomic::load(&_rdc_buffers[index]._tail);\n-\n-      while (next != nullptr) {\n-        BufferNode* node = next;\n-        next = Atomic::cmpxchg(&_rdc_buffers[index]._head, node, (node != tail ) ? node->next() : nullptr);\n-        if (next == node) {\n-          cl.apply_to_buffer(node, worker_id);\n-          next = (node != tail ) ? node->next() : nullptr;\n-        } else {\n-          break; \/\/ If there is contention, move to the next BufferNodeList\n-        }\n-      }\n-    }\n-    record_work_item(worker_id, 0, cl.num_dirtied());\n-  }\n-};\n-\n@@ -800,1 +721,0 @@\n-\n@@ -911,1 +831,1 @@\n-class G1PostEvacuateCollectionSetCleanupTask2::ResizeTLABsTask : public G1AbstractSubTask {\n+class G1PostEvacuateCollectionSetCleanupTask2::ResizeTLABsAndSwapCardTableTask : public G1AbstractSubTask {\n@@ -913,0 +833,1 @@\n+  volatile bool _non_java_threads_claim;\n@@ -918,1 +839,5 @@\n-  ResizeTLABsTask() : G1AbstractSubTask(G1GCPhaseTimes::ResizeThreadLABs), _claimer(ThreadsPerWorker) { }\n+  ResizeTLABsAndSwapCardTableTask()\n+    : G1AbstractSubTask(G1GCPhaseTimes::ResizeThreadLABs), _claimer(ThreadsPerWorker), _non_java_threads_claim(false)\n+  {\n+    G1BarrierSet::g1_barrier_set()->swap_global_card_table();\n+  }\n@@ -921,1 +846,1 @@\n-    class ResizeClosure : public ThreadClosure {\n+    class SwapCardTableClosure : public ThreadClosure {\n@@ -923,0 +848,15 @@\n+      void do_thread(Thread* thread) {\n+        \/\/ The global card table references have already been swapped.\n+        G1CardTable::CardValue* new_card_table_base = G1CollectedHeap::heap()->card_table_base();\n+        G1ThreadLocalData::set_byte_map_base(thread, new_card_table_base);\n+      }\n+    } swap_cl;\n+\n+    \/\/ We do not expect too many non-Java threads compared to Java threads, so just\n+    \/\/ let one worker claim that work.\n+    if (!_non_java_threads_claim && !Atomic::cmpxchg(&_non_java_threads_claim, false, true, memory_order_relaxed)) {\n+      Threads::non_java_threads_do(&swap_cl);\n+    }\n+\n+    class ResizeAndSwapCardTableClosure : public ThreadClosure {\n+    SwapCardTableClosure _cl;\n@@ -924,0 +864,1 @@\n+    public:\n@@ -925,1 +866,4 @@\n-        static_cast<JavaThread*>(thread)->tlab().resize();\n+        if (UseTLAB && ResizeTLAB) {\n+          static_cast<JavaThread*>(thread)->tlab().resize();\n+        }\n+        _cl.do_thread(thread);\n@@ -927,2 +871,3 @@\n-    } cl;\n-    _claimer.apply(&cl);\n+    } resize_and_swap_cl;\n+\n+    _claimer.apply(&resize_and_swap_cl);\n@@ -971,3 +916,0 @@\n-  add_parallel_task(new RedirtyLoggedCardsTask(evac_failure_regions,\n-                                               per_thread_states->rdc_buffers(),\n-                                               per_thread_states->num_workers()));\n@@ -975,3 +917,1 @@\n-  if (UseTLAB && ResizeTLAB) {\n-    add_parallel_task(new ResizeTLABsTask());\n-  }\n+  add_parallel_task(new ResizeTLABsAndSwapCardTableTask());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.cpp","additions":34,"deletions":94,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,1 +58,0 @@\n-\/\/ - Redirty Logged Cards\n@@ -60,1 +59,1 @@\n-\/\/ - Resize TLABs\n+\/\/ - Resize TLABs and Swap Card Table\n@@ -69,1 +68,0 @@\n-  class RedirtyLoggedCardsTask;\n@@ -71,1 +69,1 @@\n-  class ResizeTLABsTask;\n+  class ResizeTLABsAndSwapCardTableTask;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPostEvacuateTasks.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -38,1 +37,1 @@\n-class G1PreEvacuateCollectionSetBatchTask::JavaThreadRetireTLABAndFlushLogs : public G1AbstractSubTask {\n+class G1PreEvacuateCollectionSetBatchTask::JavaThreadRetireTLABs : public G1AbstractSubTask {\n@@ -43,1 +42,0 @@\n-  G1ConcurrentRefineStats* _local_refinement_stats;\n@@ -50,1 +48,1 @@\n-  struct RetireTLABAndFlushLogsClosure : public ThreadClosure {\n+  struct RetireTLABClosure : public ThreadClosure {\n@@ -52,1 +50,0 @@\n-    G1ConcurrentRefineStats _refinement_stats;\n@@ -54,1 +51,1 @@\n-    RetireTLABAndFlushLogsClosure() : _tlab_stats(), _refinement_stats() { }\n+    RetireTLABClosure() : _tlab_stats() { }\n@@ -64,3 +61,0 @@\n-      \/\/ Concatenate logs.\n-      G1DirtyCardQueueSet& qset = G1BarrierSet::dirty_card_queue_set();\n-      _refinement_stats += qset.concatenate_log_and_stats(thread);\n@@ -73,2 +67,2 @@\n-  JavaThreadRetireTLABAndFlushLogs() :\n-    G1AbstractSubTask(G1GCPhaseTimes::RetireTLABsAndFlushLogs),\n+  JavaThreadRetireTLABs() :\n+    G1AbstractSubTask(G1GCPhaseTimes::RetireTLABs),\n@@ -77,1 +71,0 @@\n-    _local_refinement_stats(nullptr),\n@@ -81,4 +74,1 @@\n-  ~JavaThreadRetireTLABAndFlushLogs() {\n-    static_assert(std::is_trivially_destructible<G1ConcurrentRefineStats>::value, \"must be\");\n-    FREE_C_HEAP_ARRAY(G1ConcurrentRefineStats, _local_refinement_stats);\n-\n+  ~JavaThreadRetireTLABs() {\n@@ -90,1 +80,1 @@\n-    RetireTLABAndFlushLogsClosure tc;\n+    RetireTLABClosure tc;\n@@ -94,1 +84,0 @@\n-    _local_refinement_stats[worker_id] = tc._refinement_stats;\n@@ -104,1 +93,0 @@\n-    _local_refinement_stats = NEW_C_HEAP_ARRAY(G1ConcurrentRefineStats, _num_workers, mtGC);\n@@ -108,1 +96,0 @@\n-      ::new (&_local_refinement_stats[i]) G1ConcurrentRefineStats();\n@@ -119,36 +106,0 @@\n-\n-  G1ConcurrentRefineStats refinement_stats() const {\n-    G1ConcurrentRefineStats result;\n-    for (uint i = 0; i < _num_workers; i++) {\n-      result += _local_refinement_stats[i];\n-    }\n-    return result;\n-  }\n-};\n-\n-class G1PreEvacuateCollectionSetBatchTask::NonJavaThreadFlushLogs : public G1AbstractSubTask {\n-  struct FlushLogsClosure : public ThreadClosure {\n-    G1ConcurrentRefineStats _refinement_stats;\n-\n-    FlushLogsClosure() : _refinement_stats() { }\n-\n-    void do_thread(Thread* thread) override {\n-      G1DirtyCardQueueSet& qset = G1BarrierSet::dirty_card_queue_set();\n-      _refinement_stats += qset.concatenate_log_and_stats(thread);\n-\n-      assert(G1ThreadLocalData::pin_count_cache(thread).count() == 0, \"NonJava thread has pinned Java objects\");\n-    }\n-  } _tc;\n-\n-public:\n-  NonJavaThreadFlushLogs() : G1AbstractSubTask(G1GCPhaseTimes::NonJavaThreadFlushLogs), _tc() { }\n-\n-  void do_work(uint worker_id) override {\n-    Threads::non_java_threads_do(&_tc);\n-  }\n-\n-  double worker_cost() const override {\n-    return 1.0;\n-  }\n-\n-  G1ConcurrentRefineStats refinement_stats() const { return _tc._refinement_stats; }\n@@ -159,3 +110,1 @@\n-  _old_pending_cards(G1BarrierSet::dirty_card_queue_set().num_cards()),\n-  _java_retire_task(new JavaThreadRetireTLABAndFlushLogs()),\n-  _non_java_retire_task(new NonJavaThreadFlushLogs()) {\n+  _java_retire_task(new JavaThreadRetireTLABs()) {\n@@ -163,4 +112,0 @@\n-  \/\/ Disable mutator refinement until concurrent refinement decides otherwise.\n-  G1BarrierSet::dirty_card_queue_set().set_mutator_refinement_threshold(SIZE_MAX);\n-\n-  add_serial_task(_non_java_retire_task);\n@@ -170,15 +115,0 @@\n-static void verify_empty_dirty_card_logs() {\n-#ifdef ASSERT\n-  ResourceMark rm;\n-\n-  struct Verifier : public ThreadClosure {\n-    Verifier() {}\n-    void do_thread(Thread* t) override {\n-      G1DirtyCardQueue& queue = G1ThreadLocalData::dirty_card_queue(t);\n-      assert(queue.is_empty(), \"non-empty dirty card queue for thread %s\", t->name());\n-    }\n-  } verifier;\n-  Threads::threads_do(&verifier);\n-#endif\n-}\n-\n@@ -187,13 +117,0 @@\n-\n-  G1DirtyCardQueueSet& qset = G1BarrierSet::dirty_card_queue_set();\n-\n-  G1ConcurrentRefineStats total_refinement_stats;\n-  total_refinement_stats += _java_retire_task->refinement_stats();\n-  total_refinement_stats += _non_java_retire_task->refinement_stats();\n-  qset.update_refinement_stats(total_refinement_stats);\n-\n-  verify_empty_dirty_card_logs();\n-\n-  size_t pending_cards = qset.num_cards();\n-  size_t thread_buffer_cards = pending_cards - _old_pending_cards;\n-  G1CollectedHeap::heap()->policy()->record_concurrent_refinement_stats(pending_cards, thread_buffer_cards);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPreEvacuateTasks.cpp","additions":8,"deletions":91,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-\/\/ - Retire TLAB and Flush Logs (Java threads)\n+\/\/ - Retire TLABs (Java threads)\n@@ -33,1 +33,0 @@\n-\/\/ - Flush Logs (s) (Non-Java threads)\n@@ -35,4 +34,1 @@\n-  class JavaThreadRetireTLABAndFlushLogs;\n-  class NonJavaThreadFlushLogs;\n-\n-  size_t _old_pending_cards;\n+  class JavaThreadRetireTLABs;\n@@ -41,2 +37,1 @@\n-  JavaThreadRetireTLABAndFlushLogs* _java_retire_task;\n-  NonJavaThreadFlushLogs* _non_java_retire_task;\n+  JavaThreadRetireTLABs* _java_retire_task;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1YoungGCPreEvacuateTasks.hpp","additions":4,"deletions":9,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,2 +85,1 @@\n-  declare_constant(BarrierSet::G1BarrierSet)                                  \\\n-  declare_constant(G1CardTable::g1_young_gen)\n+  declare_constant(BarrierSet::G1BarrierSet)\n@@ -103,1 +102,0 @@\n-  declare_toplevel_type(G1DirtyCardQueue)                                     \\\n","filename":"src\/hotspot\/share\/gc\/g1\/vmStructs_g1.hpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -231,0 +231,3 @@\n+  if (mr.is_empty()) {\n+    return;\n+  }\n@@ -261,2 +264,3 @@\n-void CardTable::print_on(outputStream* st) const {\n-  st->print_cr(\"Card table byte_map: [\" PTR_FORMAT \",\" PTR_FORMAT \"] _byte_map_base: \" PTR_FORMAT,\n+void CardTable::print_on(outputStream* st, const char* description) const {\n+  st->print_cr(\"%s table byte_map: [\" PTR_FORMAT \",\" PTR_FORMAT \"] _byte_map_base: \" PTR_FORMAT,\n+               description,\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -206,2 +206,2 @@\n-  \/\/ Print a description of the memory for the card table\n-  virtual void print_on(outputStream* st) const;\n+  \/\/ Print card table information.\n+  void print_on(outputStream* st, const char* description = \"Card\") const;\n@@ -211,1 +211,1 @@\n-  void verify_region(MemRegion mr, CardValue val, bool val_equals) PRODUCT_RETURN;\n+  virtual void verify_region(MemRegion mr, CardValue val, bool val_equals) PRODUCT_RETURN;\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-  static const uint MaxThreadWorkItems = 9;\n+  static const uint MaxThreadWorkItems = 10;\n","filename":"src\/hotspot\/share\/gc\/shared\/workerDataArray.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -592,4 +592,0 @@\n-void JVMCIRuntime::write_barrier_post(JavaThread* thread, volatile CardValue* card_addr) {\n-  G1BarrierSetRuntime::write_ref_field_post_entry(card_addr, thread);\n-}\n-\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -537,0 +537,1 @@\n+  declare_constant_with_value(\"CardTable::clean_card\", CardTable::clean_card_val()) \\\n@@ -880,1 +881,0 @@\n-  G1GC_ONLY(declare_function(JVMCIRuntime::write_barrier_post))           \\\n@@ -892,1 +892,0 @@\n-  declare_constant_with_value(\"G1CardTable::g1_young_gen\", G1CardTable::g1_young_card_val()) \\\n@@ -896,2 +895,1 @@\n-  declare_constant_with_value(\"G1ThreadLocalData::dirty_card_queue_index_offset\", in_bytes(G1ThreadLocalData::dirty_card_queue_index_offset())) \\\n-  declare_constant_with_value(\"G1ThreadLocalData::dirty_card_queue_buffer_offset\", in_bytes(G1ThreadLocalData::dirty_card_queue_buffer_offset()))\n+  declare_constant_with_value(\"G1ThreadLocalData::card_table_base_offset\", in_bytes(G1ThreadLocalData::card_table_base_offset())) \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -90,1 +90,10 @@\n-    klass()->oop_print_value_on(obj, st);\n+    Klass* k = klass_without_asserts();\n+    if (k == nullptr) {\n+      st->print(\"null klass\");\n+    } else if (!Metaspace::contains(k)) {\n+      st->print(\"klass not in Metaspace\");\n+    } else if (!k->is_klass()) {\n+      st->print(\"klass not a Klass\");\n+    } else {\n+      k->oop_print_value_on(obj, st);\n+    }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -39,0 +39,2 @@\n+    case CPUTimeType::gc_conc_refine_control:\n+      return \"gc_conc_refine_control\";\n@@ -56,0 +58,1 @@\n+    case CPUTimeType::gc_conc_refine_control:\n","filename":"src\/hotspot\/share\/runtime\/cpuTimeCounters.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+    gc_conc_refine_control,\n","filename":"src\/hotspot\/share\/runtime\/cpuTimeCounters.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,0 +62,1 @@\n+  template(G1RendezvousGCThreads)                 \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -111,2 +111,1 @@\n-        new LogMessageWithLevel(\"JT Retire TLABs And Flush Logs \\\\(ms\\\\):\", Level.DEBUG),\n-        new LogMessageWithLevel(\"Non-JT Flush Logs \\\\(ms\\\\):\", Level.DEBUG),\n+        new LogMessageWithLevel(\"JavaThread Retire TLABs \\\\(ms\\\\):\", Level.DEBUG),\n@@ -129,4 +128,5 @@\n-        new LogMessageWithLevel(\"Log Buffers \\\\(ms\\\\):\", Level.DEBUG),\n-        new LogMessageWithLevel(\"Dirty Cards:\", Level.DEBUG),\n-        new LogMessageWithLevel(\"Merged Cards:\", Level.DEBUG),\n-        new LogMessageWithLevel(\"Skipped Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Merged From RS Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Total Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Merge Refinement Table:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Sweep \\\\(ms\\\\):\", Level.DEBUG),\n+\n@@ -176,1 +176,4 @@\n-        new LogMessageWithLevel(\"Evac Fail Extra Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Pending Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"To-Young-Gen Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Evac-Fail Cards:\", Level.DEBUG),\n+        new LogMessageWithLevel(\"Marked Cards:\", Level.DEBUG),\n@@ -183,2 +186,0 @@\n-        new LogMessageWithLevel(\"Redirty Logged Cards \\\\(ms\\\\):\", Level.DEBUG),\n-        new LogMessageWithLevel(\"Redirtied Cards:\", Level.DEBUG),\n@@ -246,3 +247,1 @@\n-        new LogMessageWithLevel(\"Mutator refinement: \", Level.DEBUG),\n-        new LogMessageWithLevel(\"Concurrent refinement: \", Level.DEBUG),\n-        new LogMessageWithLevel(\"Total refinement: \", Level.DEBUG),\n+        new LogMessageWithLevel(\"Refinement: sweep: \", Level.DEBUG),\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestGCLogMessages.java","additions":11,"deletions":12,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -90,2 +90,0 @@\n-            \"RetireTLABsAndFlushLogs\",\n-            \"NonJavaThreadFlushLogs\",\n@@ -103,1 +101,0 @@\n-            \"MergeLB\",\n@@ -108,1 +105,0 @@\n-            \"RedirtyCards\",\n@@ -122,1 +118,2 @@\n-            \"NoteStartOfMark\"\n+            \"NoteStartOfMark\",\n+            \"RetireTLABs\"\n@@ -128,0 +125,2 @@\n+            \/\/ Does not always occur\n+            \"SweepRT\",\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/collection\/TestG1ParallelPhases.java","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"}]}