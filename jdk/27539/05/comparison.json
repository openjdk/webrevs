{"files":[{"patch":"@@ -248,1 +248,1 @@\n-  switch (AtomicAccess::load(&_dead_state)) {\n+  switch (_dead_state.load_relaxed()) {\n@@ -250,1 +250,1 @@\n-    AtomicAccess::store(&_dead_count, num_dead);\n+    _dead_count.store_relaxed(num_dead);\n@@ -256,2 +256,2 @@\n-    AtomicAccess::store(&_dead_count, num_dead);\n-    AtomicAccess::release_store(&_dead_state, DeadState::good);\n+    _dead_count.store_relaxed(num_dead);\n+    _dead_state.release_store(DeadState::good);\n@@ -261,1 +261,1 @@\n-    AtomicAccess::release_store(&_dead_state, DeadState::wait1);\n+    _dead_state.release_store(DeadState::wait1);\n@@ -426,2 +426,4 @@\n-volatile size_t StringDedup::Table::_dead_count = 0;\n-volatile StringDedup::Table::DeadState StringDedup::Table::_dead_state = DeadState::good;\n+Atomic<size_t> StringDedup::Table::_dead_count{};\n+\n+Atomic<StringDedup::Table::DeadState>\n+StringDedup::Table::_dead_state{DeadState::good};\n@@ -480,1 +482,1 @@\n-  return AtomicAccess::load_acquire(&_dead_state) == DeadState::good;\n+  return _dead_state.load_acquire() == DeadState::good;\n@@ -486,1 +488,1 @@\n-         ((_number_of_entries - AtomicAccess::load(&_dead_count)) > _grow_threshold);\n+         ((_number_of_entries - _dead_count.load_relaxed()) > _grow_threshold);\n@@ -492,1 +494,1 @@\n-         Config::should_cleanup_table(_number_of_entries, AtomicAccess::load(&_dead_count));\n+         Config::should_cleanup_table(_number_of_entries, _dead_count.load_relaxed());\n@@ -654,1 +656,1 @@\n-  size_t dead_count = AtomicAccess::load(&_dead_count);\n+  size_t dead_count = _dead_count.load_relaxed();\n@@ -678,2 +680,2 @@\n-  AtomicAccess::store(&_dead_count, size_t(0));\n-  AtomicAccess::store(&_dead_state, DeadState::cleaning);\n+  _dead_count.store_relaxed(0);\n+  _dead_state.store_relaxed(DeadState::cleaning);\n@@ -713,1 +715,1 @@\n-  AtomicAccess::store(&_dead_state, DeadState::wait2);\n+  _dead_state.store_relaxed(DeadState::wait2);\n@@ -735,2 +737,2 @@\n-    dead_count = _dead_count;\n-    dead_state = static_cast<int>(_dead_state);\n+    dead_count = _dead_count.load_relaxed();\n+    dead_state = static_cast<int>(_dead_state.load_relaxed());\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupTable.cpp","additions":18,"deletions":16,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -89,3 +90,3 @@\n-  \/\/ read by the dedup thread without holding the lock lock.\n-  static volatile size_t _dead_count;\n-  static volatile DeadState _dead_state;\n+  \/\/ read by the dedup thread without holding the lock.\n+  static Atomic<size_t> _dead_count;\n+  static Atomic<DeadState> _dead_state;\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupTable.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,553 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_ATOMIC_HPP\n+#define SHARE_RUNTIME_ATOMIC_HPP\n+\n+#include \"cppstdlib\/type_traits.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Atomic<T> is used to declare a variable of type T with atomic access.\n+\/\/\n+\/\/ The following value types T are supported:\n+\/\/\n+\/\/ (1) Integers with sizeof the same as sizeof int32_t or int64_t. These are\n+\/\/ referred to as atomic integers below.\n+\/\/\n+\/\/ (2) Integers with sizeof 1, including bool. These are referred to as atomic\n+\/\/ bytes below.\n+\/\/\n+\/\/ (3) Pointers. These are referred to as atomic pointers below.\n+\/\/\n+\/\/ (4) Types with a PrimitiveValues::Translate definition. These are referred\n+\/\/ to as atomic translated types below. The atomic value for the associated\n+\/\/ decayed type is referred to as the atomic decayed type.\n+\/\/\n+\/\/ The interface provided by an Atomic<T> depends on the value type.\n+\/\/\n+\/\/ If T is the value type, v is an Atomic<T>, x and y are instances of T, i is\n+\/\/ an integer, and o is an atomic_memory_order, then:\n+\/\/\n+\/\/ (1) All Atomic types provide\n+\/\/\n+\/\/   nested types:\n+\/\/     ValueType -> T\n+\/\/\n+\/\/   special functions:\n+\/\/     explicit constructor(T)\n+\/\/     noncopyable\n+\/\/     destructor\n+\/\/\n+\/\/   static member functions:\n+\/\/     value_offset_in_bytes() -> int   \/\/ constexpr\n+\/\/     value_size_in_bytes() -> int     \/\/ constexpr\n+\/\/       These provide the compiler and the like with direct access to the\n+\/\/       value field. They shouldn't be used directly to bypass normal access.\n+\/\/\n+\/\/   member functions:\n+\/\/     v.load_relaxed() -> T\n+\/\/     v.load_acquire() -> T\n+\/\/     v.store_relaxed(x) -> void\n+\/\/     v.release_store(x) -> void\n+\/\/     v.release_store_fence(x) -> void\n+\/\/     v.compare_exchange(x, y [, o]) -> T\n+\/\/\n+\/\/ (2) All atomic types are default constructible.\n+\/\/\n+\/\/ Default construction of an atomic integer or atomic byte initializes the\n+\/\/ value to zero. Default construction of an atomic pointer initializes the\n+\/\/ value to null.\n+\/\/\n+\/\/ If the value type of an atomic translated type is default constructible,\n+\/\/ then default construction of the atomic translated type will initialize the\n+\/\/ value to a default constructed object of the value type. Otherwise, the\n+\/\/ value will be initialized as if by translating the value that would be\n+\/\/ provided by default constructing an atomic type for the value type's\n+\/\/ decayed type.\n+\n+\/\/ (3) Atomic pointers and atomic integers additionally provide\n+\/\/\n+\/\/   member functions:\n+\/\/     v.exchange(x [, o]) -> T\n+\/\/     v.add_then_fetch(i [, o]) -> T\n+\/\/     v.sub_then_fetch(i [, o]) -> T\n+\/\/     v.fetch_then_add(i [, o]) -> T\n+\/\/     v.fetch_then_sub(i [, o]) -> T\n+\/\/\n+\/\/ sizeof(i) must not exceed sizeof(T). For atomic integers, both T and the\n+\/\/ type of i must be signed, or both must be unsigned. Atomic pointers perform\n+\/\/ element arithmetic.\n+\/\/\n+\/\/ (4) An atomic translated type additionally provides the exchange\n+\/\/ function if its associated atomic decayed type provides that function.\n+\/\/\n+\/\/ (5) Atomic integers additionally provide\n+\/\/\n+\/\/   member functions:\n+\/\/     v.and_then_fetch(x [, o]) -> T\n+\/\/     v.or_then_fetch(x [, o]) -> T\n+\/\/     v.xor_then_fetch(x [, o]) -> T\n+\/\/     v.fetch_then_and(x [, o]) -> T\n+\/\/     v.fetch_then_or(x [, o]) -> T\n+\/\/     v.fetch_then_xor(x [, o]) -> T\n+\/\/\n+\/\/ (6) Atomic pointers additionally provide\n+\/\/\n+\/\/   nested types:\n+\/\/     ElementType -> std::remove_pointer_t<T>\n+\/\/\n+\/\/ Some of the function names provided by (some variants of) Atomic<T> differ\n+\/\/ from the corresponding functions provided by the AtomicAccess class. In\n+\/\/ some cases this is done for regularity; there are some inconsistencies in\n+\/\/ the AtomicAccess names. Some of the naming choices are also to make them\n+\/\/ stand out a little more when used in surrounding non-atomic code. Without\n+\/\/ the \"AtomicAccess::\" qualifier, some of those names are easily overlooked.\n+\/\/\n+\/\/ Atomic bytes don't provide exchange(). This is because that operation\n+\/\/ hasn't been implemented for 1 byte values. That could be changed if needed.\n+\/\/\n+\/\/ Atomic for 2 byte integers is not supported. This is because atomic\n+\/\/ operations of that size have not been implemented. There haven't been\n+\/\/ required use-cases. Many platforms don't provide hardware support.\n+\/\/\n+\/\/ Atomic translated types don't provide the full interface of the associated\n+\/\/ atomic decayed type. They could do so, perhaps under the control of an\n+\/\/ associated type trait.\n+\/\/\n+\/\/ Atomic<T> is not intended to be anything approaching a drop-in replacement\n+\/\/ for std::atomic<T>. Rather, it's wrapping up a long-established HotSpot\n+\/\/ idiom in a tidier and more rigorous package. Some of the differences from\n+\/\/ std::atomic<T> include\n+\/\/\n+\/\/ * Atomic<T> supports a much more limited set of value types.\n+\/\/\n+\/\/ * All supported Atomic<T> types are \"lock free\", so the standard mechanisms\n+\/\/ for testing for that are not provided. (There might have been some types on\n+\/\/ some platforms that used a lock long-ago, but that's no longer the case.)\n+\/\/\n+\/\/ * Rather than load and store operations with a memory order parameter,\n+\/\/ Atomic<T> provides load_relaxed(), load_acquire(), release_store(),\n+\/\/ store_relaxed(), and release_store_fence() operations.\n+\/\/\n+\/\/ * Atomic<T> doesn't provide operator overloads that perform various\n+\/\/ operations with sequentially consistent ordering semantics. The rationale\n+\/\/ for not providing these is similar to that for having different (often\n+\/\/ longer) names for some operations than the corresponding AtomicAccess\n+\/\/ functions.\n+\n+\/\/ Implementation support for Atomic<T>.\n+class AtomicImpl {\n+  enum class Category {\n+    Integer,\n+    Byte,\n+    Pointer,\n+    Translated\n+  };\n+\n+#if defined(__GNUC__) && !defined(__clang__)\n+  \/\/ Workaround for gcc bug. Make category() public, else we get this error\n+  \/\/   error: 'static constexpr AtomicImpl::Category AtomicImpl::category()\n+  \/\/     [with T = unsigned int]' is private within this context\n+  \/\/ The only reference is the default template parameter value in the Atomic\n+  \/\/ class a couple lines below, in this same class!\n+  \/\/ https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=122098\n+public:\n+#endif\n+  \/\/ Selection of Atomic<T> category, based on T.\n+  template<typename T>\n+  static constexpr Category category();\n+private:\n+\n+  \/\/ Helper base classes, providing various parts of the APIs.\n+  template<typename T> class CommonCore;\n+  template<typename T> class SupportsExchange;\n+  template<typename T> class SupportsArithmetic;\n+\n+  \/\/ Support conditional exchange() for atomic translated types.\n+  template<typename T> class HasExchange;\n+  template<typename T> class DecayedHasExchange;\n+  template<typename Derived, typename T, bool = DecayedHasExchange<T>::value>\n+  class TranslatedExchange;\n+\n+public:\n+  template<typename T, Category = category<T>()>\n+  class Atomic;\n+};\n+\n+\/\/ The Atomic<T> type.\n+template<typename T>\n+using Atomic = AtomicImpl::Atomic<T>;\n+\n+template<typename T>\n+constexpr auto AtomicImpl::category() -> Category {\n+  static_assert(std::is_same_v<T, std::remove_cv_t<T>>,\n+                \"Value type must not be cv-qualified\");\n+  if constexpr (std::is_integral_v<T>) {\n+    if constexpr ((sizeof(T) == sizeof(int32_t)) || (sizeof(T) == sizeof(int64_t))) {\n+      return Category::Integer;\n+    } else if constexpr (sizeof(T) == 1) {\n+      return Category::Byte;\n+    } else {\n+      static_assert(DependentAlwaysFalse<T>, \"Invalid atomic integer type\");\n+    }\n+  } else if constexpr (std::is_pointer_v<T>) {\n+    return Category::Pointer;\n+  } else if constexpr (PrimitiveConversions::Translate<T>::value) {\n+    return Category::Translated;\n+  } else {\n+    static_assert(DependentAlwaysFalse<T>, \"Invalid atomic value type\");\n+  }\n+}\n+\n+\/\/ Atomic<T> implementation classes.\n+\n+template<typename T>\n+class AtomicImpl::CommonCore {\n+  T volatile _value;\n+\n+protected:\n+  explicit CommonCore(T value) : _value(value) {}\n+  ~CommonCore() = default;\n+\n+  T volatile* value_ptr() { return &_value; }\n+  T const volatile* value_ptr() const { return &_value; }\n+\n+  \/\/ Support for value_offset_in_bytes.\n+  template<typename Derived>\n+  static constexpr int value_offset_in_bytes_impl() {\n+    return offsetof(Derived, _value);\n+  }\n+\n+public:\n+  NONCOPYABLE(CommonCore);\n+\n+  static constexpr int value_size_in_bytes() {\n+    return sizeof(_value);\n+  }\n+\n+  \/\/ Common core Atomic<T> operations.\n+\n+  T load_relaxed() const {\n+    return AtomicAccess::load(value_ptr());\n+  }\n+\n+  T load_acquire() const {\n+    return AtomicAccess::load_acquire(value_ptr());\n+  }\n+\n+  void store_relaxed(T value) {\n+    AtomicAccess::store(value_ptr(), value);\n+  }\n+\n+  void release_store(T value) {\n+    AtomicAccess::release_store(value_ptr(), value);\n+  }\n+\n+  void release_store_fence(T value) {\n+    AtomicAccess::release_store_fence(value_ptr(), value);\n+  }\n+\n+  T compare_exchange(T compare_value, T new_value,\n+                     atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::cmpxchg(value_ptr(), compare_value, new_value);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::SupportsExchange : public CommonCore<T> {\n+  using Base = CommonCore<T>;\n+\n+protected:\n+  explicit SupportsExchange(T value) : Base(value) {}\n+  ~SupportsExchange() = default;\n+\n+public:\n+  T exchange(T new_value,\n+             atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::xchg(this->value_ptr(), new_value);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::SupportsArithmetic : public SupportsExchange<T> {\n+  using Base = SupportsExchange<T>;\n+\n+  \/\/ Guarding the AtomicAccess calls with constexpr checking of I produces\n+  \/\/ better compile-time error messages.\n+  template<typename I>\n+  static constexpr bool check_i() {\n+    static_assert(sizeof(I) <= sizeof(T), \"offset size exceeds value size\");\n+    if constexpr (!std::is_integral_v<T>) {\n+      static_assert(std::is_pointer_v<T>, \"must be\");\n+    } else if constexpr (std::is_signed_v<T>) {\n+      static_assert(std::is_signed_v<I>, \"value is signed but offset is unsigned\");\n+    } else {\n+      static_assert(std::is_unsigned_v<I>, \"value is unsigned but offset is signed\");\n+    }\n+    return true;\n+  }\n+\n+protected:\n+  explicit SupportsArithmetic(T value) : Base(value) {}\n+  ~SupportsArithmetic() = default;\n+\n+public:\n+  template<typename I>\n+  T add_then_fetch(I add_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      return AtomicAccess::add(this->value_ptr(), add_value, order);\n+    }\n+  }\n+\n+  template<typename I>\n+  T fetch_then_add(I add_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      return AtomicAccess::fetch_then_add(this->value_ptr(), add_value, order);\n+    }\n+  }\n+\n+  template<typename I>\n+  T sub_then_fetch(I sub_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      return AtomicAccess::sub(this->value_ptr(), sub_value, order);\n+    }\n+  }\n+\n+  template<typename I>\n+  T fetch_then_sub(I sub_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      \/\/ AtomicAccess doesn't currently provide fetch_then_sub.\n+      return sub_then_fetch(sub_value, order) + sub_value;\n+    }\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Integer>\n+  : public SupportsArithmetic<T>\n+{\n+  using Base = SupportsArithmetic<T>;\n+\n+public:\n+  explicit Atomic(T value = 0) : Base(value) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  using ValueType = T;\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return CommonCore<T>::template value_offset_in_bytes_impl<Atomic>();\n+  }\n+\n+  T fetch_then_and(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::fetch_then_and(this->value_ptr(), bits, order);\n+  }\n+\n+  T fetch_then_or(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::fetch_then_or(this->value_ptr(), bits, order);\n+  }\n+\n+  T fetch_then_xor(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::fetch_then_xor(this->value_ptr(), bits, order);\n+  }\n+\n+  T and_then_fetch(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::and_then_fetch(this->value_ptr(), bits, order);\n+  }\n+\n+  T or_then_fetch(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::or_then_fetch(this->value_ptr(), bits, order);\n+  }\n+\n+  T xor_then_fetch(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::xor_then_fetch(this->value_ptr(), bits, order);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Byte>\n+  : public CommonCore<T>\n+{\n+  using Base = CommonCore<T>;\n+\n+public:\n+  explicit Atomic(T value = 0) : Base(value) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  using ValueType = T;\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return CommonCore<T>::template value_offset_in_bytes_impl<Atomic>();\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Pointer>\n+  : public SupportsArithmetic<T>\n+{\n+  using Base = SupportsArithmetic<T>;\n+\n+public:\n+  explicit Atomic(T value = nullptr) : Base(value) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  using ValueType = T;\n+  using ElementType = std::remove_pointer_t<T>;\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return CommonCore<T>::template value_offset_in_bytes_impl<Atomic>();\n+  }\n+};\n+\n+\/\/ Atomic translated type\n+\n+\/\/ Test whether Atomic<T> has exchange().\n+template<typename T>\n+class AtomicImpl::HasExchange {\n+  template<typename Check> static char* test(decltype(&Check::exchange));\n+  template<typename> static char test(...);\n+  using test_type = decltype(test<Atomic<T>>(nullptr));\n+public:\n+  static constexpr bool value = std::is_pointer_v<test_type>;\n+};\n+\n+\/\/ Test whether the atomic decayed type associated with T has exchange().\n+template<typename T>\n+class AtomicImpl::DecayedHasExchange {\n+  using Translator = PrimitiveConversions::Translate<T>;\n+  using Decayed = typename Translator::Decayed;\n+\n+  \/\/ \"Unit test\" HasExchange<>.\n+  static_assert(HasExchange<int>::value);\n+  static_assert(HasExchange<int*>::value);\n+  static_assert(!HasExchange<char>::value);\n+\n+public:\n+  static constexpr bool value = HasExchange<Decayed>::value;\n+};\n+\n+\/\/ Base class for atomic translated type if atomic decayed type doesn't have\n+\/\/ exchange().\n+template<typename Derived, typename T, bool>\n+class AtomicImpl::TranslatedExchange {};\n+\n+\/\/ Base class for atomic translated type if atomic decayed type does have\n+\/\/ exchange().\n+template<typename Derived, typename T>\n+class AtomicImpl::TranslatedExchange<Derived, T, true> {\n+public:\n+  T exchange(T new_value,\n+             atomic_memory_order order = memory_order_conservative) {\n+    return static_cast<Derived*>(this)->exchange_impl(new_value, order);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Translated>\n+  : public TranslatedExchange<Atomic<T>, T>\n+{\n+  \/\/ Give TranslatedExchange<> access to exchange_impl() if needed.\n+  friend class TranslatedExchange<Atomic<T>, T>;\n+\n+  using Translator = PrimitiveConversions::Translate<T>;\n+  using Decayed = typename Translator::Decayed;\n+\n+  Atomic<Decayed> _value;\n+\n+  static Decayed decay(T x) { return Translator::decay(x); }\n+  static T recover(Decayed x) { return Translator::recover(x); }\n+\n+  \/\/ Support for default construction via the default construction of _value.\n+  struct UseDecayedCtor {};\n+  explicit Atomic(UseDecayedCtor) : _value() {}\n+  using DefaultCtorSelect =\n+    std::conditional_t<std::is_default_constructible_v<T>, T, UseDecayedCtor>;\n+\n+public:\n+  using ValueType = T;\n+\n+  \/\/ If T is default constructible, construct from a default constructed T.\n+  \/\/ Otherwise, default construct the underlying Atomic<Decayed>.\n+  Atomic() : Atomic(DefaultCtorSelect()) {}\n+\n+  explicit Atomic(T value) : _value(decay(value)) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return (offsetof(Atomic, _value) +\n+            Atomic<Decayed>::value_offset_in_bytes());\n+  }\n+\n+  static constexpr int value_size_in_bytes() {\n+    return Atomic<Decayed>::value_size_in_bytes();\n+  }\n+\n+  T load_relaxed() const {\n+    return recover(_value.load_relaxed());\n+  }\n+\n+  T load_acquire() const {\n+    return recover(_value.load_acquire());\n+  }\n+\n+  void store_relaxed(T value) {\n+    _value.store_relaxed(decay(value));\n+  }\n+\n+  void release_store(T value) {\n+    _value.release_store(decay(value));\n+  }\n+\n+  void release_store_fence(T value) {\n+    _value.release_store_fence(decay(value));\n+  }\n+\n+  T compare_exchange(T compare_value, T new_value,\n+                     atomic_memory_order order = memory_order_conservative) {\n+    return recover(_value.compare_exchange(decay(compare_value),\n+                                           decay(new_value),\n+                                           order));\n+  }\n+\n+private:\n+  \/\/ Implementation of exchange() if needed.\n+  \/\/ Exclude when not needed, to prevent reference to non-existent function\n+  \/\/ of atomic decayed type if someone explicitly instantiates Atomic<T>.\n+  template<typename Dep = Decayed, ENABLE_IF(HasExchange<Dep>::value)>\n+  T exchange_impl(T new_value, atomic_memory_order order) {\n+    return recover(_value.exchange(decay(new_value), order));\n+  }\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_ATOMIC_HPP\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":553,"deletions":0,"binary":false,"changes":553,"status":"added"},{"patch":"@@ -1377,0 +1377,8 @@\n+\/\/ This provides a workaround for static_assert(false) in discarded or\n+\/\/ otherwise uninstantiated places.  Instead use\n+\/\/   static_assert(DependentAlwaysFalse<T>, \"...\")\n+\/\/ See http:\/\/wg21.link\/p2593r1. Some, but not all, compiler versions we're\n+\/\/ using have implemented that change as a DR:\n+\/\/ https:\/\/cplusplus.github.io\/CWG\/issues\/2518.html\n+template<typename T> inline constexpr bool DependentAlwaysFalse = false;\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  assert(AtomicAccess::add(&_writers, 1u) == 1u, \"multiple writers\");\n+  assert(_writers.fetch_then_add(1u) == 0u, \"multiple writers\");\n@@ -53,1 +53,1 @@\n-  uint value = _enter;\n+  uint value = _enter.load_relaxed();\n@@ -56,1 +56,1 @@\n-  volatile uint* new_ptr = &_exit[(value + 1) & 1];\n+  Atomic<uint>& new_exit = _exit[(value + 1) & 1];\n@@ -65,2 +65,2 @@\n-    *new_ptr = ++value;\n-    value = AtomicAccess::cmpxchg(&_enter, old, value);\n+    new_exit.store_relaxed(++value);\n+    value = _enter.compare_exchange(old, value);\n@@ -71,2 +71,2 @@\n-  volatile uint* old_ptr = &_exit[old & 1];\n-  assert(old_ptr != new_ptr, \"invariant\");\n+  Atomic<uint>& old_exit = _exit[old & 1];\n+  assert(&new_exit != &old_exit, \"invariant\");\n@@ -77,1 +77,1 @@\n-  _waiting_for = old;\n+  _waiting_for.store_relaxed(old);\n@@ -84,1 +84,1 @@\n-  \/\/ to complete, e.g. for the value of old_ptr to catch up with old.\n+  \/\/ to complete, e.g. for the value of old_exit to catch up with old.\n@@ -87,1 +87,1 @@\n-  while (old != AtomicAccess::load_acquire(old_ptr)) {\n+  while (old != old_exit.load_acquire()) {\n@@ -98,1 +98,1 @@\n-  DEBUG_ONLY(AtomicAccess::dec(&_writers);)\n+  assert(_writers.sub_then_fetch(1u) == 0u, \"invariant\");\n","filename":"src\/hotspot\/share\/utilities\/singleWriterSynchronizer.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomicAccess.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -51,3 +51,3 @@\n-  volatile uint _enter;\n-  volatile uint _exit[2];\n-  volatile uint _waiting_for;\n+  Atomic<uint> _enter;\n+  Atomic<uint> _exit[2];\n+  Atomic<uint> _waiting_for;\n@@ -56,1 +56,1 @@\n-  DEBUG_ONLY(volatile uint _writers;)\n+  DEBUG_ONLY(Atomic<uint> _writers;)\n@@ -90,1 +90,1 @@\n-  return AtomicAccess::add(&_enter, 2u);\n+  return _enter.add_then_fetch(2u);\n@@ -94,1 +94,1 @@\n-  uint exit_value = AtomicAccess::add(&_exit[enter_value & 1], 2u);\n+  uint exit_value = _exit[enter_value & 1].add_then_fetch(2u);\n@@ -98,1 +98,1 @@\n-  if (exit_value == _waiting_for) {\n+  if (exit_value == _waiting_for.load_relaxed()) {\n","filename":"src\/hotspot\/share\/utilities\/singleWriterSynchronizer.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,640 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cppstdlib\/type_traits.hpp\"\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+#include \"unittest.hpp\"\n+\n+\/\/ These tests of Atomic<T> only verify functionality.  They don't verify\n+\/\/ atomicity.\n+\n+template<typename T>\n+struct AtomicIntegerArithmeticTestSupport {\n+  Atomic<T> _test_value;\n+\n+  static constexpr T _old_value =    static_cast<T>(UCONST64(0x2000000020000));\n+  static constexpr T _change_value = static_cast<T>(UCONST64(    0x100000001));\n+\n+  AtomicIntegerArithmeticTestSupport() : _test_value(0) {}\n+\n+  void fetch_then_add() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value + _change_value;\n+    T result = _test_value.fetch_then_add(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_sub() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value - _change_value;\n+    T result = _test_value.fetch_then_sub(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void add_then_fetch() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value + _change_value;\n+    T result = _test_value.add_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void sub_then_fetch() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value - _change_value;\n+    T result = _test_value.sub_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+#define TEST_ARITHMETIC(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_ARITHMETIC(fetch_then_add)\n+    TEST_ARITHMETIC(fetch_then_sub)\n+    TEST_ARITHMETIC(add_then_fetch)\n+    TEST_ARITHMETIC(sub_then_fetch)\n+  }\n+\n+#undef TEST_ARITHMETIC\n+};\n+\n+TEST_VM(AtomicIntegerTest, arith_int32) {\n+  AtomicIntegerArithmeticTestSupport<int32_t>()();\n+}\n+\n+TEST_VM(AtomicIntegerTest, arith_uint32) {\n+  AtomicIntegerArithmeticTestSupport<uint32_t>()();\n+}\n+\n+TEST_VM(AtomicIntegerTest, arith_int64) {\n+  AtomicIntegerArithmeticTestSupport<int64_t>()();\n+}\n+\n+TEST_VM(AtomicIntegerTest, arith_uint64) {\n+  AtomicIntegerArithmeticTestSupport<uint64_t>()();\n+}\n+\n+template<typename T>\n+struct AtomicIntegerXchgTestSupport {\n+  Atomic<T> _test_value;\n+\n+  AtomicIntegerXchgTestSupport() : _test_value{} {}\n+\n+  void test() {\n+    T zero = 0;\n+    T five = 5;\n+    _test_value.store_relaxed(zero);\n+    T res = _test_value.exchange(five);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(five, _test_value.load_relaxed());\n+  }\n+};\n+\n+TEST_VM(AtomicIntegerTest, xchg_int32) {\n+  using Support = AtomicIntegerXchgTestSupport<int32_t>;\n+  Support().test();\n+}\n+\n+TEST_VM(AtomicIntegerTest, xchg_int64) {\n+  using Support = AtomicIntegerXchgTestSupport<int64_t>;\n+  Support().test();\n+}\n+\n+template<typename T>\n+struct AtomicIntegerCmpxchgTestSupport {\n+  Atomic<T> _test_value;\n+\n+  AtomicIntegerCmpxchgTestSupport() : _test_value{} {}\n+\n+  void test() {\n+    T zero = 0;\n+    T five = 5;\n+    T ten = 10;\n+    _test_value.store_relaxed(zero);\n+    T res = _test_value.compare_exchange(five, ten);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(zero, _test_value.load_relaxed());\n+    res = _test_value.compare_exchange(zero, ten);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(ten, _test_value.load_relaxed());\n+  }\n+};\n+\n+TEST_VM(AtomicIntegerTest, cmpxchg_int32) {\n+  using Support = AtomicIntegerCmpxchgTestSupport<int32_t>;\n+  Support().test();\n+}\n+\n+TEST_VM(AtomicIntegerTest, cmpxchg_int64) {\n+  \/\/ Check if 64-bit atomics are available on the machine.\n+  if (!VM_Version::supports_cx8()) return;\n+\n+  using Support = AtomicIntegerCmpxchgTestSupport<int64_t>;\n+  Support().test();\n+}\n+\n+struct AtomicCmpxchg1ByteStressSupport {\n+  char _default_val;\n+  int  _base;\n+  Atomic<char> _array[7+32+7];\n+\n+  AtomicCmpxchg1ByteStressSupport() : _default_val(0x7a), _base(7) {}\n+\n+  void validate(char val, char val2, int index) {\n+    for (int i = 0; i < 7; i++) {\n+      EXPECT_EQ(_array[i].load_relaxed(), _default_val);\n+    }\n+    for (int i = 7; i < (7+32); i++) {\n+      if (i == index) {\n+        EXPECT_EQ(_array[i].load_relaxed(), val2);\n+      } else {\n+        EXPECT_EQ(_array[i].load_relaxed(), val);\n+      }\n+    }\n+    for (int i = 0; i < 7; i++) {\n+      EXPECT_EQ(_array[i].load_relaxed(), _default_val);\n+    }\n+  }\n+\n+  void test_index(int index) {\n+    char one = 1;\n+    _array[index].compare_exchange(_default_val, one);\n+    validate(_default_val, one, index);\n+\n+    _array[index].compare_exchange(one, _default_val);\n+    validate(_default_val, _default_val, index);\n+  }\n+\n+  void test() {\n+    for (size_t i = 0; i < ARRAY_SIZE(_array); ++i) {\n+      _array[i].store_relaxed(_default_val);\n+    }\n+    for (int i = _base; i < (_base+32); i++) {\n+      test_index(i);\n+    }\n+  }\n+};\n+\n+TEST_VM(AtomicCmpxchg1Byte, stress) {\n+  AtomicCmpxchg1ByteStressSupport support;\n+  support.test();\n+}\n+\n+template<typename T>\n+struct AtomicEnumTestSupport {\n+  Atomic<T> _test_value;\n+\n+  AtomicEnumTestSupport() : _test_value{} {}\n+\n+  void test_store_load(T value) {\n+    EXPECT_NE(value, _test_value.load_relaxed());\n+    _test_value.store_relaxed(value);\n+    EXPECT_EQ(value, _test_value.load_relaxed());\n+  }\n+\n+  void test_cmpxchg(T value1, T value2) {\n+    EXPECT_NE(value1, _test_value.load_relaxed());\n+    _test_value.store_relaxed(value1);\n+    EXPECT_EQ(value1, _test_value.compare_exchange(value2, value2));\n+    EXPECT_EQ(value1, _test_value.load_relaxed());\n+    EXPECT_EQ(value1, _test_value.compare_exchange(value1, value2));\n+    EXPECT_EQ(value2, _test_value.load_relaxed());\n+  }\n+\n+  void test_xchg(T value1, T value2) {\n+    EXPECT_NE(value1, _test_value.load_relaxed());\n+    _test_value.store_relaxed(value1);\n+    EXPECT_EQ(value1, _test_value.exchange(value2));\n+    EXPECT_EQ(value2, _test_value.load_relaxed());\n+  }\n+};\n+\n+namespace AtomicEnumTestUnscoped {       \/\/ Scope the enumerators.\n+  enum TestEnum { A, B, C };\n+}\n+\n+TEST_VM(AtomicEnumTest, unscoped_enum) {\n+  using namespace AtomicEnumTestUnscoped;\n+  using Support = AtomicEnumTestSupport<TestEnum>;\n+\n+  Support().test_store_load(B);\n+  Support().test_cmpxchg(B, C);\n+  Support().test_xchg(B, C);\n+}\n+\n+enum class AtomicEnumTestScoped { A, B, C };\n+\n+TEST_VM(AtomicEnumTest, scoped_enum) {\n+  const AtomicEnumTestScoped B = AtomicEnumTestScoped::B;\n+  const AtomicEnumTestScoped C = AtomicEnumTestScoped::C;\n+  using Support = AtomicEnumTestSupport<AtomicEnumTestScoped>;\n+\n+  Support().test_store_load(B);\n+  Support().test_cmpxchg(B, C);\n+  Support().test_xchg(B, C);\n+}\n+\n+template<typename T>\n+struct AtomicBitopsTestSupport {\n+  Atomic<T> _test_value;\n+\n+  \/\/ At least one byte differs between _old_value and _old_value op _change_value.\n+  static constexpr T _old_value =    static_cast<T>(UCONST64(0x7f5300007f530044));\n+  static constexpr T _change_value = static_cast<T>(UCONST64(0x3800530038005322));\n+\n+  AtomicBitopsTestSupport() : _test_value(0) {}\n+\n+  void fetch_then_and() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value & _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.fetch_then_and(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_or() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value | _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.fetch_then_or(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_xor() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value ^ _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.fetch_then_xor(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void and_then_fetch() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value & _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.and_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void or_then_fetch() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value | _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.or_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void xor_then_fetch() {\n+    _test_value.store_relaxed(_old_value);\n+    T expected = _old_value ^ _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.xor_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+#define TEST_BITOP(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_BITOP(fetch_then_and)\n+    TEST_BITOP(fetch_then_or)\n+    TEST_BITOP(fetch_then_xor)\n+    TEST_BITOP(and_then_fetch)\n+    TEST_BITOP(or_then_fetch)\n+    TEST_BITOP(xor_then_fetch)\n+  }\n+\n+#undef TEST_BITOP\n+};\n+\n+TEST_VM(AtomicBitopsTest, int32) {\n+  AtomicBitopsTestSupport<int32_t>()();\n+}\n+\n+TEST_VM(AtomicBitopsTest, uint32) {\n+  AtomicBitopsTestSupport<uint32_t>()();\n+}\n+\n+TEST_VM(AtomicBitopsTest, int64) {\n+  AtomicBitopsTestSupport<int64_t>()();\n+}\n+\n+TEST_VM(AtomicBitopsTest, uint64) {\n+  AtomicBitopsTestSupport<uint64_t>()();\n+}\n+\n+template<typename T>\n+struct AtomicPointerTestSupport {\n+  static T _test_values[10];\n+  static T* _initial_ptr;\n+\n+  Atomic<T*> _test_value;\n+\n+  AtomicPointerTestSupport() : _test_value(nullptr) {}\n+\n+  void fetch_then_add() {\n+    _test_value.store_relaxed(_initial_ptr);\n+    T* expected = _initial_ptr + 2;\n+    T* result = _test_value.fetch_then_add(2);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_sub() {\n+    _test_value.store_relaxed(_initial_ptr);\n+    T* expected = _initial_ptr - 2;\n+    T* result = _test_value.fetch_then_sub(2);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void add_then_fetch() {\n+    _test_value.store_relaxed(_initial_ptr);\n+    T* expected = _initial_ptr + 2;\n+    T* result = _test_value.add_then_fetch(2);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void sub_then_fetch() {\n+    _test_value.store_relaxed(_initial_ptr);\n+    T* expected = _initial_ptr - 2;\n+    T* result = _test_value.sub_then_fetch(2);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void exchange() {\n+    _test_value.store_relaxed(_initial_ptr);\n+    T* replace = _initial_ptr + 3;\n+    T* result = _test_value.exchange(replace);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(replace, _test_value.load_relaxed());\n+  }\n+\n+  void compare_exchange() {\n+    _test_value.store_relaxed(_initial_ptr);\n+    T* not_initial_ptr = _initial_ptr - 1;\n+    T* replace = _initial_ptr + 3;\n+\n+    T* result = _test_value.compare_exchange(not_initial_ptr, replace);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(_initial_ptr, _test_value.load_relaxed());\n+\n+    result = _test_value.compare_exchange(_initial_ptr, replace);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(replace, _test_value.load_relaxed());\n+  }\n+\n+#define TEST_OP(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_OP(fetch_then_add)\n+    TEST_OP(fetch_then_sub)\n+    TEST_OP(add_then_fetch)\n+    TEST_OP(sub_then_fetch)\n+    TEST_OP(exchange)\n+    TEST_OP(compare_exchange)\n+  }\n+\n+#undef TEST_OP\n+};\n+\n+template<typename T>\n+T AtomicPointerTestSupport<T>::_test_values[10] = {};\n+\n+template<typename T>\n+T* AtomicPointerTestSupport<T>::_initial_ptr = &_test_values[5];\n+\n+TEST_VM(AtomicPointerTest, ptr_to_char) {\n+  AtomicPointerTestSupport<char>()();\n+}\n+\n+TEST_VM(AtomicPointerTest, ptr_to_int32) {\n+  AtomicPointerTestSupport<int32_t>()();\n+}\n+\n+TEST_VM(AtomicPointerTest, ptr_to_int64) {\n+  AtomicPointerTestSupport<int64_t>()();\n+}\n+\n+\/\/ Test translation, including chaining.\n+\n+struct TranslatedAtomicTestObject1 {\n+  int _value;\n+\n+  \/\/ NOT default constructible.\n+\n+  explicit TranslatedAtomicTestObject1(int value)\n+    : _value(value) {}\n+};\n+\n+template<>\n+struct PrimitiveConversions::Translate<TranslatedAtomicTestObject1>\n+  : public std::true_type\n+{\n+  using Value = TranslatedAtomicTestObject1;\n+  using Decayed = int;\n+\n+  static Decayed decay(Value x) { return x._value; }\n+  static Value recover(Decayed x) { return Value(x); }\n+};\n+\n+struct TranslatedAtomicTestObject2 {\n+  TranslatedAtomicTestObject1 _value;\n+\n+  static constexpr int DefaultObject1Value = 3;\n+\n+  TranslatedAtomicTestObject2()\n+    : TranslatedAtomicTestObject2(TranslatedAtomicTestObject1(DefaultObject1Value))\n+  {}\n+\n+  explicit TranslatedAtomicTestObject2(TranslatedAtomicTestObject1 value)\n+    : _value(value) {}\n+};\n+\n+template<>\n+struct PrimitiveConversions::Translate<TranslatedAtomicTestObject2>\n+  : public std::true_type\n+{\n+  using Value = TranslatedAtomicTestObject2;\n+  using Decayed = TranslatedAtomicTestObject1;\n+\n+  static Decayed decay(Value x) { return x._value; }\n+  static Value recover(Decayed x) { return Value(x); }\n+};\n+\n+struct TranslatedAtomicByteObject {\n+  uint8_t _value;\n+\n+  \/\/ NOT default constructible.\n+\n+  explicit TranslatedAtomicByteObject(uint8_t value = 0) : _value(value) {}\n+};\n+\n+template<>\n+struct PrimitiveConversions::Translate<TranslatedAtomicByteObject>\n+  : public std::true_type\n+{\n+  using Value = TranslatedAtomicByteObject;\n+  using Decayed = uint8_t;\n+\n+  static Decayed decay(Value x) { return x._value; }\n+  static Value recover(Decayed x) { return Value(x); }\n+};\n+\n+\/\/ Test whether Atomic<T> has exchange().\n+\/\/ Note: This is intentionally a different implementation from what is used\n+\/\/ by the atomic translated type to decide whether to provide exchange().\n+\/\/ The intent is to make related testing non-tautological.\n+\/\/ The two implementations must agree; it's a bug if they don't.\n+template<typename T>\n+class AtomicTypeHasExchange {\n+  template<typename U,\n+           typename AU = Atomic<U>,\n+           typename = decltype(declval<AU>().exchange(declval<U>()))>\n+  static char* test(int);\n+\n+  template<typename> static char test(...);\n+\n+  using test_type = decltype(test<T>(0));\n+\n+public:\n+  static constexpr bool value = std::is_pointer_v<test_type>;\n+};\n+\n+\/\/ Unit tests for AtomicTypeHasExchange.\n+static_assert(AtomicTypeHasExchange<int>::value);\n+static_assert(AtomicTypeHasExchange<int*>::value);\n+static_assert(AtomicTypeHasExchange<TranslatedAtomicTestObject1>::value);\n+static_assert(AtomicTypeHasExchange<TranslatedAtomicTestObject2>::value);\n+static_assert(!AtomicTypeHasExchange<uint8_t>::value);\n+\n+\/\/ Verify translated byte type *doesn't* have exchange.\n+static_assert(!AtomicTypeHasExchange<TranslatedAtomicByteObject>::value);\n+\n+\/\/ Verify that explicit instantiation doesn't attempt to reference the\n+\/\/ non-existent exchange of the atomic decayed type.\n+template class AtomicImpl::Atomic<TranslatedAtomicByteObject>;\n+\n+template<typename T>\n+static void test_atomic_translated_type() {\n+  \/\/ This works even if T is not default constructible.\n+  Atomic<T> _test_value{};\n+\n+  using Translated = PrimitiveConversions::Translate<T>;\n+\n+  EXPECT_EQ(0, Translated::decay(_test_value.load_relaxed()));\n+  _test_value.store_relaxed(Translated::recover(5));\n+  EXPECT_EQ(5, Translated::decay(_test_value.load_relaxed()));\n+  EXPECT_EQ(5, Translated::decay(_test_value.compare_exchange(Translated::recover(5),\n+                                                              Translated::recover(10))));\n+  EXPECT_EQ(10, Translated::decay(_test_value.load_relaxed()));\n+\n+  if constexpr (AtomicTypeHasExchange<T>::value) {\n+    EXPECT_EQ(10, Translated::decay(_test_value.exchange(Translated::recover(20))));\n+    EXPECT_EQ(20, Translated::decay(_test_value.load_relaxed()));\n+  }\n+}\n+\n+TEST_VM(AtomicTranslatedTypeTest, int_test) {\n+  test_atomic_translated_type<TranslatedAtomicTestObject1>();\n+}\n+\n+TEST_VM(AtomicTranslatedTypeTest, byte_test) {\n+  test_atomic_translated_type<TranslatedAtomicByteObject>();\n+}\n+\n+TEST_VM(AtomicTranslatedTypeTest, chain) {\n+  Atomic<TranslatedAtomicTestObject2> _test_value{};\n+\n+  using Translated1 = PrimitiveConversions::Translate<TranslatedAtomicTestObject1>;\n+  using Translated2 = PrimitiveConversions::Translate<TranslatedAtomicTestObject2>;\n+\n+  auto resolve = [&](TranslatedAtomicTestObject2 x) {\n+    return Translated1::decay(Translated2::decay(x));\n+  };\n+\n+  auto construct = [&](int x) {\n+    return Translated2::recover(Translated1::recover(x));\n+  };\n+\n+  EXPECT_EQ(TranslatedAtomicTestObject2::DefaultObject1Value,\n+            resolve(_test_value.load_relaxed()));\n+  _test_value.store_relaxed(construct(5));\n+  EXPECT_EQ(5, resolve(_test_value.load_relaxed()));\n+  EXPECT_EQ(5, resolve(_test_value.compare_exchange(construct(5), construct(10))));\n+  EXPECT_EQ(10, resolve(_test_value.load_relaxed()));\n+  EXPECT_EQ(10, resolve(_test_value.exchange(construct(20))));\n+  EXPECT_EQ(20, resolve(_test_value.load_relaxed()));\n+};\n+\n+template<typename T>\n+static void test_value_access() {\n+  using AT = Atomic<T>;\n+  \/\/ In addition to verifying values are as expected, also verify the\n+  \/\/ operations are constexpr.\n+  static_assert(sizeof(T) == AT::value_size_in_bytes(), \"value size differs\");\n+  static_assert(0 == AT::value_offset_in_bytes(), \"unexpected offset\");\n+  \/\/ Also verify no unexpected increase in size for Atomic wrapper.\n+  static_assert(sizeof(T) == sizeof(AT), \"unexpected size difference\");\n+};\n+\n+TEST_VM(AtomicValueAccessTest, access_char) {\n+  test_value_access<char>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_bool) {\n+  test_value_access<bool>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_int32) {\n+  test_value_access<int32_t>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_int64) {\n+  test_value_access<int64_t>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_ptr) {\n+  test_value_access<char*>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_trans1) {\n+  test_value_access<TranslatedAtomicTestObject1>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_trans2) {\n+  test_value_access<TranslatedAtomicTestObject2>();\n+}\n","filename":"test\/hotspot\/gtest\/runtime\/test_atomic.cpp","additions":640,"deletions":0,"binary":false,"changes":640,"status":"added"}]}