{"files":[{"patch":"@@ -44,1 +44,1 @@\n-  FreeNode* old_head = AtomicAccess::xchg(&_head, node);\n+  FreeNode* old_head = _head.fetch_then_set(node);\n@@ -51,1 +51,1 @@\n-  return AtomicAccess::add(&_count, size_t(1));\n+  return _count.add_then_fetch(1u);\n@@ -55,2 +55,2 @@\n-  NodeList result{AtomicAccess::load(&_head), _tail, AtomicAccess::load(&_count)};\n-  AtomicAccess::store(&_head, (FreeNode*)nullptr);\n+  NodeList result{_head.load_relaxed(), _tail, _count.load_relaxed()};\n+  _head.relaxed_store(nullptr);\n@@ -58,1 +58,1 @@\n-  AtomicAccess::store(&_count, size_t(0));\n+  _count.relaxed_store(0u);\n@@ -63,1 +63,1 @@\n-  return  AtomicAccess::load(&_count);\n+  return _count.load_relaxed();\n@@ -88,1 +88,1 @@\n-  uint index = AtomicAccess::load(&_active_pending_list);\n+  uint index = _active_pending_list.load_relaxed();\n@@ -96,1 +96,1 @@\n-  uint index = AtomicAccess::load(&_active_pending_list);\n+  uint index = _active_pending_list.load_relaxed();\n@@ -99,1 +99,1 @@\n-  _free_count = 0;\n+  _free_count.relaxed_store(0u);\n@@ -103,1 +103,1 @@\n-  return AtomicAccess::load(&_free_count);\n+  return _free_count.load_relaxed();\n@@ -107,1 +107,1 @@\n-  uint index = AtomicAccess::load(&_active_pending_list);\n+  uint index = _active_pending_list.load_relaxed();\n@@ -127,1 +127,1 @@\n-    size_t count = AtomicAccess::sub(&_free_count, 1u);\n+    size_t count = _free_count.sub_then_fetch(1u);\n@@ -152,1 +152,1 @@\n-    uint index = AtomicAccess::load_acquire(&_active_pending_list);\n+    uint index = _active_pending_list.load_acquire();\n@@ -167,2 +167,2 @@\n-  if (AtomicAccess::load(&_transfer_lock) || \/\/ Skip CAS if likely to fail.\n-      AtomicAccess::cmpxchg(&_transfer_lock, false, true)) {\n+  if (_transfer_lock.load_relaxed() || \/\/ Skip CAS if likely to fail.\n+      _transfer_lock.cmpxchg(false, true)) {\n@@ -175,1 +175,1 @@\n-  uint index = AtomicAccess::load(&_active_pending_list);\n+  uint index = _active_pending_list.load_relaxed();\n@@ -177,1 +177,1 @@\n-  AtomicAccess::release_store(&_active_pending_list, new_active);\n+  _active_pending_list.release_store(new_active);\n@@ -189,1 +189,1 @@\n-    AtomicAccess::add(&_free_count, count);\n+    _free_count.add_then_fetch(count);\n@@ -194,1 +194,1 @@\n-  AtomicAccess::release_store(&_transfer_lock, false);\n+  _transfer_lock.release_store(false);\n","filename":"src\/hotspot\/share\/gc\/shared\/freeListAllocator.cpp","additions":19,"deletions":19,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"runtime\/atomicAccess.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -65,1 +65,1 @@\n-    FreeNode* volatile _next;\n+    Atomic<FreeNode*> _next;\n@@ -69,1 +69,1 @@\n-    FreeNode* next() { return AtomicAccess::load(&_next); }\n+    FreeNode* next() { return _next.load_relaxed(); }\n@@ -71,1 +71,1 @@\n-    FreeNode* volatile* next_addr() { return &_next; }\n+    Atomic<FreeNode*>* next_addr() { return &_next; }\n@@ -73,1 +73,1 @@\n-    void set_next(FreeNode* next) { AtomicAccess::store(&_next, next); }\n+    void set_next(FreeNode* next) { _next.relaxed_store(next); }\n@@ -88,2 +88,2 @@\n-    FreeNode* volatile _head;\n-    volatile size_t _count;\n+    Atomic<FreeNode*> _head;\n+    Atomic<size_t> _count;\n@@ -108,1 +108,1 @@\n-  static FreeNode* volatile* next_ptr(FreeNode& node) { return node.next_addr(); }\n+  static Atomic<FreeNode*>* next_ptr(FreeNode& node) { return node.next_addr(); }\n@@ -116,1 +116,1 @@\n-  DECLARE_PADDED_MEMBER(1, volatile size_t, _free_count);\n+  DECLARE_PADDED_MEMBER(1, Atomic<size_t>, _free_count);\n@@ -118,1 +118,1 @@\n-  DECLARE_PADDED_MEMBER(3, volatile bool, _transfer_lock);\n+  DECLARE_PADDED_MEMBER(3, Atomic<bool>, _transfer_lock);\n@@ -121,1 +121,1 @@\n-  volatile uint _active_pending_list;\n+  Atomic<uint> _active_pending_list;\n","filename":"src\/hotspot\/share\/gc\/shared\/freeListAllocator.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -246,1 +246,1 @@\n-  switch (AtomicAccess::load(&_dead_state)) {\n+  switch (_dead_state.load_relaxed()) {\n@@ -248,1 +248,1 @@\n-    AtomicAccess::store(&_dead_count, num_dead);\n+    _dead_count.relaxed_store(num_dead);\n@@ -254,2 +254,2 @@\n-    AtomicAccess::store(&_dead_count, num_dead);\n-    AtomicAccess::release_store(&_dead_state, DeadState::good);\n+    _dead_count.relaxed_store(num_dead);\n+    _dead_state.release_store(DeadState::good);\n@@ -259,1 +259,1 @@\n-    AtomicAccess::release_store(&_dead_state, DeadState::wait1);\n+    _dead_state.release_store(DeadState::wait1);\n@@ -424,2 +424,4 @@\n-volatile size_t StringDedup::Table::_dead_count = 0;\n-volatile StringDedup::Table::DeadState StringDedup::Table::_dead_state = DeadState::good;\n+Atomic<size_t> StringDedup::Table::_dead_count{};\n+\n+Atomic<StringDedup::Table::DeadState>\n+StringDedup::Table::_dead_state{DeadState::good};\n@@ -478,1 +480,1 @@\n-  return AtomicAccess::load_acquire(&_dead_state) == DeadState::good;\n+  return _dead_state.load_acquire() == DeadState::good;\n@@ -484,1 +486,1 @@\n-         ((_number_of_entries - AtomicAccess::load(&_dead_count)) > _grow_threshold);\n+         ((_number_of_entries - _dead_count.load_relaxed()) > _grow_threshold);\n@@ -490,1 +492,1 @@\n-         Config::should_cleanup_table(_number_of_entries, AtomicAccess::load(&_dead_count));\n+         Config::should_cleanup_table(_number_of_entries, _dead_count.load_relaxed());\n@@ -649,1 +651,1 @@\n-  size_t dead_count = AtomicAccess::load(&_dead_count);\n+  size_t dead_count = _dead_count.load_relaxed();\n@@ -673,2 +675,2 @@\n-  AtomicAccess::store(&_dead_count, size_t(0));\n-  AtomicAccess::store(&_dead_state, DeadState::cleaning);\n+  _dead_count.relaxed_store(0);\n+  _dead_state.relaxed_store(DeadState::cleaning);\n@@ -708,1 +710,1 @@\n-  AtomicAccess::store(&_dead_state, DeadState::wait2);\n+  _dead_state.relaxed_store(DeadState::wait2);\n@@ -730,2 +732,2 @@\n-    dead_count = _dead_count;\n-    dead_state = static_cast<int>(_dead_state);\n+    dead_count = _dead_count.load_relaxed();\n+    dead_state = static_cast<int>(_dead_state.load_relaxed());\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupTable.cpp","additions":18,"deletions":16,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -89,3 +90,3 @@\n-  \/\/ read by the dedup thread without holding the lock lock.\n-  static volatile size_t _dead_count;\n-  static volatile DeadState _dead_state;\n+  \/\/ read by the dedup thread without holding the lock.\n+  static Atomic<size_t> _dead_count;\n+  static Atomic<DeadState> _dead_state;\n","filename":"src\/hotspot\/share\/gc\/shared\/stringdedup\/stringDedupTable.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,577 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_ATOMIC_HPP\n+#define SHARE_RUNTIME_ATOMIC_HPP\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#include <type_traits>\n+\n+\/\/ Atomic<T> is used to declare a variable of type T with atomic access.\n+\/\/\n+\/\/ The following value types T are supported:\n+\/\/\n+\/\/ (1) Integers with sizeof the same as sizeof int32_t or int64_t. These are\n+\/\/ referred to as atomic integers below.\n+\/\/\n+\/\/ (2) Integers with sizeof 1, including bool. These are referred to as atomic\n+\/\/ bytes below.\n+\/\/\n+\/\/ (3) Pointers. These are referred to as atomic pointers below.\n+\/\/\n+\/\/ (4) Types with a PrimitiveValues::Translate definition. These are referred\n+\/\/ to as atomic translated types below. The atomic value for the associated\n+\/\/ decayed type is referred to as the atomic decayed type.\n+\/\/\n+\/\/ The interface provided by an Atomic<T> depends on the value type.\n+\/\/\n+\/\/ If T is the value type, v is an Atomic<T>, x and y are instances of T, i is\n+\/\/ an integer, and o is an atomic_memory_order, then:\n+\/\/\n+\/\/ (1) All Atomic types provide\n+\/\/\n+\/\/   nested types:\n+\/\/     ValueType -> T\n+\/\/\n+\/\/   special functions:\n+\/\/     explicit constructor(T)\n+\/\/     noncopyable\n+\/\/     destructor\n+\/\/\n+\/\/   static member functions:\n+\/\/     value_offset_in_bytes() -> int   \/\/ constexpr\n+\/\/     value_size_in_bytes() -> int     \/\/ constexpr\n+\/\/       These provide the compiler and the like with direct access to the\n+\/\/       value field. They shouldn't be used directly to bypass normal access.\n+\/\/\n+\/\/   member functions:\n+\/\/     v.load_relaxed() -> T\n+\/\/     v.load_acquire() -> T\n+\/\/     v.relaxed_store(x) -> void\n+\/\/     v.release_store(x) -> void\n+\/\/     v.release_store_fence(x) -> void\n+\/\/     v.cmpxchg(x, y [, o]) -> T\n+\/\/\n+\/\/ (2) All atomic types are default constructible.\n+\/\/\n+\/\/ Default construction of an atomic integer or atomic byte initializes the\n+\/\/ value to zero. Default construction of an atomic pointer initializes the\n+\/\/ value to null.\n+\/\/\n+\/\/ If the value type of an atomic translated type is default constructible,\n+\/\/ then default construction of the atomic translated type will initialize the\n+\/\/ value to a default constructed object of the value type. Otherwise, the\n+\/\/ value will be initialized as if by translating the value that would be\n+\/\/ provided by default constructing an atomic type for the value type's\n+\/\/ decayed type.\n+\n+\/\/ (3) Atomic pointers and atomic integers additionally provide\n+\/\/\n+\/\/   member functions:\n+\/\/     v.fetch_then_set(x [, o]) -> T\n+\/\/     v.add_then_fetch(i [, o]) -> T\n+\/\/     v.sub_then_fetch(i [, o]) -> T\n+\/\/     v.fetch_then_add(i [, o]) -> T\n+\/\/     v.fetch_then_sub(i [, o]) -> T\n+\/\/     v.atomic_inc([o]) -> void\n+\/\/     v.atomic_dec([o]) -> void\n+\/\/\n+\/\/ sizeof(i) must not exceed sizeof(T). For atomic integers, both T and the\n+\/\/ type of i must be signed, or both must be unsigned. Atomic pointers perform\n+\/\/ element arithmetic.\n+\/\/\n+\/\/ (4) An atomic translated type additionally provides the fetch_then_set\n+\/\/ function if its associated atomic decayed type provides that function.\n+\/\/\n+\/\/ (5) Atomic integers additionally provide\n+\/\/\n+\/\/   member functions:\n+\/\/     v.and_then_fetch(x [, o]) -> T\n+\/\/     v.or_then_fetch(x [, o]) -> T\n+\/\/     v.xor_then_fetch(x [, o]) -> T\n+\/\/     v.fetch_then_and(x [, o]) -> T\n+\/\/     v.fetch_then_or(x [, o]) -> T\n+\/\/     v.fetch_then_xor(x [, o]) -> T\n+\/\/\n+\/\/ (6) Atomic pointers additionally provide\n+\/\/\n+\/\/   nested types:\n+\/\/     ElementType -> std::remove_pointer_t<T>\n+\/\/\n+\/\/   member functions:\n+\/\/     v.replace_if_null(x [, o]) -> bool\n+\/\/     v.clear_if_equal(x [, o]) -> bool\n+\/\/\n+\/\/ Some of the function names provided by (some variants of) Atomic<T> differ\n+\/\/ from the corresponding functions provided by the AtomicAccess class. In\n+\/\/ some cases this is done for regularity; there are some inconsistencies in\n+\/\/ the AtomicAccess names. Some of the naming choices are also to make them\n+\/\/ stand out a little more when used in surrounding non-atomic code. Without\n+\/\/ the \"AtomicAccess::\" qualifier, some of those names are easily overlooked.\n+\/\/\n+\/\/ Atomic bytes don't provide fetch_then_set. This is because that operation\n+\/\/ hasn't been implemented for 1 byte values. That could be changed if needed.\n+\/\/\n+\/\/ Atomic for 2 byte integers is not supported. This is because atomic\n+\/\/ operations of that size have not been implemented. There haven't been\n+\/\/ required use-cases. Many platforms don't provide hardware support.\n+\/\/\n+\/\/ Atomic translated types don't provide the full interface of the associated\n+\/\/ atomic decayed type. They could do so, perhaps under the control of an\n+\/\/ associated type trait.\n+\/\/\n+\/\/ Atomic<T> is not intended to be anything approaching a drop-in replacement\n+\/\/ for std::atomic<T>. Rather, it's wrapping up a long-established HotSpot\n+\/\/ idiom in a tidier and more rigorous package. Some of the differences from\n+\/\/ std::atomic<T> include\n+\/\/\n+\/\/ * Atomic<T> supports a much more limited set of value types.\n+\/\/\n+\/\/ * All supported Atomic<T> types are \"lock free\", so the standard mechanisms\n+\/\/ for testing for that are not provided. (There might have been some types on\n+\/\/ some platforms that used a lock long-ago, but that's no longer the case.)\n+\/\/\n+\/\/ * Rather than load\/store operations with a memory order parameter,\n+\/\/ Atomic<T> provides load_{relaxed,acquire}() and {relaxed,release}_store()\n+\/\/ operations, as well as release_store_fence().\n+\/\/\n+\/\/ * Atomic<T> doesn't provide operator overloads that perform various\n+\/\/ operations with sequentially consistent ordering semantics. The rationale\n+\/\/ for not providing these is similar to that for having different (often\n+\/\/ longer) names for some operations than the corresponding AtomicAccess\n+\/\/ functions.\n+\n+\/\/ Implementation support for Atomic<T>.\n+class AtomicImpl {\n+  enum class Category {\n+    Integer,\n+    Byte,\n+    Pointer,\n+    Translated\n+  };\n+\n+#if defined(__GNUC__) && !defined(__clang__)\n+  \/\/ Workaround for gcc bug. Make category() public, else we get this error\n+  \/\/   error: 'static constexpr AtomicImpl::Category AtomicImpl::category()\n+  \/\/     [with T = unsigned int]' is private within this context\n+  \/\/ The only reference is the default template parameter value in the Atomic\n+  \/\/ class a couple lines below, in this same class!\n+  \/\/ https:\/\/gcc.gnu.org\/bugzilla\/show_bug.cgi?id=122098\n+public:\n+#endif\n+  \/\/ Selection of Atomic<T> category, based on T.\n+  template<typename T>\n+  static constexpr Category category();\n+private:\n+\n+  \/\/ Helper base classes, providing various parts of the APIs.\n+  template<typename T> class CommonCore;\n+  template<typename T> class SupportsFetchThenSet;\n+  template<typename T> class SupportsArithmetic;\n+\n+  \/\/ Support conditional fetch_then_set() for atomic translated types.\n+  template<typename T> class HasFetchThenSet;\n+  template<typename T> class DecayedHasFetchThenSet;\n+  template<typename Derived, typename T, bool = DecayedHasFetchThenSet<T>::value>\n+  class TranslatedFetchThenSet;\n+\n+public:\n+  template<typename T, Category = category<T>()>\n+  class Atomic;\n+};\n+\n+\/\/ The Atomic<T> type.\n+template<typename T>\n+using Atomic = AtomicImpl::Atomic<T>;\n+\n+template<typename T>\n+constexpr auto AtomicImpl::category() -> Category {\n+  static_assert(std::is_same_v<T, std::remove_cv_t<T>>,\n+                \"Value type must not be cv-qualified\");\n+  if constexpr (std::is_integral_v<T>) {\n+    if constexpr ((sizeof(T) == sizeof(int32_t)) || (sizeof(T) == sizeof(int64_t))) {\n+      return Category::Integer;\n+    } else if constexpr (sizeof(T) == 1) {\n+      return Category::Byte;\n+    } else {\n+      static_assert(DependentAlwaysFalse<T>, \"Invalid atomic integer type\");\n+    }\n+  } else if constexpr (std::is_pointer_v<T>) {\n+    return Category::Pointer;\n+  } else if constexpr (PrimitiveConversions::Translate<T>::value) {\n+    return Category::Translated;\n+  } else {\n+    static_assert(DependentAlwaysFalse<T>, \"Invalid atomic value type\");\n+  }\n+}\n+\n+\/\/ Atomic<T> implementation classes.\n+\n+template<typename T>\n+class AtomicImpl::CommonCore {\n+  T volatile _value;\n+\n+protected:\n+  explicit CommonCore(T value) : _value(value) {}\n+  ~CommonCore() = default;\n+\n+  T volatile* value_ptr() { return &_value; }\n+  T const volatile* value_ptr() const { return &_value; }\n+\n+  \/\/ Support for value_offset_in_bytes.\n+  template<typename Derived>\n+  static constexpr int value_offset_in_bytes_impl() {\n+    return offsetof(Derived, _value);\n+  }\n+\n+public:\n+  NONCOPYABLE(CommonCore);\n+\n+  static constexpr int value_size_in_bytes() {\n+    return sizeof(_value);\n+  }\n+\n+  \/\/ Common core Atomic<T> operations.\n+\n+  T load_relaxed() const {\n+    return AtomicAccess::load(value_ptr());\n+  }\n+\n+  T load_acquire() const {\n+    return AtomicAccess::load_acquire(value_ptr());\n+  }\n+\n+  void relaxed_store(T value) {\n+    AtomicAccess::store(value_ptr(), value);\n+  }\n+\n+  void release_store(T value) {\n+    AtomicAccess::release_store(value_ptr(), value);\n+  }\n+\n+  void release_store_fence(T value) {\n+    AtomicAccess::release_store_fence(value_ptr(), value);\n+  }\n+\n+  T cmpxchg(T compare_value, T new_value,\n+            atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::cmpxchg(value_ptr(), compare_value, new_value);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::SupportsFetchThenSet : public CommonCore<T> {\n+  using Base = CommonCore<T>;\n+\n+protected:\n+  explicit SupportsFetchThenSet(T value) : Base(value) {}\n+  ~SupportsFetchThenSet() = default;\n+\n+public:\n+  T fetch_then_set(T new_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::xchg(this->value_ptr(), new_value);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::SupportsArithmetic : public SupportsFetchThenSet<T> {\n+  using Base = SupportsFetchThenSet<T>;\n+\n+  \/\/ Guarding the AtomicAccess calls with constexpr checking of I produces\n+  \/\/ better compile-time error messages.\n+  template<typename I>\n+  static constexpr bool check_i() {\n+    static_assert(sizeof(I) <= sizeof(T), \"offset size exceeds value size\");\n+    if constexpr (!std::is_integral_v<T>) {\n+      static_assert(std::is_pointer_v<T>, \"must be\");\n+    } else if constexpr (std::is_signed_v<T>) {\n+      static_assert(std::is_signed_v<I>, \"value is signed but offset is unsigned\");\n+    } else {\n+      static_assert(std::is_unsigned_v<I>, \"value is unsigned but offset is signed\");\n+    }\n+    return true;\n+  }\n+\n+protected:\n+  explicit SupportsArithmetic(T value) : Base(value) {}\n+  ~SupportsArithmetic() = default;\n+\n+public:\n+  template<typename I>\n+  T add_then_fetch(I add_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      return AtomicAccess::add(this->value_ptr(), add_value, order);\n+    }\n+  }\n+\n+  template<typename I>\n+  T fetch_then_add(I add_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      return AtomicAccess::fetch_then_add(this->value_ptr(), add_value, order);\n+    }\n+  }\n+\n+  template<typename I>\n+  T sub_then_fetch(I sub_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      return AtomicAccess::sub(this->value_ptr(), sub_value, order);\n+    }\n+  }\n+\n+  template<typename I>\n+  T fetch_then_sub(I sub_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    if constexpr (check_i<I>()) {\n+      \/\/ AtomicAccess doesn't currently provide fetch_then_sub.\n+      return sub_then_fetch(sub_value, order) + sub_value;\n+    }\n+  }\n+\n+  void atomic_inc(atomic_memory_order order = memory_order_conservative) {\n+    AtomicAccess::inc(this->value_ptr(), order);\n+  }\n+\n+  void atomic_dec(atomic_memory_order order = memory_order_conservative) {\n+    AtomicAccess::dec(this->value_ptr(), order);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Integer>\n+  : public SupportsArithmetic<T>\n+{\n+  using Base = SupportsArithmetic<T>;\n+\n+public:\n+  explicit Atomic(T value = 0) : Base(value) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  using ValueType = T;\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return CommonCore<T>::template value_offset_in_bytes_impl<Atomic>();\n+  }\n+\n+  T fetch_then_and(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::fetch_then_and(this->value_ptr(), bits, order);\n+  }\n+\n+  T fetch_then_or(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::fetch_then_or(this->value_ptr(), bits, order);\n+  }\n+\n+  T fetch_then_xor(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::fetch_then_xor(this->value_ptr(), bits, order);\n+  }\n+\n+  T and_then_fetch(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::and_then_fetch(this->value_ptr(), bits, order);\n+  }\n+\n+  T or_then_fetch(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::or_then_fetch(this->value_ptr(), bits, order);\n+  }\n+\n+  T xor_then_fetch(T bits, atomic_memory_order order = memory_order_conservative) {\n+    return AtomicAccess::xor_then_fetch(this->value_ptr(), bits, order);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Byte>\n+  : public CommonCore<T>\n+{\n+  using Base = CommonCore<T>;\n+\n+public:\n+  explicit Atomic(T value = 0) : Base(value) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  using ValueType = T;\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return CommonCore<T>::template value_offset_in_bytes_impl<Atomic>();\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Pointer>\n+  : public SupportsArithmetic<T>\n+{\n+  using Base = SupportsArithmetic<T>;\n+\n+public:\n+  explicit Atomic(T value = nullptr) : Base(value) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  using ValueType = T;\n+  using ElementType = std::remove_pointer_t<T>;\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return CommonCore<T>::template value_offset_in_bytes_impl<Atomic>();\n+  }\n+\n+  bool replace_if_null(T new_value,\n+                       atomic_memory_order order = memory_order_conservative) {\n+    return nullptr == this->cmpxchg(nullptr, new_value, order);\n+  }\n+\n+  bool clear_if_equal(T compare_value,\n+                      atomic_memory_order order = memory_order_conservative) {\n+    return compare_value == this->cmpxchg(compare_value, nullptr, order);\n+  }\n+};\n+\n+\/\/ Atomic translated type\n+\n+\/\/ Test whether Atomic<T> has fetch_then_set().\n+template<typename T>\n+class AtomicImpl::HasFetchThenSet {\n+  template<typename Check> static char* test(decltype(&Check::fetch_then_set));\n+  template<typename> static char test(...);\n+  using test_type = decltype(test<Atomic<T>>(nullptr));\n+public:\n+  static constexpr bool value = std::is_pointer_v<test_type>;\n+};\n+\n+\/\/ Test whether the atomic decayed type associated with T has fetch_then_set().\n+template<typename T>\n+class AtomicImpl::DecayedHasFetchThenSet {\n+  using Translator = PrimitiveConversions::Translate<T>;\n+  using Decayed = typename Translator::Decayed;\n+\n+  \/\/ \"Unit test\" HasFetchThenSet<>.\n+  static_assert(HasFetchThenSet<int>::value);\n+  static_assert(HasFetchThenSet<int*>::value);\n+  static_assert(!HasFetchThenSet<char>::value);\n+\n+public:\n+  static constexpr bool value = HasFetchThenSet<Decayed>::value;\n+};\n+\n+\/\/ Base class for atomic translated type if atomic decayed type doesn't have\n+\/\/ fetch_then_set().\n+template<typename Derived, typename T, bool>\n+class AtomicImpl::TranslatedFetchThenSet {};\n+\n+\/\/ Base class for atomic translated type if atomic decayed type does have\n+\/\/ fetch_then_set().\n+template<typename Derived, typename T>\n+class AtomicImpl::TranslatedFetchThenSet<Derived, T, true> {\n+public:\n+  T fetch_then_set(T new_value,\n+                   atomic_memory_order order = memory_order_conservative) {\n+    return static_cast<Derived*>(this)->fetch_then_set_impl(new_value, order);\n+  }\n+};\n+\n+template<typename T>\n+class AtomicImpl::Atomic<T, AtomicImpl::Category::Translated>\n+  : public TranslatedFetchThenSet<Atomic<T>, T>\n+{\n+  \/\/ Give TranslatedFetchThenSet<> access to fetch_then_set_impl() if needed.\n+  friend class TranslatedFetchThenSet<Atomic<T>, T>;\n+\n+  using Translator = PrimitiveConversions::Translate<T>;\n+  using Decayed = typename Translator::Decayed;\n+\n+  Atomic<Decayed> _value;\n+\n+  static Decayed decay(T x) { return Translator::decay(x); }\n+  static T recover(Decayed x) { return Translator::recover(x); }\n+\n+public:\n+  using ValueType = T;\n+\n+  \/\/ If T is default constructible, construct from a default constructed T.\n+  template<typename Dep = T, ENABLE_IF(std::is_default_constructible_v<Dep>)>\n+  Atomic() : Atomic(T()) {}\n+\n+  \/\/ If T is not default constructible, default construct the underlying\n+  \/\/ Atomic<Decayed>.\n+  template<typename Dep = T, ENABLE_IF(!std::is_default_constructible_v<Dep>)>\n+  Atomic() : _value() {}\n+\n+  explicit Atomic(T value) : _value(decay(value)) {}\n+\n+  NONCOPYABLE(Atomic);\n+\n+  static constexpr int value_offset_in_bytes() {\n+    return (offsetof(Atomic, _value) +\n+            Atomic<Decayed>::value_offset_in_bytes());\n+  }\n+\n+  static constexpr int value_size_in_bytes() {\n+    return Atomic<Decayed>::value_size_in_bytes();\n+  }\n+\n+  T load_relaxed() const {\n+    return recover(_value.load_relaxed());\n+  }\n+\n+  T load_acquire() const {\n+    return recover(_value.load_acquire());\n+  }\n+\n+  void relaxed_store(T value) {\n+    _value.relaxed_store(decay(value));\n+  }\n+\n+  void release_store(T value) {\n+    _value.release_store(decay(value));\n+  }\n+\n+  void release_store_fence(T value) {\n+    _value.release_store_fence(decay(value));\n+  }\n+\n+  T cmpxchg(T compare_value, T new_value,\n+            atomic_memory_order order = memory_order_conservative) {\n+    return recover(_value.cmpxchg(decay(compare_value),\n+                                  decay(new_value),\n+                                  order));\n+  }\n+\n+private:\n+  \/\/ Implementation of fetch_then_set() if needed.\n+  \/\/ Exclude when not needed, to prevent reference to non-existent function\n+  \/\/ of atomic decayed type if someone explicitly instantiates Atomic<T>.\n+  template<typename Dep = Decayed, ENABLE_IF(HasFetchThenSet<Dep>::value)>\n+  T fetch_then_set_impl(T new_value, atomic_memory_order order) {\n+    return recover(_value.fetch_then_set(decay(new_value), order));\n+  }\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_ATOMIC_HPP\n","filename":"src\/hotspot\/share\/runtime\/atomic.hpp","additions":577,"deletions":0,"binary":false,"changes":577,"status":"added"},{"patch":"@@ -0,0 +1,83 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_ATOMICNEXTACCESS_HPP\n+#define SHARE_UTILITIES_ATOMICNEXTACCESS_HPP\n+\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n+\n+\/\/ A helper class for LockFreeStack and similar intrusive-list style data\n+\/\/ structures that involve atomicity.  These classes require the list element\n+\/\/ provide a function pointer template parameter for getting the \"next\" field\n+\/\/ from an element object.  That function pointer may take one of two forms,\n+\/\/ where T is the element type:\n+\/\/\n+\/\/ - T* volatile* (*)(T&)\n+\/\/ - Atomic<T>* (*)(T&)\n+\/\/\n+\/\/ Specializations of this class provide functions that manipulate the next\n+\/\/ field according to the access mechanism.\n+template<typename T, auto next_access>\n+struct AtomicNextAccess;\n+\n+template<typename T, T* volatile* (*next_access)(T&)>\n+struct AtomicNextAccess<T, next_access> {\n+  static T* next(const T& value) {\n+    return AtomicAccess::load(next_access(const_cast<T&>(value)));\n+  }\n+\n+  static T* next_acquire(const T& value) {\n+    return AtomicAccess::load_acquire(next_access(const_cast<T&>(value)));\n+  }\n+\n+  static void set_next(T& value, T* new_next) {\n+    AtomicAccess::store(next_access(value), new_next);\n+  }\n+\n+  static T* cmpxchg(T& value, const T* compare, T* exchange) {\n+    return AtomicAccess::cmpxchg(next_access(value), compare, exchange);\n+  }\n+};\n+\n+template<typename T, Atomic<T*>* (*next_access)(T&)>\n+struct AtomicNextAccess<T, next_access> {\n+  static T* next(const T& value) {\n+    return next_access(const_cast<T&>(value))->load_relaxed();\n+  }\n+\n+  static T* next_acquire(const T& value) {\n+    return next_access(const_cast<T&>(value))->load_acquire();\n+  }\n+\n+  static void set_next(T& value, T* new_next) {\n+    next_access(value)->relaxed_store(new_next);\n+  }\n+\n+  static T* cmpxchg(T& value, const T* compare, T* exchange) {\n+    return next_access(value)->cmpxchg(compare, exchange);\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_ATOMICNEXTACCESS_HPP\n","filename":"src\/hotspot\/share\/utilities\/atomicNextAccess.hpp","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -1368,0 +1368,8 @@\n+\/\/ This provides a workaround for static_assert(false) in discarded or\n+\/\/ otherwise uninstantiated places.  Instead use\n+\/\/   static_assert(DependentAlwaysFalse<T>, \"...\")\n+\/\/ See http:\/\/wg21.link\/p2593r1. Some, but not all, compiler versions we're\n+\/\/ using have implemented that change as a DR:\n+\/\/ https:\/\/cplusplus.github.io\/CWG\/issues\/2518.html\n+template<typename T> inline constexpr bool DependentAlwaysFalse = false;\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"runtime\/atomicAccess.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/atomicNextAccess.hpp\"\n@@ -37,5 +38,8 @@\n-\/\/ To be used in a LockFreeStack of objects of type T, an object of\n-\/\/ type T must have a list entry member of type T* volatile, with an\n-\/\/ non-member accessor function returning a pointer to that member.  A\n-\/\/ LockFreeStack is associated with the class of its elements and an\n-\/\/ entry member from that class.\n+\/\/ To be used in a LockFreeStack of objects of type T, an object of type T\n+\/\/ must have a list entry member. A list entry member is a data member whose\n+\/\/ type is either (1) Atomic<T*>, or (2) T* volatile. There must be a\n+\/\/ non-member or static member function returning a pointer to that member,\n+\/\/ which is used to provide access to it by a LockFreeStack.  A LockFreeStack\n+\/\/ is associated with the class of its elements and an entry member from that\n+\/\/ class by being specialized on the element class and a pointer to the\n+\/\/ function for accessing that entry member.\n@@ -55,1 +59,1 @@\n-\/\/ \\tparam next_ptr is a function pointer.  Applying this function to\n+\/\/ \\tparam next_access is a function pointer.  Applying this function to\n@@ -58,1 +62,1 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n+template<typename T, auto next_access>\n@@ -60,1 +64,1 @@\n-  T* volatile _top;\n+  Atomic<T*> _top;\n@@ -68,1 +72,1 @@\n-      cur = AtomicAccess::cmpxchg(&_top, cur, first);\n+      cur = _top.cmpxchg(cur, first);\n@@ -92,1 +96,1 @@\n-      result = AtomicAccess::cmpxchg(&_top, result, new_top);\n+      result = _top.cmpxchg(result, new_top);\n@@ -104,1 +108,1 @@\n-    return AtomicAccess::xchg(&_top, (T*)nullptr);\n+    return _top.fetch_then_set(nullptr);\n@@ -148,1 +152,1 @@\n-  T* top() const { return AtomicAccess::load(&_top); }\n+  T* top() const { return _top.load_relaxed(); }\n@@ -163,1 +167,1 @@\n-    return AtomicAccess::load(next_ptr(const_cast<T&>(value)));\n+    return AtomicNextAccess<T, next_access>::next(value);\n@@ -171,1 +175,1 @@\n-    AtomicAccess::store(next_ptr(value), new_next);\n+    AtomicNextAccess<T, next_access>::set_next(value, new_next);\n","filename":"src\/hotspot\/share\/utilities\/lockFreeStack.hpp","additions":19,"deletions":15,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,2 @@\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/atomicNextAccess.hpp\"\n@@ -36,2 +38,3 @@\n-\/\/ the link to the next element provided by a member of each element.\n-\/\/ Access to this member is provided by the next_ptr function.\n+\/\/ the link to the next element provided by a member of each element. The\n+\/\/ type of this list entry member must be either (1) Atomic<T*>, or\n+\/\/ (2) T* volatile. The next_access template parameter provides access to it.\n@@ -58,1 +61,1 @@\n-\/\/ \\tparam next_ptr is a function pointer.  Applying this function to\n+\/\/ \\tparam next_access is a function pointer.  Applying this function to\n@@ -61,1 +64,1 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n+template<typename T, auto next_access>\n@@ -63,1 +66,1 @@\n-  T* volatile _head;\n+  Atomic<T*> _head;\n@@ -66,1 +69,1 @@\n-  T* volatile _tail;\n+  Atomic<T*> _tail;\n@@ -70,0 +73,2 @@\n+  using NextAccess = AtomicNextAccess<T, next_access>;\n+\n","filename":"src\/hotspot\/share\/utilities\/nonblockingQueue.hpp","additions":12,"deletions":7,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"utilities\/atomicNextAccess.hpp\"\n@@ -32,3 +34,3 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-T* NonblockingQueue<T, next_ptr>::next(const T& node) {\n-  return AtomicAccess::load(next_ptr(const_cast<T&>(node)));\n+template<typename T, auto next_access>\n+T* NonblockingQueue<T, next_access>::next(const T& node) {\n+  return NextAccess::next(node);\n@@ -37,3 +39,3 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-void NonblockingQueue<T, next_ptr>::set_next(T& node, T* new_next) {\n-  AtomicAccess::store(next_ptr(node), new_next);\n+template<typename T, auto next_access>\n+void NonblockingQueue<T, next_access>::set_next(T& node, T* new_next) {\n+  NextAccess::set_next(node, new_next);\n@@ -42,2 +44,3 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-NonblockingQueue<T, next_ptr>::NonblockingQueue() : _head(nullptr), _tail(nullptr) {}\n+template<typename T, auto next_access>\n+NonblockingQueue<T, next_access>::NonblockingQueue()\n+  : _head(nullptr), _tail(nullptr) {}\n@@ -46,4 +49,4 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-NonblockingQueue<T, next_ptr>::~NonblockingQueue() {\n-  assert(_head == nullptr, \"precondition\");\n-  assert(_tail == nullptr, \"precondition\");\n+template<typename T, auto next_access>\n+NonblockingQueue<T, next_access>::~NonblockingQueue() {\n+  assert(_head.load_relaxed() == nullptr, \"precondition\");\n+  assert(_tail.load_relaxed() == nullptr, \"precondition\");\n@@ -56,2 +59,2 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-T* NonblockingQueue<T, next_ptr>::end_marker() const {\n+template<typename T, auto next_access>\n+T* NonblockingQueue<T, next_access>::end_marker() const {\n@@ -61,3 +64,3 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-T* NonblockingQueue<T, next_ptr>::first() const {\n-  T* head = AtomicAccess::load(&_head);\n+template<typename T, auto next_access>\n+T* NonblockingQueue<T, next_access>::first() const {\n+  T* head = _head.load_relaxed();\n@@ -67,2 +70,2 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-bool NonblockingQueue<T, next_ptr>::is_end(const T* entry) const {\n+template<typename T, auto next_access>\n+bool NonblockingQueue<T, next_access>::is_end(const T* entry) const {\n@@ -72,3 +75,3 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-bool NonblockingQueue<T, next_ptr>::empty() const {\n-  return AtomicAccess::load(&_head) == nullptr;\n+template<typename T, auto next_access>\n+bool NonblockingQueue<T, next_access>::empty() const {\n+  return _head.load_relaxed() == nullptr;\n@@ -77,2 +80,2 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-size_t NonblockingQueue<T, next_ptr>::length() const {\n+template<typename T, auto next_access>\n+size_t NonblockingQueue<T, next_access>::length() const {\n@@ -101,2 +104,2 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-void NonblockingQueue<T, next_ptr>::append(T& first, T& last) {\n+template<typename T, auto next_access>\n+void NonblockingQueue<T, next_access>::append(T& first, T& last) {\n@@ -108,1 +111,1 @@\n-  T* old_tail = AtomicAccess::xchg(&_tail, &last);\n+  T* old_tail = _tail.fetch_then_set(&last);\n@@ -113,1 +116,1 @@\n-    assert(AtomicAccess::load(&_head) == nullptr, \"invariant\");\n+    assert(_head.load_relaxed() == nullptr, \"invariant\");\n@@ -115,1 +118,1 @@\n-  } else if (is_end(AtomicAccess::cmpxchg(next_ptr(*old_tail), end_marker(), &first))) {\n+  } else if (is_end(NextAccess::cmpxchg(*old_tail, end_marker(), &first))) {\n@@ -131,1 +134,1 @@\n-    DEBUG_ONLY(T* old_head = AtomicAccess::load(&_head);)\n+    DEBUG_ONLY(T* old_head = _head.load_relaxed();)\n@@ -137,1 +140,1 @@\n-  AtomicAccess::store(&_head, &first);\n+  _head.relaxed_store(&first);\n@@ -140,2 +143,2 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-bool NonblockingQueue<T, next_ptr>::try_pop(T** node_ptr) {\n+template<typename T, auto next_access>\n+bool NonblockingQueue<T, next_access>::try_pop(T** node_ptr) {\n@@ -144,1 +147,1 @@\n-  T* old_head = AtomicAccess::load_acquire(&_head);\n+  T* old_head = _head.load_acquire();\n@@ -150,1 +153,1 @@\n-  T* next_node = AtomicAccess::load_acquire(next_ptr(*old_head));\n+  T* next_node = NextAccess::next_acquire(*old_head);\n@@ -163,1 +166,1 @@\n-    if (old_head != AtomicAccess::cmpxchg(&_head, old_head, next_node)) {\n+    if (old_head != _head.cmpxchg(old_head, next_node)) {\n@@ -191,1 +194,1 @@\n-  } else if (is_end(AtomicAccess::cmpxchg(next_ptr(*old_head), next_node, (T*)nullptr))) {\n+  } else if (is_end(NextAccess::cmpxchg(*old_head, next_node, nullptr))) {\n@@ -206,1 +209,1 @@\n-    AtomicAccess::cmpxchg(&_head, old_head, (T*)nullptr);\n+    _head.cmpxchg(old_head, nullptr);\n@@ -212,1 +215,1 @@\n-    AtomicAccess::cmpxchg(&_tail, old_head, (T*)nullptr);\n+    _tail.cmpxchg(old_head, nullptr);\n@@ -227,2 +230,2 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-T* NonblockingQueue<T, next_ptr>::pop() {\n+template<typename T, auto next_access>\n+T* NonblockingQueue<T, next_access>::pop() {\n@@ -238,3 +241,3 @@\n-template<typename T, T* volatile* (*next_ptr)(T&)>\n-Pair<T*, T*> NonblockingQueue<T, next_ptr>::take_all() {\n-  T* tail = AtomicAccess::load(&_tail);\n+template<typename T, auto next_access>\n+Pair<T*, T*> NonblockingQueue<T, next_access>::take_all() {\n+  T* tail = _tail.load_relaxed();\n@@ -242,3 +245,3 @@\n-  Pair<T*, T*> result(AtomicAccess::load(&_head), tail);\n-  AtomicAccess::store(&_head, (T*)nullptr);\n-  AtomicAccess::store(&_tail, (T*)nullptr);\n+  Pair<T*, T*> result(_head.load_relaxed(), tail);\n+  _head.relaxed_store(nullptr);\n+  _tail.relaxed_store(nullptr);\n","filename":"src\/hotspot\/share\/utilities\/nonblockingQueue.inline.hpp","additions":50,"deletions":47,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  assert(AtomicAccess::add(&_writers, 1u) == 1u, \"multiple writers\");\n+  assert(_writers.fetch_then_add(1u) == 0u, \"multiple writers\");\n@@ -53,1 +53,1 @@\n-  uint value = _enter;\n+  uint value = _enter.load_relaxed();\n@@ -56,1 +56,1 @@\n-  volatile uint* new_ptr = &_exit[(value + 1) & 1];\n+  Atomic<uint>& new_exit = _exit[(value + 1) & 1];\n@@ -65,2 +65,2 @@\n-    *new_ptr = ++value;\n-    value = AtomicAccess::cmpxchg(&_enter, old, value);\n+    new_exit.relaxed_store(++value);\n+    value = _enter.cmpxchg(old, value);\n@@ -71,2 +71,2 @@\n-  volatile uint* old_ptr = &_exit[old & 1];\n-  assert(old_ptr != new_ptr, \"invariant\");\n+  Atomic<uint>& old_exit = _exit[old & 1];\n+  assert(&new_exit != &old_exit, \"invariant\");\n@@ -77,1 +77,1 @@\n-  _waiting_for = old;\n+  _waiting_for.relaxed_store(old);\n@@ -84,1 +84,1 @@\n-  \/\/ to complete, e.g. for the value of old_ptr to catch up with old.\n+  \/\/ to complete, e.g. for the value of old_exit to catch up with old.\n@@ -87,1 +87,1 @@\n-  while (old != AtomicAccess::load_acquire(old_ptr)) {\n+  while (old != old_exit.load_acquire()) {\n@@ -98,1 +98,1 @@\n-  DEBUG_ONLY(AtomicAccess::dec(&_writers);)\n+  DEBUG_ONLY(_writers.atomic_dec();)\n","filename":"src\/hotspot\/share\/utilities\/singleWriterSynchronizer.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomicAccess.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -51,3 +51,3 @@\n-  volatile uint _enter;\n-  volatile uint _exit[2];\n-  volatile uint _waiting_for;\n+  Atomic<uint> _enter;\n+  Atomic<uint> _exit[2];\n+  Atomic<uint> _waiting_for;\n@@ -56,1 +56,1 @@\n-  DEBUG_ONLY(volatile uint _writers;)\n+  DEBUG_ONLY(Atomic<uint> _writers;)\n@@ -90,1 +90,1 @@\n-  return AtomicAccess::add(&_enter, 2u);\n+  return _enter.add_then_fetch(2u);\n@@ -94,1 +94,1 @@\n-  uint exit_value = AtomicAccess::add(&_exit[enter_value & 1], 2u);\n+  uint exit_value = _exit[enter_value & 1].add_then_fetch(2u);\n@@ -98,1 +98,1 @@\n-  if (exit_value == _waiting_for) {\n+  if (exit_value == _waiting_for.load_relaxed()) {\n","filename":"src\/hotspot\/share\/utilities\/singleWriterSynchronizer.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,695 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"metaprogramming\/primitiveConversions.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+#include <type_traits>\n+\n+#include \"unittest.hpp\"\n+\n+\/\/ These tests of Atomic<T> only verify functionality.  They don't verify\n+\/\/ atomicity.\n+\n+template<typename T>\n+struct AtomicIntegerArithmeticTestSupport {\n+  Atomic<T> _test_value;\n+\n+  static constexpr T _old_value =    static_cast<T>(UCONST64(0x2000000020000));\n+  static constexpr T _change_value = static_cast<T>(UCONST64(    0x100000001));\n+\n+  AtomicIntegerArithmeticTestSupport() : _test_value(0) {}\n+\n+  void fetch_then_add() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value + _change_value;\n+    T result = _test_value.fetch_then_add(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_sub() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value - _change_value;\n+    T result = _test_value.fetch_then_sub(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void add_then_fetch() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value + _change_value;\n+    T result = _test_value.add_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void sub_then_fetch() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value - _change_value;\n+    T result = _test_value.sub_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void atomic_inc() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value + 1;\n+    _test_value.atomic_inc();\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void atomic_dec() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value - 1;\n+    _test_value.atomic_dec();\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+#define TEST_ARITHMETIC(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_ARITHMETIC(fetch_then_add)\n+    TEST_ARITHMETIC(fetch_then_sub)\n+    TEST_ARITHMETIC(add_then_fetch)\n+    TEST_ARITHMETIC(sub_then_fetch)\n+    TEST_ARITHMETIC(atomic_inc)\n+    TEST_ARITHMETIC(atomic_dec)\n+  }\n+\n+#undef TEST_ARITHMETIC\n+};\n+\n+TEST_VM(AtomicIntegerTest, arith_int32) {\n+  AtomicIntegerArithmeticTestSupport<int32_t>()();\n+}\n+\n+TEST_VM(AtomicIntegerTest, arith_uint32) {\n+  AtomicIntegerArithmeticTestSupport<uint32_t>()();\n+}\n+\n+TEST_VM(AtomicIntegerTest, arith_int64) {\n+  AtomicIntegerArithmeticTestSupport<int64_t>()();\n+}\n+\n+TEST_VM(AtomicIntegerTest, arith_uint64) {\n+  AtomicIntegerArithmeticTestSupport<uint64_t>()();\n+}\n+\n+template<typename T>\n+struct AtomicIntegerXchgTestSupport {\n+  Atomic<T> _test_value;\n+\n+  AtomicIntegerXchgTestSupport() : _test_value{} {}\n+\n+  void test() {\n+    T zero = 0;\n+    T five = 5;\n+    _test_value.relaxed_store(zero);\n+    T res = _test_value.fetch_then_set(five);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(five, _test_value.load_relaxed());\n+  }\n+};\n+\n+TEST_VM(AtomicIntegerTest, xchg_int32) {\n+  using Support = AtomicIntegerXchgTestSupport<int32_t>;\n+  Support().test();\n+}\n+\n+TEST_VM(AtomicIntegerTest, xchg_int64) {\n+  using Support = AtomicIntegerXchgTestSupport<int64_t>;\n+  Support().test();\n+}\n+\n+template<typename T>\n+struct AtomicIntegerCmpxchgTestSupport {\n+  Atomic<T> _test_value;\n+\n+  AtomicIntegerCmpxchgTestSupport() : _test_value{} {}\n+\n+  void test() {\n+    T zero = 0;\n+    T five = 5;\n+    T ten = 10;\n+    _test_value.relaxed_store(zero);\n+    T res = _test_value.cmpxchg(five, ten);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(zero, _test_value.load_relaxed());\n+    res = _test_value.cmpxchg(zero, ten);\n+    EXPECT_EQ(zero, res);\n+    EXPECT_EQ(ten, _test_value.load_relaxed());\n+  }\n+};\n+\n+TEST_VM(AtomicIntegerTest, cmpxchg_int32) {\n+  using Support = AtomicIntegerCmpxchgTestSupport<int32_t>;\n+  Support().test();\n+}\n+\n+TEST_VM(AtomicIntegerTest, cmpxchg_int64) {\n+  \/\/ Check if 64-bit atomics are available on the machine.\n+  if (!VM_Version::supports_cx8()) return;\n+\n+  using Support = AtomicIntegerCmpxchgTestSupport<int64_t>;\n+  Support().test();\n+}\n+\n+struct AtomicCmpxchg1ByteStressSupport {\n+  char _default_val;\n+  int  _base;\n+  Atomic<char> _array[7+32+7];\n+\n+  AtomicCmpxchg1ByteStressSupport() : _default_val(0x7a), _base(7) {}\n+\n+  void validate(char val, char val2, int index) {\n+    for (int i = 0; i < 7; i++) {\n+      EXPECT_EQ(_array[i].load_relaxed(), _default_val);\n+    }\n+    for (int i = 7; i < (7+32); i++) {\n+      if (i == index) {\n+        EXPECT_EQ(_array[i].load_relaxed(), val2);\n+      } else {\n+        EXPECT_EQ(_array[i].load_relaxed(), val);\n+      }\n+    }\n+    for (int i = 0; i < 7; i++) {\n+      EXPECT_EQ(_array[i].load_relaxed(), _default_val);\n+    }\n+  }\n+\n+  void test_index(int index) {\n+    char one = 1;\n+    _array[index].cmpxchg(_default_val, one);\n+    validate(_default_val, one, index);\n+\n+    _array[index].cmpxchg(one, _default_val);\n+    validate(_default_val, _default_val, index);\n+  }\n+\n+  void test() {\n+    for (size_t i = 0; i < ARRAY_SIZE(_array); ++i) {\n+      _array[i].relaxed_store(_default_val);\n+    }\n+    for (int i = _base; i < (_base+32); i++) {\n+      test_index(i);\n+    }\n+  }\n+};\n+\n+TEST_VM(AtomicCmpxchg1Byte, stress) {\n+  AtomicCmpxchg1ByteStressSupport support;\n+  support.test();\n+}\n+\n+template<typename T>\n+struct AtomicEnumTestSupport {\n+  Atomic<T> _test_value;\n+\n+  AtomicEnumTestSupport() : _test_value{} {}\n+\n+  void test_store_load(T value) {\n+    EXPECT_NE(value, _test_value.load_relaxed());\n+    _test_value.relaxed_store(value);\n+    EXPECT_EQ(value, _test_value.load_relaxed());\n+  }\n+\n+  void test_cmpxchg(T value1, T value2) {\n+    EXPECT_NE(value1, _test_value.load_relaxed());\n+    _test_value.relaxed_store(value1);\n+    EXPECT_EQ(value1, _test_value.cmpxchg(value2, value2));\n+    EXPECT_EQ(value1, _test_value.load_relaxed());\n+    EXPECT_EQ(value1, _test_value.cmpxchg(value1, value2));\n+    EXPECT_EQ(value2, _test_value.load_relaxed());\n+  }\n+\n+  void test_xchg(T value1, T value2) {\n+    EXPECT_NE(value1, _test_value.load_relaxed());\n+    _test_value.relaxed_store(value1);\n+    EXPECT_EQ(value1, _test_value.fetch_then_set(value2));\n+    EXPECT_EQ(value2, _test_value.load_relaxed());\n+  }\n+};\n+\n+namespace AtomicEnumTestUnscoped {       \/\/ Scope the enumerators.\n+  enum TestEnum { A, B, C };\n+}\n+\n+TEST_VM(AtomicEnumTest, unscoped_enum) {\n+  using namespace AtomicEnumTestUnscoped;\n+  using Support = AtomicEnumTestSupport<TestEnum>;\n+\n+  Support().test_store_load(B);\n+  Support().test_cmpxchg(B, C);\n+  Support().test_xchg(B, C);\n+}\n+\n+enum class AtomicEnumTestScoped { A, B, C };\n+\n+TEST_VM(AtomicEnumTest, scoped_enum) {\n+  const AtomicEnumTestScoped B = AtomicEnumTestScoped::B;\n+  const AtomicEnumTestScoped C = AtomicEnumTestScoped::C;\n+  using Support = AtomicEnumTestSupport<AtomicEnumTestScoped>;\n+\n+  Support().test_store_load(B);\n+  Support().test_cmpxchg(B, C);\n+  Support().test_xchg(B, C);\n+}\n+\n+template<typename T>\n+struct AtomicBitopsTestSupport {\n+  Atomic<T> _test_value;\n+\n+  \/\/ At least one byte differs between _old_value and _old_value op _change_value.\n+  static constexpr T _old_value =    static_cast<T>(UCONST64(0x7f5300007f530044));\n+  static constexpr T _change_value = static_cast<T>(UCONST64(0x3800530038005322));\n+\n+  AtomicBitopsTestSupport() : _test_value(0) {}\n+\n+  void fetch_then_and() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value & _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.fetch_then_and(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_or() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value | _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.fetch_then_or(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_xor() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value ^ _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.fetch_then_xor(_change_value);\n+    EXPECT_EQ(_old_value, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void and_then_fetch() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value & _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.and_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void or_then_fetch() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value | _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.or_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void xor_then_fetch() {\n+    _test_value.relaxed_store(_old_value);\n+    T expected = _old_value ^ _change_value;\n+    EXPECT_NE(_old_value, expected);\n+    T result = _test_value.xor_then_fetch(_change_value);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+#define TEST_BITOP(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_BITOP(fetch_then_and)\n+    TEST_BITOP(fetch_then_or)\n+    TEST_BITOP(fetch_then_xor)\n+    TEST_BITOP(and_then_fetch)\n+    TEST_BITOP(or_then_fetch)\n+    TEST_BITOP(xor_then_fetch)\n+  }\n+\n+#undef TEST_BITOP\n+};\n+\n+TEST_VM(AtomicBitopsTest, int32) {\n+  AtomicBitopsTestSupport<int32_t>()();\n+}\n+\n+TEST_VM(AtomicBitopsTest, uint32) {\n+  AtomicBitopsTestSupport<uint32_t>()();\n+}\n+\n+TEST_VM(AtomicBitopsTest, int64) {\n+  AtomicBitopsTestSupport<int64_t>()();\n+}\n+\n+TEST_VM(AtomicBitopsTest, uint64) {\n+  AtomicBitopsTestSupport<uint64_t>()();\n+}\n+\n+template<typename T>\n+struct AtomicPointerTestSupport {\n+  static T _test_values[10];\n+  static T* _initial_ptr;\n+\n+  Atomic<T*> _test_value;\n+\n+  AtomicPointerTestSupport() : _test_value(nullptr) {}\n+\n+  void fetch_then_add() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* expected = _initial_ptr + 2;\n+    T* result = _test_value.fetch_then_add(2);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_sub() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* expected = _initial_ptr - 2;\n+    T* result = _test_value.fetch_then_sub(2);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void add_then_fetch() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* expected = _initial_ptr + 2;\n+    T* result = _test_value.add_then_fetch(2);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void sub_then_fetch() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* expected = _initial_ptr - 2;\n+    T* result = _test_value.sub_then_fetch(2);\n+    EXPECT_EQ(expected, result);\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void atomic_inc() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* expected = _initial_ptr + 1;\n+    _test_value.atomic_inc();\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void atomic_dec() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* expected = _initial_ptr - 1;\n+    _test_value.atomic_dec();\n+    EXPECT_EQ(expected, _test_value.load_relaxed());\n+  }\n+\n+  void fetch_then_set() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* replace = _initial_ptr + 3;\n+    T* result = _test_value.fetch_then_set(replace);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(replace, _test_value.load_relaxed());\n+  }\n+\n+  void cmpxchg() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* not_initial_ptr = _initial_ptr - 1;\n+    T* replace = _initial_ptr + 3;\n+\n+    T* result = _test_value.cmpxchg(not_initial_ptr, replace);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(_initial_ptr, _test_value.load_relaxed());\n+\n+    result = _test_value.cmpxchg(_initial_ptr, replace);\n+    EXPECT_EQ(_initial_ptr, result);\n+    EXPECT_EQ(replace, _test_value.load_relaxed());\n+  }\n+\n+  void replace_if_null() {\n+    _test_value.relaxed_store(nullptr);\n+    EXPECT_TRUE(_test_value.replace_if_null(_initial_ptr));\n+    EXPECT_EQ(_initial_ptr, _test_value.load_relaxed());\n+\n+    T* replace = _initial_ptr + 3;\n+    EXPECT_FALSE(_test_value.replace_if_null(replace));\n+    EXPECT_EQ(_initial_ptr, _test_value.load_relaxed());\n+  }\n+\n+  void clear_if_equal() {\n+    _test_value.relaxed_store(_initial_ptr);\n+    T* compare = _initial_ptr + 3;\n+    EXPECT_FALSE(_test_value.clear_if_equal(compare));\n+    EXPECT_EQ(_initial_ptr, _test_value.load_relaxed());\n+\n+    EXPECT_TRUE(_test_value.clear_if_equal(_initial_ptr));\n+    EXPECT_EQ(nullptr, _test_value.load_relaxed());\n+  }\n+\n+#define TEST_OP(name) { SCOPED_TRACE(XSTR(name)); name(); }\n+\n+  void operator()() {\n+    TEST_OP(fetch_then_add)\n+    TEST_OP(fetch_then_sub)\n+    TEST_OP(add_then_fetch)\n+    TEST_OP(sub_then_fetch)\n+    TEST_OP(atomic_inc)\n+    TEST_OP(atomic_dec)\n+    TEST_OP(fetch_then_set)\n+    TEST_OP(cmpxchg)\n+    TEST_OP(replace_if_null)\n+    TEST_OP(clear_if_equal)\n+  }\n+\n+#undef TEST_OP\n+};\n+\n+template<typename T>\n+T AtomicPointerTestSupport<T>::_test_values[10] = {};\n+\n+template<typename T>\n+T* AtomicPointerTestSupport<T>::_initial_ptr = &_test_values[5];\n+\n+TEST_VM(AtomicPointerTest, ptr_to_char) {\n+  AtomicPointerTestSupport<char>()();\n+}\n+\n+TEST_VM(AtomicPointerTest, ptr_to_int32) {\n+  AtomicPointerTestSupport<int32_t>()();\n+}\n+\n+TEST_VM(AtomicPointerTest, ptr_to_int64) {\n+  AtomicPointerTestSupport<int64_t>()();\n+}\n+\n+\/\/ Test translation, including chaining.\n+\n+struct TranslatedAtomicTestObject1 {\n+  int _value;\n+\n+  \/\/ NOT default constructible.\n+\n+  explicit TranslatedAtomicTestObject1(int value)\n+    : _value(value) {}\n+};\n+\n+template<>\n+struct PrimitiveConversions::Translate<TranslatedAtomicTestObject1>\n+  : public std::true_type\n+{\n+  using Value = TranslatedAtomicTestObject1;\n+  using Decayed = int;\n+\n+  static Decayed decay(Value x) { return x._value; }\n+  static Value recover(Decayed x) { return Value(x); }\n+};\n+\n+struct TranslatedAtomicTestObject2 {\n+  TranslatedAtomicTestObject1 _value;\n+\n+  static constexpr int DefaultObject1Value = 3;\n+\n+  TranslatedAtomicTestObject2()\n+    : TranslatedAtomicTestObject2(TranslatedAtomicTestObject1(DefaultObject1Value))\n+  {}\n+\n+  explicit TranslatedAtomicTestObject2(TranslatedAtomicTestObject1 value)\n+    : _value(value) {}\n+};\n+\n+template<>\n+struct PrimitiveConversions::Translate<TranslatedAtomicTestObject2>\n+  : public std::true_type\n+{\n+  using Value = TranslatedAtomicTestObject2;\n+  using Decayed = TranslatedAtomicTestObject1;\n+\n+  static Decayed decay(Value x) { return x._value; }\n+  static Value recover(Decayed x) { return Value(x); }\n+};\n+\n+struct TranslatedAtomicByteObject {\n+  uint8_t _value;\n+\n+  \/\/ NOT default constructible.\n+\n+  explicit TranslatedAtomicByteObject(uint8_t value = 0) : _value(value) {}\n+};\n+\n+template<>\n+struct PrimitiveConversions::Translate<TranslatedAtomicByteObject>\n+  : public std::true_type\n+{\n+  using Value = TranslatedAtomicByteObject;\n+  using Decayed = uint8_t;\n+\n+  static Decayed decay(Value x) { return x._value; }\n+  static Value recover(Decayed x) { return Value(x); }\n+};\n+\n+\/\/ Test whether Atomic<T> has fetch_then_set().\n+\/\/ Note: This is intentionally a different implementation from what is used\n+\/\/ by the atomic translated type to decide whether to provide fetch_then_set().\n+\/\/ The intent is to make related testing non-tautological.\n+\/\/ The two implementations must agree; it's a bug if they don't.\n+template<typename T>\n+class AtomicTypeHasFetchThenSet {\n+  template<typename U,\n+           typename AU = Atomic<U>,\n+           typename = decltype(declval<AU>().fetch_then_set(declval<U>()))>\n+  static char* test(int);\n+\n+  template<typename> static char test(...);\n+\n+  using test_type = decltype(test<T>(0));\n+\n+public:\n+  static constexpr bool value = std::is_pointer_v<test_type>;\n+};\n+\n+\/\/ Unit tests for AtomicTypeHasFetchThenSet.\n+static_assert(AtomicTypeHasFetchThenSet<int>::value);\n+static_assert(AtomicTypeHasFetchThenSet<int*>::value);\n+static_assert(AtomicTypeHasFetchThenSet<TranslatedAtomicTestObject1>::value);\n+static_assert(AtomicTypeHasFetchThenSet<TranslatedAtomicTestObject2>::value);\n+static_assert(!AtomicTypeHasFetchThenSet<uint8_t>::value);\n+\n+\/\/ Verify translated byte type *doesn't* have fetch_then_set.\n+static_assert(!AtomicTypeHasFetchThenSet<TranslatedAtomicByteObject>::value);\n+\n+\/\/ Verify that explicit instantiation doesn't attempt to reference the\n+\/\/ non-existent fetch_then_set of the atomic decayed type.\n+template class AtomicImpl::Atomic<TranslatedAtomicByteObject>;\n+\n+template<typename T>\n+static void test_atomic_translated_type() {\n+  \/\/ This works even if T is not default constructible.\n+  Atomic<T> _test_value{};\n+\n+  using Translated = PrimitiveConversions::Translate<T>;\n+\n+  EXPECT_EQ(0, Translated::decay(_test_value.load_relaxed()));\n+  _test_value.relaxed_store(Translated::recover(5));\n+  EXPECT_EQ(5, Translated::decay(_test_value.load_relaxed()));\n+  EXPECT_EQ(5, Translated::decay(_test_value.cmpxchg(Translated::recover(5),\n+                                                     Translated::recover(10))));\n+  EXPECT_EQ(10, Translated::decay(_test_value.load_relaxed()));\n+\n+  if constexpr (AtomicTypeHasFetchThenSet<T>::value) {\n+    EXPECT_EQ(10, Translated::decay(_test_value.fetch_then_set(Translated::recover(20))));\n+    EXPECT_EQ(20, Translated::decay(_test_value.load_relaxed()));\n+  }\n+}\n+\n+TEST_VM(AtomicTranslatedTypeTest, int_test) {\n+  test_atomic_translated_type<TranslatedAtomicTestObject1>();\n+}\n+\n+TEST_VM(AtomicTranslatedTypeTest, byte_test) {\n+  test_atomic_translated_type<TranslatedAtomicByteObject>();\n+}\n+\n+TEST_VM(AtomicTranslatedTypeTest, chain) {\n+  Atomic<TranslatedAtomicTestObject2> _test_value{};\n+\n+  using Translated1 = PrimitiveConversions::Translate<TranslatedAtomicTestObject1>;\n+  using Translated2 = PrimitiveConversions::Translate<TranslatedAtomicTestObject2>;\n+\n+  auto resolve = [&](TranslatedAtomicTestObject2 x) {\n+    return Translated1::decay(Translated2::decay(x));\n+  };\n+\n+  auto construct = [&](int x) {\n+    return Translated2::recover(Translated1::recover(x));\n+  };\n+\n+  EXPECT_EQ(TranslatedAtomicTestObject2::DefaultObject1Value,\n+            resolve(_test_value.load_relaxed()));\n+  _test_value.relaxed_store(construct(5));\n+  EXPECT_EQ(5, resolve(_test_value.load_relaxed()));\n+  EXPECT_EQ(5, resolve(_test_value.cmpxchg(construct(5), construct(10))));\n+  EXPECT_EQ(10, resolve(_test_value.load_relaxed()));\n+  EXPECT_EQ(10, resolve(_test_value.fetch_then_set(construct(20))));\n+  EXPECT_EQ(20, resolve(_test_value.load_relaxed()));\n+};\n+\n+template<typename T>\n+static void test_value_access() {\n+  using AT = Atomic<T>;\n+  \/\/ In addition to verifying values are as expected, also verify the\n+  \/\/ operations are constexpr.\n+  static_assert(sizeof(T) == AT::value_size_in_bytes(), \"value size differs\");\n+  static_assert(0 == AT::value_offset_in_bytes(), \"unexpected offset\");\n+  \/\/ Also verify no unexpected increase in size for Atomic wrapper.\n+  static_assert(sizeof(T) == sizeof(AT), \"unexpected size difference\");\n+};\n+\n+TEST_VM(AtomicValueAccessTest, access_char) {\n+  test_value_access<char>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_bool) {\n+  test_value_access<bool>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_int32) {\n+  test_value_access<int32_t>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_int64) {\n+  test_value_access<int64_t>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_ptr) {\n+  test_value_access<char*>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_trans1) {\n+  test_value_access<TranslatedAtomicTestObject1>();\n+}\n+\n+TEST_VM(AtomicValueAccessTest, access_trans2) {\n+  test_value_access<TranslatedAtomicTestObject2>();\n+}\n","filename":"test\/hotspot\/gtest\/runtime\/test_atomic.cpp","additions":695,"deletions":0,"binary":false,"changes":695,"status":"added"}]}