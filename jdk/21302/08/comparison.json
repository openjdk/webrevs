{"files":[{"patch":"@@ -393,0 +393,179 @@\n+ArchiveWorkers ArchiveWorkers::_workers;\n+\n+ArchiveWorkers::ArchiveWorkers() :\n+        _start_semaphore(0),\n+        _end_semaphore(0),\n+        _num_workers(0),\n+        _started_workers(0),\n+        _waiting_workers(0),\n+        _running_workers(0),\n+        _state(state_uninitialized),\n+        _task(nullptr) {\n+}\n+\n+void ArchiveWorkers::initialize() {\n+  assert(Atomic::load(&_state) == state_uninitialized, \"Should be\");\n+\n+  Atomic::store(&_num_workers, max_workers());\n+  Atomic::store(&_state, state_initialized);\n+\n+  \/\/ Kick off pool startup by creating a single worker.\n+  start_worker_if_needed();\n+}\n+\n+int ArchiveWorkers::max_workers() {\n+  \/\/ The pool is used for short-lived bursty tasks. We do not want to spend\n+  \/\/ too much time creating and waking up threads unnecessarily. Plus, we do\n+  \/\/ not want to overwhelm large machines. This is why we want to be very\n+  \/\/ conservative about the number of workers actually needed.\n+  return MAX2(0, log2i_graceful(os::active_processor_count()));\n+}\n+\n+bool ArchiveWorkers::is_parallel() {\n+  return _num_workers > 0;\n+}\n+\n+void ArchiveWorkers::shutdown() {\n+  if (is_parallel()) {\n+    if (Atomic::cmpxchg(&_state, state_initialized, state_shutdown, memory_order_relaxed) == false) {\n+      \/\/ Execute a shutdown task and block until all workers respond.\n+      run_task(&_shutdown_task);\n+    }\n+  }\n+}\n+\n+void ArchiveWorkers::start_worker_if_needed() {\n+  while (true) {\n+    int cur = Atomic::load(&_started_workers);\n+    if (cur >= _num_workers) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_started_workers, cur, cur + 1, memory_order_relaxed) == cur) {\n+      break;\n+    }\n+  }\n+\n+  new ArchiveWorkerThread(this);\n+}\n+\n+void ArchiveWorkers::signal_worker_if_needed() {\n+  while (true) {\n+    int cur = Atomic::load(&_waiting_workers);\n+    if (cur == 0) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_waiting_workers, cur, cur - 1, memory_order_relaxed) == cur) {\n+      break;\n+    }\n+  }\n+  _start_semaphore.signal(1);\n+}\n+\n+void ArchiveWorkers::run_task(ArchiveWorkerTask* task) {\n+  assert((Atomic::load(&_state) == state_initialized) ||\n+         ((Atomic::load(&_state) == state_shutdown) && (task == &_shutdown_task)),\n+         \"Should be in correct state\");\n+  assert(Atomic::load(&_task) == nullptr, \"Should not have running tasks\");\n+\n+  if (is_parallel()) {\n+    run_task_multi(task);\n+  } else {\n+    run_task_single(task);\n+  }\n+}\n+\n+void ArchiveWorkers::run_task_single(ArchiveWorkerTask* task) {\n+  \/\/ Single thread needs no chunking.\n+  task->configure_max_chunks(1);\n+\n+  \/\/ Execute the task ourselves, as there are no workers.\n+  task->work(0, 1);\n+}\n+\n+void ArchiveWorkers::run_task_multi(ArchiveWorkerTask* task) {\n+  \/\/ Multiple threads can work with multiple chunks.\n+  task->configure_max_chunks(_num_workers * CHUNKS_PER_WORKER);\n+\n+  \/\/ Set up the run and publish the task.\n+  Atomic::store(&_waiting_workers, _num_workers);\n+  Atomic::store(&_running_workers, _num_workers);\n+  Atomic::release_store(&_task, task);\n+\n+  \/\/ Kick off pool wakeup by signaling a single worker, and proceed\n+  \/\/ immediately to executing the task locally.\n+  signal_worker_if_needed();\n+\n+  \/\/ Execute the task ourselves, while workers are catching up.\n+  \/\/ This allows us to hide parts of task handoff latency.\n+  task->run();\n+\n+  \/\/ Done executing task locally, wait for any remaining workers to complete,\n+  \/\/ and then do the final housekeeping.\n+  _end_semaphore.wait();\n+  Atomic::store(&_task, (ArchiveWorkerTask *) nullptr);\n+  OrderAccess::fence();\n+\n+  assert(Atomic::load(&_waiting_workers) == 0, \"All workers were signaled\");\n+  assert(Atomic::load(&_running_workers) == 0, \"No workers are running\");\n+}\n+\n+void ArchiveWorkerTask::run() {\n+  while (true) {\n+    int chunk = Atomic::load(&_chunk);\n+    if (chunk >= _max_chunks) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_chunk, chunk, chunk + 1, memory_order_relaxed) == chunk) {\n+      assert(0 <= chunk && chunk < _max_chunks, \"Sanity\");\n+      work(chunk, _max_chunks);\n+    }\n+  }\n+}\n+\n+void ArchiveWorkerTask::configure_max_chunks(int max_chunks) {\n+  if (_max_chunks == 0) {\n+    _max_chunks = max_chunks;\n+  }\n+}\n+\n+bool ArchiveWorkers::run_as_worker() {\n+  assert(is_parallel(), \"Should be in parallel mode\");\n+  _start_semaphore.wait();\n+\n+  \/\/ Avalanche wakeups: each worker signals two others.\n+  signal_worker_if_needed();\n+  signal_worker_if_needed();\n+\n+  ArchiveWorkerTask* task = Atomic::load_acquire(&_task);\n+  task->run();\n+\n+  \/\/ All work done in threads should be visible to caller.\n+  OrderAccess::fence();\n+\n+  \/\/ Signal the pool the tasks are complete, if this is the last worker.\n+  if (Atomic::sub(&_running_workers, 1, memory_order_relaxed) == 0) {\n+    _end_semaphore.signal();\n+  }\n+\n+  \/\/ Continue if task was not a termination task.\n+  return (task != &_shutdown_task);\n+}\n+\n+ArchiveWorkerThread::ArchiveWorkerThread(ArchiveWorkers* pool) : NamedThread(), _pool(pool) {\n+  set_name(\"ArchiveWorkerThread\");\n+  os::create_thread(this, os::os_thread);\n+  os::start_thread(this);\n+}\n+\n+void ArchiveWorkerThread::run() {\n+  \/\/ Avalanche thread startup: each starting worker starts two others.\n+  _pool->start_worker_if_needed();\n+  _pool->start_worker_if_needed();\n+\n+  \/\/ Set ourselves up.\n+  os::set_priority(this, NearMaxPriority);\n+\n+  while (_pool->run_as_worker()) {\n+    \/\/ Work until terminated.\n+  }\n+}\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":179,"deletions":0,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"runtime\/nonJavaThread.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n@@ -293,0 +295,89 @@\n+class ArchiveWorkers;\n+\n+\/\/ A task to be worked on by worker threads\n+class ArchiveWorkerTask : public CHeapObj<mtInternal> {\n+  friend class ArchiveWorkers;\n+  friend class ArchiveWorkerShutdownTask;\n+private:\n+  const char* _name;\n+  int _max_chunks;\n+  volatile int _chunk;\n+\n+  void run();\n+\n+  void configure_max_chunks(int max_chunks);\n+\n+public:\n+  ArchiveWorkerTask(const char* name) :\n+      _name(name), _max_chunks(0), _chunk(0) {}\n+  const char* name() const { return _name; }\n+  virtual void work(int chunk, int max_chunks) = 0;\n+};\n+\n+class ArchiveWorkerThread : public NamedThread {\n+  friend class ArchiveWorkers;\n+private:\n+  ArchiveWorkers* const _pool;\n+\n+public:\n+  ArchiveWorkerThread(ArchiveWorkers* pool);\n+  const char* type_name() const override { return \"Archive Worker Thread\"; }\n+  void run() override;\n+};\n+\n+class ArchiveWorkerShutdownTask : public ArchiveWorkerTask {\n+public:\n+  ArchiveWorkerShutdownTask() : ArchiveWorkerTask(\"Archive Worker Shutdown\") {\n+    \/\/ This task always have only one chunk.\n+    configure_max_chunks(1);\n+  }\n+  void work(int chunk, int max_chunks) override {\n+    \/\/ Do nothing.\n+  }\n+};\n+\n+\/\/ Special worker pool for archive workers. The goal for this pool is to\n+\/\/ startup fast, distribute spiky workloads efficiently, and being able to\n+\/\/ shutdown after use. This makes the implementation quite different from\n+\/\/ the normal GC worker pool.\n+class ArchiveWorkers {\n+  friend class ArchiveWorkerThread;\n+private:\n+  \/\/ Target number of chunks per worker. This should be large enough to even\n+  \/\/ out work imbalance, and small enough to keep bookkeeping overheads low.\n+  static constexpr int CHUNKS_PER_WORKER = 4;\n+  static int max_workers();\n+\n+  \/\/ Global shared instance. Can be uninitialized, can be shut down.\n+  static ArchiveWorkers _workers;\n+\n+  ArchiveWorkerShutdownTask _shutdown_task;\n+  Semaphore _start_semaphore;\n+  Semaphore _end_semaphore;\n+\n+  int _num_workers;\n+  int _started_workers;\n+  int _waiting_workers;\n+  int _running_workers;\n+\n+  enum { state_uninitialized, state_initialized, state_shutdown} _state;\n+  ArchiveWorkerTask* _task;\n+\n+  bool run_as_worker();\n+  void start_worker_if_needed();\n+  void signal_worker_if_needed();\n+\n+  void run_task_single(ArchiveWorkerTask* task);\n+  void run_task_multi(ArchiveWorkerTask* task);\n+\n+  bool is_parallel();\n+\n+  ArchiveWorkers();\n+\n+public:\n+  static ArchiveWorkers* workers() { return &_workers; }\n+  void initialize();\n+  void shutdown();\n+  void run_task(ArchiveWorkerTask* task);\n+};\n+\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -97,0 +97,4 @@\n+                                                                            \\\n+  product(bool, AOTCacheParallelRelocation, true, DIAGNOSTIC,               \\\n+          \"Use parallel relocation code to speed up startup.\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/cds\/cds_globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1949,0 +1949,26 @@\n+class SharedDataRelocationTask : public ArchiveWorkerTask {\n+private:\n+  BitMapView* const _rw_bm;\n+  BitMapView* const _ro_bm;\n+  SharedDataRelocator* const _rw_reloc;\n+  SharedDataRelocator* const _ro_reloc;\n+\n+public:\n+  SharedDataRelocationTask(BitMapView* rw_bm, BitMapView* ro_bm, SharedDataRelocator* rw_reloc, SharedDataRelocator* ro_reloc) :\n+                           ArchiveWorkerTask(\"Shared Data Relocation\"),\n+                           _rw_bm(rw_bm), _ro_bm(ro_bm), _rw_reloc(rw_reloc), _ro_reloc(ro_reloc) {}\n+\n+  void work(int chunk, int max_chunks) override {\n+    work_on(chunk, max_chunks, _rw_bm, _rw_reloc);\n+    work_on(chunk, max_chunks, _ro_bm, _ro_reloc);\n+  }\n+\n+  void work_on(int chunk, int max_chunks, BitMapView* bm, SharedDataRelocator* reloc) {\n+    BitMap::idx_t size  = bm->size();\n+    BitMap::idx_t start = MIN2(size, size * chunk \/ max_chunks);\n+    BitMap::idx_t end   = MIN2(size, size * (chunk + 1) \/ max_chunks);\n+    assert(end > start, \"Sanity: no empty slices\");\n+    bm->iterate(reloc, start, end);\n+  }\n+};\n+\n@@ -1987,2 +2013,8 @@\n-    rw_ptrmap.iterate(&rw_patcher);\n-    ro_ptrmap.iterate(&ro_patcher);\n+\n+    if (AOTCacheParallelRelocation) {\n+      SharedDataRelocationTask task(&rw_ptrmap, &ro_ptrmap, &rw_patcher, &ro_patcher);\n+      ArchiveWorkers::workers()->run_task(&task);\n+    } else {\n+      rw_ptrmap.iterate(&rw_patcher);\n+      ro_ptrmap.iterate(&ro_patcher);\n+    }\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":34,"deletions":2,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -964,0 +964,3 @@\n+  \/\/ We are about to open the archives. Initialize workers now.\n+  ArchiveWorkers::workers()->initialize();\n+\n@@ -1573,0 +1576,3 @@\n+\n+  \/\/ Workers are no longer needed.\n+  ArchiveWorkers::workers()->shutdown();\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"}]}