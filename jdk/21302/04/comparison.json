{"files":[{"patch":"@@ -393,0 +393,116 @@\n+ArchiveWorkers::ArchiveWorkers() :\n+        _start_semaphore(0),\n+        _end_semaphore(0),\n+        _num_workers(MAX2(1, os::active_processor_count() \/ CPUS_PER_WORKER - 1)),\n+        _started_workers(0),\n+        _running_workers(0),\n+        _in_shutdown(false),\n+        _task(nullptr) {\n+  \/\/ Kick off pool startup by creating a single worker.\n+  start_worker_if_needed();\n+}\n+\n+ArchiveWorkers::~ArchiveWorkers() {\n+  \/\/ If nothing called shutdown yet, we need to gracefully shutdown now.\n+  shutdown();\n+}\n+\n+void ArchiveWorkers::shutdown() {\n+  if (Atomic::cmpxchg(&_in_shutdown, false, true) == false) {\n+    \/\/ Execute a shutdown task and block until all workers respond.\n+    run_task(&_shutdown_task);\n+  }\n+}\n+\n+void ArchiveWorkers::start_worker_if_needed() {\n+  while (true) {\n+    int cur = Atomic::load(&_started_workers);\n+    if (cur >= _num_workers) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_started_workers, cur, cur + 1) == cur) {\n+      break;\n+    }\n+  }\n+\n+  new ArchiveWorkerThread(this);\n+}\n+\n+void ArchiveWorkers::run_task(ArchiveWorkerTask* task) {\n+  assert(task == &_shutdown_task || !_in_shutdown, \"Should not be shutdown\");\n+  assert(_task == nullptr, \"Should not have running tasks\");\n+\n+  \/\/ Configure the execution.\n+  task->maybe_override_max_chunks(_num_workers * CHUNKS_PER_WORKER);\n+  Atomic::store(&_running_workers, _num_workers);\n+\n+  \/\/ Publish the task and signal workers to pick it up.\n+  Atomic::release_store(&_task, task);\n+  _start_semaphore.signal(_num_workers);\n+\n+  \/\/ Execute the task ourselves, while workers are catching up.\n+  \/\/ This allows us to hide parts of task handoff latency.\n+  task->run();\n+\n+  \/\/ Done executing task locally, wait for any remaining workers to complete,\n+  \/\/ and then do the final housekeeping.\n+  _end_semaphore.wait();\n+  Atomic::store(&_task, (ArchiveWorkerTask*)nullptr);\n+  OrderAccess::fence();\n+}\n+\n+void ArchiveWorkerTask::run() {\n+  while (true) {\n+    int chunk = Atomic::load(&_chunk);\n+    if (chunk >= _max_chunks) {\n+      return;\n+    }\n+    if (Atomic::cmpxchg(&_chunk, chunk, chunk + 1, memory_order_relaxed) == chunk) {\n+      assert(0 <= chunk && chunk < _max_chunks, \"Sanity\");\n+      work(chunk, _max_chunks);\n+    }\n+  }\n+}\n+\n+void ArchiveWorkerTask::maybe_override_max_chunks(int max_chunks) {\n+  if (_max_chunks == -1) {\n+    _max_chunks = max_chunks;\n+  }\n+}\n+\n+bool ArchiveWorkers::run_as_worker() {\n+  _start_semaphore.wait();\n+\n+  ArchiveWorkerTask* task = Atomic::load_acquire(&_task);\n+  task->run();\n+\n+  \/\/ Signal the pool the tasks are complete, if this is the last worker.\n+  if (Atomic::sub(&_running_workers, 1) == 0) {\n+    _end_semaphore.signal();\n+  }\n+\n+  \/\/ Continue if task was not a termination task.\n+  return (task != &_shutdown_task);\n+}\n+\n+ArchiveWorkerThread::ArchiveWorkerThread(ArchiveWorkers* pool) : NamedThread(), _pool(pool) {\n+  set_name(\"ArchiveWorkerThread\");\n+  os::create_thread(this, os::os_thread);\n+  os::start_thread(this);\n+}\n+\n+void ArchiveWorkerThread::run() {\n+  \/\/ Avalanche thread startup: each starting worker starts two others.\n+  _pool->start_worker_if_needed();\n+  _pool->start_worker_if_needed();\n+\n+  \/\/ Set ourselves up.\n+  os::set_priority(this, NearMaxPriority);\n+\n+  while (_pool->run_as_worker()) {\n+    \/\/ Work until terminated.\n+  }\n+\n+  \/\/ All work done in threads should be visible to caller.\n+  OrderAccess::fence();\n+}\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":116,"deletions":0,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"runtime\/nonJavaThread.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n@@ -294,0 +296,76 @@\n+class ArchiveWorkers;\n+\n+\/\/ A task to be worked on by worker threads\n+class ArchiveWorkerTask : public CHeapObj<mtInternal> {\n+  friend class ArchiveWorkers;\n+private:\n+  const char* _name;\n+  int _max_chunks;\n+  volatile int _chunk;\n+\n+  void run();\n+\n+  void maybe_override_max_chunks(int max_chunks);\n+\n+public:\n+  ArchiveWorkerTask(const char* name, int max_chunks = -1) :\n+      _name(name), _max_chunks(max_chunks), _chunk(0) {}\n+  const char* name() const { return _name; }\n+  virtual void work(int chunk, int max_chunks) = 0;\n+};\n+\n+class ArchiveWorkerThread : public NamedThread {\n+  friend class ArchiveWorkers;\n+private:\n+  ArchiveWorkers* const _pool;\n+\n+public:\n+  ArchiveWorkerThread(ArchiveWorkers* pool);\n+  const char* type_name() const override { return \"Archive Worker Thread\"; }\n+  void run() override;\n+};\n+\n+class ArchiveWorkerShutdownTask : public ArchiveWorkerTask {\n+public:\n+  ArchiveWorkerShutdownTask() : ArchiveWorkerTask(\"Archive Worker Shutdown\", 1) {}\n+  void work(int chunk, int max_chunks) override {\n+    \/\/ Do nothing.\n+  }\n+};\n+\n+\/\/ Special worker pool for archive workers. The goal for this pool is to\n+\/\/ startup fast, distribute spiky workloads efficiently, and being able to\n+\/\/ shutdown after use. This makes the implementation quite different from\n+\/\/ the normal GC worker pool.\n+class ArchiveWorkers {\n+  friend class ArchiveWorkerThread;\n+private:\n+  \/\/ The reciprocal ratio for number of workers per CPU. We are targeting\n+  \/\/ to take 1\/4 CPUs to provide decent parallelism without letting workers\n+  \/\/ stumble over each other.\n+  static constexpr int CPUS_PER_WORKER = 4;\n+\n+  \/\/ Target number of chunks per worker. This should be large enough to even\n+  \/\/ out work imbalance, and small enough to keep bookkeeping overheads low.\n+  static constexpr int CHUNKS_PER_WORKER = 4;\n+\n+  ArchiveWorkerShutdownTask _shutdown_task;\n+  Semaphore _start_semaphore;\n+  Semaphore _end_semaphore;\n+\n+  int _num_workers;\n+  int _started_workers;\n+  int _running_workers;\n+  bool _in_shutdown;\n+  ArchiveWorkerTask* _task;\n+\n+  bool run_as_worker();\n+  void start_worker_if_needed();\n+\n+public:\n+  ArchiveWorkers();\n+  ~ArchiveWorkers();\n+  void shutdown();\n+  void run_task(ArchiveWorkerTask* task);\n+};\n+\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -97,0 +97,7 @@\n+                                                                            \\\n+  product(bool, ArchivePreTouch, true, DIAGNOSTIC,                          \\\n+          \"Pre-touch archive regions, regardless of AlwaysPreTouch mode.\")  \\\n+                                                                            \\\n+  product(bool, ArchiveParallelRelocation, true, DIAGNOSTIC,                \\\n+          \"Use parallel relocation code to speed up startup.\")              \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/cds\/cds_globals.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1711,0 +1711,3 @@\n+\n+  \/\/ Workers are no longer needed.\n+  _archive_workers.shutdown();\n@@ -1713,0 +1716,16 @@\n+class ArchiveRegionPretouchTask : public ArchiveWorkerTask {\n+private:\n+  char* const _from;\n+  size_t const _bytes;\n+\n+public:\n+  ArchiveRegionPretouchTask(char* from, size_t bytes) :\n+    ArchiveWorkerTask(\"Archive Regions Pretouch\"), _from(from), _bytes(bytes) {}\n+\n+  void work(int chunk, int max_chunks) override {\n+    char* start = _from + MIN2(_bytes, _bytes * chunk \/ max_chunks);\n+    char* end   = _from + MIN2(_bytes, _bytes * (chunk + 1) \/ max_chunks);\n+    os::pretouch_memory(start, end);\n+  }\n+};\n+\n@@ -1714,1 +1733,2 @@\n- * Same as os::map_memory() but also pretouches if AlwaysPreTouch is enabled.\n+ * Same as os::map_memory() but also pretouches memory unconditionally,\n+ * as we are very likely to start writing into that memory during archive load.\n@@ -1716,3 +1736,2 @@\n-static char* map_memory(int fd, const char* file_name, size_t file_offset,\n-                        char *addr, size_t bytes, bool read_only,\n-                        bool allow_exec, MemTag mem_tag = mtNone) {\n+char* FileMapInfo::map_memory(int fd, const char* file_name, size_t file_offset,\n+                              char *addr, size_t bytes, bool read_only, bool allow_exec) {\n@@ -1720,4 +1739,5 @@\n-                             AlwaysPreTouch ? false : read_only,\n-                             allow_exec, mem_tag);\n-  if (mem != nullptr && AlwaysPreTouch) {\n-    os::pretouch_memory(mem, mem + bytes);\n+                             (ArchivePreTouch || AlwaysPreTouch) ? false : read_only,\n+                             allow_exec, mtClassShared);\n+  if (mem != nullptr && (ArchivePreTouch || AlwaysPreTouch)) {\n+    ArchiveRegionPretouchTask pretouch(mem, bytes);\n+    _archive_workers.run_task(&pretouch);\n@@ -1868,1 +1888,1 @@\n-                            r->allow_exec(), mtClassShared);\n+                            r->allow_exec());\n@@ -1896,1 +1916,1 @@\n-                                 requested_addr, r->used_aligned(), read_only, allow_exec, mtClassShared);\n+                                 requested_addr, r->used_aligned(), read_only, allow_exec);\n@@ -1919,0 +1939,25 @@\n+class SharedDataRelocationTask : public ArchiveWorkerTask {\n+private:\n+  BitMapView* const _rw_bm;\n+  BitMapView* const _ro_bm;\n+  SharedDataRelocator* const _rw_reloc;\n+  SharedDataRelocator* const _ro_reloc;\n+\n+public:\n+  SharedDataRelocationTask(BitMapView* rw_bm, BitMapView* ro_bm, SharedDataRelocator* rw_reloc, SharedDataRelocator* ro_reloc) :\n+                           ArchiveWorkerTask(\"Shared Data Relocation\"),\n+                           _rw_bm(rw_bm), _ro_bm(ro_bm), _rw_reloc(rw_reloc), _ro_reloc(ro_reloc) {}\n+\n+  void work(int chunk, int max_chunks) override {\n+    work_on(chunk, max_chunks, _rw_bm, _rw_reloc);\n+    work_on(chunk, max_chunks, _ro_bm, _ro_reloc);\n+  }\n+\n+  void work_on(int chunk, int max_chunks, BitMapView* bm, SharedDataRelocator* reloc) {\n+    BitMap::idx_t size  = bm->size();\n+    BitMap::idx_t start = MIN2(size, size * chunk \/ max_chunks);\n+    BitMap::idx_t end   = MIN2(size, size * (chunk + 1) \/ max_chunks);\n+    bm->iterate(reloc, start, end);\n+  }\n+};\n+\n@@ -1957,2 +2002,8 @@\n-    rw_ptrmap.iterate(&rw_patcher);\n-    ro_ptrmap.iterate(&ro_patcher);\n+\n+    if (ArchiveParallelRelocation) {\n+      SharedDataRelocationTask task(&rw_ptrmap, &ro_ptrmap, &rw_patcher, &ro_patcher);\n+      _archive_workers.run_task(&task);\n+    } else {\n+      rw_ptrmap.iterate(&rw_patcher);\n+      ro_ptrmap.iterate(&ro_patcher);\n+    }\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":63,"deletions":12,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -344,0 +344,2 @@\n+  ArchiveWorkers _archive_workers;\n+\n@@ -470,0 +472,3 @@\n+  char* map_memory(int fd, const char* file_name, size_t file_offset,\n+                   char *addr, size_t bytes, bool read_only, bool allow_exec);\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}