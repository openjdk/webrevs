{"files":[{"patch":"@@ -1759,0 +1759,16 @@\n+void Assembler::cmpb(Address dst, Register reg) {\n+  assert(reg->has_byte_register(), \"must have byte register\");\n+  InstructionMark im(this);\n+  prefix(dst, reg, true);\n+  emit_int8((unsigned char)0x38);\n+  emit_operand(reg, dst, 0);\n+}\n+\n+void Assembler::cmpb(Register reg, Address dst) {\n+  assert(reg->has_byte_register(), \"must have byte register\");\n+  InstructionMark im(this);\n+  prefix(dst, reg, true);\n+  emit_int8((unsigned char)0x3a);\n+  emit_operand(reg, dst, 0);\n+}\n+\n@@ -1790,0 +1806,7 @@\n+void Assembler::cmpl(Address dst,  Register reg) {\n+  InstructionMark im(this);\n+  prefix(dst, reg);\n+  emit_int8(0x39);\n+  emit_operand(reg, dst, 0);\n+}\n+\n@@ -1805,0 +1828,8 @@\n+void Assembler::cmpw(Address dst, Register reg) {\n+  InstructionMark im(this);\n+  emit_int8(0x66);\n+  prefix(dst, reg);\n+  emit_int8((unsigned char)0x39);\n+  emit_operand(reg, dst, 1);\n+}\n+\n@@ -4418,0 +4449,11 @@\n+void Assembler::vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() : VM_Version::supports_avx2(), \"\");\n+  assert(vector_len <= AVX_256bit, \"evex encoding is different - has k register as dest\");\n+  InstructionAttr attributes(vector_len, \/* rex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+\/\/  int encode = vex_prefix_and_encode(dst->encoding(), src1->encoding(), src2, VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+\/\/  emit_int16(0x74, (0xC0 | encode));\n+  vex_prefix(src2, src1->encoding(), dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int8(0x74);\n+  emit_operand(dst, src2, 0);\n+}\n+\n@@ -7579,0 +7621,8 @@\n+void Assembler::vpminub(XMMRegister dst, XMMRegister nds, XMMRegister src, int vector_len) {\n+  assert(vector_len == AVX_128bit ? VM_Version::supports_avx() :\n+        (vector_len == AVX_256bit ? VM_Version::supports_avx2() : VM_Version::supports_avx512bw()), \"\");\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ false, \/* legacy_mode *\/ _legacy_mode_bw, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F, &attributes);\n+  emit_int16(0xDA, (0xC0 | encode));\n+}\n+\n@@ -12187,0 +12237,7 @@\n+  emit_int16((unsigned char)0xF5, (0xC0 | encode));\n+}\n+\n+void Assembler::bzhil(Register dst, Register src1, Register src2) {\n+  assert(VM_Version::supports_bmi2(), \"bit manipulation instructions not supported\");\n+  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ true, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n+  int encode = vex_prefix_and_encode(dst->encoding(), src2->encoding(), src1->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_38, &attributes);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -1112,0 +1112,2 @@\n+  void cmpb(Address dst, Register reg);\n+  void cmpb(Register reg, Address dst);\n@@ -1118,0 +1120,1 @@\n+  void cmpl(Address dst,  Register reg);\n@@ -1126,0 +1129,1 @@\n+  void cmpw(Address dst, Register reg);\n@@ -1801,0 +1805,1 @@\n+  void vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len);\n@@ -2298,0 +2303,1 @@\n+  void bzhil(Register dst, Register src1, Register src2);\n@@ -2573,0 +2579,1 @@\n+  void vpminub(XMMRegister dst, XMMRegister src1, XMMRegister src2, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1171,0 +1171,8 @@\n+\/\/ Alignment specifying the maximum number of allowed bytes to pad.\n+\/\/ If padding > max, no padding is inserted.\n+void MacroAssembler::p2align(int modulus, int maxbytes) {\n+  if (modulus - (offset() % modulus) <= maxbytes) {\n+    align(modulus, offset());\n+  }\n+}\n+\n@@ -3574,0 +3582,4 @@\n+void MacroAssembler::vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len) {\n+  Assembler::vpcmpeqb(dst, src1, src2, vector_len);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -218,0 +218,1 @@\n+  void p2align(int modulus, int maxbytes);\n@@ -933,0 +934,68 @@\n+  \/\/ Adding more natural conditional jump instructions\n+  void ALWAYSINLINE jo(Label& L, bool maybe_short = true) { jcc(Assembler::overflow, L, maybe_short); }\n+  void ALWAYSINLINE jno(Label& L, bool maybe_short = true) { jcc(Assembler::noOverflow, L, maybe_short); }\n+  void ALWAYSINLINE js(Label& L, bool maybe_short = true) { jcc(Assembler::positive, L, maybe_short); }\n+  void ALWAYSINLINE jns(Label& L, bool maybe_short = true) { jcc(Assembler::negative, L, maybe_short); }\n+  void ALWAYSINLINE je(Label& L, bool maybe_short = true) { jcc(Assembler::equal, L, maybe_short); }\n+  void ALWAYSINLINE jz(Label& L, bool maybe_short = true) { jcc(Assembler::zero, L, maybe_short); }\n+  void ALWAYSINLINE jne(Label& L, bool maybe_short = true) { jcc(Assembler::notEqual, L, maybe_short); }\n+  void ALWAYSINLINE jnz(Label& L, bool maybe_short = true) { jcc(Assembler::notZero, L, maybe_short); }\n+  void ALWAYSINLINE jb(Label& L, bool maybe_short = true) { jcc(Assembler::below, L, maybe_short); }\n+  void ALWAYSINLINE jnae(Label& L, bool maybe_short = true) { jcc(Assembler::below, L, maybe_short); }\n+  void ALWAYSINLINE jc(Label& L, bool maybe_short = true) { jcc(Assembler::carrySet, L, maybe_short); }\n+  void ALWAYSINLINE jnb(Label& L, bool maybe_short = true) { jcc(Assembler::aboveEqual, L, maybe_short); }\n+  void ALWAYSINLINE jae(Label& L, bool maybe_short = true) { jcc(Assembler::aboveEqual, L, maybe_short); }\n+  void ALWAYSINLINE jnc(Label& L, bool maybe_short = true) { jcc(Assembler::carryClear, L, maybe_short); }\n+  void ALWAYSINLINE jbe(Label& L, bool maybe_short = true) { jcc(Assembler::belowEqual, L, maybe_short); }\n+  void ALWAYSINLINE jna(Label& L, bool maybe_short = true) { jcc(Assembler::belowEqual, L, maybe_short); }\n+  void ALWAYSINLINE ja(Label& L, bool maybe_short = true) { jcc(Assembler::above, L, maybe_short); }\n+  void ALWAYSINLINE jnbe(Label& L, bool maybe_short = true) { jcc(Assembler::above, L, maybe_short); }\n+  void ALWAYSINLINE jl(Label& L, bool maybe_short = true) { jcc(Assembler::less, L, maybe_short); }\n+  void ALWAYSINLINE jnge(Label& L, bool maybe_short = true) { jcc(Assembler::less, L, maybe_short); }\n+  void ALWAYSINLINE jge(Label& L, bool maybe_short = true) { jcc(Assembler::greaterEqual, L, maybe_short); }\n+  void ALWAYSINLINE jnl(Label& L, bool maybe_short = true) { jcc(Assembler::greaterEqual, L, maybe_short); }\n+  void ALWAYSINLINE jle(Label& L, bool maybe_short = true) { jcc(Assembler::lessEqual, L, maybe_short); }\n+  void ALWAYSINLINE jng(Label& L, bool maybe_short = true) { jcc(Assembler::lessEqual, L, maybe_short); }\n+  void ALWAYSINLINE jg(Label& L, bool maybe_short = true) { jcc(Assembler::greater, L, maybe_short); }\n+  void ALWAYSINLINE jnle(Label& L, bool maybe_short = true) { jcc(Assembler::greater, L, maybe_short); }\n+  void ALWAYSINLINE jp(Label& L, bool maybe_short = true) { jcc(Assembler::parity, L, maybe_short); }\n+  void ALWAYSINLINE jpe(Label& L, bool maybe_short = true) { jcc(Assembler::parity, L, maybe_short); }\n+  void ALWAYSINLINE jnp(Label& L, bool maybe_short = true) { jcc(Assembler::noParity, L, maybe_short); }\n+  void ALWAYSINLINE jpo(Label& L, bool maybe_short = true) { jcc(Assembler::noParity, L, maybe_short); }\n+\/\/ * No condition for this *  void ALWAYSINLINE jcxz(Label& L, bool maybe_short = true) { jcc(Assembler::cxz, L, maybe_short); }\n+\/\/ * No condition for this *  void ALWAYSINLINE jecxz(Label& L, bool maybe_short = true) { jcc(Assembler::cxz, L, maybe_short); }\n+\n+\/\/ Short versions of the above\n+  void ALWAYSINLINE jo_b(Label& L) { jccb(Assembler::overflow, L); }\n+  void ALWAYSINLINE jno_b(Label& L) { jccb(Assembler::noOverflow, L); }\n+  void ALWAYSINLINE js_b(Label& L) { jccb(Assembler::positive, L); }\n+  void ALWAYSINLINE jns_b(Label& L) { jccb(Assembler::negative, L); }\n+  void ALWAYSINLINE je_b(Label& L) { jccb(Assembler::equal, L); }\n+  void ALWAYSINLINE jz_b(Label& L) { jccb(Assembler::zero, L); }\n+  void ALWAYSINLINE jne_b(Label& L) { jccb(Assembler::notEqual, L); }\n+  void ALWAYSINLINE jnz_b(Label& L) { jccb(Assembler::notZero, L); }\n+  void ALWAYSINLINE jb_b(Label& L) { jccb(Assembler::below, L); }\n+  void ALWAYSINLINE jnae_b(Label& L) { jccb(Assembler::below, L); }\n+  void ALWAYSINLINE jc_b(Label& L) { jccb(Assembler::carrySet, L); }\n+  void ALWAYSINLINE jnb_b(Label& L) { jccb(Assembler::aboveEqual, L); }\n+  void ALWAYSINLINE jae_b(Label& L) { jccb(Assembler::aboveEqual, L); }\n+  void ALWAYSINLINE jnc_b(Label& L) { jccb(Assembler::carryClear, L); }\n+  void ALWAYSINLINE jbe_b(Label& L) { jccb(Assembler::belowEqual, L); }\n+  void ALWAYSINLINE jna_b(Label& L) { jccb(Assembler::belowEqual, L); }\n+  void ALWAYSINLINE ja_b(Label& L) { jccb(Assembler::above, L); }\n+  void ALWAYSINLINE jnbe_b(Label& L) { jccb(Assembler::above, L); }\n+  void ALWAYSINLINE jl_b(Label& L) { jccb(Assembler::less, L); }\n+  void ALWAYSINLINE jnge_b(Label& L) { jccb(Assembler::less, L); }\n+  void ALWAYSINLINE jge_b(Label& L) { jccb(Assembler::greaterEqual, L); }\n+  void ALWAYSINLINE jnl_b(Label& L) { jccb(Assembler::greaterEqual, L); }\n+  void ALWAYSINLINE jle_b(Label& L) { jccb(Assembler::lessEqual, L); }\n+  void ALWAYSINLINE jng_b(Label& L) { jccb(Assembler::lessEqual, L); }\n+  void ALWAYSINLINE jg_b(Label& L) { jccb(Assembler::greater, L); }\n+  void ALWAYSINLINE jnle_b(Label& L) { jccb(Assembler::greater, L); }\n+  void ALWAYSINLINE jp_b(Label& L) { jccb(Assembler::parity, L); }\n+  void ALWAYSINLINE jpe_b(Label& L) { jccb(Assembler::parity, L); }\n+  void ALWAYSINLINE jnp_b(Label& L) { jccb(Assembler::noParity, L); }\n+  void ALWAYSINLINE jpo_b(Label& L) { jccb(Assembler::noParity, L); }\n+\/\/ * No condition for this *  void ALWAYSINLINE jcxz_b(Label& L) { jccb(Assembler::cxz, L); }\n+\/\/ * No condition for this *  void ALWAYSINLINE jecxz_b(Label& L) { jccb(Assembler::cxz, L); }\n+\n@@ -1363,0 +1432,1 @@\n+  void vpcmpeqb(XMMRegister dst, XMMRegister src1, Address src2, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -4107,0 +4107,4 @@\n+  if ((UseAVX == 2) && VM_Version::supports_avx2()) {\n+    StubRoutines::_string_indexof = generate_string_indexof(); \/\/ ASGASG\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -542,0 +542,2 @@\n+  void loop_helper(int size, Label& bailout, Label& loop_top);\n+  address generate_string_indexof();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,2585 @@\n+\/*\n+ * Copyright (c) 2023, Intel Corporation. All rights reserved.\n+ * Intel Math Library (LIBM) Source Code\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"macroAssembler_x86.hpp\"\n+#include \"precompiled.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+\/******************************************************************************\/\n+\/\/                     String handling intrinsics\n+\/\/                     --------------------------\n+\/\/\n+\/\/ Currently implements scheme described in http:\/\/0x80.pl\/articles\/simd-strfind.html\n+\/\/ Implementation can be found at https:\/\/github.com\/WojciechMula\/sse4-strstr\n+\/\/\n+\/\/ The general idea is as follows:\n+\/\/ 1. Broadcast the first byte of the needle to a ymm register (32 bytes)\n+\/\/ 2. Broadcast the last byte of the needle to a different ymm register\n+\/\/ 3. Compare the first-byte ymm register to the first 32 bytes of the haystack\n+\/\/ 4. Compare the last-byte register to the 32 bytes of the haystack at the (k-1)st position\n+\/\/ 5. Logically AND the results of the comparison\n+\/\/\n+\/\/ The result of the AND yields the position within the haystack where both the first\n+\/\/ and last bytes of the needle exist in their correct relative positions.  Check the full\n+\/\/ needle value against the haystack to confirm a match.\n+\/\/\n+\/\/ This implementation uses memcmp to compare when the size of the needle is >= 32 bytes.\n+\/\/ For other needle sizes, the comparison is done with register compares to eliminate the\n+\/\/ overhead of the call (including range checks, etc.).  The size of the comparison is\n+\/\/ known, and it is also known to be safe reading the haystack for the full width of the needle.\n+\/\/\n+\/\/ The original algorithm as implemented will potentially read past the end of the haystack.\n+\/\/ This implementation protects against that.  Instead of reading as many 32-byte chunks as\n+\/\/ possible and then handling the tail, we calculate the last position of a vaild 32-byte\n+\/\/ read and adjust the starting position of the second read such that the last read will not\n+\/\/ go beyond the end of the haystack.  So the first comparison is to the first 32 bytes of the\n+\/\/ haystack, and the second is offset by an amount to make the last read legal.  The remainder of\n+\/\/ the comparisons are done incrementing by 32 bytes.\n+\/\/\n+\/\/ This will cause 16 bytes on average to be examined twice, but that is cheaper than the\n+\/\/ logic required for tail processing.\n+\/\/\n+\/******************************************************************************\/\n+\n+#define __ _masm->\n+\n+\/******************************************************************************\/\n+\/\/                     Helper for loop construct\n+\/\/                     --------------------------\n+\/\/\n+\/\/ Code:\n+\/\/\n+\/\/ template <size_t k, typename MEMCMP>\n+\/\/ size_t FORCE_INLINE avx2_strstr_memcmp_ptr(const char *s, size_t n, const char *needle, MEMCMP memcmp_fun)\n+\/\/ {\n+\/\/   char *start = (char *)s;\n+\/\/   char *end = (char *)&s[(n)]; \/\/ & ~0x1f];\n+\/\/   long long incr = (n <= 32) ? 32 : (n - k - 31) % 32;\n+\n+\/\/   const __m256i first = _mm256_set1_epi8(needle[0]);\n+\/\/   const __m256i last = _mm256_set1_epi8(needle[k - 1]);\n+\n+\/\/   while (s < end)\n+\/\/   {\n+\n+\/\/     const __m256i block_first = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(s));\n+\/\/     CHECK_BOUNDS(s, 32, start);\n+\/\/     const __m256i block_last = _mm256_loadu_si256(reinterpret_cast<const __m256i *>(s + k - 1));\n+\/\/     CHECK_BOUNDS(s + k - 1, 32, start);\n+\n+\/\/     const __m256i eq_first = _mm256_cmpeq_epi8(first, block_first);\n+\/\/     const __m256i eq_last = _mm256_cmpeq_epi8(last, block_last);\n+\n+\/\/     uint32_t mask = _mm256_movemask_epi8(_mm256_and_si256(eq_first, eq_last));\n+\n+\/\/     while (mask != 0)\n+\/\/     {\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/  Helper code ends here, before comparing full needle\n+\/\/       const auto bitpos = bits::get_first_bit_set(mask);\n+\/\/       if (memcmp_fun(s + bitpos + 1, needle + 1, k - 2) == 0)\n+\/\/       {\n+\/\/         return s - start + bitpos;\n+\/\/       }\n+\n+\/\/       mask = bits::clear_leftmost_set(mask);\n+\/\/     }\n+\/\/     s += incr;\n+\/\/     incr = 32;\n+\/\/   }\n+\n+\/\/   return std::string::npos;\n+\/\/ }\n+\/******************************************************************************\/\n+\n+void StubGenerator::loop_helper(int size, Label& bailout, Label& loop_top) {\n+  Label temp;\n+\n+  __ movq(r13, -1);\n+  __ testq(r15, r15);\n+  __ jle(bailout);\n+  __ vpbroadcastb(xmm0, Address(r10, 0), Assembler::AVX_256bit);\n+  __ vpbroadcastb(xmm1, Address(r10, size - 1), Assembler::AVX_256bit);\n+  __ leaq(rax, Address(r11, r15, Address::times_1));\n+  __ leal(rcx, Address(r15, 33 - size));\n+  __ andl(rcx, 0x1f);\n+  __ cmpl(r15, 0x21);\n+  __ movl(r15, 0x20);\n+  __ cmovl(Assembler::aboveEqual, r15, rcx);\n+  __ movq(rcx, r11);\n+  __ jmpb(temp);\n+  __ bind(loop_top);\n+  __ addq(rcx, r15);\n+  __ movl(r15, 0x20);\n+  __ cmpq(rcx, rax);\n+  __ jae(bailout);\n+  __ bind(temp);\n+  __ vpcmpeqb(xmm2, xmm0, Address(rcx, 0), Assembler::AVX_256bit);\n+  __ vpcmpeqb(xmm3, xmm1, Address(rcx, size - 1), Assembler::AVX_256bit);\n+  __ vpand(xmm2, xmm3, xmm2, Assembler::AVX_256bit);\n+  __ vpmovmskb(rdx, xmm2, Assembler::AVX_256bit);\n+  __ testl(rdx, rdx);\n+  __ je_b(loop_top);\n+}\n+\n+address StubGenerator::generate_string_indexof() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"stringIndexOf\");\n+  address large_hs_jmp_table[32];   \/\/ Jump table for large haystacks\n+  address small_hs_jmp_table[32];   \/\/ Jump table for small haystacks\n+  int jmp_ndx = 0;\n+  __ align(CodeEntryAlignment);\n+  address start = __ pc();\n+  __ enter();  \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  \/\/                         AVX2 code\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  if (VM_Version::supports_avx2()) {  \/\/ AVX2 version\n+    Label memcmp_avx2;\n+\n+    Label L_begin, L_0x406044, L_CASE_0, L_0x406019;\n+    Label L_trampoline, L_anysize, L_0x404912;\n+    Label L_exit, L_long_compare, L_top_loop_1, L_0x4049cc, L_error;\n+    Label L_small_string, L_0x405cee, L_0x405f5d, L_0x406008;\n+    Label L_0x405fff, L_final_check, L_mid_anysize_loop, L_top_anysize_loop;\n+    Label L_inner_anysize_loop, L_0x40602e, L_0x40607f, L_0x405018;\n+    Label L_0x40605e, L_0x406093, L_0x40559d, L_0x404933, L_byte_copy;\n+    Label L_set_s, L_small_string2, L_0x4060a3;\n+\n+    address jump_table;\n+    address jump_table_1;\n+\n+    __ jmp(L_begin);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ Small-ish string\n+    \/\/ On entry:\n+    \/\/    r15, rsi <= n\n+    \/\/    rax <= scratch\n+    \/\/    rdi, r11 <= s\n+    \/\/    r10, rdx <= needle\n+    \/\/    r12, rcx <= k\n+    \/\/    rbx <= n - k\n+    \/\/ if (n <= STACK_SZ) {\n+    \/\/   \/\/ Copy haystack to stack and adjust parameters\n+    \/\/   int i = n;\n+    \/\/   char *hs = (char *) s;\n+    \/\/   char *dst = tmp_string;\n+    \/\/   int ndx = 0;\n+    __ bind(L_small_string);\n+    __ cmpq(r15, 0x20);\n+    __ ja(L_small_string2);\n+    __ leaq(r12, Address(rsp, 0x80));  \/\/ tmp_string\n+\n+    \/\/ if (i <= 16) {\n+    \/\/   hs = (char *) &s[i - 16];\n+    \/\/   ndx = 16 - i;\n+    \/\/   const __m128i *A = reinterpret_cast<const __m128i*>(hs);\n+    \/\/   __m128i *B = reinterpret_cast<__m128i*>(dst);\n+    \/\/   *B = *A;\n+    __ cmpl(r15, 0x10);\n+    __ ja(L_byte_copy);\n+    __ movdqu(xmm0, Address(r11, r15, Address::times_1, -0x10));\n+    __ movdqu(Address(r12, 0), xmm0);\n+    __ movl(rax, 0x10);\n+    __ subl(rax, r15);  \/\/ 16 - i\n+\n+    __ bind(L_set_s);\n+    \/\/ s = &tmp_string[ndx];\n+    __ leaq(rdi, Address(r12, rax, Address::times_1));\n+    __ movq(r12, rcx);\n+    __ jmp(L_0x404933);\n+\n+\/\/  Copy the haystack (16 < size < 32) to the stack\n+    __ bind(L_byte_copy);\n+    {\n+      Label L_8, L_4, L_2, L_1, L_restore;\n+      __ cmpl(r15, 0x10);\n+      __ jb_b(L_8);\n+      __ movdqu(xmm0, Address(r11, 0));\n+      __ movdqu(Address(r12, 0), xmm0);\n+      __ subl(r15, 0x10);\n+      __ addptr(r11, 0x10);\n+      __ addptr(r12, 0x10);\n+\n+      __ bind(L_8);\n+      __ cmpl(r15, 0x8);\n+      __ jb_b(L_4);\n+      __ movq(rax, Address(r11, 0));\n+      __ movq(Address(r12, 0), rax);\n+      __ subl(r15, 0x8);\n+      __ addptr(r11, 0x8);\n+      __ addptr(r12, 0x8);\n+\n+      __ bind(L_4);\n+      __ cmpl(r15, 0x4);\n+      __ jb_b(L_2);\n+      __ movl(rax, Address(r11, 0));\n+      __ movl(Address(r12, 0), rax);\n+      __ subl(r15, 0x4);\n+      __ addptr(r11, 0x4);\n+      __ addptr(r12, 0x4);\n+\n+      __ bind(L_2);\n+      __ cmpl(r15, 0x2);\n+      __ jb_b(L_1);\n+      __ movzwl(rax, Address(r11, 0));\n+      __ movw(Address(r12, 0), rax);\n+      __ subl(r15, 0x2);\n+      __ addptr(r11, 0x2);\n+      __ addptr(r12, 0x2);\n+\n+      __ bind(L_1);\n+      __ cmpl(r15, 0x1);\n+      __ jb_b(L_restore);\n+      __ movzbl(rax, Address(r11, 0));\n+      __ movb(Address(r12, 0), rax);\n+\n+      __ bind(L_restore);\n+      __ xorq(rax, rax);\n+      __ movq(r15, rsi);\n+      __ movq(r11, rdi);\n+      __ jmp(L_set_s);\n+    }\n+\n+    \/\/    Move through the small-ish string char by char\n+    \/\/ for (size_t i = 0; i < n - k + 1; i++) {\n+    \/\/   if (s[i] == needle[0] && s[i + k - 1] == needle[k - 1]) {\n+    \/\/     switch (k) {\n+    __ bind(L_small_string2);\n+    __ incrementq(r15);\n+    __ subq(r15, r12);\n+    __ je(L_error);\n+    __ movzbl(rbp, Address(r10, 0));\n+    __ leaq(rcx, Address(r10, 0x1));\n+    __ leaq(rdx, Address(r12, -0x2));\n+    __ cmpq(r15, 0x2);\n+    __ movl(r13, 1);\n+    __ cmovq(Assembler::aboveEqual, r13, r15);\n+    __ leaq(rbx, Address(r11, 0x1d));\n+    __ leaq(r14, Address(r12, r11, Address::times_1));\n+    __ decrementq(r14);\n+    __ incrementq(r11);\n+    __ xorl(r15, r15);\n+    __ jmpb(L_0x4049cc);\n+\n+    __ align(8);\n+    __ bind(L_top_loop_1);\n+    __ incrementq(r15);\n+    __ cmpq(r13, r15);\n+    __ je(L_error);\n+    __ bind(L_0x4049cc);\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x1d), rbp);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r14, r15, Address::times_1));\n+    __ cmpb(rax, Address(r10, r12, Address::times_1, -0x1));\n+    __ jne(L_top_loop_1);\n+\n+    __ leaq(rax, Address(r12, -0x1));\n+    __ cmpq(rax, 0x1e);\n+    __ ja_b(L_long_compare);\n+    __ jmp(L_trampoline);  \/\/ Jump to the correct case for small haystacks\n+\n+    \/\/  Needle size >= 32 - use memcmp\n+    __ bind(L_long_compare);\n+    __ leaq(rdi, Address(r11, r15, Address::times_1));\n+    __ movq(Address(rsp, 0x10), rcx);\n+    __ movq(rsi, Address(rsp, 0x10));\n+    __ movq(Address(rsp, 0x8), rdx);\n+    __ movq(rdx, Address(rsp, 0x8));\n+    __ movq(Address(rsp, 0x18), r11);\n+    __ movq(Address(rsp, 0x30), r10);\n+    __ call(memcmp_avx2, relocInfo::none);\n+    __ movq(rdx, Address(rsp, 0x8));\n+    __ movq(rcx, Address(rsp, 0x10));\n+    __ movq(r10, Address(rsp, 0x30));\n+    __ movq(r11, Address(rsp, 0x18));\n+    __ testl(rax, rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 2:\n+    small_hs_jmp_table[2] = __ pc();\n+    __ movzbl(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpb(rax, Address(rcx, 0));\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 3:\n+    small_hs_jmp_table[3] = __ pc();\n+    __ movzwl(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpw(Address(rcx, 0), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 4: CASE 5:\n+    small_hs_jmp_table[4] = __ pc();\n+    small_hs_jmp_table[5] = __ pc();\n+    __ movl(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpl(rax, Address(rcx, 0));\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 6:\n+    small_hs_jmp_table[6] = __ pc();\n+    __ movl(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpl(rax, Address(rcx, 0));\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(rbx, r15, Address::times_1, -0x18));\n+    __ cmpb(rax, Address(r10, 0x5));\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 7:\n+    small_hs_jmp_table[7] = __ pc();\n+    __ movl(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpl(rax, Address(rcx, 0));\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(rbx, r15, Address::times_1, -0x18));\n+    __ cmpw(Address(r10, 0x5), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 8 CASE 9:\n+    small_hs_jmp_table[8] = __ pc();\n+    small_hs_jmp_table[9] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(rcx, 0));\n+    __ je(L_0x406019);\n+\n+    \/\/  CASE 10:\n+    small_hs_jmp_table[10] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x9));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 11:\n+    small_hs_jmp_table[11] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0x9));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 12:\n+    small_hs_jmp_table[12] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(rcx, 0));\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(rbx, r15, Address::times_1, -0x14));\n+    __ cmpw(Address(r10, 0x9), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(rbx, r15, Address::times_1, -0x12));\n+    __ cmpb(rax, Address(r10, 0xb));\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 13:\n+    small_hs_jmp_table[13] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x9));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 14:\n+    small_hs_jmp_table[14] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x9));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0xd));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x10), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 15:\n+    small_hs_jmp_table[15] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x9));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0xd));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x10), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 16:\n+    small_hs_jmp_table[16] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x9));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0xd));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x10), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0xf));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0xe), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 17:\n+    small_hs_jmp_table[17] = __ pc();\n+    __ movq(rax, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ cmpq(rax, Address(r10, 0x1));\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x9));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0x14), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 18:\n+    small_hs_jmp_table[18] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x11));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 19:\n+    small_hs_jmp_table[19] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0x11));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 20:\n+    small_hs_jmp_table[20] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0x11));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x13));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0xa), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 21:\n+    small_hs_jmp_table[21] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x11));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 22:\n+    small_hs_jmp_table[22] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x11));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x15));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x8), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 23:\n+    small_hs_jmp_table[23] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x11));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movw(rax, Address(r10, 0x15));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x8), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 24:\n+    small_hs_jmp_table[24] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x11));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movw(rax, Address(r10, 0x15));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x8), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x17));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x6), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 25:\n+    small_hs_jmp_table[25] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x11));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 26:\n+    small_hs_jmp_table[26] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x11));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x19));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x4), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 27:\n+    small_hs_jmp_table[27] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x11));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0x19));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x4), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 28:\n+    small_hs_jmp_table[28] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x11));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzwl(rax, Address(r10, 0x19));\n+    __ cmpw(Address(rbx, r15, Address::times_1, -0x4), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x1b));\n+    __ cmpb(Address(rbx, r15, Address::times_1, -0x2), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 29:\n+    small_hs_jmp_table[29] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x11));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x19));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0x4), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/  CASE 30:\n+    small_hs_jmp_table[30] = __ pc();\n+    __ movdqu(xmm0, Address(rbx, r15, Address::times_1, -0x1c));\n+    __ vpsubb(xmm0, xmm0, Address(r10, 0x1), Assembler::AVX_128bit);\n+    __ vptest(xmm0, xmm0, Assembler::AVX_128bit);\n+    __ jne(L_top_loop_1);\n+    __ movq(rax, Address(r10, 0x11));\n+    __ cmpq(Address(rbx, r15, Address::times_1, -0xc), rax);\n+    __ jne(L_top_loop_1);\n+    __ movl(rax, Address(r10, 0x19));\n+    __ cmpl(Address(rbx, r15, Address::times_1, -0x4), rax);\n+    __ jne(L_top_loop_1);\n+    __ movzbl(rax, Address(r10, 0x1d));\n+    __ cmpb(Address(rbx, r15, Address::times_1), rax);\n+    __ jne(L_top_loop_1);\n+    __ jmp(L_0x406019);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/ anysize - needle >= 32 and haystack > 32\n+    __ bind(L_anysize);\n+    __ movq(r13, -1);\n+    __ testq(r15, r15);\n+    __ jle(L_exit);\n+    __ movq(Address(rsp, 0x20), rbx);\n+    __ leaq(rax, Address(r11, r15, Address::times_1));\n+    __ movq(Address(rsp, 0x28), rax);\n+    __ vpbroadcastb(xmm0, Address(r10, 0), Assembler::AVX_256bit);\n+    __ vmovdqu(Address(rsp, 0x30), xmm0);\n+    __ vpbroadcastb(xmm0, Address(r12, r10, Address::times_1, -0x1),\n+                    Assembler::AVX_256bit);\n+    __ vmovdqu(Address(rsp, 0x50), xmm0);\n+    __ subl(r15, r12);\n+    __ incrementl(r15);\n+    __ andl(r15, 0x1f);\n+    __ incrementq(r10);\n+    __ leaq(rax, Address(r12, -0x2));\n+    __ movq(Address(rsp, 0x10), rax);\n+    __ movq(Address(rsp, 0x18), r11);\n+    __ jmpb(L_mid_anysize_loop);\n+\n+    __ bind(L_top_anysize_loop);\n+    __ movq(r11, Address(rsp, 0x8));\n+    __ addq(r11, r15);\n+    __ movl(r15, 0x20);\n+    __ cmpq(r11, Address(rsp, 0x28));\n+    __ jae(L_0x4060a3);\n+\n+    __ bind(L_mid_anysize_loop);\n+    __ vmovdqu(xmm0, Address(rsp, 0x30));\n+    __ vpcmpeqb(xmm0, xmm0, Address(r11, 0), Assembler::AVX_256bit);\n+    __ vmovdqu(xmm1, Address(rsp, 0x50));\n+    __ movq(Address(rsp, 0x8), r11);\n+    __ vpcmpeqb(xmm1, xmm1, Address(r11, r12, Address::times_1, -0x1),\n+                Assembler::AVX_256bit);\n+    __ vpand(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+    __ vpmovmskb(rbx, xmm0);\n+    __ testl(rbx, rbx);\n+    __ je(L_top_anysize_loop);\n+    __ movq(rax, Address(rsp, 0x8));\n+    __ leaq(r14, Address(rax, 1));\n+\n+    __ bind(L_inner_anysize_loop);\n+    __ tzcntl(rbp, rbx);\n+    __ leaq(rdi, Address(r14, rbp, Address::times_1));\n+    __ movq(r13, r10);\n+    __ movq(rsi, r10);\n+    __ movq(rdx, Address(rsp, 0x10));\n+    __ vzeroupper();\n+    __ call(memcmp_avx2, relocInfo::none);\n+    __ testl(rax, rax);\n+    __ je(L_0x40602e);\n+    __ blsrl(rbx, rbx);\n+    __ movq(r10, r13);\n+    __ jne_b(L_inner_anysize_loop);\n+    __ jmpb(L_top_anysize_loop);\n+\n+    large_hs_jmp_table[0] = __ pc();  \/\/ Case for needle size == 1\n+    __ vpbroadcastb(xmm0, Address(r10, 0), Assembler::AVX_256bit);\n+    __ vpcmpeqb(xmm1, xmm0, Address(r11, 0), Assembler::AVX_256bit);\n+    __ vpmovmskb(rax, xmm1);\n+    __ testl(rax, rax);\n+    __ je(L_0x406044);\n+    __ tzcntl(r13, rax);\n+    __ jmp(L_exit);\n+\n+    __ bind(L_CASE_0);  \/\/ Needle size == 0\n+    __ xorl(r15, r15);\n+    __ jmp(L_0x406019);\n+\n+    \/\/    case 2:   \/\/ case for needle size == 2\n+    large_hs_jmp_table[1] = __ pc();\n+    __ movq(r13, -1);\n+    __ testq(r15, r15);\n+    __ jle(L_exit);\n+    __ vpbroadcastb(xmm0, Address(r10, 0), Assembler::AVX_256bit);\n+    __ vpbroadcastb(xmm1, Address(r10, 0x1), Assembler::AVX_256bit);\n+    __ leaq(rcx, Address(r11, r15, Address::times_1));\n+    __ decl(r15);\n+    __ andl(r15, 0x1f);\n+    __ cmpl(r15, 0x21);\n+    __ movl(rdx, 0x20);\n+    __ cmovl(Assembler::aboveEqual, rdx, r15);\n+    __ movl(r15, rdx);\n+    __ movq(rax, r11);\n+    __ bind(L_0x405018);\n+    __ vpcmpeqb(xmm2, xmm0, Address(rax, 0), Assembler::AVX_256bit);\n+    __ vpcmpeqb(xmm3, xmm1, Address(rax, 0x1), Assembler::AVX_256bit);\n+    __ vpand(xmm2, xmm3, xmm2, Assembler::AVX_256bit);\n+    __ vpmovmskb(rdx, xmm2, Assembler::AVX_256bit);\n+    __ testl(rdx, rdx);\n+    __ jne(L_0x40607f);\n+    __ addq(rax, r15);\n+    __ cmpq(rax, rcx);\n+    __ jae(L_exit);\n+    __ vpcmpeqb(xmm2, xmm0, Address(rax, 0), Assembler::AVX_256bit);\n+    __ vpcmpeqb(xmm3, xmm1, Address(rax, 0x1), Assembler::AVX_256bit);\n+    __ vpand(xmm2, xmm3, xmm2, Assembler::AVX_256bit);\n+    __ vpmovmskb(rdx, xmm2, Assembler::AVX_256bit);\n+    __ testl(rdx, rdx);\n+    __ jne(L_0x40607f);\n+    __ addq(rax, 0x20);\n+    __ movl(r15, 0x20);\n+    __ cmpq(rax, rcx);\n+    __ jb(L_0x405018);\n+    __ jmp(L_exit);\n+\n+    \/\/    case 3:\n+    large_hs_jmp_table[2] = __ pc();\n+    {\n+      Label L_top, L_inner;\n+      loop_helper(3, L_exit, L_top);\n+      __ movzbl(rsi, Address(r10, 0x1));\n+      __ bind(L_inner);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpb(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ je(L_0x405cee);\n+      __ blsrl(rdx, rdx);\n+      __ jne_b(L_inner);\n+      __ jmp(L_top);\n+    }\n+\n+    \/\/    case 4:\n+    large_hs_jmp_table[3] = __ pc();\n+    {\n+      Label L_top, L_inner;\n+      loop_helper(4, L_exit, L_top);\n+      __ movzwl(rsi, Address(r10, 0x1));\n+      __ bind(L_inner);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpw(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ je(L_0x405cee);\n+      __ blsrl(rdx, rdx);\n+      __ jne_b(L_inner);\n+      __ jmp(L_top);\n+    }\n+\n+    \/\/    case 5:\n+    large_hs_jmp_table[4] = __ pc();\n+    {\n+      Label L_top, L_inner;\n+      loop_helper(5, L_exit, L_top);\n+      __ movl(rsi, Address(r10, 0x1));\n+      __ bind(L_inner);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpl(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ je(L_0x405cee);\n+      __ blsrl(rdx, rdx);\n+      __ jne_b(L_inner);\n+      __ jmp(L_top);\n+    }\n+\n+    \/\/    case 6:\n+    large_hs_jmp_table[5] = __ pc();\n+    {\n+      Label L_top, L_inner;\n+      loop_helper(6, L_exit, L_top);\n+      __ movl(rsi, Address(r10, 0x1));\n+      __ bind(L_inner);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpl(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ je(L_0x405cee);\n+      __ blsrl(rdx, rdx);\n+      __ jne_b(L_inner);\n+      __ jmp(L_top);\n+    }\n+\n+    \/\/    case 7:\n+    large_hs_jmp_table[6] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(7, L_exit, L_top);\n+      __ movl(rsi, Address(r10, 0x1));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpl(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ movzbl(r8, Address(rcx, rdi, Address::times_1, 0x5));\n+      __ cmpb(r8, Address(r10, 0x5));\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x40559d);\n+    }\n+\n+    \/\/    case 8:\n+    large_hs_jmp_table[7] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(8, L_exit, L_top);\n+      __ movl(rsi, Address(r10, 0x1));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpl(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ movzwl(r8, Address(rcx, rdi, Address::times_1, 0x5));\n+      __ cmpw(Address(r10, 0x5), r8);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x40559d);\n+    }\n+\n+    \/\/    case 9:\n+    large_hs_jmp_table[8] = __ pc();\n+    {\n+      Label L_top, L_inner;\n+      loop_helper(9, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ bind(L_inner);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpq(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ je(L_0x405cee);\n+      __ blsrl(rdx, rdx);\n+      __ jne_b(L_inner);\n+      __ jmp(L_top);\n+    }\n+\n+    \/\/    case 10:\n+    large_hs_jmp_table[9] = __ pc();\n+    {\n+      Label L_top, L_inner;\n+      loop_helper(10, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ bind(L_inner);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpq(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ je(L_0x405cee);\n+      __ blsrl(rdx, rdx);\n+      __ jne_b(L_inner);\n+      __ jmp(L_top);\n+    }\n+\n+    \/\/    case 11:\n+    large_hs_jmp_table[10] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(11, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ movzbl(rdi, Address(r10, 0x9));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r8, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 12:\n+    large_hs_jmp_table[11] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(12, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ movzwl(rdi, Address(r10, 0x9));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r8, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 13:\n+    large_hs_jmp_table[12] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(13, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ jmpb(L_tmp);\n+      __ align(8);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ cmpq(Address(rcx, rdi, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ movzwl(r8, Address(rcx, rdi, Address::times_1, 0x9));\n+      __ cmpw(Address(r10, 0x9), r8);\n+      __ jne_b(L_inner);\n+      __ movzbl(r8, Address(rcx, rdi, Address::times_1, 0xb));\n+      __ cmpb(r8, Address(r10, 0xb));\n+      __ jne_b(L_inner);\n+      __ bind(L_0x40559d);\n+      __ subq(rcx, r11);\n+      __ addq(rcx, rdi);\n+      __ jmp(L_0x406008);\n+    }\n+\n+    \/\/    case 14:\n+    large_hs_jmp_table[13] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(14, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ movl(rdi, Address(r10, 0x9));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r8, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 15:\n+    large_hs_jmp_table[14] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(15, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ movl(rdi, Address(r10, 0x9));\n+      __ movzbl(r8, Address(r10, 0xd));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r9, rdx);\n+      __ cmpq(Address(rcx, r9, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r9, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r9, Address::times_1, 0xd), r8);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405fff);\n+    }\n+\n+    \/\/    case 16:\n+    large_hs_jmp_table[15] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(16, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ movl(rdi, Address(r10, 0x9));\n+      __ movzwl(r8, Address(r10, 0xd));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r9, rdx);\n+      __ cmpq(Address(rcx, r9, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r9, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r9, Address::times_1, 0xd), r8);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405fff);\n+    }\n+\n+    \/\/    case 17:\n+    large_hs_jmp_table[16] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      __ movq(r14, r10);\n+      loop_helper(17, L_exit, L_top);\n+      __ movq(r9, r14);\n+      __ movq(rsi, Address(r14, 0x1));\n+      __ movl(rdi, Address(r14, 0x9));\n+      __ movzwl(r8, Address(r14, 0xd));\n+      __ movzbl(r9, Address(r14, 0xf));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r10, rdx);\n+      __ cmpq(Address(rcx, r10, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r10, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r10, Address::times_1, 0xd), r8);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r10, Address::times_1, 0xf), r9);\n+      __ jne_b(L_inner);\n+      __ movl(rax, r10);\n+      __ jmp(L_final_check);\n+    }\n+\n+    \/\/    case 18:\n+    large_hs_jmp_table[17] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(18, L_exit, L_top);\n+      __ movq(rsi, Address(r10, 0x1));\n+      __ movq(rdi, Address(r10, 0x9));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x1), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x9), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 19:\n+    large_hs_jmp_table[18] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(19, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movzbl(rsi, Address(r10, 0x11));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ movdqu(xmm3, Address(rcx, rdi, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, rdi, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405cee);\n+    }\n+\n+    \/\/    case 20:\n+    large_hs_jmp_table[19] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(20, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movzwl(rsi, Address(r10, 0x11));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ movdqu(xmm3, Address(rcx, rdi, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, rdi, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405cee);\n+    }\n+\n+    \/\/    case 21:\n+    large_hs_jmp_table[20] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(21, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movzwl(rsi, Address(r10, 0x11));\n+      __ movzbl(rdi, Address(r10, 0x13));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ movdqu(xmm3, Address(rcx, r8, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r8, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r8, Address::times_1, 0x13), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 22:\n+    large_hs_jmp_table[21] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(22, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movl(rsi, Address(r10, 0x11));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ movdqu(xmm3, Address(rcx, rdi, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, rdi, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405cee);\n+    }\n+\n+    \/\/    case 23:\n+    large_hs_jmp_table[22] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(23, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movl(rsi, Address(r10, 0x11));\n+      __ movzbl(rdi, Address(r10, 0x15));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ movdqu(xmm3, Address(rcx, r8, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r8, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r8, Address::times_1, 0x15), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 24:\n+    large_hs_jmp_table[23] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(24, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movl(rsi, Address(r10, 0x11));\n+      __ movzwl(rdi, Address(r10, 0x15));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ movdqu(xmm3, Address(rcx, r8, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r8, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r8, Address::times_1, 0x15), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 25:\n+    large_hs_jmp_table[24] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(25, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movl(rsi, Address(r10, 0x11));\n+      __ movzwl(rdi, Address(r10, 0x15));\n+      __ movzbl(r8, Address(r10, 0x17));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r9, rdx);\n+      __ movdqu(xmm3, Address(rcx, r9, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r9, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r9, Address::times_1, 0x15), rdi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r9, Address::times_1, 0x17), r8);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405fff);\n+    }\n+\n+    \/\/    case 26:\n+    large_hs_jmp_table[25] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(26, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movq(rsi, Address(r10, 0x11));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(rdi, rdx);\n+      __ movdqu(xmm3, Address(rcx, rdi, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, rdi, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ bind(L_0x405cee);\n+      __ movl(rax, rdi);\n+      __ jmp(L_final_check);\n+    }\n+\n+    \/\/    case 27:\n+    large_hs_jmp_table[26] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(27, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movq(rsi, Address(r10, 0x11));\n+      __ movzbl(rdi, Address(r10, 0x19));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ movdqu(xmm3, Address(rcx, r8, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r8, Address::times_1, 0x19), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 28:\n+    large_hs_jmp_table[27] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(28, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movq(rsi, Address(r10, 0x11));\n+      __ movzwl(rdi, Address(r10, 0x19));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ movdqu(xmm3, Address(rcx, r8, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r8, Address::times_1, 0x19), rdi);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405f5d);\n+    }\n+\n+    \/\/    case 29:\n+    large_hs_jmp_table[28] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(29, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movq(rsi, Address(r10, 0x11));\n+      __ movzwl(rdi, Address(r10, 0x19));\n+      __ movzbl(r8, Address(r10, 0x1b));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r9, rdx);\n+      __ movdqu(xmm3, Address(rcx, r9, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, r9, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpw(Address(rcx, r9, Address::times_1, 0x19), rdi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r9, Address::times_1, 0x1b), r8);\n+      __ jne_b(L_inner);\n+      __ jmp(L_0x405fff);\n+    }\n+\n+    \/\/    case 30:\n+    large_hs_jmp_table[29] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(30, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movq(rsi, Address(r10, 0x11));\n+      __ movl(rdi, Address(r10, 0x19));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r8, rdx);\n+      __ movdqu(xmm3, Address(rcx, r8, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, r8, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r8, Address::times_1, 0x19), rdi);\n+      __ jne_b(L_inner);\n+      __ bind(L_0x405f5d);\n+      __ movl(rax, r8);\n+      __ jmp(L_final_check);\n+    }\n+\n+    \/\/    case 31:\n+    large_hs_jmp_table[30] = __ pc();\n+    {\n+      Label L_top, L_inner, L_tmp;\n+      loop_helper(31, L_exit, L_top);\n+      __ movdqu(xmm2, Address(r10, 0x1));\n+      __ movq(rsi, Address(r10, 0x11));\n+      __ movl(rdi, Address(r10, 0x19));\n+      __ movzbl(r8, Address(r10, 0x1d));\n+      __ jmpb(L_tmp);\n+      __ bind(L_inner);\n+      __ blsrl(rdx, rdx);\n+      __ je(L_top);\n+      __ bind(L_tmp);\n+      __ tzcntl(r9, rdx);\n+      __ movdqu(xmm3, Address(rcx, r9, Address::times_1, 0x1));\n+      __ vpsubb(xmm3, xmm3, xmm2, Assembler::AVX_128bit);\n+      __ vptest(xmm3, xmm3, Assembler::AVX_128bit);\n+      __ jne_b(L_inner);\n+      __ cmpq(Address(rcx, r9, Address::times_1, 0x11), rsi);\n+      __ jne_b(L_inner);\n+      __ cmpl(Address(rcx, r9, Address::times_1, 0x19), rdi);\n+      __ jne_b(L_inner);\n+      __ cmpb(Address(rcx, r9, Address::times_1, 0x1d), r8);\n+      __ jne_b(L_inner);\n+    }\n+    __ bind(L_0x405fff);\n+    __ movl(rax, r9);\n+\n+    \/\/   if (result <= n - k)\n+    \/\/   {\n+    \/\/     return result;\n+    \/\/   }\n+    __ bind(L_final_check);\n+    __ subq(rcx, r11);\n+    __ addq(rcx, rax);\n+\n+    __ bind(L_0x406008);\n+    __ movq(r13, rcx);\n+\n+    __ bind(L_exit);\n+    __ cmpq(r13, rbx);\n+    __ movq(r15, -1);\n+    __ cmovq(Assembler::belowEqual, r15, r13);\n+    __ bind(L_0x406019);\n+\n+    \/\/ CASE 0: CASE 1:\n+    small_hs_jmp_table[0] = __ pc();\n+    small_hs_jmp_table[1] = __ pc();\n+    __ movq(rax, r15);\n+    __ addptr(rsp, 0xf0);\n+#ifdef _WIN64\n+    __ pop(r9);\n+    __ pop(r8);\n+    __ pop(rcx);\n+    __ pop(rdi);\n+    __ pop(rsi);\n+#endif\n+    __ pop(rbp);\n+    __ pop(rbx);\n+    __ pop(r12);\n+    __ pop(r13);\n+    __ pop(r14);\n+    __ pop(r15);\n+    __ vzeroupper();\n+\n+    __ leave();  \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+\n+\/\/  Tail cleanup stuff\n+    __ bind(L_0x40602e);\n+    __ movl(rax, rbp);\n+    __ movq(r13, Address(rsp, 0x8));\n+    __ subq(r13, Address(rsp, 0x18));\n+    __ addq(r13, rax);\n+    __ movq(rbx, Address(rsp, 0x20));\n+    __ jmpb(L_exit);\n+\n+    __ bind(L_0x406044);\n+    __ movq(rax, r15);\n+    __ andq(rax, -32);\n+    __ andl(r15, 0x1f);\n+    __ movq(r13, -1);\n+    __ cmpq(r15, rax);\n+    __ jge(L_exit);\n+    __ addq(rax, r11);\n+\n+    __ bind(L_0x40605e);\n+    __ vpcmpeqb(xmm1, xmm0, Address(r11, r15, Address::times_1),\n+                Assembler::AVX_256bit);\n+    __ vpmovmskb(rcx, xmm1, Assembler::AVX_256bit);\n+    __ testl(rcx, rcx);\n+    __ jne_b(L_0x406093);\n+    __ leaq(rcx, Address(r11, r15, Address::times_1));\n+    __ addq(rcx, 0x20);\n+    __ addq(r15, 0x20);\n+    __ cmpq(rcx, rax);\n+    __ jb(L_0x40605e);\n+    __ jmp(L_exit);\n+\n+    __ bind(L_0x40607f);\n+    __ tzcntl(rcx, rdx);\n+    __ subq(rax, r11);\n+    __ addq(rax, rcx);\n+    __ movq(r13, rax);\n+    __ jmp(L_exit);\n+\n+    __ bind(L_0x406093);\n+    __ tzcntl(r13, rcx);\n+    __ addq(r13, r15);\n+    __ jmp(L_exit);\n+\n+    __ bind(L_0x4060a3);\n+    __ movq(rbx, Address(rsp, 0x20));\n+    __ movq(r13, -1);\n+    __ jmp(L_exit);\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    __ align(8);\n+\n+    jump_table = __ pc();\n+\n+    for (jmp_ndx = 0; jmp_ndx < 32; jmp_ndx++) {\n+      __ emit_address(large_hs_jmp_table[jmp_ndx]);\n+    }\n+\n+    jump_table_1 = __ pc();\n+\n+    for (jmp_ndx = 0; jmp_ndx < 32; jmp_ndx++) {\n+      __ emit_address(small_hs_jmp_table[jmp_ndx]);\n+    }\n+\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+    \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    __ align(16);\n+    __ bind(L_begin);\n+    __ push(r15);\n+    __ push(r14);\n+    __ push(r13);\n+    __ push(r12);\n+    __ push(rbx);\n+    __ push(rbp);\n+#ifdef _WIN64\n+    __ push(rsi);\n+    __ push(rdi);\n+    __ push(rcx);\n+    __ push(r8);\n+    __ push(r9);\n+\n+    __ movq(rdi, rcx);\n+    __ movq(rsi, rdx);\n+    __ movq(rdx, r8);\n+    __ movq(rcx, r9);\n+#endif\n+\n+    __ subptr(rsp, 0xf0);\n+    \/\/ if (n < k) {\n+    \/\/   return result;\n+    \/\/ }\n+    __ movq(rbx, rsi);\n+    __ subq(rbx, rcx);\n+    __ jae_b(L_0x404912);\n+\n+    __ bind(L_error);\n+    __ movq(r15, -1);\n+    __ jmp(L_0x406019);\n+\n+    __ bind(L_0x404912);\n+\n+    \/\/ if (k == 0) {\n+    \/\/   return 0;\n+    \/\/ }\n+    __ movq(r12, rcx);\n+    __ testq(rcx, rcx);\n+    __ je(L_CASE_0);\n+    __ movq(r10, rdx);\n+    __ movq(r15, rsi);\n+    __ movq(r11, rdi);\n+    __ cmpq(rsi, 0x20);\n+    __ jb(L_small_string);\n+    __ leaq(rax, Address(r12, 0x1f));\n+    __ cmpq(rax, r15);\n+    __ jg(L_small_string);\n+\n+    \/\/ if ((n < 32) || ((long long)n < 32 + (long long)k - 1))\n+    __ bind(L_0x404933);\n+    __ leaq(rax, Address(r12, -0x1));\n+    __ cmpq(rax, 0x1e);\n+    __ ja(L_anysize);\n+    __ mov64(r13, (int64_t)jump_table);\n+    __ jmp(Address(r13, rax, Address::times_8));\n+\n+    __ bind(L_trampoline);\n+    __ mov64(rdi, (int64_t)jump_table_1);\n+    __ jmp(Address(rdi, rax, Address::times_8));\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  \/\/                         memcmp_avx2\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+    __ align(CodeEntryAlignment);\n+    __ bind(memcmp_avx2);\n+\n+    \/\/    1 \/* memcmp\/wmemcmp optimized with AVX2.\n+    \/\/    2    Copyright (C) 2017-2023 Free Software Foundation, Inc.\n+    \/\/    3    This file is part of the GNU C Library.\n+    \/\/    4\n+    \/\/    5    The GNU C Library is free software; you can redistribute it\n+    \/\/    and\/or 6    modify it under the terms of the GNU Lesser General Public\n+    \/\/    7    License as published by the Free Software Foundation; either\n+    \/\/    8    version 2.1 of the License, or (at your option) any later\n+    \/\/    version.\n+    \/\/    9\n+    \/\/   10    The GNU C Library is distributed in the hope that it will be\n+    \/\/   useful, 11    but WITHOUT ANY WARRANTY; without even the implied\n+    \/\/   warranty of 12    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+    \/\/   See the GNU 13    Lesser General Public License for more details. 14 15\n+    \/\/   You should have received a copy of the GNU Lesser General Public 16\n+    \/\/   License along with the GNU C Library; if not, see 17\n+    \/\/   <https:\/\/www.gnu.org\/licenses\/>.  *\/ 18 23 \/* memcmp\/wmemcmp is\n+    \/\/   implemented as: 24    1. Use ymm vector compares when possible. The\n+    \/\/   only case where 25       vector compares is not possible for when size\n+    \/\/   < VEC_SIZE 26       and loading from either s1 or s2 would cause a page\n+    \/\/   cross. 27    2. For size from 2 to 7 bytes on page cross, load as big\n+    \/\/   endian 28       with movbe and bswap to avoid branches. 29    3. Use\n+    \/\/   xmm vector compare when size >= 4 bytes for memcmp or 30       size >=\n+    \/\/   8 bytes for wmemcmp. 31    4. Optimistically compare up to first 4 *\n+    \/\/   VEC_SIZE one at a 32       to check for early mismatches. Only do this\n+    \/\/   if its guaranteed the 33       work is not wasted. 34    5. If size is\n+    \/\/   8 * VEC_SIZE or less, unroll the loop. 35    6. Compare 4 * VEC_SIZE at\n+    \/\/   a time with the aligned first memory 36       area. 37    7. Use 2\n+    \/\/   vector compares when size is 2 * VEC_SIZE or less. 38    8. Use 4\n+    \/\/   vector compares when size is 4 * VEC_SIZE or less. 39    9. Use 8\n+    \/\/   vector compares when size is 8 * VEC_SIZE or less.  *\/\n+\n+    Label L_less_vec, L_return_vec_0, L_last_1x_vec, L_return_vec_1,\n+        L_last_2x_vec;\n+    Label L_return_vec_2, L_retun_vec_3, L_more_8x_vec, L_return_vec_0_1_2_3;\n+    Label L_return_vzeroupper, L_8x_return_vec_0_1_2_3, L_loop_4x_vec;\n+    Label L_return_vec_3, L_8x_last_1x_vec, L_8x_last_2x_vec, L_8x_return_vec_2;\n+    Label L_8x_return_vec_3, L_return_vec_1_end, L_return_vec_0_end,\n+        L_one_or_less;\n+    Label L_page_cross_less_vec, L_between_16_31, L_between_8_15, L_between_2_3,\n+        L_zero;\n+    Label L_ret_nonzero;\n+\n+    \/\/ 72              .section SECTION(.text),\"ax\",@progbits\n+    \/\/ 73      ENTRY (MEMCMP)\n+    \/\/ 74      # ifdef USE_AS_WMEMCMP\n+    \/\/ 75              shl     $2, %RDX_LP\n+    \/\/ 76      # elif defined __ILP32__\n+    \/\/ 77              \/* Clear the upper 32 bits.  *\/\n+    \/\/ 78              movl    %edx, %edx\n+\n+    \/\/ 79      # endif\n+\n+    \/\/ 80              cmp     $VEC_SIZE, %RDX_LP\n+    \/\/ 81              jb      L(less_vec)\n+    \/\/ 82\n+    __ cmpq(rdx, 0x20);\n+    __ jb(L_less_vec);\n+    __ vmovdqu(xmm1, Address(rsi, 0));\n+\n+    \/\/ 83              \/* From VEC to 2 * VEC.  No branch when size == VEC_SIZE.\n+    \/\/ *\/\n+    __ vpcmpeqb(xmm1, xmm1, Address(rdi, 0), Assembler::AVX_256bit);\n+\n+    \/\/ 84              vmovdqu (%rsi), %ymm1\n+    __ vpmovmskb(rax, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 85              VPCMPEQ (%rdi), %ymm1, %ymm1\n+    \/\/ 86              vpmovmskb %ymm1, %eax\n+    \/\/ 87              \/* NB: eax must be destination register if going to\n+    \/\/ 88                 L(return_vec_[0,2]). For L(return_vec_3 destination\n+    \/\/ register 89                 must be ecx.  *\/\n+    __ incrementl(rax);\n+    __ jne(L_return_vec_0);\n+\n+    \/\/ 90              incl    %eax\n+    \/\/ 91              jnz     L(return_vec_0)\n+    __ cmpq(rdx, 0x40);\n+\n+    \/\/ 92\n+    __ jbe(L_last_1x_vec);\n+\n+    \/\/ 93              cmpq    $(VEC_SIZE * 2), %rdx\n+    \/\/ 94              jbe     L(last_1x_vec)\n+    \/\/ 95\n+    __ vmovdqu(xmm2, Address(rsi, 0x20));\n+\n+    \/\/ 96              \/* Check second VEC no matter what.  *\/\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdi, 0x20), Assembler::AVX_256bit);\n+\n+    \/\/ 97              vmovdqu VEC_SIZE(%rsi), %ymm2\n+    __ vpmovmskb(rax, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ 98              VPCMPEQ VEC_SIZE(%rdi), %ymm2, %ymm2\n+    \/\/ 99              vpmovmskb %ymm2, %eax\n+    \/\/ 100             \/* If all 4 VEC where equal eax will be all 1s so incl\n+    \/\/ will\n+    __ incrementl(rax);\n+\n+    \/\/ 101                overflow and set zero flag.  *\/\n+    __ jne(L_return_vec_1);\n+\n+    \/\/ 102             incl    %eax\n+    \/\/ 103             jnz     L(return_vec_1)\n+    \/\/ 104\n+    __ cmpq(rdx, 0x80);\n+\n+    \/\/ 105             \/* Less than 4 * VEC.  *\/\n+    __ jbe(L_last_2x_vec);\n+\n+    \/\/ 106             cmpq    $(VEC_SIZE * 4), %rdx\n+    \/\/ 107             jbe     L(last_2x_vec)\n+    \/\/ 108\n+    __ vmovdqu(xmm3, Address(rsi, 0x40));\n+\n+    \/\/ 109             \/* Check third and fourth VEC no matter what.  *\/\n+    __ vpcmpeqb(xmm3, xmm3, Address(rdi, 0x40), Assembler::AVX_256bit);\n+\n+    \/\/ 110             vmovdqu (VEC_SIZE * 2)(%rsi), %ymm3\n+    __ vpmovmskb(rax, xmm3, Assembler::AVX_256bit);\n+\n+    \/\/ 111             VPCMPEQ (VEC_SIZE * 2)(%rdi), %ymm3, %ymm3\n+    __ incrementl(rax);\n+\n+    \/\/ 112             vpmovmskb %ymm3, %eax\n+    __ jne(L_return_vec_2);\n+\n+    \/\/ 113             incl    %eax\n+    __ vmovdqu(xmm4, Address(rsi, 0x60));\n+\n+    \/\/ 114             jnz     L(return_vec_2)\n+    __ vpcmpeqb(xmm4, xmm4, Address(rdi, 0x60), Assembler::AVX_256bit);\n+\n+    \/\/ 115             vmovdqu (VEC_SIZE * 3)(%rsi), %ymm4\n+    __ vpmovmskb(rcx, xmm4, Assembler::AVX_256bit);\n+\n+    \/\/ 116             VPCMPEQ (VEC_SIZE * 3)(%rdi), %ymm4, %ymm4\n+    __ incrementl(rcx);\n+\n+    \/\/ 117             vpmovmskb %ymm4, %ecx\n+    __ jne(L_return_vec_3);\n+\n+    \/\/ 118             incl    %ecx\n+    \/\/ 119             jnz     L(return_vec_3)\n+    \/\/ 120\n+    __ cmpq(rdx, 0x100);\n+\n+    \/\/ 121             \/* Go to 4x VEC loop.  *\/\n+    __ ja(L_more_8x_vec);\n+\n+    \/\/ 122             cmpq    $(VEC_SIZE * 8), %rdx\n+    \/\/ 123             ja      L(more_8x_vec)\n+    \/\/ 124\n+    \/\/ 125             \/* Handle remainder of size = 4 * VEC + 1 to 8 * VEC\n+    \/\/ without any 126                branches.  *\/ 127\n+    __ vmovdqu(xmm1, Address(rsi, rdx, Address::times_1, -0x80));\n+\n+    \/\/ 128             \/* Load first two VEC from s2 before adjusting addresses.\n+    \/\/ *\/\n+    __ vmovdqu(xmm2, Address(rsi, rdx, Address::times_1, -0x60));\n+\n+    \/\/ 129             vmovdqu -(VEC_SIZE * 4)(%rsi, %rdx), %ymm1\n+    __ leaq(rdi, Address(rdi, rdx, Address::times_1, -0x80));\n+\n+    \/\/ 130             vmovdqu -(VEC_SIZE * 3)(%rsi, %rdx), %ymm2\n+    __ leaq(rsi, Address(rsi, rdx, Address::times_1, -0x80));\n+\n+    \/\/ 131             leaq    -(4 * VEC_SIZE)(%rdi, %rdx), %rdi\n+    \/\/ 132             leaq    -(4 * VEC_SIZE)(%rsi, %rdx), %rsi\n+    \/\/ 133\n+    \/\/ 134             \/* Wait to load from s1 until addressed adjust due to\n+    __ vpcmpeqb(xmm1, xmm1, Address(rdi, 0), Assembler::AVX_256bit);\n+\n+    \/\/ 135                unlamination of microfusion with complex address mode.\n+    \/\/ *\/\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdi, 0x20), Assembler::AVX_256bit);\n+\n+    \/\/ 136             VPCMPEQ (%rdi), %ymm1, %ymm1\n+    \/\/ 137             VPCMPEQ (VEC_SIZE)(%rdi), %ymm2, %ymm2\n+    __ vmovdqu(xmm3, Address(rsi, 0x40));\n+\n+    \/\/ 138\n+    __ vpcmpeqb(xmm3, xmm3, Address(rdi, 0x40), Assembler::AVX_256bit);\n+\n+    \/\/ 139             vmovdqu (VEC_SIZE * 2)(%rsi), %ymm3\n+    __ vmovdqu(xmm4, Address(rsi, 0x60));\n+\n+    \/\/ 140             VPCMPEQ (VEC_SIZE * 2)(%rdi), %ymm3, %ymm3\n+    __ vpcmpeqb(xmm4, xmm4, Address(rdi, 0x60), Assembler::AVX_256bit);\n+\n+    \/\/ 141             vmovdqu (VEC_SIZE * 3)(%rsi), %ymm4\n+    \/\/ 142             VPCMPEQ (VEC_SIZE * 3)(%rdi), %ymm4, %ymm4\n+    \/\/ 143\n+    __ vpand(xmm5, xmm2, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 144             \/* Reduce VEC0 - VEC4.  *\/\n+    __ vpand(xmm6, xmm4, xmm3, Assembler::AVX_256bit);\n+\n+    \/\/ 145             vpand   %ymm1, %ymm2, %ymm5\n+    __ vpand(xmm7, xmm6, xmm5, Assembler::AVX_256bit);\n+\n+    \/\/ 146             vpand   %ymm3, %ymm4, %ymm6\n+    __ vpmovmskb(rcx, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ 147             vpand   %ymm5, %ymm6, %ymm7\n+    __ incrementl(rcx);\n+\n+    \/\/ 148             vpmovmskb %ymm7, %ecx\n+    __ jne_b(L_return_vec_0_1_2_3);\n+\n+    \/\/ 149             incl    %ecx\n+    \/\/ 150             jnz     L(return_vec_0_1_2_3)\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_return_vec_0);\n+\n+    \/\/ 151             \/* NB: eax must be zero to reach here.  *\/\n+    \/\/ 152             VZEROUPPER_RETURN\n+    \/\/ 153\n+    \/\/ 154             .p2align 4\n+    __ tzcntl(rax, rax);\n+\n+    \/\/ 155     L(return_vec_0):\n+    \/\/ 156             tzcntl  %eax, %eax\n+    \/\/ 157     # ifdef USE_AS_WMEMCMP\n+    \/\/ 158             movl    (%rdi, %rax), %ecx\n+    \/\/ 159             xorl    %edx, %edx\n+    \/\/ 160             cmpl    (%rsi, %rax), %ecx\n+    \/\/ 161             \/* NB: no partial register stall here because xorl zero\n+    \/\/ idiom 162                above.  *\/ 163             setg    %dl 164 leal\n+    \/\/ -1(%rdx, %rdx), %eax\n+    __ movzbl(rcx, Address(rsi, rax, Address::times_1));\n+\n+    \/\/ 165     # else\n+    __ movzbl(rax, Address(rdi, rax, Address::times_1));\n+\n+    \/\/ 166             movzbl  (%rsi, %rax), %ecx\n+    __ subl(rax, rcx);\n+\n+    \/\/ 167             movzbl  (%rdi, %rax), %eax\n+    \/\/ 168             subl    %ecx, %eax\n+    \/\/ 169     # endif\n+    __ bind(L_return_vzeroupper);\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_return_vec_1);\n+\n+    \/\/ 170     L(return_vzeroupper):\n+    \/\/ 171             ZERO_UPPER_VEC_REGISTERS_RETURN\n+    \/\/ 172\n+    \/\/ 173             .p2align 4\n+    __ tzcntl(rax, rax);\n+\n+    \/\/ 174     L(return_vec_1):\n+    \/\/ 175             tzcntl  %eax, %eax\n+    \/\/ 176     # ifdef USE_AS_WMEMCMP\n+    \/\/ 177             movl    VEC_SIZE(%rdi, %rax), %ecx\n+    \/\/ 178             xorl    %edx, %edx\n+    \/\/ 179             cmpl    VEC_SIZE(%rsi, %rax), %ecx\n+    \/\/ 180             setg    %dl\n+    \/\/ 181             leal    -1(%rdx, %rdx), %eax\n+    __ movzbl(rcx, Address(rsi, rax, Address::times_1, 0x20));\n+\n+    \/\/ 182     # else\n+    __ movzbl(rax, Address(rdi, rax, Address::times_1, 0x20));\n+\n+    \/\/ 183             movzbl  VEC_SIZE(%rsi, %rax), %ecx\n+    __ subl(rax, rcx);\n+\n+    \/\/ 184             movzbl  VEC_SIZE(%rdi, %rax), %eax\n+    \/\/ 185             subl    %ecx, %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_return_vec_2);\n+\n+    \/\/ 186     # endif\n+    \/\/ 187             VZEROUPPER_RETURN\n+    \/\/ 188\n+    \/\/ 189             .p2align 4\n+    __ tzcntl(rax, rax);\n+\n+    \/\/ 190     L(return_vec_2):\n+    \/\/ 191             tzcntl  %eax, %eax\n+    \/\/ 192     # ifdef USE_AS_WMEMCMP\n+    \/\/ 193             movl    (VEC_SIZE * 2)(%rdi, %rax), %ecx\n+    \/\/ 194             xorl    %edx, %edx\n+    \/\/ 195             cmpl    (VEC_SIZE * 2)(%rsi, %rax), %ecx\n+    \/\/ 196             setg    %dl\n+    \/\/ 197             leal    -1(%rdx, %rdx), %eax\n+    __ movzbl(rcx, Address(rsi, rax, Address::times_1, 0x40));\n+\n+    \/\/ 198     # else\n+    __ movzbl(rax, Address(rdi, rax, Address::times_1, 0x40));\n+\n+    \/\/ 199             movzbl  (VEC_SIZE * 2)(%rsi, %rax), %ecx\n+    __ subl(rax, rcx);\n+\n+    \/\/ 200             movzbl  (VEC_SIZE * 2)(%rdi, %rax), %eax\n+    \/\/ 201             subl    %ecx, %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(32);\n+\n+    __ bind(L_8x_return_vec_0_1_2_3);\n+\n+    \/\/ 202     # endif\n+    \/\/ 203             VZEROUPPER_RETURN\n+    \/\/ 204\n+    \/\/ 205             \/* NB: p2align 5 here to ensure 4x loop is 32 byte\n+    \/\/ aligned.  *\/ 206             .p2align 5 207     L(8x_return_vec_0_1_2_3):\n+    __ addq(rsi, rdi);\n+\n+    \/\/ 208             \/* Returning from L(more_8x_vec) requires restoring rsi.\n+    \/\/ *\/ 209             addq    %rdi, %rsi\n+    __ bind(L_return_vec_0_1_2_3);\n+    __ vpmovmskb(rax, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 210     L(return_vec_0_1_2_3):\n+    __ incrementl(rax);\n+\n+    \/\/ 211             vpmovmskb %ymm1, %eax\n+    __ jne_b(L_return_vec_0);\n+\n+    \/\/ 212             incl    %eax\n+    \/\/ 213             jnz     L(return_vec_0)\n+    __ vpmovmskb(rax, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ 214\n+    __ incrementl(rax);\n+\n+    \/\/ 215             vpmovmskb %ymm2, %eax\n+    __ jne_b(L_return_vec_1);\n+\n+    \/\/ 216             incl    %eax\n+    \/\/ 217             jnz     L(return_vec_1)\n+    __ vpmovmskb(rax, xmm3, Assembler::AVX_256bit);\n+\n+    \/\/ 218\n+    __ incrementl(rax);\n+\n+    \/\/ 219             vpmovmskb %ymm3, %eax\n+    __ jne_b(L_return_vec_2);\n+\n+    \/\/ 220             incl    %eax\n+    \/\/ 221             jnz     L(return_vec_2)\n+    __ bind(L_return_vec_3);\n+    __ tzcntl(rcx, rcx);\n+\n+    \/\/ 222     L(return_vec_3):\n+    \/\/ 223             tzcntl  %ecx, %ecx\n+    \/\/ 224     # ifdef USE_AS_WMEMCMP\n+    \/\/ 225             movl    (VEC_SIZE * 3)(%rdi, %rcx), %eax\n+    \/\/ 226             xorl    %edx, %edx\n+    \/\/ 227             cmpl    (VEC_SIZE * 3)(%rsi, %rcx), %eax\n+    \/\/ 228             setg    %dl\n+    \/\/ 229             leal    -1(%rdx, %rdx), %eax\n+    __ movzbl(rax, Address(rdi, rcx, Address::times_1, 0x60));\n+\n+    \/\/ 230     # else\n+    __ movzbl(rcx, Address(rsi, rcx, Address::times_1, 0x60));\n+\n+    \/\/ 231             movzbl  (VEC_SIZE * 3)(%rdi, %rcx), %eax\n+    __ subl(rax, rcx);\n+\n+    \/\/ 232             movzbl  (VEC_SIZE * 3)(%rsi, %rcx), %ecx\n+    \/\/ 233             subl    %ecx, %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_more_8x_vec);\n+\n+    \/\/ 234     # endif\n+    \/\/ 235             VZEROUPPER_RETURN\n+    \/\/ 236\n+    \/\/ 237             .p2align 4\n+    \/\/ 238     L(more_8x_vec):\n+    __ leaq(rdx, Address(rdi, rdx, Address::times_1, -0x80));\n+\n+    \/\/ 239             \/* Set end of s1 in rdx.  *\/\n+    \/\/ 240             leaq    -(VEC_SIZE * 4)(%rdi, %rdx), %rdx\n+    \/\/ 241             \/* rsi stores s2 - s1. This allows loop to only update\n+    \/\/ one\n+    __ subq(rsi, rdi);\n+\n+    \/\/ 242                pointer.  *\/\n+    \/\/ 243             subq    %rdi, %rsi\n+    __ andq(rdi, -32);\n+\n+    \/\/ 244             \/* Align s1 pointer.  *\/\n+    \/\/ 245             andq    $-VEC_SIZE, %rdi\n+    __ subq(rdi, -128);\n+    __ align(16);\n+    __ bind(L_loop_4x_vec);\n+\n+    \/\/ 246             \/* Adjust because first 4x vec where check already.  *\/\n+    \/\/ 247             subq    $-(VEC_SIZE * 4), %rdi\n+    \/\/ 248             .p2align 4\n+    \/\/ 249     L(loop_4x_vec):\n+    \/\/ 250             \/* rsi has s2 - s1 so get correct address by adding s1\n+    \/\/ (in rdi).\n+    __ vmovdqu(xmm1, Address(rsi, rdi, Address::times_1));\n+\n+    \/\/ 251              *\/\n+    __ vpcmpeqb(xmm1, xmm1, Address(rdi, 0), Assembler::AVX_256bit);\n+\n+    \/\/ 252             vmovdqu (%rsi, %rdi), %ymm1\n+    \/\/ 253             VPCMPEQ (%rdi), %ymm1, %ymm1\n+    __ vmovdqu(xmm2, Address(rsi, rdi, Address::times_1, 0x20));\n+\n+    \/\/ 254\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdi, 0x20), Assembler::AVX_256bit);\n+\n+    \/\/ 255             vmovdqu VEC_SIZE(%rsi, %rdi), %ymm2\n+    \/\/ 256             VPCMPEQ VEC_SIZE(%rdi), %ymm2, %ymm2\n+    __ vmovdqu(xmm3, Address(rsi, rdi, Address::times_1, 0x40));\n+\n+    \/\/ 257\n+    __ vpcmpeqb(xmm3, xmm3, Address(rdi, 0x40), Assembler::AVX_256bit);\n+\n+    \/\/ 258             vmovdqu (VEC_SIZE * 2)(%rsi, %rdi), %ymm3\n+    \/\/ 259             VPCMPEQ (VEC_SIZE * 2)(%rdi), %ymm3, %ymm3\n+    __ vmovdqu(xmm4, Address(rsi, rdi, Address::times_1, 0x60));\n+\n+    \/\/ 260\n+    __ vpcmpeqb(xmm4, xmm4, Address(rdi, 0x60), Assembler::AVX_256bit);\n+\n+    \/\/ 261             vmovdqu (VEC_SIZE * 3)(%rsi, %rdi), %ymm4\n+    \/\/ 262             VPCMPEQ (VEC_SIZE * 3)(%rdi), %ymm4, %ymm4\n+    __ vpand(xmm5, xmm2, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 263\n+    __ vpand(xmm6, xmm4, xmm3, Assembler::AVX_256bit);\n+\n+    \/\/ 264             vpand   %ymm1, %ymm2, %ymm5\n+    __ vpand(xmm7, xmm6, xmm5, Assembler::AVX_256bit);\n+\n+    \/\/ 265             vpand   %ymm3, %ymm4, %ymm6\n+    __ vpmovmskb(rcx, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ 266             vpand   %ymm5, %ymm6, %ymm7\n+    __ incrementl(rcx);\n+\n+    \/\/ 267             vpmovmskb %ymm7, %ecx\n+    __ jne_b(L_8x_return_vec_0_1_2_3);\n+\n+    \/\/ 268             incl    %ecx\n+    __ subq(rdi, -128);\n+\n+    \/\/ 269             jnz     L(8x_return_vec_0_1_2_3)\n+    \/\/ 270             subq    $-(VEC_SIZE * 4), %rdi\n+    __ cmpq(rdi, rdx);\n+\n+    \/\/ 271             \/* Check if s1 pointer at end.  *\/\n+    __ jb_b(L_loop_4x_vec);\n+\n+    \/\/ 272             cmpq    %rdx, %rdi\n+    \/\/ 273             jb      L(loop_4x_vec)\n+    __ subq(rdi, rdx);\n+\n+    \/\/ 274\n+    \/\/ 275             subq    %rdx, %rdi\n+    __ cmpl(rdi, 0x60);\n+\n+    \/\/ 276             \/* rdi has 4 * VEC_SIZE - remaining length.  *\/\n+    __ jae_b(L_8x_last_1x_vec);\n+\n+    \/\/ 277             cmpl    $(VEC_SIZE * 3), %edi\n+    \/\/ 278             jae     L(8x_last_1x_vec)\n+    __ vmovdqu(xmm3, Address(rsi, rdx, Address::times_1, 0x40));\n+\n+    \/\/ 279             \/* Load regardless of branch.  *\/\n+    __ cmpl(rdi, 0x40);\n+\n+    \/\/ 280             vmovdqu (VEC_SIZE * 2)(%rsi, %rdx), %ymm3\n+    __ jae_b(L_8x_last_2x_vec);\n+\n+    \/\/ 281             cmpl    $(VEC_SIZE * 2), %edi\n+    \/\/ 282             jae     L(8x_last_2x_vec)\n+    \/\/ 283\n+    __ vmovdqu(xmm1, Address(rsi, rdx, Address::times_1));\n+\n+    \/\/ 284             \/* Check last 4 VEC.  *\/\n+    __ vpcmpeqb(xmm1, xmm1, Address(rdx, 0), Assembler::AVX_256bit);\n+\n+    \/\/ 285             vmovdqu (%rsi, %rdx), %ymm1\n+    \/\/ 286             VPCMPEQ (%rdx), %ymm1, %ymm1\n+    __ vmovdqu(xmm2, Address(rsi, rdx, Address::times_1, 0x20));\n+\n+    \/\/ 287\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdx, 0x20), Assembler::AVX_256bit);\n+\n+    \/\/ 288             vmovdqu VEC_SIZE(%rsi, %rdx), %ymm2\n+    \/\/ 289             VPCMPEQ VEC_SIZE(%rdx), %ymm2, %ymm2\n+    __ vpcmpeqb(xmm3, xmm3, Address(rdx, 0x40), Assembler::AVX_256bit);\n+\n+    \/\/ 290\n+    \/\/ 291             VPCMPEQ (VEC_SIZE * 2)(%rdx), %ymm3, %ymm3\n+    __ vmovdqu(xmm4, Address(rsi, rdx, Address::times_1, 0x60));\n+\n+    \/\/ 292\n+    __ vpcmpeqb(xmm4, xmm4, Address(rdx, 0x60), Assembler::AVX_256bit);\n+\n+    \/\/ 293             vmovdqu (VEC_SIZE * 3)(%rsi, %rdx), %ymm4\n+    \/\/ 294             VPCMPEQ (VEC_SIZE * 3)(%rdx), %ymm4, %ymm4\n+    __ vpand(xmm5, xmm2, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 295\n+    __ vpand(xmm6, xmm4, xmm3, Assembler::AVX_256bit);\n+\n+    \/\/ 296             vpand   %ymm1, %ymm2, %ymm5\n+    __ vpand(xmm7, xmm6, xmm5, Assembler::AVX_256bit);\n+\n+    \/\/ 297             vpand   %ymm3, %ymm4, %ymm6\n+    __ vpmovmskb(rcx, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ 298             vpand   %ymm5, %ymm6, %ymm7\n+    \/\/ 299             vpmovmskb %ymm7, %ecx\n+    __ movq(rdi, rdx);\n+\n+    \/\/ 300             \/* Restore s1 pointer to rdi.  *\/\n+    __ incrementl(rcx);\n+\n+    \/\/ 301             movq    %rdx, %rdi\n+    __ jne(L_8x_return_vec_0_1_2_3);\n+\n+    \/\/ 302             incl    %ecx\n+    \/\/ 303             jnz     L(8x_return_vec_0_1_2_3)\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_8x_last_2x_vec);\n+\n+    \/\/ 304             \/* NB: eax must be zero to reach here.  *\/\n+    \/\/ 305             VZEROUPPER_RETURN\n+    \/\/ 306\n+    \/\/ 307             \/* Only entry is from L(more_8x_vec).  *\/\n+    \/\/ 308             .p2align 4\n+    \/\/ 309     L(8x_last_2x_vec):\n+    \/\/ 310             \/* Check second to last VEC. rdx store end pointer of s1\n+    \/\/ and 311                ymm3 has already been loaded with second to last\n+    \/\/ VEC from s2.\n+    __ vpcmpeqb(xmm3, xmm3, Address(rdx, 0x40), Assembler::AVX_256bit);\n+\n+    \/\/ 312              *\/\n+    __ vpmovmskb(rax, xmm3, Assembler::AVX_256bit);\n+\n+    \/\/ 313             VPCMPEQ (VEC_SIZE * 2)(%rdx), %ymm3, %ymm3\n+    __ incrementl(rax);\n+\n+    \/\/ 314             vpmovmskb %ymm3, %eax\n+    __ jne_b(L_8x_return_vec_2);\n+    __ align(16);\n+\n+    __ bind(L_8x_last_1x_vec);\n+\n+    \/\/ 315             incl    %eax\n+    \/\/ 316             jnz     L(8x_return_vec_2)\n+    \/\/ 317             \/* Check last VEC.  *\/\n+    \/\/ 318             .p2align 4\n+    __ vmovdqu(xmm4, Address(rsi, rdx, Address::times_1, 0x60));\n+\n+    \/\/ 319     L(8x_last_1x_vec):\n+    __ vpcmpeqb(xmm4, xmm4, Address(rdx, 0x60), Assembler::AVX_256bit);\n+\n+    \/\/ 320             vmovdqu (VEC_SIZE * 3)(%rsi, %rdx), %ymm4\n+    __ vpmovmskb(rax, xmm4, Assembler::AVX_256bit);\n+\n+    \/\/ 321             VPCMPEQ (VEC_SIZE * 3)(%rdx), %ymm4, %ymm4\n+    __ incrementl(rax);\n+\n+    \/\/ 322             vpmovmskb %ymm4, %eax\n+    __ jne_b(L_8x_return_vec_3);\n+\n+    \/\/ 323             incl    %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_last_2x_vec);\n+\n+    \/\/ 324             jnz     L(8x_return_vec_3)\n+    \/\/ 325             VZEROUPPER_RETURN\n+    \/\/ 326\n+    \/\/ 327             .p2align 4\n+    \/\/ 328     L(last_2x_vec):\n+    __ vmovdqu(xmm1, Address(rsi, rdx, Address::times_1, -0x40));\n+\n+    \/\/ 329             \/* Check second to last VEC.  *\/\n+    __ vpcmpeqb(xmm1, xmm1, Address(rdi, rdx, Address::times_1, -0x40),\n+                Assembler::AVX_256bit);\n+\n+    \/\/ 330             vmovdqu -(VEC_SIZE * 2)(%rsi, %rdx), %ymm1\n+    __ vpmovmskb(rax, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 331             VPCMPEQ -(VEC_SIZE * 2)(%rdi, %rdx), %ymm1, %ymm1\n+    __ incrementl(rax);\n+\n+    \/\/ 332             vpmovmskb %ymm1, %eax\n+    __ jne_b(L_return_vec_1_end);\n+\n+    __ bind(L_last_1x_vec);\n+\n+    \/\/ 333             incl    %eax\n+    \/\/ 334             jnz     L(return_vec_1_end)\n+    \/\/ 335             \/* Check last VEC.  *\/\n+    __ vmovdqu(xmm1, Address(rsi, rdx, Address::times_1, -0x20));\n+\n+    \/\/ 336     L(last_1x_vec):\n+    __ vpcmpeqb(xmm1, xmm1, Address(rdi, rdx, Address::times_1, -0x20),\n+                Assembler::AVX_256bit);\n+\n+    \/\/ 337             vmovdqu -(VEC_SIZE * 1)(%rsi, %rdx), %ymm1\n+    __ vpmovmskb(rax, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ 338             VPCMPEQ -(VEC_SIZE * 1)(%rdi, %rdx), %ymm1, %ymm1\n+    __ incrementl(rax);\n+\n+    \/\/ 339             vpmovmskb %ymm1, %eax\n+    __ jne_b(L_return_vec_0_end);\n+\n+    \/\/ 340             incl    %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_8x_return_vec_2);\n+\n+    \/\/ 341             jnz     L(return_vec_0_end)\n+    \/\/ 342             VZEROUPPER_RETURN\n+    \/\/ 343\n+    \/\/ 344             .p2align 4\n+    __ subq(rdx, 0x20);\n+    __ bind(L_8x_return_vec_3);\n+\n+    \/\/ 345     L(8x_return_vec_2):\n+    \/\/ 346             subq    $VEC_SIZE, %rdx\n+    __ tzcntl(rax, rax);\n+\n+    \/\/ 347     L(8x_return_vec_3):\n+    __ addq(rax, rdx);\n+\n+    \/\/ 348             tzcntl  %eax, %eax\n+    \/\/ 349             addq    %rdx, %rax\n+    \/\/ 350     # ifdef USE_AS_WMEMCMP\n+    \/\/ 351             movl    (VEC_SIZE * 3)(%rax), %ecx\n+    \/\/ 352             xorl    %edx, %edx\n+    \/\/ 353             cmpl    (VEC_SIZE * 3)(%rsi, %rax), %ecx\n+    \/\/ 354             setg    %dl\n+    \/\/ 355             leal    -1(%rdx, %rdx), %eax\n+    __ movzbl(rcx, Address(rsi, rax, Address::times_1, 0x60));\n+\n+    \/\/ 356     # else\n+    __ movzbl(rax, Address(rax, 0x60));\n+\n+    \/\/ 357             movzbl  (VEC_SIZE * 3)(%rsi, %rax), %ecx\n+    __ subl(rax, rcx);\n+\n+    \/\/ 358             movzbl  (VEC_SIZE * 3)(%rax), %eax\n+    \/\/ 359             subl    %ecx, %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_return_vec_1_end);\n+\n+    \/\/ 360     # endif\n+    \/\/ 361             VZEROUPPER_RETURN\n+    \/\/ 362\n+    \/\/ 363             .p2align 4\n+    __ tzcntl(rax, rax);\n+\n+    \/\/ 364     L(return_vec_1_end):\n+    __ addl(rax, rdx);\n+\n+    \/\/ 365             tzcntl  %eax, %eax\n+    \/\/ 366             addl    %edx, %eax\n+    \/\/ 367     # ifdef USE_AS_WMEMCMP\n+    \/\/ 368             movl    -(VEC_SIZE * 2)(%rdi, %rax), %ecx\n+    \/\/ 369             xorl    %edx, %edx\n+    \/\/ 370             cmpl    -(VEC_SIZE * 2)(%rsi, %rax), %ecx\n+    \/\/ 371             setg    %dl\n+    \/\/ 372             leal    -1(%rdx, %rdx), %eax\n+    __ movzbl(rcx, Address(rsi, rax, Address::times_1, -0x40));\n+\n+    \/\/ 373     # else\n+    __ movzbl(rax, Address(rdi, rax, Address::times_1, -0x40));\n+\n+    \/\/ 374             movzbl  -(VEC_SIZE * 2)(%rsi, %rax), %ecx\n+    __ subl(rax, rcx);\n+\n+    \/\/ 375             movzbl  -(VEC_SIZE * 2)(%rdi, %rax), %eax\n+    \/\/ 376             subl    %ecx, %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_return_vec_0_end);\n+\n+    \/\/ 377     # endif\n+    \/\/ 378             VZEROUPPER_RETURN\n+    \/\/ 379\n+    \/\/ 380             .p2align 4\n+    __ tzcntl(rax, rax);\n+\n+    \/\/ 381     L(return_vec_0_end):\n+    __ addl(rax, rdx);\n+\n+    \/\/ 382             tzcntl  %eax, %eax\n+    \/\/ 383             addl    %edx, %eax\n+    \/\/ 384     # ifdef USE_AS_WMEMCMP\n+    \/\/ 385             movl    -VEC_SIZE(%rdi, %rax), %ecx\n+    \/\/ 386             xorl    %edx, %edx\n+    \/\/ 387             cmpl    -VEC_SIZE(%rsi, %rax), %ecx\n+    \/\/ 388             setg    %dl\n+    \/\/ 389             leal    -1(%rdx, %rdx), %eax\n+    __ movzbl(rcx, Address(rsi, rax, Address::times_1, -0x20));\n+\n+    \/\/ 390     # else\n+    __ movzbl(rax, Address(rdi, rax, Address::times_1, -0x20));\n+\n+    \/\/ 391             movzbl  -VEC_SIZE(%rsi, %rax), %ecx\n+    __ subl(rax, rcx);\n+\n+    \/\/ 392             movzbl  -VEC_SIZE(%rdi, %rax), %eax\n+    \/\/ 393             subl    %ecx, %eax\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_less_vec);\n+\n+    \/\/ 394     # endif\n+    \/\/ 395             VZEROUPPER_RETURN\n+    \/\/ 396\n+    \/\/ 397             .p2align 4\n+    \/\/ 398     L(less_vec):\n+    \/\/ 399             \/* Check if one or less CHAR. This is necessary for size\n+    \/\/ = 0 but\n+    __ cmpl(rdx, 0x1);\n+\n+    \/\/ 400                is also faster for size = CHAR_SIZE.  *\/\n+    __ jbe_b(L_one_or_less);\n+\n+    \/\/ 401             cmpl    $CHAR_SIZE, %edx\n+    \/\/ 402             jbe     L(one_or_less)\n+    \/\/ 403\n+    \/\/ 404             \/* Check if loading one VEC from either s1 or s2 could\n+    \/\/ cause a 405                page cross. This can have false positives but\n+    \/\/ is by far the\n+    __ movl(rax, rdi);\n+\n+    \/\/ 406                fastest method.  *\/\n+    __ orl(rax, rsi);\n+\n+    \/\/ 407             movl    %edi, %eax\n+    __ andl(rax, 0xfff);\n+\n+    \/\/ 408             orl     %esi, %eax\n+    __ cmpl(rax, 0xfe0);\n+\n+    \/\/ 409             andl    $(PAGE_SIZE - 1), %eax\n+    __ jg_b(L_page_cross_less_vec);\n+\n+    \/\/ 410             cmpl    $(PAGE_SIZE - VEC_SIZE), %eax\n+    \/\/ 411             jg      L(page_cross_less_vec)\n+    \/\/ 412\n+    __ vmovdqu(xmm2, Address(rsi, 0));\n+\n+    \/\/ 413             \/* No page cross possible.  *\/\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdi, 0), Assembler::AVX_256bit);\n+\n+    \/\/ 414             vmovdqu (%rsi), %ymm2\n+    __ vpmovmskb(rax, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ 415             VPCMPEQ (%rdi), %ymm2, %ymm2\n+    __ incrementl(rax);\n+\n+    \/\/ 416             vpmovmskb %ymm2, %eax\n+    \/\/ 417             incl    %eax\n+    \/\/ 418             \/* Result will be zero if s1 and s2 match. Otherwise\n+    \/\/ first set\n+    __ bzhil(rdx, rax, rdx);\n+\n+    \/\/ 419                bit will be first mismatch.  *\/\n+    __ jne(L_return_vec_0);\n+\n+    \/\/ 420             bzhil   %edx, %eax, %edx\n+    __ xorl(rax, rax);\n+\n+    \/\/ 421             jnz     L(return_vec_0)\n+    __ vzeroupper();\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_page_cross_less_vec);\n+\n+    \/\/ 422             xorl    %eax, %eax\n+    \/\/ 423             VZEROUPPER_RETURN\n+    \/\/ 424\n+    \/\/ 425             .p2align 4\n+    \/\/ 426     L(page_cross_less_vec):\n+    \/\/ 427             \/* if USE_AS_WMEMCMP it can only be 0, 4, 8, 12, 16, 20,\n+    \/\/ 24, 28\n+    __ cmpl(rdx, 0x10);\n+\n+    \/\/ 428                bytes.  *\/\n+    __ jae(L_between_16_31);\n+\n+    \/\/ 429             cmpl    $16, %edx\n+    \/\/ 430             jae     L(between_16_31)\n+    __ cmpl(rdx, 0x8);\n+\n+    \/\/ 431     # ifndef USE_AS_WMEMCMP\n+    __ jae_b(L_between_8_15);\n+\n+    \/\/ 432             cmpl    $8, %edx\n+    __ cmpl(rdx, 0x4);\n+\n+    \/\/ 433             jae     L(between_8_15)\n+    __ jae(L_between_2_3);\n+\n+    \/\/ 434             \/* Fall through for [4, 7].  *\/\n+    \/\/ 435             cmpl    $4, %edx\n+    \/\/ 436             jb      L(between_2_3)\n+    __ movzbl(rax, Address(rdi, 0));\n+\n+    \/\/ 437\n+    __ movzbl(rcx, Address(rsi, 0));\n+\n+    \/\/ 438             movbe   (%rdi), %eax\n+    \/\/ 439             movbe   (%rsi), %ecx\n+\n+    __ shlq(rax, 0x20);\n+    \/\/   shlq  $32, %rcx\n+    __ shlq(rcx, 0x20);\n+    \/\/   movbe  -4(%rdi, %rdx), %edi\n+    __ movzbl(rdi, Address(rdi, rdx, Address::times_1, -0x4));\n+    \/\/   movbe  -4(%rsi, %rdx), %esi\n+    __ movzbl(rsi, Address(rsi, rdx, Address::times_1, -0x4));\n+    \/\/   orq  %rdi, %rax\n+    __ orq(rax, rdi);\n+    \/\/   orq  %rsi, %rcx\n+    __ orq(rcx, rsi);\n+    \/\/   subq  %rcx, %rax\n+    __ subq(rax, rcx);\n+    \/\/   \/* Fast path for return zero.  *\/\n+    \/\/   jnz  L(ret_nonzero)\n+    __ jne_b(L_ret_nonzero);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_one_or_less);\n+\n+    \/\/   .p2align 4\n+    \/\/ L(one_or_less):\n+    \/\/   jb  L(zero)\n+    __ jb_b(L_zero);\n+    \/\/   movzbl  (%rsi), %ecx\n+    __ movzbl(rcx, Address(rsi, 0));\n+    \/\/   movzbl  (%rdi), %eax\n+    __ movzbl(rax, Address(rdi, 0));\n+    \/\/   subl  %ecx, %eax\n+    __ subl(rax, rcx);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    __ ret(0);\n+    __ p2align(16, 5);\n+\n+    __ bind(L_ret_nonzero);\n+\n+    \/\/   .p2align 4,, 5\n+    \/\/ L(ret_nonzero):\n+    \/\/   sbbl  %eax, %eax\n+    __ sbbl(rax, rax);\n+    \/\/   orl  $1, %eax\n+    __ orl(rax, 0x1);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    __ ret(0);\n+    __ p2align(16, 2);\n+\n+    __ bind(L_zero);\n+\n+    \/\/   .p2align 4,, 2\n+    \/\/ L(zero):\n+    \/\/   xorl  %eax, %eax\n+    __ xorl(rax, rax);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_between_8_15);\n+\n+    \/\/   .p2align 4\n+    \/\/ L(between_8_15):\n+    \/\/   movbe  (%rdi), %rax\n+    __ movzbl(rax, Address(rdi, 0));\n+    \/\/   movbe  (%rsi), %rcx\n+    __ movzbl(rcx, Address(rsi, 0));\n+    \/\/   subq  %rcx, %rax\n+    __ subq(rax, rcx);\n+    \/\/   jnz  L(ret_nonzero)\n+    __ jne_b(L_ret_nonzero);\n+    \/\/   movbe  -8(%rdi, %rdx), %rax\n+    __ movzbl(rax, Address(rdi, rdx, Address::times_1, -0x8));\n+    \/\/   movbe  -8(%rsi, %rdx), %rcx\n+    __ movzbl(rcx, Address(rsi, rdx, Address::times_1, -0x8));\n+    \/\/   subq  %rcx, %rax\n+    __ subq(rax, rcx);\n+    \/\/   \/* Fast path for return zero.  *\/\n+    \/\/   jnz  L(ret_nonzero)\n+    __ jne_b(L_ret_nonzero);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    \/\/ # endif\n+    __ ret(0);\n+    __ p2align(16, 10);\n+\n+    __ bind(L_between_16_31);\n+\n+    \/\/   .p2align 4,, 10\n+    \/\/ L(between_16_31):\n+    \/\/   \/* From 16 to 31 bytes.  No branch when size == 16.  *\/\n+    \/\/   vmovdqu  (%rsi), %xmm2\n+    __ movdqu(xmm2, Address(rsi, 0));\n+    \/\/   VPCMPEQ  (%rdi), %xmm2, %xmm2\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdi, 0), Assembler::AVX_128bit);\n+    \/\/   vpmovmskb %xmm2, %eax\n+    __ vpmovmskb(rax, xmm2, Assembler::AVX_128bit);\n+    \/\/   subl  $0xffff, %eax\n+    __ subl(rax, 0xffff);\n+    \/\/   jnz  L(return_vec_0)\n+    __ jne(L_return_vec_0);\n+\n+    \/\/   \/* Use overlapping loads to avoid branches.  *\/\n+\n+    \/\/   vmovdqu  -16(%rsi, %rdx), %xmm2\n+    __ movdqu(xmm2, Address(rsi, rdx, Address::times_1, -0x10));\n+    \/\/   leaq  -16(%rdi, %rdx), %rdi\n+    __ leaq(rdi, Address(rdi, rdx, Address::times_1, -0x10));\n+    \/\/   leaq  -16(%rsi, %rdx), %rsi\n+    __ leaq(rsi, Address(rsi, rdx, Address::times_1, -0x10));\n+    \/\/   VPCMPEQ  (%rdi), %xmm2, %xmm2\n+    __ vpcmpeqb(xmm2, xmm2, Address(rdi, 0), Assembler::AVX_128bit);\n+    \/\/   vpmovmskb %xmm2, %eax\n+    __ vpmovmskb(rax, xmm2, Assembler::AVX_128bit);\n+    \/\/   subl  $0xffff, %eax\n+    __ subl(rax, 0xffff);\n+    \/\/   \/* Fast path for return zero.  *\/\n+    \/\/   jnz  L(return_vec_0)\n+    __ jne(L_return_vec_0);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    \/\/ # else\n+    __ ret(0);\n+    __ align(16);\n+\n+    __ bind(L_between_2_3);\n+\n+    \/\/   .p2align 4\n+    \/\/ L(between_2_3):\n+    \/\/   \/* Load as big endian to avoid branches.  *\/\n+    \/\/   movzwl  (%rdi), %eax\n+    __ movzwl(rax, Address(rdi, 0));\n+    \/\/   movzwl  (%rsi), %ecx\n+    __ movzwl(rcx, Address(rsi, 0));\n+    \/\/   bswap  %eax\n+    __ bswapl(rax);\n+    \/\/   bswap  %ecx\n+    __ bswapl(rcx);\n+    \/\/   shrl  %eax\n+    __ shrl(rax, 1);\n+    \/\/   shrl  %ecx\n+    __ shrl(rcx, 1);\n+    \/\/   movzbl  -1(%rdi, %rdx), %edi\n+    __ movzbl(rdi, Address(rdi, rdx, Address::times_1, -0x1));\n+    \/\/   movzbl  -1(%rsi, %rdx), %esi\n+    __ movzbl(rsi, Address(rsi, rdx, Address::times_1, -0x1));\n+    \/\/   orl  %edi, %eax\n+    __ orl(rax, rdi);\n+    \/\/   orl  %esi, %ecx\n+    __ orl(rcx, rsi);\n+    \/\/   \/* Subtraction is okay because the upper bit is zero.  *\/\n+    \/\/   subl  %ecx, %eax\n+    __ subl(rax, rcx);\n+    \/\/   \/* No ymm register was touched.  *\/\n+    \/\/   ret\n+    __ ret(0);\n+\n+  } else {  \/\/ SSE version\n+    assert(false, \"Only supports AVX2\");\n+  }\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_string.cpp","additions":2585,"deletions":0,"binary":false,"changes":2585,"status":"added"},{"patch":"@@ -1608,0 +1608,1 @@\n+                  strcmp(call->as_CallLeaf()->_name, \"stringIndexOf\") == 0 ||\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1201,0 +1201,2 @@\n+  Node* result = nullptr;\n+\n@@ -1210,1 +1212,10 @@\n-  Node* result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, result_rgn, result_phi, ae);\n+    \/\/ ASGASG if AVX2 && ptr != null and LL, make runtime call - like base64\n+  if ((StubRoutines::string_indexof() != nullptr) && (ae == StrIntrinsicNode::LL)) {\n+    Node* call = make_runtime_call(RC_LEAF,\n+                                   OptoRuntime::string_IndexOf_Type(),\n+                                   StubRoutines::string_indexof(), \"stringIndexOf\", TypePtr::BOTTOM,\n+                                   src_start, src_count, tgt_start, tgt_count);\n+    result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  } else {\n+    result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, result_rgn, result_phi, ae);\n+  }\n@@ -1255,0 +1266,1 @@\n+  Node* result = nullptr;\n@@ -1256,1 +1268,10 @@\n-  Node* result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, region, phi, ae);\n+    \/\/ ASGASG if AVX2 && ptr != null and LL, make runtime call - like base64\n+  if ((StubRoutines::string_indexof() != nullptr) && (ae == StrIntrinsicNode::LL)) {\n+    Node* call = make_runtime_call(RC_LEAF,\n+                                   OptoRuntime::string_IndexOf_Type(),\n+                                   StubRoutines::string_indexof(), \"stringIndexOf\", TypePtr::BOTTOM,\n+                                   src_start, src_count, tgt_start, tgt_count);\n+    result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  } else {\n+    result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, region, phi, ae);\n+  }\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1337,0 +1337,21 @@\n+\n+\/\/ String IndexOf function\n+const TypeFunc* OptoRuntime::string_IndexOf_Type() {\n+  int argcnt = 4;\n+\n+  const Type** fields = TypeTuple::fields(argcnt);\n+  int argp = TypeFunc::Parms;\n+  fields[argp++] = TypePtr::NOTNULL;    \/\/ needle array\n+  fields[argp++] = TypeInt::INT;        \/\/ needle length\n+  fields[argp++] = TypePtr::NOTNULL;    \/\/ haystack array\n+  fields[argp++] = TypeInt::INT;        \/\/ haystack length\n+  assert(argp == TypeFunc::Parms + argcnt, \"correct decoding\");\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+argcnt, fields);\n+\n+  \/\/ result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms + 0] = TypeInt::INT; \/\/ Index of needle in haystack\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n+  return TypeFunc::make(domain, range);\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -298,0 +298,1 @@\n+  static const TypeFunc* string_IndexOf_Type();\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -147,0 +147,2 @@\n+address StubRoutines::_string_indexof =    nullptr;\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -230,0 +230,2 @@\n+  static address _string_indexof;\n+\n@@ -420,0 +422,2 @@\n+  static address string_indexof()  { return _string_indexof; }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,2 +34,2 @@\n-    static Random generator = new Random();\n-    private static boolean failure = false;\n+  static Random generator = new Random(1999);\n+  private static boolean failure = false;\n@@ -37,4 +37,7 @@\n-    public static void main(String[] args) throws Exception {\n-        simpleTest();\n-        compareIndexOfLastIndexOf();\n-        compareStringStringBuffer();\n+  public static void main(String[] args) throws Exception {\n+    String testName = \"IndexOf\";\n+    if (false) {\n+    String xx = \"0#.!,))\\\"7-0#:02\/62;+-\\\"\\\"0$25-5$#)1263'.&&(127+'*$%\\\"1+9,45'-\/&,0;97*\/, ,$':'8+#3%5:6+#  '3-:.!\";\n+    String yy = \"0#:02\/62;+-\\\"\\\"0$25-5$#)1263\";\n+    int gg = xx.indexOf(yy, 50);\n+    System.err.println(gg);\n@@ -42,3 +45,1 @@\n-        if (failure)\n-           throw new RuntimeException(\"One or more BitSet failures.\");\n-    }\n+    } else {\n@@ -46,5 +47,2 @@\n-    private static void report(String testName, int failCount) {\n-        System.err.println(testName+\": \" +\n-                         (failCount==0 ? \"Passed\":\"Failed(\"+failCount+\")\"));\n-        if (failCount > 0)\n-            failure = true;\n+    for (int i = 0; i < 20000; i++) {\n+      int foo = testName.indexOf(\"dex\");\n@@ -52,10 +50,9 @@\n-\n-    private static String generateTestString(int min, int max) {\n-        StringBuffer aNewString = new StringBuffer(120);\n-        int aNewLength = getRandomIndex(min, max);\n-        for(int y=0; y<aNewLength; y++) {\n-            int achar = generator.nextInt(30)+30;\n-            char test = (char)(achar);\n-            aNewString.append(test);\n-        }\n-        return aNewString.toString();\n+    System.out.println(\"\");\n+    generator.setSeed(1999);\n+    simpleTest();\n+    generator.setSeed(1999);\n+    compareIndexOfLastIndexOf();\n+    generator.setSeed(1999);\n+    compareStringStringBuffer();\n+    generator.setSeed(1999);\n+    compareExhaustive();\n@@ -64,4 +61,18 @@\n-    private static int getRandomIndex(int constraint1, int constraint2) {\n-        int range = constraint2 - constraint1;\n-        int x = generator.nextInt(range);\n-        return constraint1 + x;\n+    if (failure)\n+      throw new RuntimeException(\"One or more BitSet failures.\");\n+  }\n+\n+  private static void report(String testName, int failCount) {\n+    System.err.println(testName + \": \" +\n+        (failCount == 0 ? \"Passed\" : \"Failed(\" + failCount + \")\"));\n+    if (failCount > 0)\n+      failure = true;\n+  }\n+\n+  private static String generateTestString(int min, int max) {\n+    StringBuffer aNewString = new StringBuffer(120);\n+    int aNewLength = getRandomIndex(min, max);\n+    for (int y = 0; y < aNewLength; y++) {\n+      int achar = generator.nextInt(30) + 30;\n+      char test = (char) (achar);\n+      aNewString.append(test);\n@@ -69,23 +80,21 @@\n-\n-    private static void simpleTest() {\n-        int failCount = 0;\n-        String sourceString;\n-        StringBuffer sourceBuffer;\n-        String targetString;\n-\n-        for (int i=0; i<10000; i++) {\n-            do {\n-                sourceString = generateTestString(99, 100);\n-                sourceBuffer = new StringBuffer(sourceString);\n-                targetString = generateTestString(10, 11);\n-            } while (sourceString.indexOf(targetString) != -1);\n-\n-            int index1 = generator.nextInt(90) + 5;\n-            sourceBuffer = sourceBuffer.replace(index1, index1, targetString);\n-\n-            if (sourceBuffer.indexOf(targetString) != index1)\n-                failCount++;\n-            if (sourceBuffer.indexOf(targetString, 5) != index1)\n-                failCount++;\n-            if (sourceBuffer.indexOf(targetString, 99) == index1)\n-                failCount++;\n+    return aNewString.toString();\n+  }\n+\n+  private static int getRandomIndex(int constraint1, int constraint2) {\n+    int range = constraint2 - constraint1;\n+    int x = generator.nextInt(range);\n+    return constraint1 + x;\n+  }\n+\n+  private static int naiveFind(String haystack, String needle, int offset) {\n+    int x = offset;\n+    int y = 0;\n+    int len = haystack.length() - offset;\n+    if (needle.length() == 0) return 0;\n+    if (needle.length() > len) return -1;\n+    for (x = offset; x < len - needle.length() + 1; x++) {\n+      if (haystack.charAt(x) == needle.charAt(0)) {\n+        for (y = 1; y < needle.length(); y++) {\n+          if (haystack.charAt(x + y) != needle.charAt(y)) {\n+            break;\n+          }\n@@ -93,2 +102,2 @@\n-\n-        report(\"Basic Test                   \", failCount);\n+        if (y == needle.length()) return x;\n+      }\n@@ -96,34 +105,39 @@\n-\n-    \/\/ Note: it is possible although highly improbable that failCount will\n-    \/\/ be > 0 even if everthing is working ok\n-    private static void compareIndexOfLastIndexOf() {\n-        int failCount = 0;\n-        String sourceString;\n-        StringBuffer sourceBuffer;\n-        String targetString;\n-\n-        for (int i=0; i<10000; i++) {\n-            do {\n-                sourceString = generateTestString(99, 100);\n-                sourceBuffer = new StringBuffer(sourceString);\n-                targetString = generateTestString(10, 11);\n-            } while (sourceString.indexOf(targetString) != -1);\n-\n-            int index1 = generator.nextInt(100);\n-            sourceBuffer = sourceBuffer.replace(index1, index1, targetString);\n-\n-            \/\/ extremely remote possibility of > 1 match\n-            int matches = 0;\n-            int index2 = -1;\n-            while((index2 = sourceBuffer.indexOf(targetString,index2+1)) != -1)\n-                matches++;\n-            if (matches > 1)\n-                continue;\n-\n-            if (sourceBuffer.indexOf(targetString) !=\n-                sourceBuffer.lastIndexOf(targetString))\n-                failCount++;\n-            sourceString = sourceBuffer.toString();\n-            if (sourceString.indexOf(targetString) !=\n-                sourceString.lastIndexOf(targetString))\n-                failCount++;\n+    return -1;\n+  }\n+\n+  private static void compareExhaustive() {\n+    int failCount = 0;\n+    String sourceString;\n+    StringBuffer sourceBuffer;\n+    String targetString;\n+    String targetStringBeginning;\n+    String targetStringMiddle;\n+    String targetStringEnd;\n+    int hsLen = 97;\n+    int maxNeedleLen = hsLen \/ 2;\n+    int haystackLen;\n+    int needleLen;\n+    int hsBegin, hsEnd, nBegin, nEnd;\n+\n+    for (int i = 0; i < 10000; i++) {\n+      do {\n+        sourceString = generateTestString(hsLen - 1, hsLen);\n+        sourceBuffer = new StringBuffer(sourceString);\n+        targetString = generateTestString(maxNeedleLen - 1, maxNeedleLen);\n+      } while (naiveFind(sourceString, targetString, 0) != -1);\n+\n+      for (haystackLen = 0; haystackLen < hsLen; haystackLen += 7) {\n+        for (needleLen = 0; (needleLen < maxNeedleLen) && (needleLen <= haystackLen); needleLen++) {\n+          for (hsBegin = 0; (hsBegin < haystackLen - needleLen) && (hsBegin + haystackLen < hsLen); hsBegin += 3) {\n+            for (nBegin = 0; (nBegin < needleLen) && (nBegin + needleLen < maxNeedleLen); nBegin += 3) {\n+              int nResult = naiveFind(sourceString.substring(hsBegin, hsBegin + haystackLen),\n+                                      targetString.substring(nBegin, nBegin + needleLen), 0);\n+              int iResult = sourceString.substring(hsBegin, hsBegin + haystackLen).indexOf(targetString.substring(nBegin, nBegin + needleLen));\n+              if (iResult != nResult) {\n+                System.out.println(\"Source=\"+sourceString.substring(hsBegin, hsBegin + haystackLen));\n+                System.out.println(\"Target=\"+targetString.substring(nBegin, nBegin + needleLen));\n+                System.out.println(\"haystackLen=\"+haystackLen+\" neeldeLen=\"+needleLen+\" hsBegin=\"+hsBegin+\" nBegin=\"+nBegin+\n+                                   \" iResult=\"+iResult+\" nResult=\"+nResult);\n+              }\n+            }\n+          }\n@@ -131,2 +145,1 @@\n-\n-        report(\"IndexOf vs LastIndexOf       \", failCount);\n+      }\n@@ -135,28 +148,29 @@\n-    private static void compareStringStringBuffer() {\n-        int failCount = 0;\n-\n-        for (int x=0; x<10000; x++) {\n-            String testString = generateTestString(1, 100);\n-            int len = testString.length();\n-\n-            StringBuffer testBuffer = new StringBuffer(len);\n-            testBuffer.append(testString);\n-            if (!testString.equals(testBuffer.toString()))\n-                throw new RuntimeException(\"Initial equality failure\");\n-\n-            int x1 = 0;\n-            int x2 = 1000;\n-            while(x2 > testString.length()) {\n-                x1 = generator.nextInt(len);\n-                x2 = generator.nextInt(100);\n-                x2 = x1 + x2;\n-            }\n-            String fragment = testString.substring(x1,x2);\n-\n-            int sAnswer = testString.indexOf(fragment);\n-            int sbAnswer = testBuffer.indexOf(fragment);\n-\n-            if (sAnswer != sbAnswer)\n-                failCount++;\n-\n-            int testIndex = getRandomIndex(-100, 100);\n+    report(\"Exhaustive                   \", failCount);\n+  }\n+\n+  private static void simpleTest() {\n+    int failCount = 0;\n+    String sourceString;\n+    StringBuffer sourceBuffer;\n+    String targetString;\n+\n+    for (int i = 0; i < 10000; i++) {\n+      do {\n+        sourceString = generateTestString(99, 100);\n+        sourceBuffer = new StringBuffer(sourceString);\n+        targetString = generateTestString(10, 11);\n+      } while (sourceString.indexOf(targetString) != -1);\n+\n+      int index1 = generator.nextInt(90) + 5;\n+      sourceBuffer = sourceBuffer.replace(index1, index1, targetString);\n+\n+      if ((sourceBuffer.indexOf(targetString) != index1) ||\n+          (index1 != naiveFind(sourceBuffer.toString(), targetString, 0)))\n+        failCount++;\n+      if ((sourceBuffer.indexOf(targetString, 5) != index1) ||\n+          (index1 != naiveFind(sourceBuffer.toString(), targetString, 0)))\n+        failCount++;\n+      if ((sourceBuffer.indexOf(targetString, 99) == index1) ||\n+          (index1 != naiveFind(sourceBuffer.toString(), targetString, 0)))\n+        failCount++;\n+    }\n@@ -164,2 +178,35 @@\n-            sAnswer = testString.indexOf(fragment, testIndex);\n-            sbAnswer = testBuffer.indexOf(fragment, testIndex);\n+    report(\"Basic Test                   \", failCount);\n+  }\n+\n+  \/\/ Note: it is possible although highly improbable that failCount will\n+  \/\/ be > 0 even if everthing is working ok\n+  private static void compareIndexOfLastIndexOf() {\n+    int failCount = 0;\n+    String sourceString;\n+    StringBuffer sourceBuffer;\n+    String targetString;\n+\n+    for (int i = 0; i < 10000; i++) {\n+      do {\n+        sourceString = generateTestString(99, 100);\n+        sourceBuffer = new StringBuffer(sourceString);\n+        targetString = generateTestString(10, 11);\n+      } while (sourceString.indexOf(targetString) != -1);\n+\n+      int index1 = generator.nextInt(100);\n+      sourceBuffer = sourceBuffer.replace(index1, index1, targetString);\n+\n+      \/\/ extremely remote possibility of > 1 match\n+      int matches = 0;\n+      int index2 = -1;\n+      while ((index2 = sourceBuffer.indexOf(targetString, index2 + 1)) != -1)\n+        matches++;\n+      if (matches > 1)\n+        continue;\n+\n+      if (sourceBuffer.indexOf(targetString) != sourceBuffer.lastIndexOf(targetString))\n+        failCount++;\n+      sourceString = sourceBuffer.toString();\n+      if (sourceString.indexOf(targetString) != sourceString.lastIndexOf(targetString))\n+        failCount++;\n+    }\n@@ -167,2 +214,32 @@\n-            if (sAnswer != sbAnswer)\n-                failCount++;\n+    report(\"IndexOf vs LastIndexOf       \", failCount);\n+  }\n+\n+  private static void compareStringStringBuffer() {\n+    int failCount = 0;\n+  boolean make_new = true;\n+\n+      String fragment = null;\n+      StringBuffer testBuffer = null;\n+      String testString = null;\n+      int testIndex = 0;\n+    generator.setSeed(1999);\n+\n+    for (int x = 0; x < 1000000; x++) {\n+    if(make_new) {\n+        testString = generateTestString(1, 100);\n+        int len = testString.length();\n+\n+        testBuffer = new StringBuffer(len);\n+        testBuffer.append(testString);\n+        if (!testString.equals(testBuffer.toString()))\n+        throw new RuntimeException(\"Initial equality failure\");\n+\n+        int x1 = 0;\n+        int x2 = 1000;\n+        while (x2 > testString.length()) {\n+        x1 = generator.nextInt(len);\n+        x2 = generator.nextInt(100);\n+        x2 = x1 + x2;\n+        }\n+        fragment = testString.substring(x1, x2);\n+    }\n@@ -170,2 +247,45 @@\n-            sAnswer = testString.lastIndexOf(fragment);\n-            sbAnswer = testBuffer.lastIndexOf(fragment);\n+      int sAnswer = testString.indexOf(fragment);\n+      int sbAnswer = testBuffer.indexOf(fragment);\n+\n+      if (sAnswer != sbAnswer) {\n+        System.err.println(\"IndexOf fragment '\" + fragment + \"' (\" + fragment.length() + \") len String = \"\n+            + testString.length() + \" len Buffer = \" + testBuffer.length());\n+        System.err.println(\"  sAnswer = \" + sAnswer + \", sbAnswer = \" + sbAnswer);\n+        System.err.println(\"  testString = '\" + testString + \"'\");\n+        System.err.println(\"  testBuffer = '\" + testBuffer + \"'\");\n+        failCount++;\n+      } else {\n+        if (sAnswer > testString.length()) {\n+          System.err.println(\n+              \"IndexOf returned value out of range; return: \" + sAnswer + \" length max: \" + testBuffer.length());\n+        }\n+      }\n+\n+      if ((fragment == \"0#:02\/62;+-\\\"\\\"0$25-5$#)1263\") && (testBuffer.length() == 94)) {\n+        String xx = \"abc\";\n+        String yy = \"abcdefg\";\n+        int sA = xx.indexOf(yy);\n+      }\n+\n+    if(make_new)\n+        testIndex = getRandomIndex(-100, 100);\n+\n+      sAnswer = testString.indexOf(fragment, testIndex);\n+      sbAnswer = testBuffer.indexOf(fragment, testIndex);\n+\n+      if (sAnswer != sbAnswer) {\n+        System.err.println(\"IndexOf fragment '\" + fragment + \"' (\" + fragment.length() + \") index = \" + testIndex\n+            + \" len String = \" + testString.length() + \" len Buffer = \" + testBuffer.length());\n+        System.err.println(\"  sAnswer = \" + sAnswer + \", sbAnswer = \" + sbAnswer);\n+        System.err.println(\"  testString = '\" + testString + \"'\");\n+        System.err.println(\"  testBuffer = '\" + testBuffer + \"'\");\n+        failCount++;\n+    make_new = false;\n+      } else {\n+        if ((sAnswer > testString.length()) || ((sAnswer != -1) && (sAnswer < testIndex) && (fragment.length() != 0))) {\n+          System.err.println(\"IndexOf returned value out of range; return: \" + sAnswer + \" length max: \"\n+              + testString.length() + \" index: \" + testIndex);\n+          System.err.println(\"IndexOf fragment '\" + fragment + \"' (\" + fragment.length() + \") index = \" + testIndex\n+              + \" len String = \" + testString.length() + \" len Buffer = \" + testBuffer.length());\n+        }\n+      }\n@@ -173,2 +293,2 @@\n-            if (sAnswer != sbAnswer)\n-                failCount++;\n+      sAnswer = testString.lastIndexOf(fragment);\n+      sbAnswer = testBuffer.lastIndexOf(fragment);\n@@ -176,1 +296,6 @@\n-            testIndex = getRandomIndex(-100, 100);\n+      if (sAnswer != sbAnswer) {\n+        System.err.println(\"lastIndexOf fragment '\" + fragment + \"' len String = \" + testString.length()\n+            + \" len Buffer = \" + testBuffer.length());\n+        System.err.println(\"  sAnswer = \" + sAnswer + \", sbAnswer = \" + sbAnswer);\n+        failCount++;\n+      }\n@@ -178,2 +303,1 @@\n-            sAnswer = testString.lastIndexOf(fragment, testIndex);\n-            sbAnswer = testBuffer.lastIndexOf(fragment, testIndex);\n+      if(make_new) testIndex = getRandomIndex(-100, 100);\n@@ -181,3 +305,2 @@\n-            if (sAnswer != sbAnswer)\n-                failCount++;\n-        }\n+      sAnswer = testString.lastIndexOf(fragment, testIndex);\n+      sbAnswer = testBuffer.lastIndexOf(fragment, testIndex);\n@@ -185,1 +308,5 @@\n-        report(\"String vs StringBuffer       \", failCount);\n+      if (sAnswer != sbAnswer) {\n+        System.err.println(\"lastIndexOf fragment '\" + fragment + \"' index = \" + testIndex + \" len String = \"\n+            + testString.length() + \" len Buffer = \" + testBuffer.length());\n+        failCount++;\n+      }\n@@ -188,0 +315,3 @@\n+    report(\"String vs StringBuffer       \", failCount);\n+  }\n+\n","filename":"test\/jdk\/java\/lang\/StringBuffer\/IndexOf.java","additions":262,"deletions":132,"binary":false,"changes":394,"status":"modified"}]}