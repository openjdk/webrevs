[{"commit":{"message":"Merge branch 'master' into JDK-8346964\n\nChange-Id: Ib47ed4f9c6d69326a0b7cb8ba7c29f604b8fc1ec"},"files":[{"filename":"src\/hotspot\/share\/opto\/mulnode.cpp"},{"filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java"}],"sha":"011c61012638b7dfd5e3ad68a24fd3e51746d1c3"},{"commit":{"message":"8346964: C2: Improve integer multiplication with constant in MulINode::Ideal()\n\nConstant multiplication x*C can be optimized as LEFT SHIFT, ADD or SUB\ninstructions since generally these instructions have smaller latency\nand larger throughput on most architectures. For example:\n1. x*8 can be optimized as x<<3.\n2. x*9 can be optimized as x+x<<3, and x+x<<3 can be lowered as one\nSHIFT-ADD (ADD instruction combined with LEFT SHIFT) instruction on\nsome architectures, like aarch64 and x86_64.\n\nCurrently OpenJDK implemented a few such patterns in mid-end, including:\n1. |C| = 1<<n                (n>0)\n2. |C| = (1<<n) - 1          (n>0)\n3. |C| = (1<<m) + (1<<n)     (m>n, n>=0)\n\nThe first two are ok. Because on most architectures they are lowered as\nonly one ADD\/SUB\/SHIFT instruction.\n\nBut the third pattern doesn't always perform well on some architectures,\nsuch as aarch64. The third pattern can be split as the following sub\npatterns:\n3.1. C = (1<<n) + 1          (n>0)\n3.2. C = -((1<<n) + 1)       (n>0)\n3.3. C = (1<<m) + (1<<n)     (m>n, n>0)\n3.4. C = -((1<<m) + (1<<n))  (m>n, n>0)\n\nAccording to Arm optimization guide, if the shift amount > 4, the\nlatency and throughput of ADD instruction is the same with MUL\ninstruction. So in this case, converting MUL to ADD is not profitable.\nTake a[i] * C on aarch64 as an example.\n\nBefore (MUL is not converted):\n```\n  mov x1, #C\n  mul x2, x1, x0\n```\n\nNow (MUL is converted):\n  For 3.1:\n```\n    add x2, x0, x0, lsl #n\n```\n\n  For 3.2:\n```\n    add x2, x0, x0, lsl #n  \/\/ same cost with mul if n > 4\n    neg x2, x2\n```\n\n  For 3.3:\n```\n    lsl x1, x0, #m\n    add x2, x1, x0, lsl #n  \/\/ same cost with mul if n > 4\n```\n\n  For 3.4:\n```\n    lsl x1, x0, #m\n    add x2, x1, x0, lsl #n  \/\/ same cost with mul if n > 4\n    neg x2, x2\n```\n\nTest results (ns\/op) on Arm Neoverse V2:\n```\n\t\tBefore\tNow\tUplift\t\tPattern\tNotes\ntestInt9\t103.379\t60.702\t1.70305756\t3.1\ntestIntN33\t103.231\t106.825\t0.96635619\t3.2\tn > 4\ntestIntN9\t103.448\t103.005\t1.004300762\t3.2\tn <= 4\ntestInt18\t103.354\t99.271\t1.041129837\t3.3\tm <= 4, n <= 4\ntestInt36\t103.396\t99.186\t1.042445506\t3.3\tm > 4, n <= 4\ntestInt96\t103.337\t105.416\t0.980278136\t3.3\tm > 4, n > 4\ntestIntN18\t103.333\t139.258\t0.742025593\t3.4\tm <= 4, n <= 4\ntestIntN36\t103.208\t139.132\t0.741799155\t3.4\tm > 4, n <= 4\ntestIntN96\t103.367\t139.471\t0.74113615\t3.4\tm > 4, n > 4\n```\n\n**(S1) From this point on, we should treat pattern 3 as follows:**\n3.1  C = (1<<n) + 1          (n>0)\n3.2  C = -((1<<n) + 1)       (0<n<=4)\n3.3  C = (1<<m) + (1<<n)     (m>n, 0<n<=4)\n3.4  C = -((1<<m) + (1<<n))  (disable)\n\nSince this conversion is implemented in mid-end, it impacts other\noptimizations, such as auto-vectorization. Assume there's the following\nloop which can be vectorized.\nVector-A:\n```\nfor (int i=0; i<len; i++) {\n  sum += a[i] * C;\n}\n```\n\nBefore:\n```\n  movi    v19.4s, #C  \/\/ this will be hoisted out of the loop\n  mla     v16.4s, v17.4s, v19.4s\n```\n\nAfter:\n  For 3.1:\n```\n    shl     v19.4s, v17.4s, #m\n    add     v17.4s, v19.4s, v17.4s\n    add     v16.4s, v16.4s, v17.4s\n```\n\n  For 3.2:\n```\n    (add    w11, w11, w11, lsl #m\n    sub     w11, w12, w11) * 4  \/\/ *4 is for 4 ints\n```\n\n  For 3.3:\n```\n    shl     v18.4s, v17.4s, #m\n    shl     v19.4s, v17.4s, #n\n    add     v18.4s, v19.4s, v18.4s\n    add     v16.4s, v16.4s, v18.4s\n```\n\n  For 3.4:\n```\n    (lsl    w12, w4, #m\n    add     w11, w12, w4, lsl #n\n    sub     w13, w13, w11) * 4  \/\/ *4 is for 4 ints\n```\n\nThe generated instruction before is more simple and performing:\n```\n\t\t\tBefore\tNow\tUplift\t\tPattern\ntestInt9AddSum\t\t47.958\t63.696\t0.752920121\t3.1\ntestIntN33AddSum\t48.013\t147.834\t0.324776438\t3.2\ntestIntN9AddSum\t\t48.026\t149.149\t0.322000148\t3.2\ntestInt18AddSum\t\t47.971\t69.393\t0.691294511\t3.3\ntestInt36AddSum\t\t47.98\t69.395\t0.69140428\t3.3\ntestInt96AddSum\t\t47.992\t69.453\t0.690999669\t3.3\ntestIntN18AddSum\t48.014\t157.132\t0.305564748\t3.4\ntestIntN36AddSum\t48.02\t157.094\t0.305676856\t3.4\ntestIntN96AddSum\t48.032\t153.642\t0.312622851\t3.4\n```\n\n**(S2) From this point on, we should disable pattern 3 totally.**\n\nBut we can have different cases, for example:\nVector-B:\n```\nfor (int i=0; i<100000; i++) {\n  a[i] = a[i] * C;\n}\n```\nTest results:\n```\n\t\tBefore\tNow\tUplift\t\tPattern\ntestInt9Store\t43.392\t33.338\t1.301577779\t3.1\ntestIntN33Store\t43.365\t75.993\t0.570644665\t3.2\ntestIntN9Store\t43.5\t75.452\t0.576525473\t3.2\ntestInt18Store\t43.442\t41.847\t1.038115038\t3.3\ntestInt36Store\t43.369\t41.843\t1.03646966\t3.3\ntestInt96Store\t43.389\t41.931\t1.03477141\t3.3\ntestIntN18Store\t43.372\t57.909\t0.748968209\t3.4\ntestIntN36Store\t43.373\t57.042\t0.760369552\t3.4\ntestIntN96Store\t43.405\t58.145\t0.746495829\t3.4\n```\n\n**(S3) From this point on, we should treat pattern 3 as follow:**\n3.1  C = (1<<n) + 1          (n>0)\n3.2  C = -((1<<n) + 1)       (disable)\n3.3  C = (1<<m) + (1<<n)     (m>n, n>0)\n3.4  C = -((1<<m) + (1<<n))  (disable)\n\nCombining S1, S2 and S3, we get:\nPattern\t\tS1\t\tS2\t\t\tS3\n3.1\t(n>0, 1.7)\t\t(disable, 0.75)\t\t(n>0, 1.3)\n3.2\t(0<n<=4, 1.0)\t\t(disable, 0.32)\t\t(disable, 0.57)\n3.3\t(m>n, 0<n<=4, 1.04)\t(disable, 0.69)\t\t(m>n, n>0, 1.03)\n3.4\t(disable, 0.74)\t\t(disable, 0.30)\t\t(disable, 0.74)\n\nFor 3.1, it's similar with pattern 2, usually be lowered as only one\ninstruction, so we tend to keep it in mid-end.\nFor 3.2, we tend to disable it in mid-end, and do S1 in back-end if\nit's profitable.\nFor 3.3, although S3 has 3% performance gain, but S2 has 31% performance\nregression. So we tend to disable it in mid-end and redo S1 in back-end.\nFor 3.4, we shouldn't do this optimization anywhere.\n\nIn theory, auto-vectorization should be able to generate the best\nvectorized code, and cases that cannot be vectorized will be converted\ninto other more optimal scalar instructions in the architecture backend\n(this is what gcc and llvm do). However, we currently do not have a cost\nmodel and vplan, and the results of auto-vectorization are significantly\naffected by its input. Therefore, this patch turns off pattern 3.2, 3.3\nand 3.4 in mid-end. Then if it's profitable, implement these patterns in\nthe backend. If we implement a cost model and vplan in the future, it is\nbest to move all patterns to the backend, this patch does not conflict\nwith this direction.\n\nI also tested this patch on Arm N1, Intel SPR and AMD Genoa machines,\nNo noticeable performance degradation was seen on any of the machines.\n\nHere are the test results on an Arm V2 and an AMD Genoa machine:\n```\nBenchmark\tV2-now\tV2-after\tUplift\tGenoa-now\tGenoa-after\tUplift\tPattern\tNotes\ntestInt8\t60.36989\t60.276736\t1\t116.768294\t116.772547\t0.99\t1\ntestInt8AddSum\t63.658064\t63.797732\t0.99\t16.04973\t16.051491\t0.99\t1\ntestInt8Store\t38.829618\t39.054129\t0.99\t19.857453\t20.006321\t0.99\t1\ntestIntN8\t59.99655\t60.150053\t0.99\t132.269926\t132.252473\t1\t1\ntestIntN8AddSum\t145.678098\t146.181549\t0.99\t158.546226\t158.806476\t0.99\t1\ntestIntN8Store\t32.802445\t32.897907\t0.99\t19.047873\t19.065941\t0.99\t1\ntestInt7\t98.978213\t99.176574\t0.99\t114.07026\t113.08989\t1\t2\ntestInt7AddSum\t62.675636\t62.310799\t1\t23.370851\t20.971655\t1.11\t2\ntestInt7Store\t32.850828\t32.923315\t0.99\t23.884952\t23.628681\t1.01\t2\ntestIntN7\t60.27949\t60.668158\t0.99\t174.224893\t174.102295\t1\t2\ntestIntN7AddSum\t62.746696\t62.288476\t1\t20.93192\t20.964557\t0.99\t2\ntestIntN7Store\t32.812906\t32.851355\t0.99\t23.810024\t23.526074\t1.01\t2\ntestInt9\t60.820402\t60.331938\t1\t108.850777\t108.846161\t1\t3.1\ntestInt9AddSum\t62.24679\t62.374637\t0.99\t20.698749\t20.741137\t0.99\t3.1\ntestInt9Store\t32.871723\t32.912065\t0.99\t19.055537\t19.080735\t0.99\t3.1\ntestIntN33\t106.517618\t103.450746\t1.02\t153.894345\t140.641135\t1.09\t3.2\tn > 4\ntestIntN33AddSum\t147.589815\t47.911612\t3.08\t153.851885\t17.008453\t9.04\t3.2\ntestIntN33Store\t75.434513\t43.473053\t1.73\t26.612181\t20.436323\t1.3\t3.2\ntestIntN9\t102.173268\t103.70682\t0.98\t155.858169\t140.718967\t1.1\t3.2\tn <= 4\ntestIntN9AddSum\t148.724952\t47.963305\t3.1\t186.902111\t20.249414\t9.23\t3.2\ntestIntN9Store\t74.783788\t43.339188\t1.72\t20.150159\t20.888448\t0.96\t3.2\ntestInt18\t98.905625\t102.942092\t0.96\t142.480636\t140.748778\t1.01\t3.3\tm <= 4, n <= 4\ntestInt18AddSum\t68.695585\t48.103536\t1.42\t26.88524\t16.77886\t1.6\t3.3\ntestInt18Store\t41.307909\t43.385183\t0.95\t21.233238\t20.875026\t1.01\t3.3\ntestInt36\t99.039742\t103.714745\t0.95\t142.265806\t142.334039\t0.99\t3.3\tm > 4, n <= 4\ntestInt36AddSum\t68.736756\t47.952189\t1.43\t26.868362\t17.030035\t1.57\t3.3\ntestInt36Store\t41.403698\t43.414093\t0.95\t21.225454\t20.52266\t1.03\t3.3\ntestInt96\t105.00287\t103.528144\t1.01\t237.649526\t140.643255\t1.68\t3.3\tm > 4, n > 4\ntestInt96AddSum\t68.481133\t48.04549\t1.42\t26.877407\t16.918209\t1.58\t3.3\ntestInt96Store\t41.276292\t43.512994\t0.94\t23.456117\t20.540181\t1.14\t3.3\ntestIntN18\t138.629044\t103.269657\t1.34\t210.315628\t140.716818\t1.49\t3.4\tm <= 4, n <= 4\ntestIntN18AddSum\t156.635652\t48.003989\t3.26\t215.807135\t16.917665\t12.75\t3.4\ntestIntN18Store\t57.584487\t43.410415\t1.32\t26.819827\t20.707778\t1.29\t3.4\ntestIntN36\t139.068861\t103.766774\t1.34\t209.522432\t140.720322\t1.48\t3.4\tm > 4, n <= 4\ntestIntN36AddSum\t156.36928\t48.027779\t3.25\t215.705842\t16.893192\t12.76\t3.4\ntestIntN36Store\t57.715418\t43.493958\t1.32\t21.651252\t20.676877\t1.04\t3.4\ntestIntN96\t139.151761\t103.453665\t1.34\t269.254161\t140.753499\t1.91\t3.4\tm > 4, n > 4\ntestIntN96AddSum\t153.123557\t48.110524\t3.18\t263.262635\t17.011144\t15.47\t3.4\ntestIntN96Store\t57.793179\t43.47574\t1.32\t24.444592\t20.530219\t1.19\t3.4\n```\n\nlimitations:\n1, This patch only analyzes two vector cases, there may be other vector\ncases that may get performance regression with this patch.\n2, This patch does not implement the disabled patterns in the backend,\nI will propose a follow-up patch to implement these patterns in the\naarch64 backend.\n3, This patch does not handle the long type, because different\narchitectures have different auto-vectorization support for long type,\nresulting in very different performance, and it is difficult to find a\nsolution that does not introduce significant performance degradation."},"files":[{"filename":"src\/hotspot\/share\/opto\/mulnode.cpp"},{"filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestSerialAdditions.java"},{"filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/MulINodeIdealizationTests.java"},{"filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java"},{"filename":"test\/micro\/org\/openjdk\/bench\/vm\/compiler\/MultWithConst.java"}],"sha":"193dc4e5760007784cffd64ef14e0050b0be92b3"}]