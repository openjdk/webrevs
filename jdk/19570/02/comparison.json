{"files":[{"patch":"@@ -33,10 +33,0 @@\n-#include \"runtime\/os.inline.hpp\"\n-\n-\/\/ These are inline variants of Thread::SpinAcquire with optional blocking in VM.\n-\n-class ShenandoahNoBlockOp : public StackObj {\n-public:\n-  ShenandoahNoBlockOp(JavaThread* java_thread) {\n-    assert(java_thread == nullptr, \"Should not pass anything\");\n-  }\n-};\n@@ -46,2 +36,2 @@\n-  if (allow_block_for_safepoint && thread->is_Java_thread()) {\n-    contended_lock_internal<ThreadBlockInVM>(JavaThread::cast(thread));\n+  if (thread->is_Java_thread() && allow_block_for_safepoint) {\n+    contended_lock_internal<true>(JavaThread::cast(thread));\n@@ -49,1 +39,1 @@\n-    contended_lock_internal<ShenandoahNoBlockOp>(nullptr);\n+    contended_lock_internal<false>(nullptr);\n@@ -53,1 +43,1 @@\n-template<typename BlockOp>\n+template<bool ALLOW_BLOCK>\n@@ -55,2 +45,4 @@\n-  int ctr = 0;\n-  int yields = 0;\n+  assert(!ALLOW_BLOCK || java_thread != nullptr, \"Must have a Java thread when allowing block.\");\n+  \/\/ Spin this much on multi-processor, do not spin on multi-processor.\n+  int ctr = os::is_MP() ? 0x1F : 0;\n+  \/\/ Apply TTAS to avoid more expenseive CAS calls if the lock is still held by other thread.\n@@ -58,5 +50,26 @@\n-         Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n-    if ((++ctr & 0xFFF) == 0) {\n-      BlockOp block(java_thread);\n-      if (yields > 5) {\n-        os::naked_short_sleep(1);\n+    Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+    if (ctr > 0 && !SafepointSynchronize::is_synchronizing()) {\n+      \/\/ Lightly contended, spin a little if no safepoint is pending.\n+      SpinPause();\n+      ctr--;\n+    } else {\n+      if (ALLOW_BLOCK) {\n+        ThreadBlockInVM block(java_thread);\n+        if (SafepointSynchronize::is_synchronizing()) {\n+          \/\/ If safepoint is pending, we want to block and allow safepoint to proceed.\n+          \/\/ Normally, TBIVM above would block us in its destructor.\n+          \/\/\n+          \/\/ But that blocking only happens when TBIVM knows the thread poll is armed.\n+          \/\/ There is a window between announcing a safepoint and arming the thread poll\n+          \/\/ during which trying to continuously enter TBIVM is counter-productive.\n+          \/\/ Under high contention, we may end up going in circles thousands of times.\n+          \/\/ To avoid it, we wait here until local poll is armed and then proceed\n+          \/\/ to TBVIM exit for blocking. We do not SpinPause, but yield to let\n+          \/\/ VM thread to arm the poll sooner.\n+          while (SafepointSynchronize::is_synchronizing() &&\n+            !SafepointMechanism::local_poll_armed(java_thread)) {\n+            os::naked_yield();\n+          }\n+        } else {\n+          os::naked_yield();\n+        }\n@@ -65,1 +78,0 @@\n-        yields++;\n@@ -67,2 +79,0 @@\n-    } else {\n-      SpinPause();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.cpp","additions":34,"deletions":24,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  volatile Thread* _owner;\n+  Thread* volatile _owner;\n@@ -43,1 +43,1 @@\n-  template<typename BlockOp>\n+  template<bool ALLOW_BLOCK>\n@@ -45,1 +45,0 @@\n-\n@@ -52,2 +51,3 @@\n-    \/\/ Try to lock fast, or dive into contended lock handling.\n-    if (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+    if (allow_block_for_safepoint && SafepointSynchronize::is_synchronizing()) {\n+      \/\/ Java thread, and there is a pending safepoint. Dive into contended locking\n+      \/\/ immediately without trying anything else, and block.\n@@ -55,0 +55,5 @@\n+    } else {\n+      \/\/ Try to lock fast, or dive into contended lock handling.\n+      if (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+        contended_lock(allow_block_for_safepoint);\n+      }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"}]}