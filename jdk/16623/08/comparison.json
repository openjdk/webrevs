{"files":[{"patch":"@@ -53,1 +53,0 @@\n-class GenerationSpec;\n@@ -234,1 +233,1 @@\n-  \/\/ Note: in the current (1.4) implementation, when genCollectedHeap's\n+  \/\/ Note: in the current (1.4) implementation, when serialHeap's\n","filename":"src\/hotspot\/share\/gc\/serial\/generation.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,52 @@\n+#include \"serialVMOperations.hpp\"\n+\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/serial\/cardTableRS.hpp\"\n+#include \"gc\/serial\/genMarkSweep.hpp\"\n+#include \"gc\/serial\/markSweep.hpp\"\n+#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcInitLogger.hpp\"\n+#include \"gc\/shared\/gcPolicyCounters.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/genArguments.hpp\"\n+#include \"gc\/shared\/generationSpec.hpp\"\n+#include \"gc\/shared\/locationPrinter.inline.hpp\"\n+#include \"gc\/shared\/oopStorage.inline.hpp\"\n+#include \"gc\/shared\/oopStorageParState.inline.hpp\"\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n+#include \"gc\/shared\/scavengableNMethods.hpp\"\n+#include \"gc\/shared\/space.hpp\"\n+#include \"gc\/shared\/weakProcessor.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/metaspaceCounters.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/autoRestore.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n@@ -42,3 +94,11 @@\n-    GenCollectedHeap(Generation::DefNew,\n-                     Generation::MarkSweepCompact,\n-                     \"Copy:MSC\"),\n+    CollectedHeap(),\n+    _young_gen(nullptr),\n+    _old_gen(nullptr),\n+    _rem_set(nullptr),\n+    _soft_ref_policy(),\n+    _gc_policy_counters(new GCPolicyCounters(\"Copy:MSC\", 2, 2)),\n+    _incremental_collection_failed(false),\n+    _full_collections_completed(0),\n+    _young_manager(nullptr),\n+    _old_manager(nullptr),\n+\n@@ -135,0 +195,943 @@\n+\n+jint SerialHeap::initialize() {\n+  \/\/ Allocate space for the heap.\n+\n+  ReservedHeapSpace heap_rs = allocate(HeapAlignment);\n+\n+  if (!heap_rs.is_reserved()) {\n+    vm_shutdown_during_initialization(\n+      \"Could not reserve enough space for object heap\");\n+    return JNI_ENOMEM;\n+  }\n+\n+  initialize_reserved_region(heap_rs);\n+\n+  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n+  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n+\n+  _rem_set = create_rem_set(heap_rs.region());\n+  _rem_set->initialize(young_rs.base(), old_rs.base());\n+\n+  CardTableBarrierSet *bs = new CardTableBarrierSet(_rem_set);\n+  bs->initialize();\n+  BarrierSet::set_barrier_set(bs);\n+\n+  _young_gen = static_cast<DefNewGeneration*>(\n+    _young_gen_spec->init(young_rs, rem_set()));\n+\n+  _old_gen = static_cast<TenuredGeneration*>(\n+    _old_gen_spec->init(old_rs, rem_set()));\n+\n+  GCInitLogger::print();\n+\n+  return JNI_OK;\n+}\n+\n+\n+CardTableRS* SerialHeap::create_rem_set(const MemRegion& reserved_region) {\n+  return new CardTableRS(reserved_region);\n+}\n+\n+ReservedHeapSpace SerialHeap::allocate(size_t alignment) {\n+  \/\/ Now figure out the total size.\n+  const size_t pageSize = UseLargePages ? os::large_page_size() : os::vm_page_size();\n+  assert(alignment % pageSize == 0, \"Must be\");\n+\n+  \/\/ Check for overflow.\n+  size_t total_reserved = _young_gen_spec->max_size() + _old_gen_spec->max_size();\n+  if (total_reserved < _young_gen_spec->max_size()) {\n+    vm_exit_during_initialization(\"The size of the object heap + VM data exceeds \"\n+                                  \"the maximum representable size\");\n+  }\n+  assert(total_reserved % alignment == 0,\n+         \"Gen size; total_reserved=\" SIZE_FORMAT \", alignment=\"\n+         SIZE_FORMAT, total_reserved, alignment);\n+\n+  ReservedHeapSpace heap_rs = Universe::reserve_heap(total_reserved, alignment);\n+  size_t used_page_size = heap_rs.page_size();\n+\n+  os::trace_page_sizes(\"Heap\",\n+                       MinHeapSize,\n+                       total_reserved,\n+                       heap_rs.base(),\n+                       heap_rs.size(),\n+                       used_page_size);\n+\n+  return heap_rs;\n+}\n+\n+class GenIsScavengable : public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop obj) {\n+    return SerialHeap::heap()->is_in_young(obj);\n+  }\n+};\n+\n+static GenIsScavengable _is_scavengable;\n+\n+void SerialHeap::post_initialize() {\n+  CollectedHeap::post_initialize();\n+\n+  DefNewGeneration* def_new_gen = (DefNewGeneration*)_young_gen;\n+\n+  def_new_gen->ref_processor_init();\n+\n+  MarkSweep::initialize();\n+\n+  ScavengableNMethods::initialize(&_is_scavengable);\n+}\n+\n+PreGenGCValues SerialHeap::get_pre_gc_values() const {\n+  const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();\n+\n+  return PreGenGCValues(def_new_gen->used(),\n+                        def_new_gen->capacity(),\n+                        def_new_gen->eden()->used(),\n+                        def_new_gen->eden()->capacity(),\n+                        def_new_gen->from()->used(),\n+                        def_new_gen->from()->capacity(),\n+                        old_gen()->used(),\n+                        old_gen()->capacity());\n+}\n+\n+size_t SerialHeap::capacity() const {\n+  return _young_gen->capacity() + _old_gen->capacity();\n+}\n+\n+size_t SerialHeap::used() const {\n+  return _young_gen->used() + _old_gen->used();\n+}\n+\n+void SerialHeap::save_used_regions() {\n+  _old_gen->save_used_region();\n+  _young_gen->save_used_region();\n+}\n+\n+size_t SerialHeap::max_capacity() const {\n+  return _young_gen->max_capacity() + _old_gen->max_capacity();\n+}\n+\n+\/\/ Update the _full_collections_completed counter\n+\/\/ at the end of a stop-world full GC.\n+unsigned int SerialHeap::update_full_collections_completed() {\n+  assert(_full_collections_completed <= _total_full_collections,\n+         \"Can't complete more collections than were started\");\n+  _full_collections_completed = _total_full_collections;\n+  return _full_collections_completed;\n+}\n+\n+\/\/ Return true if any of the following is true:\n+\/\/ . the allocation won't fit into the current young gen heap\n+\/\/ . gc locker is occupied (jni critical section)\n+\/\/ . heap memory is tight -- the most recent previous collection\n+\/\/   was a full collection because a partial collection (would\n+\/\/   have) failed and is likely to fail again\n+bool SerialHeap::should_try_older_generation_allocation(size_t word_size) const {\n+  size_t young_capacity = _young_gen->capacity_before_gc();\n+  return    (word_size > heap_word_size(young_capacity))\n+         || GCLocker::is_active_and_needs_gc()\n+         || incremental_collection_failed();\n+}\n+\n+HeapWord* SerialHeap::expand_heap_and_allocate(size_t size, bool   is_tlab) {\n+  HeapWord* result = nullptr;\n+  if (_old_gen->should_allocate(size, is_tlab)) {\n+    result = _old_gen->expand_and_allocate(size, is_tlab);\n+  }\n+  if (result == nullptr) {\n+    if (_young_gen->should_allocate(size, is_tlab)) {\n+      result = _young_gen->expand_and_allocate(size, is_tlab);\n+    }\n+  }\n+  assert(result == nullptr || is_in_reserved(result), \"result not in heap\");\n+  return result;\n+}\n+\n+HeapWord* SerialHeap::mem_allocate_work(size_t size,\n+                                              bool is_tlab) {\n+\n+  HeapWord* result = nullptr;\n+\n+  \/\/ Loop until the allocation is satisfied, or unsatisfied after GC.\n+  for (uint try_count = 1, gclocker_stalled_count = 0; \/* return or throw *\/; try_count += 1) {\n+\n+    \/\/ First allocation attempt is lock-free.\n+    Generation *young = _young_gen;\n+    if (young->should_allocate(size, is_tlab)) {\n+      result = young->par_allocate(size, is_tlab);\n+      if (result != nullptr) {\n+        assert(is_in_reserved(result), \"result not in heap\");\n+        return result;\n+      }\n+    }\n+    uint gc_count_before;  \/\/ Read inside the Heap_lock locked region.\n+    {\n+      MutexLocker ml(Heap_lock);\n+      log_trace(gc, alloc)(\"SerialHeap::mem_allocate_work: attempting locked slow path allocation\");\n+      \/\/ Note that only large objects get a shot at being\n+      \/\/ allocated in later generations.\n+      bool first_only = !should_try_older_generation_allocation(size);\n+\n+      result = attempt_allocation(size, is_tlab, first_only);\n+      if (result != nullptr) {\n+        assert(is_in_reserved(result), \"result not in heap\");\n+        return result;\n+      }\n+\n+      if (GCLocker::is_active_and_needs_gc()) {\n+        if (is_tlab) {\n+          return nullptr;  \/\/ Caller will retry allocating individual object.\n+        }\n+        if (!is_maximal_no_gc()) {\n+          \/\/ Try and expand heap to satisfy request.\n+          result = expand_heap_and_allocate(size, is_tlab);\n+          \/\/ Result could be null if we are out of space.\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+\n+        if (gclocker_stalled_count > GCLockerRetryAllocationCount) {\n+          return nullptr; \/\/ We didn't get to do a GC and we didn't get any memory.\n+        }\n+\n+        \/\/ If this thread is not in a jni critical section, we stall\n+        \/\/ the requestor until the critical section has cleared and\n+        \/\/ GC allowed. When the critical section clears, a GC is\n+        \/\/ initiated by the last thread exiting the critical section; so\n+        \/\/ we retry the allocation sequence from the beginning of the loop,\n+        \/\/ rather than causing more, now probably unnecessary, GC attempts.\n+        JavaThread* jthr = JavaThread::current();\n+        if (!jthr->in_critical()) {\n+          MutexUnlocker mul(Heap_lock);\n+          \/\/ Wait for JNI critical section to be exited\n+          GCLocker::stall_until_clear();\n+          gclocker_stalled_count += 1;\n+          continue;\n+        } else {\n+          if (CheckJNICalls) {\n+            fatal(\"Possible deadlock due to allocating while\"\n+                  \" in jni critical section\");\n+          }\n+          return nullptr;\n+        }\n+      }\n+\n+      \/\/ Read the gc count while the heap lock is held.\n+      gc_count_before = total_collections();\n+    }\n+\n+    VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);\n+    VMThread::execute(&op);\n+    if (op.prologue_succeeded()) {\n+      result = op.result();\n+      if (op.gc_locked()) {\n+         assert(result == nullptr, \"must be null if gc_locked() is true\");\n+         continue;  \/\/ Retry and\/or stall as necessary.\n+      }\n+\n+      assert(result == nullptr || is_in_reserved(result),\n+             \"result not in heap\");\n+      return result;\n+    }\n+\n+    \/\/ Give a warning if we seem to be looping forever.\n+    if ((QueuedAllocationWarningCount > 0) &&\n+        (try_count % QueuedAllocationWarningCount == 0)) {\n+          log_warning(gc, ergo)(\"SerialHeap::mem_allocate_work retries %d times,\"\n+                                \" size=\" SIZE_FORMAT \" %s\", try_count, size, is_tlab ? \"(TLAB)\" : \"\");\n+    }\n+  }\n+}\n+\n+HeapWord* SerialHeap::attempt_allocation(size_t size,\n+                                               bool is_tlab,\n+                                               bool first_only) {\n+  HeapWord* res = nullptr;\n+\n+  if (_young_gen->should_allocate(size, is_tlab)) {\n+    res = _young_gen->allocate(size, is_tlab);\n+    if (res != nullptr || first_only) {\n+      return res;\n+    }\n+  }\n+\n+  if (_old_gen->should_allocate(size, is_tlab)) {\n+    res = _old_gen->allocate(size, is_tlab);\n+  }\n+\n+  return res;\n+}\n+\n+HeapWord* SerialHeap::mem_allocate(size_t size,\n+                                         bool* gc_overhead_limit_was_exceeded) {\n+  return mem_allocate_work(size,\n+                           false \/* is_tlab *\/);\n+}\n+\n+bool SerialHeap::must_clear_all_soft_refs() {\n+  return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||\n+         _gc_cause == GCCause::_wb_full_gc;\n+}\n+\n+void SerialHeap::collect_generation(Generation* gen, bool full, size_t size,\n+                                          bool is_tlab, bool run_verification, bool clear_soft_refs) {\n+  FormatBuffer<> title(\"Collect gen: %s\", gen->short_name());\n+  GCTraceTime(Trace, gc, phases) t1(title);\n+  TraceCollectorStats tcs(gen->counters());\n+  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause(), heap()->is_young_gen(gen) ? \"end of minor GC\" : \"end of major GC\");\n+\n+  gen->stat_record()->invocations++;\n+  gen->stat_record()->accumulated_time.start();\n+\n+  \/\/ Must be done anew before each collection because\n+  \/\/ a previous collection will do mangling and will\n+  \/\/ change top of some spaces.\n+  record_gen_tops_before_GC();\n+\n+  log_trace(gc)(\"%s invoke=%d size=\" SIZE_FORMAT, heap()->is_young_gen(gen) ? \"Young\" : \"Old\", gen->stat_record()->invocations, size * HeapWordSize);\n+\n+  if (run_verification && VerifyBeforeGC) {\n+    Universe::verify(\"Before GC\");\n+  }\n+  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());\n+\n+  \/\/ Do collection work\n+  {\n+    save_marks();   \/\/ save marks for all gens\n+\n+    gen->collect(full, clear_soft_refs, size, is_tlab);\n+  }\n+\n+  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());\n+\n+  gen->stat_record()->accumulated_time.stop();\n+\n+  update_gc_stats(gen, full);\n+\n+  if (run_verification && VerifyAfterGC) {\n+    Universe::verify(\"After GC\");\n+  }\n+}\n+\n+void SerialHeap::do_collection(bool           full,\n+                                     bool           clear_all_soft_refs,\n+                                     size_t         size,\n+                                     bool           is_tlab,\n+                                     GenerationType max_generation) {\n+  ResourceMark rm;\n+  DEBUG_ONLY(Thread* my_thread = Thread::current();)\n+\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(my_thread->is_VM_thread(), \"only VM thread\");\n+  assert(Heap_lock->is_locked(),\n+         \"the requesting thread should have the Heap_lock\");\n+  guarantee(!is_gc_active(), \"collection is not reentrant\");\n+\n+  if (GCLocker::check_active_before_gc()) {\n+    return; \/\/ GC is disabled (e.g. JNI GetXXXCritical operation)\n+  }\n+\n+  const bool do_clear_all_soft_refs = clear_all_soft_refs ||\n+                          soft_ref_policy()->should_clear_all_soft_refs();\n+\n+  ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());\n+\n+  AutoModifyRestore<bool> temporarily(_is_gc_active, true);\n+\n+  bool complete = full && (max_generation == OldGen);\n+  bool old_collects_young = complete && !ScavengeBeforeFullGC;\n+  bool do_young_collection = !old_collects_young && _young_gen->should_collect(full, size, is_tlab);\n+\n+  const PreGenGCValues pre_gc_values = get_pre_gc_values();\n+\n+  bool run_verification = total_collections() >= VerifyGCStartAt;\n+  bool prepared_for_verification = false;\n+  bool do_full_collection = false;\n+\n+  if (do_young_collection) {\n+    GCIdMark gc_id_mark;\n+    GCTraceCPUTime tcpu(((DefNewGeneration*)_young_gen)->gc_tracer());\n+    GCTraceTime(Info, gc) t(\"Pause Young\", nullptr, gc_cause(), true);\n+\n+    print_heap_before_gc();\n+\n+    if (run_verification && VerifyGCLevel <= 0 && VerifyBeforeGC) {\n+      prepare_for_verify();\n+      prepared_for_verification = true;\n+    }\n+\n+    gc_prologue(complete);\n+    increment_total_collections(complete);\n+\n+    collect_generation(_young_gen,\n+                       full,\n+                       size,\n+                       is_tlab,\n+                       run_verification && VerifyGCLevel <= 0,\n+                       do_clear_all_soft_refs);\n+\n+    if (size > 0 && (!is_tlab || _young_gen->supports_tlab_allocation()) &&\n+        size * HeapWordSize <= _young_gen->unsafe_max_alloc_nogc()) {\n+      \/\/ Allocation request was met by young GC.\n+      size = 0;\n+    }\n+\n+    \/\/ Ask if young collection is enough. If so, do the final steps for young collection,\n+    \/\/ and fallthrough to the end.\n+    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);\n+    if (!do_full_collection) {\n+      \/\/ Adjust generation sizes.\n+      _young_gen->compute_new_size();\n+\n+      print_heap_change(pre_gc_values);\n+\n+      \/\/ Track memory usage and detect low memory after GC finishes\n+      MemoryService::track_memory_usage();\n+\n+      gc_epilogue(complete);\n+    }\n+\n+    print_heap_after_gc();\n+\n+  } else {\n+    \/\/ No young collection, ask if we need to perform Full collection.\n+    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);\n+  }\n+\n+  if (do_full_collection) {\n+    GCIdMark gc_id_mark;\n+    GCTraceCPUTime tcpu(GenMarkSweep::gc_tracer());\n+    GCTraceTime(Info, gc) t(\"Pause Full\", nullptr, gc_cause(), true);\n+\n+    print_heap_before_gc();\n+\n+    if (!prepared_for_verification && run_verification &&\n+        VerifyGCLevel <= 1 && VerifyBeforeGC) {\n+      prepare_for_verify();\n+    }\n+\n+    if (!do_young_collection) {\n+      gc_prologue(complete);\n+      increment_total_collections(complete);\n+    }\n+\n+    \/\/ Accounting quirk: total full collections would be incremented when \"complete\"\n+    \/\/ is set, by calling increment_total_collections above. However, we also need to\n+    \/\/ account Full collections that had \"complete\" unset.\n+    if (!complete) {\n+      increment_total_full_collections();\n+    }\n+\n+    CodeCache::on_gc_marking_cycle_start();\n+\n+    collect_generation(_old_gen,\n+                       full,\n+                       size,\n+                       is_tlab,\n+                       run_verification && VerifyGCLevel <= 1,\n+                       do_clear_all_soft_refs);\n+\n+    CodeCache::on_gc_marking_cycle_finish();\n+    CodeCache::arm_all_nmethods();\n+\n+    \/\/ Adjust generation sizes.\n+    _old_gen->compute_new_size();\n+    _young_gen->compute_new_size();\n+\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+\n+    \/\/ Need to clear claim bits for the next mark.\n+    ClassLoaderDataGraph::clear_claimed_marks();\n+\n+    \/\/ Resize the metaspace capacity after full collections\n+    MetaspaceGC::compute_new_size();\n+    update_full_collections_completed();\n+\n+    print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory after GC finishes\n+    MemoryService::track_memory_usage();\n+\n+    \/\/ Need to tell the epilogue code we are done with Full GC, regardless what was\n+    \/\/ the initial value for \"complete\" flag.\n+    gc_epilogue(true);\n+\n+    print_heap_after_gc();\n+  }\n+}\n+\n+bool SerialHeap::should_do_full_collection(size_t size, bool full, bool is_tlab,\n+                                                 SerialHeap::GenerationType max_gen) const {\n+  return max_gen == OldGen && _old_gen->should_collect(full, size, is_tlab);\n+}\n+\n+void SerialHeap::register_nmethod(nmethod* nm) {\n+  ScavengableNMethods::register_nmethod(nm);\n+}\n+\n+void SerialHeap::unregister_nmethod(nmethod* nm) {\n+  ScavengableNMethods::unregister_nmethod(nm);\n+}\n+\n+void SerialHeap::verify_nmethod(nmethod* nm) {\n+  ScavengableNMethods::verify_nmethod(nm);\n+}\n+\n+void SerialHeap::prune_scavengable_nmethods() {\n+  ScavengableNMethods::prune_nmethods();\n+}\n+\n+HeapWord* SerialHeap::satisfy_failed_allocation(size_t size, bool is_tlab) {\n+  GCCauseSetter x(this, GCCause::_allocation_failure);\n+  HeapWord* result = nullptr;\n+\n+  assert(size != 0, \"Precondition violated\");\n+  if (GCLocker::is_active_and_needs_gc()) {\n+    \/\/ GC locker is active; instead of a collection we will attempt\n+    \/\/ to expand the heap, if there's room for expansion.\n+    if (!is_maximal_no_gc()) {\n+      result = expand_heap_and_allocate(size, is_tlab);\n+    }\n+    return result;   \/\/ Could be null if we are out of space.\n+  } else if (!incremental_collection_will_fail(false \/* don't consult_young *\/)) {\n+    \/\/ Do an incremental collection.\n+    do_collection(false,                     \/\/ full\n+                  false,                     \/\/ clear_all_soft_refs\n+                  size,                      \/\/ size\n+                  is_tlab,                   \/\/ is_tlab\n+                  SerialHeap::OldGen); \/\/ max_generation\n+  } else {\n+    log_trace(gc)(\" :: Trying full because partial may fail :: \");\n+    \/\/ Try a full collection; see delta for bug id 6266275\n+    \/\/ for the original code and why this has been simplified\n+    \/\/ with from-space allocation criteria modified and\n+    \/\/ such allocation moved out of the safepoint path.\n+    do_collection(true,                      \/\/ full\n+                  false,                     \/\/ clear_all_soft_refs\n+                  size,                      \/\/ size\n+                  is_tlab,                   \/\/ is_tlab\n+                  SerialHeap::OldGen); \/\/ max_generation\n+  }\n+\n+  result = attempt_allocation(size, is_tlab, false \/*first_only*\/);\n+\n+  if (result != nullptr) {\n+    assert(is_in_reserved(result), \"result not in heap\");\n+    return result;\n+  }\n+\n+  \/\/ OK, collection failed, try expansion.\n+  result = expand_heap_and_allocate(size, is_tlab);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n+  \/\/ If we reach this point, we're really out of memory. Try every trick\n+  \/\/ we can to reclaim memory. Force collection of soft references. Force\n+  \/\/ a complete compaction of the heap. Any additional methods for finding\n+  \/\/ free memory should be here, especially if they are expensive. If this\n+  \/\/ attempt fails, an OOM exception will be thrown.\n+  {\n+    UIntFlagSetting flag_change(MarkSweepAlwaysCompactCount, 1); \/\/ Make sure the heap is fully compacted\n+\n+    do_collection(true,                      \/\/ full\n+                  true,                      \/\/ clear_all_soft_refs\n+                  size,                      \/\/ size\n+                  is_tlab,                   \/\/ is_tlab\n+                  SerialHeap::OldGen); \/\/ max_generation\n+  }\n+\n+  result = attempt_allocation(size, is_tlab, false \/* first_only *\/);\n+  if (result != nullptr) {\n+    assert(is_in_reserved(result), \"result not in heap\");\n+    return result;\n+  }\n+\n+  assert(!soft_ref_policy()->should_clear_all_soft_refs(),\n+    \"Flag should have been handled and cleared prior to this point\");\n+\n+  \/\/ What else?  We might try synchronous finalization later.  If the total\n+  \/\/ space available is large enough for the allocation, then a more\n+  \/\/ complete compaction phase than we've tried so far might be\n+  \/\/ appropriate.\n+  return nullptr;\n+}\n+\n+#ifdef ASSERT\n+class AssertNonScavengableClosure: public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    assert(!SerialHeap::heap()->is_in_partial_collection(*p),\n+      \"Referent should not be scavengable.\");  }\n+  virtual void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n+};\n+static AssertNonScavengableClosure assert_is_non_scavengable_closure;\n+#endif\n+\n+void SerialHeap::process_roots(ScanningOption so,\n+                                     OopClosure* strong_roots,\n+                                     CLDClosure* strong_cld_closure,\n+                                     CLDClosure* weak_cld_closure,\n+                                     CodeBlobToOopClosure* code_roots) {\n+  \/\/ General roots.\n+  assert(code_roots != nullptr, \"code root closure should always be set\");\n+\n+  ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);\n+\n+  \/\/ Only process code roots from thread stacks if we aren't visiting the entire CodeCache anyway\n+  CodeBlobToOopClosure* roots_from_code_p = (so & SO_AllCodeCache) ? nullptr : code_roots;\n+\n+  Threads::oops_do(strong_roots, roots_from_code_p);\n+\n+  OopStorageSet::strong_oops_do(strong_roots);\n+\n+  if (so & SO_ScavengeCodeCache) {\n+    assert(code_roots != nullptr, \"must supply closure for code cache\");\n+\n+    \/\/ We only visit parts of the CodeCache when scavenging.\n+    ScavengableNMethods::nmethods_do(code_roots);\n+  }\n+  if (so & SO_AllCodeCache) {\n+    assert(code_roots != nullptr, \"must supply closure for code cache\");\n+\n+    \/\/ CMSCollector uses this to do intermediate-strength collections.\n+    \/\/ We scan the entire code cache, since CodeCache::do_unloading is not called.\n+    CodeCache::blobs_do(code_roots);\n+  }\n+  \/\/ Verify that the code cache contents are not subject to\n+  \/\/ movement by a scavenging collection.\n+  DEBUG_ONLY(CodeBlobToOopClosure assert_code_is_non_scavengable(&assert_is_non_scavengable_closure, !CodeBlobToOopClosure::FixRelocations));\n+  DEBUG_ONLY(ScavengableNMethods::asserted_non_scavengable_nmethods_do(&assert_code_is_non_scavengable));\n+}\n+\n+void SerialHeap::gen_process_weak_roots(OopClosure* root_closure) {\n+  WeakProcessor::oops_do(root_closure);\n+}\n+\n+bool SerialHeap::no_allocs_since_save_marks() {\n+  return _young_gen->no_allocs_since_save_marks() &&\n+         _old_gen->no_allocs_since_save_marks();\n+}\n+\n+\/\/ public collection interfaces\n+void SerialHeap::collect(GCCause::Cause cause) {\n+  \/\/ The caller doesn't have the Heap_lock\n+  assert(!Heap_lock->owned_by_self(), \"this thread should not own the Heap_lock\");\n+\n+  unsigned int gc_count_before;\n+  unsigned int full_gc_count_before;\n+\n+  {\n+    MutexLocker ml(Heap_lock);\n+    \/\/ Read the GC count while holding the Heap_lock\n+    gc_count_before      = total_collections();\n+    full_gc_count_before = total_full_collections();\n+  }\n+\n+  if (GCLocker::should_discard(cause, gc_count_before)) {\n+    return;\n+  }\n+\n+  bool should_run_young_gc =  (cause == GCCause::_wb_young_gc)\n+                           || (cause == GCCause::_gc_locker)\n+                DEBUG_ONLY(|| (cause == GCCause::_scavenge_alot));\n+\n+  const GenerationType max_generation = should_run_young_gc\n+                                      ? YoungGen\n+                                      : OldGen;\n+\n+  while (true) {\n+    VM_GenCollectFull op(gc_count_before, full_gc_count_before,\n+                        cause, max_generation);\n+    VMThread::execute(&op);\n+\n+    if (!GCCause::is_explicit_full_gc(cause)) {\n+      return;\n+    }\n+\n+    {\n+      MutexLocker ml(Heap_lock);\n+      \/\/ Read the GC count while holding the Heap_lock\n+      if (full_gc_count_before != total_full_collections()) {\n+        return;\n+      }\n+    }\n+\n+    if (GCLocker::is_active_and_needs_gc()) {\n+      \/\/ If GCLocker is active, wait until clear before retrying.\n+      GCLocker::stall_until_clear();\n+    }\n+  }\n+}\n+\n+void SerialHeap::do_full_collection(bool clear_all_soft_refs) {\n+   do_full_collection(clear_all_soft_refs, OldGen);\n+}\n+\n+void SerialHeap::do_full_collection(bool clear_all_soft_refs,\n+                                          GenerationType last_generation) {\n+  do_collection(true,                   \/\/ full\n+                clear_all_soft_refs,    \/\/ clear_all_soft_refs\n+                0,                      \/\/ size\n+                false,                  \/\/ is_tlab\n+                last_generation);       \/\/ last_generation\n+  \/\/ Hack XXX FIX ME !!!\n+  \/\/ A scavenge may not have been attempted, or may have\n+  \/\/ been attempted and failed, because the old gen was too full\n+  if (gc_cause() == GCCause::_gc_locker && incremental_collection_failed()) {\n+    log_debug(gc, jni)(\"GC locker: Trying a full collection because scavenge failed\");\n+    \/\/ This time allow the old gen to be collected as well\n+    do_collection(true,                \/\/ full\n+                  clear_all_soft_refs, \/\/ clear_all_soft_refs\n+                  0,                   \/\/ size\n+                  false,               \/\/ is_tlab\n+                  OldGen);             \/\/ last_generation\n+  }\n+}\n+\n+bool SerialHeap::is_in_young(const void* p) const {\n+  bool result = p < _old_gen->reserved().start();\n+  assert(result == _young_gen->is_in_reserved(p),\n+         \"incorrect test - result=%d, p=\" PTR_FORMAT, result, p2i(p));\n+  return result;\n+}\n+\n+bool SerialHeap::requires_barriers(stackChunkOop obj) const {\n+  return !is_in_young(obj);\n+}\n+\n+\/\/ Returns \"TRUE\" iff \"p\" points into the committed areas of the heap.\n+bool SerialHeap::is_in(const void* p) const {\n+  return _young_gen->is_in(p) || _old_gen->is_in(p);\n+}\n+\n+#ifdef ASSERT\n+\/\/ Don't implement this by using is_in_young().  This method is used\n+\/\/ in some cases to check that is_in_young() is correct.\n+bool SerialHeap::is_in_partial_collection(const void* p) {\n+  assert(is_in_reserved(p) || p == nullptr,\n+    \"Does not work if address is non-null and outside of the heap\");\n+  return p < _young_gen->reserved().end() && p != nullptr;\n+}\n+#endif\n+\n+void SerialHeap::object_iterate(ObjectClosure* cl) {\n+  _young_gen->object_iterate(cl);\n+  _old_gen->object_iterate(cl);\n+}\n+\n+HeapWord* SerialHeap::block_start(const void* addr) const {\n+  assert(is_in_reserved(addr), \"block_start of address outside of heap\");\n+  if (_young_gen->is_in_reserved(addr)) {\n+    assert(_young_gen->is_in(addr), \"addr should be in allocated part of generation\");\n+    return _young_gen->block_start(addr);\n+  }\n+\n+  assert(_old_gen->is_in_reserved(addr), \"Some generation should contain the address\");\n+  assert(_old_gen->is_in(addr), \"addr should be in allocated part of generation\");\n+  return _old_gen->block_start(addr);\n+}\n+\n+bool SerialHeap::block_is_obj(const HeapWord* addr) const {\n+  assert(is_in_reserved(addr), \"block_is_obj of address outside of heap\");\n+  assert(block_start(addr) == addr, \"addr must be a block start\");\n+  if (_young_gen->is_in_reserved(addr)) {\n+    return _young_gen->block_is_obj(addr);\n+  }\n+\n+  assert(_old_gen->is_in_reserved(addr), \"Some generation should contain the address\");\n+  return _old_gen->block_is_obj(addr);\n+}\n+\n+size_t SerialHeap::tlab_capacity(Thread* thr) const {\n+  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n+  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n+  return _young_gen->tlab_capacity();\n+}\n+\n+size_t SerialHeap::tlab_used(Thread* thr) const {\n+  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n+  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n+  return _young_gen->tlab_used();\n+}\n+\n+size_t SerialHeap::unsafe_max_tlab_alloc(Thread* thr) const {\n+  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n+  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n+  return _young_gen->unsafe_max_tlab_alloc();\n+}\n+\n+HeapWord* SerialHeap::allocate_new_tlab(size_t min_size,\n+                                              size_t requested_size,\n+                                              size_t* actual_size) {\n+  HeapWord* result = mem_allocate_work(requested_size \/* size *\/,\n+                                       true \/* is_tlab *\/);\n+  if (result != nullptr) {\n+    *actual_size = requested_size;\n+  }\n+\n+  return result;\n+}\n+\n+void SerialHeap::prepare_for_verify() {\n+  ensure_parsability(false);        \/\/ no need to retire TLABs\n+}\n+\n+void SerialHeap::generation_iterate(GenClosure* cl,\n+                                          bool old_to_young) {\n+  if (old_to_young) {\n+    cl->do_generation(_old_gen);\n+    cl->do_generation(_young_gen);\n+  } else {\n+    cl->do_generation(_young_gen);\n+    cl->do_generation(_old_gen);\n+  }\n+}\n+\n+bool SerialHeap::is_maximal_no_gc() const {\n+  return _young_gen->is_maximal_no_gc() && _old_gen->is_maximal_no_gc();\n+}\n+\n+void SerialHeap::save_marks() {\n+  _young_gen->save_marks();\n+  _old_gen->save_marks();\n+}\n+\n+#if INCLUDE_SERIALGC\n+void SerialHeap::prepare_for_compaction() {\n+  \/\/ Start by compacting into same gen.\n+  CompactPoint cp(_old_gen);\n+  _old_gen->prepare_for_compaction(&cp);\n+  _young_gen->prepare_for_compaction(&cp);\n+}\n+#endif \/\/ INCLUDE_SERIALGC\n+\n+void SerialHeap::verify(VerifyOption option \/* ignored *\/) {\n+  log_debug(gc, verify)(\"%s\", _old_gen->name());\n+  _old_gen->verify();\n+\n+  log_debug(gc, verify)(\"%s\", _young_gen->name());\n+  _young_gen->verify();\n+\n+  log_debug(gc, verify)(\"RemSet\");\n+  rem_set()->verify();\n+}\n+\n+void SerialHeap::print_on(outputStream* st) const {\n+  if (_young_gen != nullptr) {\n+    _young_gen->print_on(st);\n+  }\n+  if (_old_gen != nullptr) {\n+    _old_gen->print_on(st);\n+  }\n+  MetaspaceUtils::print_on(st);\n+}\n+\n+void SerialHeap::gc_threads_do(ThreadClosure* tc) const {\n+}\n+\n+bool SerialHeap::print_location(outputStream* st, void* addr) const {\n+  return BlockLocationPrinter<SerialHeap>::print_location(st, addr);\n+}\n+\n+void SerialHeap::print_tracing_info() const {\n+  if (log_is_enabled(Debug, gc, heap, exit)) {\n+    LogStreamHandle(Debug, gc, heap, exit) lsh;\n+    _young_gen->print_summary_info_on(&lsh);\n+    _old_gen->print_summary_info_on(&lsh);\n+  }\n+}\n+\n+void SerialHeap::print_heap_change(const PreGenGCValues& pre_gc_values) const {\n+  const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();\n+\n+  log_info(gc, heap)(HEAP_CHANGE_FORMAT\" \"\n+                     HEAP_CHANGE_FORMAT\" \"\n+                     HEAP_CHANGE_FORMAT,\n+                     HEAP_CHANGE_FORMAT_ARGS(def_new_gen->short_name(),\n+                                             pre_gc_values.young_gen_used(),\n+                                             pre_gc_values.young_gen_capacity(),\n+                                             def_new_gen->used(),\n+                                             def_new_gen->capacity()),\n+                     HEAP_CHANGE_FORMAT_ARGS(\"Eden\",\n+                                             pre_gc_values.eden_used(),\n+                                             pre_gc_values.eden_capacity(),\n+                                             def_new_gen->eden()->used(),\n+                                             def_new_gen->eden()->capacity()),\n+                     HEAP_CHANGE_FORMAT_ARGS(\"From\",\n+                                             pre_gc_values.from_used(),\n+                                             pre_gc_values.from_capacity(),\n+                                             def_new_gen->from()->used(),\n+                                             def_new_gen->from()->capacity()));\n+  log_info(gc, heap)(HEAP_CHANGE_FORMAT,\n+                     HEAP_CHANGE_FORMAT_ARGS(old_gen()->short_name(),\n+                                             pre_gc_values.old_gen_used(),\n+                                             pre_gc_values.old_gen_capacity(),\n+                                             old_gen()->used(),\n+                                             old_gen()->capacity()));\n+  MetaspaceUtils::print_metaspace_change(pre_gc_values.metaspace_sizes());\n+}\n+\n+class GenGCPrologueClosure: public SerialHeap::GenClosure {\n+ private:\n+  bool _full;\n+ public:\n+  void do_generation(Generation* gen) {\n+    gen->gc_prologue(_full);\n+  }\n+  GenGCPrologueClosure(bool full) : _full(full) {};\n+};\n+\n+void SerialHeap::gc_prologue(bool full) {\n+  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n+\n+  \/\/ Fill TLAB's and such\n+  ensure_parsability(true);   \/\/ retire TLABs\n+\n+  \/\/ Walk generations\n+  GenGCPrologueClosure blk(full);\n+  generation_iterate(&blk, false);  \/\/ not old-to-young.\n+};\n+\n+class GenGCEpilogueClosure: public SerialHeap::GenClosure {\n+ private:\n+  bool _full;\n+ public:\n+  void do_generation(Generation* gen) {\n+    gen->gc_epilogue(_full);\n+  }\n+  GenGCEpilogueClosure(bool full) : _full(full) {};\n+};\n+\n+void SerialHeap::gc_epilogue(bool full) {\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_empty(), \"derived pointer present\");\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+  resize_all_tlabs();\n+\n+  GenGCEpilogueClosure blk(full);\n+  generation_iterate(&blk, false);  \/\/ not old-to-young.\n+\n+  MetaspaceCounters::update_performance_counters();\n+};\n+\n+#ifndef PRODUCT\n+class GenGCSaveTopsBeforeGCClosure: public SerialHeap::GenClosure {\n+ private:\n+ public:\n+  void do_generation(Generation* gen) {\n+    gen->record_spaces_top();\n+  }\n+};\n+\n+void SerialHeap::record_gen_tops_before_GC() {\n+  if (ZapUnusedHeapArea) {\n+    GenGCSaveTopsBeforeGCClosure blk;\n+    generation_iterate(&blk, false);  \/\/ not old-to-young.\n+  }\n+}\n+#endif  \/\/ not PRODUCT\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":1006,"deletions":3,"binary":false,"changes":1009,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -32,0 +31,8 @@\n+#include \"gc\/serial\/generation.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/oopStorageParState.hpp\"\n+#include \"gc\/shared\/preGCValues.hpp\"\n+#include \"gc\/shared\/softRefPolicy.hpp\"\n+\n+class CardTableRS;\n+class GCPolicyCounters;\n@@ -38,2 +45,0 @@\n-\/\/ SerialHeap is the implementation of CollectedHeap for Serial GC.\n-\/\/\n@@ -58,1 +63,304 @@\n-class SerialHeap : public GenCollectedHeap {\n+class SerialHeap : public CollectedHeap {\n+  friend class Generation;\n+  friend class DefNewGeneration;\n+  friend class TenuredGeneration;\n+  friend class GenMarkSweep;\n+  friend class VM_GenCollectForAllocation;\n+  friend class VM_GenCollectFull;\n+  friend class VM_GC_HeapInspection;\n+  friend class VM_HeapDumper;\n+  friend class HeapInspection;\n+  friend class GCCauseSetter;\n+  friend class VMStructs;\n+public:\n+  friend class VM_PopulateDumpSharedSpace;\n+\n+  enum GenerationType {\n+    YoungGen,\n+    OldGen\n+  };\n+\n+private:\n+  DefNewGeneration* _young_gen;\n+  TenuredGeneration* _old_gen;\n+\n+private:\n+  \/\/ The singleton CardTable Remembered Set.\n+  CardTableRS* _rem_set;\n+\n+  SoftRefPolicy _soft_ref_policy;\n+\n+  GCPolicyCounters* _gc_policy_counters;\n+\n+  \/\/ Indicates that the most recent previous incremental collection failed.\n+  \/\/ The flag is cleared when an action is taken that might clear the\n+  \/\/ condition that caused that incremental collection to fail.\n+  bool _incremental_collection_failed;\n+\n+  \/\/ In support of ExplicitGCInvokesConcurrent functionality\n+  unsigned int _full_collections_completed;\n+\n+  \/\/ Collects the given generation.\n+  void collect_generation(Generation* gen, bool full, size_t size, bool is_tlab,\n+                          bool run_verification, bool clear_soft_refs);\n+\n+  \/\/ Reserve aligned space for the heap as needed by the contained generations.\n+  ReservedHeapSpace allocate(size_t alignment);\n+\n+  PreGenGCValues get_pre_gc_values() const;\n+\n+private:\n+  GCMemoryManager* _young_manager;\n+  GCMemoryManager* _old_manager;\n+\n+  \/\/ Helper functions for allocation\n+  HeapWord* attempt_allocation(size_t size,\n+                               bool   is_tlab,\n+                               bool   first_only);\n+\n+  \/\/ Helper function for two callbacks below.\n+  \/\/ Considers collection of the first max_level+1 generations.\n+  void do_collection(bool           full,\n+                     bool           clear_all_soft_refs,\n+                     size_t         size,\n+                     bool           is_tlab,\n+                     GenerationType max_generation);\n+\n+  \/\/ Callback from VM_GenCollectForAllocation operation.\n+  \/\/ This function does everything necessary\/possible to satisfy an\n+  \/\/ allocation request that failed in the youngest generation that should\n+  \/\/ have handled it (including collection, expansion, etc.)\n+  HeapWord* satisfy_failed_allocation(size_t size, bool is_tlab);\n+\n+  \/\/ Callback from VM_GenCollectFull operation.\n+  \/\/ Perform a full collection of the first max_level+1 generations.\n+  void do_full_collection(bool clear_all_soft_refs) override;\n+  void do_full_collection(bool clear_all_soft_refs, GenerationType max_generation);\n+\n+  \/\/ Does the \"cause\" of GC indicate that\n+  \/\/ we absolutely __must__ clear soft refs?\n+  bool must_clear_all_soft_refs();\n+\n+public:\n+  \/\/ Returns JNI_OK on success\n+  jint initialize() override;\n+  virtual CardTableRS* create_rem_set(const MemRegion& reserved_region);\n+\n+  \/\/ Does operations required after initialization has been done.\n+  void post_initialize() override;\n+\n+  bool is_young_gen(const Generation* gen) const { return gen == _young_gen; }\n+  bool is_old_gen(const Generation* gen) const { return gen == _old_gen; }\n+\n+  MemRegion reserved_region() const { return _reserved; }\n+  bool is_in_reserved(const void* addr) const { return _reserved.contains(addr); }\n+\n+  SoftRefPolicy* soft_ref_policy() override { return &_soft_ref_policy; }\n+\n+  \/\/ Performance Counter support\n+  GCPolicyCounters* counters()     { return _gc_policy_counters; }\n+\n+  size_t capacity() const override;\n+  size_t used() const override;\n+\n+  \/\/ Save the \"used_region\" for both generations.\n+  void save_used_regions();\n+\n+  size_t max_capacity() const override;\n+\n+  HeapWord* mem_allocate(size_t size, bool*  gc_overhead_limit_was_exceeded) override;\n+\n+  \/\/ Perform a full collection of the heap; intended for use in implementing\n+  \/\/ \"System.gc\". This implies as full a collection as the CollectedHeap\n+  \/\/ supports. Caller does not hold the Heap_lock on entry.\n+  void collect(GCCause::Cause cause) override;\n+\n+  \/\/ Returns \"TRUE\" iff \"p\" points into the committed areas of the heap.\n+  \/\/ The methods is_in() and is_in_youngest() may be expensive to compute\n+  \/\/ in general, so, to prevent their inadvertent use in product jvm's, we\n+  \/\/ restrict their use to assertion checking or verification only.\n+  bool is_in(const void* p) const override;\n+\n+  \/\/ Returns true if p points into the reserved space for the young generation.\n+  \/\/ Assumes the young gen address range is less than that of the old gen.\n+  bool is_in_young(const void* p) const;\n+\n+  bool requires_barriers(stackChunkOop obj) const override;\n+\n+#ifdef ASSERT\n+  bool is_in_partial_collection(const void* p);\n+#endif\n+\n+  \/\/ Optimized nmethod scanning support routines\n+  void register_nmethod(nmethod* nm) override;\n+  void unregister_nmethod(nmethod* nm) override;\n+  void verify_nmethod(nmethod* nm) override;\n+\n+  void prune_scavengable_nmethods();\n+\n+  \/\/ Iteration functions.\n+  void object_iterate(ObjectClosure* cl) override;\n+\n+  \/\/ A CollectedHeap is divided into a dense sequence of \"blocks\"; that is,\n+  \/\/ each address in the (reserved) heap is a member of exactly\n+  \/\/ one block.  The defining characteristic of a block is that it is\n+  \/\/ possible to find its size, and thus to progress forward to the next\n+  \/\/ block.  (Blocks may be of different sizes.)  Thus, blocks may\n+  \/\/ represent Java objects, or they might be free blocks in a\n+  \/\/ free-list-based heap (or subheap), as long as the two kinds are\n+  \/\/ distinguishable and the size of each is determinable.\n+\n+  \/\/ Returns the address of the start of the \"block\" that contains the\n+  \/\/ address \"addr\".  We say \"blocks\" instead of \"object\" since some heaps\n+  \/\/ may not pack objects densely; a chunk may either be an object or a\n+  \/\/ non-object.\n+  HeapWord* block_start(const void* addr) const;\n+\n+  \/\/ Requires \"addr\" to be the start of a block, and returns \"TRUE\" iff\n+  \/\/ the block is an object. Assumes (and verifies in non-product\n+  \/\/ builds) that addr is in the allocated part of the heap and is\n+  \/\/ the start of a chunk.\n+  bool block_is_obj(const HeapWord* addr) const;\n+\n+  \/\/ Section on TLAB's.\n+  size_t tlab_capacity(Thread* thr) const override;\n+  size_t tlab_used(Thread* thr) const override;\n+  size_t unsafe_max_tlab_alloc(Thread* thr) const override;\n+  HeapWord* allocate_new_tlab(size_t min_size,\n+                              size_t requested_size,\n+                              size_t* actual_size) override;\n+\n+  \/\/ Total number of full collections completed.\n+  unsigned int total_full_collections_completed() {\n+    assert(_full_collections_completed <= _total_full_collections,\n+           \"Can't complete more collections than were started\");\n+    return _full_collections_completed;\n+  }\n+\n+  \/\/ Update above counter, as appropriate, at the end of a stop-world GC cycle\n+  unsigned int update_full_collections_completed();\n+\n+  \/\/ Update the gc statistics for each generation.\n+  void update_gc_stats(Generation* current_generation, bool full) {\n+    _old_gen->update_gc_stats(current_generation, full);\n+  }\n+\n+  bool no_gc_in_progress() { return !is_gc_active(); }\n+\n+  void prepare_for_verify() override;\n+  void verify(VerifyOption option) override;\n+\n+  void print_on(outputStream* st) const override;\n+  void gc_threads_do(ThreadClosure* tc) const override;\n+  void print_tracing_info() const override;\n+\n+  \/\/ Used to print information about locations in the hs_err file.\n+  bool print_location(outputStream* st, void* addr) const override;\n+\n+  void print_heap_change(const PreGenGCValues& pre_gc_values) const;\n+\n+  \/\/ The functions below are helper functions that a subclass of\n+  \/\/ \"CollectedHeap\" can use in the implementation of its virtual\n+  \/\/ functions.\n+\n+  class GenClosure : public StackObj {\n+   public:\n+    virtual void do_generation(Generation* gen) = 0;\n+  };\n+\n+  \/\/ Apply \"cl.do_generation\" to all generations in the heap\n+  \/\/ If \"old_to_young\" determines the order.\n+  void generation_iterate(GenClosure* cl, bool old_to_young);\n+\n+  \/\/ Return \"true\" if all generations have reached the\n+  \/\/ maximal committed limit that they can reach, without a garbage\n+  \/\/ collection.\n+  virtual bool is_maximal_no_gc() const override;\n+\n+  \/\/ This function returns the CardTableRS object that allows us to scan\n+  \/\/ generations in a fully generational heap.\n+  CardTableRS* rem_set() { return _rem_set; }\n+\n+  \/\/ The ScanningOption determines which of the roots\n+  \/\/ the closure is applied to:\n+  \/\/ \"SO_None\" does none;\n+  enum ScanningOption {\n+    SO_None                =  0x0,\n+    SO_AllCodeCache        =  0x8,\n+    SO_ScavengeCodeCache   = 0x10\n+  };\n+\n+ protected:\n+  virtual void gc_prologue(bool full);\n+  virtual void gc_epilogue(bool full);\n+\n+ public:\n+  \/\/ Apply closures on various roots in Young GC or marking\/adjust phases of Full GC.\n+  void process_roots(ScanningOption so,\n+                     OopClosure* strong_roots,\n+                     CLDClosure* strong_cld_closure,\n+                     CLDClosure* weak_cld_closure,\n+                     CodeBlobToOopClosure* code_roots);\n+\n+  \/\/ Apply \"root_closure\" to all the weak roots of the system.\n+  \/\/ These include JNI weak roots, string table,\n+  \/\/ and referents of reachable weak refs.\n+  void gen_process_weak_roots(OopClosure* root_closure);\n+\n+  \/\/ Set the saved marks of generations, if that makes sense.\n+  \/\/ In particular, if any generation might iterate over the oops\n+  \/\/ in other generations, it should call this method.\n+  void save_marks();\n+\n+  \/\/ Returns \"true\" iff no allocations have occurred since the last\n+  \/\/ call to \"save_marks\".\n+  bool no_allocs_since_save_marks();\n+\n+  \/\/ Returns true if an incremental collection is likely to fail.\n+  \/\/ We optionally consult the young gen, if asked to do so;\n+  \/\/ otherwise we base our answer on whether the previous incremental\n+  \/\/ collection attempt failed with no corrective action as of yet.\n+  bool incremental_collection_will_fail(bool consult_young) {\n+    \/\/ The first disjunct remembers if an incremental collection failed, even\n+    \/\/ when we thought (second disjunct) that it would not.\n+    return incremental_collection_failed() ||\n+           (consult_young && !_young_gen->collection_attempt_is_safe());\n+  }\n+\n+  \/\/ If a generation bails out of an incremental collection,\n+  \/\/ it sets this flag.\n+  bool incremental_collection_failed() const {\n+    return _incremental_collection_failed;\n+  }\n+  void set_incremental_collection_failed() {\n+    _incremental_collection_failed = true;\n+  }\n+  void clear_incremental_collection_failed() {\n+    _incremental_collection_failed = false;\n+  }\n+\n+private:\n+  \/\/ Return true if an allocation should be attempted in the older generation\n+  \/\/ if it fails in the younger generation.  Return false, otherwise.\n+  bool should_try_older_generation_allocation(size_t word_size) const;\n+\n+  \/\/ Try to allocate space by expanding the heap.\n+  HeapWord* expand_heap_and_allocate(size_t size, bool is_tlab);\n+\n+  HeapWord* mem_allocate_work(size_t size,\n+                              bool is_tlab);\n+\n+#if INCLUDE_SERIALGC\n+  \/\/ For use by mark-sweep.  As implemented, mark-sweep-compact is global\n+  \/\/ in an essential way: compaction is performed across generations, by\n+  \/\/ iterating over spaces.\n+  void prepare_for_compaction();\n+#endif\n+\n+  \/\/ Save the tops of the spaces in all generations\n+  void record_gen_tops_before_GC() PRODUCT_RETURN;\n+\n+  \/\/ Return true if we need to perform full collection.\n+  bool should_do_full_collection(size_t size, bool full,\n+                                 bool is_tlab, GenerationType max_gen) const;\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.hpp","additions":312,"deletions":4,"binary":false,"changes":316,"status":"modified"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/serial\/serialVMOperations.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+\n+void VM_GenCollectForAllocation::doit() {\n+  SvcGCMarker sgcm(SvcGCMarker::MINOR);\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+  GCCauseSetter gccs(gch, _gc_cause);\n+  _result = gch->satisfy_failed_allocation(_word_size, _tlab);\n+  assert(_result == nullptr || gch->is_in_reserved(_result), \"result not in heap\");\n+\n+  if (_result == nullptr && GCLocker::is_active_and_needs_gc()) {\n+    set_gc_locked();\n+  }\n+}\n+\n+void VM_GenCollectFull::doit() {\n+  SvcGCMarker sgcm(SvcGCMarker::FULL);\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+  GCCauseSetter gccs(gch, _gc_cause);\n+  gch->do_full_collection(gch->must_clear_all_soft_refs(), _max_generation);\n+}\n","filename":"src\/hotspot\/share\/gc\/serial\/serialVMOperations.cpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,66 @@\n+\/*\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SERIAL_SERIALVMOPERATIONS_HPP\n+#define SHARE_GC_SERIAL_SERIALVMOPERATIONS_HPP\n+\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/serial\/serialHeap.hpp\"\n+\n+class VM_GenCollectForAllocation : public VM_CollectForAllocation {\n+ private:\n+  bool        _tlab;                       \/\/ alloc is of a tlab.\n+ public:\n+  VM_GenCollectForAllocation(size_t word_size,\n+                             bool tlab,\n+                             uint gc_count_before)\n+    : VM_CollectForAllocation(word_size, gc_count_before, GCCause::_allocation_failure),\n+      _tlab(tlab) {\n+    assert(word_size != 0, \"An allocation should always be requested with this operation.\");\n+  }\n+  ~VM_GenCollectForAllocation()  {}\n+  virtual VMOp_Type type() const { return VMOp_GenCollectForAllocation; }\n+  virtual void doit();\n+};\n+\n+\/\/ VM operation to invoke a collection of the heap as a\n+\/\/ SerialHeap heap.\n+class VM_GenCollectFull: public VM_GC_Operation {\n+ private:\n+  SerialHeap::GenerationType _max_generation;\n+ public:\n+  VM_GenCollectFull(uint gc_count_before,\n+                    uint full_gc_count_before,\n+                    GCCause::Cause gc_cause,\n+                    SerialHeap::GenerationType max_generation)\n+    : VM_GC_Operation(gc_count_before, gc_cause, full_gc_count_before,\n+                      max_generation != SerialHeap::YoungGen \/* full *\/),\n+      _max_generation(max_generation) { }\n+  ~VM_GenCollectFull() {}\n+  virtual VMOp_Type type() const { return VMOp_GenCollectFull; }\n+  virtual void doit();\n+};\n+\n+\n+#endif \/\/ SHARE_GC_SERIAL_SERIALVMOPERATIONS_HPP\n","filename":"src\/hotspot\/share\/gc\/serial\/serialVMOperations.hpp","additions":66,"deletions":0,"binary":false,"changes":66,"status":"added"},{"patch":"@@ -56,1 +56,4 @@\n-  nonstatic_field(TenuredSpace,                      _offsets,               SerialBlockOffsetTable)\n+  nonstatic_field(TenuredSpace,                      _offsets,               SerialBlockOffsetTable)        \\\n+                                                                                                            \\\n+  nonstatic_field(SerialHeap,                        _young_gen,             DefNewGeneration*)             \\\n+  nonstatic_field(SerialHeap,                        _old_gen,               TenuredGeneration*)            \\\n@@ -61,1 +64,1 @@\n-  declare_type(SerialHeap,                   GenCollectedHeap)                \\\n+  declare_type(SerialHeap,                   CollectedHeap)                   \\\n","filename":"src\/hotspot\/share\/gc\/serial\/vmStructs_serial.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-\/\/     GenCollectedHeap(DefNew,Tenured) and\n+\/\/     SerialHeap(DefNew,Tenured) and\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTableBarrierSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -87,2 +87,1 @@\n-\/\/   GenCollectedHeap\n-\/\/     SerialHeap\n+\/\/   SerialHeap\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/softRefPolicy.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -197,22 +197,0 @@\n-\n-void VM_GenCollectForAllocation::doit() {\n-  SvcGCMarker sgcm(SvcGCMarker::MINOR);\n-\n-  GenCollectedHeap* gch = GenCollectedHeap::heap();\n-  GCCauseSetter gccs(gch, _gc_cause);\n-  _result = gch->satisfy_failed_allocation(_word_size, _tlab);\n-  assert(_result == nullptr || gch->is_in_reserved(_result), \"result not in heap\");\n-\n-  if (_result == nullptr && GCLocker::is_active_and_needs_gc()) {\n-    set_gc_locked();\n-  }\n-}\n-\n-void VM_GenCollectFull::doit() {\n-  SvcGCMarker sgcm(SvcGCMarker::FULL);\n-\n-  GenCollectedHeap* gch = GenCollectedHeap::heap();\n-  GCCauseSetter gccs(gch, _gc_cause);\n-  gch->do_full_collection(gch->must_clear_all_soft_refs(), _max_generation);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/gcVMOperations.cpp","additions":1,"deletions":23,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n@@ -195,15 +195,0 @@\n-class VM_GenCollectForAllocation : public VM_CollectForAllocation {\n- private:\n-  bool        _tlab;                       \/\/ alloc is of a tlab.\n- public:\n-  VM_GenCollectForAllocation(size_t word_size,\n-                             bool tlab,\n-                             uint gc_count_before)\n-    : VM_CollectForAllocation(word_size, gc_count_before, GCCause::_allocation_failure),\n-      _tlab(tlab) {\n-    assert(word_size != 0, \"An allocation should always be requested with this operation.\");\n-  }\n-  ~VM_GenCollectForAllocation()  {}\n-  virtual VMOp_Type type() const { return VMOp_GenCollectForAllocation; }\n-  virtual void doit();\n-};\n@@ -211,17 +196,0 @@\n-\/\/ VM operation to invoke a collection of the heap as a\n-\/\/ GenCollectedHeap heap.\n-class VM_GenCollectFull: public VM_GC_Operation {\n- private:\n-  GenCollectedHeap::GenerationType _max_generation;\n- public:\n-  VM_GenCollectFull(uint gc_count_before,\n-                    uint full_gc_count_before,\n-                    GCCause::Cause gc_cause,\n-                    GenCollectedHeap::GenerationType max_generation)\n-    : VM_GC_Operation(gc_count_before, gc_cause, full_gc_count_before,\n-                      max_generation != GenCollectedHeap::YoungGen \/* full *\/),\n-      _max_generation(max_generation) { }\n-  ~VM_GenCollectFull() {}\n-  virtual VMOp_Type type() const { return VMOp_GenCollectFull; }\n-  virtual void doit();\n-};\n","filename":"src\/hotspot\/share\/gc\/shared\/gcVMOperations.hpp","additions":1,"deletions":33,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -1,1055 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"classfile\/classLoaderDataGraph.hpp\"\n-#include \"classfile\/stringTable.hpp\"\n-#include \"classfile\/symbolTable.hpp\"\n-#include \"classfile\/vmSymbols.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"code\/icBuffer.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/serial\/cardTableRS.hpp\"\n-#include \"gc\/serial\/defNewGeneration.hpp\"\n-#include \"gc\/serial\/genMarkSweep.hpp\"\n-#include \"gc\/serial\/markSweep.hpp\"\n-#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n-#include \"gc\/shared\/collectedHeap.inline.hpp\"\n-#include \"gc\/shared\/collectorCounters.hpp\"\n-#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n-#include \"gc\/shared\/gcId.hpp\"\n-#include \"gc\/shared\/gcInitLogger.hpp\"\n-#include \"gc\/shared\/gcLocker.hpp\"\n-#include \"gc\/shared\/gcPolicyCounters.hpp\"\n-#include \"gc\/shared\/gcTrace.hpp\"\n-#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n-#include \"gc\/shared\/gcVMOperations.hpp\"\n-#include \"gc\/shared\/genArguments.hpp\"\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n-#include \"gc\/shared\/generationSpec.hpp\"\n-#include \"gc\/shared\/locationPrinter.inline.hpp\"\n-#include \"gc\/shared\/oopStorage.inline.hpp\"\n-#include \"gc\/shared\/oopStorageParState.inline.hpp\"\n-#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n-#include \"gc\/shared\/scavengableNMethods.hpp\"\n-#include \"gc\/shared\/space.hpp\"\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n-#include \"gc\/shared\/weakProcessor.hpp\"\n-#include \"gc\/shared\/workerThread.hpp\"\n-#include \"memory\/iterator.hpp\"\n-#include \"memory\/metaspaceCounters.hpp\"\n-#include \"memory\/metaspaceUtils.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/handles.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/java.hpp\"\n-#include \"runtime\/threads.hpp\"\n-#include \"runtime\/vmThread.hpp\"\n-#include \"services\/memoryService.hpp\"\n-#include \"utilities\/autoRestore.hpp\"\n-#include \"utilities\/debug.hpp\"\n-#include \"utilities\/formatBuffer.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n-#include \"utilities\/vmError.hpp\"\n-#if INCLUDE_JVMCI\n-#include \"jvmci\/jvmci.hpp\"\n-#endif\n-\n-GenCollectedHeap::GenCollectedHeap(Generation::Name young,\n-                                   Generation::Name old,\n-                                   const char* policy_counters_name) :\n-  CollectedHeap(),\n-  _young_gen(nullptr),\n-  _old_gen(nullptr),\n-  _young_gen_spec(new GenerationSpec(young,\n-                                     NewSize,\n-                                     MaxNewSize,\n-                                     GenAlignment)),\n-  _old_gen_spec(new GenerationSpec(old,\n-                                   OldSize,\n-                                   MaxOldSize,\n-                                   GenAlignment)),\n-  _rem_set(nullptr),\n-  _soft_ref_policy(),\n-  _gc_policy_counters(new GCPolicyCounters(policy_counters_name, 2, 2)),\n-  _incremental_collection_failed(false),\n-  _full_collections_completed(0),\n-  _young_manager(nullptr),\n-  _old_manager(nullptr) {\n-}\n-\n-jint GenCollectedHeap::initialize() {\n-  \/\/ Allocate space for the heap.\n-\n-  ReservedHeapSpace heap_rs = allocate(HeapAlignment);\n-\n-  if (!heap_rs.is_reserved()) {\n-    vm_shutdown_during_initialization(\n-      \"Could not reserve enough space for object heap\");\n-    return JNI_ENOMEM;\n-  }\n-\n-  initialize_reserved_region(heap_rs);\n-\n-  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n-  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n-\n-  _rem_set = create_rem_set(heap_rs.region());\n-  _rem_set->initialize(young_rs.base(), old_rs.base());\n-\n-  CardTableBarrierSet *bs = new CardTableBarrierSet(_rem_set);\n-  bs->initialize();\n-  BarrierSet::set_barrier_set(bs);\n-\n-  _young_gen = _young_gen_spec->init(young_rs, rem_set());\n-  _old_gen = _old_gen_spec->init(old_rs, rem_set());\n-\n-  GCInitLogger::print();\n-\n-  return JNI_OK;\n-}\n-\n-CardTableRS* GenCollectedHeap::create_rem_set(const MemRegion& reserved_region) {\n-  return new CardTableRS(reserved_region);\n-}\n-\n-ReservedHeapSpace GenCollectedHeap::allocate(size_t alignment) {\n-  \/\/ Now figure out the total size.\n-  const size_t pageSize = UseLargePages ? os::large_page_size() : os::vm_page_size();\n-  assert(alignment % pageSize == 0, \"Must be\");\n-\n-  \/\/ Check for overflow.\n-  size_t total_reserved = _young_gen_spec->max_size() + _old_gen_spec->max_size();\n-  if (total_reserved < _young_gen_spec->max_size()) {\n-    vm_exit_during_initialization(\"The size of the object heap + VM data exceeds \"\n-                                  \"the maximum representable size\");\n-  }\n-  assert(total_reserved % alignment == 0,\n-         \"Gen size; total_reserved=\" SIZE_FORMAT \", alignment=\"\n-         SIZE_FORMAT, total_reserved, alignment);\n-\n-  ReservedHeapSpace heap_rs = Universe::reserve_heap(total_reserved, alignment);\n-  size_t used_page_size = heap_rs.page_size();\n-\n-  os::trace_page_sizes(\"Heap\",\n-                       MinHeapSize,\n-                       total_reserved,\n-                       heap_rs.base(),\n-                       heap_rs.size(),\n-                       used_page_size);\n-\n-  return heap_rs;\n-}\n-\n-class GenIsScavengable : public BoolObjectClosure {\n-public:\n-  bool do_object_b(oop obj) {\n-    return GenCollectedHeap::heap()->is_in_young(obj);\n-  }\n-};\n-\n-static GenIsScavengable _is_scavengable;\n-\n-void GenCollectedHeap::post_initialize() {\n-  CollectedHeap::post_initialize();\n-\n-  DefNewGeneration* def_new_gen = (DefNewGeneration*)_young_gen;\n-\n-  def_new_gen->ref_processor_init();\n-\n-  MarkSweep::initialize();\n-\n-  ScavengableNMethods::initialize(&_is_scavengable);\n-}\n-\n-PreGenGCValues GenCollectedHeap::get_pre_gc_values() const {\n-  const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();\n-\n-  return PreGenGCValues(def_new_gen->used(),\n-                        def_new_gen->capacity(),\n-                        def_new_gen->eden()->used(),\n-                        def_new_gen->eden()->capacity(),\n-                        def_new_gen->from()->used(),\n-                        def_new_gen->from()->capacity(),\n-                        old_gen()->used(),\n-                        old_gen()->capacity());\n-}\n-\n-GenerationSpec* GenCollectedHeap::young_gen_spec() const {\n-  return _young_gen_spec;\n-}\n-\n-GenerationSpec* GenCollectedHeap::old_gen_spec() const {\n-  return _old_gen_spec;\n-}\n-\n-size_t GenCollectedHeap::capacity() const {\n-  return _young_gen->capacity() + _old_gen->capacity();\n-}\n-\n-size_t GenCollectedHeap::used() const {\n-  return _young_gen->used() + _old_gen->used();\n-}\n-\n-void GenCollectedHeap::save_used_regions() {\n-  _old_gen->save_used_region();\n-  _young_gen->save_used_region();\n-}\n-\n-size_t GenCollectedHeap::max_capacity() const {\n-  return _young_gen->max_capacity() + _old_gen->max_capacity();\n-}\n-\n-\/\/ Update the _full_collections_completed counter\n-\/\/ at the end of a stop-world full GC.\n-unsigned int GenCollectedHeap::update_full_collections_completed() {\n-  assert(_full_collections_completed <= _total_full_collections,\n-         \"Can't complete more collections than were started\");\n-  _full_collections_completed = _total_full_collections;\n-  return _full_collections_completed;\n-}\n-\n-\/\/ Return true if any of the following is true:\n-\/\/ . the allocation won't fit into the current young gen heap\n-\/\/ . gc locker is occupied (jni critical section)\n-\/\/ . heap memory is tight -- the most recent previous collection\n-\/\/   was a full collection because a partial collection (would\n-\/\/   have) failed and is likely to fail again\n-bool GenCollectedHeap::should_try_older_generation_allocation(size_t word_size) const {\n-  size_t young_capacity = _young_gen->capacity_before_gc();\n-  return    (word_size > heap_word_size(young_capacity))\n-         || GCLocker::is_active_and_needs_gc()\n-         || incremental_collection_failed();\n-}\n-\n-HeapWord* GenCollectedHeap::expand_heap_and_allocate(size_t size, bool   is_tlab) {\n-  HeapWord* result = nullptr;\n-  if (_old_gen->should_allocate(size, is_tlab)) {\n-    result = _old_gen->expand_and_allocate(size, is_tlab);\n-  }\n-  if (result == nullptr) {\n-    if (_young_gen->should_allocate(size, is_tlab)) {\n-      result = _young_gen->expand_and_allocate(size, is_tlab);\n-    }\n-  }\n-  assert(result == nullptr || is_in_reserved(result), \"result not in heap\");\n-  return result;\n-}\n-\n-HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,\n-                                              bool is_tlab) {\n-\n-  HeapWord* result = nullptr;\n-\n-  \/\/ Loop until the allocation is satisfied, or unsatisfied after GC.\n-  for (uint try_count = 1, gclocker_stalled_count = 0; \/* return or throw *\/; try_count += 1) {\n-\n-    \/\/ First allocation attempt is lock-free.\n-    Generation *young = _young_gen;\n-    if (young->should_allocate(size, is_tlab)) {\n-      result = young->par_allocate(size, is_tlab);\n-      if (result != nullptr) {\n-        assert(is_in_reserved(result), \"result not in heap\");\n-        return result;\n-      }\n-    }\n-    uint gc_count_before;  \/\/ Read inside the Heap_lock locked region.\n-    {\n-      MutexLocker ml(Heap_lock);\n-      log_trace(gc, alloc)(\"GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation\");\n-      \/\/ Note that only large objects get a shot at being\n-      \/\/ allocated in later generations.\n-      bool first_only = !should_try_older_generation_allocation(size);\n-\n-      result = attempt_allocation(size, is_tlab, first_only);\n-      if (result != nullptr) {\n-        assert(is_in_reserved(result), \"result not in heap\");\n-        return result;\n-      }\n-\n-      if (GCLocker::is_active_and_needs_gc()) {\n-        if (is_tlab) {\n-          return nullptr;  \/\/ Caller will retry allocating individual object.\n-        }\n-        if (!is_maximal_no_gc()) {\n-          \/\/ Try and expand heap to satisfy request.\n-          result = expand_heap_and_allocate(size, is_tlab);\n-          \/\/ Result could be null if we are out of space.\n-          if (result != nullptr) {\n-            return result;\n-          }\n-        }\n-\n-        if (gclocker_stalled_count > GCLockerRetryAllocationCount) {\n-          return nullptr; \/\/ We didn't get to do a GC and we didn't get any memory.\n-        }\n-\n-        \/\/ If this thread is not in a jni critical section, we stall\n-        \/\/ the requestor until the critical section has cleared and\n-        \/\/ GC allowed. When the critical section clears, a GC is\n-        \/\/ initiated by the last thread exiting the critical section; so\n-        \/\/ we retry the allocation sequence from the beginning of the loop,\n-        \/\/ rather than causing more, now probably unnecessary, GC attempts.\n-        JavaThread* jthr = JavaThread::current();\n-        if (!jthr->in_critical()) {\n-          MutexUnlocker mul(Heap_lock);\n-          \/\/ Wait for JNI critical section to be exited\n-          GCLocker::stall_until_clear();\n-          gclocker_stalled_count += 1;\n-          continue;\n-        } else {\n-          if (CheckJNICalls) {\n-            fatal(\"Possible deadlock due to allocating while\"\n-                  \" in jni critical section\");\n-          }\n-          return nullptr;\n-        }\n-      }\n-\n-      \/\/ Read the gc count while the heap lock is held.\n-      gc_count_before = total_collections();\n-    }\n-\n-    VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);\n-    VMThread::execute(&op);\n-    if (op.prologue_succeeded()) {\n-      result = op.result();\n-      if (op.gc_locked()) {\n-         assert(result == nullptr, \"must be null if gc_locked() is true\");\n-         continue;  \/\/ Retry and\/or stall as necessary.\n-      }\n-\n-      assert(result == nullptr || is_in_reserved(result),\n-             \"result not in heap\");\n-      return result;\n-    }\n-\n-    \/\/ Give a warning if we seem to be looping forever.\n-    if ((QueuedAllocationWarningCount > 0) &&\n-        (try_count % QueuedAllocationWarningCount == 0)) {\n-          log_warning(gc, ergo)(\"GenCollectedHeap::mem_allocate_work retries %d times,\"\n-                                \" size=\" SIZE_FORMAT \" %s\", try_count, size, is_tlab ? \"(TLAB)\" : \"\");\n-    }\n-  }\n-}\n-\n-HeapWord* GenCollectedHeap::attempt_allocation(size_t size,\n-                                               bool is_tlab,\n-                                               bool first_only) {\n-  HeapWord* res = nullptr;\n-\n-  if (_young_gen->should_allocate(size, is_tlab)) {\n-    res = _young_gen->allocate(size, is_tlab);\n-    if (res != nullptr || first_only) {\n-      return res;\n-    }\n-  }\n-\n-  if (_old_gen->should_allocate(size, is_tlab)) {\n-    res = _old_gen->allocate(size, is_tlab);\n-  }\n-\n-  return res;\n-}\n-\n-HeapWord* GenCollectedHeap::mem_allocate(size_t size,\n-                                         bool* gc_overhead_limit_was_exceeded) {\n-  return mem_allocate_work(size,\n-                           false \/* is_tlab *\/);\n-}\n-\n-bool GenCollectedHeap::must_clear_all_soft_refs() {\n-  return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||\n-         _gc_cause == GCCause::_wb_full_gc;\n-}\n-\n-void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,\n-                                          bool is_tlab, bool run_verification, bool clear_soft_refs) {\n-  FormatBuffer<> title(\"Collect gen: %s\", gen->short_name());\n-  GCTraceTime(Trace, gc, phases) t1(title);\n-  TraceCollectorStats tcs(gen->counters());\n-  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause(), heap()->is_young_gen(gen) ? \"end of minor GC\" : \"end of major GC\");\n-\n-  gen->stat_record()->invocations++;\n-  gen->stat_record()->accumulated_time.start();\n-\n-  \/\/ Must be done anew before each collection because\n-  \/\/ a previous collection will do mangling and will\n-  \/\/ change top of some spaces.\n-  record_gen_tops_before_GC();\n-\n-  log_trace(gc)(\"%s invoke=%d size=\" SIZE_FORMAT, heap()->is_young_gen(gen) ? \"Young\" : \"Old\", gen->stat_record()->invocations, size * HeapWordSize);\n-\n-  if (run_verification && VerifyBeforeGC) {\n-    Universe::verify(\"Before GC\");\n-  }\n-  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());\n-\n-  \/\/ Do collection work\n-  {\n-    save_marks();   \/\/ save marks for all gens\n-\n-    gen->collect(full, clear_soft_refs, size, is_tlab);\n-  }\n-\n-  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());\n-\n-  gen->stat_record()->accumulated_time.stop();\n-\n-  update_gc_stats(gen, full);\n-\n-  if (run_verification && VerifyAfterGC) {\n-    Universe::verify(\"After GC\");\n-  }\n-}\n-\n-void GenCollectedHeap::do_collection(bool           full,\n-                                     bool           clear_all_soft_refs,\n-                                     size_t         size,\n-                                     bool           is_tlab,\n-                                     GenerationType max_generation) {\n-  ResourceMark rm;\n-  DEBUG_ONLY(Thread* my_thread = Thread::current();)\n-\n-  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n-  assert(my_thread->is_VM_thread(), \"only VM thread\");\n-  assert(Heap_lock->is_locked(),\n-         \"the requesting thread should have the Heap_lock\");\n-  guarantee(!is_gc_active(), \"collection is not reentrant\");\n-\n-  if (GCLocker::check_active_before_gc()) {\n-    return; \/\/ GC is disabled (e.g. JNI GetXXXCritical operation)\n-  }\n-\n-  const bool do_clear_all_soft_refs = clear_all_soft_refs ||\n-                          soft_ref_policy()->should_clear_all_soft_refs();\n-\n-  ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());\n-\n-  AutoModifyRestore<bool> temporarily(_is_gc_active, true);\n-\n-  bool complete = full && (max_generation == OldGen);\n-  bool old_collects_young = complete && !ScavengeBeforeFullGC;\n-  bool do_young_collection = !old_collects_young && _young_gen->should_collect(full, size, is_tlab);\n-\n-  const PreGenGCValues pre_gc_values = get_pre_gc_values();\n-\n-  bool run_verification = total_collections() >= VerifyGCStartAt;\n-  bool prepared_for_verification = false;\n-  bool do_full_collection = false;\n-\n-  if (do_young_collection) {\n-    GCIdMark gc_id_mark;\n-    GCTraceCPUTime tcpu(((DefNewGeneration*)_young_gen)->gc_tracer());\n-    GCTraceTime(Info, gc) t(\"Pause Young\", nullptr, gc_cause(), true);\n-\n-    print_heap_before_gc();\n-\n-    if (run_verification && VerifyGCLevel <= 0 && VerifyBeforeGC) {\n-      prepare_for_verify();\n-      prepared_for_verification = true;\n-    }\n-\n-    gc_prologue(complete);\n-    increment_total_collections(complete);\n-\n-    collect_generation(_young_gen,\n-                       full,\n-                       size,\n-                       is_tlab,\n-                       run_verification && VerifyGCLevel <= 0,\n-                       do_clear_all_soft_refs);\n-\n-    if (size > 0 && (!is_tlab || _young_gen->supports_tlab_allocation()) &&\n-        size * HeapWordSize <= _young_gen->unsafe_max_alloc_nogc()) {\n-      \/\/ Allocation request was met by young GC.\n-      size = 0;\n-    }\n-\n-    \/\/ Ask if young collection is enough. If so, do the final steps for young collection,\n-    \/\/ and fallthrough to the end.\n-    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);\n-    if (!do_full_collection) {\n-      \/\/ Adjust generation sizes.\n-      _young_gen->compute_new_size();\n-\n-      print_heap_change(pre_gc_values);\n-\n-      \/\/ Track memory usage and detect low memory after GC finishes\n-      MemoryService::track_memory_usage();\n-\n-      gc_epilogue(complete);\n-    }\n-\n-    print_heap_after_gc();\n-\n-  } else {\n-    \/\/ No young collection, ask if we need to perform Full collection.\n-    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);\n-  }\n-\n-  if (do_full_collection) {\n-    GCIdMark gc_id_mark;\n-    GCTraceCPUTime tcpu(GenMarkSweep::gc_tracer());\n-    GCTraceTime(Info, gc) t(\"Pause Full\", nullptr, gc_cause(), true);\n-\n-    print_heap_before_gc();\n-\n-    if (!prepared_for_verification && run_verification &&\n-        VerifyGCLevel <= 1 && VerifyBeforeGC) {\n-      prepare_for_verify();\n-    }\n-\n-    if (!do_young_collection) {\n-      gc_prologue(complete);\n-      increment_total_collections(complete);\n-    }\n-\n-    \/\/ Accounting quirk: total full collections would be incremented when \"complete\"\n-    \/\/ is set, by calling increment_total_collections above. However, we also need to\n-    \/\/ account Full collections that had \"complete\" unset.\n-    if (!complete) {\n-      increment_total_full_collections();\n-    }\n-\n-    CodeCache::on_gc_marking_cycle_start();\n-\n-    collect_generation(_old_gen,\n-                       full,\n-                       size,\n-                       is_tlab,\n-                       run_verification && VerifyGCLevel <= 1,\n-                       do_clear_all_soft_refs);\n-\n-    CodeCache::on_gc_marking_cycle_finish();\n-    CodeCache::arm_all_nmethods();\n-\n-    \/\/ Adjust generation sizes.\n-    _old_gen->compute_new_size();\n-    _young_gen->compute_new_size();\n-\n-    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n-    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n-    DEBUG_ONLY(MetaspaceUtils::verify();)\n-\n-    \/\/ Need to clear claim bits for the next mark.\n-    ClassLoaderDataGraph::clear_claimed_marks();\n-\n-    \/\/ Resize the metaspace capacity after full collections\n-    MetaspaceGC::compute_new_size();\n-    update_full_collections_completed();\n-\n-    print_heap_change(pre_gc_values);\n-\n-    \/\/ Track memory usage and detect low memory after GC finishes\n-    MemoryService::track_memory_usage();\n-\n-    \/\/ Need to tell the epilogue code we are done with Full GC, regardless what was\n-    \/\/ the initial value for \"complete\" flag.\n-    gc_epilogue(true);\n-\n-    print_heap_after_gc();\n-  }\n-}\n-\n-bool GenCollectedHeap::should_do_full_collection(size_t size, bool full, bool is_tlab,\n-                                                 GenCollectedHeap::GenerationType max_gen) const {\n-  return max_gen == OldGen && _old_gen->should_collect(full, size, is_tlab);\n-}\n-\n-void GenCollectedHeap::register_nmethod(nmethod* nm) {\n-  ScavengableNMethods::register_nmethod(nm);\n-}\n-\n-void GenCollectedHeap::unregister_nmethod(nmethod* nm) {\n-  ScavengableNMethods::unregister_nmethod(nm);\n-}\n-\n-void GenCollectedHeap::verify_nmethod(nmethod* nm) {\n-  ScavengableNMethods::verify_nmethod(nm);\n-}\n-\n-void GenCollectedHeap::prune_scavengable_nmethods() {\n-  ScavengableNMethods::prune_nmethods();\n-}\n-\n-HeapWord* GenCollectedHeap::satisfy_failed_allocation(size_t size, bool is_tlab) {\n-  GCCauseSetter x(this, GCCause::_allocation_failure);\n-  HeapWord* result = nullptr;\n-\n-  assert(size != 0, \"Precondition violated\");\n-  if (GCLocker::is_active_and_needs_gc()) {\n-    \/\/ GC locker is active; instead of a collection we will attempt\n-    \/\/ to expand the heap, if there's room for expansion.\n-    if (!is_maximal_no_gc()) {\n-      result = expand_heap_and_allocate(size, is_tlab);\n-    }\n-    return result;   \/\/ Could be null if we are out of space.\n-  } else if (!incremental_collection_will_fail(false \/* don't consult_young *\/)) {\n-    \/\/ Do an incremental collection.\n-    do_collection(false,                     \/\/ full\n-                  false,                     \/\/ clear_all_soft_refs\n-                  size,                      \/\/ size\n-                  is_tlab,                   \/\/ is_tlab\n-                  GenCollectedHeap::OldGen); \/\/ max_generation\n-  } else {\n-    log_trace(gc)(\" :: Trying full because partial may fail :: \");\n-    \/\/ Try a full collection; see delta for bug id 6266275\n-    \/\/ for the original code and why this has been simplified\n-    \/\/ with from-space allocation criteria modified and\n-    \/\/ such allocation moved out of the safepoint path.\n-    do_collection(true,                      \/\/ full\n-                  false,                     \/\/ clear_all_soft_refs\n-                  size,                      \/\/ size\n-                  is_tlab,                   \/\/ is_tlab\n-                  GenCollectedHeap::OldGen); \/\/ max_generation\n-  }\n-\n-  result = attempt_allocation(size, is_tlab, false \/*first_only*\/);\n-\n-  if (result != nullptr) {\n-    assert(is_in_reserved(result), \"result not in heap\");\n-    return result;\n-  }\n-\n-  \/\/ OK, collection failed, try expansion.\n-  result = expand_heap_and_allocate(size, is_tlab);\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ If we reach this point, we're really out of memory. Try every trick\n-  \/\/ we can to reclaim memory. Force collection of soft references. Force\n-  \/\/ a complete compaction of the heap. Any additional methods for finding\n-  \/\/ free memory should be here, especially if they are expensive. If this\n-  \/\/ attempt fails, an OOM exception will be thrown.\n-  {\n-    UIntFlagSetting flag_change(MarkSweepAlwaysCompactCount, 1); \/\/ Make sure the heap is fully compacted\n-\n-    do_collection(true,                      \/\/ full\n-                  true,                      \/\/ clear_all_soft_refs\n-                  size,                      \/\/ size\n-                  is_tlab,                   \/\/ is_tlab\n-                  GenCollectedHeap::OldGen); \/\/ max_generation\n-  }\n-\n-  result = attempt_allocation(size, is_tlab, false \/* first_only *\/);\n-  if (result != nullptr) {\n-    assert(is_in_reserved(result), \"result not in heap\");\n-    return result;\n-  }\n-\n-  assert(!soft_ref_policy()->should_clear_all_soft_refs(),\n-    \"Flag should have been handled and cleared prior to this point\");\n-\n-  \/\/ What else?  We might try synchronous finalization later.  If the total\n-  \/\/ space available is large enough for the allocation, then a more\n-  \/\/ complete compaction phase than we've tried so far might be\n-  \/\/ appropriate.\n-  return nullptr;\n-}\n-\n-#ifdef ASSERT\n-class AssertNonScavengableClosure: public OopClosure {\n-public:\n-  virtual void do_oop(oop* p) {\n-    assert(!GenCollectedHeap::heap()->is_in_partial_collection(*p),\n-      \"Referent should not be scavengable.\");  }\n-  virtual void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n-};\n-static AssertNonScavengableClosure assert_is_non_scavengable_closure;\n-#endif\n-\n-void GenCollectedHeap::process_roots(ScanningOption so,\n-                                     OopClosure* strong_roots,\n-                                     CLDClosure* strong_cld_closure,\n-                                     CLDClosure* weak_cld_closure,\n-                                     CodeBlobToOopClosure* code_roots) {\n-  \/\/ General roots.\n-  assert(code_roots != nullptr, \"code root closure should always be set\");\n-\n-  ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);\n-\n-  \/\/ Only process code roots from thread stacks if we aren't visiting the entire CodeCache anyway\n-  CodeBlobToOopClosure* roots_from_code_p = (so & SO_AllCodeCache) ? nullptr : code_roots;\n-\n-  Threads::oops_do(strong_roots, roots_from_code_p);\n-\n-  OopStorageSet::strong_oops_do(strong_roots);\n-\n-  if (so & SO_ScavengeCodeCache) {\n-    assert(code_roots != nullptr, \"must supply closure for code cache\");\n-\n-    \/\/ We only visit parts of the CodeCache when scavenging.\n-    ScavengableNMethods::nmethods_do(code_roots);\n-  }\n-  if (so & SO_AllCodeCache) {\n-    assert(code_roots != nullptr, \"must supply closure for code cache\");\n-\n-    \/\/ CMSCollector uses this to do intermediate-strength collections.\n-    \/\/ We scan the entire code cache, since CodeCache::do_unloading is not called.\n-    CodeCache::blobs_do(code_roots);\n-  }\n-  \/\/ Verify that the code cache contents are not subject to\n-  \/\/ movement by a scavenging collection.\n-  DEBUG_ONLY(CodeBlobToOopClosure assert_code_is_non_scavengable(&assert_is_non_scavengable_closure, !CodeBlobToOopClosure::FixRelocations));\n-  DEBUG_ONLY(ScavengableNMethods::asserted_non_scavengable_nmethods_do(&assert_code_is_non_scavengable));\n-}\n-\n-void GenCollectedHeap::gen_process_weak_roots(OopClosure* root_closure) {\n-  WeakProcessor::oops_do(root_closure);\n-}\n-\n-bool GenCollectedHeap::no_allocs_since_save_marks() {\n-  return _young_gen->no_allocs_since_save_marks() &&\n-         _old_gen->no_allocs_since_save_marks();\n-}\n-\n-\/\/ public collection interfaces\n-void GenCollectedHeap::collect(GCCause::Cause cause) {\n-  \/\/ The caller doesn't have the Heap_lock\n-  assert(!Heap_lock->owned_by_self(), \"this thread should not own the Heap_lock\");\n-\n-  unsigned int gc_count_before;\n-  unsigned int full_gc_count_before;\n-\n-  {\n-    MutexLocker ml(Heap_lock);\n-    \/\/ Read the GC count while holding the Heap_lock\n-    gc_count_before      = total_collections();\n-    full_gc_count_before = total_full_collections();\n-  }\n-\n-  if (GCLocker::should_discard(cause, gc_count_before)) {\n-    return;\n-  }\n-\n-  bool should_run_young_gc =  (cause == GCCause::_wb_young_gc)\n-                           || (cause == GCCause::_gc_locker)\n-                DEBUG_ONLY(|| (cause == GCCause::_scavenge_alot));\n-\n-  const GenerationType max_generation = should_run_young_gc\n-                                      ? YoungGen\n-                                      : OldGen;\n-\n-  while (true) {\n-    VM_GenCollectFull op(gc_count_before, full_gc_count_before,\n-                        cause, max_generation);\n-    VMThread::execute(&op);\n-\n-    if (!GCCause::is_explicit_full_gc(cause)) {\n-      return;\n-    }\n-\n-    {\n-      MutexLocker ml(Heap_lock);\n-      \/\/ Read the GC count while holding the Heap_lock\n-      if (full_gc_count_before != total_full_collections()) {\n-        return;\n-      }\n-    }\n-\n-    if (GCLocker::is_active_and_needs_gc()) {\n-      \/\/ If GCLocker is active, wait until clear before retrying.\n-      GCLocker::stall_until_clear();\n-    }\n-  }\n-}\n-\n-void GenCollectedHeap::do_full_collection(bool clear_all_soft_refs) {\n-   do_full_collection(clear_all_soft_refs, OldGen);\n-}\n-\n-void GenCollectedHeap::do_full_collection(bool clear_all_soft_refs,\n-                                          GenerationType last_generation) {\n-  do_collection(true,                   \/\/ full\n-                clear_all_soft_refs,    \/\/ clear_all_soft_refs\n-                0,                      \/\/ size\n-                false,                  \/\/ is_tlab\n-                last_generation);       \/\/ last_generation\n-  \/\/ Hack XXX FIX ME !!!\n-  \/\/ A scavenge may not have been attempted, or may have\n-  \/\/ been attempted and failed, because the old gen was too full\n-  if (gc_cause() == GCCause::_gc_locker && incremental_collection_failed()) {\n-    log_debug(gc, jni)(\"GC locker: Trying a full collection because scavenge failed\");\n-    \/\/ This time allow the old gen to be collected as well\n-    do_collection(true,                \/\/ full\n-                  clear_all_soft_refs, \/\/ clear_all_soft_refs\n-                  0,                   \/\/ size\n-                  false,               \/\/ is_tlab\n-                  OldGen);             \/\/ last_generation\n-  }\n-}\n-\n-bool GenCollectedHeap::is_in_young(const void* p) const {\n-  bool result = p < _old_gen->reserved().start();\n-  assert(result == _young_gen->is_in_reserved(p),\n-         \"incorrect test - result=%d, p=\" PTR_FORMAT, result, p2i(p));\n-  return result;\n-}\n-\n-bool GenCollectedHeap::requires_barriers(stackChunkOop obj) const {\n-  return !is_in_young(obj);\n-}\n-\n-\/\/ Returns \"TRUE\" iff \"p\" points into the committed areas of the heap.\n-bool GenCollectedHeap::is_in(const void* p) const {\n-  return _young_gen->is_in(p) || _old_gen->is_in(p);\n-}\n-\n-#ifdef ASSERT\n-\/\/ Don't implement this by using is_in_young().  This method is used\n-\/\/ in some cases to check that is_in_young() is correct.\n-bool GenCollectedHeap::is_in_partial_collection(const void* p) {\n-  assert(is_in_reserved(p) || p == nullptr,\n-    \"Does not work if address is non-null and outside of the heap\");\n-  return p < _young_gen->reserved().end() && p != nullptr;\n-}\n-#endif\n-\n-void GenCollectedHeap::object_iterate(ObjectClosure* cl) {\n-  _young_gen->object_iterate(cl);\n-  _old_gen->object_iterate(cl);\n-}\n-\n-HeapWord* GenCollectedHeap::block_start(const void* addr) const {\n-  assert(is_in_reserved(addr), \"block_start of address outside of heap\");\n-  if (_young_gen->is_in_reserved(addr)) {\n-    assert(_young_gen->is_in(addr), \"addr should be in allocated part of generation\");\n-    return _young_gen->block_start(addr);\n-  }\n-\n-  assert(_old_gen->is_in_reserved(addr), \"Some generation should contain the address\");\n-  assert(_old_gen->is_in(addr), \"addr should be in allocated part of generation\");\n-  return _old_gen->block_start(addr);\n-}\n-\n-bool GenCollectedHeap::block_is_obj(const HeapWord* addr) const {\n-  assert(is_in_reserved(addr), \"block_is_obj of address outside of heap\");\n-  assert(block_start(addr) == addr, \"addr must be a block start\");\n-  if (_young_gen->is_in_reserved(addr)) {\n-    return _young_gen->block_is_obj(addr);\n-  }\n-\n-  assert(_old_gen->is_in_reserved(addr), \"Some generation should contain the address\");\n-  return _old_gen->block_is_obj(addr);\n-}\n-\n-size_t GenCollectedHeap::tlab_capacity(Thread* thr) const {\n-  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n-  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n-  return _young_gen->tlab_capacity();\n-}\n-\n-size_t GenCollectedHeap::tlab_used(Thread* thr) const {\n-  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n-  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n-  return _young_gen->tlab_used();\n-}\n-\n-size_t GenCollectedHeap::unsafe_max_tlab_alloc(Thread* thr) const {\n-  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n-  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n-  return _young_gen->unsafe_max_tlab_alloc();\n-}\n-\n-HeapWord* GenCollectedHeap::allocate_new_tlab(size_t min_size,\n-                                              size_t requested_size,\n-                                              size_t* actual_size) {\n-  HeapWord* result = mem_allocate_work(requested_size \/* size *\/,\n-                                       true \/* is_tlab *\/);\n-  if (result != nullptr) {\n-    *actual_size = requested_size;\n-  }\n-\n-  return result;\n-}\n-\n-void GenCollectedHeap::prepare_for_verify() {\n-  ensure_parsability(false);        \/\/ no need to retire TLABs\n-}\n-\n-void GenCollectedHeap::generation_iterate(GenClosure* cl,\n-                                          bool old_to_young) {\n-  if (old_to_young) {\n-    cl->do_generation(_old_gen);\n-    cl->do_generation(_young_gen);\n-  } else {\n-    cl->do_generation(_young_gen);\n-    cl->do_generation(_old_gen);\n-  }\n-}\n-\n-bool GenCollectedHeap::is_maximal_no_gc() const {\n-  return _young_gen->is_maximal_no_gc() && _old_gen->is_maximal_no_gc();\n-}\n-\n-void GenCollectedHeap::save_marks() {\n-  _young_gen->save_marks();\n-  _old_gen->save_marks();\n-}\n-\n-GenCollectedHeap* GenCollectedHeap::heap() {\n-  \/\/ SerialHeap is the only subtype of GenCollectedHeap.\n-  return named_heap<GenCollectedHeap>(CollectedHeap::Serial);\n-}\n-\n-#if INCLUDE_SERIALGC\n-void GenCollectedHeap::prepare_for_compaction() {\n-  \/\/ Start by compacting into same gen.\n-  CompactPoint cp(_old_gen);\n-  _old_gen->prepare_for_compaction(&cp);\n-  _young_gen->prepare_for_compaction(&cp);\n-}\n-#endif \/\/ INCLUDE_SERIALGC\n-\n-void GenCollectedHeap::verify(VerifyOption option \/* ignored *\/) {\n-  log_debug(gc, verify)(\"%s\", _old_gen->name());\n-  _old_gen->verify();\n-\n-  log_debug(gc, verify)(\"%s\", _young_gen->name());\n-  _young_gen->verify();\n-\n-  log_debug(gc, verify)(\"RemSet\");\n-  rem_set()->verify();\n-}\n-\n-void GenCollectedHeap::print_on(outputStream* st) const {\n-  if (_young_gen != nullptr) {\n-    _young_gen->print_on(st);\n-  }\n-  if (_old_gen != nullptr) {\n-    _old_gen->print_on(st);\n-  }\n-  MetaspaceUtils::print_on(st);\n-}\n-\n-void GenCollectedHeap::gc_threads_do(ThreadClosure* tc) const {\n-}\n-\n-bool GenCollectedHeap::print_location(outputStream* st, void* addr) const {\n-  return BlockLocationPrinter<GenCollectedHeap>::print_location(st, addr);\n-}\n-\n-void GenCollectedHeap::print_tracing_info() const {\n-  if (log_is_enabled(Debug, gc, heap, exit)) {\n-    LogStreamHandle(Debug, gc, heap, exit) lsh;\n-    _young_gen->print_summary_info_on(&lsh);\n-    _old_gen->print_summary_info_on(&lsh);\n-  }\n-}\n-\n-void GenCollectedHeap::print_heap_change(const PreGenGCValues& pre_gc_values) const {\n-  const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();\n-\n-  log_info(gc, heap)(HEAP_CHANGE_FORMAT\" \"\n-                     HEAP_CHANGE_FORMAT\" \"\n-                     HEAP_CHANGE_FORMAT,\n-                     HEAP_CHANGE_FORMAT_ARGS(def_new_gen->short_name(),\n-                                             pre_gc_values.young_gen_used(),\n-                                             pre_gc_values.young_gen_capacity(),\n-                                             def_new_gen->used(),\n-                                             def_new_gen->capacity()),\n-                     HEAP_CHANGE_FORMAT_ARGS(\"Eden\",\n-                                             pre_gc_values.eden_used(),\n-                                             pre_gc_values.eden_capacity(),\n-                                             def_new_gen->eden()->used(),\n-                                             def_new_gen->eden()->capacity()),\n-                     HEAP_CHANGE_FORMAT_ARGS(\"From\",\n-                                             pre_gc_values.from_used(),\n-                                             pre_gc_values.from_capacity(),\n-                                             def_new_gen->from()->used(),\n-                                             def_new_gen->from()->capacity()));\n-  log_info(gc, heap)(HEAP_CHANGE_FORMAT,\n-                     HEAP_CHANGE_FORMAT_ARGS(old_gen()->short_name(),\n-                                             pre_gc_values.old_gen_used(),\n-                                             pre_gc_values.old_gen_capacity(),\n-                                             old_gen()->used(),\n-                                             old_gen()->capacity()));\n-  MetaspaceUtils::print_metaspace_change(pre_gc_values.metaspace_sizes());\n-}\n-\n-class GenGCPrologueClosure: public GenCollectedHeap::GenClosure {\n- private:\n-  bool _full;\n- public:\n-  void do_generation(Generation* gen) {\n-    gen->gc_prologue(_full);\n-  }\n-  GenGCPrologueClosure(bool full) : _full(full) {};\n-};\n-\n-void GenCollectedHeap::gc_prologue(bool full) {\n-  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n-\n-  \/\/ Fill TLAB's and such\n-  ensure_parsability(true);   \/\/ retire TLABs\n-\n-  \/\/ Walk generations\n-  GenGCPrologueClosure blk(full);\n-  generation_iterate(&blk, false);  \/\/ not old-to-young.\n-};\n-\n-class GenGCEpilogueClosure: public GenCollectedHeap::GenClosure {\n- private:\n-  bool _full;\n- public:\n-  void do_generation(Generation* gen) {\n-    gen->gc_epilogue(_full);\n-  }\n-  GenGCEpilogueClosure(bool full) : _full(full) {};\n-};\n-\n-void GenCollectedHeap::gc_epilogue(bool full) {\n-#if COMPILER2_OR_JVMCI\n-  assert(DerivedPointerTable::is_empty(), \"derived pointer present\");\n-#endif \/\/ COMPILER2_OR_JVMCI\n-\n-  resize_all_tlabs();\n-\n-  GenGCEpilogueClosure blk(full);\n-  generation_iterate(&blk, false);  \/\/ not old-to-young.\n-\n-  MetaspaceCounters::update_performance_counters();\n-};\n-\n-#ifndef PRODUCT\n-class GenGCSaveTopsBeforeGCClosure: public GenCollectedHeap::GenClosure {\n- private:\n- public:\n-  void do_generation(Generation* gen) {\n-    gen->record_spaces_top();\n-  }\n-};\n-\n-void GenCollectedHeap::record_gen_tops_before_GC() {\n-  if (ZapUnusedHeapArea) {\n-    GenGCSaveTopsBeforeGCClosure blk;\n-    generation_iterate(&blk, false);  \/\/ not old-to-young.\n-  }\n-}\n-#endif  \/\/ not PRODUCT\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":0,"deletions":1055,"binary":false,"changes":1055,"status":"deleted"},{"patch":"@@ -1,364 +0,0 @@\n-\/*\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SHARED_GENCOLLECTEDHEAP_HPP\n-#define SHARE_GC_SHARED_GENCOLLECTEDHEAP_HPP\n-\n-#include \"gc\/serial\/generation.hpp\"\n-#include \"gc\/shared\/collectedHeap.hpp\"\n-#include \"gc\/shared\/oopStorageParState.hpp\"\n-#include \"gc\/shared\/preGCValues.hpp\"\n-#include \"gc\/shared\/softRefPolicy.hpp\"\n-\n-class CardTableRS;\n-class GCPolicyCounters;\n-class GenerationSpec;\n-\n-\/\/ A \"GenCollectedHeap\" is a CollectedHeap that uses generational\n-\/\/ collection.  It has two generations, young and old.\n-class GenCollectedHeap : public CollectedHeap {\n-  friend class Generation;\n-  friend class DefNewGeneration;\n-  friend class TenuredGeneration;\n-  friend class GenMarkSweep;\n-  friend class VM_GenCollectForAllocation;\n-  friend class VM_GenCollectFull;\n-  friend class VM_GC_HeapInspection;\n-  friend class VM_HeapDumper;\n-  friend class HeapInspection;\n-  friend class GCCauseSetter;\n-  friend class VMStructs;\n-public:\n-  friend class VM_PopulateDumpSharedSpace;\n-\n-  enum GenerationType {\n-    YoungGen,\n-    OldGen\n-  };\n-\n-protected:\n-  Generation* _young_gen;\n-  Generation* _old_gen;\n-\n-private:\n-  GenerationSpec* _young_gen_spec;\n-  GenerationSpec* _old_gen_spec;\n-\n-  \/\/ The singleton CardTable Remembered Set.\n-  CardTableRS* _rem_set;\n-\n-  SoftRefPolicy _soft_ref_policy;\n-\n-  GCPolicyCounters* _gc_policy_counters;\n-\n-  \/\/ Indicates that the most recent previous incremental collection failed.\n-  \/\/ The flag is cleared when an action is taken that might clear the\n-  \/\/ condition that caused that incremental collection to fail.\n-  bool _incremental_collection_failed;\n-\n-  \/\/ In support of ExplicitGCInvokesConcurrent functionality\n-  unsigned int _full_collections_completed;\n-\n-  \/\/ Collects the given generation.\n-  void collect_generation(Generation* gen, bool full, size_t size, bool is_tlab,\n-                          bool run_verification, bool clear_soft_refs);\n-\n-  \/\/ Reserve aligned space for the heap as needed by the contained generations.\n-  ReservedHeapSpace allocate(size_t alignment);\n-\n-  PreGenGCValues get_pre_gc_values() const;\n-\n-protected:\n-\n-  GCMemoryManager* _young_manager;\n-  GCMemoryManager* _old_manager;\n-\n-  \/\/ Helper functions for allocation\n-  HeapWord* attempt_allocation(size_t size,\n-                               bool   is_tlab,\n-                               bool   first_only);\n-\n-  \/\/ Helper function for two callbacks below.\n-  \/\/ Considers collection of the first max_level+1 generations.\n-  void do_collection(bool           full,\n-                     bool           clear_all_soft_refs,\n-                     size_t         size,\n-                     bool           is_tlab,\n-                     GenerationType max_generation);\n-\n-  \/\/ Callback from VM_GenCollectForAllocation operation.\n-  \/\/ This function does everything necessary\/possible to satisfy an\n-  \/\/ allocation request that failed in the youngest generation that should\n-  \/\/ have handled it (including collection, expansion, etc.)\n-  HeapWord* satisfy_failed_allocation(size_t size, bool is_tlab);\n-\n-  \/\/ Callback from VM_GenCollectFull operation.\n-  \/\/ Perform a full collection of the first max_level+1 generations.\n-  void do_full_collection(bool clear_all_soft_refs) override;\n-  void do_full_collection(bool clear_all_soft_refs, GenerationType max_generation);\n-\n-  \/\/ Does the \"cause\" of GC indicate that\n-  \/\/ we absolutely __must__ clear soft refs?\n-  bool must_clear_all_soft_refs();\n-\n-  GenCollectedHeap(Generation::Name young,\n-                   Generation::Name old,\n-                   const char* policy_counters_name);\n-\n-public:\n-\n-  \/\/ Returns JNI_OK on success\n-  jint initialize() override;\n-  virtual CardTableRS* create_rem_set(const MemRegion& reserved_region);\n-\n-  \/\/ Does operations required after initialization has been done.\n-  void post_initialize() override;\n-\n-  Generation* young_gen() const { return _young_gen; }\n-  Generation* old_gen()   const { return _old_gen; }\n-\n-  bool is_young_gen(const Generation* gen) const { return gen == _young_gen; }\n-  bool is_old_gen(const Generation* gen) const { return gen == _old_gen; }\n-\n-  MemRegion reserved_region() const { return _reserved; }\n-  bool is_in_reserved(const void* addr) const { return _reserved.contains(addr); }\n-\n-  GenerationSpec* young_gen_spec() const;\n-  GenerationSpec* old_gen_spec() const;\n-\n-  SoftRefPolicy* soft_ref_policy() override { return &_soft_ref_policy; }\n-\n-  \/\/ Performance Counter support\n-  GCPolicyCounters* counters()     { return _gc_policy_counters; }\n-\n-  size_t capacity() const override;\n-  size_t used() const override;\n-\n-  \/\/ Save the \"used_region\" for both generations.\n-  void save_used_regions();\n-\n-  size_t max_capacity() const override;\n-\n-  HeapWord* mem_allocate(size_t size, bool*  gc_overhead_limit_was_exceeded) override;\n-\n-  \/\/ Perform a full collection of the heap; intended for use in implementing\n-  \/\/ \"System.gc\". This implies as full a collection as the CollectedHeap\n-  \/\/ supports. Caller does not hold the Heap_lock on entry.\n-  void collect(GCCause::Cause cause) override;\n-\n-  \/\/ Returns \"TRUE\" iff \"p\" points into the committed areas of the heap.\n-  \/\/ The methods is_in() and is_in_youngest() may be expensive to compute\n-  \/\/ in general, so, to prevent their inadvertent use in product jvm's, we\n-  \/\/ restrict their use to assertion checking or verification only.\n-  bool is_in(const void* p) const override;\n-\n-  \/\/ Returns true if p points into the reserved space for the young generation.\n-  \/\/ Assumes the young gen address range is less than that of the old gen.\n-  bool is_in_young(const void* p) const;\n-\n-  bool requires_barriers(stackChunkOop obj) const override;\n-\n-#ifdef ASSERT\n-  bool is_in_partial_collection(const void* p);\n-#endif\n-\n-  \/\/ Optimized nmethod scanning support routines\n-  void register_nmethod(nmethod* nm) override;\n-  void unregister_nmethod(nmethod* nm) override;\n-  void verify_nmethod(nmethod* nm) override;\n-\n-  void prune_scavengable_nmethods();\n-\n-  \/\/ Iteration functions.\n-  void object_iterate(ObjectClosure* cl) override;\n-\n-  \/\/ A CollectedHeap is divided into a dense sequence of \"blocks\"; that is,\n-  \/\/ each address in the (reserved) heap is a member of exactly\n-  \/\/ one block.  The defining characteristic of a block is that it is\n-  \/\/ possible to find its size, and thus to progress forward to the next\n-  \/\/ block.  (Blocks may be of different sizes.)  Thus, blocks may\n-  \/\/ represent Java objects, or they might be free blocks in a\n-  \/\/ free-list-based heap (or subheap), as long as the two kinds are\n-  \/\/ distinguishable and the size of each is determinable.\n-\n-  \/\/ Returns the address of the start of the \"block\" that contains the\n-  \/\/ address \"addr\".  We say \"blocks\" instead of \"object\" since some heaps\n-  \/\/ may not pack objects densely; a chunk may either be an object or a\n-  \/\/ non-object.\n-  HeapWord* block_start(const void* addr) const;\n-\n-  \/\/ Requires \"addr\" to be the start of a block, and returns \"TRUE\" iff\n-  \/\/ the block is an object. Assumes (and verifies in non-product\n-  \/\/ builds) that addr is in the allocated part of the heap and is\n-  \/\/ the start of a chunk.\n-  bool block_is_obj(const HeapWord* addr) const;\n-\n-  \/\/ Section on TLAB's.\n-  size_t tlab_capacity(Thread* thr) const override;\n-  size_t tlab_used(Thread* thr) const override;\n-  size_t unsafe_max_tlab_alloc(Thread* thr) const override;\n-  HeapWord* allocate_new_tlab(size_t min_size,\n-                              size_t requested_size,\n-                              size_t* actual_size) override;\n-\n-  \/\/ Total number of full collections completed.\n-  unsigned int total_full_collections_completed() {\n-    assert(_full_collections_completed <= _total_full_collections,\n-           \"Can't complete more collections than were started\");\n-    return _full_collections_completed;\n-  }\n-\n-  \/\/ Update above counter, as appropriate, at the end of a stop-world GC cycle\n-  unsigned int update_full_collections_completed();\n-\n-  \/\/ Update the gc statistics for each generation.\n-  void update_gc_stats(Generation* current_generation, bool full) {\n-    _old_gen->update_gc_stats(current_generation, full);\n-  }\n-\n-  bool no_gc_in_progress() { return !is_gc_active(); }\n-\n-  void prepare_for_verify() override;\n-  void verify(VerifyOption option) override;\n-\n-  void print_on(outputStream* st) const override;\n-  void gc_threads_do(ThreadClosure* tc) const override;\n-  void print_tracing_info() const override;\n-\n-  \/\/ Used to print information about locations in the hs_err file.\n-  bool print_location(outputStream* st, void* addr) const override;\n-\n-  void print_heap_change(const PreGenGCValues& pre_gc_values) const;\n-\n-  \/\/ The functions below are helper functions that a subclass of\n-  \/\/ \"CollectedHeap\" can use in the implementation of its virtual\n-  \/\/ functions.\n-\n-  class GenClosure : public StackObj {\n-   public:\n-    virtual void do_generation(Generation* gen) = 0;\n-  };\n-\n-  \/\/ Apply \"cl.do_generation\" to all generations in the heap\n-  \/\/ If \"old_to_young\" determines the order.\n-  void generation_iterate(GenClosure* cl, bool old_to_young);\n-\n-  \/\/ Return \"true\" if all generations have reached the\n-  \/\/ maximal committed limit that they can reach, without a garbage\n-  \/\/ collection.\n-  virtual bool is_maximal_no_gc() const override;\n-\n-  \/\/ This function returns the CardTableRS object that allows us to scan\n-  \/\/ generations in a fully generational heap.\n-  CardTableRS* rem_set() { return _rem_set; }\n-\n-  \/\/ Convenience function to be used in situations where the heap type can be\n-  \/\/ asserted to be this type.\n-  static GenCollectedHeap* heap();\n-\n-  \/\/ The ScanningOption determines which of the roots\n-  \/\/ the closure is applied to:\n-  \/\/ \"SO_None\" does none;\n-  enum ScanningOption {\n-    SO_None                =  0x0,\n-    SO_AllCodeCache        =  0x8,\n-    SO_ScavengeCodeCache   = 0x10\n-  };\n-\n- protected:\n-  virtual void gc_prologue(bool full);\n-  virtual void gc_epilogue(bool full);\n-\n- public:\n-  \/\/ Apply closures on various roots in Young GC or marking\/adjust phases of Full GC.\n-  void process_roots(ScanningOption so,\n-                     OopClosure* strong_roots,\n-                     CLDClosure* strong_cld_closure,\n-                     CLDClosure* weak_cld_closure,\n-                     CodeBlobToOopClosure* code_roots);\n-\n-  \/\/ Apply \"root_closure\" to all the weak roots of the system.\n-  \/\/ These include JNI weak roots, string table,\n-  \/\/ and referents of reachable weak refs.\n-  void gen_process_weak_roots(OopClosure* root_closure);\n-\n-  \/\/ Set the saved marks of generations, if that makes sense.\n-  \/\/ In particular, if any generation might iterate over the oops\n-  \/\/ in other generations, it should call this method.\n-  void save_marks();\n-\n-  \/\/ Returns \"true\" iff no allocations have occurred since the last\n-  \/\/ call to \"save_marks\".\n-  bool no_allocs_since_save_marks();\n-\n-  \/\/ Returns true if an incremental collection is likely to fail.\n-  \/\/ We optionally consult the young gen, if asked to do so;\n-  \/\/ otherwise we base our answer on whether the previous incremental\n-  \/\/ collection attempt failed with no corrective action as of yet.\n-  bool incremental_collection_will_fail(bool consult_young) {\n-    \/\/ The first disjunct remembers if an incremental collection failed, even\n-    \/\/ when we thought (second disjunct) that it would not.\n-    return incremental_collection_failed() ||\n-           (consult_young && !_young_gen->collection_attempt_is_safe());\n-  }\n-\n-  \/\/ If a generation bails out of an incremental collection,\n-  \/\/ it sets this flag.\n-  bool incremental_collection_failed() const {\n-    return _incremental_collection_failed;\n-  }\n-  void set_incremental_collection_failed() {\n-    _incremental_collection_failed = true;\n-  }\n-  void clear_incremental_collection_failed() {\n-    _incremental_collection_failed = false;\n-  }\n-\n-private:\n-  \/\/ Return true if an allocation should be attempted in the older generation\n-  \/\/ if it fails in the younger generation.  Return false, otherwise.\n-  bool should_try_older_generation_allocation(size_t word_size) const;\n-\n-  \/\/ Try to allocate space by expanding the heap.\n-  HeapWord* expand_heap_and_allocate(size_t size, bool is_tlab);\n-\n-  HeapWord* mem_allocate_work(size_t size,\n-                              bool is_tlab);\n-\n-#if INCLUDE_SERIALGC\n-  \/\/ For use by mark-sweep.  As implemented, mark-sweep-compact is global\n-  \/\/ in an essential way: compaction is performed across generations, by\n-  \/\/ iterating over spaces.\n-  void prepare_for_compaction();\n-#endif\n-\n-  \/\/ Save the tops of the spaces in all generations\n-  void record_gen_tops_before_GC() PRODUCT_RETURN;\n-\n-  \/\/ Return true if we need to perform full collection.\n-  bool should_do_full_collection(size_t size, bool full,\n-                                 bool is_tlab, GenerationType max_gen) const;\n-};\n-\n-#endif \/\/ SHARE_GC_SHARED_GENCOLLECTEDHEAP_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":0,"deletions":364,"binary":false,"changes":364,"status":"deleted"},{"patch":"@@ -1,49 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/serial\/cardTableRS.hpp\"\n-#include \"gc\/shared\/generationSpec.hpp\"\n-#include \"runtime\/java.hpp\"\n-#include \"utilities\/macros.hpp\"\n-#if INCLUDE_SERIALGC\n-#include \"gc\/serial\/defNewGeneration.hpp\"\n-#include \"gc\/serial\/tenuredGeneration.hpp\"\n-#endif\n-\n-Generation* GenerationSpec::init(ReservedSpace rs, CardTableRS* remset) {\n-  switch (name()) {\n-#if INCLUDE_SERIALGC\n-    case Generation::DefNew:\n-      return new DefNewGeneration(rs, _init_size, _min_size, _max_size);\n-\n-    case Generation::MarkSweepCompact:\n-      return new TenuredGeneration(rs, _init_size, _min_size, _max_size, remset);\n-#endif\n-\n-    default:\n-      guarantee(false, \"unrecognized GenerationName\");\n-      return nullptr;\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/shared\/generationSpec.cpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"deleted"},{"patch":"@@ -1,61 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SHARED_GENERATIONSPEC_HPP\n-#define SHARE_GC_SHARED_GENERATIONSPEC_HPP\n-\n-#include \"gc\/serial\/generation.hpp\"\n-#include \"utilities\/align.hpp\"\n-\n-\/\/ The specification of a generation.  This class also encapsulates\n-\/\/ some generation-specific behavior.  This is done here rather than as a\n-\/\/ virtual function of Generation because these methods are needed in\n-\/\/ initialization of the Generations.\n-class GenerationSpec : public CHeapObj<mtGC> {\n-  friend class VMStructs;\n-private:\n-  Generation::Name _name;\n-  size_t           _init_size;\n-  size_t           _min_size;\n-  size_t           _max_size;\n-\n-public:\n-  GenerationSpec(Generation::Name name, size_t init_size, size_t max_size, size_t alignment) :\n-    _name(name),\n-    _init_size(align_up(init_size, alignment)),\n-    _min_size(_init_size),\n-    _max_size(align_up(max_size, alignment))\n-  { }\n-\n-  Generation* init(ReservedSpace rs, CardTableRS* remset);\n-\n-  Generation::Name name() const { return _name; }\n-  size_t init_size()      const { return _init_size; }\n-  size_t min_size()       const { return _min_size; }\n-  size_t max_size()       const { return _max_size; }\n-};\n-\n-typedef GenerationSpec* GenerationSpecPtr;\n-\n-#endif \/\/ SHARE_GC_SHARED_GENERATIONSPEC_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/generationSpec.hpp","additions":0,"deletions":61,"binary":false,"changes":61,"status":"deleted"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -47,0 +46,1 @@\n+#include \"gc\/serial\/serialHeap.hpp\"\n@@ -129,1 +129,4 @@\n-      cp->gen = GenCollectedHeap::heap()->young_gen();\n+#if INCLUDE_SERIALGC\n+      cp->gen = SerialHeap::heap()->young_gen();\n+#endif \/\/ INCLUDE_SERIALGC\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -68,1 +68,1 @@\n-\/\/   GenSpaceMangler is used with the GenCollectedHeap collectors and\n+\/\/   GenSpaceMangler is used with the SerialHeap collectors and\n@@ -125,1 +125,1 @@\n-\/\/ For use with GenCollectedHeap's\n+\/\/ For use with SerialHeap's\n","filename":"src\/hotspot\/share\/gc\/shared\/spaceDecorator.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,2 +31,0 @@\n-#include \"gc\/shared\/genCollectedHeap.hpp\"\n-#include \"gc\/shared\/generationSpec.hpp\"\n@@ -117,9 +115,0 @@\n-  nonstatic_field(GenerationSpec,              _name,                                         Generation::Name)                      \\\n-  nonstatic_field(GenerationSpec,              _init_size,                                    size_t)                                \\\n-  nonstatic_field(GenerationSpec,              _max_size,                                     size_t)                                \\\n-                                                                                                                                     \\\n-  nonstatic_field(GenCollectedHeap,            _young_gen,                                    Generation*)                           \\\n-  nonstatic_field(GenCollectedHeap,            _old_gen,                                      Generation*)                           \\\n-  nonstatic_field(GenCollectedHeap,            _young_gen_spec,                               GenerationSpec*)                       \\\n-  nonstatic_field(GenCollectedHeap,            _old_gen_spec,                                 GenerationSpec*)                       \\\n-                                                                                                                                     \\\n@@ -160,1 +149,0 @@\n-           declare_type(GenCollectedHeap,             CollectedHeap)      \\\n@@ -175,1 +163,0 @@\n-  declare_toplevel_type(GenerationSpec)                                   \\\n@@ -191,1 +178,0 @@\n-  declare_toplevel_type(GenCollectedHeap*)                                \\\n@@ -193,1 +179,0 @@\n-  declare_toplevel_type(GenerationSpec**)                                 \\\n","filename":"src\/hotspot\/share\/gc\/shared\/vmStructs_gc.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"}]}