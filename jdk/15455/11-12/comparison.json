{"files":[{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2013, 2016 SAP SE. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2023 SAP SE. All rights reserved.\n","filename":"src\/hotspot\/cpu\/ppc\/templateTable_ppc.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -340,12 +341,0 @@\n-\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, Register cpe_offset,\n-                                                           int bcp_offset, size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_and_index_at_bcp {\");\n-  assert_different_registers(cache, cpe_offset);\n-  get_cache_index_at_bcp(cpe_offset, bcp_offset, index_size);\n-  z_lg(cache, Address(Z_fp, _z_ijava_state_neg(cpoolCache)));\n-  \/\/ Convert from field index to ConstantPoolCache offset in bytes.\n-  z_sllg(cpe_offset, cpe_offset, exact_log2(in_words(ConstantPoolCacheEntry::size()) * BytesPerWord));\n-  BLOCK_COMMENT(\"}\");\n-}\n-\n@@ -392,18 +381,3 @@\n-\/\/ Kills Z_R0_scratch.\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register cpe_offset,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_and_index_and_bytecode_at_bcp {\");\n-  get_cache_and_index_at_bcp(cache, cpe_offset, bcp_offset, index_size);\n-\n-  \/\/ We want to load (from CP cache) the bytecode that corresponds to the passed-in byte_no.\n-  \/\/ It is located at (cache + cpe_offset + base_offset + indices_offset + (8-1) (last byte in DW) - (byte_no+1).\n-  \/\/ Instead of loading, shifting and masking a DW, we just load that one byte of interest with z_llgc (unsigned).\n-  const int base_ix_off = in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset());\n-  const int off_in_DW   = (8-1) - (1+byte_no);\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == ConstantPoolCacheEntry::bytecode_2_mask, \"common mask\");\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == 0xff, \"\");\n-  load_sized_value(bytecode, Address(cache, cpe_offset, base_ix_off+off_in_DW), 1, false \/*signed*\/);\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get field index out of bytecode pointer.\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n@@ -411,1 +385,14 @@\n-  BLOCK_COMMENT(\"}\");\n+  \/\/ Get the address of the ResolvedMethodEntry array.\n+  get_constant_pool_cache(cache);\n+  z_lg(cache, Address(cache, in_bytes(ConstantPoolCache::method_entries_offset())));\n+\n+  \/\/ Scale the index to form a byte offset into the ResolvedMethodEntry array\n+  size_t entry_size = sizeof(ResolvedMethodEntry);\n+  if (is_power_of_2(entry_size)) {\n+    z_sllg(index, index, exact_log2(entry_size));\n+  } else {\n+    z_mghi(index, entry_size);\n+  }\n+\n+  \/\/ Calculate the final field address.\n+  z_la(cache, Array<ResolvedMethodEntry>::base_offset_in_bytes(), index, cache);\n@@ -451,23 +438,0 @@\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_entry_pointer_at_bcp {\");\n-    get_cache_and_index_at_bcp(cache, tmp, bcp_offset, index_size);\n-    add2reg_with_index(cache, in_bytes(ConstantPoolCache::base_offset()), tmp, cache);\n-  BLOCK_COMMENT(\"}\");\n-}\n-\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register cache,\n-                                                              Register cpe_offset,\n-                                                              Register method) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  z_lg(method, Address(cache, cpe_offset, method_offset)); \/\/ get f1 Method*\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":18,"deletions":54,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -114,1 +114,0 @@\n-  void get_cache_and_index_at_bcp(Register cache, Register cpe_offset, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -116,4 +115,2 @@\n-  void load_field_entry(Register cache, Register index, int bcp_offset = 1);\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register cpe_offset, Register bytecode,\n-                                               int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n+  void load_field_entry (Register cache, Register index, int bcp_offset = 1);\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n@@ -125,2 +122,0 @@\n-  void load_resolved_method_at_index(int byte_no, Register cache, Register cpe_offset, Register method);\n-\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.hpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -661,6 +662,3 @@\n-    const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                      ConstantPoolCacheEntry::flags_offset());\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-\n-    \/\/ #args is in rightmost byte of the _flags field.\n-    __ z_llgc(size, Address(cache, index, flags_offset + (sizeof(size_t) - 1)));\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_sized_value(size, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())), sizeof(u2), false \/*is_signed*\/);\n","filename":"src\/hotspot\/cpu\/s390\/templateInterpreterGenerator_s390.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -547,1 +548,1 @@\n-  assert(ConstantPoolCacheEntry::field_index_mask == 0xffff, \"or use other instructions\");\n+  assert(ConstantPoolCache::field_index_mask == 0xffff, \"or use other instructions\");\n@@ -552,3 +553,1 @@\n-  __ z_srl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags for tos_state after the above shift.\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n+  __ z_srl(flags, ConstantPoolCache::tos_state_shift);\n@@ -2354,8 +2353,6 @@\n-\/\/ NOTE: index is already computed as byte offset, so we must not\n-\/\/ shift it afterwards!\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register cache,\n-                                            Register index,\n-                                            size_t index_size) {\n-\n-  assert_different_registers(cache, index, Z_R1_scratch);\n+\/\/ Register Killed: Z_R1_scratch\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register Rcache,\n+                                                       Register index) {\n+  BLOCK_COMMENT(\"resolve_cache_and_index_for_method {\");\n+  assert_different_registers(Rcache, index);\n@@ -2364,2 +2361,2 @@\n-  const Register  bytecode_in_cpcache = Z_R1_scratch;\n-  NearLabel       resolved, clinit_barrier_slow;\n+  Label resolved, clinit_barrier_slow;\n+\n@@ -2367,0 +2364,6 @@\n+  switch (code) {\n+    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n+    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n+    default:\n+      break;\n+  }\n@@ -2368,1 +2371,2 @@\n-  BLOCK_COMMENT(\"resolve_cache_and_index {\");\n+  const int bc_offset = (byte_no == f1_byte) ? in_bytes(ResolvedMethodEntry::bytecode1_offset())\n+                                             : in_bytes(ResolvedMethodEntry::bytecode2_offset());\n@@ -2370,3 +2374,3 @@\n-  __ get_cache_and_index_and_bytecode_at_bcp(cache, index, bytecode_in_cpcache, byte_no, 1, index_size);\n-  \/\/ Have we resolved this bytecode?\n-  __ compare32_and_branch(bytecode_in_cpcache, (int)code, Assembler::bcondEqual, resolved);\n+  __ load_method_entry(Rcache, index);\n+  __ z_cli(Address(Rcache, bc_offset), code);\n+  __ z_bre(resolved);\n@@ -2374,1 +2378,1 @@\n-  \/\/ Resolve first time through via runtime call.\n+  \/\/ Resolve first time through\n@@ -2380,2 +2384,0 @@\n-  \/\/ Update registers with resolved info.\n-  __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n@@ -2383,0 +2385,2 @@\n+  \/\/ Update registers with resolved info.\n+  __ load_method_entry(Rcache, index);\n@@ -2389,2 +2393,1 @@\n-\n-    __ load_resolved_method_at_index(byte_no, cache, index, method);\n+    __ z_lg(method, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2395,1 +2398,1 @@\n-  BLOCK_COMMENT(\"} resolve_cache_and_index\");\n+  BLOCK_COMMENT(\"} resolve_cache_and_index_for_method\");\n@@ -2401,0 +2404,1 @@\n+  BLOCK_COMMENT(\"resolve_cache_and_index_for_field {\");\n@@ -2415,5 +2419,4 @@\n-  if (byte_no == f1_byte) {\n-    __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::get_code_offset())), code);\n-  } else {\n-    __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::put_code_offset())), code);\n-  }\n+  const int code_offset = (byte_no == f1_byte) ? in_bytes(ResolvedFieldEntry::get_code_offset()) :\n+                                                 in_bytes(ResolvedFieldEntry::put_code_offset()) ;\n+\n+  __ z_cli(Address(cache, code_offset), code);\n@@ -2431,0 +2434,2 @@\n+\n+  BLOCK_COMMENT(\"} resolve_cache_and_index_for_field\");\n@@ -2461,24 +2466,0 @@\n-\/\/ The Rcache and index registers must be set before call.\n-\/\/ Index is already a byte offset, don't shift!\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-\n-  \/\/ Field offset\n-  __ mem2reg_opt(off, Address(cache, index, cp_base_offset + ConstantPoolCacheEntry::f2_offset()));\n-  \/\/ Flags. Must load 64 bits.\n-  __ mem2reg_opt(flags, Address(cache, index, cp_base_offset + ConstantPoolCacheEntry::flags_offset()));\n-\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ mem2reg_opt(obj, Address(cache, index, cp_base_offset + ConstantPoolCacheEntry::f1_offset()));\n-    __ mem2reg_opt(obj, Address(obj, Klass::java_mirror_offset()));\n-    __ resolve_oop_handle(obj);\n-  }\n-}\n-\n@@ -2496,1 +2477,1 @@\n-  __ z_clgij(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n+  __ compare64_and_branch(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n@@ -2506,1 +2487,1 @@\n-  __ z_clgij(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n+  __ compare64_and_branch(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n@@ -2512,1 +2493,1 @@\n-  __ z_llgc(index, Address(cache, in_bytes(ResolvedIndyEntry::flags_offset())));\n+  __ load_sized_value(index, Address(cache, in_bytes(ResolvedIndyEntry::flags_offset())), sizeof(u1), false \/*is_signed*\/);\n@@ -2516,1 +2497,1 @@\n-  __ z_llgh(index, Address(cache, in_bytes(ResolvedIndyEntry::resolved_references_index_offset())));\n+  __ load_sized_value(index, Address(cache, in_bytes(ResolvedIndyEntry::resolved_references_index_offset())), sizeof(u2), false \/*is_signed*\/);\n@@ -2527,1 +2508,1 @@\n-  __ z_llgc(ret_type, Address(cache, in_bytes(ResolvedIndyEntry::result_type_offset())));\n+  __ load_sized_value(ret_type, Address(cache, in_bytes(ResolvedIndyEntry::result_type_offset())), sizeof(u1), false \/*is_signed*\/);\n@@ -2533,5 +2514,0 @@\n-  \/\/ const int r_bitpos  = 63 - bit_shift;\n-  \/\/ const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::tos_state_bits + 1;\n-  \/\/ const int n_rotate  = bit_shift-ConstantPoolCacheEntry::tos_state_shift;\n-  \/\/ __ rotate_then_insert(ret_type, Z_R0_scratch, l_bitpos, r_bitpos, n_rotate, true);\n-  \/\/ Make sure we don't need to mask flags for tos_state after the above shift.\n@@ -2539,1 +2515,0 @@\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n@@ -2543,32 +2518,5 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/\/ unused\n-                                               bool is_invokedynamic \/* unused *\/) {\n-  BLOCK_COMMENT(\"load_invoke_cp_cache_entry {\");\n-  \/\/ Setup registers.\n-  const Register cache     = Z_ARG1;\n-  const Register cpe_offset= flags;\n-  const ByteSize base_off  = ConstantPoolCache::base_offset();\n-  const ByteSize f1_off    = ConstantPoolCacheEntry::f1_offset();\n-  const ByteSize f2_off    = ConstantPoolCacheEntry::f2_offset();\n-  const ByteSize flags_off = ConstantPoolCacheEntry::flags_offset();\n-  const int method_offset  = in_bytes(base_off + ((byte_no == f2_byte) ? f2_off : f1_off));\n-  const int flags_offset   = in_bytes(base_off + flags_off);\n-  \/\/ Access constant pool cache fields.\n-  const int index_offset   = in_bytes(base_off + f2_off);\n-\n-  assert_different_registers(method, itable_index, flags, cache);\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-\n-  if (is_invokevfinal) {\n-    \/\/ Already resolved.\n-     assert(itable_index == noreg, \"register not used\");\n-     __ get_cache_and_index_at_bcp(cache, cpe_offset, 1);\n-  } else {\n-    \/\/ Need to resolve.\n-    resolve_cache_and_index(byte_no, cache, cpe_offset, sizeof(u2));\n-  }\n-  __ z_lg(method, Address(cache, cpe_offset, method_offset));\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  assert_different_registers(method, cache, ref_index, flags);\n@@ -2576,3 +2524,2 @@\n-  if (itable_index != noreg) {\n-    __ z_lg(itable_index, Address(cache, cpe_offset, index_offset));\n-  }\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, method \/* index *\/);\n@@ -2580,4 +2527,15 @@\n-  \/\/ Only load the lower 4 bytes and fill high bytes of flags with zeros.\n-  \/\/ Callers depend on this zero-extension!!!\n-  \/\/ Attention: overwrites cpe_offset == flags\n-  __ z_llgf(flags, Address(cache, cpe_offset, flags_offset + (BytesPerLong-BytesPerInt)));\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/* is signed *\/);\n+  __ testbit(flags, ResolvedMethodEntry::has_appendix_shift); \/\/ life ended for flags\n+  __ z_bfalse(L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_sized_value(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())), sizeof(u2), false \/* is signed *\/);\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ verify_oop(appendix);\n+  __ push_ptr(appendix);  \/\/ Push appendix (MethodType, CallSite, etc.).\n+  __ bind(L_no_push);\n@@ -2585,1 +2543,1 @@\n-  BLOCK_COMMENT(\"} load_invoke_cp_cache_entry\");\n+  __ z_lg(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -3542,40 +3500,10 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method,  \/\/ linked method (or i-klass)\n-                                   Register index,   \/\/ itable index, MethodType, etc.\n-                                   Register recv,    \/\/ If caller wants to see it.\n-                                   Register flags) { \/\/ If caller wants to test it.\n-  \/\/ Determine flags.\n-  const Bytecodes::Code code = bytecode();\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = false; \/\/ should not reach here with invokedynamic\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-\n-  \/\/ Setup registers & access constant pool cache.\n-  if (recv  == noreg) { recv  = Z_ARG1; }\n-  if (flags == noreg) { flags = Z_ARG2; }\n-  assert_different_registers(method, Z_R14, index, recv, flags);\n-\n-  BLOCK_COMMENT(\"prepare_invoke {\");\n-\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ Maybe push appendix to arguments.\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    Register resolved_reference = Z_R1_scratch;\n-    __ testbit(flags, ConstantPoolCacheEntry::has_appendix_shift);\n-    __ z_bfalse(L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ load_resolved_reference_at_index(resolved_reference, index);\n-    __ verify_oop(resolved_reference);\n-    __ push_ptr(resolved_reference);  \/\/ Push appendix (MethodType, CallSite, etc.).\n-    __ bind(L_no_push);\n-  }\n-\n-  \/\/ Load receiver if needed (after appendix is pushed so parameter size is correct).\n+void TemplateTable::prepare_invoke(Register cache, Register recv) {\n+  Bytecodes::Code code     = bytecode();\n+  const Register  ret_type = Z_R1_scratch;\n+  const bool load_receiver = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n+  assert_different_registers(ret_type, recv);\n+\n+  \/\/ Load TOS state for later\n+  __ load_sized_value(ret_type, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())), sizeof(u1), false \/* is signed *\/);\n+\n+  \/\/ load receiver if needed (note: no return address pushed yet)\n@@ -3583,14 +3511,4 @@\n-    assert(!is_invokedynamic, \"\");\n-    \/\/ recv := int2long(flags & ConstantPoolCacheEntry::parameter_size_mask) << 3\n-    \/\/ Flags is zero-extended int2long when loaded during load_invoke_cp_cache_entry().\n-    \/\/ Only the least significant byte (psize) of flags is used.\n-    {\n-      const unsigned int logSES = Interpreter::logStackElementSize;\n-      const int bit_shift = logSES;\n-      const int r_bitpos  = 63 - bit_shift;\n-      const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::parameter_size_bits + 1;\n-      const int n_rotate  = bit_shift;\n-      assert(ConstantPoolCacheEntry::parameter_size_mask == 255, \"adapt bitpositions\");\n-      __ rotate_then_insert(recv, flags, l_bitpos, r_bitpos, n_rotate, true);\n-    }\n-    \/\/ Recv now contains #arguments * StackElementSize.\n+    __ load_sized_value(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())), sizeof(u2), false \/* is signed *\/);\n+    const unsigned int bit_shift = Interpreter::logStackElementSize;\n+    __ z_sllg(recv, recv, bit_shift);\n+    \/\/ recv now contains #arguments * StackElementSize.\n@@ -3605,3 +3523,0 @@\n-  Register ret_type = Z_R1_scratch;\n-  assert_different_registers(ret_type, method);\n-\n@@ -3610,0 +3525,28 @@\n+  __ z_sllg(ret_type, ret_type, LogBytesPerWord);\n+\n+  __ z_lg(Z_R14, Address(Z_R14, ret_type)); \/\/ Load return address.\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  assert_different_registers(method_or_table_index, cache, flags, klass);\n+  BLOCK_COMMENT(\"load_resolved_method_entry_interface {\");\n+  \/\/ determine constant pool cache field offsets\n+  const Register index = flags; \/\/ not containing anything important, let's kill it.\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/* is signed*\/);\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+  Label NotVirtual, NotVFinal, Done;\n+  __ testbit(flags, ResolvedMethodEntry::is_forced_virtual_shift);\n+  __ z_brz(NotVirtual);\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n+  __ z_brz(NotVFinal);\n+  __ z_lg(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ z_bru(Done);\n@@ -3611,9 +3554,3 @@\n-  {\n-    const int bit_shift = LogBytesPerWord;           \/\/ Size of each table entry.\n-    const int r_bitpos  = 63 - bit_shift;\n-    const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::tos_state_bits + 1;\n-    const int n_rotate  = bit_shift-ConstantPoolCacheEntry::tos_state_shift;\n-    __ rotate_then_insert(ret_type, flags, l_bitpos, r_bitpos, n_rotate, true);\n-    \/\/ Make sure we don't need to mask flags for tos_state after the above shift.\n-    ConstantPoolCacheEntry::verify_tos_state_shift();\n-  }\n+  __ bind(NotVFinal);\n+  __ load_sized_value(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())), sizeof(u2), false \/* is signed *\/);\n+  __ z_bru(Done);\n@@ -3621,2 +3558,6 @@\n-    __ z_lg(Z_R14, Address(Z_R14, ret_type)); \/\/ Load return address.\n-  BLOCK_COMMENT(\"} prepare_invoke\");\n+  __ bind(NotVirtual);\n+  __ z_lg(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ z_lg(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+\n+  BLOCK_COMMENT(\"} load_resolved_method_entry_interface\");\n@@ -3625,0 +3566,24 @@\n+\/\/ Registers Killed: Z_R1\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  BLOCK_COMMENT(\"load_resolved_method_entry_virtual {\");\n+  assert_different_registers(method_or_table_index, cache, flags);\n+  const Register index = flags; \/\/ doesn't contain valuable content, could be used as index for once\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/*is_signed*\/);\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal, Done;\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n+  __ z_brz(NotVFinal);\n+  __ z_lg(method_or_table_index,  Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ z_bru(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_sized_value(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())), sizeof(u2), false \/* is signed *\/);\n+  __ bind(Done);\n+  BLOCK_COMMENT(\"} load_resolved_method_entry_virtual\");\n+}\n@@ -3637,1 +3602,1 @@\n-  __ testbit(flags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3676,5 +3641,8 @@\n-  prepare_invoke(byte_no,\n-                 Z_ARG3,  \/\/ method or vtable index\n-                 noreg,   \/\/ unused itable index\n-                 Z_ARG1,  \/\/ recv\n-                 Z_ARG2); \/\/ flags\n+  const Register Rrecv   = Z_ARG1;\n+  const Register Rmethod = Z_ARG3;\n+  const Register Rflags  = Z_ARG2;\n+\n+  load_resolved_method_entry_virtual(Rrecv,\n+                                     Rmethod,\n+                                     Rflags);\n+  prepare_invoke(Rrecv, Rrecv);\n@@ -3685,1 +3653,1 @@\n-  invokevirtual_helper(Z_ARG3, Z_ARG1, Z_ARG2);\n+  invokevirtual_helper(Rmethod, Rrecv, Rflags);\n@@ -3693,4 +3661,7 @@\n-  prepare_invoke(byte_no, Rmethod, noreg, \/\/ Get f1 method.\n-                 Z_ARG3);   \/\/ Get receiver also for null check.\n-  __ verify_oop(Z_ARG3);\n-  __ null_check(Z_ARG3);\n+  Register Rrecv   = Z_ARG3;\n+  load_resolved_method_entry_special_or_static(Rrecv,\n+                                               Rmethod,\n+                                               noreg); \/* flags are not used here *\/\n+  prepare_invoke(Rrecv, Rrecv);\n+  __ verify_oop(Rrecv);\n+  __ null_check(Rrecv);\n@@ -3703,0 +3674,17 @@\n+\/*\n+ * There are only two callsite (invokestatic, invokespecial) for this method and both of them are passing\n+ * \"noreg\" for flags registers at present.\n+ *\/\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  assert_different_registers(method, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, method \/* index (temp) *\/);\n+  if (flags != noreg) {\n+    __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/* is signed *\/);\n+  }\n+  __ z_lg(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n@@ -3708,1 +3696,6 @@\n-  prepare_invoke(byte_no, Rmethod);   \/\/ Get f1 method.\n+  Register Rrecv   = Z_ARG1;\n+\n+  load_resolved_method_entry_special_or_static(Rrecv,\n+                                               Rmethod,\n+                                               noreg); \/* flags are not used here *\/\n+  prepare_invoke(Rrecv, Rrecv);  \/\/ get receiver also for null check and flags\n@@ -3734,2 +3727,5 @@\n-  prepare_invoke(byte_no, interface, method,  \/\/ Get f1 klassOop, f2 Method*.\n-                 receiver, flags);\n+  load_resolved_method_entry_interface(receiver,   \/\/ ResolvedMethodEntry*\n+                                       interface,  \/\/ Klass*  (interface klass (from f1))\n+                                       method,     \/\/ Method* or itable\/vtable index\n+                                       flags);     \/\/ flags\n+  prepare_invoke(receiver, receiver);\n@@ -3745,1 +3741,1 @@\n-  __ testbit(flags, ConstantPoolCacheEntry::is_forced_virtual_shift);\n+  __ testbit(flags, ResolvedMethodEntry::is_forced_virtual_shift);\n@@ -3752,1 +3748,1 @@\n-  __ testbit(flags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3846,3 +3842,6 @@\n-  prepare_invoke(byte_no,\n-                 method, mtype,   \/\/ Get f2 method, f1 MethodType.\n-                 recv);\n+  const Register flags  = Z_R1_scratch;\n+  load_resolved_method_entry_handle(recv,\n+                                    method,\n+                                    mtype \/* index *\/,\n+                                    flags );\n+  prepare_invoke(recv, recv);\n@@ -3869,2 +3868,2 @@\n-  \/\/ Rmethod: CallSite object (from f1)\n-  \/\/ Rcallsite: MH.linkToCallSite method (from f2)\n+  \/\/ Rmethod: CallSite object\n+  \/\/ Rcallsite: MH.linkToCallSite method\n@@ -3872,1 +3871,1 @@\n-  \/\/ Note: Callsite is already pushed by prepare_invoke.\n+  \/\/ Note: Callsite is already pushed\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.cpp","additions":193,"deletions":194,"binary":false,"changes":387,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023 SAP SE. All rights reserved.\n@@ -29,5 +29,1 @@\n-  static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ If caller wants to see it.\n-                             Register flags = noreg); \/\/ If caller wants to test it.\n+  static void prepare_invoke(Register cache, Register recv);\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  u1 _bytecode1, _bytecode2;         \/\/ Resovled invoke codes\n+  u1 _bytecode1, _bytecode2;         \/\/ Resolved invoke codes\n","filename":"src\/hotspot\/share\/oops\/resolvedMethodEntry.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}