{"files":[{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -204,58 +205,0 @@\n-\/\/ Return\n-\/\/ Rindex: index into constant pool\n-\/\/ Rcache: address of cache entry - ConstantPoolCache::base_offset()\n-\/\/\n-\/\/ A caller must add ConstantPoolCache::base_offset() to Rcache to get\n-\/\/ the true address of the cache entry.\n-\/\/\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache,\n-                                                           Register index,\n-                                                           int bcp_offset,\n-                                                           size_t index_size) {\n-  assert_different_registers(cache, index);\n-  assert_different_registers(cache, rcpool);\n-  get_cache_index_at_bcp(index, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry\n-  \/\/ aarch64 already has the cache in rcpool so there is no need to\n-  \/\/ install it in cache. instead we pre-add the indexed offset to\n-  \/\/ rcpool and return it in cache. All clients of this method need to\n-  \/\/ be modified accordingly.\n-  add(cache, rcpool, index, Assembler::LSL, 5);\n-}\n-\n-\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register index,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n-  \/\/ We use a 32-bit load here since the layout of 64-bit words on\n-  \/\/ little-endian machines allow us that.\n-  \/\/ n.b. unlike x86 cache already includes the index offset\n-  lea(bytecode, Address(cache,\n-                         ConstantPoolCache::base_offset()\n-                         + ConstantPoolCacheEntry::indices_offset()));\n-  ldarw(bytecode, bytecode);\n-  const int shift_count = (1 + byte_no) * BitsPerByte;\n-  ubfx(bytecode, bytecode, shift_count, BitsPerByte);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  assert(cache != tmp, \"must use different register\");\n-  get_cache_index_at_bcp(tmp, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  \/\/ and from word offset to byte offset\n-  assert(exact_log2(in_bytes(ConstantPoolCacheEntry::size_in_bytes())) == 2 + LogBytesPerWord, \"else change next line\");\n-  ldr(cache, Address(rfp, frame::interpreter_frame_cache_offset * wordSize));\n-  \/\/ skip past the header\n-  add(cache, cache, in_bytes(ConstantPoolCache::base_offset()));\n-  add(cache, cache, tmp, Assembler::LSL, 2 + LogBytesPerWord);  \/\/ construct pointer to cache entry\n-}\n-\n@@ -298,12 +241,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register method,\n-                                                              Register cache) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  ldr(method, Address(cache, method_offset)); \/\/ get f1 Method*\n-}\n-\n@@ -1869,0 +1800,12 @@\n+\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+  mov(cache, sizeof(ResolvedMethodEntry));\n+  mul(index, index, cache); \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+\n+  \/\/ Get address of field entries array\n+  ldr(cache, Address(rcpool, ConstantPoolCache::method_entries_offset()));\n+  add(cache, cache, Array<ResolvedMethodEntry>::base_offset_in_bytes());\n+  lea(cache, Address(cache, index));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":13,"deletions":70,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -143,3 +143,0 @@\n-  void get_cache_and_index_at_bcp(Register cache, Register index, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register index, Register bytecode, int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -155,2 +152,0 @@\n-  void load_resolved_method_at_index(int byte_no, Register method, Register cache);\n-\n@@ -327,0 +322,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -496,4 +497,3 @@\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-    __ ldr(cache, Address(cache, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n-    __ andr(cache, cache, ConstantPoolCacheEntry::parameter_size_mask);\n-\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_unsigned_short(cache, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -491,1 +492,1 @@\n-  __ andw(off, off, ConstantPoolCacheEntry::field_index_mask);\n+  __ andw(off, off, ConstantPoolCache::field_index_mask);\n@@ -498,2 +499,2 @@\n-  __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, flags, ConstantPoolCache::tos_state_shift,\n+           ConstantPoolCache::tos_state_bits);\n@@ -2260,1 +2261,1 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n@@ -2262,2 +2263,1 @@\n-                                            Register index,\n-                                            size_t index_size) {\n+                                            Register index) {\n@@ -2266,0 +2266,1 @@\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n@@ -2270,3 +2271,11 @@\n-\n-  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ lea(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::bytecode1_offset())));\n+      break;\n+    case f2_byte:\n+      __ lea(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::bytecode2_offset())));\n+      break;\n+  }\n+  \/\/ Load-acquire the bytecode to match store-release in InterpreterRuntime\n+  __ ldarb(temp, temp);\n@@ -2284,1 +2293,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n@@ -2291,1 +2300,1 @@\n-    __ load_resolved_method_at_index(byte_no, temp, Rcache);\n+    __ ldr(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2360,9 +2369,3 @@\n-\/\/ The Rcache and index registers must be set before call\n-\/\/ n.b unlike x86 cache already includes the index offset\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n@@ -2370,7 +2373,3 @@\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ ldr(off, Address(cache, in_bytes(cp_base_offset +\n-                                          ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ ldrw(flags, Address(cache, in_bytes(cp_base_offset +\n-                                           ConstantPoolCacheEntry::flags_offset())));\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method, cache, flags);\n@@ -2378,8 +2377,89 @@\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ ldr(obj, Address(cache, in_bytes(cp_base_offset +\n-                                        ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ ldr(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, r5, rscratch2);\n-  }\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ ldr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  \/\/ setup registers\n+  const Register index = ref_index;\n+  assert_different_registers(method, flags);\n+  assert_different_registers(method, cache, index);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ tbz(flags, ResolvedMethodEntry::has_appendix_shift, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ ldr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = method_or_table_index;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_forced_virtual_shift, NotVirtual);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVirtual);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ ldr(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ bind(Done);\n@@ -2457,38 +2537,0 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n-  \/\/ setup registers\n-  const Register cache = rscratch2;\n-  const Register index = r4;\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n-  \/\/ determine constant pool cache field offsets\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      (is_invokevirtual\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n-\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ ldr(method, Address(cache, method_offset));\n-\n-  if (itable_index != noreg) {\n-    __ ldr(itable_index, Address(cache, index_offset));\n-  }\n-  __ ldrw(flags, Address(cache, flags_offset));\n-}\n-\n-\n@@ -3245,7 +3287,2 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method, \/\/ linked method (or i-klass)\n-                                   Register index,  \/\/ itable index, MethodType, etc.\n-                                   Register recv,   \/\/ if caller wants to see it\n-                                   Register flags   \/\/ if caller wants to test it\n-                                   ) {\n-  \/\/ determine flags\n+void TemplateTable::prepare_invoke(Register cache, Register recv) {\n+\n@@ -3253,16 +3290,1 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == r3, \"\");\n-  assert(recv  == noreg || recv  == r2, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = r2;\n-  if (flags == noreg)  flags = r3;\n-  assert_different_registers(method, index, recv, flags);\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n@@ -3273,16 +3295,2 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ tbz(flags, ConstantPoolCacheEntry::has_appendix_shift, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push(r19);\n-    __ mov(r19, index);\n-    __ load_resolved_reference_at_index(index, r19);\n-    __ pop(r19);\n-    __ push(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Load TOS state for later\n+  __ load_unsigned_byte(rscratch2, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3292,7 +3300,2 @@\n-    __ andw(recv, flags, ConstantPoolCacheEntry::parameter_size_mask);\n-    \/\/ FIXME -- is this actually correct? looks like it should be 2\n-    \/\/ const int no_return_pc_pushed_yet = -1;  \/\/ argument slot correction before we push return address\n-    \/\/ const int receiver_is_at_end      = -1;  \/\/ back off one slot to get receiver\n-    \/\/ Address recv_addr = __ argument_address(recv, no_return_pc_pushed_yet + receiver_is_at_end);\n-    \/\/ __ movptr(recv, recv_addr);\n-    __ add(rscratch1, esp, recv, ext::uxtx, 3); \/\/ FIXME: uxtb here?\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n+    __ add(rscratch1, esp, recv, ext::uxtx, 3);\n@@ -3303,4 +3306,0 @@\n-  \/\/ compute return type\n-  \/\/ x86 uses a shift and mask or wings it with a shift plus assert\n-  \/\/ the mask is not needed. aarch64 just uses bitfield extract\n-  __ ubfxw(rscratch2, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);\n@@ -3324,1 +3323,1 @@\n-  __ tbz(flags, ConstantPoolCacheEntry::is_vfinal_shift, notFinal);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, notFinal);\n@@ -3363,1 +3362,4 @@\n-  prepare_invoke(byte_no, rmethod, noreg, r2, r3);\n+  load_resolved_method_entry_virtual(r2,      \/\/ ResolvedMethodEntry*\n+                                     rmethod, \/\/ Method* or itable index\n+                                     r3);     \/\/ flags\n+  prepare_invoke(r2, r2); \/\/ recv\n@@ -3377,2 +3379,4 @@\n-  prepare_invoke(byte_no, rmethod, noreg,  \/\/ get f1 Method*\n-                 r2);  \/\/ get receiver also for null check\n+  load_resolved_method_entry_special_or_static(r2,      \/\/ ResolvedMethodEntry*\n+                                               rmethod, \/\/ Method*\n+                                               r3);     \/\/ flags\n+  prepare_invoke(r2, r2);  \/\/ get receiver also for null check\n@@ -3392,1 +3396,5 @@\n-  prepare_invoke(byte_no, rmethod);  \/\/ get f1 Method*\n+  load_resolved_method_entry_special_or_static(r2,      \/\/ ResolvedMethodEntry*\n+                                               rmethod, \/\/ Method*\n+                                               r3);     \/\/ flags\n+  prepare_invoke(r2, r2);  \/\/ get receiver also for null check\n+\n@@ -3408,2 +3416,5 @@\n-  prepare_invoke(byte_no, r0, rmethod,  \/\/ get f1 Klass*, f2 Method*\n-                 r2, r3); \/\/ recv, flags\n+  load_resolved_method_entry_interface(r2,      \/\/ ResolvedMethodEntry*\n+                                       r0,      \/\/ Klass*\n+                                       rmethod, \/\/ Method* or itable\/vtable index\n+                                       r3);     \/\/ flags\n+  prepare_invoke(r2, r2); \/\/ receiver\n@@ -3422,1 +3433,1 @@\n-  __ tbz(r3, ConstantPoolCacheEntry::is_forced_virtual_shift, notObjectMethod);\n+  __ tbz(r3, ResolvedMethodEntry::is_forced_virtual_shift, notObjectMethod);\n@@ -3431,1 +3442,1 @@\n-  __ tbz(r3, ConstantPoolCacheEntry::is_vfinal_shift, notVFinal);\n+  __ tbz(r3, ResolvedMethodEntry::is_vfinal_shift, notVFinal);\n@@ -3528,1 +3539,6 @@\n-  prepare_invoke(byte_no, rmethod, r0, r2);\n+  load_resolved_method_entry_handle(r2,      \/\/ ResolvedMethodEntry*\n+                                    rmethod, \/\/ Method*\n+                                    r0,      \/\/ Resolved reference\n+                                    r3);     \/\/ flags\n+  prepare_invoke(r2, r2);\n+\n@@ -3550,1 +3566,1 @@\n-  \/\/ rmethod: MH.linkToCallSite method (from f2)\n+  \/\/ rmethod: MH.linkToCallSite method\n@@ -3552,1 +3568,1 @@\n-  \/\/ Note:  r0_callsite is already pushed by prepare_invoke\n+  \/\/ Note:  r0_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":151,"deletions":135,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -29,6 +29,1 @@\n-static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ if caller wants to see it\n-                             Register flags = noreg   \/\/ if caller wants to test it\n-                             );\n+  static void prepare_invoke(Register cache, Register recv);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -87,2 +87,0 @@\n-  void load_resolved_method_at_index(int byte_no, Register cache, Register method);\n-\n@@ -129,1 +127,0 @@\n-  void get_cache_and_index_at_bcp(Register cache, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -132,0 +129,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -456,7 +457,0 @@\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, int bcp_offset,\n-                                                           size_t index_size) {\n-  get_cache_index_at_bcp(cache, bcp_offset, index_size);\n-  sldi(cache, cache, exact_log2(in_words(ConstantPoolCacheEntry::size()) * BytesPerWord));\n-  add(cache, R27_constPoolCache, cache);\n-}\n-\n@@ -487,1 +481,1 @@\n-  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  \/\/ Get index out of bytecode pointer\n@@ -494,0 +488,1 @@\n+  addi(cache, cache, Array<ResolvedIndyEntry>::base_offset_in_bytes());\n@@ -514,0 +509,12 @@\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+  \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+  mulli(index, index, sizeof(ResolvedMethodEntry));\n+\n+  \/\/ Get address of field entries array\n+  ld_ptr(cache, ConstantPoolCache::method_entries_offset(), R27_constPoolCache);\n+  addi(cache, cache, Array<ResolvedMethodEntry>::base_offset_in_bytes());\n+  add(cache, cache, index); \/\/ method_entries + base_offset + scaled index\n+}\n+\n@@ -567,12 +574,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register cache,\n-                                                              Register method) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  ld(method, method_offset, cache); \/\/ get f1 Method*\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":15,"deletions":20,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -650,1 +651,1 @@\n-    __ lhz(size, Array<ResolvedIndyEntry>::base_offset_in_bytes() + in_bytes(ResolvedIndyEntry::num_parameters_offset()), cache);\n+    __ lhz(size, in_bytes(ResolvedIndyEntry::num_parameters_offset()), cache);\n@@ -652,8 +653,3 @@\n-    __ get_cache_and_index_at_bcp(cache, 1, index_size);\n-\n-    \/\/ Get least significant byte of 64 bit value:\n-#if defined(VM_LITTLE_ENDIAN)\n-    __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()), cache);\n-#else\n-    __ lbz(size, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()) + 7, cache);\n-#endif\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, size \/* tmp *\/);\n+    __ lhz(size, in_bytes(ResolvedMethodEntry::num_parameters_offset()), cache);\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2014, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2013, 2016 SAP SE. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2013, 2023 SAP SE. All rights reserved.\n@@ -29,3 +29,2 @@\n-  static void prepare_invoke(int byte_no, Register Rmethod, Register Rret_addr, Register Rindex, Register Rrecv, Register Rflags,\n-                             Register Rscratch1, Register Rscratch2);\n-  static void invokevfinal_helper(Register Rmethod, Register Rflags, Register Rscratch1, Register Rscratch2);\n+  static void prepare_invoke(Register Rcache, Register Rret_addr, Register Rrecv, Register Rscratch);\n+  static void invokevfinal_helper(Register Rcache, Register Rscratch1, Register Rscratch2, Register Rscratch3);\n@@ -33,1 +32,1 @@\n-  static void invokeinterface_object_method(Register Rrecv_klass, Register Rret, Register Rflags, Register Rindex, Register Rtemp, Register Rtemp2);\n+  static void invokeinterface_object_method(Register Rrecv_klass, Register Rret, Register Rflags, Register Rcache, Register Rtemp, Register Rtemp2);\n","filename":"src\/hotspot\/cpu\/ppc\/templateTable_ppc.hpp","additions":5,"deletions":6,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -392,1 +393,1 @@\n-  __ andi(off, flags, ConstantPoolCacheEntry::field_index_mask);\n+  __ andi(off, flags, ConstantPoolCache::field_index_mask);\n@@ -395,1 +396,1 @@\n-  __ rldicl(flags, flags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);\n+  __ rldicl(flags, flags, 64-ConstantPoolCache::tos_state_shift, 64-ConstantPoolCache::tos_state_bits);\n@@ -2194,1 +2195,0 @@\n-\/\/   - Rresult: Either noreg or output for f1\/f2.\n@@ -2198,3 +2198,2 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no, Register Rcache, Register Rscratch, size_t index_size) {\n-\n-  __ get_cache_and_index_at_bcp(Rcache, 1, index_size);\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no, Register Rcache, Register Rscratch) {\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n@@ -2202,0 +2201,1 @@\n+  Register Rindex = Rscratch;\n@@ -2211,7 +2211,5 @@\n-  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-  \/\/ We are resolved if the indices offset contains the current bytecode.\n-#if defined(VM_LITTLE_ENDIAN)\n-  __ lbz(Rscratch, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()) + byte_no + 1, Rcache);\n-#else\n-  __ lbz(Rscratch, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()) + 7 - (byte_no + 1), Rcache);\n-#endif\n+  const int bytecode_offset = (byte_no == f1_byte) ? in_bytes(ResolvedMethodEntry::bytecode1_offset())\n+                                                   : in_bytes(ResolvedMethodEntry::bytecode2_offset());\n+  __ load_method_entry(Rcache, Rindex);\n+  \/\/ Load-acquire the bytecode to match store-release in InterpreterRuntime\n+  __ lbz(Rscratch, bytecode_offset, Rcache);\n@@ -2230,1 +2228,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, 1, index_size);\n+  __ load_method_entry(Rcache, Rindex);\n@@ -2241,1 +2239,1 @@\n-    __ load_resolved_method_at_index(byte_no, Rcache, method);\n+    __ ld(method, in_bytes(ResolvedMethodEntry::method_offset()), Rcache);\n@@ -2311,16 +2309,4 @@\n-\/\/ Load the constant pool cache entry at field accesses into registers.\n-\/\/ The Rcache and Rindex registers must be set before call.\n-\/\/ Input:\n-\/\/   - Rcache, Rindex\n-\/\/ Output:\n-\/\/   - Robj, Roffset, Rflags\n-\/\/ Kills:\n-\/\/   - R11, R12\n-void TemplateTable::load_field_cp_cache_entry(Register Robj,\n-                                              Register Rcache,\n-                                              Register Rindex \/* unused on PPC64 *\/,\n-                                              Register Roffset,\n-                                              Register Rflags,\n-                                              bool is_static) {\n-  assert_different_registers(Rcache, Rflags, Roffset, R11_scratch1, R12_scratch2);\n-  assert(Rindex == noreg, \"parameter not used on PPC64\");\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  assert_different_registers(cache, method, flags);\n@@ -2328,8 +2314,4 @@\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  __ ld(Rflags, in_bytes(cp_base_offset) + in_bytes(ConstantPoolCacheEntry::flags_offset()), Rcache);\n-  __ ld(Roffset, in_bytes(cp_base_offset) + in_bytes(ConstantPoolCacheEntry::f2_offset()), Rcache);\n-  if (is_static) {\n-    __ ld(Robj, in_bytes(cp_base_offset) + in_bytes(ConstantPoolCacheEntry::f1_offset()), Rcache);\n-    __ ld(Robj, in_bytes(Klass::java_mirror_offset()), Robj);\n-    __ resolve_oop_handle(Robj, R11_scratch1, R12_scratch2, MacroAssembler::PRESERVATION_NONE);\n-    \/\/ Acquire not needed here. Following access has an address dependency on this value.\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, method \/* tmp *\/);\n+  if (flags != noreg) {\n+    __ lbz(flags, in_bytes(ResolvedMethodEntry::flags_offset()), cache);\n@@ -2337,0 +2319,57 @@\n+  __ ld(method, in_bytes(ResolvedMethodEntry::method_offset()), cache);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  \/\/ setup registers\n+  assert_different_registers(cache, method, ref_index, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, method \/* tmp *\/);\n+  __ lbz(flags, in_bytes(ResolvedMethodEntry::flags_offset()), cache);\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ testbitdi(CCR0, R0, flags, ResolvedMethodEntry::has_appendix_shift);\n+  __ bfalse(CCR0, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ lhz(ref_index, in_bytes(ResolvedMethodEntry::resolved_references_index_offset()), cache);\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  assert(cache->is_nonvolatile(), \"C-call in resolve_oop_handle\");\n+  __ load_resolved_reference_at_index(appendix, ref_index, R11_scratch1, R12_scratch2);\n+  __ verify_oop(appendix);\n+  __ push_ptr(appendix); \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ ld(method, in_bytes(ResolvedMethodEntry::method_offset()), cache);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  assert_different_registers(method_or_table_index, cache, flags);\n+  assert(klass == noreg, \"to be determined by caller\");\n+  assert(method_or_table_index == noreg, \"to be determined by caller\");\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, flags \/* tmp *\/);\n+  __ lbz(flags, in_bytes(ResolvedMethodEntry::flags_offset()), cache);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  assert_different_registers(cache, flags);\n+  assert(method_or_table_index == noreg, \"to be determined by caller\");\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, flags \/* tmp *\/);\n+  __ lbz(flags, in_bytes(ResolvedMethodEntry::flags_offset()), cache);\n@@ -2351,1 +2390,0 @@\n-  const int array_base_offset = Array<ResolvedIndyEntry>::base_offset_in_bytes();\n@@ -2357,1 +2395,1 @@\n-  __ ld_ptr(method, array_base_offset + in_bytes(ResolvedIndyEntry::method_offset()), cache);\n+  __ ld_ptr(method, in_bytes(ResolvedIndyEntry::method_offset()), cache);\n@@ -2371,1 +2409,1 @@\n-  __ ld_ptr(method, array_base_offset + in_bytes(ResolvedIndyEntry::method_offset()), cache);\n+  __ ld_ptr(method, in_bytes(ResolvedIndyEntry::method_offset()), cache);\n@@ -2380,1 +2418,1 @@\n-  __ lbz(index, array_base_offset + in_bytes(ResolvedIndyEntry::flags_offset()), cache);\n+  __ lbz(index, in_bytes(ResolvedIndyEntry::flags_offset()), cache);\n@@ -2385,1 +2423,1 @@\n-  __ lhz(index, array_base_offset + in_bytes(ResolvedIndyEntry::resolved_references_index_offset()), cache);\n+  __ lhz(index, in_bytes(ResolvedIndyEntry::resolved_references_index_offset()), cache);\n@@ -2399,1 +2437,1 @@\n-    __ lbz(index, array_base_offset + in_bytes(ResolvedIndyEntry::result_type_offset()), cache);\n+    __ lbz(index, in_bytes(ResolvedIndyEntry::result_type_offset()), cache);\n@@ -2407,48 +2445,0 @@\n-\/\/ Load the constant pool cache entry at invokes into registers.\n-\/\/ Resolve if necessary.\n-\n-\/\/ Input Registers:\n-\/\/   - None, bcp is used, though\n-\/\/\n-\/\/ Return registers:\n-\/\/   - Rmethod       (f1 field or f2 if invokevirtual)\n-\/\/   - Ritable_index (f2 field)\n-\/\/   - Rflags        (flags field)\n-\/\/\n-\/\/ Kills:\n-\/\/   - R21\n-\/\/\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register Rmethod,\n-                                               Register Ritable_index,\n-                                               Register Rflags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal,\n-                                               bool is_invokedynamic \/*unused*\/) {\n-\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Determine constant pool cache field offsets.\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int method_offset = in_bytes(cp_base_offset + (is_invokevirtual ? ConstantPoolCacheEntry::f2_offset() : ConstantPoolCacheEntry::f1_offset()));\n-  const int flags_offset  = in_bytes(cp_base_offset + ConstantPoolCacheEntry::flags_offset());\n-  \/\/ Access constant pool cache fields.\n-  const int index_offset  = in_bytes(cp_base_offset + ConstantPoolCacheEntry::f2_offset());\n-\n-  Register Rcache = R21_tmp1; \/\/ Note: same register as R21_sender_SP.\n-\n-  if (is_invokevfinal) {\n-    assert(Ritable_index == noreg, \"register not used\");\n-    \/\/ Already resolved.\n-    __ get_cache_and_index_at_bcp(Rcache, 1);\n-  } else {\n-    resolve_cache_and_index(byte_no, Rcache, \/* temp *\/ Rmethod, sizeof(u2));\n-  }\n-\n-  __ ld(Rmethod, method_offset, Rcache);\n-  __ ld(Rflags, flags_offset, Rcache);\n-\n-  if (Ritable_index != noreg) {\n-    __ ld(Ritable_index, index_offset, Rcache);\n-  }\n-}\n-\n@@ -3425,17 +3415,1 @@\n-\/\/ Common code for invoke\n-\/\/\n-\/\/ Input:\n-\/\/   - byte_no\n-\/\/\n-\/\/ Output:\n-\/\/   - Rmethod:        The method to invoke next or i-klass (invokeinterface).\n-\/\/   - Rret_addr:      The return address to return to.\n-\/\/   - Rindex:         MethodType (invokehandle), CallSite obj (invokedynamic) or Method (invokeinterface)\n-\/\/   - Rrecv:          Cache for \"this\" pointer, might be noreg if static call.\n-\/\/   - Rflags:         Method flags from const pool cache.\n-\/\/\n-\/\/  Kills:\n-\/\/   - Rscratch1\n-\/\/\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register Rmethod,  \/\/ linked method (or i-klass)\n+void TemplateTable::prepare_invoke(Register Rcache,\n@@ -3443,1 +3417,0 @@\n-                                   Register Rindex,   \/\/ itable index, MethodType, Method, etc.\n@@ -3445,3 +3418,1 @@\n-                                   Register Rflags,   \/\/ If caller wants to test it.\n-                                   Register Rscratch1,\n-                                   Register Rscratch2\n+                                   Register Rscratch\n@@ -3451,6 +3422,1 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = false; \/\/ should not reach here with invokedynamic\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (Rrecv != noreg);\n+  const bool load_receiver = (Rrecv != noreg);\n@@ -3459,25 +3425,0 @@\n-  assert_different_registers(Rmethod, Rindex, Rflags, Rscratch1);\n-  assert_different_registers(Rmethod, Rrecv, Rflags, Rscratch1);\n-  assert_different_registers(Rret_addr, Rscratch1);\n-\n-  load_invoke_cp_cache_entry(byte_no, Rmethod, Rindex, Rflags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ Saving of SP done in call_from_interpreter.\n-\n-  \/\/ Maybe push \"appendix\" to arguments.\n-  if (is_invokehandle) {\n-    Label Ldone;\n-    Register reference = Rscratch1;\n-\n-    __ rldicl_(R0, Rflags, 64-ConstantPoolCacheEntry::has_appendix_shift, 63);\n-    __ beq(CCR0, Ldone);\n-    \/\/ Push \"appendix\" (MethodType, CallSite, etc.).\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ load_resolved_reference_at_index(reference, Rindex, \/* temp *\/ Rret_addr, Rscratch2);\n-    __ verify_oop(reference);\n-    __ push_ptr(reference);\n-\n-    __ bind(Ldone);\n-  }\n-\n@@ -3486,2 +3427,2 @@\n-    Register Rparam_count = Rscratch1;\n-    __ andi(Rparam_count, Rflags, ConstantPoolCacheEntry::parameter_size_mask);\n+    Register Rparam_count = Rscratch;\n+    __ lhz(Rparam_count, in_bytes(ResolvedMethodEntry::num_parameters_offset()), Rcache);\n@@ -3494,1 +3435,1 @@\n-    Register Rtable_addr = Rscratch1;\n+    Register Rtable_addr = Rscratch;\n@@ -3498,2 +3439,1 @@\n-    \/\/ Get return type. It's coded into the upper 4 bits of the lower half of the 64 bit value.\n-    __ rldicl(Rret_type, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);\n+    __ lbz(Rret_type, in_bytes(ResolvedMethodEntry::type_offset()), Rcache);\n@@ -3530,3 +3470,1 @@\n-  Register Rtable_addr = R11_scratch1,\n-           Rret_type = R12_scratch2,\n-           Rret_addr = R5_ARG3,\n+  Register Rret_addr = R5_ARG3,\n@@ -3537,2 +3475,2 @@\n-           Rnum_params = R4_ARG2,\n-           Rnew_bc = R6_ARG4;\n+           Rnew_bc = R6_ARG4,\n+           Rcache = R7_ARG5;\n@@ -3542,1 +3480,1 @@\n-  load_invoke_cp_cache_entry(byte_no, Rvtableindex_or_method, noreg, Rflags, \/*virtual*\/ true, false, false);\n+  load_resolved_method_entry_virtual(Rcache, noreg, Rflags);\n@@ -3544,1 +3482,2 @@\n-  __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  \/\/ Handle final method separately.\n+  __ testbitdi(CCR0, R0, Rflags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3550,1 +3489,1 @@\n-  invokevfinal_helper(Rvtableindex_or_method, Rflags, R11_scratch1, R12_scratch2);\n+  invokevfinal_helper(Rcache, R11_scratch1, R12_scratch2, Rflags \/* tmp *\/);\n@@ -3554,4 +3493,1 @@\n-  \/\/ Load \"this\" pointer (receiver).\n-  __ rldicl(Rnum_params, Rflags, 64, 48);\n-  __ load_receiver(Rnum_params, Rrecv);\n-  __ verify_oop(Rrecv);\n+  prepare_invoke(Rcache, Rret_addr, Rrecv, R11_scratch1);\n@@ -3559,5 +3495,4 @@\n-  \/\/ Get return type. It's coded into the upper 4 bits of the lower half of the 64 bit value.\n-  __ rldicl(Rret_type, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);\n-  __ load_dispatch_table(Rtable_addr, Interpreter::invoke_return_entry_table());\n-  __ sldi(Rret_type, Rret_type, LogBytesPerWord);\n-  __ ldx(Rret_addr, Rret_type, Rtable_addr);\n+  \/\/ Get vtable index.\n+  __ lhz(Rvtableindex_or_method, in_bytes(ResolvedMethodEntry::table_index_offset()), Rcache);\n+\n+  \/\/ Get receiver klass.\n@@ -3575,4 +3510,3 @@\n-  Register Rflags  = R22_tmp2,\n-           Rmethod = R31;\n-  load_invoke_cp_cache_entry(byte_no, Rmethod, noreg, Rflags, \/*virtual*\/ true, \/*is_invokevfinal*\/ true, false);\n-  invokevfinal_helper(Rmethod, Rflags, R11_scratch1, R12_scratch2);\n+  Register Rcache  = R31;\n+  __ load_method_entry(Rcache, R11_scratch1);\n+  invokevfinal_helper(Rcache, R11_scratch1, R12_scratch2, R22_tmp2);\n@@ -3581,1 +3515,1 @@\n-void TemplateTable::invokevfinal_helper(Register Rmethod, Register Rflags, Register Rscratch1, Register Rscratch2) {\n+void TemplateTable::invokevfinal_helper(Register Rcache, Register Rscratch1, Register Rscratch2, Register Rscratch3) {\n@@ -3583,1 +3517,1 @@\n-  assert_different_registers(Rmethod, Rflags, Rscratch1, Rscratch2);\n+  assert_different_registers(Rcache, Rscratch1, Rscratch2, Rscratch3);\n@@ -3586,5 +3520,2 @@\n-  Register Rrecv = Rscratch2;\n-  Register Rnum_params = Rrecv;\n-\n-  __ ld(Rnum_params, in_bytes(Method::const_offset()), Rmethod);\n-  __ lhz(Rnum_params \/* number of params *\/, in_bytes(ConstMethod::size_of_parameters_offset()), Rnum_params);\n+  Register Rmethod = Rscratch3;\n+  __ ld(Rmethod, in_bytes(ResolvedMethodEntry::method_offset()), Rcache);\n@@ -3593,3 +3524,3 @@\n-  Register Rtable_addr = Rscratch1,\n-           Rret_addr   = Rflags,\n-           Rret_type   = Rret_addr;\n+  Register Rtable_addr = Rscratch2,\n+           Rret_addr   = Rcache,\n+           Rret_type   = Rscratch1;\n@@ -3597,1 +3528,1 @@\n-  __ rldicl(Rret_type, Rflags, 64-ConstantPoolCacheEntry::tos_state_shift, 64-ConstantPoolCacheEntry::tos_state_bits);\n+  __ lbz(Rret_type, in_bytes(ResolvedMethodEntry::type_offset()), Rcache);\n@@ -3600,1 +3531,6 @@\n-  __ ldx(Rret_addr, Rret_type, Rtable_addr);\n+  __ ldx(Rret_addr, Rret_type, Rtable_addr); \/\/ kills Rcache\n+\n+  Register Rnum_params = Rscratch2,\n+           Rrecv       = Rscratch2;\n+  __ ld(Rnum_params, in_bytes(Method::const_offset()), Rmethod);\n+  __ lhz(Rnum_params \/* number of params *\/, in_bytes(ConstMethod::size_of_parameters_offset()), Rnum_params);\n@@ -3618,1 +3554,1 @@\n-  Register Rtable_addr = R3_ARG1,\n+  Register Rcache      = R3_ARG1,\n@@ -3620,2 +3556,1 @@\n-           Rflags      = R5_ARG3,\n-           Rreceiver   = R6_ARG4,\n+           Rreceiver   = R5_ARG3,\n@@ -3624,1 +3559,4 @@\n-  prepare_invoke(byte_no, Rmethod, Rret_addr, noreg, Rreceiver, Rflags, R11_scratch1, R12_scratch2);\n+  load_resolved_method_entry_special_or_static(Rcache,  \/\/ ResolvedMethodEntry*\n+                                               Rmethod, \/\/ Method* or itable index\n+                                               noreg);  \/\/ flags\n+  prepare_invoke(Rcache, Rret_addr, Rreceiver, R11_scratch1); \/\/ recv\n@@ -3639,3 +3577,2 @@\n-  Register Rtable_addr = R3_ARG1,\n-           Rret_addr   = R4_ARG2,\n-           Rflags      = R5_ARG3;\n+  Register Rcache    = R3_ARG1,\n+           Rret_addr = R4_ARG2;\n@@ -3643,1 +3580,4 @@\n-  prepare_invoke(byte_no, R19_method, Rret_addr, noreg, noreg, Rflags, R11_scratch1, R12_scratch2);\n+  load_resolved_method_entry_special_or_static(Rcache,  \/\/ ResolvedMethodEntry*\n+                                               R19_method, \/\/ Method* or itable index\n+                                               noreg); \/\/ flags\n+  prepare_invoke(Rcache, Rret_addr, noreg, R11_scratch1); \/\/ recv\n@@ -3654,1 +3594,1 @@\n-                                                  Register Rmethod,\n+                                                  Register Rcache,\n@@ -3658,1 +3598,1 @@\n-  assert_different_registers(Rmethod, Rret, Rrecv_klass, Rflags, Rtemp1, Rtemp2);\n+  assert_different_registers(Rcache, Rret, Rrecv_klass, Rflags, Rtemp1, Rtemp2);\n@@ -3662,1 +3602,1 @@\n-  __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ testbitdi(CCR0, R0, Rflags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3665,1 +3605,3 @@\n-  Register Rscratch = Rflags; \/\/ Rflags is dead now.\n+  Register Rscratch = Rflags, \/\/ Rflags is dead now.\n+           Rmethod  = Rtemp2,\n+           Rindex   = Rtemp2;\n@@ -3670,0 +3612,1 @@\n+  __ ld(Rmethod, in_bytes(ResolvedMethodEntry::method_offset()), Rcache);\n@@ -3676,0 +3619,1 @@\n+  __ lhz(Rindex, in_bytes(ResolvedMethodEntry::table_index_offset()), Rcache);\n@@ -3677,1 +3621,1 @@\n-  generate_vtable_call(Rrecv_klass, Rmethod, Rret, Rscratch);\n+  generate_vtable_call(Rrecv_klass, Rindex, Rret, Rscratch);\n@@ -3686,5 +3630,0 @@\n-                 Rmethod          = R6_ARG4,\n-                 Rmethod2         = R9_ARG7,\n-                 Rinterface_klass = R5_ARG3,\n-                 Rret_addr        = R8_ARG6,\n-                 Rindex           = R10_ARG8,\n@@ -3693,1 +3632,7 @@\n-                 Rflags           = R7_ARG5;\n+                 Rinterface_klass = R5_ARG3,\n+                 Rmethod          = R6_ARG4,\n+                 Rmethod2         = R7_ARG5,\n+                 Rret_addr        = R8_ARG6,\n+                 Rindex           = R9_ARG7,\n+                 Rflags           = R10_ARG8,\n+                 Rcache           = R31;\n@@ -3695,1 +3640,2 @@\n-  prepare_invoke(byte_no, Rinterface_klass, Rret_addr, Rmethod, Rreceiver, Rflags, Rscratch1, \/* temp *\/ Rrecv_klass);\n+  load_resolved_method_entry_interface(Rcache, noreg, noreg, Rflags);\n+  prepare_invoke(Rcache, Rret_addr, Rreceiver, Rscratch1); \/\/ recv\n@@ -3705,1 +3651,1 @@\n-  \/\/ java.lang.Object. See ConstantPoolCacheEntry::set_method() for details:\n+  \/\/ java.lang.Object. See ResolvedMethodEntry for details:\n@@ -3710,1 +3656,1 @@\n-  __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_forced_virtual_shift);\n+  __ testbitdi(CCR0, R0, Rflags, ResolvedMethodEntry::is_forced_virtual_shift);\n@@ -3712,1 +3658,1 @@\n-  invokeinterface_object_method(Rrecv_klass, Rret_addr, Rflags, Rmethod, Rscratch1, Rscratch2);\n+  invokeinterface_object_method(Rrecv_klass, Rret_addr, Rflags, Rcache, Rscratch1, Rscratch2);\n@@ -3715,0 +3661,3 @@\n+  __ ld(Rinterface_klass, in_bytes(ResolvedMethodEntry::klass_offset()), Rcache);\n+  __ ld(Rmethod, in_bytes(ResolvedMethodEntry::method_offset()), Rcache);\n+\n@@ -3718,1 +3667,1 @@\n-  __ testbitdi(CCR0, R0, Rflags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ testbitdi(CCR0, R0, Rflags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3736,1 +3685,0 @@\n-\n@@ -3803,1 +3751,1 @@\n-                 Rflags    = R31,\n+                 Rflags    = R12_scratch2,\n@@ -3808,1 +3756,1 @@\n-                 Rscratch3 = R12_scratch2;\n+                 Rcache    = R31;\n@@ -3810,1 +3758,5 @@\n-  prepare_invoke(byte_no, Rmethod, Rret_addr, Rscratch1, Rrecv, Rflags, Rscratch2, Rscratch3);\n+  load_resolved_method_entry_handle(Rcache,  \/\/ ResolvedMethodEntry*\n+                                    Rmethod, \/\/ Method* or itable index\n+                                    Rscratch1,\n+                                    Rflags);\n+  prepare_invoke(Rcache, Rret_addr, Rrecv, Rscratch1);\n","filename":"src\/hotspot\/cpu\/ppc\/templateTable_ppc_64.cpp","additions":163,"deletions":211,"binary":false,"changes":374,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -232,65 +233,0 @@\n-\/\/ Return\n-\/\/ Rindex: index into constant pool\n-\/\/ Rcache: address of cache entry - ConstantPoolCache::base_offset()\n-\/\/\n-\/\/ A caller must add ConstantPoolCache::base_offset() to Rcache to get\n-\/\/ the true address of the cache entry.\n-\/\/\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache,\n-                                                           Register index,\n-                                                           int bcp_offset,\n-                                                           size_t index_size) {\n-  assert_different_registers(cache, index);\n-  assert_different_registers(cache, xcpool);\n-  \/\/ register \"cache\" is trashed in next shadd, so lets use it as a temporary register\n-  get_cache_index_at_bcp(index, cache, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ Convert from field index to ConstantPoolCacheEntry\n-  \/\/ riscv already has the cache in xcpool so there is no need to\n-  \/\/ install it in cache. Instead we pre-add the indexed offset to\n-  \/\/ xcpool and return it in cache. All clients of this method need to\n-  \/\/ be modified accordingly.\n-  shadd(cache, index, xcpool, cache, 5);\n-}\n-\n-\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register index,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n-  \/\/ We use a 32-bit load here since the layout of 64-bit words on\n-  \/\/ little-endian machines allow us that.\n-  \/\/ n.b. unlike x86 cache already includes the index offset\n-  la(bytecode, Address(cache,\n-                       ConstantPoolCache::base_offset() +\n-                       ConstantPoolCacheEntry::indices_offset()));\n-  membar(MacroAssembler::AnyAny);\n-  lwu(bytecode, bytecode);\n-  membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n-  const int shift_count = (1 + byte_no) * BitsPerByte;\n-  slli(bytecode, bytecode, XLEN - (shift_count + BitsPerByte));\n-  srli(bytecode, bytecode, XLEN - BitsPerByte);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  assert_different_registers(cache, tmp);\n-  \/\/ register \"cache\" is trashed in next ld, so lets use it as a temporary register\n-  get_cache_index_at_bcp(tmp, cache, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ Convert from field index to ConstantPoolCacheEntry index\n-  \/\/ and from word offset to byte offset\n-  assert(exact_log2(in_bytes(ConstantPoolCacheEntry::size_in_bytes())) == 2 + LogBytesPerWord,\n-         \"else change next line\");\n-  ld(cache, Address(fp, frame::interpreter_frame_cache_offset * wordSize));\n-  \/\/ skip past the header\n-  add(cache, cache, in_bytes(ConstantPoolCache::base_offset()));\n-  \/\/ construct pointer to cache entry\n-  shadd(cache, tmp, cache, tmp, 2 + LogBytesPerWord);\n-}\n-\n@@ -322,12 +258,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register method,\n-                                                              Register cache) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  ld(method, Address(cache, method_offset)); \/\/ get f1 Method*\n-}\n-\n@@ -1963,1 +1887,0 @@\n-  la(cache, Address(cache, 0));\n@@ -1980,1 +1903,0 @@\n-  la(cache, Address(cache, 0));\n@@ -1995,0 +1917,12 @@\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  get_cache_index_at_bcp(index, cache, bcp_offset, sizeof(u2));\n+  mv(cache, sizeof(ResolvedMethodEntry));\n+  mul(index, index, cache); \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+\n+  \/\/ Get address of field entries array\n+  ld(cache, Address(xcpool, ConstantPoolCache::method_entries_offset()));\n+  add(cache, cache, Array<ResolvedMethodEntry>::base_offset_in_bytes());\n+  add(cache, cache, index);\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":13,"deletions":79,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -139,3 +139,0 @@\n-  void get_cache_and_index_at_bcp(Register cache, Register index, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register index, Register bytecode, int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -151,2 +148,0 @@\n-  void load_resolved_method_at_index(int byte_no, Register method, Register cache);\n-\n@@ -305,0 +300,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -457,3 +458,3 @@\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-    __ ld(cache, Address(cache, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n-    __ andi(cache, cache, ConstantPoolCacheEntry::parameter_size_mask);\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_unsigned_short(cache, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n","filename":"src\/hotspot\/cpu\/riscv\/templateInterpreterGenerator_riscv.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -474,1 +475,1 @@\n-  __ mv(t0, ConstantPoolCacheEntry::field_index_mask);\n+  __ mv(t0, ConstantPoolCache::field_index_mask);\n@@ -480,2 +481,2 @@\n-  __ slli(flags, flags, XLEN - (ConstantPoolCacheEntry::tos_state_shift + ConstantPoolCacheEntry::tos_state_bits));\n-  __ srli(flags, flags, XLEN - ConstantPoolCacheEntry::tos_state_bits); \/\/ (1 << 5) - 4 --> 28~31==> flags:0~3\n+  __ slli(flags, flags, XLEN - (ConstantPoolCache::tos_state_shift + ConstantPoolCache::tos_state_bits));\n+  __ srli(flags, flags, XLEN - ConstantPoolCache::tos_state_bits); \/\/ (1 << 5) - 4 --> 28~31==> flags:0~3\n@@ -2171,4 +2172,3 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register Rcache,\n-                                            Register index,\n-                                            size_t index_size) {\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register Rcache,\n+                                                       Register index) {\n@@ -2177,0 +2177,1 @@\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n@@ -2181,0 +2182,13 @@\n+  __ load_method_entry(Rcache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ add(temp, Rcache, in_bytes(ResolvedMethodEntry::bytecode1_offset()));\n+      break;\n+    case f2_byte:\n+      __ add(temp, Rcache, in_bytes(ResolvedMethodEntry::bytecode2_offset()));\n+      break;\n+  }\n+  \/\/ Load-acquire the bytecode to match store-release in InterpreterRuntime\n+  __ membar(MacroAssembler::AnyAny);\n+  __ lbu(temp, Address(temp, 0));\n+  __ membar(MacroAssembler::LoadLoad | MacroAssembler::LoadStore);\n@@ -2182,2 +2196,0 @@\n-  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);\n@@ -2185,1 +2197,1 @@\n-  __ beq(temp, t0, resolved);\n+  __ beq(temp, t0, resolved);  \/\/ have we resolved this bytecode?\n@@ -2196,1 +2208,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n@@ -2203,1 +2215,1 @@\n-    __ load_resolved_method_at_index(byte_no, temp, Rcache);\n+    __ ld(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2274,9 +2286,3 @@\n-\/\/ The Rcache and index registers must be set before call\n-\/\/ n.b unlike x86 cache already includes the index offset\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n@@ -2284,7 +2290,3 @@\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ ld(off, Address(cache, in_bytes(cp_base_offset +\n-                                     ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ lwu(flags, Address(cache, in_bytes(cp_base_offset +\n-                                        ConstantPoolCacheEntry::flags_offset())));\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method, cache, flags);\n@@ -2292,8 +2294,93 @@\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ ld(obj, Address(cache, in_bytes(cp_base_offset +\n-                                       ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ ld(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, x15, t1);\n-  }\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ ld(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  \/\/ setup registers\n+  const Register index = ref_index;\n+  assert_different_registers(method, flags);\n+  assert_different_registers(method, cache, index);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ test_bit(t0, flags, ResolvedMethodEntry::has_appendix_shift);\n+  __ beqz(t0, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push_reg(appendix); \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ ld(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = method_or_table_index;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ test_bit(t0, flags, ResolvedMethodEntry::is_forced_virtual_shift);\n+  __ beqz(t0, NotVirtual);\n+  __ test_bit(t0, flags, ResolvedMethodEntry::is_vfinal_shift);\n+  __ beqz(t0, NotVFinal);\n+  __ ld(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ j(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ j(Done);\n+\n+  __ bind(NotVirtual);\n+  __ ld(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ ld(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal; Label Done;\n+  __ test_bit(t0, flags, ResolvedMethodEntry::is_vfinal_shift);\n+  __ beqz(t0, NotVFinal);\n+  __ ld(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ j(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ bind(Done);\n@@ -2372,36 +2459,0 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n-  \/\/ setup registers\n-  const Register cache = t1;\n-  const Register index = x14;\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n-  \/\/ determine constant pool cache field offsets\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int method_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                     (is_invokevirtual ?\n-                                      ConstantPoolCacheEntry::f2_offset() :\n-                                      ConstantPoolCacheEntry::f1_offset()));\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n-\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ ld(method, Address(cache, method_offset));\n-\n-  if (itable_index != noreg) {\n-    __ ld(itable_index, Address(cache, index_offset));\n-  }\n-  __ lwu(flags, Address(cache, flags_offset));\n-}\n-\n@@ -3184,28 +3235,4 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method, \/\/ linked method (or i-klass)\n-                                   Register index,  \/\/ itable index, MethodType, etc.\n-                                   Register recv,   \/\/ if caller wants to see it\n-                                   Register flags   \/\/ if caller wants to test it\n-                                   ) {\n-  \/\/ determine flags\n-  const Bytecodes::Code code = bytecode();\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == x13, \"\");\n-  assert(recv  == noreg || recv  == x12, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv == noreg) {\n-    recv = x12;\n-  }\n-  if (flags == noreg) {\n-    flags = x13;\n-  }\n-  assert_different_registers(method, index, recv, flags);\n+void TemplateTable::prepare_invoke(Register cache, Register recv) {\n+\n+  Bytecodes::Code code = bytecode();\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n@@ -3216,17 +3243,2 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ test_bit(t0, flags, ConstantPoolCacheEntry::has_appendix_shift);\n-    __ beqz(t0, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push_reg(x9);\n-    __ mv(x9, index);\n-    __ load_resolved_reference_at_index(index, x9);\n-    __ pop_reg(x9);\n-    __ push_reg(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Load TOS state for later\n+  __ load_unsigned_byte(t1, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3236,1 +3248,1 @@\n-    __ andi(recv, flags, ConstantPoolCacheEntry::parameter_size_mask); \/\/ parameter_size_mask = 1 << 8\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n@@ -3242,4 +3254,0 @@\n-  \/\/ compute return type\n-  __ slli(t1, flags, XLEN - (ConstantPoolCacheEntry::tos_state_shift + ConstantPoolCacheEntry::tos_state_bits));\n-  __ srli(t1, t1, XLEN - ConstantPoolCacheEntry::tos_state_bits); \/\/ (1 << 5) - 4 --> 28~31==> t1:0~3\n-\n@@ -3262,1 +3270,1 @@\n-  __ test_bit(t0, flags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ test_bit(t0, flags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3298,1 +3306,4 @@\n-  prepare_invoke(byte_no, xmethod, noreg, x12, x13);\n+  load_resolved_method_entry_virtual(x12,      \/\/ ResolvedMethodEntry*\n+                                     xmethod, \/\/ Method* or itable index\n+                                     x13);     \/\/ flags\n+  prepare_invoke(x12, x12); \/\/ recv\n@@ -3311,2 +3322,5 @@\n-  prepare_invoke(byte_no, xmethod, noreg,  \/\/ get f1 Method*\n-                 x12);  \/\/ get receiver also for null check\n+  load_resolved_method_entry_special_or_static(x12,      \/\/ ResolvedMethodEntry*\n+                                               xmethod, \/\/ Method*\n+                                               x13);     \/\/ flags\n+  prepare_invoke(x12, x12);  \/\/ get receiver also for null check\n+\n@@ -3325,1 +3339,5 @@\n-  prepare_invoke(byte_no, xmethod);  \/\/ get f1 Method*\n+  load_resolved_method_entry_special_or_static(x12,      \/\/ ResolvedMethodEntry*\n+                                               xmethod, \/\/ Method*\n+                                               x13);     \/\/ flags\n+  prepare_invoke(x12, x12);  \/\/ get receiver also for null check\n+\n@@ -3340,2 +3358,5 @@\n-  prepare_invoke(byte_no, x10, xmethod,  \/\/ get f1 Klass*, f2 Method*\n-                 x12, x13);  \/\/ recv, flags\n+  load_resolved_method_entry_interface(x12,      \/\/ ResolvedMethodEntry*\n+                                       x10,      \/\/ Klass*\n+                                       xmethod, \/\/ Method* or itable\/vtable index\n+                                       x13);     \/\/ flags\n+  prepare_invoke(x12, x12); \/\/ receiver\n@@ -3354,1 +3375,1 @@\n-  __ test_bit(t0, x13, ConstantPoolCacheEntry::is_forced_virtual_shift);\n+  __ test_bit(t0, x13, ResolvedMethodEntry::is_forced_virtual_shift);\n@@ -3364,1 +3385,1 @@\n-  __ test_bit(t0, x13, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ test_bit(t0, x13, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3461,1 +3482,6 @@\n-  prepare_invoke(byte_no, xmethod, x10, x12);\n+  load_resolved_method_entry_handle(x12,      \/\/ ResolvedMethodEntry*\n+                                    xmethod, \/\/ Method*\n+                                    x10,      \/\/ Resolved reference\n+                                    x13);     \/\/ flags\n+  prepare_invoke(x12, x12);\n+\n@@ -3483,1 +3509,1 @@\n-  \/\/ xmethod: MH.linkToCallSite method (from f2)\n+  \/\/ xmethod: MH.linkToCallSite method\n@@ -3485,1 +3511,1 @@\n-  \/\/ Note: x10_callsite is already pushed by prepare_invoke\n+  \/\/ Note: x10_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":160,"deletions":134,"binary":false,"changes":294,"status":"modified"},{"patch":"@@ -30,6 +30,1 @@\n-static void prepare_invoke(int byte_no,\n-                           Register method,         \/\/ linked method (or i-klass)\n-                           Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                           Register recv  = noreg,  \/\/ if caller wants to see it\n-                           Register flags = noreg   \/\/ if caller wants to test it\n-                           );\n+static void prepare_invoke(Register cache, Register recv);\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.hpp","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -340,12 +341,0 @@\n-\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache, Register cpe_offset,\n-                                                           int bcp_offset, size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_and_index_at_bcp {\");\n-  assert_different_registers(cache, cpe_offset);\n-  get_cache_index_at_bcp(cpe_offset, bcp_offset, index_size);\n-  z_lg(cache, Address(Z_fp, _z_ijava_state_neg(cpoolCache)));\n-  \/\/ Convert from field index to ConstantPoolCache offset in bytes.\n-  z_sllg(cpe_offset, cpe_offset, exact_log2(in_words(ConstantPoolCacheEntry::size()) * BytesPerWord));\n-  BLOCK_COMMENT(\"}\");\n-}\n-\n@@ -392,18 +381,3 @@\n-\/\/ Kills Z_R0_scratch.\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register cpe_offset,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_and_index_and_bytecode_at_bcp {\");\n-  get_cache_and_index_at_bcp(cache, cpe_offset, bcp_offset, index_size);\n-\n-  \/\/ We want to load (from CP cache) the bytecode that corresponds to the passed-in byte_no.\n-  \/\/ It is located at (cache + cpe_offset + base_offset + indices_offset + (8-1) (last byte in DW) - (byte_no+1).\n-  \/\/ Instead of loading, shifting and masking a DW, we just load that one byte of interest with z_llgc (unsigned).\n-  const int base_ix_off = in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset());\n-  const int off_in_DW   = (8-1) - (1+byte_no);\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == ConstantPoolCacheEntry::bytecode_2_mask, \"common mask\");\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == 0xff, \"\");\n-  load_sized_value(bytecode, Address(cache, cpe_offset, base_ix_off+off_in_DW), 1, false \/*signed*\/);\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get field index out of bytecode pointer.\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n@@ -411,1 +385,14 @@\n-  BLOCK_COMMENT(\"}\");\n+  \/\/ Get the address of the ResolvedMethodEntry array.\n+  get_constant_pool_cache(cache);\n+  z_lg(cache, Address(cache, in_bytes(ConstantPoolCache::method_entries_offset())));\n+\n+  \/\/ Scale the index to form a byte offset into the ResolvedMethodEntry array\n+  size_t entry_size = sizeof(ResolvedMethodEntry);\n+  if (is_power_of_2(entry_size)) {\n+    z_sllg(index, index, exact_log2(entry_size));\n+  } else {\n+    z_mghi(index, entry_size);\n+  }\n+\n+  \/\/ Calculate the final field address.\n+  z_la(cache, Array<ResolvedMethodEntry>::base_offset_in_bytes(), index, cache);\n@@ -451,23 +438,0 @@\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  BLOCK_COMMENT(\"get_cache_entry_pointer_at_bcp {\");\n-    get_cache_and_index_at_bcp(cache, tmp, bcp_offset, index_size);\n-    add2reg_with_index(cache, in_bytes(ConstantPoolCache::base_offset()), tmp, cache);\n-  BLOCK_COMMENT(\"}\");\n-}\n-\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register cache,\n-                                                              Register cpe_offset,\n-                                                              Register method) {\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  z_lg(method, Address(cache, cpe_offset, method_offset)); \/\/ get f1 Method*\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":18,"deletions":54,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -114,1 +114,0 @@\n-  void get_cache_and_index_at_bcp(Register cache, Register cpe_offset, int bcp_offset, size_t index_size = sizeof(u2));\n@@ -116,4 +115,2 @@\n-  void load_field_entry(Register cache, Register index, int bcp_offset = 1);\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache, Register cpe_offset, Register bytecode,\n-                                               int byte_no, int bcp_offset, size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache, Register tmp, int bcp_offset, size_t index_size = sizeof(u2));\n+  void load_field_entry (Register cache, Register index, int bcp_offset = 1);\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n@@ -125,2 +122,0 @@\n-  void load_resolved_method_at_index(int byte_no, Register cache, Register cpe_offset, Register method);\n-\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.hpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -661,6 +662,3 @@\n-    const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                      ConstantPoolCacheEntry::flags_offset());\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-\n-    \/\/ #args is in rightmost byte of the _flags field.\n-    __ z_llgc(size, Address(cache, index, flags_offset + (sizeof(size_t) - 1)));\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_sized_value(size, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())), sizeof(u2), false \/*is_signed*\/);\n","filename":"src\/hotspot\/cpu\/s390\/templateInterpreterGenerator_s390.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -547,1 +548,1 @@\n-  assert(ConstantPoolCacheEntry::field_index_mask == 0xffff, \"or use other instructions\");\n+  assert(ConstantPoolCache::field_index_mask == 0xffff, \"or use other instructions\");\n@@ -552,3 +553,1 @@\n-  __ z_srl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags for tos_state after the above shift.\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n+  __ z_srl(flags, ConstantPoolCache::tos_state_shift);\n@@ -2354,8 +2353,6 @@\n-\/\/ NOTE: index is already computed as byte offset, so we must not\n-\/\/ shift it afterwards!\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register cache,\n-                                            Register index,\n-                                            size_t index_size) {\n-\n-  assert_different_registers(cache, index, Z_R1_scratch);\n+\/\/ Register Killed: Z_R1_scratch\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register Rcache,\n+                                                       Register index) {\n+  BLOCK_COMMENT(\"resolve_cache_and_index_for_method {\");\n+  assert_different_registers(Rcache, index);\n@@ -2364,2 +2361,2 @@\n-  const Register  bytecode_in_cpcache = Z_R1_scratch;\n-  NearLabel       resolved, clinit_barrier_slow;\n+  Label resolved, clinit_barrier_slow;\n+\n@@ -2367,0 +2364,6 @@\n+  switch (code) {\n+    case Bytecodes::_nofast_getfield: code = Bytecodes::_getfield; break;\n+    case Bytecodes::_nofast_putfield: code = Bytecodes::_putfield; break;\n+    default:\n+      break;\n+  }\n@@ -2368,1 +2371,2 @@\n-  BLOCK_COMMENT(\"resolve_cache_and_index {\");\n+  const int bc_offset = (byte_no == f1_byte) ? in_bytes(ResolvedMethodEntry::bytecode1_offset())\n+                                             : in_bytes(ResolvedMethodEntry::bytecode2_offset());\n@@ -2370,3 +2374,3 @@\n-  __ get_cache_and_index_and_bytecode_at_bcp(cache, index, bytecode_in_cpcache, byte_no, 1, index_size);\n-  \/\/ Have we resolved this bytecode?\n-  __ compare32_and_branch(bytecode_in_cpcache, (int)code, Assembler::bcondEqual, resolved);\n+  __ load_method_entry(Rcache, index);\n+  __ z_cli(Address(Rcache, bc_offset), code);\n+  __ z_bre(resolved);\n@@ -2374,1 +2378,1 @@\n-  \/\/ Resolve first time through via runtime call.\n+  \/\/ Resolve first time through\n@@ -2380,2 +2384,0 @@\n-  \/\/ Update registers with resolved info.\n-  __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n@@ -2383,0 +2385,2 @@\n+  \/\/ Update registers with resolved info.\n+  __ load_method_entry(Rcache, index);\n@@ -2389,2 +2393,1 @@\n-\n-    __ load_resolved_method_at_index(byte_no, cache, index, method);\n+    __ z_lg(method, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2395,1 +2398,1 @@\n-  BLOCK_COMMENT(\"} resolve_cache_and_index\");\n+  BLOCK_COMMENT(\"} resolve_cache_and_index_for_method\");\n@@ -2401,0 +2404,1 @@\n+  BLOCK_COMMENT(\"resolve_cache_and_index_for_field {\");\n@@ -2415,5 +2419,4 @@\n-  if (byte_no == f1_byte) {\n-    __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::get_code_offset())), code);\n-  } else {\n-    __ z_cli(Address(cache, in_bytes(ResolvedFieldEntry::put_code_offset())), code);\n-  }\n+  const int code_offset = (byte_no == f1_byte) ? in_bytes(ResolvedFieldEntry::get_code_offset()) :\n+                                                 in_bytes(ResolvedFieldEntry::put_code_offset()) ;\n+\n+  __ z_cli(Address(cache, code_offset), code);\n@@ -2431,0 +2434,2 @@\n+\n+  BLOCK_COMMENT(\"} resolve_cache_and_index_for_field\");\n@@ -2461,24 +2466,0 @@\n-\/\/ The Rcache and index registers must be set before call.\n-\/\/ Index is already a byte offset, don't shift!\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-\n-  \/\/ Field offset\n-  __ mem2reg_opt(off, Address(cache, index, cp_base_offset + ConstantPoolCacheEntry::f2_offset()));\n-  \/\/ Flags. Must load 64 bits.\n-  __ mem2reg_opt(flags, Address(cache, index, cp_base_offset + ConstantPoolCacheEntry::flags_offset()));\n-\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ mem2reg_opt(obj, Address(cache, index, cp_base_offset + ConstantPoolCacheEntry::f1_offset()));\n-    __ mem2reg_opt(obj, Address(obj, Klass::java_mirror_offset()));\n-    __ resolve_oop_handle(obj);\n-  }\n-}\n-\n@@ -2496,1 +2477,1 @@\n-  __ z_clgij(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n+  __ compare64_and_branch(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n@@ -2506,1 +2487,1 @@\n-  __ z_clgij(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n+  __ compare64_and_branch(method, (unsigned long)nullptr, Assembler::bcondNotEqual, resolved); \/\/ method != 0, jump to resolved\n@@ -2512,1 +2493,1 @@\n-  __ z_llgc(index, Address(cache, in_bytes(ResolvedIndyEntry::flags_offset())));\n+  __ load_sized_value(index, Address(cache, in_bytes(ResolvedIndyEntry::flags_offset())), sizeof(u1), false \/*is_signed*\/);\n@@ -2516,1 +2497,1 @@\n-  __ z_llgh(index, Address(cache, in_bytes(ResolvedIndyEntry::resolved_references_index_offset())));\n+  __ load_sized_value(index, Address(cache, in_bytes(ResolvedIndyEntry::resolved_references_index_offset())), sizeof(u2), false \/*is_signed*\/);\n@@ -2527,1 +2508,1 @@\n-  __ z_llgc(ret_type, Address(cache, in_bytes(ResolvedIndyEntry::result_type_offset())));\n+  __ load_sized_value(ret_type, Address(cache, in_bytes(ResolvedIndyEntry::result_type_offset())), sizeof(u1), false \/*is_signed*\/);\n@@ -2533,5 +2514,0 @@\n-  \/\/ const int r_bitpos  = 63 - bit_shift;\n-  \/\/ const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::tos_state_bits + 1;\n-  \/\/ const int n_rotate  = bit_shift-ConstantPoolCacheEntry::tos_state_shift;\n-  \/\/ __ rotate_then_insert(ret_type, Z_R0_scratch, l_bitpos, r_bitpos, n_rotate, true);\n-  \/\/ Make sure we don't need to mask flags for tos_state after the above shift.\n@@ -2539,1 +2515,0 @@\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n@@ -2543,32 +2518,5 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/\/ unused\n-                                               bool is_invokedynamic \/* unused *\/) {\n-  BLOCK_COMMENT(\"load_invoke_cp_cache_entry {\");\n-  \/\/ Setup registers.\n-  const Register cache     = Z_ARG1;\n-  const Register cpe_offset= flags;\n-  const ByteSize base_off  = ConstantPoolCache::base_offset();\n-  const ByteSize f1_off    = ConstantPoolCacheEntry::f1_offset();\n-  const ByteSize f2_off    = ConstantPoolCacheEntry::f2_offset();\n-  const ByteSize flags_off = ConstantPoolCacheEntry::flags_offset();\n-  const int method_offset  = in_bytes(base_off + ((byte_no == f2_byte) ? f2_off : f1_off));\n-  const int flags_offset   = in_bytes(base_off + flags_off);\n-  \/\/ Access constant pool cache fields.\n-  const int index_offset   = in_bytes(base_off + f2_off);\n-\n-  assert_different_registers(method, itable_index, flags, cache);\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-\n-  if (is_invokevfinal) {\n-    \/\/ Already resolved.\n-     assert(itable_index == noreg, \"register not used\");\n-     __ get_cache_and_index_at_bcp(cache, cpe_offset, 1);\n-  } else {\n-    \/\/ Need to resolve.\n-    resolve_cache_and_index(byte_no, cache, cpe_offset, sizeof(u2));\n-  }\n-  __ z_lg(method, Address(cache, cpe_offset, method_offset));\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  assert_different_registers(method, cache, ref_index, flags);\n@@ -2576,3 +2524,2 @@\n-  if (itable_index != noreg) {\n-    __ z_lg(itable_index, Address(cache, cpe_offset, index_offset));\n-  }\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, method \/* index *\/);\n@@ -2580,4 +2527,15 @@\n-  \/\/ Only load the lower 4 bytes and fill high bytes of flags with zeros.\n-  \/\/ Callers depend on this zero-extension!!!\n-  \/\/ Attention: overwrites cpe_offset == flags\n-  __ z_llgf(flags, Address(cache, cpe_offset, flags_offset + (BytesPerLong-BytesPerInt)));\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/* is signed *\/);\n+  __ testbit(flags, ResolvedMethodEntry::has_appendix_shift); \/\/ life ended for flags\n+  __ z_bfalse(L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_sized_value(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())), sizeof(u2), false \/* is signed *\/);\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ verify_oop(appendix);\n+  __ push_ptr(appendix);  \/\/ Push appendix (MethodType, CallSite, etc.).\n+  __ bind(L_no_push);\n@@ -2585,1 +2543,1 @@\n-  BLOCK_COMMENT(\"} load_invoke_cp_cache_entry\");\n+  __ z_lg(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -3542,40 +3500,10 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method,  \/\/ linked method (or i-klass)\n-                                   Register index,   \/\/ itable index, MethodType, etc.\n-                                   Register recv,    \/\/ If caller wants to see it.\n-                                   Register flags) { \/\/ If caller wants to test it.\n-  \/\/ Determine flags.\n-  const Bytecodes::Code code = bytecode();\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = false; \/\/ should not reach here with invokedynamic\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-\n-  \/\/ Setup registers & access constant pool cache.\n-  if (recv  == noreg) { recv  = Z_ARG1; }\n-  if (flags == noreg) { flags = Z_ARG2; }\n-  assert_different_registers(method, Z_R14, index, recv, flags);\n-\n-  BLOCK_COMMENT(\"prepare_invoke {\");\n-\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ Maybe push appendix to arguments.\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    Register resolved_reference = Z_R1_scratch;\n-    __ testbit(flags, ConstantPoolCacheEntry::has_appendix_shift);\n-    __ z_bfalse(L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ load_resolved_reference_at_index(resolved_reference, index);\n-    __ verify_oop(resolved_reference);\n-    __ push_ptr(resolved_reference);  \/\/ Push appendix (MethodType, CallSite, etc.).\n-    __ bind(L_no_push);\n-  }\n-\n-  \/\/ Load receiver if needed (after appendix is pushed so parameter size is correct).\n+void TemplateTable::prepare_invoke(Register cache, Register recv) {\n+  Bytecodes::Code code     = bytecode();\n+  const Register  ret_type = Z_R1_scratch;\n+  const bool load_receiver = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n+  assert_different_registers(ret_type, recv);\n+\n+  \/\/ Load TOS state for later\n+  __ load_sized_value(ret_type, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())), sizeof(u1), false \/* is signed *\/);\n+\n+  \/\/ load receiver if needed (note: no return address pushed yet)\n@@ -3583,14 +3511,4 @@\n-    assert(!is_invokedynamic, \"\");\n-    \/\/ recv := int2long(flags & ConstantPoolCacheEntry::parameter_size_mask) << 3\n-    \/\/ Flags is zero-extended int2long when loaded during load_invoke_cp_cache_entry().\n-    \/\/ Only the least significant byte (psize) of flags is used.\n-    {\n-      const unsigned int logSES = Interpreter::logStackElementSize;\n-      const int bit_shift = logSES;\n-      const int r_bitpos  = 63 - bit_shift;\n-      const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::parameter_size_bits + 1;\n-      const int n_rotate  = bit_shift;\n-      assert(ConstantPoolCacheEntry::parameter_size_mask == 255, \"adapt bitpositions\");\n-      __ rotate_then_insert(recv, flags, l_bitpos, r_bitpos, n_rotate, true);\n-    }\n-    \/\/ Recv now contains #arguments * StackElementSize.\n+    __ load_sized_value(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())), sizeof(u2), false \/* is signed *\/);\n+    const unsigned int bit_shift = Interpreter::logStackElementSize;\n+    __ z_sllg(recv, recv, bit_shift);\n+    \/\/ recv now contains #arguments * StackElementSize.\n@@ -3605,3 +3523,0 @@\n-  Register ret_type = Z_R1_scratch;\n-  assert_different_registers(ret_type, method);\n-\n@@ -3610,0 +3525,28 @@\n+  __ z_sllg(ret_type, ret_type, LogBytesPerWord);\n+\n+  __ z_lg(Z_R14, Address(Z_R14, ret_type)); \/\/ Load return address.\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  assert_different_registers(method_or_table_index, cache, flags, klass);\n+  BLOCK_COMMENT(\"load_resolved_method_entry_interface {\");\n+  \/\/ determine constant pool cache field offsets\n+  const Register index = flags; \/\/ not containing anything important, let's kill it.\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/* is signed*\/);\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+  Label NotVirtual, NotVFinal, Done;\n+  __ testbit(flags, ResolvedMethodEntry::is_forced_virtual_shift);\n+  __ z_brz(NotVirtual);\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n+  __ z_brz(NotVFinal);\n+  __ z_lg(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ z_bru(Done);\n@@ -3611,9 +3554,3 @@\n-  {\n-    const int bit_shift = LogBytesPerWord;           \/\/ Size of each table entry.\n-    const int r_bitpos  = 63 - bit_shift;\n-    const int l_bitpos  = r_bitpos - ConstantPoolCacheEntry::tos_state_bits + 1;\n-    const int n_rotate  = bit_shift-ConstantPoolCacheEntry::tos_state_shift;\n-    __ rotate_then_insert(ret_type, flags, l_bitpos, r_bitpos, n_rotate, true);\n-    \/\/ Make sure we don't need to mask flags for tos_state after the above shift.\n-    ConstantPoolCacheEntry::verify_tos_state_shift();\n-  }\n+  __ bind(NotVFinal);\n+  __ load_sized_value(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())), sizeof(u2), false \/* is signed *\/);\n+  __ z_bru(Done);\n@@ -3621,2 +3558,6 @@\n-    __ z_lg(Z_R14, Address(Z_R14, ret_type)); \/\/ Load return address.\n-  BLOCK_COMMENT(\"} prepare_invoke\");\n+  __ bind(NotVirtual);\n+  __ z_lg(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ z_lg(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+\n+  BLOCK_COMMENT(\"} load_resolved_method_entry_interface\");\n@@ -3625,0 +3566,24 @@\n+\/\/ Registers Killed: Z_R1\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  BLOCK_COMMENT(\"load_resolved_method_entry_virtual {\");\n+  assert_different_registers(method_or_table_index, cache, flags);\n+  const Register index = flags; \/\/ doesn't contain valuable content, could be used as index for once\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/*is_signed*\/);\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal, Done;\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n+  __ z_brz(NotVFinal);\n+  __ z_lg(method_or_table_index,  Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ z_bru(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_sized_value(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())), sizeof(u2), false \/* is signed *\/);\n+  __ bind(Done);\n+  BLOCK_COMMENT(\"} load_resolved_method_entry_virtual\");\n+}\n@@ -3637,1 +3602,1 @@\n-  __ testbit(flags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3676,5 +3641,8 @@\n-  prepare_invoke(byte_no,\n-                 Z_ARG3,  \/\/ method or vtable index\n-                 noreg,   \/\/ unused itable index\n-                 Z_ARG1,  \/\/ recv\n-                 Z_ARG2); \/\/ flags\n+  const Register Rrecv   = Z_ARG1;\n+  const Register Rmethod = Z_ARG3;\n+  const Register Rflags  = Z_ARG2;\n+\n+  load_resolved_method_entry_virtual(Rrecv,\n+                                     Rmethod,\n+                                     Rflags);\n+  prepare_invoke(Rrecv, Rrecv);\n@@ -3685,1 +3653,1 @@\n-  invokevirtual_helper(Z_ARG3, Z_ARG1, Z_ARG2);\n+  invokevirtual_helper(Rmethod, Rrecv, Rflags);\n@@ -3693,4 +3661,7 @@\n-  prepare_invoke(byte_no, Rmethod, noreg, \/\/ Get f1 method.\n-                 Z_ARG3);   \/\/ Get receiver also for null check.\n-  __ verify_oop(Z_ARG3);\n-  __ null_check(Z_ARG3);\n+  Register Rrecv   = Z_ARG3;\n+  load_resolved_method_entry_special_or_static(Rrecv,\n+                                               Rmethod,\n+                                               noreg); \/* flags are not used here *\/\n+  prepare_invoke(Rrecv, Rrecv);\n+  __ verify_oop(Rrecv);\n+  __ null_check(Rrecv);\n@@ -3703,0 +3674,17 @@\n+\/*\n+ * There are only two callsite (invokestatic, invokespecial) for this method and both of them are passing\n+ * \"noreg\" for flags registers at present.\n+ *\/\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  assert_different_registers(method, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, method \/* index (temp) *\/);\n+  if (flags != noreg) {\n+    __ load_sized_value(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())), sizeof(u1), false \/* is signed *\/);\n+  }\n+  __ z_lg(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n@@ -3708,1 +3696,6 @@\n-  prepare_invoke(byte_no, Rmethod);   \/\/ Get f1 method.\n+  Register Rrecv   = Z_ARG1;\n+\n+  load_resolved_method_entry_special_or_static(Rrecv,\n+                                               Rmethod,\n+                                               noreg); \/* flags are not used here *\/\n+  prepare_invoke(Rrecv, Rrecv);  \/\/ get receiver also for null check and flags\n@@ -3734,2 +3727,5 @@\n-  prepare_invoke(byte_no, interface, method,  \/\/ Get f1 klassOop, f2 Method*.\n-                 receiver, flags);\n+  load_resolved_method_entry_interface(receiver,   \/\/ ResolvedMethodEntry*\n+                                       interface,  \/\/ Klass*  (interface klass (from f1))\n+                                       method,     \/\/ Method* or itable\/vtable index\n+                                       flags);     \/\/ flags\n+  prepare_invoke(receiver, receiver);\n@@ -3745,1 +3741,1 @@\n-  __ testbit(flags, ConstantPoolCacheEntry::is_forced_virtual_shift);\n+  __ testbit(flags, ResolvedMethodEntry::is_forced_virtual_shift);\n@@ -3752,1 +3748,1 @@\n-  __ testbit(flags, ConstantPoolCacheEntry::is_vfinal_shift);\n+  __ testbit(flags, ResolvedMethodEntry::is_vfinal_shift);\n@@ -3846,3 +3842,6 @@\n-  prepare_invoke(byte_no,\n-                 method, mtype,   \/\/ Get f2 method, f1 MethodType.\n-                 recv);\n+  const Register flags  = Z_R1_scratch;\n+  load_resolved_method_entry_handle(recv,\n+                                    method,\n+                                    mtype \/* index *\/,\n+                                    flags );\n+  prepare_invoke(recv, recv);\n@@ -3869,2 +3868,2 @@\n-  \/\/ Rmethod: CallSite object (from f1)\n-  \/\/ Rcallsite: MH.linkToCallSite method (from f2)\n+  \/\/ Rmethod: CallSite object\n+  \/\/ Rcallsite: MH.linkToCallSite method\n@@ -3872,1 +3871,1 @@\n-  \/\/ Note: Callsite is already pushed by prepare_invoke.\n+  \/\/ Note: Callsite is already pushed\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.cpp","additions":193,"deletions":194,"binary":false,"changes":387,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023 SAP SE. All rights reserved.\n@@ -29,5 +29,1 @@\n-  static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ If caller wants to see it.\n-                             Register flags = noreg); \/\/ If caller wants to test it.\n+  static void prepare_invoke(Register cache, Register recv);\n","filename":"src\/hotspot\/cpu\/s390\/templateTable_s390.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -457,50 +458,0 @@\n-void InterpreterMacroAssembler::get_cache_and_index_at_bcp(Register cache,\n-                                                           Register index,\n-                                                           int bcp_offset,\n-                                                           size_t index_size) {\n-  assert_different_registers(cache, index);\n-  get_cache_index_at_bcp(index, bcp_offset, index_size);\n-  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  assert(exact_log2(in_words(ConstantPoolCacheEntry::size())) == 2, \"else change next line\");\n-  shll(index, 2);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                                                        Register index,\n-                                                                        Register bytecode,\n-                                                                        int byte_no,\n-                                                                        int bcp_offset,\n-                                                                        size_t index_size) {\n-  get_cache_and_index_at_bcp(cache, index, bcp_offset, index_size);\n-  \/\/ We use a 32-bit load here since the layout of 64-bit words on\n-  \/\/ little-endian machines allow us that.\n-  movl(bytecode, Address(cache, index, Address::times_ptr, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::indices_offset()));\n-  const int shift_count = (1 + byte_no) * BitsPerByte;\n-  assert((byte_no == TemplateTable::f1_byte && shift_count == ConstantPoolCacheEntry::bytecode_1_shift) ||\n-         (byte_no == TemplateTable::f2_byte && shift_count == ConstantPoolCacheEntry::bytecode_2_shift),\n-         \"correct shift count\");\n-  shrl(bytecode, shift_count);\n-  assert(ConstantPoolCacheEntry::bytecode_1_mask == ConstantPoolCacheEntry::bytecode_2_mask, \"common mask\");\n-  andl(bytecode, ConstantPoolCacheEntry::bytecode_1_mask);\n-}\n-\n-void InterpreterMacroAssembler::get_cache_entry_pointer_at_bcp(Register cache,\n-                                                               Register tmp,\n-                                                               int bcp_offset,\n-                                                               size_t index_size) {\n-  assert_different_registers(cache, tmp);\n-\n-  get_cache_index_at_bcp(tmp, bcp_offset, index_size);\n-  assert(sizeof(ConstantPoolCacheEntry) == 4 * wordSize, \"adjust code below\");\n-  \/\/ convert from field index to ConstantPoolCacheEntry index\n-  \/\/ and from word offset to byte offset\n-  assert(exact_log2(in_bytes(ConstantPoolCacheEntry::size_in_bytes())) == 2 + LogBytesPerWord, \"else change next line\");\n-  shll(tmp, 2 + LogBytesPerWord);\n-  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n-  \/\/ skip past the header\n-  addptr(cache, in_bytes(ConstantPoolCache::base_offset()));\n-  addptr(cache, tmp);  \/\/ construct pointer to cache entry\n-}\n-\n@@ -535,15 +486,0 @@\n-void InterpreterMacroAssembler::load_resolved_method_at_index(int byte_no,\n-                                                              Register method,\n-                                                              Register cache,\n-                                                              Register index) {\n-  assert_different_registers(cache, index);\n-\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      ((byte_no == TemplateTable::f2_byte)\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-\n-  movptr(method, Address(cache, index, Address::times_ptr, method_offset)); \/\/ get f1 Method*\n-}\n-\n@@ -2073,1 +2009,1 @@\n-  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  \/\/ Get index out of bytecode pointer\n@@ -2100,0 +2036,10 @@\n+\n+void InterpreterMacroAssembler::load_method_entry(Register cache, Register index, int bcp_offset) {\n+  \/\/ Get index out of bytecode pointer\n+  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n+  get_cache_index_at_bcp(index, bcp_offset, sizeof(u2));\n+\n+  movptr(cache, Address(cache, ConstantPoolCache::method_entries_offset()));\n+  imull(index, index, sizeof(ResolvedMethodEntry)); \/\/ Scale the index to be the entry index * sizeof(ResolvedMethodEntry)\n+  lea(cache, Address(cache, index, Address::times_1, Array<ResolvedMethodEntry>::base_offset_in_bytes()));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":12,"deletions":66,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -106,14 +106,1 @@\n-  void get_cache_and_index_at_bcp(Register cache,\n-                                  Register index,\n-                                  int bcp_offset,\n-                                  size_t index_size = sizeof(u2));\n-  void get_cache_and_index_and_bytecode_at_bcp(Register cache,\n-                                               Register index,\n-                                               Register bytecode,\n-                                               int byte_no,\n-                                               int bcp_offset,\n-                                               size_t index_size = sizeof(u2));\n-  void get_cache_entry_pointer_at_bcp(Register cache,\n-                                      Register tmp,\n-                                      int bcp_offset,\n-                                      size_t index_size = sizeof(u2));\n+\n@@ -132,5 +119,0 @@\n-  void load_resolved_method_at_index(int byte_no,\n-                                     Register method,\n-                                     Register cache,\n-                                     Register index);\n-\n@@ -311,0 +293,1 @@\n+  void load_method_entry(Register cache, Register index, int bcp_offset = 1);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":2,"deletions":19,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -233,5 +234,4 @@\n-    __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n-    Register flags = cache;\n-    __ movl(flags, Address(cache, index, Address::times_ptr, ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset()));\n-    __ andl(flags, ConstantPoolCacheEntry::parameter_size_mask);\n-    __ lea(rsp, Address(rsp, flags, Interpreter::stackElementScale()));\n+    assert(index_size == sizeof(u2), \"Can only be u2\");\n+    __ load_method_entry(cache, index);\n+    __ load_unsigned_short(cache, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n+    __ lea(rsp, Address(rsp, cache, Interpreter::stackElementScale()));\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -527,1 +528,1 @@\n-  __ andl(off, ConstantPoolCacheEntry::field_index_mask);\n+  __ andl(off, ConstantPoolCache::field_index_mask);\n@@ -531,2 +532,2 @@\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  __ andl(flags, ConstantPoolCacheEntry::tos_state_mask);\n+  __ shrl(flags, ConstantPoolCache::tos_state_shift);\n+  __ andl(flags, ConstantPoolCache::tos_state_mask);\n@@ -2656,4 +2657,3 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register cache,\n-                                            Register index,\n-                                            size_t index_size) {\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register cache,\n+                                                       Register index) {\n@@ -2669,1 +2669,12 @@\n-  __ get_cache_and_index_and_bytecode_at_bcp(cache, index, temp, byte_no, 1, index_size);\n+\n+  __ load_method_entry(cache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedMethodEntry::bytecode1_offset())));\n+      break;\n+    case f2_byte:\n+      __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedMethodEntry::bytecode2_offset())));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -2680,1 +2691,1 @@\n-  __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n+  __ load_method_entry(cache, index);\n@@ -2691,1 +2702,1 @@\n-    __ load_resolved_method_at_index(byte_no, method, cache, index);\n+    __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2759,30 +2770,0 @@\n-\/\/ The cache and index registers must be set before call\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n-\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ movptr(off, Address(cache, index, Address::times_ptr,\n-                         in_bytes(cp_base_offset +\n-                                  ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ movl(flags, Address(cache, index, Address::times_ptr,\n-                         in_bytes(cp_base_offset +\n-                                  ConstantPoolCacheEntry::flags_offset())));\n-\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ movptr(obj, Address(cache, index, Address::times_ptr,\n-                           in_bytes(cp_base_offset +\n-                                    ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ movptr(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, rscratch2);\n-  }\n-}\n-\n@@ -2857,1 +2838,15 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(cache, index);\n+  assert_different_registers(method, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n@@ -2859,5 +2854,2 @@\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n+                                               Register ref_index,\n+                                               Register flags) {\n@@ -2865,1 +2857,0 @@\n-  const Register cache = rcx;\n@@ -2867,4 +2858,3 @@\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n+  assert_different_registers(cache, index);\n+  assert_different_registers(cache, method, ref_index, flags);\n+\n@@ -2872,6 +2862,2 @@\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n@@ -2879,3 +2865,13 @@\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ load_resolved_method_at_index(byte_no, method, cache, index);\n+  \/\/ Maybe push appendix\n+  Label L_no_push;\n+  __ testl(flags, (1 << ResolvedMethodEntry::has_appendix_shift));\n+  __ jcc(Assembler::zero, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n@@ -2883,5 +2879,60 @@\n-  if (itable_index != noreg) {\n-    \/\/ pick up itable or appendix index from f2 also:\n-    __ movptr(itable_index, Address(cache, index, Address::times_ptr, index_offset));\n-  }\n-  __ movl(flags, Address(cache, index, Address::times_ptr, flags_offset));\n+  __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(cache, klass, method_or_table_index, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ testl(flags, 1 << ResolvedMethodEntry::is_forced_virtual_shift);\n+  __ jcc(Assembler::zero, NotVirtual);\n+  __ testl(flags, (1 << ResolvedMethodEntry::is_vfinal_shift));\n+  __ jcc(Assembler::zero, NotVFinal);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ jmp(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ jmp(Done);\n+\n+  __ bind(NotVirtual);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ movptr(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(index, cache);\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label isVFinal; Label Done;\n+  __ testl(flags, (1 << ResolvedMethodEntry::is_vfinal_shift));\n+  __ jcc(Assembler::notZero, isVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ jmp(Done);\n+  __ bind(isVFinal);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ bind(Done);\n@@ -3634,6 +3685,1 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method,  \/\/ linked method (or i-klass)\n-                                   Register index,   \/\/ itable index, MethodType, etc.\n-                                   Register recv,    \/\/ if caller wants to see it\n-                                   Register flags    \/\/ if caller wants to test it\n-                                   ) {\n+void TemplateTable::prepare_invoke(Register cache, Register recv, Register flags) {\n@@ -3642,16 +3688,2 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == rdx, \"\");\n-  assert(recv  == noreg || recv  == rcx, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = rcx;\n-  if (flags == noreg)  flags = rdx;\n-  assert_different_registers(method, index, recv, flags);\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n+  assert_different_registers(recv, flags);\n@@ -3662,17 +3694,3 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ testl(flags, (1 << ConstantPoolCacheEntry::has_appendix_shift));\n-    __ jcc(Assembler::zero, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push(rbx);\n-    __ mov(rbx, index);\n-    __ load_resolved_reference_at_index(index, rbx);\n-    __ pop(rbx);\n-    __ push(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Save flags and load TOS\n+  __ movl(rbcp, flags);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3683,2 +3701,1 @@\n-    __ movl(recv, flags);\n-    __ andl(recv, ConstantPoolCacheEntry::parameter_size_mask);\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n@@ -3692,8 +3709,0 @@\n-  if (save_flags) {\n-    __ movl(rbcp, flags);\n-  }\n-\n-  \/\/ compute return type\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n@@ -3715,1 +3724,1 @@\n-  \/\/ Restore flags value from the constant pool cache, and restore rsi\n+  \/\/ Restore flags value from the constant pool cache entry, and restore rsi\n@@ -3717,4 +3726,2 @@\n-  if (save_flags) {\n-    __ movl(flags, rbcp);\n-    __ restore_bcp();\n-  }\n+  __ movl(flags, rbcp);\n+  __ restore_bcp();\n@@ -3734,1 +3741,1 @@\n-  __ andl(rax, (1 << ConstantPoolCacheEntry::is_vfinal_shift));\n+  __ andl(rax, (1 << ResolvedMethodEntry::is_vfinal_shift));\n@@ -3770,4 +3777,7 @@\n-  prepare_invoke(byte_no,\n-                 rbx,    \/\/ method or vtable index\n-                 noreg,  \/\/ unused itable index\n-                 rcx, rdx); \/\/ recv, flags\n+\n+  load_resolved_method_entry_virtual(rcx,  \/\/ ResolvedMethodEntry*\n+                                     rbx,  \/\/ Method or itable index\n+                                     rdx); \/\/ Flags\n+  prepare_invoke(rcx,  \/\/ ResolvedMethodEntry*\n+                 rcx,  \/\/ Receiver\n+                 rdx); \/\/ flags\n@@ -3778,1 +3788,0 @@\n-\n@@ -3785,2 +3794,8 @@\n-  prepare_invoke(byte_no, rbx, noreg,  \/\/ get f1 Method*\n-                 rcx);  \/\/ get receiver also for null check\n+\n+  load_resolved_method_entry_special_or_static(rcx,  \/\/ ResolvedMethodEntry*\n+                                               rbx,  \/\/ Method*\n+                                               rdx); \/\/ flags\n+  prepare_invoke(rcx,\n+                 rcx,  \/\/ get receiver also for null check\n+                 rdx); \/\/ flags\n+\n@@ -3798,1 +3813,7 @@\n-  prepare_invoke(byte_no, rbx);  \/\/ get f1 Method*\n+\n+  load_resolved_method_entry_special_or_static(rcx, \/\/ ResolvedMethodEntry*\n+                                               rbx, \/\/ Method*\n+                                               rdx  \/\/ flags\n+                                               );\n+  prepare_invoke(rcx, rcx, rdx);  \/\/ cache and flags\n+\n@@ -3816,2 +3837,0 @@\n-  prepare_invoke(byte_no, rax, rbx,  \/\/ get f1 Klass*, f2 Method*\n-                 rcx, rdx); \/\/ recv, flags\n@@ -3819,4 +3838,5 @@\n-  \/\/ rax: reference klass (from f1) if interface method\n-  \/\/ rbx: method (from f2)\n-  \/\/ rcx: receiver\n-  \/\/ rdx: flags\n+  load_resolved_method_entry_interface(rcx,  \/\/ ResolvedMethodEntry*\n+                                       rax,  \/\/ Klass*\n+                                       rbx,  \/\/ Method* or itable\/vtable index\n+                                       rdx); \/\/ flags\n+  prepare_invoke(rcx, rcx, rdx); \/\/ receiver, flags\n@@ -3831,1 +3851,1 @@\n-  __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_forced_virtual_shift));\n+  __ andl(rlocals, (1 << ResolvedMethodEntry::is_forced_virtual_shift));\n@@ -3833,0 +3853,1 @@\n+\n@@ -3843,1 +3864,1 @@\n-  __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_vfinal_shift));\n+  __ andl(rlocals, (1 << ResolvedMethodEntry::is_vfinal_shift));\n@@ -3964,1 +3985,3 @@\n-  prepare_invoke(byte_no, rbx_method, rax_mtype, rcx_recv);\n+  load_resolved_method_entry_handle(rcx, rbx_method, rax_mtype, rdx_flags);\n+  prepare_invoke(rcx, rcx_recv, rdx_flags);\n+\n@@ -3970,1 +3993,1 @@\n-  \/\/ rbx: MH.invokeExact_MT method (from f2)\n+  \/\/ rbx: MH.invokeExact_MT method\n@@ -3972,1 +3995,1 @@\n-  \/\/ Note:  rax_mtype is already pushed (if necessary) by prepare_invoke\n+  \/\/ Note:  rax_mtype is already pushed (if necessary)\n@@ -3989,2 +4012,2 @@\n-  \/\/ rax: CallSite object (from cpool->resolved_references[f1])\n-  \/\/ rbx: MH.linkToCallSite method (from f2)\n+  \/\/ rax: CallSite object (from cpool->resolved_references[])\n+  \/\/ rbx: MH.linkToCallSite method\n@@ -3992,1 +4015,1 @@\n-  \/\/ Note:  rax_callsite is already pushed by prepare_invoke\n+  \/\/ Note:  rax_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":165,"deletions":142,"binary":false,"changes":307,"status":"modified"},{"patch":"@@ -28,6 +28,3 @@\n-  static void prepare_invoke(int byte_no,\n-                             Register method,         \/\/ linked method (or i-klass)\n-                             Register index = noreg,  \/\/ itable index, MethodType, etc.\n-                             Register recv  = noreg,  \/\/ if caller wants to see it\n-                             Register flags = noreg   \/\/ if caller wants to test it\n-                             );\n+  static void prepare_invoke(Register cache,\n+                             Register recv,\n+                             Register flags);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.hpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1057,5 +1057,2 @@\n-        int cache_index = ConstantPool::decode_cpcache_index(index, true);\n-        assert(cache_index >= 0 && cache_index < pool->cache()->length(), \"unexpected cache index\");\n-        ConstantPoolCacheEntry* cpce = pool->cache()->entry_at(cache_index);\n-        cpce->set_method_handle(pool, info);\n-        appendix = Handle(current, cpce->appendix_if_resolved(pool)); \/\/ just in case somebody already resolved the entry\n+        ResolvedMethodEntry* entry = pool->cache()->set_method_handle(index, info);\n+        appendix = Handle(current, pool->cache()->appendix_if_resolved(entry));\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1538,2 +1538,2 @@\n-    ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-    if (cp_cache_entry->is_resolved(Bytecodes::_invokehandle)) {\n+    ResolvedMethodEntry* method_entry = cp->resolved_method_entry_at(index);\n+    if (method_entry->is_resolved(Bytecodes::_invokehandle)) {\n@@ -1541,2 +1541,2 @@\n-      Method* adapter = cp_cache_entry->f1_as_method();\n-      oop appendix = cp_cache_entry->appendix_if_resolved(cp);\n+      Method* adapter = method_entry->method();\n+      oop appendix = cp->cache()->appendix_if_resolved(method_entry);\n@@ -1594,1 +1594,1 @@\n-              int cp_cache_index = bcs.get_index_u2_cpcache();\n+              int cp_cache_index = bcs.get_index_u2();\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -410,1 +410,0 @@\n-      ConstantPoolCacheEntry* cp_cache_entry = nullptr;\n@@ -415,4 +414,0 @@\n-      \/\/ ResolvedIndyEntry and ConstantPoolCacheEntry must currently coexist.\n-      \/\/ To address this, the variables below contain the values that *might*\n-      \/\/ be used to avoid multiple blocks of similar code. When CPCE is obsoleted\n-      \/\/ these can be removed\n@@ -436,6 +431,4 @@\n-        cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-        cp_cache_entry->set_method_handle(cp, callInfo);\n-\n-        appendix = cp_cache_entry->appendix_if_resolved(cp);\n-        adapter_method = cp_cache_entry->f1_as_method();\n-        pool_index = cp_cache_entry->constant_pool_index();\n+        ResolvedMethodEntry* method_entry = cp->cache()->set_method_handle(index, callInfo);\n+        appendix = cp->cache()->appendix_if_resolved(method_entry);\n+        adapter_method = method_entry->method();\n+        pool_index = method_entry->constant_pool_index();\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -359,1 +359,1 @@\n-  return get_index_u2_cpcache();\n+  return get_index_u2();\n","filename":"src\/hotspot\/share\/ci\/ciStreams.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -164,5 +164,0 @@\n-  \/\/ Get 2-byte index in native byte order.  (Rewriter::rewrite makes these.)\n-  int get_index_u2_cpcache() const {\n-    return bytecode().get_index_u2_cpcache(cur_bc_raw());\n-  }\n-\n","filename":"src\/hotspot\/share\/ci\/ciStreams.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -59,4 +59,3 @@\n-  \/\/ This function is used to encode an index to differentiate it from a\n-  \/\/ constant pool index.  It assumes it is being called with a cpCache index\n-  \/\/ (that is less than 0).\n-  static int encode_cpcache_index(int index) {\n+  \/\/ This function is used to encode an invokedynamic index to differentiate it from a\n+  \/\/ constant pool index.  It assumes it is being called with a index that is less than 0\n+  static int encode_indy_index(int index) {\n","filename":"src\/hotspot\/share\/classfile\/resolutionErrors.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -113,1 +113,0 @@\n-class ConstantPoolCacheEntry;\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -275,1 +275,1 @@\n-        int method_index = invoke_bc.get_index_u2_cpcache(code);\n+        int method_index = invoke_bc.get_index_u2(code);\n@@ -383,1 +383,1 @@\n-        method->constants()->cache()->entry_at(index)->set_parameter_size(callee_parameters);\n+        method->constants()->cache()->resolved_method_entry_at(index)->set_num_parameters(callee_parameters);\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-    int encoded_index = ResolutionErrorTable::encode_cpcache_index(ConstantPool::encode_invokedynamic_index(_indy_index));\n+    int encoded_index = ResolutionErrorTable::encode_indy_index(ConstantPool::encode_invokedynamic_index(_indy_index));\n","filename":"src\/hotspot\/share\/interpreter\/bootstrapInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -167,2 +167,0 @@\n-  else if (Bytecodes::is_field_code(rawc))\n-    return get_index_u2(rawc);\n@@ -170,1 +168,1 @@\n-    return get_index_u2_cpcache(rawc);\n+    return get_index_u2(rawc);\n@@ -177,1 +175,1 @@\n-    return cpcache_entry()->constant_pool_index();\n+    return resolved_method_entry()->constant_pool_index();\n@@ -181,6 +179,0 @@\n-ConstantPoolCacheEntry* Bytecode_member_ref::cpcache_entry() const {\n-  int index = this->index();\n-  assert(invoke_code() != Bytecodes::_invokedynamic, \"should not call this\");\n-  return cpcache()->entry_at(ConstantPool::decode_cpcache_index(index, true));\n-}\n-\n@@ -193,0 +185,6 @@\n+ResolvedMethodEntry* Bytecode_member_ref::resolved_method_entry() const {\n+  int index = this->index();\n+  assert(invoke_code() != Bytecodes::_invokedynamic, \"should not call this\");\n+  return cpcache()->resolved_method_entry_at(index);\n+}\n+\n","filename":"src\/hotspot\/share\/interpreter\/bytecode.cpp","additions":8,"deletions":10,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -87,8 +87,0 @@\n-  int get_index_u1_cpcache(Bytecodes::Code bc) const {\n-    assert_same_format_as(bc); assert_index_size(1, bc);\n-    return *(u1*)addr_at(1) + ConstantPool::CPCACHE_INDEX_TAG;\n-  }\n-  int get_index_u2_cpcache(Bytecodes::Code bc) const {\n-    assert_same_format_as(bc); assert_index_size(2, bc); assert_native_index(bc);\n-    return Bytes::get_native_u2(addr_at(1)) + ConstantPool::CPCACHE_INDEX_TAG;\n-  }\n@@ -191,1 +183,0 @@\n-  ConstantPoolCacheEntry* cpcache_entry() const;\n@@ -193,0 +184,1 @@\n+  ResolvedMethodEntry* resolved_method_entry() const;\n","filename":"src\/hotspot\/share\/interpreter\/bytecode.hpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-    return cpcache_entry()->has_appendix();\n+    return resolved_method_entry()->has_appendix();\n","filename":"src\/hotspot\/share\/interpreter\/bytecode.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -225,3 +225,0 @@\n-  \/\/ Get an unsigned 2-byte index in native order.\n-  int             get_index_u2_cpcache() const   { assert_raw_stream(false);\n-                                                   return bytecode().get_index_u2_cpcache(raw_code()); }\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeStream.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -329,7 +330,0 @@\n-void BytecodePrinter::print_cpcache_entry(int cpc_index, outputStream* st) {\n-  ConstantPool* constants = method()->constants();\n-  ConstantPoolCacheEntry* cpce = constants->cache()->entry_at(cpc_index);\n-  st->print(\"  ConstantPoolCacheEntry: \");\n-  cpce->print(st, cpc_index, constants->cache());\n-}\n-\n@@ -519,1 +513,0 @@\n-        int cpcache_index;\n@@ -521,2 +514,14 @@\n-          cpcache_index = get_native_index_u2();\n-          cp_index = cpcache()->entry_at(cpcache_index)->constant_pool_index();\n+          int method_index = get_native_index_u2();\n+          ResolvedMethodEntry* method_entry = cpcache()->resolved_method_entry_at(method_index);\n+          cp_index = method_entry->constant_pool_index();\n+          print_field_or_method(cp_index, st);\n+\n+          if (raw_code() == Bytecodes::_invokehandle &&\n+              ClassPrinter::has_mode(_flags, ClassPrinter::PRINT_METHOD_HANDLE)) {\n+            assert(is_linked(), \"invokehandle is only in rewritten methods\");\n+            method_entry->print_on(st);\n+            if (method_entry->has_appendix()) {\n+              st->print(\"  appendix: \");\n+              constants()->resolved_reference_from_method(method_index)->print_on(st);\n+            }\n+          }\n@@ -524,1 +529,0 @@\n-          cpcache_index = -1;\n@@ -526,7 +530,1 @@\n-        }\n-        print_field_or_method(cp_index, st);\n-        if (raw_code() == Bytecodes::_invokehandle &&\n-            ClassPrinter::has_mode(_flags, ClassPrinter::PRINT_METHOD_HANDLE)) {\n-          assert(is_linked(), \"invokehandle is only in rewritten methods\");\n-          assert(cpcache_index >= 0, \"must be\");\n-          print_cpcache_entry(cpcache_index, st);\n+          print_field_or_method(cp_index, st);\n@@ -541,2 +539,2 @@\n-          int cpcache_index = get_native_index_u2();\n-          cp_index = cpcache()->entry_at(cpcache_index)->constant_pool_index();\n+          int method_index = get_native_index_u2();\n+          cp_index = cpcache()->resolved_method_entry_at(method_index)->constant_pool_index();\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeTracer.cpp","additions":18,"deletions":20,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1006,1 +1006,1 @@\n-        cp_index = Bytes::get_native_u2(code_base + pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+        cp_index = Bytes::get_native_u2(code_base + pos);\n@@ -1146,1 +1146,1 @@\n-        int cp_index = Bytes::get_native_u2(code_base+ pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+        int cp_index = Bytes::get_native_u2(code_base+ pos);\n@@ -1349,1 +1349,1 @@\n-      int cp_index = Bytes::get_native_u2(code_base + pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+      int cp_index = Bytes::get_native_u2(code_base + pos);\n@@ -1431,1 +1431,1 @@\n-        int cp_index = Bytes::get_native_u2(code_base+ pos) DEBUG_ONLY(+ ConstantPool::CPCACHE_INDEX_TAG);\n+        int cp_index = Bytes::get_native_u2(code_base+ pos);\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -107,2 +107,0 @@\n-  int get_index_u2_cpcache(Bytecodes::Code bc) const\n-                                                 { return bytecode().get_index_u2_cpcache(bc); }\n@@ -111,3 +109,0 @@\n-  ConstantPoolCacheEntry* cache_entry_at(int i) const\n-                                                 { return method()->constants()->cache()->entry_at(i); }\n-  ConstantPoolCacheEntry* cache_entry() const    { return cache_entry_at(Bytes::get_native_u2(bcp() + 1)); }\n@@ -210,2 +205,2 @@\n-    intptr_t flags = ((as_TosState(type) << ConstantPoolCacheEntry::tos_state_shift)\n-                      | (offset & ConstantPoolCacheEntry::field_index_mask));\n+    intptr_t flags = ((as_TosState(type) << ConstantPoolCache::tos_state_shift)\n+                      | (offset & ConstantPoolCache::field_index_mask));\n@@ -843,0 +838,1 @@\n+  ConstantPoolCache* cache = pool->cache();\n@@ -846,0 +842,1 @@\n+  int method_index = last_frame.get_index_u2(bytecode);\n@@ -850,1 +847,1 @@\n-                                 last_frame.get_index_u2_cpcache(bytecode), bytecode,\n+                                 method_index, bytecode,\n@@ -871,2 +868,1 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = last_frame.cache_entry();\n-  if (cp_cache_entry->is_resolved(bytecode)) return;\n+  if (cache->resolved_method_entry_at(method_index)->is_resolved(bytecode)) return;\n@@ -906,4 +902,1 @@\n-    cp_cache_entry->set_direct_call(\n-      bytecode,\n-      resolved_method,\n-      sender->is_interface());\n+    cache->set_direct_call(bytecode, method_index, resolved_method, sender->is_interface());\n@@ -912,4 +905,1 @@\n-    cp_cache_entry->set_vtable_call(\n-      bytecode,\n-      resolved_method,\n-      info.vtable_index());\n+    cache->set_vtable_call(bytecode, method_index, resolved_method, info.vtable_index());\n@@ -918,1 +908,1 @@\n-    cp_cache_entry->set_itable_call(\n+    cache->set_itable_call(\n@@ -920,0 +910,1 @@\n+      method_index,\n@@ -937,0 +928,1 @@\n+  int method_index = last_frame.get_index_u2(bytecode);\n@@ -941,1 +933,1 @@\n-                                 last_frame.get_index_u2_cpcache(bytecode), bytecode,\n+                                 method_index, bytecode,\n@@ -945,2 +937,1 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = last_frame.cache_entry();\n-  cp_cache_entry->set_method_handle(pool, info);\n+  pool->cache()->set_method_handle(method_index, info);\n@@ -1504,1 +1495,1 @@\n-  int cp_index = Bytes::get_native_u2(bcp + 1) + ConstantPool::CPCACHE_INDEX_TAG;\n+  int cp_index = Bytes::get_native_u2(bcp + 1);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":14,"deletions":23,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -1702,3 +1702,2 @@\n-  int cache_index = ConstantPool::decode_cpcache_index(index, true);\n-  ConstantPoolCacheEntry* cpce = pool->cache()->entry_at(cache_index);\n-  if (!cpce->is_f1_null()) {\n+  ResolvedMethodEntry* method_entry = pool->cache()->resolved_method_entry_at(index);\n+  if (method_entry->method() != nullptr) {\n@@ -1706,2 +1705,2 @@\n-    methodHandle method(THREAD, cpce->f1_as_method());\n-    Handle     appendix(THREAD, cpce->appendix_if_resolved(pool));\n+    methodHandle method(THREAD, method_entry->method());\n+    Handle     appendix(THREAD, pool->cache()->appendix_if_resolved(method_entry));\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -61,1 +62,3 @@\n-        add_cp_cache_entry(i);\n+        _cp_map.at_put(i, _method_entry_index);\n+        _method_entry_index++;\n+        _initialized_method_entries.push(ResolvedMethodEntry((u2)i));\n@@ -84,2 +87,2 @@\n-  guarantee((int) _cp_cache_map.length() - 1 <= (int) ((u2)-1),\n-            \"all cp cache indexes fit in a u2\");\n+  guarantee(_initialized_field_entries.length() - 1 <= (int)((u2)-1), \"All resolved field indices fit in a u2\");\n+  guarantee(_initialized_method_entries.length() - 1 <= (int)((u2)-1), \"All resolved method indices fit in a u2\");\n@@ -107,0 +110,2 @@\n+  assert(_field_entry_index == _initialized_field_entries.length(), \"Field entry size mismatch\");\n+  assert(_method_entry_index == _initialized_method_entries.length(), \"Method entry size mismatch\");\n@@ -108,2 +113,3 @@\n-      ConstantPoolCache::allocate(loader_data, _cp_cache_map,\n-                                  _invokedynamic_references_map, _initialized_indy_entries, _initialized_field_entries, CHECK);\n+      ConstantPoolCache::allocate(loader_data, _invokedynamic_references_map,\n+                                  _initialized_indy_entries, _initialized_field_entries, _initialized_method_entries,\n+                                  CHECK);\n@@ -127,2 +133,0 @@\n-    } else {\n-      cache->save_for_archive(THREAD);\n@@ -197,2 +201,1 @@\n-\/\/ Rewrite a classfile-order CP index into a native-order CPC index.\n-void Rewriter::rewrite_member_reference(address bcp, int offset, bool reverse) {\n+void Rewriter::rewrite_method_reference(address bcp, int offset, bool reverse) {\n@@ -202,4 +205,5 @@\n-    int  cache_index = cp_entry_to_cp_cache(cp_index);\n-    Bytes::put_native_u2(p, (u2)cache_index);\n-    if (!_method_handle_invokers.is_empty())\n-      maybe_rewrite_invokehandle(p - 1, cp_index, cache_index, reverse);\n+    int  method_entry_index = _cp_map.at(cp_index);\n+    Bytes::put_native_u2(p, (u2)method_entry_index);\n+    if (!_method_handle_invokers.is_empty()) {\n+      maybe_rewrite_invokehandle(p - 1, cp_index, method_entry_index, reverse);\n+    }\n@@ -207,2 +211,2 @@\n-    int cache_index = Bytes::get_native_u2(p);\n-    int pool_index = cp_cache_entry_pool_index(cache_index);\n+    int method_entry_index = Bytes::get_native_u2(p);\n+    int pool_index = _initialized_method_entries.at(method_entry_index).constant_pool_index();\n@@ -210,2 +214,3 @@\n-    if (!_method_handle_invokers.is_empty())\n-      maybe_rewrite_invokehandle(p - 1, pool_index, cache_index, reverse);\n+    if (!_method_handle_invokers.is_empty()) {\n+      maybe_rewrite_invokehandle(p - 1, pool_index, method_entry_index, reverse);\n+    }\n@@ -224,2 +229,4 @@\n-      int cache_index = add_invokespecial_cp_cache_entry(cp_index);\n-      if (cache_index != (int)(jushort) cache_index) {\n+      _initialized_method_entries.push(ResolvedMethodEntry((u2)cp_index));\n+      Bytes::put_native_u2(p, (u2)_method_entry_index);\n+      _method_entry_index++;\n+      if (_method_entry_index != (int)(u2)_method_entry_index) {\n@@ -228,1 +235,0 @@\n-      Bytes::put_native_u2(p, (u2)cache_index);\n@@ -230,1 +236,1 @@\n-      rewrite_member_reference(bcp, offset, reverse);\n+      rewrite_method_reference(bcp, offset, reverse);\n@@ -233,1 +239,1 @@\n-    rewrite_member_reference(bcp, offset, reverse);\n+    rewrite_method_reference(bcp, offset, reverse);\n@@ -237,1 +243,0 @@\n-\n@@ -243,2 +248,2 @@\n-        (*opc) == (u1)Bytecodes::_invokespecial) {\n-      assert(_pool->tag_at(cp_index).is_method(), \"wrong index\");\n+        ((*opc) == (u1)Bytecodes::_invokespecial)) {\n+          assert(_pool->tag_at(cp_index).is_method(), \"wrong index\");\n@@ -254,1 +259,2 @@\n-          add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          int resolved_index = add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          _initialized_method_entries.at(cache_index).set_resolved_references_index((u2)resolved_index);\n@@ -260,1 +266,2 @@\n-          add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          int resolved_index = add_invokedynamic_resolved_references_entry(cp_index, cache_index);\n+          _initialized_method_entries.at(cache_index).set_resolved_references_index((u2)resolved_index);\n@@ -474,1 +481,1 @@\n-        rewrite_member_reference(bcp, prefix_length+1, reverse);\n+        rewrite_method_reference(bcp, prefix_length+1, reverse);\n@@ -580,1 +587,0 @@\n-    _cp_cache_map(cpool->length() \/ 2),\n@@ -586,1 +592,2 @@\n-    _field_entry_index(0)\n+    _field_entry_index(0),\n+    _method_entry_index(0)\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":37,"deletions":30,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -43,2 +44,0 @@\n-  GrowableArray<int>  _cp_cache_map;  \/\/ for Methodref, Fieldref,\n-                                      \/\/ InterfaceMethodref and InvokeDynamic\n@@ -52,0 +51,1 @@\n+  int                 _method_entry_index;\n@@ -53,2 +53,4 @@\n-  \/\/ For collecting information about invokedynamic bytecodes before resolution\n-  \/\/ With this, we can know how many indy calls there are and resolve them later\n+  \/\/ For collecting initialization information for field, method, and invokedynamic\n+  \/\/ constant pool cache entries. The number of entries of each type will be known\n+  \/\/ at the end of rewriting and these arrays will be used to build the proper arrays\n+  \/\/ in the Constant Pool Cache.\n@@ -57,0 +59,1 @@\n+  GrowableArray<ResolvedMethodEntry> _initialized_method_entries;\n@@ -62,1 +65,0 @@\n-    _cp_cache_map.trunc_to(0);\n@@ -71,1 +73,0 @@\n-    _first_iteration_cp_cache_limit = -1;\n@@ -74,1 +75,0 @@\n-  int _first_iteration_cp_cache_limit;\n@@ -78,1 +78,0 @@\n-    _first_iteration_cp_cache_limit = _cp_cache_map.length();\n@@ -82,7 +81,0 @@\n-  int cp_cache_delta() {\n-    \/\/ How many cp cache entries were added since recording map limits after\n-    \/\/ cp cache initialization?\n-    assert(_first_iteration_cp_cache_limit != -1, \"only valid after first iteration\");\n-    return _cp_cache_map.length() - _first_iteration_cp_cache_limit;\n-  }\n-\n@@ -99,26 +91,0 @@\n-  int add_cp_cache_entry(int cp_index) {\n-    assert(_pool->tag_at(cp_index).value() != JVM_CONSTANT_InvokeDynamic, \"use indy version\");\n-    assert(_first_iteration_cp_cache_limit == -1, \"do not add cache entries after first iteration\");\n-    int cache_index = add_map_entry(cp_index, &_cp_map, &_cp_cache_map);\n-    assert(cp_entry_to_cp_cache(cp_index) == cache_index, \"\");\n-    assert(cp_cache_entry_pool_index(cache_index) == cp_index, \"\");\n-    return cache_index;\n-  }\n-\n-  \/\/ add a new CP cache entry beyond the normal cache for the special case of\n-  \/\/ invokespecial with InterfaceMethodref as cpool operand.\n-  int add_invokespecial_cp_cache_entry(int cp_index) {\n-    assert(_first_iteration_cp_cache_limit >= 0, \"add these special cache entries after first iteration\");\n-    \/\/ Don't add InterfaceMethodref if it already exists at the end.\n-    for (int i = _first_iteration_cp_cache_limit; i < _cp_cache_map.length(); i++) {\n-      if (cp_cache_entry_pool_index(i) == cp_index) {\n-        return i;\n-      }\n-    }\n-    int cache_index = _cp_cache_map.append(cp_index);\n-    assert(cache_index >= _first_iteration_cp_cache_limit, \"\");\n-    \/\/ do not update _cp_map, since the mapping is one-to-many\n-    assert(cp_cache_entry_pool_index(cache_index) == cp_index, \"\");\n-    return cache_index;\n-  }\n-\n@@ -156,6 +122,0 @@\n-  \/\/ Access the contents of _cp_cache_map to determine CP cache layout.\n-  int cp_cache_entry_pool_index(int cache_index) {\n-    int cp_index = _cp_cache_map.at(cache_index);\n-    return cp_index;\n-  }\n-\n@@ -170,0 +130,1 @@\n+  void rewrite_method_reference(address bcp, int offset, bool reverse);\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.hpp","additions":8,"deletions":47,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -265,4 +265,0 @@\n-  static void resolve_cache_and_index(int byte_no,       \/\/ one of 1,2,11\n-                                      Register cache,    \/\/ output for CP cache\n-                                      Register index,    \/\/ output for CP index\n-                                      size_t index_size); \/\/ one of 1,2,4\n@@ -272,0 +268,3 @@\n+  static void resolve_cache_and_index_for_method(int byte_no,\n+                                                 Register cache,\n+                                                 Register index);\n@@ -279,0 +278,14 @@\n+  static void load_resolved_method_entry_special_or_static(Register cache,\n+                                                           Register method,\n+                                                           Register flags);\n+  static void load_resolved_method_entry_handle(Register cache,\n+                                                Register method,\n+                                                Register ref_index,\n+                                                Register flags);\n+  static void load_resolved_method_entry_interface(Register cache,\n+                                                   Register klass,\n+                                                   Register method_or_table_index,\n+                                                   Register flags);\n+  static void load_resolved_method_entry_virtual(Register cache,\n+                                                 Register method_or_table_index,\n+                                                 Register flags);\n","filename":"src\/hotspot\/share\/interpreter\/templateTable.hpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -2278,1 +2279,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n@@ -2280,1 +2281,1 @@\n-        if (! cache->is_resolved((Bytecodes::Code) opcode)) {\n+        if (! entry->is_resolved((Bytecodes::Code) opcode)) {\n@@ -2283,1 +2284,1 @@\n-          cache = cp->entry_at(index);\n+          entry = cp->resolved_method_entry_at(index);\n@@ -2286,1 +2287,1 @@\n-        Method* method = cache->f1_as_method();\n+        Method* method = entry->method();\n@@ -2289,1 +2290,1 @@\n-        if (cache->has_appendix()) {\n+        if (entry->has_appendix()) {\n@@ -2291,1 +2292,1 @@\n-          SET_STACK_OBJECT(cache->appendix_if_resolved(cp), 0);\n+          SET_STACK_OBJECT(cp->cache()->appendix_if_resolved(entry), 0);\n@@ -2309,2 +2310,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        if (!cache->is_resolved((Bytecodes::Code)opcode)) {\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n+        if (!entry->is_resolved((Bytecodes::Code)opcode)) {\n@@ -2313,1 +2314,0 @@\n-          cache = cp->entry_at(index);\n@@ -2321,4 +2321,4 @@\n-        if (cache->is_forced_virtual()) {\n-          CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n-          if (cache->is_vfinal()) {\n-            callee = cache->f2_as_vfinal_method();\n+        if (entry->is_forced_virtual()) {\n+          CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n+          if (entry->is_vfinal()) {\n+            callee = entry->method();\n@@ -2327,1 +2327,1 @@\n-            int parms = cache->parameter_size();\n+            int parms = entry->number_of_parameters();\n@@ -2332,1 +2332,1 @@\n-            callee = (Method*) rcvrKlass->method_at_vtable(cache->f2_as_index());\n+            callee = (Method*) rcvrKlass->method_at_vtable(entry->table_index());\n@@ -2334,1 +2334,1 @@\n-        } else if (cache->is_vfinal()) {\n+        } else if (entry->is_vfinal()) {\n@@ -2341,1 +2341,1 @@\n-          int parms = cache->parameter_size();\n+          int parms = entry->number_of_parameters();\n@@ -2345,1 +2345,1 @@\n-          Klass* resolved_klass = cache->f1_as_klass();\n+          Klass* resolved_klass = entry->interface_klass();\n@@ -2354,1 +2354,1 @@\n-          callee = cache->f2_as_vfinal_method();\n+          callee = entry->method();\n@@ -2367,1 +2367,1 @@\n-        Method *interface_method = cache->f2_as_interface_method();\n+        Method *interface_method = entry->method();\n@@ -2371,1 +2371,1 @@\n-        int parms = cache->parameter_size();\n+        int parms = entry->number_of_parameters();\n@@ -2378,1 +2378,1 @@\n-          Klass* refc = cache->f1_as_klass();\n+          Klass* refc = entry->interface_klass();\n@@ -2431,1 +2431,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n@@ -2435,1 +2435,1 @@\n-        if (!cache->is_resolved((Bytecodes::Code)opcode)) {\n+        if (!entry->is_resolved((Bytecodes::Code)opcode)) {\n@@ -2438,1 +2438,1 @@\n-          cache = cp->entry_at(index);\n+          entry = cp->resolved_method_entry_at(index);\n@@ -2445,3 +2445,3 @@\n-            CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n-            if (cache->is_vfinal()) {\n-              callee = cache->f2_as_vfinal_method();\n+            CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n+            if (entry->is_vfinal()) {\n+              callee = entry->method();\n@@ -2454,1 +2454,1 @@\n-              int parms = cache->parameter_size();\n+              int parms = entry->number_of_parameters();\n@@ -2480,1 +2480,1 @@\n-              callee = (Method*) rcvrKlass->method_at_vtable(cache->f2_as_index());\n+              callee = (Method*) rcvrKlass->method_at_vtable(entry->table_index());\n@@ -2484,1 +2484,1 @@\n-              CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n+              CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n@@ -2486,1 +2486,1 @@\n-            callee = cache->f1_as_method();\n+            callee = entry->method();\n@@ -2891,1 +2891,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n@@ -2893,1 +2893,1 @@\n-        assert(cache->is_resolved(Bytecodes::_invokevirtual), \"Should be resolved before rewriting\");\n+        assert(entry->is_resolved(Bytecodes::_invokevirtual), \"Should be resolved before rewriting\");\n@@ -2897,2 +2897,2 @@\n-        CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n-        Method* callee = cache->f2_as_vfinal_method();\n+        CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n+        Method* callee = entry->method();\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":36,"deletions":36,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -937,5 +937,0 @@\n-C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n-  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n-  return cp->remap_instruction_operand_from_cache(index);\n-C2V_END\n-\n@@ -1663,0 +1658,8 @@\n+C2V_VMENTRY_0(int, decodeMethodIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint method_index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n+  if (method_index < 0 || method_index >= cp->resolved_method_entries_length()) {\n+    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"invalid method index %d\", method_index));\n+  }\n+  return cp->resolved_method_entry_at(method_index)->constant_pool_index();\n+C2V_END\n+\n@@ -1670,2 +1673,1 @@\n-    ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-    cp_cache_entry->set_method_handle(cp, callInfo);\n+    cp->cache()->set_method_handle(index, callInfo);\n@@ -1677,2 +1679,2 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-  if (cp_cache_entry->is_resolved(Bytecodes::_invokehandle)) {\n+  ResolvedMethodEntry* entry = cp->cache()->resolved_method_entry_at(index);\n+  if (entry->is_resolved(Bytecodes::_invokehandle)) {\n@@ -1691,1 +1693,1 @@\n-    methodHandle adapter_method(THREAD, cp_cache_entry->f1_as_method());\n+    methodHandle adapter_method(THREAD, entry->method());\n@@ -1700,1 +1702,1 @@\n-      vmassert(cp_cache_entry->appendix_if_resolved(cp) == nullptr, \"!\");\n+      vmassert(cp->cache()->appendix_if_resolved(entry) == nullptr, \"!\");\n@@ -1710,1 +1712,1 @@\n-    if (cp->resolved_indy_entry_at(cp->decode_cpcache_index(index))->is_resolved()) {\n+    if (cp->resolved_indy_entry_at(cp->decode_invokedynamic_index(index))->is_resolved()) {\n@@ -3223,1 +3225,0 @@\n-  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n@@ -3230,0 +3231,1 @@\n+  {CC \"decodeMethodIndexToCPIndex\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(decodeMethodIndexToCPIndex)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":15,"deletions":13,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -615,1 +615,0 @@\n-  declare_constant(ConstantPool::CPCACHE_INDEX_TAG)                       \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -642,2 +642,1 @@\n-  int cache_index = decode_cpcache_index(which, true);\n-  if (!(cache_index >= 0 && cache_index < cpool->cache()->length())) {\n+  if (!(which >= 0 && which < cpool->resolved_method_entries_length())) {\n@@ -648,2 +647,1 @@\n-  ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-  return e->method_if_resolved(cpool);\n+  return cpool->cache()->method_if_resolved(which);\n@@ -659,3 +657,1 @@\n-    int cache_index = decode_cpcache_index(which, true);\n-    ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-    return e->has_appendix();\n+    return cpool->resolved_method_entry_at(which)->has_appendix();\n@@ -671,3 +667,1 @@\n-    int cache_index = decode_cpcache_index(which, true);\n-    ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-    return e->appendix_if_resolved(cpool);\n+    return cpool->cache()->appendix_if_resolved(which);\n@@ -680,1 +674,0 @@\n-  int cache_index = decode_cpcache_index(which, true);\n@@ -682,1 +675,2 @@\n-    return cpool->resolved_indy_entry_at(cache_index)->has_local_signature();\n+    int indy_index = decode_invokedynamic_index(which);\n+    return cpool->resolved_indy_entry_at(indy_index)->has_local_signature();\n@@ -684,2 +678,1 @@\n-    ConstantPoolCacheEntry* e = cpool->cache()->entry_at(cache_index);\n-    return e->has_local_signature();\n+    return cpool->resolved_method_entry_at(which)->has_local_signature();\n@@ -705,1 +698,2 @@\n-      \/\/ TODO: handle resolved method entries with new structure\n+    case Bytecodes::_fast_invokevfinal: \/\/ Bytecode interpreter uses this\n+      return resolved_method_entry_at(index)->constant_pool_index();\n@@ -707,2 +701,3 @@\n-      \/\/ change byte-ordering and go via cache\n-      return remap_instruction_operand_from_cache(index);\n+      tty->print_cr(\"Unexpected bytecode: %d\", code);\n+      ShouldNotReachHere(); \/\/ All cases should have been handled\n+      return -1;\n@@ -750,9 +745,0 @@\n-int ConstantPool::remap_instruction_operand_from_cache(int operand) {\n-  int cpc_index = operand;\n-  DEBUG_ONLY(cpc_index -= CPCACHE_INDEX_TAG);\n-  assert((int)(u2)cpc_index == cpc_index, \"clean u2\");\n-  int member_index = cache()->entry_at(cpc_index)->constant_pool_index();\n-  return member_index;\n-}\n-\n-\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":12,"deletions":26,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -646,2 +646,3 @@\n-  \/\/ actually rewritten constant pool cache indices.\n-  \/\/ The routine remap_instruction_operand_from_cache manages the adjustment\n+  \/\/ actually rewritten indices that point to entries in their respective structures\n+  \/\/ i.e. ResolvedMethodEntries or ResolvedFieldEntries.\n+  \/\/ The routine to_cp_index manages the adjustment\n@@ -652,5 +653,0 @@\n-  \/\/ FIXME: Consider renaming these with a prefix \"cached_\" to make the distinction clear.\n-  \/\/ In a few cases (the verifier) there are uses before a cpcache has been built,\n-  \/\/ which are handled by a dynamic check in remap_instruction_operand_from_cache.\n-  \/\/ FIXME: Remove the dynamic check, and adjust all callers to specify the correct mode.\n-\n@@ -672,2 +668,0 @@\n-  int remap_instruction_operand_from_cache(int operand);  \/\/ operand must be biased by CPCACHE_INDEX_TAG\n-\n@@ -796,13 +790,0 @@\n-#ifdef ASSERT\n-  enum { CPCACHE_INDEX_TAG = 0x10000 };  \/\/ helps keep CP cache indices distinct from CP indices\n-#else\n-  enum { CPCACHE_INDEX_TAG = 0 };        \/\/ in product mode, this zero value is a no-op\n-#endif \/\/ASSERT\n-\n-  static int decode_cpcache_index(int raw_index, bool invokedynamic_ok = false) {\n-    if (invokedynamic_ok && is_invokedynamic_index(raw_index))\n-      return decode_invokedynamic_index(raw_index);\n-    else\n-      return raw_index - CPCACHE_INDEX_TAG;\n-  }\n-\n@@ -926,0 +907,5 @@\n+  \/\/ ResolvedMethodEntry getters\n+  inline ResolvedMethodEntry* resolved_method_entry_at(int method_index);\n+  inline int resolved_method_entries_length() const;\n+  inline oop appendix_if_resolved(int method_index) const;\n+\n@@ -930,0 +916,1 @@\n+  inline oop resolved_reference_from_method(int index) const;\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":9,"deletions":22,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -54,0 +55,16 @@\n+inline ResolvedMethodEntry* ConstantPool::resolved_method_entry_at(int method_index) {\n+    return cache()->resolved_method_entry_at(method_index);\n+}\n+\n+inline int ConstantPool::resolved_method_entries_length() const {\n+    return cache()->resolved_method_entries_length();\n+}\n+\n+inline oop ConstantPool::appendix_if_resolved(int method_index) const {\n+  ResolvedMethodEntry* entry = cache()->resolved_method_entry_at(method_index);\n+  if (!entry->has_appendix())\n+    return nullptr;\n+  const int ref_index = entry->resolved_references_index();\n+  return resolved_reference_at(ref_index);\n+}\n+\n@@ -69,0 +86,4 @@\n+\n+inline oop ConstantPool::resolved_reference_from_method(int index) const {\n+  return resolved_references()->obj_at(cache()->resolved_method_entry_at(index)->resolved_references_index());\n+}\n","filename":"src\/hotspot\/share\/oops\/constantPool.inline.hpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -60,80 +61,1 @@\n-\/\/ Implementation of ConstantPoolCacheEntry\n-\n-void ConstantPoolCacheEntry::initialize_entry(int index) {\n-  assert(0 < index && index < 0x10000, \"sanity check\");\n-  _indices = index;\n-  _f1 = nullptr;\n-  _f2 = _flags = 0;\n-  assert(constant_pool_index() == index, \"\");\n-}\n-\n-intx ConstantPoolCacheEntry::make_flags(TosState state,\n-                                       int option_bits,\n-                                       int field_index_or_method_params) {\n-  assert(state < number_of_states, \"Invalid state in make_flags\");\n-  intx f = ((int)state << tos_state_shift) | option_bits | field_index_or_method_params;\n-  \/\/ Preserve existing flag bit values\n-  \/\/ The low bits are a field offset, or else the method parameter size.\n-#ifdef ASSERT\n-  TosState old_state = flag_state();\n-  assert(old_state == (TosState)0 || old_state == state,\n-         \"inconsistent cpCache flags state\");\n-#endif\n-  return (_flags | f) ;\n-}\n-\n-void ConstantPoolCacheEntry::set_bytecode_1(Bytecodes::Code code) {\n-#ifdef ASSERT\n-  \/\/ Read once.\n-  volatile Bytecodes::Code c = bytecode_1();\n-  assert(c == 0 || c == code || code == 0, \"update must be consistent\");\n-#endif\n-  \/\/ Need to flush pending stores here before bytecode is written.\n-  Atomic::release_store(&_indices, _indices | ((u_char)code << bytecode_1_shift));\n-}\n-\n-void ConstantPoolCacheEntry::set_bytecode_2(Bytecodes::Code code) {\n-#ifdef ASSERT\n-  \/\/ Read once.\n-  volatile Bytecodes::Code c = bytecode_2();\n-  assert(c == 0 || c == code || code == 0, \"update must be consistent\");\n-#endif\n-  \/\/ Need to flush pending stores here before bytecode is written.\n-  Atomic::release_store(&_indices, _indices | ((u_char)code << bytecode_2_shift));\n-}\n-\n-\/\/ Sets f1, ordering with previous writes.\n-void ConstantPoolCacheEntry::release_set_f1(Metadata* f1) {\n-  assert(f1 != nullptr, \"\");\n-  Atomic::release_store(&_f1, f1);\n-}\n-\n-void ConstantPoolCacheEntry::set_indy_resolution_failed() {\n-  Atomic::release_store(&_flags, _flags | (1 << indy_resolution_failed_shift));\n-}\n-\n-\/\/ Note that concurrent update of both bytecodes can leave one of them\n-\/\/ reset to zero.  This is harmless; the interpreter will simply re-resolve\n-\/\/ the damaged entry.  More seriously, the memory synchronization is needed\n-\/\/ to flush other fields (f1, f2) completely to memory before the bytecodes\n-\/\/ are updated, lest other processors see a non-zero bytecode but zero f1\/f2.\n-void ConstantPoolCacheEntry::set_field(Bytecodes::Code get_code,\n-                                       Bytecodes::Code put_code,\n-                                       Klass* field_holder,\n-                                       int field_index,\n-                                       int field_offset,\n-                                       TosState field_type,\n-                                       bool is_final,\n-                                       bool is_volatile) {\n-  set_f1(field_holder);\n-  set_f2(field_offset);\n-  assert((field_index & field_index_mask) == field_index,\n-         \"field index does not fit in low flag bits\");\n-  set_field_flags(field_type,\n-                  ((is_volatile ? 1 : 0) << is_volatile_shift) |\n-                  ((is_final    ? 1 : 0) << is_final_shift),\n-                  field_index);\n-  set_bytecode_1(get_code);\n-  set_bytecode_2(put_code);\n-  NOT_PRODUCT(verify(tty));\n-}\n+\/\/ Implementation of ConstantPoolCache\n@@ -141,16 +63,11 @@\n-void ConstantPoolCacheEntry::set_parameter_size(int value) {\n-  \/\/ This routine is called only in corner cases where the CPCE is not yet initialized.\n-  \/\/ See AbstractInterpreter::deopt_continue_after_entry.\n-  assert(_flags == 0 || parameter_size() == 0 || parameter_size() == value,\n-         \"size must not change: parameter_size=%d, value=%d\", parameter_size(), value);\n-  \/\/ Setting the parameter size by itself is only safe if the\n-  \/\/ current value of _flags is 0, otherwise another thread may have\n-  \/\/ updated it and we don't want to overwrite that value.  Don't\n-  \/\/ bother trying to update it once it's nonzero but always make\n-  \/\/ sure that the final parameter size agrees with what was passed.\n-  if (_flags == 0) {\n-    intx newflags = (value & parameter_size_mask);\n-    Atomic::cmpxchg(&_flags, (intx)0, newflags);\n-  }\n-  guarantee(parameter_size() == value,\n-            \"size must not change: parameter_size=%d, value=%d\", parameter_size(), value);\n+template <class T>\n+static Array<T>* initialize_resolved_entries_array(ClassLoaderData* loader_data, GrowableArray<T> entries, TRAPS) {\n+  Array<T>* resolved_entries;\n+  if (entries.length() != 0) {\n+    resolved_entries = MetadataFactory::new_array<T>(loader_data, entries.length(), CHECK_NULL);\n+    for (int i = 0; i < entries.length(); i++) {\n+      resolved_entries->at_put(i, entries.at(i));\n+    }\n+    return resolved_entries;\n+  }\n+  return nullptr;\n@@ -159,1 +76,2 @@\n-void ConstantPoolCacheEntry::set_direct_or_vtable_call(Bytecodes::Code invoke_code,\n+void ConstantPoolCache::set_direct_or_vtable_call(Bytecodes::Code invoke_code,\n+                                                       int method_index,\n@@ -170,0 +88,1 @@\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n@@ -177,6 +96,6 @@\n-        \/\/ set_f2_as_vfinal_method checks if is_vfinal flag is true.\n-        set_method_flags(as_TosState(method->result_type()),\n-                         (                             1      << is_vfinal_shift) |\n-                         ((method->is_final_method() ? 1 : 0) << is_final_shift),\n-                         method()->size_of_parameters());\n-        set_f2_as_vfinal_method(method());\n+\n+        method_entry->set_flags((                             1      << ResolvedMethodEntry::is_vfinal_shift) |\n+                                ((method->is_final_method() ? 1 : 0) << ResolvedMethodEntry::is_final_shift));\n+        method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+        assert(method_entry->is_vfinal(), \"flags must be set\");\n+        method_entry->set_method(method());\n@@ -184,1 +103,1 @@\n-        set_f1(holder); \/\/ interface klass*\n+        method_entry->set_klass(holder);\n@@ -204,7 +123,6 @@\n-          \/\/ set_f2_as_vfinal_method checks if is_vfinal flag is true.\n-          set_method_flags(as_TosState(method->result_type()),\n-                           (                             1      << is_vfinal_shift) |\n-                           ((method->is_final_method() ? 1 : 0) << is_final_shift)  |\n-                           ((change_to_virtual         ? 1 : 0) << is_forced_virtual_shift),\n-                           method()->size_of_parameters());\n-          set_f2_as_vfinal_method(method());\n+          method_entry->set_flags((                             1      << ResolvedMethodEntry::is_vfinal_shift) |\n+                                  ((method->is_final_method() ? 1 : 0) << ResolvedMethodEntry::is_final_shift)  |\n+                                  ((change_to_virtual         ? 1 : 0) << ResolvedMethodEntry::is_forced_virtual_shift));\n+          method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+          assert(method_entry->is_vfinal(), \"flags must be set\");\n+          method_entry->set_method(method());\n@@ -215,4 +133,4 @@\n-          set_method_flags(as_TosState(method->result_type()),\n-                           ((change_to_virtual ? 1 : 0) << is_forced_virtual_shift),\n-                           method()->size_of_parameters());\n-          set_f2(vtable_index);\n+          method_entry->set_flags((change_to_virtual ? 1 : 0) << ResolvedMethodEntry::is_forced_virtual_shift);\n+          method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+          assert(!method_entry->is_vfinal(), \"flags must not be set\");\n+          method_entry->set_table_index(vtable_index);\n@@ -225,1 +143,1 @@\n-    case Bytecodes::_invokestatic:\n+    case Bytecodes::_invokestatic: {\n@@ -231,5 +149,5 @@\n-      set_method_flags(as_TosState(method->result_type()),\n-                       ((is_vfinal()               ? 1 : 0) << is_vfinal_shift) |\n-                       ((method->is_final_method() ? 1 : 0) << is_final_shift),\n-                       method()->size_of_parameters());\n-      set_f1(method());\n+      bool vfinal = method_entry->is_vfinal();\n+      method_entry->set_flags(((method->is_final_method() ? 1 : 0) << ResolvedMethodEntry::is_final_shift));\n+      assert(vfinal == method_entry->is_vfinal(), \"Vfinal flag must be preserved\");\n+      method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+      method_entry->set_method(method());\n@@ -238,0 +156,1 @@\n+    }\n@@ -273,1 +192,1 @@\n-      set_bytecode_1(invoke_code);\n+      method_entry->set_bytecode1(invoke_code);\n@@ -303,1 +222,1 @@\n-        set_bytecode_1(invoke_code);\n+        method_entry->set_bytecode1(invoke_code);\n@@ -307,1 +226,1 @@\n-    set_bytecode_2(Bytecodes::_invokevirtual);\n+    method_entry->set_bytecode2(Bytecodes::_invokevirtual);\n@@ -311,1 +230,0 @@\n-  NOT_PRODUCT(verify(tty));\n@@ -314,2 +232,2 @@\n-void ConstantPoolCacheEntry::set_direct_call(Bytecodes::Code invoke_code, const methodHandle& method,\n-                                             bool sender_is_interface) {\n+void ConstantPoolCache::set_direct_call(Bytecodes::Code invoke_code, int method_index, const methodHandle& method,\n+                                        bool sender_is_interface) {\n@@ -318,1 +236,1 @@\n-  set_direct_or_vtable_call(invoke_code, method, index, sender_is_interface);\n+  set_direct_or_vtable_call(invoke_code, method_index, method, index, sender_is_interface);\n@@ -321,1 +239,1 @@\n-void ConstantPoolCacheEntry::set_vtable_call(Bytecodes::Code invoke_code, const methodHandle& method, int index) {\n+void ConstantPoolCache::set_vtable_call(Bytecodes::Code invoke_code, int method_index, const methodHandle& method, int index) {\n@@ -325,1 +243,1 @@\n-  set_direct_or_vtable_call(invoke_code, method, index, false);\n+  set_direct_or_vtable_call(invoke_code, method_index, method, index, false);\n@@ -328,3 +246,4 @@\n-void ConstantPoolCacheEntry::set_itable_call(Bytecodes::Code invoke_code,\n-                                             Klass* referenced_klass,\n-                                             const methodHandle& method, int index) {\n+void ConstantPoolCache::set_itable_call(Bytecodes::Code invoke_code,\n+                                        int method_index,\n+                                        Klass* referenced_klass,\n+                                        const methodHandle& method, int index) {\n@@ -336,20 +255,12 @@\n-  set_f1(referenced_klass);\n-  set_f2((intx)method());\n-  set_method_flags(as_TosState(method->result_type()),\n-                   0,  \/\/ no option bits\n-                   method()->size_of_parameters());\n-  set_bytecode_1(Bytecodes::_invokeinterface);\n-}\n-\n-\n-void ConstantPoolCacheEntry::set_method_handle(const constantPoolHandle& cpool, const CallInfo &call_info) {\n-  set_method_handle_common(cpool, Bytecodes::_invokehandle, call_info);\n-}\n-\n-void ConstantPoolCacheEntry::set_method_handle_common(const constantPoolHandle& cpool,\n-                                                      Bytecodes::Code invoke_code,\n-                                                      const CallInfo &call_info) {\n-  \/\/ NOTE: This CPCE can be the subject of data races.\n-  \/\/ There are three words to update: flags, refs[f2], f1 (in that order).\n-  \/\/ Writers must store all other values before f1.\n-  \/\/ Readers must test f1 first for non-null before reading other fields.\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+  method_entry->set_klass(static_cast<InstanceKlass*>(referenced_klass));\n+  method_entry->set_method(method());\n+  method_entry->fill_in((u1)as_TosState(method->result_type()), (u2)method()->size_of_parameters());\n+  method_entry->set_bytecode1(Bytecodes::_invokeinterface);\n+}\n+\n+ResolvedMethodEntry* ConstantPoolCache::set_method_handle(int method_index, const CallInfo &call_info) {\n+  \/\/ NOTE: This method entry can be the subject of data races.\n+  \/\/ There are three words to update: flags, refs[appendix_index], method (in that order).\n+  \/\/ Writers must store all other values before method.\n+  \/\/ Readers must test the method first for non-null before reading other fields.\n@@ -357,1 +268,1 @@\n-  \/\/ A losing writer waits on the lock until the winner writes f1 and leaves\n+  \/\/ A losing writer waits on the lock until the winner writes the method and leaves\n@@ -361,5 +272,3 @@\n-  MutexLocker ml(cpool->pool_holder()->init_monitor());\n-\n-  if (!is_f1_null()) {\n-    return;\n-  }\n+  Bytecodes::Code invoke_code = Bytecodes::_invokehandle;\n+  MutexLocker ml(constant_pool()->pool_holder()->init_monitor());\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n@@ -367,17 +276,2 @@\n-  if (indy_resolution_failed()) {\n-    \/\/ Before we got here, another thread got a LinkageError exception during\n-    \/\/ resolution.  Ignore our success and throw their exception.\n-    ConstantPoolCache* cpCache = cpool->cache();\n-    int index = -1;\n-    for (int i = 0; i < cpCache->length(); i++) {\n-      if (cpCache->entry_at(i) == this) {\n-        index = i;\n-        break;\n-      }\n-    }\n-    guarantee(index >= 0, \"Didn't find cpCache entry!\");\n-    int encoded_index = ResolutionErrorTable::encode_cpcache_index(\n-                          ConstantPool::encode_invokedynamic_index(index));\n-    JavaThread* THREAD = JavaThread::current(); \/\/ For exception macros.\n-    ConstantPool::throw_resolution_error(cpool, encoded_index, THREAD);\n-    return;\n+  if (method_entry->is_resolved(invoke_code)) {\n+    return method_entry;\n@@ -391,23 +285,8 @@\n-  \/\/ MHs and indy are always sig-poly and have a local signature.\n-  set_method_flags(as_TosState(adapter->result_type()),\n-                   ((has_appendix    ? 1 : 0) << has_appendix_shift        ) |\n-                   (                   1      << has_local_signature_shift ) |\n-                   (                   1      << is_final_shift            ),\n-                   adapter->size_of_parameters());\n-\n-  LogStream* log_stream = nullptr;\n-  LogStreamHandle(Debug, methodhandles, indy) lsh_indy;\n-  if (lsh_indy.is_enabled()) {\n-    ResourceMark rm;\n-    log_stream = &lsh_indy;\n-    log_stream->print_cr(\"set_method_handle bc=%d appendix=\" PTR_FORMAT \"%s method=\" PTR_FORMAT \" (local signature) \",\n-                         invoke_code,\n-                         p2i(appendix()),\n-                         (has_appendix ? \"\" : \" (unused)\"),\n-                         p2i(adapter));\n-    adapter->print_on(log_stream);\n-    if (has_appendix)  appendix()->print_on(log_stream);\n-  }\n-\n-  \/\/ Method handle invokes and invokedynamic sites use both cp cache words.\n-  \/\/ refs[f2], if not null, contains a value passed as a trailing argument to the adapter.\n+  \/\/ MHs are always sig-poly and have a local signature.\n+  method_entry->fill_in((u1)as_TosState(adapter->result_type()), (u2)adapter->size_of_parameters());\n+  method_entry->set_flags(((has_appendix    ? 1 : 0) << ResolvedMethodEntry::has_appendix_shift        ) |\n+                          (                   1      << ResolvedMethodEntry::has_local_signature_shift ) |\n+                          (                   1      << ResolvedMethodEntry::is_final_shift            ));\n+\n+  \/\/ Method handle invokes use both a method and a resolved references index.\n+  \/\/ refs[appendix_index], if not null, contains a value passed as a trailing argument to the adapter.\n@@ -416,1 +295,1 @@\n-  \/\/ f1 contains the adapter method which manages the actual call.\n+  \/\/ method_entry->method() contains the adapter method which manages the actual call.\n@@ -420,1 +299,1 @@\n-  \/\/ JVM-level linking is via f1, as if for invokespecial, and signatures are erased.\n+  \/\/ JVM-level linking is via the method, as if for invokespecial, and signatures are erased.\n@@ -425,1 +304,1 @@\n-  \/\/ the f1 method has signature '(Ljl\/Object;Ljl\/invoke\/MethodType;)Ljl\/Object;',\n+  \/\/ the method has signature '(Ljl\/Object;Ljl\/invoke\/MethodType;)Ljl\/Object;',\n@@ -427,1 +306,1 @@\n-  \/\/ The fact that String and List are involved is encoded in the MethodType in refs[f2].\n+  \/\/ The fact that String and List are involved is encoded in the MethodType in refs[appendix_index].\n@@ -433,3 +312,5 @@\n-    const int appendix_index = f2_as_index();\n-    oop old_oop = cpool->set_resolved_reference_at(appendix_index, appendix());\n-    assert(old_oop == nullptr, \"init just once\");\n+    const int appendix_index = method_entry->resolved_references_index();\n+    objArrayOop resolved_references = constant_pool()->resolved_references();\n+    assert(appendix_index >= 0 && appendix_index < resolved_references->length(), \"oob\");\n+    assert(resolved_references->obj_at(appendix_index) == nullptr, \"init just once\");\n+    resolved_references->obj_at_put(appendix_index, appendix());\n@@ -438,1 +319,1 @@\n-  release_set_f1(adapter);  \/\/ This must be the last one to set (see NOTE above)!\n+  method_entry->set_method(adapter); \/\/ This must be the last one to set (see NOTE above)!\n@@ -442,2 +323,1 @@\n-  set_bytecode_1(invoke_code);\n-  NOT_PRODUCT(verify(tty));\n+  method_entry->set_bytecode1(invoke_code);\n@@ -445,6 +325,3 @@\n-  if (log_stream != nullptr) {\n-    this->print(log_stream, 0, cpool->cache());\n-  }\n-\n-  assert(has_appendix == this->has_appendix(), \"proper storage of appendix flag\");\n-  assert(this->has_local_signature(), \"proper storage of signature flag\");\n+  assert(has_appendix == method_entry->has_appendix(), \"proper storage of appendix flag\");\n+  assert(method_entry->has_local_signature(), \"proper storage of signature flag\");\n+  return method_entry;\n@@ -453,1 +330,1 @@\n-Method* ConstantPoolCacheEntry::method_if_resolved(const constantPoolHandle& cpool) const {\n+Method* ConstantPoolCache::method_if_resolved(int method_index) const {\n@@ -455,38 +332,13 @@\n-  Bytecodes::Code invoke_code = bytecode_1();\n-  if (invoke_code != (Bytecodes::Code)0) {\n-    Metadata* f1 = f1_ord();\n-    if (f1 != nullptr) {\n-      switch (invoke_code) {\n-      case Bytecodes::_invokeinterface:\n-        assert(f1->is_klass(), \"\");\n-        return f2_as_interface_method();\n-      case Bytecodes::_invokestatic:\n-      case Bytecodes::_invokespecial:\n-        assert(!has_appendix(), \"\");\n-      case Bytecodes::_invokehandle:\n-        assert(f1->is_method(), \"\");\n-        return (Method*)f1;\n-      case Bytecodes::_invokedynamic:\n-        ShouldNotReachHere();\n-      default:\n-        break;\n-      }\n-    }\n-  }\n-  invoke_code = bytecode_2();\n-  if (invoke_code != (Bytecodes::Code)0) {\n-    switch (invoke_code) {\n-    case Bytecodes::_invokevirtual:\n-      if (is_vfinal()) {\n-        \/\/ invokevirtual\n-        Method* m = f2_as_vfinal_method();\n-        assert(m->is_method(), \"\");\n-        return m;\n-      } else {\n-        int holder_index = cpool->uncached_klass_ref_index_at(constant_pool_index());\n-        if (cpool->tag_at(holder_index).is_klass()) {\n-          Klass* klass = cpool->resolved_klass_at(holder_index);\n-          return klass->method_at_vtable(f2_as_index());\n-        }\n-      }\n-      break;\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+\n+  Bytecodes::Code invoke_code = (Bytecodes::Code)method_entry->bytecode1();\n+  switch (invoke_code) {\n+    case Bytecodes::_invokeinterface:\n+    case Bytecodes::_invokestatic:\n+    case Bytecodes::_invokespecial:\n+      assert(!method_entry->has_appendix(), \"\");\n+      \/\/ fall through\n+    case Bytecodes::_invokehandle:\n+      return method_entry->method();\n+    case Bytecodes::_invokedynamic:\n+      ShouldNotReachHere();\n@@ -494,0 +346,1 @@\n+      assert(invoke_code == (Bytecodes::Code)0, \"unexpected bytecode\");\n@@ -495,22 +348,0 @@\n-    }\n-  }\n-  return nullptr;\n-}\n-\n-\n-oop ConstantPoolCacheEntry::appendix_if_resolved(const constantPoolHandle& cpool) const {\n-  if (!has_appendix())\n-    return nullptr;\n-  const int ref_index = f2_as_index();\n-  return cpool->resolved_reference_at(ref_index);\n-}\n-\n-\n-#if INCLUDE_JVMTI\n-\n-void log_adjust(const char* entry_type, Method* old_method, Method* new_method, bool* trace_name_printed) {\n-  ResourceMark rm;\n-\n-  if (!(*trace_name_printed)) {\n-    log_info(redefine, class, update)(\"adjust: name=%s\", old_method->method_holder()->external_name());\n-    *trace_name_printed = true;\n@@ -518,3 +349,0 @@\n-  log_trace(redefine, class, update, constantpool)\n-    (\"cpc %s entry update: %s\", entry_type, new_method->external_name());\n-}\n@@ -522,59 +350,4 @@\n-\/\/ RedefineClasses() API support:\n-\/\/ If this ConstantPoolCacheEntry refers to old_method then update it\n-\/\/ to refer to new_method.\n-void ConstantPoolCacheEntry::adjust_method_entry(Method* old_method,\n-       Method* new_method, bool * trace_name_printed) {\n-\n-  if (is_vfinal()) {\n-    \/\/ virtual and final so _f2 contains method ptr instead of vtable index\n-    if (f2_as_vfinal_method() == old_method) {\n-      \/\/ match old_method so need an update\n-      \/\/ NOTE: can't use set_f2_as_vfinal_method as it asserts on different values\n-      _f2 = (intptr_t)new_method;\n-      log_adjust(\"vfinal\", old_method, new_method, trace_name_printed);\n-    }\n-    return;\n-  }\n-\n-  assert (_f1 != nullptr, \"should not call with uninteresting entry\");\n-\n-  if (!(_f1->is_method())) {\n-    \/\/ _f1 is a Klass* for an interface, _f2 is the method\n-    if (f2_as_interface_method() == old_method) {\n-      _f2 = (intptr_t)new_method;\n-      log_adjust(\"interface\", old_method, new_method, trace_name_printed);\n-    }\n-  } else if (_f1 == old_method) {\n-    _f1 = new_method;\n-    log_adjust(\"special, static or dynamic\", old_method, new_method, trace_name_printed);\n-  }\n-}\n-\n-\/\/ a constant pool cache entry should never contain old or obsolete methods\n-bool ConstantPoolCacheEntry::check_no_old_or_obsolete_entries() {\n-  Method* m = get_interesting_method_entry();\n-  \/\/ return false if m refers to a non-deleted old or obsolete method\n-  if (m != nullptr) {\n-    assert(m->is_valid() && m->is_method(), \"m is a valid method\");\n-    return !m->is_old() && !m->is_obsolete(); \/\/ old is always set for old and obsolete\n-  } else {\n-    return true;\n-  }\n-}\n-\n-Method* ConstantPoolCacheEntry::get_interesting_method_entry() {\n-  if (!is_method_entry()) {\n-    \/\/ not a method entry so not interesting by default\n-    return nullptr;\n-  }\n-  Method* m = nullptr;\n-  if (is_vfinal()) {\n-    \/\/ virtual and final so _f2 contains method ptr instead of vtable index\n-    m = f2_as_vfinal_method();\n-  } else if (is_f1_null()) {\n-    \/\/ null _f1 means this is a virtual entry so also not interesting\n-    return nullptr;\n-  } else {\n-    if (!(_f1->is_method())) {\n-      \/\/ _f1 is a Klass* for an interface\n-      m = f2_as_interface_method();\n+  invoke_code = (Bytecodes::Code)method_entry->bytecode2();\n+  if (invoke_code == Bytecodes::_invokevirtual) {\n+    if (method_entry->is_vfinal()) {\n+      return method_entry->method();\n@@ -582,43 +355,4 @@\n-      m = f1_as_method();\n-    }\n-  }\n-  assert(m != nullptr && m->is_method(), \"sanity check\");\n-  if (m == nullptr || !m->is_method()) {\n-    return nullptr;\n-  }\n-  return m;\n-}\n-#endif \/\/ INCLUDE_JVMTI\n-\n-void ConstantPoolCacheEntry::print(outputStream* st, int index, const ConstantPoolCache* cache) const {\n-  \/\/ print separator\n-  if (index == 0) st->print_cr(\"                 -------------\");\n-  \/\/ print universal entry info\n-  st->print_cr(\"%3d\", index);\n-  st->print_cr(\" - this: \" PTR_FORMAT, p2i(this));\n-  st->print_cr(\" - bytecode 1: %s %02x\", Bytecodes::name(bytecode_1()), bytecode_1());\n-  st->print_cr(\" - bytecode 2: %s %02x\", Bytecodes::name(bytecode_2()), bytecode_2());\n-  st->print_cr(\" - cp index: %5d\", constant_pool_index());\n-  if (is_method_entry()) {\n-    ResourceMark rm;\n-    constantPoolHandle cph(Thread::current(), cache->constant_pool());\n-    Method* m = method_if_resolved(cph);\n-    st->print_cr(\" - F1:  [   \" PTR_FORMAT \"]\", (intptr_t)_f1);\n-    st->print_cr(\" - F2:  [   \" PTR_FORMAT \"]\", (intptr_t)_f2);\n-    st->print_cr(\" - method: \" INTPTR_FORMAT \" %s\", p2i(m), m != nullptr ? m->external_name() : nullptr);\n-    st->print_cr(\" - flag values: [%02x|0|0|%01x|%01x|%01x|%01x|0|%01x|%01x|00|00|%02x]\",\n-                 flag_state(), has_local_signature(), has_appendix(),\n-                 is_forced_virtual(), is_final(), is_vfinal(),\n-                 indy_resolution_failed(), parameter_size());\n-    st->print_cr(\" - tos: %s\\n - local signature: %01x\\n\"\n-                 \" - has appendix: %01x\\n - forced virtual: %01x\\n\"\n-                 \" - final: %01x\\n - virtual final: %01x\\n - resolution failed: %01x\\n\"\n-                 \" - num parameters: %02x\",\n-                 type2name(as_BasicType(flag_state())), has_local_signature(), has_appendix(),\n-                 is_forced_virtual(), is_final(), is_vfinal(),\n-                 indy_resolution_failed(), parameter_size());\n-    if ((bytecode_1() == Bytecodes::_invokehandle)) {\n-      oop appendix = appendix_if_resolved(cph);\n-      if (appendix != nullptr) {\n-        st->print(\"  appendix: \");\n-        appendix->print_on(st);\n+      int holder_index = constant_pool()->uncached_klass_ref_index_at(method_entry->constant_pool_index());\n+      if (constant_pool()->tag_at(holder_index).is_klass()) {\n+        Klass* klass = constant_pool()->resolved_klass_at(holder_index);\n+        return klass->method_at_vtable(method_entry->table_index());\n@@ -627,27 +361,0 @@\n-  } else {\n-    assert(is_field_entry(), \"must be a field entry\");\n-    st->print_cr(\" - F1:  [   \" PTR_FORMAT \"]\", (intptr_t)_f1);\n-    st->print_cr(\" - F2:  [   \" PTR_FORMAT \"]\", (intptr_t)_f2);\n-    st->print_cr(\" - flag values: [%02x|0|1|0|0|0|%01x|%01x|0|0|%04x]\",\n-                 flag_state(), is_final(), is_volatile(), field_index());\n-    st->print_cr(\" - tos: %s\\n - final: %d\\n - volatile: %d\\n - field index: %04x\",\n-                 type2name(as_BasicType(flag_state())), is_final(), is_volatile(), field_index());\n-  }\n-  st->print_cr(\"                 -------------\");\n-}\n-\n-void ConstantPoolCacheEntry::verify(outputStream* st) const {\n-  \/\/ not implemented yet\n-}\n-\n-\/\/ Implementation of ConstantPoolCache\n-\n-template <class T>\n-static Array<T>* initialize_resolved_entries_array(ClassLoaderData* loader_data, GrowableArray<T> entries, TRAPS) {\n-  Array<T>* resolved_entries;\n-  if (entries.length() != 0) {\n-    resolved_entries = MetadataFactory::new_array<T>(loader_data, entries.length(), CHECK_NULL);\n-    for (int i = 0; i < entries.length(); i++) {\n-      resolved_entries->at_put(i, entries.at(i));\n-    }\n-    return resolved_entries;\n@@ -659,1 +366,0 @@\n-                                     const intStack& index_map,\n@@ -663,0 +369,1 @@\n+                                     const GrowableArray<ResolvedMethodEntry> method_entries,\n@@ -665,2 +372,1 @@\n-  const int length = index_map.length();\n-  int size = ConstantPoolCache::size(length);\n+  int size = ConstantPoolCache::size();\n@@ -671,0 +377,1 @@\n+  Array<ResolvedMethodEntry>* resolved_method_entries = initialize_resolved_entries_array(loader_data, method_entries, CHECK_NULL);\n@@ -673,18 +380,1 @@\n-              ConstantPoolCache(length, index_map, invokedynamic_map, resolved_indy_entries, resolved_field_entries);\n-}\n-\n-void ConstantPoolCache::initialize(const intArray& inverse_index_map,\n-                                   const intArray& invokedynamic_references_map) {\n-  for (int i = 0; i < inverse_index_map.length(); i++) {\n-    ConstantPoolCacheEntry* e = entry_at(i);\n-    int original_index = inverse_index_map.at(i);\n-    e->initialize_entry(original_index);\n-    assert(entry_at(i) == e, \"sanity\");\n-  }\n-\n-  for (int ref = 0; ref < invokedynamic_references_map.length(); ref++) {\n-    const int cpci = invokedynamic_references_map.at(ref);\n-    if (cpci >= 0) {\n-      entry_at(cpci)->initialize_resolved_reference_index(ref);\n-    }\n-  }\n+              ConstantPoolCache(invokedynamic_map, resolved_indy_entries, resolved_field_entries, resolved_method_entries);\n@@ -699,8 +389,0 @@\n-void ConstantPoolCache::save_for_archive(TRAPS) {\n-  ClassLoaderData* loader_data = constant_pool()->pool_holder()->class_loader_data();\n-  _initial_entries = MetadataFactory::new_array<ConstantPoolCacheEntry>(loader_data, length(), CHECK);\n-  for (int i = 0; i < length(); i++) {\n-    _initial_entries->at_put(i, *entry_at(i));\n-  }\n-}\n-\n@@ -712,9 +394,0 @@\n-  assert(_initial_entries != nullptr, \"archived cpcache must have been initialized\");\n-  assert(!ArchiveBuilder::current()->is_in_buffer_space(_initial_entries), \"must be\");\n-  for (int i=0; i<length(); i++) {\n-    \/\/ Restore each entry to the initial state -- just after Rewriter::make_constant_pool_cache()\n-    \/\/ has finished.\n-    *entry_at(i) = _initial_entries->at(i);\n-  }\n-  _initial_entries = nullptr;\n-\n@@ -731,0 +404,5 @@\n+  if (_resolved_method_entries != nullptr) {\n+    for (int i = 0; i < _resolved_method_entries->length(); i++) {\n+      resolved_method_entry_at(i)->remove_unshareable_info();\n+    }\n+  }\n@@ -741,12 +419,11 @@\n-  if (_initial_entries != nullptr) {\n-    assert(CDSConfig::is_dumping_archive(), \"sanity\");\n-    MetadataFactory::free_array<ConstantPoolCacheEntry>(data, _initial_entries);\n-    if (_resolved_indy_entries) {\n-      MetadataFactory::free_array<ResolvedIndyEntry>(data, _resolved_indy_entries);\n-      _resolved_indy_entries = nullptr;\n-    }\n-    if (_resolved_field_entries) {\n-      MetadataFactory::free_array<ResolvedFieldEntry>(data, _resolved_field_entries);\n-      _resolved_field_entries = nullptr;\n-    }\n-    _initial_entries = nullptr;\n+  if (_resolved_indy_entries != nullptr) {\n+    MetadataFactory::free_array<ResolvedIndyEntry>(data, _resolved_indy_entries);\n+    _resolved_indy_entries = nullptr;\n+  }\n+  if (_resolved_field_entries != nullptr) {\n+    MetadataFactory::free_array<ResolvedFieldEntry>(data, _resolved_field_entries);\n+    _resolved_field_entries = nullptr;\n+  }\n+  if (_resolved_method_entries != nullptr) {\n+    MetadataFactory::free_array<ResolvedMethodEntry>(data, _resolved_method_entries);\n+    _resolved_method_entries = nullptr;\n@@ -779,0 +456,11 @@\n+void log_adjust(const char* entry_type, Method* old_method, Method* new_method, bool* trace_name_printed) {\n+  ResourceMark rm;\n+\n+  if (!(*trace_name_printed)) {\n+    log_info(redefine, class, update)(\"adjust: name=%s\", old_method->method_holder()->external_name());\n+    *trace_name_printed = true;\n+  }\n+  log_trace(redefine, class, update, constantpool)\n+    (\"cpc %s entry update: %s\", entry_type, new_method->external_name());\n+}\n+\n@@ -794,10 +482,16 @@\n-  for (int i = 0; i < length(); i++) {\n-    ConstantPoolCacheEntry* entry = entry_at(i);\n-    Method* old_method = entry->get_interesting_method_entry();\n-    if (old_method == nullptr || !old_method->is_old()) {\n-      continue; \/\/ skip uninteresting entries\n-    }\n-    if (old_method->is_deleted()) {\n-      \/\/ clean up entries with deleted methods\n-      entry->initialize_entry(entry->constant_pool_index());\n-      continue;\n+  if (_resolved_method_entries != nullptr) {\n+    for (int i = 0; i < _resolved_method_entries->length(); i++) {\n+      ResolvedMethodEntry* method_entry = resolved_method_entry_at(i);\n+      \/\/ get interesting method entry\n+      Method* old_method = method_entry->method();\n+      if (old_method == nullptr || !old_method->is_old()) {\n+        continue; \/\/ skip uninteresting entries\n+      }\n+      if (old_method->is_deleted()) {\n+        \/\/ clean up entries with deleted methods\n+        method_entry->reset_entry();\n+        continue;\n+      }\n+      Method* new_method = old_method->get_new_method();\n+      method_entry->adjust_method_entry(new_method);\n+      log_adjust(\"non-indy\", old_method, new_method, trace_name_printed);\n@@ -805,2 +499,0 @@\n-    Method* new_method = old_method->get_new_method();\n-    entry_at(i)->adjust_method_entry(old_method, new_method, trace_name_printed);\n@@ -813,1 +505,1 @@\n-  if (_resolved_indy_entries) {\n+  if (_resolved_indy_entries != nullptr) {\n@@ -824,8 +516,10 @@\n-\n-  for (int i = 1; i < length(); i++) {\n-    Method* m = entry_at(i)->get_interesting_method_entry();\n-    if (m != nullptr && !entry_at(i)->check_no_old_or_obsolete_entries()) {\n-      log_trace(redefine, class, update, constantpool)\n-        (\"cpcache check found old method entry: class: %s, old: %d, obsolete: %d, method: %s\",\n-         constant_pool()->pool_holder()->external_name(), m->is_old(), m->is_obsolete(), m->external_name());\n-      return false;\n+  if (_resolved_method_entries != nullptr) {\n+    for (int i = 0; i < _resolved_method_entries->length(); i++) {\n+      ResolvedMethodEntry* method_entry = resolved_method_entry_at(i);\n+      Method* m = method_entry->method();\n+      if (m != nullptr && !method_entry->check_no_old_or_obsolete_entry()) {\n+        log_trace(redefine, class, update, constantpool)\n+          (\"cpcache check found old method entry: class: %s, old: %d, obsolete: %d, method: %s\",\n+           constant_pool()->pool_holder()->external_name(), m->is_old(), m->is_obsolete(), m->external_name());\n+        return false;\n+      }\n@@ -838,5 +532,1 @@\n-  for (int i = 1; i < length(); i++) {\n-    if (entry_at(i)->get_interesting_method_entry() != nullptr) {\n-      entry_at(i)->print(tty, i, this);\n-    }\n-  }\n+  print_on(tty);\n@@ -856,0 +546,3 @@\n+  if (_resolved_method_entries != nullptr) {\n+    it->push(&_resolved_method_entries, MetaspaceClosure::_writable);\n+  }\n@@ -880,1 +573,1 @@\n-  int encoded_index = ResolutionErrorTable::encode_cpcache_index(\n+  int encoded_index = ResolutionErrorTable::encode_indy_index(\n@@ -900,1 +593,1 @@\n-    int encoded_index = ResolutionErrorTable::encode_cpcache_index(\n+    int encoded_index = ResolutionErrorTable::encode_indy_index(\n@@ -944,0 +637,12 @@\n+oop ConstantPoolCache::appendix_if_resolved(int method_index) const {\n+  ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+  return appendix_if_resolved(method_entry);\n+}\n+\n+oop ConstantPoolCache::appendix_if_resolved(ResolvedMethodEntry* method_entry) const {\n+  if (!method_entry->has_appendix())\n+    return nullptr;\n+  const int ref_index = method_entry->resolved_references_index();\n+  return constant_pool()->resolved_reference_at(ref_index);\n+}\n+\n@@ -949,1 +654,0 @@\n-  for (int i = 0; i < length(); i++) entry_at(i)->print(st, i, this);\n@@ -951,0 +655,1 @@\n+  print_resolved_method_entries(st);\n@@ -954,8 +659,0 @@\n-void ConstantPoolCache::print_value_on(outputStream* st) const {\n-  st->print(\"cache [%d]\", length());\n-  print_address_on(st);\n-  st->print(\" for \");\n-  constant_pool()->print_value_on(st);\n-}\n-\n-\n@@ -968,0 +665,11 @@\n+void ConstantPoolCache::print_resolved_method_entries(outputStream* st) const {\n+  for (int method_index = 0; method_index < resolved_method_entries_length(); method_index++) {\n+    ResolvedMethodEntry* method_entry = resolved_method_entry_at(method_index);\n+    method_entry->print_on(st);\n+    if (method_entry->has_appendix()) {\n+      st->print(\"  appendix: \");\n+      constant_pool()->resolved_reference_from_method(method_index)->print_on(st);\n+    }\n+  }\n+}\n+\n@@ -978,7 +686,0 @@\n-\n-\/\/ Verification\n-\n-void ConstantPoolCache::verify_on(outputStream* st) {\n-  \/\/ print constant pool cache entries\n-  for (int i = 0; i < length(); i++) entry_at(i)->verify(st);\n-}\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":203,"deletions":502,"binary":false,"changes":705,"status":"modified"},{"patch":"@@ -41,89 +41,0 @@\n-\/\/ A ConstantPoolCacheEntry describes an individual entry of the constant\n-\/\/ pool cache. There's 2 principal kinds of entries: field entries for in-\n-\/\/ stance & static field access, and method entries for invokes. Some of\n-\/\/ the entry layout is shared and looks as follows:\n-\/\/\n-\/\/ bit number |31                0|\n-\/\/ bit length |-8--|-8--|---16----|\n-\/\/ --------------------------------\n-\/\/ _indices   [ b2 | b1 |  index  ]  index = constant_pool_index\n-\/\/ _f1        [  entry specific   ]  metadata ptr (method or klass)\n-\/\/ _f2        [  entry specific   ]  vtable or res_ref index, or vfinal method ptr\n-\/\/ _flags     [tos|0|F=1|0|0|0|f|v|0 |0000|field_index] (for field entries)\n-\/\/ bit length [ 4 |1| 1 |1|1|1|1|1|1 |1     |-3-|----16-----]\n-\/\/ _flags     [tos|0|F=0|S|A|I|f|0|vf|indy_rf|000|00000|psize] (for method entries)\n-\/\/ bit length [ 4 |1| 1 |1|1|1|1|1|1 |-4--|--8--|--8--]\n-\n-\/\/ --------------------------------\n-\/\/\n-\/\/ with:\n-\/\/ index  = original constant pool index\n-\/\/ b1     = bytecode 1\n-\/\/ b2     = bytecode 2\n-\/\/ psize  = parameters size (method entries only)\n-\/\/ field_index = index into field information in holder InstanceKlass\n-\/\/          The index max is 0xffff (max number of fields in constant pool)\n-\/\/          and is multiplied by (InstanceKlass::next_offset) when accessing.\n-\/\/ tos    = TosState\n-\/\/ F      = the entry is for a field (or F=0 for a method)\n-\/\/ A      = call site has an appendix argument (loaded from resolved references)\n-\/\/ I      = interface call is forced virtual (must use a vtable index or vfinal)\n-\/\/ f      = field or method is final\n-\/\/ v      = field is volatile\n-\/\/ vf     = virtual but final (method entries only: is_vfinal())\n-\/\/ indy_rf = call site specifier method resolution failed\n-\/\/\n-\/\/ The flags after TosState have the following interpretation:\n-\/\/ bit 27: 0 for fields, 1 for methods\n-\/\/ f  flag true if field is marked final\n-\/\/ v  flag true if field is volatile (only for fields)\n-\/\/ f2 flag true if f2 contains an oop (e.g., virtual final method)\n-\/\/ fv flag true if invokeinterface used for method in class Object\n-\/\/\n-\/\/ The flags 31, 30, 29, 28 together build a 4 bit number 0 to 16 with the\n-\/\/ following mapping to the TosState states:\n-\/\/\n-\/\/ btos: 0\n-\/\/ ztos: 1\n-\/\/ ctos: 2\n-\/\/ stos: 3\n-\/\/ itos: 4\n-\/\/ ltos: 5\n-\/\/ ftos: 6\n-\/\/ dtos: 7\n-\/\/ atos: 8\n-\/\/ vtos: 9\n-\/\/\n-\/\/ Entry specific: field entries:\n-\/\/ _indices = get (b1 section) and put (b2 section) bytecodes, original constant pool index\n-\/\/ _f1      = field holder (as a java.lang.Class, not a Klass*)\n-\/\/ _f2      = field offset in bytes\n-\/\/ _flags   = field type information, original FieldInfo index in field holder\n-\/\/            (field_index section)\n-\/\/\n-\/\/ Entry specific: method entries:\n-\/\/ _indices = invoke code for f1 (b1 section), invoke code for f2 (b2 section),\n-\/\/            original constant pool index\n-\/\/ _f1      = Method* for non-virtual calls, unused by virtual calls.\n-\/\/            for interface calls, which are essentially virtual but need a klass,\n-\/\/            contains Klass* for the corresponding interface.\n-\/\/            for invokedynamic and invokehandle, f1 contains the adapter method which\n-\/\/            manages the actual call. The appendix is stored in the ConstantPool\n-\/\/            resolved_references array.\n-\/\/            (upcoming metadata changes will move the appendix to a separate array)\n-\/\/ _f2      = vtable\/itable index (or final Method*) for virtual calls only,\n-\/\/            unused by non-virtual.  The is_vfinal flag indicates this is a\n-\/\/            method pointer for a final method, not an index.\n-\/\/ _flags   = has local signature (MHs and indy),\n-\/\/            virtual final bit (vfinal),\n-\/\/            parameter size (psize section)\n-\/\/\n-\/\/ Note: invokevirtual & invokespecial bytecodes can share the same constant\n-\/\/       pool entry and thus the same constant pool cache entry. All invoke\n-\/\/       bytecodes but invokevirtual use only _f1 and the corresponding b1\n-\/\/       bytecode, while invokevirtual uses only _f2 and the corresponding\n-\/\/       b2 bytecode.  The value of _flags is shared for both types of entries.\n-\/\/\n-\/\/ The fields are volatile so that they are stored in the order written in the\n-\/\/ source code.  The _indices field with the bytecode must be written last.\n-\n@@ -133,247 +44,1 @@\n-\n-class ConstantPoolCacheEntry {\n-  friend class VMStructs;\n-  friend class ConstantPool;\n-  friend class InterpreterRuntime;\n-\n- private:\n-  volatile intx     _indices;  \/\/ constant pool index & rewrite bytecodes\n-  Metadata* volatile   _f1;       \/\/ entry specific metadata field\n-  volatile intx        _f2;       \/\/ entry specific int\/metadata field\n-  volatile intx     _flags;    \/\/ flags\n-\n-\n-  void set_bytecode_1(Bytecodes::Code code);\n-  void set_bytecode_2(Bytecodes::Code code);\n-  void set_f1(Metadata* f1) {\n-    Metadata* existing_f1 = _f1; \/\/ read once\n-    assert(existing_f1 == nullptr || existing_f1 == f1, \"illegal field change\");\n-    _f1 = f1;\n-  }\n-  void release_set_f1(Metadata* f1);\n-  void set_f2(intx f2) {\n-    intx existing_f2 = _f2; \/\/ read once\n-    assert(existing_f2 == 0 || existing_f2 == f2, \"illegal field change\");\n-    _f2 = f2;\n-  }\n-  void set_f2_as_vfinal_method(Method* f2) {\n-    assert(is_vfinal(), \"flags must be set\");\n-    set_f2((intx)f2);\n-  }\n-  intx make_flags(TosState state, int option_bits, int field_index_or_method_params);\n-  void set_flags(intx flags)                     { _flags = flags; }\n-  void set_field_flags(TosState field_type, int option_bits, int field_index) {\n-    assert((field_index & field_index_mask) == field_index, \"field_index in range\");\n-    set_flags(make_flags(field_type, option_bits | (1 << is_field_entry_shift), field_index));\n-  }\n-  void set_method_flags(TosState return_type, int option_bits, int method_params) {\n-    assert((method_params & parameter_size_mask) == method_params, \"method_params in range\");\n-    set_flags(make_flags(return_type, option_bits, method_params));\n-  }\n-\n- public:\n-  \/\/ specific bit definitions for the flags field:\n-  \/\/ (Note: the interpreter must use these definitions to access the CP cache.)\n-  enum {\n-    \/\/ high order bits are the TosState corresponding to field type or method return type\n-    tos_state_bits             = 4,\n-    tos_state_mask             = right_n_bits(tos_state_bits),\n-    tos_state_shift            = BitsPerInt - tos_state_bits,  \/\/ see verify_tos_state_shift below\n-    \/\/ misc. option bits; can be any bit position in [16..27]\n-    is_field_entry_shift       = 26,  \/\/ (F) is it a field or a method?\n-    has_local_signature_shift  = 25,  \/\/ (S) does the call site have a per-site signature (sig-poly methods)?\n-    has_appendix_shift         = 24,  \/\/ (A) does the call site have an appendix argument?\n-    is_forced_virtual_shift    = 23,  \/\/ (I) is the interface reference forced to virtual mode?\n-    is_final_shift             = 22,  \/\/ (f) is the field or method final?\n-    is_volatile_shift          = 21,  \/\/ (v) is the field volatile?\n-    is_vfinal_shift            = 20,  \/\/ (vf) did the call resolve to a final method?\n-    indy_resolution_failed_shift= 19, \/\/ (indy_rf) did call site specifier resolution fail ?\n-    \/\/ low order bits give field index (for FieldInfo) or method parameter size:\n-    field_index_bits           = 16,\n-    field_index_mask           = right_n_bits(field_index_bits),\n-    parameter_size_bits        = 8,  \/\/ subset of field_index_mask, range is 0..255\n-    parameter_size_mask        = right_n_bits(parameter_size_bits),\n-    option_bits_mask           = ~(((~0u) << tos_state_shift) | (field_index_mask | parameter_size_mask))\n-  };\n-\n-  \/\/ specific bit definitions for the indices field:\n-  enum {\n-    cp_index_bits              = 2*BitsPerByte,\n-    cp_index_mask              = right_n_bits(cp_index_bits),\n-    bytecode_1_shift           = cp_index_bits,\n-    bytecode_1_mask            = right_n_bits(BitsPerByte), \/\/ == (u1)0xFF\n-    bytecode_2_shift           = cp_index_bits + BitsPerByte,\n-    bytecode_2_mask            = right_n_bits(BitsPerByte)  \/\/ == (u1)0xFF\n-  };\n-\n-\n-  \/\/ Initialization\n-  void initialize_entry(int original_index);     \/\/ initialize primary entry\n-  void initialize_resolved_reference_index(int ref_index) {\n-    assert(_f2 == 0, \"set once\");  \/\/ note: ref_index might be zero also\n-    _f2 = ref_index;\n-  }\n-\n-  void set_field(                                \/\/ sets entry to resolved field state\n-    Bytecodes::Code get_code,                    \/\/ the bytecode used for reading the field\n-    Bytecodes::Code put_code,                    \/\/ the bytecode used for writing the field\n-    Klass*          field_holder,                \/\/ the object\/klass holding the field\n-    int             orig_field_index,            \/\/ the original field index in the field holder\n-    int             field_offset,                \/\/ the field offset in words in the field holder\n-    TosState        field_type,                  \/\/ the (machine) field type\n-    bool            is_final,                    \/\/ the field is final\n-    bool            is_volatile                  \/\/ the field is volatile\n-  );\n-\n- private:\n-  void set_direct_or_vtable_call(\n-    Bytecodes::Code invoke_code,                 \/\/ the bytecode used for invoking the method\n-    const methodHandle& method,                  \/\/ the method\/prototype if any (null, otherwise)\n-    int             vtable_index,                \/\/ the vtable index if any, else negative\n-    bool            sender_is_interface\n-  );\n-\n- public:\n-  void set_direct_call(                          \/\/ sets entry to exact concrete method entry\n-    Bytecodes::Code invoke_code,                 \/\/ the bytecode used for invoking the method\n-    const methodHandle& method,                  \/\/ the method to call\n-    bool            sender_is_interface\n-  );\n-\n-  void set_vtable_call(                          \/\/ sets entry to vtable index\n-    Bytecodes::Code invoke_code,                 \/\/ the bytecode used for invoking the method\n-    const methodHandle& method,                  \/\/ resolved method which declares the vtable index\n-    int             vtable_index                 \/\/ the vtable index\n-  );\n-\n-  void set_itable_call(\n-    Bytecodes::Code invoke_code,                 \/\/ the bytecode used; must be invokeinterface\n-    Klass* referenced_klass,                     \/\/ the referenced klass in the InterfaceMethodref\n-    const methodHandle& method,                  \/\/ the resolved interface method\n-    int itable_index                             \/\/ index into itable for the method\n-  );\n-\n-  void set_method_handle(\n-    const constantPoolHandle& cpool,             \/\/ holding constant pool (required for locking)\n-    const CallInfo &call_info                    \/\/ Call link information\n-  );\n-\n-  \/\/ Common code for invokedynamic and MH invocations.\n-\n-  \/\/ The \"appendix\" is an optional call-site-specific parameter which is\n-  \/\/ pushed by the JVM at the end of the argument list.  This argument may\n-  \/\/ be a MethodType for the MH.invokes and a CallSite for an invokedynamic\n-  \/\/ instruction.  However, its exact type and use depends on the Java upcall,\n-  \/\/ which simply returns a compiled LambdaForm along with any reference\n-  \/\/ that LambdaForm needs to complete the call.  If the upcall returns a\n-  \/\/ null appendix, the argument is not passed at all.\n-  \/\/\n-  \/\/ The appendix is *not* represented in the signature of the symbolic\n-  \/\/ reference for the call site, but (if present) it *is* represented in\n-  \/\/ the Method* bound to the site.  This means that static and dynamic\n-  \/\/ resolution logic needs to make slightly different assessments about the\n-  \/\/ number and types of arguments.\n-  void set_method_handle_common(\n-    const constantPoolHandle& cpool,                    \/\/ holding constant pool (required for locking)\n-    Bytecodes::Code invoke_code,                 \/\/ _invokehandle or _invokedynamic\n-    const CallInfo &call_info                    \/\/ Call link information\n-  );\n-\n-  \/\/ invokedynamic and invokehandle call sites have an \"appendix\" item in the\n-  \/\/ resolved references array.\n-  Method*      method_if_resolved(const constantPoolHandle& cpool) const;\n-  oop        appendix_if_resolved(const constantPoolHandle& cpool) const;\n-\n-  void set_parameter_size(int value);\n-\n-  \/\/ Which bytecode number (1 or 2) in the index field is valid for this bytecode?\n-  \/\/ Returns -1 if neither is valid.\n-  static int bytecode_number(Bytecodes::Code code) {\n-    switch (code) {\n-      case Bytecodes::_getstatic       :    \/\/ fall through\n-      case Bytecodes::_getfield        :    \/\/ fall through\n-      case Bytecodes::_invokespecial   :    \/\/ fall through\n-      case Bytecodes::_invokestatic    :    \/\/ fall through\n-      case Bytecodes::_invokehandle    :    \/\/ fall through\n-      case Bytecodes::_invokedynamic   :    \/\/ fall through\n-      case Bytecodes::_invokeinterface : return 1;\n-      case Bytecodes::_putstatic       :    \/\/ fall through\n-      case Bytecodes::_putfield        :    \/\/ fall through\n-      case Bytecodes::_invokevirtual   : return 2;\n-      default                          : break;\n-    }\n-    return -1;\n-  }\n-\n-  \/\/ Has this bytecode been resolved? Only valid for invokes and get\/put field\/static.\n-  bool is_resolved(Bytecodes::Code code) const;\n-\n-  \/\/ Accessors\n-  intx indices() const                           { return _indices; }\n-  intx indices_ord() const;\n-  int constant_pool_index() const                { return (indices() & cp_index_mask); }\n-  Bytecodes::Code bytecode_1() const;\n-  Bytecodes::Code bytecode_2() const;\n-  Metadata* f1_ord() const;\n-  Method*   f1_as_method() const;\n-  Klass*    f1_as_klass() const;\n-  \/\/ Use the accessor f1() to acquire _f1's value. This is needed for\n-  \/\/ example in BytecodeInterpreter::run(), where is_f1_null() is\n-  \/\/ called to check if an invokedynamic call is resolved. This load\n-  \/\/ of _f1 must be ordered with the loads performed by\n-  \/\/ cache->main_entry_index().\n-  bool      is_f1_null() const;  \/\/ classifies a CPC entry as unbound\n-  int       f2_as_index() const                  { assert(!is_vfinal(), \"\"); return (int) _f2; }\n-  Method*   f2_as_vfinal_method() const          { assert(is_vfinal(), \"\"); return (Method*)_f2; }\n-  Method*   f2_as_interface_method() const;\n-  intx flags_ord() const;\n-  int  field_index() const                       { assert(is_field_entry(),  \"\"); return (_flags & field_index_mask); }\n-  int  parameter_size() const                    { assert(is_method_entry(), \"\"); return (_flags & parameter_size_mask); }\n-  bool is_volatile() const                       { return (_flags & (1 << is_volatile_shift))       != 0; }\n-  bool is_final() const                          { return (_flags & (1 << is_final_shift))          != 0; }\n-  bool is_forced_virtual() const                 { return (_flags & (1 << is_forced_virtual_shift)) != 0; }\n-  bool is_vfinal() const                         { return (_flags & (1 << is_vfinal_shift))         != 0; }\n-  bool indy_resolution_failed() const;\n-  bool has_appendix() const;\n-  bool has_local_signature() const;\n-  bool is_method_entry() const                   { return (_flags & (1 << is_field_entry_shift))    == 0; }\n-  bool is_field_entry() const                    { return (_flags & (1 << is_field_entry_shift))    != 0; }\n-  bool is_long() const                           { return flag_state() == ltos; }\n-  bool is_double() const                         { return flag_state() == dtos; }\n-  TosState flag_state() const                    { assert((uint)number_of_states <= (uint)tos_state_mask+1, \"\");\n-                                                   return (TosState)((_flags >> tos_state_shift) & tos_state_mask); }\n-  void set_indy_resolution_failed();\n-\n-  \/\/ Code generation support\n-  static WordSize size()                         {\n-    return in_WordSize(align_up((int)sizeof(ConstantPoolCacheEntry), wordSize) \/ wordSize);\n-  }\n-  static ByteSize size_in_bytes()                { return in_ByteSize(sizeof(ConstantPoolCacheEntry)); }\n-  static ByteSize indices_offset()               { return byte_offset_of(ConstantPoolCacheEntry, _indices); }\n-  static ByteSize f1_offset()                    { return byte_offset_of(ConstantPoolCacheEntry, _f1); }\n-  static ByteSize f2_offset()                    { return byte_offset_of(ConstantPoolCacheEntry, _f2); }\n-  static ByteSize flags_offset()                 { return byte_offset_of(ConstantPoolCacheEntry, _flags); }\n-\n-#if INCLUDE_JVMTI\n-  \/\/ RedefineClasses() API support:\n-  \/\/ If this ConstantPoolCacheEntry refers to old_method then update it\n-  \/\/ to refer to new_method.\n-  \/\/ trace_name_printed is set to true if the current call has\n-  \/\/ printed the klass name so that other routines in the adjust_*\n-  \/\/ group don't print the klass name.\n-  void adjust_method_entry(Method* old_method, Method* new_method,\n-         bool* trace_name_printed);\n-  bool check_no_old_or_obsolete_entries();\n-  Method* get_interesting_method_entry();\n-#endif \/\/ INCLUDE_JVMTI\n-\n-  \/\/ Debugging & Printing\n-  void print (outputStream* st, int index, const ConstantPoolCache* cache) const;\n-  void verify(outputStream* st) const;\n-\n-  static void verify_tos_state_shift() {\n-    \/\/ When shifting flags as a 32-bit int, make sure we don't need an extra mask for tos_state:\n-    assert((((u4)-1 >> tos_state_shift) & ~tos_state_mask) == 0, \"no need for tos_state mask\");\n-  }\n-};\n-\n+class ResolvedMethodEntry;\n@@ -382,1 +47,1 @@\n-\/\/ holds interpreter runtime information for all field access and invoke bytecodes. The cache\n+\/\/ holds runtime information for all field access and invoke bytecodes. The cache\n@@ -392,1 +57,0 @@\n-  int             _length;\n@@ -410,4 +74,3 @@\n-  Array<ResolvedIndyEntry>*  _resolved_indy_entries;\n-  Array<ResolvedFieldEntry>* _resolved_field_entries;\n-\n-  CDS_ONLY(Array<ConstantPoolCacheEntry>* _initial_entries;)\n+  Array<ResolvedIndyEntry>*   _resolved_indy_entries;\n+  Array<ResolvedFieldEntry>*  _resolved_field_entries;\n+  Array<ResolvedMethodEntry>* _resolved_method_entries;\n@@ -418,0 +81,12 @@\n+  public:\n+    \/\/ specific but defiinitions for ldc\n+    enum {\n+      \/\/ high order bits are the TosState corresponding to field type or method return type\n+      tos_state_bits             = 4,\n+      tos_state_mask             = right_n_bits(tos_state_bits),\n+      tos_state_shift            = BitsPerInt - tos_state_bits,  \/\/ see verify_tos_state_shift below\n+      \/\/ low order bits give field index (for FieldInfo) or method parameter size:\n+      field_index_bits           = 16,\n+      field_index_mask           = right_n_bits(field_index_bits),\n+    };\n+\n@@ -419,3 +94,1 @@\n-  ConstantPoolCache(int length,\n-                    const intStack& inverse_index_map,\n-                    const intStack& invokedynamic_references_map,\n+  ConstantPoolCache(const intStack& invokedynamic_references_map,\n@@ -423,1 +96,2 @@\n-                    Array<ResolvedFieldEntry>* field_entries);\n+                    Array<ResolvedFieldEntry>* field_entries,\n+                    Array<ResolvedMethodEntry>* mehtod_entries);\n@@ -426,2 +100,1 @@\n-  void initialize(const intArray& inverse_index_map,\n-                  const intArray& invokedynamic_references_map);\n+  void initialize(const intArray& invokedynamic_references_map);\n@@ -430,1 +103,0 @@\n-                                     const intStack& cp_cache_map,\n@@ -434,0 +106,1 @@\n+                                     const GrowableArray<ResolvedMethodEntry> method_entries,\n@@ -436,1 +109,0 @@\n-  int length() const                      { return _length; }\n@@ -449,0 +121,52 @@\n+ private:\n+  void set_direct_or_vtable_call(\n+    Bytecodes::Code invoke_code,                 \/\/ the bytecode used for invoking the method\n+    int method_index,                            \/\/ Index into the resolved method entry array\n+    const methodHandle& method,                  \/\/ the method\/prototype if any (null, otherwise)\n+    int             vtable_index,                \/\/ the vtable index if any, else negative\n+    bool            sender_is_interface\n+  );\n+\n+ public:\n+  void set_direct_call(                          \/\/ sets entry to exact concrete method entry\n+    Bytecodes::Code invoke_code,                 \/\/ the bytecode used for invoking the method\n+    int method_index,                            \/\/ Index into the resolved method entry array\n+    const methodHandle& method,                  \/\/ the method to call\n+    bool            sender_is_interface\n+  );\n+\n+  void set_vtable_call(                          \/\/ sets entry to vtable index\n+    Bytecodes::Code invoke_code,                 \/\/ the bytecode used for invoking the method\n+    int method_index,                            \/\/ Index into the resolved method entry array\n+    const methodHandle& method,                  \/\/ resolved method which declares the vtable index\n+    int             vtable_index                 \/\/ the vtable index\n+  );\n+\n+  void set_itable_call(\n+    Bytecodes::Code invoke_code,                 \/\/ the bytecode used; must be invokeinterface\n+    int method_index,                            \/\/ Index into the resolved method entry array\n+    Klass* referenced_klass,                     \/\/ the referenced klass in the InterfaceMethodref\n+    const methodHandle& method,                  \/\/ the resolved interface method\n+    int itable_index                             \/\/ index into itable for the method\n+  );\n+\n+  \/\/ The \"appendix\" is an optional call-site-specific parameter which is\n+  \/\/ pushed by the JVM at the end of the argument list.  This argument may\n+  \/\/ be a MethodType for the MH.invokes and a CallSite for an invokedynamic\n+  \/\/ instruction.  However, its exact type and use depends on the Java upcall,\n+  \/\/ which simply returns a compiled LambdaForm along with any reference\n+  \/\/ that LambdaForm needs to complete the call.  If the upcall returns a\n+  \/\/ null appendix, the argument is not passed at all.\n+  \/\/\n+  \/\/ The appendix is *not* represented in the signature of the symbolic\n+  \/\/ reference for the call site, but (if present) it *is* represented in\n+  \/\/ the Method* bound to the site.  This means that static and dynamic\n+  \/\/ resolution logic needs to make slightly different assessments about the\n+  \/\/ number and types of arguments.\n+  ResolvedMethodEntry* set_method_handle(\n+    int method_index,\n+    const CallInfo &call_info                    \/\/ Call link information\n+  );\n+\n+  Method*      method_if_resolved(int method_index) const;\n+\n@@ -459,0 +183,5 @@\n+  Array<ResolvedMethodEntry>* resolved_method_entries()          { return _resolved_method_entries; }\n+  inline ResolvedMethodEntry* resolved_method_entry_at(int method_index) const;\n+  inline int resolved_method_entries_length() const;\n+  void print_resolved_method_entries(outputStream* st) const;\n+\n@@ -460,3 +189,4 @@\n-  static ByteSize resolved_references_offset()     { return byte_offset_of(ConstantPoolCache, _resolved_references);    }\n-  static ByteSize invokedynamic_entries_offset()   { return byte_offset_of(ConstantPoolCache, _resolved_indy_entries);  }\n-  static ByteSize field_entries_offset()           { return byte_offset_of(ConstantPoolCache, _resolved_field_entries); }\n+  static ByteSize resolved_references_offset()     { return byte_offset_of(ConstantPoolCache, _resolved_references);     }\n+  static ByteSize invokedynamic_entries_offset()   { return byte_offset_of(ConstantPoolCache, _resolved_indy_entries);   }\n+  static ByteSize field_entries_offset()           { return byte_offset_of(ConstantPoolCache, _resolved_field_entries);  }\n+  static ByteSize method_entries_offset()          { return byte_offset_of(ConstantPoolCache, _resolved_method_entries); }\n@@ -469,6 +199,0 @@\n- private:\n-  void walk_entries_for_initialization(bool check_only);\n-  void set_length(int length)                    { _length = length; }\n-\n-  static int header_size()                       { return sizeof(ConstantPoolCache) \/ wordSize; }\n-  static int size(int length)                    { return align_metadata_size(header_size() + length * in_words(ConstantPoolCacheEntry::size())); }\n@@ -476,1 +200,2 @@\n-  int size() const                               { return size(length()); }\n+  static int size() { return align_metadata_size(sizeof(ConstantPoolCache) \/ wordSize); }\n+\n@@ -481,3 +206,0 @@\n-  ConstantPoolCacheEntry* base() const           { return (ConstantPoolCacheEntry*)((address)this + in_bytes(base_offset())); }\n-\n-  friend class ConstantPoolCacheEntry;\n@@ -489,6 +211,0 @@\n-  \/\/ Fetches the entry at the given index.\n-  \/\/ In either case the index must not be encoded or byte-swapped in any way.\n-  ConstantPoolCacheEntry* entry_at(int i) const {\n-    assert(0 <= i && i < length(), \"index out of bounds\");\n-    return base() + i;\n-  }\n@@ -498,4 +214,0 @@\n-  static ByteSize entry_offset(int raw_index) {\n-    int index = raw_index;\n-    return (base_offset() + ConstantPoolCacheEntry::size_in_bytes() * index);\n-  }\n@@ -528,0 +240,2 @@\n+  oop appendix_if_resolved(int method_index) const;\n+  oop appendix_if_resolved(ResolvedMethodEntry* method_entry) const;\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":87,"deletions":373,"binary":false,"changes":460,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -35,53 +36,0 @@\n-inline intx ConstantPoolCacheEntry::indices_ord() const { return Atomic::load_acquire(&_indices); }\n-\n-inline Bytecodes::Code ConstantPoolCacheEntry::bytecode_1() const {\n-  return Bytecodes::cast((indices_ord() >> bytecode_1_shift) & bytecode_1_mask);\n-}\n-\n-inline Bytecodes::Code ConstantPoolCacheEntry::bytecode_2() const {\n-  return Bytecodes::cast((indices_ord() >> bytecode_2_shift) & bytecode_2_mask);\n-}\n-\n-\/\/ Has this bytecode been resolved? Only valid for invokes and get\/put field\/static.\n-inline bool ConstantPoolCacheEntry::is_resolved(Bytecodes::Code code) const {\n-  switch (bytecode_number(code)) {\n-    case 1:  return (bytecode_1() == code);\n-    case 2:  return (bytecode_2() == code);\n-  }\n-  return false;      \/\/ default: not resolved\n-}\n-\n-inline Method* ConstantPoolCacheEntry::f2_as_interface_method() const {\n-  assert(bytecode_1() == Bytecodes::_invokeinterface, \"\");\n-  return (Method*)_f2;\n-}\n-\n-inline Metadata* ConstantPoolCacheEntry::f1_ord() const { return (Metadata *)Atomic::load_acquire(&_f1); }\n-\n-inline Method* ConstantPoolCacheEntry::f1_as_method() const {\n-  Metadata* f1 = f1_ord(); assert(f1 == nullptr || f1->is_method(), \"\");\n-  return (Method*)f1;\n-}\n-\n-inline Klass* ConstantPoolCacheEntry::f1_as_klass() const {\n-  Metadata* f1 = f1_ord(); assert(f1 == nullptr || f1->is_klass(), \"\");\n-  return (Klass*)f1;\n-}\n-\n-inline bool ConstantPoolCacheEntry::is_f1_null() const { Metadata* f1 = f1_ord(); return f1 == nullptr; }\n-\n-inline bool ConstantPoolCacheEntry::has_appendix() const {\n-  return (!is_f1_null()) && (_flags & (1 << has_appendix_shift)) != 0;\n-}\n-\n-inline bool ConstantPoolCacheEntry::has_local_signature() const {\n-  return (!is_f1_null()) && (_flags & (1 << has_local_signature_shift)) != 0;\n-}\n-\n-inline intx ConstantPoolCacheEntry::flags_ord() const   { return (intx)Atomic::load_acquire(&_flags); }\n-\n-inline bool ConstantPoolCacheEntry::indy_resolution_failed() const {\n-  intx flags = flags_ord();\n-  return (flags & (1 << indy_resolution_failed_shift)) != 0;\n-}\n-\n@@ -89,3 +37,1 @@\n-inline ConstantPoolCache::ConstantPoolCache(int length,\n-                                            const intStack& inverse_index_map,\n-                                            const intStack& invokedynamic_references_map,\n+inline ConstantPoolCache::ConstantPoolCache(const intStack& invokedynamic_references_map,\n@@ -93,2 +39,2 @@\n-                                            Array<ResolvedFieldEntry>* field_entries) :\n-                                                  _length(length),\n+                                            Array<ResolvedFieldEntry>* field_entries,\n+                                            Array<ResolvedMethodEntry>* method_entries) :\n@@ -98,1 +44,2 @@\n-                                                  _resolved_field_entries(field_entries) {\n+                                                  _resolved_field_entries(field_entries),\n+                                                  _resolved_method_entries(method_entries) {\n@@ -100,5 +47,0 @@\n-  initialize(inverse_index_map,\n-             invokedynamic_references_map);\n-  for (int i = 0; i < length; i++) {\n-    assert(entry_at(i)->is_f1_null(), \"Failed to clear?\");\n-  }\n@@ -121,0 +63,8 @@\n+inline ResolvedMethodEntry* ConstantPoolCache::resolved_method_entry_at(int method_index) const {\n+  return _resolved_method_entries->adr_at(method_index);\n+}\n+\n+inline int ConstantPoolCache::resolved_method_entries_length() const {\n+  return _resolved_method_entries->length();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/cpCache.inline.hpp","additions":14,"deletions":64,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -1321,1 +1321,1 @@\n-      int idx = currentBC->has_index_u4() ? currentBC->get_index_u4() : currentBC->get_index_u2_cpcache();\n+      int idx = currentBC->has_index_u4() ? currentBC->get_index_u4() : currentBC->get_index_u2();\n@@ -1600,12 +1600,4 @@\n-    case Bytecodes::_getstatic:\n-      do_field(true,  true,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n-    case Bytecodes::_putstatic:\n-      do_field(false,  true,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n-    case Bytecodes::_getfield:\n-      do_field(true,  false,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n-    case Bytecodes::_putfield:\n-      do_field(false,  false,  itr->get_index_u2(), itr->bci(), itr->code());\n-      break;\n+    case Bytecodes::_getstatic:         do_field(true,   true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_putstatic:         do_field(false,  true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_getfield:          do_field(true,   false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_putfield:          do_field(false,  false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n@@ -1614,4 +1606,4 @@\n-    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(),         itr->bci(), itr->code()); break;\n-    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokestatic:      do_method(true,  false, itr->get_index_u2(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true,  false, itr->get_index_u4(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokeinterface:   do_method(false, true,  itr->get_index_u2(), itr->bci(), itr->code()); break;\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":9,"deletions":17,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-\/\/ within the ConstantPoolCache and are accessed with indices added to the invokedynamic bytecode after\n+\/\/ within the ConstantPoolCache and are accessed with indices added to the bytecode after\n@@ -43,1 +43,1 @@\n-\/\/ Field bytecodes start with a constant pool index as their operate, which is then rewritten to\n+\/\/ Field bytecodes start with a constant pool index as their operand, which is then rewritten to\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,91 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/resolvedMethodEntry.hpp\"\n+\n+bool ResolvedMethodEntry::check_no_old_or_obsolete_entry() {\n+  \/\/ return false if m refers to a non-deleted old or obsolete method\n+  if (_method != nullptr) {\n+    assert(_method->is_valid() && _method->is_method(), \"m is a valid method\");\n+    return !_method->is_old() && !_method->is_obsolete(); \/\/ old is always set for old and obsolete\n+  } else {\n+    return true;\n+  }\n+}\n+\n+void ResolvedMethodEntry::reset_entry() {\n+  if (has_resolved_ref_index()) {\n+    u2 saved_resolved_references_index = _entry_specific._resolved_references_index;\n+    u2 saved_cpool_index = _cpool_index;\n+    memset(this, 0, sizeof(*this));\n+    _entry_specific._resolved_references_index = saved_resolved_references_index;\n+    _cpool_index = saved_cpool_index;\n+  } else {\n+    u2 saved_cpool_index = _cpool_index;\n+    memset(this, 0, sizeof(*this));\n+    _cpool_index = saved_cpool_index;\n+  }\n+}\n+\n+void ResolvedMethodEntry::remove_unshareable_info() {\n+  reset_entry();\n+}\n+\n+void ResolvedMethodEntry::print_on(outputStream* st) const {\n+  st->print_cr(\"Method Entry:\");\n+\n+  if (method() != nullptr) {\n+    st->print_cr(\" - Method: \" INTPTR_FORMAT \" %s\", p2i(method()), method()->external_name());\n+  } else {\n+    st->print_cr(\"- Method: null\");\n+  }\n+  \/\/ Some fields are mutually exclusive and are only used by certain invoke codes\n+  if (bytecode1() == Bytecodes::_invokeinterface && interface_klass() != nullptr) {\n+    st->print_cr(\" - Klass: \" INTPTR_FORMAT \" %s\", p2i(interface_klass()), interface_klass()->external_name());\n+  } else {\n+    st->print_cr(\"- Klass: null\");\n+  }\n+  if (bytecode1() == Bytecodes::_invokehandle) {\n+    st->print_cr(\" - Resolved References Index: %d\", resolved_references_index());\n+  } else {\n+    st->print_cr(\" - Resolved References Index: none\");\n+  }\n+  if (bytecode2() == Bytecodes::_invokevirtual) {\n+    st->print_cr(\" - Table Index: %d\", table_index());\n+  } else {\n+    st->print_cr(\" - Table Index: none\");\n+  }\n+  st->print_cr(\" - CP Index: %d\", constant_pool_index());\n+  st->print_cr(\" - TOS: %s\", type2name(as_BasicType((TosState)tos_state())));\n+  st->print_cr(\" - Number of Parameters: %d\", number_of_parameters());\n+  st->print_cr(\" - Is Virtual Final: %d\", is_vfinal());\n+  st->print_cr(\" - Is Final: %d\", is_final());\n+  st->print_cr(\" - Is Forced Virtual: %d\", is_forced_virtual());\n+  st->print_cr(\" - Has Appendix: %d\", has_appendix());\n+  st->print_cr(\" - Has Local Signature: %d\", has_local_signature());\n+  st->print_cr(\" - Bytecode 1: %s\", Bytecodes::name((Bytecodes::Code)bytecode1()));\n+  st->print_cr(\" - Bytecode 2: %s\", Bytecodes::name((Bytecodes::Code)bytecode2()));\n+}\n","filename":"src\/hotspot\/share\/oops\/resolvedMethodEntry.cpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"added"},{"patch":"@@ -0,0 +1,220 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_RESOLVEDMETHODENTRY_HPP\n+#define SHARE_OOPS_RESOLVEDMETHODENTRY_HPP\n+\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+\/\/ ResolvedMethodEntry contains the resolution information for the invoke bytecodes\n+\/\/ invokestatic, invokespecial, invokeinterface, invokevirtual, and invokehandle but\n+\/\/ NOT invokedynamic (see resolvedIndyEntry.hpp). A member of this class can be initialized\n+\/\/ with the constant pool index associated with the bytecode before any resolution is done,\n+\/\/ where \"resolution\" refers to populating the bytecode1 and bytecode2 fields and other\n+\/\/ relevant information. These entries are contained within the ConstantPoolCache and are\n+\/\/ accessed with indices added to the bytecode after rewriting.\n+\n+\/\/ Invoke bytecodes start with a constant pool index as their operand, which is then\n+\/\/ rewritten to a \"method index\", which is an index into the array of ResolvedMethodEntry.\n+\/\/ This structure has fields for every type of invoke bytecode but each entry may only\n+\/\/ use some of the fields. All entries have a TOS state, number of parameters, flags,\n+\/\/ and a constant pool index.\n+\n+\/\/ Types of invokes\n+\/\/ invokestatic\n+\/\/ invokespecial\n+\/\/   Method*\n+\/\/ invokehandle\n+\/\/   Method*\n+\/\/   resolved references index\n+\/\/ invokevirtual\n+\/\/   Method* (if vfinal is true)\n+\/\/   vtable\/itable index\n+\/\/ invokeinterface\n+\/\/   Klass*\n+\/\/   Method*\n+\n+\/\/ Note: invokevirtual & invokespecial bytecodes can share the same constant\n+\/\/       pool entry and thus the same resolved method entry.\n+\/\/ The is_vfinal flag indicates method pointer for a final method or an index.\n+\n+class InstanceKlass;\n+class ResolvedMethodEntry {\n+  friend class VMStructs;\n+\n+  Method* _method;                   \/\/ Method for non virtual calls, adapter method for invokevirtual, final method for virtual\n+  union {                            \/\/ These fields are mutually exclusive and are only used by some invoke codes\n+    InstanceKlass* _interface_klass; \/\/ for interface and static\n+    u2 _resolved_references_index;   \/\/ Index of resolved references array that holds the appendix oop for invokehandle\n+    u2 _table_index;                 \/\/ vtable\/itable index for virtual and interface calls\n+  } _entry_specific;\n+\n+  u2 _cpool_index;                   \/\/ Constant pool index\n+  u2 _number_of_parameters;          \/\/ Number of arguments for method\n+  u1 _tos_state;                     \/\/ TOS state\n+  u1 _flags;                         \/\/ Flags: [00|has_resolved_ref_index|has_local_signature|has_appendix|forced_virtual|final|virtual_final]\n+  u1 _bytecode1, _bytecode2;         \/\/ Resolved invoke codes\n+\n+  \/\/ Constructors\n+  public:\n+    ResolvedMethodEntry(u2 cpi) :\n+      _method(nullptr),\n+      _cpool_index(cpi),\n+      _number_of_parameters(0),\n+      _tos_state(0),\n+      _flags(0),\n+      _bytecode1(0),\n+      _bytecode2(0) { _entry_specific._interface_klass = nullptr; }\n+    ResolvedMethodEntry() :\n+      ResolvedMethodEntry(0) {}\n+\n+  \/\/ Bit shift to get flags\n+  enum {\n+      is_vfinal_shift           = 0,\n+      is_final_shift            = 1,\n+      is_forced_virtual_shift   = 2,\n+      has_appendix_shift        = 3,\n+      has_local_signature_shift = 4,\n+      has_resolved_ref_shift    = 5\n+  };\n+\n+  \/\/ Getters\n+  Method* method() const { return Atomic::load_acquire(&_method); }\n+  InstanceKlass* interface_klass() const {\n+    assert(_bytecode1 == Bytecodes::_invokeinterface, \"Only invokeinterface has a klass %d\", _bytecode1);\n+    return _entry_specific._interface_klass;\n+  }\n+  u2 resolved_references_index() const {\n+    \/\/ This index may be read before resolution completes\n+    return _entry_specific._resolved_references_index;\n+  }\n+  u2 table_index() const {\n+    assert(_bytecode2 == Bytecodes::_invokevirtual, \"Only invokevirtual has a vtable\/itable index %d\", _bytecode2);\n+    return _entry_specific._table_index;\n+  }\n+  u2 constant_pool_index() const { return _cpool_index; }\n+  u1 tos_state() const { return _tos_state; }\n+  u2 number_of_parameters() const { return _number_of_parameters; }\n+  u1 bytecode1() const { return Atomic::load_acquire(&_bytecode1); }\n+  u1 bytecode2() const { return Atomic::load_acquire(&_bytecode2); }\n+\n+  \/\/ Flags\n+  bool is_vfinal()              const { return (_flags & (1 << is_vfinal_shift))           != 0; }\n+  bool is_final()               const { return (_flags & (1 << is_final_shift))            != 0; }\n+  bool is_forced_virtual()      const { return (_flags & (1 << is_forced_virtual_shift))   != 0; }\n+  bool has_appendix()           const { return (_flags & (1 << has_appendix_shift))        != 0; }\n+  bool has_local_signature()    const { return (_flags & (1 << has_local_signature_shift)) != 0; }\n+  bool has_resolved_ref_index() const { return (_flags & (1 << has_resolved_ref_shift))    != 0; }\n+\n+  bool is_resolved(Bytecodes::Code code) const {\n+    switch(code) {\n+      case Bytecodes::_invokeinterface:\n+      case Bytecodes::_invokehandle:\n+      case Bytecodes::_invokespecial:\n+      case Bytecodes::_invokestatic:\n+        return (bytecode1() == code);\n+      case Bytecodes::_invokevirtual:\n+        return (bytecode2() == code);\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+    }\n+  }\n+\n+  void adjust_method_entry(Method* new_method) {\n+    \/\/ this is done during the redefinition safepoint\n+    _method = new_method;\n+  }\n+  bool check_no_old_or_obsolete_entry();\n+\n+  \/\/ Printing\n+  void print_on(outputStream* st) const;\n+\n+  \/\/ Setters\n+  void set_flags(u1 flags) { _flags |= flags; }\n+\n+  inline void set_bytecode(u1* code, u1 new_code) {\n+  #ifdef ASSERT\n+    \/\/ Read once.\n+    volatile Bytecodes::Code c = (Bytecodes::Code)*code;\n+    assert(c == 0 || c == new_code || new_code == 0, \"update must be consistent old: %d, new: %d\", c, new_code);\n+  #endif\n+    Atomic::release_store(code, new_code);\n+  }\n+\n+  void set_bytecode1(u1 b1) {\n+    set_bytecode(&_bytecode1, b1);\n+  }\n+\n+  void set_bytecode2(u1 b2) {\n+    set_bytecode(&_bytecode2, b2);\n+  }\n+\n+  void set_method(Method* m) {\n+    Atomic::release_store(&_method, m);\n+  }\n+\n+  void set_klass(InstanceKlass* klass) {\n+    _entry_specific._interface_klass = klass;\n+  }\n+\n+  void set_resolved_references_index(u2 ref_index) {\n+    set_flags(1 << has_resolved_ref_shift);\n+    _entry_specific._resolved_references_index = ref_index;\n+  }\n+\n+  void set_table_index(u2 table_index) {\n+    _entry_specific._table_index = table_index;\n+  }\n+\n+  void set_num_parameters(u2 num_params) {\n+    _number_of_parameters = num_params;\n+  }\n+\n+  void fill_in(u1 tos_state, u2 num_params) {\n+    _tos_state = tos_state;\n+    _number_of_parameters = num_params;\n+  }\n+\n+  void reset_entry();\n+\n+  \/\/ CDS\n+  void remove_unshareable_info();\n+\n+  \/\/ Offsets\n+  static ByteSize klass_offset()                     { return byte_offset_of(ResolvedMethodEntry, _entry_specific._interface_klass); }\n+  static ByteSize method_offset()                    { return byte_offset_of(ResolvedMethodEntry, _method);       }\n+  static ByteSize resolved_references_index_offset() { return byte_offset_of(ResolvedMethodEntry, _entry_specific._resolved_references_index); }\n+  static ByteSize table_index_offset()               { return byte_offset_of(ResolvedMethodEntry, _entry_specific._table_index);       }\n+  static ByteSize num_parameters_offset()            { return byte_offset_of(ResolvedMethodEntry, _number_of_parameters);      }\n+  static ByteSize type_offset()                      { return byte_offset_of(ResolvedMethodEntry, _tos_state); }\n+  static ByteSize flags_offset()                     { return byte_offset_of(ResolvedMethodEntry, _flags);        }\n+  static ByteSize bytecode1_offset()                 { return byte_offset_of(ResolvedMethodEntry, _bytecode1);        }\n+  static ByteSize bytecode2_offset()                 { return byte_offset_of(ResolvedMethodEntry, _bytecode2);        }\n+\n+};\n+\n+#endif \/\/SHARE_OOPS_RESOLVEDMETHODENTRY_HPP\n","filename":"src\/hotspot\/share\/oops\/resolvedMethodEntry.hpp","additions":220,"deletions":0,"binary":false,"changes":220,"status":"added"},{"patch":"@@ -511,1 +511,1 @@\n-      int index = iter.get_index_u2_cpcache();\n+      int index = iter.get_index_u2();\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1049,1 +1049,0 @@\n-        ConstantPoolCacheEntry* entry;\n@@ -1055,3 +1054,2 @@\n-        \/\/ cache cannot be pre-fetched since some classes won't have it yet\n-          entry = mh->constants()->cache()->entry_at(cpci);\n-          pool_index = entry->constant_pool_index();\n+          \/\/ cache cannot be pre-fetched since some classes won't have it yet\n+          pool_index = mh->constants()->resolved_method_entry_at(cpci)->constant_pool_index();\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -107,2 +107,2 @@\n-    int cpci_old = s_old->get_index_u2_cpcache();\n-    int cpci_new = s_new->get_index_u2_cpcache();\n+    int index_old = s_old->get_index_u2();\n+    int index_new = s_new->get_index_u2();\n@@ -112,3 +112,3 @@\n-    if ((old_cp->klass_ref_at_noresolve(cpci_old, c_old) != new_cp->klass_ref_at_noresolve(cpci_new, c_old)) ||\n-        (old_cp->name_ref_at(cpci_old, c_old) != new_cp->name_ref_at(cpci_new, c_old)) ||\n-        (old_cp->signature_ref_at(cpci_old, c_old) != new_cp->signature_ref_at(cpci_new, c_old)))\n+    if ((old_cp->klass_ref_at_noresolve(index_old, c_old) != new_cp->klass_ref_at_noresolve(index_new, c_old)) ||\n+        (old_cp->name_ref_at(index_old, c_old) != new_cp->name_ref_at(index_new, c_old)) ||\n+        (old_cp->signature_ref_at(index_old, c_old) != new_cp->signature_ref_at(index_new, c_old)))\n","filename":"src\/hotspot\/share\/prims\/methodComparator.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1871,13 +1871,0 @@\n-WB_ENTRY(jint, WB_GetConstantPoolCacheIndexTag(JNIEnv* env, jobject wb))\n-  return ConstantPool::CPCACHE_INDEX_TAG;\n-WB_END\n-\n-WB_ENTRY(jint, WB_GetConstantPoolCacheLength(JNIEnv* env, jobject wb, jclass klass))\n-  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n-  ConstantPool* cp = ik->constants();\n-  if (cp->cache() == nullptr) {\n-      return -1;\n-  }\n-  return cp->cache()->length();\n-WB_END\n-\n@@ -1890,15 +1877,0 @@\n-WB_ENTRY(jint, WB_ConstantPoolRemapInstructionOperandFromCache(JNIEnv* env, jobject wb, jclass klass, jint index))\n-  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n-  ConstantPool* cp = ik->constants();\n-  if (cp->cache() == nullptr) {\n-    THROW_MSG_0(vmSymbols::java_lang_IllegalStateException(), \"Constant pool does not have a cache\");\n-  }\n-  jint cpci = index;\n-  jint cpciTag = ConstantPool::CPCACHE_INDEX_TAG;\n-  if (cpciTag > cpci || cpci >= cp->cache()->length() + cpciTag) {\n-    THROW_MSG_0(vmSymbols::java_lang_IllegalArgumentException(), \"Constant pool cache index is out of range\");\n-  }\n-  jint cpi = cp->remap_instruction_operand_from_cache(cpci);\n-  return cpi;\n-WB_END\n-\n@@ -1927,0 +1899,18 @@\n+WB_ENTRY(jint, WB_getMethodEntriesLength(JNIEnv* env, jobject wb, jclass klass))\n+  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n+  ConstantPool* cp = ik->constants();\n+  if (cp->cache() == nullptr) {\n+    return -1;\n+  }\n+  return cp->resolved_method_entries_length();\n+WB_END\n+\n+WB_ENTRY(jint, WB_getMethodCPIndex(JNIEnv* env, jobject wb, jclass klass, jint index))\n+  InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(klass)));\n+  ConstantPool* cp = ik->constants();\n+  if (cp->cache() == NULL) {\n+      return -1;\n+  }\n+  return cp->resolved_method_entry_at(index)->constant_pool_index();\n+WB_END\n+\n@@ -2815,2 +2805,0 @@\n-  {CC\"getConstantPoolCacheIndexTag0\", CC\"()I\",  (void*)&WB_GetConstantPoolCacheIndexTag},\n-  {CC\"getConstantPoolCacheLength0\", CC\"(Ljava\/lang\/Class;)I\",  (void*)&WB_GetConstantPoolCacheLength},\n@@ -2818,2 +2806,0 @@\n-  {CC\"remapInstructionOperandFromCPCache0\",\n-      CC\"(Ljava\/lang\/Class;I)I\",                      (void*)&WB_ConstantPoolRemapInstructionOperandFromCache},\n@@ -2824,0 +2810,2 @@\n+  {CC\"getMethodEntriesLength0\", CC\"(Ljava\/lang\/Class;)I\",  (void*)&WB_getMethodEntriesLength},\n+  {CC\"getMethodCPIndex0\",    CC\"(Ljava\/lang\/Class;I)I\", (void*)&WB_getMethodCPIndex},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":20,"deletions":32,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -227,1 +228,0 @@\n-  nonstatic_field(ConstantPoolCache,           _length,                                       int)                                   \\\n@@ -231,0 +231,2 @@\n+  nonstatic_field(ConstantPoolCache,           _resolved_method_entries,                      Array<ResolvedMethodEntry>*)           \\\n+  nonstatic_field(ResolvedMethodEntry,         _cpool_index,                                  u2)                                    \\\n@@ -340,9 +342,0 @@\n-  \/***********************\/                                                                                                          \\\n-  \/* Constant Pool Cache *\/                                                                                                          \\\n-  \/***********************\/                                                                                                          \\\n-                                                                                                                                     \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _indices,                             intx)                                  \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _f1,                                  Metadata*)                             \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _f2,                                  intx)                                  \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _flags,                               intx)                                  \\\n-                                                                                                                                     \\\n@@ -491,0 +484,2 @@\n+  nonstatic_field(Array<ResolvedMethodEntry>,  _length,                                       int)                                   \\\n+  nonstatic_field(Array<ResolvedMethodEntry>,  _data[0],                                      ResolvedMethodEntry)                   \\\n@@ -978,0 +973,1 @@\n+  unchecked_nonstatic_field(Array<ResolvedMethodEntry>,_data,                                 sizeof(ResolvedMethodEntry))           \\\n@@ -1906,0 +1902,1 @@\n+            declare_type(Array<ResolvedMethodEntry>, MetaspaceObj)        \\\n@@ -1923,1 +1920,0 @@\n-  declare_toplevel_type(ConstantPoolCacheEntry)                           \\\n@@ -1925,0 +1921,1 @@\n+  declare_toplevel_type(ResolvedMethodEntry)                              \\\n@@ -2201,12 +2198,0 @@\n-  declare_constant(ConstantPool::CPCACHE_INDEX_TAG)                       \\\n-                                                                          \\\n-  \/********************************\/                                      \\\n-  \/* ConstantPoolCacheEntry enums *\/                                      \\\n-  \/********************************\/                                      \\\n-                                                                          \\\n-  declare_constant(ConstantPoolCacheEntry::is_volatile_shift)             \\\n-  declare_constant(ConstantPoolCacheEntry::is_final_shift)                \\\n-  declare_constant(ConstantPoolCacheEntry::is_forced_virtual_shift)       \\\n-  declare_constant(ConstantPoolCacheEntry::is_vfinal_shift)               \\\n-  declare_constant(ConstantPoolCacheEntry::is_field_entry_shift)          \\\n-  declare_constant(ConstantPoolCacheEntry::tos_state_shift)               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":8,"deletions":23,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -59,0 +59,2 @@\n+     } else if (Bytecodes.isFieldCode(code())) {\n+        return cpCache.getFieldEntryAt(cpCacheIndex).getConstantPoolIndex();\n@@ -60,1 +62,1 @@\n-        return cpCache.getEntryAt((int) (0xFFFF & cpCacheIndex)).getConstantPoolIndex();\n+        return cpCache.getMethodEntryAt(cpCacheIndex).getConstantPoolIndex();\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/interpreter\/BytecodeWithCPIndex.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -404,0 +404,1 @@\n+  public static boolean   isFieldCode  (int code) { return (_getstatic <= code && code <= _putfield); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/interpreter\/Bytecodes.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -230,1 +230,1 @@\n-  public int getFieldOrMethodAt(int which) {\n+  public int getFieldOrMethodAt(int which, int code) {\n@@ -240,1 +240,1 @@\n-      i = cache.getEntryAt(0xFFFF & which).getConstantPoolIndex();\n+      i = to_cp_index(which, code);\n@@ -272,1 +272,1 @@\n-        \/\/ TODO: handle resolved method entries with new structure\n+        return getCache().getMethodEntryAt(index).getConstantPoolIndex();\n@@ -274,2 +274,1 @@\n-        \/\/ change byte-ordering and go via cache\n-        return remapInstructionOperandFromCache(index);\n+        throw new InternalError(\"Unexpected bytecode: \" + code);\n@@ -322,6 +321,0 @@\n-  ConstantPoolCacheEntry invokedynamicCPCacheEntryAt(int index) {\n-    \/\/ decode index that invokedynamic points to.\n-    int cpCacheIndex = invokedynamicCPCacheIndex(index);\n-    return getCache().getEntryAt(cpCacheIndex);\n-  }\n-\n@@ -344,8 +337,0 @@\n-  private int remapInstructionOperandFromCache(int operand) {\n-    int cpc_index = operand;\n-    \/\/ DEBUG_ONLY(cpc_index -= CPCACHE_INDEX_TAG);\n-    \/\/ assert((int)(u2)cpc_index == cpc_index, \"clean u2\");\n-    int member_index = getCache().getEntryAt(cpc_index).getConstantPoolIndex();\n-    return member_index;\n-  }\n-\n@@ -375,2 +360,2 @@\n-  public Klass getFieldOrMethodKlassRefAt(int which) {\n-    int refIndex = getFieldOrMethodAt(which);\n+  public Klass getFieldOrMethodKlassRefAt(int which, int code) {\n+    int refIndex = getFieldOrMethodAt(which, code);\n@@ -383,1 +368,1 @@\n-    Klass klass = getFieldOrMethodKlassRefAt(which);\n+    Klass klass = getFieldOrMethodKlassRefAt(which, code);\n@@ -396,1 +381,1 @@\n-    InstanceKlass klass = (InstanceKlass)getFieldOrMethodKlassRefAt(which);\n+    InstanceKlass klass = (InstanceKlass)getFieldOrMethodKlassRefAt(which, code);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ConstantPool.java","additions":8,"deletions":23,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -52,3 +52,0 @@\n-    Type elType    = db.lookupType(\"ConstantPoolCacheEntry\");\n-    elementSize    = elType.getSize();\n-    length         = new CIntField(type.getCIntegerField(\"_length\"), 0);\n@@ -59,0 +56,1 @@\n+    resolvedMethodArray = type.getAddressField(\"_resolved_method_entries\");\n@@ -77,0 +75,1 @@\n+  private static AddressField  resolvedMethodArray;\n@@ -82,6 +81,1 @@\n-    return alignSize(baseOffset + getLength() * elementSize);\n-  }\n-\n-  public ConstantPoolCacheEntry getEntryAt(int i) {\n-    Objects.checkIndex(i, getLength());\n-    return new ConstantPoolCacheEntry(this, i);\n+    return alignSize(baseOffset);\n@@ -102,0 +96,6 @@\n+  public ResolvedMethodEntry getMethodEntryAt(int i) {\n+    Address addr = resolvedMethodArray.getValue(getAddress());\n+    ResolvedMethodArray array = new ResolvedMethodArray(addr);\n+    return array.getAt(i);\n+  }\n+\n@@ -112,13 +112,0 @@\n-  public int getLength() {\n-    return (int) length.getValue(getAddress());\n-  }\n-\n-  public void iterateFields(MetadataVisitor visitor) {\n-    super.iterateFields(visitor);\n-    visitor.doMetadata(constants, true);\n-      for (int i = 0; i < getLength(); i++) {\n-        ConstantPoolCacheEntry entry = getEntryAt(i);\n-        entry.iterateFields(visitor);\n-      }\n-    }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ConstantPoolCache.java","additions":9,"deletions":22,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -1,104 +0,0 @@\n-\/*\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-package sun.jvm.hotspot.oops;\n-\n-import java.util.*;\n-import sun.jvm.hotspot.debugger.*;\n-import sun.jvm.hotspot.runtime.*;\n-import sun.jvm.hotspot.types.*;\n-import sun.jvm.hotspot.utilities.*;\n-import sun.jvm.hotspot.utilities.Observable;\n-import sun.jvm.hotspot.utilities.Observer;\n-\n-public class ConstantPoolCacheEntry {\n-  private static long          size;\n-  private static long          baseOffset;\n-  private static CIntegerField indices;\n-  private static AddressField  f1;\n-  private static CIntegerField f2;\n-  private static CIntegerField flags;\n-\n-  private ConstantPoolCache cp;\n-  private long      offset;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static synchronized void initialize(TypeDataBase db) throws WrongTypeException {\n-    Type type      = db.lookupType(\"ConstantPoolCacheEntry\");\n-    size = type.getSize();\n-\n-    indices = type.getCIntegerField(\"_indices\");\n-    f1      = type.getAddressField (\"_f1\");\n-    f2      = type.getCIntegerField(\"_f2\");\n-    flags   = type.getCIntegerField(\"_flags\");\n-\n-    type = db.lookupType(\"ConstantPoolCache\");\n-    baseOffset = type.getSize();\n-  }\n-\n-  ConstantPoolCacheEntry(ConstantPoolCache cp, int index) {\n-    this.cp = cp;\n-    offset  = baseOffset + index * size;\n-  }\n-\n-  public int getConstantPoolIndex() {\n-    if (Assert.ASSERTS_ENABLED) {\n-      Assert.that((getIndices() & 0xFFFF) != 0, \"must be main entry\");\n-    }\n-    return (int) (getIndices() & 0xFFFF);\n-  }\n-\n-  private long getIndices() {\n-    return cp.getAddress().getCIntegerAt(indices.getOffset() + offset, indices.getSize(), indices.isUnsigned());\n-  }\n-\n-  public Metadata getF1() {\n-    return Metadata.instantiateWrapperFor(cp.getAddress().getAddressAt(f1.getOffset() + offset));\n-  }\n-\n-  public int getF2() {\n-    return cp.getAddress().getJIntAt(f1.getOffset() + offset);\n-  }\n-\n-  public int getFlags() {\n-    return cp.getAddress().getJIntAt(flags.getOffset() + offset);\n-  }\n-\n-  static NamedFieldIdentifier f1FieldName = new NamedFieldIdentifier(\"_f1\");\n-  static NamedFieldIdentifier f2FieldName = new NamedFieldIdentifier(\"_f2\");\n-  static NamedFieldIdentifier flagsFieldName = new NamedFieldIdentifier(\"_flags\");\n-\n-  public void iterateFields(MetadataVisitor visitor) {\n-    visitor.doOop(new OopField(f1FieldName, f1.getOffset() + offset, true), true);\n-    visitor.doInt(new IntField(f2FieldName, f2.getOffset() + offset, true), true);\n-    visitor.doInt(new IntField(flagsFieldName, flags.getOffset() + offset, true), true);\n-  }\n-}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ConstantPoolCacheEntry.java","additions":0,"deletions":104,"binary":false,"changes":104,"status":"deleted"},{"patch":"@@ -0,0 +1,73 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+ package sun.jvm.hotspot.oops;\n+\n+ import sun.jvm.hotspot.debugger.Address;\n+ import sun.jvm.hotspot.runtime.VM;\n+ import sun.jvm.hotspot.types.Type;\n+ import sun.jvm.hotspot.types.TypeDataBase;\n+ import sun.jvm.hotspot.types.WrongTypeException;\n+ import sun.jvm.hotspot.utilities.GenericArray;\n+ import sun.jvm.hotspot.utilities.Observable;\n+ import sun.jvm.hotspot.utilities.Observer;\n+\n+ public class ResolvedMethodArray extends GenericArray {\n+     static {\n+         VM.registerVMInitializedObserver(new Observer() {\n+             public void update(Observable o, Object data) {\n+                 initialize(VM.getVM().getTypeDataBase());\n+             }\n+         });\n+     }\n+\n+     private static synchronized void initialize(TypeDataBase db) throws WrongTypeException {\n+         elemType = db.lookupType(\"ResolvedMethodEntry\");\n+\n+         Type type = db.lookupType(\"Array<ResolvedMethodEntry>\");\n+         dataFieldOffset = type.getAddressField(\"_data\").getOffset();\n+     }\n+\n+     private static long dataFieldOffset;\n+     protected static Type elemType;\n+\n+     public ResolvedMethodArray(Address addr) {\n+         super(addr, dataFieldOffset);\n+     }\n+\n+     public ResolvedMethodEntry getAt(int index) {\n+         if (index < 0 || index >= length()) throw new ArrayIndexOutOfBoundsException(index + \" \" + length());\n+\n+         Type elemType = getElemType();\n+\n+         Address data = getAddress().addOffsetTo(dataFieldOffset);\n+         long elemSize = elemType.getSize();\n+\n+         return new ResolvedMethodEntry(data.addOffsetTo(index* elemSize));\n+     }\n+\n+     public Type getElemType() {\n+         return elemType;\n+     }\n+ }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ResolvedMethodArray.java","additions":73,"deletions":0,"binary":false,"changes":73,"status":"added"},{"patch":"@@ -35,1 +35,1 @@\n- public class ResolvedFieldEntry extends VMObject {\n+ public class ResolvedMethodEntry extends VMObject {\n@@ -49,1 +49,1 @@\n-         Type type = db.lookupType(\"ResolvedFieldEntry\");\n+         Type type = db.lookupType(\"ResolvedMethodEntry\");\n@@ -55,1 +55,1 @@\n-     ResolvedFieldEntry(Address addr) {\n+     ResolvedMethodEntry(Address addr) {\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ResolvedMethodEntry.java","additions":3,"deletions":3,"binary":false,"changes":6,"previous_filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/ResolvedFieldEntry.java","status":"copied"},{"patch":"@@ -129,2 +129,2 @@\n-                    int cpci = method.getNativeShortArg(bci + 1);\n-                    cpoolIndex = (short) cpCache.getEntryAt(cpci).getConstantPoolIndex();\n+                    int methodIndex = method.getNativeShortArg(bci + 1);\n+                    cpoolIndex = (short) cpCache.getMethodEntryAt(methodIndex).getConstantPoolIndex();\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/tools\/jcore\/ByteCodeRewriter.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -450,1 +450,1 @@\n-     * @return {@code JVM_CONSTANT_FieldRef} constant pool entry index for the invokedynamic\n+     * @return {@code JVM_CONSTANT_FieldRef} constant pool entry index for the instruction\n@@ -458,0 +458,13 @@\n+    \/**\n+     * Converts the {@code rawIndex} operand of a rewritten invokestatic\/invokespecial\/invokeinterface\/invokevirtual instruction\n+     * to an index directly into {@code constantPool}.\n+     *\n+     * @throws IllegalArgumentException if {@code rawIndex} is out of range.\n+     * @return {@code JVM_CONSTANT_MethodRef} or {@code JVM_CONSTANT_InterfaceMethodRef} constant pool entry index for the instruction\n+     *\/\n+    int decodeMethodIndexToCPIndex(HotSpotConstantPool constantPool, int rawIndex) {\n+      return decodeMethodIndexToCPIndex(constantPool, constantPool.getConstantPoolPointer(), rawIndex);\n+  }\n+\n+  private native int decodeMethodIndexToCPIndex(HotSpotConstantPool constantPool, long constantPoolPointer, int rawIndex);\n+\n@@ -579,13 +592,0 @@\n-    \/**\n-     * Converts {@code cpci} from an index into the cache for {@code constantPool} to an index\n-     * directly into {@code constantPool}.\n-     *\n-     * The behavior of this method is undefined if {@code cpci} is an invalid constant pool cache\n-     * index.\n-     *\/\n-    int constantPoolRemapInstructionOperandFromCache(HotSpotConstantPool constantPool, int cpci) {\n-        return constantPoolRemapInstructionOperandFromCache(constantPool, constantPool.getConstantPoolPointer(), cpci);\n-    }\n-\n-    private native int constantPoolRemapInstructionOperandFromCache(HotSpotConstantPool constantPool, long constantPoolPointer, int cpci);\n-\n@@ -596,1 +596,1 @@\n-     *              Otherwise, it's treated as a constant pool cache index (returned by HotSpotConstantPool::rawIndexToConstantPoolCacheIndex)\n+     *              Otherwise, it's treated as a constant pool cache index\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/CompilerToVM.java","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n- *                It's the same as {@code rawIndex + HotSpotVMConfig::constantPoolCpCacheIndexTag}. <\/li>\n+ *                It's the same as {@code rawIndex}. <\/li>\n@@ -262,20 +262,0 @@\n-    \/**\n-     * Converts a raw index from the bytecodes to a constant pool cache index by adding a\n-     * {@link HotSpotVMConfig#constantPoolCpCacheIndexTag constant}.\n-     *\n-     * @param rawIndex index from the bytecode\n-     * @param opcode bytecode to convert the index for\n-     * @return constant pool cache index\n-     *\/\n-    private static int rawIndexToConstantPoolCacheIndex(int rawIndex, int opcode) {\n-        if (opcode == Bytecodes.INVOKEINTERFACE ||\n-            opcode == Bytecodes.INVOKEVIRTUAL ||\n-            opcode == Bytecodes.INVOKESPECIAL ||\n-            opcode == Bytecodes.INVOKESTATIC) {\n-            return rawIndex + config().constantPoolCpCacheIndexTag;\n-        } else {\n-            \/\/ Only the above 4 bytecodes use ConstantPoolCacheIndex\n-            throw new IllegalArgumentException(\"unexpected opcode \" + opcode);\n-        }\n-    }\n-\n@@ -738,1 +718,1 @@\n-          return compilerToVM().lookupAppendixInPool(this, rawIndexToConstantPoolCacheIndex(rawIndex, opcode));\n+          return compilerToVM().lookupAppendixInPool(this, rawIndex);\n@@ -765,1 +745,1 @@\n-            which = rawIndexToConstantPoolCacheIndex(rawIndex, opcode);\n+            which = rawIndex;\n@@ -823,2 +803,1 @@\n-                int cpci = rawIndexToConstantPoolCacheIndex(rawIndex, opcode);\n-                cpi = getKlassRefIndexAt(cpci, opcode);\n+                cpi = getKlassRefIndexAt(rawIndex, opcode);\n@@ -925,3 +904,1 @@\n-                \/\/ invoke and field instructions point to a constant pool cache entry.\n-                int cpci = rawIndexToConstantPoolCacheIndex(rawIndex, opcode);\n-                cpi = compilerToVM().constantPoolRemapInstructionOperandFromCache(this, cpci);\n+                cpi = compilerToVM().decodeMethodIndexToCPIndex(this, rawIndex);\n@@ -957,3 +934,2 @@\n-                        final int methodRefCacheIndex = rawIndexToConstantPoolCacheIndex(rawIndex, opcode);\n-                        checkTag(compilerToVM().constantPoolRemapInstructionOperandFromCache(this, methodRefCacheIndex), constants.jvmMethodref);\n-                        compilerToVM().resolveInvokeHandleInPool(this, methodRefCacheIndex);\n+                        checkTag(compilerToVM().decodeMethodIndexToCPIndex(this, rawIndex), constants.jvmMethodref);\n+                        compilerToVM().resolveInvokeHandleInPool(this, rawIndex);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotConstantPool.java","additions":7,"deletions":31,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -222,1 +222,0 @@\n-    final int constantPoolCpCacheIndexTag = getConstant(\"ConstantPool::CPCACHE_INDEX_TAG\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,4 +47,3 @@\n-    \"this\", \"bytecode 1:\", \"bytecode 2:\", \"cp index:\", \"F1:\", \"F2:\",\n-    \"method:\", \"flag values:\", \"tos:\", \"local signature:\", \"has appendix:\",\n-    \"forced virtual:\", \"final:\", \"virtual final:\", \"resolution failed:\",\n-    \"num parameters:\",\n+    \"Klass:\", \"Method:\", \"CP Index:\", \"Resolved References Index:\", \"Table Index:\",\n+    \"TOS:\", \"Number of Parameters:\", \"Is Virtual Final:\", \"Is Final\", \"Is Forced Virtual\",\n+    \"Has Appendix:\", \"Has Local Signature\", \"Bytecode 1:\", \"Bytecode 2:\",\n","filename":"test\/hotspot\/gtest\/oops\/test_cpCache_output.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -146,5 +146,0 @@\n-    public static int constantPoolRemapInstructionOperandFromCache(\n-            ConstantPool constantPool, int cpci) {\n-        return CTVM.constantPoolRemapInstructionOperandFromCache((HotSpotConstantPool) constantPool, cpci);\n-    }\n-\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/common\/patches\/jdk.internal.vm.ci\/jdk\/vm\/ci\/hotspot\/CompilerToVMHelper.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -251,4 +251,2 @@\n-            boolean isCPCached = WB.getConstantPoolCacheLength(dummyClass.klass) > -1;\n-            System.out.printf(\"Testing dummy %s with constant pool cached = %b%n\",\n-                              dummyClass.klass,\n-                              isCPCached);\n+            System.out.printf(\"Testing dummy %s with constant pool\",\n+                              dummyClass.klass);\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/compilerToVM\/ConstantPoolTestCase.java","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -94,6 +94,4 @@\n-            int cacheLength = WB.getConstantPoolCacheLength(this.klass);\n-            int indexTag = WB.getConstantPoolCacheIndexTag();\n-            for (int cpci = indexTag; cpci < cacheLength + indexTag; cpci++) {\n-                if (WB.remapInstructionOperandFromCPCache(this.klass, cpci) == cpi) {\n-                    if (constantPoolSS.getTagAt(cpi).equals(Tag.INVOKEDYNAMIC)) {\n-                        return WB.encodeConstantPoolIndyIndex(cpci) + indexTag;\n+            if (constantPoolSS.getTagAt(cpi).equals(Tag.METHODREF) || constantPoolSS.getTagAt(cpi).equals(Tag.INTERFACEMETHODREF)) {\n+                for (int method_index = 0; method_index < WB.getMethodEntriesLength(this.klass); method_index++) {\n+                    if (WB.getMethodCPIndex(this.klass, method_index) == cpi) {\n+                        return method_index;\n@@ -101,1 +99,0 @@\n-                    return cpci;\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/compilerToVM\/ConstantPoolTestsHelper.java","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -132,11 +132,0 @@\n-  private native int getConstantPoolCacheIndexTag0();\n-  public         int getConstantPoolCacheIndexTag() {\n-    return getConstantPoolCacheIndexTag0();\n-  }\n-\n-  private native int getConstantPoolCacheLength0(Class<?> aClass);\n-  public         int getConstantPoolCacheLength(Class<?> aClass) {\n-    Objects.requireNonNull(aClass);\n-    return getConstantPoolCacheLength0(aClass);\n-  }\n-\n@@ -172,0 +161,12 @@\n+  private native int getMethodEntriesLength0(Class<?> aClass);\n+  public         int getMethodEntriesLength(Class<?> aClass) {\n+    Objects.requireNonNull(aClass);\n+    return getMethodEntriesLength0(aClass);\n+  }\n+\n+  private native int getMethodCPIndex0(Class<?> aClass, int index);\n+  public         int getMethodCPIndex(Class<?> aClass, int index) {\n+    Objects.requireNonNull(aClass);\n+    return getMethodCPIndex0(aClass, index);\n+  }\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"}]}