{"files":[{"patch":"@@ -35,5 +35,0 @@\n-\/\/ explicit rounding operations are required to implement the strictFP mode\n-enum {\n-  pd_strict_fp_requires_explicit_rounding = false\n-};\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Defs_aarch64.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,31 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/\/--------------------------------------------------------\n-\/\/               FpuStackSim\n-\/\/--------------------------------------------------------\n-\n-\/\/ No FPU stack on AARCH64\n-#include \"precompiled.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_FpuStackSim_aarch64.cpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"deleted"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, Red Hat Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_AARCH64_C1_FPUSTACKSIM_AARCH64_HPP\n-#define CPU_AARCH64_C1_FPUSTACKSIM_AARCH64_HPP\n-\n-\/\/ No FPU stack on AARCH64\n-class FpuStackSim;\n-\n-#endif \/\/ CPU_AARCH64_C1_FPUSTACKSIM_AARCH64_HPP\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_FpuStackSim_aarch64.hpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -411,1 +411,1 @@\n-  set_result(x, round_item(reg));\n+  set_result(x, reg);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2010, Oracle and\/or its affiliates. All rights reserved. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_Instruction.hpp\"\n-#include \"c1\/c1_LinearScan.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-\n-void LinearScan::allocate_fpu_stack() {\n-  \/\/ No FPU stack on AArch64\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LinearScan_aarch64.cpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -1405,3 +1405,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) { ; }\n-\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -306,2 +306,0 @@\n-  \/\/ only if +VerifyFPU  && (state == ftos || state == dtos)\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -118,3 +118,0 @@\n-  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-  static const bool strict_fp_requires_explicit_rounding = false;\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/matcher_aarch64.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1804,6 +1804,0 @@\n-\/\/ Not supported\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4513,12 +4513,0 @@\n-\/\/ instruct roundDouble_nop(regD dst) %{\n-\/\/   match(Set dst (RoundDouble dst));\n-\/\/   ins_pipe(empty);\n-\/\/ %}\n-\n-\n-\/\/ instruct roundFloat_nop(regF dst) %{\n-\/\/   match(Set dst (RoundFloat dst));\n-\/\/   ins_pipe(empty);\n-\/\/ %}\n-\n-\n","filename":"src\/hotspot\/cpu\/arm\/arm.ad","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,5 +34,0 @@\n-\/\/ explicit rounding operations are required to implement the strictFP mode\n-enum {\n-  pd_strict_fp_requires_explicit_rounding = false\n-};\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_Defs_arm.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,25 +0,0 @@\n-\/*\n- * Copyright (c) 2008, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/\/ Nothing needed here\n","filename":"src\/hotspot\/cpu\/arm\/c1_FpuStackSim_arm.cpp","additions":0,"deletions":25,"binary":false,"changes":25,"status":"deleted"},{"patch":"@@ -1,30 +0,0 @@\n-\/*\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_ARM_C1_FPUSTACKSIM_ARM_HPP\n-#define CPU_ARM_C1_FPUSTACKSIM_ARM_HPP\n-\n-\/\/ Nothing needed here\n-\n-#endif \/\/ CPU_ARM_C1_FPUSTACKSIM_ARM_HPP\n","filename":"src\/hotspot\/cpu\/arm\/c1_FpuStackSim_arm.hpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"deleted"},{"patch":"@@ -928,1 +928,1 @@\n-      __ convert(x->op(), value.result(), reg, nullptr);\n+      __ convert(x->op(), value.result(), reg);\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRGenerator_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2008, 2011, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_Instruction.hpp\"\n-#include \"c1\/c1_LinearScan.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-\n-void LinearScan::allocate_fpu_stack() {\n-  \/\/ No FPU stack on ARM\n-}\n","filename":"src\/hotspot\/cpu\/arm\/c1_LinearScan_arm.cpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -201,4 +201,0 @@\n-  void verify_FPU(int stack_depth, TosState state = ftos) {\n-    \/\/ No VFP state verification is required for ARM\n-  }\n-\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -104,3 +104,0 @@\n-  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-  static const bool strict_fp_requires_explicit_rounding = false;\n-\n","filename":"src\/hotspot\/cpu\/arm\/matcher_arm.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -798,4 +798,0 @@\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n","filename":"src\/hotspot\/cpu\/arm\/templateInterpreterGenerator_arm.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,6 +41,0 @@\n-\/\/ Explicit rounding operations are not required to implement the strictFP mode.\n-enum {\n-  pd_strict_fp_requires_explicit_rounding = false\n-};\n-\n-\n","filename":"src\/hotspot\/cpu\/ppc\/c1_Defs_ppc.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2015 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_PPC_C1_FPUSTACKSIM_PPC_HPP\n-#define CPU_PPC_C1_FPUSTACKSIM_PPC_HPP\n-\n-\/\/ No FPU stack on PPC.\n-class FpuStackSim;\n-\n-#endif \/\/ CPU_PPC_C1_FPUSTACKSIM_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/c1_FpuStackSim_ppc.hpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -1,34 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2015, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2015 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_Instruction.hpp\"\n-#include \"c1\/c1_LinearScan.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-\n-void LinearScan::allocate_fpu_stack() {\n-  Unimplemented();\n-  \/\/ No FPU stack on PPC\n-}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LinearScan_ppc.cpp","additions":0,"deletions":34,"binary":false,"changes":34,"status":"deleted"},{"patch":"@@ -268,1 +268,0 @@\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2385,6 +2385,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {\n-  if (VerifyFPU) {\n-    unimplemented(\"verfiyFPU\");\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -116,3 +116,0 @@\n-  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-  static const bool strict_fp_requires_explicit_rounding = false;\n-\n","filename":"src\/hotspot\/cpu\/ppc\/matcher_ppc.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -9535,22 +9535,0 @@\n-instruct roundDouble_nop(regD dst) %{\n-  match(Set dst (RoundDouble dst));\n-  ins_cost(0);\n-\n-  format %{ \" -- \\t\/\/ RoundDouble not needed - empty\" %}\n-  size(0);\n-  \/\/ PPC results are already \"rounded\" (i.e., normal-format IEEE).\n-  ins_encode( \/*empty*\/ );\n-  ins_pipe(pipe_class_default);\n-%}\n-\n-instruct roundFloat_nop(regF dst) %{\n-  match(Set dst (RoundFloat dst));\n-  ins_cost(0);\n-\n-  format %{ \" -- \\t\/\/ RoundFloat not needed - empty\" %}\n-  size(0);\n-  \/\/ PPC results are already \"rounded\" (i.e., normal-format IEEE).\n-  ins_encode( \/*empty*\/ );\n-  ins_pipe(pipe_class_default);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":0,"deletions":22,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2002,4 +2002,0 @@\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n","filename":"src\/hotspot\/cpu\/ppc\/templateInterpreterGenerator_ppc.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,5 +35,0 @@\n-\/\/ explicit rounding operations are required to implement the strictFP mode\n-enum {\n-  pd_strict_fp_requires_explicit_rounding = false\n-};\n-\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Defs_riscv.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,30 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/\/--------------------------------------------------------\n-\/\/               FpuStackSim\n-\/\/--------------------------------------------------------\n-\n-\/\/ No FPU stack on RISCV\n","filename":"src\/hotspot\/cpu\/riscv\/c1_FpuStackSim_riscv.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"deleted"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_RISCV_C1_FPUSTACKSIM_RISCV_HPP\n-#define CPU_RISCV_C1_FPUSTACKSIM_RISCV_HPP\n-\n-\/\/ No FPU stack on RISCV\n-class FpuStackSim;\n-\n-#endif \/\/ CPU_RISCV_C1_FPUSTACKSIM_RISCV_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/c1_FpuStackSim_riscv.hpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -359,1 +359,1 @@\n-  set_result(x, round_item(reg));\n+  set_result(x, reg);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRGenerator_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,33 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_Instruction.hpp\"\n-#include \"c1\/c1_LinearScan.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-\n-void LinearScan::allocate_fpu_stack() {\n-  \/\/ No FPU stack on RISCV\n-}\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LinearScan_riscv.cpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"deleted"},{"patch":"@@ -1435,2 +1435,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) { ; }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -287,4 +287,0 @@\n-  \/\/ Debugging\n-  \/\/ only if +VerifyFPU  && (state == ftos || state == dtos)\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n-\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -117,3 +117,0 @@\n-  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-  static const bool strict_fp_requires_explicit_rounding = false;\n-\n","filename":"src\/hotspot\/cpu\/riscv\/matcher_riscv.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -942,4 +942,0 @@\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n","filename":"src\/hotspot\/cpu\/riscv\/templateInterpreterGenerator_riscv.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,5 +35,0 @@\n-\/\/ Explicit rounding operations are not required to implement the strictFP mode.\n-enum {\n-  pd_strict_fp_requires_explicit_rounding = false\n-};\n-\n","filename":"src\/hotspot\/cpu\/s390\/c1_Defs_s390.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,32 +0,0 @@\n-\/*\n- * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_S390_C1_FPUSTACKSIM_S390_HPP\n-#define CPU_S390_C1_FPUSTACKSIM_S390_HPP\n-\n-\/\/ No FPU stack on ZARCH_64\n-class FpuStackSim;\n-\n-#endif \/\/ CPU_S390_C1_FPUSTACKSIM_S390_HPP\n","filename":"src\/hotspot\/cpu\/s390\/c1_FpuStackSim_s390.hpp","additions":0,"deletions":32,"binary":false,"changes":32,"status":"deleted"},{"patch":"@@ -1,33 +0,0 @@\n-\/*\n- * Copyright (c) 2016, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016 SAP SE. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_LinearScan.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-void LinearScan::allocate_fpu_stack() {\n-  \/\/ No FPU stack on ZARCH_64.\n-  ShouldNotCallThis();\n-}\n","filename":"src\/hotspot\/cpu\/s390\/c1_LinearScan_s390.cpp","additions":0,"deletions":33,"binary":false,"changes":33,"status":"deleted"},{"patch":"@@ -97,2 +97,0 @@\n-  verify_FPU(1, state);\n-\n@@ -2193,6 +2191,0 @@\n-\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {\n-  if (VerifyFPU) {\n-    unimplemented(\"verifyFPU\");\n-  }\n-}\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -316,1 +316,0 @@\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -5320,17 +5320,0 @@\n-instruct roundDouble_nop(regD dst) %{\n-  match(Set dst (RoundDouble dst));\n-  ins_cost(0);\n-  \/\/ TODO: s390 port size(FIXED_SIZE);\n-  \/\/ z\/Architecture results are already \"rounded\" (i.e., normal-format IEEE).\n-  ins_encode();\n-  ins_pipe(pipe_class_dummy);\n-%}\n-\n-instruct roundFloat_nop(regF dst) %{\n-  match(Set dst (RoundFloat dst));\n-  ins_cost(0);\n-  \/\/ TODO: s390 port size(FIXED_SIZE);\n-  \/\/ z\/Architecture results are already \"rounded\" (i.e., normal-format IEEE).\n-  ins_encode();\n-  ins_pipe(pipe_class_dummy);\n-%}\n","filename":"src\/hotspot\/cpu\/s390\/s390.ad","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2010,4 +2010,0 @@\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n","filename":"src\/hotspot\/cpu\/s390\/templateInterpreterGenerator_s390.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -121,21 +121,0 @@\n-#ifndef _LP64\n-int AbstractInterpreter::BasicType_as_index(BasicType type) {\n-  int i = 0;\n-  switch (type) {\n-    case T_BOOLEAN: i = 0; break;\n-    case T_CHAR   : i = 1; break;\n-    case T_BYTE   : i = 2; break;\n-    case T_SHORT  : i = 3; break;\n-    case T_INT    : \/\/ fall through\n-    case T_LONG   : \/\/ fall through\n-    case T_VOID   : i = 4; break;\n-    case T_FLOAT  : i = 5; break;  \/\/ have to treat float and double separately for SSE\n-    case T_DOUBLE : i = 6; break;\n-    case T_OBJECT : \/\/ fall through\n-    case T_ARRAY  : i = 7; break;\n-    default       : ShouldNotReachHere();\n-  }\n-  assert(0 <= i && i < AbstractInterpreter::number_of_result_handlers, \"index out of bounds\");\n-  return i;\n-}\n-#else\n@@ -162,1 +141,0 @@\n-#endif \/\/ _LP64\n@@ -174,3 +152,0 @@\n-#ifndef _LP64\n-  const int stub_code = 4;  \/\/ see generate_call_stub\n-#else\n@@ -178,1 +153,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/abstractInterpreter_x86.cpp","additions":0,"deletions":26,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"code\/codeCache.hpp\"\n@@ -123,2 +125,0 @@\n-#ifdef _LP64\n-\n@@ -161,24 +161,0 @@\n-#else \/\/ LP64\n-\n-Address Address::make_array(ArrayAddress adr) {\n-  AddressLiteral base = adr.base();\n-  Address index = adr.index();\n-  assert(index._disp == 0, \"must not have disp\"); \/\/ maybe it can?\n-  Address array(index._base, index._index, index._scale, (intptr_t) base.target());\n-  array._rspec = base._rspec;\n-  return array;\n-}\n-\n-\/\/ exceedingly dangerous constructor\n-Address::Address(address loc, RelocationHolder spec) {\n-  _base  = noreg;\n-  _index = noreg;\n-  _scale = no_scale;\n-  _disp  = (intptr_t) loc;\n-  _rspec = spec;\n-  _xmmindex = xnoreg;\n-  _isxmmindex = false;\n-}\n-\n-#endif \/\/ _LP64\n-\n@@ -218,1 +194,0 @@\n-  NOT_LP64(_is_managed = false;)\n@@ -734,1 +709,1 @@\n-      LP64_ONLY(adjusted -=  (next_ip - inst_mark()));\n+      adjusted -= (next_ip - inst_mark());\n@@ -835,1 +810,1 @@\n-    LP64_ONLY(assert(false, \"shouldn't have that prefix\"));\n+    assert(false, \"shouldn't have that prefix\");\n@@ -848,1 +823,0 @@\n-    NOT_LP64(assert(false, \"64bit prefixes\"));\n@@ -852,1 +826,0 @@\n-    NOT_LP64(assert(false, \"64bit prefixes\"));\n@@ -866,1 +839,0 @@\n-    NOT_LP64(assert(false, \"64bit prefixes\"));\n@@ -905,1 +877,0 @@\n-      NOT_LP64(assert(false, \"64bit prefix found\"));\n@@ -909,1 +880,0 @@\n-      NOT_LP64(assert(false, \"64bit prefix found\"));\n@@ -934,4 +904,0 @@\n-#ifndef _LP64\n-    assert(which == imm_operand || which == disp32_operand,\n-           \"which %d is_64_bit %d ip \" INTPTR_FORMAT, which, is_64bit, p2i(ip));\n-#else\n@@ -941,1 +907,0 @@\n-#endif \/\/ _LP64\n@@ -1101,3 +1066,0 @@\n-    \/\/ Check second byte\n-    NOT_LP64(assert((0xC0 & *ip) == 0xC0, \"shouldn't have LDS and LES instructions\"));\n-\n@@ -1205,1 +1167,0 @@\n-      NOT_LP64(assert(false, \"found 64bit prefix\"));\n@@ -1221,1 +1182,0 @@\n-#ifdef _LP64\n@@ -1223,4 +1183,0 @@\n-#else\n-  \/\/ assert(which != imm_operand || has_imm32, \"instruction has no imm32 field\");\n-  assert(which != imm_operand || has_disp32, \"instruction has no imm32 field\");\n-#endif \/\/ LP64\n@@ -1281,1 +1237,0 @@\n-#ifdef _LP64\n@@ -1283,3 +1238,0 @@\n-#else\n-  assert(which == imm_operand, \"instruction has only an imm field\");\n-#endif \/\/ LP64\n@@ -1308,2 +1260,1 @@\n-    assert(format == imm_operand || format == disp32_operand\n-           LP64_ONLY(|| format == narrow_oop_operand), \"format ok\");\n+    assert(format == imm_operand || format == disp32_operand || format == narrow_oop_operand, \"format ok\");\n@@ -1538,1 +1489,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -1546,1 +1496,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -1557,1 +1506,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -1564,1 +1512,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -1823,3 +1770,0 @@\n-  \/\/ suspect disp32 is always good\n-  int operand = LP64_ONLY(disp32_operand) NOT_LP64(imm_operand);\n-\n@@ -1833,1 +1777,1 @@\n-    emit_data(offs - long_size, rtype, operand);\n+    emit_data(offs - long_size, rtype, disp32_operand);\n@@ -1840,1 +1784,1 @@\n-    emit_data(int(0), rtype, operand);\n+    emit_data(int(0), rtype, disp32_operand);\n@@ -1867,2 +1811,1 @@\n-  int operand = LP64_ONLY(disp32_operand) NOT_LP64(call32_operand);\n-  emit_data((int) disp, rspec, operand);\n+  emit_data((int) disp, rspec, disp32_operand);\n@@ -1880,1 +1823,0 @@\n-  NOT_LP64(guarantee(VM_Version::supports_cmov(), \"illegal instruction\"));\n@@ -1893,1 +1835,0 @@\n-  NOT_LP64(guarantee(VM_Version::supports_cmov(), \"illegal instruction\"));\n@@ -2021,1 +1962,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2032,1 +1972,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2040,1 +1979,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2050,1 +1988,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2088,1 +2025,1 @@\n-    LP64_ONLY(case 8:)\n+    case 8:\n@@ -2108,1 +2045,1 @@\n-    LP64_ONLY(prefix(crc, v, p);)\n+    prefix(crc, v, p);\n@@ -2137,1 +2074,1 @@\n-    LP64_ONLY(case 8:)\n+    case 8:\n@@ -2145,1 +2082,1 @@\n-    LP64_ONLY(prefix(crc, adr, p);)\n+    prefix(crc, adr, p);\n@@ -2152,1 +2089,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2215,1 +2151,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2229,1 +2164,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2237,1 +2171,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2248,1 +2181,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2255,1 +2187,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2265,1 +2196,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2272,1 +2202,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2282,1 +2211,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2289,1 +2217,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2296,1 +2223,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2307,1 +2233,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2314,1 +2239,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2321,1 +2245,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2328,1 +2251,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2541,1 +2463,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2552,1 +2473,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2560,1 +2480,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2570,1 +2489,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2842,1 +2760,0 @@\n-    NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2857,1 +2774,0 @@\n-#ifdef _LP64\n@@ -2875,1 +2791,0 @@\n-#endif\n@@ -2924,1 +2839,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"unsupported\");)\n@@ -2930,1 +2844,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"unsupported\");)\n@@ -2935,1 +2848,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2939,1 +2852,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -2948,1 +2860,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2956,1 +2867,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -2963,1 +2873,0 @@\n-  NOT_LP64(assert(dst->has_byte_register(), \"must have byte register\"));\n@@ -3392,1 +3301,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3399,1 +3307,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3407,1 +3314,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3417,1 +3323,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3427,1 +3332,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3434,1 +3338,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3444,1 +3347,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3454,1 +3356,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3461,1 +3362,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3845,1 +3745,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3856,1 +3755,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3867,1 +3765,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3878,1 +3775,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3886,1 +3782,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3894,1 +3789,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3908,1 +3802,0 @@\n-  NOT_LP64(assert(src->has_byte_register(), \"must have byte register\"));\n@@ -3914,1 +3807,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3922,1 +3814,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3933,1 +3824,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -3953,1 +3843,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -3960,1 +3849,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -3970,1 +3858,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -3993,1 +3880,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -4013,1 +3899,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -4066,1 +3951,0 @@\n-  NOT_LP64(assert(src->has_byte_register(), \"must have byte register\"));\n@@ -4111,1 +3995,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -4122,1 +4005,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -4130,1 +4012,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -4140,1 +4021,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -4603,1 +4483,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -4617,1 +4496,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -4631,1 +4509,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -4642,1 +4519,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -4823,1 +4699,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -4971,1 +4846,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -5020,1 +4894,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -5125,1 +4998,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -5191,1 +5063,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -5277,1 +5148,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -5284,1 +5154,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -5594,1 +5463,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5810,10 +5678,0 @@\n-#ifndef _LP64 \/\/ no 32bit push\/pop on amd64\n-void Assembler::popl(Address dst) {\n-  \/\/ NOTE: this will adjust stack by 8byte on 64bits\n-  InstructionMark im(this);\n-  prefix(dst);\n-  emit_int8((unsigned char)0x8F);\n-  emit_operand(rax, dst, 0);\n-}\n-#endif\n-\n@@ -5821,1 +5679,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"must support\"));\n@@ -5837,1 +5694,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"must support\"));\n@@ -5845,1 +5701,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"must support\"));\n@@ -5853,1 +5708,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"must support\"));\n@@ -5930,1 +5784,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5941,1 +5794,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5949,1 +5801,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5962,1 +5813,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5972,1 +5822,0 @@\n-    NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5980,1 +5829,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -5988,1 +5836,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6003,1 +5850,0 @@\n-    NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6020,1 +5866,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6035,1 +5880,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6049,1 +5893,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6066,1 +5909,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6162,1 +6004,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6173,1 +6014,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6180,1 +6020,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6191,1 +6030,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6198,1 +6036,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -6241,1 +6078,0 @@\n-#ifdef _LP64\n@@ -6303,2 +6139,0 @@\n-#endif \/\/_LP64\n-\n@@ -6322,10 +6156,0 @@\n-#ifndef _LP64 \/\/ no 32bit push\/pop on amd64\n-void Assembler::pushl(Address src) {\n-  \/\/ Note this will push 64bit on 64bit\n-  InstructionMark im(this);\n-  prefix(src);\n-  emit_int8((unsigned char)0xFF);\n-  emit_operand(rsi, src, 0);\n-}\n-#endif\n-\n@@ -6354,1 +6178,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -6361,1 +6184,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -6376,2 +6198,1 @@\n-  LP64_ONLY(emit_int24((unsigned char)0xF3, REX_W, (unsigned char)0xA5);)\n-  NOT_LP64( emit_int16((unsigned char)0xF3,        (unsigned char)0xA5);)\n+  emit_int24((unsigned char)0xF3, REX_W, (unsigned char)0xA5);\n@@ -6384,2 +6205,1 @@\n-  LP64_ONLY(emit_int24((unsigned char)0xF3, REX_W, (unsigned char)0xAA);)\n-  NOT_LP64( emit_int16((unsigned char)0xF3,        (unsigned char)0xAA);)\n+  emit_int24((unsigned char)0xF3, REX_W, (unsigned char)0xAA);\n@@ -6392,3 +6212,2 @@\n-  \/\/ LP64:STOSQ, LP32:STOSD\n-  LP64_ONLY(emit_int24((unsigned char)0xF3, REX_W, (unsigned char)0xAB);)\n-  NOT_LP64( emit_int16((unsigned char)0xF3,        (unsigned char)0xAB);)\n+  \/\/ STOSQ\n+  emit_int24((unsigned char)0xF3, REX_W, (unsigned char)0xAB);\n@@ -6401,2 +6220,1 @@\n-  LP64_ONLY(emit_int24((unsigned char)0xF2, REX_W, (unsigned char)0xAF);)\n-  NOT_LP64( emit_int16((unsigned char)0xF2,        (unsigned char)0xAF);)\n+  emit_int24((unsigned char)0xF2, REX_W, (unsigned char)0xAF);\n@@ -6405,1 +6223,0 @@\n-#ifdef _LP64\n@@ -6412,1 +6229,0 @@\n-#endif\n@@ -6487,1 +6303,0 @@\n-#ifdef _LP64\n@@ -6551,1 +6366,0 @@\n-#endif\n@@ -6554,4 +6368,0 @@\n-#ifdef _LP64\n-  \/\/ Not supported in 64bit mode\n-  ShouldNotReachHere();\n-#endif\n@@ -7011,1 +6821,0 @@\n-#ifdef _LP64\n@@ -7037,1 +6846,0 @@\n-#endif\n@@ -7062,1 +6870,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7070,1 +6877,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7081,1 +6887,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7092,1 +6897,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7113,1 +6917,0 @@\n-    NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7204,1 +7007,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7212,1 +7014,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7223,1 +7024,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7230,1 +7030,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7240,1 +7039,0 @@\n-  NOT_LP64(assert(dst->has_byte_register(), \"must have byte register\"));\n@@ -7366,1 +7164,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7377,1 +7174,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7385,1 +7181,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7395,1 +7190,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -7787,1 +7581,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7795,1 +7588,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7807,1 +7599,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7850,1 +7641,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7858,1 +7648,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7901,1 +7690,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7909,1 +7697,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7920,1 +7707,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -7997,1 +7783,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8005,1 +7790,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8139,1 +7923,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8147,1 +7930,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8155,1 +7937,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8162,1 +7943,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8172,1 +7952,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8219,1 +7998,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8228,1 +8006,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8236,1 +8013,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8244,1 +8020,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8251,1 +8026,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8262,1 +8036,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8325,1 +8098,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8332,1 +8104,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8339,1 +8110,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8346,1 +8116,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8355,1 +8124,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8602,1 +8370,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8609,1 +8376,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8622,1 +8388,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8701,1 +8466,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -8715,1 +8479,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -8816,1 +8579,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -8854,1 +8616,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8867,1 +8628,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8895,1 +8655,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\");)\n@@ -8933,1 +8692,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -8947,1 +8705,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -9302,1 +9059,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9310,1 +9066,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9318,1 +9073,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9326,1 +9080,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9333,1 +9086,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9340,1 +9092,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9357,1 +9108,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9397,1 +9147,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9405,1 +9154,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9415,1 +9163,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9424,1 +9171,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9431,1 +9177,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9438,1 +9183,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9510,1 +9254,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9518,1 +9261,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9528,1 +9270,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9535,1 +9276,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9591,1 +9331,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9694,1 +9433,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9709,1 +9447,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -9770,1 +9507,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -12404,1 +12140,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\");)\n@@ -12412,1 +12147,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\");)\n@@ -12904,422 +12638,0 @@\n-#ifndef _LP64\n-\/\/ 32bit only pieces of the assembler\n-\n-void Assembler::emms() {\n-  NOT_LP64(assert(VM_Version::supports_mmx(), \"\"));\n-  emit_int16(0x0F, 0x77);\n-}\n-\n-void Assembler::vzeroupper() {\n-  vzeroupper_uncached();\n-}\n-\n-void Assembler::cmp_literal32(Register src1, int32_t imm32, RelocationHolder const& rspec) {\n-  \/\/ NO PREFIX AS NEVER 64BIT\n-  InstructionMark im(this);\n-  emit_int16((unsigned char)0x81, (0xF8 | src1->encoding()));\n-  emit_data(imm32, rspec, 0);\n-}\n-\n-void Assembler::cmp_literal32(Address src1, int32_t imm32, RelocationHolder const& rspec) {\n-  \/\/ NO PREFIX AS NEVER 64BIT (not even 32bit versions of 64bit regs\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0x81);\n-  emit_operand(rdi, src1, 4);\n-  emit_data(imm32, rspec, 0);\n-}\n-\n-\/\/ The 64-bit (32bit platform) cmpxchg compares the value at adr with the contents of rdx:rax,\n-\/\/ and stores rcx:rbx into adr if so; otherwise, the value at adr is loaded\n-\/\/ into rdx:rax.  The ZF is set if the compared values were equal, and cleared otherwise.\n-void Assembler::cmpxchg8(Address adr) {\n-  InstructionMark im(this);\n-  emit_int16(0x0F, (unsigned char)0xC7);\n-  emit_operand(rcx, adr, 0);\n-}\n-\n-void Assembler::decl(Register dst) {\n-  \/\/ Don't use it directly. Use MacroAssembler::decrementl() instead.\n- emit_int8(0x48 | dst->encoding());\n-}\n-\n-void Assembler::edecl(Register dst, Register src, bool no_flags) {\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  (void) evex_prefix_and_encode_ndd(0, dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_3C, &attributes, no_flags);\n-  emit_int8(0x48 | src->encoding());\n-}\n-\n-\/\/ 64bit doesn't use the x87\n-\n-void Assembler::fabs() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xE1);\n-}\n-\n-void Assembler::fadd(int i) {\n-  emit_farith(0xD8, 0xC0, i);\n-}\n-\n-void Assembler::fadd_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rax, src, 0);\n-}\n-\n-void Assembler::fadd_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rax, src, 0);\n-}\n-\n-void Assembler::fadda(int i) {\n-  emit_farith(0xDC, 0xC0, i);\n-}\n-\n-void Assembler::faddp(int i) {\n-  emit_farith(0xDE, 0xC0, i);\n-}\n-\n-void Assembler::fchs() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xE0);\n-}\n-\n-void Assembler::fcom(int i) {\n-  emit_farith(0xD8, 0xD0, i);\n-}\n-\n-void Assembler::fcomp(int i) {\n-  emit_farith(0xD8, 0xD8, i);\n-}\n-\n-void Assembler::fcomp_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rbx, src, 0);\n-}\n-\n-void Assembler::fcomp_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rbx, src, 0);\n-}\n-\n-void Assembler::fcompp() {\n-  emit_int16((unsigned char)0xDE, (unsigned char)0xD9);\n-}\n-\n-void Assembler::fcos() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xFF);\n-}\n-\n-void Assembler::fdecstp() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xF6);\n-}\n-\n-void Assembler::fdiv(int i) {\n-  emit_farith(0xD8, 0xF0, i);\n-}\n-\n-void Assembler::fdiv_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rsi, src, 0);\n-}\n-\n-void Assembler::fdiv_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rsi, src, 0);\n-}\n-\n-void Assembler::fdiva(int i) {\n-  emit_farith(0xDC, 0xF8, i);\n-}\n-\n-\/\/ Note: The Intel manual (Pentium Processor User's Manual, Vol.3, 1994)\n-\/\/       is erroneous for some of the floating-point instructions below.\n-\n-void Assembler::fdivp(int i) {\n-  emit_farith(0xDE, 0xF8, i);                    \/\/ ST(0) <- ST(0) \/ ST(1) and pop (Intel manual wrong)\n-}\n-\n-void Assembler::fdivr(int i) {\n-  emit_farith(0xD8, 0xF8, i);\n-}\n-\n-void Assembler::fdivr_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rdi, src, 0);\n-}\n-\n-void Assembler::fdivr_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rdi, src, 0);\n-}\n-\n-void Assembler::fdivra(int i) {\n-  emit_farith(0xDC, 0xF0, i);\n-}\n-\n-void Assembler::fdivrp(int i) {\n-  emit_farith(0xDE, 0xF0, i);                    \/\/ ST(0) <- ST(1) \/ ST(0) and pop (Intel manual wrong)\n-}\n-\n-void Assembler::ffree(int i) {\n-  emit_farith(0xDD, 0xC0, i);\n-}\n-\n-void Assembler::fild_d(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDF);\n-  emit_operand32(rbp, adr, 0);\n-}\n-\n-void Assembler::fild_s(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDB);\n-  emit_operand32(rax, adr, 0);\n-}\n-\n-void Assembler::fincstp() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xF7);\n-}\n-\n-void Assembler::finit() {\n-  emit_int24((unsigned char)0x9B, (unsigned char)0xDB, (unsigned char)0xE3);\n-}\n-\n-void Assembler::fist_s(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDB);\n-  emit_operand32(rdx, adr, 0);\n-}\n-\n-void Assembler::fistp_d(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDF);\n-  emit_operand32(rdi, adr, 0);\n-}\n-\n-void Assembler::fistp_s(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDB);\n-  emit_operand32(rbx, adr, 0);\n-}\n-\n-void Assembler::fld1() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xE8);\n-}\n-\n-void Assembler::fld_s(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD9);\n-  emit_operand32(rax, adr, 0);\n-}\n-\n-\n-void Assembler::fld_s(int index) {\n-  emit_farith(0xD9, 0xC0, index);\n-}\n-\n-void Assembler::fldcw(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD9);\n-  emit_operand32(rbp, src, 0);\n-}\n-\n-void Assembler::fldenv(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD9);\n-  emit_operand32(rsp, src, 0);\n-}\n-\n-void Assembler::fldlg2() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xEC);\n-}\n-\n-void Assembler::fldln2() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xED);\n-}\n-\n-void Assembler::fldz() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xEE);\n-}\n-\n-void Assembler::flog() {\n-  fldln2();\n-  fxch();\n-  fyl2x();\n-}\n-\n-void Assembler::flog10() {\n-  fldlg2();\n-  fxch();\n-  fyl2x();\n-}\n-\n-void Assembler::fmul(int i) {\n-  emit_farith(0xD8, 0xC8, i);\n-}\n-\n-void Assembler::fmul_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rcx, src, 0);\n-}\n-\n-void Assembler::fmul_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rcx, src, 0);\n-}\n-\n-void Assembler::fmula(int i) {\n-  emit_farith(0xDC, 0xC8, i);\n-}\n-\n-void Assembler::fmulp(int i) {\n-  emit_farith(0xDE, 0xC8, i);\n-}\n-\n-void Assembler::fnsave(Address dst) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDD);\n-  emit_operand32(rsi, dst, 0);\n-}\n-\n-void Assembler::fnstcw(Address src) {\n-  InstructionMark im(this);\n-  emit_int16((unsigned char)0x9B, (unsigned char)0xD9);\n-  emit_operand32(rdi, src, 0);\n-}\n-\n-void Assembler::fprem1() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xF5);\n-}\n-\n-void Assembler::frstor(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDD);\n-  emit_operand32(rsp, src, 0);\n-}\n-\n-void Assembler::fsin() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xFE);\n-}\n-\n-void Assembler::fsqrt() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xFA);\n-}\n-\n-void Assembler::fst_d(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDD);\n-  emit_operand32(rdx, adr, 0);\n-}\n-\n-void Assembler::fst_s(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD9);\n-  emit_operand32(rdx, adr, 0);\n-}\n-\n-void Assembler::fstp_s(Address adr) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD9);\n-  emit_operand32(rbx, adr, 0);\n-}\n-\n-void Assembler::fsub(int i) {\n-  emit_farith(0xD8, 0xE0, i);\n-}\n-\n-void Assembler::fsub_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rsp, src, 0);\n-}\n-\n-void Assembler::fsub_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rsp, src, 0);\n-}\n-\n-void Assembler::fsuba(int i) {\n-  emit_farith(0xDC, 0xE8, i);\n-}\n-\n-void Assembler::fsubp(int i) {\n-  emit_farith(0xDE, 0xE8, i);                    \/\/ ST(0) <- ST(0) - ST(1) and pop (Intel manual wrong)\n-}\n-\n-void Assembler::fsubr(int i) {\n-  emit_farith(0xD8, 0xE8, i);\n-}\n-\n-void Assembler::fsubr_d(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xDC);\n-  emit_operand32(rbp, src, 0);\n-}\n-\n-void Assembler::fsubr_s(Address src) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xD8);\n-  emit_operand32(rbp, src, 0);\n-}\n-\n-void Assembler::fsubra(int i) {\n-  emit_farith(0xDC, 0xE0, i);\n-}\n-\n-void Assembler::fsubrp(int i) {\n-  emit_farith(0xDE, 0xE0, i);                    \/\/ ST(0) <- ST(1) - ST(0) and pop (Intel manual wrong)\n-}\n-\n-void Assembler::ftan() {\n-  emit_int32((unsigned char)0xD9, (unsigned char)0xF2, (unsigned char)0xDD, (unsigned char)0xD8);\n-}\n-\n-void Assembler::ftst() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xE4);\n-}\n-\n-void Assembler::fucomi(int i) {\n-  \/\/ make sure the instruction is supported (introduced for P6, together with cmov)\n-  guarantee(VM_Version::supports_cmov(), \"illegal instruction\");\n-  emit_farith(0xDB, 0xE8, i);\n-}\n-\n-void Assembler::fucomip(int i) {\n-  \/\/ make sure the instruction is supported (introduced for P6, together with cmov)\n-  guarantee(VM_Version::supports_cmov(), \"illegal instruction\");\n-  emit_farith(0xDF, 0xE8, i);\n-}\n-\n-void Assembler::fwait() {\n-  emit_int8((unsigned char)0x9B);\n-}\n-\n-void Assembler::fxch(int i) {\n-  emit_farith(0xD9, 0xC8, i);\n-}\n-\n-void Assembler::fyl2x() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xF1);\n-}\n-\n-void Assembler::frndint() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xFC);\n-}\n-\n-void Assembler::f2xm1() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xF0);\n-}\n-\n-void Assembler::fldl2e() {\n-  emit_int16((unsigned char)0xD9, (unsigned char)0xEA);\n-}\n-#endif \/\/ !_LP64\n-\n@@ -13459,1 +12771,1 @@\n-    if (UseAVX > 2 && !attributes->is_evex_instruction() && !is_managed()) {\n+    if (UseAVX > 2 && !attributes->is_evex_instruction()) {\n@@ -13474,1 +12786,0 @@\n-  clear_managed();\n@@ -13522,1 +12833,1 @@\n-    if (UseAVX > 2 && !attributes->is_evex_instruction() && !is_managed()) {\n+    if (UseAVX > 2 && !attributes->is_evex_instruction()) {\n@@ -13544,1 +12855,0 @@\n-  clear_managed();\n@@ -14414,49 +13724,0 @@\n-#ifndef _LP64\n-\n-void Assembler::incl(Register dst) {\n-  \/\/ Don't use it directly. Use MacroAssembler::incrementl() instead.\n-  emit_int8(0x40 | dst->encoding());\n-}\n-\n-void Assembler::eincl(Register dst, Register src, bool no_flags) {\n-  InstructionAttr attributes(AVX_128bit, \/* vex_w *\/ false, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ false);\n-  (void) evex_prefix_and_encode_ndd(0, dst->encoding(), src->encoding(), VEX_SIMD_NONE, VEX_OPCODE_0F_3C, &attributes, no_flags);\n-  emit_int8(0x40 | src->encoding());\n-}\n-\n-void Assembler::lea(Register dst, Address src) {\n-  leal(dst, src);\n-}\n-\n-void Assembler::mov_literal32(Address dst, int32_t imm32, RelocationHolder const& rspec) {\n-  InstructionMark im(this);\n-  emit_int8((unsigned char)0xC7);\n-  emit_operand(rax, dst, 4);\n-  emit_data((int)imm32, rspec, 0);\n-}\n-\n-void Assembler::mov_literal32(Register dst, int32_t imm32, RelocationHolder const& rspec) {\n-  InstructionMark im(this);\n-  int encode = prefix_and_encode(dst->encoding());\n-  emit_int8((0xB8 | encode));\n-  emit_data((int)imm32, rspec, 0);\n-}\n-\n-void Assembler::popa() { \/\/ 32bit\n-  emit_int8(0x61);\n-}\n-\n-void Assembler::push_literal32(int32_t imm32, RelocationHolder const& rspec) {\n-  InstructionMark im(this);\n-  emit_int8(0x68);\n-  emit_data(imm32, rspec, 0);\n-}\n-\n-void Assembler::pusha() { \/\/ 32bit\n-  emit_int8(0x60);\n-}\n-\n-#else \/\/ LP64\n-\n-\/\/ 64bit only pieces of the assembler\n-\n@@ -15416,1 +14677,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -15423,1 +14683,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -15433,1 +14692,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -15443,1 +14701,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -15454,1 +14711,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -15461,1 +14717,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -15468,1 +14723,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse(), \"\"));\n@@ -15815,1 +15069,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -15823,1 +15076,0 @@\n-  NOT_LP64(assert(VM_Version::supports_sse2(), \"\"));\n@@ -16426,1 +15678,0 @@\n-#ifdef _LP64\n@@ -16581,1 +15832,0 @@\n-#endif\n@@ -16926,2 +16176,0 @@\n-#endif \/\/ !LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":20,"deletions":772,"binary":false,"changes":792,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-#ifdef _LP64\n@@ -52,5 +51,0 @@\n-#else\n-    n_register_parameters = 0,   \/\/ 0 registers used to pass arguments\n-    n_int_register_parameters_j   = 0,\n-    n_float_register_parameters_j = 0\n-#endif \/\/ _LP64\n@@ -61,1 +55,0 @@\n-#ifdef _LP64\n@@ -141,8 +134,0 @@\n-#else\n-\/\/ rscratch1 will appear in 32bit code that is dead but of course must compile\n-\/\/ Using noreg ensures if the dead code is incorrectly live and executed it\n-\/\/ will cause an assertion failure\n-#define rscratch1 noreg\n-#define rscratch2 noreg\n-\n-#endif \/\/ _LP64\n@@ -171,1 +156,1 @@\n-    times_ptr = LP64_ONLY(times_8) NOT_LP64(times_4)\n+    times_ptr = times_8\n@@ -200,1 +185,0 @@\n-  NOT_LP64(Address(address loc, RelocationHolder spec);)\n@@ -459,1 +443,1 @@\n-const int FPUStateSizeInWords = NOT_LP64(27) LP64_ONLY(2688 \/ wordSize);\n+const int FPUStateSizeInWords = 2688 \/ wordSize;\n@@ -629,4 +613,1 @@\n-#ifndef _LP64\n-    _WhichOperand_limit = 3\n-#else\n-     narrow_oop_operand = 3,     \/\/ embedded 32-bit immediate narrow oop\n+    narrow_oop_operand = 3,      \/\/ embedded 32-bit immediate narrow oop\n@@ -634,1 +615,0 @@\n-#endif\n@@ -722,1 +702,0 @@\n-  NOT_LP64(bool _is_managed;)\n@@ -907,2 +886,2 @@\n-  bool always_reachable(AddressLiteral adr) NOT_LP64( { return true; } );\n-  bool        reachable(AddressLiteral adr) NOT_LP64( { return true; } );\n+  bool always_reachable(AddressLiteral adr);\n+  bool        reachable(AddressLiteral adr);\n@@ -914,12 +893,0 @@\n-  \/\/ 32BIT ONLY SECTION\n-#ifndef _LP64\n-  \/\/ Make these disappear in 64bit mode since they would never be correct\n-  void cmp_literal32(Register src1, int32_t imm32, RelocationHolder const& rspec);   \/\/ 32BIT ONLY\n-  void cmp_literal32(Address src1, int32_t imm32, RelocationHolder const& rspec);    \/\/ 32BIT ONLY\n-\n-  void mov_literal32(Register dst, int32_t imm32, RelocationHolder const& rspec);    \/\/ 32BIT ONLY\n-  void mov_literal32(Address dst, int32_t imm32, RelocationHolder const& rspec);     \/\/ 32BIT ONLY\n-\n-  void push_literal32(int32_t imm32, RelocationHolder const& rspec);                 \/\/ 32BIT ONLY\n-#else\n-  \/\/ 64BIT ONLY SECTION\n@@ -933,1 +900,0 @@\n-#endif \/\/ _LP64\n@@ -1011,2 +977,0 @@\n-  \/\/ Does 32bit or 64bit as needed for the platform. In some sense these\n-  \/\/ belong in macro assembler but there is no need for both varieties to exist\n@@ -1017,6 +981,0 @@\n-  void set_managed(void) { NOT_LP64(_is_managed = true;) }\n-  void clear_managed(void) { NOT_LP64(_is_managed = false;) }\n-  bool is_managed(void) {\n-    NOT_LP64(return _is_managed;)\n-    LP64_ONLY(return false;) }\n-\n@@ -1027,1 +985,0 @@\n-#ifdef _LP64\n@@ -1047,1 +1004,0 @@\n-#endif\n@@ -1069,1 +1025,0 @@\n-#ifdef _LP64\n@@ -1071,1 +1026,0 @@\n-#endif\n@@ -1121,2 +1075,1 @@\n-#ifdef _LP64\n- \/\/Add Unsigned Integers with Carry Flag\n+  \/\/ Add Unsigned Integers with Carry Flag\n@@ -1126,1 +1079,1 @@\n- \/\/Add Unsigned Integers with Overflow Flag\n+  \/\/ Add Unsigned Integers with Overflow Flag\n@@ -1129,1 +1082,0 @@\n-#endif\n@@ -1206,1 +1158,0 @@\n-#ifdef _LP64\n@@ -1209,1 +1160,0 @@\n-#endif\n@@ -1395,132 +1345,0 @@\n-#ifndef _LP64\n-  void emms();\n-\n-  void fabs();\n-\n-  void fadd(int i);\n-\n-  void fadd_d(Address src);\n-  void fadd_s(Address src);\n-\n-  \/\/ \"Alternate\" versions of x87 instructions place result down in FPU\n-  \/\/ stack instead of on TOS\n-\n-  void fadda(int i); \/\/ \"alternate\" fadd\n-  void faddp(int i = 1);\n-\n-  void fchs();\n-\n-  void fcom(int i);\n-\n-  void fcomp(int i = 1);\n-  void fcomp_d(Address src);\n-  void fcomp_s(Address src);\n-\n-  void fcompp();\n-\n-  void fcos();\n-\n-  void fdecstp();\n-\n-  void fdiv(int i);\n-  void fdiv_d(Address src);\n-  void fdivr_s(Address src);\n-  void fdiva(int i);  \/\/ \"alternate\" fdiv\n-  void fdivp(int i = 1);\n-\n-  void fdivr(int i);\n-  void fdivr_d(Address src);\n-  void fdiv_s(Address src);\n-\n-  void fdivra(int i); \/\/ \"alternate\" reversed fdiv\n-\n-  void fdivrp(int i = 1);\n-\n-  void ffree(int i = 0);\n-\n-  void fild_d(Address adr);\n-  void fild_s(Address adr);\n-\n-  void fincstp();\n-\n-  void finit();\n-\n-  void fist_s (Address adr);\n-  void fistp_d(Address adr);\n-  void fistp_s(Address adr);\n-\n-  void fld1();\n-\n-  void fld_s(Address adr);\n-  void fld_s(int index);\n-\n-  void fldcw(Address src);\n-\n-  void fldenv(Address src);\n-\n-  void fldlg2();\n-\n-  void fldln2();\n-\n-  void fldz();\n-\n-  void flog();\n-  void flog10();\n-\n-  void fmul(int i);\n-\n-  void fmul_d(Address src);\n-  void fmul_s(Address src);\n-\n-  void fmula(int i);  \/\/ \"alternate\" fmul\n-\n-  void fmulp(int i = 1);\n-\n-  void fnsave(Address dst);\n-\n-  void fnstcw(Address src);\n-  void fprem1();\n-\n-  void frstor(Address src);\n-\n-  void fsin();\n-\n-  void fsqrt();\n-\n-  void fst_d(Address adr);\n-  void fst_s(Address adr);\n-\n-  void fstp_s(Address adr);\n-\n-  void fsub(int i);\n-  void fsub_d(Address src);\n-  void fsub_s(Address src);\n-\n-  void fsuba(int i);  \/\/ \"alternate\" fsub\n-\n-  void fsubp(int i = 1);\n-\n-  void fsubr(int i);\n-  void fsubr_d(Address src);\n-  void fsubr_s(Address src);\n-\n-  void fsubra(int i); \/\/ \"alternate\" reversed fsub\n-\n-  void fsubrp(int i = 1);\n-\n-  void ftan();\n-\n-  void ftst();\n-\n-  void fucomi(int i = 1);\n-  void fucomip(int i = 1);\n-\n-  void fwait();\n-\n-  void fxch(int i = 1);\n-\n-  void fyl2x();\n-  void frndint();\n-  void f2xm1();\n-  void fldl2e();\n-#endif \/\/ !_LP64\n@@ -1546,1 +1364,0 @@\n-#ifdef _LP64\n@@ -1551,1 +1368,0 @@\n-#endif\n@@ -1564,1 +1380,0 @@\n-#ifdef _LP64\n@@ -1577,1 +1392,0 @@\n-#endif\n@@ -1629,1 +1443,0 @@\n-#ifdef _LP64\n@@ -1631,1 +1444,0 @@\n-#endif\n@@ -1643,1 +1455,0 @@\n-#ifdef _LP64\n@@ -1648,1 +1459,0 @@\n-#endif\n@@ -1800,1 +1610,0 @@\n-#ifdef _LP64\n@@ -1806,1 +1615,0 @@\n-#endif\n@@ -1818,1 +1626,0 @@\n-#ifdef _LP64\n@@ -1827,1 +1634,0 @@\n-#endif\n@@ -1832,1 +1638,0 @@\n-#ifdef _LP64\n@@ -1835,1 +1640,0 @@\n-#endif\n@@ -1849,1 +1653,0 @@\n-#ifdef _LP64\n@@ -1852,1 +1655,0 @@\n-#endif\n@@ -1857,1 +1659,0 @@\n-#ifdef _LP64\n@@ -1860,1 +1661,0 @@\n-#endif\n@@ -1868,1 +1668,0 @@\n-#ifdef _LP64\n@@ -1874,1 +1673,0 @@\n-#endif\n@@ -1889,1 +1687,0 @@\n-#ifdef _LP64\n@@ -1894,1 +1691,0 @@\n-#endif\n@@ -1901,1 +1697,0 @@\n-#ifdef _LP64\n@@ -1908,1 +1703,0 @@\n-#endif\n@@ -2127,5 +1921,0 @@\n-#ifndef _LP64 \/\/ no 32bit push\/pop on amd64\n-  void popl(Address dst);\n-#endif\n-\n-#ifdef _LP64\n@@ -2134,1 +1923,0 @@\n-#endif\n@@ -2146,1 +1934,0 @@\n-#ifdef _LP64\n@@ -2151,1 +1938,0 @@\n-#endif\n@@ -2243,4 +2029,0 @@\n-#ifndef _LP64 \/\/ no 32bit push\/pop on amd64\n-  void pushl(Address src);\n-#endif\n-\n@@ -2278,1 +2060,0 @@\n-#ifdef _LP64\n@@ -2291,1 +2072,0 @@\n-#endif\n@@ -2313,1 +2093,0 @@\n-#ifdef _LP64\n@@ -2331,1 +2110,0 @@\n-#endif\n@@ -2372,1 +2150,0 @@\n-#ifdef _LP64\n@@ -2377,1 +2154,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":7,"deletions":231,"binary":false,"changes":238,"status":"modified"},{"patch":"@@ -28,53 +28,1 @@\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"asm\/codeBuffer.hpp\"\n-#include \"code\/codeCache.hpp\"\n-\n-#ifndef _LP64\n-inline int Assembler::prefix_and_encode(int reg_enc, bool byteinst, bool is_map1)\n-{\n-    int opc_prefix = is_map1 ? 0x0F00 : 0;\n-    return opc_prefix | reg_enc;\n-}\n-\n-inline int Assembler::prefixq_and_encode(int reg_enc, bool is_map1) {\n-    int opc_prefix = is_map1 ? 0xF00 : 0;\n-    return opc_prefix | reg_enc;\n-}\n-\n-inline int Assembler::prefix_and_encode(int dst_enc, bool dst_is_byte, int src_enc, bool src_is_byte, bool is_map1) {\n-    int opc_prefix = is_map1 ? 0xF00 : 0;\n-    return opc_prefix | (dst_enc << 3 | src_enc);\n-}\n-\n-inline int Assembler::prefixq_and_encode(int dst_enc, int src_enc, bool is_map1) {\n-    int opc_prefix = is_map1 ? 0xF00 : 0;\n-    return opc_prefix | dst_enc << 3 | src_enc;\n-}\n-\n-inline void Assembler::prefix(Register reg) {}\n-inline void Assembler::prefix(Register dst, Register src, Prefix p) {}\n-inline void Assembler::prefix(Register dst, Address adr, Prefix p) {}\n-\n-inline void Assembler::prefix(Address adr, bool is_map1) {\n-    if (is_map1) {\n-        emit_int8(0x0F);\n-    }\n-}\n-\n-inline void Assembler::prefixq(Address adr) {}\n-\n-inline void Assembler::prefix(Address adr, Register reg,  bool byteinst, bool is_map1) {\n-    if (is_map1) {\n-        emit_int8(0x0F);\n-    }\n-}\n-inline void Assembler::prefixq(Address adr, Register reg, bool is_map1) {\n-    if (is_map1) {\n-        emit_int8(0x0F);\n-    }\n-}\n-\n-inline void Assembler::prefix(Address adr, XMMRegister reg) {}\n-inline void Assembler::prefixq(Address adr, XMMRegister reg) {}\n-\n-#endif \/\/ _LP64\n+\/\/ Nothing is here. Left to allow CPU_HEADER_INLINE macro to work.\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.inline.hpp","additions":1,"deletions":53,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -41,41 +41,0 @@\n-#ifndef _LP64\n-float ConversionStub::float_zero = 0.0;\n-double ConversionStub::double_zero = 0.0;\n-\n-void ConversionStub::emit_code(LIR_Assembler* ce) {\n-  __ bind(_entry);\n-  assert(bytecode() == Bytecodes::_f2i || bytecode() == Bytecodes::_d2i, \"other conversions do not require stub\");\n-\n-\n-  if (input()->is_single_xmm()) {\n-    __ comiss(input()->as_xmm_float_reg(),\n-              ExternalAddress((address)&float_zero));\n-  } else if (input()->is_double_xmm()) {\n-    __ comisd(input()->as_xmm_double_reg(),\n-              ExternalAddress((address)&double_zero));\n-  } else {\n-    __ push(rax);\n-    __ ftst();\n-    __ fnstsw_ax();\n-    __ sahf();\n-    __ pop(rax);\n-  }\n-\n-  Label NaN, do_return;\n-  __ jccb(Assembler::parity, NaN);\n-  __ jccb(Assembler::below, do_return);\n-\n-  \/\/ input is > 0 -> return maxInt\n-  \/\/ result register already contains 0x80000000, so subtracting 1 gives 0x7fffffff\n-  __ decrement(result()->as_register());\n-  __ jmpb(do_return);\n-\n-  \/\/ input is NaN -> return 0\n-  __ bind(NaN);\n-  __ xorptr(result()->as_register(), result()->as_register());\n-\n-  __ bind(do_return);\n-  __ jmp(_continuation);\n-}\n-#endif \/\/ !_LP64\n-\n@@ -85,1 +44,0 @@\n-#ifdef _LP64\n@@ -88,13 +46,0 @@\n-#else\n-  const Register tmp1 = rcx;\n-  const Register tmp2 = rdx;\n-  __ push(tmp1);\n-  __ push(tmp2);\n-\n-  __ lea(tmp1, safepoint_pc);\n-  __ get_thread(tmp2);\n-  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n-\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif \/* _LP64 *\/\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":0,"deletions":55,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  pd_strict_fp_requires_explicit_rounding = LP64_ONLY( false ) NOT_LP64 ( true )\n+  pd_strict_fp_requires_explicit_rounding = false\n@@ -42,1 +42,1 @@\n-  pd_nof_cpu_regs_frame_map = NOT_LP64(8) LP64_ONLY(16),           \/\/ number of registers used during code emission\n+  pd_nof_cpu_regs_frame_map = 16,                                  \/\/ number of registers used during code emission\n@@ -46,1 +46,0 @@\n-#ifdef _LP64\n@@ -48,3 +47,0 @@\n-#else\n-  #define UNALLOCATED 2    \/\/ rsp, rbp\n-#endif \/\/ LP64\n@@ -63,3 +59,3 @@\n-  pd_last_cpu_reg = NOT_LP64(5) LP64_ONLY(11),\n-  pd_first_byte_reg = NOT_LP64(2) LP64_ONLY(0),\n-  pd_last_byte_reg = NOT_LP64(5) LP64_ONLY(11),\n+  pd_last_cpu_reg = 11,\n+  pd_first_byte_reg = 0,\n+  pd_last_byte_reg = 11,\n","filename":"src\/hotspot\/cpu\/x86\/c1_Defs_x86.hpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1,201 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2017, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_FpuStackSim.hpp\"\n-#include \"c1\/c1_FrameMap.hpp\"\n-#include \"utilities\/growableArray.hpp\"\n-#include \"utilities\/ostream.hpp\"\n-\n-\/\/--------------------------------------------------------\n-\/\/               FpuStackSim\n-\/\/--------------------------------------------------------\n-\n-\/\/ This class maps the FPU registers to their stack locations; it computes\n-\/\/ the offsets between individual registers and simulates the FPU stack.\n-\n-const int EMPTY = -1;\n-\n-int FpuStackSim::regs_at(int i) const {\n-  assert(i >= 0 && i < FrameMap::nof_fpu_regs, \"out of bounds\");\n-  return _regs[i];\n-}\n-\n-void FpuStackSim::set_regs_at(int i, int val) {\n-  assert(i >= 0 && i < FrameMap::nof_fpu_regs, \"out of bounds\");\n-  _regs[i] = val;\n-}\n-\n-void FpuStackSim::dec_stack_size() {\n-  _stack_size--;\n-  assert(_stack_size >= 0, \"FPU stack underflow\");\n-}\n-\n-void FpuStackSim::inc_stack_size() {\n-  _stack_size++;\n-  assert(_stack_size <= FrameMap::nof_fpu_regs, \"FPU stack overflow\");\n-}\n-\n-FpuStackSim::FpuStackSim(Compilation* compilation)\n- : _compilation(compilation)\n-{\n-  _stack_size = 0;\n-  for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n-    set_regs_at(i, EMPTY);\n-  }\n-}\n-\n-\n-void FpuStackSim::pop() {\n-  if (TraceFPUStack) { tty->print(\"FPU-pop \"); print(); tty->cr(); }\n-  set_regs_at(tos_index(), EMPTY);\n-  dec_stack_size();\n-}\n-\n-void FpuStackSim::pop(int rnr) {\n-  if (TraceFPUStack) { tty->print(\"FPU-pop %d\", rnr); print(); tty->cr(); }\n-  assert(regs_at(tos_index()) == rnr, \"rnr is not on TOS\");\n-  set_regs_at(tos_index(), EMPTY);\n-  dec_stack_size();\n-}\n-\n-\n-void FpuStackSim::push(int rnr) {\n-  if (TraceFPUStack) { tty->print(\"FPU-push %d\", rnr); print(); tty->cr(); }\n-  assert(regs_at(stack_size()) == EMPTY, \"should be empty\");\n-  set_regs_at(stack_size(), rnr);\n-  inc_stack_size();\n-}\n-\n-\n-void FpuStackSim::swap(int offset) {\n-  if (TraceFPUStack) { tty->print(\"FPU-swap %d\", offset); print(); tty->cr(); }\n-  int t = regs_at(tos_index() - offset);\n-  set_regs_at(tos_index() - offset, regs_at(tos_index()));\n-  set_regs_at(tos_index(), t);\n-}\n-\n-\n-int FpuStackSim::offset_from_tos(int rnr) const {\n-  for (int i = tos_index(); i >= 0; i--) {\n-    if (regs_at(i) == rnr) {\n-      return tos_index() - i;\n-    }\n-  }\n-  assert(false, \"FpuStackSim: register not found\");\n-  BAILOUT_(\"FpuStackSim: register not found\", 0);\n-}\n-\n-\n-int FpuStackSim::get_slot(int tos_offset) const {\n-  return regs_at(tos_index() - tos_offset);\n-}\n-\n-void FpuStackSim::set_slot(int tos_offset, int rnr) {\n-  set_regs_at(tos_index() - tos_offset, rnr);\n-}\n-\n-void FpuStackSim::rename(int old_rnr, int new_rnr) {\n-  if (TraceFPUStack) { tty->print(\"FPU-rename %d %d\", old_rnr, new_rnr); print(); tty->cr(); }\n-  if (old_rnr == new_rnr)\n-    return;\n-  bool found = false;\n-  for (int i = 0; i < stack_size(); i++) {\n-    assert(regs_at(i) != new_rnr, \"should not see old occurrences of new_rnr on the stack\");\n-    if (regs_at(i) == old_rnr) {\n-      set_regs_at(i, new_rnr);\n-      found = true;\n-    }\n-  }\n-  assert(found, \"should have found at least one instance of old_rnr\");\n-}\n-\n-\n-bool FpuStackSim::contains(int rnr) {\n-  for (int i = 0; i < stack_size(); i++) {\n-    if (regs_at(i) == rnr) {\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-bool FpuStackSim::is_empty() {\n-#ifdef ASSERT\n-  if (stack_size() == 0) {\n-    for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n-      assert(regs_at(i) == EMPTY, \"must be empty\");\n-    }\n-  }\n-#endif\n-  return stack_size() == 0;\n-}\n-\n-\n-bool FpuStackSim::slot_is_empty(int tos_offset) {\n-  return (regs_at(tos_index() - tos_offset) == EMPTY);\n-}\n-\n-\n-void FpuStackSim::clear() {\n-  if (TraceFPUStack) { tty->print(\"FPU-clear\"); print(); tty->cr(); }\n-  for (int i = tos_index(); i >= 0; i--) {\n-    set_regs_at(i, EMPTY);\n-  }\n-  _stack_size = 0;\n-}\n-\n-\n-intArray* FpuStackSim::write_state() {\n-  intArray* res = new intArray(1 + FrameMap::nof_fpu_regs);\n-  res->append(stack_size());\n-  for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n-    res->append(regs_at(i));\n-  }\n-  return res;\n-}\n-\n-\n-void FpuStackSim::read_state(intArray* fpu_stack_state) {\n-  _stack_size = fpu_stack_state->at(0);\n-  for (int i = 0; i < FrameMap::nof_fpu_regs; i++) {\n-    set_regs_at(i, fpu_stack_state->at(1 + i));\n-  }\n-}\n-\n-\n-#ifndef PRODUCT\n-void FpuStackSim::print() {\n-  tty->print(\" N=%d[\", stack_size());\\\n-  for (int i = 0; i < stack_size(); i++) {\n-    int reg = regs_at(i);\n-    if (reg != EMPTY) {\n-      tty->print(\"%d\", reg);\n-    } else {\n-      tty->print(\"_\");\n-    }\n-  };\n-  tty->print(\" ]\");\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_FpuStackSim_x86.cpp","additions":0,"deletions":201,"binary":false,"changes":201,"status":"deleted"},{"patch":"@@ -1,72 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_X86_C1_FPUSTACKSIM_X86_HPP\n-#define CPU_X86_C1_FPUSTACKSIM_X86_HPP\n-\n-\/\/  Simulates the FPU stack and maintains mapping [fpu-register -> stack offset]\n-\/\/  FPU registers are described as numbers from 0..nof_fpu_regs-1\n-\n-class Compilation;\n-\n-class FpuStackSim {\n- private:\n-  Compilation* _compilation;\n-  int          _stack_size;\n-  int          _regs[FrameMap::nof_fpu_regs];\n-\n-  int tos_index() const                        { return _stack_size - 1; }\n-\n-  int regs_at(int i) const;\n-  void set_regs_at(int i, int val);\n-  void dec_stack_size();\n-  void inc_stack_size();\n-\n-  \/\/ unified bailout support\n-  Compilation*  compilation() const              { return _compilation; }\n-  void          bailout(const char* msg) const   { compilation()->bailout(msg); }\n-  bool          bailed_out() const               { return compilation()->bailed_out(); }\n-\n- public:\n-  FpuStackSim(Compilation* compilation);\n-  void pop ();\n-  void pop (int rnr);                          \/\/ rnr must be on tos\n-  void push(int rnr);\n-  void swap(int offset);                       \/\/ exchange tos with tos + offset\n-  int offset_from_tos(int rnr) const;          \/\/ return the offset of the topmost instance of rnr from TOS\n-  int  get_slot(int tos_offset) const;         \/\/ return the entry at the given offset from TOS\n-  void set_slot(int tos_offset, int rnr);      \/\/ set the entry at the given offset from TOS\n-  void rename(int old_rnr, int new_rnr);       \/\/ rename all instances of old_rnr to new_rnr\n-  bool contains(int rnr);                      \/\/ debugging support only\n-  bool is_empty();\n-  bool slot_is_empty(int tos_offset);\n-  int stack_size() const                       { return _stack_size; }\n-  void clear();\n-  intArray* write_state();\n-  void read_state(intArray* fpu_stack_state);\n-\n-  void print() PRODUCT_RETURN;\n-};\n-\n-#endif \/\/ CPU_X86_C1_FPUSTACKSIM_X86_HPP\n","filename":"src\/hotspot\/cpu\/x86\/c1_FpuStackSim_x86.hpp","additions":0,"deletions":72,"binary":false,"changes":72,"status":"deleted"},{"patch":"@@ -47,1 +47,0 @@\n-#ifdef _LP64\n@@ -50,3 +49,0 @@\n-#else\n-      opr = as_long_opr(reg2, reg);\n-#endif \/\/ _LP64\n@@ -112,2 +108,0 @@\n-LIR_Opr FrameMap::fpu0_float_opr;\n-LIR_Opr FrameMap::fpu0_double_opr;\n@@ -117,2 +111,0 @@\n-#ifdef _LP64\n-\n@@ -143,1 +135,0 @@\n-#endif \/\/ _LP64\n@@ -163,1 +154,1 @@\n-  assert(nof_cpu_regs == LP64_ONLY(16) NOT_LP64(8), \"wrong number of CPU registers\");\n+  assert(nof_cpu_regs == 16, \"wrong number of CPU registers\");\n@@ -171,5 +162,0 @@\n-#ifndef _LP64\n-  \/\/ The unallocatable registers are at the end\n-  map_register(6, rsp);\n-  map_register(7, rbp);\n-#else\n@@ -189,1 +175,0 @@\n-#endif \/\/ _LP64\n@@ -191,1 +176,0 @@\n-#ifdef _LP64\n@@ -194,6 +178,1 @@\n-#else\n-  long0_opr = LIR_OprFact::double_cpu(3 \/*eax*\/, 4 \/*edx*\/);\n-  long1_opr = LIR_OprFact::double_cpu(2 \/*ebx*\/, 5 \/*ecx*\/);\n-#endif \/\/ _LP64\n-  fpu0_float_opr   = LIR_OprFact::single_fpu(0);\n-  fpu0_double_opr  = LIR_OprFact::double_fpu(0);\n+\n@@ -210,1 +189,0 @@\n-#ifdef _LP64\n@@ -217,2 +195,0 @@\n-#endif \/\/ _LP64\n-\n@@ -228,27 +204,24 @@\n-\n-#ifdef _LP64\n-  _xmm_regs[8]   = xmm8;\n-  _xmm_regs[9]   = xmm9;\n-  _xmm_regs[10]  = xmm10;\n-  _xmm_regs[11]  = xmm11;\n-  _xmm_regs[12]  = xmm12;\n-  _xmm_regs[13]  = xmm13;\n-  _xmm_regs[14]  = xmm14;\n-  _xmm_regs[15]  = xmm15;\n-  _xmm_regs[16]  = xmm16;\n-  _xmm_regs[17]  = xmm17;\n-  _xmm_regs[18]  = xmm18;\n-  _xmm_regs[19]  = xmm19;\n-  _xmm_regs[20]  = xmm20;\n-  _xmm_regs[21]  = xmm21;\n-  _xmm_regs[22]  = xmm22;\n-  _xmm_regs[23]  = xmm23;\n-  _xmm_regs[24]  = xmm24;\n-  _xmm_regs[25]  = xmm25;\n-  _xmm_regs[26]  = xmm26;\n-  _xmm_regs[27]  = xmm27;\n-  _xmm_regs[28]  = xmm28;\n-  _xmm_regs[29]  = xmm29;\n-  _xmm_regs[30]  = xmm30;\n-  _xmm_regs[31]  = xmm31;\n-#endif \/\/ _LP64\n+  _xmm_regs[8] = xmm8;\n+  _xmm_regs[9] = xmm9;\n+  _xmm_regs[10] = xmm10;\n+  _xmm_regs[11] = xmm11;\n+  _xmm_regs[12] = xmm12;\n+  _xmm_regs[13] = xmm13;\n+  _xmm_regs[14] = xmm14;\n+  _xmm_regs[15] = xmm15;\n+  _xmm_regs[16] = xmm16;\n+  _xmm_regs[17] = xmm17;\n+  _xmm_regs[18] = xmm18;\n+  _xmm_regs[19] = xmm19;\n+  _xmm_regs[20] = xmm20;\n+  _xmm_regs[21] = xmm21;\n+  _xmm_regs[22] = xmm22;\n+  _xmm_regs[23] = xmm23;\n+  _xmm_regs[24] = xmm24;\n+  _xmm_regs[25] = xmm25;\n+  _xmm_regs[26] = xmm26;\n+  _xmm_regs[27] = xmm27;\n+  _xmm_regs[28] = xmm28;\n+  _xmm_regs[29] = xmm29;\n+  _xmm_regs[30] = xmm30;\n+  _xmm_regs[31] = xmm31;\n@@ -284,1 +257,0 @@\n-#ifdef _LP64\n@@ -298,1 +270,0 @@\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/c1_FrameMap_x86.cpp","additions":26,"deletions":55,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -44,4 +44,0 @@\n-#ifndef _LP64\n-    frame_pad_in_bytes = 8,\n-    nof_reg_args = 2\n-#else\n@@ -50,1 +46,0 @@\n-#endif \/\/ _LP64\n@@ -84,2 +79,0 @@\n-#ifdef _LP64\n-\n@@ -111,2 +104,0 @@\n-#endif \/\/ _LP64\n-\n@@ -115,2 +106,0 @@\n-  static LIR_Opr fpu0_float_opr;\n-  static LIR_Opr fpu0_double_opr;\n@@ -120,1 +109,0 @@\n-#ifdef _LP64\n@@ -127,8 +115,0 @@\n-#else\n-  static LIR_Opr as_long_opr(Register r, Register r2) {\n-    return LIR_OprFact::double_cpu(cpu_reg2rnr(r), cpu_reg2rnr(r2));\n-  }\n-  static LIR_Opr as_pointer_opr(Register r) {\n-    return LIR_OprFact::single_cpu(cpu_reg2rnr(r));\n-  }\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/c1_FrameMap_x86.hpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -165,18 +165,0 @@\n-#ifndef _LP64\n-void LIR_Assembler::fpop() {\n-  __ fpop();\n-}\n-\n-void LIR_Assembler::fxch(int i) {\n-  __ fxch(i);\n-}\n-\n-void LIR_Assembler::fld(int i) {\n-  __ fld_s(i);\n-}\n-\n-void LIR_Assembler::ffree(int i) {\n-  __ ffree(i);\n-}\n-#endif \/\/ !_LP64\n-\n@@ -191,1 +173,0 @@\n-    NOT_LP64(__ push_reg(opr->as_register_hi()));\n@@ -347,2 +328,0 @@\n-  Register thread = LP64_ONLY( r15_thread ) NOT_LP64( noreg );\n-  assert(thread != noreg, \"x86_32 not implemented\");\n@@ -351,1 +330,1 @@\n-  __ clinit_barrier(klass, thread, &L_skip_barrier \/*L_fast_path*\/);\n+  __ clinit_barrier(klass, r15_thread, &L_skip_barrier \/*L_fast_path*\/);\n@@ -423,5 +402,3 @@\n-  Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n-  __ movptr(rax, Address(thread, JavaThread::exception_oop_offset()));\n-  __ movptr(Address(thread, JavaThread::exception_oop_offset()), NULL_WORD);\n-  __ movptr(Address(thread, JavaThread::exception_pc_offset()), NULL_WORD);\n+  __ movptr(rax, Address(r15_thread, JavaThread::exception_oop_offset()));\n+  __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()), NULL_WORD);\n+  __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), NULL_WORD);\n@@ -449,1 +426,0 @@\n-#ifdef _LP64\n@@ -452,5 +428,0 @@\n-#else\n-    __ get_thread(rax);\n-    __ movptr(Address(rsp, 0), rax);\n-    __ mov_metadata(Address(rsp, sizeof(void*)), method()->constant_encoding(), noreg);\n-#endif\n@@ -513,6 +484,0 @@\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = rbx;\n-  __ get_thread(thread);\n-#endif\n@@ -521,1 +486,1 @@\n-  __ safepoint_poll(*code_stub->entry(), thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n+  __ safepoint_poll(*code_stub->entry(), r15_thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n@@ -529,1 +494,0 @@\n-#ifdef _LP64\n@@ -532,6 +496,0 @@\n-#else\n-  assert(tmp->is_cpu_register(), \"needed\");\n-  const Register poll_addr = tmp->as_register();\n-  __ get_thread(poll_addr);\n-  __ movptr(poll_addr, Address(poll_addr, in_bytes(JavaThread::polling_page_offset())));\n-#endif\n@@ -543,1 +501,1 @@\n-  guarantee(pointer_delta(post_pc, pre_pc, 1) == 2 LP64_ONLY(+1), \"must be exact length\");\n+  guarantee(pointer_delta(post_pc, pre_pc, 1) == 3, \"must be exact length\");\n@@ -577,1 +535,0 @@\n-#ifdef _LP64\n@@ -579,4 +536,0 @@\n-#else\n-      __ movptr(dest->as_register_lo(), c->as_jint_lo());\n-      __ movptr(dest->as_register_hi(), c->as_jint_hi());\n-#endif \/\/ _LP64\n@@ -606,1 +559,1 @@\n-        if (LP64_ONLY(UseAVX <= 2 &&) c->is_zero_float()) {\n+        if (UseAVX <= 2 && c->is_zero_float()) {\n@@ -613,11 +566,0 @@\n-#ifndef _LP64\n-        assert(dest->is_single_fpu(), \"must be\");\n-        assert(dest->fpu_regnr() == 0, \"dest must be TOS\");\n-        if (c->is_zero_float()) {\n-          __ fldz();\n-        } else if (c->is_one_float()) {\n-          __ fld1();\n-        } else {\n-          __ fld_s (InternalAddress(float_constant(c->as_jfloat())));\n-        }\n-#else\n@@ -625,1 +567,0 @@\n-#endif \/\/ !_LP64\n@@ -632,1 +573,1 @@\n-        if (LP64_ONLY(UseAVX <= 2 &&) c->is_zero_double()) {\n+        if (UseAVX <= 2 && c->is_zero_double()) {\n@@ -639,11 +580,0 @@\n-#ifndef _LP64\n-        assert(dest->is_double_fpu(), \"must be\");\n-        assert(dest->fpu_regnrLo() == 0, \"dest must be TOS\");\n-        if (c->is_zero_double()) {\n-          __ fldz();\n-        } else if (c->is_one_double()) {\n-          __ fld1();\n-        } else {\n-          __ fld_d (InternalAddress(double_constant(c->as_jdouble())));\n-        }\n-#else\n@@ -651,1 +581,0 @@\n-#endif \/\/ !_LP64\n@@ -682,1 +611,0 @@\n-#ifdef _LP64\n@@ -687,6 +615,0 @@\n-#else\n-      __ movptr(frame_map()->address_for_slot(dest->double_stack_ix(),\n-                                              lo_word_offset_in_bytes), c->as_jint_lo_bits());\n-      __ movptr(frame_map()->address_for_slot(dest->double_stack_ix(),\n-                                              hi_word_offset_in_bytes), c->as_jint_hi_bits());\n-#endif \/\/ _LP64\n@@ -723,1 +645,0 @@\n-#ifdef _LP64\n@@ -727,3 +648,0 @@\n-#else\n-          __ movptr(as_Address(addr), NULL_WORD);\n-#endif\n@@ -736,1 +654,0 @@\n-#ifdef _LP64\n@@ -746,3 +663,0 @@\n-#else\n-          __ movoop(as_Address(addr), c->as_jobject(), noreg);\n-#endif\n@@ -755,1 +669,0 @@\n-#ifdef _LP64\n@@ -764,5 +677,0 @@\n-#else\n-      \/\/ Always reachable in 32bit so this doesn't produce useless move literal\n-      __ movptr(as_Address_hi(addr), c->as_jint_hi_bits());\n-      __ movptr(as_Address_lo(addr), c->as_jint_lo_bits());\n-#endif \/\/ _LP64\n@@ -797,1 +705,0 @@\n-#ifdef _LP64\n@@ -803,1 +710,0 @@\n-#endif\n@@ -811,1 +717,0 @@\n-#ifdef _LP64\n@@ -818,1 +723,0 @@\n-#endif\n@@ -824,1 +728,0 @@\n-#ifdef _LP64\n@@ -828,33 +731,0 @@\n-#else\n-    assert(f_lo != f_hi && t_lo != t_hi, \"invalid register allocation\");\n-\n-\n-    if (f_lo == t_hi && f_hi == t_lo) {\n-      swap_reg(f_lo, f_hi);\n-    } else if (f_hi == t_lo) {\n-      assert(f_lo != t_hi, \"overwriting register\");\n-      move_regs(f_hi, t_hi);\n-      move_regs(f_lo, t_lo);\n-    } else {\n-      assert(f_hi != t_lo, \"overwriting register\");\n-      move_regs(f_lo, t_lo);\n-      move_regs(f_hi, t_hi);\n-    }\n-#endif \/\/ LP64\n-\n-#ifndef _LP64\n-    \/\/ special moves from fpu-register to xmm-register\n-    \/\/ necessary for method results\n-  } else if (src->is_single_xmm() && !dest->is_single_xmm()) {\n-    __ movflt(Address(rsp, 0), src->as_xmm_float_reg());\n-    __ fld_s(Address(rsp, 0));\n-  } else if (src->is_double_xmm() && !dest->is_double_xmm()) {\n-    __ movdbl(Address(rsp, 0), src->as_xmm_double_reg());\n-    __ fld_d(Address(rsp, 0));\n-  } else if (dest->is_single_xmm() && !src->is_single_xmm()) {\n-    __ fstp_s(Address(rsp, 0));\n-    __ movflt(dest->as_xmm_float_reg(), Address(rsp, 0));\n-  } else if (dest->is_double_xmm() && !src->is_double_xmm()) {\n-    __ fstp_d(Address(rsp, 0));\n-    __ movdbl(dest->as_xmm_double_reg(), Address(rsp, 0));\n-#endif \/\/ !_LP64\n@@ -870,7 +740,0 @@\n-#ifndef _LP64\n-    \/\/ move between fpu-registers (no instruction necessary because of fpu-stack)\n-  } else if (dest->is_single_fpu() || dest->is_double_fpu()) {\n-    assert(src->is_single_fpu() || src->is_double_fpu(), \"must match\");\n-    assert(src->fpu() == dest->fpu(), \"currently should be nothing to do\");\n-#endif \/\/ !_LP64\n-\n@@ -901,1 +764,0 @@\n-    NOT_LP64(__ movptr (dstHI, src->as_register_hi()));\n@@ -911,14 +773,0 @@\n-#ifndef _LP64\n-  } else if (src->is_single_fpu()) {\n-    assert(src->fpu_regnr() == 0, \"argument must be on TOS\");\n-    Address dst_addr = frame_map()->address_for_slot(dest->single_stack_ix());\n-    if (pop_fpu_stack)     __ fstp_s (dst_addr);\n-    else                   __ fst_s  (dst_addr);\n-\n-  } else if (src->is_double_fpu()) {\n-    assert(src->fpu_regnrLo() == 0, \"argument must be on TOS\");\n-    Address dst_addr = frame_map()->address_for_slot(dest->double_stack_ix());\n-    if (pop_fpu_stack)     __ fstp_d (dst_addr);\n-    else                   __ fst_d  (dst_addr);\n-#endif \/\/ !_LP64\n-\n@@ -938,1 +786,0 @@\n-#ifdef _LP64\n@@ -946,1 +793,0 @@\n-#endif\n@@ -958,1 +804,0 @@\n-#ifdef _LP64\n@@ -961,10 +806,0 @@\n-#else\n-      if (src->is_single_xmm()) {\n-        __ movflt(as_Address(to_addr), src->as_xmm_float_reg());\n-      } else {\n-        assert(src->is_single_fpu(), \"must be\");\n-        assert(src->fpu_regnr() == 0, \"argument must be on TOS\");\n-        if (pop_fpu_stack)      __ fstp_s(as_Address(to_addr));\n-        else                    __ fst_s (as_Address(to_addr));\n-      }\n-#endif \/\/ _LP64\n@@ -975,1 +810,0 @@\n-#ifdef _LP64\n@@ -978,10 +812,0 @@\n-#else\n-      if (src->is_double_xmm()) {\n-        __ movdbl(as_Address(to_addr), src->as_xmm_double_reg());\n-      } else {\n-        assert(src->is_double_fpu(), \"must be\");\n-        assert(src->fpu_regnrLo() == 0, \"argument must be on TOS\");\n-        if (pop_fpu_stack)      __ fstp_d(as_Address(to_addr));\n-        else                    __ fst_d (as_Address(to_addr));\n-      }\n-#endif \/\/ _LP64\n@@ -1004,2 +828,1 @@\n-      LP64_ONLY(ShouldNotReachHere());\n-      __ movptr(as_Address(to_addr), src->as_register());\n+      ShouldNotReachHere();\n@@ -1017,1 +840,0 @@\n-#ifdef _LP64\n@@ -1019,27 +841,0 @@\n-#else\n-      Register base = to_addr->base()->as_register();\n-      Register index = noreg;\n-      if (to_addr->index()->is_register()) {\n-        index = to_addr->index()->as_register();\n-      }\n-      if (base == from_lo || index == from_lo) {\n-        assert(base != from_hi, \"can't be\");\n-        assert(index == noreg || (index != base && index != from_hi), \"can't handle this\");\n-        __ movl(as_Address_hi(to_addr), from_hi);\n-        if (patch != nullptr) {\n-          patching_epilog(patch, lir_patch_high, base, info);\n-          patch = new PatchingStub(_masm, PatchingStub::access_field_id);\n-          patch_code = lir_patch_low;\n-        }\n-        __ movl(as_Address_lo(to_addr), from_lo);\n-      } else {\n-        assert(index == noreg || (index != base && index != from_lo), \"can't handle this\");\n-        __ movl(as_Address_lo(to_addr), from_lo);\n-        if (patch != nullptr) {\n-          patching_epilog(patch, lir_patch_low, base, info);\n-          patch = new PatchingStub(_masm, PatchingStub::access_field_id);\n-          patch_code = lir_patch_high;\n-        }\n-        __ movl(as_Address_hi(to_addr), from_hi);\n-      }\n-#endif \/\/ _LP64\n@@ -1094,1 +889,0 @@\n-    NOT_LP64(__ movptr(dest->as_register_hi(), src_addr_HI));\n@@ -1104,12 +898,0 @@\n-#ifndef _LP64\n-  } else if (dest->is_single_fpu()) {\n-    assert(dest->fpu_regnr() == 0, \"dest must be TOS\");\n-    Address src_addr = frame_map()->address_for_slot(src->single_stack_ix());\n-    __ fld_s(src_addr);\n-\n-  } else if (dest->is_double_fpu()) {\n-    assert(dest->fpu_regnrLo() == 0, \"dest must be TOS\");\n-    Address src_addr = frame_map()->address_for_slot(src->double_stack_ix());\n-    __ fld_d(src_addr);\n-#endif \/\/ _LP64\n-\n@@ -1128,5 +910,0 @@\n-#ifndef _LP64\n-      __ pushl(frame_map()->address_for_slot(src ->single_stack_ix()));\n-      __ popl (frame_map()->address_for_slot(dest->single_stack_ix()));\n-#else\n-      \/\/no pushl on 64bits\n@@ -1135,1 +912,0 @@\n-#endif\n@@ -1139,1 +915,0 @@\n-#ifdef _LP64\n@@ -1142,8 +917,0 @@\n-#else\n-    __ pushl(frame_map()->address_for_slot(src ->double_stack_ix(), 0));\n-    \/\/ push and pop the part at src + wordSize, adding wordSize for the previous push\n-    __ pushl(frame_map()->address_for_slot(src ->double_stack_ix(), 2 * wordSize));\n-    __ popl (frame_map()->address_for_slot(dest->double_stack_ix(), 2 * wordSize));\n-    __ popl (frame_map()->address_for_slot(dest->double_stack_ix(), 0));\n-#endif \/\/ _LP64\n-\n@@ -1198,5 +965,0 @@\n-#ifndef _LP64\n-        assert(dest->is_single_fpu(), \"must be\");\n-        assert(dest->fpu_regnr() == 0, \"dest must be TOS\");\n-        __ fld_s(from_addr);\n-#else\n@@ -1204,1 +966,0 @@\n-#endif \/\/ !LP64\n@@ -1213,5 +974,0 @@\n-#ifndef _LP64\n-        assert(dest->is_double_fpu(), \"must be\");\n-        assert(dest->fpu_regnrLo() == 0, \"dest must be TOS\");\n-        __ fld_d(from_addr);\n-#else\n@@ -1219,1 +975,0 @@\n-#endif \/\/ !LP64\n@@ -1243,1 +998,0 @@\n-#ifdef _LP64\n@@ -1245,36 +999,0 @@\n-#else\n-      Register base = addr->base()->as_register();\n-      Register index = noreg;\n-      if (addr->index()->is_register()) {\n-        index = addr->index()->as_register();\n-      }\n-      if ((base == to_lo && index == to_hi) ||\n-          (base == to_hi && index == to_lo)) {\n-        \/\/ addresses with 2 registers are only formed as a result of\n-        \/\/ array access so this code will never have to deal with\n-        \/\/ patches or null checks.\n-        assert(info == nullptr && patch == nullptr, \"must be\");\n-        __ lea(to_hi, as_Address(addr));\n-        __ movl(to_lo, Address(to_hi, 0));\n-        __ movl(to_hi, Address(to_hi, BytesPerWord));\n-      } else if (base == to_lo || index == to_lo) {\n-        assert(base != to_hi, \"can't be\");\n-        assert(index == noreg || (index != base && index != to_hi), \"can't handle this\");\n-        __ movl(to_hi, as_Address_hi(addr));\n-        if (patch != nullptr) {\n-          patching_epilog(patch, lir_patch_high, base, info);\n-          patch = new PatchingStub(_masm, PatchingStub::access_field_id);\n-          patch_code = lir_patch_low;\n-        }\n-        __ movl(to_lo, as_Address_lo(addr));\n-      } else {\n-        assert(index == noreg || (index != base && index != to_lo), \"can't handle this\");\n-        __ movl(to_lo, as_Address_lo(addr));\n-        if (patch != nullptr) {\n-          patching_epilog(patch, lir_patch_low, base, info);\n-          patch = new PatchingStub(_masm, PatchingStub::access_field_id);\n-          patch_code = lir_patch_high;\n-        }\n-        __ movl(to_hi, as_Address_hi(addr));\n-      }\n-#endif \/\/ _LP64\n@@ -1330,1 +1048,0 @@\n-#ifdef _LP64\n@@ -1334,2 +1051,0 @@\n-#endif\n-\n@@ -1429,1 +1144,0 @@\n-#ifdef _LP64\n@@ -1431,5 +1145,0 @@\n-#else\n-      move_regs(src->as_register(), dest->as_register_lo());\n-      move_regs(src->as_register(), dest->as_register_hi());\n-      __ sarl(dest->as_register_hi(), 31);\n-#endif \/\/ LP64\n@@ -1439,1 +1148,0 @@\n-#ifdef _LP64\n@@ -1441,3 +1149,0 @@\n-#else\n-      move_regs(src->as_register_lo(), dest->as_register());\n-#endif\n@@ -1461,2 +1166,0 @@\n-\n-#ifdef _LP64\n@@ -1502,68 +1205,0 @@\n-#else\n-    case Bytecodes::_f2d:\n-    case Bytecodes::_d2f:\n-      if (dest->is_single_xmm()) {\n-        __ cvtsd2ss(dest->as_xmm_float_reg(), src->as_xmm_double_reg());\n-      } else if (dest->is_double_xmm()) {\n-        __ cvtss2sd(dest->as_xmm_double_reg(), src->as_xmm_float_reg());\n-      } else {\n-        assert(src->fpu() == dest->fpu(), \"register must be equal\");\n-        \/\/ do nothing (float result is rounded later through spilling)\n-      }\n-      break;\n-\n-    case Bytecodes::_i2f:\n-    case Bytecodes::_i2d:\n-      if (dest->is_single_xmm()) {\n-        __ cvtsi2ssl(dest->as_xmm_float_reg(), src->as_register());\n-      } else if (dest->is_double_xmm()) {\n-        __ cvtsi2sdl(dest->as_xmm_double_reg(), src->as_register());\n-      } else {\n-        assert(dest->fpu() == 0, \"result must be on TOS\");\n-        __ movl(Address(rsp, 0), src->as_register());\n-        __ fild_s(Address(rsp, 0));\n-      }\n-      break;\n-\n-    case Bytecodes::_l2f:\n-    case Bytecodes::_l2d:\n-      assert(!dest->is_xmm_register(), \"result in xmm register not supported (no SSE instruction present)\");\n-      assert(dest->fpu() == 0, \"result must be on TOS\");\n-      __ movptr(Address(rsp, 0),          src->as_register_lo());\n-      __ movl(Address(rsp, BytesPerWord), src->as_register_hi());\n-      __ fild_d(Address(rsp, 0));\n-      \/\/ float result is rounded later through spilling\n-      break;\n-\n-    case Bytecodes::_f2i:\n-    case Bytecodes::_d2i:\n-      if (src->is_single_xmm()) {\n-        __ cvttss2sil(dest->as_register(), src->as_xmm_float_reg());\n-      } else if (src->is_double_xmm()) {\n-        __ cvttsd2sil(dest->as_register(), src->as_xmm_double_reg());\n-      } else {\n-        assert(src->fpu() == 0, \"input must be on TOS\");\n-        __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_trunc()));\n-        __ fist_s(Address(rsp, 0));\n-        __ movl(dest->as_register(), Address(rsp, 0));\n-        __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-      }\n-      \/\/ IA32 conversion instructions do not match JLS for overflow, underflow and NaN -> fixup in stub\n-      assert(op->stub() != nullptr, \"stub required\");\n-      __ cmpl(dest->as_register(), 0x80000000);\n-      __ jcc(Assembler::equal, *op->stub()->entry());\n-      __ bind(*op->stub()->continuation());\n-      break;\n-\n-    case Bytecodes::_f2l:\n-    case Bytecodes::_d2l:\n-      assert(!src->is_xmm_register(), \"input in xmm register not supported (no SSE instruction present)\");\n-      assert(src->fpu() == 0, \"input must be on TOS\");\n-      assert(dest == FrameMap::long0_opr, \"runtime stub places result in these registers\");\n-\n-      \/\/ instruction sequence too long to inline it here\n-      {\n-        __ call(RuntimeAddress(Runtime1::entry_for(C1StubId::fpu2long_stub_id)));\n-      }\n-      break;\n-#endif \/\/ _LP64\n@@ -1596,1 +1231,1 @@\n-  LP64_ONLY( __ movslq(len, len); )\n+  __ movslq(len, len);\n@@ -1664,1 +1299,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register tmp_load_klass = rscratch1;\n@@ -1726,1 +1361,0 @@\n-#ifdef _LP64\n@@ -1728,1 +1362,0 @@\n-#endif \/\/ _LP64\n@@ -1735,1 +1368,0 @@\n-#ifdef _LP64\n@@ -1742,7 +1374,0 @@\n-#else\n-    if (k->is_loaded()) {\n-      __ cmpklass(Address(obj, oopDesc::klass_offset_in_bytes()), k->constant_encoding());\n-    } else {\n-      __ cmpptr(k_RInfo, Address(obj, oopDesc::klass_offset_in_bytes()));\n-    }\n-#endif\n@@ -1757,1 +1382,0 @@\n-#ifdef _LP64\n@@ -1759,3 +1383,0 @@\n-#else\n-      __ cmpklass(Address(klass_RInfo, k->super_check_offset()), k->constant_encoding());\n-#endif \/\/ _LP64\n@@ -1769,1 +1390,0 @@\n-#ifdef _LP64\n@@ -1771,3 +1391,0 @@\n-#else\n-        __ cmpklass(klass_RInfo, k->constant_encoding());\n-#endif \/\/ _LP64\n@@ -1777,1 +1394,0 @@\n-#ifdef _LP64\n@@ -1779,3 +1395,0 @@\n-#else\n-        __ pushklass(k->constant_encoding(), noreg);\n-#endif \/\/ _LP64\n@@ -1810,1 +1423,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register tmp_load_klass = rscratch1;\n@@ -1914,11 +1527,1 @@\n-  if (LP64_ONLY(false &&) op->code() == lir_cas_long) {\n-    assert(op->cmp_value()->as_register_lo() == rax, \"wrong register\");\n-    assert(op->cmp_value()->as_register_hi() == rdx, \"wrong register\");\n-    assert(op->new_value()->as_register_lo() == rbx, \"wrong register\");\n-    assert(op->new_value()->as_register_hi() == rcx, \"wrong register\");\n-    Register addr = op->addr()->as_register();\n-    __ lock();\n-    NOT_LP64(__ cmpxchg8(Address(addr, 0)));\n-\n-  } else if (op->code() == lir_cas_int || op->code() == lir_cas_obj ) {\n-    NOT_LP64(assert(op->addr()->is_single_cpu(), \"must be single\");)\n+  if (op->code() == lir_cas_int || op->code() == lir_cas_obj ) {\n@@ -1934,2 +1537,1 @@\n-    if ( op->code() == lir_cas_obj) {\n-#ifdef _LP64\n+    if (op->code() == lir_cas_obj) {\n@@ -1943,3 +1545,1 @@\n-      } else\n-#endif\n-      {\n+      } else {\n@@ -1954,1 +1554,0 @@\n-#ifdef _LP64\n@@ -1966,1 +1565,0 @@\n-#endif \/\/ _LP64\n@@ -2009,1 +1607,0 @@\n-      NOT_LP64(__ cmovptr(ncond, result->as_register_hi(), opr2->as_register_hi());)\n@@ -2014,1 +1611,0 @@\n-      NOT_LP64(__ cmovptr(ncond, result->as_register_hi(), frame_map()->address_for_slot(opr2->double_stack_ix(), hi_word_offset_in_bytes));)\n@@ -2090,2 +1686,1 @@\n-      NOT_LP64(assert_different_registers(lreg_lo, lreg_hi, rreg_lo, rreg_hi));\n-      LP64_ONLY(assert_different_registers(lreg_lo, rreg_lo));\n+      assert_different_registers(lreg_lo, rreg_lo);\n@@ -2095,1 +1690,0 @@\n-          NOT_LP64(__ adcl(lreg_hi, rreg_hi));\n@@ -2099,1 +1693,0 @@\n-          NOT_LP64(__ sbbl(lreg_hi, rreg_hi));\n@@ -2102,1 +1695,0 @@\n-#ifdef _LP64\n@@ -2104,8 +1696,0 @@\n-#else\n-          assert(lreg_lo == rax && lreg_hi == rdx, \"must be\");\n-          __ imull(lreg_hi, rreg_lo);\n-          __ imull(rreg_hi, lreg_lo);\n-          __ addl (rreg_hi, lreg_hi);\n-          __ mull (rreg_lo);\n-          __ addl (lreg_hi, rreg_hi);\n-#endif \/\/ _LP64\n@@ -2119,1 +1703,0 @@\n-#ifdef _LP64\n@@ -2132,17 +1715,0 @@\n-#else\n-      jint c_lo = right->as_constant_ptr()->as_jint_lo();\n-      jint c_hi = right->as_constant_ptr()->as_jint_hi();\n-      switch (code) {\n-        case lir_add:\n-          __ addptr(lreg_lo, c_lo);\n-          __ adcl(lreg_hi, c_hi);\n-          break;\n-        case lir_sub:\n-          __ subptr(lreg_lo, c_lo);\n-          __ sbbl(lreg_hi, c_hi);\n-          break;\n-        default:\n-          ShouldNotReachHere();\n-      }\n-#endif \/\/ _LP64\n-\n@@ -2217,74 +1783,0 @@\n-#ifndef _LP64\n-  } else if (left->is_single_fpu()) {\n-    assert(dest->is_single_fpu(),  \"fpu stack allocation required\");\n-\n-    if (right->is_single_fpu()) {\n-      arith_fpu_implementation(code, left->fpu_regnr(), right->fpu_regnr(), dest->fpu_regnr(), pop_fpu_stack);\n-\n-    } else {\n-      assert(left->fpu_regnr() == 0, \"left must be on TOS\");\n-      assert(dest->fpu_regnr() == 0, \"dest must be on TOS\");\n-\n-      Address raddr;\n-      if (right->is_single_stack()) {\n-        raddr = frame_map()->address_for_slot(right->single_stack_ix());\n-      } else if (right->is_constant()) {\n-        address const_addr = float_constant(right->as_jfloat());\n-        assert(const_addr != nullptr, \"incorrect float\/double constant maintenance\");\n-        \/\/ hack for now\n-        raddr = __ as_Address(InternalAddress(const_addr));\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-\n-      switch (code) {\n-        case lir_add: __ fadd_s(raddr); break;\n-        case lir_sub: __ fsub_s(raddr); break;\n-        case lir_mul: __ fmul_s(raddr); break;\n-        case lir_div: __ fdiv_s(raddr); break;\n-        default:      ShouldNotReachHere();\n-      }\n-    }\n-\n-  } else if (left->is_double_fpu()) {\n-    assert(dest->is_double_fpu(),  \"fpu stack allocation required\");\n-\n-    if (code == lir_mul || code == lir_div) {\n-      \/\/ Double values require special handling for strictfp mul\/div on x86\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias1()));\n-      __ fmulp(left->fpu_regnrLo() + 1);\n-    }\n-\n-    if (right->is_double_fpu()) {\n-      arith_fpu_implementation(code, left->fpu_regnrLo(), right->fpu_regnrLo(), dest->fpu_regnrLo(), pop_fpu_stack);\n-\n-    } else {\n-      assert(left->fpu_regnrLo() == 0, \"left must be on TOS\");\n-      assert(dest->fpu_regnrLo() == 0, \"dest must be on TOS\");\n-\n-      Address raddr;\n-      if (right->is_double_stack()) {\n-        raddr = frame_map()->address_for_slot(right->double_stack_ix());\n-      } else if (right->is_constant()) {\n-        \/\/ hack for now\n-        raddr = __ as_Address(InternalAddress(double_constant(right->as_jdouble())));\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-\n-      switch (code) {\n-        case lir_add: __ fadd_d(raddr); break;\n-        case lir_sub: __ fsub_d(raddr); break;\n-        case lir_mul: __ fmul_d(raddr); break;\n-        case lir_div: __ fdiv_d(raddr); break;\n-        default: ShouldNotReachHere();\n-      }\n-    }\n-\n-    if (code == lir_mul || code == lir_div) {\n-      \/\/ Double values require special handling for strictfp mul\/div on x86\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias2()));\n-      __ fmulp(dest->fpu_regnrLo() + 1);\n-    }\n-#endif \/\/ !_LP64\n-\n@@ -2332,59 +1824,0 @@\n-#ifndef _LP64\n-void LIR_Assembler::arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack) {\n-  assert(pop_fpu_stack  || (left_index     == dest_index || right_index     == dest_index), \"invalid LIR\");\n-  assert(!pop_fpu_stack || (left_index - 1 == dest_index || right_index - 1 == dest_index), \"invalid LIR\");\n-  assert(left_index == 0 || right_index == 0, \"either must be on top of stack\");\n-\n-  bool left_is_tos = (left_index == 0);\n-  bool dest_is_tos = (dest_index == 0);\n-  int non_tos_index = (left_is_tos ? right_index : left_index);\n-\n-  switch (code) {\n-    case lir_add:\n-      if (pop_fpu_stack)       __ faddp(non_tos_index);\n-      else if (dest_is_tos)    __ fadd (non_tos_index);\n-      else                     __ fadda(non_tos_index);\n-      break;\n-\n-    case lir_sub:\n-      if (left_is_tos) {\n-        if (pop_fpu_stack)     __ fsubrp(non_tos_index);\n-        else if (dest_is_tos)  __ fsub  (non_tos_index);\n-        else                   __ fsubra(non_tos_index);\n-      } else {\n-        if (pop_fpu_stack)     __ fsubp (non_tos_index);\n-        else if (dest_is_tos)  __ fsubr (non_tos_index);\n-        else                   __ fsuba (non_tos_index);\n-      }\n-      break;\n-\n-    case lir_mul:\n-      if (pop_fpu_stack)       __ fmulp(non_tos_index);\n-      else if (dest_is_tos)    __ fmul (non_tos_index);\n-      else                     __ fmula(non_tos_index);\n-      break;\n-\n-    case lir_div:\n-      if (left_is_tos) {\n-        if (pop_fpu_stack)     __ fdivrp(non_tos_index);\n-        else if (dest_is_tos)  __ fdiv  (non_tos_index);\n-        else                   __ fdivra(non_tos_index);\n-      } else {\n-        if (pop_fpu_stack)     __ fdivp (non_tos_index);\n-        else if (dest_is_tos)  __ fdivr (non_tos_index);\n-        else                   __ fdiva (non_tos_index);\n-      }\n-      break;\n-\n-    case lir_rem:\n-      assert(left_is_tos && dest_is_tos && right_index == 1, \"must be guaranteed by FPU stack allocation\");\n-      __ fremr(noreg);\n-      break;\n-\n-    default:\n-      ShouldNotReachHere();\n-  }\n-}\n-#endif \/\/ _LP64\n-\n-\n@@ -2396,1 +1829,0 @@\n-#ifdef _LP64\n@@ -2400,3 +1832,1 @@\n-          } else\n-#endif\n-          {\n+          } else {\n@@ -2419,9 +1849,0 @@\n-#ifndef _LP64\n-  } else if (value->is_double_fpu()) {\n-    assert(value->fpu_regnrLo() == 0 && dest->fpu_regnrLo() == 0, \"both must be on TOS\");\n-    switch(code) {\n-      case lir_abs   : __ fabs() ; break;\n-      case lir_sqrt  : __ fsqrt(); break;\n-      default      : ShouldNotReachHere();\n-    }\n-#endif \/\/ !_LP64\n@@ -2472,1 +1893,0 @@\n-#ifdef _LP64\n@@ -2486,19 +1906,0 @@\n-#else\n-      int r_lo = right->as_constant_ptr()->as_jint_lo();\n-      int r_hi = right->as_constant_ptr()->as_jint_hi();\n-      switch (code) {\n-        case lir_logic_and:\n-          __ andl(l_lo, r_lo);\n-          __ andl(l_hi, r_hi);\n-          break;\n-        case lir_logic_or:\n-          __ orl(l_lo, r_lo);\n-          __ orl(l_hi, r_hi);\n-          break;\n-        case lir_logic_xor:\n-          __ xorl(l_lo, r_lo);\n-          __ xorl(l_hi, r_hi);\n-          break;\n-        default: ShouldNotReachHere();\n-      }\n-#endif \/\/ _LP64\n@@ -2506,1 +1907,0 @@\n-#ifdef _LP64\n@@ -2513,5 +1913,0 @@\n-#else\n-      Register r_lo = right->as_register_lo();\n-      Register r_hi = right->as_register_hi();\n-      assert(l_lo != r_hi, \"overwriting registers\");\n-#endif\n@@ -2521,1 +1916,0 @@\n-          NOT_LP64(__ andptr(l_hi, r_hi);)\n@@ -2525,1 +1919,0 @@\n-          NOT_LP64(__ orptr(l_hi, r_hi);)\n@@ -2529,1 +1922,0 @@\n-          NOT_LP64(__ xorptr(l_hi, r_hi);)\n@@ -2538,1 +1930,0 @@\n-#ifdef _LP64\n@@ -2540,11 +1931,0 @@\n-#else\n-    if (dst_lo == l_hi) {\n-      assert(dst_hi != l_lo, \"overwriting registers\");\n-      move_regs(l_hi, dst_hi);\n-      move_regs(l_lo, dst_lo);\n-    } else {\n-      assert(dst_lo != l_hi, \"overwriting registers\");\n-      move_regs(l_lo, dst_lo);\n-      move_regs(l_hi, dst_hi);\n-    }\n-#endif \/\/ _LP64\n@@ -2678,1 +2058,0 @@\n-#ifdef _LP64\n@@ -2680,10 +2059,0 @@\n-#else\n-      \/\/ cpu register - cpu register\n-      Register ylo = opr2->as_register_lo();\n-      Register yhi = opr2->as_register_hi();\n-      __ subl(xlo, ylo);\n-      __ sbbl(xhi, yhi);\n-      if (condition == lir_cond_equal || condition == lir_cond_notEqual) {\n-        __ orl(xhi, xlo);\n-      }\n-#endif \/\/ _LP64\n@@ -2693,1 +2062,0 @@\n-#ifdef _LP64\n@@ -2695,4 +2063,0 @@\n-#else\n-      assert(condition == lir_cond_equal || condition == lir_cond_notEqual, \"only handles equals case\");\n-      __ orl(xhi, xlo);\n-#endif \/\/ _LP64\n@@ -2745,7 +2109,0 @@\n-#ifndef _LP64\n-  } else if(opr1->is_single_fpu() || opr1->is_double_fpu()) {\n-    assert(opr1->is_fpu_register() && opr1->fpu() == 0, \"currently left-hand side must be on TOS (relax this restriction)\");\n-    assert(opr2->is_fpu_register(), \"both must be registers\");\n-    __ fcmp(noreg, opr2->fpu(), op->fpu_pop_count() > 0, op->fpu_pop_count() > 1);\n-#endif \/\/ LP64\n-\n@@ -2754,1 +2111,0 @@\n-#ifdef _LP64\n@@ -2759,1 +2115,0 @@\n-#endif \/\/ LP64\n@@ -2768,1 +2123,0 @@\n-#ifdef _LP64\n@@ -2772,3 +2126,0 @@\n-#else\n-      __ cmpoop(as_Address(addr), c->as_jobject());\n-#endif \/\/ _LP64\n@@ -2794,1 +2145,0 @@\n-#ifdef _LP64\n@@ -2796,8 +2146,0 @@\n-#else\n-      assert(left->is_single_fpu() || left->is_double_fpu(), \"must be\");\n-      assert(right->is_single_fpu() || right->is_double_fpu(), \"must match\");\n-\n-      assert(left->fpu() == 0, \"left must be on TOS\");\n-      __ fcmp2int(dst->as_register(), code == lir_ucmp_fd2i, right->fpu(),\n-                  op->fpu_pop_count() > 0, op->fpu_pop_count() > 1);\n-#endif \/\/ LP64\n@@ -2807,1 +2149,0 @@\n-#ifdef _LP64\n@@ -2816,7 +2157,0 @@\n-#else\n-    __ lcmp2int(left->as_register_hi(),\n-                left->as_register_lo(),\n-                right->as_register_hi(),\n-                right->as_register_lo());\n-    move_regs(left->as_register_hi(), dst->as_register());\n-#endif \/\/ _LP64\n@@ -2948,1 +2282,0 @@\n-#ifdef _LP64\n@@ -2955,9 +2288,0 @@\n-#else\n-\n-    switch (code) {\n-      case lir_shl:  __ lshl(hi, lo);        break;\n-      case lir_shr:  __ lshr(hi, lo, true);  break;\n-      case lir_ushr: __ lshr(hi, lo, false); break;\n-      default: ShouldNotReachHere();\n-    }\n-#endif \/\/ LP64\n@@ -2984,3 +2308,0 @@\n-#ifndef _LP64\n-    Unimplemented();\n-#else\n@@ -2998,1 +2319,0 @@\n-#endif \/\/ _LP64\n@@ -3048,1 +2368,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register tmp_load_klass = rscratch1;\n@@ -3073,1 +2393,0 @@\n-    NOT_LP64(assert(src == rcx && src_pos == rdx, \"mismatch in calling convention\");)\n@@ -3079,1 +2398,0 @@\n-#ifdef _LP64\n@@ -3110,15 +2428,0 @@\n-#else\n-    __ push(length);\n-    __ push(dst_pos);\n-    __ push(dst);\n-    __ push(src_pos);\n-    __ push(src);\n-\n-#ifndef PRODUCT\n-    if (PrintC1Statistics) {\n-      __ incrementl(ExternalAddress((address)&Runtime1::_generic_arraycopystub_cnt), rscratch1);\n-    }\n-#endif\n-    __ call_VM_leaf(copyfunc_addr, 5); \/\/ removes pushed parameter from the stack\n-\n-#endif \/\/ _LP64\n@@ -3230,1 +2533,0 @@\n-#ifdef _LP64\n@@ -3233,1 +2535,0 @@\n-#endif\n@@ -3297,15 +2598,0 @@\n-#ifndef _LP64\n-       Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n-        __ movptr(tmp, dst_klass_addr);\n-        __ movptr(tmp, Address(tmp, ObjArrayKlass::element_klass_offset()));\n-        __ push(tmp);\n-        __ movl(tmp, Address(tmp, Klass::super_check_offset_offset()));\n-        __ push(tmp);\n-        __ push(length);\n-        __ lea(tmp, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));\n-        __ push(tmp);\n-        __ lea(tmp, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));\n-        __ push(tmp);\n-\n-        __ call_VM_leaf(copyfunc_addr, 5);\n-#else\n@@ -3338,2 +2624,0 @@\n-#endif\n-\n@@ -3395,1 +2679,0 @@\n-#ifdef _LP64\n@@ -3399,1 +2682,0 @@\n-#endif\n@@ -3424,1 +2706,0 @@\n-#ifdef _LP64\n@@ -3431,8 +2712,0 @@\n-#else\n-  __ lea(tmp, Address(src, src_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));\n-  store_parameter(tmp, 0);\n-  __ lea(tmp, Address(dst, dst_pos, scale, arrayOopDesc::base_offset_in_bytes(basic_type)));\n-  store_parameter(tmp, 1);\n-  store_parameter(length, 2);\n-#endif \/\/ _LP64\n-\n@@ -3511,1 +2784,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register tmp_load_klass = rscratch1;\n@@ -3582,1 +2855,1 @@\n-  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  Register tmp_load_klass = rscratch1;\n@@ -3602,1 +2875,0 @@\n-#ifdef _LP64\n@@ -3604,3 +2876,0 @@\n-#else\n-    assert_different_registers(obj, mdo_addr.base(), mdo_addr.index());\n-#endif\n@@ -3608,1 +2877,0 @@\n-#ifdef _LP64\n@@ -3610,3 +2878,0 @@\n-#else\n-    assert_different_registers(obj, tmp, mdo_addr.base(), mdo_addr.index());\n-#endif\n@@ -3666,1 +2931,0 @@\n-#ifdef _LP64\n@@ -3668,1 +2932,0 @@\n-#endif\n@@ -3681,1 +2944,0 @@\n-#ifdef _LP64\n@@ -3689,1 +2951,0 @@\n-#endif\n@@ -3783,1 +3044,0 @@\n-#ifdef _LP64\n@@ -3787,12 +3047,0 @@\n-#else\n-    Register hi = left->as_register_hi();\n-    __ lneg(hi, lo);\n-    if (dest->as_register_lo() == hi) {\n-      assert(dest->as_register_hi() != lo, \"destroying register\");\n-      move_regs(hi, dest->as_register_hi());\n-      move_regs(lo, dest->as_register_lo());\n-    } else {\n-      move_regs(lo, dest->as_register_lo());\n-      move_regs(hi, dest->as_register_hi());\n-    }\n-#endif \/\/ _LP64\n@@ -3801,1 +3049,0 @@\n-#ifdef _LP64\n@@ -3806,4 +3053,1 @@\n-    }\n-    else\n-#endif\n-    {\n+    } else {\n@@ -3819,1 +3063,0 @@\n-#ifdef _LP64\n@@ -3824,4 +3067,1 @@\n-    }\n-    else\n-#endif\n-    {\n+    } else {\n@@ -3836,6 +3076,0 @@\n-#ifndef _LP64\n-  } else if (left->is_single_fpu() || left->is_double_fpu()) {\n-    assert(left->fpu() == 0, \"arg must be on TOS\");\n-    assert(dest->fpu() == 0, \"dest must be TOS\");\n-    __ fchs();\n-#endif \/\/ !_LP64\n@@ -3888,1 +3122,0 @@\n-#ifdef _LP64\n@@ -3890,5 +3123,0 @@\n-#else\n-      __ movdl(dest->as_register_lo(), src->as_xmm_double_reg());\n-      __ psrlq(src->as_xmm_double_reg(), 32);\n-      __ movdl(dest->as_register_hi(), src->as_xmm_double_reg());\n-#endif \/\/ _LP64\n@@ -3912,22 +3140,0 @@\n-#ifndef _LP64\n-  } else if (src->is_double_fpu()) {\n-    assert(src->fpu_regnrLo() == 0, \"must be TOS\");\n-    if (dest->is_double_stack()) {\n-      __ fistp_d(frame_map()->address_for_slot(dest->double_stack_ix()));\n-    } else if (dest->is_address()) {\n-      __ fistp_d(as_Address(dest->as_address_ptr()));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-\n-  } else if (dest->is_double_fpu()) {\n-    assert(dest->fpu_regnrLo() == 0, \"must be TOS\");\n-    if (src->is_double_stack()) {\n-      __ fild_d(frame_map()->address_for_slot(src->double_stack_ix()));\n-    } else if (src->is_address()) {\n-      __ fild_d(as_Address(src->as_address_ptr()));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-#endif \/\/ !_LP64\n-\n@@ -4016,2 +3222,0 @@\n-#ifdef _LP64\n-  \/\/ __ get_thread(result_reg->as_register_lo());\n@@ -4019,3 +3223,0 @@\n-#else\n-  __ get_thread(result_reg->as_register());\n-#endif \/\/ _LP64\n@@ -4042,1 +3243,0 @@\n-#ifdef _LP64\n@@ -4050,3 +3250,0 @@\n-#else\n-    __ xchgl(obj, as_Address(src->as_address_ptr()));\n-#endif\n@@ -4054,1 +3251,0 @@\n-#ifdef _LP64\n@@ -4062,3 +3258,0 @@\n-#else\n-    ShouldNotReachHere();\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":22,"deletions":829,"binary":false,"changes":851,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-    _call_stub_size = NOT_LP64(15) LP64_ONLY(28),\n+    _call_stub_size = 28,\n@@ -51,1 +51,1 @@\n-    _deopt_handler_size = NOT_LP64(10) LP64_ONLY(17)\n+    _deopt_handler_size = 17\n@@ -61,9 +61,0 @@\n-#ifndef _LP64\n-  void arith_fpu_implementation(LIR_Code code, int left_index, int right_index, int dest_index, bool pop_fpu_stack);\n-\n-  void fpop();\n-  void fxch(int i);\n-  void fld(int i);\n-  void ffree(int i);\n-#endif \/\/ !_LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.hpp","additions":2,"deletions":11,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -96,1 +96,0 @@\n-#ifdef _LP64\n@@ -99,4 +98,0 @@\n-#else\n-    case floatTag:   opr = UseSSE >= 1 ? FrameMap::xmm0_float_opr  : FrameMap::fpu0_float_opr;  break;\n-    case doubleTag:  opr = UseSSE >= 2 ? FrameMap::xmm0_double_opr : FrameMap::fpu0_double_opr;  break;\n-#endif \/\/ _LP64\n@@ -151,1 +146,0 @@\n-  NOT_LP64( return new_register(T_ADDRESS); )\n@@ -161,1 +155,0 @@\n-#ifdef _LP64\n@@ -176,5 +169,0 @@\n-#else\n-    return new LIR_Address(base,\n-                           ((intx)(constant->as_jint()) << shift) + disp,\n-                           type);\n-#endif\n@@ -194,1 +182,0 @@\n-#ifdef _LP64\n@@ -206,14 +193,0 @@\n-#else\n-    \/\/ A displacement overflow can also occur for x86 but that is not a problem due to the 32-bit address range!\n-    \/\/ Let's assume an array 'a' and an access with displacement 'disp'. When disp overflows, then \"a + disp\" will\n-    \/\/ always be negative (i.e. underflows the 32-bit address range):\n-    \/\/ Let N = 2^32: a + signed_overflow(disp) = a + disp - N.\n-    \/\/ \"a + disp\" is always smaller than N. If an index was chosen which would point to an address beyond N, then\n-    \/\/ range checks would catch that and throw an exception. Thus, a + disp < 0 holds which means that it always\n-    \/\/ underflows the 32-bit address range:\n-    \/\/ unsigned_underflow(a + signed_overflow(disp)) = unsigned_underflow(a + disp - N)\n-    \/\/                                              = (a + disp - N) + N = a + disp\n-    \/\/ This shows that we still end up at the correct address with a displacement overflow due to the 32-bit address\n-    \/\/ range limitation. This overflow only needs to be handled if addresses can be larger as on 64-bit platforms.\n-    addr = new LIR_Address(array_opr, offset_in_bytes + (intx)(index_opr->as_jint()) * elem_size, type);\n-#endif \/\/ _LP64\n@@ -221,1 +194,0 @@\n-#ifdef _LP64\n@@ -227,1 +199,0 @@\n-#endif \/\/ _LP64\n@@ -348,1 +319,0 @@\n-#ifdef _LP64\n@@ -359,1 +329,0 @@\n-#endif\n@@ -362,1 +331,1 @@\n-  set_result(x, round_item(reg));\n+  set_result(x, reg);\n@@ -380,18 +349,0 @@\n-#ifndef _LP64\n-  \/\/ do not load right operand if it is a constant.  only 0 and 1 are\n-  \/\/ loaded because there are special instructions for loading them\n-  \/\/ without memory access (not needed for SSE2 instructions)\n-  bool must_load_right = false;\n-  if (right.is_constant()) {\n-    LIR_Const* c = right.result()->as_constant_ptr();\n-    assert(c != nullptr, \"invalid constant\");\n-    assert(c->type() == T_FLOAT || c->type() == T_DOUBLE, \"invalid type\");\n-\n-    if (c->type() == T_FLOAT) {\n-      must_load_right = UseSSE < 1 && (c->is_one_float() || c->is_zero_float());\n-    } else {\n-      must_load_right = UseSSE < 2 && (c->is_one_double() || c->is_zero_double());\n-    }\n-  }\n-#endif \/\/ !LP64\n-\n@@ -404,4 +355,0 @@\n-#ifndef _LP64\n-  } else if (must_load_right) {\n-    right.load_item();\n-#endif \/\/ !LP64\n@@ -417,1 +364,0 @@\n-#ifdef _LP64\n@@ -450,20 +396,1 @@\n-    set_result(x, round_item(reg));\n-  }\n-#else\n-  if ((UseSSE >= 1 && x->op() == Bytecodes::_frem) || (UseSSE >= 2 && x->op() == Bytecodes::_drem)) {\n-    \/\/ special handling for frem and drem: no SSE instruction, so must use FPU with temporary fpu stack slots\n-    LIR_Opr fpu0, fpu1;\n-    if (x->op() == Bytecodes::_frem) {\n-      fpu0 = LIR_OprFact::single_fpu(0);\n-      fpu1 = LIR_OprFact::single_fpu(1);\n-    } else {\n-      fpu0 = LIR_OprFact::double_fpu(0);\n-      fpu1 = LIR_OprFact::double_fpu(1);\n-    }\n-    __ move(right.result(), fpu1); \/\/ order of left and right operand is important!\n-    __ move(left.result(), fpu0);\n-    __ rem (fpu0, fpu1, fpu0);\n-    __ move(fpu0, reg);\n-\n-  } else {\n-    arithmetic_op_fpu(x->op(), reg, left.result(), right.result(), tmp);\n+    set_result(x, reg);\n@@ -471,2 +398,0 @@\n-  set_result(x, round_item(reg));\n-#endif \/\/ _LP64\n@@ -762,1 +687,1 @@\n-  assert(type == T_INT || is_oop LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  assert(type == T_INT || is_oop || type == T_LONG, \"unexpected type\");\n@@ -772,1 +697,1 @@\n-  assert(type == T_INT LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  assert(type == T_INT || type == T_LONG, \"unexpected type\");\n@@ -810,5 +735,1 @@\n-      x->id() == vmIntrinsics::_dlog10\n-#ifdef _LP64\n-      || x->id() == vmIntrinsics::_dtanh\n-#endif\n-      ) {\n+      x->id() == vmIntrinsics::_dlog10 || x->id() == vmIntrinsics::_dtanh) {\n@@ -822,5 +743,0 @@\n-#ifndef _LP64\n-  if (UseSSE < 2) {\n-    value.set_destroys_register();\n-  }\n-#endif \/\/ !LP64\n@@ -833,1 +749,0 @@\n-#ifdef _LP64\n@@ -839,1 +754,0 @@\n-#endif\n@@ -895,56 +809,0 @@\n-#ifndef _LP64\n-  LIR_Opr tmp = FrameMap::fpu0_double_opr;\n-  result_reg = tmp;\n-  switch(x->id()) {\n-    case vmIntrinsics::_dexp:\n-      if (StubRoutines::dexp() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dexp(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dexp), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dlog:\n-      if (StubRoutines::dlog() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dlog(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dlog), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dlog10:\n-      if (StubRoutines::dlog10() != nullptr) {\n-       __ call_runtime_leaf(StubRoutines::dlog10(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dlog10), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dpow:\n-      if (StubRoutines::dpow() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dpow(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dpow), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dsin:\n-      if (VM_Version::supports_sse2() && StubRoutines::dsin() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dsin(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dsin), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dcos:\n-      if (VM_Version::supports_sse2() && StubRoutines::dcos() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dcos(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dcos), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    case vmIntrinsics::_dtan:\n-      if (StubRoutines::dtan() != nullptr) {\n-        __ call_runtime_leaf(StubRoutines::dtan(), getThreadTemp(), result_reg, cc->args());\n-      } else {\n-        __ call_runtime_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtan), getThreadTemp(), result_reg, cc->args());\n-      }\n-      break;\n-    default:  ShouldNotReachHere();\n-  }\n-#else\n@@ -1009,1 +867,0 @@\n-#endif \/\/ _LP64\n@@ -1042,14 +899,0 @@\n-#ifndef _LP64\n-  src.load_item_force     (FrameMap::rcx_oop_opr);\n-  src_pos.load_item_force (FrameMap::rdx_opr);\n-  dst.load_item_force     (FrameMap::rax_oop_opr);\n-  dst_pos.load_item_force (FrameMap::rbx_opr);\n-  length.load_item_force  (FrameMap::rdi_opr);\n-  LIR_Opr tmp =           (FrameMap::rsi_opr);\n-\n-  if (expected_type != nullptr && flags == 0) {\n-    FrameMap* f = Compilation::current()->frame_map();\n-    f->update_reserved_argument_area_size(3 * BytesPerWord);\n-  }\n-#else\n-\n@@ -1071,1 +914,0 @@\n-#endif \/\/ LP64\n@@ -1113,6 +955,0 @@\n-#ifndef _LP64\n-      if (!is_updateBytes) { \/\/ long b raw address\n-         base_op = new_register(T_INT);\n-         __ convert(Bytecodes::_l2i, buf.result(), base_op);\n-      }\n-#else\n@@ -1124,1 +960,0 @@\n-#endif\n@@ -1193,8 +1028,0 @@\n-#ifndef _LP64\n-  result_a = new_register(T_INT);\n-  __ convert(Bytecodes::_l2i, a.result(), result_a);\n-  result_b = new_register(T_INT);\n-  __ convert(Bytecodes::_l2i, b.result(), result_b);\n-#endif\n-\n-\n@@ -1234,13 +1061,0 @@\n-#ifndef _LP64\n-\/\/ _i2l, _i2f, _i2d, _l2i, _l2f, _l2d, _f2i, _f2l, _f2d, _d2i, _d2l, _d2f\n-\/\/ _i2b, _i2c, _i2s\n-static LIR_Opr fixed_register_for(BasicType type) {\n-  switch (type) {\n-    case T_FLOAT:  return FrameMap::fpu0_float_opr;\n-    case T_DOUBLE: return FrameMap::fpu0_double_opr;\n-    case T_INT:    return FrameMap::rax_opr;\n-    case T_LONG:   return FrameMap::long0_opr;\n-    default:       ShouldNotReachHere(); return LIR_OprFact::illegalOpr;\n-  }\n-}\n-#endif\n@@ -1249,1 +1063,0 @@\n-#ifdef _LP64\n@@ -1257,60 +1070,0 @@\n-#else\n-  \/\/ flags that vary for the different operations and different SSE-settings\n-  bool fixed_input = false, fixed_result = false, round_result = false, needs_stub = false;\n-\n-  switch (x->op()) {\n-    case Bytecodes::_i2l: \/\/ fall through\n-    case Bytecodes::_l2i: \/\/ fall through\n-    case Bytecodes::_i2b: \/\/ fall through\n-    case Bytecodes::_i2c: \/\/ fall through\n-    case Bytecodes::_i2s: fixed_input = false;       fixed_result = false;       round_result = false;      needs_stub = false; break;\n-\n-    case Bytecodes::_f2d: fixed_input = UseSSE == 1; fixed_result = false;       round_result = false;      needs_stub = false; break;\n-    case Bytecodes::_d2f: fixed_input = false;       fixed_result = UseSSE == 1; round_result = UseSSE < 1; needs_stub = false; break;\n-    case Bytecodes::_i2f: fixed_input = false;       fixed_result = false;       round_result = UseSSE < 1; needs_stub = false; break;\n-    case Bytecodes::_i2d: fixed_input = false;       fixed_result = false;       round_result = false;      needs_stub = false; break;\n-    case Bytecodes::_f2i: fixed_input = false;       fixed_result = false;       round_result = false;      needs_stub = true;  break;\n-    case Bytecodes::_d2i: fixed_input = false;       fixed_result = false;       round_result = false;      needs_stub = true;  break;\n-    case Bytecodes::_l2f: fixed_input = false;       fixed_result = UseSSE >= 1; round_result = UseSSE < 1; needs_stub = false; break;\n-    case Bytecodes::_l2d: fixed_input = false;       fixed_result = UseSSE >= 2; round_result = UseSSE < 2; needs_stub = false; break;\n-    case Bytecodes::_f2l: fixed_input = true;        fixed_result = true;        round_result = false;      needs_stub = false; break;\n-    case Bytecodes::_d2l: fixed_input = true;        fixed_result = true;        round_result = false;      needs_stub = false; break;\n-    default: ShouldNotReachHere();\n-  }\n-\n-  LIRItem value(x->value(), this);\n-  value.load_item();\n-  LIR_Opr input = value.result();\n-  LIR_Opr result = rlock(x);\n-\n-  \/\/ arguments of lir_convert\n-  LIR_Opr conv_input = input;\n-  LIR_Opr conv_result = result;\n-  ConversionStub* stub = nullptr;\n-\n-  if (fixed_input) {\n-    conv_input = fixed_register_for(input->type());\n-    __ move(input, conv_input);\n-  }\n-\n-  assert(fixed_result == false || round_result == false, \"cannot set both\");\n-  if (fixed_result) {\n-    conv_result = fixed_register_for(result->type());\n-  } else if (round_result) {\n-    result = new_register(result->type());\n-    set_vreg_flag(result, must_start_in_memory);\n-  }\n-\n-  if (needs_stub) {\n-    stub = new ConversionStub(x->op(), conv_input, conv_result);\n-  }\n-\n-  __ convert(x->op(), conv_input, conv_result, stub);\n-\n-  if (result != conv_result) {\n-    __ move(conv_result, result);\n-  }\n-\n-  assert(result->is_virtual(), \"result must be virtual register\");\n-  set_result(x, result);\n-#endif \/\/ _LP64\n@@ -1577,1 +1330,0 @@\n-#ifdef _LP64\n@@ -1579,5 +1331,0 @@\n-#else\n-  LIR_Opr result = new_register(T_INT);\n-  __ get_thread(result);\n-  return result;\n-#endif \/\/\n@@ -1628,6 +1375,0 @@\n-#ifndef _LP64\n-    if (UseSSE < 2) {\n-      \/\/ no spill slot needed in SSE2 mode because xmm->cpu register move is possible\n-      set_vreg_flag(result, must_start_in_memory);\n-    }\n-#endif \/\/ !LP64\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":5,"deletions":264,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-#ifdef _LP64\n@@ -67,6 +66,0 @@\n-#else\n-  assert(base()->is_single_cpu(), \"wrong base operand\");\n-  assert(index()->is_illegal() || index()->is_single_cpu(), \"wrong index operand\");\n-  assert(base()->type() == T_ADDRESS || base()->type() == T_OBJECT || base()->type() == T_INT || base()->type() == T_METADATA,\n-         \"wrong type for addresses\");\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIR_x86.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1,1127 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"c1\/c1_Instruction.hpp\"\n-#include \"c1\/c1_LinearScan.hpp\"\n-#include \"utilities\/bitMap.inline.hpp\"\n-\n-\n-#ifdef _LP64\n-void LinearScan::allocate_fpu_stack() {\n-  \/\/ No FPU stack used on x86-64\n-}\n-#else\n-\/\/----------------------------------------------------------------------\n-\/\/ Allocation of FPU stack slots (Intel x86 only)\n-\/\/----------------------------------------------------------------------\n-\n-void LinearScan::allocate_fpu_stack() {\n-  \/\/ First compute which FPU registers are live at the start of each basic block\n-  \/\/ (To minimize the amount of work we have to do if we have to merge FPU stacks)\n-  if (ComputeExactFPURegisterUsage) {\n-    Interval* intervals_in_register, *intervals_in_memory;\n-    create_unhandled_lists(&intervals_in_register, &intervals_in_memory, is_in_fpu_register, nullptr);\n-\n-    \/\/ ignore memory intervals by overwriting intervals_in_memory\n-    \/\/ the dummy interval is needed to enforce the walker to walk until the given id:\n-    \/\/ without it, the walker stops when the unhandled-list is empty -> live information\n-    \/\/ beyond this point would be incorrect.\n-    Interval* dummy_interval = new Interval(any_reg);\n-    dummy_interval->add_range(max_jint - 2, max_jint - 1);\n-    dummy_interval->set_next(Interval::end());\n-    intervals_in_memory = dummy_interval;\n-\n-    IntervalWalker iw(this, intervals_in_register, intervals_in_memory);\n-\n-    const int num_blocks = block_count();\n-    for (int i = 0; i < num_blocks; i++) {\n-      BlockBegin* b = block_at(i);\n-\n-      \/\/ register usage is only needed for merging stacks -> compute only\n-      \/\/ when more than one predecessor.\n-      \/\/ the block must not have any spill moves at the beginning (checked by assertions)\n-      \/\/ spill moves would use intervals that are marked as handled and so the usage bit\n-      \/\/ would been set incorrectly\n-\n-      \/\/ NOTE: the check for number_of_preds > 1 is necessary. A block with only one\n-      \/\/       predecessor may have spill moves at the begin of the block.\n-      \/\/       If an interval ends at the current instruction id, it is not possible\n-      \/\/       to decide if the register is live or not at the block begin -> the\n-      \/\/       register information would be incorrect.\n-      if (b->number_of_preds() > 1) {\n-        int id = b->first_lir_instruction_id();\n-        ResourceBitMap regs(FrameMap::nof_fpu_regs);\n-\n-        iw.walk_to(id);   \/\/ walk after the first instruction (always a label) of the block\n-        assert(iw.current_position() == id, \"did not walk completely to id\");\n-\n-        \/\/ Only consider FPU values in registers\n-        Interval* interval = iw.active_first(fixedKind);\n-        while (interval != Interval::end()) {\n-          int reg = interval->assigned_reg();\n-          assert(reg >= pd_first_fpu_reg && reg <= pd_last_fpu_reg, \"no fpu register\");\n-          assert(interval->assigned_regHi() == -1, \"must not have hi register (doubles stored in one register)\");\n-          assert(interval->from() <= id && id < interval->to(), \"interval out of range\");\n-\n-#ifndef PRODUCT\n-          if (TraceFPURegisterUsage) {\n-            tty->print(\"fpu reg %d is live because of \", reg - pd_first_fpu_reg); interval->print();\n-          }\n-#endif\n-\n-          regs.set_bit(reg - pd_first_fpu_reg);\n-          interval = interval->next();\n-        }\n-\n-        b->set_fpu_register_usage(regs);\n-\n-#ifndef PRODUCT\n-        if (TraceFPURegisterUsage) {\n-          tty->print(\"FPU regs for block %d, LIR instr %d): \", b->block_id(), id); regs.print_on(tty); tty->cr();\n-        }\n-#endif\n-      }\n-    }\n-  }\n-\n-  FpuStackAllocator alloc(ir()->compilation(), this);\n-  _fpu_stack_allocator = &alloc;\n-  alloc.allocate();\n-  _fpu_stack_allocator = nullptr;\n-}\n-\n-\n-FpuStackAllocator::FpuStackAllocator(Compilation* compilation, LinearScan* allocator)\n-  : _compilation(compilation)\n-  , _allocator(allocator)\n-  , _lir(nullptr)\n-  , _pos(-1)\n-  , _sim(compilation)\n-  , _temp_sim(compilation)\n-{}\n-\n-void FpuStackAllocator::allocate() {\n-  int num_blocks = allocator()->block_count();\n-  for (int i = 0; i < num_blocks; i++) {\n-    \/\/ Set up to process block\n-    BlockBegin* block = allocator()->block_at(i);\n-    intArray* fpu_stack_state = block->fpu_stack_state();\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->cr();\n-      tty->print_cr(\"------- Begin of new Block %d -------\", block->block_id());\n-    }\n-#endif\n-\n-    assert(fpu_stack_state != nullptr ||\n-           block->end()->as_Base() != nullptr ||\n-           block->is_set(BlockBegin::exception_entry_flag),\n-           \"FPU stack state must be present due to linear-scan order for FPU stack allocation\");\n-    \/\/ note: exception handler entries always start with an empty fpu stack\n-    \/\/       because stack merging would be too complicated\n-\n-    if (fpu_stack_state != nullptr) {\n-      sim()->read_state(fpu_stack_state);\n-    } else {\n-      sim()->clear();\n-    }\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->print(\"Reading FPU state for block %d:\", block->block_id());\n-      sim()->print();\n-      tty->cr();\n-    }\n-#endif\n-\n-    allocate_block(block);\n-    CHECK_BAILOUT();\n-  }\n-}\n-\n-void FpuStackAllocator::allocate_block(BlockBegin* block) {\n-  bool processed_merge = false;\n-  LIR_OpList* insts = block->lir()->instructions_list();\n-  set_lir(block->lir());\n-  set_pos(0);\n-\n-\n-  \/\/ Note: insts->length() may change during loop\n-  while (pos() < insts->length()) {\n-    LIR_Op* op = insts->at(pos());\n-    _debug_information_computed = false;\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      op->print();\n-    }\n-    check_invalid_lir_op(op);\n-#endif\n-\n-    LIR_OpBranch* branch = op->as_OpBranch();\n-    LIR_Op1* op1 = op->as_Op1();\n-    LIR_Op2* op2 = op->as_Op2();\n-    LIR_OpCall* opCall = op->as_OpCall();\n-\n-    if (branch != nullptr && branch->block() != nullptr) {\n-      if (!processed_merge) {\n-        \/\/ propagate stack at first branch to a successor\n-        processed_merge = true;\n-        bool required_merge = merge_fpu_stack_with_successors(block);\n-\n-        assert(!required_merge || branch->cond() == lir_cond_always, \"splitting of critical edges should prevent FPU stack mismatches at cond branches\");\n-      }\n-\n-    } else if (op1 != nullptr) {\n-      handle_op1(op1);\n-    } else if (op2 != nullptr) {\n-      handle_op2(op2);\n-    } else if (opCall != nullptr) {\n-      handle_opCall(opCall);\n-    }\n-\n-    compute_debug_information(op);\n-\n-    set_pos(1 + pos());\n-  }\n-\n-  \/\/ Propagate stack when block does not end with branch\n-  if (!processed_merge) {\n-    merge_fpu_stack_with_successors(block);\n-  }\n-}\n-\n-\n-void FpuStackAllocator::compute_debug_information(LIR_Op* op) {\n-  if (!_debug_information_computed && op->id() != -1 && allocator()->has_info(op->id())) {\n-    visitor.visit(op);\n-\n-    \/\/ exception handling\n-    if (allocator()->compilation()->has_exception_handlers()) {\n-      XHandlers* xhandlers = visitor.all_xhandler();\n-      int n = xhandlers->length();\n-      for (int k = 0; k < n; k++) {\n-        allocate_exception_handler(xhandlers->handler_at(k));\n-      }\n-    } else {\n-      assert(visitor.all_xhandler()->length() == 0, \"missed exception handler\");\n-    }\n-\n-    \/\/ compute debug information\n-    int n = visitor.info_count();\n-    assert(n > 0, \"should not visit operation otherwise\");\n-\n-    for (int j = 0; j < n; j++) {\n-      CodeEmitInfo* info = visitor.info_at(j);\n-      \/\/ Compute debug information\n-      allocator()->compute_debug_info(info, op->id());\n-    }\n-  }\n-  _debug_information_computed = true;\n-}\n-\n-void FpuStackAllocator::allocate_exception_handler(XHandler* xhandler) {\n-  if (!sim()->is_empty()) {\n-    LIR_List* old_lir = lir();\n-    int old_pos = pos();\n-    intArray* old_state = sim()->write_state();\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->cr();\n-      tty->print_cr(\"------- begin of exception handler -------\");\n-    }\n-#endif\n-\n-    if (xhandler->entry_code() == nullptr) {\n-      \/\/ need entry code to clear FPU stack\n-      LIR_List* entry_code = new LIR_List(_compilation);\n-      entry_code->jump(xhandler->entry_block());\n-      xhandler->set_entry_code(entry_code);\n-    }\n-\n-    LIR_OpList* insts = xhandler->entry_code()->instructions_list();\n-    set_lir(xhandler->entry_code());\n-    set_pos(0);\n-\n-    \/\/ Note: insts->length() may change during loop\n-    while (pos() < insts->length()) {\n-      LIR_Op* op = insts->at(pos());\n-\n-#ifndef PRODUCT\n-      if (TraceFPUStack) {\n-        op->print();\n-      }\n-      check_invalid_lir_op(op);\n-#endif\n-\n-      switch (op->code()) {\n-        case lir_move:\n-          assert(op->as_Op1() != nullptr, \"must be LIR_Op1\");\n-          assert(pos() != insts->length() - 1, \"must not be last operation\");\n-\n-          handle_op1((LIR_Op1*)op);\n-          break;\n-\n-        case lir_branch:\n-          assert(op->as_OpBranch()->cond() == lir_cond_always, \"must be unconditional branch\");\n-          assert(pos() == insts->length() - 1, \"must be last operation\");\n-\n-          \/\/ remove all remaining dead registers from FPU stack\n-          clear_fpu_stack(LIR_OprFact::illegalOpr);\n-          break;\n-\n-        default:\n-          \/\/ other operations not allowed in exception entry code\n-          ShouldNotReachHere();\n-      }\n-\n-      set_pos(pos() + 1);\n-    }\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->cr();\n-      tty->print_cr(\"------- end of exception handler -------\");\n-    }\n-#endif\n-\n-    set_lir(old_lir);\n-    set_pos(old_pos);\n-    sim()->read_state(old_state);\n-  }\n-}\n-\n-\n-int FpuStackAllocator::fpu_num(LIR_Opr opr) {\n-  assert(opr->is_fpu_register() && !opr->is_xmm_register(), \"shouldn't call this otherwise\");\n-  return opr->is_single_fpu() ? opr->fpu_regnr() : opr->fpu_regnrLo();\n-}\n-\n-int FpuStackAllocator::tos_offset(LIR_Opr opr) {\n-  return sim()->offset_from_tos(fpu_num(opr));\n-}\n-\n-\n-LIR_Opr FpuStackAllocator::to_fpu_stack(LIR_Opr opr) {\n-  assert(opr->is_fpu_register() && !opr->is_xmm_register(), \"shouldn't call this otherwise\");\n-\n-  int stack_offset = tos_offset(opr);\n-  if (opr->is_single_fpu()) {\n-    return LIR_OprFact::single_fpu(stack_offset)->make_fpu_stack_offset();\n-  } else {\n-    assert(opr->is_double_fpu(), \"shouldn't call this otherwise\");\n-    return LIR_OprFact::double_fpu(stack_offset)->make_fpu_stack_offset();\n-  }\n-}\n-\n-LIR_Opr FpuStackAllocator::to_fpu_stack_top(LIR_Opr opr, bool dont_check_offset) {\n-  assert(opr->is_fpu_register() && !opr->is_xmm_register(), \"shouldn't call this otherwise\");\n-  assert(dont_check_offset || tos_offset(opr) == 0, \"operand is not on stack top\");\n-\n-  int stack_offset = 0;\n-  if (opr->is_single_fpu()) {\n-    return LIR_OprFact::single_fpu(stack_offset)->make_fpu_stack_offset();\n-  } else {\n-    assert(opr->is_double_fpu(), \"shouldn't call this otherwise\");\n-    return LIR_OprFact::double_fpu(stack_offset)->make_fpu_stack_offset();\n-  }\n-}\n-\n-\n-\n-void FpuStackAllocator::insert_op(LIR_Op* op) {\n-  lir()->insert_before(pos(), op);\n-  set_pos(1 + pos());\n-}\n-\n-\n-void FpuStackAllocator::insert_exchange(int offset) {\n-  if (offset > 0) {\n-    LIR_Op1* fxch_op = new LIR_Op1(lir_fxch, LIR_OprFact::intConst(offset), LIR_OprFact::illegalOpr);\n-    insert_op(fxch_op);\n-    sim()->swap(offset);\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->print(\"Exchanged register: %d         New state: \", sim()->get_slot(0)); sim()->print(); tty->cr();\n-    }\n-#endif\n-\n-  }\n-}\n-\n-void FpuStackAllocator::insert_exchange(LIR_Opr opr) {\n-  insert_exchange(tos_offset(opr));\n-}\n-\n-\n-void FpuStackAllocator::insert_free(int offset) {\n-  \/\/ move stack slot to the top of stack and then pop it\n-  insert_exchange(offset);\n-\n-  LIR_Op* fpop = new LIR_Op0(lir_fpop_raw);\n-  insert_op(fpop);\n-  sim()->pop();\n-\n-#ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->print(\"Inserted pop                   New state: \"); sim()->print(); tty->cr();\n-    }\n-#endif\n-}\n-\n-\n-void FpuStackAllocator::insert_free_if_dead(LIR_Opr opr) {\n-  if (sim()->contains(fpu_num(opr))) {\n-    int res_slot = tos_offset(opr);\n-    insert_free(res_slot);\n-  }\n-}\n-\n-void FpuStackAllocator::insert_free_if_dead(LIR_Opr opr, LIR_Opr ignore) {\n-  if (fpu_num(opr) != fpu_num(ignore) && sim()->contains(fpu_num(opr))) {\n-    int res_slot = tos_offset(opr);\n-    insert_free(res_slot);\n-  }\n-}\n-\n-void FpuStackAllocator::insert_copy(LIR_Opr from, LIR_Opr to) {\n-  int offset = tos_offset(from);\n-  LIR_Op1* fld = new LIR_Op1(lir_fld, LIR_OprFact::intConst(offset), LIR_OprFact::illegalOpr);\n-  insert_op(fld);\n-\n-  sim()->push(fpu_num(to));\n-\n-#ifndef PRODUCT\n-  if (TraceFPUStack) {\n-    tty->print(\"Inserted copy (%d -> %d)         New state: \", fpu_num(from), fpu_num(to)); sim()->print(); tty->cr();\n-  }\n-#endif\n-}\n-\n-void FpuStackAllocator::do_rename(LIR_Opr from, LIR_Opr to) {\n-  sim()->rename(fpu_num(from), fpu_num(to));\n-}\n-\n-void FpuStackAllocator::do_push(LIR_Opr opr) {\n-  sim()->push(fpu_num(opr));\n-}\n-\n-void FpuStackAllocator::pop_if_last_use(LIR_Op* op, LIR_Opr opr) {\n-  assert(op->fpu_pop_count() == 0, \"fpu_pop_count already set\");\n-  assert(tos_offset(opr) == 0, \"can only pop stack top\");\n-\n-  if (opr->is_last_use()) {\n-    op->set_fpu_pop_count(1);\n-    sim()->pop();\n-  }\n-}\n-\n-void FpuStackAllocator::pop_always(LIR_Op* op, LIR_Opr opr) {\n-  assert(op->fpu_pop_count() == 0, \"fpu_pop_count already set\");\n-  assert(tos_offset(opr) == 0, \"can only pop stack top\");\n-\n-  op->set_fpu_pop_count(1);\n-  sim()->pop();\n-}\n-\n-void FpuStackAllocator::clear_fpu_stack(LIR_Opr preserve) {\n-  int result_stack_size = (preserve->is_fpu_register() && !preserve->is_xmm_register() ? 1 : 0);\n-  while (sim()->stack_size() > result_stack_size) {\n-    assert(!sim()->slot_is_empty(0), \"not allowed\");\n-\n-    if (result_stack_size == 0 || sim()->get_slot(0) != fpu_num(preserve)) {\n-      insert_free(0);\n-    } else {\n-      \/\/ move \"preserve\" to bottom of stack so that all other stack slots can be popped\n-      insert_exchange(sim()->stack_size() - 1);\n-    }\n-  }\n-}\n-\n-\n-void FpuStackAllocator::handle_op1(LIR_Op1* op1) {\n-  LIR_Opr in  = op1->in_opr();\n-  LIR_Opr res = op1->result_opr();\n-\n-  LIR_Opr new_in  = in;  \/\/ new operands relative to the actual fpu stack top\n-  LIR_Opr new_res = res;\n-\n-  \/\/ Note: this switch is processed for all LIR_Op1, regardless if they have FPU-arguments,\n-  \/\/       so checks for is_float_kind() are necessary inside the cases\n-  switch (op1->code()) {\n-\n-    case lir_return: {\n-      \/\/ FPU-Stack must only contain the (optional) fpu return value.\n-      \/\/ All remaining dead values are popped from the stack\n-      \/\/ If the input operand is a fpu-register, it is exchanged to the bottom of the stack\n-\n-      clear_fpu_stack(in);\n-      if (in->is_fpu_register() && !in->is_xmm_register()) {\n-        new_in = to_fpu_stack_top(in);\n-      }\n-\n-      break;\n-    }\n-\n-    case lir_move: {\n-      if (in->is_fpu_register() && !in->is_xmm_register()) {\n-        if (res->is_xmm_register()) {\n-          \/\/ move from fpu register to xmm register (necessary for operations that\n-          \/\/ are not available in the SSE instruction set)\n-          insert_exchange(in);\n-          new_in = to_fpu_stack_top(in);\n-          pop_always(op1, in);\n-\n-        } else if (res->is_fpu_register() && !res->is_xmm_register()) {\n-          \/\/ move from fpu-register to fpu-register:\n-          \/\/ * input and result register equal:\n-          \/\/   nothing to do\n-          \/\/ * input register is last use:\n-          \/\/   rename the input register to result register -> input register\n-          \/\/   not present on fpu-stack afterwards\n-          \/\/ * input register not last use:\n-          \/\/   duplicate input register to result register to preserve input\n-          \/\/\n-          \/\/ Note: The LIR-Assembler does not produce any code for fpu register moves,\n-          \/\/       so input and result stack index must be equal\n-\n-          if (fpu_num(in) == fpu_num(res)) {\n-            \/\/ nothing to do\n-          } else if (in->is_last_use()) {\n-            insert_free_if_dead(res);\/\/, in);\n-            do_rename(in, res);\n-          } else {\n-            insert_free_if_dead(res);\n-            insert_copy(in, res);\n-          }\n-          new_in = to_fpu_stack(res);\n-          new_res = new_in;\n-\n-        } else {\n-          \/\/ move from fpu-register to memory\n-          \/\/ input operand must be on top of stack\n-\n-          insert_exchange(in);\n-\n-          \/\/ create debug information here because afterwards the register may have been popped\n-          compute_debug_information(op1);\n-\n-          new_in = to_fpu_stack_top(in);\n-          pop_if_last_use(op1, in);\n-        }\n-\n-      } else if (res->is_fpu_register() && !res->is_xmm_register()) {\n-        \/\/ move from memory\/constant to fpu register\n-        \/\/ result is pushed on the stack\n-\n-        insert_free_if_dead(res);\n-\n-        \/\/ create debug information before register is pushed\n-        compute_debug_information(op1);\n-\n-        do_push(res);\n-        new_res = to_fpu_stack_top(res);\n-      }\n-      break;\n-    }\n-\n-    case lir_convert: {\n-      Bytecodes::Code bc = op1->as_OpConvert()->bytecode();\n-      switch (bc) {\n-        case Bytecodes::_d2f:\n-        case Bytecodes::_f2d:\n-          assert(res->is_fpu_register(), \"must be\");\n-          assert(in->is_fpu_register(), \"must be\");\n-\n-          if (!in->is_xmm_register() && !res->is_xmm_register()) {\n-            \/\/ this is quite the same as a move from fpu-register to fpu-register\n-            \/\/ Note: input and result operands must have different types\n-            if (fpu_num(in) == fpu_num(res)) {\n-              \/\/ nothing to do\n-              new_in = to_fpu_stack(in);\n-            } else if (in->is_last_use()) {\n-              insert_free_if_dead(res);\/\/, in);\n-              new_in = to_fpu_stack(in);\n-              do_rename(in, res);\n-            } else {\n-              insert_free_if_dead(res);\n-              insert_copy(in, res);\n-              new_in = to_fpu_stack_top(in, true);\n-            }\n-            new_res = to_fpu_stack(res);\n-          }\n-\n-          break;\n-\n-        case Bytecodes::_i2f:\n-        case Bytecodes::_l2f:\n-        case Bytecodes::_i2d:\n-        case Bytecodes::_l2d:\n-          assert(res->is_fpu_register(), \"must be\");\n-          if (!res->is_xmm_register()) {\n-            insert_free_if_dead(res);\n-            do_push(res);\n-            new_res = to_fpu_stack_top(res);\n-          }\n-          break;\n-\n-        case Bytecodes::_f2i:\n-        case Bytecodes::_d2i:\n-          assert(in->is_fpu_register(), \"must be\");\n-          if (!in->is_xmm_register()) {\n-            insert_exchange(in);\n-            new_in = to_fpu_stack_top(in);\n-\n-            \/\/ TODO: update registers of stub\n-          }\n-          break;\n-\n-        case Bytecodes::_f2l:\n-        case Bytecodes::_d2l:\n-          assert(in->is_fpu_register(), \"must be\");\n-          if (!in->is_xmm_register()) {\n-            insert_exchange(in);\n-            new_in = to_fpu_stack_top(in);\n-            pop_always(op1, in);\n-          }\n-          break;\n-\n-        case Bytecodes::_i2l:\n-        case Bytecodes::_l2i:\n-        case Bytecodes::_i2b:\n-        case Bytecodes::_i2c:\n-        case Bytecodes::_i2s:\n-          \/\/ no fpu operands\n-          break;\n-\n-        default:\n-          ShouldNotReachHere();\n-      }\n-      break;\n-    }\n-\n-    case lir_roundfp: {\n-      assert(in->is_fpu_register() && !in->is_xmm_register(), \"input must be in register\");\n-      assert(res->is_stack(), \"result must be on stack\");\n-\n-      insert_exchange(in);\n-      new_in = to_fpu_stack_top(in);\n-      pop_if_last_use(op1, in);\n-      break;\n-    }\n-\n-    default: {\n-      assert(!in->is_float_kind() && !res->is_float_kind(), \"missed a fpu-operation\");\n-    }\n-  }\n-\n-  op1->set_in_opr(new_in);\n-  op1->set_result_opr(new_res);\n-}\n-\n-void FpuStackAllocator::handle_op2(LIR_Op2* op2) {\n-  LIR_Opr left  = op2->in_opr1();\n-  if (!left->is_float_kind()) {\n-    return;\n-  }\n-  if (left->is_xmm_register()) {\n-    return;\n-  }\n-\n-  LIR_Opr right = op2->in_opr2();\n-  LIR_Opr res   = op2->result_opr();\n-  LIR_Opr new_left  = left;  \/\/ new operands relative to the actual fpu stack top\n-  LIR_Opr new_right = right;\n-  LIR_Opr new_res   = res;\n-\n-  assert(!left->is_xmm_register() && !right->is_xmm_register() && !res->is_xmm_register(), \"not for xmm registers\");\n-\n-  switch (op2->code()) {\n-    case lir_cmp:\n-    case lir_cmp_fd2i:\n-    case lir_ucmp_fd2i:\n-    case lir_assert: {\n-      assert(left->is_fpu_register(), \"invalid LIR\");\n-      assert(right->is_fpu_register(), \"invalid LIR\");\n-\n-      \/\/ the left-hand side must be on top of stack.\n-      \/\/ the right-hand side is never popped, even if is_last_use is set\n-      insert_exchange(left);\n-      new_left = to_fpu_stack_top(left);\n-      new_right = to_fpu_stack(right);\n-      pop_if_last_use(op2, left);\n-      break;\n-    }\n-\n-    case lir_mul:\n-    case lir_div: {\n-      if (res->is_double_fpu()) {\n-        assert(op2->tmp1_opr()->is_fpu_register(), \"strict operations need temporary fpu stack slot\");\n-        insert_free_if_dead(op2->tmp1_opr());\n-        assert(sim()->stack_size() <= 7, \"at least one stack slot must be free\");\n-      }\n-      \/\/ fall-through: continue with the normal handling of lir_mul and lir_div\n-    }\n-    case lir_add:\n-    case lir_sub: {\n-      assert(left->is_fpu_register(), \"must be\");\n-      assert(res->is_fpu_register(), \"must be\");\n-      assert(left->is_equal(res), \"must be\");\n-\n-      \/\/ either the left-hand or the right-hand side must be on top of stack\n-      \/\/ (if right is not a register, left must be on top)\n-      if (!right->is_fpu_register()) {\n-        insert_exchange(left);\n-        new_left = to_fpu_stack_top(left);\n-      } else {\n-        \/\/ no exchange necessary if right is already on top of stack\n-        if (tos_offset(right) == 0) {\n-          new_left = to_fpu_stack(left);\n-          new_right = to_fpu_stack_top(right);\n-        } else {\n-          insert_exchange(left);\n-          new_left = to_fpu_stack_top(left);\n-          new_right = to_fpu_stack(right);\n-        }\n-\n-        if (right->is_last_use()) {\n-          op2->set_fpu_pop_count(1);\n-\n-          if (tos_offset(right) == 0) {\n-            sim()->pop();\n-          } else {\n-            \/\/ if left is on top of stack, the result is placed in the stack\n-            \/\/ slot of right, so a renaming from right to res is necessary\n-            assert(tos_offset(left) == 0, \"must be\");\n-            sim()->pop();\n-            do_rename(right, res);\n-          }\n-        }\n-      }\n-      new_res = to_fpu_stack(res);\n-\n-      break;\n-    }\n-\n-    case lir_rem: {\n-      assert(left->is_fpu_register(), \"must be\");\n-      assert(right->is_fpu_register(), \"must be\");\n-      assert(res->is_fpu_register(), \"must be\");\n-      assert(left->is_equal(res), \"must be\");\n-\n-      \/\/ Must bring both operands to top of stack with following operand ordering:\n-      \/\/ * fpu stack before rem: ... right left\n-      \/\/ * fpu stack after rem:  ... left\n-      if (tos_offset(right) != 1) {\n-        insert_exchange(right);\n-        insert_exchange(1);\n-      }\n-      insert_exchange(left);\n-      assert(tos_offset(right) == 1, \"check\");\n-      assert(tos_offset(left) == 0, \"check\");\n-\n-      new_left = to_fpu_stack_top(left);\n-      new_right = to_fpu_stack(right);\n-\n-      op2->set_fpu_pop_count(1);\n-      sim()->pop();\n-      do_rename(right, res);\n-\n-      new_res = to_fpu_stack_top(res);\n-      break;\n-    }\n-\n-    case lir_abs:\n-    case lir_sqrt:\n-    case lir_neg: {\n-      \/\/ Right argument appears to be unused\n-      assert(right->is_illegal(), \"must be\");\n-      assert(left->is_fpu_register(), \"must be\");\n-      assert(res->is_fpu_register(), \"must be\");\n-      assert(left->is_last_use(), \"old value gets destroyed\");\n-\n-      insert_free_if_dead(res, left);\n-      insert_exchange(left);\n-      do_rename(left, res);\n-\n-      new_left = to_fpu_stack_top(res);\n-      new_res = new_left;\n-\n-      op2->set_fpu_stack_size(sim()->stack_size());\n-      break;\n-    }\n-\n-    default: {\n-      assert(false, \"missed a fpu-operation\");\n-    }\n-  }\n-\n-  op2->set_in_opr1(new_left);\n-  op2->set_in_opr2(new_right);\n-  op2->set_result_opr(new_res);\n-}\n-\n-void FpuStackAllocator::handle_opCall(LIR_OpCall* opCall) {\n-  LIR_Opr res = opCall->result_opr();\n-\n-  \/\/ clear fpu-stack before call\n-  \/\/ it may contain dead values that could not have been removed by previous operations\n-  clear_fpu_stack(LIR_OprFact::illegalOpr);\n-  assert(sim()->is_empty(), \"fpu stack must be empty now\");\n-\n-  \/\/ compute debug information before (possible) fpu result is pushed\n-  compute_debug_information(opCall);\n-\n-  if (res->is_fpu_register() && !res->is_xmm_register()) {\n-    do_push(res);\n-    opCall->set_result_opr(to_fpu_stack_top(res));\n-  }\n-}\n-\n-#ifndef PRODUCT\n-void FpuStackAllocator::check_invalid_lir_op(LIR_Op* op) {\n-  switch (op->code()) {\n-    case lir_fpop_raw:\n-    case lir_fxch:\n-    case lir_fld:\n-      assert(false, \"operations only inserted by FpuStackAllocator\");\n-      break;\n-\n-    default:\n-      break;\n-  }\n-}\n-#endif\n-\n-\n-void FpuStackAllocator::merge_insert_add(LIR_List* instrs, FpuStackSim* cur_sim, int reg) {\n-  LIR_Op1* move = new LIR_Op1(lir_move, LIR_OprFact::doubleConst(0), LIR_OprFact::double_fpu(reg)->make_fpu_stack_offset());\n-\n-  instrs->instructions_list()->push(move);\n-\n-  cur_sim->push(reg);\n-  move->set_result_opr(to_fpu_stack(move->result_opr()));\n-\n-  #ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->print(\"Added new register: %d         New state: \", reg); cur_sim->print(); tty->cr();\n-    }\n-  #endif\n-}\n-\n-void FpuStackAllocator::merge_insert_xchg(LIR_List* instrs, FpuStackSim* cur_sim, int slot) {\n-  assert(slot > 0, \"no exchange necessary\");\n-\n-  LIR_Op1* fxch = new LIR_Op1(lir_fxch, LIR_OprFact::intConst(slot));\n-  instrs->instructions_list()->push(fxch);\n-  cur_sim->swap(slot);\n-\n-  #ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->print(\"Exchanged register: %d         New state: \", cur_sim->get_slot(slot)); cur_sim->print(); tty->cr();\n-    }\n-  #endif\n-}\n-\n-void FpuStackAllocator::merge_insert_pop(LIR_List* instrs, FpuStackSim* cur_sim) {\n-  int reg = cur_sim->get_slot(0);\n-\n-  LIR_Op* fpop = new LIR_Op0(lir_fpop_raw);\n-  instrs->instructions_list()->push(fpop);\n-  cur_sim->pop(reg);\n-\n-  #ifndef PRODUCT\n-    if (TraceFPUStack) {\n-      tty->print(\"Removed register: %d           New state: \", reg); cur_sim->print(); tty->cr();\n-    }\n-  #endif\n-}\n-\n-bool FpuStackAllocator::merge_rename(FpuStackSim* cur_sim, FpuStackSim* sux_sim, int start_slot, int change_slot) {\n-  int reg = cur_sim->get_slot(change_slot);\n-\n-  for (int slot = start_slot; slot >= 0; slot--) {\n-    int new_reg = sux_sim->get_slot(slot);\n-\n-    if (!cur_sim->contains(new_reg)) {\n-      cur_sim->set_slot(change_slot, new_reg);\n-\n-      #ifndef PRODUCT\n-        if (TraceFPUStack) {\n-          tty->print(\"Renamed register %d to %d       New state: \", reg, new_reg); cur_sim->print(); tty->cr();\n-        }\n-      #endif\n-\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-\n-void FpuStackAllocator::merge_fpu_stack(LIR_List* instrs, FpuStackSim* cur_sim, FpuStackSim* sux_sim) {\n-#ifndef PRODUCT\n-  if (TraceFPUStack) {\n-    tty->cr();\n-    tty->print(\"before merging: pred: \"); cur_sim->print(); tty->cr();\n-    tty->print(\"                 sux: \"); sux_sim->print(); tty->cr();\n-  }\n-\n-  int slot;\n-  for (slot = 0; slot < cur_sim->stack_size(); slot++) {\n-    assert(!cur_sim->slot_is_empty(slot), \"not handled by algorithm\");\n-  }\n-  for (slot = 0; slot < sux_sim->stack_size(); slot++) {\n-    assert(!sux_sim->slot_is_empty(slot), \"not handled by algorithm\");\n-  }\n-#endif\n-\n-  \/\/ size difference between cur and sux that must be resolved by adding or removing values form the stack\n-  int size_diff = cur_sim->stack_size() - sux_sim->stack_size();\n-\n-  if (!ComputeExactFPURegisterUsage) {\n-    \/\/ add slots that are currently free, but used in successor\n-    \/\/ When the exact FPU register usage is computed, the stack does\n-    \/\/ not contain dead values at merging -> no values must be added\n-\n-    int sux_slot = sux_sim->stack_size() - 1;\n-    while (size_diff < 0) {\n-      assert(sux_slot >= 0, \"slot out of bounds -> error in algorithm\");\n-\n-      int reg = sux_sim->get_slot(sux_slot);\n-      if (!cur_sim->contains(reg)) {\n-        merge_insert_add(instrs, cur_sim, reg);\n-        size_diff++;\n-\n-        if (sux_slot + size_diff != 0) {\n-          merge_insert_xchg(instrs, cur_sim, sux_slot + size_diff);\n-        }\n-      }\n-     sux_slot--;\n-    }\n-  }\n-\n-  assert(cur_sim->stack_size() >= sux_sim->stack_size(), \"stack size must be equal or greater now\");\n-  assert(size_diff == cur_sim->stack_size() - sux_sim->stack_size(), \"must be\");\n-\n-  \/\/ stack merge algorithm:\n-  \/\/ 1) as long as the current stack top is not in the right location (that means\n-  \/\/    it should not be on the stack top), exchange it into the right location\n-  \/\/ 2) if the stack top is right, but the remaining stack is not ordered correctly,\n-  \/\/    the stack top is exchanged away to get another value on top ->\n-  \/\/    now step 1) can be continued\n-  \/\/ the stack can also contain unused items -> these items are removed from stack\n-\n-  int finished_slot = sux_sim->stack_size() - 1;\n-  while (finished_slot >= 0 || size_diff > 0) {\n-    while (size_diff > 0 || (cur_sim->stack_size() > 0 && cur_sim->get_slot(0) != sux_sim->get_slot(0))) {\n-      int reg = cur_sim->get_slot(0);\n-      if (sux_sim->contains(reg)) {\n-        int sux_slot = sux_sim->offset_from_tos(reg);\n-        merge_insert_xchg(instrs, cur_sim, sux_slot + size_diff);\n-\n-      } else if (!merge_rename(cur_sim, sux_sim, finished_slot, 0)) {\n-        assert(size_diff > 0, \"must be\");\n-\n-        merge_insert_pop(instrs, cur_sim);\n-        size_diff--;\n-      }\n-      assert(cur_sim->stack_size() == 0 || cur_sim->get_slot(0) != reg, \"register must have been changed\");\n-    }\n-\n-    while (finished_slot >= 0 && cur_sim->get_slot(finished_slot) == sux_sim->get_slot(finished_slot)) {\n-      finished_slot--;\n-    }\n-\n-    if (finished_slot >= 0) {\n-      int reg = cur_sim->get_slot(finished_slot);\n-\n-      if (sux_sim->contains(reg) || !merge_rename(cur_sim, sux_sim, finished_slot, finished_slot)) {\n-        assert(sux_sim->contains(reg) || size_diff > 0, \"must be\");\n-        merge_insert_xchg(instrs, cur_sim, finished_slot);\n-      }\n-      assert(cur_sim->get_slot(finished_slot) != reg, \"register must have been changed\");\n-    }\n-  }\n-\n-#ifndef PRODUCT\n-  if (TraceFPUStack) {\n-    tty->print(\"after merging:  pred: \"); cur_sim->print(); tty->cr();\n-    tty->print(\"                 sux: \"); sux_sim->print(); tty->cr();\n-    tty->cr();\n-  }\n-#endif\n-  assert(cur_sim->stack_size() == sux_sim->stack_size(), \"stack size must be equal now\");\n-}\n-\n-\n-void FpuStackAllocator::merge_cleanup_fpu_stack(LIR_List* instrs, FpuStackSim* cur_sim, BitMap& live_fpu_regs) {\n-#ifndef PRODUCT\n-  if (TraceFPUStack) {\n-    tty->cr();\n-    tty->print(\"before cleanup: state: \"); cur_sim->print(); tty->cr();\n-    tty->print(\"                live:  \"); live_fpu_regs.print_on(tty); tty->cr();\n-  }\n-#endif\n-\n-  int slot = 0;\n-  while (slot < cur_sim->stack_size()) {\n-    int reg = cur_sim->get_slot(slot);\n-    if (!live_fpu_regs.at(reg)) {\n-      if (slot != 0) {\n-        merge_insert_xchg(instrs, cur_sim, slot);\n-      }\n-      merge_insert_pop(instrs, cur_sim);\n-    } else {\n-      slot++;\n-    }\n-  }\n-\n-#ifndef PRODUCT\n-  if (TraceFPUStack) {\n-    tty->print(\"after cleanup:  state: \"); cur_sim->print(); tty->cr();\n-    tty->print(\"                live:  \"); live_fpu_regs.print_on(tty); tty->cr();\n-    tty->cr();\n-  }\n-\n-  \/\/ check if fpu stack only contains live registers\n-  for (unsigned int i = 0; i < live_fpu_regs.size(); i++) {\n-    if (live_fpu_regs.at(i) != cur_sim->contains(i)) {\n-      tty->print_cr(\"mismatch between required and actual stack content\");\n-      break;\n-    }\n-  }\n-#endif\n-}\n-\n-\n-bool FpuStackAllocator::merge_fpu_stack_with_successors(BlockBegin* block) {\n-#ifndef PRODUCT\n-  if (TraceFPUStack) {\n-    tty->print_cr(\"Propagating FPU stack state for B%d at LIR_Op position %d to successors:\",\n-                  block->block_id(), pos());\n-    sim()->print();\n-    tty->cr();\n-  }\n-#endif\n-\n-  bool changed = false;\n-  int number_of_sux = block->number_of_sux();\n-\n-  if (number_of_sux == 1 && block->sux_at(0)->number_of_preds() > 1) {\n-    \/\/ The successor has at least two incoming edges, so a stack merge will be necessary\n-    \/\/ If this block is the first predecessor, cleanup the current stack and propagate it\n-    \/\/ If this block is not the first predecessor, a stack merge will be necessary\n-\n-    BlockBegin* sux = block->sux_at(0);\n-    intArray* state = sux->fpu_stack_state();\n-    LIR_List* instrs = new LIR_List(_compilation);\n-\n-    if (state != nullptr) {\n-      \/\/ Merge with a successors that already has a FPU stack state\n-      \/\/ the block must only have one successor because critical edges must been split\n-      FpuStackSim* cur_sim = sim();\n-      FpuStackSim* sux_sim = temp_sim();\n-      sux_sim->read_state(state);\n-\n-      merge_fpu_stack(instrs, cur_sim, sux_sim);\n-\n-    } else {\n-      \/\/ propagate current FPU stack state to successor without state\n-      \/\/ clean up stack first so that there are no dead values on the stack\n-      if (ComputeExactFPURegisterUsage) {\n-        FpuStackSim* cur_sim = sim();\n-        ResourceBitMap live_fpu_regs = block->sux_at(0)->fpu_register_usage();\n-        assert(live_fpu_regs.size() == FrameMap::nof_fpu_regs, \"missing register usage\");\n-\n-        merge_cleanup_fpu_stack(instrs, cur_sim, live_fpu_regs);\n-      }\n-\n-      intArray* state = sim()->write_state();\n-      if (TraceFPUStack) {\n-        tty->print_cr(\"Setting FPU stack state of B%d (merge path)\", sux->block_id());\n-        sim()->print(); tty->cr();\n-      }\n-      sux->set_fpu_stack_state(state);\n-    }\n-\n-    if (instrs->instructions_list()->length() > 0) {\n-      lir()->insert_before(pos(), instrs);\n-      set_pos(instrs->instructions_list()->length() + pos());\n-      changed = true;\n-    }\n-\n-  } else {\n-    \/\/ Propagate unmodified Stack to successors where a stack merge is not necessary\n-    intArray* state = sim()->write_state();\n-    for (int i = 0; i < number_of_sux; i++) {\n-      BlockBegin* sux = block->sux_at(i);\n-\n-#ifdef ASSERT\n-      for (int j = 0; j < sux->number_of_preds(); j++) {\n-        assert(block == sux->pred_at(j), \"all critical edges must be broken\");\n-      }\n-\n-      \/\/ check if new state is same\n-      if (sux->fpu_stack_state() != nullptr) {\n-        intArray* sux_state = sux->fpu_stack_state();\n-        assert(state->length() == sux_state->length(), \"overwriting existing stack state\");\n-        for (int j = 0; j < state->length(); j++) {\n-          assert(state->at(j) == sux_state->at(j), \"overwriting existing stack state\");\n-        }\n-      }\n-#endif\n-#ifndef PRODUCT\n-      if (TraceFPUStack) {\n-        tty->print_cr(\"Setting FPU stack state of B%d\", sux->block_id());\n-        sim()->print(); tty->cr();\n-      }\n-#endif\n-\n-      sux->set_fpu_stack_state(state);\n-    }\n-  }\n-\n-#ifndef PRODUCT\n-  \/\/ assertions that FPU stack state conforms to all successors' states\n-  intArray* cur_state = sim()->write_state();\n-  for (int i = 0; i < number_of_sux; i++) {\n-    BlockBegin* sux = block->sux_at(i);\n-    intArray* sux_state = sux->fpu_stack_state();\n-\n-    assert(sux_state != nullptr, \"no fpu state\");\n-    assert(cur_state->length() == sux_state->length(), \"incorrect length\");\n-    for (int i = 0; i < cur_state->length(); i++) {\n-      assert(cur_state->at(i) == sux_state->at(i), \"element not equal\");\n-    }\n-  }\n-#endif\n-\n-  return changed;\n-}\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/c1_LinearScan_x86.cpp","additions":0,"deletions":1127,"binary":false,"changes":1127,"status":"deleted"},{"patch":"@@ -29,6 +29,0 @@\n-#ifndef _LP64\n-  \/\/ rsp and rbp (numbers 6 ancd 7) are ignored\n-  assert(FrameMap::rsp_opr->cpu_regnr() == 6, \"wrong assumption below\");\n-  assert(FrameMap::rbp_opr->cpu_regnr() == 7, \"wrong assumption below\");\n-  assert(reg_num >= 0, \"invalid reg_num\");\n-#else\n@@ -43,1 +37,0 @@\n-#endif \/\/ _LP64\n@@ -48,5 +41,0 @@\n-  \/\/ Intel requires two cpu registers for long,\n-  \/\/ but requires only one fpu register for double\n-  if (LP64_ONLY(false &&) type == T_LONG) {\n-    return 2;\n-  }\n@@ -80,1 +68,0 @@\n-      if (UseSSE > 0) {\n@@ -82,3 +69,3 @@\n-        if (TraceLinearScanLevel >= 2) {\n-          tty->print_cr(\"killing XMMs for trig\");\n-        }\n+      if (TraceLinearScanLevel >= 2) {\n+        tty->print_cr(\"killing XMMs for trig\");\n+      }\n@@ -86,6 +73,5 @@\n-        int num_caller_save_xmm_regs = FrameMap::get_num_caller_save_xmms();\n-        int op_id = op->id();\n-        for (int xmm = 0; xmm < num_caller_save_xmm_regs; xmm++) {\n-          LIR_Opr opr = FrameMap::caller_save_xmm_reg_at(xmm);\n-          add_temp(reg_num(opr), op_id, noUse, T_ILLEGAL);\n-        }\n+      int num_caller_save_xmm_regs = FrameMap::get_num_caller_save_xmms();\n+      int op_id = op->id();\n+      for (int xmm = 0; xmm < num_caller_save_xmm_regs; xmm++) {\n+        LIR_Opr opr = FrameMap::caller_save_xmm_reg_at(xmm);\n+        add_temp(reg_num(opr), op_id, noUse, T_ILLEGAL);\n@@ -110,1 +96,1 @@\n-  } else if ((UseSSE >= 1 && cur->type() == T_FLOAT) || (UseSSE >= 2 && cur->type() == T_DOUBLE)) {\n+  } else if ((cur->type() == T_FLOAT) || (cur->type() == T_DOUBLE)) {\n@@ -119,74 +105,0 @@\n-\n-class FpuStackAllocator {\n- private:\n-  Compilation* _compilation;\n-  LinearScan* _allocator;\n-\n-  LIR_OpVisitState visitor;\n-\n-  LIR_List* _lir;\n-  int _pos;\n-  FpuStackSim _sim;\n-  FpuStackSim _temp_sim;\n-\n-  bool _debug_information_computed;\n-\n-  LinearScan*   allocator()                      { return _allocator; }\n-  Compilation*  compilation() const              { return _compilation; }\n-\n-  \/\/ unified bailout support\n-  void          bailout(const char* msg) const   { compilation()->bailout(msg); }\n-  bool          bailed_out() const               { return compilation()->bailed_out(); }\n-\n-  int pos() { return _pos; }\n-  void set_pos(int pos) { _pos = pos; }\n-  LIR_Op* cur_op() { return lir()->instructions_list()->at(pos()); }\n-  LIR_List* lir() { return _lir; }\n-  void set_lir(LIR_List* lir) { _lir = lir; }\n-  FpuStackSim* sim() { return &_sim; }\n-  FpuStackSim* temp_sim() { return &_temp_sim; }\n-\n-  int fpu_num(LIR_Opr opr);\n-  int tos_offset(LIR_Opr opr);\n-  LIR_Opr to_fpu_stack_top(LIR_Opr opr, bool dont_check_offset = false);\n-\n-  \/\/ Helper functions for handling operations\n-  void insert_op(LIR_Op* op);\n-  void insert_exchange(int offset);\n-  void insert_exchange(LIR_Opr opr);\n-  void insert_free(int offset);\n-  void insert_free_if_dead(LIR_Opr opr);\n-  void insert_free_if_dead(LIR_Opr opr, LIR_Opr ignore);\n-  void insert_copy(LIR_Opr from, LIR_Opr to);\n-  void do_rename(LIR_Opr from, LIR_Opr to);\n-  void do_push(LIR_Opr opr);\n-  void pop_if_last_use(LIR_Op* op, LIR_Opr opr);\n-  void pop_always(LIR_Op* op, LIR_Opr opr);\n-  void clear_fpu_stack(LIR_Opr preserve);\n-  void handle_op1(LIR_Op1* op1);\n-  void handle_op2(LIR_Op2* op2);\n-  void handle_opCall(LIR_OpCall* opCall);\n-  void compute_debug_information(LIR_Op* op);\n-  void allocate_exception_handler(XHandler* xhandler);\n-  void allocate_block(BlockBegin* block);\n-\n-#ifndef PRODUCT\n-  void check_invalid_lir_op(LIR_Op* op);\n-#endif\n-\n-  \/\/ Helper functions for merging of fpu stacks\n-  void merge_insert_add(LIR_List* instrs, FpuStackSim* cur_sim, int reg);\n-  void merge_insert_xchg(LIR_List* instrs, FpuStackSim* cur_sim, int slot);\n-  void merge_insert_pop(LIR_List* instrs, FpuStackSim* cur_sim);\n-  bool merge_rename(FpuStackSim* cur_sim, FpuStackSim* sux_sim, int start_slot, int change_slot);\n-  void merge_fpu_stack(LIR_List* instrs, FpuStackSim* cur_sim, FpuStackSim* sux_sim);\n-  void merge_cleanup_fpu_stack(LIR_List* instrs, FpuStackSim* cur_sim, BitMap& live_fpu_regs);\n-  bool merge_fpu_stack_with_successors(BlockBegin* block);\n-\n- public:\n-  LIR_Opr to_fpu_stack(LIR_Opr opr); \/\/ used by LinearScan for creation of debug information\n-\n-  FpuStackAllocator(Compilation* compilation, LinearScan* allocator);\n-  void allocate();\n-};\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LinearScan_x86.hpp","additions":9,"deletions":97,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -66,9 +66,1 @@\n-#ifdef _LP64\n-    const Register thread = r15_thread;\n-    lightweight_lock(disp_hdr, obj, hdr, thread, tmp, slow_case);\n-#else\n-    \/\/ Implicit null check.\n-    movptr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n-    \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-    jmp(slow_case);\n-#endif\n+    lightweight_lock(disp_hdr, obj, hdr, r15_thread, tmp, slow_case);\n@@ -139,1 +131,0 @@\n-#ifdef _LP64\n@@ -141,4 +132,0 @@\n-#else\n-    \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-    jmp(slow_case);\n-#endif\n@@ -173,1 +160,0 @@\n-#ifdef _LP64\n@@ -182,3 +168,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -191,1 +175,0 @@\n-#ifdef _LP64\n@@ -199,4 +182,1 @@\n-#endif\n-  }\n-#ifdef _LP64\n-  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -206,1 +186,0 @@\n-#endif\n@@ -269,2 +248,0 @@\n-        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (2*BytesPerWord)),\n-               t1_zero);)\n@@ -336,6 +313,0 @@\n-#if !defined(_LP64) && defined(COMPILER2)\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n-    \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-    empty_FPU_stack();\n-  }\n-#endif \/\/ !_LP64 && COMPILER2\n@@ -357,1 +328,1 @@\n-  if (breakAtEntry || VerifyFPU) {\n+  if (breakAtEntry) {\n@@ -361,1 +332,1 @@\n-    \/\/ Breakpoint and VerifyFPU have one byte first instruction.\n+    \/\/ Breakpoint has one byte first instruction.\n@@ -368,2 +339,0 @@\n-  \/\/ build frame\n-  IA32_ONLY( verify_FPU(0, \"method_entry\"); )\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":5,"deletions":36,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread); \/\/ is callee-saved register (Visual C++ calling conventions)\n+  const Register thread = r15_thread; \/\/ is callee-saved register (Visual C++ calling conventions)\n@@ -60,1 +60,0 @@\n-#ifdef _LP64\n@@ -64,1 +63,0 @@\n-#endif\n@@ -66,1 +64,0 @@\n-#ifdef _LP64\n@@ -69,7 +66,0 @@\n-#else\n-  set_num_rt_args(1 + args_size);\n-\n-  \/\/ push java thread (becomes first argument of C function)\n-  get_thread(thread);\n-  push(thread);\n-#endif \/\/ _LP64\n@@ -108,3 +98,0 @@\n-  \/\/ discard thread and arguments\n-  NOT_LP64(addptr(rsp, num_rt_args()*BytesPerWord));\n-\n@@ -148,1 +135,0 @@\n-#ifdef _LP64\n@@ -150,3 +136,0 @@\n-#else\n-  push(arg1);\n-#endif \/\/ _LP64\n@@ -158,1 +141,0 @@\n-#ifdef _LP64\n@@ -170,4 +152,0 @@\n-#else\n-  push(arg2);\n-  push(arg1);\n-#endif \/\/ _LP64\n@@ -179,1 +157,0 @@\n-#ifdef _LP64\n@@ -195,5 +172,0 @@\n-#else\n-  push(arg3);\n-  push(arg2);\n-  push(arg1);\n-#endif \/\/ _LP64\n@@ -266,7 +238,2 @@\n-#ifdef _LP64\n-  #define SLOT2(x) x,\n-  #define SLOT_PER_WORD 2\n-#else\n-  #define SLOT2(x)\n-  #define SLOT_PER_WORD 1\n-#endif \/\/ _LP64\n+#define SLOT2(x) x,\n+#define SLOT_PER_WORD 2\n@@ -277,1 +244,0 @@\n-#ifdef _LP64\n@@ -279,1 +245,0 @@\n-#endif \/\/ _LP64\n@@ -295,1 +260,0 @@\n-#ifdef _LP64\n@@ -305,3 +269,0 @@\n-#else\n-  rdi_off = extra_space_offset,\n-#endif \/\/ _LP64\n@@ -333,2 +294,2 @@\n-  LP64_ONLY(num_rt_args = 0);\n-  LP64_ONLY(assert((reg_save_frame_size * VMRegImpl::stack_slot_size) % 16 == 0, \"must be 16 byte aligned\");)\n+  num_rt_args = 0;\n+  assert((reg_save_frame_size * VMRegImpl::stack_slot_size) % 16 == 0, \"must be 16 byte aligned\");\n@@ -347,1 +308,0 @@\n-#ifdef _LP64\n@@ -373,1 +333,0 @@\n-#endif \/\/ _LP64\n@@ -378,6 +337,5 @@\n-#ifndef _LP64\n-    if (UseSSE < 2) {\n-      int fpu_off = float_regs_as_doubles_off;\n-      for (int n = 0; n < FrameMap::nof_fpu_regs; n++) {\n-        VMReg fpu_name_0 = FrameMap::fpu_regname(n);\n-        map->set_callee_saved(VMRegImpl::stack2reg(fpu_off +     num_rt_args), fpu_name_0);\n+    int xmm_off = xmm_regs_as_doubles_off;\n+    for (int n = 0; n < FrameMap::nof_xmm_regs; n++) {\n+      if (n < xmm_bypass_limit) {\n+        VMReg xmm_name_0 = as_XMMRegister(n)->as_VMReg();\n+        map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + num_rt_args), xmm_name_0);\n@@ -385,15 +343,1 @@\n-        if (true) {\n-          map->set_callee_saved(VMRegImpl::stack2reg(fpu_off + 1 + num_rt_args), fpu_name_0->next());\n-        }\n-        fpu_off += 2;\n-      }\n-      assert(fpu_off == fpu_state_off, \"incorrect number of fpu stack slots\");\n-\n-      if (UseSSE == 1) {\n-        int xmm_off = xmm_regs_as_doubles_off;\n-        for (int n = 0; n < FrameMap::nof_fpu_regs; n++) {\n-          VMReg xmm_name_0 = as_XMMRegister(n)->as_VMReg();\n-          map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + num_rt_args), xmm_name_0);\n-          xmm_off += 2;\n-        }\n-        assert(xmm_off == float_regs_as_doubles_off, \"incorrect number of xmm registers\");\n+        map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + 1 + num_rt_args), xmm_name_0->next());\n@@ -401,0 +345,1 @@\n+      xmm_off += 2;\n@@ -402,17 +347,1 @@\n-#endif \/\/ !LP64\n-\n-    if (UseSSE >= 2) {\n-      int xmm_off = xmm_regs_as_doubles_off;\n-      for (int n = 0; n < FrameMap::nof_xmm_regs; n++) {\n-        if (n < xmm_bypass_limit) {\n-          VMReg xmm_name_0 = as_XMMRegister(n)->as_VMReg();\n-          map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + num_rt_args), xmm_name_0);\n-          \/\/ %%% This is really a waste but we'll keep things as they were for now\n-          if (true) {\n-            map->set_callee_saved(VMRegImpl::stack2reg(xmm_off + 1 + num_rt_args), xmm_name_0->next());\n-          }\n-        }\n-        xmm_off += 2;\n-      }\n-      assert(xmm_off == float_regs_as_doubles_off, \"incorrect number of xmm registers\");\n-    }\n+    assert(xmm_off == float_regs_as_doubles_off, \"incorrect number of xmm registers\");\n@@ -430,1 +359,0 @@\n-#ifdef _LP64\n@@ -432,3 +360,0 @@\n-#else\n-  __ pusha();\n-#endif\n@@ -446,53 +371,11 @@\n-#ifndef _LP64\n-    if (UseSSE < 2) {\n-      \/\/ save FPU stack\n-      __ fnsave(Address(rsp, fpu_state_off * VMRegImpl::stack_slot_size));\n-      __ fwait();\n-\n-#ifdef ASSERT\n-      Label ok;\n-      __ cmpw(Address(rsp, fpu_state_off * VMRegImpl::stack_slot_size), StubRoutines::x86::fpu_cntrl_wrd_std());\n-      __ jccb(Assembler::equal, ok);\n-      __ stop(\"corrupted control word detected\");\n-      __ bind(ok);\n-#endif\n-\n-      \/\/ Reset the control word to guard against exceptions being unmasked\n-      \/\/ since fstp_d can cause FPU stack underflow exceptions.  Write it\n-      \/\/ into the on stack copy and then reload that to make sure that the\n-      \/\/ current and future values are correct.\n-      __ movw(Address(rsp, fpu_state_off * VMRegImpl::stack_slot_size), StubRoutines::x86::fpu_cntrl_wrd_std());\n-      __ frstor(Address(rsp, fpu_state_off * VMRegImpl::stack_slot_size));\n-\n-      \/\/ Save the FPU registers in de-opt-able form\n-      int offset = 0;\n-      for (int n = 0; n < FrameMap::nof_fpu_regs; n++) {\n-        __ fstp_d(Address(rsp, float_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset));\n-        offset += 8;\n-      }\n-\n-      if (UseSSE == 1) {\n-        \/\/ save XMM registers as float because double not supported without SSE2(num MMX == num fpu)\n-        int offset = 0;\n-        for (int n = 0; n < FrameMap::nof_fpu_regs; n++) {\n-          XMMRegister xmm_name = as_XMMRegister(n);\n-          __ movflt(Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset), xmm_name);\n-          offset += 8;\n-        }\n-      }\n-    }\n-#endif \/\/ !_LP64\n-\n-    if (UseSSE >= 2) {\n-      \/\/ save XMM registers\n-      \/\/ XMM registers can contain float or double values, but this is not known here,\n-      \/\/ so always save them as doubles.\n-      \/\/ note that float values are _not_ converted automatically, so for float values\n-      \/\/ the second word contains only garbage data.\n-      int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n-      int offset = 0;\n-      for (int n = 0; n < xmm_bypass_limit; n++) {\n-        XMMRegister xmm_name = as_XMMRegister(n);\n-        __ movdbl(Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset), xmm_name);\n-        offset += 8;\n-      }\n+    \/\/ save XMM registers\n+    \/\/ XMM registers can contain float or double values, but this is not known here,\n+    \/\/ so always save them as doubles.\n+    \/\/ note that float values are _not_ converted automatically, so for float values\n+    \/\/ the second word contains only garbage data.\n+    int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n+    int offset = 0;\n+    for (int n = 0; n < xmm_bypass_limit; n++) {\n+      XMMRegister xmm_name = as_XMMRegister(n);\n+      __ movdbl(Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset), xmm_name);\n+      offset += 8;\n@@ -501,3 +384,0 @@\n-\n-  \/\/ FPU stack must be empty now\n-  NOT_LP64( __ verify_FPU(0, \"save_live_registers\"); )\n@@ -510,1 +390,0 @@\n-#ifdef _LP64\n@@ -521,32 +400,0 @@\n-#else\n-  if (restore_fpu_registers) {\n-    if (UseSSE >= 2) {\n-      \/\/ restore XMM registers\n-      int xmm_bypass_limit = FrameMap::nof_xmm_regs;\n-      int offset = 0;\n-      for (int n = 0; n < xmm_bypass_limit; n++) {\n-        XMMRegister xmm_name = as_XMMRegister(n);\n-        __ movdbl(xmm_name, Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset));\n-        offset += 8;\n-      }\n-    } else if (UseSSE == 1) {\n-      \/\/ restore XMM registers(num MMX == num fpu)\n-      int offset = 0;\n-      for (int n = 0; n < FrameMap::nof_fpu_regs; n++) {\n-        XMMRegister xmm_name = as_XMMRegister(n);\n-        __ movflt(xmm_name, Address(rsp, xmm_regs_as_doubles_off * VMRegImpl::stack_slot_size + offset));\n-        offset += 8;\n-      }\n-    }\n-\n-    if (UseSSE < 2) {\n-      __ frstor(Address(rsp, fpu_state_off * VMRegImpl::stack_slot_size));\n-    } else {\n-      \/\/ check that FPU stack is really empty\n-      __ verify_FPU(0, \"restore_live_registers\");\n-    }\n-  } else {\n-    \/\/ check that FPU stack is really empty\n-    __ verify_FPU(0, \"restore_live_registers\");\n-  }\n-#endif \/\/ _LP64\n@@ -574,1 +421,0 @@\n-#ifdef _LP64\n@@ -576,4 +422,0 @@\n-#else\n-  __ popa();\n-#endif\n-\n@@ -588,1 +430,0 @@\n-#ifdef _LP64\n@@ -606,11 +447,0 @@\n-#else\n-\n-  __ pop(rdi);\n-  __ pop(rsi);\n-  __ pop(rbp);\n-  __ pop(rbx); \/\/ skip this value\n-  __ pop(rbx);\n-  __ pop(rdx);\n-  __ pop(rcx);\n-  __ addptr(rsp, BytesPerWord);\n-#endif \/\/ _LP64\n@@ -643,1 +473,0 @@\n-#ifdef _LP64\n@@ -645,4 +474,0 @@\n-#else\n-  Unimplemented();\n-  return 0;\n-#endif\n@@ -668,1 +493,0 @@\n-#ifdef _LP64\n@@ -671,6 +495,0 @@\n-#else\n-    __ movptr(temp_reg, Address(rbp, 3*BytesPerWord));\n-    __ push(temp_reg);\n-    __ movptr(temp_reg, Address(rbp, 2*BytesPerWord));\n-    __ push(temp_reg);\n-#endif \/\/ _LP64\n@@ -696,1 +514,1 @@\n-  const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -729,1 +547,1 @@\n-    const int frame_size = 2 \/*BP, return address*\/ NOT_LP64(+ 1 \/*thread*\/) WIN64_ONLY(+ frame::arg_reg_save_area_bytes \/ BytesPerWord);\n+    const int frame_size = 2 \/*BP, return address*\/ WIN64_ONLY(+ frame::arg_reg_save_area_bytes \/ BytesPerWord);\n@@ -738,7 +556,0 @@\n-#if !defined(_LP64) && defined(COMPILER2)\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n-    \/\/ C2 can leave the fpu stack dirty\n-    __ empty_FPU_stack();\n-  }\n-#endif \/\/ !_LP64 && COMPILER2\n-\n@@ -750,3 +561,0 @@\n-  \/\/ load address of JavaThread object for thread-local data\n-  NOT_LP64(__ get_thread(thread);)\n-\n@@ -819,1 +627,1 @@\n-  const Register exception_oop_callee_saved = NOT_LP64(rsi) LP64_ONLY(r14);\n+  const Register exception_oop_callee_saved = r14;\n@@ -823,1 +631,1 @@\n-  const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -838,1 +646,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n@@ -852,3 +659,0 @@\n-  \/\/ clear the FPU stack in case any FPU results are left behind\n-  NOT_LP64( __ empty_FPU_stack(); )\n-\n@@ -859,1 +663,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n@@ -909,1 +712,0 @@\n-#ifdef _LP64\n@@ -913,8 +715,0 @@\n-#else\n-  __ push(rax); \/\/ push dummy\n-\n-  const Register thread = rdi; \/\/ is callee-saved register (Visual C++ calling conventions)\n-  \/\/ push java thread (becomes first argument of C function)\n-  __ get_thread(thread);\n-  __ push(thread);\n-#endif \/\/ _LP64\n@@ -940,4 +734,0 @@\n-#ifndef _LP64\n-  __ pop(rcx); \/\/ discard thread arg\n-  __ pop(rcx); \/\/ discard dummy\n-#endif \/\/ _LP64\n@@ -1170,1 +960,0 @@\n-#ifdef _LP64\n@@ -1173,6 +962,0 @@\n-#else\n-        \/\/ The object is passed on the stack and we haven't pushed a\n-        \/\/ frame yet so it's one work away from top of stack.\n-        __ movptr(rax, Address(rsp, 1 * BytesPerWord));\n-        __ verify_oop(rax);\n-#endif \/\/ _LP64\n@@ -1417,1 +1200,1 @@\n-        __ NOT_LP64(push(rax)) LP64_ONLY(mov(c_rarg0, rax));\n+        __ mov(c_rarg0, rax);\n@@ -1419,1 +1202,0 @@\n-        NOT_LP64(__ pop(rax));\n@@ -1427,1 +1209,0 @@\n-#ifdef _LP64\n@@ -1439,72 +1220,0 @@\n-#else\n-        \/\/ rax, and rdx are destroyed, but should be free since the result is returned there\n-        \/\/ preserve rsi,ecx\n-        __ push(rsi);\n-        __ push(rcx);\n-\n-        \/\/ check for NaN\n-        Label return0, do_return, return_min_jlong, do_convert;\n-\n-        Address value_high_word(rsp, wordSize + 4);\n-        Address value_low_word(rsp, wordSize);\n-        Address result_high_word(rsp, 3*wordSize + 4);\n-        Address result_low_word(rsp, 3*wordSize);\n-\n-        __ subptr(rsp, 32);                    \/\/ more than enough on 32bit\n-        __ fst_d(value_low_word);\n-        __ movl(rax, value_high_word);\n-        __ andl(rax, 0x7ff00000);\n-        __ cmpl(rax, 0x7ff00000);\n-        __ jcc(Assembler::notEqual, do_convert);\n-        __ movl(rax, value_high_word);\n-        __ andl(rax, 0xfffff);\n-        __ orl(rax, value_low_word);\n-        __ jcc(Assembler::notZero, return0);\n-\n-        __ bind(do_convert);\n-        __ fnstcw(Address(rsp, 0));\n-        __ movzwl(rax, Address(rsp, 0));\n-        __ orl(rax, 0xc00);\n-        __ movw(Address(rsp, 2), rax);\n-        __ fldcw(Address(rsp, 2));\n-        __ fwait();\n-        __ fistp_d(result_low_word);\n-        __ fldcw(Address(rsp, 0));\n-        __ fwait();\n-        \/\/ This gets the entire long in rax on 64bit\n-        __ movptr(rax, result_low_word);\n-        \/\/ testing of high bits\n-        __ movl(rdx, result_high_word);\n-        __ mov(rcx, rax);\n-        \/\/ What the heck is the point of the next instruction???\n-        __ xorl(rcx, 0x0);\n-        __ movl(rsi, 0x80000000);\n-        __ xorl(rsi, rdx);\n-        __ orl(rcx, rsi);\n-        __ jcc(Assembler::notEqual, do_return);\n-        __ fldz();\n-        __ fcomp_d(value_low_word);\n-        __ fnstsw_ax();\n-        __ sahf();\n-        __ jcc(Assembler::above, return_min_jlong);\n-        \/\/ return max_jlong\n-        __ movl(rdx, 0x7fffffff);\n-        __ movl(rax, 0xffffffff);\n-        __ jmp(do_return);\n-\n-        __ bind(return_min_jlong);\n-        __ movl(rdx, 0x80000000);\n-        __ xorl(rax, rax);\n-        __ jmp(do_return);\n-\n-        __ bind(return0);\n-        __ fpop();\n-        __ xorptr(rdx,rdx);\n-        __ xorptr(rax,rax);\n-\n-        __ bind(do_return);\n-        __ addptr(rsp, 32);\n-        __ pop(rcx);\n-        __ pop(rsi);\n-        __ ret(0);\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":29,"deletions":320,"binary":false,"changes":349,"status":"modified"},{"patch":"@@ -47,1 +47,0 @@\n-#ifdef _LP64\n@@ -50,13 +49,0 @@\n-#else\n-  const Register tmp1 = rcx;\n-  const Register tmp2 = rdx;\n-  __ push(tmp1);\n-  __ push(tmp2);\n-\n-  __ lea(tmp1, safepoint_pc);\n-  __ get_thread(tmp2);\n-  __ movptr(Address(tmp2, JavaThread::saved_exception_pc_offset()), tmp1);\n-\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool is_stub) {\n@@ -111,10 +111,0 @@\n-#ifndef _LP64\n-  \/\/ If method sets FPU control word do it now\n-  if (fp_mode_24b) {\n-    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    verify_FPU(0, \"FPU stack must be clean on entry\");\n-  }\n-#endif\n-\n@@ -137,1 +127,0 @@\n- #ifdef _LP64\n@@ -153,4 +142,0 @@\n-#else\n-    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n-    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n-#endif\n@@ -305,1 +290,1 @@\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+    andptr(tmpReg, (int32_t) (7 - (int)os::vm_page_size()));\n@@ -313,4 +298,0 @@\n-#ifndef _LP64\n-  \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-  orl(boxReg, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-#else\n@@ -335,1 +316,1 @@\n-#endif \/\/ _LP64\n+\n@@ -344,1 +325,0 @@\n-#ifdef _LP64\n@@ -347,1 +327,0 @@\n-#endif\n@@ -410,5 +389,0 @@\n-#ifndef _LP64\n-  \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-  orl(boxReg, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-  jmpb(DONE_LABEL);\n-#else\n@@ -471,1 +445,0 @@\n-#endif  \/\/ _LP64\n@@ -491,1 +464,0 @@\n-#ifdef _LP64\n@@ -493,1 +465,0 @@\n-#endif\n@@ -572,5 +543,0 @@\n-#ifndef _LP64\n-    \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-    orl(box, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-    jmpb(slow_path);\n-#else\n@@ -642,1 +608,0 @@\n-#endif  \/\/ _LP64\n@@ -755,5 +720,0 @@\n-#ifndef _LP64\n-    \/\/ Just take slow path to avoid dealing with 64 bit atomic instructions here.\n-    orl(t, 1);  \/\/ set ICC.ZF=0 to indicate failure\n-    jmpb(slow_path);\n-#else\n@@ -811,1 +771,0 @@\n-#endif  \/\/ _LP64\n@@ -1185,1 +1144,0 @@\n-    assert(UseSSE > 0, \"required\");\n@@ -1193,1 +1151,0 @@\n-    assert(UseSSE > 1, \"required\");\n@@ -1533,1 +1490,0 @@\n-#ifdef _LP64\n@@ -1572,1 +1528,0 @@\n-#endif \/\/ _LP64\n@@ -1644,1 +1599,1 @@\n-      LP64_ONLY(vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc));\n+      vgather8b_masked_offset(elem_ty, temp_dst, base, idx_base, offset, mask, mask_idx, rtmp, vlen_enc);\n@@ -2048,1 +2003,0 @@\n-#ifdef _LP64\n@@ -2060,1 +2014,0 @@\n-#endif \/\/ _LP64\n@@ -2310,1 +2263,0 @@\n-#ifdef _LP64\n@@ -2336,1 +2288,0 @@\n-#endif \/\/ _LP64\n@@ -2752,1 +2703,0 @@\n-#ifdef _LP64\n@@ -2781,1 +2731,0 @@\n-#endif\n@@ -3849,1 +3798,0 @@\n-#ifdef _LP64\n@@ -3873,2 +3821,0 @@\n-#endif \/\/ _LP64\n-\n@@ -4043,1 +3989,0 @@\n-#ifdef _LP64\n@@ -4069,1 +4014,0 @@\n-#endif \/\/ _LP64\n@@ -4144,1 +4088,0 @@\n-#ifdef _LP64\n@@ -4152,25 +4095,0 @@\n-#else\n-    Label k_init;\n-    jmp(k_init);\n-\n-    \/\/ We could not read 64-bits from a general purpose register thus we move\n-    \/\/ data required to compose 64 1's to the instruction stream\n-    \/\/ We emit 64 byte wide series of elements from 0..63 which later on would\n-    \/\/ be used as a compare targets with tail count contained in tmp1 register.\n-    \/\/ Result would be a k register having tmp1 consecutive number or 1\n-    \/\/ counting from least significant bit.\n-    address tmp = pc();\n-    emit_int64(0x0706050403020100);\n-    emit_int64(0x0F0E0D0C0B0A0908);\n-    emit_int64(0x1716151413121110);\n-    emit_int64(0x1F1E1D1C1B1A1918);\n-    emit_int64(0x2726252423222120);\n-    emit_int64(0x2F2E2D2C2B2A2928);\n-    emit_int64(0x3736353433323130);\n-    emit_int64(0x3F3E3D3C3B3A3938);\n-\n-    bind(k_init);\n-    lea(len, InternalAddress(tmp));\n-    \/\/ create mask to test for negative byte inside a vector\n-    evpbroadcastb(vec1, tmp1, Assembler::AVX_512bit);\n-    evpcmpgtb(mask2, vec1, Address(len, 0), Assembler::AVX_512bit);\n@@ -4178,1 +4096,0 @@\n-#endif\n@@ -4201,1 +4118,1 @@\n-    if (UseAVX >= 2 && UseSSE >= 2) {\n+    if (UseAVX >= 2) {\n@@ -4348,1 +4265,1 @@\n-  if (UseAVX >= 2 && UseSSE >= 2) {\n+  if (UseAVX >= 2) {\n@@ -4425,1 +4342,0 @@\n-#ifdef _LP64\n@@ -4462,1 +4378,1 @@\n-#endif \/\/_LP64\n+\n@@ -4629,2 +4545,0 @@\n-#ifdef _LP64\n-\n@@ -4677,2 +4591,0 @@\n-#endif \/\/ _LP64\n-\n@@ -5338,1 +5250,0 @@\n-#ifdef _LP64\n@@ -5390,1 +5301,0 @@\n-#endif \/\/ _LP64\n@@ -5521,1 +5431,0 @@\n-#ifdef _LP64\n@@ -5779,1 +5688,0 @@\n-#endif\n@@ -5844,2 +5752,1 @@\n-  bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n-  if ((is_LP64 || lane_size < 8) &&\n+  if ((lane_size < 8) &&\n@@ -5859,1 +5766,1 @@\n-    LP64_ONLY(movq(dst, rtmp)) NOT_LP64(movdl(dst, rtmp));\n+    movq(dst, rtmp);\n@@ -5994,8 +5901,0 @@\n-#ifndef _LP64\n-void C2_MacroAssembler::vector_maskall_operation32(KRegister dst, Register src, KRegister tmp, int mask_len) {\n-  assert(VM_Version::supports_avx512bw(), \"\");\n-  kmovdl(tmp, src);\n-  kunpckdql(dst, tmp, tmp);\n-}\n-#endif\n-\n@@ -6455,1 +6354,0 @@\n-#ifdef _LP64\n@@ -6619,1 +6517,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":9,"deletions":112,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(int framesize, int stack_bang_size, bool is_stub);\n@@ -133,1 +133,0 @@\n-#ifdef _LP64\n@@ -135,1 +134,0 @@\n-#endif\n@@ -155,1 +153,0 @@\n-#ifdef _LP64\n@@ -158,1 +155,0 @@\n-#endif \/\/ _LP64\n@@ -205,1 +201,0 @@\n-#ifdef _LP64\n@@ -209,1 +204,0 @@\n-#endif \/\/ _LP64\n@@ -240,1 +234,0 @@\n-#ifdef _LP64\n@@ -249,1 +242,0 @@\n-#endif\n@@ -253,4 +245,0 @@\n-#ifndef _LP64\n-  void vector_maskall_operation32(KRegister dst, Register src, KRegister ktmp, int mask_len);\n-#endif\n-\n@@ -316,1 +304,0 @@\n-#ifdef _LP64\n@@ -318,1 +305,0 @@\n-#endif\n@@ -393,1 +379,0 @@\n-#ifdef _LP64\n@@ -406,1 +391,0 @@\n-#endif \/\/ _LP64\n@@ -412,1 +396,0 @@\n-#ifdef _LP64\n@@ -420,1 +403,0 @@\n-#endif\n@@ -513,1 +495,0 @@\n-#ifdef _LP64\n@@ -516,1 +497,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":1,"deletions":21,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -48,1 +48,0 @@\n-#ifdef AMD64\n@@ -58,11 +57,0 @@\n-#else\n-define_pd_global(intx,  InteriorEntryAlignment,      4);\n-define_pd_global(size_t, NewSizeThreadIncrease,      4*K);\n-define_pd_global(intx,  LoopUnrollLimit,             50);     \/\/ Design center runs on 1.3.1\n-\/\/ InitialCodeCacheSize derived from specjbb2000 run.\n-define_pd_global(uintx, InitialCodeCacheSize,        2304*K); \/\/ Integral multiple of CodeCacheExpansionSize\n-define_pd_global(uintx, CodeCacheExpansionSize,      32*K);\n-\n-\/\/ Ergonomics related flags\n-define_pd_global(uint64_t, MaxRAM,                   4ULL*G);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/c2_globals_x86.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -37,7 +37,0 @@\n-  \/\/ QQQ presumably all 64bit cpu's support this. Seems like the ifdef could\n-  \/\/ simply be left out.\n-#ifndef AMD64\n-  if (!VM_Version::supports_cmov()) {\n-    ConditionalMoveLimit = 0;\n-  }\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/c2_init_x86.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -65,2 +65,1 @@\n-  return NOT_LP64(10)    \/\/ movl; jmp\n-         LP64_ONLY(15);  \/\/ movq (1+1+8); jmp (1+4)\n+  return 15;  \/\/ movq (1+1+8); jmp (1+4)\n","filename":"src\/hotspot\/cpu\/x86\/compiledIC_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,2 +28,0 @@\n-#ifdef _LP64\n-\n@@ -48,2 +46,0 @@\n-\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -264,1 +264,0 @@\n-#ifdef _LP64\n@@ -271,1 +270,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/continuationFreezeThaw_x86.inline.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-#ifdef _LP64\n@@ -60,3 +59,0 @@\n-#else\n-  return 0;\n-#endif\n@@ -66,4 +62,1 @@\n-#ifdef _LP64\n-  sp = align_down(sp, frame::frame_alignment);\n-#endif\n-  return sp;\n+  return align_down(sp, frame::frame_alignment);\n","filename":"src\/hotspot\/cpu\/x86\/continuationHelper_x86.inline.hpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#ifdef AMD64\n@@ -37,7 +36,0 @@\n-#else\n-  juint* to = (juint*)tohw;\n-  count *= HeapWordSize \/ BytesPerInt;\n-  while (count-- > 0) {\n-    *to++ = value;\n-  }\n-#endif \/\/ AMD64\n@@ -63,1 +55,0 @@\n-#if defined AMD64 || defined _WINDOWS\n@@ -65,40 +56,0 @@\n-#else\n-  \/\/ Includes a zero-count check.\n-  intx temp = 0;\n-  __asm__ volatile(\"        testl   %6,%6         ;\"\n-                   \"        jz      7f            ;\"\n-                   \"        cmpl    %4,%5         ;\"\n-                   \"        leal    -4(%4,%6,4),%3;\"\n-                   \"        jbe     1f            ;\"\n-                   \"        cmpl    %7,%5         ;\"\n-                   \"        jbe     4f            ;\"\n-                   \"1:      cmpl    $32,%6        ;\"\n-                   \"        ja      3f            ;\"\n-                   \"        subl    %4,%1         ;\"\n-                   \"2:      movl    (%4),%3       ;\"\n-                   \"        movl    %7,(%5,%4,1)  ;\"\n-                   \"        addl    $4,%0         ;\"\n-                   \"        subl    $1,%2          ;\"\n-                   \"        jnz     2b            ;\"\n-                   \"        jmp     7f            ;\"\n-                   \"3:      rep;    smovl         ;\"\n-                   \"        jmp     7f            ;\"\n-                   \"4:      cmpl    $32,%2        ;\"\n-                   \"        movl    %7,%0         ;\"\n-                   \"        leal    -4(%5,%6,4),%1;\"\n-                   \"        ja      6f            ;\"\n-                   \"        subl    %4,%1         ;\"\n-                   \"5:      movl    (%4),%3       ;\"\n-                   \"        movl    %7,(%5,%4,1)  ;\"\n-                   \"        subl    $4,%0         ;\"\n-                   \"        subl    $1,%2          ;\"\n-                   \"        jnz     5b            ;\"\n-                   \"        jmp     7f            ;\"\n-                   \"6:      std                   ;\"\n-                   \"        rep;    smovl         ;\"\n-                   \"        cld                   ;\"\n-                   \"7:      nop                    \"\n-                   : \"=S\" (from), \"=D\" (to), \"=c\" (count), \"=r\" (temp)\n-                   : \"0\"  (from), \"1\"  (to), \"2\"  (count), \"3\"  (temp)\n-                   : \"memory\", \"flags\");\n-#endif \/\/ AMD64\n@@ -108,1 +59,0 @@\n-#ifdef AMD64\n@@ -123,24 +73,0 @@\n-#else\n-#if defined _WINDOWS\n-  (void)memcpy(to, from, count * HeapWordSize);\n-#else\n-  \/\/ Includes a zero-count check.\n-  intx temp = 0;\n-  __asm__ volatile(\"        testl   %6,%6       ;\"\n-                   \"        jz      3f          ;\"\n-                   \"        cmpl    $32,%6      ;\"\n-                   \"        ja      2f          ;\"\n-                   \"        subl    %4,%1       ;\"\n-                   \"1:      movl    (%4),%3     ;\"\n-                   \"        movl    %7,(%5,%4,1);\"\n-                   \"        addl    $4,%0       ;\"\n-                   \"        subl    $1,%2        ;\"\n-                   \"        jnz     1b          ;\"\n-                   \"        jmp     3f          ;\"\n-                   \"2:      rep;    smovl       ;\"\n-                   \"3:      nop                  \"\n-                   : \"=S\" (from), \"=D\" (to), \"=c\" (count), \"=r\" (temp)\n-                   : \"0\"  (from), \"1\"  (to), \"2\"  (count), \"3\"  (temp)\n-                   : \"memory\", \"cc\");\n-#endif \/\/ _WINDOWS\n-#endif \/\/ AMD64\n@@ -150,1 +76,0 @@\n-#ifdef AMD64\n@@ -152,4 +77,0 @@\n-#else\n-  \/\/ pd_disjoint_words is word-atomic in this implementation.\n-  pd_disjoint_words(from, to, count);\n-#endif \/\/ AMD64\n@@ -167,1 +88,0 @@\n-#if defined AMD64 || defined _WINDOWS\n@@ -169,74 +89,0 @@\n-#else\n-  \/\/ Includes a zero-count check.\n-  intx temp = 0;\n-  __asm__ volatile(\"        testl   %6,%6          ;\"\n-                   \"        jz      13f            ;\"\n-                   \"        cmpl    %4,%5          ;\"\n-                   \"        leal    -1(%4,%6),%3   ;\"\n-                   \"        jbe     1f             ;\"\n-                   \"        cmpl    %7,%5          ;\"\n-                   \"        jbe     8f             ;\"\n-                   \"1:      cmpl    $3,%6          ;\"\n-                   \"        jbe     6f             ;\"\n-                   \"        movl    %6,%3          ;\"\n-                   \"        movl    $4,%2          ;\"\n-                   \"        subl    %4,%2          ;\"\n-                   \"        andl    $3,%2          ;\"\n-                   \"        jz      2f             ;\"\n-                   \"        subl    %6,%3          ;\"\n-                   \"        rep;    smovb          ;\"\n-                   \"2:      movl    %7,%2          ;\"\n-                   \"        shrl    $2,%2          ;\"\n-                   \"        jz      5f             ;\"\n-                   \"        cmpl    $32,%2         ;\"\n-                   \"        ja      4f             ;\"\n-                   \"        subl    %4,%1          ;\"\n-                   \"3:      movl    (%4),%%edx     ;\"\n-                   \"        movl    %%edx,(%5,%4,1);\"\n-                   \"        addl    $4,%0          ;\"\n-                   \"        subl    $1,%2           ;\"\n-                   \"        jnz     3b             ;\"\n-                   \"        addl    %4,%1          ;\"\n-                   \"        jmp     5f             ;\"\n-                   \"4:      rep;    smovl          ;\"\n-                   \"5:      movl    %7,%2          ;\"\n-                   \"        andl    $3,%2          ;\"\n-                   \"        jz      13f            ;\"\n-                   \"6:      xorl    %7,%3          ;\"\n-                   \"7:      movb    (%4,%7,1),%%dl ;\"\n-                   \"        movb    %%dl,(%5,%7,1) ;\"\n-                   \"        addl    $1,%3          ;\"\n-                   \"        subl    $1,%2           ;\"\n-                   \"        jnz     7b             ;\"\n-                   \"        jmp     13f            ;\"\n-                   \"8:      std                    ;\"\n-                   \"        cmpl    $12,%2         ;\"\n-                   \"        ja      9f             ;\"\n-                   \"        movl    %7,%0          ;\"\n-                   \"        leal    -1(%6,%5),%1   ;\"\n-                   \"        jmp     11f            ;\"\n-                   \"9:      xchgl   %3,%2          ;\"\n-                   \"        movl    %6,%0          ;\"\n-                   \"        addl    $1,%2          ;\"\n-                   \"        leal    -1(%7,%5),%1   ;\"\n-                   \"        andl    $3,%2          ;\"\n-                   \"        jz      10f            ;\"\n-                   \"        subl    %6,%3          ;\"\n-                   \"        rep;    smovb          ;\"\n-                   \"10:     movl    %7,%2          ;\"\n-                   \"        subl    $3,%0          ;\"\n-                   \"        shrl    $2,%2          ;\"\n-                   \"        subl    $3,%1          ;\"\n-                   \"        rep;    smovl          ;\"\n-                   \"        andl    $3,%3          ;\"\n-                   \"        jz      12f            ;\"\n-                   \"        movl    %7,%2          ;\"\n-                   \"        addl    $3,%0          ;\"\n-                   \"        addl    $3,%1          ;\"\n-                   \"11:     rep;    smovb          ;\"\n-                   \"12:     cld                    ;\"\n-                   \"13:     nop                    ;\"\n-                   : \"=S\" (from), \"=D\" (to), \"=c\" (count), \"=r\" (temp)\n-                   : \"0\"  (from), \"1\"  (to), \"2\"  (count), \"3\"  (temp)\n-                   : \"memory\", \"flags\", \"%edx\");\n-#endif \/\/ AMD64\n@@ -256,1 +102,0 @@\n-#ifdef AMD64\n@@ -258,5 +103,0 @@\n-#else\n-  assert(HeapWordSize == BytesPerInt, \"heapwords and jints must be the same size\");\n-  \/\/ pd_conjoint_words is word-atomic in this implementation.\n-  pd_conjoint_words((const HeapWord*)from, (HeapWord*)to, count);\n-#endif \/\/ AMD64\n@@ -266,1 +106,0 @@\n-#ifdef AMD64\n@@ -268,20 +107,0 @@\n-#else\n-  \/\/ Guarantee use of fild\/fistp or xmm regs via some asm code, because compilers won't.\n-  if (from > to) {\n-    while (count-- > 0) {\n-      __asm__ volatile(\"fildll (%0); fistpll (%1)\"\n-                       :\n-                       : \"r\" (from), \"r\" (to)\n-                       : \"memory\" );\n-      ++from;\n-      ++to;\n-    }\n-  } else {\n-    while (count-- > 0) {\n-      __asm__ volatile(\"fildll (%0,%2,8); fistpll (%1,%2,8)\"\n-                       :\n-                       : \"r\" (from), \"r\" (to), \"r\" (count)\n-                       : \"memory\" );\n-    }\n-  }\n-#endif \/\/ AMD64\n@@ -291,1 +110,0 @@\n-#ifdef AMD64\n@@ -294,5 +112,0 @@\n-#else\n-  assert(HeapWordSize == BytesPerOop, \"heapwords and oops must be the same size\");\n-  \/\/ pd_conjoint_words is word-atomic in this implementation.\n-  pd_conjoint_words((const HeapWord*)from, (HeapWord*)to, count);\n-#endif \/\/ AMD64\n@@ -310,5 +123,1 @@\n-#ifdef AMD64\n-   _Copy_arrayof_conjoint_jints(from, to, count);\n-#else\n-  pd_conjoint_jints_atomic((const jint*)from, (jint*)to, count);\n-#endif \/\/ AMD64\n+  _Copy_arrayof_conjoint_jints(from, to, count);\n@@ -318,1 +127,0 @@\n-#ifdef AMD64\n@@ -320,3 +128,0 @@\n-#else\n-  pd_conjoint_jlongs_atomic((const jlong*)from, (jlong*)to, count);\n-#endif \/\/ AMD64\n@@ -326,1 +131,0 @@\n-#ifdef AMD64\n@@ -329,3 +133,0 @@\n-#else\n-  pd_conjoint_oops_atomic((const oop*)from, (oop*)to, count);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/copy_x86.hpp","additions":1,"deletions":200,"binary":false,"changes":201,"status":"modified"},{"patch":"@@ -1,43 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"prims\/downcallLinker.hpp\"\n-\n-RuntimeStub* DowncallLinker::make_downcall_stub(BasicType* signature,\n-                                                int num_args,\n-                                                BasicType ret_bt,\n-                                                const ABIDescriptor& abi,\n-                                                const GrowableArray<VMStorage>& input_registers,\n-                                                const GrowableArray<VMStorage>& output_registers,\n-                                                bool needs_return_buffer,\n-                                                int captured_state_mask,\n-                                                bool needs_transition) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-void DowncallLinker::StubGenerator::pd_add_offset_to_oop(VMStorage reg_oop, VMStorage reg_offset,\n-                                                         VMStorage tmp1, VMStorage tmp2) const {\n-  Unimplemented();\n-}\n","filename":"src\/hotspot\/cpu\/x86\/downcallLinker_x86_32.cpp","additions":0,"deletions":43,"binary":false,"changes":43,"status":"deleted"},{"patch":"@@ -1,55 +0,0 @@\n-\/*\n- * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"code\/vmreg.hpp\"\n-#include \"prims\/foreignGlobals.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-class MacroAssembler;\n-\n-bool ForeignGlobals::is_foreign_linker_supported() {\n-  return false;\n-}\n-\n-const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n-  Unimplemented();\n-  return {};\n-}\n-\n-int RegSpiller::pd_reg_size(VMStorage reg) {\n-  Unimplemented();\n-  return -1;\n-}\n-\n-void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMStorage reg) {\n-  Unimplemented();\n-}\n-\n-void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMStorage reg) {\n-  Unimplemented();\n-}\n-\n-void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMStorage tmp, int in_stk_bias, int out_stk_bias) const {\n-  Unimplemented();\n-}\n","filename":"src\/hotspot\/cpu\/x86\/foreignGlobals_x86_32.cpp","additions":0,"deletions":55,"binary":false,"changes":55,"status":"deleted"},{"patch":"@@ -577,2 +577,0 @@\n-    \/\/ QQQ seems like this code is equivalent on the two platforms\n-#ifdef AMD64\n@@ -582,3 +580,0 @@\n-#else\n-      tos_addr += 2;\n-#endif \/\/ AMD64\n@@ -610,13 +605,1 @@\n-    case T_FLOAT   : {\n-#ifdef AMD64\n-        value_result->f = *(jfloat*)tos_addr;\n-#else\n-      if (method->is_native()) {\n-        jdouble d = *(jdouble*)tos_addr;  \/\/ Result was in ST0 so need to convert to jfloat\n-        value_result->f = (jfloat)d;\n-      } else {\n-        value_result->f = *(jfloat*)tos_addr;\n-      }\n-#endif \/\/ AMD64\n-      break;\n-    }\n+    case T_FLOAT   : value_result->f = *(jfloat*)tos_addr; break;\n@@ -652,1 +635,0 @@\n-#ifdef AMD64\n@@ -660,1 +642,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":1,"deletions":20,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -83,1 +83,0 @@\n-#ifdef AMD64\n@@ -95,3 +94,0 @@\n-#else\n-    entry_frame_call_wrapper_offset                  =  2,\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -442,1 +442,0 @@\n-#ifdef AMD64\n@@ -447,4 +446,1 @@\n-  if (true) {\n-    map->set_location(rbp->as_VMReg()->next(), (address) link_addr);\n-  }\n-#endif \/\/ AMD64\n+  map->set_location(rbp->as_VMReg()->next(), (address) link_addr);\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -53,6 +53,0 @@\n-    Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-#ifndef _LP64\n-    __ push(thread);\n-    __ get_thread(thread);\n-#endif\n-\n@@ -60,1 +54,1 @@\n-    Address in_progress(thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n+    Address in_progress(r15_thread, in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset()));\n@@ -69,2 +63,0 @@\n-    NOT_LP64(__ pop(thread);)\n-\n@@ -74,1 +66,0 @@\n-#ifdef _LP64\n@@ -92,4 +83,0 @@\n-#else\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_pre_oop_entry),\n-                    addr, count);\n-#endif\n@@ -105,1 +92,0 @@\n-#ifdef _LP64\n@@ -116,4 +102,0 @@\n-#else\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, G1BarrierSetRuntime::write_ref_array_post_entry),\n-                  addr, count);\n-#endif\n@@ -131,19 +113,0 @@\n-    Register thread = NOT_LP64(tmp_thread) LP64_ONLY(r15_thread);\n-\n-#ifndef _LP64\n-    \/\/ Work around the x86_32 bug that only manifests with Loom for some reason.\n-    \/\/ MacroAssembler::resolve_weak_handle calls this barrier with tmp_thread == noreg.\n-    if (thread == noreg) {\n-      if (dst != rcx && tmp1 != rcx) {\n-        thread = rcx;\n-      } else if (dst != rdx && tmp1 != rdx) {\n-        thread = rdx;\n-      } else if (dst != rdi && tmp1 != rdi) {\n-        thread = rdi;\n-      }\n-    }\n-    assert_different_registers(dst, tmp1, thread);\n-    __ push(thread);\n-    __ get_thread(thread);\n-#endif\n-\n@@ -155,1 +118,1 @@\n-                         thread \/* thread *\/,\n+                         r15_thread \/* thread *\/,\n@@ -160,3 +123,0 @@\n-#ifndef _LP64\n-    __ pop(thread);\n-#endif\n@@ -227,1 +187,0 @@\n-#ifdef _LP64\n@@ -229,1 +188,0 @@\n-#endif \/\/ _LP64\n@@ -264,2 +222,1 @@\n-    LP64_ONLY( assert(pre_val != c_rarg1, \"smashed arg\"); )\n-#ifdef _LP64\n+    assert(pre_val != c_rarg1, \"smashed arg\");\n@@ -272,4 +229,0 @@\n-#else\n-    __ push(thread);\n-    __ push(pre_val);\n-#endif\n@@ -340,1 +293,0 @@\n-#ifdef _LP64\n@@ -342,1 +294,0 @@\n-#endif \/\/ _LP64\n@@ -354,1 +305,1 @@\n-  RegSet saved = RegSet::of(store_addr NOT_LP64(COMMA thread));\n+  RegSet saved = RegSet::of(store_addr);\n@@ -365,1 +316,0 @@\n-#ifdef _LP64\n@@ -377,3 +327,0 @@\n-#else\n-  Unimplemented();\n-#endif \/\/ _LP64\n@@ -388,1 +335,0 @@\n-#ifdef _LP64\n@@ -390,1 +336,0 @@\n-#endif \/\/ _LP64\n@@ -430,1 +375,0 @@\n-#ifdef _LP64\n@@ -432,1 +376,0 @@\n-#endif \/\/ _LP64\n@@ -471,1 +414,0 @@\n-  Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n@@ -482,7 +424,0 @@\n-#ifndef _LP64\n-  InterpreterMacroAssembler *imasm = static_cast<InterpreterMacroAssembler*>(masm);\n-#endif\n-\n-  NOT_LP64(__ get_thread(rcx));\n-  NOT_LP64(imasm->save_bcp());\n-\n@@ -493,1 +428,1 @@\n-                         rthread \/* thread *\/,\n+                         r15_thread \/* thread *\/,\n@@ -514,1 +449,1 @@\n-                            rthread \/* thread *\/,\n+                            r15_thread \/* thread *\/,\n@@ -519,1 +454,0 @@\n-  NOT_LP64(imasm->restore_bcp());\n@@ -579,1 +513,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -582,2 +516,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n-\n@@ -645,1 +577,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -663,2 +595,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":8,"deletions":78,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-#ifdef _LP64\n@@ -62,3 +61,1 @@\n-      } else\n-#endif\n-      {\n+      } else {\n@@ -89,1 +86,0 @@\n-#ifdef _LP64\n@@ -91,12 +87,0 @@\n-#else\n-    if (atomic) {\n-      __ fild_d(src);               \/\/ Must load atomically\n-      __ subptr(rsp,2*wordSize);    \/\/ Make space for store\n-      __ fistp_d(Address(rsp,0));\n-      __ pop(rax);\n-      __ pop(rdx);\n-    } else {\n-      __ movl(rax, src);\n-      __ movl(rdx, src.plus_disp(wordSize));\n-    }\n-#endif\n@@ -121,1 +105,0 @@\n-#ifdef _LP64\n@@ -127,3 +110,0 @@\n-#else\n-        __ movl(dst, NULL_WORD);\n-#endif\n@@ -131,1 +111,0 @@\n-#ifdef _LP64\n@@ -140,3 +119,1 @@\n-        } else\n-#endif\n-        {\n+        } else {\n@@ -171,1 +148,0 @@\n-#ifdef _LP64\n@@ -173,12 +149,0 @@\n-#else\n-    if (atomic) {\n-      __ push(rdx);\n-      __ push(rax);                 \/\/ Must update atomically with FIST\n-      __ fild_d(Address(rsp,0));    \/\/ So load into FPU register\n-      __ fistp_d(dst);              \/\/ and put into memory atomically\n-      __ addptr(rsp, 2*wordSize);\n-    } else {\n-      __ movptr(dst, rax);\n-      __ movptr(dst.plus_disp(wordSize), rdx);\n-    }\n-#endif\n@@ -220,1 +184,0 @@\n-#ifdef _LP64\n@@ -222,3 +185,0 @@\n-#else\n-    fatal(\"No support for 8 bytes copy\");\n-#endif\n@@ -229,1 +189,0 @@\n-#ifdef _LP64\n@@ -233,1 +192,0 @@\n-#endif\n@@ -243,1 +201,0 @@\n-#ifdef _LP64\n@@ -247,1 +204,0 @@\n-#endif\n@@ -260,1 +216,0 @@\n-#ifdef _LP64\n@@ -262,3 +217,0 @@\n-#else\n-    fatal(\"No support for 8 bytes copy\");\n-#endif\n@@ -325,1 +277,0 @@\n-#ifdef _LP64\n@@ -327,5 +278,0 @@\n-#else\n-    assert(t1->is_valid(), \"need temp reg\");\n-    thread = t1;\n-    __ get_thread(thread);\n-#endif\n@@ -355,1 +301,0 @@\n-#ifdef _LP64\n@@ -382,21 +327,0 @@\n-#else\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label*, Label*) {\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  if (bs_nm == nullptr) {\n-    return;\n-  }\n-\n-  Label continuation;\n-\n-  Register tmp = rdi;\n-  __ push(tmp);\n-  __ movptr(tmp, (intptr_t)bs_nm->disarmed_guard_value_address());\n-  Address disarmed_addr(tmp, 0);\n-  __ align(4);\n-  __ cmpl_imm32(disarmed_addr, 0);\n-  __ pop(tmp);\n-  __ jcc(Assembler::equal, continuation);\n-  __ call(RuntimeAddress(StubRoutines::method_entry_barrier()));\n-  __ bind(continuation);\n-}\n-#endif\n@@ -414,6 +338,2 @@\n-  Register tmp1 = LP64_ONLY( rscratch1 ) NOT_LP64( rax );\n-  Register tmp2 = LP64_ONLY( rscratch2 ) NOT_LP64( rcx );\n-#ifndef _LP64\n-  __ push(tmp1);\n-  __ push(tmp2);\n-#endif \/\/ !_LP64\n+  Register tmp1 = rscratch1;\n+  Register tmp2 = rscratch2;\n@@ -435,5 +355,0 @@\n-#ifndef _LP64\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n-\n@@ -443,5 +358,0 @@\n-\n-#ifndef _LP64\n-  __ pop(tmp2);\n-  __ pop(tmp1);\n-#endif\n@@ -467,2 +377,0 @@\n-#ifdef _LP64\n-\n@@ -744,8 +652,0 @@\n-#else \/\/ !_LP64\n-\n-OptoReg::Name BarrierSetAssembler::refine_register(const Node* node, OptoReg::Name opto_reg) {\n-  Unimplemented(); \/\/ This must be implemented to support late barrier expansion.\n-}\n-\n-#endif \/\/ _LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":4,"deletions":104,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -117,2 +117,0 @@\n-#ifdef _LP64\n-\n@@ -163,2 +161,0 @@\n-#endif \/\/ _LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-#ifdef _LP64\n@@ -51,8 +50,0 @@\n-#else\n-  enum Intel_specific_constants {\n-    instruction_code        = 0x81,\n-    instruction_size        = 7,\n-    imm_offset              = 2,\n-    instruction_modrm       = 0x3f  \/\/ [rdi]\n-  };\n-#endif\n@@ -74,1 +65,0 @@\n-#ifdef _LP64\n@@ -101,23 +91,0 @@\n-#else\n-bool NativeNMethodCmpBarrier::check_barrier(err_msg& msg) const {\n-  if (((uintptr_t) instruction_address()) & 0x3) {\n-    msg.print(\"Addr: \" INTPTR_FORMAT \" not properly aligned\", p2i(instruction_address()));\n-    return false;\n-  }\n-\n-  int inst = ubyte_at(0);\n-  if (inst != instruction_code) {\n-    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n-        inst);\n-    return false;\n-  }\n-\n-  int modrm = ubyte_at(1);\n-  if (modrm != instruction_modrm) {\n-    msg.print(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n-        modrm);\n-    return false;\n-  }\n-  return true;\n-}\n-#endif \/\/ _LP64\n@@ -173,1 +140,0 @@\n-#ifdef _LP64\n@@ -179,3 +145,0 @@\n-#else\n-  return -18;\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetNMethod_x86.cpp","additions":0,"deletions":37,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-#ifdef _LP64\n@@ -73,11 +72,0 @@\n-#else\n-  __ lea(end,  Address(addr, count, Address::times_ptr, -wordSize));\n-  __ shrptr(addr, CardTable::card_shift());\n-  __ shrptr(end,   CardTable::card_shift());\n-  __ subptr(end, addr); \/\/ end --> count\n-__ BIND(L_loop);\n-  Address cardtable(addr, count, Address::times_1, disp);\n-  __ movb(cardtable, 0);\n-  __ decrement(count);\n-  __ jcc(Assembler::greaterEqual, L_loop);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-  bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+  bool obj_int = (type == T_OBJECT) && UseCompressedOops;\n@@ -38,1 +38,0 @@\n-#ifdef _LP64\n@@ -48,5 +47,0 @@\n-#else\n-    if (disjoint) {\n-      __ mov(rdx, dst);          \/\/ save 'to'\n-    }\n-#endif\n@@ -61,1 +55,1 @@\n-  bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+  bool obj_int = (type == T_OBJECT) && UseCompressedOops;\n@@ -65,1 +59,0 @@\n-#ifdef _LP64\n@@ -77,5 +70,0 @@\n-#else\n-    if (disjoint) {\n-      __ mov(dst, rdx); \/\/ restore 'to'\n-    }\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/modRefBarrierSetAssembler_x86.cpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-  NOT_LP64(assert(_addr->is_single_cpu(), \"must be single\");)\n@@ -50,1 +49,0 @@\n-#ifdef _LP64\n@@ -57,1 +55,0 @@\n-#endif\n@@ -109,1 +106,1 @@\n-  assert(type == T_INT || is_reference_type(type) LP64_ONLY( || type == T_LONG ), \"unexpected type\");\n+  assert(type == T_INT || is_reference_type(type) || type == T_LONG, \"unexpected type\");\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -55,27 +55,12 @@\n-    LP64_ONLY(assert(Argument::n_float_register_parameters_j == 8, \"8 fp registers to save at java call\"));\n-\n-    if (UseSSE >= 2) {\n-      const int xmm_size = wordSize * LP64_ONLY(2) NOT_LP64(4);\n-      __ subptr(rsp, xmm_size * 8);\n-      __ movdbl(Address(rsp, xmm_size * 0), xmm0);\n-      __ movdbl(Address(rsp, xmm_size * 1), xmm1);\n-      __ movdbl(Address(rsp, xmm_size * 2), xmm2);\n-      __ movdbl(Address(rsp, xmm_size * 3), xmm3);\n-      __ movdbl(Address(rsp, xmm_size * 4), xmm4);\n-      __ movdbl(Address(rsp, xmm_size * 5), xmm5);\n-      __ movdbl(Address(rsp, xmm_size * 6), xmm6);\n-      __ movdbl(Address(rsp, xmm_size * 7), xmm7);\n-    } else if (UseSSE >= 1) {\n-      const int xmm_size = wordSize * LP64_ONLY(1) NOT_LP64(2);\n-      __ subptr(rsp, xmm_size * 8);\n-      __ movflt(Address(rsp, xmm_size * 0), xmm0);\n-      __ movflt(Address(rsp, xmm_size * 1), xmm1);\n-      __ movflt(Address(rsp, xmm_size * 2), xmm2);\n-      __ movflt(Address(rsp, xmm_size * 3), xmm3);\n-      __ movflt(Address(rsp, xmm_size * 4), xmm4);\n-      __ movflt(Address(rsp, xmm_size * 5), xmm5);\n-      __ movflt(Address(rsp, xmm_size * 6), xmm6);\n-      __ movflt(Address(rsp, xmm_size * 7), xmm7);\n-    } else {\n-      __ push_FPU_state();\n-    }\n+    assert(Argument::n_float_register_parameters_j == 8, \"8 fp registers to save at java call\");\n+\n+    const int xmm_size = wordSize * 2;\n+    __ subptr(rsp, xmm_size * 8);\n+    __ movdbl(Address(rsp, xmm_size * 0), xmm0);\n+    __ movdbl(Address(rsp, xmm_size * 1), xmm1);\n+    __ movdbl(Address(rsp, xmm_size * 2), xmm2);\n+    __ movdbl(Address(rsp, xmm_size * 3), xmm3);\n+    __ movdbl(Address(rsp, xmm_size * 4), xmm4);\n+    __ movdbl(Address(rsp, xmm_size * 5), xmm5);\n+    __ movdbl(Address(rsp, xmm_size * 6), xmm6);\n+    __ movdbl(Address(rsp, xmm_size * 7), xmm7);\n@@ -87,25 +72,10 @@\n-    if (UseSSE >= 2) {\n-      const int xmm_size = wordSize * LP64_ONLY(2) NOT_LP64(4);\n-      __ movdbl(xmm0, Address(rsp, xmm_size * 0));\n-      __ movdbl(xmm1, Address(rsp, xmm_size * 1));\n-      __ movdbl(xmm2, Address(rsp, xmm_size * 2));\n-      __ movdbl(xmm3, Address(rsp, xmm_size * 3));\n-      __ movdbl(xmm4, Address(rsp, xmm_size * 4));\n-      __ movdbl(xmm5, Address(rsp, xmm_size * 5));\n-      __ movdbl(xmm6, Address(rsp, xmm_size * 6));\n-      __ movdbl(xmm7, Address(rsp, xmm_size * 7));\n-      __ addptr(rsp, xmm_size * 8);\n-    } else if (UseSSE >= 1) {\n-      const int xmm_size = wordSize * LP64_ONLY(1) NOT_LP64(2);\n-      __ movflt(xmm0, Address(rsp, xmm_size * 0));\n-      __ movflt(xmm1, Address(rsp, xmm_size * 1));\n-      __ movflt(xmm2, Address(rsp, xmm_size * 2));\n-      __ movflt(xmm3, Address(rsp, xmm_size * 3));\n-      __ movflt(xmm4, Address(rsp, xmm_size * 4));\n-      __ movflt(xmm5, Address(rsp, xmm_size * 5));\n-      __ movflt(xmm6, Address(rsp, xmm_size * 6));\n-      __ movflt(xmm7, Address(rsp, xmm_size * 7));\n-      __ addptr(rsp, xmm_size * 8);\n-    } else {\n-      __ pop_FPU_state();\n-    }\n+    const int xmm_size = wordSize * 2;\n+    __ movdbl(xmm0, Address(rsp, xmm_size * 0));\n+    __ movdbl(xmm1, Address(rsp, xmm_size * 1));\n+    __ movdbl(xmm2, Address(rsp, xmm_size * 2));\n+    __ movdbl(xmm3, Address(rsp, xmm_size * 3));\n+    __ movdbl(xmm4, Address(rsp, xmm_size * 4));\n+    __ movdbl(xmm5, Address(rsp, xmm_size * 5));\n+    __ movdbl(xmm6, Address(rsp, xmm_size * 6));\n+    __ movdbl(xmm7, Address(rsp, xmm_size * 7));\n+    __ addptr(rsp, xmm_size * 8);\n@@ -128,1 +98,1 @@\n-      bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+      bool obj_int = type == T_OBJECT && UseCompressedOops;\n@@ -132,1 +102,0 @@\n-#ifdef _LP64\n@@ -142,5 +111,0 @@\n-#else\n-      if (disjoint) {\n-        __ mov(rdx, dst);          \/\/ save 'to'\n-      }\n-#endif\n@@ -150,17 +114,1 @@\n-#ifdef _LP64\n-      Register thread = r15_thread;\n-#else\n-      Register thread = rax;\n-      if (thread == src || thread == dst || thread == count) {\n-        thread = rbx;\n-      }\n-      if (thread == src || thread == dst || thread == count) {\n-        thread = rcx;\n-      }\n-      if (thread == src || thread == dst || thread == count) {\n-        thread = rdx;\n-      }\n-      __ push(thread);\n-      __ get_thread(thread);\n-#endif\n-      assert_different_registers(src, dst, count, thread);\n+      assert_different_registers(src, dst, count, r15_thread);\n@@ -174,1 +122,1 @@\n-      Address gc_state(thread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+      Address gc_state(r15_thread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -186,1 +134,0 @@\n-#ifdef _LP64\n@@ -193,3 +140,1 @@\n-      } else\n-#endif\n-      {\n+      } else {\n@@ -203,1 +148,0 @@\n-      NOT_LP64(__ pop(thread);)\n@@ -215,1 +159,1 @@\n-    bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+    bool obj_int = type == T_OBJECT && UseCompressedOops;\n@@ -218,1 +162,0 @@\n-#ifdef _LP64\n@@ -230,5 +173,0 @@\n-#else\n-    if (disjoint) {\n-      __ mov(dst, rdx); \/\/ restore 'to'\n-    }\n-#endif\n@@ -263,1 +201,0 @@\n-#ifdef _LP64\n@@ -265,1 +202,0 @@\n-#endif \/\/ _LP64\n@@ -331,3 +267,0 @@\n-  NOT_LP64( __ push(thread); )\n-\n-#ifdef _LP64\n@@ -341,1 +274,0 @@\n-#endif\n@@ -344,2 +276,1 @@\n-    LP64_ONLY( assert(pre_val != c_rarg1, \"smashed arg\"); )\n-#ifdef _LP64\n+    assert(pre_val != c_rarg1, \"smashed arg\");\n@@ -350,4 +281,0 @@\n-#else\n-    __ push(thread);\n-    __ push(pre_val);\n-#endif\n@@ -356,1 +283,1 @@\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre), LP64_ONLY(c_rarg0) NOT_LP64(pre_val), thread);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre), c_rarg0, thread);\n@@ -359,2 +286,0 @@\n-  NOT_LP64( __ pop(thread); )\n-\n@@ -387,1 +312,0 @@\n-#ifdef _LP64\n@@ -389,8 +313,0 @@\n-#else\n-  Register thread = rcx;\n-  if (thread == dst) {\n-    thread = rbx;\n-  }\n-  __ push(thread);\n-  __ get_thread(thread);\n-#endif\n@@ -442,1 +358,1 @@\n-  uint num_saved_regs = 4 + (dst != rax ? 1 : 0) LP64_ONLY(+4);\n+  uint num_saved_regs = 4 + (dst != rax ? 1 : 0) + 4;\n@@ -452,1 +368,0 @@\n-#ifdef _LP64\n@@ -458,1 +373,0 @@\n-#endif\n@@ -462,1 +376,0 @@\n-#ifdef _LP64\n@@ -464,3 +377,0 @@\n-#else\n-  Register arg0 = rdi, arg1 = rsi;\n-#endif\n@@ -493,1 +403,0 @@\n-#ifdef _LP64\n@@ -498,1 +407,0 @@\n-#endif\n@@ -524,4 +432,0 @@\n-\n-#ifndef _LP64\n-    __ pop(thread);\n-#endif\n@@ -593,6 +497,1 @@\n-    Register thread = NOT_LP64(tmp_thread) LP64_ONLY(r15_thread);\n-    assert_different_registers(dst, tmp1, tmp_thread);\n-    if (!thread->is_valid()) {\n-      thread = rdx;\n-    }\n-    NOT_LP64(__ get_thread(thread));\n+    assert_different_registers(dst, tmp1, r15_thread);\n@@ -604,1 +503,1 @@\n-                                 thread \/* thread *\/,\n+                                 r15_thread \/* thread *\/,\n@@ -665,1 +564,0 @@\n-    Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n@@ -676,7 +574,1 @@\n-    assert_different_registers(val, tmp1, tmp2, tmp3, rthread);\n-\n-#ifndef _LP64\n-    __ get_thread(rthread);\n-    InterpreterMacroAssembler *imasm = static_cast<InterpreterMacroAssembler*>(masm);\n-    imasm->save_bcp();\n-#endif\n+    assert_different_registers(val, tmp1, tmp2, tmp3, r15_thread);\n@@ -688,1 +580,1 @@\n-                                   rthread \/* thread *\/,\n+                                   r15_thread \/* thread *\/,\n@@ -700,1 +592,0 @@\n-    NOT_LP64(imasm->restore_bcp());\n@@ -735,1 +626,0 @@\n-#ifdef _LP64\n@@ -738,3 +628,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -748,1 +636,0 @@\n-#ifdef _LP64\n@@ -752,3 +639,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -775,7 +660,1 @@\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = tmp2;\n-  __ get_thread(thread);\n-#endif\n-  Address gc_state(thread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+  Address gc_state(r15_thread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n@@ -785,1 +664,0 @@\n-#ifdef _LP64\n@@ -789,3 +667,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -806,1 +682,0 @@\n-#ifdef _LP64\n@@ -810,1 +685,0 @@\n-#endif\n@@ -826,1 +700,0 @@\n-#ifdef _LP64\n@@ -829,5 +702,0 @@\n-  }\n-#endif\n-\n-#ifdef _LP64\n-  if (UseCompressedOops) {\n@@ -836,3 +704,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -850,1 +716,0 @@\n-#ifdef _LP64\n@@ -853,8 +718,0 @@\n-  } else\n-#endif\n-  {\n-    __ movptr(oldval, tmp2);\n-  }\n-\n-#ifdef _LP64\n-  if (UseCompressedOops) {\n@@ -863,3 +720,2 @@\n-  } else\n-#endif\n-  {\n+  } else {\n+    __ movptr(oldval, tmp2);\n@@ -921,1 +777,0 @@\n-#ifdef _LP64\n@@ -935,12 +790,0 @@\n-#else\n-  __ lea(end, Address(addr, count, Address::times_ptr, -wordSize));\n-  __ shrptr(addr, CardTable::card_shift());\n-  __ shrptr(end,  CardTable::card_shift());\n-  __ subptr(end, addr); \/\/ end --> count\n-\n-  __ BIND(L_loop);\n-  Address cardtable(addr, count, Address::times_1, disp);\n-  __ movb(cardtable, 0);\n-  __ decrement(count);\n-  __ jccb(Assembler::greaterEqual, L_loop);\n-#endif\n@@ -1011,1 +854,0 @@\n-#ifdef _LP64\n@@ -1014,6 +856,0 @@\n-#else\n-    \/\/ On x86_32, C1 register allocator can give us the register without 8-bit support.\n-    \/\/ Do the full-register access and test to avoid compilation failures.\n-    __ movptr(tmp2, Address(tmp2, tmp1, Address::times_1));\n-    __ testptr(tmp2, 0xFF);\n-#endif\n@@ -1053,1 +889,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n+  const Register thread = r15_thread;\n@@ -1056,2 +892,0 @@\n-  NOT_LP64(__ get_thread(thread);)\n-\n@@ -1112,1 +946,0 @@\n-#ifdef _LP64\n@@ -1137,12 +970,0 @@\n-#else\n-  __ load_parameter(0, rax);\n-  __ load_parameter(1, rbx);\n-  if (is_strong) {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_strong), rax, rbx);\n-  } else if (is_weak) {\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_weak), rax, rbx);\n-  } else {\n-    assert(is_phantom, \"only remaining strength\");\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom), rax, rbx);\n-  }\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":42,"deletions":221,"binary":false,"changes":263,"status":"modified"},{"patch":"@@ -1,71 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\/\/\n-\n-source_hpp %{\n-#include \"gc\/shenandoah\/shenandoahBarrierSetAssembler.hpp\"\n-#include \"gc\/shenandoah\/c2\/shenandoahSupport.hpp\"\n-%}\n-\n-instruct compareAndSwapP_shenandoah(rRegI res,\n-                                    memory mem_ptr,\n-                                    eRegP tmp1, eRegP tmp2,\n-                                    eAXRegP oldval, eRegP newval,\n-                                    eFlagsReg cr)\n-%{\n-  match(Set res (ShenandoahCompareAndSwapP mem_ptr (Binary oldval newval)));\n-  match(Set res (ShenandoahWeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n-  effect(TEMP tmp1, TEMP tmp2, KILL cr, KILL oldval);\n-\n-  format %{ \"shenandoah_cas_oop $mem_ptr,$newval\" %}\n-\n-  ins_encode %{\n-    ShenandoahBarrierSet::assembler()->cmpxchg_oop(masm,\n-                                                   $res$$Register, $mem_ptr$$Address, $oldval$$Register, $newval$$Register,\n-                                                   false, \/\/ swap\n-                                                   $tmp1$$Register, $tmp2$$Register\n-                                                   );\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeP_shenandoah(memory mem_ptr,\n-                                        eAXRegP oldval, eRegP newval,\n-                                        eRegP tmp1, eRegP tmp2,\n-                                        eFlagsReg cr)\n-%{\n-  match(Set oldval (ShenandoahCompareAndExchangeP mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, TEMP tmp1, TEMP tmp2);\n-  ins_cost(1000);\n-\n-  format %{ \"shenandoah_cas_oop $mem_ptr,$newval\" %}\n-\n-  ins_encode %{\n-    ShenandoahBarrierSet::assembler()->cmpxchg_oop(masm,\n-                                                   noreg, $mem_ptr$$Address, $oldval$$Register, $newval$$Register,\n-                                                   true,  \/\/ exchange\n-                                                   $tmp1$$Register, $tmp2$$Register\n-                                                   );\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoah_x86_32.ad","additions":0,"deletions":71,"binary":false,"changes":71,"status":"deleted"},{"patch":"@@ -37,1 +37,0 @@\n-#ifdef _LP64\n@@ -39,1 +38,0 @@\n-#endif\n@@ -47,1 +45,0 @@\n-#ifdef _LP64\n@@ -53,3 +50,0 @@\n-#else\n-#define DEFAULT_PADDING_SIZE DEFAULT_CACHE_LINE_SIZE\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/globalDefinitions_x86.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-#ifdef _LP64\n@@ -72,4 +71,0 @@\n-#else\n-#define DEFAULT_STACK_SHADOW_PAGES (4 DEBUG_ONLY(+5))\n-#define MIN_STACK_SHADOW_PAGES DEFAULT_STACK_SHADOW_PAGES\n-#endif \/\/ _LP64\n@@ -82,1 +77,0 @@\n-#ifdef _LP64\n@@ -84,3 +78,0 @@\n-#else\n-define_pd_global(bool, VMContinuations, false);\n-#endif\n@@ -104,3 +95,0 @@\n-  develop(bool, IEEEPrecision, true,                                        \\\n-          \"Enables IEEE precision (for INTEL only)\")                        \\\n-                                                                            \\\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-#ifdef AMD64\n@@ -60,4 +59,0 @@\n-#else\n-  const Address magic(rsp, 3*wordSize);\n-  __ lock(); __ addl(Address(rsp, 0), 0);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/icache_x86.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-#ifdef AMD64\n@@ -49,9 +48,0 @@\n-\n-  \/\/ Use default implementation\n-#else\n-  enum {\n-    stub_size      = 16,                 \/\/ Size of the icache flush stub in bytes\n-    line_size      = BytesPerWord,      \/\/ conservative\n-    log2_line_size = LogBytesPerWord    \/\/ log2(line_size)\n-  };\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/icache_x86.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -57,1 +57,0 @@\n-#ifdef _LP64\n@@ -59,3 +58,0 @@\n-#else\n-  assert_different_registers(obj, mdo_addr.base(), mdo_addr.index());\n-#endif\n@@ -76,1 +72,0 @@\n-#ifdef _LP64\n@@ -78,1 +73,0 @@\n-#endif\n@@ -93,1 +87,0 @@\n-#ifdef _LP64\n@@ -101,1 +94,0 @@\n-#endif\n@@ -318,1 +310,0 @@\n-  NOT_LP64(assert(java_thread == noreg , \"not expecting a precomputed java thread\");)\n@@ -339,1 +330,0 @@\n-#ifdef _LP64\n@@ -391,7 +381,0 @@\n-#else\n-void InterpreterMacroAssembler::call_VM_preemptable(Register oop_result,\n-                         address entry_point,\n-                         Register arg_1) {\n-  MacroAssembler::call_VM(oop_result, entry_point, arg_1);\n-}\n-#endif  \/\/ _LP64\n@@ -408,2 +391,1 @@\n-    Register pop_cond = NOT_LP64(java_thread) \/\/ Not clear if any other register is available on 32 bit\n-                        LP64_ONLY(c_rarg0);\n+    Register pop_cond = c_rarg0;\n@@ -420,1 +402,0 @@\n-    NOT_LP64(get_thread(java_thread);)\n@@ -425,2 +406,1 @@\n-  Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  NOT_LP64(get_thread(thread);)\n+  Register thread = r15_thread;\n@@ -431,1 +411,1 @@\n-#ifdef _LP64\n+\n@@ -450,24 +430,0 @@\n-#else\n-  const Address val_addr1(rcx, JvmtiThreadState::earlyret_value_offset()\n-                             + in_ByteSize(wordSize));\n-  switch (state) {\n-    case atos: movptr(rax, oop_addr);\n-               movptr(oop_addr, NULL_WORD);\n-               interp_verify_oop(rax, state);         break;\n-    case ltos:\n-               movl(rdx, val_addr1);               \/\/ fall through\n-    case btos:                                     \/\/ fall through\n-    case ztos:                                     \/\/ fall through\n-    case ctos:                                     \/\/ fall through\n-    case stos:                                     \/\/ fall through\n-    case itos: movl(rax, val_addr);                   break;\n-    case ftos: load_float(val_addr);                  break;\n-    case dtos: load_double(val_addr);                 break;\n-    case vtos: \/* nothing to do *\/                    break;\n-    default  : ShouldNotReachHere();\n-  }\n-#endif \/\/ _LP64\n-  \/\/ Clean up tos value in the thread object\n-  movl(tos_addr, ilgl);\n-  movptr(val_addr, NULL_WORD);\n-  NOT_LP64(movptr(val_addr1, NULL_WORD);)\n@@ -480,2 +436,2 @@\n-    Register tmp = LP64_ONLY(c_rarg0) NOT_LP64(java_thread);\n-    Register rthread = LP64_ONLY(r15_thread) NOT_LP64(java_thread);\n+    Register tmp = c_rarg0;\n+    Register rthread = r15_thread;\n@@ -496,1 +452,0 @@\n-    NOT_LP64(get_thread(java_thread);)\n@@ -498,1 +453,0 @@\n-#ifdef _LP64\n@@ -501,4 +455,0 @@\n-#else\n-    pushl(Address(tmp, JvmtiThreadState::earlyret_tos_offset()));\n-    call_VM_leaf(CAST_FROM_FN_PTR(address, Interpreter::remove_activation_early_entry), 1);\n-#endif \/\/ _LP64\n@@ -507,1 +457,0 @@\n-    NOT_LP64(get_thread(java_thread);)\n@@ -573,2 +522,2 @@\n-  LP64_ONLY(assert(Rsub_klass != r14, \"r14 holds locals\");)\n-  LP64_ONLY(assert(Rsub_klass != r13, \"r13 holds bcp\");)\n+  assert(Rsub_klass != r14, \"r14 holds locals\");\n+  assert(Rsub_klass != r13, \"r13 holds bcp\");\n@@ -585,18 +534,0 @@\n-\n-#ifndef _LP64\n-void InterpreterMacroAssembler::f2ieee() {\n-  if (IEEEPrecision) {\n-    fstp_s(Address(rsp, 0));\n-    fld_s(Address(rsp, 0));\n-  }\n-}\n-\n-\n-void InterpreterMacroAssembler::d2ieee() {\n-  if (IEEEPrecision) {\n-    fstp_d(Address(rsp, 0));\n-    fld_d(Address(rsp, 0));\n-  }\n-}\n-#endif \/\/ _LP64\n-\n@@ -641,1 +572,0 @@\n-#ifdef _LP64\n@@ -692,99 +622,0 @@\n-#else\n-void InterpreterMacroAssembler::pop_i(Register r) {\n-  pop(r);\n-}\n-\n-void InterpreterMacroAssembler::pop_l(Register lo, Register hi) {\n-  pop(lo);\n-  pop(hi);\n-}\n-\n-void InterpreterMacroAssembler::pop_f() {\n-  fld_s(Address(rsp, 0));\n-  addptr(rsp, 1 * wordSize);\n-}\n-\n-void InterpreterMacroAssembler::pop_d() {\n-  fld_d(Address(rsp, 0));\n-  addptr(rsp, 2 * wordSize);\n-}\n-\n-\n-void InterpreterMacroAssembler::pop(TosState state) {\n-  switch (state) {\n-    case atos: pop_ptr(rax);                                 break;\n-    case btos:                                               \/\/ fall through\n-    case ztos:                                               \/\/ fall through\n-    case ctos:                                               \/\/ fall through\n-    case stos:                                               \/\/ fall through\n-    case itos: pop_i(rax);                                   break;\n-    case ltos: pop_l(rax, rdx);                              break;\n-    case ftos:\n-      if (UseSSE >= 1) {\n-        pop_f(xmm0);\n-      } else {\n-        pop_f();\n-      }\n-      break;\n-    case dtos:\n-      if (UseSSE >= 2) {\n-        pop_d(xmm0);\n-      } else {\n-        pop_d();\n-      }\n-      break;\n-    case vtos: \/* nothing to do *\/                           break;\n-    default  : ShouldNotReachHere();\n-  }\n-  interp_verify_oop(rax, state);\n-}\n-\n-\n-void InterpreterMacroAssembler::push_l(Register lo, Register hi) {\n-  push(hi);\n-  push(lo);\n-}\n-\n-void InterpreterMacroAssembler::push_f() {\n-  \/\/ Do not schedule for no AGI! Never write beyond rsp!\n-  subptr(rsp, 1 * wordSize);\n-  fstp_s(Address(rsp, 0));\n-}\n-\n-void InterpreterMacroAssembler::push_d() {\n-  \/\/ Do not schedule for no AGI! Never write beyond rsp!\n-  subptr(rsp, 2 * wordSize);\n-  fstp_d(Address(rsp, 0));\n-}\n-\n-\n-void InterpreterMacroAssembler::push(TosState state) {\n-  interp_verify_oop(rax, state);\n-  switch (state) {\n-    case atos: push_ptr(rax); break;\n-    case btos:                                               \/\/ fall through\n-    case ztos:                                               \/\/ fall through\n-    case ctos:                                               \/\/ fall through\n-    case stos:                                               \/\/ fall through\n-    case itos: push_i(rax);                                    break;\n-    case ltos: push_l(rax, rdx);                               break;\n-    case ftos:\n-      if (UseSSE >= 1) {\n-        push_f(xmm0);\n-      } else {\n-        push_f();\n-      }\n-      break;\n-    case dtos:\n-      if (UseSSE >= 2) {\n-        push_d(xmm0);\n-      } else {\n-        push_d();\n-      }\n-      break;\n-    case vtos: \/* nothing to do *\/                             break;\n-    default  : ShouldNotReachHere();\n-  }\n-}\n-#endif \/\/ _LP64\n-\n@@ -825,2 +656,1 @@\n-    LP64_ONLY(temp = r15_thread;)\n-    NOT_LP64(get_thread(temp);)\n+    temp = r15_thread;\n@@ -850,1 +680,0 @@\n-  verify_FPU(1, state);\n@@ -868,1 +697,0 @@\n-#ifdef _LP64\n@@ -883,21 +711,0 @@\n-\n-#else\n-  Address index(noreg, rbx, Address::times_ptr);\n-  if (table != safepoint_table && generate_poll) {\n-    NOT_PRODUCT(block_comment(\"Thread-local Safepoint poll\"));\n-    Label no_safepoint;\n-    const Register thread = rcx;\n-    get_thread(thread);\n-    testb(Address(thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n-\n-    jccb(Assembler::zero, no_safepoint);\n-    ArrayAddress dispatch_addr(ExternalAddress((address)safepoint_table), index);\n-    jump(dispatch_addr, noreg);\n-    bind(no_safepoint);\n-  }\n-\n-  {\n-    ArrayAddress dispatch_addr(ExternalAddress((address)table), index);\n-    jump(dispatch_addr, noreg);\n-  }\n-#endif \/\/ _LP64\n@@ -955,3 +762,1 @@\n-  LP64_ONLY(movsbl(result, result);)\n-  NOT_LP64(shll(result, 24);)      \/\/ truncate upper 24 bits\n-  NOT_LP64(sarl(result, 24);)      \/\/ and sign-extend byte\n+  movsbl(result, result);\n@@ -963,2 +768,1 @@\n-  LP64_ONLY(movzwl(result, result);)\n-  NOT_LP64(andl(result, 0xFFFF);)  \/\/ truncate upper 16 bits\n+  movzwl(result, result);\n@@ -970,3 +774,1 @@\n-  LP64_ONLY(movswl(result, result);)\n-  NOT_LP64(shll(result, 16);)      \/\/ truncate upper 16 bits\n-  NOT_LP64(sarl(result, 16);)      \/\/ and sign-extend short\n+  movswl(result, result);\n@@ -1002,3 +804,3 @@\n-  const Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  const Register robj    = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n-  const Register rmon    = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n+  const Register rthread = r15_thread;\n+  const Register robj    = c_rarg1;\n+  const Register rmon    = c_rarg1;\n@@ -1007,1 +809,0 @@\n-  NOT_LP64(get_thread(rthread);)\n@@ -1020,1 +821,0 @@\n-  NOT_LP64(get_thread(rthread);) \/\/ call_VM clobbered it, restore\n@@ -1061,1 +861,0 @@\n-    NOT_LP64(empty_FPU_stack();)  \/\/ remove possible return value from FPU-stack, otherwise stack could overflow\n@@ -1070,1 +869,0 @@\n-      NOT_LP64(empty_FPU_stack();)\n@@ -1112,1 +910,0 @@\n-      NOT_LP64(empty_FPU_stack();)\n@@ -1128,1 +925,0 @@\n-        NOT_LP64(empty_FPU_stack();)\n@@ -1163,1 +959,1 @@\n-    Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n+    Register rthread = r15_thread;\n@@ -1166,2 +962,0 @@\n-    NOT_LP64(get_thread(rthread);)\n-\n@@ -1213,2 +1007,1 @@\n-  assert(lock_reg == LP64_ONLY(c_rarg1) NOT_LP64(rdx),\n-         \"The argument is only for looks. It must be c_rarg1\");\n+  assert(lock_reg == c_rarg1, \"The argument is only for looks. It must be c_rarg1\");\n@@ -1225,1 +1018,1 @@\n-    const Register obj_reg = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ Will contain the oop\n+    const Register obj_reg = c_rarg3; \/\/ Will contain the oop\n@@ -1243,7 +1036,1 @@\n-#ifdef _LP64\n-      const Register thread = r15_thread;\n-      lightweight_lock(lock_reg, obj_reg, swap_reg, thread, tmp_reg, slow_case);\n-#else\n-      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-      jmp(slow_case);\n-#endif\n+      lightweight_lock(lock_reg, obj_reg, swap_reg, r15_thread, tmp_reg, slow_case);\n@@ -1267,1 +1054,1 @@\n-      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+      const int zero_bits = 7;\n@@ -1332,2 +1119,1 @@\n-  assert(lock_reg == LP64_ONLY(c_rarg1) NOT_LP64(rdx),\n-         \"The argument is only for looks. It must be c_rarg1\");\n+  assert(lock_reg == c_rarg1, \"The argument is only for looks. It must be c_rarg1\");\n@@ -1341,2 +1127,2 @@\n-    const Register header_reg = LP64_ONLY(c_rarg2) NOT_LP64(rbx);  \/\/ Will contain the old oopMark\n-    const Register obj_reg    = LP64_ONLY(c_rarg3) NOT_LP64(rcx);  \/\/ Will contain the oop\n+    const Register header_reg = c_rarg2; \/\/ Will contain the old oopMark\n+    const Register obj_reg    = c_rarg3; \/\/ Will contain the oop\n@@ -1359,1 +1145,0 @@\n-#ifdef _LP64\n@@ -1361,4 +1146,0 @@\n-#else\n-      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-      jmp(slow_case);\n-#endif\n@@ -1440,2 +1221,2 @@\n-  Register arg3_reg = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-  Register arg2_reg = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n+  Register arg3_reg = c_rarg3;\n+  Register arg2_reg = c_rarg2;\n@@ -1899,2 +1680,0 @@\n-      NOT_LP64(assert(reg2 == rdi, \"we know how to fix this blown reg\");)\n-      NOT_LP64(restore_locals();)         \/\/ Restore EDI\n@@ -1968,9 +1747,0 @@\n-void InterpreterMacroAssembler::verify_FPU(int stack_depth, TosState state) {\n-#ifndef _LP64\n-  if ((state == ftos && UseSSE < 1) ||\n-      (state == dtos && UseSSE < 2)) {\n-    MacroAssembler::verify_FPU(stack_depth);\n-  }\n-#endif\n-}\n-\n@@ -1996,2 +1766,2 @@\n-  Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rbx);\n+  Register rthread = r15_thread;\n+  Register rarg = c_rarg1;\n@@ -2000,1 +1770,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2010,1 +1779,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2018,1 +1786,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2032,2 +1799,2 @@\n-  Register rthread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rbx);\n+  Register rthread = r15_thread;\n+  Register rarg = c_rarg1;\n@@ -2043,1 +1810,0 @@\n-    NOT_LP64(get_thread(rthread);)\n@@ -2055,1 +1821,0 @@\n-    NOT_LP64(get_thread(rthread);)\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":28,"deletions":263,"binary":false,"changes":291,"status":"modified"},{"patch":"@@ -56,2 +56,2 @@\n-    _locals_register(LP64_ONLY(r14) NOT_LP64(rdi)),\n-    _bcp_register(LP64_ONLY(r13) NOT_LP64(rsi)) {}\n+    _locals_register(r14),\n+    _bcp_register(r13) {}\n@@ -124,3 +124,0 @@\n-  NOT_LP64(void f2ieee();)        \/\/ truncate ftos to 32bits\n-  NOT_LP64(void d2ieee();)        \/\/ truncate dtos to 64bits\n-\n@@ -146,1 +143,0 @@\n-#ifdef _LP64\n@@ -149,9 +145,0 @@\n-#else\n-  void pop_l(Register lo = rax, Register hi = rdx);\n-  void pop_f();\n-  void pop_d();\n-\n-  void push_l(Register lo = rax, Register hi = rdx);\n-  void push_d();\n-  void push_f();\n-#endif \/\/ _LP64\n@@ -171,1 +158,0 @@\n-    NOT_LP64(empty_FPU_stack());\n@@ -276,2 +262,0 @@\n-  \/\/ only if +VerifyFPU  && (state == ftos || state == dtos)\n-  void verify_FPU(int stack_depth, TosState state = ftos);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":2,"deletions":18,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#ifdef AMD64\n@@ -44,4 +43,0 @@\n-#else\n-  void move(int from_offset, int to_offset);\n-  void box(int from_offset, int to_offset);\n-#endif \/\/ AMD64\n@@ -52,1 +47,0 @@\n-#ifdef AMD64\n@@ -54,1 +48,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/interpreterRT_x86.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1,146 +0,0 @@\n-\/*\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"interpreter\/interp_masm.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"interpreter\/interpreterRuntime.hpp\"\n-#include \"memory\/allocation.inline.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/icache.hpp\"\n-#include \"runtime\/interfaceSupport.inline.hpp\"\n-#include \"runtime\/signature.hpp\"\n-\n-\n-#define __ _masm->\n-\n-\n-\/\/ Implementation of SignatureHandlerGenerator\n-InterpreterRuntime::SignatureHandlerGenerator::SignatureHandlerGenerator(const methodHandle& method, CodeBuffer* buffer) :\n-    NativeSignatureIterator(method) {\n-  _masm = new MacroAssembler(buffer);\n-}\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::pass_int() {\n-  move(offset(), jni_offset() + 1);\n-}\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::pass_float() {\n-  move(offset(), jni_offset() + 1);\n-}\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::pass_long() {\n-   move(offset(), jni_offset() + 2);\n-   move(offset() + 1, jni_offset() + 1);\n-}\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::pass_object() {\n-  box (offset(), jni_offset() + 1);\n-}\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::move(int from_offset, int to_offset) {\n-  __ movl(temp(), Address(from(), Interpreter::local_offset_in_bytes(from_offset)));\n-  __ movl(Address(to(), to_offset * wordSize), temp());\n-}\n-\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::box(int from_offset, int to_offset) {\n-  __ lea(temp(), Address(from(), Interpreter::local_offset_in_bytes(from_offset)));\n-  __ cmpptr(Address(from(), Interpreter::local_offset_in_bytes(from_offset)), NULL_WORD); \/\/ do not use temp() to avoid AGI\n-  Label L;\n-  __ jcc(Assembler::notZero, L);\n-  __ movptr(temp(), NULL_WORD);\n-  __ bind(L);\n-  __ movptr(Address(to(), to_offset * wordSize), temp());\n-}\n-\n-\n-void InterpreterRuntime::SignatureHandlerGenerator::generate( uint64_t fingerprint) {\n-  \/\/ generate code to handle arguments\n-  iterate(fingerprint);\n-  \/\/ return result handler\n-  __ lea(rax,\n-         ExternalAddress((address)Interpreter::result_handler(method()->result_type())));\n-  \/\/ return\n-  __ ret(0);\n-  __ flush();\n-}\n-\n-\n-Register InterpreterRuntime::SignatureHandlerGenerator::from()       { return rdi; }\n-Register InterpreterRuntime::SignatureHandlerGenerator::to()         { return rsp; }\n-Register InterpreterRuntime::SignatureHandlerGenerator::temp()       { return rcx; }\n-\n-\n-\/\/ Implementation of SignatureHandlerLibrary\n-\n-void SignatureHandlerLibrary::pd_set_handler(address handler) {}\n-\n-class SlowSignatureHandler: public NativeSignatureIterator {\n- private:\n-  address   _from;\n-  intptr_t* _to;\n-\n-  virtual void pass_int() {\n-    *_to++ = *(jint *)(_from+Interpreter::local_offset_in_bytes(0));\n-    _from -= Interpreter::stackElementSize;\n-  }\n-\n-  virtual void pass_float() {\n-    *_to++ = *(jint *)(_from+Interpreter::local_offset_in_bytes(0));\n-    _from -= Interpreter::stackElementSize;\n-  }\n-\n-  virtual void pass_long() {\n-    _to[0] = *(intptr_t*)(_from+Interpreter::local_offset_in_bytes(1));\n-    _to[1] = *(intptr_t*)(_from+Interpreter::local_offset_in_bytes(0));\n-    _to += 2;\n-    _from -= 2*Interpreter::stackElementSize;\n-  }\n-\n-  virtual void pass_object() {\n-    \/\/ pass address of from\n-    intptr_t from_addr = (intptr_t)(_from + Interpreter::local_offset_in_bytes(0));\n-    *_to++ = (*(intptr_t*)from_addr == 0) ? NULL_WORD : from_addr;\n-    _from -= Interpreter::stackElementSize;\n-   }\n-\n- public:\n-  SlowSignatureHandler(const methodHandle& method, address from, intptr_t* to) :\n-    NativeSignatureIterator(method) {\n-    _from = from;\n-    _to   = to + (is_static() ? 2 : 1);\n-  }\n-};\n-\n-JRT_ENTRY(address, InterpreterRuntime::slow_signature_handler(JavaThread* current, Method* method, intptr_t* from, intptr_t* to))\n-  methodHandle m(current, (Method*)method);\n-  assert(m->is_native(), \"sanity check\");\n-  \/\/ handle arguments\n-  SlowSignatureHandler(m, (address)from, to + 1).iterate((uint64_t)CONST64(-1));\n-  \/\/ return result handler\n-  return Interpreter::result_handler(m->result_type());\n-JRT_END\n","filename":"src\/hotspot\/cpu\/x86\/interpreterRT_x86_32.cpp","additions":0,"deletions":146,"binary":false,"changes":146,"status":"deleted"},{"patch":"@@ -1,324 +0,0 @@\n-\/*\n- * Copyright (c) 2004, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"prims\/jniFastGetField.hpp\"\n-#include \"prims\/jvm_misc.hpp\"\n-#include \"prims\/jvmtiExport.hpp\"\n-#include \"runtime\/os.inline.hpp\"\n-#include \"runtime\/safepoint.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-\n-#define __ masm->\n-\n-#define BUFFER_SIZE 30\n-\n-\/\/ Instead of issuing lfence for LoadLoad barrier, we create data dependency\n-\/\/ between loads, which is much more efficient than lfence.\n-\n-address JNI_FastGetField::generate_fast_get_int_field0(BasicType type) {\n-  const char *name = nullptr;\n-  switch (type) {\n-    case T_BOOLEAN: name = \"jni_fast_GetBooleanField\"; break;\n-    case T_BYTE:    name = \"jni_fast_GetByteField\";    break;\n-    case T_CHAR:    name = \"jni_fast_GetCharField\";    break;\n-    case T_SHORT:   name = \"jni_fast_GetShortField\";   break;\n-    case T_INT:     name = \"jni_fast_GetIntField\";     break;\n-    default:        ShouldNotReachHere();\n-  }\n-  ResourceMark rm;\n-  BufferBlob* blob = BufferBlob::create(name, BUFFER_SIZE*wordSize);\n-  CodeBuffer cbuf(blob);\n-  MacroAssembler* masm = new MacroAssembler(&cbuf);\n-  address fast_entry = __ pc();\n-\n-  Label slow;\n-\n-  \/\/ stack layout:    offset from rsp (in words):\n-  \/\/  return pc        0\n-  \/\/  jni env          1\n-  \/\/  obj              2\n-  \/\/  jfieldID         3\n-\n-  ExternalAddress counter(SafepointSynchronize::safepoint_counter_addr());\n-  __ mov32 (rcx, counter);\n-  __ testb (rcx, 1);\n-  __ jcc (Assembler::notZero, slow);\n-\n-  if (JvmtiExport::can_post_field_access()) {\n-    \/\/ Check to see if a field access watch has been set before we\n-    \/\/ take the fast path.\n-    __ cmp32(ExternalAddress((address) JvmtiExport::get_field_access_count_addr()), 0);\n-    __ jcc(Assembler::notZero, slow);\n-  }\n-\n-  __ mov(rax, rcx);\n-  __ andptr(rax, 1);                         \/\/ rax, must end up 0\n-  __ movptr(rdx, Address(rsp, rax, Address::times_1, 2*wordSize));\n-                                            \/\/ obj, notice rax, is 0.\n-                                            \/\/ rdx is data dependent on rcx.\n-  __ movptr(rax, Address(rsp, 3*wordSize));  \/\/ jfieldID\n-\n-  __ clear_jobject_tag(rdx);\n-\n-  __ movptr(rdx, Address(rdx, 0));           \/\/ *obj\n-  __ shrptr (rax, 2);                         \/\/ offset\n-\n-  assert(count < LIST_CAPACITY, \"LIST_CAPACITY too small\");\n-  speculative_load_pclist[count] = __ pc();\n-  switch (type) {\n-    case T_BOOLEAN: __ movzbl (rax, Address(rdx, rax, Address::times_1)); break;\n-    case T_BYTE:    __ movsbl (rax, Address(rdx, rax, Address::times_1)); break;\n-    case T_CHAR:    __ movzwl (rax, Address(rdx, rax, Address::times_1)); break;\n-    case T_SHORT:   __ movswl (rax, Address(rdx, rax, Address::times_1)); break;\n-    case T_INT:     __ movl   (rax, Address(rdx, rax, Address::times_1)); break;\n-    default:        ShouldNotReachHere();\n-  }\n-\n-  Address ca1;\n-  __ lea(rdx, counter);\n-  __ xorptr(rdx, rax);\n-  __ xorptr(rdx, rax);\n-  __ cmp32(rcx, Address(rdx, 0));\n-  \/\/ ca1 is the same as ca because\n-  \/\/ rax, ^ counter_addr ^ rax, = address\n-  \/\/ ca1 is data dependent on rax,.\n-  __ jcc (Assembler::notEqual, slow);\n-\n-  __ ret (0);\n-\n-  slowcase_entry_pclist[count++] = __ pc();\n-  __ bind (slow);\n-  address slow_case_addr = nullptr;\n-  switch (type) {\n-    case T_BOOLEAN: slow_case_addr = jni_GetBooleanField_addr(); break;\n-    case T_BYTE:    slow_case_addr = jni_GetByteField_addr();    break;\n-    case T_CHAR:    slow_case_addr = jni_GetCharField_addr();    break;\n-    case T_SHORT:   slow_case_addr = jni_GetShortField_addr();   break;\n-    case T_INT:     slow_case_addr = jni_GetIntField_addr();     break;\n-    default:        ShouldNotReachHere();\n-  }\n-  \/\/ tail call\n-  __ jump (RuntimeAddress(slow_case_addr));\n-\n-  __ flush ();\n-\n-  return fast_entry;\n-}\n-\n-address JNI_FastGetField::generate_fast_get_boolean_field() {\n-  return generate_fast_get_int_field0(T_BOOLEAN);\n-}\n-\n-address JNI_FastGetField::generate_fast_get_byte_field() {\n-  return generate_fast_get_int_field0(T_BYTE);\n-}\n-\n-address JNI_FastGetField::generate_fast_get_char_field() {\n-  return generate_fast_get_int_field0(T_CHAR);\n-}\n-\n-address JNI_FastGetField::generate_fast_get_short_field() {\n-  return generate_fast_get_int_field0(T_SHORT);\n-}\n-\n-address JNI_FastGetField::generate_fast_get_int_field() {\n-  return generate_fast_get_int_field0(T_INT);\n-}\n-\n-address JNI_FastGetField::generate_fast_get_long_field() {\n-  const char *name = \"jni_fast_GetLongField\";\n-  ResourceMark rm;\n-  BufferBlob* blob = BufferBlob::create(name, BUFFER_SIZE*wordSize);\n-  CodeBuffer cbuf(blob);\n-  MacroAssembler* masm = new MacroAssembler(&cbuf);\n-  address fast_entry = __ pc();\n-\n-  Label slow;\n-\n-  \/\/ stack layout:    offset from rsp (in words):\n-  \/\/  old rsi          0\n-  \/\/  return pc        1\n-  \/\/  jni env          2\n-  \/\/  obj              3\n-  \/\/  jfieldID         4\n-\n-  ExternalAddress counter(SafepointSynchronize::safepoint_counter_addr());\n-\n-  __ push  (rsi);\n-  __ mov32 (rcx, counter);\n-  __ testb (rcx, 1);\n-  __ jcc (Assembler::notZero, slow);\n-\n-  if (JvmtiExport::can_post_field_access()) {\n-    \/\/ Check to see if a field access watch has been set before we\n-    \/\/ take the fast path.\n-    __ cmp32(ExternalAddress((address) JvmtiExport::get_field_access_count_addr()), 0);\n-    __ jcc(Assembler::notZero, slow);\n-  }\n-\n-  __ mov(rax, rcx);\n-  __ andptr(rax, 1);                         \/\/ rax, must end up 0\n-  __ movptr(rdx, Address(rsp, rax, Address::times_1, 3*wordSize));\n-                                            \/\/ obj, notice rax, is 0.\n-                                            \/\/ rdx is data dependent on rcx.\n-  __ movptr(rsi, Address(rsp, 4*wordSize));  \/\/ jfieldID\n-\n-  __ clear_jobject_tag(rdx);\n-\n-  __ movptr(rdx, Address(rdx, 0));           \/\/ *obj\n-  __ shrptr(rsi, 2);                         \/\/ offset\n-\n-  assert(count < LIST_CAPACITY-1, \"LIST_CAPACITY too small\");\n-  speculative_load_pclist[count++] = __ pc();\n-  __ movptr(rax, Address(rdx, rsi, Address::times_1));\n-  speculative_load_pclist[count] = __ pc();\n-  __ movl(rdx, Address(rdx, rsi, Address::times_1, 4));\n-\n-  __ lea(rsi, counter);\n-  __ xorptr(rsi, rdx);\n-  __ xorptr(rsi, rax);\n-  __ xorptr(rsi, rdx);\n-  __ xorptr(rsi, rax);\n-  __ cmp32(rcx, Address(rsi, 0));\n-  \/\/ ca1 is the same as ca because\n-  \/\/ rax, ^ rdx ^ counter_addr ^ rax, ^ rdx = address\n-  \/\/ ca1 is data dependent on both rax, and rdx.\n-  __ jcc (Assembler::notEqual, slow);\n-\n-  __ pop (rsi);\n-\n-  __ ret (0);\n-\n-  slowcase_entry_pclist[count-1] = __ pc();\n-  slowcase_entry_pclist[count++] = __ pc();\n-  __ bind (slow);\n-  __ pop  (rsi);\n-  address slow_case_addr = jni_GetLongField_addr();;\n-  \/\/ tail call\n-  __ jump (RuntimeAddress(slow_case_addr));\n-\n-  __ flush ();\n-\n-  return fast_entry;\n-}\n-\n-address JNI_FastGetField::generate_fast_get_float_field0(BasicType type) {\n-  const char *name = nullptr;\n-  switch (type) {\n-    case T_FLOAT:  name = \"jni_fast_GetFloatField\";  break;\n-    case T_DOUBLE: name = \"jni_fast_GetDoubleField\"; break;\n-    default:       ShouldNotReachHere();\n-  }\n-  ResourceMark rm;\n-  BufferBlob* blob = BufferBlob::create(name, BUFFER_SIZE*wordSize);\n-  CodeBuffer cbuf(blob);\n-  MacroAssembler* masm = new MacroAssembler(&cbuf);\n-  address fast_entry = __ pc();\n-\n-  Label slow_with_pop, slow;\n-\n-  \/\/ stack layout:    offset from rsp (in words):\n-  \/\/  return pc        0\n-  \/\/  jni env          1\n-  \/\/  obj              2\n-  \/\/  jfieldID         3\n-\n-  ExternalAddress counter(SafepointSynchronize::safepoint_counter_addr());\n-\n-  __ mov32 (rcx, counter);\n-  __ testb (rcx, 1);\n-  __ jcc (Assembler::notZero, slow);\n-\n-  if (JvmtiExport::can_post_field_access()) {\n-    \/\/ Check to see if a field access watch has been set before we\n-    \/\/ take the fast path.\n-    __ cmp32(ExternalAddress((address) JvmtiExport::get_field_access_count_addr()), 0);\n-    __ jcc(Assembler::notZero, slow);\n-  }\n-\n-  __ mov(rax, rcx);\n-  __ andptr(rax, 1);                         \/\/ rax, must end up 0\n-  __ movptr(rdx, Address(rsp, rax, Address::times_1, 2*wordSize));\n-                                            \/\/ obj, notice rax, is 0.\n-                                            \/\/ rdx is data dependent on rcx.\n-  __ movptr(rax, Address(rsp, 3*wordSize));  \/\/ jfieldID\n-\n-  __ clear_jobject_tag(rdx);\n-\n-  __ movptr(rdx, Address(rdx, 0));           \/\/ *obj\n-  __ shrptr(rax, 2);                         \/\/ offset\n-\n-  assert(count < LIST_CAPACITY, \"LIST_CAPACITY too small\");\n-  speculative_load_pclist[count] = __ pc();\n-  switch (type) {\n-    case T_FLOAT:  __ fld_s (Address(rdx, rax, Address::times_1)); break;\n-    case T_DOUBLE: __ fld_d (Address(rdx, rax, Address::times_1)); break;\n-    default:       ShouldNotReachHere();\n-  }\n-\n-  Address ca1;\n-  __ fst_s (Address(rsp, -4));\n-  __ lea(rdx, counter);\n-  __ movl (rax, Address(rsp, -4));\n-  \/\/ garbage hi-order bits on 64bit are harmless.\n-  __ xorptr(rdx, rax);\n-  __ xorptr(rdx, rax);\n-  __ cmp32(rcx, Address(rdx, 0));\n-  \/\/ rax, ^ counter_addr ^ rax, = address\n-  \/\/ ca1 is data dependent on the field\n-  \/\/ access.\n-  __ jcc (Assembler::notEqual, slow_with_pop);\n-\n-  __ ret (0);\n-\n-  __ bind (slow_with_pop);\n-  \/\/ invalid load. pop FPU stack.\n-  __ fstp_d (0);\n-\n-  slowcase_entry_pclist[count++] = __ pc();\n-  __ bind (slow);\n-  address slow_case_addr = nullptr;\n-  switch (type) {\n-    case T_FLOAT:  slow_case_addr = jni_GetFloatField_addr();  break;\n-    case T_DOUBLE: slow_case_addr = jni_GetDoubleField_addr(); break;\n-    default:       ShouldNotReachHere();\n-  }\n-  \/\/ tail call\n-  __ jump (RuntimeAddress(slow_case_addr));\n-\n-  __ flush ();\n-\n-  return fast_entry;\n-}\n-\n-address JNI_FastGetField::generate_fast_get_float_field() {\n-  return generate_fast_get_float_field0(T_FLOAT);\n-}\n-\n-address JNI_FastGetField::generate_fast_get_double_field() {\n-  return generate_fast_get_float_field0(T_DOUBLE);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/jniFastGetField_x86_32.cpp","additions":0,"deletions":324,"binary":false,"changes":324,"status":"deleted"},{"patch":"@@ -47,7 +47,0 @@\n-#ifndef AMD64\n-  \/\/ 32bit Helper routines.\n-  static inline void    put_int2r(jint *from, intptr_t *to)           { *(jint *)(to++) = from[1];\n-                                                                        *(jint *)(to  ) = from[0]; }\n-  static inline void    put_int2r(jint *from, intptr_t *to, int& pos) { put_int2r(from, to + pos); pos += 2; }\n-#endif \/\/ AMD64\n-\n@@ -60,1 +53,0 @@\n-#ifdef AMD64\n@@ -76,7 +68,0 @@\n-#else\n-  \/\/ Longs are stored in big-endian word format in two JavaCallArgument slots at *to.\n-  \/\/ The high half is in *to and the low half in *(to+1).\n-  static inline void    put_long(jlong  from, intptr_t *to)           { put_int2r((jint *)&from, to); }\n-  static inline void    put_long(jlong  from, intptr_t *to, int& pos) { put_int2r((jint *)&from, to, pos); }\n-  static inline void    put_long(jlong *from, intptr_t *to, int& pos) { put_int2r((jint *) from, to, pos); }\n-#endif \/\/ AMD64\n@@ -94,1 +79,1 @@\n-#ifdef AMD64\n+\n@@ -111,9 +96,0 @@\n-#else\n-#define _JNI_SLOT_OFFSET 0\n-  \/\/ Doubles are stored in big-endian word format in two JavaCallArgument slots at *to.\n-  \/\/ The high half is in *to and the low half in *(to+1).\n-  static inline void    put_double(jdouble  from, intptr_t *to)           { put_int2r((jint *)&from, to); }\n-  static inline void    put_double(jdouble  from, intptr_t *to, int& pos) { put_int2r((jint *)&from, to, pos); }\n-  static inline void    put_double(jdouble *from, intptr_t *to, int& pos) { put_int2r((jint *) from, to, pos); }\n-#endif \/\/ AMD64\n-\n","filename":"src\/hotspot\/cpu\/x86\/jniTypes_x86.hpp","additions":1,"deletions":25,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -81,1 +81,0 @@\n-#ifdef _LP64\n@@ -86,3 +85,0 @@\n-#else\n-    JVMCI_ERROR(\"compressed oop on 32bit\");\n-#endif\n@@ -100,1 +96,0 @@\n-#ifdef _LP64\n@@ -104,3 +99,0 @@\n-#else\n-    JVMCI_ERROR(\"compressed Klass* on 32bit\");\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/jvmciCodeInstaller_x86.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -97,389 +97,0 @@\n-\/\/ First all the versions that have distinct versions depending on 32\/64 bit\n-\/\/ Unless the difference is trivial (1 line or so).\n-\n-#ifndef _LP64\n-\n-\/\/ 32bit versions\n-\n-Address MacroAssembler::as_Address(AddressLiteral adr) {\n-  return Address(adr.target(), adr.rspec());\n-}\n-\n-Address MacroAssembler::as_Address(ArrayAddress adr, Register rscratch) {\n-  assert(rscratch == noreg, \"\");\n-  return Address::make_array(adr);\n-}\n-\n-void MacroAssembler::call_VM_leaf_base(address entry_point,\n-                                       int number_of_arguments) {\n-  call(RuntimeAddress(entry_point));\n-  increment(rsp, number_of_arguments * wordSize);\n-}\n-\n-void MacroAssembler::cmpklass(Address src1, Metadata* obj) {\n-  cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-\n-void MacroAssembler::cmpklass(Register src1, Metadata* obj) {\n-  cmp_literal32(src1, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop(Address src1, jobject obj) {\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::cmpoop(Register src1, jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  cmp_literal32(src1, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::extend_sign(Register hi, Register lo) {\n-  \/\/ According to Intel Doc. AP-526, \"Integer Divide\", p.18.\n-  if (VM_Version::is_P6() && hi == rdx && lo == rax) {\n-    cdql();\n-  } else {\n-    movl(hi, lo);\n-    sarl(hi, 31);\n-  }\n-}\n-\n-void MacroAssembler::jC2(Register tmp, Label& L) {\n-  \/\/ set parity bit if FPU flag C2 is set (via rax)\n-  save_rax(tmp);\n-  fwait(); fnstsw_ax();\n-  sahf();\n-  restore_rax(tmp);\n-  \/\/ branch\n-  jcc(Assembler::parity, L);\n-}\n-\n-void MacroAssembler::jnC2(Register tmp, Label& L) {\n-  \/\/ set parity bit if FPU flag C2 is set (via rax)\n-  save_rax(tmp);\n-  fwait(); fnstsw_ax();\n-  sahf();\n-  restore_rax(tmp);\n-  \/\/ branch\n-  jcc(Assembler::noParity, L);\n-}\n-\n-\/\/ 32bit can do a case table jump in one instruction but we no longer allow the base\n-\/\/ to be installed in the Address class\n-void MacroAssembler::jump(ArrayAddress entry, Register rscratch) {\n-  assert(rscratch == noreg, \"not needed\");\n-  jmp(as_Address(entry, noreg));\n-}\n-\n-\/\/ Note: y_lo will be destroyed\n-void MacroAssembler::lcmp2int(Register x_hi, Register x_lo, Register y_hi, Register y_lo) {\n-  \/\/ Long compare for Java (semantics as described in JVM spec.)\n-  Label high, low, done;\n-\n-  cmpl(x_hi, y_hi);\n-  jcc(Assembler::less, low);\n-  jcc(Assembler::greater, high);\n-  \/\/ x_hi is the return register\n-  xorl(x_hi, x_hi);\n-  cmpl(x_lo, y_lo);\n-  jcc(Assembler::below, low);\n-  jcc(Assembler::equal, done);\n-\n-  bind(high);\n-  xorl(x_hi, x_hi);\n-  increment(x_hi);\n-  jmp(done);\n-\n-  bind(low);\n-  xorl(x_hi, x_hi);\n-  decrementl(x_hi);\n-\n-  bind(done);\n-}\n-\n-void MacroAssembler::lea(Register dst, AddressLiteral src) {\n-  mov_literal32(dst, (int32_t)src.target(), src.rspec());\n-}\n-\n-void MacroAssembler::lea(Address dst, AddressLiteral adr, Register rscratch) {\n-  assert(rscratch == noreg, \"not needed\");\n-\n-  \/\/ leal(dst, as_Address(adr));\n-  \/\/ see note in movl as to why we must use a move\n-  mov_literal32(dst, (int32_t)adr.target(), adr.rspec());\n-}\n-\n-void MacroAssembler::leave() {\n-  mov(rsp, rbp);\n-  pop(rbp);\n-}\n-\n-void MacroAssembler::lmul(int x_rsp_offset, int y_rsp_offset) {\n-  \/\/ Multiplication of two Java long values stored on the stack\n-  \/\/ as illustrated below. Result is in rdx:rax.\n-  \/\/\n-  \/\/ rsp ---> [  ??  ] \\               \\\n-  \/\/            ....    | y_rsp_offset  |\n-  \/\/          [ y_lo ] \/  (in bytes)    | x_rsp_offset\n-  \/\/          [ y_hi ]                  | (in bytes)\n-  \/\/            ....                    |\n-  \/\/          [ x_lo ]                 \/\n-  \/\/          [ x_hi ]\n-  \/\/            ....\n-  \/\/\n-  \/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-  \/\/             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)\n-  Address x_hi(rsp, x_rsp_offset + wordSize); Address x_lo(rsp, x_rsp_offset);\n-  Address y_hi(rsp, y_rsp_offset + wordSize); Address y_lo(rsp, y_rsp_offset);\n-  Label quick;\n-  \/\/ load x_hi, y_hi and check if quick\n-  \/\/ multiplication is possible\n-  movl(rbx, x_hi);\n-  movl(rcx, y_hi);\n-  movl(rax, rbx);\n-  orl(rbx, rcx);                                 \/\/ rbx, = 0 <=> x_hi = 0 and y_hi = 0\n-  jcc(Assembler::zero, quick);                   \/\/ if rbx, = 0 do quick multiply\n-  \/\/ do full multiplication\n-  \/\/ 1st step\n-  mull(y_lo);                                    \/\/ x_hi * y_lo\n-  movl(rbx, rax);                                \/\/ save lo(x_hi * y_lo) in rbx,\n-  \/\/ 2nd step\n-  movl(rax, x_lo);\n-  mull(rcx);                                     \/\/ x_lo * y_hi\n-  addl(rbx, rax);                                \/\/ add lo(x_lo * y_hi) to rbx,\n-  \/\/ 3rd step\n-  bind(quick);                                   \/\/ note: rbx, = 0 if quick multiply!\n-  movl(rax, x_lo);\n-  mull(y_lo);                                    \/\/ x_lo * y_lo\n-  addl(rdx, rbx);                                \/\/ correct hi(x_lo * y_lo)\n-}\n-\n-void MacroAssembler::lneg(Register hi, Register lo) {\n-  negl(lo);\n-  adcl(hi, 0);\n-  negl(hi);\n-}\n-\n-void MacroAssembler::lshl(Register hi, Register lo) {\n-  \/\/ Java shift left long support (semantics as described in JVM spec., p.305)\n-  \/\/ (basic idea for shift counts s >= n: x << s == (x << n) << (s - n))\n-  \/\/ shift value is in rcx !\n-  assert(hi != rcx, \"must not use rcx\");\n-  assert(lo != rcx, \"must not use rcx\");\n-  const Register s = rcx;                        \/\/ shift count\n-  const int      n = BitsPerWord;\n-  Label L;\n-  andl(s, 0x3f);                                 \/\/ s := s & 0x3f (s < 0x40)\n-  cmpl(s, n);                                    \/\/ if (s < n)\n-  jcc(Assembler::less, L);                       \/\/ else (s >= n)\n-  movl(hi, lo);                                  \/\/ x := x << n\n-  xorl(lo, lo);\n-  \/\/ Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!\n-  bind(L);                                       \/\/ s (mod n) < n\n-  shldl(hi, lo);                                 \/\/ x := x << s\n-  shll(lo);\n-}\n-\n-\n-void MacroAssembler::lshr(Register hi, Register lo, bool sign_extension) {\n-  \/\/ Java shift right long support (semantics as described in JVM spec., p.306 & p.310)\n-  \/\/ (basic idea for shift counts s >= n: x >> s == (x >> n) >> (s - n))\n-  assert(hi != rcx, \"must not use rcx\");\n-  assert(lo != rcx, \"must not use rcx\");\n-  const Register s = rcx;                        \/\/ shift count\n-  const int      n = BitsPerWord;\n-  Label L;\n-  andl(s, 0x3f);                                 \/\/ s := s & 0x3f (s < 0x40)\n-  cmpl(s, n);                                    \/\/ if (s < n)\n-  jcc(Assembler::less, L);                       \/\/ else (s >= n)\n-  movl(lo, hi);                                  \/\/ x := x >> n\n-  if (sign_extension) sarl(hi, 31);\n-  else                xorl(hi, hi);\n-  \/\/ Note: subl(s, n) is not needed since the Intel shift instructions work rcx mod n!\n-  bind(L);                                       \/\/ s (mod n) < n\n-  shrdl(lo, hi);                                 \/\/ x := x >> s\n-  if (sign_extension) sarl(hi);\n-  else                shrl(hi);\n-}\n-\n-void MacroAssembler::movoop(Register dst, jobject obj) {\n-  mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::movoop(Address dst, jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  mov_literal32(dst, (int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::mov_metadata(Register dst, Metadata* obj) {\n-  mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::mov_metadata(Address dst, Metadata* obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  mov_literal32(dst, (int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::movptr(Register dst, AddressLiteral src) {\n-  if (src.is_lval()) {\n-    mov_literal32(dst, (intptr_t)src.target(), src.rspec());\n-  } else {\n-    movl(dst, as_Address(src));\n-  }\n-}\n-\n-void MacroAssembler::movptr(ArrayAddress dst, Register src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  movl(as_Address(dst, noreg), src);\n-}\n-\n-void MacroAssembler::movptr(Register dst, ArrayAddress src) {\n-  movl(dst, as_Address(src, noreg));\n-}\n-\n-void MacroAssembler::movptr(Address dst, intptr_t src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  movl(dst, src);\n-}\n-\n-void MacroAssembler::pushoop(jobject obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  push_literal32((int32_t)obj, oop_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::pushklass(Metadata* obj, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  push_literal32((int32_t)obj, metadata_Relocation::spec_for_immediate());\n-}\n-\n-void MacroAssembler::pushptr(AddressLiteral src, Register rscratch) {\n-  assert(rscratch == noreg, \"redundant\");\n-  if (src.is_lval()) {\n-    push_literal32((int32_t)src.target(), src.rspec());\n-  } else {\n-    pushl(as_Address(src));\n-  }\n-}\n-\n-static void pass_arg0(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg1(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg2(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-static void pass_arg3(MacroAssembler* masm, Register arg) {\n-  masm->push(arg);\n-}\n-\n-#ifndef PRODUCT\n-extern \"C\" void findpc(intptr_t x);\n-#endif\n-\n-void MacroAssembler::debug32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip, char* msg) {\n-  \/\/ In order to get locks to work, we need to fake a in_VM state\n-  JavaThread* thread = JavaThread::current();\n-  JavaThreadState saved_state = thread->thread_state();\n-  thread->set_thread_state(_thread_in_vm);\n-  if (ShowMessageBoxOnError) {\n-    JavaThread* thread = JavaThread::current();\n-    JavaThreadState saved_state = thread->thread_state();\n-    thread->set_thread_state(_thread_in_vm);\n-    if (CountBytecodes || TraceBytecodes || StopInterpreterAt) {\n-      ttyLocker ttyl;\n-      BytecodeCounter::print();\n-    }\n-    \/\/ To see where a verify_oop failed, get $ebx+40\/X for this frame.\n-    \/\/ This is the value of eip which points to where verify_oop will return.\n-    if (os::message_box(msg, \"Execution stopped, print registers?\")) {\n-      print_state32(rdi, rsi, rbp, rsp, rbx, rdx, rcx, rax, eip);\n-      BREAKPOINT;\n-    }\n-  }\n-  fatal(\"DEBUG MESSAGE: %s\", msg);\n-}\n-\n-void MacroAssembler::print_state32(int rdi, int rsi, int rbp, int rsp, int rbx, int rdx, int rcx, int rax, int eip) {\n-  ttyLocker ttyl;\n-  DebuggingContext debugging{};\n-  tty->print_cr(\"eip = 0x%08x\", eip);\n-#ifndef PRODUCT\n-  if ((WizardMode || Verbose) && PrintMiscellaneous) {\n-    tty->cr();\n-    findpc(eip);\n-    tty->cr();\n-  }\n-#endif\n-#define PRINT_REG(rax) \\\n-  { tty->print(\"%s = \", #rax); os::print_location(tty, rax); }\n-  PRINT_REG(rax);\n-  PRINT_REG(rbx);\n-  PRINT_REG(rcx);\n-  PRINT_REG(rdx);\n-  PRINT_REG(rdi);\n-  PRINT_REG(rsi);\n-  PRINT_REG(rbp);\n-  PRINT_REG(rsp);\n-#undef PRINT_REG\n-  \/\/ Print some words near top of staack.\n-  int* dump_sp = (int*) rsp;\n-  for (int col1 = 0; col1 < 8; col1++) {\n-    tty->print(\"(rsp+0x%03x) 0x%08x: \", (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);\n-    os::print_location(tty, *dump_sp++);\n-  }\n-  for (int row = 0; row < 16; row++) {\n-    tty->print(\"(rsp+0x%03x) 0x%08x: \", (int)((intptr_t)dump_sp - (intptr_t)rsp), (intptr_t)dump_sp);\n-    for (int col = 0; col < 8; col++) {\n-      tty->print(\" 0x%08x\", *dump_sp++);\n-    }\n-    tty->cr();\n-  }\n-  \/\/ Print some instructions around pc:\n-  Disassembler::decode((address)eip-64, (address)eip);\n-  tty->print_cr(\"--------\");\n-  Disassembler::decode((address)eip, (address)eip+32);\n-}\n-\n-void MacroAssembler::stop(const char* msg) {\n-  \/\/ push address of message\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr(), noreg);\n-  { Label L; call(L, relocInfo::none); bind(L); }     \/\/ push eip\n-  pusha();                                            \/\/ push registers\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug32)));\n-  hlt();\n-}\n-\n-void MacroAssembler::warn(const char* msg) {\n-  push_CPU_state();\n-\n-  \/\/ push address of message\n-  ExternalAddress message((address)msg);\n-  pushptr(message.addr(), noreg);\n-\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, warning)));\n-  addl(rsp, wordSize);       \/\/ discard argument\n-  pop_CPU_state();\n-}\n-\n-void MacroAssembler::print_state() {\n-  { Label L; call(L, relocInfo::none); bind(L); }     \/\/ push eip\n-  pusha();                                            \/\/ push registers\n-\n-  push_CPU_state();\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::print_state32)));\n-  pop_CPU_state();\n-\n-  popa();\n-  addl(rsp, wordSize);\n-}\n-\n-#else \/\/ _LP64\n-\n-\/\/ 64 bit versions\n-\n@@ -1100,4 +711,0 @@\n-#endif \/\/ _LP64\n-\n-\/\/ Now versions that are common to 32\/64 bit\n-\n@@ -1105,1 +712,1 @@\n-  LP64_ONLY(addq(dst, imm32)) NOT_LP64(addl(dst, imm32));\n+  addq(dst, imm32);\n@@ -1109,1 +716,1 @@\n-  LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));\n+  addq(dst, src);\n@@ -1113,1 +720,1 @@\n-  LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src));\n+  addq(dst, src);\n@@ -1219,1 +826,1 @@\n-  LP64_ONLY(andq(dst, imm32)) NOT_LP64(andl(dst, imm32));\n+  andq(dst, imm32);\n@@ -1222,1 +829,0 @@\n-#ifdef _LP64\n@@ -1233,1 +839,0 @@\n-#endif\n@@ -1251,1 +856,0 @@\n-#ifdef _LP64\n@@ -1267,1 +871,0 @@\n-#endif\n@@ -1299,2 +902,1 @@\n-  Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-  NOT_LP64(get_thread(rsi);)\n+  Register thread = r15_thread;\n@@ -1343,1 +945,0 @@\n-#ifdef _LP64\n@@ -1346,3 +947,0 @@\n-#else\n-  movptr(rax, (intptr_t)Universe::non_oop_word());\n-#endif\n@@ -1353,2 +951,1 @@\n-  return\n-      LP64_ONLY(UseCompactObjectHeaders ? 17 : 14) NOT_LP64(12);\n+  return UseCompactObjectHeaders ? 17 : 14;\n@@ -1358,1 +955,1 @@\n-  Register receiver = LP64_ONLY(j_rarg0) NOT_LP64(rcx);\n+  Register receiver = j_rarg0;\n@@ -1360,1 +957,1 @@\n-  Register temp = LP64_ONLY(rscratch1) NOT_LP64(rbx);\n+  Register temp = rscratch1;\n@@ -1370,1 +967,0 @@\n-#ifdef _LP64\n@@ -1374,3 +970,1 @@\n-  } else\n-#endif\n-  if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers) {\n@@ -1441,1 +1035,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1463,2 +1057,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1479,2 +1073,1 @@\n-  Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);\n-  call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, r15_thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n@@ -1499,1 +1092,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1512,2 +1105,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1525,2 +1118,1 @@\n-  Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);\n-  MacroAssembler::call_VM_base(oop_result, thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+  MacroAssembler::call_VM_base(oop_result, r15_thread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n@@ -1545,1 +1137,1 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1558,2 +1150,2 @@\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1574,1 +1166,0 @@\n-#ifdef _LP64\n@@ -1576,4 +1167,0 @@\n-#else\n-    java_thread = rdi;\n-    get_thread(java_thread);\n-#endif \/\/ LP64\n@@ -1587,1 +1174,1 @@\n-  LP64_ONLY(assert(java_thread == r15_thread, \"unexpected register\"));\n+  assert(java_thread == r15_thread, \"unexpected register\");\n@@ -1591,1 +1178,3 @@\n-  LP64_ONLY(if (UseCompressedOops && !TraceBytecodes) verify_heapbase(\"call_VM_base: heap base corrupted?\");)\n+  if (UseCompressedOops && !TraceBytecodes) {\n+    verify_heapbase(\"call_VM_base: heap base corrupted?\");\n+  }\n@@ -1599,2 +1188,1 @@\n-  NOT_LP64(push(java_thread); number_of_arguments++);\n-  LP64_ONLY(mov(c_rarg0, r15_thread));\n+  mov(c_rarg0, r15_thread);\n@@ -1614,2 +1202,1 @@\n-  if (LP64_ONLY(true ||) java_thread == rdi || java_thread == rsi) {\n-    \/\/ rdi & rsi (also r15) are callee saved -> nothing to do\n+  \/\/ rdi & rsi (also r15) are callee saved -> nothing to do\n@@ -1628,3 +1215,1 @@\n-  } else {\n-    get_thread(java_thread);\n-  }\n+\n@@ -1642,4 +1227,1 @@\n-#ifndef _LP64\n-    jump_cc(Assembler::notEqual,\n-            RuntimeAddress(StubRoutines::forward_exception_entry()));\n-#else\n+\n@@ -1654,1 +1236,0 @@\n-#endif \/\/ LP64\n@@ -1675,1 +1256,0 @@\n-#ifdef _LP64\n@@ -1678,3 +1258,0 @@\n-#else\n-  lea(rax, Address(rsp, (1 + number_of_arguments) * wordSize));\n-#endif \/\/ LP64\n@@ -1701,2 +1278,1 @@\n-\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1709,2 +1285,2 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1718,3 +1294,3 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1734,1 +1310,1 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1));\n+  assert_different_registers(arg_0, c_rarg1);\n@@ -1741,2 +1317,2 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2);\n+  assert_different_registers(arg_1, c_rarg2);\n@@ -1750,3 +1326,3 @@\n-  LP64_ONLY(assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_1, c_rarg2, c_rarg3));\n-  LP64_ONLY(assert_different_registers(arg_2, c_rarg3));\n+  assert_different_registers(arg_0, c_rarg1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_1, c_rarg2, c_rarg3);\n+  assert_different_registers(arg_2, c_rarg3);\n@@ -1865,1 +1441,0 @@\n-#ifdef _LP64\n@@ -1877,8 +1452,0 @@\n-#else\n-  assert(rscratch == noreg, \"not needed\");\n-  if (src2.is_lval()) {\n-    cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n-  } else {\n-    cmpl(src1, as_Address(src2));\n-  }\n-#endif \/\/ _LP64\n@@ -1889,1 +1456,0 @@\n-#ifdef _LP64\n@@ -1893,4 +1459,0 @@\n-#else\n-  assert(rscratch == noreg, \"not needed\");\n-  cmp_literal32(src1, (int32_t)src2.target(), src2.rspec());\n-#endif \/\/ _LP64\n@@ -1907,1 +1469,0 @@\n-#ifdef _LP64\n@@ -1912,1 +1473,0 @@\n-#endif\n@@ -1928,1 +1488,1 @@\n-  LP64_ONLY(cmpxchgq(reg, adr)) NOT_LP64(cmpxchgl(reg, adr));\n+  cmpxchgq(reg, adr);\n@@ -2091,109 +1651,0 @@\n-#ifndef _LP64\n-void MacroAssembler::fcmp(Register tmp) {\n-  fcmp(tmp, 1, true, true);\n-}\n-\n-void MacroAssembler::fcmp(Register tmp, int index, bool pop_left, bool pop_right) {\n-  assert(!pop_right || pop_left, \"usage error\");\n-  if (VM_Version::supports_cmov()) {\n-    assert(tmp == noreg, \"unneeded temp\");\n-    if (pop_left) {\n-      fucomip(index);\n-    } else {\n-      fucomi(index);\n-    }\n-    if (pop_right) {\n-      fpop();\n-    }\n-  } else {\n-    assert(tmp != noreg, \"need temp\");\n-    if (pop_left) {\n-      if (pop_right) {\n-        fcompp();\n-      } else {\n-        fcomp(index);\n-      }\n-    } else {\n-      fcom(index);\n-    }\n-    \/\/ convert FPU condition into eflags condition via rax,\n-    save_rax(tmp);\n-    fwait(); fnstsw_ax();\n-    sahf();\n-    restore_rax(tmp);\n-  }\n-  \/\/ condition codes set as follows:\n-  \/\/\n-  \/\/ CF (corresponds to C0) if x < y\n-  \/\/ PF (corresponds to C2) if unordered\n-  \/\/ ZF (corresponds to C3) if x = y\n-}\n-\n-void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less) {\n-  fcmp2int(dst, unordered_is_less, 1, true, true);\n-}\n-\n-void MacroAssembler::fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right) {\n-  fcmp(VM_Version::supports_cmov() ? noreg : dst, index, pop_left, pop_right);\n-  Label L;\n-  if (unordered_is_less) {\n-    movl(dst, -1);\n-    jcc(Assembler::parity, L);\n-    jcc(Assembler::below , L);\n-    movl(dst, 0);\n-    jcc(Assembler::equal , L);\n-    increment(dst);\n-  } else { \/\/ unordered is greater\n-    movl(dst, 1);\n-    jcc(Assembler::parity, L);\n-    jcc(Assembler::above , L);\n-    movl(dst, 0);\n-    jcc(Assembler::equal , L);\n-    decrementl(dst);\n-  }\n-  bind(L);\n-}\n-\n-void MacroAssembler::fld_d(AddressLiteral src) {\n-  fld_d(as_Address(src));\n-}\n-\n-void MacroAssembler::fld_s(AddressLiteral src) {\n-  fld_s(as_Address(src));\n-}\n-\n-void MacroAssembler::fldcw(AddressLiteral src) {\n-  fldcw(as_Address(src));\n-}\n-\n-void MacroAssembler::fpop() {\n-  ffree();\n-  fincstp();\n-}\n-\n-void MacroAssembler::fremr(Register tmp) {\n-  save_rax(tmp);\n-  { Label L;\n-    bind(L);\n-    fprem();\n-    fwait(); fnstsw_ax();\n-    sahf();\n-    jcc(Assembler::parity, L);\n-  }\n-  restore_rax(tmp);\n-  \/\/ Result is in ST0.\n-  \/\/ Note: fxch & fpop to get rid of ST1\n-  \/\/ (otherwise FPU stack could overflow eventually)\n-  fxch(1);\n-  fpop();\n-}\n-\n-void MacroAssembler::empty_FPU_stack() {\n-  if (VM_Version::supports_mmx()) {\n-    emms();\n-  } else {\n-    for (int i = 8; i-- > 0; ) ffree(i);\n-  }\n-}\n-#endif \/\/ !LP64\n-\n@@ -2211,1 +1662,0 @@\n-#ifdef _LP64\n@@ -2213,7 +1663,0 @@\n-#else\n-  if (UseSSE >= 1) {\n-    movflt(xmm0, src);\n-  } else {\n-    fld_s(src);\n-  }\n-#endif \/\/ LP64\n@@ -2223,1 +1666,0 @@\n-#ifdef _LP64\n@@ -2225,7 +1667,0 @@\n-#else\n-  if (UseSSE >= 1) {\n-    movflt(dst, xmm0);\n-  } else {\n-    fstp_s(dst);\n-  }\n-#endif \/\/ LP64\n@@ -2235,1 +1670,0 @@\n-#ifdef _LP64\n@@ -2237,7 +1671,0 @@\n-#else\n-  if (UseSSE >= 2) {\n-    movdbl(xmm0, src);\n-  } else {\n-    fld_d(src);\n-  }\n-#endif \/\/ LP64\n@@ -2247,1 +1674,0 @@\n-#ifdef _LP64\n@@ -2249,7 +1675,0 @@\n-#else\n-  if (UseSSE >= 2) {\n-    movdbl(dst, xmm0);\n-  } else {\n-    fstp_d(dst);\n-  }\n-#endif \/\/ LP64\n@@ -2391,9 +1810,2 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    off = offset();\n-    movsbl(dst, src); \/\/ movsxb\n-  } else {\n-    off = load_unsigned_byte(dst, src);\n-    shll(dst, 24);\n-    sarl(dst, 24);\n-  }\n+  int off = offset();\n+  movsbl(dst, src); \/\/ movsxb\n@@ -2408,12 +1820,5 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    \/\/ This is dubious to me since it seems safe to do a signed 16 => 64 bit\n-    \/\/ version but this is what 64bit has always done. This seems to imply\n-    \/\/ that users are only using 32bits worth.\n-    off = offset();\n-    movswl(dst, src); \/\/ movsxw\n-  } else {\n-    off = load_unsigned_short(dst, src);\n-    shll(dst, 16);\n-    sarl(dst, 16);\n-  }\n+  int off = offset();\n+  \/\/ This is dubious to me since it seems safe to do a signed 16 => 64 bit\n+  \/\/ version but this is what 64bit has always done. This seems to imply\n+  \/\/ that users are only using 32bits worth.\n+  movswl(dst, src); \/\/ movsxw\n@@ -2426,9 +1831,2 @@\n-  int off;\n-  if (LP64_ONLY(true || ) VM_Version::is_P6() || src.uses(dst)) {\n-    off = offset();\n-    movzbl(dst, src); \/\/ movzxb\n-  } else {\n-    xorl(dst, dst);\n-    off = offset();\n-    movb(dst, src);\n-  }\n+  int off = offset();\n+  movzbl(dst, src); \/\/ movzxb\n@@ -2442,9 +1840,2 @@\n-  int off;\n-  if (LP64_ONLY(true ||) VM_Version::is_P6() || src.uses(dst)) {\n-    off = offset();\n-    movzwl(dst, src); \/\/ movzxw\n-  } else {\n-    xorl(dst, dst);\n-    off = offset();\n-    movw(dst, src);\n-  }\n+  int off = offset();\n+  movzwl(dst, src); \/\/ movzxw\n@@ -2456,7 +1847,0 @@\n-#ifndef _LP64\n-  case  8:\n-    assert(dst2 != noreg, \"second dest register required\");\n-    movl(dst,  src);\n-    movl(dst2, src.plus_disp(BytesPerInt));\n-    break;\n-#else\n@@ -2464,1 +1848,0 @@\n-#endif\n@@ -2474,7 +1857,0 @@\n-#ifndef _LP64\n-  case  8:\n-    assert(src2 != noreg, \"second source register required\");\n-    movl(dst,                        src);\n-    movl(dst.plus_disp(BytesPerInt), src2);\n-    break;\n-#else\n@@ -2482,1 +1858,0 @@\n-#endif\n@@ -2601,1 +1976,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2605,1 +1980,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2610,1 +1985,0 @@\n-#ifdef _LP64\n@@ -2618,3 +1992,0 @@\n-#else\n-  movl(dst, src);\n-#endif\n@@ -2624,1 +1995,1 @@\n-  LP64_ONLY(movq(dst, src)) NOT_LP64(movl(dst, src));\n+  movq(dst, src);\n@@ -2628,1 +1999,1 @@\n-  LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src));\n+  movslq(dst, src);\n@@ -2929,1 +2300,0 @@\n-#ifdef _LP64\n@@ -2931,1 +2301,0 @@\n-#endif\n@@ -2939,3 +2308,0 @@\n-#ifndef _LP64\n-  frstor(Address(rsp, 0));\n-#else\n@@ -2943,1 +2309,0 @@\n-#endif\n@@ -2949,1 +2314,1 @@\n-  LP64_ONLY(addq(rsp, 8));\n+  addq(rsp, 8);\n@@ -2962,4 +2327,0 @@\n-#ifndef _LP64\n-  fnsave(Address(rsp, 0));\n-  fwait();\n-#else\n@@ -2967,1 +2328,0 @@\n-#endif \/\/ LP64\n@@ -2974,1 +2334,1 @@\n-  LP64_ONLY(subq(rsp, 8));\n+  subq(rsp, 8);\n@@ -2981,13 +2341,0 @@\n-#ifndef _LP64\n-  Register rthread = rax;\n-  Register rrealsp = rbx;\n-  push(rthread);\n-  push(rrealsp);\n-\n-  get_thread(rthread);\n-\n-  \/\/ The code below wants the original RSP.\n-  \/\/ Move it back after the pushes above.\n-  movptr(rrealsp, rsp);\n-  addptr(rrealsp, 2*wordSize);\n-#else\n@@ -2996,1 +2343,0 @@\n-#endif\n@@ -3003,5 +2349,0 @@\n-\n-#ifndef _LP64\n-  pop(rrealsp);\n-  pop(rthread);\n-#endif\n@@ -3013,13 +2354,0 @@\n-#ifndef _LP64\n-  Register rthread = rax;\n-  Register rrealsp = rbx;\n-  push(rthread);\n-  push(rrealsp);\n-\n-  get_thread(rthread);\n-\n-  \/\/ The code below wants the original RSP.\n-  \/\/ Move it back after the pushes above.\n-  movptr(rrealsp, rsp);\n-  addptr(rrealsp, 2*wordSize);\n-#else\n@@ -3028,1 +2356,0 @@\n-#endif\n@@ -3035,5 +2362,0 @@\n-\n-#ifndef _LP64\n-  pop(rrealsp);\n-  pop(rthread);\n-#endif\n@@ -3043,1 +2365,0 @@\n-#ifdef _LP64\n@@ -3045,1 +2366,0 @@\n-#endif\n@@ -3049,1 +2369,0 @@\n-#ifdef _LP64\n@@ -3051,1 +2370,0 @@\n-#endif\n@@ -3056,1 +2374,0 @@\n-#ifdef _LP64\n@@ -3063,3 +2380,0 @@\n-#else\n-  Unimplemented();\n-#endif\n@@ -3146,1 +2460,0 @@\n-#ifdef _LP64\n@@ -3155,1 +2468,0 @@\n-#endif\n@@ -3158,1 +2470,1 @@\n-  LP64_ONLY(shlq(dst, imm8)) NOT_LP64(shll(dst, imm8));\n+  shlq(dst, imm8);\n@@ -3162,1 +2474,1 @@\n-  LP64_ONLY(shrq(dst, imm8)) NOT_LP64(shrl(dst, imm8));\n+  shrq(dst, imm8);;\n@@ -3166,6 +2478,1 @@\n-  if (LP64_ONLY(true ||) (VM_Version::is_P6() && reg->has_byte_register())) {\n-    movsbl(reg, reg); \/\/ movsxb\n-  } else {\n-    shll(reg, 24);\n-    sarl(reg, 24);\n-  }\n+  movsbl(reg, reg); \/\/ movsxb\n@@ -3175,6 +2482,1 @@\n-  if (LP64_ONLY(true ||) VM_Version::is_P6()) {\n-    movswl(reg, reg); \/\/ movsxw\n-  } else {\n-    shll(reg, 16);\n-    sarl(reg, 16);\n-  }\n+  movswl(reg, reg); \/\/ movsxw\n@@ -3204,2 +2506,0 @@\n-#ifdef _LP64\n-\n@@ -3222,2 +2522,0 @@\n-#endif\n-\n@@ -4056,1 +3354,1 @@\n-  LP64_ONLY(subq(dst, imm32)) NOT_LP64(subl(dst, imm32));\n+  subq(dst, imm32);\n@@ -4061,1 +3359,1 @@\n-  LP64_ONLY(subq_imm32(dst, imm32)) NOT_LP64(subl_imm32(dst, imm32));\n+  subq_imm32(dst, imm32);\n@@ -4065,1 +3363,1 @@\n-  LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src));\n+  subq(dst, src);\n@@ -4083,1 +3381,1 @@\n-  LP64_ONLY(testq(dst, src)) NOT_LP64(testl(dst, src));\n+  testq(dst, src);\n@@ -4099,1 +3397,0 @@\n-#ifdef _LP64\n@@ -4105,4 +3402,0 @@\n-#else\n-  regs += RegSet::of(rax, rcx, rdx);\n-#endif\n-#ifdef _LP64\n@@ -4112,1 +3405,0 @@\n-#endif\n@@ -4129,7 +3421,0 @@\n-static int FPUSaveAreaSize = align_up(108, StackAlignmentInBytes); \/\/ 108 bytes needed for FPU state by fsave\/frstor\n-\n-#ifndef _LP64\n-static bool use_x87_registers() { return UseSSE < 2; }\n-#endif\n-static bool use_xmm_registers() { return UseSSE >= 1; }\n-\n@@ -4137,1 +3422,1 @@\n-static int xmm_save_size() { return UseSSE >= 2 ? sizeof(double) : sizeof(float); }\n+static int xmm_save_size() { return sizeof(double); }\n@@ -4140,5 +3425,1 @@\n-  if (UseSSE == 1) {\n-    masm->movflt(Address(rsp, offset), reg);\n-  } else {\n-    masm->movdbl(Address(rsp, offset), reg);\n-  }\n+  masm->movdbl(Address(rsp, offset), reg);\n@@ -4148,5 +3429,1 @@\n-  if (UseSSE == 1) {\n-    masm->movflt(reg, Address(rsp, offset));\n-  } else {\n-    masm->movdbl(reg, Address(rsp, offset));\n-  }\n+  masm->movdbl(reg, Address(rsp, offset));\n@@ -4161,1 +3438,0 @@\n-#ifdef _LP64\n@@ -4163,4 +3439,1 @@\n-#else\n-  fp_area_size = (save_fpu && use_x87_registers()) ? FPUSaveAreaSize : 0;\n-#endif\n-  xmm_area_size = (save_fpu && use_xmm_registers()) ? xmm_registers.size() * xmm_save_size() : 0;\n+  xmm_area_size = save_fpu ? xmm_registers.size() * xmm_save_size() : 0;\n@@ -4185,7 +3458,1 @@\n-#ifndef _LP64\n-  if (save_fpu && use_x87_registers()) {\n-    fnsave(Address(rsp, gp_area_size));\n-    fwait();\n-  }\n-#endif\n-  if (save_fpu && use_xmm_registers()) {\n+  if (save_fpu) {\n@@ -4209,1 +3476,1 @@\n-  if (restore_fpu && use_xmm_registers()) {\n+  if (restore_fpu) {\n@@ -4212,5 +3479,0 @@\n-#ifndef _LP64\n-  if (restore_fpu && use_x87_registers()) {\n-    frstor(Address(rsp, gp_area_size));\n-  }\n-#endif\n@@ -4316,15 +3578,1 @@\n-#ifndef _LP64\n-  \/\/ index could have not been a multiple of 8 (i.e., bit 2 was set)\n-  {\n-    Label even;\n-    \/\/ note: if index was a multiple of 8, then it cannot\n-    \/\/       be 0 now otherwise it must have been 0 before\n-    \/\/       => if it is even, we don't need to check for 0 again\n-    jcc(Assembler::carryClear, even);\n-    \/\/ clear topmost word (no jump would be needed if conditional assignment worked here)\n-    movptr(Address(address, index, Address::times_8, offset_in_bytes - 0*BytesPerWord), temp);\n-    \/\/ index could be 0 now, must check again\n-    jcc(Assembler::zero, done);\n-    bind(even);\n-  }\n-#endif \/\/ !_LP64\n+\n@@ -4336,1 +3584,0 @@\n-    NOT_LP64(movptr(Address(address, index, Address::times_8, offset_in_bytes - 2*BytesPerWord), temp);)\n@@ -4713,3 +3960,2 @@\n-  NOT_LP64(  incrementl(pst_counter_addr) );\n-  LP64_ONLY( lea(rcx, pst_counter_addr) );\n-  LP64_ONLY( incrementl(Address(rcx, 0)) );\n+  lea(rcx, pst_counter_addr);\n+  incrementl(Address(rcx, 0));\n@@ -4761,16 +4007,0 @@\n-#ifndef _LP64\n-\n-\/\/ 32-bit x86 only: always use the linear search.\n-void MacroAssembler::check_klass_subtype_slow_path(Register sub_klass,\n-                                                   Register super_klass,\n-                                                   Register temp_reg,\n-                                                   Register temp2_reg,\n-                                                   Label* L_success,\n-                                                   Label* L_failure,\n-                                                   bool set_cond_codes) {\n-  check_klass_subtype_slow_path_linear\n-    (sub_klass, super_klass, temp_reg, temp2_reg, L_success, L_failure, set_cond_codes);\n-}\n-\n-#else \/\/ _LP64\n-\n@@ -5360,2 +4590,0 @@\n-#endif \/\/ LP64\n-\n@@ -5416,1 +4644,0 @@\n-#ifdef _LP64\n@@ -5418,1 +4645,0 @@\n-#endif\n@@ -5476,1 +4702,0 @@\n-#ifdef _LP64\n@@ -5478,1 +4703,0 @@\n-#endif\n@@ -5486,1 +4710,1 @@\n-    pushptr(Address(rax, LP64_ONLY(2 *) BytesPerWord));\n+    pushptr(Address(rax, 2 * BytesPerWord));\n@@ -5513,1 +4737,1 @@\n-    Register thread_reg = NOT_LP64(rbx) LP64_ONLY(r15_thread);\n+    Register thread_reg = r15_thread;\n@@ -5516,2 +4740,0 @@\n-    NOT_LP64(push(thread_reg));\n-    NOT_LP64(get_thread(thread_reg));\n@@ -5533,1 +4755,0 @@\n-    NOT_LP64(pop(thread_reg));\n@@ -5814,78 +5035,0 @@\n-#ifndef _LP64\n-static bool _verify_FPU(int stack_depth, char* s, CPU_State* state) {\n-  static int counter = 0;\n-  FPU_State* fs = &state->_fpu_state;\n-  counter++;\n-  \/\/ For leaf calls, only verify that the top few elements remain empty.\n-  \/\/ We only need 1 empty at the top for C2 code.\n-  if( stack_depth < 0 ) {\n-    if( fs->tag_for_st(7) != 3 ) {\n-      printf(\"FPR7 not empty\\n\");\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-    return true;                \/\/ All other stack states do not matter\n-  }\n-\n-  assert((fs->_control_word._value & 0xffff) == StubRoutines::x86::fpu_cntrl_wrd_std(),\n-         \"bad FPU control word\");\n-\n-  \/\/ compute stack depth\n-  int i = 0;\n-  while (i < FPU_State::number_of_registers && fs->tag_for_st(i)  < 3) i++;\n-  int d = i;\n-  while (i < FPU_State::number_of_registers && fs->tag_for_st(i) == 3) i++;\n-  \/\/ verify findings\n-  if (i != FPU_State::number_of_registers) {\n-    \/\/ stack not contiguous\n-    printf(\"%s: stack not contiguous at ST%d\\n\", s, i);\n-    state->print();\n-    assert(false, \"error\");\n-    return false;\n-  }\n-  \/\/ check if computed stack depth corresponds to expected stack depth\n-  if (stack_depth < 0) {\n-    \/\/ expected stack depth is -stack_depth or less\n-    if (d > -stack_depth) {\n-      \/\/ too many elements on the stack\n-      printf(\"%s: <= %d stack elements expected but found %d\\n\", s, -stack_depth, d);\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-  } else {\n-    \/\/ expected stack depth is stack_depth\n-    if (d != stack_depth) {\n-      \/\/ wrong stack depth\n-      printf(\"%s: %d stack elements expected but found %d\\n\", s, stack_depth, d);\n-      state->print();\n-      assert(false, \"error\");\n-      return false;\n-    }\n-  }\n-  \/\/ everything is cool\n-  return true;\n-}\n-\n-void MacroAssembler::verify_FPU(int stack_depth, const char* s) {\n-  if (!VerifyFPU) return;\n-  push_CPU_state();\n-  push(rsp);                \/\/ pass CPU state\n-  ExternalAddress msg((address) s);\n-  \/\/ pass message string s\n-  pushptr(msg.addr(), noreg);\n-  push(stack_depth);        \/\/ pass stack depth\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, _verify_FPU)));\n-  addptr(rsp, 3 * wordSize);   \/\/ discard arguments\n-  \/\/ check for error\n-  { Label L;\n-    testl(rax, rax);\n-    jcc(Assembler::notZero, L);\n-    int3();                  \/\/ break if error condition\n-    bind(L);\n-  }\n-  pop_CPU_state();\n-}\n-#endif \/\/ _LP64\n-\n@@ -5904,8 +5047,0 @@\n-\n-#ifndef _LP64\n-  \/\/ Either restore the x87 floating pointer control word after returning\n-  \/\/ from the JNI call or verify that it wasn't changed.\n-  if (CheckJNICalls) {\n-    call(RuntimeAddress(StubRoutines::x86::verify_fpu_cntrl_wrd_entry()));\n-  }\n-#endif \/\/ _LP64\n@@ -5961,1 +5096,0 @@\n-#ifdef _LP64\n@@ -5967,1 +5101,0 @@\n-#endif\n@@ -5972,1 +5105,0 @@\n-#ifdef _LP64\n@@ -5979,3 +5111,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -5990,1 +5120,0 @@\n-#ifdef _LP64\n@@ -5994,2 +5123,1 @@\n-  } else\n-#endif\n+  } else {\n@@ -5997,0 +5125,1 @@\n+  }\n@@ -6000,1 +5129,0 @@\n-#ifdef _LP64\n@@ -6008,3 +5136,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6016,1 +5142,0 @@\n-#ifdef _LP64\n@@ -6026,3 +5151,1 @@\n-  } else\n-#endif\n-  {\n+  } else {\n@@ -6079,1 +5202,0 @@\n-#ifdef _LP64\n@@ -6401,2 +5523,0 @@\n-#endif \/\/ _LP64\n-\n@@ -6581,2 +5701,0 @@\n-    NOT_LP64(shlptr(cnt, 1);) \/\/ convert to number of 32-bit words for 32-bit VM\n-\n@@ -6603,1 +5721,0 @@\n-    NOT_LP64(shlptr(cnt, 1);) \/\/ convert to number of 32-bit words for 32-bit VM\n@@ -6621,1 +5738,1 @@\n-#if defined(COMPILER2) && defined(_LP64)\n+#if defined(COMPILER2)\n@@ -6682,33 +5799,1 @@\n-  if (UseSSE < 2) {\n-    Label L_fill_32_bytes_loop, L_check_fill_8_bytes, L_fill_8_bytes_loop, L_fill_8_bytes;\n-    \/\/ Fill 32-byte chunks\n-    subptr(count, 8 << shift);\n-    jcc(Assembler::less, L_check_fill_8_bytes);\n-    align(16);\n-\n-    BIND(L_fill_32_bytes_loop);\n-\n-    for (int i = 0; i < 32; i += 4) {\n-      movl(Address(to, i), value);\n-    }\n-\n-    addptr(to, 32);\n-    subptr(count, 8 << shift);\n-    jcc(Assembler::greaterEqual, L_fill_32_bytes_loop);\n-    BIND(L_check_fill_8_bytes);\n-    addptr(count, 8 << shift);\n-    jccb(Assembler::zero, L_exit);\n-    jmpb(L_fill_8_bytes);\n-\n-    \/\/\n-    \/\/ length is too short, just fill qwords\n-    \/\/\n-    BIND(L_fill_8_bytes_loop);\n-    movl(Address(to, 0), value);\n-    movl(Address(to, 4), value);\n-    addptr(to, 8);\n-    BIND(L_fill_8_bytes);\n-    subptr(count, 1 << (shift + 1));\n-    jcc(Assembler::greaterEqual, L_fill_8_bytes_loop);\n-    \/\/ fall through to fill 4 bytes\n-  } else {\n+  {\n@@ -6726,1 +5811,0 @@\n-      assert( UseSSE >= 2, \"supported cpu only\" );\n@@ -7034,1 +6118,0 @@\n-#ifdef _LP64\n@@ -8209,1 +7292,0 @@\n-#endif\n@@ -8432,1 +7514,0 @@\n-#ifdef _LP64\n@@ -8950,147 +8031,0 @@\n-#else\n-void MacroAssembler::crc32c_ipl_alg4(Register in_out, uint32_t n,\n-                                     Register tmp1, Register tmp2, Register tmp3,\n-                                     XMMRegister xtmp1, XMMRegister xtmp2) {\n-  lea(tmp3, ExternalAddress(StubRoutines::crc32c_table_addr()));\n-  if (n > 0) {\n-    addl(tmp3, n * 256 * 8);\n-  }\n-  \/\/    Q1 = TABLEExt[n][B & 0xFF];\n-  movl(tmp1, in_out);\n-  andl(tmp1, 0x000000FF);\n-  shll(tmp1, 3);\n-  addl(tmp1, tmp3);\n-  movq(xtmp1, Address(tmp1, 0));\n-\n-  \/\/    Q2 = TABLEExt[n][B >> 8 & 0xFF];\n-  movl(tmp2, in_out);\n-  shrl(tmp2, 8);\n-  andl(tmp2, 0x000000FF);\n-  shll(tmp2, 3);\n-  addl(tmp2, tmp3);\n-  movq(xtmp2, Address(tmp2, 0));\n-\n-  psllq(xtmp2, 8);\n-  pxor(xtmp1, xtmp2);\n-\n-  \/\/    Q3 = TABLEExt[n][B >> 16 & 0xFF];\n-  movl(tmp2, in_out);\n-  shrl(tmp2, 16);\n-  andl(tmp2, 0x000000FF);\n-  shll(tmp2, 3);\n-  addl(tmp2, tmp3);\n-  movq(xtmp2, Address(tmp2, 0));\n-\n-  psllq(xtmp2, 16);\n-  pxor(xtmp1, xtmp2);\n-\n-  \/\/    Q4 = TABLEExt[n][B >> 24 & 0xFF];\n-  shrl(in_out, 24);\n-  andl(in_out, 0x000000FF);\n-  shll(in_out, 3);\n-  addl(in_out, tmp3);\n-  movq(xtmp2, Address(in_out, 0));\n-\n-  psllq(xtmp2, 24);\n-  pxor(xtmp1, xtmp2); \/\/ Result in CXMM\n-  \/\/    return Q1 ^ Q2 << 8 ^ Q3 << 16 ^ Q4 << 24;\n-}\n-\n-void MacroAssembler::crc32c_pclmulqdq(XMMRegister w_xtmp1,\n-                                      Register in_out,\n-                                      uint32_t const_or_pre_comp_const_index, bool is_pclmulqdq_supported,\n-                                      XMMRegister w_xtmp2,\n-                                      Register tmp1,\n-                                      Register n_tmp2, Register n_tmp3) {\n-  if (is_pclmulqdq_supported) {\n-    movdl(w_xtmp1, in_out);\n-\n-    movl(tmp1, const_or_pre_comp_const_index);\n-    movdl(w_xtmp2, tmp1);\n-    pclmulqdq(w_xtmp1, w_xtmp2, 0);\n-    \/\/ Keep result in XMM since GPR is 32 bit in length\n-  } else {\n-    crc32c_ipl_alg4(in_out, const_or_pre_comp_const_index, tmp1, n_tmp2, n_tmp3, w_xtmp1, w_xtmp2);\n-  }\n-}\n-\n-void MacroAssembler::crc32c_rec_alt2(uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported, Register in_out, Register in1, Register in2,\n-                                     XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                     Register tmp1, Register tmp2,\n-                                     Register n_tmp3) {\n-  crc32c_pclmulqdq(w_xtmp1, in_out, const_or_pre_comp_const_index_u1, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);\n-  crc32c_pclmulqdq(w_xtmp2, in1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, w_xtmp3, tmp1, tmp2, n_tmp3);\n-\n-  psllq(w_xtmp1, 1);\n-  movdl(tmp1, w_xtmp1);\n-  psrlq(w_xtmp1, 32);\n-  movdl(in_out, w_xtmp1);\n-\n-  xorl(tmp2, tmp2);\n-  crc32(tmp2, tmp1, 4);\n-  xorl(in_out, tmp2);\n-\n-  psllq(w_xtmp2, 1);\n-  movdl(tmp1, w_xtmp2);\n-  psrlq(w_xtmp2, 32);\n-  movdl(in1, w_xtmp2);\n-\n-  xorl(tmp2, tmp2);\n-  crc32(tmp2, tmp1, 4);\n-  xorl(in1, tmp2);\n-  xorl(in_out, in1);\n-  xorl(in_out, in2);\n-}\n-\n-void MacroAssembler::crc32c_proc_chunk(uint32_t size, uint32_t const_or_pre_comp_const_index_u1, uint32_t const_or_pre_comp_const_index_u2, bool is_pclmulqdq_supported,\n-                                       Register in_out1, Register in_out2, Register in_out3,\n-                                       Register tmp1, Register tmp2, Register tmp3,\n-                                       XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                       Register tmp4, Register tmp5,\n-                                       Register n_tmp6) {\n-  Label L_processPartitions;\n-  Label L_processPartition;\n-  Label L_exit;\n-\n-  bind(L_processPartitions);\n-  cmpl(in_out1, 3 * size);\n-  jcc(Assembler::less, L_exit);\n-    xorl(tmp1, tmp1);\n-    xorl(tmp2, tmp2);\n-    movl(tmp3, in_out2);\n-    addl(tmp3, size);\n-\n-    bind(L_processPartition);\n-      crc32(in_out3, Address(in_out2, 0), 4);\n-      crc32(tmp1, Address(in_out2, size), 4);\n-      crc32(tmp2, Address(in_out2, size*2), 4);\n-      crc32(in_out3, Address(in_out2, 0+4), 4);\n-      crc32(tmp1, Address(in_out2, size+4), 4);\n-      crc32(tmp2, Address(in_out2, size*2+4), 4);\n-      addl(in_out2, 8);\n-      cmpl(in_out2, tmp3);\n-      jcc(Assembler::less, L_processPartition);\n-\n-        push(tmp3);\n-        push(in_out1);\n-        push(in_out2);\n-        tmp4 = tmp3;\n-        tmp5 = in_out1;\n-        n_tmp6 = in_out2;\n-\n-      crc32c_rec_alt2(const_or_pre_comp_const_index_u1, const_or_pre_comp_const_index_u2, is_pclmulqdq_supported, in_out3, tmp1, tmp2,\n-            w_xtmp1, w_xtmp2, w_xtmp3,\n-            tmp4, tmp5,\n-            n_tmp6);\n-\n-        pop(in_out2);\n-        pop(in_out1);\n-        pop(tmp3);\n-\n-    addl(in_out2, 2 * size);\n-    subl(in_out1, 3 * size);\n-    jmp(L_processPartitions);\n-\n-  bind(L_exit);\n-}\n-#endif \/\/LP64\n@@ -9098,1 +8032,0 @@\n-#ifdef _LP64\n@@ -9190,78 +8123,0 @@\n-#else\n-void MacroAssembler::crc32c_ipl_alg2_alt2(Register in_out, Register in1, Register in2,\n-                                          Register tmp1, Register  tmp2, Register tmp3,\n-                                          Register tmp4, Register  tmp5, Register tmp6,\n-                                          XMMRegister w_xtmp1, XMMRegister w_xtmp2, XMMRegister w_xtmp3,\n-                                          bool is_pclmulqdq_supported) {\n-  uint32_t const_or_pre_comp_const_index[CRC32C_NUM_PRECOMPUTED_CONSTANTS];\n-  Label L_wordByWord;\n-  Label L_byteByByteProlog;\n-  Label L_byteByByte;\n-  Label L_exit;\n-\n-  if (is_pclmulqdq_supported) {\n-    const_or_pre_comp_const_index[1] = *(uint32_t *)StubRoutines::_crc32c_table_addr;\n-    const_or_pre_comp_const_index[0] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 1);\n-\n-    const_or_pre_comp_const_index[3] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 2);\n-    const_or_pre_comp_const_index[2] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 3);\n-\n-    const_or_pre_comp_const_index[5] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 4);\n-    const_or_pre_comp_const_index[4] = *((uint32_t *)StubRoutines::_crc32c_table_addr + 5);\n-  } else {\n-    const_or_pre_comp_const_index[0] = 1;\n-    const_or_pre_comp_const_index[1] = 0;\n-\n-    const_or_pre_comp_const_index[2] = 3;\n-    const_or_pre_comp_const_index[3] = 2;\n-\n-    const_or_pre_comp_const_index[4] = 5;\n-    const_or_pre_comp_const_index[5] = 4;\n-  }\n-  crc32c_proc_chunk(CRC32C_HIGH, const_or_pre_comp_const_index[0], const_or_pre_comp_const_index[1], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  crc32c_proc_chunk(CRC32C_MIDDLE, const_or_pre_comp_const_index[2], const_or_pre_comp_const_index[3], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  crc32c_proc_chunk(CRC32C_LOW, const_or_pre_comp_const_index[4], const_or_pre_comp_const_index[5], is_pclmulqdq_supported,\n-                    in2, in1, in_out,\n-                    tmp1, tmp2, tmp3,\n-                    w_xtmp1, w_xtmp2, w_xtmp3,\n-                    tmp4, tmp5,\n-                    tmp6);\n-  movl(tmp1, in2);\n-  andl(tmp1, 0x00000007);\n-  negl(tmp1);\n-  addl(tmp1, in2);\n-  addl(tmp1, in1);\n-\n-  BIND(L_wordByWord);\n-  cmpl(in1, tmp1);\n-  jcc(Assembler::greaterEqual, L_byteByByteProlog);\n-    crc32(in_out, Address(in1,0), 4);\n-    addl(in1, 4);\n-    jmp(L_wordByWord);\n-\n-  BIND(L_byteByByteProlog);\n-  andl(in2, 0x00000007);\n-  movl(tmp2, 1);\n-\n-  BIND(L_byteByByte);\n-  cmpl(tmp2, in2);\n-  jccb(Assembler::greater, L_exit);\n-    movb(tmp1, Address(in1, 0));\n-    crc32(in_out, tmp1, 1);\n-    incl(in1);\n-    incl(tmp2);\n-    jmp(L_byteByByte);\n-\n-  BIND(L_exit);\n-}\n-#endif \/\/ LP64\n@@ -10261,1 +9116,0 @@\n-#ifdef _LP64\n@@ -10438,1 +9292,0 @@\n-#endif\n@@ -10442,1 +9295,0 @@\n-#ifdef _LP64\n@@ -10606,2 +9458,0 @@\n-#endif \/\/ _LP64\n-\n@@ -10636,2 +9486,2 @@\n-  LP64_ONLY(push(rdi);)\n-  LP64_ONLY(push(rsi);)\n+  push(rdi);\n+  push(rsi);\n@@ -10640,1 +9490,0 @@\n-#ifdef _LP64\n@@ -10645,1 +9494,0 @@\n-#endif\n@@ -10649,1 +9497,0 @@\n-#ifdef _LP64\n@@ -10654,1 +9501,0 @@\n-#endif\n@@ -10657,2 +9503,2 @@\n-  LP64_ONLY(pop(rsi);)\n-  LP64_ONLY(pop(rdi);)\n+  pop(rsi);\n+  pop(rdi);\n@@ -10793,1 +9639,0 @@\n-#ifdef _LP64\n@@ -10842,1 +9687,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":88,"deletions":1244,"binary":false,"changes":1332,"status":"modified"},{"patch":"@@ -151,4 +151,4 @@\n-  void increment(Register reg, int value = 1) { LP64_ONLY(incrementq(reg, value)) NOT_LP64(incrementl(reg, value)) ; }\n-  void decrement(Register reg, int value = 1) { LP64_ONLY(decrementq(reg, value)) NOT_LP64(decrementl(reg, value)) ; }\n-  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n-  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n+  void increment(Register reg, int value = 1) { incrementq(reg, value); }\n+  void decrement(Register reg, int value = 1) { decrementq(reg, value); }\n+  void increment(Address dst, int value = 1)  { incrementq(dst, value); }\n+  void decrement(Address dst, int value = 1)  { decrementq(dst, value); }\n@@ -231,1 +231,0 @@\n-#ifdef _LP64\n@@ -247,1 +246,0 @@\n-#endif \/\/ _LP64\n@@ -339,1 +337,0 @@\n-#ifdef _LP64\n@@ -344,1 +341,0 @@\n-#endif\n@@ -374,1 +370,0 @@\n-#ifdef _LP64\n@@ -376,1 +371,0 @@\n-#endif\n@@ -404,1 +398,0 @@\n-#ifdef _LP64\n@@ -439,2 +432,0 @@\n-#endif \/\/ _LP64\n-\n@@ -480,33 +471,0 @@\n-#ifndef _LP64\n-  \/\/ Compares the top-most stack entries on the FPU stack and sets the eflags as follows:\n-  \/\/\n-  \/\/ CF (corresponds to C0) if x < y\n-  \/\/ PF (corresponds to C2) if unordered\n-  \/\/ ZF (corresponds to C3) if x = y\n-  \/\/\n-  \/\/ The arguments are in reversed order on the stack (i.e., top of stack is first argument).\n-  \/\/ tmp is a temporary register, if none is available use noreg (only matters for non-P6 code)\n-  void fcmp(Register tmp);\n-  \/\/ Variant of the above which allows y to be further down the stack\n-  \/\/ and which only pops x and y if specified. If pop_right is\n-  \/\/ specified then pop_left must also be specified.\n-  void fcmp(Register tmp, int index, bool pop_left, bool pop_right);\n-\n-  \/\/ Floating-point comparison for Java\n-  \/\/ Compares the top-most stack entries on the FPU stack and stores the result in dst.\n-  \/\/ The arguments are in reversed order on the stack (i.e., top of stack is first argument).\n-  \/\/ (semantics as described in JVM spec.)\n-  void fcmp2int(Register dst, bool unordered_is_less);\n-  \/\/ Variant of the above which allows y to be further down the stack\n-  \/\/ and which only pops x and y if specified. If pop_right is\n-  \/\/ specified then pop_left must also be specified.\n-  void fcmp2int(Register dst, bool unordered_is_less, int index, bool pop_left, bool pop_right);\n-\n-  \/\/ Floating-point remainder for Java (ST0 = ST0 fremr ST1, ST1 is empty afterwards)\n-  \/\/ tmp is a temporary register, if none is available use noreg\n-  void fremr(Register tmp);\n-\n-  \/\/ only if +VerifyFPU\n-  void verify_FPU(int stack_depth, const char* s = \"illegal FPU state\");\n-#endif \/\/ !LP64\n-\n@@ -527,7 +485,1 @@\n-  \/\/ branch to L if FPU flag C2 is set\/not set\n-  \/\/ tmp is a temporary register, if none is available use noreg\n-  void jC2 (Register tmp, Label& L);\n-  void jnC2(Register tmp, Label& L);\n-\n-  \/\/ Load float value from 'address'. If UseSSE >= 1, the value is loaded into\n-  \/\/ register xmm0. Otherwise, the value is loaded onto the FPU stack.\n+  \/\/ Load float value from 'address'. The value is loaded into register xmm0.\n@@ -536,2 +488,1 @@\n-  \/\/ Store float value to 'address'. If UseSSE >= 1, the value is stored\n-  \/\/ from register xmm0. Otherwise, the value is stored from the FPU stack.\n+  \/\/ Store float value to 'address'. The value is stored from register xmm0.\n@@ -540,2 +491,1 @@\n-  \/\/ Load double value from 'address'. If UseSSE >= 2, the value is loaded into\n-  \/\/ register xmm0. Otherwise, the value is loaded onto the FPU stack.\n+  \/\/ Load double value from 'address'. The value is loaded into register xmm0.\n@@ -544,2 +494,1 @@\n-  \/\/ Store double value to 'address'. If UseSSE >= 2, the value is stored\n-  \/\/ from register xmm0. Otherwise, the value is stored from the FPU stack.\n+  \/\/ Store double value to 'address'. The value is stored from register xmm0.\n@@ -548,7 +497,0 @@\n-#ifndef _LP64\n-  \/\/ Pop ST (ffree & fincstp combined)\n-  void fpop();\n-\n-  void empty_FPU_stack();\n-#endif \/\/ !_LP64\n-\n@@ -669,1 +611,0 @@\n-#ifdef _LP64\n@@ -679,1 +620,0 @@\n-#endif\n@@ -716,1 +656,0 @@\n-#ifdef _LP64\n@@ -744,1 +683,0 @@\n-#endif\n@@ -854,1 +792,1 @@\n-  void addptr(Address dst, int32_t src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)) ; }\n+  void addptr(Address dst, int32_t src) { addq(dst, src); }\n@@ -857,1 +795,1 @@\n-  void addptr(Register dst, Address src) { LP64_ONLY(addq(dst, src)) NOT_LP64(addl(dst, src)); }\n+  void addptr(Register dst, Address src) { addq(dst, src); }\n@@ -866,1 +804,1 @@\n-  void andptr(Register src1, Register src2) { LP64_ONLY(andq(src1, src2)) NOT_LP64(andl(src1, src2)) ; }\n+  void andptr(Register src1, Register src2) { andq(src1, src2); }\n@@ -868,1 +806,0 @@\n-#ifdef _LP64\n@@ -871,1 +808,0 @@\n-#endif\n@@ -884,6 +820,0 @@\n-#ifndef _LP64\n-  void cmpklass(Address dst, Metadata* obj);\n-  void cmpklass(Register dst, Metadata* obj);\n-  void cmpoop(Address dst, jobject obj);\n-#endif \/\/ _LP64\n-\n@@ -899,3 +829,2 @@\n-  void cmpptr(Register src1, Register src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n-  void cmpptr(Register src1, Address src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n-  \/\/ void cmpptr(Address src1, Register src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n+  void cmpptr(Register src1, Register src2) { cmpq(src1, src2); }\n+  void cmpptr(Register src1, Address src2) { cmpq(src1, src2); }\n@@ -903,2 +832,2 @@\n-  void cmpptr(Register src1, int32_t src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n-  void cmpptr(Address src1, int32_t src2) { LP64_ONLY(cmpq(src1, src2)) NOT_LP64(cmpl(src1, src2)) ; }\n+  void cmpptr(Register src1, int32_t src2) { cmpq(src1, src2); }\n+  void cmpptr(Address src1, int32_t src2) { cmpq(src1, src2); }\n@@ -913,2 +842,2 @@\n-  void imulptr(Register dst, Register src) { LP64_ONLY(imulq(dst, src)) NOT_LP64(imull(dst, src)); }\n-  void imulptr(Register dst, Register src, int imm32) { LP64_ONLY(imulq(dst, src, imm32)) NOT_LP64(imull(dst, src, imm32)); }\n+  void imulptr(Register dst, Register src) { imulq(dst, src); }\n+  void imulptr(Register dst, Register src, int imm32) { imulq(dst, src, imm32); }\n@@ -917,1 +846,1 @@\n-  void negptr(Register dst) { LP64_ONLY(negq(dst)) NOT_LP64(negl(dst)); }\n+  void negptr(Register dst) { negq(dst); }\n@@ -919,1 +848,1 @@\n-  void notptr(Register dst) { LP64_ONLY(notq(dst)) NOT_LP64(notl(dst)); }\n+  void notptr(Register dst) { notq(dst); }\n@@ -922,1 +851,1 @@\n-  void shlptr(Register dst) { LP64_ONLY(shlq(dst)) NOT_LP64(shll(dst)); }\n+  void shlptr(Register dst) { shlq(dst); }\n@@ -925,1 +854,1 @@\n-  void shrptr(Register dst) { LP64_ONLY(shrq(dst)) NOT_LP64(shrl(dst)); }\n+  void shrptr(Register dst) { shrq(dst); }\n@@ -927,2 +856,2 @@\n-  void sarptr(Register dst) { LP64_ONLY(sarq(dst)) NOT_LP64(sarl(dst)); }\n-  void sarptr(Register dst, int32_t src) { LP64_ONLY(sarq(dst, src)) NOT_LP64(sarl(dst, src)); }\n+  void sarptr(Register dst) { sarq(dst); }\n+  void sarptr(Register dst, int32_t src) { sarq(dst, src); }\n@@ -930,1 +859,1 @@\n-  void subptr(Address dst, int32_t src) { LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src)); }\n+  void subptr(Address dst, int32_t src) { subq(dst, src); }\n@@ -932,1 +861,1 @@\n-  void subptr(Register dst, Address src) { LP64_ONLY(subq(dst, src)) NOT_LP64(subl(dst, src)); }\n+  void subptr(Register dst, Address src) { subq(dst, src); }\n@@ -942,2 +871,2 @@\n-  void sbbptr(Address dst, int32_t src) { LP64_ONLY(sbbq(dst, src)) NOT_LP64(sbbl(dst, src)); }\n-  void sbbptr(Register dst, int32_t src) { LP64_ONLY(sbbq(dst, src)) NOT_LP64(sbbl(dst, src)); }\n+  void sbbptr(Address dst, int32_t src) { sbbq(dst, src); }\n+  void sbbptr(Register dst, int32_t src) { sbbq(dst, src); }\n@@ -945,2 +874,2 @@\n-  void xchgptr(Register src1, Register src2) { LP64_ONLY(xchgq(src1, src2)) NOT_LP64(xchgl(src1, src2)) ; }\n-  void xchgptr(Register src1, Address src2) { LP64_ONLY(xchgq(src1, src2)) NOT_LP64(xchgl(src1, src2)) ; }\n+  void xchgptr(Register src1, Register src2) { xchgq(src1, src2); }\n+  void xchgptr(Register src1, Address src2) { xchgq(src1, src2); }\n@@ -948,1 +877,1 @@\n-  void xaddptr(Address src1, Register src2) { LP64_ONLY(xaddq(src1, src2)) NOT_LP64(xaddl(src1, src2)) ; }\n+  void xaddptr(Address src1, Register src2) { xaddq(src1, src2); }\n@@ -958,1 +887,0 @@\n-#ifdef _LP64\n@@ -961,3 +889,2 @@\n-#endif\n-  void atomic_incptr(AddressLiteral counter_addr, Register rscratch = noreg) { LP64_ONLY(atomic_incq(counter_addr, rscratch)) NOT_LP64(atomic_incl(counter_addr, rscratch)) ; }\n-  void atomic_incptr(Address counter_addr) { LP64_ONLY(atomic_incq(counter_addr)) NOT_LP64(atomic_incl(counter_addr)) ; }\n+  void atomic_incptr(AddressLiteral counter_addr, Register rscratch = noreg) { atomic_incq(counter_addr, rscratch); }\n+  void atomic_incptr(Address counter_addr) { atomic_incq(counter_addr); }\n@@ -981,4 +908,4 @@\n-  void orptr(Register dst, Address src) { LP64_ONLY(orq(dst, src)) NOT_LP64(orl(dst, src)); }\n-  void orptr(Register dst, Register src) { LP64_ONLY(orq(dst, src)) NOT_LP64(orl(dst, src)); }\n-  void orptr(Register dst, int32_t src) { LP64_ONLY(orq(dst, src)) NOT_LP64(orl(dst, src)); }\n-  void orptr(Address dst, int32_t imm32) { LP64_ONLY(orq(dst, imm32)) NOT_LP64(orl(dst, imm32)); }\n+  void orptr(Register dst, Address src)  { orq(dst, src); }\n+  void orptr(Register dst, Register src) { orq(dst, src); }\n+  void orptr(Register dst, int32_t src)  { orq(dst, src); }\n+  void orptr(Address dst, int32_t imm32) { orq(dst, imm32); }\n@@ -986,3 +913,3 @@\n-  void testptr(Register src, int32_t imm32) {  LP64_ONLY(testq(src, imm32)) NOT_LP64(testl(src, imm32)); }\n-  void testptr(Register src1, Address src2) { LP64_ONLY(testq(src1, src2)) NOT_LP64(testl(src1, src2)); }\n-  void testptr(Address src, int32_t imm32) {  LP64_ONLY(testq(src, imm32)) NOT_LP64(testl(src, imm32)); }\n+  void testptr(Register src, int32_t imm32) { testq(src, imm32); }\n+  void testptr(Register src1, Address src2) { testq(src1, src2); }\n+  void testptr(Address src, int32_t imm32)  { testq(src, imm32); }\n@@ -991,2 +918,2 @@\n-  void xorptr(Register dst, Register src) { LP64_ONLY(xorq(dst, src)) NOT_LP64(xorl(dst, src)); }\n-  void xorptr(Register dst, Address src) { LP64_ONLY(xorq(dst, src)) NOT_LP64(xorl(dst, src)); }\n+  void xorptr(Register dst, Register src) { xorq(dst, src); }\n+  void xorptr(Register dst, Address src) { xorq(dst, src); }\n@@ -1117,21 +1044,0 @@\n-#ifndef _LP64\n-  void fadd_s(Address        src) { Assembler::fadd_s(src); }\n-  void fadd_s(AddressLiteral src) { Assembler::fadd_s(as_Address(src)); }\n-\n-  void fldcw(Address        src) { Assembler::fldcw(src); }\n-  void fldcw(AddressLiteral src);\n-\n-  void fld_s(int index)          { Assembler::fld_s(index); }\n-  void fld_s(Address        src) { Assembler::fld_s(src); }\n-  void fld_s(AddressLiteral src);\n-\n-  void fld_d(Address        src) { Assembler::fld_d(src); }\n-  void fld_d(AddressLiteral src);\n-\n-  void fld_x(Address        src) { Assembler::fld_x(src); }\n-  void fld_x(AddressLiteral src) { Assembler::fld_x(as_Address(src)); }\n-\n-  void fmul_s(Address        src) { Assembler::fmul_s(src); }\n-  void fmul_s(AddressLiteral src) { Assembler::fmul_s(as_Address(src)); }\n-#endif \/\/ !_LP64\n-\n@@ -1141,1 +1047,0 @@\n-#ifdef _LP64\n@@ -1191,1 +1096,0 @@\n-#endif \/\/ _LP64\n@@ -1201,1 +1105,0 @@\n-#ifdef _LP64\n@@ -1206,6 +1109,0 @@\n-#else\n-  void fast_sha256(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n-                   XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n-                   Register buf, Register state, Register ofs, Register limit, Register rsp,\n-                   bool multi_block);\n-#endif\n@@ -1217,46 +1114,0 @@\n-#ifndef _LP64\n- private:\n-  \/\/ Initialized in macroAssembler_x86_constants.cpp\n-  static address ONES;\n-  static address L_2IL0FLOATPACKET_0;\n-  static address PI4_INV;\n-  static address PI4X3;\n-  static address PI4X4;\n-\n- public:\n-  void fast_log(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1);\n-\n-  void fast_log10(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp);\n-\n-  void fast_pow(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n-                XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register rax, Register rcx,\n-                Register rdx, Register tmp);\n-\n-  void fast_sin(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rbx, Register rdx);\n-\n-  void fast_cos(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp);\n-\n-  void libm_sincos_huge(XMMRegister xmm0, XMMRegister xmm1, Register eax, Register ecx,\n-                        Register edx, Register ebx, Register esi, Register edi,\n-                        Register ebp, Register esp);\n-\n-  void libm_reduce_pi04l(Register eax, Register ecx, Register edx, Register ebx,\n-                         Register esi, Register edi, Register ebp, Register esp);\n-\n-  void libm_tancot_huge(XMMRegister xmm0, XMMRegister xmm1, Register eax, Register ecx,\n-                        Register edx, Register ebx, Register esi, Register edi,\n-                        Register ebp, Register esp);\n-\n-  void fast_tan(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp);\n-#endif \/\/ !_LP64\n-\n@@ -2019,2 +1870,2 @@\n-  void cmovptr(Condition cc, Register dst, Address  src) { LP64_ONLY(cmovq(cc, dst, src)) NOT_LP64(cmov32(cc, dst, src)); }\n-  void cmovptr(Condition cc, Register dst, Register src) { LP64_ONLY(cmovq(cc, dst, src)) NOT_LP64(cmov32(cc, dst, src)); }\n+  void cmovptr(Condition cc, Register dst, Address  src) { cmovq(cc, dst, src); }\n+  void cmovptr(Condition cc, Register dst, Register src) { cmovq(cc, dst, src); }\n@@ -2059,2 +1910,2 @@\n-  void pushptr(Address src) { LP64_ONLY(pushq(src)) NOT_LP64(pushl(src)); }\n-  void popptr(Address src) { LP64_ONLY(popq(src)) NOT_LP64(popl(src)); }\n+  void pushptr(Address src) { pushq(src); }\n+  void popptr(Address src)  { popq(src); }\n@@ -2066,2 +1917,2 @@\n-  void movl2ptr(Register dst, Address src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(movl(dst, src)); }\n-  void movl2ptr(Register dst, Register src) { LP64_ONLY(movslq(dst, src)) NOT_LP64(if (dst != src) movl(dst, src)); }\n+  void movl2ptr(Register dst, Address src) { movslq(dst, src); }\n+  void movl2ptr(Register dst, Register src) { movslq(dst, src); }\n@@ -2090,1 +1941,0 @@\n-#ifdef _LP64\n@@ -2131,1 +1981,0 @@\n-#endif\n@@ -2138,1 +1987,0 @@\n-#ifdef _LP64\n@@ -2143,1 +1991,0 @@\n-#endif \/\/ _LP64\n@@ -2149,1 +1996,0 @@\n-#ifdef _LP64\n@@ -2152,5 +1998,0 @@\n-#else\n-  void crc32c_ipl_alg4(Register in_out, uint32_t n,\n-                       Register tmp1, Register tmp2, Register tmp3,\n-                       XMMRegister xtmp1, XMMRegister xtmp2);\n-#endif\n@@ -2181,1 +2022,0 @@\n-#ifdef _LP64\n@@ -2184,1 +2024,0 @@\n-#endif \/\/ _LP64\n@@ -2218,1 +2057,0 @@\n-#ifdef _LP64\n@@ -2233,1 +2071,0 @@\n-#endif \/\/ _LP64\n@@ -2242,1 +2079,0 @@\n-#ifdef _LP64\n@@ -2246,1 +2082,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":47,"deletions":212,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -1,53 +0,0 @@\n-\/*\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _ONES[] = {\n-    0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xbff00000UL\n-};\n-address MacroAssembler::ONES = (address)_ONES;\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _PI4_INV[] = {\n-    0x6dc9c883UL, 0x3ff45f30UL\n-};\n-address MacroAssembler::PI4_INV = (address)_PI4_INV;\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _PI4X3[] = {\n-    0x54443000UL, 0xbfe921fbUL, 0x3b39a000UL, 0x3d373dcbUL, 0xe0e68948UL,\n-    0xba845c06UL\n-};\n-address MacroAssembler::PI4X3 = (address)_PI4X3;\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _PI4X4[] = {\n-    0x54400000UL, 0xbfe921fbUL, 0x1a600000UL, 0xbdc0b461UL, 0x2e000000UL,\n-    0xbb93198aUL, 0x252049c1UL, 0xb96b839aUL\n-};\n-address MacroAssembler::PI4X4 = (address)_PI4X4;\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _L_2IL0FLOATPACKET_0[] = {\n-    0xffffffffUL, 0x7fffffffUL, 0x00000000UL, 0x00000000UL\n-};\n-address MacroAssembler::L_2IL0FLOATPACKET_0 = (address)_L_2IL0FLOATPACKET_0;\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_constants.cpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"deleted"},{"patch":"@@ -1,428 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION - COS()\n-\/\/                     ---------------------\n-\/\/\n-\/\/     1. RANGE REDUCTION\n-\/\/\n-\/\/     We perform an initial range reduction from X to r with\n-\/\/\n-\/\/          X =~= N * pi\/32 + r\n-\/\/\n-\/\/     so that |r| <= pi\/64 + epsilon. We restrict inputs to those\n-\/\/     where |N| <= 932560. Beyond this, the range reduction is\n-\/\/     insufficiently accurate. For extremely small inputs,\n-\/\/     denormalization can occur internally, impacting performance.\n-\/\/     This means that the main path is actually only taken for\n-\/\/     2^-252 <= |X| < 90112.\n-\/\/\n-\/\/     To avoid branches, we perform the range reduction to full\n-\/\/     accuracy each time.\n-\/\/\n-\/\/          X - N * (P_1 + P_2 + P_3)\n-\/\/\n-\/\/     where P_1 and P_2 are 32-bit numbers (so multiplication by N\n-\/\/     is exact) and P_3 is a 53-bit number. Together, these\n-\/\/     approximate pi well enough for all cases in the restricted\n-\/\/     range.\n-\/\/\n-\/\/     The main reduction sequence is:\n-\/\/\n-\/\/             y = 32\/pi * x\n-\/\/             N = integer(y)\n-\/\/     (computed by adding and subtracting off SHIFTER)\n-\/\/\n-\/\/             m_1 = N * P_1\n-\/\/             m_2 = N * P_2\n-\/\/             r_1 = x - m_1\n-\/\/             r = r_1 - m_2\n-\/\/     (this r can be used for most of the calculation)\n-\/\/\n-\/\/             c_1 = r_1 - r\n-\/\/             m_3 = N * P_3\n-\/\/             c_2 = c_1 - m_2\n-\/\/             c = c_2 - m_3\n-\/\/\n-\/\/     2. MAIN ALGORITHM\n-\/\/\n-\/\/     The algorithm uses a table lookup based on B = M * pi \/ 32\n-\/\/     where M = N mod 64. The stored values are:\n-\/\/       sigma             closest power of 2 to cos(B)\n-\/\/       C_hl              53-bit cos(B) - sigma\n-\/\/       S_hi + S_lo       2 * 53-bit sin(B)\n-\/\/\n-\/\/     The computation is organized as follows:\n-\/\/\n-\/\/          sin(B + r + c) = [sin(B) + sigma * r] +\n-\/\/                           r * (cos(B) - sigma) +\n-\/\/                           sin(B) * [cos(r + c) - 1] +\n-\/\/                           cos(B) * [sin(r + c) - r]\n-\/\/\n-\/\/     which is approximately:\n-\/\/\n-\/\/          [S_hi + sigma * r] +\n-\/\/          C_hl * r +\n-\/\/          S_lo + S_hi * [(cos(r) - 1) - r * c] +\n-\/\/          (C_hl + sigma) * [(sin(r) - r) + c]\n-\/\/\n-\/\/     and this is what is actually computed. We separate this sum\n-\/\/     into four parts:\n-\/\/\n-\/\/          hi + med + pols + corr\n-\/\/\n-\/\/     where\n-\/\/\n-\/\/          hi       = S_hi + sigma r\n-\/\/          med      = C_hl * r\n-\/\/          pols     = S_hi * (cos(r) - 1) + (C_hl + sigma) * (sin(r) - r)\n-\/\/          corr     = S_lo + c * ((C_hl + sigma) - S_hi * r)\n-\/\/\n-\/\/     3. POLYNOMIAL\n-\/\/\n-\/\/     The polynomial S_hi * (cos(r) - 1) + (C_hl + sigma) *\n-\/\/     (sin(r) - r) can be rearranged freely, since it is quite\n-\/\/     small, so we exploit parallelism to the fullest.\n-\/\/\n-\/\/          psc4       =   SC_4 * r_1\n-\/\/          msc4       =   psc4 * r\n-\/\/          r2         =   r * r\n-\/\/          msc2       =   SC_2 * r2\n-\/\/          r4         =   r2 * r2\n-\/\/          psc3       =   SC_3 + msc4\n-\/\/          psc1       =   SC_1 + msc2\n-\/\/          msc3       =   r4 * psc3\n-\/\/          sincospols =   psc1 + msc3\n-\/\/          pols       =   sincospols *\n-\/\/                         <S_hi * r^2 | (C_hl + sigma) * r^3>\n-\/\/\n-\/\/     4. CORRECTION TERM\n-\/\/\n-\/\/     This is where the \"c\" component of the range reduction is\n-\/\/     taken into account; recall that just \"r\" is used for most of\n-\/\/     the calculation.\n-\/\/\n-\/\/          -c   = m_3 - c_2\n-\/\/          -d   = S_hi * r - (C_hl + sigma)\n-\/\/          corr = -c * -d + S_lo\n-\/\/\n-\/\/     5. COMPENSATED SUMMATIONS\n-\/\/\n-\/\/     The two successive compensated summations add up the high\n-\/\/     and medium parts, leaving just the low parts to add up at\n-\/\/     the end.\n-\/\/\n-\/\/          rs        =  sigma * r\n-\/\/          res_int   =  S_hi + rs\n-\/\/          k_0       =  S_hi - res_int\n-\/\/          k_2       =  k_0 + rs\n-\/\/          med       =  C_hl * r\n-\/\/          res_hi    =  res_int + med\n-\/\/          k_1       =  res_int - res_hi\n-\/\/          k_3       =  k_1 + med\n-\/\/\n-\/\/     6. FINAL SUMMATION\n-\/\/\n-\/\/     We now add up all the small parts:\n-\/\/\n-\/\/          res_lo = pols(hi) + pols(lo) + corr + k_1 + k_3\n-\/\/\n-\/\/     Now the overall result is just:\n-\/\/\n-\/\/          res_hi + res_lo\n-\/\/\n-\/\/     7. SMALL ARGUMENTS\n-\/\/\n-\/\/     Inputs with |X| < 2^-252 are treated specially as\n-\/\/     1 - |x|.\n-\/\/\n-\/\/ Special cases:\n-\/\/  cos(NaN) = quiet NaN, and raise invalid exception\n-\/\/  cos(INF) = NaN and raise invalid exception\n-\/\/  cos(0) = 1\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table_cos[] =\n-{\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x3ff00000UL, 0x176d6d31UL, 0xbf73b92eUL,\n-    0xbc29b42cUL, 0x3fb917a6UL, 0xe0000000UL, 0xbc3e2718UL, 0x00000000UL,\n-    0x3ff00000UL, 0x011469fbUL, 0xbf93ad06UL, 0x3c69a60bUL, 0x3fc8f8b8UL,\n-    0xc0000000UL, 0xbc626d19UL, 0x00000000UL, 0x3ff00000UL, 0x939d225aUL,\n-    0xbfa60beaUL, 0x2ed59f06UL, 0x3fd29406UL, 0xa0000000UL, 0xbc75d28dUL,\n-    0x00000000UL, 0x3ff00000UL, 0x866b95cfUL, 0xbfb37ca1UL, 0xa6aea963UL,\n-    0x3fd87de2UL, 0xe0000000UL, 0xbc672cedUL, 0x00000000UL, 0x3ff00000UL,\n-    0x73fa1279UL, 0xbfbe3a68UL, 0x3806f63bUL, 0x3fde2b5dUL, 0x20000000UL,\n-    0x3c5e0d89UL, 0x00000000UL, 0x3ff00000UL, 0x5bc57974UL, 0xbfc59267UL,\n-    0x39ae68c8UL, 0x3fe1c73bUL, 0x20000000UL, 0x3c8b25ddUL, 0x00000000UL,\n-    0x3ff00000UL, 0x53aba2fdUL, 0xbfcd0dfeUL, 0x25091dd6UL, 0x3fe44cf3UL,\n-    0x20000000UL, 0x3c68076aUL, 0x00000000UL, 0x3ff00000UL, 0x99fcef32UL,\n-    0x3fca8279UL, 0x667f3bcdUL, 0x3fe6a09eUL, 0x20000000UL, 0xbc8bdd34UL,\n-    0x00000000UL, 0x3fe00000UL, 0x94247758UL, 0x3fc133ccUL, 0x6b151741UL,\n-    0x3fe8bc80UL, 0x20000000UL, 0xbc82c5e1UL, 0x00000000UL, 0x3fe00000UL,\n-    0x9ae68c87UL, 0x3fac73b3UL, 0x290ea1a3UL, 0x3fea9b66UL, 0xe0000000UL,\n-    0x3c39f630UL, 0x00000000UL, 0x3fe00000UL, 0x7f909c4eUL, 0xbf9d4a2cUL,\n-    0xf180bdb1UL, 0x3fec38b2UL, 0x80000000UL, 0xbc76e0b1UL, 0x00000000UL,\n-    0x3fe00000UL, 0x65455a75UL, 0xbfbe0875UL, 0xcf328d46UL, 0x3fed906bUL,\n-    0x20000000UL, 0x3c7457e6UL, 0x00000000UL, 0x3fe00000UL, 0x76acf82dUL,\n-    0x3fa4a031UL, 0x56c62ddaUL, 0x3fee9f41UL, 0xe0000000UL, 0x3c8760b1UL,\n-    0x00000000UL, 0x3fd00000UL, 0x0e5967d5UL, 0xbfac1d1fUL, 0xcff75cb0UL,\n-    0x3fef6297UL, 0x20000000UL, 0x3c756217UL, 0x00000000UL, 0x3fd00000UL,\n-    0x0f592f50UL, 0xbf9ba165UL, 0xa3d12526UL, 0x3fefd88dUL, 0x40000000UL,\n-    0xbc887df6UL, 0x00000000UL, 0x3fc00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x0f592f50UL, 0x3f9ba165UL, 0xa3d12526UL, 0x3fefd88dUL,\n-    0x40000000UL, 0xbc887df6UL, 0x00000000UL, 0xbfc00000UL, 0x0e5967d5UL,\n-    0x3fac1d1fUL, 0xcff75cb0UL, 0x3fef6297UL, 0x20000000UL, 0x3c756217UL,\n-    0x00000000UL, 0xbfd00000UL, 0x76acf82dUL, 0xbfa4a031UL, 0x56c62ddaUL,\n-    0x3fee9f41UL, 0xe0000000UL, 0x3c8760b1UL, 0x00000000UL, 0xbfd00000UL,\n-    0x65455a75UL, 0x3fbe0875UL, 0xcf328d46UL, 0x3fed906bUL, 0x20000000UL,\n-    0x3c7457e6UL, 0x00000000UL, 0xbfe00000UL, 0x7f909c4eUL, 0x3f9d4a2cUL,\n-    0xf180bdb1UL, 0x3fec38b2UL, 0x80000000UL, 0xbc76e0b1UL, 0x00000000UL,\n-    0xbfe00000UL, 0x9ae68c87UL, 0xbfac73b3UL, 0x290ea1a3UL, 0x3fea9b66UL,\n-    0xe0000000UL, 0x3c39f630UL, 0x00000000UL, 0xbfe00000UL, 0x94247758UL,\n-    0xbfc133ccUL, 0x6b151741UL, 0x3fe8bc80UL, 0x20000000UL, 0xbc82c5e1UL,\n-    0x00000000UL, 0xbfe00000UL, 0x99fcef32UL, 0xbfca8279UL, 0x667f3bcdUL,\n-    0x3fe6a09eUL, 0x20000000UL, 0xbc8bdd34UL, 0x00000000UL, 0xbfe00000UL,\n-    0x53aba2fdUL, 0x3fcd0dfeUL, 0x25091dd6UL, 0x3fe44cf3UL, 0x20000000UL,\n-    0x3c68076aUL, 0x00000000UL, 0xbff00000UL, 0x5bc57974UL, 0x3fc59267UL,\n-    0x39ae68c8UL, 0x3fe1c73bUL, 0x20000000UL, 0x3c8b25ddUL, 0x00000000UL,\n-    0xbff00000UL, 0x73fa1279UL, 0x3fbe3a68UL, 0x3806f63bUL, 0x3fde2b5dUL,\n-    0x20000000UL, 0x3c5e0d89UL, 0x00000000UL, 0xbff00000UL, 0x866b95cfUL,\n-    0x3fb37ca1UL, 0xa6aea963UL, 0x3fd87de2UL, 0xe0000000UL, 0xbc672cedUL,\n-    0x00000000UL, 0xbff00000UL, 0x939d225aUL, 0x3fa60beaUL, 0x2ed59f06UL,\n-    0x3fd29406UL, 0xa0000000UL, 0xbc75d28dUL, 0x00000000UL, 0xbff00000UL,\n-    0x011469fbUL, 0x3f93ad06UL, 0x3c69a60bUL, 0x3fc8f8b8UL, 0xc0000000UL,\n-    0xbc626d19UL, 0x00000000UL, 0xbff00000UL, 0x176d6d31UL, 0x3f73b92eUL,\n-    0xbc29b42cUL, 0x3fb917a6UL, 0xe0000000UL, 0xbc3e2718UL, 0x00000000UL,\n-    0xbff00000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0xbff00000UL, 0x176d6d31UL,\n-    0x3f73b92eUL, 0xbc29b42cUL, 0xbfb917a6UL, 0xe0000000UL, 0x3c3e2718UL,\n-    0x00000000UL, 0xbff00000UL, 0x011469fbUL, 0x3f93ad06UL, 0x3c69a60bUL,\n-    0xbfc8f8b8UL, 0xc0000000UL, 0x3c626d19UL, 0x00000000UL, 0xbff00000UL,\n-    0x939d225aUL, 0x3fa60beaUL, 0x2ed59f06UL, 0xbfd29406UL, 0xa0000000UL,\n-    0x3c75d28dUL, 0x00000000UL, 0xbff00000UL, 0x866b95cfUL, 0x3fb37ca1UL,\n-    0xa6aea963UL, 0xbfd87de2UL, 0xe0000000UL, 0x3c672cedUL, 0x00000000UL,\n-    0xbff00000UL, 0x73fa1279UL, 0x3fbe3a68UL, 0x3806f63bUL, 0xbfde2b5dUL,\n-    0x20000000UL, 0xbc5e0d89UL, 0x00000000UL, 0xbff00000UL, 0x5bc57974UL,\n-    0x3fc59267UL, 0x39ae68c8UL, 0xbfe1c73bUL, 0x20000000UL, 0xbc8b25ddUL,\n-    0x00000000UL, 0xbff00000UL, 0x53aba2fdUL, 0x3fcd0dfeUL, 0x25091dd6UL,\n-    0xbfe44cf3UL, 0x20000000UL, 0xbc68076aUL, 0x00000000UL, 0xbff00000UL,\n-    0x99fcef32UL, 0xbfca8279UL, 0x667f3bcdUL, 0xbfe6a09eUL, 0x20000000UL,\n-    0x3c8bdd34UL, 0x00000000UL, 0xbfe00000UL, 0x94247758UL, 0xbfc133ccUL,\n-    0x6b151741UL, 0xbfe8bc80UL, 0x20000000UL, 0x3c82c5e1UL, 0x00000000UL,\n-    0xbfe00000UL, 0x9ae68c87UL, 0xbfac73b3UL, 0x290ea1a3UL, 0xbfea9b66UL,\n-    0xe0000000UL, 0xbc39f630UL, 0x00000000UL, 0xbfe00000UL, 0x7f909c4eUL,\n-    0x3f9d4a2cUL, 0xf180bdb1UL, 0xbfec38b2UL, 0x80000000UL, 0x3c76e0b1UL,\n-    0x00000000UL, 0xbfe00000UL, 0x65455a75UL, 0x3fbe0875UL, 0xcf328d46UL,\n-    0xbfed906bUL, 0x20000000UL, 0xbc7457e6UL, 0x00000000UL, 0xbfe00000UL,\n-    0x76acf82dUL, 0xbfa4a031UL, 0x56c62ddaUL, 0xbfee9f41UL, 0xe0000000UL,\n-    0xbc8760b1UL, 0x00000000UL, 0xbfd00000UL, 0x0e5967d5UL, 0x3fac1d1fUL,\n-    0xcff75cb0UL, 0xbfef6297UL, 0x20000000UL, 0xbc756217UL, 0x00000000UL,\n-    0xbfd00000UL, 0x0f592f50UL, 0x3f9ba165UL, 0xa3d12526UL, 0xbfefd88dUL,\n-    0x40000000UL, 0x3c887df6UL, 0x00000000UL, 0xbfc00000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0xbff00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x0f592f50UL, 0xbf9ba165UL, 0xa3d12526UL,\n-    0xbfefd88dUL, 0x40000000UL, 0x3c887df6UL, 0x00000000UL, 0x3fc00000UL,\n-    0x0e5967d5UL, 0xbfac1d1fUL, 0xcff75cb0UL, 0xbfef6297UL, 0x20000000UL,\n-    0xbc756217UL, 0x00000000UL, 0x3fd00000UL, 0x76acf82dUL, 0x3fa4a031UL,\n-    0x56c62ddaUL, 0xbfee9f41UL, 0xe0000000UL, 0xbc8760b1UL, 0x00000000UL,\n-    0x3fd00000UL, 0x65455a75UL, 0xbfbe0875UL, 0xcf328d46UL, 0xbfed906bUL,\n-    0x20000000UL, 0xbc7457e6UL, 0x00000000UL, 0x3fe00000UL, 0x7f909c4eUL,\n-    0xbf9d4a2cUL, 0xf180bdb1UL, 0xbfec38b2UL, 0x80000000UL, 0x3c76e0b1UL,\n-    0x00000000UL, 0x3fe00000UL, 0x9ae68c87UL, 0x3fac73b3UL, 0x290ea1a3UL,\n-    0xbfea9b66UL, 0xe0000000UL, 0xbc39f630UL, 0x00000000UL, 0x3fe00000UL,\n-    0x94247758UL, 0x3fc133ccUL, 0x6b151741UL, 0xbfe8bc80UL, 0x20000000UL,\n-    0x3c82c5e1UL, 0x00000000UL, 0x3fe00000UL, 0x99fcef32UL, 0x3fca8279UL,\n-    0x667f3bcdUL, 0xbfe6a09eUL, 0x20000000UL, 0x3c8bdd34UL, 0x00000000UL,\n-    0x3fe00000UL, 0x53aba2fdUL, 0xbfcd0dfeUL, 0x25091dd6UL, 0xbfe44cf3UL,\n-    0x20000000UL, 0xbc68076aUL, 0x00000000UL, 0x3ff00000UL, 0x5bc57974UL,\n-    0xbfc59267UL, 0x39ae68c8UL, 0xbfe1c73bUL, 0x20000000UL, 0xbc8b25ddUL,\n-    0x00000000UL, 0x3ff00000UL, 0x73fa1279UL, 0xbfbe3a68UL, 0x3806f63bUL,\n-    0xbfde2b5dUL, 0x20000000UL, 0xbc5e0d89UL, 0x00000000UL, 0x3ff00000UL,\n-    0x866b95cfUL, 0xbfb37ca1UL, 0xa6aea963UL, 0xbfd87de2UL, 0xe0000000UL,\n-    0x3c672cedUL, 0x00000000UL, 0x3ff00000UL, 0x939d225aUL, 0xbfa60beaUL,\n-    0x2ed59f06UL, 0xbfd29406UL, 0xa0000000UL, 0x3c75d28dUL, 0x00000000UL,\n-    0x3ff00000UL, 0x011469fbUL, 0xbf93ad06UL, 0x3c69a60bUL, 0xbfc8f8b8UL,\n-    0xc0000000UL, 0x3c626d19UL, 0x00000000UL, 0x3ff00000UL, 0x176d6d31UL,\n-    0xbf73b92eUL, 0xbc29b42cUL, 0xbfb917a6UL, 0xe0000000UL, 0x3c3e2718UL,\n-    0x00000000UL, 0x3ff00000UL, 0x55555555UL, 0xbfc55555UL, 0x00000000UL,\n-    0xbfe00000UL, 0x11111111UL, 0x3f811111UL, 0x55555555UL, 0x3fa55555UL,\n-    0x1a01a01aUL, 0xbf2a01a0UL, 0x16c16c17UL, 0xbf56c16cUL, 0xa556c734UL,\n-    0x3ec71de3UL, 0x1a01a01aUL, 0x3efa01a0UL, 0x1a600000UL, 0x3d90b461UL,\n-    0x1a600000UL, 0x3d90b461UL, 0x54400000UL, 0x3fb921fbUL, 0x00000000UL,\n-    0x00000000UL, 0x2e037073UL, 0x3b63198aUL, 0x00000000UL, 0x00000000UL,\n-    0x6dc9c883UL, 0x40245f30UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x43380000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x3ff00000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x80000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x80000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x3fe00000UL, 0x00000000UL, 0x3fe00000UL\n-};\n-\/\/registers,\n-\/\/ input: (rbp + 8)\n-\/\/ scratch: xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7\n-\/\/          eax, ecx, edx, ebx (tmp)\n-\n-\/\/ Code generated by Intel C compiler for LIBM library\n-\n-void MacroAssembler::fast_cos(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                              XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                              Register eax, Register ecx, Register edx, Register tmp) {\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label start;\n-\n-  assert_different_registers(tmp, eax, ecx, edx);\n-\n-  address static_const_table_cos = (address)_static_const_table_cos;\n-\n-  bind(start);\n-  subl(rsp, 120);\n-  movl(Address(rsp, 56), tmp);\n-  lea(tmp, ExternalAddress(static_const_table_cos));\n-  movsd(xmm0, Address(rsp, 128));\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32767);\n-  subl(eax, 12336);\n-  cmpl(eax, 4293);\n-  jcc(Assembler::above, L_2TAG_PACKET_0_0_2);\n-  movsd(xmm1, Address(tmp, 2160));\n-  mulsd(xmm1, xmm0);\n-  movdqu(xmm5, Address(tmp, 2240));\n-  movsd(xmm4, Address(tmp, 2224));\n-  pand(xmm4, xmm0);\n-  por(xmm5, xmm4);\n-  movsd(xmm3, Address(tmp, 2128));\n-  movdqu(xmm2, Address(tmp, 2112));\n-  addpd(xmm1, xmm5);\n-  cvttsd2sil(edx, xmm1);\n-  cvtsi2sdl(xmm1, edx);\n-  mulsd(xmm3, xmm1);\n-  unpcklpd(xmm1, xmm1);\n-  addl(edx, 1865232);\n-  movdqu(xmm4, xmm0);\n-  andl(edx, 63);\n-  movdqu(xmm5, Address(tmp, 2096));\n-  lea(eax, Address(tmp, 0));\n-  shll(edx, 5);\n-  addl(eax, edx);\n-  mulpd(xmm2, xmm1);\n-  subsd(xmm0, xmm3);\n-  mulsd(xmm1, Address(tmp, 2144));\n-  subsd(xmm4, xmm3);\n-  movsd(xmm7, Address(eax, 8));\n-  unpcklpd(xmm0, xmm0);\n-  movapd(xmm3, xmm4);\n-  subsd(xmm4, xmm2);\n-  mulpd(xmm5, xmm0);\n-  subpd(xmm0, xmm2);\n-  movdqu(xmm6, Address(tmp, 2064));\n-  mulsd(xmm7, xmm4);\n-  subsd(xmm3, xmm4);\n-  mulpd(xmm5, xmm0);\n-  mulpd(xmm0, xmm0);\n-  subsd(xmm3, xmm2);\n-  movdqu(xmm2, Address(eax, 0));\n-  subsd(xmm1, xmm3);\n-  movsd(xmm3, Address(eax, 24));\n-  addsd(xmm2, xmm3);\n-  subsd(xmm7, xmm2);\n-  mulsd(xmm2, xmm4);\n-  mulpd(xmm6, xmm0);\n-  mulsd(xmm3, xmm4);\n-  mulpd(xmm2, xmm0);\n-  mulpd(xmm0, xmm0);\n-  addpd(xmm5, Address(tmp, 2080));\n-  mulsd(xmm4, Address(eax, 0));\n-  addpd(xmm6, Address(tmp, 2048));\n-  mulpd(xmm5, xmm0);\n-  movapd(xmm0, xmm3);\n-  addsd(xmm3, Address(eax, 8));\n-  mulpd(xmm1, xmm7);\n-  movapd(xmm7, xmm4);\n-  addsd(xmm4, xmm3);\n-  addpd(xmm6, xmm5);\n-  movsd(xmm5, Address(eax, 8));\n-  subsd(xmm5, xmm3);\n-  subsd(xmm3, xmm4);\n-  addsd(xmm1, Address(eax, 16));\n-  mulpd(xmm6, xmm2);\n-  addsd(xmm5, xmm0);\n-  addsd(xmm3, xmm7);\n-  addsd(xmm1, xmm5);\n-  addsd(xmm1, xmm3);\n-  addsd(xmm1, xmm6);\n-  unpckhpd(xmm6, xmm6);\n-  addsd(xmm1, xmm6);\n-  addsd(xmm4, xmm1);\n-  movsd(Address(rsp, 0), xmm4);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  jcc(Assembler::greater, L_2TAG_PACKET_2_0_2);\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32767);\n-  pinsrw(xmm0, eax, 3);\n-  movsd(xmm1, Address(tmp, 2192));\n-  subsd(xmm1, xmm0);\n-  movsd(Address(rsp, 0), xmm1);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movl(eax, Address(rsp, 132));\n-  andl(eax, 2146435072);\n-  cmpl(eax, 2146435072);\n-  jcc(Assembler::equal, L_2TAG_PACKET_3_0_2);\n-  subl(rsp, 32);\n-  movsd(Address(rsp, 0), xmm0);\n-  lea(eax, Address(rsp, 40));\n-  movl(Address(rsp, 8), eax);\n-  movl(eax, 1);\n-  movl(Address(rsp, 12), eax);\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlibm_sin_cos_huge())));\n-  addl(rsp, 32);\n-  fld_d(Address(rsp, 8));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  fld_d(Address(rsp, 128));\n-  fmul_d(Address(tmp, 2208));\n-\n-  bind(L_2TAG_PACKET_1_0_2);\n-  movl(tmp, Address(rsp, 56));\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_cos.cpp","additions":0,"deletions":428,"binary":false,"changes":428,"status":"deleted"},{"patch":"@@ -1,330 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION - EXP()\n-\/\/                     ---------------------\n-\/\/\n-\/\/ Description:\n-\/\/  Let K = 64 (table size).\n-\/\/        x    x\/log(2)     n\n-\/\/       e  = 2          = 2 * T[j] * (1 + P(y))\n-\/\/  where\n-\/\/       x = m*log(2)\/K + y,    y in [-log(2)\/K..log(2)\/K]\n-\/\/       m = n*K + j,           m,n,j - signed integer, j in [-K\/2..K\/2]\n-\/\/                  j\/K\n-\/\/       values of 2   are tabulated as T[j] = T_hi[j] ( 1 + T_lo[j]).\n-\/\/\n-\/\/       P(y) is a minimax polynomial approximation of exp(x)-1\n-\/\/       on small interval [-log(2)\/K..log(2)\/K] (were calculated by Maple V).\n-\/\/\n-\/\/  To avoid problems with arithmetic overflow and underflow,\n-\/\/            n                        n1  n2\n-\/\/  value of 2  is safely computed as 2 * 2 where n1 in [-BIAS\/2..BIAS\/2]\n-\/\/  where BIAS is a value of exponent bias.\n-\/\/\n-\/\/ Special cases:\n-\/\/  exp(NaN) = NaN\n-\/\/  exp(+INF) = +INF\n-\/\/  exp(-INF) = 0\n-\/\/  exp(x) = 1 for subnormals\n-\/\/  for finite argument, only exp(0)=1 is exact\n-\/\/  For IEEE double\n-\/\/    if x >  709.782712893383973096 then exp(x) overflow\n-\/\/    if x < -745.133219101941108420 then exp(x) underflow\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table[] =\n-{\n-    0x00000000UL, 0xfff00000UL, 0x00000000UL, 0xfff00000UL, 0xffffffc0UL,\n-    0x00000000UL, 0xffffffc0UL, 0x00000000UL, 0x0000ffc0UL, 0x00000000UL,\n-    0x0000ffc0UL, 0x00000000UL, 0x00000000UL, 0x43380000UL, 0x00000000UL,\n-    0x43380000UL, 0x652b82feUL, 0x40571547UL, 0x652b82feUL, 0x40571547UL,\n-    0xfefa0000UL, 0x3f862e42UL, 0xfefa0000UL, 0x3f862e42UL, 0xbc9e3b3aUL,\n-    0x3d1cf79aUL, 0xbc9e3b3aUL, 0x3d1cf79aUL, 0xfffffffeUL, 0x3fdfffffUL,\n-    0xfffffffeUL, 0x3fdfffffUL, 0xe3289860UL, 0x3f56c15cUL, 0x555b9e25UL,\n-    0x3fa55555UL, 0xc090cf0fUL, 0x3f811115UL, 0x55548ba1UL, 0x3fc55555UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x0e03754dUL,\n-    0x3cad7bbfUL, 0x3e778060UL, 0x00002c9aUL, 0x3567f613UL, 0x3c8cd252UL,\n-    0xd3158574UL, 0x000059b0UL, 0x61e6c861UL, 0x3c60f74eUL, 0x18759bc8UL,\n-    0x00008745UL, 0x5d837b6cUL, 0x3c979aa6UL, 0x6cf9890fUL, 0x0000b558UL,\n-    0x702f9cd1UL, 0x3c3ebe3dUL, 0x32d3d1a2UL, 0x0000e3ecUL, 0x1e63bcd8UL,\n-    0x3ca3516eUL, 0xd0125b50UL, 0x00011301UL, 0x26f0387bUL, 0x3ca4c554UL,\n-    0xaea92ddfUL, 0x0001429aUL, 0x62523fb6UL, 0x3ca95153UL, 0x3c7d517aUL,\n-    0x000172b8UL, 0x3f1353bfUL, 0x3c8b898cUL, 0xeb6fcb75UL, 0x0001a35bUL,\n-    0x3e3a2f5fUL, 0x3c9aecf7UL, 0x3168b9aaUL, 0x0001d487UL, 0x44a6c38dUL,\n-    0x3c8a6f41UL, 0x88628cd6UL, 0x0002063bUL, 0xe3a8a894UL, 0x3c968efdUL,\n-    0x6e756238UL, 0x0002387aUL, 0x981fe7f2UL, 0x3c80472bUL, 0x65e27cddUL,\n-    0x00026b45UL, 0x6d09ab31UL, 0x3c82f7e1UL, 0xf51fdee1UL, 0x00029e9dUL,\n-    0x720c0ab3UL, 0x3c8b3782UL, 0xa6e4030bUL, 0x0002d285UL, 0x4db0abb6UL,\n-    0x3c834d75UL, 0x0a31b715UL, 0x000306feUL, 0x5dd3f84aUL, 0x3c8fdd39UL,\n-    0xb26416ffUL, 0x00033c08UL, 0xcc187d29UL, 0x3ca12f8cUL, 0x373aa9caUL,\n-    0x000371a7UL, 0x738b5e8bUL, 0x3ca7d229UL, 0x34e59ff6UL, 0x0003a7dbUL,\n-    0xa72a4c6dUL, 0x3c859f48UL, 0x4c123422UL, 0x0003dea6UL, 0x259d9205UL,\n-    0x3ca8b846UL, 0x21f72e29UL, 0x0004160aUL, 0x60c2ac12UL, 0x3c4363edUL,\n-    0x6061892dUL, 0x00044e08UL, 0xdaa10379UL, 0x3c6ecce1UL, 0xb5c13cd0UL,\n-    0x000486a2UL, 0xbb7aafb0UL, 0x3c7690ceUL, 0xd5362a27UL, 0x0004bfdaUL,\n-    0x9b282a09UL, 0x3ca083ccUL, 0x769d2ca6UL, 0x0004f9b2UL, 0xc1aae707UL,\n-    0x3ca509b0UL, 0x569d4f81UL, 0x0005342bUL, 0x18fdd78eUL, 0x3c933505UL,\n-    0x36b527daUL, 0x00056f47UL, 0xe21c5409UL, 0x3c9063e1UL, 0xdd485429UL,\n-    0x0005ab07UL, 0x2b64c035UL, 0x3c9432e6UL, 0x15ad2148UL, 0x0005e76fUL,\n-    0x99f08c0aUL, 0x3ca01284UL, 0xb03a5584UL, 0x0006247eUL, 0x0073dc06UL,\n-    0x3c99f087UL, 0x82552224UL, 0x00066238UL, 0x0da05571UL, 0x3c998d4dUL,\n-    0x667f3bccUL, 0x0006a09eUL, 0x86ce4786UL, 0x3ca52bb9UL, 0x3c651a2eUL,\n-    0x0006dfb2UL, 0x206f0dabUL, 0x3ca32092UL, 0xe8ec5f73UL, 0x00071f75UL,\n-    0x8e17a7a6UL, 0x3ca06122UL, 0x564267c8UL, 0x00075febUL, 0x461e9f86UL,\n-    0x3ca244acUL, 0x73eb0186UL, 0x0007a114UL, 0xabd66c55UL, 0x3c65ebe1UL,\n-    0x36cf4e62UL, 0x0007e2f3UL, 0xbbff67d0UL, 0x3c96fe9fUL, 0x994cce12UL,\n-    0x00082589UL, 0x14c801dfUL, 0x3c951f14UL, 0x9b4492ecUL, 0x000868d9UL,\n-    0xc1f0eab4UL, 0x3c8db72fUL, 0x422aa0dbUL, 0x0008ace5UL, 0x59f35f44UL,\n-    0x3c7bf683UL, 0x99157736UL, 0x0008f1aeUL, 0x9c06283cUL, 0x3ca360baUL,\n-    0xb0cdc5e4UL, 0x00093737UL, 0x20f962aaUL, 0x3c95e8d1UL, 0x9fde4e4fUL,\n-    0x00097d82UL, 0x2b91ce27UL, 0x3c71affcUL, 0x82a3f090UL, 0x0009c491UL,\n-    0x589a2ebdUL, 0x3c9b6d34UL, 0x7b5de564UL, 0x000a0c66UL, 0x9ab89880UL,\n-    0x3c95277cUL, 0xb23e255cUL, 0x000a5503UL, 0x6e735ab3UL, 0x3c846984UL,\n-    0x5579fdbfUL, 0x000a9e6bUL, 0x92cb3387UL, 0x3c8c1a77UL, 0x995ad3adUL,\n-    0x000ae89fUL, 0xdc2d1d96UL, 0x3ca22466UL, 0xb84f15faUL, 0x000b33a2UL,\n-    0xb19505aeUL, 0x3ca1112eUL, 0xf2fb5e46UL, 0x000b7f76UL, 0x0a5fddcdUL,\n-    0x3c74ffd7UL, 0x904bc1d2UL, 0x000bcc1eUL, 0x30af0cb3UL, 0x3c736eaeUL,\n-    0xdd85529cUL, 0x000c199bUL, 0xd10959acUL, 0x3c84e08fUL, 0x2e57d14bUL,\n-    0x000c67f1UL, 0x6c921968UL, 0x3c676b2cUL, 0xdcef9069UL, 0x000cb720UL,\n-    0x36df99b3UL, 0x3c937009UL, 0x4a07897bUL, 0x000d072dUL, 0xa63d07a7UL,\n-    0x3c74a385UL, 0xdcfba487UL, 0x000d5818UL, 0xd5c192acUL, 0x3c8e5a50UL,\n-    0x03db3285UL, 0x000da9e6UL, 0x1c4a9792UL, 0x3c98bb73UL, 0x337b9b5eUL,\n-    0x000dfc97UL, 0x603a88d3UL, 0x3c74b604UL, 0xe78b3ff6UL, 0x000e502eUL,\n-    0x92094926UL, 0x3c916f27UL, 0xa2a490d9UL, 0x000ea4afUL, 0x41aa2008UL,\n-    0x3c8ec3bcUL, 0xee615a27UL, 0x000efa1bUL, 0x31d185eeUL, 0x3c8a64a9UL,\n-    0x5b6e4540UL, 0x000f5076UL, 0x4d91cd9dUL, 0x3c77893bUL, 0x819e90d8UL,\n-    0x000fa7c1UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0x7ff00000UL,\n-    0x00000000UL, 0x00000000UL, 0xffffffffUL, 0x7fefffffUL, 0x00000000UL,\n-    0x00100000UL\n-};\n-\n-\/\/registers,\n-\/\/ input: (rbp + 8)\n-\/\/ scratch: xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7\n-\/\/          rax, rdx, rcx, rbx (tmp)\n-\n-\/\/ Code generated by Intel C compiler for LIBM library\n-\n-void MacroAssembler::fast_exp(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                              XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                              Register eax, Register ecx, Register edx, Register tmp) {\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label L_2TAG_PACKET_4_0_2, L_2TAG_PACKET_5_0_2, L_2TAG_PACKET_6_0_2, L_2TAG_PACKET_7_0_2;\n-  Label L_2TAG_PACKET_8_0_2, L_2TAG_PACKET_9_0_2, L_2TAG_PACKET_10_0_2, L_2TAG_PACKET_11_0_2;\n-  Label L_2TAG_PACKET_12_0_2;\n-\n-  assert_different_registers(tmp, eax, ecx, edx);\n-  address static_const_table = (address)_static_const_table;\n-\n-  subl(rsp, 120);\n-  movl(Address(rsp, 64), tmp);\n-  lea(tmp, ExternalAddress(static_const_table));\n-  movsd(xmm0, Address(rsp, 128));\n-  unpcklpd(xmm0, xmm0);\n-  movdqu(xmm1, Address(tmp, 64));          \/\/ 0x652b82feUL, 0x40571547UL, 0x652b82feUL, 0x40571547UL\n-  movdqu(xmm6, Address(tmp, 48));          \/\/ 0x00000000UL, 0x43380000UL, 0x00000000UL, 0x43380000UL\n-  movdqu(xmm2, Address(tmp, 80));          \/\/ 0xfefa0000UL, 0x3f862e42UL, 0xfefa0000UL, 0x3f862e42UL\n-  movdqu(xmm3, Address(tmp, 96));          \/\/ 0xbc9e3b3aUL, 0x3d1cf79aUL, 0xbc9e3b3aUL, 0x3d1cf79aUL\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32767);\n-  movl(edx, 16527);\n-  subl(edx, eax);\n-  subl(eax, 15504);\n-  orl(edx, eax);\n-  cmpl(edx, INT_MIN);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_0_0_2);\n-  mulpd(xmm1, xmm0);\n-  addpd(xmm1, xmm6);\n-  movapd(xmm7, xmm1);\n-  subpd(xmm1, xmm6);\n-  mulpd(xmm2, xmm1);\n-  movdqu(xmm4, Address(tmp, 128));         \/\/ 0xe3289860UL, 0x3f56c15cUL, 0x555b9e25UL, 0x3fa55555UL\n-  mulpd(xmm3, xmm1);\n-  movdqu(xmm5, Address(tmp, 144));         \/\/ 0xc090cf0fUL, 0x3f811115UL, 0x55548ba1UL, 0x3fc55555UL\n-  subpd(xmm0, xmm2);\n-  movdl(eax, xmm7);\n-  movl(ecx, eax);\n-  andl(ecx, 63);\n-  shll(ecx, 4);\n-  sarl(eax, 6);\n-  movl(edx, eax);\n-  movdqu(xmm6, Address(tmp, 16));          \/\/ 0xffffffc0UL, 0x00000000UL, 0xffffffc0UL, 0x00000000UL\n-  pand(xmm7, xmm6);\n-  movdqu(xmm6, Address(tmp, 32));          \/\/ 0x0000ffc0UL, 0x00000000UL, 0x0000ffc0UL, 0x00000000UL\n-  paddq(xmm7, xmm6);\n-  psllq(xmm7, 46);\n-  subpd(xmm0, xmm3);\n-  movdqu(xmm2, Address(tmp, ecx, Address::times_1, 160));\n-  mulpd(xmm4, xmm0);\n-  movapd(xmm6, xmm0);\n-  movapd(xmm1, xmm0);\n-  mulpd(xmm6, xmm6);\n-  mulpd(xmm0, xmm6);\n-  addpd(xmm5, xmm4);\n-  mulsd(xmm0, xmm6);\n-  mulpd(xmm6, Address(tmp, 112));          \/\/ 0xfffffffeUL, 0x3fdfffffUL, 0xfffffffeUL, 0x3fdfffffUL\n-  addsd(xmm1, xmm2);\n-  unpckhpd(xmm2, xmm2);\n-  mulpd(xmm0, xmm5);\n-  addsd(xmm1, xmm0);\n-  por(xmm2, xmm7);\n-  unpckhpd(xmm0, xmm0);\n-  addsd(xmm0, xmm1);\n-  addsd(xmm0, xmm6);\n-  addl(edx, 894);\n-  cmpl(edx, 1916);\n-  jcc(Assembler::above, L_2TAG_PACKET_1_0_2);\n-  mulsd(xmm0, xmm2);\n-  addsd(xmm0, xmm2);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_1_0_2);\n-  fnstcw(Address(rsp, 24));\n-  movzwl(edx, Address(rsp, 24));\n-  orl(edx, 768);\n-  movw(Address(rsp, 28), edx);\n-  fldcw(Address(rsp, 28));\n-  movl(edx, eax);\n-  sarl(eax, 1);\n-  subl(edx, eax);\n-  movdqu(xmm6, Address(tmp, 0));           \/\/ 0x00000000UL, 0xfff00000UL, 0x00000000UL, 0xfff00000UL\n-  pandn(xmm6, xmm2);\n-  addl(eax, 1023);\n-  movdl(xmm3, eax);\n-  psllq(xmm3, 52);\n-  por(xmm6, xmm3);\n-  addl(edx, 1023);\n-  movdl(xmm4, edx);\n-  psllq(xmm4, 52);\n-  movsd(Address(rsp, 8), xmm0);\n-  fld_d(Address(rsp, 8));\n-  movsd(Address(rsp, 16), xmm6);\n-  fld_d(Address(rsp, 16));\n-  fmula(1);\n-  faddp(1);\n-  movsd(Address(rsp, 8), xmm4);\n-  fld_d(Address(rsp, 8));\n-  fmulp(1);\n-  fstp_d(Address(rsp, 8));\n-  movsd(xmm0, Address(rsp, 8));\n-  fldcw(Address(rsp, 24));\n-  pextrw(ecx, xmm0, 3);\n-  andl(ecx, 32752);\n-  cmpl(ecx, 32752);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_3_0_2);\n-  cmpl(ecx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_4_0_2);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-  cmpl(ecx, INT_MIN);\n-  jcc(Assembler::below, L_2TAG_PACKET_3_0_2);\n-  cmpl(ecx, -1064950997);\n-  jcc(Assembler::below, L_2TAG_PACKET_2_0_2);\n-  jcc(Assembler::above, L_2TAG_PACKET_4_0_2);\n-  movl(edx, Address(rsp, 128));\n-  cmpl(edx, -17155601);\n-  jcc(Assembler::below, L_2TAG_PACKET_2_0_2);\n-  jmp(L_2TAG_PACKET_4_0_2);\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  movl(edx, 14);\n-  jmp(L_2TAG_PACKET_5_0_2);\n-\n-  bind(L_2TAG_PACKET_4_0_2);\n-  movl(edx, 15);\n-\n-  bind(L_2TAG_PACKET_5_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  movsd(xmm0, Address(rsp, 128));\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_6_0_2);\n-\n-  bind(L_2TAG_PACKET_7_0_2);\n-  cmpl(eax, 2146435072);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_8_0_2);\n-  movl(eax, Address(rsp, 132));\n-  cmpl(eax, INT_MIN);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_9_0_2);\n-  movsd(xmm0, Address(tmp, 1208));         \/\/ 0xffffffffUL, 0x7fefffffUL\n-  mulsd(xmm0, xmm0);\n-  movl(edx, 14);\n-  jmp(L_2TAG_PACKET_5_0_2);\n-\n-  bind(L_2TAG_PACKET_9_0_2);\n-  movsd(xmm0, Address(tmp, 1216));\n-  mulsd(xmm0, xmm0);\n-  movl(edx, 15);\n-  jmp(L_2TAG_PACKET_5_0_2);\n-\n-  bind(L_2TAG_PACKET_8_0_2);\n-  movl(edx, Address(rsp, 128));\n-  cmpl(eax, 2146435072);\n-  jcc(Assembler::above, L_2TAG_PACKET_10_0_2);\n-  cmpl(edx, 0);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_10_0_2);\n-  movl(eax, Address(rsp, 132));\n-  cmpl(eax, 2146435072);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_11_0_2);\n-  movsd(xmm0, Address(tmp, 1192));         \/\/ 0x00000000UL, 0x7ff00000UL\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_11_0_2);\n-  movsd(xmm0, Address(tmp, 1200));         \/\/ 0x00000000UL, 0x00000000UL\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_10_0_2);\n-  movsd(xmm0, Address(rsp, 128));\n-  addsd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  movl(eax, Address(rsp, 132));\n-  andl(eax, 2147483647);\n-  cmpl(eax, 1083179008);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_7_0_2);\n-  movsd(xmm0, Address(rsp, 128));\n-  addsd(xmm0, Address(tmp, 1184));         \/\/ 0x00000000UL, 0x3ff00000UL\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movsd(Address(rsp, 48), xmm0);\n-  fld_d(Address(rsp, 48));\n-\n-  bind(L_2TAG_PACKET_6_0_2);\n-  movl(tmp, Address(rsp, 64));\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_exp.cpp","additions":0,"deletions":330,"binary":false,"changes":330,"status":"deleted"},{"patch":"@@ -1,345 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION - LOG()\n-\/\/                     ---------------------\n-\/\/\n-\/\/    x=2^k * mx, mx in [1,2)\n-\/\/\n-\/\/    Get B~1\/mx based on the output of rcpss instruction (B0)\n-\/\/    B = int((B0*2^7+0.5))\/2^7\n-\/\/\n-\/\/    Reduced argument: r=B*mx-1.0 (computed accurately in high and low parts)\n-\/\/\n-\/\/    Result:  k*log(2) - log(B) + p(r) if |x-1| >= small value (2^-6)  and\n-\/\/             p(r) is a degree 7 polynomial\n-\/\/             -log(B) read from data table (high, low parts)\n-\/\/             Result is formed from high and low parts\n-\/\/\n-\/\/ Special cases:\n-\/\/  log(NaN) = quiet NaN, and raise invalid exception\n-\/\/  log(+INF) = that INF\n-\/\/  log(0) = -INF with divide-by-zero exception raised\n-\/\/  log(1) = +0\n-\/\/  log(x) = NaN with invalid exception raised if x < -0, including -INF\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-\/\/\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table_log[] =\n-{\n-    0xfefa3800UL, 0x3fe62e42UL, 0x93c76730UL, 0x3d2ef357UL, 0xaa241800UL,\n-    0x3fe5ee82UL, 0x0cda46beUL, 0x3d220238UL, 0x5c364800UL, 0x3fe5af40UL,\n-    0xac10c9fbUL, 0x3d2dfa63UL, 0x26bb8c00UL, 0x3fe5707aUL, 0xff3303ddUL,\n-    0x3d09980bUL, 0x26867800UL, 0x3fe5322eUL, 0x5d257531UL, 0x3d05ccc4UL,\n-    0x835a5000UL, 0x3fe4f45aUL, 0x6d93b8fbUL, 0xbd2e6c51UL, 0x6f970c00UL,\n-    0x3fe4b6fdUL, 0xed4c541cUL, 0x3cef7115UL, 0x27e8a400UL, 0x3fe47a15UL,\n-    0xf94d60aaUL, 0xbd22cb6aUL, 0xf2f92400UL, 0x3fe43d9fUL, 0x481051f7UL,\n-    0xbcfd984fUL, 0x2125cc00UL, 0x3fe4019cUL, 0x30f0c74cUL, 0xbd26ce79UL,\n-    0x0c36c000UL, 0x3fe3c608UL, 0x7cfe13c2UL, 0xbd02b736UL, 0x17197800UL,\n-    0x3fe38ae2UL, 0xbb5569a4UL, 0xbd218b7aUL, 0xad9d8c00UL, 0x3fe35028UL,\n-    0x9527e6acUL, 0x3d10b83fUL, 0x44340800UL, 0x3fe315daUL, 0xc5a0ed9cUL,\n-    0xbd274e93UL, 0x57b0e000UL, 0x3fe2dbf5UL, 0x07b9dc11UL, 0xbd17a6e5UL,\n-    0x6d0ec000UL, 0x3fe2a278UL, 0xe797882dUL, 0x3d206d2bUL, 0x1134dc00UL,\n-    0x3fe26962UL, 0x05226250UL, 0xbd0b61f1UL, 0xd8bebc00UL, 0x3fe230b0UL,\n-    0x6e48667bUL, 0x3d12fc06UL, 0x5fc61800UL, 0x3fe1f863UL, 0xc9fe81d3UL,\n-    0xbd2a7242UL, 0x49ae6000UL, 0x3fe1c078UL, 0xed70e667UL, 0x3cccacdeUL,\n-    0x40f23c00UL, 0x3fe188eeUL, 0xf8ab4650UL, 0x3d14cc4eUL, 0xf6f29800UL,\n-    0x3fe151c3UL, 0xa293ae49UL, 0xbd2edd97UL, 0x23c75c00UL, 0x3fe11af8UL,\n-    0xbb9ddcb2UL, 0xbd258647UL, 0x8611cc00UL, 0x3fe0e489UL, 0x07801742UL,\n-    0x3d1c2998UL, 0xe2d05400UL, 0x3fe0ae76UL, 0x887e7e27UL, 0x3d1f486bUL,\n-    0x0533c400UL, 0x3fe078bfUL, 0x41edf5fdUL, 0x3d268122UL, 0xbe760400UL,\n-    0x3fe04360UL, 0xe79539e0UL, 0xbd04c45fUL, 0xe5b20800UL, 0x3fe00e5aUL,\n-    0xb1727b1cUL, 0xbd053ba3UL, 0xaf7a4800UL, 0x3fdfb358UL, 0x3c164935UL,\n-    0x3d0085faUL, 0xee031800UL, 0x3fdf4aa7UL, 0x6f014a8bUL, 0x3d12cde5UL,\n-    0x56b41000UL, 0x3fdee2a1UL, 0x5a470251UL, 0x3d2f27f4UL, 0xc3ddb000UL,\n-    0x3fde7b42UL, 0x5372bd08UL, 0xbd246550UL, 0x1a272800UL, 0x3fde148aUL,\n-    0x07322938UL, 0xbd1326b2UL, 0x484c9800UL, 0x3fddae75UL, 0x60dc616aUL,\n-    0xbd1ea42dUL, 0x46def800UL, 0x3fdd4902UL, 0xe9a767a8UL, 0x3d235bafUL,\n-    0x18064800UL, 0x3fdce42fUL, 0x3ec7a6b0UL, 0xbd0797c3UL, 0xc7455800UL,\n-    0x3fdc7ff9UL, 0xc15249aeUL, 0xbd29b6ddUL, 0x693fa000UL, 0x3fdc1c60UL,\n-    0x7fe8e180UL, 0x3d2cec80UL, 0x1b80e000UL, 0x3fdbb961UL, 0xf40a666dUL,\n-    0x3d27d85bUL, 0x04462800UL, 0x3fdb56faUL, 0x2d841995UL, 0x3d109525UL,\n-    0x5248d000UL, 0x3fdaf529UL, 0x52774458UL, 0xbd217cc5UL, 0x3c8ad800UL,\n-    0x3fda93edUL, 0xbea77a5dUL, 0x3d1e36f2UL, 0x0224f800UL, 0x3fda3344UL,\n-    0x7f9d79f5UL, 0x3d23c645UL, 0xea15f000UL, 0x3fd9d32bUL, 0x10d0c0b0UL,\n-    0xbd26279eUL, 0x43135800UL, 0x3fd973a3UL, 0xa502d9f0UL, 0xbd152313UL,\n-    0x635bf800UL, 0x3fd914a8UL, 0x2ee6307dUL, 0xbd1766b5UL, 0xa88b3000UL,\n-    0x3fd8b639UL, 0xe5e70470UL, 0xbd205ae1UL, 0x776dc800UL, 0x3fd85855UL,\n-    0x3333778aUL, 0x3d2fd56fUL, 0x3bd81800UL, 0x3fd7fafaUL, 0xc812566aUL,\n-    0xbd272090UL, 0x687cf800UL, 0x3fd79e26UL, 0x2efd1778UL, 0x3d29ec7dUL,\n-    0x76c67800UL, 0x3fd741d8UL, 0x49dc60b3UL, 0x3d2d8b09UL, 0xe6af1800UL,\n-    0x3fd6e60eUL, 0x7c222d87UL, 0x3d172165UL, 0x3e9c6800UL, 0x3fd68ac8UL,\n-    0x2756eba0UL, 0x3d20a0d3UL, 0x0b3ab000UL, 0x3fd63003UL, 0xe731ae00UL,\n-    0xbd2db623UL, 0xdf596000UL, 0x3fd5d5bdUL, 0x08a465dcUL, 0xbd0a0b2aUL,\n-    0x53c8d000UL, 0x3fd57bf7UL, 0xee5d40efUL, 0x3d1fadedUL, 0x0738a000UL,\n-    0x3fd522aeUL, 0x8164c759UL, 0x3d2ebe70UL, 0x9e173000UL, 0x3fd4c9e0UL,\n-    0x1b0ad8a4UL, 0xbd2e2089UL, 0xc271c800UL, 0x3fd4718dUL, 0x0967d675UL,\n-    0xbd2f27ceUL, 0x23d5e800UL, 0x3fd419b4UL, 0xec90e09dUL, 0x3d08e436UL,\n-    0x77333000UL, 0x3fd3c252UL, 0xb606bd5cUL, 0x3d183b54UL, 0x76be1000UL,\n-    0x3fd36b67UL, 0xb0f177c8UL, 0x3d116ecdUL, 0xe1d36000UL, 0x3fd314f1UL,\n-    0xd3213cb8UL, 0xbd28e27aUL, 0x7cdc9000UL, 0x3fd2bef0UL, 0x4a5004f4UL,\n-    0x3d2a9cfaUL, 0x1134d800UL, 0x3fd26962UL, 0xdf5bb3b6UL, 0x3d2c93c1UL,\n-    0x6d0eb800UL, 0x3fd21445UL, 0xba46baeaUL, 0x3d0a87deUL, 0x635a6800UL,\n-    0x3fd1bf99UL, 0x5147bdb7UL, 0x3d2ca6edUL, 0xcbacf800UL, 0x3fd16b5cUL,\n-    0xf7a51681UL, 0x3d2b9acdUL, 0x8227e800UL, 0x3fd1178eUL, 0x63a5f01cUL,\n-    0xbd2c210eUL, 0x67616000UL, 0x3fd0c42dUL, 0x163ceae9UL, 0x3d27188bUL,\n-    0x604d5800UL, 0x3fd07138UL, 0x16ed4e91UL, 0x3cf89cdbUL, 0x5626c800UL,\n-    0x3fd01eaeUL, 0x1485e94aUL, 0xbd16f08cUL, 0x6cb3b000UL, 0x3fcf991cUL,\n-    0xca0cdf30UL, 0x3d1bcbecUL, 0xe4dd0000UL, 0x3fcef5adUL, 0x65bb8e11UL,\n-    0xbcca2115UL, 0xffe71000UL, 0x3fce530eUL, 0x6041f430UL, 0x3cc21227UL,\n-    0xb0d49000UL, 0x3fcdb13dUL, 0xf715b035UL, 0xbd2aff2aUL, 0xf2656000UL,\n-    0x3fcd1037UL, 0x75b6f6e4UL, 0xbd084a7eUL, 0xc6f01000UL, 0x3fcc6ffbUL,\n-    0xc5962bd2UL, 0xbcf1ec72UL, 0x383be000UL, 0x3fcbd087UL, 0x595412b6UL,\n-    0xbd2d4bc4UL, 0x575bd000UL, 0x3fcb31d8UL, 0x4eace1aaUL, 0xbd0c358dUL,\n-    0x3c8ae000UL, 0x3fca93edUL, 0x50562169UL, 0xbd287243UL, 0x07089000UL,\n-    0x3fc9f6c4UL, 0x6865817aUL, 0x3d29904dUL, 0xdcf70000UL, 0x3fc95a5aUL,\n-    0x58a0ff6fUL, 0x3d07f228UL, 0xeb390000UL, 0x3fc8beafUL, 0xaae92cd1UL,\n-    0xbd073d54UL, 0x6551a000UL, 0x3fc823c1UL, 0x9a631e83UL, 0x3d1e0ddbUL,\n-    0x85445000UL, 0x3fc7898dUL, 0x70914305UL, 0xbd1c6610UL, 0x8b757000UL,\n-    0x3fc6f012UL, 0xe59c21e1UL, 0xbd25118dUL, 0xbe8c1000UL, 0x3fc6574eUL,\n-    0x2c3c2e78UL, 0x3d19cf8bUL, 0x6b544000UL, 0x3fc5bf40UL, 0xeb68981cUL,\n-    0xbd127023UL, 0xe4a1b000UL, 0x3fc527e5UL, 0xe5697dc7UL, 0x3d2633e8UL,\n-    0x8333b000UL, 0x3fc4913dUL, 0x54fdb678UL, 0x3d258379UL, 0xa5993000UL,\n-    0x3fc3fb45UL, 0x7e6a354dUL, 0xbd2cd1d8UL, 0xb0159000UL, 0x3fc365fcUL,\n-    0x234b7289UL, 0x3cc62fa8UL, 0x0c868000UL, 0x3fc2d161UL, 0xcb81b4a1UL,\n-    0x3d039d6cUL, 0x2a49c000UL, 0x3fc23d71UL, 0x8fd3df5cUL, 0x3d100d23UL,\n-    0x7e23f000UL, 0x3fc1aa2bUL, 0x44389934UL, 0x3d2ca78eUL, 0x8227e000UL,\n-    0x3fc1178eUL, 0xce2d07f2UL, 0x3d21ef78UL, 0xb59e4000UL, 0x3fc08598UL,\n-    0x7009902cUL, 0xbd27e5ddUL, 0x39dbe000UL, 0x3fbfe891UL, 0x4fa10afdUL,\n-    0xbd2534d6UL, 0x830a2000UL, 0x3fbec739UL, 0xafe645e0UL, 0xbd2dc068UL,\n-    0x63844000UL, 0x3fbda727UL, 0x1fa71733UL, 0x3d1a8940UL, 0x01bc4000UL,\n-    0x3fbc8858UL, 0xc65aacd3UL, 0x3d2646d1UL, 0x8dad6000UL, 0x3fbb6ac8UL,\n-    0x2bf768e5UL, 0xbd139080UL, 0x40b1c000UL, 0x3fba4e76UL, 0xb94407c8UL,\n-    0xbd0e42b6UL, 0x5d594000UL, 0x3fb9335eUL, 0x3abd47daUL, 0x3d23115cUL,\n-    0x2f40e000UL, 0x3fb8197eUL, 0xf96ffdf7UL, 0x3d0f80dcUL, 0x0aeac000UL,\n-    0x3fb700d3UL, 0xa99ded32UL, 0x3cec1e8dUL, 0x4d97a000UL, 0x3fb5e95aUL,\n-    0x3c5d1d1eUL, 0xbd2c6906UL, 0x5d208000UL, 0x3fb4d311UL, 0x82f4e1efUL,\n-    0xbcf53a25UL, 0xa7d1e000UL, 0x3fb3bdf5UL, 0xa5db4ed7UL, 0x3d2cc85eUL,\n-    0xa4472000UL, 0x3fb2aa04UL, 0xae9c697dUL, 0xbd20b6e8UL, 0xd1466000UL,\n-    0x3fb1973bUL, 0x560d9e9bUL, 0xbd25325dUL, 0xb59e4000UL, 0x3fb08598UL,\n-    0x7009902cUL, 0xbd17e5ddUL, 0xc006c000UL, 0x3faeea31UL, 0x4fc93b7bUL,\n-    0xbd0e113eUL, 0xcdddc000UL, 0x3faccb73UL, 0x47d82807UL, 0xbd1a68f2UL,\n-    0xd0fb0000UL, 0x3faaaef2UL, 0x353bb42eUL, 0x3d20fc1aUL, 0x149fc000UL,\n-    0x3fa894aaUL, 0xd05a267dUL, 0xbd197995UL, 0xf2d4c000UL, 0x3fa67c94UL,\n-    0xec19afa2UL, 0xbd029efbUL, 0xd42e0000UL, 0x3fa466aeUL, 0x75bdfd28UL,\n-    0xbd2c1673UL, 0x2f8d0000UL, 0x3fa252f3UL, 0xe021b67bUL, 0x3d283e9aUL,\n-    0x89e74000UL, 0x3fa0415dUL, 0x5cf1d753UL, 0x3d0111c0UL, 0xec148000UL,\n-    0x3f9c63d2UL, 0x3f9eb2f3UL, 0x3d2578c6UL, 0x28c90000UL, 0x3f984925UL,\n-    0x325a0c34UL, 0xbd2aa0baUL, 0x25980000UL, 0x3f9432a9UL, 0x928637feUL,\n-    0x3d098139UL, 0x58938000UL, 0x3f902056UL, 0x06e2f7d2UL, 0xbd23dc5bUL,\n-    0xa3890000UL, 0x3f882448UL, 0xda74f640UL, 0xbd275577UL, 0x75890000UL,\n-    0x3f801015UL, 0x999d2be8UL, 0xbd10c76bUL, 0x59580000UL, 0x3f700805UL,\n-    0xcb31c67bUL, 0x3d2166afUL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x80000000UL, 0xfefa3800UL, 0x3fa62e42UL, 0x93c76730UL, 0x3ceef357UL,\n-    0x92492492UL, 0x3fc24924UL, 0x00000000UL, 0xbfd00000UL, 0x3d6fb175UL,\n-    0xbfc5555eUL, 0x55555555UL, 0x3fd55555UL, 0x9999999aUL, 0x3fc99999UL,\n-    0x00000000UL, 0xbfe00000UL, 0x00000000UL, 0xffffe000UL, 0x00000000UL,\n-    0xffffe000UL\n-};\n-\n-\/\/registers,\n-\/\/ input: xmm0\n-\/\/ scratch: xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7\n-\/\/          rax, rdx, rcx, rbx (tmp)\n-void MacroAssembler::fast_log(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                              XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                              Register eax, Register ecx, Register edx, Register tmp) {\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label L_2TAG_PACKET_4_0_2, L_2TAG_PACKET_5_0_2, L_2TAG_PACKET_6_0_2, L_2TAG_PACKET_7_0_2;\n-  Label L_2TAG_PACKET_8_0_2, L_2TAG_PACKET_9_0_2;\n-  Label L_2TAG_PACKET_10_0_2;\n-\n-  assert_different_registers(tmp, eax, ecx, edx);\n-  address static_const_table = (address)_static_const_table_log;\n-\n-  subl(rsp, 104);\n-  movl(Address(rsp, 40), tmp);\n-  lea(tmp, ExternalAddress(static_const_table));\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 16368);\n-  pinsrw(xmm2, eax, 3);\n-  xorpd(xmm3, xmm3);\n-  movl(edx, 30704);\n-  pinsrw(xmm3, edx, 3);\n-  movsd(xmm0, Address(rsp, 112));\n-  movapd(xmm1, xmm0);\n-  movl(ecx, 32768);\n-  movdl(xmm4, ecx);\n-  movsd(xmm5, Address(tmp, 2128));         \/\/ 0x00000000UL, 0xffffe000UL\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  psllq(xmm0, 5);\n-  movl(ecx, 16352);\n-  psrlq(xmm0, 34);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm1, 12);\n-  pshufd(xmm6, xmm5, 228);\n-  psrlq(xmm1, 12);\n-  subl(eax, 16);\n-  cmpl(eax, 32736);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_0_0_2);\n-\n-  bind(L_2TAG_PACKET_1_0_2);\n-  paddd(xmm0, xmm4);\n-  por(xmm1, xmm3);\n-  movdl(edx, xmm0);\n-  psllq(xmm0, 29);\n-  pand(xmm5, xmm1);\n-  pand(xmm0, xmm6);\n-  subsd(xmm1, xmm5);\n-  mulpd(xmm5, xmm0);\n-  andl(eax, 32752);\n-  subl(eax, ecx);\n-  cvtsi2sdl(xmm7, eax);\n-  mulsd(xmm1, xmm0);\n-  movsd(xmm6, Address(tmp, 2064));         \/\/ 0xfefa3800UL, 0x3fa62e42UL\n-  movdqu(xmm3, Address(tmp, 2080));        \/\/ 0x92492492UL, 0x3fc24924UL, 0x00000000UL, 0xbfd00000UL\n-  subsd(xmm5, xmm2);\n-  andl(edx, 16711680);\n-  shrl(edx, 12);\n-  movdqu(xmm0, Address(tmp, edx));\n-  movdqu(xmm4, Address(tmp, 2096));        \/\/ 0x3d6fb175UL, 0xbfc5555eUL, 0x55555555UL, 0x3fd55555UL\n-  addsd(xmm1, xmm5);\n-  movdqu(xmm2, Address(tmp, 2112));        \/\/ 0x9999999aUL, 0x3fc99999UL, 0x00000000UL, 0xbfe00000UL\n-  mulsd(xmm6, xmm7);\n-  pshufd(xmm5, xmm1, 68);\n-  mulsd(xmm7, Address(tmp, 2072));         \/\/ 0x93c76730UL, 0x3ceef357UL, 0x92492492UL, 0x3fc24924UL\n-  mulsd(xmm3, xmm1);\n-  addsd(xmm0, xmm6);\n-  mulpd(xmm4, xmm5);\n-  mulpd(xmm5, xmm5);\n-  pshufd(xmm6, xmm0, 228);\n-  addsd(xmm0, xmm1);\n-  addpd(xmm4, xmm2);\n-  mulpd(xmm3, xmm5);\n-  subsd(xmm6, xmm0);\n-  mulsd(xmm4, xmm1);\n-  pshufd(xmm2, xmm0, 238);\n-  addsd(xmm1, xmm6);\n-  mulsd(xmm5, xmm5);\n-  addsd(xmm7, xmm2);\n-  addpd(xmm4, xmm3);\n-  addsd(xmm1, xmm7);\n-  mulpd(xmm4, xmm5);\n-  addsd(xmm1, xmm4);\n-  pshufd(xmm5, xmm4, 238);\n-  addsd(xmm1, xmm5);\n-  addsd(xmm0, xmm1);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  movsd(xmm0, Address(rsp, 112));\n-  movdqu(xmm1, xmm0);\n-  addl(eax, 16);\n-  cmpl(eax, 32768);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_3_0_2);\n-  cmpl(eax, 16);\n-  jcc(Assembler::below, L_2TAG_PACKET_4_0_2);\n-\n-  bind(L_2TAG_PACKET_5_0_2);\n-  addsd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_6_0_2);\n-  jcc(Assembler::above, L_2TAG_PACKET_5_0_2);\n-  cmpl(edx, 0);\n-  jcc(Assembler::above, L_2TAG_PACKET_5_0_2);\n-  jmp(L_2TAG_PACKET_7_0_2);\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  movdl(edx, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(ecx, xmm1);\n-  addl(ecx, ecx);\n-  cmpl(ecx, -2097152);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_6_0_2);\n-  orl(edx, ecx);\n-  cmpl(edx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_8_0_2);\n-\n-  bind(L_2TAG_PACKET_7_0_2);\n-  xorpd(xmm1, xmm1);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 32752);\n-  pinsrw(xmm1, eax, 3);\n-  movl(edx, 3);\n-  mulsd(xmm0, xmm1);\n-\n-  bind(L_2TAG_PACKET_9_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  movsd(xmm0, Address(rsp, 112));\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_10_0_2);\n-\n-  bind(L_2TAG_PACKET_8_0_2);\n-  xorpd(xmm1, xmm1);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 49136);\n-  pinsrw(xmm0, eax, 3);\n-  divsd(xmm0, xmm1);\n-  movl(edx, 2);\n-  jmp(L_2TAG_PACKET_9_0_2);\n-\n-  bind(L_2TAG_PACKET_4_0_2);\n-  movdl(edx, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(ecx, xmm1);\n-  orl(edx, ecx);\n-  cmpl(edx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_8_0_2);\n-  xorpd(xmm1, xmm1);\n-  movl(eax, 18416);\n-  pinsrw(xmm1, eax, 3);\n-  mulsd(xmm0, xmm1);\n-  movapd(xmm1, xmm0);\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  psllq(xmm0, 5);\n-  movl(ecx, 18416);\n-  psrlq(xmm0, 34);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm1, 12);\n-  pshufd(xmm6, xmm5, 228);\n-  psrlq(xmm1, 12);\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movsd(Address(rsp, 24), xmm0);\n-  fld_d(Address(rsp, 24));\n-\n-  bind(L_2TAG_PACKET_10_0_2);\n-  movl(tmp, Address(rsp, 40));\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_log.cpp","additions":0,"deletions":345,"binary":false,"changes":345,"status":"deleted"},{"patch":"@@ -1,358 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION - LOG10()\n-\/\/                     ---------------------\n-\/\/\n-\/\/    Let x=2^k * mx, mx in [1,2)\n-\/\/\n-\/\/    Get B~1\/mx based on the output of rcpss instruction (B0)\n-\/\/    B = int((B0*LH*2^7+0.5))\/2^7\n-\/\/    LH is a short approximation for log10(e)\n-\/\/\n-\/\/    Reduced argument: r=B*mx-LH (computed accurately in high and low parts)\n-\/\/\n-\/\/    Result:  k*log10(2) - log(B) + p(r)\n-\/\/             p(r) is a degree 7 polynomial\n-\/\/             -log(B) read from data table (high, low parts)\n-\/\/             Result is formed from high and low parts\n-\/\/\n-\/\/ Special cases:\n-\/\/  log10(0) = -INF with divide-by-zero exception raised\n-\/\/  log10(1) = +0\n-\/\/  log10(x) = NaN with invalid exception raised if x < -0, including -INF\n-\/\/  log10(+INF) = +INF\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table_log10[] =\n-{\n-    0x509f7800UL, 0x3fd34413UL, 0x1f12b358UL, 0x3d1fef31UL, 0x80333400UL,\n-    0x3fd32418UL, 0xc671d9d0UL, 0xbcf542bfUL, 0x51195000UL, 0x3fd30442UL,\n-    0x78a4b0c3UL, 0x3d18216aUL, 0x6fc79400UL, 0x3fd2e490UL, 0x80fa389dUL,\n-    0xbc902869UL, 0x89d04000UL, 0x3fd2c502UL, 0x75c2f564UL, 0x3d040754UL,\n-    0x4ddd1c00UL, 0x3fd2a598UL, 0xd219b2c3UL, 0xbcfa1d84UL, 0x6baa7c00UL,\n-    0x3fd28651UL, 0xfd9abec1UL, 0x3d1be6d3UL, 0x94028800UL, 0x3fd2672dUL,\n-    0xe289a455UL, 0xbd1ede5eUL, 0x78b86400UL, 0x3fd2482cUL, 0x6734d179UL,\n-    0x3d1fe79bUL, 0xcca3c800UL, 0x3fd2294dUL, 0x981a40b8UL, 0xbced34eaUL,\n-    0x439c5000UL, 0x3fd20a91UL, 0xcc392737UL, 0xbd1a9cc3UL, 0x92752c00UL,\n-    0x3fd1ebf6UL, 0x03c9afe7UL, 0x3d1e98f8UL, 0x6ef8dc00UL, 0x3fd1cd7dUL,\n-    0x71dae7f4UL, 0x3d08a86cUL, 0x8fe4dc00UL, 0x3fd1af25UL, 0xee9185a1UL,\n-    0xbcff3412UL, 0xace59400UL, 0x3fd190eeUL, 0xc2cab353UL, 0x3cf17ed9UL,\n-    0x7e925000UL, 0x3fd172d8UL, 0x6952c1b2UL, 0x3cf1521cUL, 0xbe694400UL,\n-    0x3fd154e2UL, 0xcacb79caUL, 0xbd0bdc78UL, 0x26cbac00UL, 0x3fd1370dUL,\n-    0xf71f4de1UL, 0xbd01f8beUL, 0x72fa0800UL, 0x3fd11957UL, 0x55bf910bUL,\n-    0x3c946e2bUL, 0x5f106000UL, 0x3fd0fbc1UL, 0x39e639c1UL, 0x3d14a84bUL,\n-    0xa802a800UL, 0x3fd0de4aUL, 0xd3f31d5dUL, 0xbd178385UL, 0x0b992000UL,\n-    0x3fd0c0f3UL, 0x3843106fUL, 0xbd1f602fUL, 0x486ce800UL, 0x3fd0a3baUL,\n-    0x8819497cUL, 0x3cef987aUL, 0x1de49400UL, 0x3fd086a0UL, 0x1caa0467UL,\n-    0x3d0faec7UL, 0x4c30cc00UL, 0x3fd069a4UL, 0xa4424372UL, 0xbd1618fcUL,\n-    0x94490000UL, 0x3fd04cc6UL, 0x946517d2UL, 0xbd18384bUL, 0xb7e84000UL,\n-    0x3fd03006UL, 0xe0109c37UL, 0xbd19a6acUL, 0x798a0c00UL, 0x3fd01364UL,\n-    0x5121e864UL, 0xbd164cf7UL, 0x38ce8000UL, 0x3fcfedbfUL, 0x46214d1aUL,\n-    0xbcbbc402UL, 0xc8e62000UL, 0x3fcfb4efUL, 0xdab93203UL, 0x3d1e0176UL,\n-    0x2cb02800UL, 0x3fcf7c5aUL, 0x2a2ea8e4UL, 0xbcfec86aUL, 0xeeeaa000UL,\n-    0x3fcf43fdUL, 0xc18e49a4UL, 0x3cf110a8UL, 0x9bb6e800UL, 0x3fcf0bdaUL,\n-    0x923cc9c0UL, 0xbd15ce99UL, 0xc093f000UL, 0x3fced3efUL, 0x4d4b51e9UL,\n-    0x3d1a04c7UL, 0xec58f800UL, 0x3fce9c3cUL, 0x163cad59UL, 0x3cac8260UL,\n-    0x9a907000UL, 0x3fce2d7dUL, 0x3fa93646UL, 0x3ce4a1c0UL, 0x37311000UL,\n-    0x3fcdbf99UL, 0x32abd1fdUL, 0x3d07ea9dUL, 0x6744b800UL, 0x3fcd528cUL,\n-    0x4dcbdfd4UL, 0xbd1b08e2UL, 0xe36de800UL, 0x3fcce653UL, 0x0b7b7f7fUL,\n-    0xbd1b8f03UL, 0x77506800UL, 0x3fcc7aecUL, 0xa821c9fbUL, 0x3d13c163UL,\n-    0x00ff8800UL, 0x3fcc1053UL, 0x536bca76UL, 0xbd074ee5UL, 0x70719800UL,\n-    0x3fcba684UL, 0xd7da9b6bUL, 0xbd1fbf16UL, 0xc6f8d800UL, 0x3fcb3d7dUL,\n-    0xe2220bb3UL, 0x3d1a295dUL, 0x16c15800UL, 0x3fcad53cUL, 0xe724911eUL,\n-    0xbcf55822UL, 0x82533800UL, 0x3fca6dbcUL, 0x6d982371UL, 0x3cac567cUL,\n-    0x3c19e800UL, 0x3fca06fcUL, 0x84d17d80UL, 0x3d1da204UL, 0x85ef8000UL,\n-    0x3fc9a0f8UL, 0x54466a6aUL, 0xbd002204UL, 0xb0ac2000UL, 0x3fc93baeUL,\n-    0xd601fd65UL, 0x3d18840cUL, 0x1bb9b000UL, 0x3fc8d71cUL, 0x7bf58766UL,\n-    0xbd14f897UL, 0x34aae800UL, 0x3fc8733eUL, 0x3af6ac24UL, 0xbd0f5c45UL,\n-    0x76d68000UL, 0x3fc81012UL, 0x4303e1a1UL, 0xbd1f9a80UL, 0x6af57800UL,\n-    0x3fc7ad96UL, 0x43fbcb46UL, 0x3cf4c33eUL, 0xa6c51000UL, 0x3fc74bc7UL,\n-    0x70f0eac5UL, 0xbd192e3bUL, 0xccab9800UL, 0x3fc6eaa3UL, 0xc0093dfeUL,\n-    0xbd0faf15UL, 0x8b60b800UL, 0x3fc68a28UL, 0xde78d5fdUL, 0xbc9ea4eeUL,\n-    0x9d987000UL, 0x3fc62a53UL, 0x962bea6eUL, 0xbd194084UL, 0xc9b0e800UL,\n-    0x3fc5cb22UL, 0x888dd999UL, 0x3d1fe201UL, 0xe1634800UL, 0x3fc56c93UL,\n-    0x16ada7adUL, 0x3d1b1188UL, 0xc176c000UL, 0x3fc50ea4UL, 0x4159b5b5UL,\n-    0xbcf09c08UL, 0x51766000UL, 0x3fc4b153UL, 0x84393d23UL, 0xbcf6a89cUL,\n-    0x83695000UL, 0x3fc4549dUL, 0x9f0b8bbbUL, 0x3d1c4b8cUL, 0x538d5800UL,\n-    0x3fc3f881UL, 0xf49df747UL, 0x3cf89b99UL, 0xc8138000UL, 0x3fc39cfcUL,\n-    0xd503b834UL, 0xbd13b99fUL, 0xf0df0800UL, 0x3fc3420dUL, 0xf011b386UL,\n-    0xbd05d8beUL, 0xe7466800UL, 0x3fc2e7b2UL, 0xf39c7bc2UL, 0xbd1bb94eUL,\n-    0xcdd62800UL, 0x3fc28de9UL, 0x05e6d69bUL, 0xbd10ed05UL, 0xd015d800UL,\n-    0x3fc234b0UL, 0xe29b6c9dUL, 0xbd1ff967UL, 0x224ea800UL, 0x3fc1dc06UL,\n-    0x727711fcUL, 0xbcffb30dUL, 0x01540000UL, 0x3fc183e8UL, 0x39786c5aUL,\n-    0x3cc23f57UL, 0xb24d9800UL, 0x3fc12c54UL, 0xc905a342UL, 0x3d003a1dUL,\n-    0x82835800UL, 0x3fc0d54aUL, 0x9b9920c0UL, 0x3d03b25aUL, 0xc72ac000UL,\n-    0x3fc07ec7UL, 0x46f26a24UL, 0x3cf0fa41UL, 0xdd35d800UL, 0x3fc028caUL,\n-    0x41d9d6dcUL, 0x3d034a65UL, 0x52474000UL, 0x3fbfa6a4UL, 0x44f66449UL,\n-    0x3d19cad3UL, 0x2da3d000UL, 0x3fbefcb8UL, 0x67832999UL, 0x3d18400fUL,\n-    0x32a10000UL, 0x3fbe53ceUL, 0x9c0e3b1aUL, 0xbcff62fdUL, 0x556b7000UL,\n-    0x3fbdabe3UL, 0x02976913UL, 0xbcf8243bUL, 0x97e88000UL, 0x3fbd04f4UL,\n-    0xec793797UL, 0x3d1c0578UL, 0x09647000UL, 0x3fbc5effUL, 0x05fc0565UL,\n-    0xbd1d799eUL, 0xc6426000UL, 0x3fbbb9ffUL, 0x4625f5edUL, 0x3d1f5723UL,\n-    0xf7afd000UL, 0x3fbb15f3UL, 0xdd5aae61UL, 0xbd1a7e1eUL, 0xd358b000UL,\n-    0x3fba72d8UL, 0x3314e4d3UL, 0x3d17bc91UL, 0x9b1f5000UL, 0x3fb9d0abUL,\n-    0x9a4d514bUL, 0x3cf18c9bUL, 0x9cd4e000UL, 0x3fb92f69UL, 0x7e4496abUL,\n-    0x3cf1f96dUL, 0x31f4f000UL, 0x3fb88f10UL, 0xf56479e7UL, 0x3d165818UL,\n-    0xbf628000UL, 0x3fb7ef9cUL, 0x26bf486dUL, 0xbd1113a6UL, 0xb526b000UL,\n-    0x3fb7510cUL, 0x1a1c3384UL, 0x3ca9898dUL, 0x8e31e000UL, 0x3fb6b35dUL,\n-    0xb3875361UL, 0xbd0661acUL, 0xd01de000UL, 0x3fb6168cUL, 0x2a7cacfaUL,\n-    0xbd1bdf10UL, 0x0af23000UL, 0x3fb57a98UL, 0xff868816UL, 0x3cf046d0UL,\n-    0xd8ea0000UL, 0x3fb4df7cUL, 0x1515fbe7UL, 0xbd1fd529UL, 0xde3b2000UL,\n-    0x3fb44538UL, 0x6e59a132UL, 0x3d1faeeeUL, 0xc8df9000UL, 0x3fb3abc9UL,\n-    0xf1322361UL, 0xbd198807UL, 0x505f1000UL, 0x3fb3132dUL, 0x0888e6abUL,\n-    0x3d1e5380UL, 0x359bd000UL, 0x3fb27b61UL, 0xdfbcbb22UL, 0xbcfe2724UL,\n-    0x429ee000UL, 0x3fb1e463UL, 0x6eb4c58cUL, 0xbcfe4dd6UL, 0x4a673000UL,\n-    0x3fb14e31UL, 0x4ce1ac9bUL, 0x3d1ba691UL, 0x28b96000UL, 0x3fb0b8c9UL,\n-    0x8c7813b8UL, 0xbd0b3872UL, 0xc1f08000UL, 0x3fb02428UL, 0xc2bc8c2cUL,\n-    0x3cb5ea6bUL, 0x05a1a000UL, 0x3faf209cUL, 0x72e8f18eUL, 0xbce8df84UL,\n-    0xc0b5e000UL, 0x3fadfa6dUL, 0x9fdef436UL, 0x3d087364UL, 0xaf416000UL,\n-    0x3facd5c2UL, 0x1068c3a9UL, 0x3d0827e7UL, 0xdb356000UL, 0x3fabb296UL,\n-    0x120a34d3UL, 0x3d101a9fUL, 0x5dfea000UL, 0x3faa90e6UL, 0xdaded264UL,\n-    0xbd14c392UL, 0x6034c000UL, 0x3fa970adUL, 0x1c9d06a9UL, 0xbd1b705eUL,\n-    0x194c6000UL, 0x3fa851e8UL, 0x83996ad9UL, 0xbd0117bcUL, 0xcf4ac000UL,\n-    0x3fa73492UL, 0xb1a94a62UL, 0xbca5ea42UL, 0xd67b4000UL, 0x3fa618a9UL,\n-    0x75aed8caUL, 0xbd07119bUL, 0x9126c000UL, 0x3fa4fe29UL, 0x5291d533UL,\n-    0x3d12658fUL, 0x6f4d4000UL, 0x3fa3e50eUL, 0xcd2c5cd9UL, 0x3d1d5c70UL,\n-    0xee608000UL, 0x3fa2cd54UL, 0xd1008489UL, 0x3d1a4802UL, 0x9900e000UL,\n-    0x3fa1b6f9UL, 0x54fb5598UL, 0xbd16593fUL, 0x06bb6000UL, 0x3fa0a1f9UL,\n-    0x64ef57b4UL, 0xbd17636bUL, 0xb7940000UL, 0x3f9f1c9fUL, 0xee6a4737UL,\n-    0x3cb5d479UL, 0x91aa0000UL, 0x3f9cf7f5UL, 0x3a16373cUL, 0x3d087114UL,\n-    0x156b8000UL, 0x3f9ad5edUL, 0x836c554aUL, 0x3c6900b0UL, 0xd4764000UL,\n-    0x3f98b67fUL, 0xed12f17bUL, 0xbcffc974UL, 0x77dec000UL, 0x3f9699a7UL,\n-    0x232ce7eaUL, 0x3d1e35bbUL, 0xbfbf4000UL, 0x3f947f5dUL, 0xd84ffa6eUL,\n-    0x3d0e0a49UL, 0x82c7c000UL, 0x3f92679cUL, 0x8d170e90UL, 0xbd14d9f2UL,\n-    0xadd20000UL, 0x3f90525dUL, 0x86d9f88eUL, 0x3cdeb986UL, 0x86f10000UL,\n-    0x3f8c7f36UL, 0xb9e0a517UL, 0x3ce29faaUL, 0xb75c8000UL, 0x3f885e9eUL,\n-    0x542568cbUL, 0xbd1f7bdbUL, 0x46b30000UL, 0x3f8442e8UL, 0xb954e7d9UL,\n-    0x3d1e5287UL, 0xb7e60000UL, 0x3f802c07UL, 0x22da0b17UL, 0xbd19fb27UL,\n-    0x6c8b0000UL, 0x3f7833e3UL, 0x821271efUL, 0xbd190f96UL, 0x29910000UL,\n-    0x3f701936UL, 0xbc3491a5UL, 0xbd1bcf45UL, 0x354a0000UL, 0x3f600fe3UL,\n-    0xc0ff520aUL, 0xbd19d71cUL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x509f7800UL, 0x3f934413UL, 0x1f12b358UL, 0x3cdfef31UL,\n-    0xc1a5f12eUL, 0x40358874UL, 0x64d4ef0dUL, 0xc0089309UL, 0x385593b1UL,\n-    0xc025c917UL, 0xdc963467UL, 0x3ffc6a02UL, 0x7f9d3aa1UL, 0x4016ab9fUL,\n-    0xdc77b115UL, 0xbff27af2UL, 0xf8000000UL, 0xffffffffUL, 0x00000000UL,\n-    0xffffe000UL, 0x00000000UL, 0x3fdbc000UL, 0xbf2e4108UL, 0x3f5a7a6cUL\n-};\n-\/\/registers,\n-\/\/ input: xmm0\n-\/\/ scratch: xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7\n-\/\/          rax, rdx, rcx, rbx (tmp)\n-\n-void MacroAssembler::fast_log10(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                                Register eax, Register ecx, Register edx, Register tmp) {\n-\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label L_2TAG_PACKET_4_0_2, L_2TAG_PACKET_5_0_2, L_2TAG_PACKET_6_0_2, L_2TAG_PACKET_7_0_2;\n-  Label L_2TAG_PACKET_8_0_2, L_2TAG_PACKET_9_0_2, L_2TAG_PACKET_10_0_2;\n-\n-  assert_different_registers(tmp, eax, ecx, edx);\n-\n-  address static_const_table_log10 = (address)_static_const_table_log10;\n-\n-  subl(rsp, 104);\n-  movl(Address(rsp, 40), tmp);\n-  lea(tmp, ExternalAddress(static_const_table_log10));\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 16368);\n-  pinsrw(xmm2, eax, 3);\n-  movl(ecx, 1054736384);\n-  movdl(xmm7, ecx);\n-  xorpd(xmm3, xmm3);\n-  movl(edx, 30704);\n-  pinsrw(xmm3, edx, 3);\n-  movsd(xmm0, Address(rsp, 112));\n-  movdqu(xmm1, xmm0);\n-  movl(edx, 32768);\n-  movdl(xmm4, edx);\n-  movdqu(xmm5, Address(tmp, 2128));    \/\/0x3ffc6a02UL, 0x7f9d3aa1UL, 0x4016ab9fUL, 0xdc77b115UL\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  movl(ecx, 16352);\n-  psllq(xmm0, 5);\n-  movsd(xmm2, Address(tmp, 2144));    \/\/0xbff27af2UL, 0xf8000000UL, 0xffffffffUL, 0x00000000UL\n-  psrlq(xmm0, 34);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm1, 12);\n-  pshufd(xmm6, xmm5, 78);\n-  psrlq(xmm1, 12);\n-  subl(eax, 16);\n-  cmpl(eax, 32736);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_0_0_2);\n-\n-  bind(L_2TAG_PACKET_1_0_2);\n-  mulss(xmm0, xmm7);\n-  por(xmm1, xmm3);\n-  andpd(xmm5, xmm1);\n-  paddd(xmm0, xmm4);\n-  subsd(xmm1, xmm5);\n-  movdl(edx, xmm0);\n-  psllq(xmm0, 29);\n-  andpd(xmm0, xmm6);\n-  andl(eax, 32752);\n-  subl(eax, ecx);\n-  cvtsi2sdl(xmm7, eax);\n-  mulpd(xmm5, xmm0);\n-  mulsd(xmm1, xmm0);\n-  movsd(xmm6, Address(tmp, 2064));    \/\/0xbd19d71cUL, 0x00000000UL, 0x00000000UL, 0x00000000UL\n-  movdqu(xmm3, Address(tmp, 2080));    \/\/0x00000000UL, 0x509f7800UL, 0x3f934413UL, 0x1f12b358UL\n-  subsd(xmm5, xmm2);\n-  andl(edx, 16711680);\n-  shrl(edx, 12);\n-  movdqu(xmm0, Address(tmp, edx, Address::times_1, -1504));\n-  movdqu(xmm4, Address(tmp, 2096));    \/\/0x3cdfef31UL, 0xc1a5f12eUL, 0x40358874UL, 0x64d4ef0dUL\n-  addsd(xmm1, xmm5);\n-  movdqu(xmm2, Address(tmp, 2112));    \/\/0xc0089309UL, 0x385593b1UL, 0xc025c917UL, 0xdc963467UL\n-  mulsd(xmm6, xmm7);\n-  pshufd(xmm5, xmm1, 68);\n-  mulsd(xmm7, Address(tmp, 2072));    \/\/0x00000000UL, 0x00000000UL, 0x00000000UL, 0x509f7800UL\n-  mulsd(xmm3, xmm1);\n-  addsd(xmm0, xmm6);\n-  mulpd(xmm4, xmm5);\n-  movsd(xmm6, Address(tmp, 2152));    \/\/0xffffffffUL, 0x00000000UL, 0xffffe000UL, 0x00000000UL\n-  mulpd(xmm5, xmm5);\n-  addpd(xmm4, xmm2);\n-  mulpd(xmm3, xmm5);\n-  pshufd(xmm2, xmm0, 228);\n-  addsd(xmm0, xmm1);\n-  mulsd(xmm4, xmm1);\n-  subsd(xmm2, xmm0);\n-  mulsd(xmm6, xmm1);\n-  addsd(xmm1, xmm2);\n-  pshufd(xmm2, xmm0, 238);\n-  mulsd(xmm5, xmm5);\n-  addsd(xmm7, xmm2);\n-  addsd(xmm1, xmm6);\n-  addpd(xmm4, xmm3);\n-  addsd(xmm1, xmm7);\n-  mulpd(xmm4, xmm5);\n-  addsd(xmm1, xmm4);\n-  pshufd(xmm5, xmm4, 238);\n-  addsd(xmm1, xmm5);\n-  addsd(xmm0, xmm1);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  movsd(xmm0, Address(rsp, 112));    \/\/0xbcfa1d84UL, 0x6baa7c00UL, 0x3fd28651UL, 0xfd9abec1UL\n-  movdqu(xmm1, xmm0);\n-  addl(eax, 16);\n-  cmpl(eax, 32768);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_3_0_2);\n-  cmpl(eax, 16);\n-  jcc(Assembler::below, L_2TAG_PACKET_4_0_2);\n-\n-  bind(L_2TAG_PACKET_5_0_2);\n-  addsd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_2_0_2);\n-\n-  bind(L_2TAG_PACKET_6_0_2);\n-  jcc(Assembler::above, L_2TAG_PACKET_5_0_2);\n-  cmpl(edx, 0);\n-  jcc(Assembler::above, L_2TAG_PACKET_5_0_2);\n-  jmp(L_2TAG_PACKET_7_0_2);\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  movdl(edx, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(ecx, xmm1);\n-  addl(ecx, ecx);\n-  cmpl(ecx, -2097152);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_6_0_2);\n-  orl(edx, ecx);\n-  cmpl(edx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_8_0_2);\n-\n-  bind(L_2TAG_PACKET_7_0_2);\n-  xorpd(xmm1, xmm1);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 32752);\n-  pinsrw(xmm1, eax, 3);\n-  movl(edx, 9);\n-  mulsd(xmm0, xmm1);\n-\n-  bind(L_2TAG_PACKET_9_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  movsd(xmm0, Address(rsp, 112));    \/\/0xbcfa1d84UL, 0x6baa7c00UL, 0x3fd28651UL, 0xfd9abec1UL\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_10_0_2);\n-\n-  bind(L_2TAG_PACKET_8_0_2);\n-  xorpd(xmm1, xmm1);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 49136);\n-  pinsrw(xmm0, eax, 3);\n-  divsd(xmm0, xmm1);\n-  movl(edx, 8);\n-  jmp(L_2TAG_PACKET_9_0_2);\n-\n-  bind(L_2TAG_PACKET_4_0_2);\n-  movdl(edx, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(ecx, xmm1);\n-  orl(edx, ecx);\n-  cmpl(edx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_8_0_2);\n-  xorpd(xmm1, xmm1);\n-  movl(eax, 18416);\n-  pinsrw(xmm1, eax, 3);\n-  mulsd(xmm0, xmm1);\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 16368);\n-  pinsrw(xmm2, eax, 3);\n-  movdqu(xmm1, xmm0);\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  movl(ecx, 18416);\n-  psllq(xmm0, 5);\n-  movsd(xmm2, Address(tmp, 2144));    \/\/0xbff27af2UL, 0xf8000000UL, 0xffffffffUL, 0x00000000UL\n-  psrlq(xmm0, 34);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm1, 12);\n-  pshufd(xmm6, xmm5, 78);\n-  psrlq(xmm1, 12);\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movsd(Address(rsp, 24), xmm0);\n-  fld_d(Address(rsp, 24));\n-\n-  bind(L_2TAG_PACKET_10_0_2);\n-  movl(tmp, Address(rsp, 40));\n-\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_log10.cpp","additions":0,"deletions":358,"binary":false,"changes":358,"status":"deleted"},{"patch":"@@ -1,1856 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION  - POW()\n-\/\/                     ---------------------\n-\/\/\n-\/\/    Let x=2^k * mx, mx in [1,2)\n-\/\/\n-\/\/    log2(x) calculation:\n-\/\/\n-\/\/    Get B~1\/mx based on the output of rcpps instruction (B0)\n-\/\/    B = int((B0*LH*2^9+0.5))\/2^9\n-\/\/    LH is a short approximation for log2(e)\n-\/\/\n-\/\/    Reduced argument, scaled by LH:\n-\/\/                r=B*mx-LH (computed accurately in high and low parts)\n-\/\/\n-\/\/    log2(x) result:  k - log2(B) + p(r)\n-\/\/             p(r) is a degree 8 polynomial\n-\/\/             -log2(B) read from data table (high, low parts)\n-\/\/             log2(x) is formed from high and low parts\n-\/\/    For |x| in [1-1\/32, 1+1\/16), a slower but more accurate computation\n-\/\/    based om the same table design is performed.\n-\/\/\n-\/\/   Main path is taken if | floor(log2(|log2(|x|)|) + floor(log2|y|) | < 8,\n-\/\/   to filter out all potential OF\/UF cases.\n-\/\/   exp2(y*log2(x)) is computed using an 8-bit index table and a degree 5\n-\/\/   polynomial\n-\/\/\n-\/\/ Special cases:\n-\/\/  pow(-0,y) = -INF and raises the divide-by-zero exception for y an odd\n-\/\/  integer < 0.\n-\/\/  pow(-0,y) = +INF and raises the divide-by-zero exception for y < 0 and\n-\/\/  not an odd integer.\n-\/\/  pow(-0,y) = -0 for y an odd integer > 0.\n-\/\/  pow(-0,y) = +0 for y > 0 and not an odd integer.\n-\/\/  pow(-1,-INF) = NaN.\n-\/\/  pow(+1,y) = NaN for any y, even a NaN.\n-\/\/  pow(x,-0) = 1 for any x, even a NaN.\n-\/\/  pow(x,y) = a NaN and raises the invalid exception for finite x < 0 and\n-\/\/  finite non-integer y.\n-\/\/  pow(x,-INF) = +INF for |x|<1.\n-\/\/  pow(x,-INF) = +0 for |x|>1.\n-\/\/  pow(x,+INF) = +0 for |x|<1.\n-\/\/  pow(x,+INF) = +INF for |x|>1.\n-\/\/  pow(-INF,y) = -0 for y an odd integer < 0.\n-\/\/  pow(-INF,y) = +0 for y < 0 and not an odd integer.\n-\/\/  pow(-INF,y) = -INF for y an odd integer > 0.\n-\/\/  pow(-INF,y) = +INF for y > 0 and not an odd integer.\n-\/\/  pow(+INF,y) = +0 for y <0.\n-\/\/  pow(+INF,y) = +INF for y >0.\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table_pow[] =\n-{\n-    0x00000000UL, 0xbfd61a00UL, 0x00000000UL, 0xbf5dabe1UL, 0xf8000000UL,\n-    0xffffffffUL, 0x00000000UL, 0xfffff800UL, 0x00000000UL, 0x3ff00000UL,\n-    0x00000000UL, 0x00000000UL, 0x20000000UL, 0x3feff00aUL, 0x96621f95UL,\n-    0x3e5b1856UL, 0xe0000000UL, 0x3fefe019UL, 0xe5916f9eUL, 0xbe325278UL,\n-    0x00000000UL, 0x3fefd02fUL, 0x859a1062UL, 0x3e595fb7UL, 0xc0000000UL,\n-    0x3fefc049UL, 0xb245f18fUL, 0xbe529c38UL, 0xe0000000UL, 0x3fefb069UL,\n-    0xad2880a7UL, 0xbe501230UL, 0x60000000UL, 0x3fefa08fUL, 0xc8e72420UL,\n-    0x3e597bd1UL, 0x80000000UL, 0x3fef90baUL, 0xc30c4500UL, 0xbe5d6c75UL,\n-    0xe0000000UL, 0x3fef80eaUL, 0x02c63f43UL, 0x3e2e1318UL, 0xc0000000UL,\n-    0x3fef7120UL, 0xb3d4ccccUL, 0xbe44c52aUL, 0x00000000UL, 0x3fef615cUL,\n-    0xdbd91397UL, 0xbe4e7d6cUL, 0xa0000000UL, 0x3fef519cUL, 0x65c5cd68UL,\n-    0xbe522dc8UL, 0xa0000000UL, 0x3fef41e2UL, 0x46d1306cUL, 0xbe5a840eUL,\n-    0xe0000000UL, 0x3fef322dUL, 0xd2980e94UL, 0x3e5071afUL, 0xa0000000UL,\n-    0x3fef227eUL, 0x773abadeUL, 0xbe5891e5UL, 0xa0000000UL, 0x3fef12d4UL,\n-    0xdc6bf46bUL, 0xbe5cccbeUL, 0xe0000000UL, 0x3fef032fUL, 0xbc7247faUL,\n-    0xbe2bab83UL, 0x80000000UL, 0x3feef390UL, 0xbcaa1e46UL, 0xbe53bb3bUL,\n-    0x60000000UL, 0x3feee3f6UL, 0x5f6c682dUL, 0xbe54c619UL, 0x80000000UL,\n-    0x3feed461UL, 0x5141e368UL, 0xbe4b6d86UL, 0xe0000000UL, 0x3feec4d1UL,\n-    0xec678f76UL, 0xbe369af6UL, 0x80000000UL, 0x3feeb547UL, 0x41301f55UL,\n-    0xbe2d4312UL, 0x60000000UL, 0x3feea5c2UL, 0x676da6bdUL, 0xbe4d8dd0UL,\n-    0x60000000UL, 0x3fee9642UL, 0x57a891c4UL, 0x3e51f991UL, 0xa0000000UL,\n-    0x3fee86c7UL, 0xe4eb491eUL, 0x3e579bf9UL, 0x20000000UL, 0x3fee7752UL,\n-    0xfddc4a2cUL, 0xbe3356e6UL, 0xc0000000UL, 0x3fee67e1UL, 0xd75b5bf1UL,\n-    0xbe449531UL, 0x80000000UL, 0x3fee5876UL, 0xbd423b8eUL, 0x3df54fe4UL,\n-    0x60000000UL, 0x3fee4910UL, 0x330e51b9UL, 0x3e54289cUL, 0x80000000UL,\n-    0x3fee39afUL, 0x8651a95fUL, 0xbe55aad6UL, 0xa0000000UL, 0x3fee2a53UL,\n-    0x5e98c708UL, 0xbe2fc4a9UL, 0xe0000000UL, 0x3fee1afcUL, 0x0989328dUL,\n-    0x3e23958cUL, 0x40000000UL, 0x3fee0babUL, 0xee642abdUL, 0xbe425dd8UL,\n-    0xa0000000UL, 0x3fedfc5eUL, 0xc394d236UL, 0x3e526362UL, 0x20000000UL,\n-    0x3feded17UL, 0xe104aa8eUL, 0x3e4ce247UL, 0xc0000000UL, 0x3fedddd4UL,\n-    0x265a9be4UL, 0xbe5bb77aUL, 0x40000000UL, 0x3fedce97UL, 0x0ecac52fUL,\n-    0x3e4a7cb1UL, 0xe0000000UL, 0x3fedbf5eUL, 0x124cb3b8UL, 0x3e257024UL,\n-    0x80000000UL, 0x3fedb02bUL, 0xe6d4febeUL, 0xbe2033eeUL, 0x20000000UL,\n-    0x3feda0fdUL, 0x39cca00eUL, 0xbe3ddabcUL, 0xc0000000UL, 0x3fed91d3UL,\n-    0xef8a552aUL, 0xbe543390UL, 0x40000000UL, 0x3fed82afUL, 0xb8e85204UL,\n-    0x3e513850UL, 0xe0000000UL, 0x3fed738fUL, 0x3d59fe08UL, 0xbe5db728UL,\n-    0x40000000UL, 0x3fed6475UL, 0x3aa7ead1UL, 0x3e58804bUL, 0xc0000000UL,\n-    0x3fed555fUL, 0xf8a35ba9UL, 0xbe5298b0UL, 0x00000000UL, 0x3fed464fUL,\n-    0x9a88dd15UL, 0x3e5a8cdbUL, 0x40000000UL, 0x3fed3743UL, 0xb0b0a190UL,\n-    0x3e598635UL, 0x80000000UL, 0x3fed283cUL, 0xe2113295UL, 0xbe5c1119UL,\n-    0x80000000UL, 0x3fed193aUL, 0xafbf1728UL, 0xbe492e9cUL, 0x60000000UL,\n-    0x3fed0a3dUL, 0xe4a4ccf3UL, 0x3e19b90eUL, 0x20000000UL, 0x3fecfb45UL,\n-    0xba3cbeb8UL, 0x3e406b50UL, 0xc0000000UL, 0x3fecec51UL, 0x110f7dddUL,\n-    0x3e0d6806UL, 0x40000000UL, 0x3fecdd63UL, 0x7dd7d508UL, 0xbe5a8943UL,\n-    0x80000000UL, 0x3fecce79UL, 0x9b60f271UL, 0xbe50676aUL, 0x80000000UL,\n-    0x3fecbf94UL, 0x0b9ad660UL, 0x3e59174fUL, 0x60000000UL, 0x3fecb0b4UL,\n-    0x00823d9cUL, 0x3e5bbf72UL, 0x20000000UL, 0x3feca1d9UL, 0x38a6ec89UL,\n-    0xbe4d38f9UL, 0x80000000UL, 0x3fec9302UL, 0x3a0b7d8eUL, 0x3e53dbfdUL,\n-    0xc0000000UL, 0x3fec8430UL, 0xc6826b34UL, 0xbe27c5c9UL, 0xc0000000UL,\n-    0x3fec7563UL, 0x0c706381UL, 0xbe593653UL, 0x60000000UL, 0x3fec669bUL,\n-    0x7df34ec7UL, 0x3e461ab5UL, 0xe0000000UL, 0x3fec57d7UL, 0x40e5e7e8UL,\n-    0xbe5c3daeUL, 0x00000000UL, 0x3fec4919UL, 0x5602770fUL, 0xbe55219dUL,\n-    0xc0000000UL, 0x3fec3a5eUL, 0xec7911ebUL, 0x3e5a5d25UL, 0x60000000UL,\n-    0x3fec2ba9UL, 0xb39ea225UL, 0xbe53c00bUL, 0x80000000UL, 0x3fec1cf8UL,\n-    0x967a212eUL, 0x3e5a8ddfUL, 0x60000000UL, 0x3fec0e4cUL, 0x580798bdUL,\n-    0x3e5f53abUL, 0x00000000UL, 0x3febffa5UL, 0xb8282df6UL, 0xbe46b874UL,\n-    0x20000000UL, 0x3febf102UL, 0xe33a6729UL, 0x3e54963fUL, 0x00000000UL,\n-    0x3febe264UL, 0x3b53e88aUL, 0xbe3adce1UL, 0x60000000UL, 0x3febd3caUL,\n-    0xc2585084UL, 0x3e5cde9fUL, 0x80000000UL, 0x3febc535UL, 0xa335c5eeUL,\n-    0xbe39fd9cUL, 0x20000000UL, 0x3febb6a5UL, 0x7325b04dUL, 0x3e42ba15UL,\n-    0x60000000UL, 0x3feba819UL, 0x1564540fUL, 0x3e3a9f35UL, 0x40000000UL,\n-    0x3feb9992UL, 0x83fff592UL, 0xbe5465ceUL, 0xa0000000UL, 0x3feb8b0fUL,\n-    0xb9da63d3UL, 0xbe4b1a0aUL, 0x80000000UL, 0x3feb7c91UL, 0x6d6f1ea4UL,\n-    0x3e557657UL, 0x00000000UL, 0x3feb6e18UL, 0x5e80a1bfUL, 0x3e4ddbb6UL,\n-    0x00000000UL, 0x3feb5fa3UL, 0x1c9eacb5UL, 0x3e592877UL, 0xa0000000UL,\n-    0x3feb5132UL, 0x6d40beb3UL, 0xbe51858cUL, 0xa0000000UL, 0x3feb42c6UL,\n-    0xd740c67bUL, 0x3e427ad2UL, 0x40000000UL, 0x3feb345fUL, 0xa3e0cceeUL,\n-    0xbe5c2fc4UL, 0x40000000UL, 0x3feb25fcUL, 0x8e752b50UL, 0xbe3da3c2UL,\n-    0xc0000000UL, 0x3feb179dUL, 0xa892e7deUL, 0x3e1fb481UL, 0xc0000000UL,\n-    0x3feb0943UL, 0x21ed71e9UL, 0xbe365206UL, 0x20000000UL, 0x3feafaeeUL,\n-    0x0e1380a3UL, 0x3e5c5b7bUL, 0x20000000UL, 0x3feaec9dUL, 0x3c3d640eUL,\n-    0xbe5dbbd0UL, 0x60000000UL, 0x3feade50UL, 0x8f97a715UL, 0x3e3a8ec5UL,\n-    0x20000000UL, 0x3fead008UL, 0x23ab2839UL, 0x3e2fe98aUL, 0x40000000UL,\n-    0x3feac1c4UL, 0xf4bbd50fUL, 0x3e54d8f6UL, 0xe0000000UL, 0x3feab384UL,\n-    0x14757c4dUL, 0xbe48774cUL, 0xc0000000UL, 0x3feaa549UL, 0x7c7b0eeaUL,\n-    0x3e5b51bbUL, 0x20000000UL, 0x3fea9713UL, 0xf56f7013UL, 0x3e386200UL,\n-    0xe0000000UL, 0x3fea88e0UL, 0xbe428ebeUL, 0xbe514af5UL, 0xe0000000UL,\n-    0x3fea7ab2UL, 0x8d0e4496UL, 0x3e4f9165UL, 0x60000000UL, 0x3fea6c89UL,\n-    0xdbacc5d5UL, 0xbe5c063bUL, 0x20000000UL, 0x3fea5e64UL, 0x3f19d970UL,\n-    0xbe5a0c8cUL, 0x20000000UL, 0x3fea5043UL, 0x09ea3e6bUL, 0x3e5065dcUL,\n-    0x80000000UL, 0x3fea4226UL, 0x78df246cUL, 0x3e5e05f6UL, 0x40000000UL,\n-    0x3fea340eUL, 0x4057d4a0UL, 0x3e431b2bUL, 0x40000000UL, 0x3fea25faUL,\n-    0x82867bb5UL, 0x3e4b76beUL, 0xa0000000UL, 0x3fea17eaUL, 0x9436f40aUL,\n-    0xbe5aad39UL, 0x20000000UL, 0x3fea09dfUL, 0x4b5253b3UL, 0x3e46380bUL,\n-    0x00000000UL, 0x3fe9fbd8UL, 0x8fc52466UL, 0xbe386f9bUL, 0x20000000UL,\n-    0x3fe9edd5UL, 0x22d3f344UL, 0xbe538347UL, 0x60000000UL, 0x3fe9dfd6UL,\n-    0x1ac33522UL, 0x3e5dbc53UL, 0x00000000UL, 0x3fe9d1dcUL, 0xeabdff1dUL,\n-    0x3e40fc0cUL, 0xe0000000UL, 0x3fe9c3e5UL, 0xafd30e73UL, 0xbe585e63UL,\n-    0xe0000000UL, 0x3fe9b5f3UL, 0xa52f226aUL, 0xbe43e8f9UL, 0x20000000UL,\n-    0x3fe9a806UL, 0xecb8698dUL, 0xbe515b36UL, 0x80000000UL, 0x3fe99a1cUL,\n-    0xf2b4e89dUL, 0x3e48b62bUL, 0x20000000UL, 0x3fe98c37UL, 0x7c9a88fbUL,\n-    0x3e44414cUL, 0x00000000UL, 0x3fe97e56UL, 0xda015741UL, 0xbe5d13baUL,\n-    0xe0000000UL, 0x3fe97078UL, 0x5fdace06UL, 0x3e51b947UL, 0x00000000UL,\n-    0x3fe962a0UL, 0x956ca094UL, 0x3e518785UL, 0x40000000UL, 0x3fe954cbUL,\n-    0x01164c1dUL, 0x3e5d5b57UL, 0xc0000000UL, 0x3fe946faUL, 0xe63b3767UL,\n-    0xbe4f84e7UL, 0x40000000UL, 0x3fe9392eUL, 0xe57cc2a9UL, 0x3e34eda3UL,\n-    0xe0000000UL, 0x3fe92b65UL, 0x8c75b544UL, 0x3e5766a0UL, 0xc0000000UL,\n-    0x3fe91da1UL, 0x37d1d087UL, 0xbe5e2ab1UL, 0x80000000UL, 0x3fe90fe1UL,\n-    0xa953dc20UL, 0x3e5fa1f3UL, 0x80000000UL, 0x3fe90225UL, 0xdbd3f369UL,\n-    0x3e47d6dbUL, 0xa0000000UL, 0x3fe8f46dUL, 0x1c9be989UL, 0xbe5e2b0aUL,\n-    0xa0000000UL, 0x3fe8e6b9UL, 0x3c93d76aUL, 0x3e5c8618UL, 0xe0000000UL,\n-    0x3fe8d909UL, 0x2182fc9aUL, 0xbe41aa9eUL, 0x20000000UL, 0x3fe8cb5eUL,\n-    0xe6b3539dUL, 0xbe530d19UL, 0x60000000UL, 0x3fe8bdb6UL, 0x49e58cc3UL,\n-    0xbe3bb374UL, 0xa0000000UL, 0x3fe8b012UL, 0xa7cfeb8fUL, 0x3e56c412UL,\n-    0x00000000UL, 0x3fe8a273UL, 0x8d52bc19UL, 0x3e1429b8UL, 0x60000000UL,\n-    0x3fe894d7UL, 0x4dc32c6cUL, 0xbe48604cUL, 0xc0000000UL, 0x3fe8873fUL,\n-    0x0c868e56UL, 0xbe564ee5UL, 0x00000000UL, 0x3fe879acUL, 0x56aee828UL,\n-    0x3e5e2fd8UL, 0x60000000UL, 0x3fe86c1cUL, 0x7ceab8ecUL, 0x3e493365UL,\n-    0xc0000000UL, 0x3fe85e90UL, 0x78d4dadcUL, 0xbe4f7f25UL, 0x00000000UL,\n-    0x3fe85109UL, 0x0ccd8280UL, 0x3e31e7a2UL, 0x40000000UL, 0x3fe84385UL,\n-    0x34ba4e15UL, 0x3e328077UL, 0x80000000UL, 0x3fe83605UL, 0xa670975aUL,\n-    0xbe53eee5UL, 0xa0000000UL, 0x3fe82889UL, 0xf61b77b2UL, 0xbe43a20aUL,\n-    0xa0000000UL, 0x3fe81b11UL, 0x13e6643bUL, 0x3e5e5fe5UL, 0xc0000000UL,\n-    0x3fe80d9dUL, 0x82cc94e8UL, 0xbe5ff1f9UL, 0xa0000000UL, 0x3fe8002dUL,\n-    0x8a0c9c5dUL, 0xbe42b0e7UL, 0x60000000UL, 0x3fe7f2c1UL, 0x22a16f01UL,\n-    0x3e5d9ea0UL, 0x20000000UL, 0x3fe7e559UL, 0xc38cd451UL, 0x3e506963UL,\n-    0xc0000000UL, 0x3fe7d7f4UL, 0x9902bc71UL, 0x3e4503d7UL, 0x40000000UL,\n-    0x3fe7ca94UL, 0xdef2a3c0UL, 0x3e3d98edUL, 0xa0000000UL, 0x3fe7bd37UL,\n-    0xed49abb0UL, 0x3e24c1ffUL, 0xe0000000UL, 0x3fe7afdeUL, 0xe3b0be70UL,\n-    0xbe40c467UL, 0x00000000UL, 0x3fe7a28aUL, 0xaf9f193cUL, 0xbe5dff6cUL,\n-    0xe0000000UL, 0x3fe79538UL, 0xb74cf6b6UL, 0xbe258ed0UL, 0xa0000000UL,\n-    0x3fe787ebUL, 0x1d9127c7UL, 0x3e345fb0UL, 0x40000000UL, 0x3fe77aa2UL,\n-    0x1028c21dUL, 0xbe4619bdUL, 0xa0000000UL, 0x3fe76d5cUL, 0x7cb0b5e4UL,\n-    0x3e40f1a2UL, 0xe0000000UL, 0x3fe7601aUL, 0x2b1bc4adUL, 0xbe32e8bbUL,\n-    0xe0000000UL, 0x3fe752dcUL, 0x6839f64eUL, 0x3e41f57bUL, 0xc0000000UL,\n-    0x3fe745a2UL, 0xc4121f7eUL, 0xbe52c40aUL, 0x60000000UL, 0x3fe7386cUL,\n-    0xd6852d72UL, 0xbe5c4e6bUL, 0xc0000000UL, 0x3fe72b39UL, 0x91d690f7UL,\n-    0xbe57f88fUL, 0xe0000000UL, 0x3fe71e0aUL, 0x627a2159UL, 0xbe4425d5UL,\n-    0xc0000000UL, 0x3fe710dfUL, 0x50a54033UL, 0x3e422b7eUL, 0x60000000UL,\n-    0x3fe703b8UL, 0x3b0b5f91UL, 0x3e5d3857UL, 0xe0000000UL, 0x3fe6f694UL,\n-    0x84d628a2UL, 0xbe51f090UL, 0x00000000UL, 0x3fe6e975UL, 0x306d8894UL,\n-    0xbe414d83UL, 0xe0000000UL, 0x3fe6dc58UL, 0x30bf24aaUL, 0xbe4650caUL,\n-    0x80000000UL, 0x3fe6cf40UL, 0xd4628d69UL, 0xbe5db007UL, 0xc0000000UL,\n-    0x3fe6c22bUL, 0xa2aae57bUL, 0xbe31d279UL, 0xc0000000UL, 0x3fe6b51aUL,\n-    0x860edf7eUL, 0xbe2d4c4aUL, 0x80000000UL, 0x3fe6a80dUL, 0xf3559341UL,\n-    0xbe5f7e98UL, 0xe0000000UL, 0x3fe69b03UL, 0xa885899eUL, 0xbe5c2011UL,\n-    0xe0000000UL, 0x3fe68dfdUL, 0x2bdc6d37UL, 0x3e224a82UL, 0xa0000000UL,\n-    0x3fe680fbUL, 0xc12ad1b9UL, 0xbe40cf56UL, 0x00000000UL, 0x3fe673fdUL,\n-    0x1bcdf659UL, 0xbdf52f2dUL, 0x00000000UL, 0x3fe66702UL, 0x5df10408UL,\n-    0x3e5663e0UL, 0xc0000000UL, 0x3fe65a0aUL, 0xa4070568UL, 0xbe40b12fUL,\n-    0x00000000UL, 0x3fe64d17UL, 0x71c54c47UL, 0x3e5f5e8bUL, 0x00000000UL,\n-    0x3fe64027UL, 0xbd4b7e83UL, 0x3e42ead6UL, 0xa0000000UL, 0x3fe6333aUL,\n-    0x61598bd2UL, 0xbe4c48d4UL, 0xc0000000UL, 0x3fe62651UL, 0x6f538d61UL,\n-    0x3e548401UL, 0xa0000000UL, 0x3fe6196cUL, 0x14344120UL, 0xbe529af6UL,\n-    0x00000000UL, 0x3fe60c8bUL, 0x5982c587UL, 0xbe3e1e4fUL, 0x00000000UL,\n-    0x3fe5ffadUL, 0xfe51d4eaUL, 0xbe4c897aUL, 0x80000000UL, 0x3fe5f2d2UL,\n-    0xfd46ebe1UL, 0x3e552e00UL, 0xa0000000UL, 0x3fe5e5fbUL, 0xa4695699UL,\n-    0x3e5ed471UL, 0x60000000UL, 0x3fe5d928UL, 0x80d118aeUL, 0x3e456b61UL,\n-    0xa0000000UL, 0x3fe5cc58UL, 0x304c330bUL, 0x3e54dc29UL, 0x80000000UL,\n-    0x3fe5bf8cUL, 0x0af2dedfUL, 0xbe3aa9bdUL, 0xe0000000UL, 0x3fe5b2c3UL,\n-    0x15fc9258UL, 0xbe479a37UL, 0xc0000000UL, 0x3fe5a5feUL, 0x9292c7eaUL,\n-    0x3e188650UL, 0x20000000UL, 0x3fe5993dUL, 0x33b4d380UL, 0x3e5d6d93UL,\n-    0x20000000UL, 0x3fe58c7fUL, 0x02fd16c7UL, 0x3e2fe961UL, 0xa0000000UL,\n-    0x3fe57fc4UL, 0x4a05edb6UL, 0xbe4d55b4UL, 0xa0000000UL, 0x3fe5730dUL,\n-    0x3d443abbUL, 0xbe5e6954UL, 0x00000000UL, 0x3fe5665aUL, 0x024acfeaUL,\n-    0x3e50e61bUL, 0x00000000UL, 0x3fe559aaUL, 0xcc9edd09UL, 0xbe325403UL,\n-    0x60000000UL, 0x3fe54cfdUL, 0x1fe26950UL, 0x3e5d500eUL, 0x60000000UL,\n-    0x3fe54054UL, 0x6c5ae164UL, 0xbe4a79b4UL, 0xc0000000UL, 0x3fe533aeUL,\n-    0x154b0287UL, 0xbe401571UL, 0xa0000000UL, 0x3fe5270cUL, 0x0673f401UL,\n-    0xbe56e56bUL, 0xe0000000UL, 0x3fe51a6dUL, 0x751b639cUL, 0x3e235269UL,\n-    0xa0000000UL, 0x3fe50dd2UL, 0x7c7b2bedUL, 0x3ddec887UL, 0xc0000000UL,\n-    0x3fe5013aUL, 0xafab4e17UL, 0x3e5e7575UL, 0x60000000UL, 0x3fe4f4a6UL,\n-    0x2e308668UL, 0x3e59aed6UL, 0x80000000UL, 0x3fe4e815UL, 0xf33e2a76UL,\n-    0xbe51f184UL, 0xe0000000UL, 0x3fe4db87UL, 0x839f3e3eUL, 0x3e57db01UL,\n-    0xc0000000UL, 0x3fe4cefdUL, 0xa9eda7bbUL, 0x3e535e0fUL, 0x00000000UL,\n-    0x3fe4c277UL, 0x2a8f66a5UL, 0x3e5ce451UL, 0xc0000000UL, 0x3fe4b5f3UL,\n-    0x05192456UL, 0xbe4e8518UL, 0xc0000000UL, 0x3fe4a973UL, 0x4aa7cd1dUL,\n-    0x3e46784aUL, 0x40000000UL, 0x3fe49cf7UL, 0x8e23025eUL, 0xbe5749f2UL,\n-    0x00000000UL, 0x3fe4907eUL, 0x18d30215UL, 0x3e360f39UL, 0x20000000UL,\n-    0x3fe48408UL, 0x63dcf2f3UL, 0x3e5e00feUL, 0xc0000000UL, 0x3fe47795UL,\n-    0x46182d09UL, 0xbe5173d9UL, 0xa0000000UL, 0x3fe46b26UL, 0x8f0e62aaUL,\n-    0xbe48f281UL, 0xe0000000UL, 0x3fe45ebaUL, 0x5775c40cUL, 0xbe56aad4UL,\n-    0x60000000UL, 0x3fe45252UL, 0x0fe25f69UL, 0x3e48bd71UL, 0x40000000UL,\n-    0x3fe445edUL, 0xe9989ec5UL, 0x3e590d97UL, 0x80000000UL, 0x3fe4398bUL,\n-    0xb3d9ffe3UL, 0x3e479dbcUL, 0x20000000UL, 0x3fe42d2dUL, 0x388e4d2eUL,\n-    0xbe5eed80UL, 0xe0000000UL, 0x3fe420d1UL, 0x6f797c18UL, 0x3e554b4cUL,\n-    0x20000000UL, 0x3fe4147aUL, 0x31048bb4UL, 0xbe5b1112UL, 0x80000000UL,\n-    0x3fe40825UL, 0x2efba4f9UL, 0x3e48ebc7UL, 0x40000000UL, 0x3fe3fbd4UL,\n-    0x50201119UL, 0x3e40b701UL, 0x40000000UL, 0x3fe3ef86UL, 0x0a4db32cUL,\n-    0x3e551de8UL, 0xa0000000UL, 0x3fe3e33bUL, 0x0c9c148bUL, 0xbe50c1f6UL,\n-    0x20000000UL, 0x3fe3d6f4UL, 0xc9129447UL, 0x3e533fa0UL, 0x00000000UL,\n-    0x3fe3cab0UL, 0xaae5b5a0UL, 0xbe22b68eUL, 0x20000000UL, 0x3fe3be6fUL,\n-    0x02305e8aUL, 0xbe54fc08UL, 0x60000000UL, 0x3fe3b231UL, 0x7f908258UL,\n-    0x3e57dc05UL, 0x00000000UL, 0x3fe3a5f7UL, 0x1a09af78UL, 0x3e08038bUL,\n-    0xe0000000UL, 0x3fe399bfUL, 0x490643c1UL, 0xbe5dbe42UL, 0xe0000000UL,\n-    0x3fe38d8bUL, 0x5e8ad724UL, 0xbe3c2b72UL, 0x20000000UL, 0x3fe3815bUL,\n-    0xc67196b6UL, 0x3e1713cfUL, 0xa0000000UL, 0x3fe3752dUL, 0x6182e429UL,\n-    0xbe3ec14cUL, 0x40000000UL, 0x3fe36903UL, 0xab6eb1aeUL, 0x3e5a2cc5UL,\n-    0x40000000UL, 0x3fe35cdcUL, 0xfe5dc064UL, 0xbe5c5878UL, 0x40000000UL,\n-    0x3fe350b8UL, 0x0ba6b9e4UL, 0x3e51619bUL, 0x80000000UL, 0x3fe34497UL,\n-    0x857761aaUL, 0x3e5fff53UL, 0x00000000UL, 0x3fe3387aUL, 0xf872d68cUL,\n-    0x3e484f4dUL, 0xa0000000UL, 0x3fe32c5fUL, 0x087e97c2UL, 0x3e52842eUL,\n-    0x80000000UL, 0x3fe32048UL, 0x73d6d0c0UL, 0xbe503edfUL, 0x80000000UL,\n-    0x3fe31434UL, 0x0c1456a1UL, 0xbe5f72adUL, 0xa0000000UL, 0x3fe30823UL,\n-    0x83a1a4d5UL, 0xbe5e65ccUL, 0xe0000000UL, 0x3fe2fc15UL, 0x855a7390UL,\n-    0xbe506438UL, 0x40000000UL, 0x3fe2f00bUL, 0xa2898287UL, 0x3e3d22a2UL,\n-    0xe0000000UL, 0x3fe2e403UL, 0x8b56f66fUL, 0xbe5aa5fdUL, 0x80000000UL,\n-    0x3fe2d7ffUL, 0x52db119aUL, 0x3e3a2e3dUL, 0x60000000UL, 0x3fe2cbfeUL,\n-    0xe2ddd4c0UL, 0xbe586469UL, 0x40000000UL, 0x3fe2c000UL, 0x6b01bf10UL,\n-    0x3e352b9dUL, 0x40000000UL, 0x3fe2b405UL, 0xb07a1cdfUL, 0x3e5c5cdaUL,\n-    0x80000000UL, 0x3fe2a80dUL, 0xc7b5f868UL, 0xbe5668b3UL, 0xc0000000UL,\n-    0x3fe29c18UL, 0x185edf62UL, 0xbe563d66UL, 0x00000000UL, 0x3fe29027UL,\n-    0xf729e1ccUL, 0x3e59a9a0UL, 0x80000000UL, 0x3fe28438UL, 0x6433c727UL,\n-    0xbe43cc89UL, 0x00000000UL, 0x3fe2784dUL, 0x41782631UL, 0xbe30750cUL,\n-    0xa0000000UL, 0x3fe26c64UL, 0x914911b7UL, 0xbe58290eUL, 0x40000000UL,\n-    0x3fe2607fUL, 0x3dcc73e1UL, 0xbe4269cdUL, 0x00000000UL, 0x3fe2549dUL,\n-    0x2751bf70UL, 0xbe5a6998UL, 0xc0000000UL, 0x3fe248bdUL, 0x4248b9fbUL,\n-    0xbe4ddb00UL, 0x80000000UL, 0x3fe23ce1UL, 0xf35cf82fUL, 0x3e561b71UL,\n-    0x60000000UL, 0x3fe23108UL, 0x8e481a2dUL, 0x3e518fb9UL, 0x60000000UL,\n-    0x3fe22532UL, 0x5ab96edcUL, 0xbe5fafc5UL, 0x40000000UL, 0x3fe2195fUL,\n-    0x80943911UL, 0xbe07f819UL, 0x40000000UL, 0x3fe20d8fUL, 0x386f2d6cUL,\n-    0xbe54ba8bUL, 0x40000000UL, 0x3fe201c2UL, 0xf29664acUL, 0xbe5eb815UL,\n-    0x20000000UL, 0x3fe1f5f8UL, 0x64f03390UL, 0x3e5e320cUL, 0x20000000UL,\n-    0x3fe1ea31UL, 0x747ff696UL, 0x3e5ef0a5UL, 0x40000000UL, 0x3fe1de6dUL,\n-    0x3e9ceb51UL, 0xbe5f8d27UL, 0x20000000UL, 0x3fe1d2acUL, 0x4ae0b55eUL,\n-    0x3e5faa21UL, 0x20000000UL, 0x3fe1c6eeUL, 0x28569a5eUL, 0x3e598a4fUL,\n-    0x20000000UL, 0x3fe1bb33UL, 0x54b33e07UL, 0x3e46130aUL, 0x20000000UL,\n-    0x3fe1af7bUL, 0x024f1078UL, 0xbe4dbf93UL, 0x00000000UL, 0x3fe1a3c6UL,\n-    0xb0783bfaUL, 0x3e419248UL, 0xe0000000UL, 0x3fe19813UL, 0x2f02b836UL,\n-    0x3e4e02b7UL, 0xc0000000UL, 0x3fe18c64UL, 0x28dec9d4UL, 0x3e09064fUL,\n-    0x80000000UL, 0x3fe180b8UL, 0x45cbf406UL, 0x3e5b1f46UL, 0x40000000UL,\n-    0x3fe1750fUL, 0x03d9964cUL, 0x3e5b0a79UL, 0x00000000UL, 0x3fe16969UL,\n-    0x8b5b882bUL, 0xbe238086UL, 0xa0000000UL, 0x3fe15dc5UL, 0x73bad6f8UL,\n-    0xbdf1fca4UL, 0x20000000UL, 0x3fe15225UL, 0x5385769cUL, 0x3e5e8d76UL,\n-    0xa0000000UL, 0x3fe14687UL, 0x1676dc6bUL, 0x3e571d08UL, 0x20000000UL,\n-    0x3fe13aedUL, 0xa8c41c7fUL, 0xbe598a25UL, 0x60000000UL, 0x3fe12f55UL,\n-    0xc4e1aaf0UL, 0x3e435277UL, 0xa0000000UL, 0x3fe123c0UL, 0x403638e1UL,\n-    0xbe21aa7cUL, 0xc0000000UL, 0x3fe1182eUL, 0x557a092bUL, 0xbdd0116bUL,\n-    0xc0000000UL, 0x3fe10c9fUL, 0x7d779f66UL, 0x3e4a61baUL, 0xc0000000UL,\n-    0x3fe10113UL, 0x2b09c645UL, 0xbe5d586eUL, 0x20000000UL, 0x3fe0ea04UL,\n-    0xea2cad46UL, 0x3e5aa97cUL, 0x20000000UL, 0x3fe0d300UL, 0x23190e54UL,\n-    0x3e50f1a7UL, 0xa0000000UL, 0x3fe0bc07UL, 0x1379a5a6UL, 0xbe51619dUL,\n-    0x60000000UL, 0x3fe0a51aUL, 0x926a3d4aUL, 0x3e5cf019UL, 0xa0000000UL,\n-    0x3fe08e38UL, 0xa8c24358UL, 0x3e35241eUL, 0x20000000UL, 0x3fe07762UL,\n-    0x24317e7aUL, 0x3e512cfaUL, 0x00000000UL, 0x3fe06097UL, 0xfd9cf274UL,\n-    0xbe55bef3UL, 0x00000000UL, 0x3fe049d7UL, 0x3689b49dUL, 0xbe36d26dUL,\n-    0x40000000UL, 0x3fe03322UL, 0xf72ef6c4UL, 0xbe54cd08UL, 0xa0000000UL,\n-    0x3fe01c78UL, 0x23702d2dUL, 0xbe5900bfUL, 0x00000000UL, 0x3fe005daUL,\n-    0x3f59c14cUL, 0x3e57d80bUL, 0x40000000UL, 0x3fdfde8dUL, 0xad67766dUL,\n-    0xbe57fad4UL, 0x40000000UL, 0x3fdfb17cUL, 0x644f4ae7UL, 0x3e1ee43bUL,\n-    0x40000000UL, 0x3fdf8481UL, 0x903234d2UL, 0x3e501a86UL, 0x40000000UL,\n-    0x3fdf579cUL, 0xafe9e509UL, 0xbe267c3eUL, 0x00000000UL, 0x3fdf2acdUL,\n-    0xb7dfda0bUL, 0xbe48149bUL, 0x40000000UL, 0x3fdefe13UL, 0x3b94305eUL,\n-    0x3e5f4ea7UL, 0x80000000UL, 0x3fded16fUL, 0x5d95da61UL, 0xbe55c198UL,\n-    0x00000000UL, 0x3fdea4e1UL, 0x406960c9UL, 0xbdd99a19UL, 0x00000000UL,\n-    0x3fde7868UL, 0xd22f3539UL, 0x3e470c78UL, 0x80000000UL, 0x3fde4c04UL,\n-    0x83eec535UL, 0xbe3e1232UL, 0x40000000UL, 0x3fde1fb6UL, 0x3dfbffcbUL,\n-    0xbe4b7d71UL, 0x40000000UL, 0x3fddf37dUL, 0x7e1be4e0UL, 0xbe5b8f8fUL,\n-    0x40000000UL, 0x3fddc759UL, 0x46dae887UL, 0xbe350458UL, 0x80000000UL,\n-    0x3fdd9b4aUL, 0xed6ecc49UL, 0xbe5f0045UL, 0x80000000UL, 0x3fdd6f50UL,\n-    0x2e9e883cUL, 0x3e2915daUL, 0x80000000UL, 0x3fdd436bUL, 0xf0bccb32UL,\n-    0x3e4a68c9UL, 0x80000000UL, 0x3fdd179bUL, 0x9bbfc779UL, 0xbe54a26aUL,\n-    0x00000000UL, 0x3fdcebe0UL, 0x7cea33abUL, 0x3e43c6b7UL, 0x40000000UL,\n-    0x3fdcc039UL, 0xe740fd06UL, 0x3e5526c2UL, 0x40000000UL, 0x3fdc94a7UL,\n-    0x9eadeb1aUL, 0xbe396d8dUL, 0xc0000000UL, 0x3fdc6929UL, 0xf0a8f95aUL,\n-    0xbe5c0ab2UL, 0x80000000UL, 0x3fdc3dc0UL, 0x6ee2693bUL, 0x3e0992e6UL,\n-    0xc0000000UL, 0x3fdc126bUL, 0x5ac6b581UL, 0xbe2834b6UL, 0x40000000UL,\n-    0x3fdbe72bUL, 0x8cc226ffUL, 0x3e3596a6UL, 0x00000000UL, 0x3fdbbbffUL,\n-    0xf92a74bbUL, 0x3e3c5813UL, 0x00000000UL, 0x3fdb90e7UL, 0x479664c0UL,\n-    0xbe50d644UL, 0x00000000UL, 0x3fdb65e3UL, 0x5004975bUL, 0xbe55258fUL,\n-    0x00000000UL, 0x3fdb3af3UL, 0xe4b23194UL, 0xbe588407UL, 0xc0000000UL,\n-    0x3fdb1016UL, 0xe65d4d0aUL, 0x3e527c26UL, 0x80000000UL, 0x3fdae54eUL,\n-    0x814fddd6UL, 0x3e5962a2UL, 0x40000000UL, 0x3fdaba9aUL, 0xe19d0913UL,\n-    0xbe562f4eUL, 0x80000000UL, 0x3fda8ff9UL, 0x43cfd006UL, 0xbe4cfdebUL,\n-    0x40000000UL, 0x3fda656cUL, 0x686f0a4eUL, 0x3e5e47a8UL, 0xc0000000UL,\n-    0x3fda3af2UL, 0x7200d410UL, 0x3e5e1199UL, 0xc0000000UL, 0x3fda108cUL,\n-    0xabd2266eUL, 0x3e5ee4d1UL, 0x40000000UL, 0x3fd9e63aUL, 0x396f8f2cUL,\n-    0x3e4dbffbUL, 0x00000000UL, 0x3fd9bbfbUL, 0xe32b25ddUL, 0x3e5c3a54UL,\n-    0x40000000UL, 0x3fd991cfUL, 0x431e4035UL, 0xbe457925UL, 0x80000000UL,\n-    0x3fd967b6UL, 0x7bed3dd3UL, 0x3e40c61dUL, 0x00000000UL, 0x3fd93db1UL,\n-    0xd7449365UL, 0x3e306419UL, 0x80000000UL, 0x3fd913beUL, 0x1746e791UL,\n-    0x3e56fcfcUL, 0x40000000UL, 0x3fd8e9dfUL, 0xf3a9028bUL, 0xbe5041b9UL,\n-    0xc0000000UL, 0x3fd8c012UL, 0x56840c50UL, 0xbe26e20aUL, 0x40000000UL,\n-    0x3fd89659UL, 0x19763102UL, 0xbe51f466UL, 0x80000000UL, 0x3fd86cb2UL,\n-    0x7032de7cUL, 0xbe4d298aUL, 0x80000000UL, 0x3fd8431eUL, 0xdeb39fabUL,\n-    0xbe4361ebUL, 0x40000000UL, 0x3fd8199dUL, 0x5d01cbe0UL, 0xbe5425b3UL,\n-    0x80000000UL, 0x3fd7f02eUL, 0x3ce99aa9UL, 0x3e146fa8UL, 0x80000000UL,\n-    0x3fd7c6d2UL, 0xd1a262b9UL, 0xbe5a1a69UL, 0xc0000000UL, 0x3fd79d88UL,\n-    0x8606c236UL, 0x3e423a08UL, 0x80000000UL, 0x3fd77451UL, 0x8fd1e1b7UL,\n-    0x3e5a6a63UL, 0xc0000000UL, 0x3fd74b2cUL, 0xe491456aUL, 0x3e42c1caUL,\n-    0x40000000UL, 0x3fd7221aUL, 0x4499a6d7UL, 0x3e36a69aUL, 0x00000000UL,\n-    0x3fd6f91aUL, 0x5237df94UL, 0xbe0f8f02UL, 0x00000000UL, 0x3fd6d02cUL,\n-    0xb6482c6eUL, 0xbe5abcf7UL, 0x00000000UL, 0x3fd6a750UL, 0x1919fd61UL,\n-    0xbe57ade2UL, 0x00000000UL, 0x3fd67e86UL, 0xaa7a994dUL, 0xbe3f3fbdUL,\n-    0x00000000UL, 0x3fd655ceUL, 0x67db014cUL, 0x3e33c550UL, 0x00000000UL,\n-    0x3fd62d28UL, 0xa82856b7UL, 0xbe1409d1UL, 0xc0000000UL, 0x3fd60493UL,\n-    0x1e6a300dUL, 0x3e55d899UL, 0x80000000UL, 0x3fd5dc11UL, 0x1222bd5cUL,\n-    0xbe35bfc0UL, 0xc0000000UL, 0x3fd5b3a0UL, 0x6e8dc2d3UL, 0x3e5d4d79UL,\n-    0x00000000UL, 0x3fd58b42UL, 0xe0e4ace6UL, 0xbe517303UL, 0x80000000UL,\n-    0x3fd562f4UL, 0xb306e0a8UL, 0x3e5edf0fUL, 0xc0000000UL, 0x3fd53ab8UL,\n-    0x6574bc54UL, 0x3e5ee859UL, 0x80000000UL, 0x3fd5128eUL, 0xea902207UL,\n-    0x3e5f6188UL, 0xc0000000UL, 0x3fd4ea75UL, 0x9f911d79UL, 0x3e511735UL,\n-    0x80000000UL, 0x3fd4c26eUL, 0xf9c77397UL, 0xbe5b1643UL, 0x40000000UL,\n-    0x3fd49a78UL, 0x15fc9258UL, 0x3e479a37UL, 0x80000000UL, 0x3fd47293UL,\n-    0xd5a04dd9UL, 0xbe426e56UL, 0xc0000000UL, 0x3fd44abfUL, 0xe04042f5UL,\n-    0x3e56f7c6UL, 0x40000000UL, 0x3fd422fdUL, 0x1d8bf2c8UL, 0x3e5d8810UL,\n-    0x00000000UL, 0x3fd3fb4cUL, 0x88a8ddeeUL, 0xbe311454UL, 0xc0000000UL,\n-    0x3fd3d3abUL, 0x3e3b5e47UL, 0xbe5d1b72UL, 0x40000000UL, 0x3fd3ac1cUL,\n-    0xc2ab5d59UL, 0x3e31b02bUL, 0xc0000000UL, 0x3fd3849dUL, 0xd4e34b9eUL,\n-    0x3e51cb2fUL, 0x40000000UL, 0x3fd35d30UL, 0x177204fbUL, 0xbe2b8cd7UL,\n-    0x80000000UL, 0x3fd335d3UL, 0xfcd38c82UL, 0xbe4356e1UL, 0x80000000UL,\n-    0x3fd30e87UL, 0x64f54accUL, 0xbe4e6224UL, 0x00000000UL, 0x3fd2e74cUL,\n-    0xaa7975d9UL, 0x3e5dc0feUL, 0x80000000UL, 0x3fd2c021UL, 0x516dab3fUL,\n-    0xbe50ffa3UL, 0x40000000UL, 0x3fd29907UL, 0x2bfb7313UL, 0x3e5674a2UL,\n-    0xc0000000UL, 0x3fd271fdUL, 0x0549fc99UL, 0x3e385d29UL, 0xc0000000UL,\n-    0x3fd24b04UL, 0x55b63073UL, 0xbe500c6dUL, 0x00000000UL, 0x3fd2241cUL,\n-    0x3f91953aUL, 0x3e389977UL, 0xc0000000UL, 0x3fd1fd43UL, 0xa1543f71UL,\n-    0xbe3487abUL, 0xc0000000UL, 0x3fd1d67bUL, 0x4ec8867cUL, 0x3df6a2dcUL,\n-    0x00000000UL, 0x3fd1afc4UL, 0x4328e3bbUL, 0x3e41d9c0UL, 0x80000000UL,\n-    0x3fd1891cUL, 0x2e1cda84UL, 0x3e3bdd87UL, 0x40000000UL, 0x3fd16285UL,\n-    0x4b5331aeUL, 0xbe53128eUL, 0x00000000UL, 0x3fd13bfeUL, 0xb9aec164UL,\n-    0xbe52ac98UL, 0xc0000000UL, 0x3fd11586UL, 0xd91e1316UL, 0xbe350630UL,\n-    0x80000000UL, 0x3fd0ef1fUL, 0x7cacc12cUL, 0x3e3f5219UL, 0x40000000UL,\n-    0x3fd0c8c8UL, 0xbce277b7UL, 0x3e3d30c0UL, 0x00000000UL, 0x3fd0a281UL,\n-    0x2a63447dUL, 0xbe541377UL, 0x80000000UL, 0x3fd07c49UL, 0xfac483b5UL,\n-    0xbe5772ecUL, 0xc0000000UL, 0x3fd05621UL, 0x36b8a570UL, 0xbe4fd4bdUL,\n-    0xc0000000UL, 0x3fd03009UL, 0xbae505f7UL, 0xbe450388UL, 0x80000000UL,\n-    0x3fd00a01UL, 0x3e35aeadUL, 0xbe5430fcUL, 0x80000000UL, 0x3fcfc811UL,\n-    0x707475acUL, 0x3e38806eUL, 0x80000000UL, 0x3fcf7c3fUL, 0xc91817fcUL,\n-    0xbe40cceaUL, 0x80000000UL, 0x3fcf308cUL, 0xae05d5e9UL, 0xbe4919b8UL,\n-    0x80000000UL, 0x3fcee4f8UL, 0xae6cc9e6UL, 0xbe530b94UL, 0x00000000UL,\n-    0x3fce9983UL, 0x1efe3e8eUL, 0x3e57747eUL, 0x00000000UL, 0x3fce4e2dUL,\n-    0xda78d9bfUL, 0xbe59a608UL, 0x00000000UL, 0x3fce02f5UL, 0x8abe2c2eUL,\n-    0x3e4a35adUL, 0x00000000UL, 0x3fcdb7dcUL, 0x1495450dUL, 0xbe0872ccUL,\n-    0x80000000UL, 0x3fcd6ce1UL, 0x86ee0ba0UL, 0xbe4f59a0UL, 0x00000000UL,\n-    0x3fcd2205UL, 0xe81ca888UL, 0x3e5402c3UL, 0x00000000UL, 0x3fccd747UL,\n-    0x3b4424b9UL, 0x3e5dfdc3UL, 0x80000000UL, 0x3fcc8ca7UL, 0xd305b56cUL,\n-    0x3e202da6UL, 0x00000000UL, 0x3fcc4226UL, 0x399a6910UL, 0xbe482a1cUL,\n-    0x80000000UL, 0x3fcbf7c2UL, 0x747f7938UL, 0xbe587372UL, 0x80000000UL,\n-    0x3fcbad7cUL, 0x6fc246a0UL, 0x3e50d83dUL, 0x00000000UL, 0x3fcb6355UL,\n-    0xee9e9be5UL, 0xbe5c35bdUL, 0x80000000UL, 0x3fcb194aUL, 0x8416c0bcUL,\n-    0x3e546d4fUL, 0x00000000UL, 0x3fcacf5eUL, 0x49f7f08fUL, 0x3e56da76UL,\n-    0x00000000UL, 0x3fca858fUL, 0x5dc30de2UL, 0x3e5f390cUL, 0x00000000UL,\n-    0x3fca3bdeUL, 0x950583b6UL, 0xbe5e4169UL, 0x80000000UL, 0x3fc9f249UL,\n-    0x33631553UL, 0x3e52aeb1UL, 0x00000000UL, 0x3fc9a8d3UL, 0xde8795a6UL,\n-    0xbe59a504UL, 0x00000000UL, 0x3fc95f79UL, 0x076bf41eUL, 0x3e5122feUL,\n-    0x80000000UL, 0x3fc9163cUL, 0x2914c8e7UL, 0x3e3dd064UL, 0x00000000UL,\n-    0x3fc8cd1dUL, 0x3a30eca3UL, 0xbe21b4aaUL, 0x80000000UL, 0x3fc8841aUL,\n-    0xb2a96650UL, 0xbe575444UL, 0x80000000UL, 0x3fc83b34UL, 0x2376c0cbUL,\n-    0xbe2a74c7UL, 0x80000000UL, 0x3fc7f26bUL, 0xd8a0b653UL, 0xbe5181b6UL,\n-    0x00000000UL, 0x3fc7a9bfUL, 0x32257882UL, 0xbe4a78b4UL, 0x00000000UL,\n-    0x3fc7612fUL, 0x1eee8bd9UL, 0xbe1bfe9dUL, 0x80000000UL, 0x3fc718bbUL,\n-    0x0c603cc4UL, 0x3e36fdc9UL, 0x80000000UL, 0x3fc6d064UL, 0x3728b8cfUL,\n-    0xbe1e542eUL, 0x80000000UL, 0x3fc68829UL, 0xc79a4067UL, 0x3e5c380fUL,\n-    0x00000000UL, 0x3fc6400bUL, 0xf69eac69UL, 0x3e550a84UL, 0x80000000UL,\n-    0x3fc5f808UL, 0xb7a780a4UL, 0x3e5d9224UL, 0x80000000UL, 0x3fc5b022UL,\n-    0xad9dfb1eUL, 0xbe55242fUL, 0x00000000UL, 0x3fc56858UL, 0x659b18beUL,\n-    0xbe4bfda3UL, 0x80000000UL, 0x3fc520a9UL, 0x66ee3631UL, 0xbe57d769UL,\n-    0x80000000UL, 0x3fc4d916UL, 0x1ec62819UL, 0x3e2427f7UL, 0x80000000UL,\n-    0x3fc4919fUL, 0xdec25369UL, 0xbe435431UL, 0x00000000UL, 0x3fc44a44UL,\n-    0xa8acfc4bUL, 0xbe3c62e8UL, 0x00000000UL, 0x3fc40304UL, 0xcf1d3eabUL,\n-    0xbdfba29fUL, 0x80000000UL, 0x3fc3bbdfUL, 0x79aba3eaUL, 0xbdf1b7c8UL,\n-    0x80000000UL, 0x3fc374d6UL, 0xb8d186daUL, 0xbe5130cfUL, 0x80000000UL,\n-    0x3fc32de8UL, 0x9d74f152UL, 0x3e2285b6UL, 0x00000000UL, 0x3fc2e716UL,\n-    0x50ae7ca9UL, 0xbe503920UL, 0x80000000UL, 0x3fc2a05eUL, 0x6caed92eUL,\n-    0xbe533924UL, 0x00000000UL, 0x3fc259c2UL, 0x9cb5034eUL, 0xbe510e31UL,\n-    0x80000000UL, 0x3fc21340UL, 0x12c4d378UL, 0xbe540b43UL, 0x80000000UL,\n-    0x3fc1ccd9UL, 0xcc418706UL, 0x3e59887aUL, 0x00000000UL, 0x3fc1868eUL,\n-    0x921f4106UL, 0xbe528e67UL, 0x80000000UL, 0x3fc1405cUL, 0x3969441eUL,\n-    0x3e5d8051UL, 0x00000000UL, 0x3fc0fa46UL, 0xd941ef5bUL, 0x3e5f9079UL,\n-    0x80000000UL, 0x3fc0b44aUL, 0x5a3e81b2UL, 0xbe567691UL, 0x00000000UL,\n-    0x3fc06e69UL, 0x9d66afe7UL, 0xbe4d43fbUL, 0x00000000UL, 0x3fc028a2UL,\n-    0x0a92a162UL, 0xbe52f394UL, 0x00000000UL, 0x3fbfc5eaUL, 0x209897e5UL,\n-    0x3e529e37UL, 0x00000000UL, 0x3fbf3ac5UL, 0x8458bd7bUL, 0x3e582831UL,\n-    0x00000000UL, 0x3fbeafd5UL, 0xb8d8b4b8UL, 0xbe486b4aUL, 0x00000000UL,\n-    0x3fbe2518UL, 0xe0a3b7b6UL, 0x3e5bafd2UL, 0x00000000UL, 0x3fbd9a90UL,\n-    0x2bf2710eUL, 0x3e383b2bUL, 0x00000000UL, 0x3fbd103cUL, 0x73eb6ab7UL,\n-    0xbe56d78dUL, 0x00000000UL, 0x3fbc861bUL, 0x32ceaff5UL, 0xbe32dc5aUL,\n-    0x00000000UL, 0x3fbbfc2eUL, 0xbee04cb7UL, 0xbe4a71a4UL, 0x00000000UL,\n-    0x3fbb7274UL, 0x35ae9577UL, 0x3e38142fUL, 0x00000000UL, 0x3fbae8eeUL,\n-    0xcbaddab4UL, 0xbe5490f0UL, 0x00000000UL, 0x3fba5f9aUL, 0x95ce1114UL,\n-    0x3e597c71UL, 0x00000000UL, 0x3fb9d67aUL, 0x6d7c0f78UL, 0x3e3abc2dUL,\n-    0x00000000UL, 0x3fb94d8dUL, 0x2841a782UL, 0xbe566cbcUL, 0x00000000UL,\n-    0x3fb8c4d2UL, 0x6ed429c6UL, 0xbe3cfff9UL, 0x00000000UL, 0x3fb83c4aUL,\n-    0xe4a49fbbUL, 0xbe552964UL, 0x00000000UL, 0x3fb7b3f4UL, 0x2193d81eUL,\n-    0xbe42fa72UL, 0x00000000UL, 0x3fb72bd0UL, 0xdd70c122UL, 0x3e527a8cUL,\n-    0x00000000UL, 0x3fb6a3dfUL, 0x03108a54UL, 0xbe450393UL, 0x00000000UL,\n-    0x3fb61c1fUL, 0x30ff7954UL, 0x3e565840UL, 0x00000000UL, 0x3fb59492UL,\n-    0xdedd460cUL, 0xbe5422b5UL, 0x00000000UL, 0x3fb50d36UL, 0x950f9f45UL,\n-    0xbe5313f6UL, 0x00000000UL, 0x3fb4860bUL, 0x582cdcb1UL, 0x3e506d39UL,\n-    0x00000000UL, 0x3fb3ff12UL, 0x7216d3a6UL, 0x3e4aa719UL, 0x00000000UL,\n-    0x3fb3784aUL, 0x57a423fdUL, 0x3e5a9b9fUL, 0x00000000UL, 0x3fb2f1b4UL,\n-    0x7a138b41UL, 0xbe50b418UL, 0x00000000UL, 0x3fb26b4eUL, 0x2fbfd7eaUL,\n-    0x3e23a53eUL, 0x00000000UL, 0x3fb1e519UL, 0x18913ccbUL, 0x3e465fc1UL,\n-    0x00000000UL, 0x3fb15f15UL, 0x7ea24e21UL, 0x3e042843UL, 0x00000000UL,\n-    0x3fb0d941UL, 0x7c6d9c77UL, 0x3e59f61eUL, 0x00000000UL, 0x3fb0539eUL,\n-    0x114efd44UL, 0x3e4ccab7UL, 0x00000000UL, 0x3faf9c56UL, 0x1777f657UL,\n-    0x3e552f65UL, 0x00000000UL, 0x3fae91d2UL, 0xc317b86aUL, 0xbe5a61e0UL,\n-    0x00000000UL, 0x3fad87acUL, 0xb7664efbUL, 0xbe41f64eUL, 0x00000000UL,\n-    0x3fac7de6UL, 0x5d3d03a9UL, 0x3e0807a0UL, 0x00000000UL, 0x3fab7480UL,\n-    0x743c38ebUL, 0xbe3726e1UL, 0x00000000UL, 0x3faa6b78UL, 0x06a253f1UL,\n-    0x3e5ad636UL, 0x00000000UL, 0x3fa962d0UL, 0xa35f541bUL, 0x3e5a187aUL,\n-    0x00000000UL, 0x3fa85a88UL, 0x4b86e446UL, 0xbe508150UL, 0x00000000UL,\n-    0x3fa7529cUL, 0x2589cacfUL, 0x3e52938aUL, 0x00000000UL, 0x3fa64b10UL,\n-    0xaf6b11f2UL, 0xbe3454cdUL, 0x00000000UL, 0x3fa543e2UL, 0x97506fefUL,\n-    0xbe5fdec5UL, 0x00000000UL, 0x3fa43d10UL, 0xe75f7dd9UL, 0xbe388dd3UL,\n-    0x00000000UL, 0x3fa3369cUL, 0xa4139632UL, 0xbdea5177UL, 0x00000000UL,\n-    0x3fa23086UL, 0x352d6f1eUL, 0xbe565ad6UL, 0x00000000UL, 0x3fa12accUL,\n-    0x77449eb7UL, 0xbe50d5c7UL, 0x00000000UL, 0x3fa0256eUL, 0x7478da78UL,\n-    0x3e404724UL, 0x00000000UL, 0x3f9e40dcUL, 0xf59cef7fUL, 0xbe539d0aUL,\n-    0x00000000UL, 0x3f9c3790UL, 0x1511d43cUL, 0x3e53c2c8UL, 0x00000000UL,\n-    0x3f9a2f00UL, 0x9b8bff3cUL, 0xbe43b3e1UL, 0x00000000UL, 0x3f982724UL,\n-    0xad1e22a5UL, 0x3e46f0bdUL, 0x00000000UL, 0x3f962000UL, 0x130d9356UL,\n-    0x3e475ba0UL, 0x00000000UL, 0x3f941994UL, 0x8f86f883UL, 0xbe513d0bUL,\n-    0x00000000UL, 0x3f9213dcUL, 0x914d0dc8UL, 0xbe534335UL, 0x00000000UL,\n-    0x3f900ed8UL, 0x2d73e5e7UL, 0xbe22ba75UL, 0x00000000UL, 0x3f8c1510UL,\n-    0xc5b7d70eUL, 0x3e599c5dUL, 0x00000000UL, 0x3f880de0UL, 0x8a27857eUL,\n-    0xbe3d28c8UL, 0x00000000UL, 0x3f840810UL, 0xda767328UL, 0x3e531b3dUL,\n-    0x00000000UL, 0x3f8003b0UL, 0x77bacaf3UL, 0xbe5f04e3UL, 0x00000000UL,\n-    0x3f780150UL, 0xdf4b0720UL, 0x3e5a8bffUL, 0x00000000UL, 0x3f6ffc40UL,\n-    0x34c48e71UL, 0xbe3fcd99UL, 0x00000000UL, 0x3f5ff6c0UL, 0x1ad218afUL,\n-    0xbe4c78a7UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x80000000UL,\n-    0x00000000UL, 0xfffff800UL, 0x00000000UL, 0xfffff800UL, 0x00000000UL,\n-    0x3ff72000UL, 0x161bb241UL, 0xbf5dabe1UL, 0x6dc96112UL, 0xbf836578UL,\n-    0xee241472UL, 0xbf9b0301UL, 0x9f95985aUL, 0xbfb528dbUL, 0xb3841d2aUL,\n-    0xbfd619b6UL, 0x518775e3UL, 0x3f9004f2UL, 0xac8349bbUL, 0x3fa76c9bUL,\n-    0x486ececcUL, 0x3fc4635eUL, 0x161bb241UL, 0xbf5dabe1UL, 0x9f95985aUL,\n-    0xbfb528dbUL, 0xf8b5787dUL, 0x3ef2531eUL, 0x486ececbUL, 0x3fc4635eUL,\n-    0x412055ccUL, 0xbdd61bb2UL, 0x00000000UL, 0xfffffff8UL, 0x00000000UL,\n-    0xffffffffUL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0x3b700000UL,\n-    0xfa5abcbfUL, 0x3ff00b1aUL, 0xa7609f71UL, 0xbc84f6b2UL, 0xa9fb3335UL,\n-    0x3ff0163dUL, 0x9ab8cdb7UL, 0x3c9b6129UL, 0x143b0281UL, 0x3ff02168UL,\n-    0x0fc54eb6UL, 0xbc82bf31UL, 0x3e778061UL, 0x3ff02c9aUL, 0x535b085dUL,\n-    0xbc719083UL, 0x2e11bbccUL, 0x3ff037d4UL, 0xeeade11aUL, 0x3c656811UL,\n-    0xe86e7f85UL, 0x3ff04315UL, 0x1977c96eUL, 0xbc90a31cUL, 0x72f654b1UL,\n-    0x3ff04e5fUL, 0x3aa0d08cUL, 0x3c84c379UL, 0xd3158574UL, 0x3ff059b0UL,\n-    0xa475b465UL, 0x3c8d73e2UL, 0x0e3c1f89UL, 0x3ff0650aUL, 0x5799c397UL,\n-    0xbc95cb7bUL, 0x29ddf6deUL, 0x3ff0706bUL, 0xe2b13c27UL, 0xbc8c91dfUL,\n-    0x2b72a836UL, 0x3ff07bd4UL, 0x54458700UL, 0x3c832334UL, 0x18759bc8UL,\n-    0x3ff08745UL, 0x4bb284ffUL, 0x3c6186beUL, 0xf66607e0UL, 0x3ff092bdUL,\n-    0x800a3fd1UL, 0xbc968063UL, 0xcac6f383UL, 0x3ff09e3eUL, 0x18316136UL,\n-    0x3c914878UL, 0x9b1f3919UL, 0x3ff0a9c7UL, 0x873d1d38UL, 0x3c85d16cUL,\n-    0x6cf9890fUL, 0x3ff0b558UL, 0x4adc610bUL, 0x3c98a62eUL, 0x45e46c85UL,\n-    0x3ff0c0f1UL, 0x06d21cefUL, 0x3c94f989UL, 0x2b7247f7UL, 0x3ff0cc92UL,\n-    0x16e24f71UL, 0x3c901edcUL, 0x23395decUL, 0x3ff0d83bUL, 0xe43f316aUL,\n-    0xbc9bc14dUL, 0x32d3d1a2UL, 0x3ff0e3ecUL, 0x27c57b52UL, 0x3c403a17UL,\n-    0x5fdfa9c5UL, 0x3ff0efa5UL, 0xbc54021bUL, 0xbc949db9UL, 0xaffed31bUL,\n-    0x3ff0fb66UL, 0xc44ebd7bUL, 0xbc6b9bedUL, 0x28d7233eUL, 0x3ff10730UL,\n-    0x1692fdd5UL, 0x3c8d46ebUL, 0xd0125b51UL, 0x3ff11301UL, 0x39449b3aUL,\n-    0xbc96c510UL, 0xab5e2ab6UL, 0x3ff11edbUL, 0xf703fb72UL, 0xbc9ca454UL,\n-    0xc06c31ccUL, 0x3ff12abdUL, 0xb36ca5c7UL, 0xbc51b514UL, 0x14f204abUL,\n-    0x3ff136a8UL, 0xba48dcf0UL, 0xbc67108fUL, 0xaea92de0UL, 0x3ff1429aUL,\n-    0x9af1369eUL, 0xbc932fbfUL, 0x934f312eUL, 0x3ff14e95UL, 0x39bf44abUL,\n-    0xbc8b91e8UL, 0xc8a58e51UL, 0x3ff15a98UL, 0xb9eeab0aUL, 0x3c82406aUL,\n-    0x5471c3c2UL, 0x3ff166a4UL, 0x82ea1a32UL, 0x3c58f23bUL, 0x3c7d517bUL,\n-    0x3ff172b8UL, 0xb9d78a76UL, 0xbc819041UL, 0x8695bbc0UL, 0x3ff17ed4UL,\n-    0xe2ac5a64UL, 0x3c709e3fUL, 0x388c8deaUL, 0x3ff18af9UL, 0xd1970f6cUL,\n-    0xbc911023UL, 0x58375d2fUL, 0x3ff19726UL, 0x85f17e08UL, 0x3c94aaddUL,\n-    0xeb6fcb75UL, 0x3ff1a35bUL, 0x7b4968e4UL, 0x3c8e5b4cUL, 0xf8138a1cUL,\n-    0x3ff1af99UL, 0xa4b69280UL, 0x3c97bf85UL, 0x84045cd4UL, 0x3ff1bbe0UL,\n-    0x352ef607UL, 0xbc995386UL, 0x95281c6bUL, 0x3ff1c82fUL, 0x8010f8c9UL,\n-    0x3c900977UL, 0x3168b9aaUL, 0x3ff1d487UL, 0x00a2643cUL, 0x3c9e016eUL,\n-    0x5eb44027UL, 0x3ff1e0e7UL, 0x088cb6deUL, 0xbc96fdd8UL, 0x22fcd91dUL,\n-    0x3ff1ed50UL, 0x027bb78cUL, 0xbc91df98UL, 0x8438ce4dUL, 0x3ff1f9c1UL,\n-    0xa097af5cUL, 0xbc9bf524UL, 0x88628cd6UL, 0x3ff2063bUL, 0x814a8495UL,\n-    0x3c8dc775UL, 0x3578a819UL, 0x3ff212beUL, 0x2cfcaac9UL, 0x3c93592dUL,\n-    0x917ddc96UL, 0x3ff21f49UL, 0x9494a5eeUL, 0x3c82a97eUL, 0xa27912d1UL,\n-    0x3ff22bddUL, 0x5577d69fUL, 0x3c8d34fbUL, 0x6e756238UL, 0x3ff2387aUL,\n-    0xb6c70573UL, 0x3c99b07eUL, 0xfb82140aUL, 0x3ff2451fUL, 0x911ca996UL,\n-    0x3c8acfccUL, 0x4fb2a63fUL, 0x3ff251ceUL, 0xbef4f4a4UL, 0x3c8ac155UL,\n-    0x711ece75UL, 0x3ff25e85UL, 0x4ac31b2cUL, 0x3c93e1a2UL, 0x65e27cddUL,\n-    0x3ff26b45UL, 0x9940e9d9UL, 0x3c82bd33UL, 0x341ddf29UL, 0x3ff2780eUL,\n-    0x05f9e76cUL, 0x3c9e067cUL, 0xe1f56381UL, 0x3ff284dfUL, 0x8c3f0d7eUL,\n-    0xbc9a4c3aUL, 0x7591bb70UL, 0x3ff291baUL, 0x28401cbdUL, 0xbc82cc72UL,\n-    0xf51fdee1UL, 0x3ff29e9dUL, 0xafad1255UL, 0x3c8612e8UL, 0x66d10f13UL,\n-    0x3ff2ab8aUL, 0x191690a7UL, 0xbc995743UL, 0xd0dad990UL, 0x3ff2b87fUL,\n-    0xd6381aa4UL, 0xbc410adcUL, 0x39771b2fUL, 0x3ff2c57eUL, 0xa6eb5124UL,\n-    0xbc950145UL, 0xa6e4030bUL, 0x3ff2d285UL, 0x54db41d5UL, 0x3c900247UL,\n-    0x1f641589UL, 0x3ff2df96UL, 0xfbbce198UL, 0x3c9d16cfUL, 0xa93e2f56UL,\n-    0x3ff2ecafUL, 0x45d52383UL, 0x3c71ca0fUL, 0x4abd886bUL, 0x3ff2f9d2UL,\n-    0x532bda93UL, 0xbc653c55UL, 0x0a31b715UL, 0x3ff306feUL, 0xd23182e4UL,\n-    0x3c86f46aUL, 0xedeeb2fdUL, 0x3ff31432UL, 0xf3f3fcd1UL, 0x3c8959a3UL,\n-    0xfc4cd831UL, 0x3ff32170UL, 0x8e18047cUL, 0x3c8a9ce7UL, 0x3ba8ea32UL,\n-    0x3ff32eb8UL, 0x3cb4f318UL, 0xbc9c45e8UL, 0xb26416ffUL, 0x3ff33c08UL,\n-    0x843659a6UL, 0x3c932721UL, 0x66e3fa2dUL, 0x3ff34962UL, 0x930881a4UL,\n-    0xbc835a75UL, 0x5f929ff1UL, 0x3ff356c5UL, 0x5c4e4628UL, 0xbc8b5ceeUL,\n-    0xa2de883bUL, 0x3ff36431UL, 0xa06cb85eUL, 0xbc8c3144UL, 0x373aa9cbUL,\n-    0x3ff371a7UL, 0xbf42eae2UL, 0xbc963aeaUL, 0x231e754aUL, 0x3ff37f26UL,\n-    0x9eceb23cUL, 0xbc99f5caUL, 0x6d05d866UL, 0x3ff38caeUL, 0x3c9904bdUL,\n-    0xbc9e958dUL, 0x1b7140efUL, 0x3ff39a40UL, 0xfc8e2934UL, 0xbc99a9a5UL,\n-    0x34e59ff7UL, 0x3ff3a7dbUL, 0xd661f5e3UL, 0xbc75e436UL, 0xbfec6cf4UL,\n-    0x3ff3b57fUL, 0xe26fff18UL, 0x3c954c66UL, 0xc313a8e5UL, 0x3ff3c32dUL,\n-    0x375d29c3UL, 0xbc9efff8UL, 0x44ede173UL, 0x3ff3d0e5UL, 0x8c284c71UL,\n-    0x3c7fe8d0UL, 0x4c123422UL, 0x3ff3dea6UL, 0x11f09ebcUL, 0x3c8ada09UL,\n-    0xdf1c5175UL, 0x3ff3ec70UL, 0x7b8c9bcaUL, 0xbc8af663UL, 0x04ac801cUL,\n-    0x3ff3fa45UL, 0xf956f9f3UL, 0xbc97d023UL, 0xc367a024UL, 0x3ff40822UL,\n-    0xb6f4d048UL, 0x3c8bddf8UL, 0x21f72e2aUL, 0x3ff4160aUL, 0x1c309278UL,\n-    0xbc5ef369UL, 0x2709468aUL, 0x3ff423fbUL, 0xc0b314ddUL, 0xbc98462dUL,\n-    0xd950a897UL, 0x3ff431f5UL, 0xe35f7999UL, 0xbc81c7ddUL, 0x3f84b9d4UL,\n-    0x3ff43ffaUL, 0x9704c003UL, 0x3c8880beUL, 0x6061892dUL, 0x3ff44e08UL,\n-    0x04ef80d0UL, 0x3c489b7aUL, 0x42a7d232UL, 0x3ff45c20UL, 0x82fb1f8eUL,\n-    0xbc686419UL, 0xed1d0057UL, 0x3ff46a41UL, 0xd1648a76UL, 0x3c9c944bUL,\n-    0x668b3237UL, 0x3ff4786dUL, 0xed445733UL, 0xbc9c20f0UL, 0xb5c13cd0UL,\n-    0x3ff486a2UL, 0xb69062f0UL, 0x3c73c1a3UL, 0xe192aed2UL, 0x3ff494e1UL,\n-    0x5e499ea0UL, 0xbc83b289UL, 0xf0d7d3deUL, 0x3ff4a32aUL, 0xf3d1be56UL,\n-    0x3c99cb62UL, 0xea6db7d7UL, 0x3ff4b17dUL, 0x7f2897f0UL, 0xbc8125b8UL,\n-    0xd5362a27UL, 0x3ff4bfdaUL, 0xafec42e2UL, 0x3c7d4397UL, 0xb817c114UL,\n-    0x3ff4ce41UL, 0x690abd5dUL, 0x3c905e29UL, 0x99fddd0dUL, 0x3ff4dcb2UL,\n-    0xbc6a7833UL, 0x3c98ecdbUL, 0x81d8abffUL, 0x3ff4eb2dUL, 0x2e5d7a52UL,\n-    0xbc95257dUL, 0x769d2ca7UL, 0x3ff4f9b2UL, 0xd25957e3UL, 0xbc94b309UL,\n-    0x7f4531eeUL, 0x3ff50841UL, 0x49b7465fUL, 0x3c7a249bUL, 0xa2cf6642UL,\n-    0x3ff516daUL, 0x69bd93efUL, 0xbc8f7685UL, 0xe83f4eefUL, 0x3ff5257dUL,\n-    0x43efef71UL, 0xbc7c998dUL, 0x569d4f82UL, 0x3ff5342bUL, 0x1db13cadUL,\n-    0xbc807abeUL, 0xf4f6ad27UL, 0x3ff542e2UL, 0x192d5f7eUL, 0x3c87926dUL,\n-    0xca5d920fUL, 0x3ff551a4UL, 0xefede59bUL, 0xbc8d689cUL, 0xdde910d2UL,\n-    0x3ff56070UL, 0x168eebf0UL, 0xbc90fb6eUL, 0x36b527daUL, 0x3ff56f47UL,\n-    0x011d93adUL, 0x3c99bb2cUL, 0xdbe2c4cfUL, 0x3ff57e27UL, 0x8a57b9c4UL,\n-    0xbc90b98cUL, 0xd497c7fdUL, 0x3ff58d12UL, 0x5b9a1de8UL, 0x3c8295e1UL,\n-    0x27ff07ccUL, 0x3ff59c08UL, 0xe467e60fUL, 0xbc97e2ceUL, 0xdd485429UL,\n-    0x3ff5ab07UL, 0x054647adUL, 0x3c96324cUL, 0xfba87a03UL, 0x3ff5ba11UL,\n-    0x4c233e1aUL, 0xbc9b77a1UL, 0x8a5946b7UL, 0x3ff5c926UL, 0x816986a2UL,\n-    0x3c3c4b1bUL, 0x90998b93UL, 0x3ff5d845UL, 0xa8b45643UL, 0xbc9cd6a7UL,\n-    0x15ad2148UL, 0x3ff5e76fUL, 0x3080e65eUL, 0x3c9ba6f9UL, 0x20dceb71UL,\n-    0x3ff5f6a3UL, 0xe3cdcf92UL, 0xbc89eaddUL, 0xb976dc09UL, 0x3ff605e1UL,\n-    0x9b56de47UL, 0xbc93e242UL, 0xe6cdf6f4UL, 0x3ff6152aUL, 0x4ab84c27UL,\n-    0x3c9e4b3eUL, 0xb03a5585UL, 0x3ff6247eUL, 0x7e40b497UL, 0xbc9383c1UL,\n-    0x1d1929fdUL, 0x3ff633ddUL, 0xbeb964e5UL, 0x3c984710UL, 0x34ccc320UL,\n-    0x3ff64346UL, 0x759d8933UL, 0xbc8c483cUL, 0xfebc8fb7UL, 0x3ff652b9UL,\n-    0xc9a73e09UL, 0xbc9ae3d5UL, 0x82552225UL, 0x3ff66238UL, 0x87591c34UL,\n-    0xbc9bb609UL, 0xc70833f6UL, 0x3ff671c1UL, 0x586c6134UL, 0xbc8e8732UL,\n-    0xd44ca973UL, 0x3ff68155UL, 0x44f73e65UL, 0x3c6038aeUL, 0xb19e9538UL,\n-    0x3ff690f4UL, 0x9aeb445dUL, 0x3c8804bdUL, 0x667f3bcdUL, 0x3ff6a09eUL,\n-    0x13b26456UL, 0xbc9bdd34UL, 0xfa75173eUL, 0x3ff6b052UL, 0x2c9a9d0eUL,\n-    0x3c7a38f5UL, 0x750bdabfUL, 0x3ff6c012UL, 0x67ff0b0dUL, 0xbc728956UL,\n-    0xddd47645UL, 0x3ff6cfdcUL, 0xb6f17309UL, 0x3c9c7aa9UL, 0x3c651a2fUL,\n-    0x3ff6dfb2UL, 0x683c88abUL, 0xbc6bbe3aUL, 0x98593ae5UL, 0x3ff6ef92UL,\n-    0x9e1ac8b2UL, 0xbc90b974UL, 0xf9519484UL, 0x3ff6ff7dUL, 0x25860ef6UL,\n-    0xbc883c0fUL, 0x66f42e87UL, 0x3ff70f74UL, 0xd45aa65fUL, 0x3c59d644UL,\n-    0xe8ec5f74UL, 0x3ff71f75UL, 0x86887a99UL, 0xbc816e47UL, 0x86ead08aUL,\n-    0x3ff72f82UL, 0x2cd62c72UL, 0xbc920aa0UL, 0x48a58174UL, 0x3ff73f9aUL,\n-    0x6c65d53cUL, 0xbc90a8d9UL, 0x35d7cbfdUL, 0x3ff74fbdUL, 0x618a6e1cUL,\n-    0x3c9047fdUL, 0x564267c9UL, 0x3ff75febUL, 0x57316dd3UL, 0xbc902459UL,\n-    0xb1ab6e09UL, 0x3ff77024UL, 0x169147f8UL, 0x3c9b7877UL, 0x4fde5d3fUL,\n-    0x3ff78069UL, 0x0a02162dUL, 0x3c9866b8UL, 0x38ac1cf6UL, 0x3ff790b9UL,\n-    0x62aadd3eUL, 0x3c9349a8UL, 0x73eb0187UL, 0x3ff7a114UL, 0xee04992fUL,\n-    0xbc841577UL, 0x0976cfdbUL, 0x3ff7b17bUL, 0x8468dc88UL, 0xbc9bebb5UL,\n-    0x0130c132UL, 0x3ff7c1edUL, 0xd1164dd6UL, 0x3c9f124cUL, 0x62ff86f0UL,\n-    0x3ff7d26aUL, 0xfb72b8b4UL, 0x3c91bddbUL, 0x36cf4e62UL, 0x3ff7e2f3UL,\n-    0xba15797eUL, 0x3c705d02UL, 0x8491c491UL, 0x3ff7f387UL, 0xcf9311aeUL,\n-    0xbc807f11UL, 0x543e1a12UL, 0x3ff80427UL, 0x626d972bUL, 0xbc927c86UL,\n-    0xadd106d9UL, 0x3ff814d2UL, 0x0d151d4dUL, 0x3c946437UL, 0x994cce13UL,\n-    0x3ff82589UL, 0xd41532d8UL, 0xbc9d4c1dUL, 0x1eb941f7UL, 0x3ff8364cUL,\n-    0x31df2bd5UL, 0x3c999b9aUL, 0x4623c7adUL, 0x3ff8471aUL, 0xa341cdfbUL,\n-    0xbc88d684UL, 0x179f5b21UL, 0x3ff857f4UL, 0xf8b216d0UL, 0xbc5ba748UL,\n-    0x9b4492edUL, 0x3ff868d9UL, 0x9bd4f6baUL, 0xbc9fc6f8UL, 0xd931a436UL,\n-    0x3ff879caUL, 0xd2db47bdUL, 0x3c85d2d7UL, 0xd98a6699UL, 0x3ff88ac7UL,\n-    0xf37cb53aUL, 0x3c9994c2UL, 0xa478580fUL, 0x3ff89bd0UL, 0x4475202aUL,\n-    0x3c9d5395UL, 0x422aa0dbUL, 0x3ff8ace5UL, 0x56864b27UL, 0x3c96e9f1UL,\n-    0xbad61778UL, 0x3ff8be05UL, 0xfc43446eUL, 0x3c9ecb5eUL, 0x16b5448cUL,\n-    0x3ff8cf32UL, 0x32e9e3aaUL, 0xbc70d55eUL, 0x5e0866d9UL, 0x3ff8e06aUL,\n-    0x6fc9b2e6UL, 0xbc97114aUL, 0x99157736UL, 0x3ff8f1aeUL, 0xa2e3976cUL,\n-    0x3c85cc13UL, 0xd0282c8aUL, 0x3ff902feUL, 0x85fe3fd2UL, 0x3c9592caUL,\n-    0x0b91ffc6UL, 0x3ff9145bUL, 0x2e582524UL, 0xbc9dd679UL, 0x53aa2fe2UL,\n-    0x3ff925c3UL, 0xa639db7fUL, 0xbc83455fUL, 0xb0cdc5e5UL, 0x3ff93737UL,\n-    0x81b57ebcUL, 0xbc675fc7UL, 0x2b5f98e5UL, 0x3ff948b8UL, 0x797d2d99UL,\n-    0xbc8dc3d6UL, 0xcbc8520fUL, 0x3ff95a44UL, 0x96a5f039UL, 0xbc764b7cUL,\n-    0x9a7670b3UL, 0x3ff96bddUL, 0x7f19c896UL, 0xbc5ba596UL, 0x9fde4e50UL,\n-    0x3ff97d82UL, 0x7c1b85d1UL, 0xbc9d185bUL, 0xe47a22a2UL, 0x3ff98f33UL,\n-    0xa24c78ecUL, 0x3c7cabdaUL, 0x70ca07baUL, 0x3ff9a0f1UL, 0x91cee632UL,\n-    0xbc9173bdUL, 0x4d53fe0dUL, 0x3ff9b2bbUL, 0x4df6d518UL, 0xbc9dd84eUL,\n-    0x82a3f090UL, 0x3ff9c491UL, 0xb071f2beUL, 0x3c7c7c46UL, 0x194bb8d5UL,\n-    0x3ff9d674UL, 0xa3dd8233UL, 0xbc9516beUL, 0x19e32323UL, 0x3ff9e863UL,\n-    0x78e64c6eUL, 0x3c7824caUL, 0x8d07f29eUL, 0x3ff9fa5eUL, 0xaaf1faceUL,\n-    0xbc84a9ceUL, 0x7b5de565UL, 0x3ffa0c66UL, 0x5d1cd533UL, 0xbc935949UL,\n-    0xed8eb8bbUL, 0x3ffa1e7aUL, 0xee8be70eUL, 0x3c9c6618UL, 0xec4a2d33UL,\n-    0x3ffa309bUL, 0x7ddc36abUL, 0x3c96305cUL, 0x80460ad8UL, 0x3ffa42c9UL,\n-    0x589fb120UL, 0xbc9aa780UL, 0xb23e255dUL, 0x3ffa5503UL, 0xdb8d41e1UL,\n-    0xbc9d2f6eUL, 0x8af46052UL, 0x3ffa674aUL, 0x30670366UL, 0x3c650f56UL,\n-    0x1330b358UL, 0x3ffa799eUL, 0xcac563c7UL, 0x3c9bcb7eUL, 0x53c12e59UL,\n-    0x3ffa8bfeUL, 0xb2ba15a9UL, 0xbc94f867UL, 0x5579fdbfUL, 0x3ffa9e6bUL,\n-    0x0ef7fd31UL, 0x3c90fac9UL, 0x21356ebaUL, 0x3ffab0e5UL, 0xdae94545UL,\n-    0x3c889c31UL, 0xbfd3f37aUL, 0x3ffac36bUL, 0xcae76cd0UL, 0xbc8f9234UL,\n-    0x3a3c2774UL, 0x3ffad5ffUL, 0xb6b1b8e5UL, 0x3c97ef3bUL, 0x995ad3adUL,\n-    0x3ffae89fUL, 0x345dcc81UL, 0x3c97a1cdUL, 0xe622f2ffUL, 0x3ffafb4cUL,\n-    0x0f315ecdUL, 0xbc94b2fcUL, 0x298db666UL, 0x3ffb0e07UL, 0x4c80e425UL,\n-    0xbc9bdef5UL, 0x6c9a8952UL, 0x3ffb20ceUL, 0x4a0756ccUL, 0x3c94dd02UL,\n-    0xb84f15fbUL, 0x3ffb33a2UL, 0x3084d708UL, 0xbc62805eUL, 0x15b749b1UL,\n-    0x3ffb4684UL, 0xe9df7c90UL, 0xbc7f763dUL, 0x8de5593aUL, 0x3ffb5972UL,\n-    0xbbba6de3UL, 0xbc9c71dfUL, 0x29f1c52aUL, 0x3ffb6c6eUL, 0x52883f6eUL,\n-    0x3c92a8f3UL, 0xf2fb5e47UL, 0x3ffb7f76UL, 0x7e54ac3bUL, 0xbc75584fUL,\n-    0xf22749e4UL, 0x3ffb928cUL, 0x54cb65c6UL, 0xbc9b7216UL, 0x30a1064aUL,\n-    0x3ffba5b0UL, 0x0e54292eUL, 0xbc9efcd3UL, 0xb79a6f1fUL, 0x3ffbb8e0UL,\n-    0xc9696205UL, 0xbc3f52d1UL, 0x904bc1d2UL, 0x3ffbcc1eUL, 0x7a2d9e84UL,\n-    0x3c823dd0UL, 0xc3f3a207UL, 0x3ffbdf69UL, 0x60ea5b53UL, 0xbc3c2623UL,\n-    0x5bd71e09UL, 0x3ffbf2c2UL, 0x3f6b9c73UL, 0xbc9efdcaUL, 0x6141b33dUL,\n-    0x3ffc0628UL, 0xa1fbca34UL, 0xbc8d8a5aUL, 0xdd85529cUL, 0x3ffc199bUL,\n-    0x895048ddUL, 0x3c811065UL, 0xd9fa652cUL, 0x3ffc2d1cUL, 0x17c8a5d7UL,\n-    0xbc96e516UL, 0x5fffd07aUL, 0x3ffc40abUL, 0xe083c60aUL, 0x3c9b4537UL,\n-    0x78fafb22UL, 0x3ffc5447UL, 0x2493b5afUL, 0x3c912f07UL, 0x2e57d14bUL,\n-    0x3ffc67f1UL, 0xff483cadUL, 0x3c92884dUL, 0x8988c933UL, 0x3ffc7ba8UL,\n-    0xbe255559UL, 0xbc8e76bbUL, 0x9406e7b5UL, 0x3ffc8f6dUL, 0x48805c44UL,\n-    0x3c71acbcUL, 0x5751c4dbUL, 0x3ffca340UL, 0xd10d08f5UL, 0xbc87f2beUL,\n-    0xdcef9069UL, 0x3ffcb720UL, 0xd1e949dbUL, 0x3c7503cbUL, 0x2e6d1675UL,\n-    0x3ffccb0fUL, 0x86009092UL, 0xbc7d220fUL, 0x555dc3faUL, 0x3ffcdf0bUL,\n-    0x53829d72UL, 0xbc8dd83bUL, 0x5b5bab74UL, 0x3ffcf315UL, 0xb86dff57UL,\n-    0xbc9a08e9UL, 0x4a07897cUL, 0x3ffd072dUL, 0x43797a9cUL, 0xbc9cbc37UL,\n-    0x2b08c968UL, 0x3ffd1b53UL, 0x219a36eeUL, 0x3c955636UL, 0x080d89f2UL,\n-    0x3ffd2f87UL, 0x719d8578UL, 0xbc9d487bUL, 0xeacaa1d6UL, 0x3ffd43c8UL,\n-    0xbf5a1614UL, 0x3c93db53UL, 0xdcfba487UL, 0x3ffd5818UL, 0xd75b3707UL,\n-    0x3c82ed02UL, 0xe862e6d3UL, 0x3ffd6c76UL, 0x4a8165a0UL, 0x3c5fe87aUL,\n-    0x16c98398UL, 0x3ffd80e3UL, 0x8beddfe8UL, 0xbc911ec1UL, 0x71ff6075UL,\n-    0x3ffd955dUL, 0xbb9af6beUL, 0x3c9a052dUL, 0x03db3285UL, 0x3ffda9e6UL,\n-    0x696db532UL, 0x3c9c2300UL, 0xd63a8315UL, 0x3ffdbe7cUL, 0x926b8be4UL,\n-    0xbc9b76f1UL, 0xf301b460UL, 0x3ffdd321UL, 0x78f018c3UL, 0x3c92da57UL,\n-    0x641c0658UL, 0x3ffde7d5UL, 0x8e79ba8fUL, 0xbc9ca552UL, 0x337b9b5fUL,\n-    0x3ffdfc97UL, 0x4f184b5cUL, 0xbc91a5cdUL, 0x6b197d17UL, 0x3ffe1167UL,\n-    0xbd5c7f44UL, 0xbc72b529UL, 0x14f5a129UL, 0x3ffe2646UL, 0x817a1496UL,\n-    0xbc97b627UL, 0x3b16ee12UL, 0x3ffe3b33UL, 0x31fdc68bUL, 0xbc99f4a4UL,\n-    0xe78b3ff6UL, 0x3ffe502eUL, 0x80a9cc8fUL, 0x3c839e89UL, 0x24676d76UL,\n-    0x3ffe6539UL, 0x7522b735UL, 0xbc863ff8UL, 0xfbc74c83UL, 0x3ffe7a51UL,\n-    0xca0c8de2UL, 0x3c92d522UL, 0x77cdb740UL, 0x3ffe8f79UL, 0x80b054b1UL,\n-    0xbc910894UL, 0xa2a490daUL, 0x3ffea4afUL, 0x179c2893UL, 0xbc9e9c23UL,\n-    0x867cca6eUL, 0x3ffeb9f4UL, 0x2293e4f2UL, 0x3c94832fUL, 0x2d8e67f1UL,\n-    0x3ffecf48UL, 0xb411ad8cUL, 0xbc9c93f3UL, 0xa2188510UL, 0x3ffee4aaUL,\n-    0xa487568dUL, 0x3c91c68dUL, 0xee615a27UL, 0x3ffefa1bUL, 0x86a4b6b0UL,\n-    0x3c9dc7f4UL, 0x1cb6412aUL, 0x3fff0f9cUL, 0x65181d45UL, 0xbc932200UL,\n-    0x376bba97UL, 0x3fff252bUL, 0xbf0d8e43UL, 0x3c93a1a5UL, 0x48dd7274UL,\n-    0x3fff3ac9UL, 0x3ed837deUL, 0xbc795a5aUL, 0x5b6e4540UL, 0x3fff5076UL,\n-    0x2dd8a18bUL, 0x3c99d3e1UL, 0x798844f8UL, 0x3fff6632UL, 0x3539343eUL,\n-    0x3c9fa37bUL, 0xad9cbe14UL, 0x3fff7bfdUL, 0xd006350aUL, 0xbc9dbb12UL,\n-    0x02243c89UL, 0x3fff91d8UL, 0xa779f689UL, 0xbc612ea8UL, 0x819e90d8UL,\n-    0x3fffa7c1UL, 0xf3a5931eUL, 0x3c874853UL, 0x3692d514UL, 0x3fffbdbaUL,\n-    0x15098eb6UL, 0xbc796773UL, 0x2b8f71f1UL, 0x3fffd3c2UL, 0x966579e7UL,\n-    0x3c62eb74UL, 0x6b2a23d9UL, 0x3fffe9d9UL, 0x7442fde3UL, 0x3c74a603UL,\n-    0xe78a6731UL, 0x3f55d87fUL, 0xd704a0c0UL, 0x3fac6b08UL, 0x6fba4e77UL,\n-    0x3f83b2abUL, 0xff82c58fUL, 0x3fcebfbdUL, 0xfefa39efUL, 0x3fe62e42UL,\n-    0x00000000UL, 0x00000000UL, 0xfefa39efUL, 0x3fe62e42UL, 0xfefa39efUL,\n-    0xbfe62e42UL, 0xf8000000UL, 0xffffffffUL, 0xf8000000UL, 0xffffffffUL,\n-    0x00000000UL, 0x80000000UL, 0x00000000UL, 0x00000000UL\n-\n-};\n-\n-ATTRIBUTE_ALIGNED(8) static const double _DOUBLE2 = 2.0;\n-ATTRIBUTE_ALIGNED(8) static const double _DOUBLE0 = 0.0;\n-ATTRIBUTE_ALIGNED(8) static const double _DOUBLE0DOT5 = 0.5;\n-\n-\/\/registers,\n-\/\/ input: xmm0, xmm1\n-\/\/ scratch: xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7\n-\/\/          eax, edx, ecx, ebx\n-\n-\/\/ Code generated by Intel C compiler for LIBM library\n-\n-void MacroAssembler::fast_pow(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register eax, Register ecx, Register edx, Register tmp) {\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label L_2TAG_PACKET_4_0_2, L_2TAG_PACKET_5_0_2, L_2TAG_PACKET_6_0_2, L_2TAG_PACKET_7_0_2;\n-  Label L_2TAG_PACKET_8_0_2, L_2TAG_PACKET_9_0_2, L_2TAG_PACKET_10_0_2, L_2TAG_PACKET_11_0_2;\n-  Label L_2TAG_PACKET_12_0_2, L_2TAG_PACKET_13_0_2, L_2TAG_PACKET_14_0_2, L_2TAG_PACKET_15_0_2;\n-  Label L_2TAG_PACKET_16_0_2, L_2TAG_PACKET_17_0_2, L_2TAG_PACKET_18_0_2, L_2TAG_PACKET_19_0_2;\n-  Label L_2TAG_PACKET_20_0_2, L_2TAG_PACKET_21_0_2, L_2TAG_PACKET_22_0_2, L_2TAG_PACKET_23_0_2;\n-  Label L_2TAG_PACKET_24_0_2, L_2TAG_PACKET_25_0_2, L_2TAG_PACKET_26_0_2, L_2TAG_PACKET_27_0_2;\n-  Label L_2TAG_PACKET_28_0_2, L_2TAG_PACKET_29_0_2, L_2TAG_PACKET_30_0_2, L_2TAG_PACKET_31_0_2;\n-  Label L_2TAG_PACKET_32_0_2, L_2TAG_PACKET_33_0_2, L_2TAG_PACKET_34_0_2, L_2TAG_PACKET_35_0_2;\n-  Label L_2TAG_PACKET_36_0_2, L_2TAG_PACKET_37_0_2, L_2TAG_PACKET_38_0_2, L_2TAG_PACKET_39_0_2;\n-  Label L_2TAG_PACKET_40_0_2, L_2TAG_PACKET_41_0_2, L_2TAG_PACKET_42_0_2, L_2TAG_PACKET_43_0_2;\n-  Label L_2TAG_PACKET_44_0_2, L_2TAG_PACKET_45_0_2, L_2TAG_PACKET_46_0_2, L_2TAG_PACKET_47_0_2;\n-  Label L_2TAG_PACKET_48_0_2, L_2TAG_PACKET_49_0_2, L_2TAG_PACKET_50_0_2, L_2TAG_PACKET_51_0_2;\n-  Label L_2TAG_PACKET_52_0_2, L_2TAG_PACKET_53_0_2, L_2TAG_PACKET_54_0_2, L_2TAG_PACKET_55_0_2;\n-  Label L_2TAG_PACKET_56_0_2, L_2TAG_PACKET_57_0_2, L_2TAG_PACKET_58_0_2, start;\n-  Label L_NOT_DOUBLE2, L_NOT_DOUBLE0DOT5;\n-\n-  assert_different_registers(tmp, eax, ecx, edx);\n-\n-  address static_const_table_pow = (address)_static_const_table_pow;\n-  address DOUBLE2 = (address) &_DOUBLE2;\n-  address DOUBLE0 = (address) &_DOUBLE0;\n-  address DOUBLE0DOT5 = (address) &_DOUBLE0DOT5;\n-\n-  subl(rsp, 120);\n-  movl(Address(rsp, 64), tmp);\n-  lea(tmp, ExternalAddress(static_const_table_pow));\n-  movsd(xmm0, Address(rsp, 128));\n-  movsd(xmm1, Address(rsp, 136));\n-\n-  \/\/ Special case: pow(x, 2.0) => x * x\n-  ucomisd(xmm1, ExternalAddress(DOUBLE2));\n-  jccb(Assembler::notEqual, L_NOT_DOUBLE2);\n-  jccb(Assembler::parity, L_NOT_DOUBLE2);\n-  mulsd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_NOT_DOUBLE2);\n-  \/\/ Special case: pow(x, 0.5) => sqrt(x)\n-  ucomisd(xmm1, ExternalAddress(DOUBLE0DOT5)); \/\/ For pow(x, y), check whether y == 0.5\n-  jccb(Assembler::notEqual, L_NOT_DOUBLE0DOT5);\n-  jccb(Assembler::parity, L_NOT_DOUBLE0DOT5);\n-  ucomisd(xmm0, ExternalAddress(DOUBLE0));\n-  \/\/ According to the API specs, pow(-0.0, 0.5) = 0.0 and sqrt(-0.0) = -0.0.\n-  \/\/ So pow(-0.0, 0.5) shouldn't be replaced with sqrt(-0.0).\n-  \/\/ -0.0\/+0.0 are both excluded since floating-point comparison doesn't distinguish -0.0 from +0.0.\n-  jccb(Assembler::belowEqual, L_NOT_DOUBLE0DOT5); \/\/ pow(x, 0.5) => sqrt(x) only for x > 0.0\n-  sqrtsd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_NOT_DOUBLE0DOT5);\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 16368);\n-  pinsrw(xmm2, eax, 3);\n-  movl(ecx, 1069088768);\n-  movdl(xmm7, ecx);\n-  movsd(Address(rsp, 16), xmm1);\n-  xorpd(xmm1, xmm1);\n-  movl(edx, 30704);\n-  pinsrw(xmm1, edx, 3);\n-  movsd(Address(rsp, 8), xmm0);\n-  movdqu(xmm3, xmm0);\n-  movl(edx, 8192);\n-  movdl(xmm4, edx);\n-  movdqu(xmm6, Address(tmp, 8240));\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  psllq(xmm0, 5);\n-  movsd(xmm2, Address(tmp, 8256));\n-  psrlq(xmm0, 34);\n-  movl(edx, eax);\n-  andl(edx, 32752);\n-  subl(edx, 16368);\n-  movl(ecx, edx);\n-  sarl(edx, 31);\n-  addl(ecx, edx);\n-  xorl(ecx, edx);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm3, 12);\n-  addl(ecx, 16);\n-  bsrl(ecx, ecx);\n-  psrlq(xmm3, 12);\n-  movl(Address(rsp, 24), rsi);\n-  subl(eax, 16);\n-  cmpl(eax, 32736);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_0_0_2);\n-  movl(rsi, 0);\n-\n-  bind(L_2TAG_PACKET_1_0_2);\n-  mulss(xmm0, xmm7);\n-  movl(edx, -1);\n-  subl(ecx, 4);\n-  shll(edx);\n-  movdl(xmm5, edx);\n-  por(xmm3, xmm1);\n-  subl(eax, 16351);\n-  cmpl(eax, 1);\n-  jcc(Assembler::belowEqual, L_2TAG_PACKET_2_0_2);\n-  paddd(xmm0, xmm4);\n-  psllq(xmm5, 32);\n-  movdl(edx, xmm0);\n-  psllq(xmm0, 29);\n-  pand(xmm5, xmm3);\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  pand(xmm0, xmm6);\n-  subsd(xmm3, xmm5);\n-  subl(eax, 1);\n-  sarl(eax, 4);\n-  cvtsi2sdl(xmm7, eax);\n-  mulpd(xmm5, xmm0);\n-\n-  bind(L_2TAG_PACKET_4_0_2);\n-  mulsd(xmm3, xmm0);\n-  movdqu(xmm1, Address(tmp, 8272));\n-  subsd(xmm5, xmm2);\n-  movdqu(xmm4, Address(tmp, 8288));\n-  movl(ecx, eax);\n-  sarl(eax, 31);\n-  addl(ecx, eax);\n-  xorl(eax, ecx);\n-  addl(eax, 1);\n-  bsrl(eax, eax);\n-  unpcklpd(xmm5, xmm3);\n-  movdqu(xmm6, Address(tmp, 8304));\n-  addsd(xmm3, xmm5);\n-  andl(edx, 16760832);\n-  shrl(edx, 10);\n-  addpd(xmm5, Address(tmp, edx, Address::times_1, -3616));\n-  movdqu(xmm0, Address(tmp, 8320));\n-  pshufd(xmm2, xmm3, 68);\n-  mulsd(xmm3, xmm3);\n-  mulpd(xmm1, xmm2);\n-  mulpd(xmm4, xmm2);\n-  addsd(xmm5, xmm7);\n-  mulsd(xmm2, xmm3);\n-  addpd(xmm6, xmm1);\n-  mulsd(xmm3, xmm3);\n-  addpd(xmm0, xmm4);\n-  movsd(xmm1, Address(rsp, 16));\n-  movzwl(ecx, Address(rsp, 22));\n-  pshufd(xmm7, xmm5, 238);\n-  movsd(xmm4, Address(tmp, 8368));\n-  mulpd(xmm6, xmm2);\n-  pshufd(xmm3, xmm3, 68);\n-  mulpd(xmm0, xmm2);\n-  shll(eax, 4);\n-  subl(eax, 15872);\n-  andl(ecx, 32752);\n-  addl(eax, ecx);\n-  mulpd(xmm3, xmm6);\n-  cmpl(eax, 624);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_5_0_2);\n-  xorpd(xmm6, xmm6);\n-  movl(edx, 17080);\n-  pinsrw(xmm6, edx, 3);\n-  movdqu(xmm2, xmm1);\n-  pand(xmm4, xmm1);\n-  subsd(xmm1, xmm4);\n-  mulsd(xmm4, xmm5);\n-  addsd(xmm0, xmm7);\n-  mulsd(xmm1, xmm5);\n-  movdqu(xmm7, xmm6);\n-  addsd(xmm6, xmm4);\n-  addpd(xmm3, xmm0);\n-  movdl(edx, xmm6);\n-  subsd(xmm6, xmm7);\n-  pshufd(xmm0, xmm3, 238);\n-  subsd(xmm4, xmm6);\n-  addsd(xmm0, xmm3);\n-  movl(ecx, edx);\n-  andl(edx, 255);\n-  addl(edx, edx);\n-  movdqu(xmm5, Address(tmp, edx, Address::times_8, 8384));\n-  addsd(xmm4, xmm1);\n-  mulsd(xmm2, xmm0);\n-  movdqu(xmm7, Address(tmp, 12480));\n-  movdqu(xmm3, Address(tmp, 12496));\n-  shll(ecx, 12);\n-  xorl(ecx, rsi);\n-  andl(ecx, -1048576);\n-  movdl(xmm6, ecx);\n-  addsd(xmm2, xmm4);\n-  movsd(xmm1, Address(tmp, 12512));\n-  pshufd(xmm0, xmm2, 68);\n-  pshufd(xmm4, xmm2, 68);\n-  mulpd(xmm0, xmm0);\n-  movl(rsi, Address(rsp, 24));\n-  mulpd(xmm7, xmm4);\n-  pshufd(xmm6, xmm6, 17);\n-  mulsd(xmm1, xmm2);\n-  mulsd(xmm0, xmm0);\n-  paddd(xmm5, xmm6);\n-  addpd(xmm3, xmm7);\n-  mulsd(xmm1, xmm5);\n-  pshufd(xmm6, xmm5, 238);\n-  mulpd(xmm0, xmm3);\n-  addsd(xmm1, xmm6);\n-  pshufd(xmm3, xmm0, 238);\n-  mulsd(xmm0, xmm5);\n-  mulsd(xmm3, xmm5);\n-  addsd(xmm0, xmm1);\n-  addsd(xmm0, xmm3);\n-  addsd(xmm0, xmm5);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_6_0_2);\n-\n-  bind(L_2TAG_PACKET_7_0_2);\n-  movsd(xmm0, Address(rsp, 128));\n-  movsd(xmm1, Address(rsp, 136));\n-  mulsd(xmm0, xmm1);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_6_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  addl(eax, 16);\n-  movl(edx, 32752);\n-  andl(edx, eax);\n-  cmpl(edx, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_8_0_2);\n-  testl(eax, 32768);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_9_0_2);\n-\n-  bind(L_2TAG_PACKET_10_0_2);\n-  movl(ecx, Address(rsp, 16));\n-  xorl(edx, edx);\n-  testl(ecx, ecx);\n-  movl(ecx, 1);\n-  cmovl(Assembler::notEqual, edx, ecx);\n-  orl(edx, Address(rsp, 20));\n-  cmpl(edx, 1072693248);\n-  jcc(Assembler::equal, L_2TAG_PACKET_7_0_2);\n-  movsd(xmm0, Address(rsp, 8));\n-  movsd(xmm3, Address(rsp, 8));\n-  movdl(edx, xmm3);\n-  psrlq(xmm3, 32);\n-  movdl(ecx, xmm3);\n-  orl(edx, ecx);\n-  cmpl(edx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_11_0_2);\n-  xorpd(xmm3, xmm3);\n-  movl(eax, 18416);\n-  pinsrw(xmm3, eax, 3);\n-  mulsd(xmm0, xmm3);\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 16368);\n-  pinsrw(xmm2, eax, 3);\n-  movdqu(xmm3, xmm0);\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  movl(ecx, 18416);\n-  psllq(xmm0, 5);\n-  movsd(xmm2, Address(tmp, 8256));\n-  psrlq(xmm0, 34);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm3, 12);\n-  movdqu(xmm6, Address(tmp, 8240));\n-  psrlq(xmm3, 12);\n-  mulss(xmm0, xmm7);\n-  movl(edx, -1024);\n-  movdl(xmm5, edx);\n-  por(xmm3, xmm1);\n-  paddd(xmm0, xmm4);\n-  psllq(xmm5, 32);\n-  movdl(edx, xmm0);\n-  psllq(xmm0, 29);\n-  pand(xmm5, xmm3);\n-  movl(rsi, 0);\n-  pand(xmm0, xmm6);\n-  subsd(xmm3, xmm5);\n-  andl(eax, 32752);\n-  subl(eax, 18416);\n-  sarl(eax, 4);\n-  cvtsi2sdl(xmm7, eax);\n-  mulpd(xmm5, xmm0);\n-  jmp(L_2TAG_PACKET_4_0_2);\n-\n-  bind(L_2TAG_PACKET_12_0_2);\n-  movl(ecx, Address(rsp, 16));\n-  xorl(edx, edx);\n-  testl(ecx, ecx);\n-  movl(ecx, 1);\n-  cmovl(Assembler::notEqual, edx, ecx);\n-  orl(edx, Address(rsp, 20));\n-  cmpl(edx, 1072693248);\n-  jcc(Assembler::equal, L_2TAG_PACKET_7_0_2);\n-  movsd(xmm0, Address(rsp, 8));\n-  movsd(xmm3, Address(rsp, 8));\n-  movdl(edx, xmm3);\n-  psrlq(xmm3, 32);\n-  movdl(ecx, xmm3);\n-  orl(edx, ecx);\n-  cmpl(edx, 0);\n-  jcc(Assembler::equal, L_2TAG_PACKET_11_0_2);\n-  xorpd(xmm3, xmm3);\n-  movl(eax, 18416);\n-  pinsrw(xmm3, eax, 3);\n-  mulsd(xmm0, xmm3);\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 16368);\n-  pinsrw(xmm2, eax, 3);\n-  movdqu(xmm3, xmm0);\n-  pextrw(eax, xmm0, 3);\n-  por(xmm0, xmm2);\n-  movl(ecx, 18416);\n-  psllq(xmm0, 5);\n-  movsd(xmm2, Address(tmp, 8256));\n-  psrlq(xmm0, 34);\n-  rcpss(xmm0, xmm0);\n-  psllq(xmm3, 12);\n-  movdqu(xmm6, Address(tmp, 8240));\n-  psrlq(xmm3, 12);\n-  mulss(xmm0, xmm7);\n-  movl(edx, -1024);\n-  movdl(xmm5, edx);\n-  por(xmm3, xmm1);\n-  paddd(xmm0, xmm4);\n-  psllq(xmm5, 32);\n-  movdl(edx, xmm0);\n-  psllq(xmm0, 29);\n-  pand(xmm5, xmm3);\n-  movl(rsi, INT_MIN);\n-  pand(xmm0, xmm6);\n-  subsd(xmm3, xmm5);\n-  andl(eax, 32752);\n-  subl(eax, 18416);\n-  sarl(eax, 4);\n-  cvtsi2sdl(xmm7, eax);\n-  mulpd(xmm5, xmm0);\n-  jmp(L_2TAG_PACKET_4_0_2);\n-\n-  bind(L_2TAG_PACKET_5_0_2);\n-  cmpl(eax, 0);\n-  jcc(Assembler::less, L_2TAG_PACKET_13_0_2);\n-  cmpl(eax, 752);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_14_0_2);\n-\n-  bind(L_2TAG_PACKET_15_0_2);\n-  addsd(xmm0, xmm7);\n-  movsd(xmm2, Address(tmp, 12544));\n-  addpd(xmm3, xmm0);\n-  xorpd(xmm6, xmm6);\n-  movl(eax, 17080);\n-  pinsrw(xmm6, eax, 3);\n-  pshufd(xmm0, xmm3, 238);\n-  addsd(xmm0, xmm3);\n-  movdqu(xmm3, xmm5);\n-  addsd(xmm5, xmm0);\n-  movdqu(xmm4, xmm2);\n-  subsd(xmm3, xmm5);\n-  movdqu(xmm7, xmm5);\n-  pand(xmm5, xmm2);\n-  movdqu(xmm2, xmm1);\n-  pand(xmm4, xmm1);\n-  subsd(xmm7, xmm5);\n-  addsd(xmm0, xmm3);\n-  subsd(xmm1, xmm4);\n-  mulsd(xmm4, xmm5);\n-  addsd(xmm0, xmm7);\n-  mulsd(xmm2, xmm0);\n-  movdqu(xmm7, xmm6);\n-  mulsd(xmm1, xmm5);\n-  addsd(xmm6, xmm4);\n-  movdl(eax, xmm6);\n-  subsd(xmm6, xmm7);\n-  addsd(xmm2, xmm1);\n-  movdqu(xmm7, Address(tmp, 12480));\n-  movdqu(xmm3, Address(tmp, 12496));\n-  subsd(xmm4, xmm6);\n-  pextrw(edx, xmm6, 3);\n-  movl(ecx, eax);\n-  andl(eax, 255);\n-  addl(eax, eax);\n-  movdqu(xmm5, Address(tmp, eax, Address::times_8, 8384));\n-  addsd(xmm2, xmm4);\n-  sarl(ecx, 8);\n-  movl(eax, ecx);\n-  sarl(ecx, 1);\n-  subl(eax, ecx);\n-  shll(ecx, 20);\n-  xorl(ecx, rsi);\n-  movdl(xmm6, ecx);\n-  movsd(xmm1, Address(tmp, 12512));\n-  andl(edx, 32767);\n-  cmpl(edx, 16529);\n-  jcc(Assembler::above, L_2TAG_PACKET_14_0_2);\n-  pshufd(xmm0, xmm2, 68);\n-  pshufd(xmm4, xmm2, 68);\n-  mulpd(xmm0, xmm0);\n-  mulpd(xmm7, xmm4);\n-  pshufd(xmm6, xmm6, 17);\n-  mulsd(xmm1, xmm2);\n-  mulsd(xmm0, xmm0);\n-  paddd(xmm5, xmm6);\n-  addpd(xmm3, xmm7);\n-  mulsd(xmm1, xmm5);\n-  pshufd(xmm6, xmm5, 238);\n-  mulpd(xmm0, xmm3);\n-  addsd(xmm1, xmm6);\n-  pshufd(xmm3, xmm0, 238);\n-  mulsd(xmm0, xmm5);\n-  mulsd(xmm3, xmm5);\n-  shll(eax, 4);\n-  xorpd(xmm4, xmm4);\n-  addl(eax, 16368);\n-  pinsrw(xmm4, eax, 3);\n-  addsd(xmm0, xmm1);\n-  movl(rsi, Address(rsp, 24));\n-  addsd(xmm0, xmm3);\n-  movdqu(xmm1, xmm0);\n-  addsd(xmm0, xmm5);\n-  mulsd(xmm0, xmm4);\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_16_0_2);\n-  cmpl(eax, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_17_0_2);\n-\n-  bind(L_2TAG_PACKET_18_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_6_0_2);\n-\n-  bind(L_2TAG_PACKET_8_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movsd(xmm0, Address(rsp, 8));\n-  movdqu(xmm2, xmm0);\n-  movdl(eax, xmm2);\n-  psrlq(xmm2, 20);\n-  movdl(edx, xmm2);\n-  orl(eax, edx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_19_0_2);\n-  addsd(xmm0, xmm0);\n-  movdl(eax, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(edx, xmm1);\n-  movl(ecx, edx);\n-  addl(edx, edx);\n-  orl(eax, edx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_20_0_2);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_20_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 16368);\n-  pinsrw(xmm0, eax, 3);\n-  movl(edx, 29);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_2TAG_PACKET_22_0_2);\n-  movsd(xmm0, Address(rsp, 16));\n-  addpd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_19_0_2);\n-  movdl(eax, xmm1);\n-  movdqu(xmm2, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(edx, xmm1);\n-  movl(ecx, edx);\n-  addl(edx, edx);\n-  orl(eax, edx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_23_0_2);\n-  pextrw(eax, xmm2, 3);\n-  andl(eax, 32752);\n-  cmpl(eax, 32752);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_24_0_2);\n-  movdl(eax, xmm2);\n-  psrlq(xmm2, 20);\n-  movdl(edx, xmm2);\n-  orl(eax, edx);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_22_0_2);\n-\n-  bind(L_2TAG_PACKET_24_0_2);\n-  pextrw(eax, xmm0, 3);\n-  testl(eax, 32768);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_25_0_2);\n-  testl(ecx, INT_MIN);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_26_0_2);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_27_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movdl(eax, xmm1);\n-  testl(eax, 1);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_28_0_2);\n-  testl(eax, 2);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_29_0_2);\n-  jmp(L_2TAG_PACKET_28_0_2);\n-\n-  bind(L_2TAG_PACKET_25_0_2);\n-  shrl(ecx, 20);\n-  andl(ecx, 2047);\n-  cmpl(ecx, 1075);\n-  jcc(Assembler::above, L_2TAG_PACKET_28_0_2);\n-  jcc(Assembler::equal, L_2TAG_PACKET_30_0_2);\n-  cmpl(ecx, 1074);\n-  jcc(Assembler::above, L_2TAG_PACKET_27_0_2);\n-  cmpl(ecx, 1023);\n-  jcc(Assembler::below, L_2TAG_PACKET_28_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movl(eax, 17208);\n-  xorpd(xmm3, xmm3);\n-  pinsrw(xmm3, eax, 3);\n-  movdqu(xmm4, xmm3);\n-  addsd(xmm3, xmm1);\n-  subsd(xmm4, xmm3);\n-  addsd(xmm1, xmm4);\n-  pextrw(eax, xmm1, 3);\n-  andl(eax, 32752);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_28_0_2);\n-  movdl(eax, xmm3);\n-  andl(eax, 1);\n-  jcc(Assembler::equal, L_2TAG_PACKET_28_0_2);\n-\n-  bind(L_2TAG_PACKET_29_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  pextrw(eax, xmm1, 3);\n-  andl(eax, 32768);\n-  jcc(Assembler::equal, L_2TAG_PACKET_18_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 32768);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_28_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  pextrw(eax, xmm1, 3);\n-  andl(eax, 32768);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_26_0_2);\n-\n-  bind(L_2TAG_PACKET_31_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 32752);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_30_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movdl(eax, xmm1);\n-  andl(eax, 1);\n-  jcc(Assembler::equal, L_2TAG_PACKET_28_0_2);\n-  jmp(L_2TAG_PACKET_29_0_2);\n-\n-  bind(L_2TAG_PACKET_32_0_2);\n-  movdl(eax, xmm1);\n-  psrlq(xmm1, 20);\n-  movdl(edx, xmm1);\n-  orl(eax, edx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_33_0_2);\n-  movsd(xmm0, Address(rsp, 16));\n-  addsd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_33_0_2);\n-  movsd(xmm0, Address(rsp, 8));\n-  pextrw(eax, xmm0, 3);\n-  cmpl(eax, 49136);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_34_0_2);\n-  movdl(ecx, xmm0);\n-  psrlq(xmm0, 20);\n-  movdl(edx, xmm0);\n-  orl(ecx, edx);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_34_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 32760);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_34_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  andl(eax, 32752);\n-  subl(eax, 16368);\n-  pextrw(edx, xmm1, 3);\n-  xorpd(xmm0, xmm0);\n-  xorl(eax, edx);\n-  andl(eax, 32768);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_18_0_2);\n-  movl(ecx, 32752);\n-  pinsrw(xmm0, ecx, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_35_0_2);\n-  movdl(eax, xmm1);\n-  cmpl(edx, 17184);\n-  jcc(Assembler::above, L_2TAG_PACKET_36_0_2);\n-  testl(eax, 1);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_37_0_2);\n-  testl(eax, 2);\n-  jcc(Assembler::equal, L_2TAG_PACKET_38_0_2);\n-  jmp(L_2TAG_PACKET_39_0_2);\n-\n-  bind(L_2TAG_PACKET_36_0_2);\n-  testl(eax, 1);\n-  jcc(Assembler::equal, L_2TAG_PACKET_38_0_2);\n-  jmp(L_2TAG_PACKET_39_0_2);\n-\n-  bind(L_2TAG_PACKET_9_0_2);\n-  movsd(xmm2, Address(rsp, 8));\n-  movdl(eax, xmm2);\n-  psrlq(xmm2, 31);\n-  movdl(ecx, xmm2);\n-  orl(eax, ecx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_11_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  pextrw(edx, xmm1, 3);\n-  movdl(eax, xmm1);\n-  movdqu(xmm2, xmm1);\n-  psrlq(xmm2, 32);\n-  movdl(ecx, xmm2);\n-  addl(ecx, ecx);\n-  orl(ecx, eax);\n-  jcc(Assembler::equal, L_2TAG_PACKET_40_0_2);\n-  andl(edx, 32752);\n-  cmpl(edx, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_32_0_2);\n-  cmpl(edx, 17200);\n-  jcc(Assembler::above, L_2TAG_PACKET_38_0_2);\n-  cmpl(edx, 17184);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_35_0_2);\n-  cmpl(edx, 16368);\n-  jcc(Assembler::below, L_2TAG_PACKET_37_0_2);\n-  movl(eax, 17208);\n-  xorpd(xmm2, xmm2);\n-  pinsrw(xmm2, eax, 3);\n-  movdqu(xmm4, xmm2);\n-  addsd(xmm2, xmm1);\n-  subsd(xmm4, xmm2);\n-  addsd(xmm1, xmm4);\n-  pextrw(eax, xmm1, 3);\n-  andl(eax, 32767);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_37_0_2);\n-  movdl(eax, xmm2);\n-  andl(eax, 1);\n-  jcc(Assembler::equal, L_2TAG_PACKET_38_0_2);\n-\n-  bind(L_2TAG_PACKET_39_0_2);\n-  xorpd(xmm1, xmm1);\n-  movl(edx, 30704);\n-  pinsrw(xmm1, edx, 3);\n-  movsd(xmm2, Address(tmp, 8256));\n-  movsd(xmm4, Address(rsp, 8));\n-  pextrw(eax, xmm4, 3);\n-  movl(edx, 8192);\n-  movdl(xmm4, edx);\n-  andl(eax, 32767);\n-  subl(eax, 16);\n-  jcc(Assembler::less, L_2TAG_PACKET_12_0_2);\n-  movl(edx, eax);\n-  andl(edx, 32752);\n-  subl(edx, 16368);\n-  movl(ecx, edx);\n-  sarl(edx, 31);\n-  addl(ecx, edx);\n-  xorl(ecx, edx);\n-  addl(ecx, 16);\n-  bsrl(ecx, ecx);\n-  movl(rsi, INT_MIN);\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_37_0_2);\n-  xorpd(xmm1, xmm1);\n-  movl(eax, 32752);\n-  pinsrw(xmm1, eax, 3);\n-  xorpd(xmm0, xmm0);\n-  mulsd(xmm0, xmm1);\n-  movl(edx, 28);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_2TAG_PACKET_38_0_2);\n-  xorpd(xmm1, xmm1);\n-  movl(edx, 30704);\n-  pinsrw(xmm1, edx, 3);\n-  movsd(xmm2, Address(tmp, 8256));\n-  movsd(xmm4, Address(rsp, 8));\n-  pextrw(eax, xmm4, 3);\n-  movl(edx, 8192);\n-  movdl(xmm4, edx);\n-  andl(eax, 32767);\n-  subl(eax, 16);\n-  jcc(Assembler::less, L_2TAG_PACKET_10_0_2);\n-  movl(edx, eax);\n-  andl(edx, 32752);\n-  subl(edx, 16368);\n-  movl(ecx, edx);\n-  sarl(edx, 31);\n-  addl(ecx, edx);\n-  xorl(ecx, edx);\n-  addl(ecx, 16);\n-  bsrl(ecx, ecx);\n-  movl(rsi, 0);\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_23_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 16368);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_26_0_2);\n-  xorpd(xmm0, xmm0);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_13_0_2);\n-  addl(eax, 384);\n-  cmpl(eax, 0);\n-  jcc(Assembler::less, L_2TAG_PACKET_41_0_2);\n-  mulsd(xmm5, xmm1);\n-  addsd(xmm0, xmm7);\n-  shrl(rsi, 31);\n-  addpd(xmm3, xmm0);\n-  pshufd(xmm0, xmm3, 238);\n-  addsd(xmm3, xmm0);\n-  movsd(xmm4, Address(tmp, rsi, Address::times_8, 12528));\n-  mulsd(xmm1, xmm3);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 16368);\n-  shll(rsi, 15);\n-  orl(eax, rsi);\n-  pinsrw(xmm0, eax, 3);\n-  addsd(xmm5, xmm1);\n-  movl(rsi, Address(rsp, 24));\n-  mulsd(xmm5, xmm4);\n-  addsd(xmm0, xmm5);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_41_0_2);\n-  movl(rsi, Address(rsp, 24));\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 16368);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_40_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 16368);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_42_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 16368);\n-  pinsrw(xmm0, eax, 3);\n-  movl(edx, 26);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_2TAG_PACKET_11_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movdqu(xmm2, xmm1);\n-  pextrw(eax, xmm1, 3);\n-  andl(eax, 32752);\n-  cmpl(eax, 32752);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_43_0_2);\n-  movdl(eax, xmm2);\n-  psrlq(xmm2, 20);\n-  movdl(edx, xmm2);\n-  orl(eax, edx);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_22_0_2);\n-\n-  bind(L_2TAG_PACKET_43_0_2);\n-  movdl(eax, xmm1);\n-  psrlq(xmm1, 32);\n-  movdl(edx, xmm1);\n-  movl(ecx, edx);\n-  addl(edx, edx);\n-  orl(eax, edx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_42_0_2);\n-  shrl(edx, 21);\n-  cmpl(edx, 1075);\n-  jcc(Assembler::above, L_2TAG_PACKET_44_0_2);\n-  jcc(Assembler::equal, L_2TAG_PACKET_45_0_2);\n-  cmpl(edx, 1023);\n-  jcc(Assembler::below, L_2TAG_PACKET_44_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movl(eax, 17208);\n-  xorpd(xmm3, xmm3);\n-  pinsrw(xmm3, eax, 3);\n-  movdqu(xmm4, xmm3);\n-  addsd(xmm3, xmm1);\n-  subsd(xmm4, xmm3);\n-  addsd(xmm1, xmm4);\n-  pextrw(eax, xmm1, 3);\n-  andl(eax, 32752);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_44_0_2);\n-  movdl(eax, xmm3);\n-  andl(eax, 1);\n-  jcc(Assembler::equal, L_2TAG_PACKET_44_0_2);\n-\n-  bind(L_2TAG_PACKET_46_0_2);\n-  movsd(xmm0, Address(rsp, 8));\n-  testl(ecx, INT_MIN);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_47_0_2);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_45_0_2);\n-  movsd(xmm1, Address(rsp, 16));\n-  movdl(eax, xmm1);\n-  testl(eax, 1);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_46_0_2);\n-\n-  bind(L_2TAG_PACKET_44_0_2);\n-  testl(ecx, INT_MIN);\n-  jcc(Assembler::equal, L_2TAG_PACKET_26_0_2);\n-  xorpd(xmm0, xmm0);\n-\n-  bind(L_2TAG_PACKET_47_0_2);\n-  movl(eax, 16368);\n-  xorpd(xmm1, xmm1);\n-  pinsrw(xmm1, eax, 3);\n-  divsd(xmm1, xmm0);\n-  movdqu(xmm0, xmm1);\n-  movl(edx, 27);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_2TAG_PACKET_14_0_2);\n-  movsd(xmm2, Address(rsp, 8));\n-  movsd(xmm6, Address(rsp, 16));\n-  pextrw(eax, xmm2, 3);\n-  pextrw(edx, xmm6, 3);\n-  movl(ecx, 32752);\n-  andl(ecx, edx);\n-  cmpl(ecx, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_48_0_2);\n-  andl(eax, 32752);\n-  subl(eax, 16368);\n-  xorl(edx, eax);\n-  testl(edx, 32768);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_49_0_2);\n-\n-  bind(L_2TAG_PACKET_50_0_2);\n-  movl(eax, 32736);\n-  pinsrw(xmm0, eax, 3);\n-  shrl(rsi, 16);\n-  orl(eax, rsi);\n-  pinsrw(xmm1, eax, 3);\n-  movl(rsi, Address(rsp, 24));\n-  mulsd(xmm0, xmm1);\n-\n-  bind(L_2TAG_PACKET_17_0_2);\n-  movl(edx, 24);\n-\n-  bind(L_2TAG_PACKET_21_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_6_0_2);\n-\n-  bind(L_2TAG_PACKET_49_0_2);\n-  movl(eax, 16);\n-  pinsrw(xmm0, eax, 3);\n-  mulsd(xmm0, xmm0);\n-  testl(rsi, INT_MIN);\n-  jcc(Assembler::equal, L_2TAG_PACKET_51_0_2);\n-  movsd(xmm2, Address(tmp, 12560));\n-  xorpd(xmm0, xmm2);\n-\n-  bind(L_2TAG_PACKET_51_0_2);\n-  movl(rsi, Address(rsp, 24));\n-  movl(edx, 25);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_2TAG_PACKET_16_0_2);\n-  pextrw(ecx, xmm5, 3);\n-  pextrw(edx, xmm4, 3);\n-  movl(eax, -1);\n-  andl(ecx, 32752);\n-  subl(ecx, 16368);\n-  andl(edx, 32752);\n-  addl(edx, ecx);\n-  movl(ecx, -31);\n-  sarl(edx, 4);\n-  subl(ecx, edx);\n-  jcc(Assembler::lessEqual, L_2TAG_PACKET_52_0_2);\n-  cmpl(ecx, 20);\n-  jcc(Assembler::above, L_2TAG_PACKET_53_0_2);\n-  shll(eax);\n-\n-  bind(L_2TAG_PACKET_52_0_2);\n-  movdl(xmm0, eax);\n-  psllq(xmm0, 32);\n-  pand(xmm0, xmm5);\n-  subsd(xmm5, xmm0);\n-  addsd(xmm5, xmm1);\n-  mulsd(xmm0, xmm4);\n-  mulsd(xmm5, xmm4);\n-  addsd(xmm0, xmm5);\n-\n-  bind(L_2TAG_PACKET_53_0_2);\n-  movl(edx, 25);\n-  jmp(L_2TAG_PACKET_21_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movzwl(ecx, Address(rsp, 22));\n-  movl(edx, INT_MIN);\n-  movdl(xmm1, edx);\n-  xorpd(xmm7, xmm7);\n-  paddd(xmm0, xmm4);\n-  psllq(xmm5, 32);\n-  movdl(edx, xmm0);\n-  psllq(xmm0, 29);\n-  paddq(xmm1, xmm3);\n-  pand(xmm5, xmm1);\n-  andl(ecx, 32752);\n-  cmpl(ecx, 16560);\n-  jcc(Assembler::below, L_2TAG_PACKET_3_0_2);\n-  pand(xmm0, xmm6);\n-  subsd(xmm3, xmm5);\n-  addl(eax, 16351);\n-  shrl(eax, 4);\n-  subl(eax, 1022);\n-  cvtsi2sdl(xmm7, eax);\n-  mulpd(xmm5, xmm0);\n-  movsd(xmm4, Address(tmp, 0));\n-  mulsd(xmm3, xmm0);\n-  movsd(xmm6, Address(tmp, 0));\n-  subsd(xmm5, xmm2);\n-  movsd(xmm1, Address(tmp, 8));\n-  pshufd(xmm2, xmm3, 68);\n-  unpcklpd(xmm5, xmm3);\n-  addsd(xmm3, xmm5);\n-  movsd(xmm0, Address(tmp, 8));\n-  andl(edx, 16760832);\n-  shrl(edx, 10);\n-  addpd(xmm7, Address(tmp, edx, Address::times_1, -3616));\n-  mulsd(xmm4, xmm5);\n-  mulsd(xmm0, xmm5);\n-  mulsd(xmm6, xmm2);\n-  mulsd(xmm1, xmm2);\n-  movdqu(xmm2, xmm5);\n-  mulsd(xmm4, xmm5);\n-  addsd(xmm5, xmm0);\n-  movdqu(xmm0, xmm7);\n-  addsd(xmm2, xmm3);\n-  addsd(xmm7, xmm5);\n-  mulsd(xmm6, xmm2);\n-  subsd(xmm0, xmm7);\n-  movdqu(xmm2, xmm7);\n-  addsd(xmm7, xmm4);\n-  addsd(xmm0, xmm5);\n-  subsd(xmm2, xmm7);\n-  addsd(xmm4, xmm2);\n-  pshufd(xmm2, xmm5, 238);\n-  movdqu(xmm5, xmm7);\n-  addsd(xmm7, xmm2);\n-  addsd(xmm4, xmm0);\n-  movdqu(xmm0, Address(tmp, 8272));\n-  subsd(xmm5, xmm7);\n-  addsd(xmm6, xmm4);\n-  movdqu(xmm4, xmm7);\n-  addsd(xmm5, xmm2);\n-  addsd(xmm7, xmm1);\n-  movdqu(xmm2, Address(tmp, 8336));\n-  subsd(xmm4, xmm7);\n-  addsd(xmm6, xmm5);\n-  addsd(xmm4, xmm1);\n-  pshufd(xmm5, xmm7, 238);\n-  movdqu(xmm1, xmm7);\n-  addsd(xmm7, xmm5);\n-  subsd(xmm1, xmm7);\n-  addsd(xmm1, xmm5);\n-  movdqu(xmm5, Address(tmp, 8352));\n-  pshufd(xmm3, xmm3, 68);\n-  addsd(xmm6, xmm4);\n-  addsd(xmm6, xmm1);\n-  movdqu(xmm1, Address(tmp, 8304));\n-  mulpd(xmm0, xmm3);\n-  mulpd(xmm2, xmm3);\n-  pshufd(xmm4, xmm3, 68);\n-  mulpd(xmm3, xmm3);\n-  addpd(xmm0, xmm1);\n-  addpd(xmm5, xmm2);\n-  mulsd(xmm4, xmm3);\n-  movsd(xmm2, Address(tmp, 16));\n-  mulpd(xmm3, xmm3);\n-  movsd(xmm1, Address(rsp, 16));\n-  movzwl(ecx, Address(rsp, 22));\n-  mulpd(xmm0, xmm4);\n-  pextrw(eax, xmm7, 3);\n-  mulpd(xmm5, xmm4);\n-  mulpd(xmm0, xmm3);\n-  movsd(xmm4, Address(tmp, 8376));\n-  pand(xmm2, xmm7);\n-  addsd(xmm5, xmm6);\n-  subsd(xmm7, xmm2);\n-  addpd(xmm5, xmm0);\n-  andl(eax, 32752);\n-  subl(eax, 16368);\n-  andl(ecx, 32752);\n-  cmpl(ecx, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_48_0_2);\n-  addl(ecx, eax);\n-  cmpl(ecx, 16576);\n-  jcc(Assembler::aboveEqual, L_2TAG_PACKET_54_0_2);\n-  pshufd(xmm0, xmm5, 238);\n-  pand(xmm4, xmm1);\n-  movdqu(xmm3, xmm1);\n-  addsd(xmm5, xmm0);\n-  subsd(xmm1, xmm4);\n-  xorpd(xmm6, xmm6);\n-  movl(edx, 17080);\n-  pinsrw(xmm6, edx, 3);\n-  addsd(xmm7, xmm5);\n-  mulsd(xmm4, xmm2);\n-  mulsd(xmm1, xmm2);\n-  movdqu(xmm5, xmm6);\n-  mulsd(xmm3, xmm7);\n-  addsd(xmm6, xmm4);\n-  addsd(xmm1, xmm3);\n-  movdqu(xmm7, Address(tmp, 12480));\n-  movdl(edx, xmm6);\n-  subsd(xmm6, xmm5);\n-  movdqu(xmm3, Address(tmp, 12496));\n-  movsd(xmm2, Address(tmp, 12512));\n-  subsd(xmm4, xmm6);\n-  movl(ecx, edx);\n-  andl(edx, 255);\n-  addl(edx, edx);\n-  movdqu(xmm5, Address(tmp, edx, Address::times_8, 8384));\n-  addsd(xmm4, xmm1);\n-  pextrw(edx, xmm6, 3);\n-  shrl(ecx, 8);\n-  movl(eax, ecx);\n-  shrl(ecx, 1);\n-  subl(eax, ecx);\n-  shll(ecx, 20);\n-  movdl(xmm6, ecx);\n-  pshufd(xmm0, xmm4, 68);\n-  pshufd(xmm1, xmm4, 68);\n-  mulpd(xmm0, xmm0);\n-  mulpd(xmm7, xmm1);\n-  pshufd(xmm6, xmm6, 17);\n-  mulsd(xmm2, xmm4);\n-  andl(edx, 32767);\n-  cmpl(edx, 16529);\n-  jcc(Assembler::above, L_2TAG_PACKET_14_0_2);\n-  mulsd(xmm0, xmm0);\n-  paddd(xmm5, xmm6);\n-  addpd(xmm3, xmm7);\n-  mulsd(xmm2, xmm5);\n-  pshufd(xmm6, xmm5, 238);\n-  mulpd(xmm0, xmm3);\n-  addsd(xmm2, xmm6);\n-  pshufd(xmm3, xmm0, 238);\n-  addl(eax, 1023);\n-  shll(eax, 20);\n-  orl(eax, rsi);\n-  movdl(xmm4, eax);\n-  mulsd(xmm0, xmm5);\n-  mulsd(xmm3, xmm5);\n-  addsd(xmm0, xmm2);\n-  psllq(xmm4, 32);\n-  addsd(xmm0, xmm3);\n-  movdqu(xmm1, xmm0);\n-  addsd(xmm0, xmm5);\n-  movl(rsi, Address(rsp, 24));\n-  mulsd(xmm0, xmm4);\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_16_0_2);\n-  cmpl(eax, 32752);\n-  jcc(Assembler::equal, L_2TAG_PACKET_17_0_2);\n-\n-  bind(L_2TAG_PACKET_55_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_6_0_2);\n-\n-  bind(L_2TAG_PACKET_48_0_2);\n-  movl(rsi, Address(rsp, 24));\n-\n-  bind(L_2TAG_PACKET_56_0_2);\n-  movsd(xmm0, Address(rsp, 8));\n-  movsd(xmm1, Address(rsp, 16));\n-  addsd(xmm1, xmm1);\n-  xorpd(xmm2, xmm2);\n-  movl(eax, 49136);\n-  pinsrw(xmm2, eax, 3);\n-  addsd(xmm2, xmm0);\n-  pextrw(eax, xmm2, 3);\n-  cmpl(eax, 0);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_57_0_2);\n-  xorpd(xmm0, xmm0);\n-  movl(eax, 32760);\n-  pinsrw(xmm0, eax, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_57_0_2);\n-  movdl(edx, xmm1);\n-  movdqu(xmm3, xmm1);\n-  psrlq(xmm3, 20);\n-  movdl(ecx, xmm3);\n-  orl(ecx, edx);\n-  jcc(Assembler::equal, L_2TAG_PACKET_58_0_2);\n-  addsd(xmm1, xmm1);\n-  movdqu(xmm0, xmm1);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_58_0_2);\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32752);\n-  pextrw(edx, xmm1, 3);\n-  xorpd(xmm0, xmm0);\n-  subl(eax, 16368);\n-  xorl(eax, edx);\n-  testl(eax, 32768);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_18_0_2);\n-  movl(edx, 32752);\n-  pinsrw(xmm0, edx, 3);\n-  jmp(L_2TAG_PACKET_18_0_2);\n-\n-  bind(L_2TAG_PACKET_54_0_2);\n-  pextrw(eax, xmm1, 3);\n-  pextrw(ecx, xmm2, 3);\n-  xorl(eax, ecx);\n-  testl(eax, 32768);\n-  jcc(Assembler::equal, L_2TAG_PACKET_50_0_2);\n-  jmp(L_2TAG_PACKET_49_0_2);\n-\n-  bind(L_2TAG_PACKET_6_0_2);\n-  movl(tmp, Address(rsp, 64));\n-\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_pow.cpp","additions":0,"deletions":1856,"binary":false,"changes":1856,"status":"deleted"},{"patch":"@@ -1,1743 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"stubRoutines_x86.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION - SIN()\n-\/\/                     ---------------------\n-\/\/\n-\/\/     1. RANGE REDUCTION\n-\/\/\n-\/\/     We perform an initial range reduction from X to r with\n-\/\/\n-\/\/          X =~= N * pi\/32 + r\n-\/\/\n-\/\/     so that |r| <= pi\/64 + epsilon. We restrict inputs to those\n-\/\/     where |N| <= 932560. Beyond this, the range reduction is\n-\/\/     insufficiently accurate. For extremely small inputs,\n-\/\/     denormalization can occur internally, impacting performance.\n-\/\/     This means that the main path is actually only taken for\n-\/\/     2^-252 <= |X| < 90112.\n-\/\/\n-\/\/     To avoid branches, we perform the range reduction to full\n-\/\/     accuracy each time.\n-\/\/\n-\/\/          X - N * (P_1 + P_2 + P_3)\n-\/\/\n-\/\/     where P_1 and P_2 are 32-bit numbers (so multiplication by N\n-\/\/     is exact) and P_3 is a 53-bit number. Together, these\n-\/\/     approximate pi well enough for all cases in the restricted\n-\/\/     range.\n-\/\/\n-\/\/     The main reduction sequence is:\n-\/\/\n-\/\/             y = 32\/pi * x\n-\/\/             N = integer(y)\n-\/\/     (computed by adding and subtracting off SHIFTER)\n-\/\/\n-\/\/             m_1 = N * P_1\n-\/\/             m_2 = N * P_2\n-\/\/             r_1 = x - m_1\n-\/\/             r = r_1 - m_2\n-\/\/     (this r can be used for most of the calculation)\n-\/\/\n-\/\/             c_1 = r_1 - r\n-\/\/             m_3 = N * P_3\n-\/\/             c_2 = c_1 - m_2\n-\/\/             c = c_2 - m_3\n-\/\/\n-\/\/     2. MAIN ALGORITHM\n-\/\/\n-\/\/     The algorithm uses a table lookup based on B = M * pi \/ 32\n-\/\/     where M = N mod 64. The stored values are:\n-\/\/       sigma             closest power of 2 to cos(B)\n-\/\/       C_hl              53-bit cos(B) - sigma\n-\/\/       S_hi + S_lo       2 * 53-bit sin(B)\n-\/\/\n-\/\/     The computation is organized as follows:\n-\/\/\n-\/\/          sin(B + r + c) = [sin(B) + sigma * r] +\n-\/\/                           r * (cos(B) - sigma) +\n-\/\/                           sin(B) * [cos(r + c) - 1] +\n-\/\/                           cos(B) * [sin(r + c) - r]\n-\/\/\n-\/\/     which is approximately:\n-\/\/\n-\/\/          [S_hi + sigma * r] +\n-\/\/          C_hl * r +\n-\/\/          S_lo + S_hi * [(cos(r) - 1) - r * c] +\n-\/\/          (C_hl + sigma) * [(sin(r) - r) + c]\n-\/\/\n-\/\/     and this is what is actually computed. We separate this sum\n-\/\/     into four parts:\n-\/\/\n-\/\/          hi + med + pols + corr\n-\/\/\n-\/\/     where\n-\/\/\n-\/\/          hi       = S_hi + sigma r\n-\/\/          med      = C_hl * r\n-\/\/          pols     = S_hi * (cos(r) - 1) + (C_hl + sigma) * (sin(r) - r)\n-\/\/          corr     = S_lo + c * ((C_hl + sigma) - S_hi * r)\n-\/\/\n-\/\/     3. POLYNOMIAL\n-\/\/\n-\/\/     The polynomial S_hi * (cos(r) - 1) + (C_hl + sigma) *\n-\/\/     (sin(r) - r) can be rearranged freely, since it is quite\n-\/\/     small, so we exploit parallelism to the fullest.\n-\/\/\n-\/\/          psc4       =   SC_4 * r_1\n-\/\/          msc4       =   psc4 * r\n-\/\/          r2         =   r * r\n-\/\/          msc2       =   SC_2 * r2\n-\/\/          r4         =   r2 * r2\n-\/\/          psc3       =   SC_3 + msc4\n-\/\/          psc1       =   SC_1 + msc2\n-\/\/          msc3       =   r4 * psc3\n-\/\/          sincospols =   psc1 + msc3\n-\/\/          pols       =   sincospols *\n-\/\/                         <S_hi * r^2 | (C_hl + sigma) * r^3>\n-\/\/\n-\/\/     4. CORRECTION TERM\n-\/\/\n-\/\/     This is where the \"c\" component of the range reduction is\n-\/\/     taken into account; recall that just \"r\" is used for most of\n-\/\/     the calculation.\n-\/\/\n-\/\/          -c   = m_3 - c_2\n-\/\/          -d   = S_hi * r - (C_hl + sigma)\n-\/\/          corr = -c * -d + S_lo\n-\/\/\n-\/\/     5. COMPENSATED SUMMATIONS\n-\/\/\n-\/\/     The two successive compensated summations add up the high\n-\/\/     and medium parts, leaving just the low parts to add up at\n-\/\/     the end.\n-\/\/\n-\/\/          rs        =  sigma * r\n-\/\/          res_int   =  S_hi + rs\n-\/\/          k_0       =  S_hi - res_int\n-\/\/          k_2       =  k_0 + rs\n-\/\/          med       =  C_hl * r\n-\/\/          res_hi    =  res_int + med\n-\/\/          k_1       =  res_int - res_hi\n-\/\/          k_3       =  k_1 + med\n-\/\/\n-\/\/     6. FINAL SUMMATION\n-\/\/\n-\/\/     We now add up all the small parts:\n-\/\/\n-\/\/          res_lo = pols(hi) + pols(lo) + corr + k_1 + k_3\n-\/\/\n-\/\/     Now the overall result is just:\n-\/\/\n-\/\/          res_hi + res_lo\n-\/\/\n-\/\/     7. SMALL ARGUMENTS\n-\/\/\n-\/\/     If |x| < SNN (SNN meaning the smallest normal number), we\n-\/\/     simply perform 0.1111111 cdots 1111 * x. For SNN <= |x|, we\n-\/\/     do 2^-55 * (2^55 * x - x).\n-\/\/\n-\/\/ Special cases:\n-\/\/  sin(NaN) = quiet NaN, and raise invalid exception\n-\/\/  sin(INF) = NaN and raise invalid exception\n-\/\/  sin(+\/-0) = +\/-0\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-ATTRIBUTE_ALIGNED(8) static const juint _zero_none[] =\n-{\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0xbff00000UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint __4onpi_d[] =\n-{\n-    0x6dc9c883UL, 0x3ff45f30UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _TWO_32H[] =\n-{\n-    0x00000000UL, 0x41f80000UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _pi04_3d[] =\n-{\n-    0x54442d00UL, 0x3fe921fbUL, 0x98cc5180UL, 0x3ce84698UL, 0xcbb5bf6cUL,\n-    0xb9dfc8f8UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _pi04_5d[] =\n-{\n-    0x54400000UL, 0x3fe921fbUL, 0x1a600000UL, 0x3dc0b461UL, 0x2e000000UL,\n-    0x3b93198aUL, 0x25200000UL, 0x396b839aUL, 0x533e63a0UL, 0x37027044UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _SCALE[] =\n-{\n-    0x00000000UL, 0x32600000UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _zeros[] =\n-{\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x80000000UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _pi04_2d[] =\n-{\n-    0x54400000UL, 0x3fe921fbUL, 0x1a626331UL, 0x3dc0b461UL\n-};\n-\n-ATTRIBUTE_ALIGNED(4) static const juint _TWO_12H[] =\n-{\n-    0x00000000UL, 0x40b80000UL\n-};\n-\n-ATTRIBUTE_ALIGNED(2) static const jushort __4onpi_31l[] =\n-{\n-    0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x0000, 0x836e, 0xa2f9,\n-    0x40d8, 0x0000, 0x0000, 0x0000, 0x2a50, 0x9c88, 0x40b7, 0x0000, 0x0000, 0x0000,\n-    0xabe8, 0xfe13, 0x4099, 0x0000, 0x0000, 0x0000, 0x6ee0, 0xfa9a, 0x4079, 0x0000,\n-    0x0000, 0x0000, 0x9580, 0xdb62, 0x4058, 0x0000, 0x0000, 0x0000, 0x1c82, 0xc9e2,\n-    0x403d, 0x0000, 0x0000, 0x0000, 0xb1c0, 0xff28, 0x4019, 0x0000, 0x0000, 0x0000,\n-    0xef14, 0xaf7a, 0x3ffe, 0x0000, 0x0000, 0x0000, 0x48dc, 0xc36e, 0x3fdf, 0x0000,\n-    0x0000, 0x0000, 0x3740, 0xe909, 0x3fbe, 0x0000, 0x0000, 0x0000, 0x924a, 0xb801,\n-    0x3fa2, 0x0000, 0x0000, 0x0000, 0x3a32, 0xdd41, 0x3f83, 0x0000, 0x0000, 0x0000,\n-    0x8778, 0x873f, 0x3f62, 0x0000, 0x0000, 0x0000, 0x1298, 0xb1cb, 0x3f44, 0x0000,\n-    0x0000, 0x0000, 0xa208, 0x9cfb, 0x3f26, 0x0000, 0x0000, 0x0000, 0xbaec, 0xd7d4,\n-    0x3f06, 0x0000, 0x0000, 0x0000, 0xd338, 0x8909, 0x3ee7, 0x0000, 0x0000, 0x0000,\n-    0x68b8, 0xe04d, 0x3ec7, 0x0000, 0x0000, 0x0000, 0x4e64, 0xdf90, 0x3eaa, 0x0000,\n-    0x0000, 0x0000, 0xc1a8, 0xeb1c, 0x3e89, 0x0000, 0x0000, 0x0000, 0x2720, 0xce7d,\n-    0x3e6a, 0x0000, 0x0000, 0x0000, 0x77b8, 0x8bf1, 0x3e4b, 0x0000, 0x0000, 0x0000,\n-    0xec7e, 0xe4a0, 0x3e2e, 0x0000, 0x0000, 0x0000, 0xffbc, 0xf12f, 0x3e0f, 0x0000,\n-    0x0000, 0x0000, 0xfdc0, 0xb301, 0x3deb, 0x0000, 0x0000, 0x0000, 0xc5ac, 0x9788,\n-    0x3dd1, 0x0000, 0x0000, 0x0000, 0x47da, 0x829b, 0x3db2, 0x0000, 0x0000, 0x0000,\n-    0xd9e4, 0xa6cf, 0x3d93, 0x0000, 0x0000, 0x0000, 0x36e8, 0xf961, 0x3d73, 0x0000,\n-    0x0000, 0x0000, 0xf668, 0xf463, 0x3d54, 0x0000, 0x0000, 0x0000, 0x5168, 0xf2ff,\n-    0x3d35, 0x0000, 0x0000, 0x0000, 0x758e, 0xea4f, 0x3d17, 0x0000, 0x0000, 0x0000,\n-    0xf17a, 0xebe5, 0x3cf8, 0x0000, 0x0000, 0x0000, 0x9cfa, 0x9e83, 0x3cd9, 0x0000,\n-    0x0000, 0x0000, 0xa4ba, 0xe294, 0x3cba, 0x0000, 0x0000, 0x0000, 0xd7ec, 0x9afe,\n-    0x3c9a, 0x0000, 0x0000, 0x0000, 0xae80, 0x8fc6, 0x3c79, 0x0000, 0x0000, 0x0000,\n-    0x3304, 0x8560, 0x3c5c, 0x0000, 0x0000, 0x0000, 0x6d70, 0xdf8f, 0x3c3b, 0x0000,\n-    0x0000, 0x0000, 0x3ef0, 0xafc3, 0x3c1e, 0x0000, 0x0000, 0x0000, 0xd0d8, 0x826b,\n-    0x3bfe, 0x0000, 0x0000, 0x0000, 0x1c80, 0xed4f, 0x3bdd, 0x0000, 0x0000, 0x0000,\n-    0x730c, 0xb0af, 0x3bc1, 0x0000, 0x0000, 0x0000, 0x6660, 0xc219, 0x3ba2, 0x0000,\n-    0x0000, 0x0000, 0x940c, 0xabe2, 0x3b83, 0x0000, 0x0000, 0x0000, 0xdffc, 0x8408,\n-    0x3b64, 0x0000, 0x0000, 0x0000, 0x6b98, 0xc402, 0x3b45, 0x0000, 0x0000, 0x0000,\n-    0x1818, 0x9cc4, 0x3b26, 0x0000, 0x0000, 0x0000, 0x5390, 0xaab6, 0x3b05, 0x0000,\n-    0x0000, 0x0000, 0xb070, 0xd464, 0x3ae9, 0x0000, 0x0000, 0x0000, 0x231a, 0x9ef0,\n-    0x3aca, 0x0000, 0x0000, 0x0000, 0x0670, 0xd1f1, 0x3aaa, 0x0000, 0x0000, 0x0000,\n-    0x7738, 0xd9f3, 0x3a8a, 0x0000, 0x0000, 0x0000, 0xa834, 0x8092, 0x3a6c, 0x0000,\n-    0x0000, 0x0000, 0xb45c, 0xce23, 0x3a4d, 0x0000, 0x0000, 0x0000, 0x36e8, 0xb0e5,\n-    0x3a2d, 0x0000, 0x0000, 0x0000, 0xd156, 0xaf44, 0x3a10, 0x0000, 0x0000, 0x0000,\n-    0x9f52, 0x8c82, 0x39f1, 0x0000, 0x0000, 0x0000, 0x829c, 0xff83, 0x39d1, 0x0000,\n-    0x0000, 0x0000, 0x7d06, 0xefc6, 0x39b3, 0x0000, 0x0000, 0x0000, 0x93e0, 0xb0b7,\n-    0x3992, 0x0000, 0x0000, 0x0000, 0xedde, 0xc193, 0x3975, 0x0000, 0x0000, 0x0000,\n-    0xbbc0, 0xcf49, 0x3952, 0x0000, 0x0000, 0x0000, 0xbdf0, 0xd63c, 0x3937, 0x0000,\n-    0x0000, 0x0000, 0x1f34, 0x9f3a, 0x3918, 0x0000, 0x0000, 0x0000, 0x3f8e, 0xe579,\n-    0x38f9, 0x0000, 0x0000, 0x0000, 0x90c8, 0xc3f8, 0x38d9, 0x0000, 0x0000, 0x0000,\n-    0x48c0, 0xf8f8, 0x38b7, 0x0000, 0x0000, 0x0000, 0xed56, 0xafa6, 0x389c, 0x0000,\n-    0x0000, 0x0000, 0x8218, 0xb969, 0x387d, 0x0000, 0x0000, 0x0000, 0x1852, 0xec57,\n-    0x385e, 0x0000, 0x0000, 0x0000, 0x670c, 0xd674, 0x383e, 0x0000, 0x0000, 0x0000,\n-    0xad40, 0xc2c4, 0x3820, 0x0000, 0x0000, 0x0000, 0x2e80, 0xa696, 0x3801, 0x0000,\n-    0x0000, 0x0000, 0xd800, 0xc467, 0x37dc, 0x0000, 0x0000, 0x0000, 0x3c72, 0xc5ae,\n-    0x37c3, 0x0000, 0x0000, 0x0000, 0xb006, 0xac69, 0x37a4, 0x0000, 0x0000, 0x0000,\n-    0x34a0, 0x8cdf, 0x3782, 0x0000, 0x0000, 0x0000, 0x9ed2, 0xd25e, 0x3766, 0x0000,\n-    0x0000, 0x0000, 0x6fec, 0xaaaa, 0x3747, 0x0000, 0x0000, 0x0000, 0x6040, 0xfb5c,\n-    0x3726, 0x0000, 0x0000, 0x0000, 0x764c, 0xa3fc, 0x3708, 0x0000, 0x0000, 0x0000,\n-    0xb254, 0x954e, 0x36e9, 0x0000, 0x0000, 0x0000, 0x3e1c, 0xf5dc, 0x36ca, 0x0000,\n-    0x0000, 0x0000, 0x7b06, 0xc635, 0x36ac, 0x0000, 0x0000, 0x0000, 0xa8ba, 0xd738,\n-    0x368d, 0x0000, 0x0000, 0x0000, 0x06cc, 0xb24e, 0x366d, 0x0000, 0x0000, 0x0000,\n-    0x7108, 0xac76, 0x364f, 0x0000, 0x0000, 0x0000, 0x2324, 0xa7cb, 0x3630, 0x0000,\n-    0x0000, 0x0000, 0xac40, 0xef15, 0x360f, 0x0000, 0x0000, 0x0000, 0xae46, 0xd516,\n-    0x35f2, 0x0000, 0x0000, 0x0000, 0x615e, 0xe003, 0x35d3, 0x0000, 0x0000, 0x0000,\n-    0x0cf0, 0xefe7, 0x35b1, 0x0000, 0x0000, 0x0000, 0xfb50, 0xf98c, 0x3595, 0x0000,\n-    0x0000, 0x0000, 0x0abc, 0xf333, 0x3575, 0x0000, 0x0000, 0x0000, 0xdd60, 0xca3f,\n-    0x3555, 0x0000, 0x0000, 0x0000, 0x7eb6, 0xd87f, 0x3538, 0x0000, 0x0000, 0x0000,\n-    0x44f4, 0xb291, 0x3519, 0x0000, 0x0000, 0x0000, 0xff80, 0xc982, 0x34f6, 0x0000,\n-    0x0000, 0x0000, 0x9de0, 0xd9b8, 0x34db, 0x0000, 0x0000, 0x0000, 0xcd42, 0x9366,\n-    0x34bc, 0x0000, 0x0000, 0x0000, 0xbef0, 0xfaee, 0x349d, 0x0000, 0x0000, 0x0000,\n-    0xdac4, 0xb6f1, 0x347d, 0x0000, 0x0000, 0x0000, 0xf140, 0x94de, 0x345d, 0x0000,\n-    0x0000, 0x0000, 0xa218, 0x8b4b, 0x343e, 0x0000, 0x0000, 0x0000, 0x6380, 0xa135,\n-    0x341e, 0x0000, 0x0000, 0x0000, 0xb184, 0x8cb2, 0x3402, 0x0000, 0x0000, 0x0000,\n-    0x196e, 0xdc61, 0x33e3, 0x0000, 0x0000, 0x0000, 0x0c00, 0xde05, 0x33c4, 0x0000,\n-    0x0000, 0x0000, 0xef9a, 0xbd38, 0x33a5, 0x0000, 0x0000, 0x0000, 0xc1a0, 0xdf00,\n-    0x3385, 0x0000, 0x0000, 0x0000, 0x1090, 0x9973, 0x3365, 0x0000, 0x0000, 0x0000,\n-    0x4882, 0x8301, 0x3348, 0x0000, 0x0000, 0x0000, 0x7abe, 0xadc7, 0x3329, 0x0000,\n-    0x0000, 0x0000, 0x7cba, 0xec2b, 0x330a, 0x0000, 0x0000, 0x0000, 0xa520, 0x8f21,\n-    0x32e9, 0x0000, 0x0000, 0x0000, 0x710c, 0x8d36, 0x32cc, 0x0000, 0x0000, 0x0000,\n-    0x5212, 0xc6ed, 0x32ad, 0x0000, 0x0000, 0x0000, 0x7308, 0xfd76, 0x328d, 0x0000,\n-    0x0000, 0x0000, 0x5014, 0xd548, 0x326f, 0x0000, 0x0000, 0x0000, 0xd3f2, 0xb499,\n-    0x3250, 0x0000, 0x0000, 0x0000, 0x7f74, 0xa606, 0x3230, 0x0000, 0x0000, 0x0000,\n-    0xf0a8, 0xd720, 0x3212, 0x0000, 0x0000, 0x0000, 0x185c, 0xe20f, 0x31f2, 0x0000,\n-    0x0000, 0x0000, 0xa5a8, 0x8738, 0x31d4, 0x0000, 0x0000, 0x0000, 0xdd74, 0xcafb,\n-    0x31b4, 0x0000, 0x0000, 0x0000, 0x98b6, 0xbd8e, 0x3196, 0x0000, 0x0000, 0x0000,\n-    0xe9de, 0x977f, 0x3177, 0x0000, 0x0000, 0x0000, 0x67c0, 0x818d, 0x3158, 0x0000,\n-    0x0000, 0x0000, 0xe52a, 0x9322, 0x3139, 0x0000, 0x0000, 0x0000, 0xe568, 0x9b6c,\n-    0x3119, 0x0000, 0x0000, 0x0000, 0x2358, 0xaa0a, 0x30fa, 0x0000, 0x0000, 0x0000,\n-    0xe480, 0xe13b, 0x30d9, 0x0000, 0x0000, 0x0000, 0x3024, 0x90a1, 0x30bd, 0x0000,\n-    0x0000, 0x0000, 0x9620, 0xda30, 0x309d, 0x0000, 0x0000, 0x0000, 0x898a, 0xb388,\n-    0x307f, 0x0000, 0x0000, 0x0000, 0xb24c, 0xc891, 0x3060, 0x0000, 0x0000, 0x0000,\n-    0x8056, 0xf98b, 0x3041, 0x0000, 0x0000, 0x0000, 0x72a4, 0xa1ea, 0x3021, 0x0000,\n-    0x0000, 0x0000, 0x6af8, 0x9488, 0x3001, 0x0000, 0x0000, 0x0000, 0xe00c, 0xdfcb,\n-    0x2fe4, 0x0000, 0x0000, 0x0000, 0xeeec, 0xc941, 0x2fc4, 0x0000, 0x0000, 0x0000,\n-    0x53e0, 0xe70f, 0x2fa4, 0x0000, 0x0000, 0x0000, 0x8f60, 0x9c07, 0x2f85, 0x0000,\n-    0x0000, 0x0000, 0xb328, 0xc3e7, 0x2f68, 0x0000, 0x0000, 0x0000, 0x9404, 0xf8c7,\n-    0x2f48, 0x0000, 0x0000, 0x0000, 0x38e0, 0xc99f, 0x2f29, 0x0000, 0x0000, 0x0000,\n-    0x9778, 0xd984, 0x2f09, 0x0000, 0x0000, 0x0000, 0xe700, 0xd142, 0x2eea, 0x0000,\n-    0x0000, 0x0000, 0xd904, 0x9443, 0x2ecd, 0x0000, 0x0000, 0x0000, 0xd4ba, 0xae7e,\n-    0x2eae, 0x0000, 0x0000, 0x0000, 0x8e5e, 0x8524, 0x2e8f, 0x0000, 0x0000, 0x0000,\n-    0xb550, 0xc9ed, 0x2e6e, 0x0000, 0x0000, 0x0000, 0x53b8, 0x8648, 0x2e51, 0x0000,\n-    0x0000, 0x0000, 0xdae4, 0x87f9, 0x2e32, 0x0000, 0x0000, 0x0000, 0x2942, 0xd966,\n-    0x2e13, 0x0000, 0x0000, 0x0000, 0x4f28, 0xcf3c, 0x2df3, 0x0000, 0x0000, 0x0000,\n-    0xfa40, 0xc4ef, 0x2dd1, 0x0000, 0x0000, 0x0000, 0x4424, 0xbca7, 0x2db5, 0x0000,\n-    0x0000, 0x0000, 0x2e62, 0xcdc5, 0x2d97, 0x0000, 0x0000, 0x0000, 0xed88, 0x996b,\n-    0x2d78, 0x0000, 0x0000, 0x0000, 0x7c30, 0xd97d, 0x2d56, 0x0000, 0x0000, 0x0000,\n-    0xed26, 0xbf6e, 0x2d3a, 0x0000, 0x0000, 0x0000, 0x2918, 0x921b, 0x2d1a, 0x0000,\n-    0x0000, 0x0000, 0x4e24, 0xe84e, 0x2cfb, 0x0000, 0x0000, 0x0000, 0x6dc0, 0x92ec,\n-    0x2cdd, 0x0000, 0x0000, 0x0000, 0x4f2c, 0xacf8, 0x2cbd, 0x0000, 0x0000, 0x0000,\n-    0xc634, 0xf094, 0x2c9e, 0x0000, 0x0000, 0x0000, 0xdc70, 0xe5d3, 0x2c7e, 0x0000,\n-    0x0000, 0x0000, 0x2180, 0xa600, 0x2c5b, 0x0000, 0x0000, 0x0000, 0x8480, 0xd680,\n-    0x2c3c, 0x0000, 0x0000, 0x0000, 0x8b24, 0xd63b, 0x2c22, 0x0000, 0x0000, 0x0000,\n-    0x02e0, 0xaa47, 0x2c00, 0x0000, 0x0000, 0x0000, 0x9ad0, 0xee84, 0x2be3, 0x0000,\n-    0x0000, 0x0000, 0xf7dc, 0xf699, 0x2bc6, 0x0000, 0x0000, 0x0000, 0xddde, 0xe490,\n-    0x2ba7, 0x0000, 0x0000, 0x0000, 0x34a0, 0xb4fd, 0x2b85, 0x0000, 0x0000, 0x0000,\n-    0x91b4, 0x8ef6, 0x2b68, 0x0000, 0x0000, 0x0000, 0xa3e0, 0xa2a7, 0x2b47, 0x0000,\n-    0x0000, 0x0000, 0xcce4, 0x82b3, 0x2b2a, 0x0000, 0x0000, 0x0000, 0xe4be, 0x8207,\n-    0x2b0c, 0x0000, 0x0000, 0x0000, 0x1d92, 0xab43, 0x2aed, 0x0000, 0x0000, 0x0000,\n-    0xe818, 0xf9f6, 0x2acd, 0x0000, 0x0000, 0x0000, 0xff12, 0xba80, 0x2aaf, 0x0000,\n-    0x0000, 0x0000, 0x5254, 0x8529, 0x2a90, 0x0000, 0x0000, 0x0000, 0x1b88, 0xe032,\n-    0x2a71, 0x0000, 0x0000, 0x0000, 0x3248, 0xd86d, 0x2a50, 0x0000, 0x0000, 0x0000,\n-    0x3140, 0xc9d5, 0x2a2e, 0x0000, 0x0000, 0x0000, 0x14e6, 0xbd47, 0x2a14, 0x0000,\n-    0x0000, 0x0000, 0x5c10, 0xe544, 0x29f4, 0x0000, 0x0000, 0x0000, 0x9f50, 0x90b6,\n-    0x29d4, 0x0000, 0x0000, 0x0000, 0x9850, 0xab55, 0x29b6, 0x0000, 0x0000, 0x0000,\n-    0x2750, 0x9d07, 0x2998, 0x0000, 0x0000, 0x0000, 0x6700, 0x8bbb, 0x2973, 0x0000,\n-    0x0000, 0x0000, 0x5dba, 0xed31, 0x295a, 0x0000, 0x0000, 0x0000, 0x61dc, 0x85fe,\n-    0x293a, 0x0000, 0x0000, 0x0000, 0x9ba2, 0xd6b4, 0x291c, 0x0000, 0x0000, 0x0000,\n-    0x2d30, 0xe3a5, 0x28fb, 0x0000, 0x0000, 0x0000, 0x6630, 0xb566, 0x28dd, 0x0000,\n-    0x0000, 0x0000, 0x5ad4, 0xa829, 0x28bf, 0x0000, 0x0000, 0x0000, 0x89d8, 0xe290,\n-    0x28a0, 0x0000, 0x0000, 0x0000, 0x3916, 0xc428, 0x2881, 0x0000, 0x0000, 0x0000,\n-    0x0490, 0xbea4, 0x2860, 0x0000, 0x0000, 0x0000, 0xee06, 0x80ee, 0x2843, 0x0000,\n-    0x0000, 0x0000, 0xfc00, 0xf327, 0x2820, 0x0000, 0x0000, 0x0000, 0xea40, 0xa871,\n-    0x2800, 0x0000, 0x0000, 0x0000, 0x63d8, 0x9c26, 0x27e4, 0x0000, 0x0000, 0x0000,\n-    0x07ba, 0xc0c9, 0x27c7, 0x0000, 0x0000, 0x0000, 0x3fa2, 0x9797, 0x27a8, 0x0000,\n-    0x0000, 0x0000, 0x21c6, 0xfeca, 0x2789, 0x0000, 0x0000, 0x0000, 0xde40, 0x860d,\n-    0x2768, 0x0000, 0x0000, 0x0000, 0x9cc8, 0x98ce, 0x2749, 0x0000, 0x0000, 0x0000,\n-    0x3778, 0xa31c, 0x272a, 0x0000, 0x0000, 0x0000, 0xe778, 0xf6e2, 0x270b, 0x0000,\n-    0x0000, 0x0000, 0x59b8, 0xf841, 0x26ed, 0x0000, 0x0000, 0x0000, 0x02e0, 0xad04,\n-    0x26cd, 0x0000, 0x0000, 0x0000, 0x5a92, 0x9380, 0x26b0, 0x0000, 0x0000, 0x0000,\n-    0xc740, 0x8886, 0x268d, 0x0000, 0x0000, 0x0000, 0x0680, 0xfaf8, 0x266c, 0x0000,\n-    0x0000, 0x0000, 0xfb60, 0x897f, 0x2653, 0x0000, 0x0000, 0x0000, 0x8760, 0xf903,\n-    0x2634, 0x0000, 0x0000, 0x0000, 0xad2a, 0xc2c8, 0x2615, 0x0000, 0x0000, 0x0000,\n-    0x2d86, 0x8aef, 0x25f6, 0x0000, 0x0000, 0x0000, 0x1ef4, 0xe627, 0x25d6, 0x0000,\n-    0x0000, 0x0000, 0x09e4, 0x8020, 0x25b7, 0x0000, 0x0000, 0x0000, 0x7548, 0xd227,\n-    0x2598, 0x0000, 0x0000, 0x0000, 0x75dc, 0xfb5b, 0x2579, 0x0000, 0x0000, 0x0000,\n-    0xea84, 0xc8b6, 0x255a, 0x0000, 0x0000, 0x0000, 0xe4d0, 0x8145, 0x253b, 0x0000,\n-    0x0000, 0x0000, 0x3640, 0x9768, 0x251c, 0x0000, 0x0000, 0x0000, 0x246a, 0xccec,\n-    0x24fe, 0x0000, 0x0000, 0x0000, 0x51d0, 0xa075, 0x24dd, 0x0000, 0x0000, 0x0000,\n-    0x4638, 0xa385, 0x24bf, 0x0000, 0x0000, 0x0000, 0xd788, 0xd776, 0x24a1, 0x0000,\n-    0x0000, 0x0000, 0x1370, 0x8997, 0x2482, 0x0000, 0x0000, 0x0000, 0x1e88, 0x9b67,\n-    0x2462, 0x0000, 0x0000, 0x0000, 0x6c08, 0xd975, 0x2444, 0x0000, 0x0000, 0x0000,\n-    0xfdb0, 0xcfc0, 0x2422, 0x0000, 0x0000, 0x0000, 0x3100, 0xc026, 0x2406, 0x0000,\n-    0x0000, 0x0000, 0xc5b4, 0xae64, 0x23e6, 0x0000, 0x0000, 0x0000, 0x2280, 0xf687,\n-    0x23c3, 0x0000, 0x0000, 0x0000, 0x2de0, 0x9006, 0x23a9, 0x0000, 0x0000, 0x0000,\n-    0x24bc, 0xf631, 0x238a, 0x0000, 0x0000, 0x0000, 0xb8d4, 0xa975, 0x236b, 0x0000,\n-    0x0000, 0x0000, 0xd9a4, 0xb949, 0x234b, 0x0000, 0x0000, 0x0000, 0xb54e, 0xbd39,\n-    0x232d, 0x0000, 0x0000, 0x0000, 0x4aac, 0x9a52, 0x230e, 0x0000, 0x0000, 0x0000,\n-    0xbbbc, 0xd085, 0x22ef, 0x0000, 0x0000, 0x0000, 0xdf18, 0xc633, 0x22cf, 0x0000,\n-    0x0000, 0x0000, 0x16d0, 0xeca5, 0x22af, 0x0000, 0x0000, 0x0000, 0xf2a0, 0xdf6f,\n-    0x228e, 0x0000, 0x0000, 0x0000, 0x8c44, 0xe86b, 0x2272, 0x0000, 0x0000, 0x0000,\n-    0x35c0, 0xbbf4, 0x2253, 0x0000, 0x0000, 0x0000, 0x0c40, 0xdafb, 0x2230, 0x0000,\n-    0x0000, 0x0000, 0x92dc, 0x9935, 0x2216, 0x0000, 0x0000, 0x0000, 0x0ca0, 0xbda6,\n-    0x21f3, 0x0000, 0x0000, 0x0000, 0x5958, 0xa6fd, 0x21d6, 0x0000, 0x0000, 0x0000,\n-    0xa3dc, 0x9d7f, 0x21b9, 0x0000, 0x0000, 0x0000, 0x79dc, 0xfcb5, 0x2199, 0x0000,\n-    0x0000, 0x0000, 0xf264, 0xcebb, 0x217b, 0x0000, 0x0000, 0x0000, 0x0abe, 0x8308,\n-    0x215c, 0x0000, 0x0000, 0x0000, 0x30ae, 0xb463, 0x213d, 0x0000, 0x0000, 0x0000,\n-    0x6228, 0xb040, 0x211c, 0x0000, 0x0000, 0x0000, 0xc9b2, 0xf43b, 0x20ff, 0x0000,\n-    0x0000, 0x0000, 0x3d8e, 0xa4b3, 0x20e0, 0x0000, 0x0000, 0x0000, 0x84e6, 0x8dab,\n-    0x20c1, 0x0000, 0x0000, 0x0000, 0xa124, 0x9b74, 0x20a1, 0x0000, 0x0000, 0x0000,\n-    0xc276, 0xd497, 0x2083, 0x0000, 0x0000, 0x0000, 0x6354, 0xa466, 0x2063, 0x0000,\n-    0x0000, 0x0000, 0x8654, 0xaf0a, 0x2044, 0x0000, 0x0000, 0x0000, 0x1d20, 0xfa5c,\n-    0x2024, 0x0000, 0x0000, 0x0000, 0xbcd0, 0xf3f0, 0x2004, 0x0000, 0x0000, 0x0000,\n-    0xedf0, 0xf0b6, 0x1fe7, 0x0000, 0x0000, 0x0000, 0x45bc, 0x9182, 0x1fc9, 0x0000,\n-    0x0000, 0x0000, 0xe254, 0xdc85, 0x1faa, 0x0000, 0x0000, 0x0000, 0xb898, 0xe9b1,\n-    0x1f8a, 0x0000, 0x0000, 0x0000, 0x0ebe, 0xe6f0, 0x1f6c, 0x0000, 0x0000, 0x0000,\n-    0xa9b8, 0xf584, 0x1f4c, 0x0000, 0x0000, 0x0000, 0x12e8, 0xdf6b, 0x1f2e, 0x0000,\n-    0x0000, 0x0000, 0x9f9e, 0xcd55, 0x1f0f, 0x0000, 0x0000, 0x0000, 0x05a0, 0xec3a,\n-    0x1eef, 0x0000, 0x0000, 0x0000, 0xd8e0, 0x96f8, 0x1ed1, 0x0000, 0x0000, 0x0000,\n-    0x3bd4, 0xccc6, 0x1eb1, 0x0000, 0x0000, 0x0000, 0x4910, 0xb87b, 0x1e93, 0x0000,\n-    0x0000, 0x0000, 0xbefc, 0xd40b, 0x1e73, 0x0000, 0x0000, 0x0000, 0x317e, 0xa406,\n-    0x1e55, 0x0000, 0x0000, 0x0000, 0x6bb2, 0xc2b2, 0x1e36, 0x0000, 0x0000, 0x0000,\n-    0xb87e, 0xbb78, 0x1e17, 0x0000, 0x0000, 0x0000, 0xa03c, 0xdbbd, 0x1df7, 0x0000,\n-    0x0000, 0x0000, 0x5b6c, 0xe3c8, 0x1dd9, 0x0000, 0x0000, 0x0000, 0x8968, 0xca8e,\n-    0x1dba, 0x0000, 0x0000, 0x0000, 0xc024, 0xe6ab, 0x1d9a, 0x0000, 0x0000, 0x0000,\n-    0x4110, 0xd4eb, 0x1d7a, 0x0000, 0x0000, 0x0000, 0xa168, 0xbdb5, 0x1d5d, 0x0000,\n-    0x0000, 0x0000, 0x012e, 0xa5fa, 0x1d3e, 0x0000, 0x0000, 0x0000, 0x6838, 0x9c1f,\n-    0x1d1e, 0x0000, 0x0000, 0x0000, 0xa158, 0xaa76, 0x1d00, 0x0000, 0x0000, 0x0000,\n-    0x090a, 0xbd95, 0x1ce1, 0x0000, 0x0000, 0x0000, 0xf73e, 0x8b6d, 0x1cc2, 0x0000,\n-    0x0000, 0x0000, 0x5fda, 0xbcbf, 0x1ca3, 0x0000, 0x0000, 0x0000, 0xdbe8, 0xb89f,\n-    0x1c84, 0x0000, 0x0000, 0x0000, 0x6e4c, 0x96c7, 0x1c64, 0x0000, 0x0000, 0x0000,\n-    0x19c2, 0xf2a4, 0x1c46, 0x0000, 0x0000, 0x0000, 0xb800, 0xf855, 0x1c1e, 0x0000,\n-    0x0000, 0x0000, 0x87fc, 0x85ff, 0x1c08, 0x0000, 0x0000, 0x0000, 0x1418, 0x839f,\n-    0x1be9, 0x0000, 0x0000, 0x0000, 0x6186, 0xd9d8, 0x1bca, 0x0000, 0x0000, 0x0000,\n-    0xf500, 0xabaa, 0x1ba6, 0x0000, 0x0000, 0x0000, 0x7b36, 0xdafe, 0x1b8c, 0x0000,\n-    0x0000, 0x0000, 0xf394, 0xe6d8, 0x1b6c, 0x0000, 0x0000, 0x0000, 0x6efc, 0x9e55,\n-    0x1b4e, 0x0000, 0x0000, 0x0000, 0x5e10, 0xc523, 0x1b2e, 0x0000, 0x0000, 0x0000,\n-    0x8210, 0xb6f9, 0x1b0d, 0x0000, 0x0000, 0x0000, 0x9ab0, 0x96e3, 0x1af1, 0x0000,\n-    0x0000, 0x0000, 0x3864, 0x92e7, 0x1ad1, 0x0000, 0x0000, 0x0000, 0x9878, 0xdc65,\n-    0x1ab1, 0x0000, 0x0000, 0x0000, 0xfa20, 0xd6cb, 0x1a94, 0x0000, 0x0000, 0x0000,\n-    0x6c00, 0xa4e4, 0x1a70, 0x0000, 0x0000, 0x0000, 0xab40, 0xb41b, 0x1a53, 0x0000,\n-    0x0000, 0x0000, 0x43a4, 0x8ede, 0x1a37, 0x0000, 0x0000, 0x0000, 0x22e0, 0x9314,\n-    0x1a15, 0x0000, 0x0000, 0x0000, 0x6170, 0xb949, 0x19f8, 0x0000, 0x0000, 0x0000,\n-    0x6b00, 0xe056, 0x19d8, 0x0000, 0x0000, 0x0000, 0x9ba8, 0xa94c, 0x19b9, 0x0000,\n-    0x0000, 0x0000, 0xfaa0, 0xaa16, 0x199b, 0x0000, 0x0000, 0x0000, 0x899a, 0xf627,\n-    0x197d, 0x0000, 0x0000, 0x0000, 0x9f20, 0xfb70, 0x195d, 0x0000, 0x0000, 0x0000,\n-    0xa4b8, 0xc176, 0x193e, 0x0000, 0x0000, 0x0000, 0xb21c, 0x85c3, 0x1920, 0x0000,\n-    0x0000, 0x0000, 0x50d2, 0x9b19, 0x1901, 0x0000, 0x0000, 0x0000, 0xd4b0, 0xb708,\n-    0x18e0, 0x0000, 0x0000, 0x0000, 0xfb88, 0xf510, 0x18c1, 0x0000, 0x0000, 0x0000,\n-    0x31ec, 0xdc8d, 0x18a3, 0x0000, 0x0000, 0x0000, 0x3c00, 0xbff9, 0x1885, 0x0000,\n-    0x0000, 0x0000, 0x5020, 0xc30b, 0x1862, 0x0000, 0x0000, 0x0000, 0xd4f0, 0xda0c,\n-    0x1844, 0x0000, 0x0000, 0x0000, 0x20d2, 0x99a5, 0x1828, 0x0000, 0x0000, 0x0000,\n-    0x852e, 0xd159, 0x1809, 0x0000, 0x0000, 0x0000, 0x7cd8, 0x97a1, 0x17e9, 0x0000,\n-    0x0000, 0x0000, 0x423a, 0x997b, 0x17cb, 0x0000, 0x0000, 0x0000, 0xc1c0, 0xbe7d,\n-    0x17a8, 0x0000, 0x0000, 0x0000, 0xe8bc, 0xdcdd, 0x178d, 0x0000, 0x0000, 0x0000,\n-    0x8b28, 0xae06, 0x176e, 0x0000, 0x0000, 0x0000, 0x102e, 0xb8d4, 0x174f, 0x0000,\n-    0x0000, 0x0000, 0xaa00, 0xaa5c, 0x172f, 0x0000, 0x0000, 0x0000, 0x51f0, 0x9fc0,\n-    0x170e, 0x0000, 0x0000, 0x0000, 0xf858, 0xe181, 0x16f2, 0x0000, 0x0000, 0x0000,\n-    0x91a8, 0x8162, 0x16d3, 0x0000, 0x0000, 0x0000, 0x5f40, 0xcb6f, 0x16b1, 0x0000,\n-    0x0000, 0x0000, 0xbb50, 0xe55f, 0x1693, 0x0000, 0x0000, 0x0000, 0xacd2, 0xd895,\n-    0x1676, 0x0000, 0x0000, 0x0000, 0xef30, 0x97bf, 0x1654, 0x0000, 0x0000, 0x0000,\n-    0xf700, 0xb3d7, 0x1633, 0x0000, 0x0000, 0x0000, 0x3454, 0xa7b5, 0x1619, 0x0000,\n-    0x0000, 0x0000, 0x6b00, 0xa929, 0x15f6, 0x0000, 0x0000, 0x0000, 0x9f04, 0x89f7,\n-    0x15db, 0x0000, 0x0000, 0x0000, 0xad78, 0xd985, 0x15bc, 0x0000, 0x0000, 0x0000,\n-    0xa46a, 0xae3f, 0x159d, 0x0000, 0x0000, 0x0000, 0x63a0, 0xd0da, 0x157c, 0x0000,\n-    0x0000, 0x0000, 0x5e90, 0x817d, 0x155e, 0x0000, 0x0000, 0x0000, 0x1494, 0xb13f,\n-    0x1540, 0x0000, 0x0000, 0x0000, 0x0090, 0x9c40, 0x1521, 0x0000, 0x0000, 0x0000,\n-    0xdd70, 0xcc86, 0x1500, 0x0000, 0x0000, 0x0000, 0x64f8, 0xdb6f, 0x14e1, 0x0000,\n-    0x0000, 0x0000, 0xe22c, 0xac17, 0x14c3, 0x0000, 0x0000, 0x0000, 0x60e0, 0xa9ad,\n-    0x14a3, 0x0000, 0x0000, 0x0000, 0x4640, 0xd658, 0x1481, 0x0000, 0x0000, 0x0000,\n-    0x6490, 0xa181, 0x1467, 0x0000, 0x0000, 0x0000, 0x1df4, 0xaaa2, 0x1447, 0x0000,\n-    0x0000, 0x0000, 0xb94a, 0x8f61, 0x1429, 0x0000, 0x0000, 0x0000, 0x5198, 0x9d83,\n-    0x1409, 0x0000, 0x0000, 0x0000, 0x0f7a, 0xa818, 0x13eb, 0x0000, 0x0000, 0x0000,\n-    0xc45e, 0xc06c, 0x13cc, 0x0000, 0x0000, 0x0000, 0x4ec0, 0xfa29, 0x13a8, 0x0000,\n-    0x0000, 0x0000, 0x6418, 0x8cad, 0x138c, 0x0000, 0x0000, 0x0000, 0xbcc8, 0xe7d1,\n-    0x136f, 0x0000, 0x0000, 0x0000, 0xc934, 0xf9b0, 0x134f, 0x0000, 0x0000, 0x0000,\n-    0x6ce0, 0x98df, 0x1331, 0x0000, 0x0000, 0x0000, 0x3516, 0xe5e9, 0x1312, 0x0000,\n-    0x0000, 0x0000, 0xc6c0, 0xef8b, 0x12ef, 0x0000, 0x0000, 0x0000, 0xaf02, 0x913d,\n-    0x12d4, 0x0000, 0x0000, 0x0000, 0xd230, 0xe1d5, 0x12b5, 0x0000, 0x0000, 0x0000,\n-    0xfba8, 0xc232, 0x1295, 0x0000, 0x0000, 0x0000, 0x7ba4, 0xabeb, 0x1277, 0x0000,\n-    0x0000, 0x0000, 0x6e5c, 0xc692, 0x1258, 0x0000, 0x0000, 0x0000, 0x76a2, 0x9756,\n-    0x1239, 0x0000, 0x0000, 0x0000, 0xe180, 0xe423, 0x1214, 0x0000, 0x0000, 0x0000,\n-    0x8c3c, 0x90f8, 0x11fb, 0x0000, 0x0000, 0x0000, 0x9f3c, 0x9fd2, 0x11dc, 0x0000,\n-    0x0000, 0x0000, 0x53e0, 0xb73e, 0x11bd, 0x0000, 0x0000, 0x0000, 0x45be, 0x88d6,\n-    0x119e, 0x0000, 0x0000, 0x0000, 0x111a, 0x8bc0, 0x117f, 0x0000, 0x0000, 0x0000,\n-    0xe26a, 0xd7ff, 0x1160, 0x0000, 0x0000, 0x0000, 0xfb60, 0xdd8d, 0x113f, 0x0000,\n-    0x0000, 0x0000, 0x9370, 0xc108, 0x1120, 0x0000, 0x0000, 0x0000, 0x9654, 0x8baf,\n-    0x1103, 0x0000, 0x0000, 0x0000, 0xd6ec, 0xd6b9, 0x10e4, 0x0000, 0x0000, 0x0000,\n-    0x23e4, 0xd7b7, 0x10c4, 0x0000, 0x0000, 0x0000, 0x1aa6, 0xa847, 0x10a6, 0x0000,\n-    0x0000, 0x0000, 0xbee6, 0x9fef, 0x1087, 0x0000, 0x0000, 0x0000, 0x26d0, 0xa6eb,\n-    0x1066, 0x0000, 0x0000, 0x0000, 0x5b86, 0xa880, 0x1049, 0x0000, 0x0000, 0x0000,\n-    0x125c, 0xd971, 0x1029, 0x0000, 0x0000, 0x0000, 0x1f78, 0x9d18, 0x100a, 0x0000,\n-    0x0000, 0x0000, 0x0e84, 0xb15b, 0x0feb, 0x0000, 0x0000, 0x0000, 0xd0c0, 0xc150,\n-    0x0fcc, 0x0000, 0x0000, 0x0000, 0xa330, 0xc40c, 0x0fad, 0x0000, 0x0000, 0x0000,\n-    0x5202, 0xfc2c, 0x0f8f, 0x0000, 0x0000, 0x0000, 0x3f7c, 0xecf5, 0x0f6f, 0x0000,\n-    0x0000, 0x0000, 0xef44, 0xfdfd, 0x0f50, 0x0000, 0x0000, 0x0000, 0x3f6c, 0xab1b,\n-    0x0f31, 0x0000, 0x0000, 0x0000, 0xf658, 0x89ec, 0x0f11, 0x0000, 0x0000, 0x0000,\n-    0xbfc8, 0x9ba8, 0x0ef4, 0x0000, 0x0000, 0x0000, 0x3d40, 0xbe21, 0x0ed5, 0x0000,\n-    0x0000, 0x0000, 0xbbc4, 0xc70d, 0x0eb6, 0x0000, 0x0000, 0x0000, 0x5158, 0xdb16,\n-    0x0e96, 0x0000, 0x0000, 0x0000, 0xb5a8, 0xa8d8, 0x0e78, 0x0000, 0x0000, 0x0000,\n-    0xcccc, 0xb40e, 0x0e58, 0x0000, 0x0000, 0x0000, 0x448c, 0xcb62, 0x0e3a, 0x0000,\n-    0x0000, 0x0000, 0xf12a, 0x8aed, 0x0e1b, 0x0000, 0x0000, 0x0000, 0x79d0, 0xc59c,\n-    0x0dfb, 0x0000, 0x0000, 0x0000, 0x06b4, 0xcdc9, 0x0ddd, 0x0000, 0x0000, 0x0000,\n-    0xae70, 0xa979, 0x0dbe, 0x0000, 0x0000, 0x0000, 0x317c, 0xa8fb, 0x0d9e, 0x0000,\n-    0x0000, 0x0000, 0x5fe0, 0x8a50, 0x0d7d, 0x0000, 0x0000, 0x0000, 0x70b6, 0xfdfa,\n-    0x0d61, 0x0000, 0x0000, 0x0000, 0x1640, 0x9dc7, 0x0d41, 0x0000, 0x0000, 0x0000,\n-    0x9a9c, 0xdc50, 0x0d23, 0x0000, 0x0000, 0x0000, 0x4fcc, 0x9a9b, 0x0d04, 0x0000,\n-    0x0000, 0x0000, 0x7e48, 0x8f77, 0x0ce5, 0x0000, 0x0000, 0x0000, 0x84e4, 0xd4b9,\n-    0x0cc6, 0x0000, 0x0000, 0x0000, 0x84e0, 0xbd10, 0x0ca6, 0x0000, 0x0000, 0x0000,\n-    0x1b0a, 0xc8d9, 0x0c88, 0x0000, 0x0000, 0x0000, 0x6a48, 0xfc81, 0x0c68, 0x0000,\n-    0x0000, 0x0000, 0x070a, 0xbef6, 0x0c4a, 0x0000, 0x0000, 0x0000, 0x8a70, 0xf096,\n-    0x0c2b, 0x0000, 0x0000, 0x0000, 0xecc2, 0xc994, 0x0c0c, 0x0000, 0x0000, 0x0000,\n-    0x1540, 0x9537, 0x0bea, 0x0000, 0x0000, 0x0000, 0x1b02, 0xab5b, 0x0bce, 0x0000,\n-    0x0000, 0x0000, 0x5dc0, 0xb0c8, 0x0bad, 0x0000, 0x0000, 0x0000, 0xc928, 0xe034,\n-    0x0b8f, 0x0000, 0x0000, 0x0000, 0x2d12, 0xb4b0, 0x0b71, 0x0000, 0x0000, 0x0000,\n-    0x8fc2, 0xbb94, 0x0b52, 0x0000, 0x0000, 0x0000, 0xe236, 0xe22f, 0x0b33, 0x0000,\n-    0x0000, 0x0000, 0xb97c, 0xbe9e, 0x0b13, 0x0000, 0x0000, 0x0000, 0xe1a6, 0xe16d,\n-    0x0af5, 0x0000, 0x0000, 0x0000, 0xd330, 0xbaf0, 0x0ad6, 0x0000, 0x0000, 0x0000,\n-    0xc0bc, 0xbbd0, 0x0ab7, 0x0000, 0x0000, 0x0000, 0x8e66, 0xdd9b, 0x0a98, 0x0000,\n-    0x0000, 0x0000, 0xc95c, 0xf799, 0x0a79, 0x0000, 0x0000, 0x0000, 0xdac0, 0xbe4c,\n-    0x0a55, 0x0000, 0x0000, 0x0000, 0xafc0, 0xc378, 0x0a37, 0x0000, 0x0000, 0x0000,\n-    0xa880, 0xe341, 0x0a19, 0x0000, 0x0000, 0x0000, 0xc242, 0x81f6, 0x09fd, 0x0000,\n-    0x0000, 0x0000, 0x7470, 0xc777, 0x09de, 0x0000, 0x0000, 0x0000, 0x62bc, 0xb684,\n-    0x09be, 0x0000, 0x0000, 0x0000, 0x43ac, 0x8c58, 0x099f, 0x0000, 0x0000, 0x0000,\n-    0xcc3c, 0xf9ac, 0x0981, 0x0000, 0x0000, 0x0000, 0x1526, 0xb670, 0x0962, 0x0000,\n-    0x0000, 0x0000, 0xc9fe, 0xdf50, 0x0943, 0x0000, 0x0000, 0x0000, 0x6ae6, 0xc065,\n-    0x0924, 0x0000, 0x0000, 0x0000, 0xb114, 0xcf29, 0x0905, 0x0000, 0x0000, 0x0000,\n-    0xd388, 0x922a, 0x08e4, 0x0000, 0x0000, 0x0000, 0xcf54, 0xb926, 0x08c7, 0x0000,\n-    0x0000, 0x0000, 0x3826, 0xe855, 0x08a8, 0x0000, 0x0000, 0x0000, 0xe7c8, 0x829b,\n-    0x0888, 0x0000, 0x0000, 0x0000, 0x546c, 0xa903, 0x086a, 0x0000, 0x0000, 0x0000,\n-    0x8768, 0x99cc, 0x0849, 0x0000, 0x0000, 0x0000, 0x00ac, 0xf529, 0x082b, 0x0000,\n-    0x0000, 0x0000, 0x2658, 0x9f0b, 0x080c, 0x0000, 0x0000, 0x0000, 0xfe5c, 0x9e21,\n-    0x07ee, 0x0000, 0x0000, 0x0000, 0x6da2, 0x9910, 0x07cf, 0x0000, 0x0000, 0x0000,\n-    0x9220, 0xf9b3, 0x07b0, 0x0000, 0x0000, 0x0000, 0x3d90, 0xa541, 0x0791, 0x0000,\n-    0x0000, 0x0000, 0x6e4c, 0xe7cc, 0x0771, 0x0000, 0x0000, 0x0000, 0xa8fa, 0xe80a,\n-    0x0753, 0x0000, 0x0000, 0x0000, 0x4e14, 0xc3a7, 0x0734, 0x0000, 0x0000, 0x0000,\n-    0xf7e0, 0xbad9, 0x0712, 0x0000, 0x0000, 0x0000, 0xfea0, 0xeff2, 0x06f5, 0x0000,\n-    0x0000, 0x0000, 0xcef6, 0xbd48, 0x06d7, 0x0000, 0x0000, 0x0000, 0x7544, 0xf559,\n-    0x06b7, 0x0000, 0x0000, 0x0000, 0x2388, 0xf655, 0x0698, 0x0000, 0x0000, 0x0000,\n-    0xe900, 0xad56, 0x0676, 0x0000, 0x0000, 0x0000, 0x2cc0, 0x8437, 0x0659, 0x0000,\n-    0x0000, 0x0000, 0x3068, 0xc544, 0x063b, 0x0000, 0x0000, 0x0000, 0xdc70, 0xe73c,\n-    0x061b, 0x0000, 0x0000, 0x0000, 0xee50, 0x9d49, 0x05fc, 0x0000, 0x0000, 0x0000,\n-    0x93d2, 0x81f6, 0x05df, 0x0000, 0x0000, 0x0000, 0x941c, 0xadff, 0x05bf, 0x0000,\n-    0x0000, 0x0000, 0x2ce2, 0x8e45, 0x05a1, 0x0000, 0x0000, 0x0000, 0x4a60, 0x95fd,\n-    0x0581, 0x0000, 0x0000, 0x0000, 0x79f8, 0xb83a, 0x0563, 0x0000, 0x0000, 0x0000,\n-    0xcb58, 0xa1f5, 0x0543, 0x0000, 0x0000, 0x0000, 0x2a3a, 0xdc36, 0x0525, 0x0000,\n-    0x0000, 0x0000, 0x14ee, 0x890e, 0x0506, 0x0000, 0x0000, 0x0000, 0x8f20, 0xc432,\n-    0x04e3, 0x0000, 0x0000, 0x0000, 0x8440, 0xb21d, 0x04c6, 0x0000, 0x0000, 0x0000,\n-    0x5430, 0xf698, 0x04a7, 0x0000, 0x0000, 0x0000, 0x04ae, 0x8b20, 0x048a, 0x0000,\n-    0x0000, 0x0000, 0x04d0, 0xe872, 0x046b, 0x0000, 0x0000, 0x0000, 0xc78e, 0x8893,\n-    0x044c, 0x0000, 0x0000, 0x0000, 0x0f78, 0x9895, 0x042b, 0x0000, 0x0000, 0x0000,\n-    0x11d4, 0xdf2e, 0x040d, 0x0000, 0x0000, 0x0000, 0xe84c, 0x89d5, 0x03ef, 0x0000,\n-    0x0000, 0x0000, 0xf7be, 0x8a67, 0x03d0, 0x0000, 0x0000, 0x0000, 0x95d0, 0xc906,\n-    0x03b1, 0x0000, 0x0000, 0x0000, 0x64ce, 0xd96c, 0x0392, 0x0000, 0x0000, 0x0000,\n-    0x97ba, 0xa16f, 0x0373, 0x0000, 0x0000, 0x0000, 0x463c, 0xc51a, 0x0354, 0x0000,\n-    0x0000, 0x0000, 0xef0a, 0xe93e, 0x0335, 0x0000, 0x0000, 0x0000, 0x526a, 0xa466,\n-    0x0316, 0x0000, 0x0000, 0x0000, 0x4140, 0xa94d, 0x02f5, 0x0000, 0x0000, 0x0000,\n-    0xb4ec, 0xce68, 0x02d8, 0x0000, 0x0000, 0x0000, 0x4fa2, 0x8490, 0x02b9, 0x0000,\n-    0x0000, 0x0000, 0x4e60, 0xca98, 0x0298, 0x0000, 0x0000, 0x0000, 0x08dc, 0xe09c,\n-    0x027a, 0x0000, 0x0000, 0x0000, 0x2b90, 0xc7e3, 0x025c, 0x0000, 0x0000, 0x0000,\n-    0x5a7c, 0xf8ef, 0x023c, 0x0000, 0x0000, 0x0000, 0x5022, 0x9d58, 0x021e, 0x0000,\n-    0x0000, 0x0000, 0x553a, 0xe242, 0x01ff, 0x0000, 0x0000, 0x0000, 0x7e6e, 0xb54d,\n-    0x01e0, 0x0000, 0x0000, 0x0000, 0xd2d4, 0xa88c, 0x01c1, 0x0000, 0x0000, 0x0000,\n-    0x75b6, 0xfe6d, 0x01a2, 0x0000, 0x0000, 0x0000, 0x3bb2, 0xf04c, 0x0183, 0x0000,\n-    0x0000, 0x0000, 0xc2d0, 0xc046, 0x0163, 0x0000, 0x0000, 0x0000, 0x250c, 0xf9d6,\n-    0x0145, 0x0000, 0x0000, 0x0000, 0xb7b4, 0x8a0d, 0x0126, 0x0000, 0x0000, 0x0000,\n-    0x1a72, 0xe4f5, 0x0107, 0x0000, 0x0000, 0x0000, 0x825c, 0xa9b8, 0x00e8, 0x0000,\n-    0x0000, 0x0000, 0x6c90, 0xc9ad, 0x00c6, 0x0000, 0x0000, 0x0000, 0x4d00, 0xd1bb,\n-    0x00aa, 0x0000, 0x0000, 0x0000, 0xa4a0, 0xee01, 0x0087, 0x0000, 0x0000, 0x0000,\n-    0x89a8, 0xbe9f, 0x006b, 0x0000, 0x0000, 0x0000, 0x038e, 0xc80c, 0x004d, 0x0000,\n-    0x0000, 0x0000, 0xfe26, 0x8384, 0x002e, 0x0000, 0x0000, 0x0000, 0xcd90, 0xca57,\n-    0x000e, 0x0000\n-};\n-\n-void MacroAssembler::libm_reduce_pi04l(Register eax, Register ecx, Register edx, Register ebx, Register esi, Register edi, Register ebp, Register esp) {\n-  Label B1_1, B1_2, B1_3, B1_4, B1_5, B1_6, B1_7, B1_8, B1_9, B1_10, B1_11, B1_12;\n-  Label B1_13, B1_14, B1_15;\n-\n-  assert_different_registers(ebx, eax, ecx, edx, esi, edi, ebp, esp);\n-\n-  address zero_none  = (address)_zero_none;\n-  address _4onpi_d   = (address)__4onpi_d;\n-  address TWO_32H    = (address)_TWO_32H;\n-  address pi04_3d    = (address)_pi04_3d;\n-  address pi04_5d    = (address)_pi04_5d;\n-  address SCALE      = (address)_SCALE;\n-  address zeros      = (address)_zeros;\n-  address pi04_2d    = (address)_pi04_2d;\n-  address TWO_12H    = (address)_TWO_12H;\n-  address _4onpi_31l = (address)__4onpi_31l;\n-\n-  bind(B1_1);\n-  push(ebp);\n-  movl(ebp, esp);\n-  andl(esp, -16);\n-  push(esi);\n-  push(edi);\n-  push(ebx);\n-  subl(esp, 20);\n-  movzwl(ebx, Address(ebp, 16));\n-  andl(ebx, 32767);\n-  movl(eax, Address(ebp, 20));\n-  cmpl(ebx, 16413);\n-  movl(esi, Address(ebp, 24));\n-  movl(Address(esp, 4), eax);\n-  jcc(Assembler::greaterEqual, B1_8);\n-\n-  bind(B1_2);\n-  fld_x(Address(ebp, 8));\n-  fld_d(ExternalAddress(_4onpi_d));    \/\/0x6dc9c883UL, 0x3ff45f30UL\n-  fmul(1);\n-  fstp_x(Address(esp, 8));\n-  movzwl(ecx, Address(esp, 16));\n-  negl(ecx);\n-  addl(ecx, 30);\n-  movl(eax, Address(esp, 12));\n-  shrl(eax);\n-  cmpl(Address(esp, 4), 0);\n-  jcc(Assembler::notEqual, B1_4);\n-\n-  bind(B1_3);\n-  lea(ecx, Address(eax, 1));\n-  andl(ecx, -2);\n-  jmp(B1_5);\n-\n-  bind(B1_4);\n-  movl(ecx, eax);\n-  addl(eax, Address(esp, 4));\n-  movl(edx, eax);\n-  andl(edx, 1);\n-  addl(ecx, edx);\n-\n-  bind(B1_5);\n-  fld_d(ExternalAddress(TWO_32H));    \/\/0x00000000UL, 0x41f80000UL\n-  cmpl(ebx, 16400);\n-  movl(Address(esp, 0), ecx);\n-  fild_s(Address(esp, 0));\n-  jcc(Assembler::greaterEqual, B1_7);\n-\n-  bind(B1_6);\n-  fld_d(ExternalAddress(pi04_3d));    \/\/0x54442d00UL, 0x3fe921fbUL\n-  fmul(1);\n-  fsubp(3);\n-  fxch(1);\n-  fmul(2);\n-  fld_s(2);\n-  fadd(1);\n-  fsubrp(1);\n-  fld_s(0);\n-  fxch(1);\n-  fsuba(3);\n-  fld_d(ExternalAddress(8 + pi04_3d));    \/\/0x98cc5180UL, 0x3ce84698UL\n-  fmul(3);\n-  fsuba(2);\n-  fxch(1);\n-  fsub(2);\n-  fsubrp(1);\n-  faddp(3);\n-  fld_d(ExternalAddress(16 + pi04_3d));    \/\/0xcbb5bf6cUL, 0xb9dfc8f8UL\n-  fmulp(2);\n-  fld_s(1);\n-  fsubr(1);\n-  fsuba(1);\n-  fxch(2);\n-  fsubp(1);\n-  faddp(2);\n-  fxch(1);\n-  jmp(B1_15);\n-\n-  bind(B1_7);\n-  fld_d(ExternalAddress(pi04_5d));    \/\/0x54400000UL, 0x3fe921fbUL\n-  fmul(1);\n-  fsubp(3);\n-  fxch(1);\n-  fmul(2);\n-  fld_s(2);\n-  fadd(1);\n-  fsubrp(1);\n-  fld_s(0);\n-  fxch(1);\n-  fsuba(3);\n-  fld_d(ExternalAddress(8 + pi04_5d));    \/\/0x1a600000UL, 0x3dc0b461UL\n-  fmul(3);\n-  fsuba(2);\n-  fxch(1);\n-  fsub(2);\n-  fsubrp(1);\n-  faddp(3);\n-  fld_d(ExternalAddress(16 + pi04_5d));    \/\/0x2e000000UL, 0x3b93198aUL\n-  fmul(2);\n-  fld_s(0);\n-  fsubr(2);\n-  fsuba(2);\n-  fxch(1);\n-  fsubp(2);\n-  fxch(1);\n-  faddp(3);\n-  fld_d(ExternalAddress(24 + pi04_5d));    \/\/0x25200000UL, 0x396b839aUL\n-  fmul(2);\n-  fld_s(0);\n-  fsubr(2);\n-  fsuba(2);\n-  fxch(1);\n-  fsubp(2);\n-  fxch(1);\n-  faddp(3);\n-  fld_d(ExternalAddress(32 + pi04_5d));    \/\/0x533e63a0UL, 0x37027044UL\n-  fmulp(2);\n-  fld_s(1);\n-  fsubr(1);\n-  fsuba(1);\n-  fxch(2);\n-  fsubp(1);\n-  faddp(2);\n-  fxch(1);\n-  jmp(B1_15);\n-\n-  bind(B1_8);\n-  fld_x(Address(ebp, 8));\n-  addl(ebx, -16417);\n-  fmul_d(as_Address(ExternalAddress(SCALE)));    \/\/0x00000000UL, 0x32600000UL\n-  movl(eax, -2078209981);\n-  imull(ebx);\n-  addl(edx, ebx);\n-  movl(ecx, ebx);\n-  sarl(edx, 4);\n-  sarl(ecx, 31);\n-  subl(edx, ecx);\n-  movl(eax, edx);\n-  shll(eax, 5);\n-  fstp_x(Address(ebp, 8));\n-  fld_x(Address(ebp, 8));\n-  subl(eax, edx);\n-  movl(Address(ebp, 8), 0);\n-  subl(ebx, eax);\n-  fld_x(Address(ebp, 8));\n-  cmpl(ebx, 17);\n-  fsuba(1);\n-  jcc(Assembler::less, B1_10);\n-\n-  bind(B1_9);\n-  lea(eax, Address(noreg, edx, Address::times_8));\n-  lea(ecx, Address(eax, edx, Address::times_4));\n-  incl(edx);\n-  fld_x(Address(_4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmul(2);\n-  fld_x(Address(12 + _4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmul(2);\n-  fld_s(0);\n-  fadd(2);\n-  fsuba(2);\n-  fxch(1);\n-  faddp(2);\n-  fld_s(1);\n-  fadd(1);\n-  fstp_x(Address(esp, 8));\n-  andl(Address(esp, 8), -16777216);\n-  fld_x(Address(esp, 8));\n-  fsubp(1);\n-  jmp(B1_11);\n-\n-  bind(B1_10);\n-  fld_d(ExternalAddress(zeros));    \/\/0x00000000UL, 0x00000000UL\n-  fld_s(0);\n-\n-  bind(B1_11);\n-  fld_s(0);\n-  lea(eax, Address(noreg, edx, Address::times_8));\n-  fld_s(3);\n-  lea(edx, Address(eax, edx, Address::times_4));\n-  fld_x(Address(_4onpi_31l, RelocationHolder::none).plus_disp(edx, Address::times_1));\n-  fmul(6);\n-  movl(Address(esp, 0), edx);\n-  fadda(2);\n-  fxch(2);\n-  fsuba(3);\n-  fxch(2);\n-  faddp(3);\n-  fxch(2);\n-  faddp(3);\n-  fld_x(Address(12 + _4onpi_31l, RelocationHolder::none).plus_disp(edx, Address::times_1));\n-  fmula(2);\n-  fld_s(2);\n-  fadd(2);\n-  fld_s(0);\n-  fxch(1);\n-  fsubra(3);\n-  fxch(3);\n-  fchs();\n-  faddp(4);\n-  fxch(3);\n-  faddp(4);\n-  fxch(2);\n-  fadd(3);\n-  fxch(2);\n-  fmul(5);\n-  fadda(2);\n-  fld_s(4);\n-  fld_x(Address(24 + _4onpi_31l, RelocationHolder::none).plus_disp(edx, Address::times_1));\n-  fmula(1);\n-  fxch(1);\n-  fadda(4);\n-  fxch(4);\n-  fstp_x(Address(esp, 8));\n-  movzwl(ebx, Address(esp, 16));\n-  andl(ebx, 32767);\n-  cmpl(ebx, 16415);\n-  jcc(Assembler::greaterEqual, B1_13);\n-\n-  bind(B1_12);\n-  negl(ebx);\n-  addl(ebx, 30);\n-  movl(ecx, ebx);\n-  movl(eax, Address(esp, 12));\n-  shrl(eax);\n-  shll(eax);\n-  movl(Address(esp, 12), eax);\n-  movl(Address(esp, 8), 0);\n-  shrl(eax);\n-  jmp(B1_14);\n-\n-  bind(B1_13);\n-  negl(ebx);\n-  addl(ebx, 30);\n-  movl(ecx, ebx);\n-  movl(edx, Address(esp, 8));\n-  shrl(edx);\n-  shll(edx);\n-  negl(ecx);\n-  movl(eax, Address(esp, 12));\n-  shll(eax);\n-  movl(ecx, ebx);\n-  movl(Address(esp, 8), edx);\n-  shrl(edx);\n-  orl(eax, edx);\n-\n-  bind(B1_14);\n-  fld_x(Address(esp, 8));\n-  addl(eax, Address(esp, 4));\n-  fsubp(3);\n-  fmul(6);\n-  fld_s(4);\n-  movl(edx, eax);\n-  andl(edx, 1);\n-  fadd(3);\n-  movl(ecx, Address(esp, 0));\n-  fsuba(3);\n-  fxch(3);\n-  faddp(5);\n-  fld_s(1);\n-  fxch(3);\n-  fadd_d(Address(zero_none, RelocationHolder::none).plus_disp(edx, Address::times_8));\n-  fadda(3);\n-  fsub(3);\n-  faddp(2);\n-  fxch(1);\n-  faddp(4);\n-  fld_s(2);\n-  fadd(2);\n-  fsuba(2);\n-  fxch(3);\n-  faddp(2);\n-  fxch(1);\n-  faddp(3);\n-  fld_s(0);\n-  fadd(2);\n-  fsuba(2);\n-  fxch(1);\n-  faddp(2);\n-  fxch(1);\n-  faddp(2);\n-  fld_s(2);\n-  fld_x(Address(36 + _4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmula(1);\n-  fld_s(1);\n-  fadd(3);\n-  fsuba(3);\n-  fxch(2);\n-  faddp(3);\n-  fxch(2);\n-  faddp(3);\n-  fxch(1);\n-  fmul(4);\n-  fld_s(0);\n-  fadd(2);\n-  fsuba(2);\n-  fxch(1);\n-  faddp(2);\n-  fxch(1);\n-  faddp(2);\n-  fld_s(2);\n-  fld_x(Address(48 + _4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmula(1);\n-  fld_s(1);\n-  fadd(3);\n-  fsuba(3);\n-  fxch(2);\n-  faddp(3);\n-  fxch(2);\n-  faddp(3);\n-  fld_s(3);\n-  fxch(2);\n-  fmul(5);\n-  fld_x(Address(60 + _4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmula(3);\n-  fxch(3);\n-  faddp(1);\n-  fld_s(0);\n-  fadd(2);\n-  fsuba(2);\n-  fxch(1);\n-  faddp(2);\n-  fxch(1);\n-  faddp(3);\n-  fld_s(3);\n-  fxch(2);\n-  fmul(5);\n-  fld_x(Address(72 + _4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmula(3);\n-  fxch(3);\n-  faddp(1);\n-  fld_s(0);\n-  fadd(2);\n-  fsuba(2);\n-  fxch(1);\n-  faddp(2);\n-  fxch(1);\n-  faddp(3);\n-  fxch(1);\n-  fmulp(4);\n-  fld_x(Address(84 + _4onpi_31l, RelocationHolder::none).plus_disp(ecx, Address::times_1));\n-  fmulp(3);\n-  fxch(2);\n-  faddp(3);\n-  fld_s(2);\n-  fadd(2);\n-  fld_d(ExternalAddress(TWO_32H));    \/\/0x00000000UL, 0x41f80000UL\n-  fmul(1);\n-  fadda(1);\n-  fsubp(1);\n-  fsuba(2);\n-  fxch(3);\n-  faddp(2);\n-  faddp(1);\n-  fld_d(ExternalAddress(pi04_2d));    \/\/0x54400000UL, 0x3fe921fbUL\n-  fld_s(0);\n-  fmul(2);\n-  fxch(2);\n-  fadd(3);\n-  fxch(1);\n-  fmulp(3);\n-  fmul_d(as_Address(ExternalAddress(8 + pi04_2d)));    \/\/0x1a626331UL, 0x3dc0b461UL\n-  faddp(1);\n-\n-  bind(B1_15);\n-  fld_d(ExternalAddress(TWO_12H));    \/\/0x00000000UL, 0x40b80000UL\n-  fld_s(2);\n-  fadd(2);\n-  fmula(1);\n-  fstp_x(Address(esp, 8));\n-  fld_x(Address(esp, 8));\n-  fadd(1);\n-  fsubrp(1);\n-  fst_d(Address(esi, 0));\n-  fsubp(2);\n-  faddp(1);\n-  fstp_d(Address(esi, 8));\n-  addl(esp, 20);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-}\n-\n-\n-ATTRIBUTE_ALIGNED(16) static const jushort _SP[] =\n-{\n-    0xaaab, 0xaaaa, 0xaaaa, 0xaaaa, 0xbffc, 0x0000, 0x8887, 0x8888, 0x8888, 0x8888,\n-    0x3ff8, 0x0000, 0xc527, 0x0d00, 0x00d0, 0xd00d, 0xbff2, 0x0000, 0x45f6, 0xb616,\n-    0x1d2a, 0xb8ef, 0x3fec, 0x0000, 0x825b, 0x3997, 0x2b3f, 0xd732, 0xbfe5, 0x0000,\n-    0xbf33, 0x8bb4, 0x2fda, 0xb092, 0x3fde, 0x0000, 0x44a6, 0xed1a, 0x29ef, 0xd73e,\n-    0xbfd6, 0x0000, 0x8610, 0x307f, 0x62a1, 0xc921, 0x3fce, 0x0000\n-};\n-\n-ATTRIBUTE_ALIGNED(16) static const jushort _CP[] =\n-{\n-    0x0000, 0x0000, 0x0000, 0x8000, 0xbffe, 0x0000, 0xaaa5, 0xaaaa, 0xaaaa, 0xaaaa,\n-    0x3ffa, 0x0000, 0x9c2f, 0x0b60, 0x60b6, 0xb60b, 0xbff5, 0x0000, 0xf024, 0x0cac,\n-    0x00d0, 0xd00d, 0x3fef, 0x0000, 0x03fe, 0x3f65, 0x7dbb, 0x93f2, 0xbfe9, 0x0000,\n-    0xd84d, 0xadee, 0xc698, 0x8f76, 0x3fe2, 0x0000, 0xdaba, 0xfe79, 0xea36, 0xc9c9,\n-    0xbfda, 0x0000, 0x3ac6, 0x0ba0, 0x07ce, 0xd585, 0x3fd2, 0x0000\n-};\n-\n-void MacroAssembler::libm_sincos_huge(XMMRegister xmm0, XMMRegister xmm1, Register eax, Register ecx, Register edx, Register ebx, Register esi, Register edi, Register ebp, Register esp) {\n-  Label B1_1, B1_2, B1_3, B1_4, B1_5, B1_6, B1_7, B1_8, B1_9, B1_10, B1_11, B1_12;\n-  Label B1_13, B1_14, B1_15, B1_16, B1_17, B1_18, B1_19, B1_20, B1_21, B1_22, B1_23;\n-  Label B1_24, B1_25, B1_26, B1_27, B1_28, B1_29, B1_30, B1_31, B1_32, B1_33, B1_34;\n-  Label B1_35, B1_36, B1_37, B1_38, B1_39, B1_40, B1_41, B1_42, B1_43, B1_46;\n-\n-  assert_different_registers(ebx, eax, ecx, edx, esi, edi, ebp, esp);\n-\n-  address CP = (address)_CP;\n-  address SP = (address)_SP;\n-\n-  bind(B1_1);\n-  push(ebp);\n-  movl(ebp, esp);\n-  andl(esp, -64);\n-  push(esi);\n-  push(edi);\n-  push(ebx);\n-  subl(esp, 52);\n-  movl(eax, Address(ebp, 16));\n-  movl(edx, Address(ebp, 20));\n-  movl(Address(esp, 32), eax);\n-  movl(Address(esp, 36), edx);\n-\n-  bind(B1_2);\n-  fnstcw(Address(esp, 30));\n-\n-  bind(B1_3);\n-  movsd(xmm1, Address(ebp, 8));\n-  movl(esi, Address(ebp, 12));\n-  movl(eax, esi);\n-  andl(eax, 2147483647);\n-  andps(xmm1, ExternalAddress(L_2IL0FLOATPACKET_0));    \/\/0xffffffffUL, 0x7fffffffUL, 0x00000000UL, 0x00000000UL\n-  shrl(esi, 31);\n-  movl(Address(esp, 40), eax);\n-  cmpl(eax, 1104150528);\n-  movsd(Address(ebp, 8), xmm1);\n-  jcc(Assembler::aboveEqual, B1_11);\n-\n-  bind(B1_4);\n-  movsd(xmm0, ExternalAddress(PI4_INV));    \/\/0x6dc9c883UL, 0x3ff45f30UL\n-  mulsd(xmm0, xmm1);\n-  movzwl(edx, Address(esp, 30));\n-  movl(eax, edx);\n-  andl(eax, 768);\n-  movsd(Address(esp, 0), xmm0);\n-  cmpl(eax, 768);\n-  jcc(Assembler::equal, B1_42);\n-\n-  bind(B1_5);\n-  orl(edx, -64768);\n-  movw(Address(esp, 28), edx);\n-\n-  bind(B1_6);\n-  fldcw(Address(esp, 28));\n-\n-  bind(B1_7);\n-  movsd(xmm1, Address(ebp, 8));\n-  movl(ebx, 1);\n-\n-  bind(B1_8);\n-  movl(Address(esp, 12), ebx);\n-  movl(ebx, Address(esp, 4));\n-  movl(eax, ebx);\n-  movl(Address(esp, 8), esi);\n-  movl(esi, ebx);\n-  shrl(esi, 20);\n-  andl(eax, 1048575);\n-  movl(ecx, esi);\n-  orl(eax, 1048576);\n-  negl(ecx);\n-  movl(edx, eax);\n-  addl(ecx, 19);\n-  addl(esi, 13);\n-  movl(Address(esp, 24), ecx);\n-  shrl(edx);\n-  movl(ecx, esi);\n-  shll(eax);\n-  movl(ecx, Address(esp, 24));\n-  movl(esi, Address(esp, 0));\n-  shrl(esi);\n-  orl(eax, esi);\n-  cmpl(ebx, 1094713344);\n-  movsd(Address(esp, 16), xmm1);\n-  fld_d(Address(esp, 16));\n-  cmov32(Assembler::below, eax, edx);\n-  movl(esi, Address(esp, 8));\n-  lea(edx, Address(eax, 1));\n-  movl(ebx, edx);\n-  andl(ebx, -2);\n-  movl(Address(esp, 16), ebx);\n-  fild_s(Address(esp, 16));\n-  movl(ebx, Address(esp, 12));\n-  cmpl(Address(esp, 40), 1094713344);\n-  jcc(Assembler::aboveEqual, B1_10);\n-\n-  bind(B1_9);\n-  fld_d(ExternalAddress(PI4X3));    \/\/0x54443000UL, 0xbfe921fbUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X3 + 8));    \/\/0x3b39a000UL, 0x3d373dcbUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X3 + 16));    \/\/0xe0e68948UL, 0xba845c06UL\n-  fmulp(1);\n-  faddp(1);\n-  jmp(B1_17);\n-\n-  bind(B1_10);\n-  fld_d(ExternalAddress(PI4X4));    \/\/0x54400000UL, 0xbfe921fbUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X4 + 8));    \/\/0x1a600000UL, 0xbdc0b461UL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X4 + 16));    \/\/0x2e000000UL, 0xbb93198aUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X4 + 24));    \/\/0x252049c1UL, 0xb96b839aUL\n-  fmulp(1);\n-  faddp(1);\n-  jmp(B1_17);\n-\n-  bind(B1_11);\n-  movzwl(edx, Address(esp, 30));\n-  movl(eax, edx);\n-  andl(eax, 768);\n-  cmpl(eax, 768);\n-  jcc(Assembler::equal, B1_43);\n-  bind(B1_12);\n-  orl(edx, -64768);\n-  movw(Address(esp, 28), edx);\n-\n-  bind(B1_13);\n-  fldcw(Address(esp, 28));\n-\n-  bind(B1_14);\n-  movsd(xmm1, Address(ebp, 8));\n-  movl(ebx, 1);\n-\n-  bind(B1_15);\n-  movsd(Address(esp, 16), xmm1);\n-  fld_d(Address(esp, 16));\n-  addl(esp, -32);\n-  lea(eax, Address(esp, 32));\n-  fstp_x(Address(esp, 0));\n-  movl(Address(esp, 12), 0);\n-  movl(Address(esp, 16), eax);\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlibm_reduce_pi04l())));\n-\n-  bind(B1_46);\n-  addl(esp, 32);\n-\n-  bind(B1_16);\n-  fld_d(Address(esp, 0));\n-  lea(edx, Address(eax, 1));\n-  fld_d(Address(esp, 8));\n-  faddp(1);\n-\n-  bind(B1_17);\n-  movl(ecx, edx);\n-  addl(eax, 3);\n-  shrl(ecx, 2);\n-  andl(ecx, 1);\n-  shrl(eax, 2);\n-  xorl(esi, ecx);\n-  movl(ecx, Address(esp, 36));\n-  andl(eax, 1);\n-  andl(ecx, 3);\n-  cmpl(ecx, 3);\n-  jcc(Assembler::notEqual, B1_25);\n-\n-  bind(B1_18);\n-  fld_x(ExternalAddress(84 + SP));    \/\/0x8610, 0x307f, 0x62\n-  fld_s(1);\n-  fmul((2));\n-  testb(edx, 2);\n-  fmula((1));\n-  fld_x(ExternalAddress(72 + SP));    \/\/0x44a6, 0xed1a, 0x29\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(60 + SP));    \/\/0xbf33, 0x8bb4, 0x2f\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(48 + SP));    \/\/0x825b, 0x3997, 0x2b\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(36 + SP));    \/\/0x45f6, 0xb616, 0x1d\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(24 + SP));    \/\/0xc527, 0x0d00, 0x00\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(12 + SP));    \/\/0x8887, 0x8888, 0x88\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(SP));    \/\/0xaaab, 0xaaaa, 0xaa\n-  faddp(2);\n-  fmula(1);\n-  fld_x(ExternalAddress(84 + CP));    \/\/0x3ac6, 0x0ba0, 0x07\n-  fmul(1);\n-  fld_x(ExternalAddress(72 + CP));    \/\/0xdaba, 0xfe79, 0xea\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(62 + CP));    \/\/0xd84d, 0xadee, 0xc6\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(48 + CP));    \/\/0x03fe, 0x3f65, 0x7d\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(36 + CP));    \/\/0xf024, 0x0cac, 0x00\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(24 + CP));    \/\/0x9c2f, 0x0b60, 0x60\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(12 + CP));    \/\/0xaaa5, 0xaaaa, 0xaa\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(CP));    \/\/0x0000, 0x0000, 0x00\n-  faddp(1);\n-  fmulp(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(esi, Address::times_8));\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  jcc(Assembler::equal, B1_22);\n-\n-  bind(B1_19);\n-  fmulp(4);\n-  testl(ebx, ebx);\n-  fxch(2);\n-  fmul(3);\n-  movl(eax, Address(esp, 2));\n-  faddp(3);\n-  fxch(2);\n-  fstp_d(Address(eax, 0));\n-  fmula(1);\n-  faddp(1);\n-  fstp_d(Address(eax, 8));\n-  jcc(Assembler::equal, B1_21);\n-\n-  bind(B1_20);\n-  fldcw(Address(esp, 30));\n-\n-  bind(B1_21);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_22);\n-  fxch(1);\n-  fmulp(4);\n-  testl(ebx, ebx);\n-  fxch(2);\n-  fmul(3);\n-  movl(eax, Address(esp, 32));\n-  faddp(3);\n-  fxch(2);\n-  fstp_d(Address(eax, 8));\n-  fmula(1);\n-  faddp(1);\n-  fstp_d(Address(eax, 0));\n-  jcc(Assembler::equal, B1_24);\n-\n-  bind(B1_23);\n-  fldcw(Address(esp, 30));\n-\n-  bind(B1_24);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_25);\n-  testb(Address(esp, 36), 2);\n-  jcc(Assembler::equal, B1_33);\n-\n-  bind(B1_26);\n-  fld_s(0);\n-  testb(edx, 2);\n-  fmul(1);\n-  fld_s(0);\n-  fmul(1);\n-  jcc(Assembler::equal, B1_30);\n-\n-  bind(B1_27);\n-  fstp_d(2);\n-  fld_x(ExternalAddress(84 + CP));    \/\/0x3ac6, 0x0ba0, 0x07\n-  testl(ebx, ebx);\n-  fmul(2);\n-  fld_x(ExternalAddress(72 + CP));    \/\/0xdaba, 0xfe79, 0xea\n-  fmul(3);\n-  fld_x(ExternalAddress(60 + CP));    \/\/0xd84d, 0xadee, 0xc6\n-  movl(eax, Address(rsp, 32));\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(48 + CP));    \/\/0x03fe, 0x3f65, 0x7d\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(36 + CP));    \/\/0xf024, 0x0cac, 0x00\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(24 + CP));    \/\/0x9c2f, 0x0b60, 0x60\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(12 + CP));    \/\/0xaaa5, 0xaaaa, 0xaa\n-  faddp(2);\n-  fxch(1);\n-  fmulp(3);\n-  fld_x(ExternalAddress(CP));    \/\/0x0000, 0x0000, 0x00\n-  faddp(1);\n-  fmulp(1);\n-  faddp(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(rsi, Address::times_8));\n-  fmula(1);\n-  faddp(1);\n-  fstp_d(Address(eax, 8));\n-  jcc(Assembler::equal, B1_29);\n-\n-  bind(B1_28);\n-  fldcw(Address(esp, 30));\n-\n-  bind(B1_29);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_30);\n-  fld_x(ExternalAddress(84 + SP));    \/\/0x8610, 0x307f, 0x62\n-  testl(ebx, ebx);\n-  fmul(1);\n-  fld_x(ExternalAddress(72 + SP));    \/\/0x44a6, 0xed1a, 0x29\n-  fmul(2);\n-  fld_x(ExternalAddress(60 + SP));    \/\/0xbf33, 0x8bb4, 0x2f\n-  movl(eax, Address(rsp, 32));\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(48 + SP));    \/\/0x825b, 0x3997, 0x2b\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(36 + SP));    \/\/0x45f6, 0xb616, 0x1d\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + SP));    \/\/0xc527, 0x0d00, 0x00\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(12 + SP));    \/\/0x8887, 0x8888, 0x88\n-  faddp(2);\n-  fxch(1);\n-  fmulp(2);\n-  fld_x(ExternalAddress(SP));    \/\/0xaaab, 0xaaaa, 0xaa\n-  faddp(1);\n-  fmulp(2);\n-  faddp(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(rsi, Address::times_8));\n-  fmulp(2);\n-  fmul(1);\n-  faddp(1);\n-  fstp_d(Address(eax, 8));\n-  jcc(Assembler::equal, B1_32);\n-\n-  bind(B1_31);\n-  fldcw(Address(esp, 30));\n-\n-  bind(B1_32);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_33);\n-  testb(Address(esp, 36), 1);\n-  jcc(Assembler::equal, B1_41);\n-\n-  bind(B1_34);\n-  fld_s(0);\n-  testb(edx, 2);\n-  fmul(1);\n-  fld_s(0);\n-  fmul(1);\n-  jcc(Assembler::equal, B1_38);\n-\n-  bind(B1_35);\n-  fld_x(ExternalAddress(84 + SP));    \/\/0x8610, 0x307f, 0x62\n-  testl(ebx, ebx);\n-  fmul(1);\n-  fld_x(ExternalAddress(72 + SP));    \/\/0x44a6, 0xed1a, 0x29\n-  fmul(2);\n-  fld_x(ExternalAddress(60 + SP));    \/\/0xbf33, 0x8bb4, 0x2f\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(48 + SP));    \/\/0x825b, 0x3997, 0x2b\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(36 + SP));    \/\/0x45f6, 0xb616, 0x1d\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + SP));    \/\/0xc527, 0x0d00, 0x00\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(12 + SP));    \/\/0x8887, 0x8888, 0x88\n-  faddp(2);\n-  fxch(1);\n-  fmulp(2);\n-  fld_x(ExternalAddress(SP));    \/\/0xaaab, 0xaaaa, 0xaa\n-  faddp(1);\n-  fmulp(2);\n-  faddp(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmulp(2);\n-  fmul(1);\n-  movl(eax, Address(esp, 32));\n-  faddp(1);\n-  fstp_d(Address(eax, 0));\n-  jcc(Assembler::equal, B1_37);\n-\n-  bind(B1_36);\n-  fldcw(Address(esp, 30));\n-\n-  bind(B1_37);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_38);\n-  fstp_d(2);\n-  fld_x(ExternalAddress(84 + CP));    \/\/0x3ac6, 0x0ba0, 0x07\n-  testl(ebx, ebx);\n-  fmul(2);\n-  fld_x(ExternalAddress(72 + CP));    \/\/0xdaba, 0xfe79, 0xea\n-  fmul(3);\n-  fld_x(ExternalAddress(60 + CP));    \/\/0xd84d, 0xadee, 0xc6\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(48 + CP));    \/\/0x03fe, 0x3f65, 0x7d\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(36 + CP));    \/\/0xf024, 0x0cac, 0x00\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(24 + CP));    \/\/0x9c2f, 0x0b60, 0x60\n-  faddp(2);\n-  fxch(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(12 + CP));    \/\/0xaaa5, 0xaaaa, 0xaa\n-  faddp(2);\n-  fxch(1);\n-  fmulp(3);\n-  fld_x(ExternalAddress(CP));    \/\/0x0000, 0x0000, 0x00\n-  faddp(1);\n-  fmulp(1);\n-  faddp(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(1);\n-  movl(eax, Address(esp, 32));\n-  faddp(1);\n-  fstp_d(Address(eax, 0));\n-  jcc(Assembler::equal, B1_40);\n-\n-  bind(B1_39);\n-  fldcw(Address(esp, 30));\n-  bind(B1_40);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-  bind(B1_41);\n-  fstp_d(0);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-  bind(B1_42);\n-  xorl(ebx, ebx);\n-  jmp(B1_8);\n-  bind(B1_43);\n-  xorl(ebx, ebx);\n-  jmp(B1_15);\n-}\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table_sin[] =\n-{\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x3ff00000UL, 0x176d6d31UL, 0xbf73b92eUL,\n-    0xbc29b42cUL, 0x3fb917a6UL, 0xe0000000UL, 0xbc3e2718UL, 0x00000000UL,\n-    0x3ff00000UL, 0x011469fbUL, 0xbf93ad06UL, 0x3c69a60bUL, 0x3fc8f8b8UL,\n-    0xc0000000UL, 0xbc626d19UL, 0x00000000UL, 0x3ff00000UL, 0x939d225aUL,\n-    0xbfa60beaUL, 0x2ed59f06UL, 0x3fd29406UL, 0xa0000000UL, 0xbc75d28dUL,\n-    0x00000000UL, 0x3ff00000UL, 0x866b95cfUL, 0xbfb37ca1UL, 0xa6aea963UL,\n-    0x3fd87de2UL, 0xe0000000UL, 0xbc672cedUL, 0x00000000UL, 0x3ff00000UL,\n-    0x73fa1279UL, 0xbfbe3a68UL, 0x3806f63bUL, 0x3fde2b5dUL, 0x20000000UL,\n-    0x3c5e0d89UL, 0x00000000UL, 0x3ff00000UL, 0x5bc57974UL, 0xbfc59267UL,\n-    0x39ae68c8UL, 0x3fe1c73bUL, 0x20000000UL, 0x3c8b25ddUL, 0x00000000UL,\n-    0x3ff00000UL, 0x53aba2fdUL, 0xbfcd0dfeUL, 0x25091dd6UL, 0x3fe44cf3UL,\n-    0x20000000UL, 0x3c68076aUL, 0x00000000UL, 0x3ff00000UL, 0x99fcef32UL,\n-    0x3fca8279UL, 0x667f3bcdUL, 0x3fe6a09eUL, 0x20000000UL, 0xbc8bdd34UL,\n-    0x00000000UL, 0x3fe00000UL, 0x94247758UL, 0x3fc133ccUL, 0x6b151741UL,\n-    0x3fe8bc80UL, 0x20000000UL, 0xbc82c5e1UL, 0x00000000UL, 0x3fe00000UL,\n-    0x9ae68c87UL, 0x3fac73b3UL, 0x290ea1a3UL, 0x3fea9b66UL, 0xe0000000UL,\n-    0x3c39f630UL, 0x00000000UL, 0x3fe00000UL, 0x7f909c4eUL, 0xbf9d4a2cUL,\n-    0xf180bdb1UL, 0x3fec38b2UL, 0x80000000UL, 0xbc76e0b1UL, 0x00000000UL,\n-    0x3fe00000UL, 0x65455a75UL, 0xbfbe0875UL, 0xcf328d46UL, 0x3fed906bUL,\n-    0x20000000UL, 0x3c7457e6UL, 0x00000000UL, 0x3fe00000UL, 0x76acf82dUL,\n-    0x3fa4a031UL, 0x56c62ddaUL, 0x3fee9f41UL, 0xe0000000UL, 0x3c8760b1UL,\n-    0x00000000UL, 0x3fd00000UL, 0x0e5967d5UL, 0xbfac1d1fUL, 0xcff75cb0UL,\n-    0x3fef6297UL, 0x20000000UL, 0x3c756217UL, 0x00000000UL, 0x3fd00000UL,\n-    0x0f592f50UL, 0xbf9ba165UL, 0xa3d12526UL, 0x3fefd88dUL, 0x40000000UL,\n-    0xbc887df6UL, 0x00000000UL, 0x3fc00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x0f592f50UL, 0x3f9ba165UL, 0xa3d12526UL, 0x3fefd88dUL,\n-    0x40000000UL, 0xbc887df6UL, 0x00000000UL, 0xbfc00000UL, 0x0e5967d5UL,\n-    0x3fac1d1fUL, 0xcff75cb0UL, 0x3fef6297UL, 0x20000000UL, 0x3c756217UL,\n-    0x00000000UL, 0xbfd00000UL, 0x76acf82dUL, 0xbfa4a031UL, 0x56c62ddaUL,\n-    0x3fee9f41UL, 0xe0000000UL, 0x3c8760b1UL, 0x00000000UL, 0xbfd00000UL,\n-    0x65455a75UL, 0x3fbe0875UL, 0xcf328d46UL, 0x3fed906bUL, 0x20000000UL,\n-    0x3c7457e6UL, 0x00000000UL, 0xbfe00000UL, 0x7f909c4eUL, 0x3f9d4a2cUL,\n-    0xf180bdb1UL, 0x3fec38b2UL, 0x80000000UL, 0xbc76e0b1UL, 0x00000000UL,\n-    0xbfe00000UL, 0x9ae68c87UL, 0xbfac73b3UL, 0x290ea1a3UL, 0x3fea9b66UL,\n-    0xe0000000UL, 0x3c39f630UL, 0x00000000UL, 0xbfe00000UL, 0x94247758UL,\n-    0xbfc133ccUL, 0x6b151741UL, 0x3fe8bc80UL, 0x20000000UL, 0xbc82c5e1UL,\n-    0x00000000UL, 0xbfe00000UL, 0x99fcef32UL, 0xbfca8279UL, 0x667f3bcdUL,\n-    0x3fe6a09eUL, 0x20000000UL, 0xbc8bdd34UL, 0x00000000UL, 0xbfe00000UL,\n-    0x53aba2fdUL, 0x3fcd0dfeUL, 0x25091dd6UL, 0x3fe44cf3UL, 0x20000000UL,\n-    0x3c68076aUL, 0x00000000UL, 0xbff00000UL, 0x5bc57974UL, 0x3fc59267UL,\n-    0x39ae68c8UL, 0x3fe1c73bUL, 0x20000000UL, 0x3c8b25ddUL, 0x00000000UL,\n-    0xbff00000UL, 0x73fa1279UL, 0x3fbe3a68UL, 0x3806f63bUL, 0x3fde2b5dUL,\n-    0x20000000UL, 0x3c5e0d89UL, 0x00000000UL, 0xbff00000UL, 0x866b95cfUL,\n-    0x3fb37ca1UL, 0xa6aea963UL, 0x3fd87de2UL, 0xe0000000UL, 0xbc672cedUL,\n-    0x00000000UL, 0xbff00000UL, 0x939d225aUL, 0x3fa60beaUL, 0x2ed59f06UL,\n-    0x3fd29406UL, 0xa0000000UL, 0xbc75d28dUL, 0x00000000UL, 0xbff00000UL,\n-    0x011469fbUL, 0x3f93ad06UL, 0x3c69a60bUL, 0x3fc8f8b8UL, 0xc0000000UL,\n-    0xbc626d19UL, 0x00000000UL, 0xbff00000UL, 0x176d6d31UL, 0x3f73b92eUL,\n-    0xbc29b42cUL, 0x3fb917a6UL, 0xe0000000UL, 0xbc3e2718UL, 0x00000000UL,\n-    0xbff00000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0xbff00000UL, 0x176d6d31UL,\n-    0x3f73b92eUL, 0xbc29b42cUL, 0xbfb917a6UL, 0xe0000000UL, 0x3c3e2718UL,\n-    0x00000000UL, 0xbff00000UL, 0x011469fbUL, 0x3f93ad06UL, 0x3c69a60bUL,\n-    0xbfc8f8b8UL, 0xc0000000UL, 0x3c626d19UL, 0x00000000UL, 0xbff00000UL,\n-    0x939d225aUL, 0x3fa60beaUL, 0x2ed59f06UL, 0xbfd29406UL, 0xa0000000UL,\n-    0x3c75d28dUL, 0x00000000UL, 0xbff00000UL, 0x866b95cfUL, 0x3fb37ca1UL,\n-    0xa6aea963UL, 0xbfd87de2UL, 0xe0000000UL, 0x3c672cedUL, 0x00000000UL,\n-    0xbff00000UL, 0x73fa1279UL, 0x3fbe3a68UL, 0x3806f63bUL, 0xbfde2b5dUL,\n-    0x20000000UL, 0xbc5e0d89UL, 0x00000000UL, 0xbff00000UL, 0x5bc57974UL,\n-    0x3fc59267UL, 0x39ae68c8UL, 0xbfe1c73bUL, 0x20000000UL, 0xbc8b25ddUL,\n-    0x00000000UL, 0xbff00000UL, 0x53aba2fdUL, 0x3fcd0dfeUL, 0x25091dd6UL,\n-    0xbfe44cf3UL, 0x20000000UL, 0xbc68076aUL, 0x00000000UL, 0xbff00000UL,\n-    0x99fcef32UL, 0xbfca8279UL, 0x667f3bcdUL, 0xbfe6a09eUL, 0x20000000UL,\n-    0x3c8bdd34UL, 0x00000000UL, 0xbfe00000UL, 0x94247758UL, 0xbfc133ccUL,\n-    0x6b151741UL, 0xbfe8bc80UL, 0x20000000UL, 0x3c82c5e1UL, 0x00000000UL,\n-    0xbfe00000UL, 0x9ae68c87UL, 0xbfac73b3UL, 0x290ea1a3UL, 0xbfea9b66UL,\n-    0xe0000000UL, 0xbc39f630UL, 0x00000000UL, 0xbfe00000UL, 0x7f909c4eUL,\n-    0x3f9d4a2cUL, 0xf180bdb1UL, 0xbfec38b2UL, 0x80000000UL, 0x3c76e0b1UL,\n-    0x00000000UL, 0xbfe00000UL, 0x65455a75UL, 0x3fbe0875UL, 0xcf328d46UL,\n-    0xbfed906bUL, 0x20000000UL, 0xbc7457e6UL, 0x00000000UL, 0xbfe00000UL,\n-    0x76acf82dUL, 0xbfa4a031UL, 0x56c62ddaUL, 0xbfee9f41UL, 0xe0000000UL,\n-    0xbc8760b1UL, 0x00000000UL, 0xbfd00000UL, 0x0e5967d5UL, 0x3fac1d1fUL,\n-    0xcff75cb0UL, 0xbfef6297UL, 0x20000000UL, 0xbc756217UL, 0x00000000UL,\n-    0xbfd00000UL, 0x0f592f50UL, 0x3f9ba165UL, 0xa3d12526UL, 0xbfefd88dUL,\n-    0x40000000UL, 0x3c887df6UL, 0x00000000UL, 0xbfc00000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0xbff00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x0f592f50UL, 0xbf9ba165UL, 0xa3d12526UL,\n-    0xbfefd88dUL, 0x40000000UL, 0x3c887df6UL, 0x00000000UL, 0x3fc00000UL,\n-    0x0e5967d5UL, 0xbfac1d1fUL, 0xcff75cb0UL, 0xbfef6297UL, 0x20000000UL,\n-    0xbc756217UL, 0x00000000UL, 0x3fd00000UL, 0x76acf82dUL, 0x3fa4a031UL,\n-    0x56c62ddaUL, 0xbfee9f41UL, 0xe0000000UL, 0xbc8760b1UL, 0x00000000UL,\n-    0x3fd00000UL, 0x65455a75UL, 0xbfbe0875UL, 0xcf328d46UL, 0xbfed906bUL,\n-    0x20000000UL, 0xbc7457e6UL, 0x00000000UL, 0x3fe00000UL, 0x7f909c4eUL,\n-    0xbf9d4a2cUL, 0xf180bdb1UL, 0xbfec38b2UL, 0x80000000UL, 0x3c76e0b1UL,\n-    0x00000000UL, 0x3fe00000UL, 0x9ae68c87UL, 0x3fac73b3UL, 0x290ea1a3UL,\n-    0xbfea9b66UL, 0xe0000000UL, 0xbc39f630UL, 0x00000000UL, 0x3fe00000UL,\n-    0x94247758UL, 0x3fc133ccUL, 0x6b151741UL, 0xbfe8bc80UL, 0x20000000UL,\n-    0x3c82c5e1UL, 0x00000000UL, 0x3fe00000UL, 0x99fcef32UL, 0x3fca8279UL,\n-    0x667f3bcdUL, 0xbfe6a09eUL, 0x20000000UL, 0x3c8bdd34UL, 0x00000000UL,\n-    0x3fe00000UL, 0x53aba2fdUL, 0xbfcd0dfeUL, 0x25091dd6UL, 0xbfe44cf3UL,\n-    0x20000000UL, 0xbc68076aUL, 0x00000000UL, 0x3ff00000UL, 0x5bc57974UL,\n-    0xbfc59267UL, 0x39ae68c8UL, 0xbfe1c73bUL, 0x20000000UL, 0xbc8b25ddUL,\n-    0x00000000UL, 0x3ff00000UL, 0x73fa1279UL, 0xbfbe3a68UL, 0x3806f63bUL,\n-    0xbfde2b5dUL, 0x20000000UL, 0xbc5e0d89UL, 0x00000000UL, 0x3ff00000UL,\n-    0x866b95cfUL, 0xbfb37ca1UL, 0xa6aea963UL, 0xbfd87de2UL, 0xe0000000UL,\n-    0x3c672cedUL, 0x00000000UL, 0x3ff00000UL, 0x939d225aUL, 0xbfa60beaUL,\n-    0x2ed59f06UL, 0xbfd29406UL, 0xa0000000UL, 0x3c75d28dUL, 0x00000000UL,\n-    0x3ff00000UL, 0x011469fbUL, 0xbf93ad06UL, 0x3c69a60bUL, 0xbfc8f8b8UL,\n-    0xc0000000UL, 0x3c626d19UL, 0x00000000UL, 0x3ff00000UL, 0x176d6d31UL,\n-    0xbf73b92eUL, 0xbc29b42cUL, 0xbfb917a6UL, 0xe0000000UL, 0x3c3e2718UL,\n-    0x00000000UL, 0x3ff00000UL, 0x55555555UL, 0xbfc55555UL, 0x00000000UL,\n-    0xbfe00000UL, 0x11111111UL, 0x3f811111UL, 0x55555555UL, 0x3fa55555UL,\n-    0x1a01a01aUL, 0xbf2a01a0UL, 0x16c16c17UL, 0xbf56c16cUL, 0xa556c734UL,\n-    0x3ec71de3UL, 0x1a01a01aUL, 0x3efa01a0UL, 0x1a600000UL, 0x3d90b461UL,\n-    0x1a600000UL, 0x3d90b461UL, 0x54400000UL, 0x3fb921fbUL, 0x00000000UL,\n-    0x00000000UL, 0x2e037073UL, 0x3b63198aUL, 0x00000000UL, 0x00000000UL,\n-    0x6dc9c883UL, 0x40245f30UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x43380000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x43600000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x3c800000UL, 0x00000000UL,\n-    0x00000000UL, 0xffffffffUL, 0x3fefffffUL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x80000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x80000000UL, 0x00000000UL, 0x80000000UL, 0x00000000UL, 0x3fe00000UL,\n-    0x00000000UL, 0x3fe00000UL\n-};\n-\n-void MacroAssembler::fast_sin(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                              XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                              Register eax, Register ebx, Register edx) {\n-\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label L_2TAG_PACKET_4_0_2;\n-\n-  assert_different_registers(eax, ebx, edx);\n-\n-  address static_const_table_sin = (address)_static_const_table_sin;\n-\n-  subl(rsp, 120);\n-  movl(Address(rsp, 56), ebx);\n-  lea(ebx, ExternalAddress(static_const_table_sin));\n-  movsd(xmm0, Address(rsp, 128));\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32767);\n-  subl(eax, 12336);\n-  cmpl(eax, 4293);\n-  jcc(Assembler::above, L_2TAG_PACKET_0_0_2);\n-  movsd(xmm1, Address(ebx, 2160));\n-  mulsd(xmm1, xmm0);\n-  movsd(xmm5, Address(ebx, 2272));\n-  movdqu(xmm4, Address(ebx, 2256));\n-  pand(xmm4, xmm0);\n-  por(xmm5, xmm4);\n-  movsd(xmm3, Address(ebx, 2128));\n-  movdqu(xmm2, Address(ebx, 2112));\n-  addpd(xmm1, xmm5);\n-  cvttsd2sil(edx, xmm1);\n-  cvtsi2sdl(xmm1, edx);\n-  mulsd(xmm3, xmm1);\n-  unpcklpd(xmm1, xmm1);\n-  addl(edx, 1865216);\n-  movdqu(xmm4, xmm0);\n-  andl(edx, 63);\n-  movdqu(xmm5, Address(ebx, 2096));\n-  lea(eax, Address(ebx, 0));\n-  shll(edx, 5);\n-  addl(eax, edx);\n-  mulpd(xmm2, xmm1);\n-  subsd(xmm0, xmm3);\n-  mulsd(xmm1, Address(ebx, 2144));\n-  subsd(xmm4, xmm3);\n-  movsd(xmm7, Address(eax, 8));\n-  unpcklpd(xmm0, xmm0);\n-  movapd(xmm3, xmm4);\n-  subsd(xmm4, xmm2);\n-  mulpd(xmm5, xmm0);\n-  subpd(xmm0, xmm2);\n-  movdqu(xmm6, Address(ebx, 2064));\n-  mulsd(xmm7, xmm4);\n-  subsd(xmm3, xmm4);\n-  mulpd(xmm5, xmm0);\n-  mulpd(xmm0, xmm0);\n-  subsd(xmm3, xmm2);\n-  movdqu(xmm2, Address(eax, 0));\n-  subsd(xmm1, xmm3);\n-  movsd(xmm3, Address(eax, 24));\n-  addsd(xmm2, xmm3);\n-  subsd(xmm7, xmm2);\n-  mulsd(xmm2, xmm4);\n-  mulpd(xmm6, xmm0);\n-  mulsd(xmm3, xmm4);\n-  mulpd(xmm2, xmm0);\n-  mulpd(xmm0, xmm0);\n-  addpd(xmm5, Address(ebx, 2080));\n-  mulsd(xmm4, Address(eax, 0));\n-  addpd(xmm6, Address(ebx, 2048));\n-  mulpd(xmm5, xmm0);\n-  movapd(xmm0, xmm3);\n-  addsd(xmm3, Address(eax, 8));\n-  mulpd(xmm1, xmm7);\n-  movapd(xmm7, xmm4);\n-  addsd(xmm4, xmm3);\n-  addpd(xmm6, xmm5);\n-  movsd(xmm5, Address(eax, 8));\n-  subsd(xmm5, xmm3);\n-  subsd(xmm3, xmm4);\n-  addsd(xmm1, Address(eax, 16));\n-  mulpd(xmm6, xmm2);\n-  addsd(xmm5, xmm0);\n-  addsd(xmm3, xmm7);\n-  addsd(xmm1, xmm5);\n-  addsd(xmm1, xmm3);\n-  addsd(xmm1, xmm6);\n-  unpckhpd(xmm6, xmm6);\n-  addsd(xmm1, xmm6);\n-  addsd(xmm4, xmm1);\n-  movsd(Address(rsp, 0), xmm4);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  jcc(Assembler::greater, L_2TAG_PACKET_2_0_2);\n-  shrl(eax, 4);\n-  cmpl(eax, 268434685);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_3_0_2);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  movsd(xmm3, Address(ebx, 2192));\n-  mulsd(xmm3, xmm0);\n-  subsd(xmm3, xmm0);\n-  mulsd(xmm3, Address(ebx, 2208));\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movl(eax, Address(rsp, 132));\n-  andl(eax, 2146435072);\n-  cmpl(eax, 2146435072);\n-  jcc(Assembler::equal, L_2TAG_PACKET_4_0_2);\n-  subl(rsp, 32);\n-  movsd(Address(rsp, 0), xmm0);\n-  lea(eax, Address(rsp, 40));\n-  movl(Address(rsp, 8), eax);\n-  movl(eax, 2);\n-  movl(Address(rsp, 12), eax);\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlibm_sin_cos_huge())));\n-  addl(rsp, 32);\n-  fld_d(Address(rsp, 16));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-  bind(L_2TAG_PACKET_4_0_2);\n-  fld_d(Address(rsp, 128));\n-  fmul_d(Address(ebx, 2240));\n-  bind(L_2TAG_PACKET_1_0_2);\n-  movl(ebx, Address(rsp, 56));\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_sin.cpp","additions":0,"deletions":1743,"binary":false,"changes":1743,"status":"deleted"},{"patch":"@@ -1,1173 +0,0 @@\n-\/*\n-* Copyright (c) 2016, 2021, Intel Corporation. All rights reserved.\n-* Intel Math Library (LIBM) Source Code\n-*\n-* DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-*\n-* This code is free software; you can redistribute it and\/or modify it\n-* under the terms of the GNU General Public License version 2 only, as\n-* published by the Free Software Foundation.\n-*\n-* This code is distributed in the hope that it will be useful, but WITHOUT\n-* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-* version 2 for more details (a copy is included in the LICENSE file that\n-* accompanied this code).\n-*\n-* You should have received a copy of the GNU General Public License version\n-* 2 along with this work; if not, write to the Free Software Foundation,\n-* Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-*\n-* Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-* or visit www.oracle.com if you need additional information or have any\n-* questions.\n-*\n-*\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/assembler.hpp\"\n-#include \"asm\/assembler.inline.hpp\"\n-#include \"macroAssembler_x86.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-\n-\/******************************************************************************\/\n-\/\/                     ALGORITHM DESCRIPTION - TAN()\n-\/\/                     ---------------------\n-\/\/\n-\/\/ Polynomials coefficients and other constants.\n-\/\/\n-\/\/ Note that in this algorithm, there is a different polynomial for\n-\/\/ each breakpoint, so there are 32 sets of polynomial coefficients\n-\/\/ as well as 32 instances of the other constants.\n-\/\/\n-\/\/ The polynomial coefficients and constants are offset from the start\n-\/\/ of the main block as follows:\n-\/\/\n-\/\/   0:  c8 | c0\n-\/\/  16:  c9 | c1\n-\/\/  32: c10 | c2\n-\/\/  48: c11 | c3\n-\/\/  64: c12 | c4\n-\/\/  80: c13 | c5\n-\/\/  96: c14 | c6\n-\/\/ 112: c15 | c7\n-\/\/ 128: T_hi\n-\/\/ 136: T_lo\n-\/\/ 144: Sigma\n-\/\/ 152: T_hl\n-\/\/ 160: Tau\n-\/\/ 168: Mask\n-\/\/ 176: (end of block)\n-\/\/\n-\/\/ The total table size is therefore 5632 bytes.\n-\/\/\n-\/\/ Note that c0 and c1 are always zero. We could try storing\n-\/\/ other constants here, and just loading the low part of the\n-\/\/ SIMD register in these cases, after ensuring the high part\n-\/\/ is zero.\n-\/\/\n-\/\/ The higher terms of the polynomial are computed in the *low*\n-\/\/ part of the SIMD register. This is so we can overlap the\n-\/\/ multiplication by r^8 and the unpacking of the other part.\n-\/\/\n-\/\/ The constants are:\n-\/\/ T_hi + T_lo = accurate constant term in power series\n-\/\/ Sigma + T_hl = accurate coefficient of r in power series (Sigma=1 bit)\n-\/\/ Tau = multiplier for the reciprocal, always -1 or 0\n-\/\/\n-\/\/ The basic reconstruction formula using these constants is:\n-\/\/\n-\/\/ High = tau * recip_hi + t_hi\n-\/\/ Med = (sgn * r + t_hl * r)_hi\n-\/\/ Low = (sgn * r + t_hl * r)_lo +\n-\/\/       tau * recip_lo + T_lo + (T_hl + sigma) * c + pol\n-\/\/\n-\/\/ where pol = c0 + c1 * r + c2 * r^2 + ... + c15 * r^15\n-\/\/\n-\/\/ (c0 = c1 = 0, but using them keeps SIMD regularity)\n-\/\/\n-\/\/ We then do a compensated sum High + Med, add the low parts together\n-\/\/ and then do the final sum.\n-\/\/\n-\/\/ Here recip_hi + recip_lo is an accurate reciprocal of the remainder\n-\/\/ modulo pi\/2\n-\/\/\n-\/\/ Special cases:\n-\/\/  tan(NaN) = quiet NaN, and raise invalid exception\n-\/\/  tan(INF) = NaN and raise invalid exception\n-\/\/  tan(+\/-0) = +\/-0\n-\/\/\n-\/******************************************************************************\/\n-\n-\/\/ The 32 bit code is at most SSE2 compliant\n-\n-ATTRIBUTE_ALIGNED(16) static const jushort _TP[] =\n-{\n-    0x4cd6, 0xaf6c, 0xc710, 0xc662, 0xbffd, 0x0000, 0x4b06, 0xb0ac, 0xd3b2, 0xcc2c,\n-    0x3ff9, 0x0000, 0x00e3, 0xc850, 0xaa28, 0x9533, 0xbff3, 0x0000, 0x2ff0, 0x466d,\n-    0x1a3b, 0xb266, 0x3fe5, 0x0000\n-};\n-\n-ATTRIBUTE_ALIGNED(16) static const jushort _TQ[] =\n-{\n-    0x399c, 0x8391, 0x154c, 0x94ca, 0xbfff, 0x0000, 0xb6a3, 0xc36a, 0x44e2, 0x8a2c,\n-    0x3ffe, 0x0000, 0xb70f, 0xd068, 0xa6ce, 0xe9dd, 0xbff9, 0x0000, 0x820f, 0x51ce,\n-    0x7d76, 0x9bff, 0x3ff3, 0x0000\n-};\n-\n-ATTRIBUTE_ALIGNED(16) static const jushort _GP[] =\n-{\n-    0xaaab, 0xaaaa, 0xaaaa, 0xaaaa, 0xbffd, 0x0000, 0xb62f, 0x0b60, 0x60b6, 0xb60b,\n-    0xbff9, 0x0000, 0xdfa7, 0x08aa, 0x55e0, 0x8ab3, 0xbff6, 0x0000, 0x85a0, 0xa819,\n-    0xbc99, 0xddeb, 0xbff2, 0x0000, 0x7065, 0x6a37, 0x795f, 0xb354, 0xbfef, 0x0000,\n-    0xa8f9, 0x83f1, 0x2ec8, 0x9140, 0xbfec, 0x0000, 0xf3ca, 0x8c96, 0x8e0b, 0xeb6d,\n-    0xbfe8, 0x0000, 0x355b, 0xd910, 0x67c9, 0xbed3, 0xbfe5, 0x0000, 0x286b, 0xb49e,\n-    0xb854, 0x9a98, 0xbfe2, 0x0000, 0x0871, 0x1a2f, 0x6477, 0xfcc4, 0xbfde, 0x0000,\n-    0xa559, 0x1da9, 0xaed2, 0xba76, 0xbfdb, 0x0000, 0x00a3, 0x7fea, 0x9bc3, 0xf205,\n-    0xbfd8, 0x0000\n-};\n-\n-void MacroAssembler::libm_tancot_huge(XMMRegister xmm0, XMMRegister xmm1, Register eax, Register ecx, Register edx, Register ebx, Register esi, Register edi, Register ebp, Register esp) {\n-  Label B1_1, B1_2, B1_3, B1_4, B1_5, B1_6, B1_7, B1_8, B1_9, B1_10, B1_11, B1_12;\n-  Label B1_13, B1_14, B1_15, B1_16, B1_17, B1_18, B1_19, B1_20, B1_21, B1_22, B1_23;\n-  Label B1_24, B1_25, B1_26, B1_27, B1_28, B1_29, B1_30, B1_31, B1_32, B1_33, B1_34;\n-  Label B1_35, B1_36, B1_37, B1_38, B1_39, B1_40, B1_43;\n-\n-  assert_different_registers(ebx, eax, ecx, edx, esi, edi, ebp, esp);\n-\n-  address TP = (address)_TP;\n-  address TQ = (address)_TQ;\n-  address GP = (address)_GP;\n-\n-  bind(B1_1);\n-  push(ebp);\n-  movl(ebp, esp);\n-  andl(esp, -64);\n-  push(esi);\n-  push(edi);\n-  push(ebx);\n-  subl(esp, 52);\n-  movl(eax, Address(ebp, 16));\n-  movl(ebx, Address(ebp, 20));\n-  movl(Address(esp, 40), eax);\n-\n-  bind(B1_2);\n-  fnstcw(Address(esp, 38));\n-\n-  bind(B1_3);\n-  movl(edx, Address(ebp, 12));\n-  movl(eax, edx);\n-  andl(eax, 2147483647);\n-  shrl(edx, 31);\n-  movl(Address(esp, 44), edx);\n-  cmpl(eax, 1104150528);\n-  jcc(Assembler::aboveEqual, B1_11);\n-\n-  bind(B1_4);\n-  movsd(xmm1, Address(ebp, 8));\n-  movzwl(ecx, Address(esp, 38));\n-  movl(edx, ecx);\n-  andl(edx, 768);\n-  andps(xmm1, ExternalAddress(L_2IL0FLOATPACKET_0));    \/\/0xffffffffUL, 0x7fffffffUL, 0x00000000UL, 0x00000000UL\n-  cmpl(edx, 768);\n-  movsd(xmm0, ExternalAddress(PI4_INV));    \/\/\/\/0x6dc9c883UL, 0x3ff45f30UL\n-  mulsd(xmm0, xmm1);\n-  movsd(Address(ebp, 8), xmm1);\n-  movsd(Address(esp, 0), xmm0);\n-  jcc(Assembler::equal, B1_39);\n-\n-  bind(B1_5);\n-  orl(ecx, -64768);\n-  movw(Address(esp, 36), ecx);\n-\n-  bind(B1_6);\n-  fldcw(Address(esp, 36));\n-\n-  bind(B1_7);\n-  movsd(xmm1, Address(ebp, 8));\n-  movl(edi, 1);\n-\n-  bind(B1_8);\n-  movl(Address(esp, 12), esi);\n-  movl(esi, Address(esp, 4));\n-  movl(edx, esi);\n-  movl(Address(esp, 24), edi);\n-  movl(edi, esi);\n-  shrl(edi, 20);\n-  andl(edx, 1048575);\n-  movl(ecx, edi);\n-  orl(edx, 1048576);\n-  negl(ecx);\n-  addl(edi, 13);\n-  movl(Address(esp, 8), ebx);\n-  addl(ecx, 19);\n-  movl(ebx, edx);\n-  movl(Address(esp, 28), ecx);\n-  shrl(ebx);\n-  movl(ecx, edi);\n-  shll(edx);\n-  movl(ecx, Address(esp, 28));\n-  movl(edi, Address(esp, 0));\n-  shrl(edi);\n-  orl(edx, edi);\n-  cmpl(esi, 1094713344);\n-  movsd(Address(esp, 16), xmm1);\n-  fld_d(Address(esp, 16));\n-  cmov32(Assembler::below, edx, ebx);\n-  movl(edi, Address(esp, 24));\n-  movl(esi, Address(esp, 12));\n-  lea(ebx, Address(edx, 1));\n-  andl(ebx, -2);\n-  movl(Address(esp, 16), ebx);\n-  cmpl(eax, 1094713344);\n-  fild_s(Address(esp, 16));\n-  movl(ebx, Address(esp, 8));\n-  jcc(Assembler::aboveEqual, B1_10);\n-\n-  bind(B1_9);\n-  fld_d(ExternalAddress(PI4X3));    \/\/0x54443000UL, 0xbfe921fbUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X3 + 8));    \/\/0x3b39a000UL, 0x3d373dcbUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X3 + 16));    \/\/0xe0e68948UL, 0xba845c06UL\n-  fmulp(1);\n-  faddp(1);\n-  jmp(B1_17);\n-\n-  bind(B1_10);\n-  fld_d(ExternalAddress(PI4X4));    \/\/0x54400000UL, 0xbfe921fbUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X4 + 8));    \/\/0x1a600000UL, 0xbdc0b461UL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X4 + 16));    \/\/0x2e000000UL, 0xbb93198aUL\n-  fmul(1);\n-  faddp(2);\n-  fld_d(ExternalAddress(PI4X4 + 24));    \/\/0x252049c1UL, 0xb96b839aUL\n-  fmulp(1);\n-  faddp(1);\n-  jmp(B1_17);\n-\n-  bind(B1_11);\n-  movzwl(edx, Address(esp, 38));\n-  movl(eax, edx);\n-  andl(eax, 768);\n-  cmpl(eax, 768);\n-  jcc(Assembler::equal, B1_40);\n-\n-  bind(B1_12);\n-  orl(edx, -64768);\n-  movw(Address(esp, 36), edx);\n-\n-  bind(B1_13);\n-  fldcw(Address(esp, 36));\n-\n-  bind(B1_14);\n-  movl(edi, 1);\n-\n-  bind(B1_15);\n-  movsd(xmm0, Address(ebp, 8));\n-  addl(esp, -32);\n-  andps(xmm0, ExternalAddress(L_2IL0FLOATPACKET_0));    \/\/0xffffffffUL, 0x7fffffffUL, 0x00000000UL, 0x00000000UL\n-  lea(eax, Address(esp, 32));\n-  movsd(Address(eax, 16), xmm0);\n-  fld_d(Address(eax, 16));\n-  fstp_x(Address(esp, 0));\n-  movl(Address(esp, 12), 0);\n-  movl(Address(esp, 16), eax);\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlibm_reduce_pi04l())));\n-\n-  bind(B1_43);\n-  movl(edx, eax);\n-  addl(esp, 32);\n-\n-  bind(B1_16);\n-  fld_d(Address(esp, 0));\n-  fld_d(Address(esp, 8));\n-  faddp(1);\n-\n-  bind(B1_17);\n-  movl(eax, ebx);\n-  andl(eax, 3);\n-  cmpl(eax, 3);\n-  jcc(Assembler::notEqual, B1_24);\n-\n-  bind(B1_18);\n-  fld_d(ExternalAddress(ONES));\n-  incl(edx);\n-  fdiv(1);\n-  testb(edx, 2);\n-  fstp_x(Address(esp, 24));\n-  fld_s(0);\n-  fmul(1);\n-  fld_s(0);\n-  fmul(1);\n-  fld_x(ExternalAddress(36 + TP));    \/\/0x2ff0, 0x466d, 0x1a\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + TP));    \/\/0x00e3, 0xc850, 0xaa\n-  faddp(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(12 + TP));    \/\/0x4b06, 0xb0ac, 0xd3\n-  faddp(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(36 + TQ));    \/\/0x820f, 0x51ce, 0x7d\n-  fmul(3);\n-  fld_x(ExternalAddress(24 + TQ));    \/\/0xb70f, 0xd068, 0xa6\n-  faddp(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(12 + TQ));    \/\/0xb6a3, 0xc36a, 0x44\n-  faddp(1);\n-  fmul(3);\n-  fld_x(ExternalAddress(TQ));    \/\/0x399c, 0x8391, 0x15\n-  faddp(1);\n-  fld_x(ExternalAddress(TP));    \/\/0x4cd6, 0xaf6c, 0xc7\n-  faddp(2);\n-  fld_x(ExternalAddress(132 + GP));    \/\/0x00a3, 0x7fea, 0x9b\n-  fmul(3);\n-  fld_x(ExternalAddress(120 + GP));    \/\/0xa559, 0x1da9, 0xae\n-  fmul(4);\n-  fld_x(ExternalAddress(108 + GP));    \/\/0x0871, 0x1a2f, 0x64\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(96 + GP));    \/\/0x286b, 0xb49e, 0xb8\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(84 + GP));    \/\/0x355b, 0xd910, 0x67\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(72 + GP));    \/\/0x8c96, 0x8e0b, 0xeb\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(60 + GP));    \/\/0xa8f9, 0x83f1, 0x2e\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(48 + GP));    \/\/0x7065, 0x6a37, 0x79\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(36 + GP));    \/\/0x85a0, 0xa819, 0xbc\n-  faddp(2);\n-  fxch(1);\n-  fmul(4);\n-  fld_x(ExternalAddress(24 + GP));    \/\/0xdfa7, 0x08aa, 0x55\n-  faddp(2);\n-  fxch(1);\n-  fmulp(4);\n-  fld_x(ExternalAddress(12 + GP));    \/\/0xb62f, 0x0b60, 0x60\n-  faddp(1);\n-  fmul(4);\n-  fmul(5);\n-  fld_x(ExternalAddress(GP));    \/\/0xaaab, 0xaaaa, 0xaa\n-  faddp(4);\n-  fxch(3);\n-  fmul(5);\n-  faddp(3);\n-  jcc(Assembler::equal, B1_20);\n-\n-  bind(B1_19);\n-  fld_x(Address(esp, 24));\n-  fxch(1);\n-  fdivrp(2);\n-  fxch(1);\n-  fmulp(3);\n-  movl(eax, Address(esp, 44));\n-  xorl(eax, 1);\n-  fxch(2);\n-  fmul(3);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(2);\n-  fmula(3);\n-  fxch(3);\n-  faddp(2);\n-  fxch(1);\n-  fstp_d(Address(esp, 16));\n-  fmul(1);\n-  fxch(1);\n-  fmulp(2);\n-  movsd(xmm0, Address(esp, 16));\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  movsd(xmm1, Address(esp, 16));\n-  jmp(B1_21);\n-\n-  bind(B1_20);\n-  fdivrp(1);\n-  fmulp(2);\n-  fxch(1);\n-  fmul(2);\n-  movl(eax, Address(esp, 44));\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(1);\n-  fmula(3);\n-  fxch(3);\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  fmul(1);\n-  fld_x(Address(esp, 24));\n-  fmulp(2);\n-  movsd(xmm0, Address(esp, 16));\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  movsd(xmm1, Address(esp, 16));\n-\n-  bind(B1_21);\n-  testl(edi, edi);\n-  jcc(Assembler::equal, B1_23);\n-\n-  bind(B1_22);\n-  fldcw(Address(esp, 38));\n-\n-  bind(B1_23);\n-  movl(eax, Address(esp, 40));\n-  movsd(Address(eax, 0), xmm0);\n-  movsd(Address(eax, 8), xmm1);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_24);\n-  testb(ebx, 2);\n-  jcc(Assembler::equal, B1_31);\n-\n-  bind(B1_25);\n-  incl(edx);\n-  fld_s(0);\n-  fmul(1);\n-  testb(edx, 2);\n-  jcc(Assembler::equal, B1_27);\n-\n-  bind(B1_26);\n-  fld_d(ExternalAddress(ONES));\n-  fdiv(2);\n-  fld_s(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(132 + GP));    \/\/0x00a3, 0x7fea, 0x9b\n-  fmul(1);\n-  fld_x(ExternalAddress(120 + GP));    \/\/0xa559, 0x1da9, 0xae\n-  fmul(2);\n-  fld_x(ExternalAddress(108 + GP));    \/\/0x67c9, 0xbed3, 0xbf\n-  movl(eax, Address(esp, 44));\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  xorl(eax, 1);\n-  fld_x(ExternalAddress(96 + GP));    \/\/0x286b, 0xb49e, 0xb8\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(84 + GP));    \/\/0x355b, 0xd910, 0x67\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(72 + GP));    \/\/0xf3ca, 0x8c96, 0x8e\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(60 + GP));    \/\/0xa8f9, 0x83f1, 0x2e\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(48 + GP));    \/\/0x7065, 0x6a37, 0x79\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(36 + GP));    \/\/0x85a0, 0xa819, 0xbc\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + GP));    \/\/0xdfa7, 0x08aa, 0x55\n-  faddp(2);\n-  fxch(1);\n-  fmulp(2);\n-  fld_x(ExternalAddress(12 + GP));    \/\/0xb62f, 0x0b60, 0x60\n-  faddp(1);\n-  fmulp(3);\n-  fld_x(ExternalAddress(GP));    \/\/0xaaab, 0xaaaa, 0xaa\n-  faddp(1);\n-  fmul(3);\n-  fxch(2);\n-  fmulp(3);\n-  fxch(1);\n-  faddp(2);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(2);\n-  fmulp(1);\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  movsd(xmm0, Address(esp, 16));\n-  jmp(B1_28);\n-\n-  bind(B1_27);\n-  fld_x(ExternalAddress(36 + TP));    \/\/0x2ff0, 0x466d, 0x1a\n-  fmul(1);\n-  fld_x(ExternalAddress(24 + TP));    \/\/0x00e3, 0xc850, 0xaa\n-  movl(eax, Address(esp, 44));\n-  faddp(1);\n-  fmul(1);\n-  fld_x(ExternalAddress(36 + TQ));    \/\/0x820f, 0x51ce, 0x7d\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + TQ));    \/\/0xb70f, 0xd068, 0xa6\n-  faddp(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(12 + TQ));    \/\/0xb6a3, 0xc36a, 0x44\n-  faddp(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(TQ));    \/\/0x399c, 0x8391, 0x15\n-  faddp(1);\n-  fld_x(ExternalAddress(12 + TP));    \/\/0x4b06, 0xb0ac, 0xd3\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(TP));    \/\/0x4cd6, 0xaf6c, 0xc7\n-  faddp(1);\n-  fdivrp(1);\n-  fmulp(1);\n-  fmul(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(1);\n-  fmulp(2);\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  movsd(xmm0, Address(esp, 16));\n-\n-  bind(B1_28);\n-  testl(edi, edi);\n-  jcc(Assembler::equal, B1_30);\n-\n-  bind(B1_29);\n-  fldcw(Address(esp, 38));\n-\n-  bind(B1_30);\n-  movl(eax, Address(esp, 40));\n-  movsd(Address(eax, 0), xmm0);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  movl(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_31);\n-  testb(ebx, 1);\n-  jcc(Assembler::equal, B1_38);\n-\n-  bind(B1_32);\n-  incl(edx);\n-  fld_s(0);\n-  fmul(1);\n-  testb(edx, 2);\n-  jcc(Assembler::equal, B1_34);\n-\n-  bind(B1_33);\n-  fld_x(ExternalAddress(36 + TP));    \/\/0x2ff0, 0x466d, 0x1a\n-  fmul(1);\n-  fld_x(ExternalAddress(24 + TP));    \/\/0x00e3, 0xc850, 0xaa\n-  movl(eax, Address(esp, 44));\n-  faddp(1);\n-  fmul(1);\n-  xorl(eax, 1);\n-  fld_x(ExternalAddress(36 + TQ));    \/\/0x820f, 0x51ce, 0x7d\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + TQ));    \/\/0xb70f, 0xd068, 0xa6\n-  faddp(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(12 + TQ));    \/\/0xb6a3, 0xc36a, 0x44\n-  faddp(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(TQ));    \/\/0x399c, 0x8391, 0x15\n-  faddp(1);\n-  fld_x(ExternalAddress(12 + TP));    \/\/0x4b06, 0xb0ac, 0xd3\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(TP));    \/\/0x4cd6, 0xaf6c, 0xc7\n-  faddp(1);\n-  fdivrp(1);\n-  fmulp(1);\n-  fmul(1);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(1);\n-  fmulp(2);\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  movsd(xmm0, Address(esp, 16));\n-  jmp(B1_35);\n-\n-  bind(B1_34);\n-  fld_d(ExternalAddress(ONES));\n-  fdiv(2);\n-  fld_s(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(132 + GP));    \/\/0x00a3, 0x7fea, 0x9b\n-  fmul(1);\n-  fld_x(ExternalAddress(120 + GP));    \/\/0xa559, 0x1da9, 0xae\n-  fmul(2);\n-  fld_x(ExternalAddress(108 + GP));    \/\/0x67c9, 0xbed3, 0xbf\n-  movl(eax, Address(esp, 44));\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(96 + GP));    \/\/0x286b, 0xb49e, 0xb8\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(84 + GP));    \/\/0x355b, 0xd910, 0x67\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(72 + GP));    \/\/0xf3ca, 0x8c96, 0x8e\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(60 + GP));    \/\/0xa8f9, 0x83f1, 0x2e\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(48 + GP));    \/\/0x7065, 0x6a37, 0x79\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(36 + GP));    \/\/0x85a0, 0xa819, 0xbc\n-  faddp(2);\n-  fxch(1);\n-  fmul(2);\n-  fld_x(ExternalAddress(24 + GP));    \/\/0xdfa7, 0x08aa, 0x55\n-  faddp(2);\n-  fxch(1);\n-  fmulp(2);\n-  fld_x(ExternalAddress(12 + GP));    \/\/0xb62f, 0x0b60, 0x60\n-  faddp(1);\n-  fmulp(3);\n-  fld_x(ExternalAddress(GP));    \/\/0xaaab, 0xaaaa, 0xaa\n-  faddp(1);\n-  fmul(3);\n-  fxch(2);\n-  fmulp(3);\n-  fxch(1);\n-  faddp(2);\n-  fld_d(Address(ONES, RelocationHolder::none).plus_disp(eax, Address::times_8));\n-  fmula(2);\n-  fmulp(1);\n-  faddp(1);\n-  fstp_d(Address(esp, 16));\n-  movsd(xmm0, Address(esp, 16));\n-\n-  bind(B1_35);\n-  testl(edi, edi);\n-  jcc(Assembler::equal, B1_37);\n-\n-  bind(B1_36);\n-  fldcw(Address(esp, 38));\n-\n-  bind(B1_37);\n-  movl(eax, Address(esp, 40));\n-  movsd(Address(eax, 8), xmm0);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  mov(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_38);\n-  fstp_d(0);\n-  addl(esp, 52);\n-  pop(ebx);\n-  pop(edi);\n-  pop(esi);\n-  mov(esp, ebp);\n-  pop(ebp);\n-  ret(0);\n-\n-  bind(B1_39);\n-  xorl(edi, edi);\n-  jmp(B1_8);\n-\n-  bind(B1_40);\n-  xorl(edi, edi);\n-  jmp(B1_15);\n-}\n-\n-ATTRIBUTE_ALIGNED(16) static const juint _static_const_table_tan[] =\n-{\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x882c10faUL,\n-    0x3f9664f4UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x55e6c23dUL, 0x3f8226e3UL, 0x55555555UL,\n-    0x3fd55555UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x0e157de0UL, 0x3f6d6d3dUL, 0x11111111UL, 0x3fc11111UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x452b75e3UL, 0x3f57da36UL,\n-    0x1ba1ba1cUL, 0x3faba1baUL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x4e435f9bUL,\n-    0x3f953f83UL, 0x00000000UL, 0x00000000UL, 0x3c6e8e46UL, 0x3f9b74eaUL,\n-    0x00000000UL, 0x00000000UL, 0xda5b7511UL, 0x3f85ad63UL, 0xdc230b9bUL,\n-    0x3fb97558UL, 0x26cb3788UL, 0x3f881308UL, 0x76fc4985UL, 0x3fd62ac9UL,\n-    0x77bb08baUL, 0x3f757c85UL, 0xb6247521UL, 0x3fb1381eUL, 0x5922170cUL,\n-    0x3f754e95UL, 0x8746482dUL, 0x3fc27f83UL, 0x11055b30UL, 0x3f64e391UL,\n-    0x3e666320UL, 0x3fa3e609UL, 0x0de9dae3UL, 0x3f6301dfUL, 0x1f1dca06UL,\n-    0x3fafa8aeUL, 0x8c5b2da2UL, 0x3fb936bbUL, 0x4e88f7a5UL, 0x3c587d05UL,\n-    0x00000000UL, 0x3ff00000UL, 0xa8935dd9UL, 0x3f83dde2UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x5a279ea3UL, 0x3faa3407UL,\n-    0x00000000UL, 0x00000000UL, 0x432d65faUL, 0x3fa70153UL, 0x00000000UL,\n-    0x00000000UL, 0x891a4602UL, 0x3f9d03efUL, 0xd62ca5f8UL, 0x3fca77d9UL,\n-    0xb35f4628UL, 0x3f97a265UL, 0x433258faUL, 0x3fd8cf51UL, 0xb58fd909UL,\n-    0x3f8f88e3UL, 0x01771ceaUL, 0x3fc2b154UL, 0xf3562f8eUL, 0x3f888f57UL,\n-    0xc028a723UL, 0x3fc7370fUL, 0x20b7f9f0UL, 0x3f80f44cUL, 0x214368e9UL,\n-    0x3fb6dfaaUL, 0x28891863UL, 0x3f79b4b6UL, 0x172dbbf0UL, 0x3fb6cb8eUL,\n-    0xe0553158UL, 0x3fc975f5UL, 0x593fe814UL, 0x3c2ef5d3UL, 0x00000000UL,\n-    0x3ff00000UL, 0x03dec550UL, 0x3fa44203UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x9314533eUL, 0x3fbb8ec5UL, 0x00000000UL,\n-    0x00000000UL, 0x09aa36d0UL, 0x3fb6d3f4UL, 0x00000000UL, 0x00000000UL,\n-    0xdcb427fdUL, 0x3fb13950UL, 0xd87ab0bbUL, 0x3fd5335eUL, 0xce0ae8a5UL,\n-    0x3fabb382UL, 0x79143126UL, 0x3fddba41UL, 0x5f2b28d4UL, 0x3fa552f1UL,\n-    0x59f21a6dUL, 0x3fd015abUL, 0x22c27d95UL, 0x3fa0e984UL, 0xe19fc6aaUL,\n-    0x3fd0576cUL, 0x8f2c2950UL, 0x3f9a4898UL, 0xc0b3f22cUL, 0x3fc59462UL,\n-    0x1883a4b8UL, 0x3f94b61cUL, 0x3f838640UL, 0x3fc30eb8UL, 0x355c63dcUL,\n-    0x3fd36a08UL, 0x1dce993dUL, 0xbc6d704dUL, 0x00000000UL, 0x3ff00000UL,\n-    0x2b82ab63UL, 0x3fb78e92UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x56f37042UL, 0x3fccfc56UL, 0x00000000UL, 0x00000000UL,\n-    0xaa563951UL, 0x3fc90125UL, 0x00000000UL, 0x00000000UL, 0x3d0e7c5dUL,\n-    0x3fc50533UL, 0x9bed9b2eUL, 0x3fdf0ed9UL, 0x5fe7c47cUL, 0x3fc1f250UL,\n-    0x96c125e5UL, 0x3fe2edd9UL, 0x5a02bbd8UL, 0x3fbe5c71UL, 0x86362c20UL,\n-    0x3fda08b7UL, 0x4b4435edUL, 0x3fb9d342UL, 0x4b494091UL, 0x3fd911bdUL,\n-    0xb56658beUL, 0x3fb5e4c7UL, 0x93a2fd76UL, 0x3fd3c092UL, 0xda271794UL,\n-    0x3fb29910UL, 0x3303df2bUL, 0x3fd189beUL, 0x99fcef32UL, 0x3fda8279UL,\n-    0xb68c1467UL, 0x3c708b2fUL, 0x00000000UL, 0x3ff00000UL, 0x980c4337UL,\n-    0x3fc5f619UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0xcc03e501UL, 0x3fdff10fUL, 0x00000000UL, 0x00000000UL, 0x44a4e845UL,\n-    0x3fddb63bUL, 0x00000000UL, 0x00000000UL, 0x3768ad9fUL, 0x3fdb72a4UL,\n-    0x3dd01ccaUL, 0x3fe5fdb9UL, 0xa61d2811UL, 0x3fd972b2UL, 0x5645ad0bUL,\n-    0x3fe977f9UL, 0xd013b3abUL, 0x3fd78ca3UL, 0xbf0bf914UL, 0x3fe4f192UL,\n-    0x4d53e730UL, 0x3fd5d060UL, 0x3f8b9000UL, 0x3fe49933UL, 0xe2b82f08UL,\n-    0x3fd4322aUL, 0x5936a835UL, 0x3fe27ae1UL, 0xb1c61c9bUL, 0x3fd2b3fbUL,\n-    0xef478605UL, 0x3fe1659eUL, 0x190834ecUL, 0x3fe11ab7UL, 0xcdb625eaUL,\n-    0xbc8e564bUL, 0x00000000UL, 0x3ff00000UL, 0xb07217e3UL, 0x3fd248f1UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x2b2c49d0UL,\n-    0x3ff2de9cUL, 0x00000000UL, 0x00000000UL, 0x2655bc98UL, 0x3ff33e58UL,\n-    0x00000000UL, 0x00000000UL, 0xff691fa2UL, 0x3ff3972eUL, 0xe93463bdUL,\n-    0x3feeed87UL, 0x070e10a0UL, 0x3ff3f5b2UL, 0xf4d790a4UL, 0x3ff20c10UL,\n-    0xa04e8ea3UL, 0x3ff4541aUL, 0x386accd3UL, 0x3ff1369eUL, 0x222a66ddUL,\n-    0x3ff4b521UL, 0x22a9777eUL, 0x3ff20817UL, 0x52a04a6eUL, 0x3ff5178fUL,\n-    0xddaa0031UL, 0x3ff22137UL, 0x4447d47cUL, 0x3ff57c01UL, 0x1e9c7f1dUL,\n-    0x3ff29311UL, 0x2ab7f990UL, 0x3fe561b8UL, 0x209c7df1UL, 0x3c87a8c5UL,\n-    0x00000000UL, 0x3ff00000UL, 0x4170bcc6UL, 0x3fdc92d8UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0xc7ab4d5aUL, 0x40085e24UL,\n-    0x00000000UL, 0x00000000UL, 0xe93ea75dUL, 0x400b963dUL, 0x00000000UL,\n-    0x00000000UL, 0x94a7f25aUL, 0x400f37e2UL, 0x4b6261cbUL, 0x3ff5f984UL,\n-    0x5a9dd812UL, 0x4011aab0UL, 0x74c30018UL, 0x3ffaf5a5UL, 0x7f2ce8e3UL,\n-    0x4013fe8bUL, 0xfe8e54faUL, 0x3ffd7334UL, 0x670d618dUL, 0x4016a10cUL,\n-    0x4db97058UL, 0x4000e012UL, 0x24df44ddUL, 0x40199c5fUL, 0x697d6eceUL,\n-    0x4003006eUL, 0x83298b82UL, 0x401cfc4dUL, 0x19d490d6UL, 0x40058c19UL,\n-    0x2ae42850UL, 0x3fea4300UL, 0x118e20e6UL, 0xbc7a6db8UL, 0x00000000UL,\n-    0x40000000UL, 0xe33345b8UL, 0xbfd4e526UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x65965966UL, 0x40219659UL, 0x00000000UL,\n-    0x00000000UL, 0x882c10faUL, 0x402664f4UL, 0x00000000UL, 0x00000000UL,\n-    0x83cd3723UL, 0x402c8342UL, 0x00000000UL, 0x40000000UL, 0x55e6c23dUL,\n-    0x403226e3UL, 0x55555555UL, 0x40055555UL, 0x34451939UL, 0x40371c96UL,\n-    0xaaaaaaabUL, 0x400aaaaaUL, 0x0e157de0UL, 0x403d6d3dUL, 0x11111111UL,\n-    0x40111111UL, 0xa738201fUL, 0x4042bbceUL, 0x05b05b06UL, 0x4015b05bUL,\n-    0x452b75e3UL, 0x4047da36UL, 0x1ba1ba1cUL, 0x401ba1baUL, 0x00000000UL,\n-    0x3ff00000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x40000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x4f48b8d3UL, 0xbf33eaf9UL, 0x00000000UL, 0x00000000UL,\n-    0x0cf7586fUL, 0x3f20b8eaUL, 0x00000000UL, 0x00000000UL, 0xd0258911UL,\n-    0xbf0abaf3UL, 0x23e49fe9UL, 0xbfab5a8cUL, 0x2d53222eUL, 0x3ef60d15UL,\n-    0x21169451UL, 0x3fa172b2UL, 0xbb254dbcUL, 0xbee1d3b5UL, 0xdbf93b8eUL,\n-    0xbf84c7dbUL, 0x05b4630bUL, 0x3ecd3364UL, 0xee9aada7UL, 0x3f743924UL,\n-    0x794a8297UL, 0xbeb7b7b9UL, 0xe015f797UL, 0xbf5d41f5UL, 0xe41a4a56UL,\n-    0x3ea35dfbUL, 0xe4c2a251UL, 0x3f49a2abUL, 0x5af9e000UL, 0xbfce49ceUL,\n-    0x8c743719UL, 0x3d1eb860UL, 0x00000000UL, 0x00000000UL, 0x1b4863cfUL,\n-    0x3fd78294UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xfffffff8UL,\n-    0x535ad890UL, 0xbf2b9320UL, 0x00000000UL, 0x00000000UL, 0x018fdf1fUL,\n-    0x3f16d61dUL, 0x00000000UL, 0x00000000UL, 0x0359f1beUL, 0xbf0139e4UL,\n-    0xa4317c6dUL, 0xbfa67e17UL, 0x82672d0fUL, 0x3eebb405UL, 0x2f1b621eUL,\n-    0x3f9f455bUL, 0x51ccf238UL, 0xbed55317UL, 0xf437b9acUL, 0xbf804beeUL,\n-    0xc791a2b5UL, 0x3ec0e993UL, 0x919a1db2UL, 0x3f7080c2UL, 0x336a5b0eUL,\n-    0xbeaa48a2UL, 0x0a268358UL, 0xbf55a443UL, 0xdfd978e4UL, 0x3e94b61fUL,\n-    0xd7767a58UL, 0x3f431806UL, 0x2aea0000UL, 0xbfc9bbe8UL, 0x7723ea61UL,\n-    0xbd3a2369UL, 0x00000000UL, 0x00000000UL, 0xdf7796ffUL, 0x3fd6e642UL,\n-    0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xfffffff8UL, 0xb9ff07ceUL,\n-    0xbf231c78UL, 0x00000000UL, 0x00000000UL, 0xa5517182UL, 0x3f0ff0e0UL,\n-    0x00000000UL, 0x00000000UL, 0x790b4cbcUL, 0xbef66191UL, 0x848a46c6UL,\n-    0xbfa21ac0UL, 0xb16435faUL, 0x3ee1d3ecUL, 0x2a1aa832UL, 0x3f9c71eaUL,\n-    0xfdd299efUL, 0xbec9dd1aUL, 0x3f8dbaafUL, 0xbf793363UL, 0x309fc6eaUL,\n-    0x3eb415d6UL, 0xbee60471UL, 0x3f6b83baUL, 0x94a0a697UL, 0xbe9dae11UL,\n-    0x3e5c67b3UL, 0xbf4fd07bUL, 0x9a8f3e3eUL, 0x3e86bd75UL, 0xa4beb7a4UL,\n-    0x3f3d1eb1UL, 0x29cfc000UL, 0xbfc549ceUL, 0xbf159358UL, 0xbd397b33UL,\n-    0x00000000UL, 0x00000000UL, 0x871fee6cUL, 0x3fd666f0UL, 0x00000000UL,\n-    0x3ff00000UL, 0x00000000UL, 0xfffffff8UL, 0x7d98a556UL, 0xbf1a3958UL,\n-    0x00000000UL, 0x00000000UL, 0x9d88dc01UL, 0x3f0704c2UL, 0x00000000UL,\n-    0x00000000UL, 0x73742a2bUL, 0xbeed054aUL, 0x58844587UL, 0xbf9c2a13UL,\n-    0x55688a79UL, 0x3ed7a326UL, 0xee33f1d6UL, 0x3f9a48f4UL, 0xa8dc9888UL,\n-    0xbebf8939UL, 0xaad4b5b8UL, 0xbf72f746UL, 0x9102efa1UL, 0x3ea88f82UL,\n-    0xdabc29cfUL, 0x3f678228UL, 0x9289afb8UL, 0xbe90f456UL, 0x741fb4edUL,\n-    0xbf46f3a3UL, 0xa97f6663UL, 0x3e79b4bfUL, 0xca89ff3fUL, 0x3f36db70UL,\n-    0xa8a2a000UL, 0xbfc0ee13UL, 0x3da24be1UL, 0xbd338b9fUL, 0x00000000UL,\n-    0x00000000UL, 0x11cd6c69UL, 0x3fd601fdUL, 0x00000000UL, 0x3ff00000UL,\n-    0x00000000UL, 0xfffffff8UL, 0x1a154b97UL, 0xbf116b01UL, 0x00000000UL,\n-    0x00000000UL, 0x2d427630UL, 0x3f0147bfUL, 0x00000000UL, 0x00000000UL,\n-    0xb93820c8UL, 0xbee264d4UL, 0xbb6cbb18UL, 0xbf94ab8cUL, 0x888d4d92UL,\n-    0x3ed0568bUL, 0x60730f7cUL, 0x3f98b19bUL, 0xe4b1fb11UL, 0xbeb2f950UL,\n-    0x22cf9f74UL, 0xbf6b21cdUL, 0x4a3ff0a6UL, 0x3e9f499eUL, 0xfd2b83ceUL,\n-    0x3f64aad7UL, 0x637b73afUL, 0xbe83487cUL, 0xe522591aUL, 0xbf3fc092UL,\n-    0xa158e8bcUL, 0x3e6e3aaeUL, 0xe5e82ffaUL, 0x3f329d2fUL, 0xd636a000UL,\n-    0xbfb9477fUL, 0xc2c2d2bcUL, 0xbd135ef9UL, 0x00000000UL, 0x00000000UL,\n-    0xf2fdb123UL, 0x3fd5b566UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL,\n-    0xfffffff8UL, 0xc41acb64UL, 0xbf05448dUL, 0x00000000UL, 0x00000000UL,\n-    0xdbb03d6fUL, 0x3efb7ad2UL, 0x00000000UL, 0x00000000UL, 0x9e42962dUL,\n-    0xbed5aea5UL, 0x2579f8efUL, 0xbf8b2398UL, 0x288a1ed9UL, 0x3ec81441UL,\n-    0xb0198dc5UL, 0x3f979a3aUL, 0x2fdfe253UL, 0xbea57cd3UL, 0x5766336fUL,\n-    0xbf617caaUL, 0x600944c3UL, 0x3e954ed6UL, 0xa4e0aaf8UL, 0x3f62c646UL,\n-    0x6b8fb29cUL, 0xbe74e3a3UL, 0xdc4c0409UL, 0xbf33f952UL, 0x9bffe365UL,\n-    0x3e6301ecUL, 0xb8869e44UL, 0x3f2fc566UL, 0xe1e04000UL, 0xbfb0cc62UL,\n-    0x016b907fUL, 0xbd119cbcUL, 0x00000000UL, 0x00000000UL, 0xe6b9d8faUL,\n-    0x3fd57fb3UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xfffffff8UL,\n-    0x5daf22a6UL, 0xbef429d7UL, 0x00000000UL, 0x00000000UL, 0x06bca545UL,\n-    0x3ef7a27dUL, 0x00000000UL, 0x00000000UL, 0x7211c19aUL, 0xbec41c3eUL,\n-    0x956ed53eUL, 0xbf7ae3f4UL, 0xee750e72UL, 0x3ec3901bUL, 0x91d443f5UL,\n-    0x3f96f713UL, 0x36661e6cUL, 0xbe936e09UL, 0x506f9381UL, 0xbf5122e8UL,\n-    0xcb6dd43fUL, 0x3e9041b9UL, 0x6698b2ffUL, 0x3f61b0c7UL, 0x576bf12bUL,\n-    0xbe625a8aUL, 0xe5a0e9dcUL, 0xbf23499dUL, 0x110384ddUL, 0x3e5b1c2cUL,\n-    0x68d43db6UL, 0x3f2cb899UL, 0x6ecac000UL, 0xbfa0c414UL, 0xcd7dd58cUL,\n-    0x3d13500fUL, 0x00000000UL, 0x00000000UL, 0x85a2c8fbUL, 0x3fd55fe0UL,\n-    0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xfffffff8UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x2bf70ebeUL, 0x3ef66a8fUL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0xd644267fUL, 0x3ec22805UL, 0x16c16c17UL, 0x3f96c16cUL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0xc4e09162UL,\n-    0x3e8d6db2UL, 0xbc011567UL, 0x3f61566aUL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x1f79955cUL, 0x3e57da4eUL, 0x9334ef0bUL,\n-    0x3f2bbd77UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x55555555UL, 0x3fd55555UL, 0x00000000UL,\n-    0x3ff00000UL, 0x00000000UL, 0xfffffff8UL, 0x5daf22a6UL, 0x3ef429d7UL,\n-    0x00000000UL, 0x00000000UL, 0x06bca545UL, 0x3ef7a27dUL, 0x00000000UL,\n-    0x00000000UL, 0x7211c19aUL, 0x3ec41c3eUL, 0x956ed53eUL, 0x3f7ae3f4UL,\n-    0xee750e72UL, 0x3ec3901bUL, 0x91d443f5UL, 0x3f96f713UL, 0x36661e6cUL,\n-    0x3e936e09UL, 0x506f9381UL, 0x3f5122e8UL, 0xcb6dd43fUL, 0x3e9041b9UL,\n-    0x6698b2ffUL, 0x3f61b0c7UL, 0x576bf12bUL, 0x3e625a8aUL, 0xe5a0e9dcUL,\n-    0x3f23499dUL, 0x110384ddUL, 0x3e5b1c2cUL, 0x68d43db6UL, 0x3f2cb899UL,\n-    0x6ecac000UL, 0x3fa0c414UL, 0xcd7dd58cUL, 0xbd13500fUL, 0x00000000UL,\n-    0x00000000UL, 0x85a2c8fbUL, 0x3fd55fe0UL, 0x00000000UL, 0x3ff00000UL,\n-    0x00000000UL, 0xfffffff8UL, 0xc41acb64UL, 0x3f05448dUL, 0x00000000UL,\n-    0x00000000UL, 0xdbb03d6fUL, 0x3efb7ad2UL, 0x00000000UL, 0x00000000UL,\n-    0x9e42962dUL, 0x3ed5aea5UL, 0x2579f8efUL, 0x3f8b2398UL, 0x288a1ed9UL,\n-    0x3ec81441UL, 0xb0198dc5UL, 0x3f979a3aUL, 0x2fdfe253UL, 0x3ea57cd3UL,\n-    0x5766336fUL, 0x3f617caaUL, 0x600944c3UL, 0x3e954ed6UL, 0xa4e0aaf8UL,\n-    0x3f62c646UL, 0x6b8fb29cUL, 0x3e74e3a3UL, 0xdc4c0409UL, 0x3f33f952UL,\n-    0x9bffe365UL, 0x3e6301ecUL, 0xb8869e44UL, 0x3f2fc566UL, 0xe1e04000UL,\n-    0x3fb0cc62UL, 0x016b907fUL, 0x3d119cbcUL, 0x00000000UL, 0x00000000UL,\n-    0xe6b9d8faUL, 0x3fd57fb3UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL,\n-    0xfffffff8UL, 0x1a154b97UL, 0x3f116b01UL, 0x00000000UL, 0x00000000UL,\n-    0x2d427630UL, 0x3f0147bfUL, 0x00000000UL, 0x00000000UL, 0xb93820c8UL,\n-    0x3ee264d4UL, 0xbb6cbb18UL, 0x3f94ab8cUL, 0x888d4d92UL, 0x3ed0568bUL,\n-    0x60730f7cUL, 0x3f98b19bUL, 0xe4b1fb11UL, 0x3eb2f950UL, 0x22cf9f74UL,\n-    0x3f6b21cdUL, 0x4a3ff0a6UL, 0x3e9f499eUL, 0xfd2b83ceUL, 0x3f64aad7UL,\n-    0x637b73afUL, 0x3e83487cUL, 0xe522591aUL, 0x3f3fc092UL, 0xa158e8bcUL,\n-    0x3e6e3aaeUL, 0xe5e82ffaUL, 0x3f329d2fUL, 0xd636a000UL, 0x3fb9477fUL,\n-    0xc2c2d2bcUL, 0x3d135ef9UL, 0x00000000UL, 0x00000000UL, 0xf2fdb123UL,\n-    0x3fd5b566UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xfffffff8UL,\n-    0x7d98a556UL, 0x3f1a3958UL, 0x00000000UL, 0x00000000UL, 0x9d88dc01UL,\n-    0x3f0704c2UL, 0x00000000UL, 0x00000000UL, 0x73742a2bUL, 0x3eed054aUL,\n-    0x58844587UL, 0x3f9c2a13UL, 0x55688a79UL, 0x3ed7a326UL, 0xee33f1d6UL,\n-    0x3f9a48f4UL, 0xa8dc9888UL, 0x3ebf8939UL, 0xaad4b5b8UL, 0x3f72f746UL,\n-    0x9102efa1UL, 0x3ea88f82UL, 0xdabc29cfUL, 0x3f678228UL, 0x9289afb8UL,\n-    0x3e90f456UL, 0x741fb4edUL, 0x3f46f3a3UL, 0xa97f6663UL, 0x3e79b4bfUL,\n-    0xca89ff3fUL, 0x3f36db70UL, 0xa8a2a000UL, 0x3fc0ee13UL, 0x3da24be1UL,\n-    0x3d338b9fUL, 0x00000000UL, 0x00000000UL, 0x11cd6c69UL, 0x3fd601fdUL,\n-    0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0xfffffff8UL, 0xb9ff07ceUL,\n-    0x3f231c78UL, 0x00000000UL, 0x00000000UL, 0xa5517182UL, 0x3f0ff0e0UL,\n-    0x00000000UL, 0x00000000UL, 0x790b4cbcUL, 0x3ef66191UL, 0x848a46c6UL,\n-    0x3fa21ac0UL, 0xb16435faUL, 0x3ee1d3ecUL, 0x2a1aa832UL, 0x3f9c71eaUL,\n-    0xfdd299efUL, 0x3ec9dd1aUL, 0x3f8dbaafUL, 0x3f793363UL, 0x309fc6eaUL,\n-    0x3eb415d6UL, 0xbee60471UL, 0x3f6b83baUL, 0x94a0a697UL, 0x3e9dae11UL,\n-    0x3e5c67b3UL, 0x3f4fd07bUL, 0x9a8f3e3eUL, 0x3e86bd75UL, 0xa4beb7a4UL,\n-    0x3f3d1eb1UL, 0x29cfc000UL, 0x3fc549ceUL, 0xbf159358UL, 0x3d397b33UL,\n-    0x00000000UL, 0x00000000UL, 0x871fee6cUL, 0x3fd666f0UL, 0x00000000UL,\n-    0x3ff00000UL, 0x00000000UL, 0xfffffff8UL, 0x535ad890UL, 0x3f2b9320UL,\n-    0x00000000UL, 0x00000000UL, 0x018fdf1fUL, 0x3f16d61dUL, 0x00000000UL,\n-    0x00000000UL, 0x0359f1beUL, 0x3f0139e4UL, 0xa4317c6dUL, 0x3fa67e17UL,\n-    0x82672d0fUL, 0x3eebb405UL, 0x2f1b621eUL, 0x3f9f455bUL, 0x51ccf238UL,\n-    0x3ed55317UL, 0xf437b9acUL, 0x3f804beeUL, 0xc791a2b5UL, 0x3ec0e993UL,\n-    0x919a1db2UL, 0x3f7080c2UL, 0x336a5b0eUL, 0x3eaa48a2UL, 0x0a268358UL,\n-    0x3f55a443UL, 0xdfd978e4UL, 0x3e94b61fUL, 0xd7767a58UL, 0x3f431806UL,\n-    0x2aea0000UL, 0x3fc9bbe8UL, 0x7723ea61UL, 0x3d3a2369UL, 0x00000000UL,\n-    0x00000000UL, 0xdf7796ffUL, 0x3fd6e642UL, 0x00000000UL, 0x3ff00000UL,\n-    0x00000000UL, 0xfffffff8UL, 0x4f48b8d3UL, 0x3f33eaf9UL, 0x00000000UL,\n-    0x00000000UL, 0x0cf7586fUL, 0x3f20b8eaUL, 0x00000000UL, 0x00000000UL,\n-    0xd0258911UL, 0x3f0abaf3UL, 0x23e49fe9UL, 0x3fab5a8cUL, 0x2d53222eUL,\n-    0x3ef60d15UL, 0x21169451UL, 0x3fa172b2UL, 0xbb254dbcUL, 0x3ee1d3b5UL,\n-    0xdbf93b8eUL, 0x3f84c7dbUL, 0x05b4630bUL, 0x3ecd3364UL, 0xee9aada7UL,\n-    0x3f743924UL, 0x794a8297UL, 0x3eb7b7b9UL, 0xe015f797UL, 0x3f5d41f5UL,\n-    0xe41a4a56UL, 0x3ea35dfbUL, 0xe4c2a251UL, 0x3f49a2abUL, 0x5af9e000UL,\n-    0x3fce49ceUL, 0x8c743719UL, 0xbd1eb860UL, 0x00000000UL, 0x00000000UL,\n-    0x1b4863cfUL, 0x3fd78294UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL,\n-    0xfffffff8UL, 0x65965966UL, 0xc0219659UL, 0x00000000UL, 0x00000000UL,\n-    0x882c10faUL, 0x402664f4UL, 0x00000000UL, 0x00000000UL, 0x83cd3723UL,\n-    0xc02c8342UL, 0x00000000UL, 0xc0000000UL, 0x55e6c23dUL, 0x403226e3UL,\n-    0x55555555UL, 0x40055555UL, 0x34451939UL, 0xc0371c96UL, 0xaaaaaaabUL,\n-    0xc00aaaaaUL, 0x0e157de0UL, 0x403d6d3dUL, 0x11111111UL, 0x40111111UL,\n-    0xa738201fUL, 0xc042bbceUL, 0x05b05b06UL, 0xc015b05bUL, 0x452b75e3UL,\n-    0x4047da36UL, 0x1ba1ba1cUL, 0x401ba1baUL, 0x00000000UL, 0xbff00000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x40000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0xc7ab4d5aUL, 0xc0085e24UL, 0x00000000UL, 0x00000000UL, 0xe93ea75dUL,\n-    0x400b963dUL, 0x00000000UL, 0x00000000UL, 0x94a7f25aUL, 0xc00f37e2UL,\n-    0x4b6261cbUL, 0xbff5f984UL, 0x5a9dd812UL, 0x4011aab0UL, 0x74c30018UL,\n-    0x3ffaf5a5UL, 0x7f2ce8e3UL, 0xc013fe8bUL, 0xfe8e54faUL, 0xbffd7334UL,\n-    0x670d618dUL, 0x4016a10cUL, 0x4db97058UL, 0x4000e012UL, 0x24df44ddUL,\n-    0xc0199c5fUL, 0x697d6eceUL, 0xc003006eUL, 0x83298b82UL, 0x401cfc4dUL,\n-    0x19d490d6UL, 0x40058c19UL, 0x2ae42850UL, 0xbfea4300UL, 0x118e20e6UL,\n-    0x3c7a6db8UL, 0x00000000UL, 0x40000000UL, 0xe33345b8UL, 0xbfd4e526UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x2b2c49d0UL,\n-    0xbff2de9cUL, 0x00000000UL, 0x00000000UL, 0x2655bc98UL, 0x3ff33e58UL,\n-    0x00000000UL, 0x00000000UL, 0xff691fa2UL, 0xbff3972eUL, 0xe93463bdUL,\n-    0xbfeeed87UL, 0x070e10a0UL, 0x3ff3f5b2UL, 0xf4d790a4UL, 0x3ff20c10UL,\n-    0xa04e8ea3UL, 0xbff4541aUL, 0x386accd3UL, 0xbff1369eUL, 0x222a66ddUL,\n-    0x3ff4b521UL, 0x22a9777eUL, 0x3ff20817UL, 0x52a04a6eUL, 0xbff5178fUL,\n-    0xddaa0031UL, 0xbff22137UL, 0x4447d47cUL, 0x3ff57c01UL, 0x1e9c7f1dUL,\n-    0x3ff29311UL, 0x2ab7f990UL, 0xbfe561b8UL, 0x209c7df1UL, 0xbc87a8c5UL,\n-    0x00000000UL, 0x3ff00000UL, 0x4170bcc6UL, 0x3fdc92d8UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0xcc03e501UL, 0xbfdff10fUL,\n-    0x00000000UL, 0x00000000UL, 0x44a4e845UL, 0x3fddb63bUL, 0x00000000UL,\n-    0x00000000UL, 0x3768ad9fUL, 0xbfdb72a4UL, 0x3dd01ccaUL, 0xbfe5fdb9UL,\n-    0xa61d2811UL, 0x3fd972b2UL, 0x5645ad0bUL, 0x3fe977f9UL, 0xd013b3abUL,\n-    0xbfd78ca3UL, 0xbf0bf914UL, 0xbfe4f192UL, 0x4d53e730UL, 0x3fd5d060UL,\n-    0x3f8b9000UL, 0x3fe49933UL, 0xe2b82f08UL, 0xbfd4322aUL, 0x5936a835UL,\n-    0xbfe27ae1UL, 0xb1c61c9bUL, 0x3fd2b3fbUL, 0xef478605UL, 0x3fe1659eUL,\n-    0x190834ecUL, 0xbfe11ab7UL, 0xcdb625eaUL, 0x3c8e564bUL, 0x00000000UL,\n-    0x3ff00000UL, 0xb07217e3UL, 0x3fd248f1UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x56f37042UL, 0xbfccfc56UL, 0x00000000UL,\n-    0x00000000UL, 0xaa563951UL, 0x3fc90125UL, 0x00000000UL, 0x00000000UL,\n-    0x3d0e7c5dUL, 0xbfc50533UL, 0x9bed9b2eUL, 0xbfdf0ed9UL, 0x5fe7c47cUL,\n-    0x3fc1f250UL, 0x96c125e5UL, 0x3fe2edd9UL, 0x5a02bbd8UL, 0xbfbe5c71UL,\n-    0x86362c20UL, 0xbfda08b7UL, 0x4b4435edUL, 0x3fb9d342UL, 0x4b494091UL,\n-    0x3fd911bdUL, 0xb56658beUL, 0xbfb5e4c7UL, 0x93a2fd76UL, 0xbfd3c092UL,\n-    0xda271794UL, 0x3fb29910UL, 0x3303df2bUL, 0x3fd189beUL, 0x99fcef32UL,\n-    0xbfda8279UL, 0xb68c1467UL, 0xbc708b2fUL, 0x00000000UL, 0x3ff00000UL,\n-    0x980c4337UL, 0x3fc5f619UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x9314533eUL, 0xbfbb8ec5UL, 0x00000000UL, 0x00000000UL,\n-    0x09aa36d0UL, 0x3fb6d3f4UL, 0x00000000UL, 0x00000000UL, 0xdcb427fdUL,\n-    0xbfb13950UL, 0xd87ab0bbUL, 0xbfd5335eUL, 0xce0ae8a5UL, 0x3fabb382UL,\n-    0x79143126UL, 0x3fddba41UL, 0x5f2b28d4UL, 0xbfa552f1UL, 0x59f21a6dUL,\n-    0xbfd015abUL, 0x22c27d95UL, 0x3fa0e984UL, 0xe19fc6aaUL, 0x3fd0576cUL,\n-    0x8f2c2950UL, 0xbf9a4898UL, 0xc0b3f22cUL, 0xbfc59462UL, 0x1883a4b8UL,\n-    0x3f94b61cUL, 0x3f838640UL, 0x3fc30eb8UL, 0x355c63dcUL, 0xbfd36a08UL,\n-    0x1dce993dUL, 0x3c6d704dUL, 0x00000000UL, 0x3ff00000UL, 0x2b82ab63UL,\n-    0x3fb78e92UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL,\n-    0x5a279ea3UL, 0xbfaa3407UL, 0x00000000UL, 0x00000000UL, 0x432d65faUL,\n-    0x3fa70153UL, 0x00000000UL, 0x00000000UL, 0x891a4602UL, 0xbf9d03efUL,\n-    0xd62ca5f8UL, 0xbfca77d9UL, 0xb35f4628UL, 0x3f97a265UL, 0x433258faUL,\n-    0x3fd8cf51UL, 0xb58fd909UL, 0xbf8f88e3UL, 0x01771ceaUL, 0xbfc2b154UL,\n-    0xf3562f8eUL, 0x3f888f57UL, 0xc028a723UL, 0x3fc7370fUL, 0x20b7f9f0UL,\n-    0xbf80f44cUL, 0x214368e9UL, 0xbfb6dfaaUL, 0x28891863UL, 0x3f79b4b6UL,\n-    0x172dbbf0UL, 0x3fb6cb8eUL, 0xe0553158UL, 0xbfc975f5UL, 0x593fe814UL,\n-    0xbc2ef5d3UL, 0x00000000UL, 0x3ff00000UL, 0x03dec550UL, 0x3fa44203UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x4e435f9bUL,\n-    0xbf953f83UL, 0x00000000UL, 0x00000000UL, 0x3c6e8e46UL, 0x3f9b74eaUL,\n-    0x00000000UL, 0x00000000UL, 0xda5b7511UL, 0xbf85ad63UL, 0xdc230b9bUL,\n-    0xbfb97558UL, 0x26cb3788UL, 0x3f881308UL, 0x76fc4985UL, 0x3fd62ac9UL,\n-    0x77bb08baUL, 0xbf757c85UL, 0xb6247521UL, 0xbfb1381eUL, 0x5922170cUL,\n-    0x3f754e95UL, 0x8746482dUL, 0x3fc27f83UL, 0x11055b30UL, 0xbf64e391UL,\n-    0x3e666320UL, 0xbfa3e609UL, 0x0de9dae3UL, 0x3f6301dfUL, 0x1f1dca06UL,\n-    0x3fafa8aeUL, 0x8c5b2da2UL, 0xbfb936bbUL, 0x4e88f7a5UL, 0xbc587d05UL,\n-    0x00000000UL, 0x3ff00000UL, 0xa8935dd9UL, 0x3f83dde2UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x6dc9c883UL, 0x3fe45f30UL,\n-    0x6dc9c883UL, 0x40245f30UL, 0x00000000UL, 0x43780000UL, 0x00000000UL,\n-    0x43380000UL, 0x54444000UL, 0x3fb921fbUL, 0x54440000UL, 0x3fb921fbUL,\n-    0x67674000UL, 0xbd32e7b9UL, 0x4c4c0000UL, 0x3d468c23UL, 0x3707344aUL,\n-    0x3aa8a2e0UL, 0x03707345UL, 0x3ae98a2eUL, 0x00000000UL, 0x80000000UL,\n-    0x00000000UL, 0x80000000UL, 0x676733afUL, 0x3d32e7b9UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x3ff00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x7ff00000UL, 0x00000000UL, 0x00000000UL, 0xfffc0000UL,\n-    0xffffffffUL, 0x00000000UL, 0x00000000UL, 0x00000000UL, 0x43600000UL,\n-    0x00000000UL, 0x00000000UL, 0x00000000UL, 0x3c800000UL, 0x00000000UL,\n-    0x00000000UL, 0x00000000UL, 0x3ca00000UL, 0x00000000UL, 0x00000000UL,\n-    0x00000000UL, 0x3fe00000UL, 0x00000000UL, 0x3fe00000UL, 0x00000000UL,\n-    0x40300000UL, 0x00000000UL, 0x3ff00000UL\n-};\n-\n-void MacroAssembler::fast_tan(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register eax, Register ecx, Register edx, Register tmp) {\n-\n-  Label L_2TAG_PACKET_0_0_2, L_2TAG_PACKET_1_0_2, L_2TAG_PACKET_2_0_2, L_2TAG_PACKET_3_0_2;\n-  Label L_2TAG_PACKET_4_0_2;\n-\n-  assert_different_registers(tmp, eax, ecx, edx);\n-\n-  address static_const_table_tan = (address)_static_const_table_tan;\n-\n-  subl(rsp, 120);\n-  movl(Address(rsp, 56), tmp);\n-  lea(tmp, ExternalAddress(static_const_table_tan));\n-  movsd(xmm0, Address(rsp, 128));\n-  pextrw(eax, xmm0, 3);\n-  andl(eax, 32767);\n-  subl(eax, 14368);\n-  cmpl(eax, 2216);\n-  jcc(Assembler::above, L_2TAG_PACKET_0_0_2);\n-  movdqu(xmm5, Address(tmp, 5840));\n-  movdqu(xmm6, Address(tmp, 5856));\n-  unpcklpd(xmm0, xmm0);\n-  movdqu(xmm4, Address(tmp, 5712));\n-  andpd(xmm4, xmm0);\n-  movdqu(xmm1, Address(tmp, 5632));\n-  mulpd(xmm1, xmm0);\n-  por(xmm5, xmm4);\n-  addpd(xmm1, xmm5);\n-  movdqu(xmm7, xmm1);\n-  unpckhpd(xmm7, xmm7);\n-  cvttsd2sil(edx, xmm7);\n-  cvttpd2dq(xmm1, xmm1);\n-  cvtdq2pd(xmm1, xmm1);\n-  mulpd(xmm1, xmm6);\n-  movdqu(xmm3, Address(tmp, 5664));\n-  movsd(xmm5, Address(tmp, 5728));\n-  addl(edx, 469248);\n-  movdqu(xmm4, Address(tmp, 5680));\n-  mulpd(xmm3, xmm1);\n-  andl(edx, 31);\n-  mulsd(xmm5, xmm1);\n-  movl(ecx, edx);\n-  mulpd(xmm4, xmm1);\n-  shll(ecx, 1);\n-  subpd(xmm0, xmm3);\n-  mulpd(xmm1, Address(tmp, 5696));\n-  addl(edx, ecx);\n-  shll(ecx, 2);\n-  addl(edx, ecx);\n-  addsd(xmm5, xmm0);\n-  movdqu(xmm2, xmm0);\n-  subpd(xmm0, xmm4);\n-  movsd(xmm6, Address(tmp, 5744));\n-  shll(edx, 4);\n-  lea(eax, Address(tmp, 0));\n-  andpd(xmm5, Address(tmp, 5776));\n-  movdqu(xmm3, xmm0);\n-  addl(eax, edx);\n-  subpd(xmm2, xmm0);\n-  unpckhpd(xmm0, xmm0);\n-  divsd(xmm6, xmm5);\n-  subpd(xmm2, xmm4);\n-  movdqu(xmm7, Address(eax, 16));\n-  subsd(xmm3, xmm5);\n-  mulpd(xmm7, xmm0);\n-  subpd(xmm2, xmm1);\n-  movdqu(xmm1, Address(eax, 48));\n-  mulpd(xmm1, xmm0);\n-  movdqu(xmm4, Address(eax, 96));\n-  mulpd(xmm4, xmm0);\n-  addsd(xmm2, xmm3);\n-  movdqu(xmm3, xmm0);\n-  mulpd(xmm0, xmm0);\n-  addpd(xmm7, Address(eax, 0));\n-  addpd(xmm1, Address(eax, 32));\n-  mulpd(xmm1, xmm0);\n-  addpd(xmm4, Address(eax, 80));\n-  addpd(xmm7, xmm1);\n-  movdqu(xmm1, Address(eax, 112));\n-  mulpd(xmm1, xmm0);\n-  mulpd(xmm0, xmm0);\n-  addpd(xmm4, xmm1);\n-  movdqu(xmm1, Address(eax, 64));\n-  mulpd(xmm1, xmm0);\n-  addpd(xmm7, xmm1);\n-  movdqu(xmm1, xmm3);\n-  mulpd(xmm3, xmm0);\n-  mulsd(xmm0, xmm0);\n-  mulpd(xmm1, Address(eax, 144));\n-  mulpd(xmm4, xmm3);\n-  movdqu(xmm3, xmm1);\n-  addpd(xmm7, xmm4);\n-  movdqu(xmm4, xmm1);\n-  mulsd(xmm0, xmm7);\n-  unpckhpd(xmm7, xmm7);\n-  addsd(xmm0, xmm7);\n-  unpckhpd(xmm1, xmm1);\n-  addsd(xmm3, xmm1);\n-  subsd(xmm4, xmm3);\n-  addsd(xmm1, xmm4);\n-  movdqu(xmm4, xmm2);\n-  movsd(xmm7, Address(eax, 144));\n-  unpckhpd(xmm2, xmm2);\n-  addsd(xmm7, Address(eax, 152));\n-  mulsd(xmm7, xmm2);\n-  addsd(xmm7, Address(eax, 136));\n-  addsd(xmm7, xmm1);\n-  addsd(xmm0, xmm7);\n-  movsd(xmm7, Address(tmp, 5744));\n-  mulsd(xmm4, xmm6);\n-  movsd(xmm2, Address(eax, 168));\n-  andpd(xmm2, xmm6);\n-  mulsd(xmm5, xmm2);\n-  mulsd(xmm6, Address(eax, 160));\n-  subsd(xmm7, xmm5);\n-  subsd(xmm2, Address(eax, 128));\n-  subsd(xmm7, xmm4);\n-  mulsd(xmm7, xmm6);\n-  movdqu(xmm4, xmm3);\n-  subsd(xmm3, xmm2);\n-  addsd(xmm2, xmm3);\n-  subsd(xmm4, xmm2);\n-  addsd(xmm0, xmm4);\n-  subsd(xmm0, xmm7);\n-  addsd(xmm0, xmm3);\n-  movsd(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_0_0_2);\n-  jcc(Assembler::greater, L_2TAG_PACKET_2_0_2);\n-  shrl(eax, 4);\n-  cmpl(eax, 268434558);\n-  jcc(Assembler::notEqual, L_2TAG_PACKET_3_0_2);\n-  movdqu(xmm3, xmm0);\n-  mulsd(xmm3, Address(tmp, 5808));\n-\n-  bind(L_2TAG_PACKET_3_0_2);\n-  movsd(xmm3, Address(tmp, 5792));\n-  mulsd(xmm3, xmm0);\n-  addsd(xmm3, xmm0);\n-  mulsd(xmm3, Address(tmp, 5808));\n-  movsd(Address(rsp, 0), xmm3);\n-  fld_d(Address(rsp, 0));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_2_0_2);\n-  movq(xmm7, Address(tmp, 5712));\n-  andpd(xmm7, xmm0);\n-  xorpd(xmm7, xmm0);\n-  ucomisd(xmm7, Address(tmp, 5760));\n-  jcc(Assembler::equal, L_2TAG_PACKET_4_0_2);\n-  subl(rsp, 32);\n-  movsd(Address(rsp, 0), xmm0);\n-  lea(eax, Address(rsp, 40));\n-  movl(Address(rsp, 8), eax);\n-  movl(eax, 2);\n-  movl(Address(rsp, 12), eax);\n-  call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlibm_tan_cot_huge())));\n-  addl(rsp, 32);\n-  fld_d(Address(rsp, 8));\n-  jmp(L_2TAG_PACKET_1_0_2);\n-\n-  bind(L_2TAG_PACKET_4_0_2);\n-  movq(Address(rsp, 0), xmm0);\n-  fld_d(Address(rsp, 0));\n-  fsub_d(Address(rsp, 0));\n-\n-  bind(L_2TAG_PACKET_1_0_2);\n-  movl(tmp, Address(rsp, 56));\n-}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_32_tan.cpp","additions":0,"deletions":1173,"binary":false,"changes":1173,"status":"deleted"},{"patch":"@@ -239,1 +239,0 @@\n-#ifdef _LP64\n@@ -244,6 +243,1 @@\n-#else\n-void MacroAssembler::fast_sha256(XMMRegister msg, XMMRegister state0, XMMRegister state1, XMMRegister msgtmp0,\n-  XMMRegister msgtmp1, XMMRegister msgtmp2, XMMRegister msgtmp3, XMMRegister msgtmp4,\n-  Register buf, Register state, Register ofs, Register limit, Register rsp,\n-  bool multi_block) {\n-#endif\n+\n@@ -264,1 +258,0 @@\n-#ifdef _LP64\n@@ -266,1 +259,0 @@\n-#endif\n@@ -275,1 +267,0 @@\n-#ifdef _LP64\n@@ -277,3 +268,0 @@\n-#else\n-  pshufb(msg, ExternalAddress(pshuffle_byte_flip_mask));\n-#endif\n@@ -288,1 +276,0 @@\n-#ifdef _LP64\n@@ -290,3 +277,0 @@\n-#else\n-  pshufb(msg, ExternalAddress(pshuffle_byte_flip_mask));\n-#endif\n@@ -302,1 +286,0 @@\n-#ifdef _LP64\n@@ -304,3 +287,0 @@\n-#else\n-  pshufb(msg, ExternalAddress(pshuffle_byte_flip_mask));\n-#endif\n@@ -316,1 +296,0 @@\n-#ifdef _LP64\n@@ -318,3 +297,0 @@\n-#else\n-  pshufb(msg, ExternalAddress(pshuffle_byte_flip_mask));\n-#endif\n@@ -495,1 +471,0 @@\n-#ifdef _LP64\n@@ -1700,3 +1675,0 @@\n-\n-#endif \/\/#ifdef _LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86_sha.cpp","additions":1,"deletions":29,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-#ifdef _LP64\n@@ -66,3 +65,0 @@\n-#else\n-    return false;\n-#endif\n@@ -71,1 +67,0 @@\n-#ifdef _LP64\n@@ -74,4 +69,0 @@\n-#else\n-  \/\/ Needs 2 CMOV's for longs.\n-  static constexpr int long_cmove_cost() { return 1; }\n-#endif\n@@ -79,1 +70,0 @@\n-#ifdef _LP64\n@@ -82,4 +72,0 @@\n-#else\n-  \/\/ No CMOVF\/CMOVD with SSE\/SSE2\n-  static int float_cmove_cost() { return (UseSSE>=1) ? ConditionalMoveLimit : 0; }\n-#endif\n@@ -88,1 +74,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -94,1 +79,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -101,1 +85,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -108,1 +91,0 @@\n-    NOT_LP64(ShouldNotCallThis();)\n@@ -124,7 +106,0 @@\n-  \/\/ Advertise here if the CPU requires explicit rounding operations to implement strictfp mode.\n-#ifdef _LP64\n-  static const bool strict_fp_requires_explicit_rounding = false;\n-#else\n-  static const bool strict_fp_requires_explicit_rounding = true;\n-#endif\n-\n@@ -134,1 +109,0 @@\n-#ifdef _LP64\n@@ -138,5 +112,0 @@\n-#else\n-  static bool float_in_double() {\n-    return (UseSSE == 0);\n-  }\n-#endif\n@@ -145,1 +114,0 @@\n-#ifdef _LP64\n@@ -147,4 +115,0 @@\n-#else\n-  static const bool int_in_long = false;\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -86,2 +86,2 @@\n-#define PUSH { __ push(temp); LP64_ONLY(  __ push(rscratch1); )               }\n-#define POP  {                LP64_ONLY(  __ pop(rscratch1);  ) __ pop(temp); }\n+#define PUSH { __ push(temp); __ push(rscratch1);               }\n+#define POP  {                __ pop(rscratch1);  __ pop(temp); }\n@@ -143,1 +143,0 @@\n-#ifdef _LP64\n@@ -145,4 +144,0 @@\n-#else\n-    Register rthread = temp;\n-    __ get_thread(rthread);\n-#endif\n@@ -328,1 +323,0 @@\n-#ifdef _LP64\n@@ -337,13 +331,1 @@\n-  }\n-#else\n-  Register temp1 = (for_compiler_entry ? rsi : rdx);\n-  Register temp2 = rdi;\n-  Register temp3 = rax;\n-  if (for_compiler_entry) {\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n-    assert_different_registers(temp1,        rcx, rdx);\n-    assert_different_registers(temp2,        rcx, rdx);\n-    assert_different_registers(temp3,        rcx, rdx);\n-  }\n-#endif\n-  else {\n+  } else {\n@@ -538,1 +520,0 @@\n-#ifdef AMD64\n@@ -547,3 +528,0 @@\n-#else\n-      ls.print(\"%3s=\" PTR_FORMAT, r->name(), saved_regs[((saved_regs_count - 1) - i)]);\n-#endif\n@@ -655,1 +633,0 @@\n-#ifdef _LP64\n@@ -657,9 +634,0 @@\n-#else\n-  if  (UseSSE >= 2) {\n-    __ movdbl(Address(rsp, 0), xmm0);\n-  } else if (UseSSE == 1) {\n-    __ movflt(Address(rsp, 0), xmm0);\n-  } else {\n-    __ fst_d(Address(rsp, 0));\n-  }\n-#endif \/\/ LP64\n@@ -680,1 +648,0 @@\n-#ifdef _LP64\n@@ -682,9 +649,0 @@\n-#else\n-  if  (UseSSE >= 2) {\n-    __ movdbl(xmm0, Address(rsp, 0));\n-  } else if (UseSSE == 1) {\n-    __ movflt(xmm0, Address(rsp, 0));\n-  } else {\n-    __ fld_d(Address(rsp, 0));\n-  }\n-#endif \/\/ LP64\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":3,"deletions":45,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-    return LP64_ONLY(r13) NOT_LP64(rsi);\n+    return r13;\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,1 +71,0 @@\n-#ifdef AMD64\n@@ -73,1 +72,0 @@\n-#endif \/\/ AMD64\n@@ -161,1 +159,0 @@\n-#ifdef AMD64\n@@ -173,6 +170,0 @@\n-#else\n-  \/\/ make sure code pattern is actually a mov reg, imm32 instruction\n-  u_char test_byte = *(u_char*)instruction_address();\n-  u_char test_byte_2 = test_byte & ( 0xff ^ register_mask);\n-  if (test_byte_2 != instruction_code) fatal(\"not a mov reg, imm32\");\n-#endif \/\/ AMD64\n@@ -196,1 +187,0 @@\n-    NOT_LP64(assert((0xC0 & ubyte_at(1)) == 0xC0, \"shouldn't have LDS and LES instructions\"));\n@@ -201,1 +191,0 @@\n-    NOT_LP64(assert((0xC0 & ubyte_at(1)) == 0xC0, \"shouldn't have LDS and LES instructions\"));\n@@ -317,2 +306,1 @@\n-  if ( ! ((test_byte == lea_instruction_code)\n-          LP64_ONLY(|| (test_byte == mov64_instruction_code) ))) {\n+  if ( ! ((test_byte == lea_instruction_code) || (test_byte == mov64_instruction_code) )) {\n@@ -344,1 +332,0 @@\n-#ifdef AMD64\n@@ -346,1 +333,0 @@\n-#endif \/\/ AMD64\n@@ -359,1 +345,0 @@\n-#ifdef AMD64\n@@ -361,3 +346,0 @@\n-#else\n-  const int linesize = 32;\n-#endif \/\/ AMD64\n@@ -390,1 +372,0 @@\n-#ifdef _LP64\n@@ -406,37 +387,0 @@\n-\n-#else\n-  unsigned char code_buffer[5];\n-  code_buffer[0] = instruction_code;\n-  intptr_t disp = (intptr_t)dest - ((intptr_t)verified_entry + 1 + 4);\n-  *(int32_t*)(code_buffer + 1) = (int32_t)disp;\n-\n-  check_verified_entry_alignment(entry, verified_entry);\n-\n-  \/\/ Can't call nativeJump_at() because it's asserts jump exists\n-  NativeJump* n_jump = (NativeJump*) verified_entry;\n-\n-  \/\/First patch dummy jmp in place\n-\n-  unsigned char patch[4];\n-  assert(sizeof(patch)==sizeof(int32_t), \"sanity check\");\n-  patch[0] = 0xEB;       \/\/ jmp rel8\n-  patch[1] = 0xFE;       \/\/ jmp to self\n-  patch[2] = 0xEB;\n-  patch[3] = 0xFE;\n-\n-  \/\/ First patch dummy jmp in place\n-  *(int32_t*)verified_entry = *(int32_t *)patch;\n-\n-  n_jump->wrote(0);\n-\n-  \/\/ Patch 5th byte (from jump instruction)\n-  verified_entry[4] = code_buffer[4];\n-\n-  n_jump->wrote(4);\n-\n-  \/\/ Patch bytes 0-3 (from jump instruction)\n-  *(int32_t*)verified_entry = *(int32_t *)code_buffer;\n-  \/\/ Invalidate.  Opteron requires a flush after every write.\n-  n_jump->wrote(0);\n-#endif \/\/ _LP64\n-\n@@ -459,1 +403,0 @@\n-#ifdef AMD64\n@@ -461,1 +404,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.cpp","additions":1,"deletions":59,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -129,1 +129,0 @@\n-#ifdef AMD64\n@@ -132,1 +131,0 @@\n-#endif \/\/ AMD64\n@@ -214,1 +212,0 @@\n-#ifdef AMD64\n@@ -218,5 +215,1 @@\n-#else\n-  static const bool has_rex = false;\n-  static const int rex_size = 0;\n-  static const int rex2_size = 0;\n-#endif \/\/ AMD64\n+\n@@ -393,1 +386,0 @@\n-#ifdef AMD64\n@@ -396,4 +388,1 @@\n-#else\n-  static const bool has_rex = false;\n-  static const int rex_size = 0;\n-#endif \/\/ AMD64\n+\n@@ -450,1 +439,0 @@\n-#ifdef AMD64\n@@ -452,1 +440,0 @@\n-#endif \/\/ AMD64\n@@ -575,1 +562,0 @@\n-#ifdef AMD64\n@@ -578,3 +564,0 @@\n-#else\n-  const int test_offset = 0;\n-#endif\n@@ -587,1 +570,0 @@\n-#ifdef AMD64\n@@ -596,3 +578,0 @@\n-#else\n-  return false;\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/nativeInst_x86.hpp","additions":2,"deletions":23,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#ifdef _LP64\n@@ -41,3 +40,0 @@\n-#else\n-    \"eax\", \"ecx\", \"edx\", \"ebx\", \"esp\", \"ebp\", \"esi\", \"edi\"\n-#endif \/\/ _LP64\n@@ -58,1 +54,0 @@\n-#ifdef _LP64\n@@ -62,1 +57,0 @@\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/register_x86.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -48,3 +48,3 @@\n-    number_of_registers      = LP64_ONLY( 32 ) NOT_LP64( 8 ),\n-    number_of_byte_registers = LP64_ONLY( 32 ) NOT_LP64( 4 ),\n-    max_slots_per_register   = LP64_ONLY(  2 ) NOT_LP64( 1 )\n+    number_of_registers      = 32,\n+    number_of_byte_registers = 32,\n+    max_slots_per_register   =  2\n@@ -82,1 +82,0 @@\n-#ifdef _LP64\n@@ -86,1 +85,0 @@\n-#endif \/\/ _LP64\n@@ -119,1 +117,0 @@\n-#ifdef _LP64\n@@ -144,1 +141,0 @@\n-#endif \/\/ _LP64\n@@ -148,0 +144,1 @@\n+\/\/ TODO: This is not needed anymore, remove or set number_of_registers=0?\n@@ -221,2 +218,2 @@\n-    number_of_registers    = LP64_ONLY( 32 ) NOT_LP64(  8 ),\n-    max_slots_per_register = LP64_ONLY( 16 ) NOT_LP64( 16 )   \/\/ 512-bit\n+    number_of_registers    = 32,\n+    max_slots_per_register = 16\n@@ -253,1 +250,0 @@\n-#ifdef _LP64\n@@ -257,1 +253,0 @@\n-#endif \/\/ _LP64\n@@ -290,1 +285,0 @@\n-#ifdef _LP64\n@@ -315,1 +309,0 @@\n-#endif \/\/ _LP64\n@@ -408,4 +401,0 @@\n-    \/\/ x86_32.ad defines additional dummy FILL0-FILL7 registers, in order to tally\n-    \/\/ REG_COUNT (computed by ADLC based on the number of reg_defs seen in .ad files)\n-    \/\/ with ConcreteRegisterImpl::number_of_registers additional count of 8 is being\n-    \/\/ added for 32 bit jvm.\n@@ -413,1 +402,0 @@\n-                          NOT_LP64( 8 + ) \/\/ FILL0-FILL7 in x86_32.ad\n","filename":"src\/hotspot\/cpu\/x86\/register_x86.hpp","additions":6,"deletions":18,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#ifdef AMD64\n@@ -80,7 +79,0 @@\n-#else\n-  if (verify_only) {\n-    guarantee(*pd_address_in_code() == x, \"instructions must match\");\n-  } else {\n-    *pd_address_in_code() = x;\n-  }\n-#endif \/\/ AMD64\n@@ -154,1 +146,0 @@\n-#ifdef AMD64\n@@ -161,3 +152,0 @@\n-#else\n-  assert(which == Assembler::disp32_operand || which == Assembler::imm_operand, \"format unpacks ok\");\n-#endif \/\/ AMD64\n@@ -169,1 +157,0 @@\n-#ifdef AMD64\n@@ -186,1 +173,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/relocInfo_x86.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -34,4 +34,1 @@\n-    \/\/ Encodes Assembler::disp32_operand vs. Assembler::imm32_operand.\n-#ifndef AMD64\n-    format_width       =  1\n-#else\n+    \/\/ Encodes Assembler::disp32_operand vs. Assembler::imm32_operand\n@@ -40,1 +37,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/relocInfo_x86.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,331 +0,0 @@\n-\/*\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#ifdef COMPILER2\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/vmreg.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"opto\/runtime.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"runtime\/vframeArray.hpp\"\n-#include \"utilities\/globalDefinitions.hpp\"\n-#include \"vmreg_x86.inline.hpp\"\n-#endif\n-\n-\n-#define __ masm->\n-\n-\/\/------------------------------generate_uncommon_trap_blob--------------------\n-void OptoRuntime::generate_uncommon_trap_blob() {\n-  \/\/ allocate space for the code\n-  ResourceMark rm;\n-  \/\/ setup code generation tools\n-  CodeBuffer   buffer(\"uncommon_trap_blob\", 512, 512);\n-  MacroAssembler* masm = new MacroAssembler(&buffer);\n-\n-  enum frame_layout {\n-    arg0_off,      \/\/ thread                     sp + 0 \/\/ Arg location for\n-    arg1_off,      \/\/ unloaded_class_index       sp + 1 \/\/ calling C\n-    arg2_off,      \/\/ exec_mode                  sp + 2\n-    \/\/ The frame sender code expects that rbp will be in the \"natural\" place and\n-    \/\/ will override any oopMap setting for it. We must therefore force the layout\n-    \/\/ so that it agrees with the frame sender code.\n-    rbp_off,       \/\/ callee saved register      sp + 3\n-    return_off,    \/\/ slot for return address    sp + 4\n-    framesize\n-  };\n-\n-  address start = __ pc();\n-\n-  \/\/ Push self-frame.\n-  __ subptr(rsp, return_off*wordSize);     \/\/ Epilog!\n-\n-  \/\/ rbp, is an implicitly saved callee saved register (i.e. the calling\n-  \/\/ convention will save restore it in prolog\/epilog) Other than that\n-  \/\/ there are no callee save registers no that adapter frames are gone.\n-  __ movptr(Address(rsp, rbp_off*wordSize), rbp);\n-\n-  \/\/ Clear the floating point exception stack\n-  __ empty_FPU_stack();\n-\n-  \/\/ set last_Java_sp\n-  __ get_thread(rdx);\n-  __ set_last_Java_frame(rdx, noreg, noreg, nullptr, noreg);\n-\n-  \/\/ Call C code.  Need thread but NOT official VM entry\n-  \/\/ crud.  We cannot block on this call, no GC can happen.  Call should\n-  \/\/ capture callee-saved registers as well as return values.\n-  __ movptr(Address(rsp, arg0_off*wordSize), rdx);\n-  \/\/ argument already in ECX\n-  __ movl(Address(rsp, arg1_off*wordSize),rcx);\n-  __ movl(Address(rsp, arg2_off*wordSize), Deoptimization::Unpack_uncommon_trap);\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap)));\n-\n-  \/\/ Set an oopmap for the call site\n-  OopMapSet *oop_maps = new OopMapSet();\n-  OopMap* map =  new OopMap( framesize, 0 );\n-  \/\/ No oopMap for rbp, it is known implicitly\n-\n-  oop_maps->add_gc_map( __ pc()-start, map);\n-\n-  __ get_thread(rcx);\n-\n-  __ reset_last_Java_frame(rcx, false);\n-\n-  \/\/ Load UnrollBlock into EDI\n-  __ movptr(rdi, rax);\n-\n-#ifdef ASSERT\n-  { Label L;\n-    __ cmpptr(Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset()),\n-            (int32_t)Deoptimization::Unpack_uncommon_trap);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"OptoRuntime::generate_uncommon_trap_blob: expected Unpack_uncommon_trap\");\n-    __ bind(L);\n-  }\n-#endif\n-\n-  \/\/ Pop all the frames we must move\/replace.\n-  \/\/\n-  \/\/ Frame picture (youngest to oldest)\n-  \/\/ 1: self-frame (no frame link)\n-  \/\/ 2: deopting frame  (no frame link)\n-  \/\/ 3: caller of deopting frame (could be compiled\/interpreted).\n-\n-  \/\/ Pop self-frame.  We have no frame, and must rely only on EAX and ESP.\n-  __ addptr(rsp,(framesize-1)*wordSize);     \/\/ Epilog!\n-\n-  \/\/ Pop deoptimized frame\n-  __ movl2ptr(rcx, Address(rdi,Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n-  __ addptr(rsp, rcx);\n-\n-  \/\/ sp should be pointing at the return address to the caller (3)\n-\n-  \/\/ Pick up the initial fp we should save\n-  \/\/ restore rbp before stack bang because if stack overflow is thrown it needs to be pushed (and preserved)\n-  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset()));\n-\n-#ifdef ASSERT\n-  \/\/ Compilers generate code that bang the stack by as much as the\n-  \/\/ interpreter would need. So this stack banging should never\n-  \/\/ trigger a fault. Verify that it does not on non product builds.\n-  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n-  __ bang_stack_size(rbx, rcx);\n-#endif\n-\n-  \/\/ Load array of frame pcs into ECX\n-  __ movl(rcx,Address(rdi,Deoptimization::UnrollBlock::frame_pcs_offset()));\n-\n-  __ pop(rsi); \/\/ trash the pc\n-\n-  \/\/ Load array of frame sizes into ESI\n-  __ movptr(rsi,Address(rdi,Deoptimization::UnrollBlock::frame_sizes_offset()));\n-\n-  Address counter(rdi, Deoptimization::UnrollBlock::counter_temp_offset());\n-\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset()));\n-  __ movl(counter, rbx);\n-\n-  \/\/ Now adjust the caller's stack to make up for the extra locals\n-  \/\/ but record the original sp so that we can save it in the skeletal interpreter\n-  \/\/ frame and the stack walking of interpreter_sender will get the unextended sp\n-  \/\/ value and not the \"real\" sp value.\n-\n-  Address sp_temp(rdi, Deoptimization::UnrollBlock::sender_sp_temp_offset());\n-  __ movptr(sp_temp, rsp);\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::caller_adjustment_offset()));\n-  __ subptr(rsp, rbx);\n-\n-  \/\/ Push interpreter frames in a loop\n-  Label loop;\n-  __ bind(loop);\n-  __ movptr(rbx, Address(rsi, 0));      \/\/ Load frame size\n-  __ subptr(rbx, 2*wordSize);           \/\/ we'll push pc and rbp, by hand\n-  __ pushptr(Address(rcx, 0));          \/\/ save return address\n-  __ enter();                           \/\/ save old & set new rbp,\n-  __ subptr(rsp, rbx);                  \/\/ Prolog!\n-  __ movptr(rbx, sp_temp);              \/\/ sender's sp\n-  \/\/ This value is corrected by layout_activation_impl\n-  __ movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), NULL_WORD );\n-  __ movptr(Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize), rbx); \/\/ Make it walkable\n-  __ movptr(sp_temp, rsp);              \/\/ pass to next frame\n-  __ addptr(rsi, wordSize);             \/\/ Bump array pointer (sizes)\n-  __ addptr(rcx, wordSize);             \/\/ Bump array pointer (pcs)\n-  __ decrementl(counter);             \/\/ decrement counter\n-  __ jcc(Assembler::notZero, loop);\n-  __ pushptr(Address(rcx, 0));            \/\/ save final return address\n-\n-  \/\/ Re-push self-frame\n-  __ enter();                           \/\/ save old & set new rbp,\n-  __ subptr(rsp, (framesize-2) * wordSize);   \/\/ Prolog!\n-\n-\n-  \/\/ set last_Java_sp, last_Java_fp\n-  __ get_thread(rdi);\n-  __ set_last_Java_frame(rdi, noreg, rbp, nullptr, noreg);\n-\n-  \/\/ Call C code.  Need thread but NOT official VM entry\n-  \/\/ crud.  We cannot block on this call, no GC can happen.  Call should\n-  \/\/ restore return values to their stack-slots with the new SP.\n-  __ movptr(Address(rsp,arg0_off*wordSize),rdi);\n-  __ movl(Address(rsp,arg1_off*wordSize), Deoptimization::Unpack_uncommon_trap);\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)));\n-  \/\/ Set an oopmap for the call site\n-  oop_maps->add_gc_map( __ pc()-start, new OopMap( framesize, 0 ) );\n-\n-  __ get_thread(rdi);\n-  __ reset_last_Java_frame(rdi, true);\n-\n-  \/\/ Pop self-frame.\n-  __ leave();     \/\/ Epilog!\n-\n-  \/\/ Jump to interpreter\n-  __ ret(0);\n-\n-  \/\/ -------------\n-  \/\/ make sure all code is generated\n-  masm->flush();\n-\n-   _uncommon_trap_blob = UncommonTrapBlob::create(&buffer, oop_maps, framesize);\n-}\n-\n-\/\/------------------------------generate_exception_blob---------------------------\n-\/\/ creates exception blob at the end\n-\/\/ Using exception blob, this code is jumped from a compiled method.\n-\/\/\n-\/\/ Given an exception pc at a call we call into the runtime for the\n-\/\/ handler in this method. This handler might merely restore state\n-\/\/ (i.e. callee save registers) unwind the frame and jump to the\n-\/\/ exception handler for the nmethod if there is no Java level handler\n-\/\/ for the nmethod.\n-\/\/\n-\/\/ This code is entered with a jmp.\n-\/\/\n-\/\/ Arguments:\n-\/\/   rax: exception oop\n-\/\/   rdx: exception pc\n-\/\/\n-\/\/ Results:\n-\/\/   rax: exception oop\n-\/\/   rdx: exception pc in caller or ???\n-\/\/   destination: exception handler of caller\n-\/\/\n-\/\/ Note: the exception pc MUST be at a call (precise debug information)\n-\/\/       Only register rax, rdx, rcx are not callee saved.\n-\/\/\n-\n-void OptoRuntime::generate_exception_blob() {\n-\n-  \/\/ Capture info about frame layout\n-  enum layout {\n-    thread_off,                 \/\/ last_java_sp\n-    \/\/ The frame sender code expects that rbp will be in the \"natural\" place and\n-    \/\/ will override any oopMap setting for it. We must therefore force the layout\n-    \/\/ so that it agrees with the frame sender code.\n-    rbp_off,\n-    return_off,                 \/\/ slot for return address\n-    framesize\n-  };\n-\n-  \/\/ allocate space for the code\n-  ResourceMark rm;\n-  \/\/ setup code generation tools\n-  CodeBuffer   buffer(\"exception_blob\", 512, 512);\n-  MacroAssembler* masm = new MacroAssembler(&buffer);\n-\n-  OopMapSet *oop_maps = new OopMapSet();\n-\n-  address start = __ pc();\n-\n-  __ push(rdx);\n-  __ subptr(rsp, return_off * wordSize);   \/\/ Prolog!\n-\n-  \/\/ rbp, location is implicitly known\n-  __ movptr(Address(rsp,rbp_off  *wordSize), rbp);\n-\n-  \/\/ Store exception in Thread object. We cannot pass any arguments to the\n-  \/\/ handle_exception call, since we do not want to make any assumption\n-  \/\/ about the size of the frame where the exception happened in.\n-  __ get_thread(rcx);\n-  __ movptr(Address(rcx, JavaThread::exception_oop_offset()), rax);\n-  __ movptr(Address(rcx, JavaThread::exception_pc_offset()),  rdx);\n-\n-  \/\/ This call does all the hard work.  It checks if an exception handler\n-  \/\/ exists in the method.\n-  \/\/ If so, it returns the handler address.\n-  \/\/ If not, it prepares for stack-unwinding, restoring the callee-save\n-  \/\/ registers of the frame being removed.\n-  \/\/\n-  __ movptr(Address(rsp, thread_off * wordSize), rcx); \/\/ Thread is first argument\n-  __ set_last_Java_frame(rcx, noreg, noreg, nullptr, noreg);\n-\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C)));\n-\n-  \/\/ No registers to map, rbp is known implicitly\n-  oop_maps->add_gc_map( __ pc() - start,  new OopMap( framesize, 0 ));\n-  __ get_thread(rcx);\n-  __ reset_last_Java_frame(rcx, false);\n-\n-  \/\/ Restore callee-saved registers\n-  __ movptr(rbp, Address(rsp, rbp_off * wordSize));\n-\n-  __ addptr(rsp, return_off * wordSize);   \/\/ Epilog!\n-  __ pop(rdx); \/\/ Exception pc\n-\n-  \/\/ rax: exception handler for given <exception oop\/exception pc>\n-\n-  \/\/ We have a handler in rax, (could be deopt blob)\n-  \/\/ rdx - throwing pc, deopt blob will need it.\n-\n-  __ push(rax);\n-\n-  \/\/ Get the exception\n-  __ movptr(rax, Address(rcx, JavaThread::exception_oop_offset()));\n-  \/\/ Get the exception pc in case we are deoptimized\n-  __ movptr(rdx, Address(rcx, JavaThread::exception_pc_offset()));\n-#ifdef ASSERT\n-  __ movptr(Address(rcx, JavaThread::exception_handler_pc_offset()), NULL_WORD);\n-  __ movptr(Address(rcx, JavaThread::exception_pc_offset()), NULL_WORD);\n-#endif\n-  \/\/ Clear the exception oop so GC no longer processes it as a root.\n-  __ movptr(Address(rcx, JavaThread::exception_oop_offset()), NULL_WORD);\n-\n-  __ pop(rcx);\n-\n-  \/\/ rax: exception oop\n-  \/\/ rcx: exception handler\n-  \/\/ rdx: exception pc\n-  __ jmp (rcx);\n-\n-  \/\/ -------------\n-  \/\/ make sure all code is generated\n-  masm->flush();\n-\n-  _exception_blob = ExceptionBlob::create(&buffer, oop_maps, framesize);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/runtime_x86_32.cpp","additions":0,"deletions":331,"binary":false,"changes":331,"status":"deleted"},{"patch":"@@ -76,2 +76,0 @@\n-  \/\/ get hash\n-#ifdef _LP64\n@@ -83,3 +81,0 @@\n-#else\n-  __ andptr(result, markWord::hash_mask_in_place);\n-#endif \/\/_LP64\n@@ -89,3 +84,0 @@\n-#ifndef _LP64\n-  __ shrptr(result, markWord::hash_shift);\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1,2855 +0,0 @@\n-\/*\n- * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"code\/compiledIC.hpp\"\n-#include \"code\/debugInfoRec.hpp\"\n-#include \"code\/nativeInst.hpp\"\n-#include \"code\/vtableStubs.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/shared\/gcLocker.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetAssembler.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"oops\/klass.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n-#include \"runtime\/jniHandles.hpp\"\n-#include \"runtime\/safepointMechanism.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"runtime\/signature.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#include \"runtime\/timerTrace.hpp\"\n-#include \"runtime\/vframeArray.hpp\"\n-#include \"runtime\/vm_version.hpp\"\n-#include \"utilities\/align.hpp\"\n-#include \"vmreg_x86.inline.hpp\"\n-#ifdef COMPILER1\n-#include \"c1\/c1_Runtime1.hpp\"\n-#endif\n-#ifdef COMPILER2\n-#include \"opto\/runtime.hpp\"\n-#endif\n-\n-#define __ masm->\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/* nothing *\/\n-#else\n-#define BLOCK_COMMENT(str) __ block_comment(str)\n-#endif \/\/ PRODUCT\n-\n-const int StackAlignmentInSlots = StackAlignmentInBytes \/ VMRegImpl::stack_slot_size;\n-\n-class RegisterSaver {\n-  \/\/ Capture info about frame layout\n-#define DEF_XMM_OFFS(regnum) xmm ## regnum ## _off = xmm_off + (regnum)*16\/BytesPerInt, xmm ## regnum ## H_off\n-  enum layout {\n-                fpu_state_off = 0,\n-                fpu_state_end = fpu_state_off+FPUStateSizeInWords,\n-                st0_off, st0H_off,\n-                st1_off, st1H_off,\n-                st2_off, st2H_off,\n-                st3_off, st3H_off,\n-                st4_off, st4H_off,\n-                st5_off, st5H_off,\n-                st6_off, st6H_off,\n-                st7_off, st7H_off,\n-                xmm_off,\n-                DEF_XMM_OFFS(0),\n-                DEF_XMM_OFFS(1),\n-                DEF_XMM_OFFS(2),\n-                DEF_XMM_OFFS(3),\n-                DEF_XMM_OFFS(4),\n-                DEF_XMM_OFFS(5),\n-                DEF_XMM_OFFS(6),\n-                DEF_XMM_OFFS(7),\n-                flags_off = xmm7_off + 16\/BytesPerInt + 1, \/\/ 16-byte stack alignment fill word\n-                rdi_off,\n-                rsi_off,\n-                ignore_off,  \/\/ extra copy of rbp,\n-                rsp_off,\n-                rbx_off,\n-                rdx_off,\n-                rcx_off,\n-                rax_off,\n-                \/\/ The frame sender code expects that rbp will be in the \"natural\" place and\n-                \/\/ will override any oopMap setting for it. We must therefore force the layout\n-                \/\/ so that it agrees with the frame sender code.\n-                rbp_off,\n-                return_off,      \/\/ slot for return address\n-                reg_save_size };\n-  enum { FPU_regs_live = flags_off - fpu_state_end };\n-\n-  public:\n-\n-  static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words,\n-                                     int* total_frame_words, bool verify_fpu = true, bool save_vectors = false);\n-  static void restore_live_registers(MacroAssembler* masm, bool restore_vectors = false);\n-\n-  static int rax_offset() { return rax_off; }\n-  static int rbx_offset() { return rbx_off; }\n-\n-  \/\/ Offsets into the register save area\n-  \/\/ Used by deoptimization when it is managing result register\n-  \/\/ values on its own\n-\n-  static int raxOffset(void) { return rax_off; }\n-  static int rdxOffset(void) { return rdx_off; }\n-  static int rbxOffset(void) { return rbx_off; }\n-  static int xmm0Offset(void) { return xmm0_off; }\n-  \/\/ This really returns a slot in the fp save area, which one is not important\n-  static int fpResultOffset(void) { return st0_off; }\n-\n-  \/\/ During deoptimization only the result register need to be restored\n-  \/\/ all the other values have already been extracted.\n-\n-  static void restore_result_registers(MacroAssembler* masm);\n-\n-};\n-\n-OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, int additional_frame_words,\n-                                           int* total_frame_words, bool verify_fpu, bool save_vectors) {\n-  int num_xmm_regs = XMMRegister::number_of_registers;\n-  int ymm_bytes = num_xmm_regs * 16;\n-  int zmm_bytes = num_xmm_regs * 32;\n-#ifdef COMPILER2\n-  int opmask_state_bytes = KRegister::number_of_registers * 8;\n-  if (save_vectors) {\n-    assert(UseAVX > 0, \"Vectors larger than 16 byte long are supported only with AVX\");\n-    assert(MaxVectorSize <= 64, \"Only up to 64 byte long vectors are supported\");\n-    \/\/ Save upper half of YMM registers\n-    int vect_bytes = ymm_bytes;\n-    if (UseAVX > 2) {\n-      \/\/ Save upper half of ZMM registers as well\n-      vect_bytes += zmm_bytes;\n-      additional_frame_words += opmask_state_bytes \/ wordSize;\n-    }\n-    additional_frame_words += vect_bytes \/ wordSize;\n-  }\n-#else\n-  assert(!save_vectors, \"vectors are generated only by C2\");\n-#endif\n-  int frame_size_in_bytes = (reg_save_size + additional_frame_words) * wordSize;\n-  int frame_words = frame_size_in_bytes \/ wordSize;\n-  *total_frame_words = frame_words;\n-\n-  assert(FPUStateSizeInWords == 27, \"update stack layout\");\n-\n-  \/\/ save registers, fpu state, and flags\n-  \/\/ We assume caller has already has return address slot on the stack\n-  \/\/ We push epb twice in this sequence because we want the real rbp,\n-  \/\/ to be under the return like a normal enter and we want to use pusha\n-  \/\/ We push by hand instead of using push.\n-  __ enter();\n-  __ pusha();\n-  __ pushf();\n-  __ subptr(rsp,FPU_regs_live*wordSize); \/\/ Push FPU registers space\n-  __ push_FPU_state();          \/\/ Save FPU state & init\n-\n-  if (verify_fpu) {\n-    \/\/ Some stubs may have non standard FPU control word settings so\n-    \/\/ only check and reset the value when it required to be the\n-    \/\/ standard value.  The safepoint blob in particular can be used\n-    \/\/ in methods which are using the 24 bit control word for\n-    \/\/ optimized float math.\n-\n-#ifdef ASSERT\n-    \/\/ Make sure the control word has the expected value\n-    Label ok;\n-    __ cmpw(Address(rsp, 0), StubRoutines::x86::fpu_cntrl_wrd_std());\n-    __ jccb(Assembler::equal, ok);\n-    __ stop(\"corrupted control word detected\");\n-    __ bind(ok);\n-#endif\n-\n-    \/\/ Reset the control word to guard against exceptions being unmasked\n-    \/\/ since fstp_d can cause FPU stack underflow exceptions.  Write it\n-    \/\/ into the on stack copy and then reload that to make sure that the\n-    \/\/ current and future values are correct.\n-    __ movw(Address(rsp, 0), StubRoutines::x86::fpu_cntrl_wrd_std());\n-  }\n-\n-  __ frstor(Address(rsp, 0));\n-  if (!verify_fpu) {\n-    \/\/ Set the control word so that exceptions are masked for the\n-    \/\/ following code.\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-  }\n-\n-  int off = st0_off;\n-  int delta = st1_off - off;\n-\n-  \/\/ Save the FPU registers in de-opt-able form\n-  for (int n = 0; n < FloatRegister::number_of_registers; n++) {\n-    __ fstp_d(Address(rsp, off*wordSize));\n-    off += delta;\n-  }\n-\n-  off = xmm0_off;\n-  delta = xmm1_off - off;\n-  if(UseSSE == 1) {\n-    \/\/ Save the XMM state\n-    for (int n = 0; n < num_xmm_regs; n++) {\n-      __ movflt(Address(rsp, off*wordSize), as_XMMRegister(n));\n-      off += delta;\n-    }\n-  } else if(UseSSE >= 2) {\n-    \/\/ Save whole 128bit (16 bytes) XMM registers\n-    for (int n = 0; n < num_xmm_regs; n++) {\n-      __ movdqu(Address(rsp, off*wordSize), as_XMMRegister(n));\n-      off += delta;\n-    }\n-  }\n-\n-#ifdef COMPILER2\n-  if (save_vectors) {\n-    __ subptr(rsp, ymm_bytes);\n-    \/\/ Save upper half of YMM registers\n-    for (int n = 0; n < num_xmm_regs; n++) {\n-      __ vextractf128_high(Address(rsp, n*16), as_XMMRegister(n));\n-    }\n-    if (UseAVX > 2) {\n-      __ subptr(rsp, zmm_bytes);\n-      \/\/ Save upper half of ZMM registers\n-      for (int n = 0; n < num_xmm_regs; n++) {\n-        __ vextractf64x4_high(Address(rsp, n*32), as_XMMRegister(n));\n-      }\n-      __ subptr(rsp, opmask_state_bytes);\n-      \/\/ Save opmask registers\n-      for (int n = 0; n < KRegister::number_of_registers; n++) {\n-        __ kmov(Address(rsp, n*8), as_KRegister(n));\n-      }\n-    }\n-  }\n-#else\n-  assert(!save_vectors, \"vectors are generated only by C2\");\n-#endif\n-\n-  __ vzeroupper();\n-\n-  \/\/ Set an oopmap for the call site.  This oopmap will map all\n-  \/\/ oop-registers and debug-info registers as callee-saved.  This\n-  \/\/ will allow deoptimization at this safepoint to find all possible\n-  \/\/ debug-info recordings, as well as let GC find all oops.\n-\n-  OopMapSet *oop_maps = new OopMapSet();\n-  OopMap* map =  new OopMap( frame_words, 0 );\n-\n-#define STACK_OFFSET(x) VMRegImpl::stack2reg((x) + additional_frame_words)\n-#define NEXTREG(x) (x)->as_VMReg()->next()\n-\n-  map->set_callee_saved(STACK_OFFSET(rax_off), rax->as_VMReg());\n-  map->set_callee_saved(STACK_OFFSET(rcx_off), rcx->as_VMReg());\n-  map->set_callee_saved(STACK_OFFSET(rdx_off), rdx->as_VMReg());\n-  map->set_callee_saved(STACK_OFFSET(rbx_off), rbx->as_VMReg());\n-  \/\/ rbp, location is known implicitly, no oopMap\n-  map->set_callee_saved(STACK_OFFSET(rsi_off), rsi->as_VMReg());\n-  map->set_callee_saved(STACK_OFFSET(rdi_off), rdi->as_VMReg());\n-\n-  \/\/ %%% This is really a waste but we'll keep things as they were for now for the upper component\n-  off = st0_off;\n-  delta = st1_off - off;\n-  for (int n = 0; n < FloatRegister::number_of_registers; n++) {\n-    FloatRegister freg_name = as_FloatRegister(n);\n-    map->set_callee_saved(STACK_OFFSET(off), freg_name->as_VMReg());\n-    map->set_callee_saved(STACK_OFFSET(off+1), NEXTREG(freg_name));\n-    off += delta;\n-  }\n-  off = xmm0_off;\n-  delta = xmm1_off - off;\n-  for (int n = 0; n < num_xmm_regs; n++) {\n-    XMMRegister xmm_name = as_XMMRegister(n);\n-    map->set_callee_saved(STACK_OFFSET(off), xmm_name->as_VMReg());\n-    map->set_callee_saved(STACK_OFFSET(off+1), NEXTREG(xmm_name));\n-    off += delta;\n-  }\n-#undef NEXTREG\n-#undef STACK_OFFSET\n-\n-  return map;\n-}\n-\n-void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_vectors) {\n-  int opmask_state_bytes = 0;\n-  int additional_frame_bytes = 0;\n-  int num_xmm_regs = XMMRegister::number_of_registers;\n-  int ymm_bytes = num_xmm_regs * 16;\n-  int zmm_bytes = num_xmm_regs * 32;\n-  \/\/ Recover XMM & FPU state\n-#ifdef COMPILER2\n-  if (restore_vectors) {\n-    assert(UseAVX > 0, \"Vectors larger than 16 byte long are supported only with AVX\");\n-    assert(MaxVectorSize <= 64, \"Only up to 64 byte long vectors are supported\");\n-    \/\/ Save upper half of YMM registers\n-    additional_frame_bytes = ymm_bytes;\n-    if (UseAVX > 2) {\n-      \/\/ Save upper half of ZMM registers as well\n-      additional_frame_bytes += zmm_bytes;\n-      opmask_state_bytes = KRegister::number_of_registers * 8;\n-      additional_frame_bytes += opmask_state_bytes;\n-    }\n-  }\n-#else\n-  assert(!restore_vectors, \"vectors are generated only by C2\");\n-#endif\n-\n-  int off = xmm0_off;\n-  int delta = xmm1_off - off;\n-\n-  __ vzeroupper();\n-\n-  if (UseSSE == 1) {\n-    \/\/ Restore XMM registers\n-    assert(additional_frame_bytes == 0, \"\");\n-    for (int n = 0; n < num_xmm_regs; n++) {\n-      __ movflt(as_XMMRegister(n), Address(rsp, off*wordSize));\n-      off += delta;\n-    }\n-  } else if (UseSSE >= 2) {\n-    \/\/ Restore whole 128bit (16 bytes) XMM registers. Do this before restoring YMM and\n-    \/\/ ZMM because the movdqu instruction zeros the upper part of the XMM register.\n-    for (int n = 0; n < num_xmm_regs; n++) {\n-      __ movdqu(as_XMMRegister(n), Address(rsp, off*wordSize+additional_frame_bytes));\n-      off += delta;\n-    }\n-  }\n-\n-  if (restore_vectors) {\n-    off = additional_frame_bytes - ymm_bytes;\n-    \/\/ Restore upper half of YMM registers.\n-    for (int n = 0; n < num_xmm_regs; n++) {\n-      __ vinsertf128_high(as_XMMRegister(n), Address(rsp, n*16+off));\n-    }\n-    if (UseAVX > 2) {\n-      \/\/ Restore upper half of ZMM registers.\n-      off = opmask_state_bytes;\n-      for (int n = 0; n < num_xmm_regs; n++) {\n-        __ vinsertf64x4_high(as_XMMRegister(n), Address(rsp, n*32+off));\n-      }\n-      for (int n = 0; n < KRegister::number_of_registers; n++) {\n-        __ kmov(as_KRegister(n), Address(rsp, n*8));\n-      }\n-    }\n-    __ addptr(rsp, additional_frame_bytes);\n-  }\n-\n-  __ pop_FPU_state();\n-  __ addptr(rsp, FPU_regs_live*wordSize); \/\/ Pop FPU registers\n-\n-  __ popf();\n-  __ popa();\n-  \/\/ Get the rbp, described implicitly by the frame sender code (no oopMap)\n-  __ pop(rbp);\n-}\n-\n-void RegisterSaver::restore_result_registers(MacroAssembler* masm) {\n-\n-  \/\/ Just restore result register. Only used by deoptimization. By\n-  \/\/ now any callee save register that needs to be restore to a c2\n-  \/\/ caller of the deoptee has been extracted into the vframeArray\n-  \/\/ and will be stuffed into the c2i adapter we create for later\n-  \/\/ restoration so only result registers need to be restored here.\n-  \/\/\n-\n-  __ frstor(Address(rsp, 0));      \/\/ Restore fpu state\n-\n-  \/\/ Recover XMM & FPU state\n-  if( UseSSE == 1 ) {\n-    __ movflt(xmm0, Address(rsp, xmm0_off*wordSize));\n-  } else if( UseSSE >= 2 ) {\n-    __ movdbl(xmm0, Address(rsp, xmm0_off*wordSize));\n-  }\n-  __ movptr(rax, Address(rsp, rax_off*wordSize));\n-  __ movptr(rdx, Address(rsp, rdx_off*wordSize));\n-  \/\/ Pop all of the register save are off the stack except the return address\n-  __ addptr(rsp, return_off * wordSize);\n-}\n-\n-\/\/ Is vector's size (in bytes) bigger than a size saved by default?\n-\/\/ 16 bytes XMM registers are saved by default using SSE2 movdqu instructions.\n-\/\/ Note, MaxVectorSize == 0 with UseSSE < 2 and vectors are not generated.\n-bool SharedRuntime::is_wide_vector(int size) {\n-  return size > 16;\n-}\n-\n-\/\/ The java_calling_convention describes stack locations as ideal slots on\n-\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n-\/\/ (like the placement of the register window) the slots must be biased by\n-\/\/ the following value.\n-static int reg2offset_in(VMReg r) {\n-  \/\/ Account for saved rbp, and return address\n-  \/\/ This should really be in_preserve_stack_slots\n-  return (r->reg2stack() + 2) * VMRegImpl::stack_slot_size;\n-}\n-\n-static int reg2offset_out(VMReg r) {\n-  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n-}\n-\n-\/\/ ---------------------------------------------------------------------------\n-\/\/ Read the array of BasicTypes from a signature, and compute where the\n-\/\/ arguments should go.  Values in the VMRegPair regs array refer to 4-byte\n-\/\/ quantities.  Values less than SharedInfo::stack0 are registers, those above\n-\/\/ refer to 4-byte stack slots.  All stack slots are based off of the stack pointer\n-\/\/ as framesizes are fixed.\n-\/\/ VMRegImpl::stack0 refers to the first slot 0(sp).\n-\/\/ and VMRegImpl::stack0+1 refers to the memory word 4-byes higher.\n-\/\/ Register up to Register::number_of_registers are the 32-bit\n-\/\/ integer registers.\n-\n-\/\/ Pass first two oop\/int args in registers ECX and EDX.\n-\/\/ Pass first two float\/double args in registers XMM0 and XMM1.\n-\/\/ Doubles have precedence, so if you pass a mix of floats and doubles\n-\/\/ the doubles will grab the registers before the floats will.\n-\n-\/\/ Note: the INPUTS in sig_bt are in units of Java argument words, which are\n-\/\/ either 32-bit or 64-bit depending on the build.  The OUTPUTS are in 32-bit\n-\/\/ units regardless of build. Of course for i486 there is no 64 bit build\n-\n-\n-\/\/ ---------------------------------------------------------------------------\n-\/\/ The compiled Java calling convention.\n-\/\/ Pass first two oop\/int args in registers ECX and EDX.\n-\/\/ Pass first two float\/double args in registers XMM0 and XMM1.\n-\/\/ Doubles have precedence, so if you pass a mix of floats and doubles\n-\/\/ the doubles will grab the registers before the floats will.\n-int SharedRuntime::java_calling_convention(const BasicType *sig_bt,\n-                                           VMRegPair *regs,\n-                                           int total_args_passed) {\n-  uint    stack = 0;          \/\/ Starting stack position for args on stack\n-\n-\n-  \/\/ Pass first two oop\/int args in registers ECX and EDX.\n-  uint reg_arg0 = 9999;\n-  uint reg_arg1 = 9999;\n-\n-  \/\/ Pass first two float\/double args in registers XMM0 and XMM1.\n-  \/\/ Doubles have precedence, so if you pass a mix of floats and doubles\n-  \/\/ the doubles will grab the registers before the floats will.\n-  \/\/ CNC - TURNED OFF FOR non-SSE.\n-  \/\/       On Intel we have to round all doubles (and most floats) at\n-  \/\/       call sites by storing to the stack in any case.\n-  \/\/ UseSSE=0 ==> Don't Use ==> 9999+0\n-  \/\/ UseSSE=1 ==> Floats only ==> 9999+1\n-  \/\/ UseSSE>=2 ==> Floats or doubles ==> 9999+2\n-  enum { fltarg_dontuse = 9999+0, fltarg_float_only = 9999+1, fltarg_flt_dbl = 9999+2 };\n-  uint fargs = (UseSSE>=2) ? 2 : UseSSE;\n-  uint freg_arg0 = 9999+fargs;\n-  uint freg_arg1 = 9999+fargs;\n-\n-  \/\/ Pass doubles & longs aligned on the stack.  First count stack slots for doubles\n-  int i;\n-  for( i = 0; i < total_args_passed; i++) {\n-    if( sig_bt[i] == T_DOUBLE ) {\n-      \/\/ first 2 doubles go in registers\n-      if( freg_arg0 == fltarg_flt_dbl ) freg_arg0 = i;\n-      else if( freg_arg1 == fltarg_flt_dbl ) freg_arg1 = i;\n-      else \/\/ Else double is passed low on the stack to be aligned.\n-        stack += 2;\n-    } else if( sig_bt[i] == T_LONG ) {\n-      stack += 2;\n-    }\n-  }\n-  int dstack = 0;             \/\/ Separate counter for placing doubles\n-\n-  \/\/ Now pick where all else goes.\n-  for( i = 0; i < total_args_passed; i++) {\n-    \/\/ From the type and the argument number (count) compute the location\n-    switch( sig_bt[i] ) {\n-    case T_SHORT:\n-    case T_CHAR:\n-    case T_BYTE:\n-    case T_BOOLEAN:\n-    case T_INT:\n-    case T_ARRAY:\n-    case T_OBJECT:\n-    case T_ADDRESS:\n-      if( reg_arg0 == 9999 )  {\n-        reg_arg0 = i;\n-        regs[i].set1(rcx->as_VMReg());\n-      } else if( reg_arg1 == 9999 )  {\n-        reg_arg1 = i;\n-        regs[i].set1(rdx->as_VMReg());\n-      } else {\n-        regs[i].set1(VMRegImpl::stack2reg(stack++));\n-      }\n-      break;\n-    case T_FLOAT:\n-      if( freg_arg0 == fltarg_flt_dbl || freg_arg0 == fltarg_float_only ) {\n-        freg_arg0 = i;\n-        regs[i].set1(xmm0->as_VMReg());\n-      } else if( freg_arg1 == fltarg_flt_dbl || freg_arg1 == fltarg_float_only ) {\n-        freg_arg1 = i;\n-        regs[i].set1(xmm1->as_VMReg());\n-      } else {\n-        regs[i].set1(VMRegImpl::stack2reg(stack++));\n-      }\n-      break;\n-    case T_LONG:\n-      assert((i + 1) < total_args_passed && sig_bt[i+1] == T_VOID, \"missing Half\" );\n-      regs[i].set2(VMRegImpl::stack2reg(dstack));\n-      dstack += 2;\n-      break;\n-    case T_DOUBLE:\n-      assert((i + 1) < total_args_passed && sig_bt[i+1] == T_VOID, \"missing Half\" );\n-      if( freg_arg0 == (uint)i ) {\n-        regs[i].set2(xmm0->as_VMReg());\n-      } else if( freg_arg1 == (uint)i ) {\n-        regs[i].set2(xmm1->as_VMReg());\n-      } else {\n-        regs[i].set2(VMRegImpl::stack2reg(dstack));\n-        dstack += 2;\n-      }\n-      break;\n-    case T_VOID: regs[i].set_bad(); break;\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  }\n-\n-  return stack;\n-}\n-\n-\/\/ Patch the callers callsite with entry to compiled code if it exists.\n-static void patch_callers_callsite(MacroAssembler *masm) {\n-  Label L;\n-  __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-  __ jcc(Assembler::equal, L);\n-  \/\/ Schedule the branch target address early.\n-  \/\/ Call into the VM to patch the caller, then jump to compiled callee\n-  \/\/ rax, isn't live so capture return address while we easily can\n-  __ movptr(rax, Address(rsp, 0));\n-  __ pusha();\n-  __ pushf();\n-\n-  if (UseSSE == 1) {\n-    __ subptr(rsp, 2*wordSize);\n-    __ movflt(Address(rsp, 0), xmm0);\n-    __ movflt(Address(rsp, wordSize), xmm1);\n-  }\n-  if (UseSSE >= 2) {\n-    __ subptr(rsp, 4*wordSize);\n-    __ movdbl(Address(rsp, 0), xmm0);\n-    __ movdbl(Address(rsp, 2*wordSize), xmm1);\n-  }\n-#ifdef COMPILER2\n-  \/\/ C2 may leave the stack dirty if not in SSE2+ mode\n-  if (UseSSE >= 2) {\n-    __ verify_FPU(0, \"c2i transition should have clean FPU stack\");\n-  } else {\n-    __ empty_FPU_stack();\n-  }\n-#endif \/* COMPILER2 *\/\n-\n-  \/\/ VM needs caller's callsite\n-  __ push(rax);\n-  \/\/ VM needs target method\n-  __ push(rbx);\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)));\n-  __ addptr(rsp, 2*wordSize);\n-\n-  if (UseSSE == 1) {\n-    __ movflt(xmm0, Address(rsp, 0));\n-    __ movflt(xmm1, Address(rsp, wordSize));\n-    __ addptr(rsp, 2*wordSize);\n-  }\n-  if (UseSSE >= 2) {\n-    __ movdbl(xmm0, Address(rsp, 0));\n-    __ movdbl(xmm1, Address(rsp, 2*wordSize));\n-    __ addptr(rsp, 4*wordSize);\n-  }\n-\n-  __ popf();\n-  __ popa();\n-  __ bind(L);\n-}\n-\n-\n-static void move_c2i_double(MacroAssembler *masm, XMMRegister r, int st_off) {\n-  int next_off = st_off - Interpreter::stackElementSize;\n-  __ movdbl(Address(rsp, next_off), r);\n-}\n-\n-static void gen_c2i_adapter(MacroAssembler *masm,\n-                            int total_args_passed,\n-                            int comp_args_on_stack,\n-                            const BasicType *sig_bt,\n-                            const VMRegPair *regs,\n-                            Label& skip_fixup) {\n-  \/\/ Before we get into the guts of the C2I adapter, see if we should be here\n-  \/\/ at all.  We've come from compiled code and are attempting to jump to the\n-  \/\/ interpreter, which means the caller made a static call to get here\n-  \/\/ (vcalls always get a compiled target if there is one).  Check for a\n-  \/\/ compiled target.  If there is one, we need to patch the caller's call.\n-  patch_callers_callsite(masm);\n-\n-  __ bind(skip_fixup);\n-\n-#ifdef COMPILER2\n-  \/\/ C2 may leave the stack dirty if not in SSE2+ mode\n-  if (UseSSE >= 2) {\n-    __ verify_FPU(0, \"c2i transition should have clean FPU stack\");\n-  } else {\n-    __ empty_FPU_stack();\n-  }\n-#endif \/* COMPILER2 *\/\n-\n-  \/\/ Since all args are passed on the stack, total_args_passed * interpreter_\n-  \/\/ stack_element_size  is the\n-  \/\/ space we need.\n-  int extraspace = total_args_passed * Interpreter::stackElementSize;\n-\n-  \/\/ Get return address\n-  __ pop(rax);\n-\n-  \/\/ set senderSP value\n-  __ movptr(rsi, rsp);\n-\n-  __ subptr(rsp, extraspace);\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ st_off points to lowest address on stack.\n-    int st_off = ((total_args_passed - 1) - i) * Interpreter::stackElementSize;\n-    int next_off = st_off - Interpreter::stackElementSize;\n-\n-    \/\/ Say 4 args:\n-    \/\/ i   st_off\n-    \/\/ 0   12 T_LONG\n-    \/\/ 1    8 T_VOID\n-    \/\/ 2    4 T_OBJECT\n-    \/\/ 3    0 T_BOOL\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-\n-    if (r_1->is_stack()) {\n-      \/\/ memory to memory use fpu stack top\n-      int ld_off = r_1->reg2stack() * VMRegImpl::stack_slot_size + extraspace;\n-\n-      if (!r_2->is_valid()) {\n-        __ movl(rdi, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, st_off), rdi);\n-      } else {\n-\n-        \/\/ ld_off == LSW, ld_off+VMRegImpl::stack_slot_size == MSW\n-        \/\/ st_off == MSW, st_off-wordSize == LSW\n-\n-        __ movptr(rdi, Address(rsp, ld_off));\n-        __ movptr(Address(rsp, next_off), rdi);\n-        __ movptr(rdi, Address(rsp, ld_off + wordSize));\n-        __ movptr(Address(rsp, st_off), rdi);\n-      }\n-    } else if (r_1->is_Register()) {\n-      Register r = r_1->as_Register();\n-      if (!r_2->is_valid()) {\n-        __ movl(Address(rsp, st_off), r);\n-      } else {\n-        \/\/ long\/double in gpr\n-        ShouldNotReachHere();\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        __ movflt(Address(rsp, st_off), r_1->as_XMMRegister());\n-      } else {\n-        assert(sig_bt[i] == T_DOUBLE || sig_bt[i] == T_LONG, \"wrong type\");\n-        move_c2i_double(masm, r_1->as_XMMRegister(), st_off);\n-      }\n-    }\n-  }\n-\n-  \/\/ Schedule the branch target address early.\n-  __ movptr(rcx, Address(rbx, in_bytes(Method::interpreter_entry_offset())));\n-  \/\/ And repush original return address\n-  __ push(rax);\n-  __ jmp(rcx);\n-}\n-\n-\n-static void move_i2c_double(MacroAssembler *masm, XMMRegister r, Register saved_sp, int ld_off) {\n-  int next_val_off = ld_off - Interpreter::stackElementSize;\n-  __ movdbl(r, Address(saved_sp, next_val_off));\n-}\n-\n-static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg,\n-                        address code_start, address code_end,\n-                        Label& L_ok) {\n-  Label L_fail;\n-  __ lea(temp_reg, AddressLiteral(code_start, relocInfo::none));\n-  __ cmpptr(pc_reg, temp_reg);\n-  __ jcc(Assembler::belowEqual, L_fail);\n-  __ lea(temp_reg, AddressLiteral(code_end, relocInfo::none));\n-  __ cmpptr(pc_reg, temp_reg);\n-  __ jcc(Assembler::below, L_ok);\n-  __ bind(L_fail);\n-}\n-\n-void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,\n-                                    int total_args_passed,\n-                                    int comp_args_on_stack,\n-                                    const BasicType *sig_bt,\n-                                    const VMRegPair *regs) {\n-  \/\/ Note: rsi contains the senderSP on entry. We must preserve it since\n-  \/\/ we may do a i2c -> c2i transition if we lose a race where compiled\n-  \/\/ code goes non-entrant while we get args ready.\n-\n-  \/\/ Adapters can be frameless because they do not require the caller\n-  \/\/ to perform additional cleanup work, such as correcting the stack pointer.\n-  \/\/ An i2c adapter is frameless because the *caller* frame, which is interpreted,\n-  \/\/ routinely repairs its own stack pointer (from interpreter_frame_last_sp),\n-  \/\/ even if a callee has modified the stack pointer.\n-  \/\/ A c2i adapter is frameless because the *callee* frame, which is interpreted,\n-  \/\/ routinely repairs its caller's stack pointer (from sender_sp, which is set\n-  \/\/ up via the senderSP register).\n-  \/\/ In other words, if *either* the caller or callee is interpreted, we can\n-  \/\/ get the stack pointer repaired after a call.\n-  \/\/ This is why c2i and i2c adapters cannot be indefinitely composed.\n-  \/\/ In particular, if a c2i adapter were to somehow call an i2c adapter,\n-  \/\/ both caller and callee would be compiled methods, and neither would\n-  \/\/ clean up the stack pointer changes performed by the two adapters.\n-  \/\/ If this happens, control eventually transfers back to the compiled\n-  \/\/ caller, but with an uncorrected stack, causing delayed havoc.\n-\n-  \/\/ Pick up the return address\n-  __ movptr(rax, Address(rsp, 0));\n-\n-  if (VerifyAdapterCalls &&\n-      (Interpreter::code() != nullptr || StubRoutines::final_stubs_code() != nullptr)) {\n-    \/\/ So, let's test for cascading c2i\/i2c adapters right now.\n-    \/\/  assert(Interpreter::contains($return_addr) ||\n-    \/\/         StubRoutines::contains($return_addr),\n-    \/\/         \"i2c adapter must return to an interpreter frame\");\n-    __ block_comment(\"verify_i2c { \");\n-    Label L_ok;\n-    if (Interpreter::code() != nullptr) {\n-      range_check(masm, rax, rdi,\n-                  Interpreter::code()->code_start(), Interpreter::code()->code_end(),\n-                  L_ok);\n-    }\n-    if (StubRoutines::initial_stubs_code() != nullptr) {\n-      range_check(masm, rax, rdi,\n-                  StubRoutines::initial_stubs_code()->code_begin(),\n-                  StubRoutines::initial_stubs_code()->code_end(),\n-                  L_ok);\n-    }\n-    if (StubRoutines::final_stubs_code() != nullptr) {\n-      range_check(masm, rax, rdi,\n-                  StubRoutines::final_stubs_code()->code_begin(),\n-                  StubRoutines::final_stubs_code()->code_end(),\n-                  L_ok);\n-    }\n-    const char* msg = \"i2c adapter must return to an interpreter frame\";\n-    __ block_comment(msg);\n-    __ stop(msg);\n-    __ bind(L_ok);\n-    __ block_comment(\"} verify_i2ce \");\n-  }\n-\n-  \/\/ Must preserve original SP for loading incoming arguments because\n-  \/\/ we need to align the outgoing SP for compiled code.\n-  __ movptr(rdi, rsp);\n-\n-  \/\/ Cut-out for having no stack args.  Since up to 2 int\/oop args are passed\n-  \/\/ in registers, we will occasionally have no stack args.\n-  int comp_words_on_stack = 0;\n-  if (comp_args_on_stack) {\n-    \/\/ Sig words on the stack are greater-than VMRegImpl::stack0.  Those in\n-    \/\/ registers are below.  By subtracting stack0, we either get a negative\n-    \/\/ number (all values in registers) or the maximum stack slot accessed.\n-    \/\/ int comp_args_on_stack = VMRegImpl::reg2stack(max_arg);\n-    \/\/ Convert 4-byte stack slots to words.\n-    comp_words_on_stack = align_up(comp_args_on_stack*4, wordSize)>>LogBytesPerWord;\n-    \/\/ Round up to miminum stack alignment, in wordSize\n-    comp_words_on_stack = align_up(comp_words_on_stack, 2);\n-    __ subptr(rsp, comp_words_on_stack * wordSize);\n-  }\n-\n-  \/\/ Align the outgoing SP\n-  __ andptr(rsp, -(StackAlignmentInBytes));\n-\n-  \/\/ push the return address on the stack (note that pushing, rather\n-  \/\/ than storing it, yields the correct frame alignment for the callee)\n-  __ push(rax);\n-\n-  \/\/ Put saved SP in another register\n-  const Register saved_sp = rax;\n-  __ movptr(saved_sp, rdi);\n-\n-\n-  \/\/ Will jump to the compiled code just as if compiled code was doing it.\n-  \/\/ Pre-load the register-jump target early, to schedule it better.\n-  __ movptr(rdi, Address(rbx, in_bytes(Method::from_compiled_offset())));\n-\n-  \/\/ Now generate the shuffle code.  Pick up all register args and move the\n-  \/\/ rest through the floating point stack top.\n-  for (int i = 0; i < total_args_passed; i++) {\n-    if (sig_bt[i] == T_VOID) {\n-      \/\/ Longs and doubles are passed in native word order, but misaligned\n-      \/\/ in the 32-bit build.\n-      assert(i > 0 && (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), \"missing half\");\n-      continue;\n-    }\n-\n-    \/\/ Pick up 0, 1 or 2 words from SP+offset.\n-\n-    assert(!regs[i].second()->is_valid() || regs[i].first()->next() == regs[i].second(),\n-            \"scrambled load targets?\");\n-    \/\/ Load in argument order going down.\n-    int ld_off = (total_args_passed - i) * Interpreter::stackElementSize;\n-    \/\/ Point to interpreter value (vs. tag)\n-    int next_off = ld_off - Interpreter::stackElementSize;\n-    \/\/\n-    \/\/\n-    \/\/\n-    VMReg r_1 = regs[i].first();\n-    VMReg r_2 = regs[i].second();\n-    if (!r_1->is_valid()) {\n-      assert(!r_2->is_valid(), \"\");\n-      continue;\n-    }\n-    if (r_1->is_stack()) {\n-      \/\/ Convert stack slot to an SP offset (+ wordSize to account for return address )\n-      int st_off = regs[i].first()->reg2stack()*VMRegImpl::stack_slot_size + wordSize;\n-\n-      \/\/ We can use rsi as a temp here because compiled code doesn't need rsi as an input\n-      \/\/ and if we end up going thru a c2i because of a miss a reasonable value of rsi\n-      \/\/ we be generated.\n-      if (!r_2->is_valid()) {\n-        \/\/ __ fld_s(Address(saved_sp, ld_off));\n-        \/\/ __ fstp_s(Address(rsp, st_off));\n-        __ movl(rsi, Address(saved_sp, ld_off));\n-        __ movptr(Address(rsp, st_off), rsi);\n-      } else {\n-        \/\/ Interpreter local[n] == MSW, local[n+1] == LSW however locals\n-        \/\/ are accessed as negative so LSW is at LOW address\n-\n-        \/\/ ld_off is MSW so get LSW\n-        \/\/ st_off is LSW (i.e. reg.first())\n-        \/\/ __ fld_d(Address(saved_sp, next_off));\n-        \/\/ __ fstp_d(Address(rsp, st_off));\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE\n-        \/\/ the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the interpreter.\n-        \/\/\n-        \/\/ Interpreter local[n] == MSW, local[n+1] == LSW however locals\n-        \/\/ are accessed as negative so LSW is at LOW address\n-\n-        \/\/ ld_off is MSW so get LSW\n-        __ movptr(rsi, Address(saved_sp, next_off));\n-        __ movptr(Address(rsp, st_off), rsi);\n-        __ movptr(rsi, Address(saved_sp, ld_off));\n-        __ movptr(Address(rsp, st_off + wordSize), rsi);\n-      }\n-    } else if (r_1->is_Register()) {  \/\/ Register argument\n-      Register r = r_1->as_Register();\n-      assert(r != rax, \"must be different\");\n-      if (r_2->is_valid()) {\n-        \/\/\n-        \/\/ We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE\n-        \/\/ the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case\n-        \/\/ So we must adjust where to pick up the data to match the interpreter.\n-\n-        \/\/ this can be a misaligned move\n-        __ movptr(r, Address(saved_sp, next_off));\n-        assert(r_2->as_Register() != rax, \"need another temporary register\");\n-        \/\/ Remember r_1 is low address (and LSB on x86)\n-        \/\/ So r_2 gets loaded from high address regardless of the platform\n-        __ movptr(r_2->as_Register(), Address(saved_sp, ld_off));\n-      } else {\n-        __ movl(r, Address(saved_sp, ld_off));\n-      }\n-    } else {\n-      assert(r_1->is_XMMRegister(), \"\");\n-      if (!r_2->is_valid()) {\n-        __ movflt(r_1->as_XMMRegister(), Address(saved_sp, ld_off));\n-      } else {\n-        move_i2c_double(masm, r_1->as_XMMRegister(), saved_sp, ld_off);\n-      }\n-    }\n-  }\n-\n-  \/\/ 6243940 We might end up in handle_wrong_method if\n-  \/\/ the callee is deoptimized as we race thru here. If that\n-  \/\/ happens we don't want to take a safepoint because the\n-  \/\/ caller frame will look interpreted and arguments are now\n-  \/\/ \"compiled\" so it is much better to make this transition\n-  \/\/ invisible to the stack walking code. Unfortunately if\n-  \/\/ we try and find the callee by normal means a safepoint\n-  \/\/ is possible. So we stash the desired callee in the thread\n-  \/\/ and the vm will find there should this case occur.\n-\n-  __ get_thread(rax);\n-  __ movptr(Address(rax, JavaThread::callee_target_offset()), rbx);\n-\n-  \/\/ move Method* to rax, in case we end up in an c2i adapter.\n-  \/\/ the c2i adapters expect Method* in rax, (c2) because c2's\n-  \/\/ resolve stubs return the result (the method) in rax,.\n-  \/\/ I'd love to fix this.\n-  __ mov(rax, rbx);\n-\n-  __ jmp(rdi);\n-}\n-\n-\/\/ ---------------------------------------------------------------\n-AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,\n-                                                            int total_args_passed,\n-                                                            int comp_args_on_stack,\n-                                                            const BasicType *sig_bt,\n-                                                            const VMRegPair *regs,\n-                                                            AdapterFingerPrint* fingerprint) {\n-  address i2c_entry = __ pc();\n-\n-  gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);\n-\n-  \/\/ -------------------------------------------------------------------------\n-  \/\/ Generate a C2I adapter.  On entry we know rbx, holds the Method* during calls\n-  \/\/ to the interpreter.  The args start out packed in the compiled layout.  They\n-  \/\/ need to be unpacked into the interpreter layout.  This will almost always\n-  \/\/ require some stack space.  We grow the current (compiled) stack, then repack\n-  \/\/ the args.  We  finally end in a jump to the generic interpreter entry point.\n-  \/\/ On exit from the interpreter, the interpreter will restore our SP (lest the\n-  \/\/ compiled code, which relies solely on SP and not EBP, get sick).\n-\n-  address c2i_unverified_entry = __ pc();\n-  Label skip_fixup;\n-\n-  Register data = rax;\n-  Register receiver = rcx;\n-  Register temp = rbx;\n-\n-  {\n-    __ ic_check(1 \/* end_alignment *\/);\n-    __ movptr(rbx, Address(data, CompiledICData::speculated_method_offset()));\n-    \/\/ Method might have been compiled since the call site was patched to\n-    \/\/ interpreted if that is the case treat it as a miss so we can get\n-    \/\/ the call site corrected.\n-    __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, skip_fixup);\n-  }\n-\n-  address c2i_entry = __ pc();\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->c2i_entry_barrier(masm);\n-\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n-\n-  return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry);\n-}\n-\n-int SharedRuntime::c_calling_convention(const BasicType *sig_bt,\n-                                         VMRegPair *regs,\n-                                         int total_args_passed) {\n-\n-\/\/ We return the amount of VMRegImpl stack slots we need to reserve for all\n-\/\/ the arguments NOT counting out_preserve_stack_slots.\n-\n-  uint    stack = 0;        \/\/ All arguments on stack\n-\n-  for( int i = 0; i < total_args_passed; i++) {\n-    \/\/ From the type and the argument number (count) compute the location\n-    switch( sig_bt[i] ) {\n-    case T_BOOLEAN:\n-    case T_CHAR:\n-    case T_FLOAT:\n-    case T_BYTE:\n-    case T_SHORT:\n-    case T_INT:\n-    case T_OBJECT:\n-    case T_ARRAY:\n-    case T_ADDRESS:\n-    case T_METADATA:\n-      regs[i].set1(VMRegImpl::stack2reg(stack++));\n-      break;\n-    case T_LONG:\n-    case T_DOUBLE: \/\/ The stack numbering is reversed from Java\n-      \/\/ Since C arguments do not get reversed, the ordering for\n-      \/\/ doubles on the stack must be opposite the Java convention\n-      assert((i + 1) < total_args_passed && sig_bt[i+1] == T_VOID, \"missing Half\" );\n-      regs[i].set2(VMRegImpl::stack2reg(stack));\n-      stack += 2;\n-      break;\n-    case T_VOID: regs[i].set_bad(); break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  }\n-  return stack;\n-}\n-\n-int SharedRuntime::vector_calling_convention(VMRegPair *regs,\n-                                             uint num_bits,\n-                                             uint total_args_passed) {\n-  Unimplemented();\n-  return 0;\n-}\n-\n-\/\/ A simple move of integer like type\n-static void simple_move32(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      \/\/ __ ld(FP, reg2offset(src.first()), L5);\n-      \/\/ __ st(L5, SP, reg2offset(dst.first()));\n-      __ movl2ptr(rax, Address(rbp, reg2offset_in(src.first())));\n-      __ movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n-    } else {\n-      \/\/ stack to reg\n-      __ movl2ptr(dst.first()->as_Register(),  Address(rbp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ no need to sign extend on 64bit\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ mov(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ An oop arg. Must pass a handle not the oop itself\n-static void object_move(MacroAssembler* masm,\n-                        OopMap* map,\n-                        int oop_handle_offset,\n-                        int framesize_in_slots,\n-                        VMRegPair src,\n-                        VMRegPair dst,\n-                        bool is_receiver,\n-                        int* receiver_offset) {\n-\n-  \/\/ Because of the calling conventions we know that src can be a\n-  \/\/ register or a stack location. dst can only be a stack location.\n-\n-  assert(dst.first()->is_stack(), \"must be stack\");\n-  \/\/ must pass a handle. First figure out the location we use as a handle\n-\n-  if (src.first()->is_stack()) {\n-    \/\/ Oop is already on the stack as an argument\n-    Register rHandle = rax;\n-    Label nil;\n-    __ xorptr(rHandle, rHandle);\n-    __ cmpptr(Address(rbp, reg2offset_in(src.first())), NULL_WORD);\n-    __ jcc(Assembler::equal, nil);\n-    __ lea(rHandle, Address(rbp, reg2offset_in(src.first())));\n-    __ bind(nil);\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), rHandle);\n-\n-    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n-    if (is_receiver) {\n-      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n-    }\n-  } else {\n-    \/\/ Oop is in a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles\n-    const Register rOop = src.first()->as_Register();\n-    const Register rHandle = rax;\n-    int oop_slot = (rOop == rcx ? 0 : 1) * VMRegImpl::slots_per_word + oop_handle_offset;\n-    int offset = oop_slot*VMRegImpl::stack_slot_size;\n-    Label skip;\n-    __ movptr(Address(rsp, offset), rOop);\n-    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n-    __ xorptr(rHandle, rHandle);\n-    __ cmpptr(rOop, NULL_WORD);\n-    __ jcc(Assembler::equal, skip);\n-    __ lea(rHandle, Address(rsp, offset));\n-    __ bind(skip);\n-    \/\/ Store the handle parameter\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), rHandle);\n-    if (is_receiver) {\n-      *receiver_offset = offset;\n-    }\n-  }\n-}\n-\n-\/\/ A float arg may have to do float reg int reg conversion\n-static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(!src.second()->is_valid() && !dst.second()->is_valid(), \"bad float_move\");\n-\n-  \/\/ Because of the calling convention we know that src is either a stack location\n-  \/\/ or an xmm register. dst can only be a stack location.\n-\n-  assert(dst.first()->is_stack() && ( src.first()->is_stack() || src.first()->is_XMMRegister()), \"bad parameters\");\n-\n-  if (src.first()->is_stack()) {\n-    __ movl(rax, Address(rbp, reg2offset_in(src.first())));\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n-  } else {\n-    \/\/ reg to stack\n-    __ movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n-  }\n-}\n-\n-\/\/ A long move\n-static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-\n-  \/\/ The only legal possibility for a long_move VMRegPair is:\n-  \/\/ 1: two stack slots (possibly unaligned)\n-  \/\/ as neither the java  or C calling convention will use registers\n-  \/\/ for longs.\n-\n-  if (src.first()->is_stack() && dst.first()->is_stack()) {\n-    assert(src.second()->is_stack() && dst.second()->is_stack(), \"must be all stack\");\n-    __ movptr(rax, Address(rbp, reg2offset_in(src.first())));\n-    __ movptr(rbx, Address(rbp, reg2offset_in(src.second())));\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n-    __ movptr(Address(rsp, reg2offset_out(dst.second())), rbx);\n-  } else {\n-    ShouldNotReachHere();\n-  }\n-}\n-\n-\/\/ A double move\n-static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-\n-  \/\/ The only legal possibilities for a double_move VMRegPair are:\n-  \/\/ The painful thing here is that like long_move a VMRegPair might be\n-\n-  \/\/ Because of the calling convention we know that src is either\n-  \/\/   1: a single physical register (xmm registers only)\n-  \/\/   2: two stack slots (possibly unaligned)\n-  \/\/ dst can only be a pair of stack slots.\n-\n-  assert(dst.first()->is_stack() && (src.first()->is_XMMRegister() || src.first()->is_stack()), \"bad args\");\n-\n-  if (src.first()->is_stack()) {\n-    \/\/ source is all stack\n-    __ movptr(rax, Address(rbp, reg2offset_in(src.first())));\n-    __ movptr(rbx, Address(rbp, reg2offset_in(src.second())));\n-    __ movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n-    __ movptr(Address(rsp, reg2offset_out(dst.second())), rbx);\n-  } else {\n-    \/\/ reg to stack\n-    \/\/ No worries about stack alignment\n-    __ movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n-  }\n-}\n-\n-\n-void SharedRuntime::save_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {\n-  \/\/ We always ignore the frame_slots arg and just use the space just below frame pointer\n-  \/\/ which by this time is free to use\n-  switch (ret_type) {\n-  case T_FLOAT:\n-    __ fstp_s(Address(rbp, -wordSize));\n-    break;\n-  case T_DOUBLE:\n-    __ fstp_d(Address(rbp, -2*wordSize));\n-    break;\n-  case T_VOID:  break;\n-  case T_LONG:\n-    __ movptr(Address(rbp, -wordSize), rax);\n-    __ movptr(Address(rbp, -2*wordSize), rdx);\n-    break;\n-  default: {\n-    __ movptr(Address(rbp, -wordSize), rax);\n-    }\n-  }\n-}\n-\n-void SharedRuntime::restore_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {\n-  \/\/ We always ignore the frame_slots arg and just use the space just below frame pointer\n-  \/\/ which by this time is free to use\n-  switch (ret_type) {\n-  case T_FLOAT:\n-    __ fld_s(Address(rbp, -wordSize));\n-    break;\n-  case T_DOUBLE:\n-    __ fld_d(Address(rbp, -2*wordSize));\n-    break;\n-  case T_LONG:\n-    __ movptr(rax, Address(rbp, -wordSize));\n-    __ movptr(rdx, Address(rbp, -2*wordSize));\n-    break;\n-  case T_VOID:  break;\n-  default: {\n-    __ movptr(rax, Address(rbp, -wordSize));\n-    }\n-  }\n-}\n-\n-static void verify_oop_args(MacroAssembler* masm,\n-                            const methodHandle& method,\n-                            const BasicType* sig_bt,\n-                            const VMRegPair* regs) {\n-  Register temp_reg = rbx;  \/\/ not part of any compiled calling seq\n-  if (VerifyOops) {\n-    for (int i = 0; i < method->size_of_parameters(); i++) {\n-      if (is_reference_type(sig_bt[i])) {\n-        VMReg r = regs[i].first();\n-        assert(r->is_valid(), \"bad oop arg\");\n-        if (r->is_stack()) {\n-          __ movptr(temp_reg, Address(rsp, r->reg2stack() * VMRegImpl::stack_slot_size + wordSize));\n-          __ verify_oop(temp_reg);\n-        } else {\n-          __ verify_oop(r->as_Register());\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-static void gen_special_dispatch(MacroAssembler* masm,\n-                                 const methodHandle& method,\n-                                 const BasicType* sig_bt,\n-                                 const VMRegPair* regs) {\n-  verify_oop_args(masm, method, sig_bt, regs);\n-  vmIntrinsics::ID iid = method->intrinsic_id();\n-\n-  \/\/ Now write the args into the outgoing interpreter space\n-  bool     has_receiver   = false;\n-  Register receiver_reg   = noreg;\n-  int      member_arg_pos = -1;\n-  Register member_reg     = noreg;\n-  int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);\n-  if (ref_kind != 0) {\n-    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing MemberName argument\n-    member_reg = rbx;  \/\/ known to be free at this point\n-    has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);\n-  } else if (iid == vmIntrinsics::_invokeBasic) {\n-    has_receiver = true;\n-  } else {\n-    fatal(\"unexpected intrinsic id %d\", vmIntrinsics::as_int(iid));\n-  }\n-\n-  if (member_reg != noreg) {\n-    \/\/ Load the member_arg into register, if necessary.\n-    SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);\n-    VMReg r = regs[member_arg_pos].first();\n-    if (r->is_stack()) {\n-      __ movptr(member_reg, Address(rsp, r->reg2stack() * VMRegImpl::stack_slot_size + wordSize));\n-    } else {\n-      \/\/ no data motion is needed\n-      member_reg = r->as_Register();\n-    }\n-  }\n-\n-  if (has_receiver) {\n-    \/\/ Make sure the receiver is loaded into a register.\n-    assert(method->size_of_parameters() > 0, \"oob\");\n-    assert(sig_bt[0] == T_OBJECT, \"receiver argument must be an object\");\n-    VMReg r = regs[0].first();\n-    assert(r->is_valid(), \"bad receiver arg\");\n-    if (r->is_stack()) {\n-      \/\/ Porting note:  This assumes that compiled calling conventions always\n-      \/\/ pass the receiver oop in a register.  If this is not true on some\n-      \/\/ platform, pick a temp and load the receiver from stack.\n-      fatal(\"receiver always in a register\");\n-      receiver_reg = rcx;  \/\/ known to be free at this point\n-      __ movptr(receiver_reg, Address(rsp, r->reg2stack() * VMRegImpl::stack_slot_size + wordSize));\n-    } else {\n-      \/\/ no data motion is needed\n-      receiver_reg = r->as_Register();\n-    }\n-  }\n-\n-  \/\/ Figure out which address we are really jumping to:\n-  MethodHandles::generate_method_handle_dispatch(masm, iid,\n-                                                 receiver_reg, member_reg, \/*for_compiler_entry:*\/ true);\n-}\n-\n-\/\/ ---------------------------------------------------------------------------\n-\/\/ Generate a native wrapper for a given method.  The method takes arguments\n-\/\/ in the Java compiled code convention, marshals them to the native\n-\/\/ convention (handlizes oops, etc), transitions to native, makes the call,\n-\/\/ returns to java state (possibly blocking), unhandlizes any result and\n-\/\/ returns.\n-\/\/\n-\/\/ Critical native functions are a shorthand for the use of\n-\/\/ GetPrimtiveArrayCritical and disallow the use of any other JNI\n-\/\/ functions.  The wrapper is expected to unpack the arguments before\n-\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n-\/\/ since they cannot stop for GC.\n-\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n-\/\/ block and the check for pending exceptions it's impossible for them\n-\/\/ to be thrown.\n-\/\/\n-\/\/\n-nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,\n-                                                const methodHandle& method,\n-                                                int compile_id,\n-                                                BasicType* in_sig_bt,\n-                                                VMRegPair* in_regs,\n-                                                BasicType ret_type) {\n-  if (method->is_method_handle_intrinsic()) {\n-    vmIntrinsics::ID iid = method->intrinsic_id();\n-    intptr_t start = (intptr_t)__ pc();\n-    int vep_offset = ((intptr_t)__ pc()) - start;\n-    gen_special_dispatch(masm,\n-                         method,\n-                         in_sig_bt,\n-                         in_regs);\n-    int frame_complete = ((intptr_t)__ pc()) - start;  \/\/ not complete, period\n-    __ flush();\n-    int stack_slots = SharedRuntime::out_preserve_stack_slots();  \/\/ no out slots at all, actually\n-    return nmethod::new_native_nmethod(method,\n-                                       compile_id,\n-                                       masm->code(),\n-                                       vep_offset,\n-                                       frame_complete,\n-                                       stack_slots \/ VMRegImpl::slots_per_word,\n-                                       in_ByteSize(-1),\n-                                       in_ByteSize(-1),\n-                                       (OopMapSet*)nullptr);\n-  }\n-  address native_func = method->native_function();\n-  assert(native_func != nullptr, \"must have function\");\n-\n-  \/\/ An OopMap for lock (and class if static)\n-  OopMapSet *oop_maps = new OopMapSet();\n-\n-  \/\/ We have received a description of where all the java arg are located\n-  \/\/ on entry to the wrapper. We need to convert these args to where\n-  \/\/ the jni function will expect them. To figure out where they go\n-  \/\/ we convert the java signature to a C signature by inserting\n-  \/\/ the hidden arguments as arg[0] and possibly arg[1] (static method)\n-\n-  const int total_in_args = method->size_of_parameters();\n-  int  total_c_args       = total_in_args + (method->is_static() ? 2 : 1);\n-\n-  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);\n-  VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);\n-\n-  int argc = 0;\n-  out_sig_bt[argc++] = T_ADDRESS;\n-  if (method->is_static()) {\n-    out_sig_bt[argc++] = T_OBJECT;\n-  }\n-\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    out_sig_bt[argc++] = in_sig_bt[i];\n-  }\n-\n-  \/\/ Now figure out where the args must be stored and how much stack space\n-  \/\/ they require.\n-  int out_arg_slots;\n-  out_arg_slots = c_calling_convention(out_sig_bt, out_regs, total_c_args);\n-\n-  \/\/ Compute framesize for the wrapper.  We need to handlize all oops in\n-  \/\/ registers a max of 2 on x86.\n-\n-  \/\/ Calculate the total number of stack slots we will need.\n-\n-  \/\/ First count the abi requirement plus all of the outgoing args\n-  int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;\n-\n-  \/\/ Now the space for the inbound oop handle area\n-  int total_save_slots = 2 * VMRegImpl::slots_per_word; \/\/ 2 arguments passed in registers\n-\n-  int oop_handle_offset = stack_slots;\n-  stack_slots += total_save_slots;\n-\n-  \/\/ Now any space we need for handlizing a klass if static method\n-\n-  int klass_slot_offset = 0;\n-  int klass_offset = -1;\n-  int lock_slot_offset = 0;\n-  bool is_static = false;\n-\n-  if (method->is_static()) {\n-    klass_slot_offset = stack_slots;\n-    stack_slots += VMRegImpl::slots_per_word;\n-    klass_offset = klass_slot_offset * VMRegImpl::stack_slot_size;\n-    is_static = true;\n-  }\n-\n-  \/\/ Plus a lock if needed\n-\n-  if (method->is_synchronized()) {\n-    lock_slot_offset = stack_slots;\n-    stack_slots += VMRegImpl::slots_per_word;\n-  }\n-\n-  \/\/ Now a place (+2) to save return values or temp during shuffling\n-  \/\/ + 2 for return address (which we own) and saved rbp,\n-  stack_slots += 4;\n-\n-  \/\/ Ok The space we have allocated will look like:\n-  \/\/\n-  \/\/\n-  \/\/ FP-> |                     |\n-  \/\/      |---------------------|\n-  \/\/      | 2 slots for moves   |\n-  \/\/      |---------------------|\n-  \/\/      | lock box (if sync)  |\n-  \/\/      |---------------------| <- lock_slot_offset  (-lock_slot_rbp_offset)\n-  \/\/      | klass (if static)   |\n-  \/\/      |---------------------| <- klass_slot_offset\n-  \/\/      | oopHandle area      |\n-  \/\/      |---------------------| <- oop_handle_offset (a max of 2 registers)\n-  \/\/      | outbound memory     |\n-  \/\/      | based arguments     |\n-  \/\/      |                     |\n-  \/\/      |---------------------|\n-  \/\/      |                     |\n-  \/\/ SP-> | out_preserved_slots |\n-  \/\/\n-  \/\/\n-  \/\/ ****************************************************************************\n-  \/\/ WARNING - on Windows Java Natives use pascal calling convention and pop the\n-  \/\/ arguments off of the stack after the jni call. Before the call we can use\n-  \/\/ instructions that are SP relative. After the jni call we switch to FP\n-  \/\/ relative instructions instead of re-adjusting the stack on windows.\n-  \/\/ ****************************************************************************\n-\n-\n-  \/\/ Now compute actual number of stack words we need rounding to make\n-  \/\/ stack properly aligned.\n-  stack_slots = align_up(stack_slots, StackAlignmentInSlots);\n-\n-  int stack_size = stack_slots * VMRegImpl::stack_slot_size;\n-\n-  intptr_t start = (intptr_t)__ pc();\n-\n-  \/\/ First thing make an ic check to see if we should even be here\n-\n-  \/\/ We are free to use all registers as temps without saving them and\n-  \/\/ restoring them except rbp. rbp is the only callee save register\n-  \/\/ as far as the interpreter and the compiler(s) are concerned.\n-\n-\n-  const Register receiver = rcx;\n-  Label exception_pending;\n-\n-  __ verify_oop(receiver);\n-  \/\/ verified entry must be aligned for code patching.\n-  __ ic_check(8 \/* end_alignment *\/);\n-\n-  int vep_offset = ((intptr_t)__ pc()) - start;\n-\n-#ifdef COMPILER1\n-  \/\/ For Object.hashCode, System.identityHashCode try to pull hashCode from object header if available.\n-  if ((InlineObjectHash && method->intrinsic_id() == vmIntrinsics::_hashCode) || (method->intrinsic_id() == vmIntrinsics::_identityHashCode)) {\n-    inline_check_hashcode_from_object_header(masm, method, rcx \/*obj_reg*\/, rax \/*result*\/);\n-   }\n-#endif \/\/ COMPILER1\n-\n-  \/\/ The instruction at the verified entry point must be 5 bytes or longer\n-  \/\/ because it can be patched on the fly by make_non_entrant. The stack bang\n-  \/\/ instruction fits that requirement.\n-\n-  \/\/ Generate stack overflow check\n-  __ bang_stack_with_offset((int)StackOverflow::stack_shadow_zone_size());\n-\n-  \/\/ Generate a new frame for the wrapper.\n-  __ enter();\n-  \/\/ -2 because return address is already present and so is saved rbp\n-  __ subptr(rsp, stack_size - 2*wordSize);\n-\n-\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->nmethod_entry_barrier(masm, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n-\n-  \/\/ Frame is now completed as far as size and linkage.\n-  int frame_complete = ((intptr_t)__ pc()) - start;\n-\n-  \/\/ Calculate the difference between rsp and rbp,. We need to know it\n-  \/\/ after the native call because on windows Java Natives will pop\n-  \/\/ the arguments and it is painful to do rsp relative addressing\n-  \/\/ in a platform independent way. So after the call we switch to\n-  \/\/ rbp, relative addressing.\n-\n-  int fp_adjustment = stack_size - 2*wordSize;\n-\n-#ifdef COMPILER2\n-  \/\/ C2 may leave the stack dirty if not in SSE2+ mode\n-  if (UseSSE >= 2) {\n-    __ verify_FPU(0, \"c2i transition should have clean FPU stack\");\n-  } else {\n-    __ empty_FPU_stack();\n-  }\n-#endif \/* COMPILER2 *\/\n-\n-  \/\/ Compute the rbp, offset for any slots used after the jni call\n-\n-  int lock_slot_rbp_offset = (lock_slot_offset*VMRegImpl::stack_slot_size) - fp_adjustment;\n-\n-  \/\/ We use rdi as a thread pointer because it is callee save and\n-  \/\/ if we load it once it is usable thru the entire wrapper\n-  const Register thread = rdi;\n-\n-   \/\/ We use rsi as the oop handle for the receiver\/klass\n-   \/\/ It is callee save so it survives the call to native\n-\n-   const Register oop_handle_reg = rsi;\n-\n-   __ get_thread(thread);\n-\n-  \/\/\n-  \/\/ We immediately shuffle the arguments so that any vm call we have to\n-  \/\/ make from here on out (sync slow path, jvmti, etc.) we will have\n-  \/\/ captured the oops from our caller and have a valid oopMap for\n-  \/\/ them.\n-\n-  \/\/ -----------------\n-  \/\/ The Grand Shuffle\n-  \/\/\n-  \/\/ Natives require 1 or 2 extra arguments over the normal ones: the JNIEnv*\n-  \/\/ and, if static, the class mirror instead of a receiver.  This pretty much\n-  \/\/ guarantees that register layout will not match (and x86 doesn't use reg\n-  \/\/ parms though amd does).  Since the native abi doesn't use register args\n-  \/\/ and the java conventions does we don't have to worry about collisions.\n-  \/\/ All of our moved are reg->stack or stack->stack.\n-  \/\/ We ignore the extra arguments during the shuffle and handle them at the\n-  \/\/ last moment. The shuffle is described by the two calling convention\n-  \/\/ vectors we have in our possession. We simply walk the java vector to\n-  \/\/ get the source locations and the c vector to get the destinations.\n-\n-  int c_arg = method->is_static() ? 2 : 1;\n-\n-  \/\/ Record rsp-based slot for receiver on stack for non-static methods\n-  int receiver_offset = -1;\n-\n-  \/\/ This is a trick. We double the stack slots so we can claim\n-  \/\/ the oops in the caller's frame. Since we are sure to have\n-  \/\/ more args than the caller doubling is enough to make\n-  \/\/ sure we can capture all the incoming oop args from the\n-  \/\/ caller.\n-  \/\/\n-  OopMap* map = new OopMap(stack_slots * 2, 0 \/* arg_slots*\/);\n-\n-  \/\/ Mark location of rbp,\n-  \/\/ map->set_callee_saved(VMRegImpl::stack2reg( stack_slots - 2), stack_slots * 2, 0, rbp->as_VMReg());\n-\n-  \/\/ We know that we only have args in at most two integer registers (rcx, rdx). So rax, rbx\n-  \/\/ Are free to temporaries if we have to do  stack to steck moves.\n-  \/\/ All inbound args are referenced based on rbp, and all outbound args via rsp.\n-\n-  for (int i = 0; i < total_in_args ; i++, c_arg++ ) {\n-    switch (in_sig_bt[i]) {\n-      case T_ARRAY:\n-      case T_OBJECT:\n-        object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n-                    ((i == 0) && (!is_static)),\n-                    &receiver_offset);\n-        break;\n-      case T_VOID:\n-        break;\n-\n-      case T_FLOAT:\n-        float_move(masm, in_regs[i], out_regs[c_arg]);\n-          break;\n-\n-      case T_DOUBLE:\n-        assert( i + 1 < total_in_args &&\n-                in_sig_bt[i + 1] == T_VOID &&\n-                out_sig_bt[c_arg+1] == T_VOID, \"bad arg list\");\n-        double_move(masm, in_regs[i], out_regs[c_arg]);\n-        break;\n-\n-      case T_LONG :\n-        long_move(masm, in_regs[i], out_regs[c_arg]);\n-        break;\n-\n-      case T_ADDRESS: assert(false, \"found T_ADDRESS in java args\");\n-\n-      default:\n-        simple_move32(masm, in_regs[i], out_regs[c_arg]);\n-    }\n-  }\n-\n-  \/\/ Pre-load a static method's oop into rsi.  Used both by locking code and\n-  \/\/ the normal JNI call code.\n-  if (method->is_static()) {\n-\n-    \/\/  load opp into a register\n-    __ movoop(oop_handle_reg, JNIHandles::make_local(method->method_holder()->java_mirror()));\n-\n-    \/\/ Now handlize the static class mirror it's known not-null.\n-    __ movptr(Address(rsp, klass_offset), oop_handle_reg);\n-    map->set_oop(VMRegImpl::stack2reg(klass_slot_offset));\n-\n-    \/\/ Now get the handle\n-    __ lea(oop_handle_reg, Address(rsp, klass_offset));\n-    \/\/ store the klass handle as second argument\n-    __ movptr(Address(rsp, wordSize), oop_handle_reg);\n-  }\n-\n-  \/\/ Change state to native (we save the return address in the thread, since it might not\n-  \/\/ be pushed on the stack when we do a stack traversal). It is enough that the pc()\n-  \/\/ points into the right code segment. It does not have to be the correct return pc.\n-  \/\/ We use the same pc\/oopMap repeatedly when we call out\n-\n-  intptr_t the_pc = (intptr_t) __ pc();\n-  oop_maps->add_gc_map(the_pc - start, map);\n-\n-  __ set_last_Java_frame(thread, rsp, noreg, (address)the_pc, noreg);\n-\n-\n-  \/\/ We have all of the arguments setup at this point. We must not touch any register\n-  \/\/ argument registers at this point (what if we save\/restore them there are no oop?\n-\n-  if (DTraceMethodProbes) {\n-    __ mov_metadata(rax, method());\n-    __ call_VM_leaf(\n-         CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry),\n-         thread, rax);\n-  }\n-\n-  \/\/ RedefineClasses() tracing support for obsolete method entry\n-  if (log_is_enabled(Trace, redefine, class, obsolete)) {\n-    __ mov_metadata(rax, method());\n-    __ call_VM_leaf(\n-         CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry),\n-         thread, rax);\n-  }\n-\n-  \/\/ These are register definitions we need for locking\/unlocking\n-  const Register swap_reg = rax;  \/\/ Must use rax, for cmpxchg instruction\n-  const Register obj_reg  = rcx;  \/\/ Will contain the oop\n-  const Register lock_reg = rdx;  \/\/ Address of compiler lock object (BasicLock)\n-\n-  Label slow_path_lock;\n-  Label lock_done;\n-\n-  \/\/ Lock a synchronized method\n-  if (method->is_synchronized()) {\n-    Label count_mon;\n-\n-    const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();\n-\n-    \/\/ Get the handle (the 2nd argument)\n-    __ movptr(oop_handle_reg, Address(rsp, wordSize));\n-\n-    \/\/ Get address of the box\n-\n-    __ lea(lock_reg, Address(rbp, lock_slot_rbp_offset));\n-\n-    \/\/ Load the oop from the handle\n-    __ movptr(obj_reg, Address(oop_handle_reg, 0));\n-\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(slow_path_lock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/ Load immediate 1 into swap_reg %rax,\n-      __ movptr(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n-      __ jmp(slow_path_lock);\n-    }\n-    __ bind(count_mon);\n-    __ inc_held_monitor_count();\n-\n-    \/\/ Slow path will re-enter here\n-    __ bind(lock_done);\n-  }\n-\n-\n-  \/\/ Finally just about ready to make the JNI call\n-\n-  \/\/ get JNIEnv* which is first argument to native\n-  __ lea(rdx, Address(thread, in_bytes(JavaThread::jni_environment_offset())));\n-  __ movptr(Address(rsp, 0), rdx);\n-\n-  \/\/ Now set thread in native\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native);\n-\n-  __ call(RuntimeAddress(native_func));\n-\n-  \/\/ Verify or restore cpu control state after JNI call\n-  __ restore_cpu_control_state_after_jni(noreg);\n-\n-  \/\/ WARNING - on Windows Java Natives use pascal calling convention and pop the\n-  \/\/ arguments off of the stack. We could just re-adjust the stack pointer here\n-  \/\/ and continue to do SP relative addressing but we instead switch to FP\n-  \/\/ relative addressing.\n-\n-  \/\/ Unpack native results.\n-  switch (ret_type) {\n-  case T_BOOLEAN: __ c2bool(rax);            break;\n-  case T_CHAR   : __ andptr(rax, 0xFFFF);    break;\n-  case T_BYTE   : __ sign_extend_byte (rax); break;\n-  case T_SHORT  : __ sign_extend_short(rax); break;\n-  case T_INT    : \/* nothing to do *\/        break;\n-  case T_DOUBLE :\n-  case T_FLOAT  :\n-    \/\/ Result is in st0 we'll save as needed\n-    break;\n-  case T_ARRAY:                 \/\/ Really a handle\n-  case T_OBJECT:                \/\/ Really a handle\n-      break; \/\/ can't de-handlize until after safepoint check\n-  case T_VOID: break;\n-  case T_LONG: break;\n-  default       : ShouldNotReachHere();\n-  }\n-\n-  \/\/ Switch thread to \"native transition\" state before reading the synchronization state.\n-  \/\/ This additional state is necessary because reading and testing the synchronization\n-  \/\/ state is not atomic w.r.t. GC, as this scenario demonstrates:\n-  \/\/     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.\n-  \/\/     VM thread changes sync state to synchronizing and suspends threads for GC.\n-  \/\/     Thread A is resumed to finish this native method, but doesn't block here since it\n-  \/\/     didn't see any synchronization is progress, and escapes.\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n-\n-  \/\/ Force this write out before the read below\n-  if (!UseSystemMemoryBarrier) {\n-    __ membar(Assembler::Membar_mask_bits(\n-              Assembler::LoadLoad | Assembler::LoadStore |\n-              Assembler::StoreLoad | Assembler::StoreStore));\n-  }\n-\n-  if (AlwaysRestoreFPU) {\n-    \/\/ Make sure the control word is correct.\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-  }\n-\n-  \/\/ check for safepoint operation in progress and\/or pending suspend requests\n-  { Label Continue, slow_path;\n-\n-    __ safepoint_poll(slow_path, thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n-\n-    __ cmpl(Address(thread, JavaThread::suspend_flags_offset()), 0);\n-    __ jcc(Assembler::equal, Continue);\n-    __ bind(slow_path);\n-\n-    \/\/ Don't use call_VM as it will see a possible pending exception and forward it\n-    \/\/ and never return here preventing us from clearing _last_native_pc down below.\n-    \/\/ Also can't use call_VM_leaf either as it will check to see if rsi & rdi are\n-    \/\/ preserved and correspond to the bcp\/locals pointers. So we do a runtime call\n-    \/\/ by hand.\n-    \/\/\n-    __ vzeroupper();\n-\n-    save_native_result(masm, ret_type, stack_slots);\n-    __ push(thread);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n-                                              JavaThread::check_special_condition_for_native_trans)));\n-    __ increment(rsp, wordSize);\n-    \/\/ Restore any method result value\n-    restore_native_result(masm, ret_type, stack_slots);\n-    __ bind(Continue);\n-  }\n-\n-  \/\/ change thread state\n-  __ movl(Address(thread, JavaThread::thread_state_offset()), _thread_in_Java);\n-\n-  Label reguard;\n-  Label reguard_done;\n-  __ cmpl(Address(thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ jcc(Assembler::equal, reguard);\n-\n-  \/\/ slow path reguard  re-enters here\n-  __ bind(reguard_done);\n-\n-  \/\/ Handle possible exception (will unlock if necessary)\n-\n-  \/\/ native result if any is live\n-\n-  \/\/ Unlock\n-  Label slow_path_unlock;\n-  Label unlock_done;\n-  if (method->is_synchronized()) {\n-\n-    Label fast_done;\n-\n-    \/\/ Get locked oop from the handle we passed to jni\n-    __ movptr(obj_reg, Address(oop_handle_reg, 0));\n-\n-    if (LockingMode == LM_LEGACY) {\n-      Label not_recur;\n-      \/\/ Simple recursive lock?\n-      __ cmpptr(Address(rbp, lock_slot_rbp_offset), NULL_WORD);\n-      __ jcc(Assembler::notEqual, not_recur);\n-      __ dec_held_monitor_count();\n-      __ jmpb(fast_done);\n-      __ bind(not_recur);\n-    }\n-\n-    \/\/ Must save rax, if it is live now because cmpxchg must use it\n-    if (ret_type != T_FLOAT && ret_type != T_DOUBLE && ret_type != T_VOID) {\n-      save_native_result(masm, ret_type, stack_slots);\n-    }\n-\n-    if (LockingMode == LM_MONITOR) {\n-      __ jmp(slow_path_unlock);\n-    } else if (LockingMode == LM_LEGACY) {\n-      \/\/  get old displaced header\n-      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n-      __ dec_held_monitor_count();\n-    } else {\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-      __ lightweight_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n-      __ dec_held_monitor_count();\n-    }\n-\n-    \/\/ slow path re-enters here\n-    __ bind(unlock_done);\n-    if (ret_type != T_FLOAT && ret_type != T_DOUBLE && ret_type != T_VOID) {\n-      restore_native_result(masm, ret_type, stack_slots);\n-    }\n-\n-    __ bind(fast_done);\n-  }\n-\n-  if (DTraceMethodProbes) {\n-    \/\/ Tell dtrace about this method exit\n-    save_native_result(masm, ret_type, stack_slots);\n-    __ mov_metadata(rax, method());\n-    __ call_VM_leaf(\n-         CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),\n-         thread, rax);\n-    restore_native_result(masm, ret_type, stack_slots);\n-  }\n-\n-  \/\/ We can finally stop using that last_Java_frame we setup ages ago\n-\n-  __ reset_last_Java_frame(thread, false);\n-\n-  \/\/ Unbox oop result, e.g. JNIHandles::resolve value.\n-  if (is_reference_type(ret_type)) {\n-    __ resolve_jobject(rax \/* value *\/,\n-                       thread \/* thread *\/,\n-                       rcx \/* tmp *\/);\n-  }\n-\n-  if (CheckJNICalls) {\n-    \/\/ clear_pending_jni_exception_check\n-    __ movptr(Address(thread, JavaThread::pending_jni_exception_check_fn_offset()), NULL_WORD);\n-  }\n-\n-  \/\/ reset handle block\n-  __ movptr(rcx, Address(thread, JavaThread::active_handles_offset()));\n-  __ movl(Address(rcx, JNIHandleBlock::top_offset()), NULL_WORD);\n-\n-  \/\/ Any exception pending?\n-  __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), NULL_WORD);\n-  __ jcc(Assembler::notEqual, exception_pending);\n-\n-  \/\/ no exception, we're almost done\n-\n-  \/\/ check that only result value is on FPU stack\n-  __ verify_FPU(ret_type == T_FLOAT || ret_type == T_DOUBLE ? 1 : 0, \"native_wrapper normal exit\");\n-\n-  \/\/ Fixup floating pointer results so that result looks like a return from a compiled method\n-  if (ret_type == T_FLOAT) {\n-    if (UseSSE >= 1) {\n-      \/\/ Pop st0 and store as float and reload into xmm register\n-      __ fstp_s(Address(rbp, -4));\n-      __ movflt(xmm0, Address(rbp, -4));\n-    }\n-  } else if (ret_type == T_DOUBLE) {\n-    if (UseSSE >= 2) {\n-      \/\/ Pop st0 and store as double and reload into xmm register\n-      __ fstp_d(Address(rbp, -8));\n-      __ movdbl(xmm0, Address(rbp, -8));\n-    }\n-  }\n-\n-  \/\/ Return\n-\n-  __ leave();\n-  __ ret(0);\n-\n-  \/\/ Unexpected paths are out of line and go here\n-\n-  \/\/ Slow path locking & unlocking\n-  if (method->is_synchronized()) {\n-\n-    \/\/ BEGIN Slow path lock\n-\n-    __ bind(slow_path_lock);\n-\n-    \/\/ has last_Java_frame setup. No exceptions so do vanilla call not call_VM\n-    \/\/ args are (oop obj, BasicLock* lock, JavaThread* thread)\n-    __ push(thread);\n-    __ push(lock_reg);\n-    __ push(obj_reg);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C)));\n-    __ addptr(rsp, 3*wordSize);\n-\n-#ifdef ASSERT\n-    { Label L;\n-    __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), NULL_WORD);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"no pending exception allowed on exit from monitorenter\");\n-    __ bind(L);\n-    }\n-#endif\n-    __ jmp(lock_done);\n-\n-    \/\/ END Slow path lock\n-\n-    \/\/ BEGIN Slow path unlock\n-    __ bind(slow_path_unlock);\n-    __ vzeroupper();\n-    \/\/ Slow path unlock\n-\n-    if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {\n-      save_native_result(masm, ret_type, stack_slots);\n-    }\n-    \/\/ Save pending exception around call to VM (which contains an EXCEPTION_MARK)\n-\n-    __ pushptr(Address(thread, in_bytes(Thread::pending_exception_offset())));\n-    __ movptr(Address(thread, in_bytes(Thread::pending_exception_offset())), NULL_WORD);\n-\n-\n-    \/\/ should be a peal\n-    \/\/ +wordSize because of the push above\n-    \/\/ args are (oop obj, BasicLock* lock, JavaThread* thread)\n-    __ push(thread);\n-    __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-    __ push(rax);\n-\n-    __ push(obj_reg);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C)));\n-    __ addptr(rsp, 3*wordSize);\n-#ifdef ASSERT\n-    {\n-      Label L;\n-      __ cmpptr(Address(thread, in_bytes(Thread::pending_exception_offset())), NULL_WORD);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"no pending exception allowed on exit complete_monitor_unlocking_C\");\n-      __ bind(L);\n-    }\n-#endif \/* ASSERT *\/\n-\n-    __ popptr(Address(thread, in_bytes(Thread::pending_exception_offset())));\n-\n-    if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {\n-      restore_native_result(masm, ret_type, stack_slots);\n-    }\n-    __ jmp(unlock_done);\n-    \/\/ END Slow path unlock\n-\n-  }\n-\n-  \/\/ SLOW PATH Reguard the stack if needed\n-\n-  __ bind(reguard);\n-  __ vzeroupper();\n-  save_native_result(masm, ret_type, stack_slots);\n-  {\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n-  }\n-  restore_native_result(masm, ret_type, stack_slots);\n-  __ jmp(reguard_done);\n-\n-\n-  \/\/ BEGIN EXCEPTION PROCESSING\n-\n-  \/\/ Forward  the exception\n-  __ bind(exception_pending);\n-\n-  \/\/ remove possible return value from FPU register stack\n-  __ empty_FPU_stack();\n-\n-  \/\/ pop our frame\n-  __ leave();\n-  \/\/ and forward the exception\n-  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n-  __ flush();\n-\n-  nmethod *nm = nmethod::new_native_nmethod(method,\n-                                            compile_id,\n-                                            masm->code(),\n-                                            vep_offset,\n-                                            frame_complete,\n-                                            stack_slots \/ VMRegImpl::slots_per_word,\n-                                            (is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),\n-                                            in_ByteSize(lock_slot_offset*VMRegImpl::stack_slot_size),\n-                                            oop_maps);\n-\n-  return nm;\n-\n-}\n-\n-\/\/ this function returns the adjust size (in number of words) to a c2i adapter\n-\/\/ activation for use during deoptimization\n-int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals ) {\n-  return (callee_locals - callee_parameters) * Interpreter::stackElementWords;\n-}\n-\n-\n-\/\/ Number of stack slots between incoming argument block and the start of\n-\/\/ a new frame.  The PROLOG must add this many slots to the stack.  The\n-\/\/ EPILOG must remove this many slots.  Intel needs one slot for\n-\/\/ return address and one for rbp, (must save rbp)\n-uint SharedRuntime::in_preserve_stack_slots() {\n-  return 2+VerifyStackAtCalls;\n-}\n-\n-uint SharedRuntime::out_preserve_stack_slots() {\n-  return 0;\n-}\n-\n-VMReg SharedRuntime::thread_register() {\n-  Unimplemented();\n-  return nullptr;\n-}\n-\n-\/\/------------------------------generate_deopt_blob----------------------------\n-void SharedRuntime::generate_deopt_blob() {\n-  \/\/ allocate space for the code\n-  ResourceMark rm;\n-  \/\/ setup code generation tools\n-  \/\/ note: the buffer code size must account for StackShadowPages=50\n-  const char* name = SharedRuntime::stub_name(SharedStubId::deopt_id);\n-  CodeBuffer   buffer(name, 1536, 1024);\n-  MacroAssembler* masm = new MacroAssembler(&buffer);\n-  int frame_size_in_words;\n-  OopMap* map = nullptr;\n-  \/\/ Account for the extra args we place on the stack\n-  \/\/ by the time we call fetch_unroll_info\n-  const int additional_words = 2; \/\/ deopt kind, thread\n-\n-  OopMapSet *oop_maps = new OopMapSet();\n-\n-  \/\/ -------------\n-  \/\/ This code enters when returning to a de-optimized nmethod.  A return\n-  \/\/ address has been pushed on the stack, and return values are in\n-  \/\/ registers.\n-  \/\/ If we are doing a normal deopt then we were called from the patched\n-  \/\/ nmethod from the point we returned to the nmethod. So the return\n-  \/\/ address on the stack is wrong by NativeCall::instruction_size\n-  \/\/ We will adjust the value to it looks like we have the original return\n-  \/\/ address on the stack (like when we eagerly deoptimized).\n-  \/\/ In the case of an exception pending with deoptimized then we enter\n-  \/\/ with a return address on the stack that points after the call we patched\n-  \/\/ into the exception handler. We have the following register state:\n-  \/\/    rax,: exception\n-  \/\/    rbx,: exception handler\n-  \/\/    rdx: throwing pc\n-  \/\/ So in this case we simply jam rdx into the useless return address and\n-  \/\/ the stack looks just like we want.\n-  \/\/\n-  \/\/ At this point we need to de-opt.  We save the argument return\n-  \/\/ registers.  We call the first C routine, fetch_unroll_info().  This\n-  \/\/ routine captures the return values and returns a structure which\n-  \/\/ describes the current frame size and the sizes of all replacement frames.\n-  \/\/ The current frame is compiled code and may contain many inlined\n-  \/\/ functions, each with their own JVM state.  We pop the current frame, then\n-  \/\/ push all the new frames.  Then we call the C routine unpack_frames() to\n-  \/\/ populate these frames.  Finally unpack_frames() returns us the new target\n-  \/\/ address.  Notice that callee-save registers are BLOWN here; they have\n-  \/\/ already been captured in the vframeArray at the time the return PC was\n-  \/\/ patched.\n-  address start = __ pc();\n-  Label cont;\n-\n-  \/\/ Prolog for non exception case!\n-\n-  \/\/ Save everything in sight.\n-\n-  map = RegisterSaver::save_live_registers(masm, additional_words, &frame_size_in_words, false);\n-  \/\/ Normal deoptimization\n-  __ push(Deoptimization::Unpack_deopt);\n-  __ jmp(cont);\n-\n-  int reexecute_offset = __ pc() - start;\n-\n-  \/\/ Reexecute case\n-  \/\/ return address is the pc describes what bci to do re-execute at\n-\n-  \/\/ No need to update map as each call to save_live_registers will produce identical oopmap\n-  (void) RegisterSaver::save_live_registers(masm, additional_words, &frame_size_in_words, false);\n-\n-  __ push(Deoptimization::Unpack_reexecute);\n-  __ jmp(cont);\n-\n-  int exception_offset = __ pc() - start;\n-\n-  \/\/ Prolog for exception case\n-\n-  \/\/ all registers are dead at this entry point, except for rax, and\n-  \/\/ rdx which contain the exception oop and exception pc\n-  \/\/ respectively.  Set them in TLS and fall thru to the\n-  \/\/ unpack_with_exception_in_tls entry point.\n-\n-  __ get_thread(rdi);\n-  __ movptr(Address(rdi, JavaThread::exception_pc_offset()), rdx);\n-  __ movptr(Address(rdi, JavaThread::exception_oop_offset()), rax);\n-\n-  int exception_in_tls_offset = __ pc() - start;\n-\n-  \/\/ new implementation because exception oop is now passed in JavaThread\n-\n-  \/\/ Prolog for exception case\n-  \/\/ All registers must be preserved because they might be used by LinearScan\n-  \/\/ Exceptiop oop and throwing PC are passed in JavaThread\n-  \/\/ tos: stack at point of call to method that threw the exception (i.e. only\n-  \/\/ args are on the stack, no return address)\n-\n-  \/\/ make room on stack for the return address\n-  \/\/ It will be patched later with the throwing pc. The correct value is not\n-  \/\/ available now because loading it from memory would destroy registers.\n-  __ push(0);\n-\n-  \/\/ Save everything in sight.\n-\n-  \/\/ No need to update map as each call to save_live_registers will produce identical oopmap\n-  (void) RegisterSaver::save_live_registers(masm, additional_words, &frame_size_in_words, false);\n-\n-  \/\/ Now it is safe to overwrite any register\n-\n-  \/\/ store the correct deoptimization type\n-  __ push(Deoptimization::Unpack_exception);\n-\n-  \/\/ load throwing pc from JavaThread and patch it as the return address\n-  \/\/ of the current frame. Then clear the field in JavaThread\n-  __ get_thread(rdi);\n-  __ movptr(rdx, Address(rdi, JavaThread::exception_pc_offset()));\n-  __ movptr(Address(rbp, wordSize), rdx);\n-  __ movptr(Address(rdi, JavaThread::exception_pc_offset()), NULL_WORD);\n-\n-#ifdef ASSERT\n-  \/\/ verify that there is really an exception oop in JavaThread\n-  __ movptr(rax, Address(rdi, JavaThread::exception_oop_offset()));\n-  __ verify_oop(rax);\n-\n-  \/\/ verify that there is no pending exception\n-  Label no_pending_exception;\n-  __ movptr(rax, Address(rdi, Thread::pending_exception_offset()));\n-  __ testptr(rax, rax);\n-  __ jcc(Assembler::zero, no_pending_exception);\n-  __ stop(\"must not have pending exception here\");\n-  __ bind(no_pending_exception);\n-#endif\n-\n-  __ bind(cont);\n-\n-  \/\/ Compiled code leaves the floating point stack dirty, empty it.\n-  __ empty_FPU_stack();\n-\n-\n-  \/\/ Call C code.  Need thread and this frame, but NOT official VM entry\n-  \/\/ crud.  We cannot block on this call, no GC can happen.\n-  __ get_thread(rcx);\n-  __ push(rcx);\n-  \/\/ fetch_unroll_info needs to call last_java_frame()\n-  __ set_last_Java_frame(rcx, noreg, noreg, nullptr, noreg);\n-\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info)));\n-\n-  \/\/ Need to have an oopmap that tells fetch_unroll_info where to\n-  \/\/ find any register it might need.\n-\n-  oop_maps->add_gc_map( __ pc()-start, map);\n-\n-  \/\/ Discard args to fetch_unroll_info\n-  __ pop(rcx);\n-  __ pop(rcx);\n-\n-  __ get_thread(rcx);\n-  __ reset_last_Java_frame(rcx, false);\n-\n-  \/\/ Load UnrollBlock into EDI\n-  __ mov(rdi, rax);\n-\n-  \/\/ Move the unpack kind to a safe place in the UnrollBlock because\n-  \/\/ we are very short of registers\n-\n-  Address unpack_kind(rdi, Deoptimization::UnrollBlock::unpack_kind_offset());\n-  \/\/ retrieve the deopt kind from the UnrollBlock.\n-  __ movl(rax, unpack_kind);\n-\n-   Label noException;\n-  __ cmpl(rax, Deoptimization::Unpack_exception);   \/\/ Was exception pending?\n-  __ jcc(Assembler::notEqual, noException);\n-  __ movptr(rax, Address(rcx, JavaThread::exception_oop_offset()));\n-  __ movptr(rdx, Address(rcx, JavaThread::exception_pc_offset()));\n-  __ movptr(Address(rcx, JavaThread::exception_oop_offset()), NULL_WORD);\n-  __ movptr(Address(rcx, JavaThread::exception_pc_offset()), NULL_WORD);\n-\n-  __ verify_oop(rax);\n-\n-  \/\/ Overwrite the result registers with the exception results.\n-  __ movptr(Address(rsp, RegisterSaver::raxOffset()*wordSize), rax);\n-  __ movptr(Address(rsp, RegisterSaver::rdxOffset()*wordSize), rdx);\n-\n-  __ bind(noException);\n-\n-  \/\/ Stack is back to only having register save data on the stack.\n-  \/\/ Now restore the result registers. Everything else is either dead or captured\n-  \/\/ in the vframeArray.\n-\n-  RegisterSaver::restore_result_registers(masm);\n-\n-  \/\/ Non standard control word may be leaked out through a safepoint blob, and we can\n-  \/\/ deopt at a poll point with the non standard control word. However, we should make\n-  \/\/ sure the control word is correct after restore_result_registers.\n-  __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-\n-  \/\/ All of the register save area has been popped of the stack. Only the\n-  \/\/ return address remains.\n-\n-  \/\/ Pop all the frames we must move\/replace.\n-  \/\/\n-  \/\/ Frame picture (youngest to oldest)\n-  \/\/ 1: self-frame (no frame link)\n-  \/\/ 2: deopting frame  (no frame link)\n-  \/\/ 3: caller of deopting frame (could be compiled\/interpreted).\n-  \/\/\n-  \/\/ Note: by leaving the return address of self-frame on the stack\n-  \/\/ and using the size of frame 2 to adjust the stack\n-  \/\/ when we are done the return to frame 3 will still be on the stack.\n-\n-  \/\/ Pop deoptimized frame\n-  __ addptr(rsp, Address(rdi,Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n-\n-  \/\/ sp should be pointing at the return address to the caller (3)\n-\n-  \/\/ Pick up the initial fp we should save\n-  \/\/ restore rbp before stack bang because if stack overflow is thrown it needs to be pushed (and preserved)\n-  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset()));\n-\n-#ifdef ASSERT\n-  \/\/ Compilers generate code that bang the stack by as much as the\n-  \/\/ interpreter would need. So this stack banging should never\n-  \/\/ trigger a fault. Verify that it does not on non product builds.\n-  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n-  __ bang_stack_size(rbx, rcx);\n-#endif\n-\n-  \/\/ Load array of frame pcs into ECX\n-  __ movptr(rcx,Address(rdi,Deoptimization::UnrollBlock::frame_pcs_offset()));\n-\n-  __ pop(rsi); \/\/ trash the old pc\n-\n-  \/\/ Load array of frame sizes into ESI\n-  __ movptr(rsi,Address(rdi,Deoptimization::UnrollBlock::frame_sizes_offset()));\n-\n-  Address counter(rdi, Deoptimization::UnrollBlock::counter_temp_offset());\n-\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset()));\n-  __ movl(counter, rbx);\n-\n-  \/\/ Now adjust the caller's stack to make up for the extra locals\n-  \/\/ but record the original sp so that we can save it in the skeletal interpreter\n-  \/\/ frame and the stack walking of interpreter_sender will get the unextended sp\n-  \/\/ value and not the \"real\" sp value.\n-\n-  Address sp_temp(rdi, Deoptimization::UnrollBlock::sender_sp_temp_offset());\n-  __ movptr(sp_temp, rsp);\n-  __ movl2ptr(rbx, Address(rdi, Deoptimization::UnrollBlock::caller_adjustment_offset()));\n-  __ subptr(rsp, rbx);\n-\n-  \/\/ Push interpreter frames in a loop\n-  Label loop;\n-  __ bind(loop);\n-  __ movptr(rbx, Address(rsi, 0));      \/\/ Load frame size\n-  __ subptr(rbx, 2*wordSize);           \/\/ we'll push pc and rbp, by hand\n-  __ pushptr(Address(rcx, 0));          \/\/ save return address\n-  __ enter();                           \/\/ save old & set new rbp,\n-  __ subptr(rsp, rbx);                  \/\/ Prolog!\n-  __ movptr(rbx, sp_temp);              \/\/ sender's sp\n-  \/\/ This value is corrected by layout_activation_impl\n-  __ movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), NULL_WORD);\n-  __ movptr(Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize), rbx); \/\/ Make it walkable\n-  __ movptr(sp_temp, rsp);              \/\/ pass to next frame\n-  __ addptr(rsi, wordSize);             \/\/ Bump array pointer (sizes)\n-  __ addptr(rcx, wordSize);             \/\/ Bump array pointer (pcs)\n-  __ decrementl(counter);             \/\/ decrement counter\n-  __ jcc(Assembler::notZero, loop);\n-  __ pushptr(Address(rcx, 0));          \/\/ save final return address\n-\n-  \/\/ Re-push self-frame\n-  __ enter();                           \/\/ save old & set new rbp,\n-\n-  \/\/  Return address and rbp, are in place\n-  \/\/ We'll push additional args later. Just allocate a full sized\n-  \/\/ register save area\n-  __ subptr(rsp, (frame_size_in_words-additional_words - 2) * wordSize);\n-\n-  \/\/ Restore frame locals after moving the frame\n-  __ movptr(Address(rsp, RegisterSaver::raxOffset()*wordSize), rax);\n-  __ movptr(Address(rsp, RegisterSaver::rdxOffset()*wordSize), rdx);\n-  __ fstp_d(Address(rsp, RegisterSaver::fpResultOffset()*wordSize));   \/\/ Pop float stack and store in local\n-  if( UseSSE>=2 ) __ movdbl(Address(rsp, RegisterSaver::xmm0Offset()*wordSize), xmm0);\n-  if( UseSSE==1 ) __ movflt(Address(rsp, RegisterSaver::xmm0Offset()*wordSize), xmm0);\n-\n-  \/\/ Set up the args to unpack_frame\n-\n-  __ pushl(unpack_kind);                     \/\/ get the unpack_kind value\n-  __ get_thread(rcx);\n-  __ push(rcx);\n-\n-  \/\/ set last_Java_sp, last_Java_fp\n-  __ set_last_Java_frame(rcx, noreg, rbp, nullptr, noreg);\n-\n-  \/\/ Call C code.  Need thread but NOT official VM entry\n-  \/\/ crud.  We cannot block on this call, no GC can happen.  Call should\n-  \/\/ restore return values to their stack-slots with the new SP.\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)));\n-  \/\/ Set an oopmap for the call site\n-  oop_maps->add_gc_map( __ pc()-start, new OopMap( frame_size_in_words, 0 ));\n-\n-  \/\/ rax, contains the return result type\n-  __ push(rax);\n-\n-  __ get_thread(rcx);\n-  __ reset_last_Java_frame(rcx, false);\n-\n-  \/\/ Collect return values\n-  __ movptr(rax,Address(rsp, (RegisterSaver::raxOffset() + additional_words + 1)*wordSize));\n-  __ movptr(rdx,Address(rsp, (RegisterSaver::rdxOffset() + additional_words + 1)*wordSize));\n-\n-  \/\/ Clear floating point stack before returning to interpreter\n-  __ empty_FPU_stack();\n-\n-  \/\/ Check if we should push the float or double return value.\n-  Label results_done, yes_double_value;\n-  __ cmpl(Address(rsp, 0), T_DOUBLE);\n-  __ jcc (Assembler::zero, yes_double_value);\n-  __ cmpl(Address(rsp, 0), T_FLOAT);\n-  __ jcc (Assembler::notZero, results_done);\n-\n-  \/\/ return float value as expected by interpreter\n-  if( UseSSE>=1 ) __ movflt(xmm0, Address(rsp, (RegisterSaver::xmm0Offset() + additional_words + 1)*wordSize));\n-  else            __ fld_d(Address(rsp, (RegisterSaver::fpResultOffset() + additional_words + 1)*wordSize));\n-  __ jmp(results_done);\n-\n-  \/\/ return double value as expected by interpreter\n-  __ bind(yes_double_value);\n-  if( UseSSE>=2 ) __ movdbl(xmm0, Address(rsp, (RegisterSaver::xmm0Offset() + additional_words + 1)*wordSize));\n-  else            __ fld_d(Address(rsp, (RegisterSaver::fpResultOffset() + additional_words + 1)*wordSize));\n-\n-  __ bind(results_done);\n-\n-  \/\/ Pop self-frame.\n-  __ leave();                              \/\/ Epilog!\n-\n-  \/\/ Jump to interpreter\n-  __ ret(0);\n-\n-  \/\/ -------------\n-  \/\/ make sure all code is generated\n-  masm->flush();\n-\n-  _deopt_blob = DeoptimizationBlob::create( &buffer, oop_maps, 0, exception_offset, reexecute_offset, frame_size_in_words);\n-  _deopt_blob->set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);\n-}\n-\n-\/\/------------------------------generate_handler_blob------\n-\/\/\n-\/\/ Generate a special Compile2Runtime blob that saves all registers,\n-\/\/ setup oopmap, and calls safepoint code to stop the compiled code for\n-\/\/ a safepoint.\n-\/\/\n-SafepointBlob* SharedRuntime::generate_handler_blob(SharedStubId id, address call_ptr) {\n-\n-  \/\/ Account for thread arg in our frame\n-  const int additional_words = 1;\n-  int frame_size_in_words;\n-\n-  assert (StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n-  assert(is_polling_page_id(id), \"expected a polling page stub id\");\n-\n-  ResourceMark rm;\n-  OopMapSet *oop_maps = new OopMapSet();\n-  OopMap* map;\n-\n-  \/\/ allocate space for the code\n-  \/\/ setup code generation tools\n-  const char* name = SharedRuntime::stub_name(id);\n-  CodeBuffer   buffer(name, 2048, 1024);\n-  MacroAssembler* masm = new MacroAssembler(&buffer);\n-\n-  const Register java_thread = rdi; \/\/ callee-saved for VC++\n-  address start   = __ pc();\n-  address call_pc = nullptr;\n-  bool cause_return = (id == SharedStubId::polling_page_return_handler_id);\n-  bool save_vectors = (id == SharedStubId::polling_page_vectors_safepoint_handler_id);\n-\n-  \/\/ If cause_return is true we are at a poll_return and there is\n-  \/\/ the return address on the stack to the caller on the nmethod\n-  \/\/ that is safepoint. We can leave this return on the stack and\n-  \/\/ effectively complete the return and safepoint in the caller.\n-  \/\/ Otherwise we push space for a return address that the safepoint\n-  \/\/ handler will install later to make the stack walking sensible.\n-  if (!cause_return)\n-    __ push(rbx);  \/\/ Make room for return address (or push it again)\n-\n-  map = RegisterSaver::save_live_registers(masm, additional_words, &frame_size_in_words, false, save_vectors);\n-\n-  \/\/ The following is basically a call_VM. However, we need the precise\n-  \/\/ address of the call in order to generate an oopmap. Hence, we do all the\n-  \/\/ work ourselves.\n-\n-  \/\/ Push thread argument and setup last_Java_sp\n-  __ get_thread(java_thread);\n-  __ push(java_thread);\n-  __ set_last_Java_frame(java_thread, noreg, noreg, nullptr, noreg);\n-\n-  \/\/ if this was not a poll_return then we need to correct the return address now.\n-  if (!cause_return) {\n-    \/\/ Get the return pc saved by the signal handler and stash it in its appropriate place on the stack.\n-    \/\/ Additionally, rbx is a callee saved register and we can look at it later to determine\n-    \/\/ if someone changed the return address for us!\n-    __ movptr(rbx, Address(java_thread, JavaThread::saved_exception_pc_offset()));\n-    __ movptr(Address(rbp, wordSize), rbx);\n-  }\n-\n-  \/\/ do the call\n-  __ call(RuntimeAddress(call_ptr));\n-\n-  \/\/ Set an oopmap for the call site.  This oopmap will map all\n-  \/\/ oop-registers and debug-info registers as callee-saved.  This\n-  \/\/ will allow deoptimization at this safepoint to find all possible\n-  \/\/ debug-info recordings, as well as let GC find all oops.\n-\n-  oop_maps->add_gc_map( __ pc() - start, map);\n-\n-  \/\/ Discard arg\n-  __ pop(rcx);\n-\n-  Label noException;\n-\n-  \/\/ Clear last_Java_sp again\n-  __ get_thread(java_thread);\n-  __ reset_last_Java_frame(java_thread, false);\n-\n-  __ cmpptr(Address(java_thread, Thread::pending_exception_offset()), NULL_WORD);\n-  __ jcc(Assembler::equal, noException);\n-\n-  \/\/ Exception pending\n-  RegisterSaver::restore_live_registers(masm, save_vectors);\n-\n-  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n-  __ bind(noException);\n-\n-  Label no_adjust, bail, not_special;\n-  if (!cause_return) {\n-    \/\/ If our stashed return pc was modified by the runtime we avoid touching it\n-    __ cmpptr(rbx, Address(rbp, wordSize));\n-    __ jccb(Assembler::notEqual, no_adjust);\n-\n-    \/\/ Skip over the poll instruction.\n-    \/\/ See NativeInstruction::is_safepoint_poll()\n-    \/\/ Possible encodings:\n-    \/\/      85 00       test   %eax,(%rax)\n-    \/\/      85 01       test   %eax,(%rcx)\n-    \/\/      85 02       test   %eax,(%rdx)\n-    \/\/      85 03       test   %eax,(%rbx)\n-    \/\/      85 06       test   %eax,(%rsi)\n-    \/\/      85 07       test   %eax,(%rdi)\n-    \/\/\n-    \/\/      85 04 24    test   %eax,(%rsp)\n-    \/\/      85 45 00    test   %eax,0x0(%rbp)\n-\n-#ifdef ASSERT\n-    __ movptr(rax, rbx); \/\/ remember where 0x85 should be, for verification below\n-#endif\n-    \/\/ rsp\/rbp base encoding takes 3 bytes with the following register values:\n-    \/\/ rsp 0x04\n-    \/\/ rbp 0x05\n-    __ movzbl(rcx, Address(rbx, 1));\n-    __ andptr(rcx, 0x07); \/\/ looking for 0x04 .. 0x05\n-    __ subptr(rcx, 4);    \/\/ looking for 0x00 .. 0x01\n-    __ cmpptr(rcx, 1);\n-    __ jcc(Assembler::above, not_special);\n-    __ addptr(rbx, 1);\n-    __ bind(not_special);\n-#ifdef ASSERT\n-    \/\/ Verify the correct encoding of the poll we're about to skip.\n-    __ cmpb(Address(rax, 0), NativeTstRegMem::instruction_code_memXregl);\n-    __ jcc(Assembler::notEqual, bail);\n-    \/\/ Mask out the modrm bits\n-    __ testb(Address(rax, 1), NativeTstRegMem::modrm_mask);\n-    \/\/ rax encodes to 0, so if the bits are nonzero it's incorrect\n-    __ jcc(Assembler::notZero, bail);\n-#endif\n-    \/\/ Adjust return pc forward to step over the safepoint poll instruction\n-    __ addptr(rbx, 2);\n-    __ movptr(Address(rbp, wordSize), rbx);\n-  }\n-\n-  __ bind(no_adjust);\n-  \/\/ Normal exit, register restoring and exit\n-  RegisterSaver::restore_live_registers(masm, save_vectors);\n-\n-  __ ret(0);\n-\n-#ifdef ASSERT\n-  __ bind(bail);\n-  __ stop(\"Attempting to adjust pc to skip safepoint poll but the return point is not what we expected\");\n-#endif\n-\n-  \/\/ make sure all code is generated\n-  masm->flush();\n-\n-  \/\/ Fill-out other meta info\n-  return SafepointBlob::create(&buffer, oop_maps, frame_size_in_words);\n-}\n-\n-\/\/\n-\/\/ generate_resolve_blob - call resolution (static\/virtual\/opt-virtual\/ic-miss\n-\/\/\n-\/\/ Generate a stub that calls into vm to find out the proper destination\n-\/\/ of a java call. All the argument registers are live at this point\n-\/\/ but since this is generic code we don't know what they are and the caller\n-\/\/ must do any gc of the args.\n-\/\/\n-RuntimeStub* SharedRuntime::generate_resolve_blob(SharedStubId id, address destination) {\n-  assert (StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n-  assert(is_resolve_id(id), \"expected a resolve stub id\");\n-\n-  \/\/ allocate space for the code\n-  ResourceMark rm;\n-\n-  const char* name = SharedRuntime::stub_name(id);\n-  CodeBuffer buffer(name, 1000, 512);\n-  MacroAssembler* masm                = new MacroAssembler(&buffer);\n-\n-  int frame_size_words;\n-  enum frame_layout {\n-                thread_off,\n-                extra_words };\n-\n-  OopMapSet *oop_maps = new OopMapSet();\n-  OopMap* map = nullptr;\n-\n-  int start = __ offset();\n-\n-  map = RegisterSaver::save_live_registers(masm, extra_words, &frame_size_words);\n-\n-  int frame_complete = __ offset();\n-\n-  const Register thread = rdi;\n-  __ get_thread(rdi);\n-\n-  __ push(thread);\n-  __ set_last_Java_frame(thread, noreg, rbp, nullptr, noreg);\n-\n-  __ call(RuntimeAddress(destination));\n-\n-\n-  \/\/ Set an oopmap for the call site.\n-  \/\/ We need this not only for callee-saved registers, but also for volatile\n-  \/\/ registers that the compiler might be keeping live across a safepoint.\n-\n-  oop_maps->add_gc_map( __ offset() - start, map);\n-\n-  \/\/ rax, contains the address we are going to jump to assuming no exception got installed\n-\n-  __ addptr(rsp, wordSize);\n-\n-  \/\/ clear last_Java_sp\n-  __ reset_last_Java_frame(thread, true);\n-  \/\/ check for pending exceptions\n-  Label pending;\n-  __ cmpptr(Address(thread, Thread::pending_exception_offset()), NULL_WORD);\n-  __ jcc(Assembler::notEqual, pending);\n-\n-  \/\/ get the returned Method*\n-  __ get_vm_result_2(rbx, thread);\n-  __ movptr(Address(rsp, RegisterSaver::rbx_offset() * wordSize), rbx);\n-\n-  __ movptr(Address(rsp, RegisterSaver::rax_offset() * wordSize), rax);\n-\n-  RegisterSaver::restore_live_registers(masm);\n-\n-  \/\/ We are back to the original state on entry and ready to go.\n-\n-  __ jmp(rax);\n-\n-  \/\/ Pending exception after the safepoint\n-\n-  __ bind(pending);\n-\n-  RegisterSaver::restore_live_registers(masm);\n-\n-  \/\/ exception pending => remove activation and forward to exception handler\n-\n-  __ get_thread(thread);\n-  __ movptr(Address(thread, JavaThread::vm_result_offset()), NULL_WORD);\n-  __ movptr(rax, Address(thread, Thread::pending_exception_offset()));\n-  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n-  \/\/ -------------\n-  \/\/ make sure all code is generated\n-  masm->flush();\n-\n-  \/\/ return the  blob\n-  \/\/ frame_size_words or bytes??\n-  return RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_words, oop_maps, true);\n-}\n-\n-  \/\/------------------------------------------------------------------------------------------------------------------------\n-  \/\/ Continuation point for throwing of implicit exceptions that are not handled in\n-  \/\/ the current activation. Fabricates an exception oop and initiates normal\n-  \/\/ exception dispatching in this frame.\n-  \/\/\n-  \/\/ Previously the compiler (c2) allowed for callee save registers on Java calls.\n-  \/\/ This is no longer true after adapter frames were removed but could possibly\n-  \/\/ be brought back in the future if the interpreter code was reworked and it\n-  \/\/ was deemed worthwhile. The comment below was left to describe what must\n-  \/\/ happen here if callee saves were resurrected. As it stands now this stub\n-  \/\/ could actually be a vanilla BufferBlob and have now oopMap at all.\n-  \/\/ Since it doesn't make much difference we've chosen to leave it the\n-  \/\/ way it was in the callee save days and keep the comment.\n-\n-  \/\/ If we need to preserve callee-saved values we need a callee-saved oop map and\n-  \/\/ therefore have to make these stubs into RuntimeStubs rather than BufferBlobs.\n-  \/\/ If the compiler needs all registers to be preserved between the fault\n-  \/\/ point and the exception handler then it must assume responsibility for that in\n-  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n-  \/\/ continuation_for_implicit_division_by_zero_exception. All other implicit\n-  \/\/ exceptions (e.g., NullPointerException or AbstractMethodError on entry) are\n-  \/\/ either at call sites or otherwise assume that stack unwinding will be initiated,\n-  \/\/ so caller saved registers were assumed volatile in the compiler.\n-RuntimeStub* SharedRuntime::generate_throw_exception(SharedStubId id, address runtime_entry) {\n-  assert(is_throw_id(id), \"expected a throw stub id\");\n-\n-  const char* name = SharedRuntime::stub_name(id);\n-\n-  \/\/ Information about frame layout at time of blocking runtime call.\n-  \/\/ Note that we only have to preserve callee-saved registers since\n-  \/\/ the compilers are responsible for supplying a continuation point\n-  \/\/ if they expect all registers to be preserved.\n-  enum layout {\n-    thread_off,    \/\/ last_java_sp\n-    arg1_off,\n-    arg2_off,\n-    rbp_off,       \/\/ callee saved register\n-    ret_pc,\n-    framesize\n-  };\n-\n-  int insts_size = 256;\n-  int locs_size  = 32;\n-\n-  ResourceMark rm;\n-  const char* timer_msg = \"SharedRuntime generate_throw_exception\";\n-  TraceTime timer(timer_msg, TRACETIME_LOG(Info, startuptime));\n-\n-  CodeBuffer code(name, insts_size, locs_size);\n-  OopMapSet* oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = new MacroAssembler(&code);\n-\n-  address start = __ pc();\n-\n-  \/\/ This is an inlined and slightly modified version of call_VM\n-  \/\/ which has the ability to fetch the return PC out of\n-  \/\/ thread-local storage and also sets up last_Java_sp slightly\n-  \/\/ differently than the real call_VM\n-  Register java_thread = rbx;\n-  __ get_thread(java_thread);\n-\n-  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  \/\/ pc and rbp, already pushed\n-  __ subptr(rsp, (framesize-2) * wordSize); \/\/ prolog\n-\n-  \/\/ Frame is now completed as far as size and linkage.\n-\n-  int frame_complete = __ pc() - start;\n-\n-  \/\/ push java thread (becomes first argument of C function)\n-  __ movptr(Address(rsp, thread_off * wordSize), java_thread);\n-  \/\/ Set up last_Java_sp and last_Java_fp\n-  __ set_last_Java_frame(java_thread, rsp, rbp, nullptr, noreg);\n-\n-  \/\/ Call runtime\n-  BLOCK_COMMENT(\"call runtime_entry\");\n-  __ call(RuntimeAddress(runtime_entry));\n-  \/\/ Generate oop map\n-  OopMap* map =  new OopMap(framesize, 0);\n-  oop_maps->add_gc_map(__ pc() - start, map);\n-\n-  \/\/ restore the thread (cannot use the pushed argument since arguments\n-  \/\/ may be overwritten by C code generated by an optimizing compiler);\n-  \/\/ however can use the register value directly if it is callee saved.\n-  __ get_thread(java_thread);\n-\n-  __ reset_last_Java_frame(java_thread, true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-  \/\/ check for pending exceptions\n-#ifdef ASSERT\n-  Label L;\n-  __ cmpptr(Address(java_thread, Thread::pending_exception_offset()), NULL_WORD);\n-  __ jcc(Assembler::notEqual, L);\n-  __ should_not_reach_here();\n-  __ bind(L);\n-#endif \/* ASSERT *\/\n-  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n-\n-  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &code, frame_complete, framesize, oop_maps, false);\n-  return stub;\n-}\n-\n-#if INCLUDE_JFR\n-\n-static void jfr_prologue(address the_pc, MacroAssembler* masm) {\n-  Register java_thread = rdi;\n-  __ get_thread(java_thread);\n-  __ set_last_Java_frame(java_thread, rsp, rbp, the_pc, noreg);\n-  __ movptr(Address(rsp, 0), java_thread);\n-}\n-\n-\/\/ The handle is dereferenced through a load barrier.\n-static void jfr_epilogue(MacroAssembler* masm) {\n-  Register java_thread = rdi;\n-  __ get_thread(java_thread);\n-  __ reset_last_Java_frame(java_thread, true);\n-}\n-\n-\/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n-\/\/ It returns a jobject handle to the event writer.\n-\/\/ The handle is dereferenced and the return value is the event writer oop.\n-RuntimeStub* SharedRuntime::generate_jfr_write_checkpoint() {\n-  enum layout {\n-    FPUState_off         = 0,\n-    rbp_off              = FPUStateSizeInWords,\n-    rdi_off,\n-    rsi_off,\n-    rcx_off,\n-    rbx_off,\n-    saved_argument_off,\n-    saved_argument_off2, \/\/ 2nd half of double\n-    framesize\n-  };\n-\n-  int insts_size = 1024;\n-  int locs_size = 64;\n-  const char* name = SharedRuntime::stub_name(SharedStubId::jfr_write_checkpoint_id);\n-  CodeBuffer code(name, insts_size, locs_size);\n-  OopMapSet* oop_maps = new OopMapSet();\n-  MacroAssembler* masm = new MacroAssembler(&code);\n-\n-  address start = __ pc();\n-  __ enter();\n-  int frame_complete = __ pc() - start;\n-  address the_pc = __ pc();\n-  jfr_prologue(the_pc, masm);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n-  jfr_epilogue(masm);\n-  __ resolve_global_jobject(rax, rdi, rdx);\n-  __ leave();\n-  __ ret(0);\n-\n-  OopMap* map = new OopMap(framesize, 1); \/\/ rbp\n-  oop_maps->add_gc_map(the_pc - start, map);\n-\n-  RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub::new_runtime_stub(name, &code, frame_complete,\n-                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                  oop_maps, false);\n-  return stub;\n-}\n-\n-\/\/ For c2: call to return a leased buffer.\n-RuntimeStub* SharedRuntime::generate_jfr_return_lease() {\n-  enum layout {\n-    FPUState_off = 0,\n-    rbp_off = FPUStateSizeInWords,\n-    rdi_off,\n-    rsi_off,\n-    rcx_off,\n-    rbx_off,\n-    saved_argument_off,\n-    saved_argument_off2, \/\/ 2nd half of double\n-    framesize\n-  };\n-\n-  int insts_size = 1024;\n-  int locs_size = 64;\n-  const char* name = SharedRuntime::stub_name(SharedStubId::jfr_return_lease_id);\n-  CodeBuffer code(name, insts_size, locs_size);\n-  OopMapSet* oop_maps = new OopMapSet();\n-  MacroAssembler* masm = new MacroAssembler(&code);\n-\n-  address start = __ pc();\n-  __ enter();\n-  int frame_complete = __ pc() - start;\n-  address the_pc = __ pc();\n-  jfr_prologue(the_pc, masm);\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::return_lease), 1);\n-  jfr_epilogue(masm);\n-  __ leave();\n-  __ ret(0);\n-\n-  OopMap* map = new OopMap(framesize, 1); \/\/ rbp\n-  oop_maps->add_gc_map(the_pc - start, map);\n-\n-  RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub::new_runtime_stub(name, &code, frame_complete,\n-                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                  oop_maps, false);\n-  return stub;\n-}\n-\n-#endif \/\/ INCLUDE_JFR\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":0,"deletions":2855,"binary":false,"changes":2855,"status":"deleted"},{"patch":"@@ -1,4083 +0,0 @@\n-\/*\n- * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n-#include \"gc\/shared\/barrierSet.hpp\"\n-#include \"gc\/shared\/barrierSetAssembler.hpp\"\n-#include \"gc\/shared\/barrierSetNMethod.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"memory\/universe.hpp\"\n-#include \"nativeInst_x86.hpp\"\n-#include \"oops\/instanceOop.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/objArrayKlass.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"runtime\/stubCodeGenerator.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-#ifdef COMPILER2\n-#include \"opto\/runtime.hpp\"\n-#endif\n-\n-\/\/ Declaration and definition of StubGenerator (no .hpp file).\n-\/\/ For a more detailed description of the stub routine structure\n-\/\/ see the comment in stubRoutines.hpp\n-\n-#define __ _masm->\n-#define a__ ((Assembler*)_masm)->\n-\n-#ifdef PRODUCT\n-#define BLOCK_COMMENT(str) \/* nothing *\/\n-#else\n-#define BLOCK_COMMENT(str) __ block_comment(str)\n-#endif\n-\n-#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n-\n-const int MXCSR_MASK  = 0xFFC0;  \/\/ Mask out any pending exceptions\n-const int FPU_CNTRL_WRD_MASK = 0xFFFF;\n-\n-ATTRIBUTE_ALIGNED(16) static const uint32_t KEY_SHUFFLE_MASK[] = {\n-    0x00010203UL, 0x04050607UL, 0x08090A0BUL, 0x0C0D0E0FUL,\n-};\n-\n-ATTRIBUTE_ALIGNED(16) static const uint32_t COUNTER_SHUFFLE_MASK[] = {\n-    0x0C0D0E0FUL, 0x08090A0BUL, 0x04050607UL, 0x00010203UL,\n-};\n-\n-ATTRIBUTE_ALIGNED(16) static const uint32_t GHASH_BYTE_SWAP_MASK[] = {\n-    0x0C0D0E0FUL, 0x08090A0BUL, 0x04050607UL, 0x00010203UL,\n-};\n-\n-ATTRIBUTE_ALIGNED(16) static const uint32_t GHASH_LONG_SWAP_MASK[] = {\n-    0x0B0A0908UL, 0x0F0E0D0CUL, 0x03020100UL, 0x07060504UL,\n-};\n-\n-\/\/ -------------------------------------------------------------------------------------------------------------------------\n-\/\/ Stub Code definitions\n-\n-class StubGenerator: public StubCodeGenerator {\n- private:\n-\n-#ifdef PRODUCT\n-#define inc_counter_np(counter) ((void)0)\n-#else\n-  void inc_counter_np_(uint& counter) {\n-    __ incrementl(ExternalAddress((address)&counter));\n-  }\n-#define inc_counter_np(counter) \\\n-  BLOCK_COMMENT(\"inc_counter \" #counter); \\\n-  inc_counter_np_(counter);\n-#endif \/\/PRODUCT\n-\n-  void inc_copy_counter_np(BasicType t) {\n-#ifndef PRODUCT\n-    switch (t) {\n-    case T_BYTE:    inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); return;\n-    case T_SHORT:   inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); return;\n-    case T_INT:     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); return;\n-    case T_LONG:    inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); return;\n-    case T_OBJECT:  inc_counter_np(SharedRuntime::_oop_array_copy_ctr); return;\n-    default:        ShouldNotReachHere();\n-    }\n-#endif \/\/PRODUCT\n-  }\n-\n-  \/\/------------------------------------------------------------------------------------------------------------------------\n-  \/\/ Call stubs are used to call Java from C\n-  \/\/\n-  \/\/    [ return_from_Java     ] <--- rsp\n-  \/\/    [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -N [ argument word 1      ]\n-  \/\/ -7 [ Possible padding for stack alignment ]\n-  \/\/ -6 [ Possible padding for stack alignment ]\n-  \/\/ -5 [ Possible padding for stack alignment ]\n-  \/\/ -4 [ mxcsr save           ] <--- rsp_after_call\n-  \/\/ -3 [ saved rbx,            ]\n-  \/\/ -2 [ saved rsi            ]\n-  \/\/ -1 [ saved rdi            ]\n-  \/\/  0 [ saved rbp,            ] <--- rbp,\n-  \/\/  1 [ return address       ]\n-  \/\/  2 [ ptr. to call wrapper ]\n-  \/\/  3 [ result               ]\n-  \/\/  4 [ result_type          ]\n-  \/\/  5 [ method               ]\n-  \/\/  6 [ entry_point          ]\n-  \/\/  7 [ parameters           ]\n-  \/\/  8 [ parameter_size       ]\n-  \/\/  9 [ thread               ]\n-\n-\n-  address generate_call_stub(address& return_address) {\n-    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n-    address start = __ pc();\n-\n-    \/\/ stub code parameters \/ addresses\n-    assert(frame::entry_frame_call_wrapper_offset == 2, \"adjust this code\");\n-    bool  sse_save = false;\n-    const Address rsp_after_call(rbp, -4 * wordSize); \/\/ same as in generate_catch_exception()!\n-    const int     locals_count_in_bytes  (4*wordSize);\n-    const Address mxcsr_save    (rbp, -4 * wordSize);\n-    const Address saved_rbx     (rbp, -3 * wordSize);\n-    const Address saved_rsi     (rbp, -2 * wordSize);\n-    const Address saved_rdi     (rbp, -1 * wordSize);\n-    const Address result        (rbp,  3 * wordSize);\n-    const Address result_type   (rbp,  4 * wordSize);\n-    const Address method        (rbp,  5 * wordSize);\n-    const Address entry_point   (rbp,  6 * wordSize);\n-    const Address parameters    (rbp,  7 * wordSize);\n-    const Address parameter_size(rbp,  8 * wordSize);\n-    const Address thread        (rbp,  9 * wordSize); \/\/ same as in generate_catch_exception()!\n-    sse_save =  UseSSE > 0;\n-\n-    \/\/ stub code\n-    __ enter();\n-    __ movptr(rcx, parameter_size);              \/\/ parameter counter\n-    __ shlptr(rcx, Interpreter::logStackElementSize); \/\/ convert parameter count to bytes\n-    __ addptr(rcx, locals_count_in_bytes);       \/\/ reserve space for register saves\n-    __ subptr(rsp, rcx);\n-    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n-\n-    \/\/ save rdi, rsi, & rbx, according to C calling conventions\n-    __ movptr(saved_rdi, rdi);\n-    __ movptr(saved_rsi, rsi);\n-    __ movptr(saved_rbx, rbx);\n-\n-    \/\/ save and initialize %mxcsr\n-    if (sse_save) {\n-      Label skip_ldmx;\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ cmp32(rax, mxcsr_std);\n-      __ jcc(Assembler::equal, skip_ldmx);\n-      __ ldmxcsr(mxcsr_std);\n-      __ bind(skip_ldmx);\n-    }\n-\n-    \/\/ make sure the control word is correct.\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-\n-#ifdef ASSERT\n-    \/\/ make sure we have no pending exceptions\n-    { Label L;\n-      __ movptr(rcx, thread);\n-      __ cmpptr(Address(rcx, Thread::pending_exception_offset()), NULL_WORD);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n-      __ bind(L);\n-    }\n-#endif\n-\n-    \/\/ pass parameters if any\n-    BLOCK_COMMENT(\"pass parameters if any\");\n-    Label parameters_done;\n-    __ movl(rcx, parameter_size);  \/\/ parameter counter\n-    __ testl(rcx, rcx);\n-    __ jcc(Assembler::zero, parameters_done);\n-\n-    \/\/ parameter passing loop\n-\n-    Label loop;\n-    \/\/ Copy Java parameters in reverse order (receiver last)\n-    \/\/ Note that the argument order is inverted in the process\n-    \/\/ source is rdx[rcx: N-1..0]\n-    \/\/ dest   is rsp[rbx: 0..N-1]\n-\n-    __ movptr(rdx, parameters);          \/\/ parameter pointer\n-    __ xorptr(rbx, rbx);\n-\n-    __ BIND(loop);\n-\n-    \/\/ get parameter\n-    __ movptr(rax, Address(rdx, rcx, Interpreter::stackElementScale(), -wordSize));\n-    __ movptr(Address(rsp, rbx, Interpreter::stackElementScale(),\n-                    Interpreter::expr_offset_in_bytes(0)), rax);          \/\/ store parameter\n-    __ increment(rbx);\n-    __ decrement(rcx);\n-    __ jcc(Assembler::notZero, loop);\n-\n-    \/\/ call Java function\n-    __ BIND(parameters_done);\n-    __ movptr(rbx, method);           \/\/ get Method*\n-    __ movptr(rax, entry_point);      \/\/ get entry_point\n-    __ mov(rsi, rsp);                 \/\/ set sender sp\n-    BLOCK_COMMENT(\"call Java function\");\n-    __ call(rax);\n-\n-    BLOCK_COMMENT(\"call_stub_return_address:\");\n-    return_address = __ pc();\n-\n-#ifdef COMPILER2\n-    {\n-      Label L_skip;\n-      if (UseSSE >= 2) {\n-        __ verify_FPU(0, \"call_stub_return\");\n-      } else {\n-        for (int i = 1; i < 8; i++) {\n-          __ ffree(i);\n-        }\n-\n-        \/\/ UseSSE <= 1 so double result should be left on TOS\n-        __ movl(rsi, result_type);\n-        __ cmpl(rsi, T_DOUBLE);\n-        __ jcc(Assembler::equal, L_skip);\n-        if (UseSSE == 0) {\n-          \/\/ UseSSE == 0 so float result should be left on TOS\n-          __ cmpl(rsi, T_FLOAT);\n-          __ jcc(Assembler::equal, L_skip);\n-        }\n-        __ ffree(0);\n-      }\n-      __ BIND(L_skip);\n-    }\n-#endif \/\/ COMPILER2\n-\n-    \/\/ store result depending on type\n-    \/\/ (everything that is not T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(rdi, result);\n-    Label is_long, is_float, is_double, exit;\n-    __ movl(rsi, result_type);\n-    __ cmpl(rsi, T_LONG);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(rsi, T_FLOAT);\n-    __ jcc(Assembler::equal, is_float);\n-    __ cmpl(rsi, T_DOUBLE);\n-    __ jcc(Assembler::equal, is_double);\n-\n-    \/\/ handle T_INT case\n-    __ movl(Address(rdi, 0), rax);\n-    __ BIND(exit);\n-\n-    \/\/ check that FPU stack is empty\n-    __ verify_FPU(0, \"generate_call_stub\");\n-\n-    \/\/ pop parameters\n-    __ lea(rsp, rsp_after_call);\n-\n-    \/\/ restore %mxcsr\n-    if (sse_save) {\n-      __ ldmxcsr(mxcsr_save);\n-    }\n-\n-    \/\/ restore rdi, rsi and rbx,\n-    __ movptr(rbx, saved_rbx);\n-    __ movptr(rsi, saved_rsi);\n-    __ movptr(rdi, saved_rdi);\n-    __ addptr(rsp, 4*wordSize);\n-\n-    \/\/ return\n-    __ pop(rbp);\n-    __ ret(0);\n-\n-    \/\/ handle return types different from T_INT\n-    __ BIND(is_long);\n-    __ movl(Address(rdi, 0 * wordSize), rax);\n-    __ movl(Address(rdi, 1 * wordSize), rdx);\n-    __ jmp(exit);\n-\n-    __ BIND(is_float);\n-    \/\/ interpreter uses xmm0 for return values\n-    if (UseSSE >= 1) {\n-      __ movflt(Address(rdi, 0), xmm0);\n-    } else {\n-      __ fstp_s(Address(rdi, 0));\n-    }\n-    __ jmp(exit);\n-\n-    __ BIND(is_double);\n-    \/\/ interpreter uses xmm0 for return values\n-    if (UseSSE >= 2) {\n-      __ movdbl(Address(rdi, 0), xmm0);\n-    } else {\n-      __ fstp_d(Address(rdi, 0));\n-    }\n-    __ jmp(exit);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/------------------------------------------------------------------------------------------------------------------------\n-  \/\/ Return point for a Java call if there's an exception thrown in Java code.\n-  \/\/ The exception is caught and transformed into a pending exception stored in\n-  \/\/ JavaThread that can be tested from within the VM.\n-  \/\/\n-  \/\/ Note: Usually the parameters are removed by the callee. In case of an exception\n-  \/\/       crossing an activation frame boundary, that is not the case if the callee\n-  \/\/       is compiled code => need to setup the rsp.\n-  \/\/\n-  \/\/ rax,: exception oop\n-\n-  address generate_catch_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n-    const Address rsp_after_call(rbp, -4 * wordSize); \/\/ same as in generate_call_stub()!\n-    const Address thread        (rbp,  9 * wordSize); \/\/ same as in generate_call_stub()!\n-    address start = __ pc();\n-\n-    \/\/ get thread directly\n-    __ movptr(rcx, thread);\n-#ifdef ASSERT\n-    \/\/ verify that threads correspond\n-    { Label L;\n-      __ get_thread(rbx);\n-      __ cmpptr(rbx, rcx);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n-      __ bind(L);\n-    }\n-#endif\n-    \/\/ set pending exception\n-    __ verify_oop(rax);\n-    __ movptr(Address(rcx, Thread::pending_exception_offset()), rax);\n-    __ lea(Address(rcx, Thread::exception_file_offset()),\n-           ExternalAddress((address)__FILE__), noreg);\n-    __ movl(Address(rcx, Thread::exception_line_offset()), __LINE__ );\n-    \/\/ complete return to VM\n-    assert(StubRoutines::_call_stub_return_address != nullptr, \"_call_stub_return_address must have been generated before\");\n-    __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n-\n-    return start;\n-  }\n-\n-\n-  \/\/------------------------------------------------------------------------------------------------------------------------\n-  \/\/ Continuation point for runtime calls returning with a pending exception.\n-  \/\/ The pending exception check happened in the runtime or native call stub.\n-  \/\/ The pending exception in Thread is converted into a Java-level exception.\n-  \/\/\n-  \/\/ Contract with Java-level exception handlers:\n-  \/\/ rax: exception\n-  \/\/ rdx: throwing pc\n-  \/\/\n-  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n-\n-  address generate_forward_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n-    address start = __ pc();\n-    const Register thread = rcx;\n-\n-    \/\/ other registers used in this stub\n-    const Register exception_oop = rax;\n-    const Register handler_addr  = rbx;\n-    const Register exception_pc  = rdx;\n-\n-    \/\/ Upon entry, the sp points to the return address returning into Java\n-    \/\/ (interpreted or compiled) code; i.e., the return address becomes the\n-    \/\/ throwing pc.\n-    \/\/\n-    \/\/ Arguments pushed before the runtime call are still on the stack but\n-    \/\/ the exception handler will reset the stack pointer -> ignore them.\n-    \/\/ A potential result in registers can be ignored as well.\n-\n-#ifdef ASSERT\n-    \/\/ make sure this code is only executed if there is a pending exception\n-    { Label L;\n-      __ get_thread(thread);\n-      __ cmpptr(Address(thread, Thread::pending_exception_offset()), NULL_WORD);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n-      __ bind(L);\n-    }\n-#endif\n-\n-    \/\/ compute exception handler into rbx,\n-    __ get_thread(thread);\n-    __ movptr(exception_pc, Address(rsp, 0));\n-    BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), thread, exception_pc);\n-    __ mov(handler_addr, rax);\n-\n-    \/\/ setup rax & rdx, remove return address & clear pending exception\n-    __ get_thread(thread);\n-    __ pop(exception_pc);\n-    __ movptr(exception_oop, Address(thread, Thread::pending_exception_offset()));\n-    __ movptr(Address(thread, Thread::pending_exception_offset()), NULL_WORD);\n-\n-#ifdef ASSERT\n-    \/\/ make sure exception is set\n-    { Label L;\n-      __ testptr(exception_oop, exception_oop);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n-      __ bind(L);\n-    }\n-#endif\n-\n-    \/\/ Verify that there is really a valid exception in RAX.\n-    __ verify_oop(exception_oop);\n-\n-    \/\/ continue at exception handler (return address removed)\n-    \/\/ rax: exception\n-    \/\/ rbx: exception handler\n-    \/\/ rdx: throwing pc\n-    __ jmp(handler_addr);\n-\n-    return start;\n-  }\n-\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Support for void verify_mxcsr()\n-  \/\/\n-  \/\/ This routine is used with -Xcheck:jni to verify that native\n-  \/\/ JNI code does not return to Java code without restoring the\n-  \/\/ MXCSR register to our expected state.\n-\n-\n-  address generate_verify_mxcsr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n-    address start = __ pc();\n-\n-    const Address mxcsr_save(rsp, 0);\n-\n-    if (CheckJNICalls && UseSSE > 0 ) {\n-      Label ok_ret;\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ push(rax);\n-      __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);\n-      __ cmp32(rax, mxcsr_std);\n-      __ jcc(Assembler::equal, ok_ret);\n-\n-      __ warn(\"MXCSR changed by native JNI code.\");\n-\n-      __ ldmxcsr(mxcsr_std);\n-\n-      __ bind(ok_ret);\n-      __ addptr(rsp, wordSize);\n-      __ pop(rax);\n-    }\n-\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/---------------------------------------------------------------------------\n-  \/\/ Support for void verify_fpu_cntrl_wrd()\n-  \/\/\n-  \/\/ This routine is used with -Xcheck:jni to verify that native\n-  \/\/ JNI code does not return to Java code without restoring the\n-  \/\/ FP control word to our expected state.\n-\n-  address generate_verify_fpu_cntrl_wrd() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_spcw\");\n-    address start = __ pc();\n-\n-    const Address fpu_cntrl_wrd_save(rsp, 0);\n-\n-    if (CheckJNICalls) {\n-      Label ok_ret;\n-      __ push(rax);\n-      __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n-      __ fnstcw(fpu_cntrl_wrd_save);\n-      __ movl(rax, fpu_cntrl_wrd_save);\n-      __ andl(rax, FPU_CNTRL_WRD_MASK);\n-      ExternalAddress fpu_std(StubRoutines::x86::addr_fpu_cntrl_wrd_std());\n-      __ cmp32(rax, fpu_std);\n-      __ jcc(Assembler::equal, ok_ret);\n-\n-      __ warn(\"Floating point control word changed by native JNI code.\");\n-\n-      __ fldcw(fpu_std);\n-\n-      __ bind(ok_ret);\n-      __ addptr(rsp, wordSize);\n-      __ pop(rax);\n-    }\n-\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/---------------------------------------------------------------------------\n-  \/\/ Wrapper for slow-case handling of double-to-integer conversion\n-  \/\/ d2i or f2i fast case failed either because it is nan or because\n-  \/\/ of under\/overflow.\n-  \/\/ Input:  FPU TOS: float value\n-  \/\/ Output: rax, (rdx): integer (long) result\n-\n-  address generate_d2i_wrapper(BasicType t, address fcn) {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2i_wrapper\");\n-    address start = __ pc();\n-\n-  \/\/ Capture info about frame layout\n-  enum layout { FPUState_off         = 0,\n-                rbp_off              = FPUStateSizeInWords,\n-                rdi_off,\n-                rsi_off,\n-                rcx_off,\n-                rbx_off,\n-                saved_argument_off,\n-                saved_argument_off2, \/\/ 2nd half of double\n-                framesize\n-  };\n-\n-  assert(FPUStateSizeInWords == 27, \"update stack layout\");\n-\n-    \/\/ Save outgoing argument to stack across push_FPU_state()\n-    __ subptr(rsp, wordSize * 2);\n-    __ fstp_d(Address(rsp, 0));\n-\n-    \/\/ Save CPU & FPU state\n-    __ push(rbx);\n-    __ push(rcx);\n-    __ push(rsi);\n-    __ push(rdi);\n-    __ push(rbp);\n-    __ push_FPU_state();\n-\n-    \/\/ push_FPU_state() resets the FP top of stack\n-    \/\/ Load original double into FP top of stack\n-    __ fld_d(Address(rsp, saved_argument_off * wordSize));\n-    \/\/ Store double into stack as outgoing argument\n-    __ subptr(rsp, wordSize*2);\n-    __ fst_d(Address(rsp, 0));\n-\n-    \/\/ Prepare FPU for doing math in C-land\n-    __ empty_FPU_stack();\n-    \/\/ Call the C code to massage the double.  Result in EAX\n-    if (t == T_INT)\n-      { BLOCK_COMMENT(\"SharedRuntime::d2i\"); }\n-    else if (t == T_LONG)\n-      { BLOCK_COMMENT(\"SharedRuntime::d2l\"); }\n-    __ call_VM_leaf( fcn, 2 );\n-\n-    \/\/ Restore CPU & FPU state\n-    __ pop_FPU_state();\n-    __ pop(rbp);\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    __ pop(rcx);\n-    __ pop(rbx);\n-    __ addptr(rsp, wordSize * 2);\n-\n-    __ ret(0);\n-\n-    return start;\n-  }\n-  \/\/---------------------------------------------------------------------------------------------------\n-\n-  address generate_vector_mask(const char *stub_name, int32_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    for (int i = 0; i < 16; i++) {\n-      __ emit_data(mask, relocInfo::none, 0);\n-    }\n-\n-    return start;\n-  }\n-\n-  address generate_count_leading_zeros_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x02020304, relocInfo::none, 0);\n-    __ emit_data(0x01010101, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x02020304, relocInfo::none, 0);\n-    __ emit_data(0x01010101, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x02020304, relocInfo::none, 0);\n-    __ emit_data(0x01010101, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x02020304, relocInfo::none, 0);\n-    __ emit_data(0x01010101, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-\n-  address generate_popcount_avx_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x02010100, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x04030302, relocInfo::none, 0);\n-    __ emit_data(0x02010100, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x04030302, relocInfo::none, 0);\n-    __ emit_data(0x02010100, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x04030302, relocInfo::none, 0);\n-    __ emit_data(0x02010100, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x03020201, relocInfo::none, 0);\n-    __ emit_data(0x04030302, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-\n-  address generate_iota_indices(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    \/\/ B\n-    __ emit_data(0x03020100, relocInfo::none, 0);\n-    __ emit_data(0x07060504, relocInfo::none, 0);\n-    __ emit_data(0x0B0A0908, relocInfo::none, 0);\n-    __ emit_data(0x0F0E0D0C, relocInfo::none, 0);\n-    __ emit_data(0x13121110, relocInfo::none, 0);\n-    __ emit_data(0x17161514, relocInfo::none, 0);\n-    __ emit_data(0x1B1A1918, relocInfo::none, 0);\n-    __ emit_data(0x1F1E1D1C, relocInfo::none, 0);\n-    __ emit_data(0x23222120, relocInfo::none, 0);\n-    __ emit_data(0x27262524, relocInfo::none, 0);\n-    __ emit_data(0x2B2A2928, relocInfo::none, 0);\n-    __ emit_data(0x2F2E2D2C, relocInfo::none, 0);\n-    __ emit_data(0x33323130, relocInfo::none, 0);\n-    __ emit_data(0x37363534, relocInfo::none, 0);\n-    __ emit_data(0x3B3A3938, relocInfo::none, 0);\n-    __ emit_data(0x3F3E3D3C, relocInfo::none, 0);\n-\n-    \/\/ W\n-    __ emit_data(0x00010000, relocInfo::none, 0);\n-    __ emit_data(0x00030002, relocInfo::none, 0);\n-    __ emit_data(0x00050004, relocInfo::none, 0);\n-    __ emit_data(0x00070006, relocInfo::none, 0);\n-    __ emit_data(0x00090008, relocInfo::none, 0);\n-    __ emit_data(0x000B000A, relocInfo::none, 0);\n-    __ emit_data(0x000D000C, relocInfo::none, 0);\n-    __ emit_data(0x000F000E, relocInfo::none, 0);\n-    __ emit_data(0x00110010, relocInfo::none, 0);\n-    __ emit_data(0x00130012, relocInfo::none, 0);\n-    __ emit_data(0x00150014, relocInfo::none, 0);\n-    __ emit_data(0x00170016, relocInfo::none, 0);\n-    __ emit_data(0x00190018, relocInfo::none, 0);\n-    __ emit_data(0x001B001A, relocInfo::none, 0);\n-    __ emit_data(0x001D001C, relocInfo::none, 0);\n-    __ emit_data(0x001F001E, relocInfo::none, 0);\n-\n-    \/\/ D\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000001, relocInfo::none, 0);\n-    __ emit_data(0x00000002, relocInfo::none, 0);\n-    __ emit_data(0x00000003, relocInfo::none, 0);\n-    __ emit_data(0x00000004, relocInfo::none, 0);\n-    __ emit_data(0x00000005, relocInfo::none, 0);\n-    __ emit_data(0x00000006, relocInfo::none, 0);\n-    __ emit_data(0x00000007, relocInfo::none, 0);\n-    __ emit_data(0x00000008, relocInfo::none, 0);\n-    __ emit_data(0x00000009, relocInfo::none, 0);\n-    __ emit_data(0x0000000A, relocInfo::none, 0);\n-    __ emit_data(0x0000000B, relocInfo::none, 0);\n-    __ emit_data(0x0000000C, relocInfo::none, 0);\n-    __ emit_data(0x0000000D, relocInfo::none, 0);\n-    __ emit_data(0x0000000E, relocInfo::none, 0);\n-    __ emit_data(0x0000000F, relocInfo::none, 0);\n-\n-    \/\/ Q\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000001, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000002, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000003, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000004, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000005, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000006, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000007, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-\n-    \/\/ D - FP\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 0.0f\n-    __ emit_data(0x3F800000, relocInfo::none, 0); \/\/ 1.0f\n-    __ emit_data(0x40000000, relocInfo::none, 0); \/\/ 2.0f\n-    __ emit_data(0x40400000, relocInfo::none, 0); \/\/ 3.0f\n-    __ emit_data(0x40800000, relocInfo::none, 0); \/\/ 4.0f\n-    __ emit_data(0x40A00000, relocInfo::none, 0); \/\/ 5.0f\n-    __ emit_data(0x40C00000, relocInfo::none, 0); \/\/ 6.0f\n-    __ emit_data(0x40E00000, relocInfo::none, 0); \/\/ 7.0f\n-    __ emit_data(0x41000000, relocInfo::none, 0); \/\/ 8.0f\n-    __ emit_data(0x41100000, relocInfo::none, 0); \/\/ 9.0f\n-    __ emit_data(0x41200000, relocInfo::none, 0); \/\/ 10.0f\n-    __ emit_data(0x41300000, relocInfo::none, 0); \/\/ 11.0f\n-    __ emit_data(0x41400000, relocInfo::none, 0); \/\/ 12.0f\n-    __ emit_data(0x41500000, relocInfo::none, 0); \/\/ 13.0f\n-    __ emit_data(0x41600000, relocInfo::none, 0); \/\/ 14.0f\n-    __ emit_data(0x41700000, relocInfo::none, 0); \/\/ 15.0f\n-\n-    \/\/ Q - FP\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 0.0d\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 1.0d\n-    __ emit_data(0x3FF00000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 2.0d\n-    __ emit_data(0x40000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 3.0d\n-    __ emit_data(0x40080000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 4.0d\n-    __ emit_data(0x40100000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 5.0d\n-    __ emit_data(0x40140000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 6.0d\n-    __ emit_data(0x40180000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0); \/\/ 7.0d\n-    __ emit_data(0x401c0000, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_bit_lut(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x0C040800, relocInfo::none, 0);\n-    __ emit_data(0x0E060A02, relocInfo::none, 0);\n-    __ emit_data(0x0D050901, relocInfo::none, 0);\n-    __ emit_data(0x0F070B03, relocInfo::none, 0);\n-    __ emit_data(0x0C040800, relocInfo::none, 0);\n-    __ emit_data(0x0E060A02, relocInfo::none, 0);\n-    __ emit_data(0x0D050901, relocInfo::none, 0);\n-    __ emit_data(0x0F070B03, relocInfo::none, 0);\n-    __ emit_data(0x0C040800, relocInfo::none, 0);\n-    __ emit_data(0x0E060A02, relocInfo::none, 0);\n-    __ emit_data(0x0D050901, relocInfo::none, 0);\n-    __ emit_data(0x0F070B03, relocInfo::none, 0);\n-    __ emit_data(0x0C040800, relocInfo::none, 0);\n-    __ emit_data(0x0E060A02, relocInfo::none, 0);\n-    __ emit_data(0x0D050901, relocInfo::none, 0);\n-    __ emit_data(0x0F070B03, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x08090A0B, relocInfo::none, 0);\n-    __ emit_data(0x0C0D0E0F, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x02030001, relocInfo::none, 0);\n-    __ emit_data(0x06070405, relocInfo::none, 0);\n-    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n-    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n-    __ emit_data(0x02030001, relocInfo::none, 0);\n-    __ emit_data(0x06070405, relocInfo::none, 0);\n-    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n-    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n-    __ emit_data(0x02030001, relocInfo::none, 0);\n-    __ emit_data(0x06070405, relocInfo::none, 0);\n-    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n-    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n-    __ emit_data(0x02030001, relocInfo::none, 0);\n-    __ emit_data(0x06070405, relocInfo::none, 0);\n-    __ emit_data(0x0A0B0809, relocInfo::none, 0);\n-    __ emit_data(0x0E0F0C0D, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data(0x70707070, relocInfo::none, 0);\n-    __ emit_data(0x70707070, relocInfo::none, 0);\n-    __ emit_data(0x70707070, relocInfo::none, 0);\n-    __ emit_data(0x70707070, relocInfo::none, 0);\n-    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n-    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n-    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n-    __ emit_data(0xF0F0F0F0, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_vector_mask_long_double(const char *stub_name, int32_t maskhi, int32_t masklo) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    for (int i = 0; i < 8; i++) {\n-      __ emit_data(masklo, relocInfo::none, 0);\n-      __ emit_data(maskhi, relocInfo::none, 0);\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/----------------------------------------------------------------------------------------------------\n-\n-  address generate_vector_byte_perm_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data(0x00000001, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000003, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000005, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000007, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000002, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000004, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000006, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-\n-    return start;\n-  }\n-\n-  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n-                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n-                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n-                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n-                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n-    __ emit_data(val0, relocInfo::none, 0);\n-    __ emit_data(val1, relocInfo::none, 0);\n-    __ emit_data(val2, relocInfo::none, 0);\n-    __ emit_data(val3, relocInfo::none, 0);\n-    if (len >= Assembler::AVX_256bit) {\n-      __ emit_data(val4, relocInfo::none, 0);\n-      __ emit_data(val5, relocInfo::none, 0);\n-      __ emit_data(val6, relocInfo::none, 0);\n-      __ emit_data(val7, relocInfo::none, 0);\n-      if (len >= Assembler::AVX_512bit) {\n-        __ emit_data(val8, relocInfo::none, 0);\n-        __ emit_data(val9, relocInfo::none, 0);\n-        __ emit_data(val10, relocInfo::none, 0);\n-        __ emit_data(val11, relocInfo::none, 0);\n-        __ emit_data(val12, relocInfo::none, 0);\n-        __ emit_data(val13, relocInfo::none, 0);\n-        __ emit_data(val14, relocInfo::none, 0);\n-        __ emit_data(val15, relocInfo::none, 0);\n-      }\n-    }\n-\n-    return start;\n-  }\n-\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Non-destructive plausibility checks for oops\n-\n-  address generate_verify_oop() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n-    address start = __ pc();\n-\n-    \/\/ Incoming arguments on stack after saving rax,:\n-    \/\/\n-    \/\/ [tos    ]: saved rdx\n-    \/\/ [tos + 1]: saved EFLAGS\n-    \/\/ [tos + 2]: return address\n-    \/\/ [tos + 3]: char* error message\n-    \/\/ [tos + 4]: oop   object to verify\n-    \/\/ [tos + 5]: saved rax, - saved by caller and bashed\n-\n-    Label exit, error;\n-    __ pushf();\n-    __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));\n-    __ push(rdx);                                \/\/ save rdx\n-    \/\/ make sure object is 'reasonable'\n-    __ movptr(rax, Address(rsp, 4 * wordSize));    \/\/ get object\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, exit);               \/\/ if obj is null it is ok\n-\n-    \/\/ Check if the oop is in the right area of memory\n-    const int oop_mask = Universe::verify_oop_mask();\n-    const int oop_bits = Universe::verify_oop_bits();\n-    __ mov(rdx, rax);\n-    __ andptr(rdx, oop_mask);\n-    __ cmpptr(rdx, oop_bits);\n-    __ jcc(Assembler::notZero, error);\n-\n-    \/\/ make sure klass is 'reasonable', which is not zero.\n-    __ movptr(rax, Address(rax, oopDesc::klass_offset_in_bytes())); \/\/ get klass\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, error);              \/\/ if klass is null it is broken\n-\n-    \/\/ return if everything seems ok\n-    __ bind(exit);\n-    __ movptr(rax, Address(rsp, 5 * wordSize));  \/\/ get saved rax, back\n-    __ pop(rdx);                                 \/\/ restore rdx\n-    __ popf();                                   \/\/ restore EFLAGS\n-    __ ret(3 * wordSize);                        \/\/ pop arguments\n-\n-    \/\/ handle errors\n-    __ bind(error);\n-    __ movptr(rax, Address(rsp, 5 * wordSize));  \/\/ get saved rax, back\n-    __ pop(rdx);                                 \/\/ get saved rdx back\n-    __ popf();                                   \/\/ get saved EFLAGS off stack -- will be ignored\n-    __ pusha();                                  \/\/ push registers (eip = return address & msg are already pushed)\n-    BLOCK_COMMENT(\"call MacroAssembler::debug\");\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug32)));\n-    __ hlt();\n-    return start;\n-  }\n-\n-\n-  \/\/ Copy 64 bytes chunks\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   from        - source array address\n-  \/\/   to_from     - destination array address - from\n-  \/\/   qword_count - 8-bytes element count, negative\n-  \/\/\n-  void xmm_copy_forward(Register from, Register to_from, Register qword_count) {\n-    assert( UseSSE >= 2, \"supported cpu only\" );\n-    Label L_copy_64_bytes_loop, L_copy_64_bytes, L_copy_8_bytes, L_exit;\n-\n-    \/\/ Copy 64-byte chunks\n-    __ jmpb(L_copy_64_bytes);\n-    __ align(OptoLoopAlignment);\n-  __ BIND(L_copy_64_bytes_loop);\n-\n-    if (UseUnalignedLoadStores) {\n-      if (UseAVX > 2) {\n-        __ evmovdqul(xmm0, Address(from, 0), Assembler::AVX_512bit);\n-        __ evmovdqul(Address(from, to_from, Address::times_1, 0), xmm0, Assembler::AVX_512bit);\n-      } else if (UseAVX == 2) {\n-        __ vmovdqu(xmm0, Address(from,  0));\n-        __ vmovdqu(Address(from, to_from, Address::times_1,  0), xmm0);\n-        __ vmovdqu(xmm1, Address(from, 32));\n-        __ vmovdqu(Address(from, to_from, Address::times_1, 32), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(from, 0));\n-        __ movdqu(Address(from, to_from, Address::times_1, 0), xmm0);\n-        __ movdqu(xmm1, Address(from, 16));\n-        __ movdqu(Address(from, to_from, Address::times_1, 16), xmm1);\n-        __ movdqu(xmm2, Address(from, 32));\n-        __ movdqu(Address(from, to_from, Address::times_1, 32), xmm2);\n-        __ movdqu(xmm3, Address(from, 48));\n-        __ movdqu(Address(from, to_from, Address::times_1, 48), xmm3);\n-      }\n-    } else {\n-      __ movq(xmm0, Address(from, 0));\n-      __ movq(Address(from, to_from, Address::times_1, 0), xmm0);\n-      __ movq(xmm1, Address(from, 8));\n-      __ movq(Address(from, to_from, Address::times_1, 8), xmm1);\n-      __ movq(xmm2, Address(from, 16));\n-      __ movq(Address(from, to_from, Address::times_1, 16), xmm2);\n-      __ movq(xmm3, Address(from, 24));\n-      __ movq(Address(from, to_from, Address::times_1, 24), xmm3);\n-      __ movq(xmm4, Address(from, 32));\n-      __ movq(Address(from, to_from, Address::times_1, 32), xmm4);\n-      __ movq(xmm5, Address(from, 40));\n-      __ movq(Address(from, to_from, Address::times_1, 40), xmm5);\n-      __ movq(xmm6, Address(from, 48));\n-      __ movq(Address(from, to_from, Address::times_1, 48), xmm6);\n-      __ movq(xmm7, Address(from, 56));\n-      __ movq(Address(from, to_from, Address::times_1, 56), xmm7);\n-    }\n-\n-    __ addl(from, 64);\n-  __ BIND(L_copy_64_bytes);\n-    __ subl(qword_count, 8);\n-    __ jcc(Assembler::greaterEqual, L_copy_64_bytes_loop);\n-\n-    if (UseUnalignedLoadStores && (UseAVX == 2)) {\n-      \/\/ clean upper bits of YMM registers\n-      __ vpxor(xmm0, xmm0);\n-      __ vpxor(xmm1, xmm1);\n-    }\n-    __ addl(qword_count, 8);\n-    __ jccb(Assembler::zero, L_exit);\n-    \/\/\n-    \/\/ length is too short, just copy qwords\n-    \/\/\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(xmm0, Address(from, 0));\n-    __ movq(Address(from, to_from, Address::times_1), xmm0);\n-    __ addl(from, 8);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::greater, L_copy_8_bytes);\n-  __ BIND(L_exit);\n-  }\n-\n-  address generate_disjoint_copy(BasicType t, bool aligned,\n-                                 Address::ScaleFactor sf,\n-                                 address* entry, const char *name,\n-                                 bool dest_uninitialized = false) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_0_count, L_exit, L_skip_align1, L_skip_align2, L_copy_byte;\n-    Label L_copy_2_bytes, L_copy_4_bytes, L_copy_64_bytes;\n-\n-    int shift = Address::times_ptr - sf;\n-\n-    const Register from     = rsi;  \/\/ source array address\n-    const Register to       = rdi;  \/\/ destination array address\n-    const Register count    = rcx;  \/\/ elements count\n-    const Register to_from  = to;   \/\/ (to - from)\n-    const Register saved_to = rdx;  \/\/ saved destination array address\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ push(rsi);\n-    __ push(rdi);\n-    __ movptr(from , Address(rsp, 12+ 4));\n-    __ movptr(to   , Address(rsp, 12+ 8));\n-    __ movl(count, Address(rsp, 12+ 12));\n-\n-    if (entry != nullptr) {\n-      *entry = __ pc(); \/\/ Entry point from conjoint arraycopy stub.\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    if (t == T_OBJECT) {\n-      __ testl(count, count);\n-      __ jcc(Assembler::zero, L_0_count);\n-    }\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, t, from, to, count);\n-    {\n-      bool add_entry = (t != T_OBJECT && (!aligned || t == T_INT));\n-      \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n-      UnsafeMemoryAccessMark umam(this, add_entry, true);\n-      __ subptr(to, from); \/\/ to --> to_from\n-      __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-      __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n-      if (!UseUnalignedLoadStores && !aligned && (t == T_BYTE || t == T_SHORT)) {\n-        \/\/ align source address at 4 bytes address boundary\n-        if (t == T_BYTE) {\n-          \/\/ One byte misalignment happens only for byte arrays\n-          __ testl(from, 1);\n-          __ jccb(Assembler::zero, L_skip_align1);\n-          __ movb(rax, Address(from, 0));\n-          __ movb(Address(from, to_from, Address::times_1, 0), rax);\n-          __ increment(from);\n-          __ decrement(count);\n-        __ BIND(L_skip_align1);\n-        }\n-        \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n-        __ testl(from, 2);\n-        __ jccb(Assembler::zero, L_skip_align2);\n-        __ movw(rax, Address(from, 0));\n-        __ movw(Address(from, to_from, Address::times_1, 0), rax);\n-        __ addptr(from, 2);\n-        __ subl(count, 1<<(shift-1));\n-      __ BIND(L_skip_align2);\n-      }\n-      if (!UseXMMForArrayCopy) {\n-        __ mov(rax, count);      \/\/ save 'count'\n-        __ shrl(count, shift); \/\/ bytes count\n-        __ addptr(to_from, from);\/\/ restore 'to'\n-        __ rep_mov();\n-        __ subptr(to_from, from);\/\/ restore 'to_from'\n-        __ mov(count, rax);      \/\/ restore 'count'\n-        __ jmpb(L_copy_2_bytes); \/\/ all dwords were copied\n-      } else {\n-        if (!UseUnalignedLoadStores) {\n-          \/\/ align to 8 bytes, we know we are 4 byte aligned to start\n-          __ testptr(from, 4);\n-          __ jccb(Assembler::zero, L_copy_64_bytes);\n-          __ movl(rax, Address(from, 0));\n-          __ movl(Address(from, to_from, Address::times_1, 0), rax);\n-          __ addptr(from, 4);\n-          __ subl(count, 1<<shift);\n-        }\n-      __ BIND(L_copy_64_bytes);\n-        __ mov(rax, count);\n-        __ shrl(rax, shift+1);  \/\/ 8 bytes chunk count\n-        \/\/\n-        \/\/ Copy 8-byte chunks through XMM registers, 8 per iteration of the loop\n-        \/\/\n-        xmm_copy_forward(from, to_from, rax);\n-      }\n-      \/\/ copy tailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(count, 1<<shift);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(from, 0));\n-      __ movl(Address(from, to_from, Address::times_1, 0), rax);\n-      if (t == T_BYTE || t == T_SHORT) {\n-        __ addptr(from, 4);\n-      __ BIND(L_copy_2_bytes);\n-        \/\/ copy tailing word\n-        __ testl(count, 1<<(shift-1));\n-        __ jccb(Assembler::zero, L_copy_byte);\n-        __ movw(rax, Address(from, 0));\n-        __ movw(Address(from, to_from, Address::times_1, 0), rax);\n-        if (t == T_BYTE) {\n-          __ addptr(from, 2);\n-        __ BIND(L_copy_byte);\n-          \/\/ copy tailing byte\n-          __ testl(count, 1);\n-          __ jccb(Assembler::zero, L_exit);\n-          __ movb(rax, Address(from, 0));\n-          __ movb(Address(from, to_from, Address::times_1, 0), rax);\n-        __ BIND(L_exit);\n-        } else {\n-        __ BIND(L_copy_byte);\n-        }\n-      } else {\n-      __ BIND(L_copy_2_bytes);\n-      }\n-    }\n-\n-    __ movl(count, Address(rsp, 12+12)); \/\/ reread 'count'\n-    bs->arraycopy_epilogue(_masm, decorators, t, from, to, count);\n-\n-    if (t == T_OBJECT) {\n-    __ BIND(L_0_count);\n-    }\n-    inc_copy_counter_np(t);\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ ret(0);\n-    return start;\n-  }\n-\n-\n-  address generate_fill(BasicType t, bool aligned, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    const Register to       = rdi;  \/\/ source array address\n-    const Register value    = rdx;  \/\/ value\n-    const Register count    = rsi;  \/\/ elements count\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ push(rsi);\n-    __ push(rdi);\n-    __ movptr(to   , Address(rsp, 12+ 4));\n-    __ movl(value, Address(rsp, 12+ 8));\n-    __ movl(count, Address(rsp, 12+ 12));\n-\n-    __ generate_fill(t, aligned, to, value, count, rax, xmm0);\n-\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address generate_conjoint_copy(BasicType t, bool aligned,\n-                                 Address::ScaleFactor sf,\n-                                 address nooverlap_target,\n-                                 address* entry, const char *name,\n-                                 bool dest_uninitialized = false) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_0_count, L_exit, L_skip_align1, L_skip_align2, L_copy_byte;\n-    Label L_copy_2_bytes, L_copy_4_bytes, L_copy_8_bytes, L_copy_8_bytes_loop;\n-\n-    int shift = Address::times_ptr - sf;\n-\n-    const Register src   = rax;  \/\/ source array address\n-    const Register dst   = rdx;  \/\/ destination array address\n-    const Register from  = rsi;  \/\/ source array address\n-    const Register to    = rdi;  \/\/ destination array address\n-    const Register count = rcx;  \/\/ elements count\n-    const Register end   = rax;  \/\/ array end address\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ push(rsi);\n-    __ push(rdi);\n-    __ movptr(src  , Address(rsp, 12+ 4));   \/\/ from\n-    __ movptr(dst  , Address(rsp, 12+ 8));   \/\/ to\n-    __ movl2ptr(count, Address(rsp, 12+12)); \/\/ count\n-\n-    if (entry != nullptr) {\n-      *entry = __ pc(); \/\/ Entry point from generic arraycopy stub.\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    \/\/ nooverlap_target expects arguments in rsi and rdi.\n-    __ mov(from, src);\n-    __ mov(to  , dst);\n-\n-    \/\/ arrays overlap test: dispatch to disjoint stub if necessary.\n-    RuntimeAddress nooverlap(nooverlap_target);\n-    __ cmpptr(dst, src);\n-    __ lea(end, Address(src, count, sf, 0)); \/\/ src + count * elem_size\n-    __ jump_cc(Assembler::belowEqual, nooverlap);\n-    __ cmpptr(dst, end);\n-    __ jump_cc(Assembler::aboveEqual, nooverlap);\n-\n-    if (t == T_OBJECT) {\n-      __ testl(count, count);\n-      __ jcc(Assembler::zero, L_0_count);\n-    }\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, t, from, to, count);\n-\n-    {\n-      bool add_entry = (t != T_OBJECT && (!aligned || t == T_INT));\n-      \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n-      UnsafeMemoryAccessMark umam(this, add_entry, true);\n-      \/\/ copy from high to low\n-      __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-      __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n-      if (t == T_BYTE || t == T_SHORT) {\n-        \/\/ Align the end of destination array at 4 bytes address boundary\n-        __ lea(end, Address(dst, count, sf, 0));\n-        if (t == T_BYTE) {\n-          \/\/ One byte misalignment happens only for byte arrays\n-          __ testl(end, 1);\n-          __ jccb(Assembler::zero, L_skip_align1);\n-          __ decrement(count);\n-          __ movb(rdx, Address(from, count, sf, 0));\n-          __ movb(Address(to, count, sf, 0), rdx);\n-        __ BIND(L_skip_align1);\n-        }\n-        \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n-        __ testl(end, 2);\n-        __ jccb(Assembler::zero, L_skip_align2);\n-        __ subptr(count, 1<<(shift-1));\n-        __ movw(rdx, Address(from, count, sf, 0));\n-        __ movw(Address(to, count, sf, 0), rdx);\n-      __ BIND(L_skip_align2);\n-        __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-        __ jcc(Assembler::below, L_copy_4_bytes);\n-      }\n-\n-      if (!UseXMMForArrayCopy) {\n-        __ std();\n-        __ mov(rax, count); \/\/ Save 'count'\n-        __ mov(rdx, to);    \/\/ Save 'to'\n-        __ lea(rsi, Address(from, count, sf, -4));\n-        __ lea(rdi, Address(to  , count, sf, -4));\n-        __ shrptr(count, shift); \/\/ bytes count\n-        __ rep_mov();\n-        __ cld();\n-        __ mov(count, rax); \/\/ restore 'count'\n-        __ andl(count, (1<<shift)-1);      \/\/ mask the number of rest elements\n-        __ movptr(from, Address(rsp, 12+4)); \/\/ reread 'from'\n-        __ mov(to, rdx);   \/\/ restore 'to'\n-        __ jmpb(L_copy_2_bytes); \/\/ all dword were copied\n-      } else {\n-        \/\/ Align to 8 bytes the end of array. It is aligned to 4 bytes already.\n-        __ testptr(end, 4);\n-        __ jccb(Assembler::zero, L_copy_8_bytes);\n-        __ subl(count, 1<<shift);\n-        __ movl(rdx, Address(from, count, sf, 0));\n-        __ movl(Address(to, count, sf, 0), rdx);\n-        __ jmpb(L_copy_8_bytes);\n-\n-        __ align(OptoLoopAlignment);\n-        \/\/ Move 8 bytes\n-      __ BIND(L_copy_8_bytes_loop);\n-        __ movq(xmm0, Address(from, count, sf, 0));\n-        __ movq(Address(to, count, sf, 0), xmm0);\n-      __ BIND(L_copy_8_bytes);\n-        __ subl(count, 2<<shift);\n-        __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n-        __ addl(count, 2<<shift);\n-      }\n-    __ BIND(L_copy_4_bytes);\n-      \/\/ copy prefix qword\n-      __ testl(count, 1<<shift);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rdx, Address(from, count, sf, -4));\n-      __ movl(Address(to, count, sf, -4), rdx);\n-\n-      if (t == T_BYTE || t == T_SHORT) {\n-          __ subl(count, (1<<shift));\n-        __ BIND(L_copy_2_bytes);\n-          \/\/ copy prefix dword\n-          __ testl(count, 1<<(shift-1));\n-          __ jccb(Assembler::zero, L_copy_byte);\n-          __ movw(rdx, Address(from, count, sf, -2));\n-          __ movw(Address(to, count, sf, -2), rdx);\n-          if (t == T_BYTE) {\n-            __ subl(count, 1<<(shift-1));\n-          __ BIND(L_copy_byte);\n-            \/\/ copy prefix byte\n-            __ testl(count, 1);\n-            __ jccb(Assembler::zero, L_exit);\n-            __ movb(rdx, Address(from, 0));\n-            __ movb(Address(to, 0), rdx);\n-          __ BIND(L_exit);\n-          } else {\n-          __ BIND(L_copy_byte);\n-          }\n-      } else {\n-      __ BIND(L_copy_2_bytes);\n-      }\n-    }\n-\n-    __ movl2ptr(count, Address(rsp, 12+12)); \/\/ reread count\n-    bs->arraycopy_epilogue(_masm, decorators, t, from, to, count);\n-\n-    if (t == T_OBJECT) {\n-    __ BIND(L_0_count);\n-    }\n-    inc_copy_counter_np(t);\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ ret(0);\n-    return start;\n-  }\n-\n-\n-  address generate_disjoint_long_copy(address* entry, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_8_bytes, L_copy_8_bytes_loop;\n-    const Register from       = rax;  \/\/ source array address\n-    const Register to         = rdx;  \/\/ destination array address\n-    const Register count      = rcx;  \/\/ elements count\n-    const Register to_from    = rdx;  \/\/ (to - from)\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ movptr(from , Address(rsp, 8+0));       \/\/ from\n-    __ movptr(to   , Address(rsp, 8+4));       \/\/ to\n-    __ movl2ptr(count, Address(rsp, 8+8));     \/\/ count\n-\n-    *entry = __ pc(); \/\/ Entry point from conjoint arraycopy stub.\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n-      UnsafeMemoryAccessMark umam(this, true, true);\n-      __ subptr(to, from); \/\/ to --> to_from\n-      if (UseXMMForArrayCopy) {\n-        xmm_copy_forward(from, to_from, count);\n-      } else {\n-        __ jmpb(L_copy_8_bytes);\n-        __ align(OptoLoopAlignment);\n-      __ BIND(L_copy_8_bytes_loop);\n-        __ fild_d(Address(from, 0));\n-        __ fistp_d(Address(from, to_from, Address::times_1));\n-        __ addptr(from, 8);\n-      __ BIND(L_copy_8_bytes);\n-        __ decrement(count);\n-        __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n-      }\n-    }\n-    inc_copy_counter_np(T_LONG);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address generate_conjoint_long_copy(address nooverlap_target,\n-                                      address* entry, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_8_bytes, L_copy_8_bytes_loop;\n-    const Register from       = rax;  \/\/ source array address\n-    const Register to         = rdx;  \/\/ destination array address\n-    const Register count      = rcx;  \/\/ elements count\n-    const Register end_from   = rax;  \/\/ source array end address\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ movptr(from , Address(rsp, 8+0));       \/\/ from\n-    __ movptr(to   , Address(rsp, 8+4));       \/\/ to\n-    __ movl2ptr(count, Address(rsp, 8+8));     \/\/ count\n-\n-    *entry = __ pc(); \/\/ Entry point from generic arraycopy stub.\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    \/\/ arrays overlap test\n-    __ cmpptr(to, from);\n-    RuntimeAddress nooverlap(nooverlap_target);\n-    __ jump_cc(Assembler::belowEqual, nooverlap);\n-    __ lea(end_from, Address(from, count, Address::times_8, 0));\n-    __ cmpptr(to, end_from);\n-    __ movptr(from, Address(rsp, 8));  \/\/ from\n-    __ jump_cc(Assembler::aboveEqual, nooverlap);\n-\n-    {\n-      \/\/ UnsafeMemoryAccess page error: continue after unsafe access\n-      UnsafeMemoryAccessMark umam(this, true, true);\n-\n-      __ jmpb(L_copy_8_bytes);\n-\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_8_bytes_loop);\n-      if (UseXMMForArrayCopy) {\n-        __ movq(xmm0, Address(from, count, Address::times_8));\n-        __ movq(Address(to, count, Address::times_8), xmm0);\n-      } else {\n-        __ fild_d(Address(from, count, Address::times_8));\n-        __ fistp_d(Address(to, count, Address::times_8));\n-      }\n-    __ BIND(L_copy_8_bytes);\n-      __ decrement(count);\n-      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n-\n-    }\n-    inc_copy_counter_np(T_LONG);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ ret(0);\n-    return start;\n-  }\n-\n-\n-  \/\/ Helper for generating a dynamic type check.\n-  \/\/ The sub_klass must be one of {rbx, rdx, rsi}.\n-  \/\/ The temp is killed.\n-  void generate_type_check(Register sub_klass,\n-                           Address& super_check_offset_addr,\n-                           Address& super_klass_addr,\n-                           Register temp,\n-                           Label* L_success, Label* L_failure) {\n-    BLOCK_COMMENT(\"type_check:\");\n-\n-    Label L_fallthrough;\n-#define LOCAL_JCC(assembler_con, label_ptr)                             \\\n-    if (label_ptr != nullptr)  __ jcc(assembler_con, *(label_ptr));        \\\n-    else                    __ jcc(assembler_con, L_fallthrough) \/*omit semi*\/\n-\n-    \/\/ The following is a strange variation of the fast path which requires\n-    \/\/ one less register, because needed values are on the argument stack.\n-    \/\/ __ check_klass_subtype_fast_path(sub_klass, *super_klass*, temp,\n-    \/\/                                  L_success, L_failure, null);\n-    assert_different_registers(sub_klass, temp);\n-\n-    int sc_offset = in_bytes(Klass::secondary_super_cache_offset());\n-\n-    \/\/ if the pointers are equal, we are done (e.g., String[] elements)\n-    __ cmpptr(sub_klass, super_klass_addr);\n-    LOCAL_JCC(Assembler::equal, L_success);\n-\n-    \/\/ check the supertype display:\n-    __ movl2ptr(temp, super_check_offset_addr);\n-    Address super_check_addr(sub_klass, temp, Address::times_1, 0);\n-    __ movptr(temp, super_check_addr); \/\/ load displayed supertype\n-    __ cmpptr(temp, super_klass_addr); \/\/ test the super type\n-    LOCAL_JCC(Assembler::equal, L_success);\n-\n-    \/\/ if it was a primary super, we can just fail immediately\n-    __ cmpl(super_check_offset_addr, sc_offset);\n-    LOCAL_JCC(Assembler::notEqual, L_failure);\n-\n-    \/\/ The repne_scan instruction uses fixed registers, which will get spilled.\n-    \/\/ We happen to know this works best when super_klass is in rax.\n-    Register super_klass = temp;\n-    __ movptr(super_klass, super_klass_addr);\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg,\n-                                     L_success, L_failure);\n-\n-    __ bind(L_fallthrough);\n-\n-    if (L_success == nullptr) { BLOCK_COMMENT(\"L_success:\"); }\n-    if (L_failure == nullptr) { BLOCK_COMMENT(\"L_failure:\"); }\n-\n-#undef LOCAL_JCC\n-  }\n-\n-  \/\/\n-  \/\/  Generate checkcasting array copy stub\n-  \/\/\n-  \/\/  Input:\n-  \/\/    4(rsp)   - source array address\n-  \/\/    8(rsp)   - destination array address\n-  \/\/   12(rsp)   - element count, can be zero\n-  \/\/   16(rsp)   - size_t ckoff (super_check_offset)\n-  \/\/   20(rsp)   - oop ckval (super_klass)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax, ==  0  -  success\n-  \/\/    rax, == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_checkcast_copy(const char *name, address* entry, bool dest_uninitialized = false) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_load_element, L_store_element, L_do_card_marks, L_done;\n-\n-    \/\/ register use:\n-    \/\/  rax, rdx, rcx -- loop control (end_from, end_to, count)\n-    \/\/  rdi, rsi      -- element access (oop, klass)\n-    \/\/  rbx,           -- temp\n-    const Register from       = rax;    \/\/ source array address\n-    const Register to         = rdx;    \/\/ destination array address\n-    const Register length     = rcx;    \/\/ elements count\n-    const Register elem       = rdi;    \/\/ each oop copied\n-    const Register elem_klass = rsi;    \/\/ each elem._klass (sub_klass)\n-    const Register temp       = rbx;    \/\/ lone remaining temp\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    __ push(rsi);\n-    __ push(rdi);\n-    __ push(rbx);\n-\n-    Address   from_arg(rsp, 16+ 4);     \/\/ from\n-    Address     to_arg(rsp, 16+ 8);     \/\/ to\n-    Address length_arg(rsp, 16+12);     \/\/ elements count\n-    Address  ckoff_arg(rsp, 16+16);     \/\/ super_check_offset\n-    Address  ckval_arg(rsp, 16+20);     \/\/ super_klass\n-\n-    \/\/ Load up:\n-    __ movptr(from,     from_arg);\n-    __ movptr(to,         to_arg);\n-    __ movl2ptr(length, length_arg);\n-\n-    if (entry != nullptr) {\n-      *entry = __ pc(); \/\/ Entry point from generic arraycopy stub.\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    \/\/---------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the two arrays are subtypes of Object[] but the\n-    \/\/ destination array type is not equal to or a supertype\n-    \/\/ of the source type.  Each element must be separately\n-    \/\/ checked.\n-\n-    \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n-    Address end_from_addr(from, length, Address::times_ptr, 0);\n-    Address   end_to_addr(to,   length, Address::times_ptr, 0);\n-\n-    Register end_from = from;           \/\/ re-use\n-    Register end_to   = to;             \/\/ re-use\n-    Register count    = length;         \/\/ re-use\n-\n-    \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n-    Address from_element_addr(end_from, count, Address::times_ptr, 0);\n-    Address   to_element_addr(end_to,   count, Address::times_ptr, 0);\n-    Address elem_klass_addr(elem, oopDesc::klass_offset_in_bytes());\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-\n-    BasicType type = T_OBJECT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    \/\/ Copy from low to high addresses, indexed from the end of each array.\n-    __ lea(end_from, end_from_addr);\n-    __ lea(end_to,   end_to_addr);\n-    assert(length == count, \"\");        \/\/ else fix next line:\n-    __ negptr(count);                   \/\/ negate and test the length\n-    __ jccb(Assembler::notZero, L_load_element);\n-\n-    \/\/ Empty array:  Nothing to do.\n-    __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n-    __ jmp(L_done);\n-\n-    \/\/ ======== begin loop ========\n-    \/\/ (Loop is rotated; its entry is L_load_element.)\n-    \/\/ Loop control:\n-    \/\/   for (count = -count; count != 0; count++)\n-    \/\/ Base pointers src, dst are biased by 8*count,to last element.\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_store_element);\n-    __ movptr(to_element_addr, elem);     \/\/ store the oop\n-    __ increment(count);                \/\/ increment the count toward zero\n-    __ jccb(Assembler::zero, L_do_card_marks);\n-\n-    \/\/ ======== loop entry is here ========\n-    __ BIND(L_load_element);\n-    __ movptr(elem, from_element_addr);   \/\/ load the oop\n-    __ testptr(elem, elem);\n-    __ jccb(Assembler::zero, L_store_element);\n-\n-    \/\/ (Could do a trick here:  Remember last successful non-null\n-    \/\/ element stored and make a quick oop equality check on it.)\n-\n-    __ movptr(elem_klass, elem_klass_addr); \/\/ query the object klass\n-    generate_type_check(elem_klass, ckoff_arg, ckval_arg, temp,\n-                        &L_store_element, nullptr);\n-    \/\/ (On fall-through, we have failed the element type check.)\n-    \/\/ ======== end loop ========\n-\n-    \/\/ It was a real error; we must depend on the caller to finish the job.\n-    \/\/ Register \"count\" = -1 * number of *remaining* oops, length_arg = *total* oops.\n-    \/\/ Emit GC store barriers for the oops we have copied (length_arg + count),\n-    \/\/ and report their number to the caller.\n-    assert_different_registers(to, count, rax);\n-    Label L_post_barrier;\n-    __ addl(count, length_arg);         \/\/ transfers = (length - remaining)\n-    __ movl2ptr(rax, count);            \/\/ save the value\n-    __ notptr(rax);                     \/\/ report (-1^K) to caller (does not affect flags)\n-    __ jccb(Assembler::notZero, L_post_barrier);\n-    __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n-\n-    \/\/ Come here on success only.\n-    __ BIND(L_do_card_marks);\n-    __ xorptr(rax, rax);                \/\/ return 0 on success\n-    __ movl2ptr(count, length_arg);\n-\n-    __ BIND(L_post_barrier);\n-    __ movptr(to, to_arg);              \/\/ reload\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-\n-    \/\/ Common exit point (success or failure).\n-    __ BIND(L_done);\n-    __ pop(rbx);\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/\n-  \/\/  Generate 'unsafe' array copy stub\n-  \/\/  Though just as safe as the other stubs, it takes an unscaled\n-  \/\/  size_t argument instead of an element count.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    4(rsp)   - source array address\n-  \/\/    8(rsp)   - destination array address\n-  \/\/   12(rsp)   - byte count, can be zero\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax, ==  0  -  success\n-  \/\/    rax, == -1  -  need to call System.arraycopy\n-  \/\/\n-  \/\/ Examines the alignment of the operands and dispatches\n-  \/\/ to a long, int, short, or byte copy loop.\n-  \/\/\n-  address generate_unsafe_copy(const char *name,\n-                               address byte_copy_entry,\n-                               address short_copy_entry,\n-                               address int_copy_entry,\n-                               address long_copy_entry) {\n-\n-    Label L_long_aligned, L_int_aligned, L_short_aligned;\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    const Register from       = rax;  \/\/ source array address\n-    const Register to         = rdx;  \/\/ destination array address\n-    const Register count      = rcx;  \/\/ elements count\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ push(rsi);\n-    __ push(rdi);\n-    Address  from_arg(rsp, 12+ 4);      \/\/ from\n-    Address    to_arg(rsp, 12+ 8);      \/\/ to\n-    Address count_arg(rsp, 12+12);      \/\/ byte count\n-\n-    \/\/ Load up:\n-    __ movptr(from ,  from_arg);\n-    __ movptr(to   ,    to_arg);\n-    __ movl2ptr(count, count_arg);\n-\n-    \/\/ bump this on entry, not on exit:\n-    inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);\n-\n-    const Register bits = rsi;\n-    __ mov(bits, from);\n-    __ orptr(bits, to);\n-    __ orptr(bits, count);\n-\n-    __ testl(bits, BytesPerLong-1);\n-    __ jccb(Assembler::zero, L_long_aligned);\n-\n-    __ testl(bits, BytesPerInt-1);\n-    __ jccb(Assembler::zero, L_int_aligned);\n-\n-    __ testl(bits, BytesPerShort-1);\n-    __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n-\n-    __ BIND(L_short_aligned);\n-    __ shrptr(count, LogBytesPerShort); \/\/ size => short_count\n-    __ movl(count_arg, count);          \/\/ update 'count'\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-    __ BIND(L_int_aligned);\n-    __ shrptr(count, LogBytesPerInt); \/\/ size => int_count\n-    __ movl(count_arg, count);          \/\/ update 'count'\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-    __ BIND(L_long_aligned);\n-    __ shrptr(count, LogBytesPerLong); \/\/ size => qword_count\n-    __ movl(count_arg, count);          \/\/ update 'count'\n-    __ pop(rdi); \/\/ Do pops here since jlong_arraycopy stub does not do it.\n-    __ pop(rsi);\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ Perform range checks on the proposed arraycopy.\n-  \/\/ Smashes src_pos and dst_pos.  (Uses them up for temps.)\n-  void arraycopy_range_checks(Register src,\n-                              Register src_pos,\n-                              Register dst,\n-                              Register dst_pos,\n-                              Address& length,\n-                              Label& L_failed) {\n-    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n-    const Register src_end = src_pos;   \/\/ source array end position\n-    const Register dst_end = dst_pos;   \/\/ destination array end position\n-    __ addl(src_end, length); \/\/ src_pos + length\n-    __ addl(dst_end, length); \/\/ dst_pos + length\n-\n-    \/\/  if (src_pos + length > arrayOop(src)->length() ) FAIL;\n-    __ cmpl(src_end, Address(src, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/  if (dst_pos + length > arrayOop(dst)->length() ) FAIL;\n-    __ cmpl(dst_end, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n-  }\n-\n-\n-  \/\/\n-  \/\/  Generate generic array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/     4(rsp)    -  src oop\n-  \/\/     8(rsp)    -  src_pos\n-  \/\/    12(rsp)    -  dst oop\n-  \/\/    16(rsp)    -  dst_pos\n-  \/\/    20(rsp)    -  element count\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax, ==  0  -  success\n-  \/\/    rax, == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_generic_copy(const char *name,\n-                                address entry_jbyte_arraycopy,\n-                                address entry_jshort_arraycopy,\n-                                address entry_jint_arraycopy,\n-                                address entry_oop_arraycopy,\n-                                address entry_jlong_arraycopy,\n-                                address entry_checkcast_arraycopy) {\n-    Label L_failed, L_failed_0, L_objArray;\n-\n-    { int modulus = CodeEntryAlignment;\n-      int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n-      int advance = target - (__ offset() % modulus);\n-      if (advance < 0)  advance += modulus;\n-      if (advance > 0)  __ nop(advance);\n-    }\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n-    __ BIND(L_failed_0);\n-    __ jmp(L_failed);\n-    assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n-\n-    __ align(CodeEntryAlignment);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ push(rsi);\n-    __ push(rdi);\n-\n-    \/\/ bump this on entry, not on exit:\n-    inc_counter_np(SharedRuntime::_generic_array_copy_ctr);\n-\n-    \/\/ Input values\n-    Address SRC     (rsp, 12+ 4);\n-    Address SRC_POS (rsp, 12+ 8);\n-    Address DST     (rsp, 12+12);\n-    Address DST_POS (rsp, 12+16);\n-    Address LENGTH  (rsp, 12+20);\n-\n-    \/\/-----------------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the following conditions are met:\n-    \/\/\n-    \/\/ (1) src and dst must not be null.\n-    \/\/ (2) src_pos must not be negative.\n-    \/\/ (3) dst_pos must not be negative.\n-    \/\/ (4) length  must not be negative.\n-    \/\/ (5) src klass and dst klass should be the same and not null.\n-    \/\/ (6) src and dst should be arrays.\n-    \/\/ (7) src_pos + length must not exceed length of src.\n-    \/\/ (8) dst_pos + length must not exceed length of dst.\n-    \/\/\n-\n-    const Register src     = rax;       \/\/ source array oop\n-    const Register src_pos = rsi;\n-    const Register dst     = rdx;       \/\/ destination array oop\n-    const Register dst_pos = rdi;\n-    const Register length  = rcx;       \/\/ transfer count\n-\n-    \/\/  if (src == null) return -1;\n-    __ movptr(src, SRC);      \/\/ src oop\n-    __ testptr(src, src);\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (src_pos < 0) return -1;\n-    __ movl2ptr(src_pos, SRC_POS);  \/\/ src_pos\n-    __ testl(src_pos, src_pos);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (dst == nullptr) return -1;\n-    __ movptr(dst, DST);      \/\/ dst oop\n-    __ testptr(dst, dst);\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (dst_pos < 0) return -1;\n-    __ movl2ptr(dst_pos, DST_POS);  \/\/ dst_pos\n-    __ testl(dst_pos, dst_pos);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (length < 0) return -1;\n-    __ movl2ptr(length, LENGTH);   \/\/ length\n-    __ testl(length, length);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (src->klass() == nullptr) return -1;\n-    Address src_klass_addr(src, oopDesc::klass_offset_in_bytes());\n-    Address dst_klass_addr(dst, oopDesc::klass_offset_in_bytes());\n-    const Register rcx_src_klass = rcx;    \/\/ array klass\n-    __ movptr(rcx_src_klass, Address(src, oopDesc::klass_offset_in_bytes()));\n-\n-#ifdef ASSERT\n-    \/\/  assert(src->klass() != nullptr);\n-    BLOCK_COMMENT(\"assert klasses not null\");\n-    { Label L1, L2;\n-      __ testptr(rcx_src_klass, rcx_src_klass);\n-      __ jccb(Assembler::notZero, L2);   \/\/ it is broken if klass is null\n-      __ bind(L1);\n-      __ stop(\"broken null klass\");\n-      __ bind(L2);\n-      __ cmpptr(dst_klass_addr, NULL_WORD);\n-      __ jccb(Assembler::equal, L1);      \/\/ this would be broken also\n-      BLOCK_COMMENT(\"assert done\");\n-    }\n-#endif \/\/ASSERT\n-\n-    \/\/ Load layout helper (32-bits)\n-    \/\/\n-    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-    \/\/ 32        30    24            16              8     2                 0\n-    \/\/\n-    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-    \/\/\n-\n-    int lh_offset = in_bytes(Klass::layout_helper_offset());\n-    Address src_klass_lh_addr(rcx_src_klass, lh_offset);\n-\n-    \/\/ Handle objArrays completely differently...\n-    jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-    __ cmpl(src_klass_lh_addr, objArray_lh);\n-    __ jcc(Assembler::equal, L_objArray);\n-\n-    \/\/  if (src->klass() != dst->klass()) return -1;\n-    __ cmpptr(rcx_src_klass, dst_klass_addr);\n-    __ jccb(Assembler::notEqual, L_failed_0);\n-\n-    const Register rcx_lh = rcx;  \/\/ layout helper\n-    assert(rcx_lh == rcx_src_klass, \"known alias\");\n-    __ movl(rcx_lh, src_klass_lh_addr);\n-\n-    \/\/  if (!src->is_Array()) return -1;\n-    __ cmpl(rcx_lh, Klass::_lh_neutral_value);\n-    __ jcc(Assembler::greaterEqual, L_failed_0); \/\/ signed cmp\n-\n-    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n-#ifdef ASSERT\n-    { Label L;\n-      __ cmpl(rcx_lh, (Klass::_lh_array_tag_type_value << Klass::_lh_array_tag_shift));\n-      __ jcc(Assembler::greaterEqual, L); \/\/ signed cmp\n-      __ stop(\"must be a primitive array\");\n-      __ bind(L);\n-    }\n-#endif\n-\n-    assert_different_registers(src, src_pos, dst, dst_pos, rcx_lh);\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, LENGTH, L_failed);\n-\n-    \/\/ TypeArrayKlass\n-    \/\/\n-    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-    \/\/\n-    const Register rsi_offset = rsi; \/\/ array offset\n-    const Register src_array  = src; \/\/ src array offset\n-    const Register dst_array  = dst; \/\/ dst array offset\n-    const Register rdi_elsize = rdi; \/\/ log2 element size\n-\n-    __ mov(rsi_offset, rcx_lh);\n-    __ shrptr(rsi_offset, Klass::_lh_header_size_shift);\n-    __ andptr(rsi_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n-    __ addptr(src_array, rsi_offset);  \/\/ src array offset\n-    __ addptr(dst_array, rsi_offset);  \/\/ dst array offset\n-    __ andptr(rcx_lh, Klass::_lh_log2_element_size_mask); \/\/ log2 elsize\n-\n-    \/\/ next registers should be set before the jump to corresponding stub\n-    const Register from       = src; \/\/ source array address\n-    const Register to         = dst; \/\/ destination array address\n-    const Register count      = rcx; \/\/ elements count\n-    \/\/ some of them should be duplicated on stack\n-#define FROM   Address(rsp, 12+ 4)\n-#define TO     Address(rsp, 12+ 8)   \/\/ Not used now\n-#define COUNT  Address(rsp, 12+12)   \/\/ Only for oop arraycopy\n-\n-    BLOCK_COMMENT(\"scale indexes to element size\");\n-    __ movl2ptr(rsi, SRC_POS);  \/\/ src_pos\n-    __ shlptr(rsi);             \/\/ src_pos << rcx (log2 elsize)\n-    assert(src_array == from, \"\");\n-    __ addptr(from, rsi);       \/\/ from = src_array + SRC_POS << log2 elsize\n-    __ movl2ptr(rdi, DST_POS);  \/\/ dst_pos\n-    __ shlptr(rdi);             \/\/ dst_pos << rcx (log2 elsize)\n-    assert(dst_array == to, \"\");\n-    __ addptr(to,  rdi);        \/\/ to   = dst_array + DST_POS << log2 elsize\n-    __ movptr(FROM, from);      \/\/ src_addr\n-    __ mov(rdi_elsize, rcx_lh); \/\/ log2 elsize\n-    __ movl2ptr(count, LENGTH); \/\/ elements count\n-\n-    BLOCK_COMMENT(\"choose copy loop based on element size\");\n-    __ cmpl(rdi_elsize, 0);\n-\n-    __ jump_cc(Assembler::equal, RuntimeAddress(entry_jbyte_arraycopy));\n-    __ cmpl(rdi_elsize, LogBytesPerShort);\n-    __ jump_cc(Assembler::equal, RuntimeAddress(entry_jshort_arraycopy));\n-    __ cmpl(rdi_elsize, LogBytesPerInt);\n-    __ jump_cc(Assembler::equal, RuntimeAddress(entry_jint_arraycopy));\n-#ifdef ASSERT\n-    __ cmpl(rdi_elsize, LogBytesPerLong);\n-    __ jccb(Assembler::notEqual, L_failed);\n-#endif\n-    __ pop(rdi); \/\/ Do pops here since jlong_arraycopy stub does not do it.\n-    __ pop(rsi);\n-    __ jump(RuntimeAddress(entry_jlong_arraycopy));\n-\n-  __ BIND(L_failed);\n-    __ xorptr(rax, rax);\n-    __ notptr(rax); \/\/ return -1\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    \/\/ ObjArrayKlass\n-  __ BIND(L_objArray);\n-    \/\/ live at this point:  rcx_src_klass, src[_pos], dst[_pos]\n-\n-    Label L_plain_copy, L_checkcast_copy;\n-    \/\/  test array classes for subtyping\n-    __ cmpptr(rcx_src_klass, dst_klass_addr); \/\/ usual case is exact equality\n-    __ jccb(Assembler::notEqual, L_checkcast_copy);\n-\n-    \/\/ Identically typed arrays can be copied without element-wise checks.\n-    assert_different_registers(src, src_pos, dst, dst_pos, rcx_src_klass);\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, LENGTH, L_failed);\n-\n-  __ BIND(L_plain_copy);\n-    __ movl2ptr(count, LENGTH); \/\/ elements count\n-    __ movl2ptr(src_pos, SRC_POS);  \/\/ reload src_pos\n-    __ lea(from, Address(src, src_pos, Address::times_ptr,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n-    __ movl2ptr(dst_pos, DST_POS);  \/\/ reload dst_pos\n-    __ lea(to,   Address(dst, dst_pos, Address::times_ptr,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n-    __ movptr(FROM,  from);   \/\/ src_addr\n-    __ movptr(TO,    to);     \/\/ dst_addr\n-    __ movl(COUNT, count);  \/\/ count\n-    __ jump(RuntimeAddress(entry_oop_arraycopy));\n-\n-  __ BIND(L_checkcast_copy);\n-    \/\/ live at this point:  rcx_src_klass, dst[_pos], src[_pos]\n-    {\n-      \/\/ Handy offsets:\n-      int  ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-\n-      Register rsi_dst_klass = rsi;\n-      Register rdi_temp      = rdi;\n-      assert(rsi_dst_klass == src_pos, \"expected alias w\/ src_pos\");\n-      assert(rdi_temp      == dst_pos, \"expected alias w\/ dst_pos\");\n-      Address dst_klass_lh_addr(rsi_dst_klass, lh_offset);\n-\n-      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-      __ movptr(rsi_dst_klass, dst_klass_addr);\n-      __ cmpl(dst_klass_lh_addr, objArray_lh);\n-      __ jccb(Assembler::notEqual, L_failed);\n-\n-      \/\/ It is safe to examine both src.length and dst.length.\n-      __ movl2ptr(src_pos, SRC_POS);        \/\/ reload rsi\n-      arraycopy_range_checks(src, src_pos, dst, dst_pos, LENGTH, L_failed);\n-      \/\/ (Now src_pos and dst_pos are killed, but not src and dst.)\n-\n-      \/\/ We'll need this temp (don't forget to pop it after the type check).\n-      __ push(rbx);\n-      Register rbx_src_klass = rbx;\n-\n-      __ mov(rbx_src_klass, rcx_src_klass); \/\/ spill away from rcx\n-      __ movptr(rsi_dst_klass, dst_klass_addr);\n-      Address super_check_offset_addr(rsi_dst_klass, sco_offset);\n-      Label L_fail_array_check;\n-      generate_type_check(rbx_src_klass,\n-                          super_check_offset_addr, dst_klass_addr,\n-                          rdi_temp, nullptr, &L_fail_array_check);\n-      \/\/ (On fall-through, we have passed the array type check.)\n-      __ pop(rbx);\n-      __ jmp(L_plain_copy);\n-\n-      __ BIND(L_fail_array_check);\n-      \/\/ Reshuffle arguments so we can call checkcast_arraycopy:\n-\n-      \/\/ match initial saves for checkcast_arraycopy\n-      \/\/ push(rsi);    \/\/ already done; see above\n-      \/\/ push(rdi);    \/\/ already done; see above\n-      \/\/ push(rbx);    \/\/ already done; see above\n-\n-      \/\/ Marshal outgoing arguments now, freeing registers.\n-      Address   from_arg(rsp, 16+ 4);   \/\/ from\n-      Address     to_arg(rsp, 16+ 8);   \/\/ to\n-      Address length_arg(rsp, 16+12);   \/\/ elements count\n-      Address  ckoff_arg(rsp, 16+16);   \/\/ super_check_offset\n-      Address  ckval_arg(rsp, 16+20);   \/\/ super_klass\n-\n-      Address SRC_POS_arg(rsp, 16+ 8);\n-      Address DST_POS_arg(rsp, 16+16);\n-      Address  LENGTH_arg(rsp, 16+20);\n-      \/\/ push rbx, changed the incoming offsets (why not just use rbp,??)\n-      \/\/ assert(SRC_POS_arg.disp() == SRC_POS.disp() + 4, \"\");\n-\n-      __ movptr(rbx, Address(rsi_dst_klass, ek_offset));\n-      __ movl2ptr(length, LENGTH_arg);    \/\/ reload elements count\n-      __ movl2ptr(src_pos, SRC_POS_arg);  \/\/ reload src_pos\n-      __ movl2ptr(dst_pos, DST_POS_arg);  \/\/ reload dst_pos\n-\n-      __ movptr(ckval_arg, rbx);          \/\/ destination element type\n-      __ movl(rbx, Address(rbx, sco_offset));\n-      __ movl(ckoff_arg, rbx);          \/\/ corresponding class check offset\n-\n-      __ movl(length_arg, length);      \/\/ outgoing length argument\n-\n-      __ lea(from, Address(src, src_pos, Address::times_ptr,\n-                            arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ movptr(from_arg, from);\n-\n-      __ lea(to, Address(dst, dst_pos, Address::times_ptr,\n-                          arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ movptr(to_arg, to);\n-      __ jump(RuntimeAddress(entry_checkcast_arraycopy));\n-    }\n-\n-    return start;\n-  }\n-\n-  void generate_arraycopy_stubs() {\n-    address entry;\n-    address entry_jbyte_arraycopy;\n-    address entry_jshort_arraycopy;\n-    address entry_jint_arraycopy;\n-    address entry_oop_arraycopy;\n-    address entry_jlong_arraycopy;\n-    address entry_checkcast_arraycopy;\n-\n-    StubRoutines::_arrayof_jbyte_disjoint_arraycopy =\n-        generate_disjoint_copy(T_BYTE,  true, Address::times_1, &entry,\n-                               \"arrayof_jbyte_disjoint_arraycopy\");\n-    StubRoutines::_arrayof_jbyte_arraycopy =\n-        generate_conjoint_copy(T_BYTE,  true, Address::times_1,  entry,\n-                               nullptr, \"arrayof_jbyte_arraycopy\");\n-    StubRoutines::_jbyte_disjoint_arraycopy =\n-        generate_disjoint_copy(T_BYTE, false, Address::times_1, &entry,\n-                               \"jbyte_disjoint_arraycopy\");\n-    StubRoutines::_jbyte_arraycopy =\n-        generate_conjoint_copy(T_BYTE, false, Address::times_1,  entry,\n-                               &entry_jbyte_arraycopy, \"jbyte_arraycopy\");\n-\n-    StubRoutines::_arrayof_jshort_disjoint_arraycopy =\n-        generate_disjoint_copy(T_SHORT,  true, Address::times_2, &entry,\n-                               \"arrayof_jshort_disjoint_arraycopy\");\n-    StubRoutines::_arrayof_jshort_arraycopy =\n-        generate_conjoint_copy(T_SHORT,  true, Address::times_2,  entry,\n-                               nullptr, \"arrayof_jshort_arraycopy\");\n-    StubRoutines::_jshort_disjoint_arraycopy =\n-        generate_disjoint_copy(T_SHORT, false, Address::times_2, &entry,\n-                               \"jshort_disjoint_arraycopy\");\n-    StubRoutines::_jshort_arraycopy =\n-        generate_conjoint_copy(T_SHORT, false, Address::times_2,  entry,\n-                               &entry_jshort_arraycopy, \"jshort_arraycopy\");\n-\n-    \/\/ Next arrays are always aligned on 4 bytes at least.\n-    StubRoutines::_jint_disjoint_arraycopy =\n-        generate_disjoint_copy(T_INT, true, Address::times_4, &entry,\n-                               \"jint_disjoint_arraycopy\");\n-    StubRoutines::_jint_arraycopy =\n-        generate_conjoint_copy(T_INT, true, Address::times_4,  entry,\n-                               &entry_jint_arraycopy, \"jint_arraycopy\");\n-\n-    StubRoutines::_oop_disjoint_arraycopy =\n-        generate_disjoint_copy(T_OBJECT, true, Address::times_ptr, &entry,\n-                               \"oop_disjoint_arraycopy\");\n-    StubRoutines::_oop_arraycopy =\n-        generate_conjoint_copy(T_OBJECT, true, Address::times_ptr,  entry,\n-                               &entry_oop_arraycopy, \"oop_arraycopy\");\n-\n-    StubRoutines::_oop_disjoint_arraycopy_uninit =\n-        generate_disjoint_copy(T_OBJECT, true, Address::times_ptr, &entry,\n-                               \"oop_disjoint_arraycopy_uninit\",\n-                               \/*dest_uninitialized*\/true);\n-    StubRoutines::_oop_arraycopy_uninit =\n-        generate_conjoint_copy(T_OBJECT, true, Address::times_ptr,  entry,\n-                               nullptr, \"oop_arraycopy_uninit\",\n-                               \/*dest_uninitialized*\/true);\n-\n-    StubRoutines::_jlong_disjoint_arraycopy =\n-        generate_disjoint_long_copy(&entry, \"jlong_disjoint_arraycopy\");\n-    StubRoutines::_jlong_arraycopy =\n-        generate_conjoint_long_copy(entry, &entry_jlong_arraycopy,\n-                                    \"jlong_arraycopy\");\n-\n-    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n-\n-    StubRoutines::_arrayof_jint_disjoint_arraycopy       = StubRoutines::_jint_disjoint_arraycopy;\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy        = StubRoutines::_oop_disjoint_arraycopy;\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit = StubRoutines::_oop_disjoint_arraycopy_uninit;\n-    StubRoutines::_arrayof_jlong_disjoint_arraycopy      = StubRoutines::_jlong_disjoint_arraycopy;\n-\n-    StubRoutines::_arrayof_jint_arraycopy       = StubRoutines::_jint_arraycopy;\n-    StubRoutines::_arrayof_oop_arraycopy        = StubRoutines::_oop_arraycopy;\n-    StubRoutines::_arrayof_oop_arraycopy_uninit = StubRoutines::_oop_arraycopy_uninit;\n-    StubRoutines::_arrayof_jlong_arraycopy      = StubRoutines::_jlong_arraycopy;\n-\n-    StubRoutines::_checkcast_arraycopy =\n-        generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n-    StubRoutines::_checkcast_arraycopy_uninit =\n-        generate_checkcast_copy(\"checkcast_arraycopy_uninit\", nullptr, \/*dest_uninitialized*\/true);\n-\n-    StubRoutines::_unsafe_arraycopy =\n-        generate_unsafe_copy(\"unsafe_arraycopy\",\n-                               entry_jbyte_arraycopy,\n-                               entry_jshort_arraycopy,\n-                               entry_jint_arraycopy,\n-                               entry_jlong_arraycopy);\n-\n-    StubRoutines::_generic_arraycopy =\n-        generate_generic_copy(\"generic_arraycopy\",\n-                               entry_jbyte_arraycopy,\n-                               entry_jshort_arraycopy,\n-                               entry_jint_arraycopy,\n-                               entry_oop_arraycopy,\n-                               entry_jlong_arraycopy,\n-                               entry_checkcast_arraycopy);\n-  }\n-\n-  \/\/ AES intrinsic stubs\n-  enum {AESBlockSize = 16};\n-\n-  address key_shuffle_mask_addr() {\n-    return (address)KEY_SHUFFLE_MASK;\n-  }\n-\n-  address counter_shuffle_mask_addr() {\n-    return (address)COUNTER_SHUFFLE_MASK;\n-  }\n-\n-  \/\/ Utility routine for loading a 128-bit key word in little endian format\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != xnoreg) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(key_shuffle_mask_addr()));\n-    }\n-  }\n-\n-  \/\/ aesenc using specified key+offset\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void aes_enc_key(XMMRegister xmmdst, XMMRegister xmmtmp, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n-    load_key(xmmtmp, key, offset, xmm_shuf_mask);\n-    __ aesenc(xmmdst, xmmtmp);\n-  }\n-\n-  \/\/ aesdec using specified key+offset\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void aes_dec_key(XMMRegister xmmdst, XMMRegister xmmtmp, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n-    load_key(xmmtmp, key, offset, xmm_shuf_mask);\n-    __ aesdec(xmmdst, xmmtmp);\n-  }\n-\n-  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-  \/\/  XMM_128bit,  D3, D2, D1, D0\n-  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n-    __ pextrd(reg, xmmdst, 0x0);\n-    __ addl(reg, inc_delta);\n-    __ pinsrd(xmmdst, reg, 0x0);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-\n-    __ pextrd(reg, xmmdst, 0x01); \/\/ Carry-> D1\n-    __ addl(reg, 0x01);\n-    __ pinsrd(xmmdst, reg, 0x01);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-\n-    __ pextrd(reg, xmmdst, 0x02); \/\/ Carry-> D2\n-    __ addl(reg, 0x01);\n-    __ pinsrd(xmmdst, reg, 0x02);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-\n-    __ pextrd(reg, xmmdst, 0x03); \/\/ Carry -> D3\n-    __ addl(reg, 0x01);\n-    __ pinsrd(xmmdst, reg, 0x03);\n-\n-    __ BIND(next_block);          \/\/ next instruction\n-  }\n-\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_encryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = rdx;      \/\/ source array address\n-    const Register to          = rdx;      \/\/ destination array address\n-    const Register key         = rcx;      \/\/ key array address\n-    const Register keylen      = rax;\n-    const Address  from_param(rbp, 8+0);\n-    const Address  to_param  (rbp, 8+4);\n-    const Address  key_param (rbp, 8+8);\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter();   \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    __ movptr(from, from_param);\n-    __ movptr(key, key_param);\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n-    __ movptr(to, to_param);\n-\n-    \/\/ For encryption, the java expanded key ordering is just what we need\n-\n-    load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n-    __ pxor(xmm_result, xmm_temp1);\n-\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenclast(xmm_result, xmm_temp2);\n-    __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_decryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = rdx;      \/\/ source array address\n-    const Register to          = rdx;      \/\/ destination array address\n-    const Register key         = rcx;      \/\/ key array address\n-    const Register keylen      = rax;\n-    const Address  from_param(rbp, 8+0);\n-    const Address  to_param  (rbp, 8+4);\n-    const Address  key_param (rbp, 8+8);\n-\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    __ movptr(from, from_param);\n-    __ movptr(key, key_param);\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));\n-    __ movptr(to, to_param);\n-\n-    \/\/ for decryption java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ pxor  (xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    \/\/ for decryption the aesdeclast operation is always on key+0x00\n-    __ aesdeclast(xmm_result, xmm_temp3);\n-    __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  void handleSOERegisters(bool saving) {\n-    const int saveFrameSizeInBytes = 4 * wordSize;\n-    const Address saved_rbx     (rbp, -3 * wordSize);\n-    const Address saved_rsi     (rbp, -2 * wordSize);\n-    const Address saved_rdi     (rbp, -1 * wordSize);\n-\n-    if (saving) {\n-      __ subptr(rsp, saveFrameSizeInBytes);\n-      __ movptr(saved_rsi, rsi);\n-      __ movptr(saved_rdi, rdi);\n-      __ movptr(saved_rbx, rbx);\n-    } else {\n-      \/\/ restoring\n-      __ movptr(rsi, saved_rsi);\n-      __ movptr(rdi, saved_rdi);\n-      __ movptr(rbx, saved_rbx);\n-    }\n-  }\n-\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_encryptAESCrypt() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n-    address start = __ pc();\n-\n-    Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n-    const Register from        = rsi;      \/\/ source array address\n-    const Register to          = rdx;      \/\/ destination array address\n-    const Register key         = rcx;      \/\/ key array address\n-    const Register rvec        = rdi;      \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n-    const Register len_reg     = rbx;      \/\/ src len (must be multiple of blocksize 16)\n-    const Register pos         = rax;\n-\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_temp   = xmm1;\n-    \/\/ first 6 keys preloaded into xmm2-xmm7\n-    const int XMM_REG_NUM_KEY_FIRST = 2;\n-    const int XMM_REG_NUM_KEY_LAST  = 7;\n-    const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    handleSOERegisters(true \/*saving*\/);\n-\n-    \/\/ load registers from incoming parameters\n-    const Address  from_param(rbp, 8+0);\n-    const Address  to_param  (rbp, 8+4);\n-    const Address  key_param (rbp, 8+8);\n-    const Address  rvec_param (rbp, 8+12);\n-    const Address  len_param  (rbp, 8+16);\n-    __ movptr(from , from_param);\n-    __ movptr(to   , to_param);\n-    __ movptr(key  , key_param);\n-    __ movptr(rvec , rvec_param);\n-    __ movptr(len_reg , len_param);\n-\n-    const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs 2 thru 7 with keys 0-5\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum  <= XMM_REG_NUM_KEY_LAST; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-\n-    __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rax, 44);\n-    __ jcc(Assembler::notEqual, L_key_192_256);\n-\n-    \/\/ 128 bit code follows here\n-    __ movl(pos, 0);\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_loopTop_128);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);                                \/\/ xor with the current r vector\n-\n-    __ pxor  (xmm_result, xmm_key0);                                \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_LAST; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    for (int key_offset = 0x60; key_offset <= 0x90; key_offset += 0x10) {\n-      aes_enc_key(xmm_result, xmm_temp, key, key_offset);\n-    }\n-    load_key(xmm_temp, key, 0xa0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_128);\n-\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-\n-    handleSOERegisters(false \/*restoring*\/);\n-    __ movptr(rax, len_param); \/\/ return length\n-    __ leave();                                  \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    __ BIND(L_key_192_256);\n-    \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n-    __ cmpl(rax, 52);\n-    __ jcc(Assembler::notEqual, L_key_256);\n-\n-    \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n-    __ movl(pos, 0);\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_loopTop_192);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);                                \/\/ xor with the current r vector\n-\n-    __ pxor  (xmm_result, xmm_key0);                                \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_LAST; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    for (int key_offset = 0x60; key_offset <= 0xb0; key_offset += 0x10) {\n-      aes_enc_key(xmm_result, xmm_temp, key, key_offset);\n-    }\n-    load_key(xmm_temp, key, 0xc0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);   \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_192);\n-    __ jmp(L_exit);\n-\n-    __ BIND(L_key_256);\n-    \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n-    __ movl(pos, 0);\n-    __ align(OptoLoopAlignment);\n-    __ BIND(L_loopTop_256);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);                                \/\/ xor with the current r vector\n-\n-    __ pxor  (xmm_result, xmm_key0);                                \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_LAST; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    for (int key_offset = 0x60; key_offset <= 0xd0; key_offset += 0x10) {\n-      aes_enc_key(xmm_result, xmm_temp, key, key_offset);\n-    }\n-    load_key(xmm_temp, key, 0xe0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);   \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_256);\n-    __ jmp(L_exit);\n-\n-    return start;\n-  }\n-\n-\n-  \/\/ CBC AES Decryption.\n-  \/\/ In 32-bit stub, because of lack of registers we do not try to parallelize 4 blocks at a time.\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-\n-  address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from        = rsi;      \/\/ source array address\n-    const Register to          = rdx;      \/\/ destination array address\n-    const Register key         = rcx;      \/\/ key array address\n-    const Register rvec        = rdi;      \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n-    const Register len_reg     = rbx;      \/\/ src len (must be multiple of blocksize 16)\n-    const Register pos         = rax;\n-\n-    const int PARALLEL_FACTOR = 4;\n-    const int ROUNDS[3] = { 10, 12, 14 }; \/\/aes rounds for key128, key192, key256\n-\n-    Label L_exit;\n-    Label L_singleBlock_loopTop[3]; \/\/128, 192, 256\n-    Label L_multiBlock_loopTop[3]; \/\/128, 192, 256\n-\n-    const XMMRegister xmm_prev_block_cipher = xmm0; \/\/ holds cipher of previous block\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-\n-    const XMMRegister xmm_key_tmp0 = xmm2;\n-    const XMMRegister xmm_key_tmp1 = xmm3;\n-\n-    \/\/ registers holding the six results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm4;\n-    const XMMRegister xmm_result1 = xmm5;\n-    const XMMRegister xmm_result2 = xmm6;\n-    const XMMRegister xmm_result3 = xmm7;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    handleSOERegisters(true \/*saving*\/);\n-\n-    \/\/ load registers from incoming parameters\n-    const Address  from_param(rbp, 8+0);\n-    const Address  to_param  (rbp, 8+4);\n-    const Address  key_param (rbp, 8+8);\n-    const Address  rvec_param (rbp, 8+12);\n-    const Address  len_param  (rbp, 8+16);\n-\n-    __ movptr(from , from_param);\n-    __ movptr(to   , to_param);\n-    __ movptr(key  , key_param);\n-    __ movptr(rvec , rvec_param);\n-    __ movptr(len_reg , len_param);\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-    __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00)); \/\/ initialize with initial rvec\n-\n-    __ xorptr(pos, pos);\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    \/\/ rvec is reused\n-    __ movl(rvec, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rvec, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n-    __ cmpl(rvec, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n-\n-#define DoFour(opc, src_reg)           \\\n-  __ opc(xmm_result0, src_reg);         \\\n-  __ opc(xmm_result1, src_reg);         \\\n-  __ opc(xmm_result2, src_reg);         \\\n-  __ opc(xmm_result3, src_reg);         \\\n-\n-    for (int k = 0; k < 3; ++k) {\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-      __ jcc(Assembler::less, L_singleBlock_loopTop[k]);\n-\n-      __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n-      __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-\n-      \/\/ the java expanded key ordering is rotated one position from what we want\n-      \/\/ so we start from 0x10 here and hit 0x00 last\n-      load_key(xmm_key_tmp0, key, 0x10, xmm_key_shuf_mask);\n-      DoFour(pxor, xmm_key_tmp0); \/\/xor with first key\n-      \/\/ do the aes dec rounds\n-      for (int rnum = 1; rnum <= ROUNDS[k];) {\n-        \/\/load two keys at a time\n-        \/\/k1->0x20, ..., k9->0xa0, k10->0x00\n-        load_key(xmm_key_tmp1, key, (rnum + 1) * 0x10, xmm_key_shuf_mask);\n-        load_key(xmm_key_tmp0, key, ((rnum + 2) % (ROUNDS[k] + 1)) * 0x10, xmm_key_shuf_mask); \/\/ hit 0x00 last!\n-        DoFour(aesdec, xmm_key_tmp1);\n-        rnum++;\n-        if (rnum != ROUNDS[k]) {\n-          DoFour(aesdec, xmm_key_tmp0);\n-        }\n-        else {\n-          DoFour(aesdeclast, xmm_key_tmp0);\n-        }\n-        rnum++;\n-      }\n-\n-      \/\/ for each result, xor with the r vector of previous cipher block\n-      __ pxor(xmm_result0, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ pxor(xmm_result1, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ pxor(xmm_result2, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ pxor(xmm_result3, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize)); \/\/ this will carry over to next set of blocks\n-\n-            \/\/ store 4 results into the next 64 bytes of output\n-       __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-       __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-       __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-       __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-       __ addptr(pos, 4 * AESBlockSize);\n-       __ subptr(len_reg, 4 * AESBlockSize);\n-       __ jmp(L_multiBlock_loopTop[k]);\n-\n-       \/\/singleBlock starts here\n-       __ align(OptoLoopAlignment);\n-       __ BIND(L_singleBlock_loopTop[k]);\n-       __ cmpptr(len_reg, 0); \/\/ any blocks left?\n-       __ jcc(Assembler::equal, L_exit);\n-       __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n-       __ movdqa(xmm_result1, xmm_result0);\n-\n-       load_key(xmm_key_tmp0, key, 0x10, xmm_key_shuf_mask);\n-       __ pxor(xmm_result0, xmm_key_tmp0);\n-       \/\/ do the aes dec rounds\n-       for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n-         \/\/ the java expanded key ordering is rotated one position from what we want\n-         load_key(xmm_key_tmp0, key, (rnum + 1) * 0x10, xmm_key_shuf_mask);\n-         __ aesdec(xmm_result0, xmm_key_tmp0);\n-       }\n-       load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-       __ aesdeclast(xmm_result0, xmm_key_tmp0);\n-       __ pxor(xmm_result0, xmm_prev_block_cipher); \/\/ xor with the current r vector\n-       __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result0); \/\/ store into the next 16 bytes of output\n-       \/\/ no need to store r to memory until we exit\n-       __ movdqa(xmm_prev_block_cipher, xmm_result1); \/\/ set up next r vector with cipher input from this block\n-\n-       __ addptr(pos, AESBlockSize);\n-       __ subptr(len_reg, AESBlockSize);\n-       __ jmp(L_singleBlock_loopTop[k]);\n-    }\/\/for 128\/192\/256\n-\n-    __ BIND(L_exit);\n-    __ movptr(rvec, rvec_param);                        \/\/ restore this since reused earlier\n-    __ movdqu(Address(rvec, 0), xmm_prev_block_cipher); \/\/ final value of r stored in rvec of CipherBlockChaining object\n-    handleSOERegisters(false \/*restoring*\/);\n-    __ movptr(rax, len_param);                          \/\/ return length\n-    __ leave();                                         \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/ CTR AES crypt.\n-  \/\/ In 32-bit stub, parallelize 4 blocks at a time\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - counter vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_counterMode_AESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from        = rsi;      \/\/ source array address\n-    const Register to          = rdx;      \/\/ destination array address\n-    const Register key         = rcx;      \/\/ key array address\n-    const Register counter     = rdi;      \/\/ counter byte array initialized from initvector array address\n-                                           \/\/ and updated with the incremented counter in the end\n-    const Register len_reg     = rbx;\n-    const Register pos         = rax;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    handleSOERegisters(true \/*saving*\/); \/\/ save rbx, rsi, rdi\n-\n-    \/\/ load registers from incoming parameters\n-    const Address  from_param(rbp, 8+0);\n-    const Address  to_param  (rbp, 8+4);\n-    const Address  key_param (rbp, 8+8);\n-    const Address  rvec_param (rbp, 8+12);\n-    const Address  len_param  (rbp, 8+16);\n-    const Address  saved_counter_param(rbp, 8 + 20);\n-    const Address  used_addr_param(rbp, 8 + 24);\n-\n-    __ movptr(from , from_param);\n-    __ movptr(to   , to_param);\n-    __ movptr(len_reg , len_param);\n-\n-    \/\/ Use the partially used encrpyted counter from last invocation\n-    Label L_exit_preLoop, L_preLoop_start;\n-\n-    \/\/ Use the registers 'counter' and 'key' here in this preloop\n-    \/\/ to hold of last 2 params 'used' and 'saved_encCounter_start'\n-    Register used = counter;\n-    Register saved_encCounter_start = key;\n-    Register used_addr = saved_encCounter_start;\n-\n-    __ movptr(used_addr, used_addr_param);\n-    __ movptr(used, Address(used_addr, 0));\n-    __ movptr(saved_encCounter_start, saved_counter_param);\n-\n-    __ BIND(L_preLoop_start);\n-    __ cmpptr(used, 16);\n-    __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n-    __ cmpptr(len_reg, 0);\n-    __ jcc(Assembler::lessEqual, L_exit_preLoop);\n-    __ movb(rax, Address(saved_encCounter_start, used));\n-    __ xorb(rax, Address(from, 0));\n-    __ movb(Address(to, 0), rax);\n-    __ addptr(from, 1);\n-    __ addptr(to, 1);\n-    __ addptr(used, 1);\n-    __ subptr(len_reg, 1);\n-\n-    __ jmp(L_preLoop_start);\n-\n-    __ BIND(L_exit_preLoop);\n-    __ movptr(used_addr, used_addr_param);\n-    __ movptr(used_addr, used_addr_param);\n-    __ movl(Address(used_addr, 0), used);\n-\n-    \/\/ load the parameters 'key' and 'counter'\n-    __ movptr(key, key_param);\n-    __ movptr(counter, rvec_param);\n-\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_curr_counter      = xmm0;\n-    const XMMRegister xmm_counter_shuf_mask = xmm1;  \/\/ need to be reloaded\n-    const XMMRegister xmm_key_shuf_mask     = xmm2;  \/\/ need to be reloaded\n-    const XMMRegister xmm_key               = xmm3;\n-    const XMMRegister xmm_result0           = xmm4;\n-    const XMMRegister xmm_result1           = xmm5;\n-    const XMMRegister xmm_result2           = xmm6;\n-    const XMMRegister xmm_result3           = xmm7;\n-    const XMMRegister xmm_from0             = xmm1;   \/\/reuse XMM register\n-    const XMMRegister xmm_from1             = xmm2;\n-    const XMMRegister xmm_from2             = xmm3;\n-    const XMMRegister xmm_from3             = xmm4;\n-\n-    \/\/for key_128, key_192, key_256\n-    const int rounds[3] = {10, 12, 14};\n-    Label L_singleBlockLoopTop[3];\n-    Label L_multiBlock_loopTop[3];\n-    Label L_key192_top, L_key256_top;\n-    Label L_incCounter[3][4]; \/\/ 3: different key length,  4: 4 blocks at a time\n-    Label L_incCounter_single[3]; \/\/for single block, key128, key192, key256\n-    Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n-    Label L_processTail_extr[3], L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n-\n-    Label L_exit;\n-    const int PARALLEL_FACTOR = 4;  \/\/because of the limited register number\n-\n-    \/\/ initialize counter with initial counter\n-    __ movdqu(xmm_curr_counter, Address(counter, 0x00));\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled for increase\n-\n-    \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-    __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rax, 52);\n-    __ jcc(Assembler::equal, L_key192_top);\n-    __ cmpl(rax, 60);\n-    __ jcc(Assembler::equal, L_key256_top);\n-\n-    \/\/key128 begins here\n-    __ movptr(pos, 0); \/\/ init pos before L_multiBlock_loopTop\n-\n-#define CTR_DoFour(opc, src_reg)               \\\n-    __ opc(xmm_result0, src_reg);              \\\n-    __ opc(xmm_result1, src_reg);              \\\n-    __ opc(xmm_result2, src_reg);              \\\n-    __ opc(xmm_result3, src_reg);\n-\n-    \/\/ k == 0 :  generate code for key_128\n-    \/\/ k == 1 :  generate code for key_192\n-    \/\/ k == 2 :  generate code for key_256\n-    for (int k = 0; k < 3; ++k) {\n-      \/\/multi blocks starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n-      __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n-\n-      __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-      __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n-\n-      \/\/load, then increase counters\n-      CTR_DoFour(movdqa, xmm_curr_counter);\n-      __ push(rbx);\n-      inc_counter(rbx, xmm_result1, 0x01, L_incCounter[k][0]);\n-      inc_counter(rbx, xmm_result2, 0x02, L_incCounter[k][1]);\n-      inc_counter(rbx, xmm_result3, 0x03, L_incCounter[k][2]);\n-      inc_counter(rbx, xmm_curr_counter, 0x04, L_incCounter[k][3]);\n-      __ pop (rbx);\n-\n-      load_key(xmm_key, key, 0x00, xmm_key_shuf_mask); \/\/ load Round 0 key. interleaving for better performance\n-\n-      CTR_DoFour(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n-      CTR_DoFour(pxor, xmm_key);   \/\/PXOR with Round 0 key\n-\n-      for (int i = 1; i < rounds[k]; ++i) {\n-        load_key(xmm_key, key, (0x10 * i), xmm_key_shuf_mask);\n-        CTR_DoFour(aesenc, xmm_key);\n-      }\n-      load_key(xmm_key, key, (0x10 * rounds[k]), xmm_key_shuf_mask);\n-      CTR_DoFour(aesenclast, xmm_key);\n-\n-      \/\/ get next PARALLEL_FACTOR blocks into xmm_from registers\n-      __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-\n-      \/\/ PXOR with input text\n-      __ pxor(xmm_result0, xmm_from0); \/\/result0 is xmm4\n-      __ pxor(xmm_result1, xmm_from1);\n-      __ pxor(xmm_result2, xmm_from2);\n-\n-      \/\/ store PARALLEL_FACTOR results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-\n-      \/\/ do it here after xmm_result0 is saved, because xmm_from3 reuse the same register of xmm_result0.\n-      __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-      __ pxor(xmm_result3, xmm_from3);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ singleBlock starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlockLoopTop[k]);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::equal, L_exit);\n-      __ movdqu(xmm_key_shuf_mask, ExternalAddress(key_shuffle_mask_addr()));\n-      __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n-      __ movdqa(xmm_result0, xmm_curr_counter);\n-      load_key(xmm_key, key, 0x00, xmm_key_shuf_mask);\n-      __ push(rbx);\/\/rbx is used for increasing counter\n-      inc_counter(rbx, xmm_curr_counter, 0x01, L_incCounter_single[k]);\n-      __ pop (rbx);\n-      __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n-      __ pxor(xmm_result0, xmm_key);\n-      for (int i = 1; i < rounds[k]; i++) {\n-        load_key(xmm_key, key, (0x10 * i), xmm_key_shuf_mask);\n-        __ aesenc(xmm_result0, xmm_key);\n-      }\n-      load_key(xmm_key, key, (0x10 * rounds[k]), xmm_key_shuf_mask);\n-      __ aesenclast(xmm_result0, xmm_key);\n-      __ cmpptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::less, L_processTail_insr[k]);\n-        __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-        __ pxor(xmm_result0, xmm_from0);\n-        __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-        __ addptr(pos, AESBlockSize);\n-        __ subptr(len_reg, AESBlockSize);\n-        __ jmp(L_singleBlockLoopTop[k]);\n-\n-      __ BIND(L_processTail_insr[k]);                                               \/\/ Process the tail part of the input array\n-        __ addptr(pos, len_reg);                                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n-          __ subptr(pos,8);\n-          __ pinsrd(xmm_from0, Address(from, pos), 0);\n-          __ pinsrd(xmm_from0, Address(from, pos, Address::times_1, 4), 1);\n-        __ BIND(L_processTail_4_insr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n-          __ subptr(pos,4);\n-          __ pslldq(xmm_from0, 4);\n-          __ pinsrd(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_2_insr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n-          __ subptr(pos, 2);\n-          __ pslldq(xmm_from0, 2);\n-          __ pinsrw(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_1_insr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n-          __ subptr(pos, 1);\n-          __ pslldq(xmm_from0, 1);\n-          __ pinsrb(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_exit_insr[k]);\n-\n-        __ movptr(saved_encCounter_start, saved_counter_param);\n-        __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);               \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n-        __ pxor(xmm_result0, xmm_from0);                                          \/\/    Also the encrypted counter is saved for next invocation.\n-\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_extr[k]);                        \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n-          __ pextrd(Address(to, pos), xmm_result0, 0);\n-          __ pextrd(Address(to, pos, Address::times_1, 4), xmm_result0, 1);\n-          __ psrldq(xmm_result0, 8);\n-          __ addptr(pos, 8);\n-        __ BIND(L_processTail_4_extr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n-          __ pextrd(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 4);\n-          __ addptr(pos, 4);\n-        __ BIND(L_processTail_2_extr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n-          __ pextrb(Address(to, pos), xmm_result0, 0);\n-          __ pextrb(Address(to, pos, Address::times_1, 1), xmm_result0, 1);\n-          __ psrldq(xmm_result0, 2);\n-          __ addptr(pos, 2);\n-        __ BIND(L_processTail_1_extr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n-          __ pextrb(Address(to, pos), xmm_result0, 0);\n-\n-        __ BIND(L_processTail_exit_extr[k]);\n-        __ movptr(used_addr, used_addr_param);\n-        __ movl(Address(used_addr, 0), len_reg);\n-        __ jmp(L_exit);\n-    }\n-\n-    __ BIND(L_exit);\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(counter_shuffle_mask_addr()));\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n-    __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n-    handleSOERegisters(false \/*restoring*\/);\n-    __ movptr(rax, len_param); \/\/ return length\n-    __ leave();                \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    __ BIND (L_key192_top);\n-    __ movptr(pos, 0); \/\/ init pos before L_multiBlock_loopTop\n-    __ jmp(L_multiBlock_loopTop[1]); \/\/key192\n-\n-    __ BIND (L_key256_top);\n-    __ movptr(pos, 0); \/\/ init pos before L_multiBlock_loopTop\n-    __ jmp(L_multiBlock_loopTop[2]); \/\/key192\n-\n-    return start;\n-  }\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n-  address generate_md5_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    const Register buf_param = rbp;\n-    const Address state_param(rsp, 0 * wordSize);\n-    const Address ofs_param  (rsp, 1 * wordSize);\n-    const Address limit_param(rsp, 2 * wordSize);\n-\n-    __ enter();\n-    __ push(rbx);\n-    __ push(rdi);\n-    __ push(rsi);\n-    __ push(rbp);\n-    __ subptr(rsp, 3 * wordSize);\n-\n-    __ movptr(rsi, Address(rbp, 8 + 4));\n-    __ movptr(state_param, rsi);\n-    if (multi_block) {\n-      __ movptr(rsi, Address(rbp, 8 + 8));\n-      __ movptr(ofs_param, rsi);\n-      __ movptr(rsi, Address(rbp, 8 + 12));\n-      __ movptr(limit_param, rsi);\n-    }\n-    __ movptr(buf_param, Address(rbp, 8 + 0)); \/\/ do it last because it override rbp\n-    __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n-\n-    __ addptr(rsp, 3 * wordSize);\n-    __ pop(rbp);\n-    __ pop(rsi);\n-    __ pop(rdi);\n-    __ pop(rbx);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address generate_upper_word_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0x00000000, relocInfo::none, 0);\n-    __ emit_data(0xFFFFFFFF, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  address generate_shuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x0c0d0e0f, relocInfo::none, 0);\n-    __ emit_data(0x08090a0b, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha1_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf   = rax;\n-    Register state = rdx;\n-    Register ofs   = rcx;\n-    Register limit = rdi;\n-\n-    const Address  buf_param(rbp, 8 + 0);\n-    const Address  state_param(rbp, 8 + 4);\n-    const Address  ofs_param(rbp, 8 + 8);\n-    const Address  limit_param(rbp, 8 + 12);\n-\n-    const XMMRegister abcd = xmm0;\n-    const XMMRegister e0 = xmm1;\n-    const XMMRegister e1 = xmm2;\n-    const XMMRegister msg0 = xmm3;\n-\n-    const XMMRegister msg1 = xmm4;\n-    const XMMRegister msg2 = xmm5;\n-    const XMMRegister msg3 = xmm6;\n-    const XMMRegister shuf_mask = xmm7;\n-\n-    __ enter();\n-    __ subptr(rsp, 8 * wordSize);\n-    handleSOERegisters(true \/*saving*\/);\n-\n-    __ movptr(buf, buf_param);\n-    __ movptr(state, state_param);\n-    if (multi_block) {\n-      __ movptr(ofs, ofs_param);\n-      __ movptr(limit, limit_param);\n-    }\n-\n-    __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n-      buf, state, ofs, limit, rsp, multi_block);\n-\n-    handleSOERegisters(false \/*restoring*\/);\n-    __ addptr(rsp, 8 * wordSize);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  address generate_pshuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data(0x00010203, relocInfo::none, 0);\n-    __ emit_data(0x04050607, relocInfo::none, 0);\n-    __ emit_data(0x08090a0b, relocInfo::none, 0);\n-    __ emit_data(0x0c0d0e0f, relocInfo::none, 0);\n-    return start;\n-  }\n-\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n- address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = rbx;\n-    Register state = rsi;\n-    Register ofs = rdx;\n-    Register limit = rcx;\n-\n-    const Address  buf_param(rbp, 8 + 0);\n-    const Address  state_param(rbp, 8 + 4);\n-    const Address  ofs_param(rbp, 8 + 8);\n-    const Address  limit_param(rbp, 8 + 12);\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    __ enter();\n-    __ subptr(rsp, 8 * wordSize);\n-    handleSOERegisters(true \/*saving*\/);\n-    __ movptr(buf, buf_param);\n-    __ movptr(state, state_param);\n-    if (multi_block) {\n-     __ movptr(ofs, ofs_param);\n-     __ movptr(limit, limit_param);\n-    }\n-\n-    __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-      buf, state, ofs, limit, rsp, multi_block);\n-\n-    handleSOERegisters(false);\n-    __ addptr(rsp, 8 * wordSize);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ byte swap x86 long\n-  address ghash_long_swap_mask_addr() {\n-    return (address)GHASH_LONG_SWAP_MASK;\n-  }\n-\n-  \/\/ byte swap x86 byte array\n-  address ghash_byte_swap_mask_addr() {\n-    return (address)GHASH_BYTE_SWAP_MASK;\n-  }\n-\n-  \/* Single and multi-block ghash operations *\/\n-  address generate_ghash_processBlocks() {\n-    assert(UseGHASHIntrinsics, \"need GHASH intrinsics and CLMUL support\");\n-    __ align(CodeEntryAlignment);\n-    Label L_ghash_loop, L_exit;\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n-\n-    const Register state        = rdi;\n-    const Register subkeyH      = rsi;\n-    const Register data         = rdx;\n-    const Register blocks       = rcx;\n-\n-    const Address  state_param(rbp, 8+0);\n-    const Address  subkeyH_param(rbp, 8+4);\n-    const Address  data_param(rbp, 8+8);\n-    const Address  blocks_param(rbp, 8+12);\n-\n-    const XMMRegister xmm_temp0 = xmm0;\n-    const XMMRegister xmm_temp1 = xmm1;\n-    const XMMRegister xmm_temp2 = xmm2;\n-    const XMMRegister xmm_temp3 = xmm3;\n-    const XMMRegister xmm_temp4 = xmm4;\n-    const XMMRegister xmm_temp5 = xmm5;\n-    const XMMRegister xmm_temp6 = xmm6;\n-    const XMMRegister xmm_temp7 = xmm7;\n-\n-    __ enter();\n-    handleSOERegisters(true);  \/\/ Save registers\n-\n-    __ movptr(state, state_param);\n-    __ movptr(subkeyH, subkeyH_param);\n-    __ movptr(data, data_param);\n-    __ movptr(blocks, blocks_param);\n-\n-    __ movdqu(xmm_temp0, Address(state, 0));\n-    __ pshufb(xmm_temp0, ExternalAddress(ghash_long_swap_mask_addr()));\n-\n-    __ movdqu(xmm_temp1, Address(subkeyH, 0));\n-    __ pshufb(xmm_temp1, ExternalAddress(ghash_long_swap_mask_addr()));\n-\n-    __ BIND(L_ghash_loop);\n-    __ movdqu(xmm_temp2, Address(data, 0));\n-    __ pshufb(xmm_temp2, ExternalAddress(ghash_byte_swap_mask_addr()));\n-\n-    __ pxor(xmm_temp0, xmm_temp2);\n-\n-    \/\/\n-    \/\/ Multiply with the hash key\n-    \/\/\n-    __ movdqu(xmm_temp3, xmm_temp0);\n-    __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n-    __ movdqu(xmm_temp4, xmm_temp0);\n-    __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n-\n-    __ movdqu(xmm_temp5, xmm_temp0);\n-    __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n-    __ movdqu(xmm_temp6, xmm_temp0);\n-    __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n-\n-    __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n-\n-    __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n-    __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n-    __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n-    __ pxor(xmm_temp3, xmm_temp5);\n-    __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n-                                        \/\/ of the carry-less multiplication of\n-                                        \/\/ xmm0 by xmm1.\n-\n-    \/\/ We shift the result of the multiplication by one bit position\n-    \/\/ to the left to cope for the fact that the bits are reversed.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp4, xmm_temp6);\n-    __ pslld (xmm_temp3, 1);\n-    __ pslld(xmm_temp6, 1);\n-    __ psrld(xmm_temp7, 31);\n-    __ psrld(xmm_temp4, 31);\n-    __ movdqu(xmm_temp5, xmm_temp7);\n-    __ pslldq(xmm_temp4, 4);\n-    __ pslldq(xmm_temp7, 4);\n-    __ psrldq(xmm_temp5, 12);\n-    __ por(xmm_temp3, xmm_temp7);\n-    __ por(xmm_temp6, xmm_temp4);\n-    __ por(xmm_temp6, xmm_temp5);\n-\n-    \/\/\n-    \/\/ First phase of the reduction\n-    \/\/\n-    \/\/ Move xmm3 into xmm4, xmm5, xmm7 in order to perform the shifts\n-    \/\/ independently.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp4, xmm_temp3);\n-    __ movdqu(xmm_temp5, xmm_temp3);\n-    __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n-    __ pslld(xmm_temp4, 30);    \/\/ packed right shift shifting << 30\n-    __ pslld(xmm_temp5, 25);    \/\/ packed right shift shifting << 25\n-    __ pxor(xmm_temp7, xmm_temp4);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp7, xmm_temp5);\n-    __ movdqu(xmm_temp4, xmm_temp7);\n-    __ pslldq(xmm_temp7, 12);\n-    __ psrldq(xmm_temp4, 4);\n-    __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n-\n-    \/\/\n-    \/\/ Second phase of the reduction\n-    \/\/\n-    \/\/ Make 3 copies of xmm3 in xmm2, xmm5, xmm7 for doing these\n-    \/\/ shift operations.\n-    __ movdqu(xmm_temp2, xmm_temp3);\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp5, xmm_temp3);\n-    __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n-    __ psrld(xmm_temp7, 2);     \/\/ packed left shifting >> 2\n-    __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n-    __ pxor(xmm_temp2, xmm_temp7);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp2, xmm_temp5);\n-    __ pxor(xmm_temp2, xmm_temp4);\n-    __ pxor(xmm_temp3, xmm_temp2);\n-    __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n-\n-    __ decrement(blocks);\n-    __ jcc(Assembler::zero, L_exit);\n-    __ movdqu(xmm_temp0, xmm_temp6);\n-    __ addptr(data, 16);\n-    __ jmp(L_ghash_loop);\n-\n-    __ BIND(L_exit);\n-       \/\/ Byte swap 16-byte result\n-    __ pshufb(xmm_temp6, ExternalAddress(ghash_long_swap_mask_addr()));\n-    __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n-\n-    handleSOERegisters(false);  \/\/ restore registers\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/**\n-   *  Arguments:\n-   *\n-   * Inputs:\n-   *   rsp(4)   - int crc\n-   *   rsp(8)   - byte* buf\n-   *   rsp(12)  - int length\n-   *\n-   * Output:\n-   *       rax   - int crc result\n-   *\/\n-  address generate_updateBytesCRC32() {\n-    assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n-\n-    address start = __ pc();\n-\n-    const Register crc   = rdx;  \/\/ crc\n-    const Register buf   = rsi;  \/\/ source java byte array address\n-    const Register len   = rcx;  \/\/ length\n-    const Register table = rdi;  \/\/ crc_table address (reuse register)\n-    const Register tmp   = rbx;\n-    assert_different_registers(crc, buf, len, table, tmp, rax);\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ push(rsi);\n-    __ push(rdi);\n-    __ push(rbx);\n-\n-    Address crc_arg(rbp, 8 + 0);\n-    Address buf_arg(rbp, 8 + 4);\n-    Address len_arg(rbp, 8 + 8);\n-\n-    \/\/ Load up:\n-    __ movl(crc,   crc_arg);\n-    __ movptr(buf, buf_arg);\n-    __ movl(len,   len_arg);\n-\n-    __ kernel_crc32(crc, buf, len, table, tmp);\n-\n-    __ movl(rax, crc);\n-    __ pop(rbx);\n-    __ pop(rdi);\n-    __ pop(rsi);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/**\n-  *  Arguments:\n-  *\n-  * Inputs:\n-  *   rsp(4)   - int crc\n-  *   rsp(8)   - byte* buf\n-  *   rsp(12)  - int length\n-  *   rsp(16)  - table_start - optional (present only when doing a library_calll,\n-  *              not used by x86 algorithm)\n-  *\n-  * Output:\n-  *       rax  - int crc result\n-  *\/\n-  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n-    assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n-    address start = __ pc();\n-    const Register crc = rax;  \/\/ crc\n-    const Register buf = rcx;  \/\/ source java byte array address\n-    const Register len = rdx;  \/\/ length\n-    const Register d = rbx;\n-    const Register g = rsi;\n-    const Register h = rdi;\n-    const Register empty = noreg; \/\/ will never be used, in order not\n-                                  \/\/ to change a signature for crc32c_IPL_Alg2_Alt2\n-                                  \/\/ between 64\/32 I'm just keeping it here\n-    assert_different_registers(crc, buf, len, d, g, h);\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    Address crc_arg(rsp, 4 + 4 + 0); \/\/ ESP+4 +\n-                                     \/\/ we need to add additional 4 because __ enter\n-                                     \/\/ have just pushed ebp on a stack\n-    Address buf_arg(rsp, 4 + 4 + 4);\n-    Address len_arg(rsp, 4 + 4 + 8);\n-      \/\/ Load up:\n-      __ movl(crc, crc_arg);\n-      __ movl(buf, buf_arg);\n-      __ movl(len, len_arg);\n-      __ push(d);\n-      __ push(g);\n-      __ push(h);\n-      __ crc32c_ipl_alg2_alt2(crc, buf, len,\n-                              d, g, h,\n-                              empty, empty, empty,\n-                              xmm0, xmm1, xmm2,\n-                              is_pclmulqdq_supported);\n-      __ pop(h);\n-      __ pop(g);\n-      __ pop(d);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n- address generate_libmExp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmExp\");\n-\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ fast_exp(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, rbx);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-\n-  }\n-\n- address generate_libmLog() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libmLog\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ fast_log(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-               rax, rcx, rdx, rbx);\n-   __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ ret(0);\n-\n-   return start;\n-\n- }\n-\n- address generate_libmLog10() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libmLog10\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ fast_log10(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-               rax, rcx, rdx, rbx);\n-   __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ ret(0);\n-\n-   return start;\n-\n- }\n-\n- address generate_libmPow() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libmPow\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ fast_pow(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-               rax, rcx, rdx, rbx);\n-   __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ ret(0);\n-\n-   return start;\n-\n- }\n-\n- address generate_libm_reduce_pi04l() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libm_reduce_pi04l\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ libm_reduce_pi04l(rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n-\n-   return start;\n-\n- }\n-\n- address generate_libm_sin_cos_huge() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libm_sin_cos_huge\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ libm_sincos_huge(xmm0, xmm1, rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n-\n-   return start;\n-\n- }\n-\n- address generate_libmSin() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libmSin\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ fast_sin(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-               rax, rbx, rdx);\n-   __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ ret(0);\n-\n-   return start;\n-\n- }\n-\n- address generate_libmCos() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libmCos\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ fast_cos(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-               rax, rcx, rdx, rbx);\n-   __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ ret(0);\n-\n-   return start;\n-\n- }\n-\n- address generate_libm_tan_cot_huge() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libm_tan_cot_huge\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ libm_tancot_huge(xmm0, xmm1, rax, rcx, rdx, rbx, rsi, rdi, rbp, rsp);\n-\n-   return start;\n-\n- }\n-\n- address generate_libmTan() {\n-   StubCodeMark mark(this, \"StubRoutines\", \"libmTan\");\n-\n-   address start = __ pc();\n-\n-   BLOCK_COMMENT(\"Entry:\");\n-   __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ fast_tan(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-               rax, rcx, rdx, rbx);\n-   __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-   __ ret(0);\n-\n-   return start;\n-\n- }\n-\n-  address generate_method_entry_barrier() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n-\n-    Label deoptimize_label;\n-\n-    address start = __ pc();\n-\n-    __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ save rbp\n-\n-    \/\/ save rbx, because we want to use that value.\n-    \/\/ We could do without it but then we depend on the number of slots used by pusha\n-    __ push(rbx);\n-\n-    __ lea(rbx, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for rbx - this should be the return address\n-\n-    __ pusha();\n-\n-    \/\/ xmm0 and xmm1 may be used for passing float\/double arguments\n-\n-    if (UseSSE >= 2) {\n-      const int xmm_size = wordSize * 4;\n-      __ subptr(rsp, xmm_size * 2);\n-      __ movdbl(Address(rsp, xmm_size * 1), xmm1);\n-      __ movdbl(Address(rsp, xmm_size * 0), xmm0);\n-    } else if (UseSSE >= 1) {\n-      const int xmm_size = wordSize * 2;\n-      __ subptr(rsp, xmm_size * 2);\n-      __ movflt(Address(rsp, xmm_size * 1), xmm1);\n-      __ movflt(Address(rsp, xmm_size * 0), xmm0);\n-    }\n-\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), rbx);\n-\n-    if (UseSSE >= 2) {\n-      const int xmm_size = wordSize * 4;\n-      __ movdbl(xmm0, Address(rsp, xmm_size * 0));\n-      __ movdbl(xmm1, Address(rsp, xmm_size * 1));\n-      __ addptr(rsp, xmm_size * 2);\n-    } else if (UseSSE >= 1) {\n-      const int xmm_size = wordSize * 2;\n-      __ movflt(xmm0, Address(rsp, xmm_size * 0));\n-      __ movflt(xmm1, Address(rsp, xmm_size * 1));\n-      __ addptr(rsp, xmm_size * 2);\n-    }\n-\n-    __ cmpl(rax, 1); \/\/ 1 means deoptimize\n-    __ jcc(Assembler::equal, deoptimize_label);\n-\n-    __ popa();\n-    __ pop(rbx);\n-\n-    __ leave();\n-\n-    __ addptr(rsp, 1 * wordSize); \/\/ cookie\n-    __ ret(0);\n-\n-    __ BIND(deoptimize_label);\n-\n-    __ popa();\n-    __ pop(rbx);\n-\n-    __ leave();\n-\n-    \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n-    \/\/ here while still having a correct stack is valuable\n-    __ testptr(rsp, Address(rsp, 0));\n-\n-    __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n-    __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n-\n-    return start;\n-  }\n-\n- private:\n-\n-  void create_control_words() {\n-    \/\/ Round to nearest, 53-bit mode, exceptions masked\n-    StubRoutines::x86::_fpu_cntrl_wrd_std   = 0x027F;\n-    \/\/ Round to zero, 53-bit mode, exception mased\n-    StubRoutines::x86::_fpu_cntrl_wrd_trunc = 0x0D7F;\n-    \/\/ Round to nearest, 24-bit mode, exceptions masked\n-    StubRoutines::x86::_fpu_cntrl_wrd_24    = 0x007F;\n-    \/\/ Round to nearest, 64-bit mode, exceptions masked, flags specialized\n-    StubRoutines::x86::_mxcsr_std           = EnableX86ECoreOpts ? 0x1FBF : 0x1F80;\n-    \/\/ Note: the following two constants are 80-bit values\n-    \/\/       layout is critical for correct loading by FPU.\n-    \/\/ Bias for strict fp multiply\/divide\n-    StubRoutines::x86::_fpu_subnormal_bias1[0]= 0x00000000; \/\/ 2^(-15360) == 0x03ff 8000 0000 0000 0000\n-    StubRoutines::x86::_fpu_subnormal_bias1[1]= 0x80000000;\n-    StubRoutines::x86::_fpu_subnormal_bias1[2]= 0x03ff;\n-    \/\/ Un-Bias for strict fp multiply\/divide\n-    StubRoutines::x86::_fpu_subnormal_bias2[0]= 0x00000000; \/\/ 2^(+15360) == 0x7bff 8000 0000 0000 0000\n-    StubRoutines::x86::_fpu_subnormal_bias2[1]= 0x80000000;\n-    StubRoutines::x86::_fpu_subnormal_bias2[2]= 0x7bff;\n-  }\n-\n-  address generate_cont_thaw() {\n-    if (!Continuations::enabled()) return nullptr;\n-    Unimplemented();\n-    return nullptr;\n-  }\n-\n-  address generate_cont_returnBarrier() {\n-    if (!Continuations::enabled()) return nullptr;\n-    Unimplemented();\n-    return nullptr;\n-  }\n-\n-  address generate_cont_returnBarrier_exception() {\n-    if (!Continuations::enabled()) return nullptr;\n-    Unimplemented();\n-    return nullptr;\n-  }\n-\n-  \/\/---------------------------------------------------------------------------\n-  \/\/ Initialization\n-\n-  void generate_initial_stubs() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/------------------------------------------------------------------------------------------------------------------------\n-    \/\/ entry points that exist in all platforms\n-    \/\/ Note: This is code that could be shared among different platforms - however the benefit seems to be smaller than\n-    \/\/       the disadvantage of having a much more complicated generator structure. See also comment in stubRoutines.hpp.\n-    StubRoutines::_forward_exception_entry      = generate_forward_exception();\n-\n-    StubRoutines::_call_stub_entry              =\n-      generate_call_stub(StubRoutines::_call_stub_return_address);\n-    \/\/ is referenced by megamorphic call\n-    StubRoutines::_catch_exception_entry        = generate_catch_exception();\n-\n-    \/\/ platform dependent\n-    create_control_words();\n-\n-    \/\/ Initialize table for copy memory (arraycopy) check.\n-    if (UnsafeMemoryAccess::_table == nullptr) {\n-      UnsafeMemoryAccess::create_table(16 + 4); \/\/ 16 for copyMemory; 4 for setMemory\n-    }\n-\n-    StubRoutines::x86::_verify_mxcsr_entry         = generate_verify_mxcsr();\n-    StubRoutines::x86::_verify_fpu_cntrl_wrd_entry = generate_verify_fpu_cntrl_wrd();\n-    StubRoutines::x86::_d2i_wrapper                = generate_d2i_wrapper(T_INT,  CAST_FROM_FN_PTR(address, SharedRuntime::d2i));\n-    StubRoutines::x86::_d2l_wrapper                = generate_d2i_wrapper(T_LONG, CAST_FROM_FN_PTR(address, SharedRuntime::d2l));\n-\n-    if (UseCRC32Intrinsics) {\n-      \/\/ set table address before stub generation which use it\n-      StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n-      StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n-    }\n-\n-    if (UseCRC32CIntrinsics) {\n-      bool supports_clmul = VM_Version::supports_clmul();\n-      StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n-      StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n-      StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n-    }\n-    if (VM_Version::supports_sse2() && UseLibmIntrinsic && InlineIntrinsics) {\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n-        StubRoutines::_dexp = generate_libmExp();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n-        StubRoutines::_dlog = generate_libmLog();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n-        StubRoutines::_dlog10 = generate_libmLog10();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n-        StubRoutines::_dpow = generate_libmPow();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||\n-        vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos) ||\n-        vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::_dlibm_reduce_pi04l = generate_libm_reduce_pi04l();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||\n-        vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n-        StubRoutines::_dlibm_sin_cos_huge = generate_libm_sin_cos_huge();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n-        StubRoutines::_dsin = generate_libmSin();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n-        StubRoutines::_dcos = generate_libmCos();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::_dlibm_tan_cot_huge = generate_libm_tan_cot_huge();\n-        StubRoutines::_dtan = generate_libmTan();\n-      }\n-    }\n-  }\n-\n-  void generate_continuation_stubs() {\n-    \/\/ Continuation stubs:\n-    StubRoutines::_cont_thaw          = generate_cont_thaw();\n-    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n-    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n-  }\n-\n-  void generate_final_stubs() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != nullptr) {\n-      StubRoutines::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n-  }\n-\n-  void generate_compiler_stubs() {\n-#if COMPILER2_OR_JVMCI\n-\n-    \/\/ entry points that are C2\/JVMCI specific\n-\n-    StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF);\n-    StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x80000000);\n-    StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask_long_double(\"vector_double_sign_mask\", 0x7FFFFFFF, 0xFFFFFFFF);\n-    StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask_long_double(\"vector_double_sign_flip\", 0x80000000, 0x00000000);\n-    StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff);\n-    StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff);\n-    StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff);\n-    StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0, 0, 0);\n-    StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n-    StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x03020100);\n-    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n-    StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x01000100);\n-    StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask_long_double(\"vector_long_shuffle_mask\", 0x00000001, 0x0);\n-    StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n-    StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask_long_double(\"vector_long_sign_mask\", 0x80000000, 0x00000000);\n-    StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFF);\n-    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x00000001);\n-    StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n-    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n-\n-    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n-      \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n-      StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n-    }\n-\n-    \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n-    if (UseAESIntrinsics) {\n-      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-      StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n-      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n-    }\n-\n-    if (UseAESCTRIntrinsics) {\n-      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n-    }\n-\n-    if (UseMD5Intrinsics) {\n-      StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n-      StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n-    }\n-    if (UseSHA1Intrinsics) {\n-      StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n-      StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n-      StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n-      StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n-    }\n-    if (UseSHA256Intrinsics) {\n-      StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n-      StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n-    }\n-\n-    \/\/ Generate GHASH intrinsics code\n-    if (UseGHASHIntrinsics) {\n-      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n-    }\n-#endif \/\/ COMPILER2_OR_JVMCI\n-  }\n-\n-\n- public:\n-  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n-    switch(kind) {\n-    case Initial_stubs:\n-      generate_initial_stubs();\n-      break;\n-     case Continuation_stubs:\n-      generate_continuation_stubs();\n-      break;\n-    case Compiler_stubs:\n-      generate_compiler_stubs();\n-      break;\n-    case Final_stubs:\n-      generate_final_stubs();\n-      break;\n-    default:\n-      fatal(\"unexpected stubs kind: %d\", kind);\n-      break;\n-    };\n-  }\n-}; \/\/ end class declaration\n-\n-void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n-  StubGenerator g(code, kind);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":0,"deletions":4083,"binary":false,"changes":4083,"status":"deleted"},{"patch":"@@ -64,1 +64,0 @@\n-#ifdef _LP64\n@@ -89,1 +88,0 @@\n-#endif\n@@ -187,1 +185,0 @@\n-#ifdef _LP64\n@@ -234,1 +231,0 @@\n-#endif \/\/ _LP64\n@@ -397,1 +393,0 @@\n-#ifdef _LP64\n@@ -446,1 +441,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-  _continuation_stubs_code_size =  1000 LP64_ONLY(+2000),\n+  _continuation_stubs_code_size = 3000,\n@@ -40,2 +40,2 @@\n-  _compiler_stubs_code_size     = 20000 LP64_ONLY(+47000) WINDOWS_ONLY(+2000),\n-  _final_stubs_code_size        = 10000 LP64_ONLY(+20000) WINDOWS_ONLY(+22000) ZGC_ONLY(+20000)\n+  _compiler_stubs_code_size     = 67000 WINDOWS_ONLY(+2000),\n+  _final_stubs_code_size        = 30000 WINDOWS_ONLY(+22000) ZGC_ONLY(+20000)\n@@ -48,1 +48,0 @@\n-#ifdef _LP64\n@@ -104,27 +103,0 @@\n-#else \/\/ !LP64\n-\n- private:\n-  static address _verify_fpu_cntrl_wrd_entry;\n-  static address _d2i_wrapper;\n-  static address _d2l_wrapper;\n-\n-  static jint    _fpu_cntrl_wrd_std;\n-  static jint    _fpu_cntrl_wrd_24;\n-  static jint    _fpu_cntrl_wrd_trunc;\n-\n-  static jint    _fpu_subnormal_bias1[3];\n-  static jint    _fpu_subnormal_bias2[3];\n-\n- public:\n-  static address verify_fpu_cntrl_wrd_entry() { return _verify_fpu_cntrl_wrd_entry; }\n-  static address d2i_wrapper()                { return _d2i_wrapper; }\n-  static address d2l_wrapper()                { return _d2l_wrapper; }\n-  static address addr_fpu_cntrl_wrd_std()     { return (address)&_fpu_cntrl_wrd_std;   }\n-  static address addr_fpu_cntrl_wrd_24()      { return (address)&_fpu_cntrl_wrd_24;    }\n-  static address addr_fpu_cntrl_wrd_trunc()   { return (address)&_fpu_cntrl_wrd_trunc; }\n-  static address addr_fpu_subnormal_bias1()   { return (address)&_fpu_subnormal_bias1; }\n-  static address addr_fpu_subnormal_bias2()   { return (address)&_fpu_subnormal_bias2; }\n-\n-  static jint    fpu_cntrl_wrd_std()          { return _fpu_cntrl_wrd_std; }\n-#endif \/\/ !LP64\n-\n@@ -133,1 +105,0 @@\n-#ifdef _LP64\n@@ -135,1 +106,0 @@\n-#endif \/\/ _LP64\n@@ -142,1 +112,0 @@\n-#ifdef _LP64\n@@ -147,1 +116,0 @@\n-#endif \/\/ _LP64\n@@ -185,1 +153,0 @@\n-#ifdef _LP64\n@@ -209,1 +176,0 @@\n-#endif\n@@ -215,1 +181,0 @@\n-#ifdef _LP64\n@@ -217,1 +182,0 @@\n-#endif \/\/ _LP64\n@@ -220,1 +184,0 @@\n-#ifdef _LP64\n@@ -225,1 +188,0 @@\n-#endif \/\/ _LP64\n@@ -325,1 +287,0 @@\n-#ifdef _LP64\n@@ -349,1 +310,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":3,"deletions":43,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -1,47 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"runtime\/deoptimization.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-\n-\/\/ Implementation of the platform-specific part of StubRoutines - for\n-\/\/ a description of how to extend it, see the stubRoutines.hpp file.\n-\n-address StubRoutines::x86::_verify_fpu_cntrl_wrd_entry = nullptr;\n-\n-address StubRoutines::x86::_d2i_wrapper = nullptr;\n-address StubRoutines::x86::_d2l_wrapper = nullptr;\n-\n-jint StubRoutines::x86::_fpu_cntrl_wrd_std   = 0;\n-jint StubRoutines::x86::_fpu_cntrl_wrd_24    = 0;\n-jint StubRoutines::x86::_fpu_cntrl_wrd_trunc = 0;\n-\n-jint StubRoutines::x86::_mxcsr_std = 0;\n-\n-jint StubRoutines::x86::_fpu_subnormal_bias1[3] = { 0, 0, 0 };\n-jint StubRoutines::x86::_fpu_subnormal_bias2[3] = { 0, 0, 0 };\n-\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86_32.cpp","additions":0,"deletions":47,"binary":false,"changes":47,"status":"deleted"},{"patch":"@@ -67,1 +67,0 @@\n-#ifdef AMD64\n@@ -69,3 +68,0 @@\n-#else\n-int TemplateInterpreter::InterpreterCodeSize = 224 * 1024;\n-#endif \/\/ AMD64\n@@ -74,2 +70,2 @@\n-static const Register rbcp     = LP64_ONLY(r13) NOT_LP64(rsi);\n-static const Register rlocals  = LP64_ONLY(r14) NOT_LP64(rdi);\n+static const Register rbcp     = r13;\n+static const Register rlocals  = r14;\n@@ -124,1 +120,0 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n@@ -129,1 +124,1 @@\n-             rarg, rbx);\n+             c_rarg1, rbx);\n@@ -137,2 +132,1 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  __ pop(rarg);\n+  __ pop(c_rarg1);\n@@ -148,1 +142,1 @@\n-             rarg);\n+             c_rarg1);\n@@ -157,2 +151,2 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  Register rarg2 = NOT_LP64(rbx) LP64_ONLY(c_rarg2);\n+  Register rarg = c_rarg1;\n+  Register rarg2 = c_rarg2;\n@@ -188,24 +182,0 @@\n-#ifndef _LP64\n-#ifdef COMPILER2\n-  \/\/ The FPU stack is clean if UseSSE >= 2 but must be cleaned in other cases\n-  if ((state == ftos && UseSSE < 1) || (state == dtos && UseSSE < 2)) {\n-    for (int i = 1; i < 8; i++) {\n-        __ ffree(i);\n-    }\n-  } else if (UseSSE < 2) {\n-    __ empty_FPU_stack();\n-  }\n-#endif \/\/ COMPILER2\n-  if ((state == ftos && UseSSE < 1) || (state == dtos && UseSSE < 2)) {\n-    __ MacroAssembler::verify_FPU(1, \"generate_return_entry_for compiled\");\n-  } else {\n-    __ MacroAssembler::verify_FPU(0, \"generate_return_entry_for compiled\");\n-  }\n-\n-  if (state == ftos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 1 ? 0 : 1, \"generate_return_entry_for in interpreter\");\n-  } else if (state == dtos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 2 ? 0 : 1, \"generate_return_entry_for in interpreter\");\n-  }\n-#endif \/\/ _LP64\n-\n@@ -240,4 +210,2 @@\n-   const Register java_thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-   if (JvmtiExport::can_pop_frame()) {\n-     NOT_LP64(__ get_thread(java_thread));\n-     __ check_and_handle_popframe(java_thread);\n+  if (JvmtiExport::can_pop_frame()) {\n+     __ check_and_handle_popframe(r15_thread);\n@@ -246,2 +214,1 @@\n-     NOT_LP64(__ get_thread(java_thread));\n-     __ check_and_handle_earlyret(java_thread);\n+     __ check_and_handle_earlyret(r15_thread);\n@@ -259,8 +226,0 @@\n-#ifndef _LP64\n-  if (state == ftos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 1 ? 0 : 1, \"generate_deopt_entry_for in interpreter\");\n-  } else if (state == dtos) {\n-    __ MacroAssembler::verify_FPU(UseSSE >= 2 ? 0 : 1, \"generate_deopt_entry_for in interpreter\");\n-  }\n-#endif \/\/ _LP64\n-\n@@ -271,2 +230,1 @@\n-  const Register thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n+  const Register thread = r15_thread;\n@@ -323,3 +281,0 @@\n-#ifndef _LP64\n-  case T_CHAR   : __ andptr(rax, 0xFFFF);    break;\n-#else\n@@ -327,1 +282,0 @@\n-#endif \/\/ _LP64\n@@ -333,27 +287,0 @@\n-#ifndef _LP64\n-  case T_DOUBLE :\n-  case T_FLOAT  :\n-    { const Register t = InterpreterRuntime::SignatureHandlerGenerator::temp();\n-      __ pop(t);                            \/\/ remove return address first\n-      \/\/ Must return a result for interpreter or compiler. In SSE\n-      \/\/ mode, results are returned in xmm0 and the FPU stack must\n-      \/\/ be empty.\n-      if (type == T_FLOAT && UseSSE >= 1) {\n-        \/\/ Load ST0\n-        __ fld_d(Address(rsp, 0));\n-        \/\/ Store as float and empty fpu stack\n-        __ fstp_s(Address(rsp, 0));\n-        \/\/ and reload\n-        __ movflt(xmm0, Address(rsp, 0));\n-      } else if (type == T_DOUBLE && UseSSE >= 2 ) {\n-        __ movdbl(xmm0, Address(rsp, 0));\n-      } else {\n-        \/\/ restore ST0\n-        __ fld_d(Address(rsp, 0));\n-      }\n-      \/\/ and pop the temp\n-      __ addptr(rsp, 2 * wordSize);\n-      __ push(t);                           \/\/ restore return address\n-    }\n-    break;\n-#else\n@@ -362,1 +289,0 @@\n-#endif \/\/ _LP64\n@@ -471,2 +397,1 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  __ movl(rarg, 0);\n+  __ movl(c_rarg1, 0);\n@@ -476,1 +401,1 @@\n-             rarg);\n+             c_rarg1);\n@@ -527,5 +452,0 @@\n-  const Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-#ifndef _LP64\n-  __ push(thread);\n-  __ get_thread(thread);\n-#endif\n@@ -533,1 +453,1 @@\n-  const Address stack_limit(thread, JavaThread::stack_overflow_limit_offset());\n+  const Address stack_limit(r15_thread, JavaThread::stack_overflow_limit_offset());\n@@ -556,1 +476,0 @@\n-  NOT_LP64(__ pop(rsi));  \/\/ get saved bcp\n@@ -572,1 +491,0 @@\n-  NOT_LP64(__ pop(rsi));\n@@ -635,1 +553,1 @@\n-  const Register lockreg = NOT_LP64(rdx) LP64_ONLY(c_rarg1);\n+  const Register lockreg = c_rarg1;\n@@ -734,1 +652,0 @@\n-  NOT_LP64(__ push(rsi));\n@@ -741,2 +658,1 @@\n-  const Register sender_sp = NOT_LP64(rsi) LP64_ONLY(r13);\n-  NOT_LP64(__ pop(rsi));      \/\/ get sender sp\n+  const Register sender_sp = r13;\n@@ -768,5 +684,1 @@\n-  const Register thread = NOT_LP64(rsi) LP64_ONLY(r15_thread);\n-#ifndef _LP64\n-  __ push(thread);\n-  __ get_thread(thread);\n-#endif\n+  const Register thread = r15_thread;\n@@ -804,4 +716,0 @@\n-\n-#ifndef _LP64\n-  __ pop(thread);\n-#endif\n@@ -881,3 +789,1 @@\n-  const Register thread1 = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread1));\n-  const Address do_not_unlock_if_synchronized(thread1,\n+  const Address do_not_unlock_if_synchronized(r15_thread,\n@@ -899,1 +805,0 @@\n-  NOT_LP64(__ get_thread(thread1));\n@@ -941,2 +846,2 @@\n-  const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n-  const Register t      = NOT_LP64(rcx) LP64_ONLY(r11);\n+  const Register thread = r15_thread;\n+  const Register t      = r11;\n@@ -949,6 +854,0 @@\n-#ifndef _LP64\n-  __ shlptr(t, Interpreter::logStackElementSize); \/\/ Convert parameter count to bytes.\n-  __ addptr(t, 2*wordSize);     \/\/ allocate two more slots for JNIEnv and possible mirror\n-  __ subptr(rsp, t);\n-  __ andptr(rsp, -(StackAlignmentInBytes)); \/\/ gcc needs 16 byte aligned stacks to do XMM intrinsics\n-#else\n@@ -960,1 +859,0 @@\n-#endif \/\/ _LP64\n@@ -982,1 +880,1 @@\n-  assert(InterpreterRuntime::SignatureHandlerGenerator::temp() == NOT_LP64(t) LP64_ONLY(rscratch1),\n+  assert(InterpreterRuntime::SignatureHandlerGenerator::temp() == rscratch1,\n@@ -1011,4 +909,0 @@\n-#ifndef _LP64\n-    __ lea(t, Address(rbp, frame::interpreter_frame_oop_temp_offset * wordSize));\n-    __ movptr(Address(rsp, wordSize), t);\n-#else\n@@ -1017,1 +911,0 @@\n-#endif \/\/ _LP64\n@@ -1038,10 +931,0 @@\n-#ifndef _LP64\n-   __ get_thread(thread);\n-   __ lea(t, Address(thread, JavaThread::jni_environment_offset()));\n-   __ movptr(Address(rsp, 0), t);\n-\n-   \/\/ set_last_Java_frame_before_call\n-   \/\/ It is enough that the pc()\n-   \/\/ points into the right code segment. It does not have to be the correct return pc.\n-   __ set_last_Java_frame(thread, noreg, rbp, __ pc(), noreg);\n-#else\n@@ -1056,1 +939,0 @@\n-#endif \/\/ _LP64\n@@ -1092,26 +974,0 @@\n-#ifndef _LP64\n-  \/\/ save potential result in ST(0) & rdx:rax\n-  \/\/ (if result handler is the T_FLOAT or T_DOUBLE handler, result must be in ST0 -\n-  \/\/ the check is necessary to avoid potential Intel FPU overflow problems by saving\/restoring 'empty' FPU registers)\n-  \/\/ It is safe to do this push because state is _thread_in_native and return address will be found\n-  \/\/ via _last_native_pc and not via _last_jave_sp\n-\n-  \/\/ NOTE: the order of these push(es) is known to frame::interpreter_frame_result.\n-  \/\/ If the order changes or anything else is added to the stack the code in\n-  \/\/ interpreter_frame_result will have to be changed.\n-\n-  { Label L;\n-    Label push_double;\n-    ExternalAddress float_handler(AbstractInterpreter::result_handler(T_FLOAT));\n-    ExternalAddress double_handler(AbstractInterpreter::result_handler(T_DOUBLE));\n-    __ cmpptr(Address(rbp, (frame::interpreter_frame_result_handler_offset)*wordSize),\n-              float_handler.addr(), noreg);\n-    __ jcc(Assembler::equal, push_double);\n-    __ cmpptr(Address(rbp, (frame::interpreter_frame_result_handler_offset)*wordSize),\n-              double_handler.addr(), noreg);\n-    __ jcc(Assembler::notEqual, L);\n-    __ bind(push_double);\n-    __ push_d(); \/\/ FP values are returned using the FPU, so push FPU contents (even if UseSSE > 0).\n-    __ bind(L);\n-  }\n-#else\n@@ -1119,2 +975,0 @@\n-#endif \/\/ _LP64\n-\n@@ -1124,1 +978,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1134,6 +987,0 @@\n-#ifndef _LP64\n-  if (AlwaysRestoreFPU) {\n-    \/\/  Make sure the control word is correct.\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-  }\n-#endif \/\/ _LP64\n@@ -1159,7 +1006,0 @@\n-#ifndef _LP64\n-    __ push(thread);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address,\n-                                            JavaThread::check_special_condition_for_native_trans)));\n-    __ increment(rsp, wordSize);\n-    __ get_thread(thread);\n-#else\n@@ -1173,1 +1013,0 @@\n-#endif \/\/ _LP64\n@@ -1180,1 +1019,0 @@\n-#ifdef _LP64\n@@ -1196,1 +1034,0 @@\n-#endif \/\/ _LP64\n@@ -1238,4 +1075,0 @@\n-#ifndef _LP64\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n-    __ popa();\n-#else\n@@ -1249,1 +1082,0 @@\n-#endif \/\/ _LP64\n@@ -1297,1 +1129,1 @@\n-      const Register regmon = NOT_LP64(rdx) LP64_ONLY(c_rarg1);\n+      const Register regmon = c_rarg1;\n@@ -1329,1 +1161,1 @@\n-  LP64_ONLY( __ pop(dtos));\n+  __ pop(dtos);\n@@ -1458,2 +1290,1 @@\n-  const Register thread = NOT_LP64(rax) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n+  const Register thread = r15_thread;\n@@ -1478,1 +1309,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1545,1 +1375,1 @@\n-  LP64_ONLY(__ reinit_heapbase());  \/\/ restore r12 as heapbase.\n+  __ reinit_heapbase();  \/\/ restore r12 as heapbase.\n@@ -1552,2 +1382,2 @@\n-  Register rarg = NOT_LP64(rax) LP64_ONLY(c_rarg1);\n-  LP64_ONLY(__ mov(c_rarg1, rax));\n+  Register rarg = c_rarg1;\n+  __ mov(c_rarg1, rax);\n@@ -1592,2 +1422,1 @@\n-  const Register thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n+  const Register thread = r15_thread;\n@@ -1610,1 +1439,1 @@\n-    Register rarg = NOT_LP64(rdx) LP64_ONLY(c_rarg1);\n+    Register rarg = c_rarg1;\n@@ -1628,1 +1457,0 @@\n-    NOT_LP64(__ get_thread(thread));\n@@ -1641,1 +1469,0 @@\n-    NOT_LP64(__ get_thread(thread));\n@@ -1667,10 +1494,0 @@\n-#ifndef _LP64\n-  __ mov(rax, rsp);\n-  __ movptr(rbx, Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize));\n-  __ lea(rbx, Address(rbp, rbx, Address::times_ptr));\n-  __ get_thread(thread);\n-  \/\/ PC must point into interpreter here\n-  __ set_last_Java_frame(thread, noreg, rbp, __ pc(), noreg);\n-  __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, InterpreterRuntime::popframe_move_outgoing_args), thread, rax, rbx);\n-  __ get_thread(thread);\n-#else\n@@ -1683,1 +1500,0 @@\n-#endif\n@@ -1700,1 +1516,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1734,1 +1549,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1739,1 +1553,0 @@\n-  NOT_LP64(__ get_thread(thread));\n@@ -1775,2 +1588,1 @@\n-  const Register thread = NOT_LP64(rcx) LP64_ONLY(r15_thread);\n-  NOT_LP64(__ get_thread(thread));\n+  const Register thread = r15_thread;\n@@ -1808,8 +1620,0 @@\n-#ifndef _LP64\n-  fep = __ pc();     \/\/ ftos entry point\n-      __ push(ftos);\n-      __ jmpb(L);\n-  dep = __ pc();     \/\/ dtos entry point\n-      __ push(dtos);\n-      __ jmpb(L);\n-#else\n@@ -1822,1 +1626,0 @@\n-#endif \/\/ _LP64\n@@ -1841,13 +1644,0 @@\n-#ifndef _LP64\n-  \/\/ prepare expression stack\n-  __ pop(rcx);          \/\/ pop return address so expression stack is 'pure'\n-  __ push(state);       \/\/ save tosca\n-\n-  \/\/ pass tosca registers as arguments & call tracer\n-  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::trace_bytecode), rcx, rax, rdx);\n-  __ mov(rcx, rax);     \/\/ make sure return address is not destroyed by pop(state)\n-  __ pop(state);        \/\/ restore tosca\n-\n-  \/\/ return\n-  __ jmp(rcx);\n-#else\n@@ -1872,1 +1662,0 @@\n-#endif \/\/ _LP64\n@@ -1904,3 +1693,0 @@\n-#ifndef _LP64\n-  __ call(RuntimeAddress(Interpreter::trace_code(t->tos_in())));\n-#else\n@@ -1912,1 +1698,0 @@\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":30,"deletions":245,"binary":false,"changes":275,"status":"modified"},{"patch":"@@ -1,510 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"compiler\/disassembler.hpp\"\n-#include \"interpreter\/interp_masm.hpp\"\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"interpreter\/interpreterRuntime.hpp\"\n-#include \"interpreter\/templateInterpreterGenerator.hpp\"\n-#include \"runtime\/arguments.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"runtime\/stubRoutines.hpp\"\n-\n-#define __ Disassembler::hook<InterpreterMacroAssembler>(__FILE__, __LINE__, _masm)->\n-\n-\n-address TemplateInterpreterGenerator::generate_slow_signature_handler() {\n-  address entry = __ pc();\n-  \/\/ rbx,: method\n-  \/\/ rcx: temporary\n-  \/\/ rdi: pointer to locals\n-  \/\/ rsp: end of copied parameters area\n-  __ mov(rcx, rsp);\n-  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::slow_signature_handler), rbx, rdi, rcx);\n-  __ ret(0);\n-  return entry;\n-}\n-\n-\/**\n- * Method entry for static native methods:\n- *   int java.util.zip.CRC32.update(int crc, int b)\n- *\/\n-address TemplateInterpreterGenerator::generate_CRC32_update_entry() {\n-  assert(UseCRC32Intrinsics, \"this intrinsic is not supported\");\n-  address entry = __ pc();\n-\n-  \/\/ rbx: Method*\n-  \/\/ rsi: senderSP must preserved for slow path, set SP to it on fast path\n-  \/\/ rdx: scratch\n-  \/\/ rdi: scratch\n-\n-  Label slow_path;\n-  \/\/ If we need a safepoint check, generate full interpreter entry.\n-  __ get_thread(rdi);\n-  __ safepoint_poll(slow_path, rdi, false \/* at_return *\/, false \/* in_nmethod *\/);\n-\n-  \/\/ We don't generate local frame and don't align stack because\n-  \/\/ we call stub code and there is no safepoint on this path.\n-\n-  \/\/ Load parameters\n-  const Register crc = rax;  \/\/ crc\n-  const Register val = rdx;  \/\/ source java byte value\n-  const Register tbl = rdi;  \/\/ scratch\n-\n-  \/\/ Arguments are reversed on java expression stack\n-  __ movl(val, Address(rsp,   wordSize)); \/\/ byte value\n-  __ movl(crc, Address(rsp, 2*wordSize)); \/\/ Initial CRC\n-\n-  __ lea(tbl, ExternalAddress(StubRoutines::crc_table_addr()));\n-  __ notl(crc); \/\/ ~crc\n-  __ update_byte_crc32(crc, val, tbl);\n-  __ notl(crc); \/\/ ~crc\n-  \/\/ result in rax\n-\n-  \/\/ _areturn\n-  __ pop(rdi);                \/\/ get return address\n-  __ mov(rsp, rsi);           \/\/ set sp to sender sp\n-  __ jmp(rdi);\n-\n-  \/\/ generate a vanilla native entry as the slow path\n-  __ bind(slow_path);\n-  __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native));\n-  return entry;\n-}\n-\n-\/**\n- * Method entry for static native methods:\n- *   int java.util.zip.CRC32.updateBytes(int crc, byte[] b, int off, int len)\n- *   int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)\n- *\/\n-address TemplateInterpreterGenerator::generate_CRC32_updateBytes_entry(AbstractInterpreter::MethodKind kind) {\n-  assert(UseCRC32Intrinsics, \"this intrinsic is not supported\");\n-  address entry = __ pc();\n-\n-  \/\/ rbx,: Method*\n-  \/\/ rsi: senderSP must preserved for slow path, set SP to it on fast path\n-  \/\/ rdx: scratch\n-  \/\/ rdi: scratch\n-\n-  Label slow_path;\n-  \/\/ If we need a safepoint check, generate full interpreter entry.\n-  __ get_thread(rdi);\n-  __ safepoint_poll(slow_path, rdi, false \/* at_return *\/, false \/* in_nmethod *\/);\n-\n-  \/\/ We don't generate local frame and don't align stack because\n-  \/\/ we call stub code and there is no safepoint on this path.\n-\n-  \/\/ Load parameters\n-  const Register crc = rax;  \/\/ crc\n-  const Register buf = rdx;  \/\/ source java byte array address\n-  const Register len = rdi;  \/\/ length\n-\n-  \/\/ value              x86_32\n-  \/\/ interp. arg ptr    ESP + 4\n-  \/\/ int java.util.zip.CRC32.updateBytes(int crc, byte[] b, int off, int len)\n-  \/\/                                         3           2      1        0\n-  \/\/ int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)\n-  \/\/                                              4         2,3      1        0\n-\n-  \/\/ Arguments are reversed on java expression stack\n-  __ movl(len,   Address(rsp,   4 + 0)); \/\/ Length\n-  \/\/ Calculate address of start element\n-  if (kind == Interpreter::java_util_zip_CRC32_updateByteBuffer) {\n-    __ movptr(buf, Address(rsp, 4 + 2 * wordSize)); \/\/ long buf\n-    __ addptr(buf, Address(rsp, 4 + 1 * wordSize)); \/\/ + offset\n-    __ movl(crc,   Address(rsp, 4 + 4 * wordSize)); \/\/ Initial CRC\n-  } else {\n-    __ movptr(buf, Address(rsp, 4 + 2 * wordSize)); \/\/ byte[] array\n-    __ addptr(buf, arrayOopDesc::base_offset_in_bytes(T_BYTE)); \/\/ + header size\n-    __ addptr(buf, Address(rsp, 4 + 1 * wordSize)); \/\/ + offset\n-    __ movl(crc,   Address(rsp, 4 + 3 * wordSize)); \/\/ Initial CRC\n-  }\n-\n-  __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, StubRoutines::updateBytesCRC32()), crc, buf, len);\n-  \/\/ result in rax\n-\n-  \/\/ _areturn\n-  __ pop(rdi);                \/\/ get return address\n-  __ mov(rsp, rsi);           \/\/ set sp to sender sp\n-  __ jmp(rdi);\n-\n-  \/\/ generate a vanilla native entry as the slow path\n-  __ bind(slow_path);\n-  __ jump_to_entry(Interpreter::entry_for_kind(Interpreter::native));\n-  return entry;\n-}\n-\n-\/**\n-* Method entry for static native methods:\n-*   int java.util.zip.CRC32C.updateBytes(int crc, byte[] b, int off, int end)\n-*   int java.util.zip.CRC32C.updateByteBuffer(int crc, long address, int off, int end)\n-*\/\n-address TemplateInterpreterGenerator::generate_CRC32C_updateBytes_entry(AbstractInterpreter::MethodKind kind) {\n-  assert(UseCRC32CIntrinsics, \"this intrinsic is not supported\");\n-  address entry = __ pc();\n-  \/\/ Load parameters\n-  const Register crc = rax;  \/\/ crc\n-  const Register buf = rcx;  \/\/ source java byte array address\n-  const Register len = rdx;  \/\/ length\n-  const Register end = len;\n-\n-  \/\/ value              x86_32\n-  \/\/ interp. arg ptr    ESP + 4\n-  \/\/ int java.util.zip.CRC32.updateBytes(int crc, byte[] b, int off, int end)\n-  \/\/                                         3           2      1        0\n-  \/\/ int java.util.zip.CRC32.updateByteBuffer(int crc, long address, int off, int end)\n-  \/\/                                              4         2,3          1        0\n-\n-  \/\/ Arguments are reversed on java expression stack\n-  __ movl(end, Address(rsp, 4 + 0)); \/\/ end\n-  __ subl(len, Address(rsp, 4 + 1 * wordSize));  \/\/ end - offset == length\n-  \/\/ Calculate address of start element\n-  if (kind == Interpreter::java_util_zip_CRC32C_updateDirectByteBuffer) {\n-    __ movptr(buf, Address(rsp, 4 + 2 * wordSize)); \/\/ long address\n-    __ addptr(buf, Address(rsp, 4 + 1 * wordSize)); \/\/ + offset\n-    __ movl(crc, Address(rsp, 4 + 4 * wordSize)); \/\/ Initial CRC\n-  } else {\n-    __ movptr(buf, Address(rsp, 4 + 2 * wordSize)); \/\/ byte[] array\n-    __ addptr(buf, arrayOopDesc::base_offset_in_bytes(T_BYTE)); \/\/ + header size\n-    __ addptr(buf, Address(rsp, 4 + 1 * wordSize)); \/\/ + offset\n-    __ movl(crc, Address(rsp, 4 + 3 * wordSize)); \/\/ Initial CRC\n-  }\n-  __ super_call_VM_leaf(CAST_FROM_FN_PTR(address, StubRoutines::updateBytesCRC32C()), crc, buf, len);\n-  \/\/ result in rax\n-  \/\/ _areturn\n-  __ pop(rdi);                \/\/ get return address\n-  __ mov(rsp, rsi);           \/\/ set sp to sender sp\n-  __ jmp(rdi);\n-\n-  return entry;\n-}\n-\n-\/**\n- * Method entry for static native method:\n- *    java.lang.Float.intBitsToFloat(int bits)\n- *\/\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() {\n-  if (UseSSE >= 1) {\n-    address entry = __ pc();\n-\n-    \/\/ rsi: the sender's SP\n-\n-    \/\/ Skip safepoint check (compiler intrinsic versions of this method\n-    \/\/ do not perform safepoint checks either).\n-\n-    \/\/ Load 'bits' into xmm0 (interpreter returns results in xmm0)\n-    __ movflt(xmm0, Address(rsp, wordSize));\n-\n-    \/\/ Return\n-    __ pop(rdi); \/\/ get return address\n-    __ mov(rsp, rsi); \/\/ set rsp to the sender's SP\n-    __ jmp(rdi);\n-    return entry;\n-  }\n-\n-  return nullptr;\n-}\n-\n-\/**\n- * Method entry for static native method:\n- *    java.lang.Float.floatToRawIntBits(float value)\n- *\/\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() {\n-  if (UseSSE >= 1) {\n-    address entry = __ pc();\n-\n-    \/\/ rsi: the sender's SP\n-\n-    \/\/ Skip safepoint check (compiler intrinsic versions of this method\n-    \/\/ do not perform safepoint checks either).\n-\n-    \/\/ Load the parameter (a floating-point value) into rax.\n-    __ movl(rax, Address(rsp, wordSize));\n-\n-    \/\/ Return\n-    __ pop(rdi); \/\/ get return address\n-    __ mov(rsp, rsi); \/\/ set rsp to the sender's SP\n-    __ jmp(rdi);\n-    return entry;\n-  }\n-\n-  return nullptr;\n-}\n-\n-\n-\/**\n- * Method entry for static native method:\n- *    java.lang.Double.longBitsToDouble(long bits)\n- *\/\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() {\n-   if (UseSSE >= 2) {\n-     address entry = __ pc();\n-\n-     \/\/ rsi: the sender's SP\n-\n-     \/\/ Skip safepoint check (compiler intrinsic versions of this method\n-     \/\/ do not perform safepoint checks either).\n-\n-     \/\/ Load 'bits' into xmm0 (interpreter returns results in xmm0)\n-     __ movdbl(xmm0, Address(rsp, wordSize));\n-\n-     \/\/ Return\n-     __ pop(rdi); \/\/ get return address\n-     __ mov(rsp, rsi); \/\/ set rsp to the sender's SP\n-     __ jmp(rdi);\n-     return entry;\n-   }\n-\n-   return nullptr;\n-}\n-\n-\/**\n- * Method entry for static native method:\n- *    java.lang.Double.doubleToRawLongBits(double value)\n- *\/\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() {\n-  if (UseSSE >= 2) {\n-    address entry = __ pc();\n-\n-    \/\/ rsi: the sender's SP\n-\n-    \/\/ Skip safepoint check (compiler intrinsic versions of this method\n-    \/\/ do not perform safepoint checks either).\n-\n-    \/\/ Load the parameter (a floating-point value) into rax.\n-    __ movl(rdx, Address(rsp, 2*wordSize));\n-    __ movl(rax, Address(rsp, wordSize));\n-\n-    \/\/ Return\n-    __ pop(rdi); \/\/ get return address\n-    __ mov(rsp, rsi); \/\/ set rsp to the sender's SP\n-    __ jmp(rdi);\n-    return entry;\n-  }\n-\n-  return nullptr;\n-}\n-\n-\/**\n- * Method entry for static method:\n- *    java.lang.Float.float16ToFloat(short floatBinary16)\n- *\/\n-address TemplateInterpreterGenerator::generate_Float_float16ToFloat_entry() {\n-  assert(VM_Version::supports_float16(), \"this intrinsic is not supported\");\n-  address entry = __ pc();\n-\n-  \/\/ rsi: the sender's SP\n-\n-  \/\/ Load value into xmm0 and convert\n-  __ movswl(rax, Address(rsp, wordSize));\n-  __ flt16_to_flt(xmm0, rax);\n-\n-  \/\/ Return\n-  __ pop(rdi); \/\/ get return address\n-  __ mov(rsp, rsi); \/\/ set rsp to the sender's SP\n-  __ jmp(rdi);\n-  return entry;\n-}\n-\n-\/**\n- * Method entry for static method:\n- *    java.lang.Float.floatToFloat16(float value)\n- *\/\n-address TemplateInterpreterGenerator::generate_Float_floatToFloat16_entry() {\n-  assert(VM_Version::supports_float16(), \"this intrinsic is not supported\");\n-  address entry = __ pc();\n-\n-  \/\/ rsi: the sender's SP\n-\n-  \/\/ Load value into xmm0, convert and put result into rax\n-  __ movflt(xmm0, Address(rsp, wordSize));\n-  __ flt_to_flt16(rax, xmm0, xmm1);\n-\n-  \/\/ Return\n-  __ pop(rdi); \/\/ get return address\n-  __ mov(rsp, rsi); \/\/ set rsp to the sender's SP\n-  __ jmp(rdi);\n-  return entry;\n-}\n-\n-address TemplateInterpreterGenerator::generate_math_entry(AbstractInterpreter::MethodKind kind) {\n-\n-  \/\/ rbx,: Method*\n-  \/\/ rcx: scratrch\n-  \/\/ rsi: sender sp\n-\n-  address entry_point = __ pc();\n-\n-  \/\/ These don't need a safepoint check because they aren't virtually\n-  \/\/ callable. We won't enter these intrinsics from compiled code.\n-  \/\/ If in the future we added an intrinsic which was virtually callable\n-  \/\/ we'd have to worry about how to safepoint so that this code is used.\n-\n-  \/\/ mathematical functions inlined by compiler\n-  \/\/ (interpreter must provide identical implementation\n-  \/\/ in order to avoid monotonicity bugs when switching\n-  \/\/ from interpreter to compiler in the middle of some\n-  \/\/ computation)\n-  \/\/\n-  \/\/ stack: [ ret adr ] <-- rsp\n-  \/\/        [ lo(arg) ]\n-  \/\/        [ hi(arg) ]\n-  \/\/\n-  if (kind == Interpreter::java_lang_math_tanh) {\n-    return nullptr;\n-  }\n-\n-  if (kind == Interpreter::java_lang_math_fmaD) {\n-    if (!UseFMA) {\n-      return nullptr; \/\/ Generate a vanilla entry\n-    }\n-    __ movdbl(xmm2, Address(rsp, 5 * wordSize));\n-    __ movdbl(xmm1, Address(rsp, 3 * wordSize));\n-    __ movdbl(xmm0, Address(rsp, 1 * wordSize));\n-    __ fmad(xmm0, xmm1, xmm2, xmm0);\n-    __ pop(rdi);                               \/\/ get return address\n-    __ mov(rsp, rsi);                          \/\/ set sp to sender sp\n-    __ jmp(rdi);\n-\n-    return entry_point;\n-  } else if (kind == Interpreter::java_lang_math_fmaF) {\n-    if (!UseFMA) {\n-      return nullptr; \/\/ Generate a vanilla entry\n-    }\n-    __ movflt(xmm2, Address(rsp, 3 * wordSize));\n-    __ movflt(xmm1, Address(rsp, 2 * wordSize));\n-    __ movflt(xmm0, Address(rsp, 1 * wordSize));\n-    __ fmaf(xmm0, xmm1, xmm2, xmm0);\n-    __ pop(rdi);                               \/\/ get return address\n-    __ mov(rsp, rsi);                          \/\/ set sp to sender sp\n-    __ jmp(rdi);\n-\n-    return entry_point;\n- }\n-\n-  __ fld_d(Address(rsp, 1*wordSize));\n-  switch (kind) {\n-    case Interpreter::java_lang_math_sin :\n-        __ subptr(rsp, 2 * wordSize);\n-        __ fstp_d(Address(rsp, 0));\n-        if (VM_Version::supports_sse2() && StubRoutines::dsin() != nullptr) {\n-          __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dsin())));\n-        } else {\n-          __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dsin));\n-        }\n-        __ addptr(rsp, 2 * wordSize);\n-        break;\n-    case Interpreter::java_lang_math_cos :\n-        __ subptr(rsp, 2 * wordSize);\n-        __ fstp_d(Address(rsp, 0));\n-        if (VM_Version::supports_sse2() && StubRoutines::dcos() != nullptr) {\n-          __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dcos())));\n-        } else {\n-          __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dcos));\n-        }\n-        __ addptr(rsp, 2 * wordSize);\n-        break;\n-    case Interpreter::java_lang_math_tan :\n-        __ subptr(rsp, 2 * wordSize);\n-        __ fstp_d(Address(rsp, 0));\n-        if (StubRoutines::dtan() != nullptr) {\n-          __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dtan())));\n-        } else {\n-          __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dtan));\n-        }\n-        __ addptr(rsp, 2 * wordSize);\n-        break;\n-    case Interpreter::java_lang_math_sqrt:\n-        __ fsqrt();\n-        break;\n-    case Interpreter::java_lang_math_abs:\n-        __ fabs();\n-        break;\n-    case Interpreter::java_lang_math_log:\n-        __ subptr(rsp, 2 * wordSize);\n-        __ fstp_d(Address(rsp, 0));\n-        if (StubRoutines::dlog() != nullptr) {\n-          __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlog())));\n-        } else {\n-          __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dlog));\n-        }\n-        __ addptr(rsp, 2 * wordSize);\n-        break;\n-    case Interpreter::java_lang_math_log10:\n-        __ subptr(rsp, 2 * wordSize);\n-        __ fstp_d(Address(rsp, 0));\n-        if (StubRoutines::dlog10() != nullptr) {\n-          __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dlog10())));\n-        } else {\n-          __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dlog10));\n-        }\n-        __ addptr(rsp, 2 * wordSize);\n-        break;\n-    case Interpreter::java_lang_math_pow:\n-      __ fld_d(Address(rsp, 3*wordSize)); \/\/ second argument\n-      __ subptr(rsp, 4 * wordSize);\n-      __ fstp_d(Address(rsp, 0));\n-      __ fstp_d(Address(rsp, 2 * wordSize));\n-      if (StubRoutines::dpow() != nullptr) {\n-        __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dpow())));\n-      } else {\n-        __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dpow));\n-      }\n-      __ addptr(rsp, 4 * wordSize);\n-      break;\n-    case Interpreter::java_lang_math_exp:\n-      __ subptr(rsp, 2*wordSize);\n-      __ fstp_d(Address(rsp, 0));\n-      if (StubRoutines::dexp() != nullptr) {\n-        __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::dexp())));\n-      } else {\n-        __ call_VM_leaf0(CAST_FROM_FN_PTR(address, SharedRuntime::dexp));\n-      }\n-      __ addptr(rsp, 2*wordSize);\n-    break;\n-    default                              :\n-        ShouldNotReachHere();\n-  }\n-\n-  \/\/ return double result in xmm0 for interpreter and compilers.\n-  if (UseSSE >= 2) {\n-    __ subptr(rsp, 2*wordSize);\n-    __ fstp_d(Address(rsp, 0));\n-    __ movdbl(xmm0, Address(rsp, 0));\n-    __ addptr(rsp, 2*wordSize);\n-  }\n-\n-  \/\/ done, result in FPU ST(0) or XMM0\n-  __ pop(rdi);                               \/\/ get return address\n-  __ mov(rsp, rsi);                          \/\/ set sp to sender sp\n-  __ jmp(rdi);\n-\n-  return entry_point;\n-}\n-\n-\/\/ Not supported\n-address TemplateInterpreterGenerator::generate_currentThread() { return nullptr; }\n-\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86_32.cpp","additions":0,"deletions":510,"binary":false,"changes":510,"status":"deleted"},{"patch":"@@ -502,6 +502,0 @@\n-\/\/ Not supported\n-address TemplateInterpreterGenerator::generate_Float_intBitsToFloat_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Float_floatToRawIntBits_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_longBitsToDouble_entry() { return nullptr; }\n-address TemplateInterpreterGenerator::generate_Double_doubleToRawLongBits_entry() { return nullptr; }\n-\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86_64.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,2 +55,2 @@\n-static const Register rbcp     = LP64_ONLY(r13) NOT_LP64(rsi);\n-static const Register rlocals  = LP64_ONLY(r14) NOT_LP64(rdi);\n+static const Register rbcp     = r13;\n+static const Register rlocals  = r14;\n@@ -67,6 +67,0 @@\n-#ifndef _LP64\n-static inline Address haddress(int n) {\n-  return iaddress(n + 0);\n-}\n-#endif\n-\n@@ -93,6 +87,0 @@\n-#ifndef _LP64\n-static inline Address haddress(Register r)       {\n-  return Address(rlocals, r, Interpreter::stackElementScale(), Interpreter::local_offset_in_bytes(0));\n-}\n-#endif\n-\n@@ -160,4 +148,1 @@\n-  __ store_heap_oop(dst, val,\n-                    NOT_LP64(rdx) LP64_ONLY(rscratch2),\n-                    NOT_LP64(rbx) LP64_ONLY(r9),\n-                    NOT_LP64(rsi) LP64_ONLY(r8), decorators);\n+  __ store_heap_oop(dst, val, rscratch2, r9, r8, decorators);\n@@ -289,4 +274,0 @@\n-#ifndef _LP64\n-  assert(value >= 0, \"check this code\");\n-  __ xorptr(rdx, rdx);\n-#endif\n@@ -299,18 +280,12 @@\n-  if (UseSSE >= 1) {\n-    static float one = 1.0f, two = 2.0f;\n-    switch (value) {\n-    case 0:\n-      __ xorps(xmm0, xmm0);\n-      break;\n-    case 1:\n-      __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n-      break;\n-    case 2:\n-      __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n-#ifdef _LP64\n+  static float one = 1.0f, two = 2.0f;\n+  switch (value) {\n+  case 0:\n+    __ xorps(xmm0, xmm0);\n+    break;\n+  case 1:\n+    __ movflt(xmm0, ExternalAddress((address) &one), rscratch1);\n+    break;\n+  case 2:\n+    __ movflt(xmm0, ExternalAddress((address) &two), rscratch1);\n+    break;\n+  default:\n@@ -318,7 +293,1 @@\n-#else\n-           if (value == 0) { __ fldz();\n-    } else if (value == 1) { __ fld1();\n-    } else if (value == 2) { __ fld1(); __ fld1(); __ faddp(); \/\/ should do a better solution here\n-    } else                 { ShouldNotReachHere();\n-    }\n-#endif \/\/ _LP64\n+    break;\n@@ -330,11 +299,4 @@\n-  if (UseSSE >= 2) {\n-    static double one = 1.0;\n-    switch (value) {\n-    case 0:\n-      __ xorpd(xmm0, xmm0);\n-      break;\n-    case 1:\n-      __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n+  static double one = 1.0;\n+  switch (value) {\n+  case 0:\n+    __ xorpd(xmm0, xmm0);\n@@ -342,3 +304,4 @@\n-    }\n-  } else {\n-#ifdef _LP64\n+  case 1:\n+    __ movdbl(xmm0, ExternalAddress((address) &one), rscratch1);\n+    break;\n+  default:\n@@ -346,6 +309,1 @@\n-#else\n-           if (value == 0) { __ fldz();\n-    } else if (value == 1) { __ fld1();\n-    } else                 { ShouldNotReachHere();\n-    }\n-#endif\n+    break;\n@@ -369,1 +327,1 @@\n-  Register rarg = NOT_LP64(rcx) LP64_ONLY(c_rarg1);\n+  Register rarg = c_rarg1;\n@@ -437,1 +395,1 @@\n-  Register rarg = NOT_LP64(rcx) LP64_ONLY(c_rarg1);\n+  Register rarg = c_rarg1;\n@@ -500,1 +458,0 @@\n-  NOT_LP64(__ movptr(rdx, Address(rcx, rbx, Address::times_ptr, base_offset + 1 * wordSize)));\n@@ -514,1 +471,1 @@\n-  const Register rarg = NOT_LP64(rcx) LP64_ONLY(c_rarg1);\n+  const Register rarg = c_rarg1;\n@@ -517,6 +474,0 @@\n-#ifndef _LP64\n-  \/\/ borrow rdi from locals\n-  __ get_thread(rdi);\n-  __ get_vm_result_2(flags, rdi);\n-  __ restore_locals();\n-#else\n@@ -524,1 +475,0 @@\n-#endif\n@@ -599,1 +549,0 @@\n-      NOT_LP64(__ movptr(rdx, field.plus_disp(4)));\n@@ -640,2 +589,2 @@\n-    const Register bc = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-    LP64_ONLY(assert(rbx != bc, \"register damaged\"));\n+    const Register bc = c_rarg3;\n+    assert(rbx != bc, \"register damaged\");\n@@ -697,1 +646,0 @@\n-  NOT_LP64(__ movl(rdx, haddress(rbx)));\n@@ -735,1 +683,0 @@\n-  NOT_LP64(__ movl(rdx, haddress(rbx)));\n@@ -776,1 +723,1 @@\n-  __ mov(NOT_LP64(rax) LP64_ONLY(c_rarg1), array);\n+  __ mov(c_rarg1, array);\n@@ -797,1 +744,0 @@\n-  NOT_LP64(__ mov(rbx, rax));\n@@ -898,1 +844,0 @@\n-  NOT_LP64(__ movptr(rdx, haddress(n)));\n@@ -950,2 +895,2 @@\n-    const Register bc = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-    LP64_ONLY(assert(rbx != bc, \"register damaged\"));\n+    const Register bc = c_rarg3;\n+    assert(rbx != bc, \"register damaged\");\n@@ -1005,1 +950,0 @@\n-  NOT_LP64(__ movptr(haddress(rbx), rdx));\n@@ -1036,2 +980,1 @@\n-  NOT_LP64(__ pop_l(rax, rdx));\n-  LP64_ONLY(__ pop_l());\n+  __ pop_l();\n@@ -1040,1 +983,0 @@\n-  NOT_LP64(__ movl(haddress(rbx), rdx));\n@@ -1044,1 +986,0 @@\n-#ifdef _LP64\n@@ -1049,3 +990,0 @@\n-#else\n-  wide_istore();\n-#endif\n@@ -1055,1 +993,0 @@\n-#ifdef _LP64\n@@ -1060,3 +997,0 @@\n-#else\n-  wide_lstore();\n-#endif\n@@ -1103,1 +1037,1 @@\n-  \/\/ value is in UseSSE >= 1 ? xmm0 : ST(0)\n+  \/\/ value is in xmm0\n@@ -1116,1 +1050,1 @@\n-  \/\/ value is in UseSSE >= 2 ? xmm0 : ST(0)\n+  \/\/ value is in xmm0\n@@ -1228,1 +1162,0 @@\n-  NOT_LP64(__ movptr(haddress(n), rdx));\n@@ -1367,1 +1300,0 @@\n-#ifdef _LP64\n@@ -1376,12 +1308,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  switch (op) {\n-    case add  : __ addl(rax, rbx); __ adcl(rdx, rcx); break;\n-    case sub  : __ subl(rbx, rax); __ sbbl(rcx, rdx);\n-                __ mov (rax, rbx); __ mov (rdx, rcx); break;\n-    case _and : __ andl(rax, rbx); __ andl(rdx, rcx); break;\n-    case _or  : __ orl (rax, rbx); __ orl (rdx, rcx); break;\n-    case _xor : __ xorl(rax, rbx); __ xorl(rdx, rcx); break;\n-    default   : ShouldNotReachHere();\n-  }\n-#endif\n@@ -1415,1 +1335,0 @@\n-#ifdef _LP64\n@@ -1418,7 +1337,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  __ push(rcx); __ push(rbx);\n-  __ push(rdx); __ push(rax);\n-  __ lmul(2 * wordSize, 0);\n-  __ addptr(rsp, 4 * wordSize);  \/\/ take off temporaries\n-#endif\n@@ -1429,1 +1341,0 @@\n-#ifdef _LP64\n@@ -1441,11 +1352,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  __ push(rcx); __ push(rbx);\n-  __ push(rdx); __ push(rax);\n-  \/\/ check if y = 0\n-  __ orl(rax, rdx);\n-  __ jump_cc(Assembler::zero,\n-             RuntimeAddress(Interpreter::_throw_ArithmeticException_entry));\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::ldiv));\n-  __ addptr(rsp, 4 * wordSize);  \/\/ take off temporaries\n-#endif\n@@ -1456,1 +1356,0 @@\n-#ifdef _LP64\n@@ -1468,11 +1367,0 @@\n-#else\n-  __ pop_l(rbx, rcx);\n-  __ push(rcx); __ push(rbx);\n-  __ push(rdx); __ push(rax);\n-  \/\/ check if y = 0\n-  __ orl(rax, rdx);\n-  __ jump_cc(Assembler::zero,\n-             RuntimeAddress(Interpreter::_throw_ArithmeticException_entry));\n-  __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::lrem));\n-  __ addptr(rsp, 4 * wordSize);\n-#endif\n@@ -1484,1 +1372,0 @@\n-  #ifdef _LP64\n@@ -1487,4 +1374,0 @@\n-#else\n-  __ pop_l(rax, rdx);                            \/\/ get shift value\n-  __ lshl(rdx, rax);\n-#endif\n@@ -1494,1 +1377,0 @@\n-#ifdef _LP64\n@@ -1499,6 +1381,0 @@\n-#else\n-  transition(itos, ltos);\n-  __ mov(rcx, rax);                              \/\/ get shift count\n-  __ pop_l(rax, rdx);                            \/\/ get shift value\n-  __ lshr(rdx, rax, true);\n-#endif\n@@ -1509,1 +1385,0 @@\n-#ifdef _LP64\n@@ -1513,5 +1388,0 @@\n-#else\n-  __ mov(rcx, rax);                              \/\/ get shift count\n-  __ pop_l(rax, rdx);                            \/\/ get shift value\n-  __ lshr(rdx, rax);\n-#endif\n@@ -1523,53 +1393,31 @@\n-  if (UseSSE >= 1) {\n-    switch (op) {\n-    case add:\n-      __ addss(xmm0, at_rsp());\n-      __ addptr(rsp, Interpreter::stackElementSize);\n-      break;\n-    case sub:\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ subss(xmm0, xmm1);\n-      break;\n-    case mul:\n-      __ mulss(xmm0, at_rsp());\n-      __ addptr(rsp, Interpreter::stackElementSize);\n-      break;\n-    case div:\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ divss(xmm0, xmm1);\n-      break;\n-    case rem:\n-      \/\/ On x86_64 platforms the SharedRuntime::frem method is called to perform the\n-      \/\/ modulo operation. The frem method calls the function\n-      \/\/ double fmod(double x, double y) in math.h. The documentation of fmod states:\n-      \/\/ \"If x or y is a NaN, a NaN is returned.\" without specifying what type of NaN\n-      \/\/ (signalling or quiet) is returned.\n-      \/\/\n-      \/\/ On x86_32 platforms the FPU is used to perform the modulo operation. The\n-      \/\/ reason is that on 32-bit Windows the sign of modulo operations diverges from\n-      \/\/ what is considered the standard (e.g., -0.0f % -3.14f is 0.0f (and not -0.0f).\n-      \/\/ The fprem instruction used on x86_32 is functionally equivalent to\n-      \/\/ SharedRuntime::frem in that it returns a NaN.\n-#ifdef _LP64\n-      __ movflt(xmm1, xmm0);\n-      __ pop_f(xmm0);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), 2);\n-#else \/\/ !_LP64\n-      __ push_f(xmm0);\n-      __ pop_f();\n-      __ fld_s(at_rsp());\n-      __ fremr(rax);\n-      __ f2ieee();\n-      __ pop(rax);  \/\/ pop second operand off the stack\n-      __ push_f();\n-      __ pop_f(xmm0);\n-#endif \/\/ _LP64\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n-#ifdef _LP64\n+  switch (op) {\n+  case add:\n+    __ addss(xmm0, at_rsp());\n+    __ addptr(rsp, Interpreter::stackElementSize);\n+    break;\n+  case sub:\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ subss(xmm0, xmm1);\n+    break;\n+  case mul:\n+    __ mulss(xmm0, at_rsp());\n+    __ addptr(rsp, Interpreter::stackElementSize);\n+    break;\n+  case div:\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ divss(xmm0, xmm1);\n+    break;\n+  case rem:\n+    \/\/ On x86_64 platforms the SharedRuntime::frem method is called to perform the\n+    \/\/ modulo operation. The frem method calls the function\n+    \/\/ double fmod(double x, double y) in math.h. The documentation of fmod states:\n+    \/\/ \"If x or y is a NaN, a NaN is returned.\" without specifying what type of NaN\n+    \/\/ (signalling or quiet) is returned.\n+    \/\/\n+    __ movflt(xmm1, xmm0);\n+    __ pop_f(xmm0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::frem), 2);\n+    break;\n+  default:\n@@ -1577,12 +1425,1 @@\n-#else \/\/ !_LP64\n-    switch (op) {\n-    case add: __ fadd_s (at_rsp());                break;\n-    case sub: __ fsubr_s(at_rsp());                break;\n-    case mul: __ fmul_s (at_rsp());                break;\n-    case div: __ fdivr_s(at_rsp());                break;\n-    case rem: __ fld_s  (at_rsp()); __ fremr(rax); break;\n-    default : ShouldNotReachHere();\n-    }\n-    __ f2ieee();\n-    __ pop(rax);  \/\/ pop second operand off the stack\n-#endif \/\/ _LP64\n+    break;\n@@ -1594,46 +1431,27 @@\n-  if (UseSSE >= 2) {\n-    switch (op) {\n-    case add:\n-      __ addsd(xmm0, at_rsp());\n-      __ addptr(rsp, 2 * Interpreter::stackElementSize);\n-      break;\n-    case sub:\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ subsd(xmm0, xmm1);\n-      break;\n-    case mul:\n-      __ mulsd(xmm0, at_rsp());\n-      __ addptr(rsp, 2 * Interpreter::stackElementSize);\n-      break;\n-    case div:\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ divsd(xmm0, xmm1);\n-      break;\n-    case rem:\n-      \/\/ Similar to fop2(), the modulo operation is performed using the\n-      \/\/ SharedRuntime::drem method (on x86_64 platforms) or using the\n-      \/\/ FPU (on x86_32 platforms) for the same reasons as mentioned in fop2().\n-#ifdef _LP64\n-      __ movdbl(xmm1, xmm0);\n-      __ pop_d(xmm0);\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), 2);\n-#else \/\/ !_LP64\n-      __ push_d(xmm0);\n-      __ pop_d();\n-      __ fld_d(at_rsp());\n-      __ fremr(rax);\n-      __ d2ieee();\n-      __ pop(rax);\n-      __ pop(rdx);\n-      __ push_d();\n-      __ pop_d(xmm0);\n-#endif \/\/ _LP64\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      break;\n-    }\n-  } else {\n-#ifdef _LP64\n+  switch (op) {\n+  case add:\n+    __ addsd(xmm0, at_rsp());\n+    __ addptr(rsp, 2 * Interpreter::stackElementSize);\n+    break;\n+  case sub:\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ subsd(xmm0, xmm1);\n+    break;\n+  case mul:\n+    __ mulsd(xmm0, at_rsp());\n+    __ addptr(rsp, 2 * Interpreter::stackElementSize);\n+    break;\n+  case div:\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ divsd(xmm0, xmm1);\n+    break;\n+  case rem:\n+    \/\/ Similar to fop2(), the modulo operation is performed using the\n+    \/\/ SharedRuntime::drem method (on x86_64 platforms) or using the\n+    __ movdbl(xmm1, xmm0);\n+    __ pop_d(xmm0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::drem), 2);\n+    break;\n+  default:\n@@ -1641,30 +1459,1 @@\n-#else \/\/ !_LP64\n-    switch (op) {\n-    case add: __ fadd_d (at_rsp());                break;\n-    case sub: __ fsubr_d(at_rsp());                break;\n-    case mul: {\n-      \/\/ strict semantics\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias1()));\n-      __ fmulp();\n-      __ fmul_d (at_rsp());\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias2()));\n-      __ fmulp();\n-      break;\n-    }\n-    case div: {\n-      \/\/ strict semantics\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias1()));\n-      __ fmul_d (at_rsp());\n-      __ fdivrp();\n-      __ fld_x(ExternalAddress(StubRoutines::x86::addr_fpu_subnormal_bias2()));\n-      __ fmulp();\n-      break;\n-    }\n-    case rem: __ fld_d  (at_rsp()); __ fremr(rax); break;\n-    default : ShouldNotReachHere();\n-    }\n-    __ d2ieee();\n-    \/\/ Pop double precision number from rsp.\n-    __ pop(rax);\n-    __ pop(rdx);\n-#endif \/\/ _LP64\n+    break;\n@@ -1681,2 +1470,1 @@\n-  LP64_ONLY(__ negq(rax));\n-  NOT_LP64(__ lneg(rdx, rax));\n+  __ negq(rax);\n@@ -1702,7 +1490,2 @@\n-  if (UseSSE >= 1) {\n-    static jlong *float_signflip  = double_quadword(&float_signflip_pool[1],  CONST64(0x8000000080000000),  CONST64(0x8000000080000000));\n-    __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n-  } else {\n-    LP64_ONLY(ShouldNotReachHere());\n-    NOT_LP64(__ fchs());\n-  }\n+  static jlong *float_signflip  = double_quadword(&float_signflip_pool[1],  CONST64(0x8000000080000000),  CONST64(0x8000000080000000));\n+  __ xorps(xmm0, ExternalAddress((address) float_signflip), rscratch1);\n@@ -1713,11 +1496,3 @@\n-  if (UseSSE >= 2) {\n-    static jlong *double_signflip =\n-      double_quadword(&double_signflip_pool[1], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n-    __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n-  } else {\n-#ifdef _LP64\n-    ShouldNotReachHere();\n-#else\n-    __ fchs();\n-#endif\n-  }\n+  static jlong *double_signflip =\n+    double_quadword(&double_signflip_pool[1], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n+  __ xorpd(xmm0, ExternalAddress((address) double_signflip), rscratch1);\n@@ -1745,2 +1520,0 @@\n-#ifdef _LP64\n-  \/\/ Checking\n@@ -1873,192 +1646,0 @@\n-#else \/\/ !_LP64\n-  \/\/ Checking\n-#ifdef ASSERT\n-  { TosState tos_in  = ilgl;\n-    TosState tos_out = ilgl;\n-    switch (bytecode()) {\n-      case Bytecodes::_i2l: \/\/ fall through\n-      case Bytecodes::_i2f: \/\/ fall through\n-      case Bytecodes::_i2d: \/\/ fall through\n-      case Bytecodes::_i2b: \/\/ fall through\n-      case Bytecodes::_i2c: \/\/ fall through\n-      case Bytecodes::_i2s: tos_in = itos; break;\n-      case Bytecodes::_l2i: \/\/ fall through\n-      case Bytecodes::_l2f: \/\/ fall through\n-      case Bytecodes::_l2d: tos_in = ltos; break;\n-      case Bytecodes::_f2i: \/\/ fall through\n-      case Bytecodes::_f2l: \/\/ fall through\n-      case Bytecodes::_f2d: tos_in = ftos; break;\n-      case Bytecodes::_d2i: \/\/ fall through\n-      case Bytecodes::_d2l: \/\/ fall through\n-      case Bytecodes::_d2f: tos_in = dtos; break;\n-      default             : ShouldNotReachHere();\n-    }\n-    switch (bytecode()) {\n-      case Bytecodes::_l2i: \/\/ fall through\n-      case Bytecodes::_f2i: \/\/ fall through\n-      case Bytecodes::_d2i: \/\/ fall through\n-      case Bytecodes::_i2b: \/\/ fall through\n-      case Bytecodes::_i2c: \/\/ fall through\n-      case Bytecodes::_i2s: tos_out = itos; break;\n-      case Bytecodes::_i2l: \/\/ fall through\n-      case Bytecodes::_f2l: \/\/ fall through\n-      case Bytecodes::_d2l: tos_out = ltos; break;\n-      case Bytecodes::_i2f: \/\/ fall through\n-      case Bytecodes::_l2f: \/\/ fall through\n-      case Bytecodes::_d2f: tos_out = ftos; break;\n-      case Bytecodes::_i2d: \/\/ fall through\n-      case Bytecodes::_l2d: \/\/ fall through\n-      case Bytecodes::_f2d: tos_out = dtos; break;\n-      default             : ShouldNotReachHere();\n-    }\n-    transition(tos_in, tos_out);\n-  }\n-#endif \/\/ ASSERT\n-\n-  \/\/ Conversion\n-  \/\/ (Note: use push(rcx)\/pop(rcx) for 1\/2-word stack-ptr manipulation)\n-  switch (bytecode()) {\n-    case Bytecodes::_i2l:\n-      __ extend_sign(rdx, rax);\n-      break;\n-    case Bytecodes::_i2f:\n-      if (UseSSE >= 1) {\n-        __ cvtsi2ssl(xmm0, rax);\n-      } else {\n-        __ push(rax);          \/\/ store int on tos\n-        __ fild_s(at_rsp());   \/\/ load int to ST0\n-        __ f2ieee();           \/\/ truncate to float size\n-        __ pop(rcx);           \/\/ adjust rsp\n-      }\n-      break;\n-    case Bytecodes::_i2d:\n-      if (UseSSE >= 2) {\n-        __ cvtsi2sdl(xmm0, rax);\n-      } else {\n-      __ push(rax);          \/\/ add one slot for d2ieee()\n-      __ push(rax);          \/\/ store int on tos\n-      __ fild_s(at_rsp());   \/\/ load int to ST0\n-      __ d2ieee();           \/\/ truncate to double size\n-      __ pop(rcx);           \/\/ adjust rsp\n-      __ pop(rcx);\n-      }\n-      break;\n-    case Bytecodes::_i2b:\n-      __ shll(rax, 24);      \/\/ truncate upper 24 bits\n-      __ sarl(rax, 24);      \/\/ and sign-extend byte\n-      LP64_ONLY(__ movsbl(rax, rax));\n-      break;\n-    case Bytecodes::_i2c:\n-      __ andl(rax, 0xFFFF);  \/\/ truncate upper 16 bits\n-      LP64_ONLY(__ movzwl(rax, rax));\n-      break;\n-    case Bytecodes::_i2s:\n-      __ shll(rax, 16);      \/\/ truncate upper 16 bits\n-      __ sarl(rax, 16);      \/\/ and sign-extend short\n-      LP64_ONLY(__ movswl(rax, rax));\n-      break;\n-    case Bytecodes::_l2i:\n-      \/* nothing to do *\/\n-      break;\n-    case Bytecodes::_l2f:\n-      \/\/ On 64-bit platforms, the cvtsi2ssq instruction is used to convert\n-      \/\/ 64-bit long values to floats. On 32-bit platforms it is not possible\n-      \/\/ to use that instruction with 64-bit operands, therefore the FPU is\n-      \/\/ used to perform the conversion.\n-      __ push(rdx);          \/\/ store long on tos\n-      __ push(rax);\n-      __ fild_d(at_rsp());   \/\/ load long to ST0\n-      __ f2ieee();           \/\/ truncate to float size\n-      __ pop(rcx);           \/\/ adjust rsp\n-      __ pop(rcx);\n-      if (UseSSE >= 1) {\n-        __ push_f();\n-        __ pop_f(xmm0);\n-      }\n-      break;\n-    case Bytecodes::_l2d:\n-      \/\/ On 32-bit platforms the FPU is used for conversion because on\n-      \/\/ 32-bit platforms it is not not possible to use the cvtsi2sdq\n-      \/\/ instruction with 64-bit operands.\n-      __ push(rdx);          \/\/ store long on tos\n-      __ push(rax);\n-      __ fild_d(at_rsp());   \/\/ load long to ST0\n-      __ d2ieee();           \/\/ truncate to double size\n-      __ pop(rcx);           \/\/ adjust rsp\n-      __ pop(rcx);\n-      if (UseSSE >= 2) {\n-        __ push_d();\n-        __ pop_d(xmm0);\n-      }\n-      break;\n-    case Bytecodes::_f2i:\n-      \/\/ SharedRuntime::f2i does not differentiate between sNaNs and qNaNs\n-      \/\/ as it returns 0 for any NaN.\n-      if (UseSSE >= 1) {\n-        __ push_f(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ fstp_s(at_rsp());   \/\/ pass float argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2i), 1);\n-      break;\n-    case Bytecodes::_f2l:\n-      \/\/ SharedRuntime::f2l does not differentiate between sNaNs and qNaNs\n-      \/\/ as it returns 0 for any NaN.\n-      if (UseSSE >= 1) {\n-       __ push_f(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ fstp_s(at_rsp());   \/\/ pass float argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::f2l), 1);\n-      break;\n-    case Bytecodes::_f2d:\n-      if (UseSSE < 1) {\n-        \/* nothing to do *\/\n-      } else if (UseSSE == 1) {\n-        __ push_f(xmm0);\n-        __ pop_f();\n-      } else { \/\/ UseSSE >= 2\n-        __ cvtss2sd(xmm0, xmm0);\n-      }\n-      break;\n-    case Bytecodes::_d2i:\n-      if (UseSSE >= 2) {\n-        __ push_d(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ push(rcx);\n-        __ fstp_d(at_rsp());   \/\/ pass double argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2i), 2);\n-      break;\n-    case Bytecodes::_d2l:\n-      if (UseSSE >= 2) {\n-        __ push_d(xmm0);\n-      } else {\n-        __ push(rcx);          \/\/ reserve space for argument\n-        __ push(rcx);\n-        __ fstp_d(at_rsp());   \/\/ pass double argument on stack\n-      }\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::d2l), 2);\n-      break;\n-    case Bytecodes::_d2f:\n-      if (UseSSE <= 1) {\n-        __ push(rcx);          \/\/ reserve space for f2ieee()\n-        __ f2ieee();           \/\/ truncate to float size\n-        __ pop(rcx);           \/\/ adjust rsp\n-        if (UseSSE == 1) {\n-          \/\/ The cvtsd2ss instruction is not available if UseSSE==1, therefore\n-          \/\/ the conversion is performed using the FPU in this case.\n-          __ push_f();\n-          __ pop_f(xmm0);\n-        }\n-      } else { \/\/ UseSSE >= 2\n-        __ cvtsd2ss(xmm0, xmm0);\n-      }\n-      break;\n-    default             :\n-      ShouldNotReachHere();\n-  }\n-#endif \/\/ _LP64\n@@ -2069,1 +1650,0 @@\n-#ifdef _LP64\n@@ -2078,7 +1658,0 @@\n-#else\n-\n-  \/\/ y = rdx:rax\n-  __ pop_l(rbx, rcx);             \/\/ get x = rcx:rbx\n-  __ lcmp2int(rcx, rbx, rdx, rax);\/\/ rcx := cmp(x, y)\n-  __ mov(rax, rcx);\n-#endif\n@@ -2088,27 +1661,5 @@\n-  if ((is_float && UseSSE >= 1) ||\n-      (!is_float && UseSSE >= 2)) {\n-    Label done;\n-    if (is_float) {\n-      \/\/ XXX get rid of pop here, use ... reg, mem32\n-      __ pop_f(xmm1);\n-      __ ucomiss(xmm1, xmm0);\n-    } else {\n-      \/\/ XXX get rid of pop here, use ... reg, mem64\n-      __ pop_d(xmm1);\n-      __ ucomisd(xmm1, xmm0);\n-    }\n-    if (unordered_result < 0) {\n-      __ movl(rax, -1);\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::below, done);\n-      __ setb(Assembler::notEqual, rdx);\n-      __ movzbl(rax, rdx);\n-    } else {\n-      __ movl(rax, 1);\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::above, done);\n-      __ movl(rax, 0);\n-      __ jccb(Assembler::equal, done);\n-      __ decrementl(rax);\n-    }\n-    __ bind(done);\n+  Label done;\n+  if (is_float) {\n+    \/\/ XXX get rid of pop here, use ... reg, mem32\n+    __ pop_f(xmm1);\n+    __ ucomiss(xmm1, xmm0);\n@@ -2116,12 +1667,17 @@\n-#ifdef _LP64\n-    ShouldNotReachHere();\n-#else \/\/ !_LP64\n-    if (is_float) {\n-      __ fld_s(at_rsp());\n-    } else {\n-      __ fld_d(at_rsp());\n-      __ pop(rdx);\n-    }\n-    __ pop(rcx);\n-    __ fcmp2int(rax, unordered_result < 0);\n-#endif \/\/ _LP64\n+    \/\/ XXX get rid of pop here, use ... reg, mem64\n+    __ pop_d(xmm1);\n+    __ ucomisd(xmm1, xmm0);\n+  }\n+  if (unordered_result < 0) {\n+    __ movl(rax, -1);\n+    __ jccb(Assembler::parity, done);\n+    __ jccb(Assembler::below, done);\n+    __ setb(Assembler::notEqual, rdx);\n+    __ movzbl(rax, rdx);\n+  } else {\n+    __ movl(rax, 1);\n+    __ jccb(Assembler::parity, done);\n+    __ jccb(Assembler::above, done);\n+    __ movl(rax, 0);\n+    __ jccb(Assembler::equal, done);\n+    __ decrementl(rax);\n@@ -2129,0 +1685,1 @@\n+  __ bind(done);\n@@ -2152,1 +1709,1 @@\n-  LP64_ONLY(__ movl2ptr(rdx, rdx));\n+  __ movl2ptr(rdx, rdx);\n@@ -2271,2 +1828,0 @@\n-      NOT_LP64(__ get_thread(rcx));\n-\n@@ -2276,2 +1831,1 @@\n-      LP64_ONLY(__ mov(j_rarg0, rax));\n-      NOT_LP64(__ mov(rcx, rax));\n+      __ mov(j_rarg0, rax);\n@@ -2282,2 +1836,2 @@\n-      const Register retaddr   = LP64_ONLY(j_rarg2) NOT_LP64(rdi);\n-      const Register sender_sp = LP64_ONLY(j_rarg1) NOT_LP64(rdx);\n+      const Register retaddr   = j_rarg2;\n+      const Register sender_sp = j_rarg1;\n@@ -2354,2 +1908,1 @@\n-  LP64_ONLY(__ movslq(rbx, iaddress(rbx))); \/\/ get return bci, compute return bcp\n-  NOT_LP64(__ movptr(rbx, iaddress(rbx)));\n+  __ movslq(rbx, iaddress(rbx)); \/\/ get return bci, compute return bcp\n@@ -2399,1 +1952,1 @@\n-  LP64_ONLY(__ movl2ptr(rdx, rdx));\n+  __ movl2ptr(rdx, rdx);\n@@ -2489,2 +2042,0 @@\n-  NOT_LP64(__ save_bcp());\n-\n@@ -2547,4 +2098,1 @@\n-  LP64_ONLY(__ movslq(j, j));\n-\n-  NOT_LP64(__ restore_bcp());\n-  NOT_LP64(__ restore_locals());                           \/\/ restore rdi\n+  __ movslq(j, j);\n@@ -2561,4 +2109,1 @@\n-  LP64_ONLY(__ movslq(j, j));\n-\n-  NOT_LP64(__ restore_bcp());\n-  NOT_LP64(__ restore_locals());\n+  __ movslq(j, j);\n@@ -2579,1 +2124,1 @@\n-    Register robj = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n+    Register robj = c_rarg1;\n@@ -2594,1 +2139,0 @@\n-#ifdef _LP64\n@@ -2596,5 +2140,0 @@\n-#else\n-    const Register thread = rdi;\n-    __ get_thread(thread);\n-    __ testb(Address(thread, JavaThread::polling_word_offset()), SafepointMechanism::poll_bit());\n-#endif\n@@ -2698,2 +2237,1 @@\n-    const Register thread = LP64_ONLY(r15_thread) NOT_LP64(noreg);\n-    assert(thread != noreg, \"x86_32 not supported\");\n+    const Register thread = r15_thread;\n@@ -2825,1 +2363,0 @@\n-#ifdef _LP64\n@@ -2828,3 +2365,0 @@\n-#else\n-    __ movptr(index, ArrayAddress(table, Address(noreg, index, Address::times_ptr)));\n-#endif \/\/ _LP64\n@@ -2983,1 +2517,1 @@\n-  const Register obj   = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register obj   = c_rarg3;\n@@ -2989,1 +2523,1 @@\n-  const Register bc    = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ uses same reg as obj, so don't mix them\n+  const Register bc    = c_rarg3; \/\/ uses same reg as obj, so don't mix them\n@@ -3080,3 +2614,1 @@\n-    \/\/ Generate code as if volatile (x86_32).  There just aren't enough registers to\n-    \/\/ save that information and this code is faster than the test.\n-  __ access_load_at(T_LONG, IN_HEAP | MO_RELAXED, noreg \/* ltos *\/, field, noreg, noreg);\n+  __ access_load_at(T_LONG, IN_HEAP, noreg \/* ltos *\/, field, noreg, noreg);\n@@ -3085,1 +2617,3 @@\n-  LP64_ONLY(if (!is_static && rc == may_rewrite) patch_bytecode(Bytecodes::_fast_lgetfield, bc, rbx));\n+  if (!is_static && rc == may_rewrite) {\n+    patch_bytecode(Bytecodes::_fast_lgetfield, bc, rbx);\n+  }\n@@ -3145,3 +2679,3 @@\n-  const Register entry = LP64_ONLY(c_rarg2) NOT_LP64(rax); \/\/ ResolvedFieldEntry\n-  const Register obj = LP64_ONLY(c_rarg1) NOT_LP64(rbx);   \/\/ Object pointer\n-  const Register value = LP64_ONLY(c_rarg3) NOT_LP64(rcx); \/\/ JValue object\n+  const Register entry = c_rarg2; \/\/ ResolvedFieldEntry\n+  const Register obj =   c_rarg1; \/\/ Object pointer\n+  const Register value = c_rarg3; \/\/ JValue object\n@@ -3169,3 +2703,0 @@\n-#ifndef _LP64\n-      Label two_word, valsize_known;\n-#endif\n@@ -3173,1 +2704,0 @@\n-#ifdef _LP64\n@@ -3181,16 +2711,0 @@\n-#else\n-      __ mov(obj, rsp);\n-      __ cmpl(value, ltos);\n-      __ jccb(Assembler::equal, two_word);\n-      __ cmpl(value, dtos);\n-      __ jccb(Assembler::equal, two_word);\n-      __ addptr(obj, Interpreter::expr_offset_in_bytes(1)); \/\/ one word jvalue (not ltos, dtos)\n-      __ jmpb(valsize_known);\n-\n-      __ bind(two_word);\n-      __ addptr(obj, Interpreter::expr_offset_in_bytes(2)); \/\/ two words jvalue\n-\n-      __ bind(valsize_known);\n-      \/\/ setup object pointer\n-      __ movptr(obj, Address(obj, 0));\n-#endif\n@@ -3255,1 +2769,0 @@\n-  NOT_LP64( const Address hi(obj, off, Address::times_1, 1*wordSize);)\n@@ -3261,1 +2774,1 @@\n-  const Register bc    = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register bc    = c_rarg3;\n@@ -3362,3 +2875,1 @@\n-    \/\/ MO_RELAXED: generate atomic store for the case of volatile field (important for x86_32)\n-    __ access_store_at(T_LONG, IN_HEAP | MO_RELAXED, field, noreg \/* ltos*\/, noreg, noreg, noreg);\n-#ifdef _LP64\n+    __ access_store_at(T_LONG, IN_HEAP, field, noreg \/* ltos*\/, noreg, noreg, noreg);\n@@ -3368,1 +2879,0 @@\n-#endif \/\/ _LP64\n@@ -3429,1 +2939,1 @@\n-  const Register scratch = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n+  const Register scratch = c_rarg3;\n@@ -3460,2 +2970,1 @@\n-    LP64_ONLY(__ load_field_entry(c_rarg2, rax));\n-    NOT_LP64(__ load_field_entry(rax, rdx));\n+    __ load_field_entry(c_rarg2, rax);\n@@ -3466,2 +2975,1 @@\n-    LP64_ONLY(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, c_rarg2, c_rarg3));\n-    NOT_LP64(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, rax, rcx));\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_modification), rbx, c_rarg2, c_rarg3);\n@@ -3530,1 +3038,0 @@\n-#ifdef _LP64\n@@ -3532,3 +3039,0 @@\n-#else\n-  __ stop(\"should not be rewritten\");\n-#endif\n@@ -3574,2 +3078,1 @@\n-    LP64_ONLY(__ load_field_entry(c_rarg2, rcx));\n-    NOT_LP64(__ load_field_entry(rcx, rdx));\n+    __ load_field_entry(c_rarg2, rcx);\n@@ -3578,1 +3081,1 @@\n-    LP64_ONLY(__ mov(c_rarg1, rax));\n+    __ mov(c_rarg1, rax);\n@@ -3581,2 +3084,1 @@\n-    LP64_ONLY(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), c_rarg1, c_rarg2));\n-    NOT_LP64(__ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), rax, rcx));\n+    __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::post_field_access), c_rarg1, c_rarg2);\n@@ -3603,1 +3105,0 @@\n-#ifdef _LP64\n@@ -3605,3 +3106,0 @@\n-#else\n-  __ stop(\"should not be rewritten\");\n-#endif\n@@ -3712,1 +3210,0 @@\n-#ifdef _LP64\n@@ -3715,3 +3212,0 @@\n-#else\n-    __ movptr(flags, ArrayAddress(table, Address(noreg, flags, Address::times_ptr)));\n-#endif \/\/ _LP64\n@@ -3949,1 +3443,0 @@\n-#ifdef _LP64\n@@ -3954,4 +3447,0 @@\n-#else\n-  recvKlass = rdx;\n-  Register method    = rcx;\n-#endif\n@@ -3969,1 +3458,3 @@\n-  LP64_ONLY( if (recvKlass != rdx) { __ movq(recvKlass, rdx); } )\n+  if (recvKlass != rdx) {\n+    __ movq(recvKlass, rdx);\n+  }\n@@ -4052,1 +3543,0 @@\n-#ifdef _LP64\n@@ -4055,4 +3545,0 @@\n-#else\n-  __ cmpb(Address(rcx, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n-  __ jcc(Assembler::notEqual, slow_case);\n-#endif\n@@ -4075,1 +3561,1 @@\n-  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n+  const Register thread = r15_thread;\n@@ -4078,1 +3564,0 @@\n-    NOT_LP64(__ get_thread(thread);)\n@@ -4117,1 +3602,0 @@\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size_bytes - 2*oopSize), rcx));\n@@ -4132,1 +3616,0 @@\n-#ifdef _LP64\n@@ -4135,1 +3618,0 @@\n-#endif\n@@ -4155,6 +3637,3 @@\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n-  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n-\n-  __ get_constant_pool(rarg1);\n-  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), rarg1, rarg2);\n+  __ get_constant_pool(c_rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n@@ -4169,2 +3648,1 @@\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n-  __ load_unsigned_byte(rarg1, at_bcp(1));\n+  __ load_unsigned_byte(c_rarg1, at_bcp(1));\n@@ -4172,1 +3650,1 @@\n-          rarg1, rax);\n+          c_rarg1, rax);\n@@ -4178,5 +3656,2 @@\n-  Register rarg1 = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n-  Register rarg2 = LP64_ONLY(c_rarg2) NOT_LP64(rdx);\n-\n-  __ get_unsigned_2_byte_index_at_bcp(rarg2, 1);\n-  __ get_constant_pool(rarg1);\n+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);\n+  __ get_constant_pool(c_rarg1);\n@@ -4184,1 +3659,1 @@\n-          rarg1, rarg2, rax);\n+          c_rarg1, c_rarg2, rax);\n@@ -4211,6 +3686,0 @@\n-#ifndef _LP64\n-  \/\/ borrow rdi from locals\n-  __ get_thread(rdi);\n-  __ get_vm_result_2(rax, rdi);\n-  __ restore_locals();\n-#else\n@@ -4218,1 +3687,0 @@\n-#endif\n@@ -4275,6 +3743,0 @@\n-#ifndef _LP64\n-  \/\/ borrow rdi from locals\n-  __ get_thread(rdi);\n-  __ get_vm_result_2(rax, rdi);\n-  __ restore_locals();\n-#else\n@@ -4282,1 +3744,0 @@\n-#endif\n@@ -4330,2 +3791,0 @@\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rcx);\n-\n@@ -4333,1 +3792,1 @@\n-  __ get_method(rarg);\n+  __ get_method(c_rarg1);\n@@ -4337,1 +3796,1 @@\n-             rarg, rbcp);\n+             c_rarg1, rbcp);\n@@ -4341,1 +3800,1 @@\n-  __ get_method(rarg);\n+  __ get_method(c_rarg1);\n@@ -4344,1 +3803,1 @@\n-             rarg, rbcp);\n+             c_rarg1, rbcp);\n@@ -4390,3 +3849,3 @@\n-  Register rtop = LP64_ONLY(c_rarg3) NOT_LP64(rcx);\n-  Register rbot = LP64_ONLY(c_rarg2) NOT_LP64(rbx);\n-  Register rmon = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n+  Register rtop = c_rarg3;\n+  Register rbot = c_rarg2;\n+  Register rmon = c_rarg1;\n@@ -4488,2 +3947,2 @@\n-  Register rtop = LP64_ONLY(c_rarg1) NOT_LP64(rdx);\n-  Register rbot = LP64_ONLY(c_rarg2) NOT_LP64(rbx);\n+  Register rtop = c_rarg1;\n+  Register rbot = c_rarg2;\n@@ -4543,1 +4002,0 @@\n-  Register rarg = LP64_ONLY(c_rarg1) NOT_LP64(rax);\n@@ -4548,2 +4006,2 @@\n-  __ lea(rarg, Address(rsp, rax, Interpreter::stackElementScale(), -wordSize));\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), rarg);\n+  __ lea(c_rarg1, Address(rsp, rax, Interpreter::stackElementScale(), -wordSize));\n+  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::multianewarray), c_rarg1);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":175,"deletions":717,"binary":false,"changes":892,"status":"modified"},{"patch":"@@ -1,34 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"prims\/upcallLinker.hpp\"\n-\n-address UpcallLinker::make_upcall_stub(jobject receiver, Symbol* signature,\n-                                       BasicType* out_sig_bt, int total_out_args,\n-                                       BasicType ret_type,\n-                                       jobject jabi, jobject jconv,\n-                                       bool needs_return_buffer, int ret_buf_size) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n","filename":"src\/hotspot\/cpu\/x86\/upcallLinker_x86_32.cpp","additions":0,"deletions":34,"binary":false,"changes":34,"status":"deleted"},{"patch":"@@ -38,2 +38,2 @@\n-  LP64_ONLY(declare_constant(frame::arg_reg_save_area_bytes))       \\\n-  declare_constant(frame::interpreter_frame_sender_sp_offset)       \\\n+  declare_constant(frame::arg_reg_save_area_bytes)             \\\n+  declare_constant(frame::interpreter_frame_sender_sp_offset)  \\\n","filename":"src\/hotspot\/cpu\/x86\/vmStructs_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -76,2 +76,0 @@\n-#ifdef _LP64\n-\n@@ -91,1 +89,0 @@\n-#endif\n@@ -111,1 +108,0 @@\n-#if defined(_LP64)\n@@ -130,1 +126,0 @@\n-#endif\n@@ -158,1 +153,0 @@\n-#ifdef _LP64\n@@ -160,3 +154,0 @@\n-#else\n-    __ movptr(rbp, Address(rsp, 8)); \/\/ cpuid_info address\n-#endif\n@@ -422,1 +413,0 @@\n-#if defined(_LP64)\n@@ -457,1 +447,0 @@\n-#endif\n@@ -531,1 +520,0 @@\n-#ifdef _LP64\n@@ -534,1 +522,0 @@\n-#endif\n@@ -560,1 +547,0 @@\n-#ifdef _LP64\n@@ -563,1 +549,0 @@\n-#endif\n@@ -604,1 +589,0 @@\n-#ifdef _LP64\n@@ -607,1 +591,0 @@\n-#endif\n@@ -632,1 +615,0 @@\n-#ifdef _LP64\n@@ -635,1 +617,0 @@\n-#endif\n@@ -691,1 +672,0 @@\n-#ifdef _LP64\n@@ -694,4 +674,0 @@\n-#else\n-    __ movptr(rax, Address(rsp, 16)); \/\/ CPUID leaf\n-    __ movptr(rsi, Address(rsp, 20)); \/\/ register array address\n-#endif\n@@ -741,1 +717,0 @@\n-#ifdef _LP64\n@@ -743,3 +718,0 @@\n-#else\n-    __ movptr(rbp, Address(rsp, 8)); \/\/ cpuid_info address\n-#endif\n@@ -893,2 +865,2 @@\n-  LP64_ONLY(_supports_atomic_getset8 = true);\n-  LP64_ONLY(_supports_atomic_getadd8 = true);\n+  _supports_atomic_getset8 = true;\n+  _supports_atomic_getadd8 = true;\n@@ -896,1 +868,0 @@\n-#ifdef _LP64\n@@ -901,3 +872,4 @@\n-  \/\/ in 64 bit the use of SSE2 is the minimum\n-  if (UseSSE < 2) UseSSE = 2;\n-#endif\n+  \/\/ The use of SSE2 is the minimum\n+  if (UseSSE < 2) {\n+    UseSSE = 2;\n+  }\n@@ -905,1 +877,0 @@\n-#ifdef AMD64\n@@ -917,1 +888,0 @@\n-#endif\n@@ -919,1 +889,0 @@\n-#ifdef _LP64\n@@ -928,1 +897,0 @@\n-#endif\n@@ -947,6 +915,0 @@\n-  if (UseSSE < 2)\n-    _features &= ~CPU_SSE2;\n-\n-  if (UseSSE < 1)\n-    _features &= ~CPU_SSE;\n-\n@@ -962,12 +924,7 @@\n-  if (UseSSE > 0) {\n-    if (UseSSE > 3 && supports_sse4_1()) {\n-      use_sse_limit = 4;\n-    } else if (UseSSE > 2 && supports_sse3()) {\n-      use_sse_limit = 3;\n-    } else if (UseSSE > 1 && supports_sse2()) {\n-      use_sse_limit = 2;\n-    } else if (UseSSE > 0 && supports_sse()) {\n-      use_sse_limit = 1;\n-    } else {\n-      use_sse_limit = 0;\n-    }\n+  if (UseSSE > 3 && supports_sse4_1()) {\n+    use_sse_limit = 4;\n+  } else if (UseSSE > 2 && supports_sse3()) {\n+    use_sse_limit = 3;\n+  } else {\n+    assert(supports_sse2(), \"Checked before\");\n+    use_sse_limit = 2;\n@@ -1183,1 +1140,0 @@\n-#ifdef _LP64\n@@ -1194,6 +1150,0 @@\n-#else\n-  if (UseAdler32Intrinsics) {\n-    warning(\"Adler32Intrinsics not available on this CPU.\");\n-    FLAG_SET_DEFAULT(UseAdler32Intrinsics, false);\n-  }\n-#endif\n@@ -1223,1 +1173,0 @@\n-#ifdef _LP64\n@@ -1239,7 +1188,0 @@\n-#else\n-  \/\/ No support currently for ChaCha20 intrinsics on 32-bit platforms\n-  if (UseChaCha20Intrinsics) {\n-      warning(\"ChaCha20 intrinsics are not available on this CPU.\");\n-      FLAG_SET_DEFAULT(UseChaCha20Intrinsics, false);\n-  }\n-#endif \/\/ _LP64\n@@ -1258,1 +1200,1 @@\n-  if (supports_fma() && UseSSE >= 2) { \/\/ Check UseSSE since FMA code uses SSE instructions\n+  if (supports_fma()) {\n@@ -1271,1 +1213,1 @@\n-  if (supports_sha() LP64_ONLY(|| (supports_avx2() && supports_bmi2()))) {\n+  if (supports_sha() || (supports_avx2() && supports_bmi2())) {\n@@ -1298,1 +1240,0 @@\n-#ifdef _LP64\n@@ -1304,3 +1245,1 @@\n-  } else\n-#endif\n-  if (UseSHA512Intrinsics) {\n+  } else if (UseSHA512Intrinsics) {\n@@ -1311,1 +1250,0 @@\n-#ifdef _LP64\n@@ -1316,3 +1254,1 @@\n-  } else\n-#endif\n-   if (UseSHA3Intrinsics) {\n+  } else if (UseSHA3Intrinsics) {\n@@ -1327,9 +1263,0 @@\n-#ifdef COMPILER2\n-  if (UseFPUForSpilling) {\n-    if (UseSSE < 2) {\n-      \/\/ Only supported with SSE2+\n-      FLAG_SET_DEFAULT(UseFPUForSpilling, false);\n-    }\n-  }\n-#endif\n-\n@@ -1338,6 +1265,3 @@\n-  if (UseSSE < 2) {\n-    \/\/ Vectors (in XMM) are only supported with SSE2+\n-    \/\/ SSE is always 2 on x64.\n-    max_vector_size = 0;\n-  } else if (UseAVX == 0 || !os_supports_avx_vectors()) {\n-    \/\/ 16 byte vectors (in XMM) are supported with SSE2+\n+  if (UseAVX == 0 || !os_supports_avx_vectors()) {\n+    \/\/ 16 byte vectors (in XMM) are supported with SSE2+.\n+    \/\/ SSE2 is the minimum for x86_64.\n@@ -1353,1 +1277,0 @@\n-#ifdef _LP64\n@@ -1355,3 +1278,0 @@\n-#else\n-  int min_vector_size = 0;\n-#endif\n@@ -1381,1 +1301,1 @@\n-      int nreg = 2 LP64_ONLY(+2);\n+      int nreg = 4;\n@@ -1394,1 +1314,0 @@\n-#ifdef _LP64\n@@ -1399,3 +1318,1 @@\n-  } else\n-#endif\n-  if (UsePoly1305Intrinsics) {\n+  } else if (UsePoly1305Intrinsics) {\n@@ -1406,1 +1323,0 @@\n-#ifdef _LP64\n@@ -1411,3 +1327,1 @@\n-  } else\n-#endif\n-  if (UseIntPolyIntrinsics) {\n+  } else if (UseIntPolyIntrinsics) {\n@@ -1418,1 +1332,0 @@\n-#ifdef _LP64\n@@ -1434,32 +1347,0 @@\n-#else\n-  if (UseMultiplyToLenIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseMultiplyToLenIntrinsic)) {\n-      warning(\"multiplyToLen intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseMultiplyToLenIntrinsic, false);\n-  }\n-  if (UseMontgomeryMultiplyIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseMontgomeryMultiplyIntrinsic)) {\n-      warning(\"montgomeryMultiply intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseMontgomeryMultiplyIntrinsic, false);\n-  }\n-  if (UseMontgomerySquareIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseMontgomerySquareIntrinsic)) {\n-      warning(\"montgomerySquare intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseMontgomerySquareIntrinsic, false);\n-  }\n-  if (UseSquareToLenIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseSquareToLenIntrinsic)) {\n-      warning(\"squareToLen intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseSquareToLenIntrinsic, false);\n-  }\n-  if (UseMulAddIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseMulAddIntrinsic)) {\n-      warning(\"mulAdd intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseMulAddIntrinsic, false);\n-  }\n-#endif \/\/ _LP64\n@@ -1742,1 +1623,0 @@\n-#ifdef _LP64\n@@ -1759,14 +1639,0 @@\n-#else\n-  if (UseVectorizedMismatchIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseVectorizedMismatchIntrinsic)) {\n-      warning(\"vectorizedMismatch intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseVectorizedMismatchIntrinsic, false);\n-  }\n-  if (UseVectorizedHashCodeIntrinsic) {\n-    if (!FLAG_IS_DEFAULT(UseVectorizedHashCodeIntrinsic)) {\n-      warning(\"vectorizedHashCode intrinsic is not available in 32-bit VM\");\n-    }\n-    FLAG_SET_DEFAULT(UseVectorizedHashCodeIntrinsic, false);\n-  }\n-#endif \/\/ _LP64\n@@ -1857,1 +1723,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (!UseFastStosb && UseUnalignedLoadStores) {\n@@ -1921,1 +1787,0 @@\n-#ifdef _LP64\n@@ -1940,1 +1805,0 @@\n-#endif\n@@ -1972,1 +1836,1 @@\n-    if (AllocatePrefetchStyle <= 0 || (UseSSE == 0 && !supports_3dnow_prefetch())) {\n+    if (AllocatePrefetchStyle <= 0) {\n@@ -1976,1 +1840,7 @@\n-      if (UseSSE == 0 && supports_3dnow_prefetch()) {\n+      if (AllocatePrefetchInstr == 0) {\n+        log->print(\"PREFETCHNTA\");\n+      } else if (AllocatePrefetchInstr == 1) {\n+        log->print(\"PREFETCHT0\");\n+      } else if (AllocatePrefetchInstr == 2) {\n+        log->print(\"PREFETCHT2\");\n+      } else if (AllocatePrefetchInstr == 3) {\n@@ -1978,10 +1848,0 @@\n-      } else if (UseSSE >= 1) {\n-        if (AllocatePrefetchInstr == 0) {\n-          log->print(\"PREFETCHNTA\");\n-        } else if (AllocatePrefetchInstr == 1) {\n-          log->print(\"PREFETCHT0\");\n-        } else if (AllocatePrefetchInstr == 2) {\n-          log->print(\"PREFETCHT2\");\n-        } else if (AllocatePrefetchInstr == 3) {\n-          log->print(\"PREFETCHW\");\n-        }\n@@ -2175,1 +2035,0 @@\n-#if defined(_LP64)\n@@ -2179,1 +2038,0 @@\n-#endif\n@@ -2197,2 +2055,0 @@\n-\n-#if defined(_LP64)\n@@ -2201,1 +2057,0 @@\n-#endif\n@@ -2204,1 +2059,1 @@\n-  LP64_ONLY(Assembler::precompute_instructions();)\n+  Assembler::precompute_instructions();\n@@ -2971,1 +2826,0 @@\n-#ifdef _LP64\n@@ -2976,1 +2830,0 @@\n-#endif\n@@ -3153,1 +3006,1 @@\n-  int nreg = 2 LP64_ONLY(+2);\n+  int nreg = 4;\n@@ -3309,1 +3162,0 @@\n-#ifdef _LP64\n@@ -3311,3 +3163,0 @@\n-#else\n-        return 320;\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":34,"deletions":185,"binary":false,"changes":219,"status":"modified"},{"patch":"@@ -640,1 +640,1 @@\n-  LP64_ONLY(static void clear_apx_test_state());\n+  static void clear_apx_test_state();\n@@ -825,1 +825,1 @@\n-    return LP64_ONLY(true) NOT_LP64(false); \/\/ not implemented on x86_32\n+    return true;\n@@ -830,1 +830,1 @@\n-    return LP64_ONLY(true) NOT_LP64(false); \/\/ not implemented on x86_32\n+    return true;\n@@ -865,1 +865,0 @@\n-#ifdef _LP64\n@@ -867,3 +866,0 @@\n-#else\n-  static bool supports_clflush() { return  ((_features & CPU_FLUSH) != 0); }\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#ifdef AMD64\n@@ -38,1 +37,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-#ifdef AMD64\n@@ -60,3 +59,0 @@\n-#else\n-  return ::as_Register(value());\n-#endif \/\/ AMD64\n@@ -85,3 +81,0 @@\n-#ifndef AMD64\n-  if (is_Register()) return true;\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-  return VMRegImpl::as_VMReg(encoding() LP64_ONLY( << 1 ));\n+  return VMRegImpl::as_VMReg(encoding() << 1);\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,266 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"asm\/macroAssembler.hpp\"\n-#include \"code\/compiledIC.hpp\"\n-#include \"code\/vtableStubs.hpp\"\n-#include \"interp_masm_x86.hpp\"\n-#include \"memory\/resourceArea.hpp\"\n-#include \"oops\/instanceKlass.hpp\"\n-#include \"oops\/klassVtable.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n-#include \"vmreg_x86.inline.hpp\"\n-#ifdef COMPILER2\n-#include \"opto\/runtime.hpp\"\n-#endif\n-\n-\/\/ machine-dependent part of VtableStubs: create VtableStub of correct size and\n-\/\/ initialize its code\n-\n-#define __ masm->\n-\n-#ifndef PRODUCT\n-extern \"C\" void bad_compiled_vtable_index(JavaThread* thread, oop receiver, int index);\n-#endif\n-\n-\/\/ These stubs are used by the compiler only.\n-\/\/ Argument registers, which must be preserved:\n-\/\/   rcx - receiver (always first argument)\n-\/\/   rdx - second argument (if any)\n-\/\/ Other registers that might be usable:\n-\/\/   rax - inline cache register (is interface for itable stub)\n-\/\/   rbx - method (used when calling out to interpreter)\n-\/\/ Available now, but may become callee-save at some point:\n-\/\/   rsi, rdi\n-\/\/ Note that rax and rdx are also used for return values.\n-\n-VtableStub* VtableStubs::create_vtable_stub(int vtable_index) {\n-  \/\/ Read \"A word on VtableStub sizing\" in share\/code\/vtableStubs.hpp for details on stub sizing.\n-  const int stub_code_length = code_size_limit(true);\n-  VtableStub* s = new(stub_code_length) VtableStub(true, vtable_index);\n-  \/\/ Can be null if there is no free space in the code cache.\n-  if (s == nullptr) {\n-    return nullptr;\n-  }\n-\n-  \/\/ Count unused bytes in instruction sequences of variable size.\n-  \/\/ We add them to the computed buffer size in order to avoid\n-  \/\/ overflow in subsequently generated stubs.\n-  address   start_pc;\n-  int       slop_bytes = 0;\n-  int       slop_delta = 0;\n-  \/\/ No variance was detected in vtable stub sizes. Setting index_dependent_slop == 0 will unveil any deviation from this observation.\n-  const int index_dependent_slop     = 0;\n-\n-  ResourceMark    rm;\n-  CodeBuffer      cb(s->entry_point(), stub_code_length);\n-  MacroAssembler* masm = new MacroAssembler(&cb);\n-\n-#if (!defined(PRODUCT) && defined(COMPILER2))\n-  if (CountCompiledCalls) {\n-    __ incrementl(ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));\n-  }\n-#endif\n-\n-  \/\/ get receiver (need to skip return address on top of stack)\n-  assert(VtableStub::receiver_location() == rcx->as_VMReg(), \"receiver expected in rcx\");\n-\n-  \/\/ get receiver klass\n-  address npe_addr = __ pc();\n-  __ movptr(rax, Address(rcx, oopDesc::klass_offset_in_bytes()));\n-\n-#ifndef PRODUCT\n-  if (DebugVtables) {\n-    Label L;\n-    start_pc = __ pc();\n-    \/\/ check offset vs vtable length\n-    __ cmpl(Address(rax, Klass::vtable_length_offset()), vtable_index*vtableEntry::size());\n-    slop_delta  = 10 - (__ pc() - start_pc);  \/\/ cmpl varies in length, depending on data\n-    slop_bytes += slop_delta;\n-    assert(slop_delta >= 0, \"negative slop(%d) encountered, adjust code size estimate!\", slop_delta);\n-\n-    __ jcc(Assembler::greater, L);\n-    __ movl(rbx, vtable_index);\n-    \/\/ VTABLE TODO: find upper bound for call_VM length.\n-    start_pc = __ pc();\n-    __ call_VM(noreg, CAST_FROM_FN_PTR(address, bad_compiled_vtable_index), rcx, rbx);\n-    slop_delta  = 500 - (__ pc() - start_pc);\n-    slop_bytes += slop_delta;\n-    assert(slop_delta >= 0, \"negative slop(%d) encountered, adjust code size estimate!\", slop_delta);\n-    __ bind(L);\n-  }\n-#endif \/\/ PRODUCT\n-\n-  const Register method = rbx;\n-\n-  \/\/ load Method* and target address\n-  start_pc = __ pc();\n-  __ lookup_virtual_method(rax, vtable_index, method);\n-  slop_delta  = 6 - (int)(__ pc() - start_pc);\n-  slop_bytes += slop_delta;\n-  assert(slop_delta >= 0, \"negative slop(%d) encountered, adjust code size estimate!\", slop_delta);\n-\n-#ifndef PRODUCT\n-  if (DebugVtables) {\n-    Label L;\n-    __ cmpptr(method, NULL_WORD);\n-    __ jcc(Assembler::equal, L);\n-    __ cmpptr(Address(method, Method::from_compiled_offset()), NULL_WORD);\n-    __ jcc(Assembler::notZero, L);\n-    __ stop(\"Vtable entry is null\");\n-    __ bind(L);\n-  }\n-#endif \/\/ PRODUCT\n-\n-  \/\/ rax: receiver klass\n-  \/\/ method (rbx): Method*\n-  \/\/ rcx: receiver\n-  address ame_addr = __ pc();\n-  __ jmp( Address(method, Method::from_compiled_offset()));\n-\n-  masm->flush();\n-  slop_bytes += index_dependent_slop; \/\/ add'l slop for size variance due to large itable offsets\n-  bookkeeping(masm, tty, s, npe_addr, ame_addr, true, vtable_index, slop_bytes, index_dependent_slop);\n-\n-  return s;\n-}\n-\n-\n-VtableStub* VtableStubs::create_itable_stub(int itable_index) {\n-  \/\/ Read \"A word on VtableStub sizing\" in share\/code\/vtableStubs.hpp for details on stub sizing.\n-  const int stub_code_length = code_size_limit(false);\n-  VtableStub* s = new(stub_code_length) VtableStub(false, itable_index);\n-  \/\/ Can be null if there is no free space in the code cache.\n-  if (s == nullptr) {\n-    return nullptr;\n-  }\n-  \/\/ Count unused bytes in instruction sequences of variable size.\n-  \/\/ We add them to the computed buffer size in order to avoid\n-  \/\/ overflow in subsequently generated stubs.\n-  address   start_pc;\n-  int       slop_bytes = 0;\n-  int       slop_delta = 0;\n-  const int index_dependent_slop = (itable_index == 0) ? 4 :     \/\/ code size change with transition from 8-bit to 32-bit constant (@index == 32).\n-                                   (itable_index < 32) ? 3 : 0;  \/\/ index == 0 generates even shorter code.\n-\n-  ResourceMark    rm;\n-  CodeBuffer      cb(s->entry_point(), stub_code_length);\n-  MacroAssembler* masm = new MacroAssembler(&cb);\n-\n-#if (!defined(PRODUCT) && defined(COMPILER2))\n-  if (CountCompiledCalls) {\n-    __ incrementl(ExternalAddress((address) SharedRuntime::nof_megamorphic_calls_addr()));\n-  }\n-#endif \/* PRODUCT *\/\n-\n-  \/\/ Entry arguments:\n-  \/\/  rax: CompiledICData\n-  \/\/  rcx: Receiver\n-\n-  \/\/ Most registers are in use; we'll use rax, rbx, rcx, rdx, rsi, rdi\n-  \/\/ (If we need to make rsi, rdi callee-save, do a push\/pop here.)\n-  const Register recv_klass_reg     = rsi;\n-  const Register holder_klass_reg   = rax; \/\/ declaring interface klass (DEFC)\n-  const Register resolved_klass_reg = rdi; \/\/ resolved interface klass (REFC)\n-  const Register temp_reg           = rdx;\n-  const Register method             = rbx;\n-  const Register icdata_reg         = rax;\n-  const Register receiver           = rcx;\n-\n-  __ movptr(resolved_klass_reg, Address(icdata_reg, CompiledICData::itable_refc_klass_offset()));\n-  __ movptr(holder_klass_reg,   Address(icdata_reg, CompiledICData::itable_defc_klass_offset()));\n-\n-  Label L_no_such_interface;\n-\n-  \/\/ get receiver klass (also an implicit null-check)\n-  assert(VtableStub::receiver_location() ==  rcx->as_VMReg(), \"receiver expected in  rcx\");\n-  address npe_addr = __ pc();\n-  __ load_klass(recv_klass_reg, rcx, noreg);\n-\n-  start_pc = __ pc();\n-  __ push(rdx); \/\/ temp_reg\n-\n-  \/\/ Receiver subtype check against REFC.\n-  \/\/ Get selected method from declaring class and itable index\n-  __ lookup_interface_method_stub(recv_klass_reg, \/\/ input\n-                                  holder_klass_reg, \/\/ input\n-                                  resolved_klass_reg, \/\/ input\n-                                  method, \/\/ output\n-                                  temp_reg,\n-                                  noreg,\n-                                  receiver, \/\/ input (x86_32 only: to restore recv_klass value)\n-                                  itable_index,\n-                                  L_no_such_interface);\n-  const ptrdiff_t  lookupSize = __ pc() - start_pc;\n-\n-  \/\/ We expect we need index_dependent_slop extra bytes. Reason:\n-  \/\/ The emitted code in lookup_interface_method changes when itable_index exceeds 31.\n-  \/\/ For windows, a narrow estimate was found to be 104. Other OSes not tested.\n-  const ptrdiff_t estimate = 104;\n-  const ptrdiff_t codesize = lookupSize + index_dependent_slop;\n-  slop_delta  = (int)(estimate - codesize);\n-  slop_bytes += slop_delta;\n-  assert(slop_delta >= 0, \"itable #%d: Code size estimate (%d) for lookup_interface_method too small, required: %d\", itable_index, (int)estimate, (int)codesize);\n-\n-  \/\/ method (rbx): Method*\n-  \/\/ rcx: receiver\n-\n-#ifdef ASSERT\n-  if (DebugVtables) {\n-    Label L1;\n-    __ cmpptr(method, NULL_WORD);\n-    __ jcc(Assembler::equal, L1);\n-    __ cmpptr(Address(method, Method::from_compiled_offset()), NULL_WORD);\n-    __ jcc(Assembler::notZero, L1);\n-    __ stop(\"Method* is null\");\n-    __ bind(L1);\n-  }\n-#endif \/\/ ASSERT\n-\n-  __ pop(rdx);\n-  address ame_addr = __ pc();\n-  __ jmp(Address(method, Method::from_compiled_offset()));\n-\n-  __ bind(L_no_such_interface);\n-  \/\/ Handle IncompatibleClassChangeError in itable stubs.\n-  \/\/ More detailed error message.\n-  \/\/ We force resolving of the call site by jumping to the \"handle\n-  \/\/ wrong method\" stub, and so let the interpreter runtime do all the\n-  \/\/ dirty work.\n-  __ pop(rdx);\n-  __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n-\n-  masm->flush();\n-  slop_bytes += index_dependent_slop; \/\/ add'l slop for size variance due to large itable offsets\n-  bookkeeping(masm, tty, s, npe_addr, ame_addr, false, itable_index, slop_bytes, index_dependent_slop);\n-\n-  return s;\n-}\n-\n-int VtableStub::pd_code_alignment() {\n-  \/\/ x86 cache line size is 64 bytes, but we want to limit alignment loss.\n-  const unsigned int icache_line_size = wordSize;\n-  return icache_line_size;\n-}\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_32.cpp","additions":0,"deletions":266,"binary":false,"changes":266,"status":"deleted"},{"patch":"@@ -213,2 +213,0 @@\n-#ifdef _LP64\n-\n@@ -623,3 +621,0 @@\n-#endif \/\/ _LP64\n-\n-#ifdef _LP64\n@@ -627,3 +622,0 @@\n-#else\n-reg_def RFLAGS(SOC, SOC, 0, 8, VMRegImpl::Bad());\n-#endif \/\/ _LP64\n@@ -661,3 +653,2 @@\n-                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                  ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                   XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                   XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -670,2 +661,2 @@\n-                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-                  ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n+                   XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,\n+                   XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n@@ -687,1 +678,0 @@\n-#endif\n@@ -729,3 +719,2 @@\n-                    XMM7\n-#ifdef _LP64\n-                   ,XMM8,\n+                    XMM7,\n+                    XMM8,\n@@ -739,1 +728,0 @@\n-#endif\n@@ -750,3 +738,2 @@\n-                    XMM7\n-#ifdef _LP64\n-                   ,XMM8,\n+                    XMM7,\n+                    XMM8,\n@@ -776,1 +763,0 @@\n-#endif\n@@ -790,3 +776,2 @@\n-                     XMM7,  XMM7b\n-#ifdef _LP64\n-                    ,XMM8,  XMM8b,\n+                     XMM7,  XMM7b,\n+                     XMM8,  XMM8b,\n@@ -800,1 +785,0 @@\n-#endif\n@@ -811,3 +795,2 @@\n-                     XMM7,  XMM7b\n-#ifdef _LP64\n-                    ,XMM8,  XMM8b,\n+                     XMM7,  XMM7b,\n+                     XMM8,  XMM8b,\n@@ -837,1 +820,0 @@\n-#endif\n@@ -851,3 +833,2 @@\n-                      XMM7\n-#ifdef _LP64\n-                     ,XMM8,\n+                      XMM7,\n+                      XMM8,\n@@ -861,1 +842,0 @@\n-#endif\n@@ -872,3 +852,2 @@\n-                      XMM7\n-#ifdef _LP64\n-                     ,XMM8,\n+                      XMM7,\n+                      XMM8,\n@@ -898,1 +877,0 @@\n-#endif\n@@ -912,3 +890,2 @@\n-                      XMM7,  XMM7b\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,\n+                      XMM7,  XMM7b,\n+                      XMM8,  XMM8b,\n@@ -922,1 +899,0 @@\n-#endif\n@@ -933,3 +909,2 @@\n-                      XMM7,  XMM7b\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,\n+                      XMM7,  XMM7b,\n+                      XMM8,  XMM8b,\n@@ -959,1 +934,0 @@\n-#endif\n@@ -973,3 +947,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,\n@@ -983,1 +956,0 @@\n-#endif\n@@ -994,3 +966,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,\n@@ -1020,1 +991,0 @@\n-#endif\n@@ -1034,3 +1004,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n@@ -1044,1 +1013,0 @@\n-#endif\n@@ -1055,3 +1023,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,\n@@ -1081,1 +1048,0 @@\n-#endif\n@@ -1095,3 +1061,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -1104,2 +1069,2 @@\n-                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p\n-                     ,XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n+                      XMM15, XMM15b, XMM15c, XMM15d, XMM15e, XMM15f, XMM15g, XMM15h, XMM15i, XMM15j, XMM15k, XMM15l, XMM15m, XMM15n, XMM15o, XMM15p,\n+                      XMM16, XMM16b, XMM16c, XMM16d, XMM16e, XMM16f, XMM16g, XMM16h, XMM16i, XMM16j, XMM16k, XMM16l, XMM16m, XMM16n, XMM16o, XMM16p,\n@@ -1121,1 +1086,0 @@\n-#endif\n@@ -1132,3 +1096,2 @@\n-                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p\n-#ifdef _LP64\n-                     ,XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n+                      XMM7,  XMM7b,  XMM7c,  XMM7d,  XMM7e,  XMM7f,  XMM7g,  XMM7h,  XMM7i,  XMM7j,  XMM7k,  XMM7l,  XMM7m,  XMM7n,  XMM7o,  XMM7p,\n+                      XMM8,  XMM8b,  XMM8c,  XMM8d,  XMM8e,  XMM8f,  XMM8g,  XMM8h,  XMM8i,  XMM8j,  XMM8k,  XMM8l,  XMM8m,  XMM8n,  XMM8o,  XMM8p,\n@@ -1142,1 +1105,0 @@\n-#endif\n@@ -1202,1 +1164,0 @@\n-#ifdef _LP64\n@@ -1207,10 +1168,0 @@\n-#else\n-  static uint size_deopt_handler() {\n-    \/\/ NativeCall instruction size is the same as NativeJump.\n-    \/\/ exception handler starts out as jump and can be patched to\n-    \/\/ a call be deoptimization.  (4932387)\n-    \/\/ Note that this value is also credited (in output.cpp) to\n-    \/\/ the size of the code section.\n-    return 5 + NativeJump::instruction_size; \/\/ pushl(); jmp;\n-  }\n-#endif\n@@ -1337,1 +1288,0 @@\n-#ifdef _LP64\n@@ -1348,4 +1298,0 @@\n-#else\n-  InternalAddress here(__ pc());\n-  __ pushptr(here.addr(), noreg);\n-#endif\n@@ -1375,1 +1321,0 @@\n-#ifdef _LP64\n@@ -1380,6 +1325,0 @@\n-#else\n-  static address float_signmask()  { return (address)float_signmask_pool; }\n-  static address float_signflip()  { return (address)float_signflip_pool; }\n-  static address double_signmask() { return (address)double_signmask_pool; }\n-  static address double_signflip() { return (address)double_signflip_pool; }\n-#endif\n@@ -1407,1 +1346,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1449,3 +1387,0 @@\n-      if (UseSSE < 2) { \/\/ requires at least SSE2\n-        return false;\n-      }\n@@ -1495,1 +1430,1 @@\n-      if (!is_LP64 || (UseAVX < 2)) {\n+      if (UseAVX < 2) {\n@@ -1510,1 +1445,0 @@\n-#ifdef _LP64\n@@ -1512,1 +1446,0 @@\n-#endif\n@@ -1541,1 +1474,0 @@\n-#ifdef _LP64\n@@ -1550,1 +1482,0 @@\n-#endif\n@@ -1593,1 +1524,1 @@\n-      if (!is_LP64  || UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n@@ -1601,1 +1532,1 @@\n-      if (!is_LP64 || UseAVX < 1) {\n+      if (UseAVX < 1) {\n@@ -1607,3 +1538,0 @@\n-      if (!is_LP64) {\n-        return false;\n-      }\n@@ -1613,1 +1541,1 @@\n-      if (UseAVX < 3 || !is_LP64)  {\n+      if (UseAVX < 3)  {\n@@ -1620,18 +1548,0 @@\n-#ifndef _LP64\n-    case Op_AddReductionVF:\n-    case Op_AddReductionVD:\n-    case Op_MulReductionVF:\n-    case Op_MulReductionVD:\n-      if (UseSSE < 1) { \/\/ requires at least SSE\n-        return false;\n-      }\n-      break;\n-    case Op_MulAddVS2VI:\n-    case Op_RShiftVL:\n-    case Op_AbsVD:\n-    case Op_NegVD:\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n-      break;\n-#endif \/\/ !LP64\n@@ -1639,1 +1549,1 @@\n-      if (!VM_Version::supports_bmi2() || (!is_LP64 && UseSSE < 2)) {\n+      if (!VM_Version::supports_bmi2()) {\n@@ -1644,1 +1554,1 @@\n-      if (!VM_Version::supports_bmi2() || (!is_LP64 && (UseSSE < 2 || !VM_Version::supports_bmi1()))) {\n+      if (!VM_Version::supports_bmi2()) {\n@@ -1649,4 +1559,0 @@\n-      if (UseSSE < 1) {\n-        return false;\n-      }\n-      break;\n@@ -1654,3 +1560,0 @@\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n@@ -1664,4 +1567,0 @@\n-      if (UseSSE < 1) {\n-        return false;\n-      }\n-      break;\n@@ -1669,8 +1568,0 @@\n-#ifdef _LP64\n-      if (UseSSE < 2) {\n-        return false;\n-      }\n-#else\n-      \/\/ x86_32.ad has a special match rule for SqrtD.\n-      \/\/ Together with common x86 rules, this handles all UseSSE cases.\n-#endif\n@@ -1708,1 +1599,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -1755,1 +1645,1 @@\n-      if (!is_LP64 || !VM_Version::supports_avx512bw()) {\n+      if (!VM_Version::supports_avx512bw()) {\n@@ -1805,11 +1695,0 @@\n-#ifndef _LP64\n-      if (bt == T_BYTE || bt == T_LONG) {\n-        return false;\n-      }\n-#endif\n-      break;\n-#ifndef _LP64\n-    case Op_VectorInsert:\n-      if (bt == T_LONG || bt == T_DOUBLE) {\n-        return false;\n-      }\n@@ -1817,1 +1696,0 @@\n-#endif\n@@ -1832,5 +1710,0 @@\n-#ifndef _LP64\n-      if (bt == T_BYTE || bt == T_LONG) {\n-        return false;\n-      }\n-#endif\n@@ -1921,3 +1794,2 @@\n-         (!is_LP64                                                ||\n-         (size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n-         (size_in_bits < 64)                                      ||\n+         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+         (size_in_bits < 64)                                       ||\n@@ -1993,3 +1865,0 @@\n-      if (!is_LP64 && !VM_Version::supports_avx512vl() && size_in_bits < 512) {\n-        return false;\n-      }\n@@ -2000,1 +1869,1 @@\n-      if (UseAVX < 1 || !is_LP64) {\n+      if (UseAVX < 1) {\n@@ -2048,1 +1917,0 @@\n-  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n@@ -2384,1 +2252,0 @@\n-#ifdef _LP64\n@@ -2394,2 +2261,1 @@\n-    } else\n-#endif\n+    } else {\n@@ -2397,0 +2263,1 @@\n+    }\n@@ -2534,1 +2401,1 @@\n-        LP64_ONLY( off->get_long() == (int) (off->get_long()) && ) \/\/ immL32\n+        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n@@ -2608,3 +2475,0 @@\n-#ifndef _LP64\n-      __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#else\n@@ -2616,1 +2480,0 @@\n-#endif\n@@ -2619,3 +2482,0 @@\n-#ifndef _LP64\n-      __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#else\n@@ -2627,1 +2487,0 @@\n-#endif\n@@ -2666,3 +2525,0 @@\n-#ifndef _LP64\n-        __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-#else\n@@ -2675,1 +2531,0 @@\n-#endif\n@@ -2678,3 +2533,0 @@\n-#ifndef _LP64\n-        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-#else\n@@ -2687,1 +2539,0 @@\n-#endif\n@@ -2704,3 +2555,0 @@\n-#ifndef _LP64\n-        __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-#else\n@@ -2713,1 +2561,0 @@\n-#endif\n@@ -2716,3 +2563,0 @@\n-#ifndef _LP64\n-        __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-#else\n@@ -2725,1 +2569,0 @@\n-#endif\n@@ -3956,1 +3799,0 @@\n-#ifdef _LP64\n@@ -4027,1 +3869,0 @@\n-#endif \/\/ _LP64\n@@ -4245,1 +4086,0 @@\n-#ifdef _LP64\n@@ -4408,1 +4248,0 @@\n-#endif\n@@ -4607,1 +4446,1 @@\n-  predicate(UseSSE >= 2 && Matcher::is_non_long_integral_vector(n));\n+  predicate(Matcher::is_non_long_integral_vector(n));\n@@ -4619,1 +4458,0 @@\n-#ifdef _LP64\n@@ -4640,55 +4478,0 @@\n-#else \/\/ _LP64\n-\/\/ Replicate long (8 byte) scalar to be vector\n-instruct ReplL_reg(vec dst, eRegL src, vec tmp) %{\n-  predicate(Matcher::vector_length(n) <= 4 && Matcher::vector_element_basic_type(n) == T_LONG);\n-  match(Set dst (Replicate src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  format %{ \"replicateL $dst,$src\" %}\n-  ins_encode %{\n-    uint vlen = Matcher::vector_length(this);\n-    if (vlen == 2) {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-    } else if (VM_Version::supports_avx512vl()) { \/\/ AVX512VL for <512bit operands\n-      int vlen_enc = Assembler::AVX_256bit;\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    } else {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct ReplL_reg_leg(legVec dst, eRegL src, legVec tmp) %{\n-  predicate(Matcher::vector_length(n) == 8 && Matcher::vector_element_basic_type(n) == T_LONG);\n-  match(Set dst (Replicate src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  format %{ \"replicateL $dst,$src\" %}\n-  ins_encode %{\n-    if (VM_Version::supports_avx512vl()) {\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ punpcklqdq($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti128_high($dst$$XMMRegister, $dst$$XMMRegister);\n-      __ vinserti64x4($dst$$XMMRegister, $dst$$XMMRegister, $dst$$XMMRegister, 0x1);\n-    } else {\n-      int vlen_enc = Assembler::AVX_512bit;\n-      __ movdl($dst$$XMMRegister, $src$$Register);\n-      __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-      __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-      __ vpbroadcastq($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-#endif \/\/ _LP64\n@@ -4743,1 +4526,1 @@\n-  predicate(UseSSE >= 2 && Matcher::vector_element_basic_type(n) == T_LONG);\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n@@ -4968,1 +4751,0 @@\n-#ifdef _LP64\n@@ -5019,1 +4801,0 @@\n-#endif\n@@ -5065,1 +4846,0 @@\n-#ifdef _LP64\n@@ -5120,1 +4900,0 @@\n-#endif\n@@ -5147,1 +4926,0 @@\n-#ifdef _LP64\n@@ -5185,1 +4963,0 @@\n-#endif \/\/ _LP64\n@@ -5397,1 +5174,0 @@\n-#ifdef _LP64\n@@ -5433,1 +5209,0 @@\n-#endif\n@@ -6734,1 +6509,0 @@\n-#ifdef _LP64\n@@ -6760,2 +6534,0 @@\n-#endif \/\/ _LP64\n-\n@@ -7126,1 +6898,0 @@\n-      assert(UseSSE >= 2, \"required\");\n@@ -7934,1 +7705,0 @@\n-#ifdef _LP64\n@@ -7984,2 +7754,0 @@\n-#endif \/\/ _LP64\n-\n@@ -8195,1 +7963,0 @@\n-#ifdef _LP64\n@@ -8197,1 +7964,0 @@\n-#endif\n@@ -8213,1 +7979,0 @@\n-#ifdef _LP64\n@@ -8215,1 +7980,0 @@\n-#endif\n@@ -8228,1 +7992,0 @@\n-#ifdef _LP64\n@@ -8256,1 +8019,0 @@\n-#endif\n@@ -8509,1 +8271,0 @@\n-      assert(UseSSE >= 2, \"required\");\n@@ -8521,1 +8282,0 @@\n-#ifdef _LP64\n@@ -8589,1 +8349,0 @@\n-#endif\n@@ -8840,1 +8599,0 @@\n-#ifdef _LP64\n@@ -8872,1 +8630,1 @@\n-#endif\n+\n@@ -9453,1 +9211,0 @@\n-#ifdef _LP64\n@@ -9667,1 +9424,0 @@\n-#ifdef _LP64\n@@ -9683,1 +9439,0 @@\n-#endif\n@@ -9711,2 +9466,0 @@\n-#endif \/\/ _LP64\n-\n@@ -10433,1 +10186,0 @@\n-#ifdef _LP64\n@@ -10498,1 +10250,0 @@\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":50,"deletions":299,"binary":false,"changes":349,"status":"modified"},{"patch":"@@ -1,13846 +0,0 @@\n-\/\/\n-\/\/ Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n-\/\/ DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-\/\/\n-\/\/ This code is free software; you can redistribute it and\/or modify it\n-\/\/ under the terms of the GNU General Public License version 2 only, as\n-\/\/ published by the Free Software Foundation.\n-\/\/\n-\/\/ This code is distributed in the hope that it will be useful, but WITHOUT\n-\/\/ ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-\/\/ FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-\/\/ version 2 for more details (a copy is included in the LICENSE file that\n-\/\/ accompanied this code).\n-\/\/\n-\/\/ You should have received a copy of the GNU General Public License version\n-\/\/ 2 along with this work; if not, write to the Free Software Foundation,\n-\/\/ Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-\/\/\n-\/\/ Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-\/\/ or visit www.oracle.com if you need additional information or have any\n-\/\/ questions.\n-\/\/\n-\/\/\n-\n-\/\/ X86 Architecture Description File\n-\n-\/\/----------REGISTER DEFINITION BLOCK------------------------------------------\n-\/\/ This information is used by the matcher and the register allocator to\n-\/\/ describe individual registers and classes of registers within the target\n-\/\/ architecture.\n-\n-register %{\n-\/\/----------Architecture Description Register Definitions----------------------\n-\/\/ General Registers\n-\/\/ \"reg_def\"  name ( register save type, C convention save type,\n-\/\/                   ideal register type, encoding );\n-\/\/ Register Save Types:\n-\/\/\n-\/\/ NS  = No-Save:       The register allocator assumes that these registers\n-\/\/                      can be used without saving upon entry to the method, &\n-\/\/                      that they do not need to be saved at call sites.\n-\/\/\n-\/\/ SOC = Save-On-Call:  The register allocator assumes that these registers\n-\/\/                      can be used without saving upon entry to the method,\n-\/\/                      but that they must be saved at call sites.\n-\/\/\n-\/\/ SOE = Save-On-Entry: The register allocator assumes that these registers\n-\/\/                      must be saved before using them upon entry to the\n-\/\/                      method, but they do not need to be saved at call\n-\/\/                      sites.\n-\/\/\n-\/\/ AS  = Always-Save:   The register allocator assumes that these registers\n-\/\/                      must be saved before using them upon entry to the\n-\/\/                      method, & that they must be saved at call sites.\n-\/\/\n-\/\/ Ideal Register Type is used to determine how to save & restore a\n-\/\/ register.  Op_RegI will get spilled with LoadI\/StoreI, Op_RegP will get\n-\/\/ spilled with LoadP\/StoreP.  If the register supports both, use Op_RegI.\n-\/\/\n-\/\/ The encoding number is the actual bit-pattern placed into the opcodes.\n-\n-\/\/ General Registers\n-\/\/ Previously set EBX, ESI, and EDI as save-on-entry for java code\n-\/\/ Turn off SOE in java-code due to frequent use of uncommon-traps.\n-\/\/ Now that allocator is better, turn on ESI and EDI as SOE registers.\n-\n-reg_def EBX(SOC, SOE, Op_RegI, 3, rbx->as_VMReg());\n-reg_def ECX(SOC, SOC, Op_RegI, 1, rcx->as_VMReg());\n-reg_def ESI(SOC, SOE, Op_RegI, 6, rsi->as_VMReg());\n-reg_def EDI(SOC, SOE, Op_RegI, 7, rdi->as_VMReg());\n-\/\/ now that adapter frames are gone EBP is always saved and restored by the prolog\/epilog code\n-reg_def EBP(NS, SOE, Op_RegI, 5, rbp->as_VMReg());\n-reg_def EDX(SOC, SOC, Op_RegI, 2, rdx->as_VMReg());\n-reg_def EAX(SOC, SOC, Op_RegI, 0, rax->as_VMReg());\n-reg_def ESP( NS,  NS, Op_RegI, 4, rsp->as_VMReg());\n-\n-\/\/ Float registers.  We treat TOS\/FPR0 special.  It is invisible to the\n-\/\/ allocator, and only shows up in the encodings.\n-reg_def FPR0L( SOC, SOC, Op_RegF, 0, VMRegImpl::Bad());\n-reg_def FPR0H( SOC, SOC, Op_RegF, 0, VMRegImpl::Bad());\n-\/\/ Ok so here's the trick FPR1 is really st(0) except in the midst\n-\/\/ of emission of assembly for a machnode. During the emission the fpu stack\n-\/\/ is pushed making FPR1 == st(1) temporarily. However at any safepoint\n-\/\/ the stack will not have this element so FPR1 == st(0) from the\n-\/\/ oopMap viewpoint. This same weirdness with numbering causes\n-\/\/ instruction encoding to have to play games with the register\n-\/\/ encode to correct for this 0\/1 issue. See MachSpillCopyNode::implementation\n-\/\/ where it does flt->flt moves to see an example\n-\/\/\n-reg_def FPR1L( SOC, SOC, Op_RegF, 1, as_FloatRegister(0)->as_VMReg());\n-reg_def FPR1H( SOC, SOC, Op_RegF, 1, as_FloatRegister(0)->as_VMReg()->next());\n-reg_def FPR2L( SOC, SOC, Op_RegF, 2, as_FloatRegister(1)->as_VMReg());\n-reg_def FPR2H( SOC, SOC, Op_RegF, 2, as_FloatRegister(1)->as_VMReg()->next());\n-reg_def FPR3L( SOC, SOC, Op_RegF, 3, as_FloatRegister(2)->as_VMReg());\n-reg_def FPR3H( SOC, SOC, Op_RegF, 3, as_FloatRegister(2)->as_VMReg()->next());\n-reg_def FPR4L( SOC, SOC, Op_RegF, 4, as_FloatRegister(3)->as_VMReg());\n-reg_def FPR4H( SOC, SOC, Op_RegF, 4, as_FloatRegister(3)->as_VMReg()->next());\n-reg_def FPR5L( SOC, SOC, Op_RegF, 5, as_FloatRegister(4)->as_VMReg());\n-reg_def FPR5H( SOC, SOC, Op_RegF, 5, as_FloatRegister(4)->as_VMReg()->next());\n-reg_def FPR6L( SOC, SOC, Op_RegF, 6, as_FloatRegister(5)->as_VMReg());\n-reg_def FPR6H( SOC, SOC, Op_RegF, 6, as_FloatRegister(5)->as_VMReg()->next());\n-reg_def FPR7L( SOC, SOC, Op_RegF, 7, as_FloatRegister(6)->as_VMReg());\n-reg_def FPR7H( SOC, SOC, Op_RegF, 7, as_FloatRegister(6)->as_VMReg()->next());\n-\/\/\n-\/\/ Empty fill registers, which are never used, but supply alignment to xmm regs\n-\/\/\n-reg_def FILL0( SOC, SOC, Op_RegF, 8, VMRegImpl::Bad());\n-reg_def FILL1( SOC, SOC, Op_RegF, 9, VMRegImpl::Bad());\n-reg_def FILL2( SOC, SOC, Op_RegF, 10, VMRegImpl::Bad());\n-reg_def FILL3( SOC, SOC, Op_RegF, 11, VMRegImpl::Bad());\n-reg_def FILL4( SOC, SOC, Op_RegF, 12, VMRegImpl::Bad());\n-reg_def FILL5( SOC, SOC, Op_RegF, 13, VMRegImpl::Bad());\n-reg_def FILL6( SOC, SOC, Op_RegF, 14, VMRegImpl::Bad());\n-reg_def FILL7( SOC, SOC, Op_RegF, 15, VMRegImpl::Bad());\n-\n-\/\/ Specify priority of register selection within phases of register\n-\/\/ allocation.  Highest priority is first.  A useful heuristic is to\n-\/\/ give registers a low priority when they are required by machine\n-\/\/ instructions, like EAX and EDX.  Registers which are used as\n-\/\/ pairs must fall on an even boundary (witness the FPR#L's in this list).\n-\/\/ For the Intel integer registers, the equivalent Long pairs are\n-\/\/ EDX:EAX, EBX:ECX, and EDI:EBP.\n-alloc_class chunk0( ECX,   EBX,   EBP,   EDI,   EAX,   EDX,   ESI, ESP,\n-                    FPR0L, FPR0H, FPR1L, FPR1H, FPR2L, FPR2H,\n-                    FPR3L, FPR3H, FPR4L, FPR4H, FPR5L, FPR5H,\n-                    FPR6L, FPR6H, FPR7L, FPR7H,\n-                    FILL0, FILL1, FILL2, FILL3, FILL4, FILL5, FILL6, FILL7);\n-\n-\n-\/\/----------Architecture Description Register Classes--------------------------\n-\/\/ Several register classes are automatically defined based upon information in\n-\/\/ this architecture description.\n-\/\/ 1) reg_class inline_cache_reg           ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n-\/\/\n-\/\/ Class for no registers (empty set).\n-reg_class no_reg();\n-\n-\/\/ Class for all registers\n-reg_class any_reg_with_ebp(EAX, EDX, EBP, EDI, ESI, ECX, EBX, ESP);\n-\/\/ Class for all registers (excluding EBP)\n-reg_class any_reg_no_ebp(EAX, EDX, EDI, ESI, ECX, EBX, ESP);\n-\/\/ Dynamic register class that selects at runtime between register classes\n-\/\/ any_reg and any_no_ebp_reg (depending on the value of the flag PreserveFramePointer).\n-\/\/ Equivalent to: return PreserveFramePointer ? any_no_ebp_reg : any_reg;\n-reg_class_dynamic any_reg(any_reg_no_ebp, any_reg_with_ebp, %{ PreserveFramePointer %});\n-\n-\/\/ Class for general registers\n-reg_class int_reg_with_ebp(EAX, EDX, EBP, EDI, ESI, ECX, EBX);\n-\/\/ Class for general registers (excluding EBP).\n-\/\/ It is also safe for use by tailjumps (we don't want to allocate in ebp).\n-\/\/ Used also if the PreserveFramePointer flag is true.\n-reg_class int_reg_no_ebp(EAX, EDX, EDI, ESI, ECX, EBX);\n-\/\/ Dynamic register class that selects between int_reg and int_reg_no_ebp.\n-reg_class_dynamic int_reg(int_reg_no_ebp, int_reg_with_ebp, %{ PreserveFramePointer %});\n-\n-\/\/ Class of \"X\" registers\n-reg_class int_x_reg(EBX, ECX, EDX, EAX);\n-\n-\/\/ Class of registers that can appear in an address with no offset.\n-\/\/ EBP and ESP require an extra instruction byte for zero offset.\n-\/\/ Used in fast-unlock\n-reg_class p_reg(EDX, EDI, ESI, EBX);\n-\n-\/\/ Class for general registers excluding ECX\n-reg_class ncx_reg_with_ebp(EAX, EDX, EBP, EDI, ESI, EBX);\n-\/\/ Class for general registers excluding ECX (and EBP)\n-reg_class ncx_reg_no_ebp(EAX, EDX, EDI, ESI, EBX);\n-\/\/ Dynamic register class that selects between ncx_reg and ncx_reg_no_ebp.\n-reg_class_dynamic ncx_reg(ncx_reg_no_ebp, ncx_reg_with_ebp, %{ PreserveFramePointer %});\n-\n-\/\/ Class for general registers excluding EAX\n-reg_class nax_reg(EDX, EDI, ESI, ECX, EBX);\n-\n-\/\/ Class for general registers excluding EAX and EBX.\n-reg_class nabx_reg_with_ebp(EDX, EDI, ESI, ECX, EBP);\n-\/\/ Class for general registers excluding EAX and EBX (and EBP)\n-reg_class nabx_reg_no_ebp(EDX, EDI, ESI, ECX);\n-\/\/ Dynamic register class that selects between nabx_reg and nabx_reg_no_ebp.\n-reg_class_dynamic nabx_reg(nabx_reg_no_ebp, nabx_reg_with_ebp, %{ PreserveFramePointer %});\n-\n-\/\/ Class of EAX (for multiply and divide operations)\n-reg_class eax_reg(EAX);\n-\n-\/\/ Class of EBX (for atomic add)\n-reg_class ebx_reg(EBX);\n-\n-\/\/ Class of ECX (for shift and JCXZ operations and cmpLTMask)\n-reg_class ecx_reg(ECX);\n-\n-\/\/ Class of EDX (for multiply and divide operations)\n-reg_class edx_reg(EDX);\n-\n-\/\/ Class of EDI (for synchronization)\n-reg_class edi_reg(EDI);\n-\n-\/\/ Class of ESI (for synchronization)\n-reg_class esi_reg(ESI);\n-\n-\/\/ Singleton class for stack pointer\n-reg_class sp_reg(ESP);\n-\n-\/\/ Singleton class for instruction pointer\n-\/\/ reg_class ip_reg(EIP);\n-\n-\/\/ Class of integer register pairs\n-reg_class long_reg_with_ebp( EAX,EDX, ECX,EBX, EBP,EDI );\n-\/\/ Class of integer register pairs (excluding EBP and EDI);\n-reg_class long_reg_no_ebp( EAX,EDX, ECX,EBX );\n-\/\/ Dynamic register class that selects between long_reg and long_reg_no_ebp.\n-reg_class_dynamic long_reg(long_reg_no_ebp, long_reg_with_ebp, %{ PreserveFramePointer %});\n-\n-\/\/ Class of integer register pairs that aligns with calling convention\n-reg_class eadx_reg( EAX,EDX );\n-reg_class ebcx_reg( ECX,EBX );\n-reg_class ebpd_reg( EBP,EDI );\n-\n-\/\/ Not AX or DX, used in divides\n-reg_class nadx_reg_with_ebp(EBX, ECX, ESI, EDI, EBP);\n-\/\/ Not AX or DX (and neither EBP), used in divides\n-reg_class nadx_reg_no_ebp(EBX, ECX, ESI, EDI);\n-\/\/ Dynamic register class that selects between nadx_reg and nadx_reg_no_ebp.\n-reg_class_dynamic nadx_reg(nadx_reg_no_ebp, nadx_reg_with_ebp, %{ PreserveFramePointer %});\n-\n-\/\/ Floating point registers.  Notice FPR0 is not a choice.\n-\/\/ FPR0 is not ever allocated; we use clever encodings to fake\n-\/\/ a 2-address instructions out of Intels FP stack.\n-reg_class fp_flt_reg( FPR1L,FPR2L,FPR3L,FPR4L,FPR5L,FPR6L,FPR7L );\n-\n-reg_class fp_dbl_reg( FPR1L,FPR1H, FPR2L,FPR2H, FPR3L,FPR3H,\n-                      FPR4L,FPR4H, FPR5L,FPR5H, FPR6L,FPR6H,\n-                      FPR7L,FPR7H );\n-\n-reg_class fp_flt_reg0( FPR1L );\n-reg_class fp_dbl_reg0( FPR1L,FPR1H );\n-reg_class fp_dbl_reg1( FPR2L,FPR2H );\n-reg_class fp_dbl_notreg0( FPR2L,FPR2H, FPR3L,FPR3H, FPR4L,FPR4H,\n-                          FPR5L,FPR5H, FPR6L,FPR6H, FPR7L,FPR7H );\n-\n-%}\n-\n-\n-\/\/----------SOURCE BLOCK-------------------------------------------------------\n-\/\/ This is a block of C++ code which provides values, functions, and\n-\/\/ definitions necessary in the rest of the architecture description\n-source_hpp %{\n-\/\/ Must be visible to the DFA in dfa_x86_32.cpp\n-extern bool is_operand_hi32_zero(Node* n);\n-%}\n-\n-source %{\n-#define   RELOC_IMM32    Assembler::imm_operand\n-#define   RELOC_DISP32   Assembler::disp32_operand\n-\n-#define __ masm->\n-\n-\/\/ How to find the high register of a Long pair, given the low register\n-#define   HIGH_FROM_LOW(x) (as_Register((x)->encoding()+2))\n-#define   HIGH_FROM_LOW_ENC(x) ((x)+2)\n-\n-\/\/ These masks are used to provide 128-bit aligned bitmasks to the XMM\n-\/\/ instructions, to allow sign-masking or sign-bit flipping.  They allow\n-\/\/ fast versions of NegF\/NegD and AbsF\/AbsD.\n-\n-void reg_mask_init() {}\n-\n-\/\/ Note: 'double' and 'long long' have 32-bits alignment on x86.\n-static jlong* double_quadword(jlong *adr, jlong lo, jlong hi) {\n-  \/\/ Use the expression (adr)&(~0xF) to provide 128-bits aligned address\n-  \/\/ of 128-bits operands for SSE instructions.\n-  jlong *operand = (jlong*)(((uintptr_t)adr)&((uintptr_t)(~0xF)));\n-  \/\/ Store the value to a 128-bits operand.\n-  operand[0] = lo;\n-  operand[1] = hi;\n-  return operand;\n-}\n-\n-\/\/ Buffer for 128-bits masks used by SSE instructions.\n-static jlong fp_signmask_pool[(4+1)*2]; \/\/ 4*128bits(data) + 128bits(alignment)\n-\n-\/\/ Static initialization during VM startup.\n-static jlong *float_signmask_pool  = double_quadword(&fp_signmask_pool[1*2], CONST64(0x7FFFFFFF7FFFFFFF), CONST64(0x7FFFFFFF7FFFFFFF));\n-static jlong *double_signmask_pool = double_quadword(&fp_signmask_pool[2*2], CONST64(0x7FFFFFFFFFFFFFFF), CONST64(0x7FFFFFFFFFFFFFFF));\n-static jlong *float_signflip_pool  = double_quadword(&fp_signmask_pool[3*2], CONST64(0x8000000080000000), CONST64(0x8000000080000000));\n-static jlong *double_signflip_pool = double_quadword(&fp_signmask_pool[4*2], CONST64(0x8000000000000000), CONST64(0x8000000000000000));\n-\n-\/\/ Offset hacking within calls.\n-static int pre_call_resets_size() {\n-  int size = 0;\n-  Compile* C = Compile::current();\n-  if (C->in_24_bit_fp_mode()) {\n-    size += 6; \/\/ fldcw\n-  }\n-  if (VM_Version::supports_vzeroupper()) {\n-    size += 3; \/\/ vzeroupper\n-  }\n-  return size;\n-}\n-\n-\/\/ !!!!! Special hack to get all type of calls to specify the byte offset\n-\/\/       from the start of the call to the point where the return address\n-\/\/       will point.\n-int MachCallStaticJavaNode::ret_addr_offset() {\n-  return 5 + pre_call_resets_size();  \/\/ 5 bytes from start of call to where return address points\n-}\n-\n-int MachCallDynamicJavaNode::ret_addr_offset() {\n-  return 10 + pre_call_resets_size();  \/\/ 10 bytes from start of call to where return address points\n-}\n-\n-static int sizeof_FFree_Float_Stack_All = -1;\n-\n-int MachCallRuntimeNode::ret_addr_offset() {\n-  assert(sizeof_FFree_Float_Stack_All != -1, \"must have been emitted already\");\n-  return 5 + pre_call_resets_size() + (_leaf_no_fp ? 0 : sizeof_FFree_Float_Stack_All);\n-}\n-\n-\/\/\n-\/\/ Compute padding required for nodes which need alignment\n-\/\/\n-\n-\/\/ The address of the call instruction needs to be 4-byte aligned to\n-\/\/ ensure that it does not span a cache line so that it can be patched.\n-int CallStaticJavaDirectNode::compute_padding(int current_offset) const {\n-  current_offset += pre_call_resets_size();  \/\/ skip fldcw, if any\n-  current_offset += 1;      \/\/ skip call opcode byte\n-  return align_up(current_offset, alignment_required()) - current_offset;\n-}\n-\n-\/\/ The address of the call instruction needs to be 4-byte aligned to\n-\/\/ ensure that it does not span a cache line so that it can be patched.\n-int CallDynamicJavaDirectNode::compute_padding(int current_offset) const {\n-  current_offset += pre_call_resets_size();  \/\/ skip fldcw, if any\n-  current_offset += 5;      \/\/ skip MOV instruction\n-  current_offset += 1;      \/\/ skip call opcode byte\n-  return align_up(current_offset, alignment_required()) - current_offset;\n-}\n-\n-\/\/ EMIT_RM()\n-void emit_rm(C2_MacroAssembler *masm, int f1, int f2, int f3) {\n-  unsigned char c = (unsigned char)((f1 << 6) | (f2 << 3) | f3);\n-  __ emit_int8(c);\n-}\n-\n-\/\/ EMIT_CC()\n-void emit_cc(C2_MacroAssembler *masm, int f1, int f2) {\n-  unsigned char c = (unsigned char)( f1 | f2 );\n-  __ emit_int8(c);\n-}\n-\n-\/\/ EMIT_OPCODE()\n-void emit_opcode(C2_MacroAssembler *masm, int code) {\n-  __ emit_int8((unsigned char) code);\n-}\n-\n-\/\/ EMIT_OPCODE() w\/ relocation information\n-void emit_opcode(C2_MacroAssembler *masm, int code, relocInfo::relocType reloc, int offset = 0) {\n-  __ relocate(__ inst_mark() + offset, reloc);\n-  emit_opcode(masm, code);\n-}\n-\n-\/\/ EMIT_D8()\n-void emit_d8(C2_MacroAssembler *masm, int d8) {\n-  __ emit_int8((unsigned char) d8);\n-}\n-\n-\/\/ EMIT_D16()\n-void emit_d16(C2_MacroAssembler *masm, int d16) {\n-  __ emit_int16(d16);\n-}\n-\n-\/\/ EMIT_D32()\n-void emit_d32(C2_MacroAssembler *masm, int d32) {\n-  __ emit_int32(d32);\n-}\n-\n-\/\/ emit 32 bit value and construct relocation entry from relocInfo::relocType\n-void emit_d32_reloc(C2_MacroAssembler *masm, int d32, relocInfo::relocType reloc,\n-        int format) {\n-  __ relocate(__ inst_mark(), reloc, format);\n-  __ emit_int32(d32);\n-}\n-\n-\/\/ emit 32 bit value and construct relocation entry from RelocationHolder\n-void emit_d32_reloc(C2_MacroAssembler *masm, int d32, RelocationHolder const& rspec,\n-        int format) {\n-#ifdef ASSERT\n-  if (rspec.reloc()->type() == relocInfo::oop_type && d32 != 0 && d32 != (int)Universe::non_oop_word()) {\n-    assert(oopDesc::is_oop(cast_to_oop(d32)), \"cannot embed broken oops in code\");\n-  }\n-#endif\n-  __ relocate(__ inst_mark(), rspec, format);\n-  __ emit_int32(d32);\n-}\n-\n-\/\/ Access stack slot for load or store\n-void store_to_stackslot(C2_MacroAssembler *masm, int opcode, int rm_field, int disp) {\n-  emit_opcode( masm, opcode );               \/\/ (e.g., FILD   [ESP+src])\n-  if( -128 <= disp && disp <= 127 ) {\n-    emit_rm( masm, 0x01, rm_field, ESP_enc );  \/\/ R\/M byte\n-    emit_rm( masm, 0x00, ESP_enc, ESP_enc);    \/\/ SIB byte\n-    emit_d8 (masm, disp);     \/\/ Displacement  \/\/ R\/M byte\n-  } else {\n-    emit_rm( masm, 0x02, rm_field, ESP_enc );  \/\/ R\/M byte\n-    emit_rm( masm, 0x00, ESP_enc, ESP_enc);    \/\/ SIB byte\n-    emit_d32(masm, disp);     \/\/ Displacement  \/\/ R\/M byte\n-  }\n-}\n-\n-   \/\/ rRegI ereg, memory mem) %{    \/\/ emit_reg_mem\n-void encode_RegMem( C2_MacroAssembler *masm, int reg_encoding, int base, int index, int scale, int displace, relocInfo::relocType disp_reloc ) {\n-  \/\/ There is no index & no scale, use form without SIB byte\n-  if ((index == 0x4) &&\n-      (scale == 0) && (base != ESP_enc)) {\n-    \/\/ If no displacement, mode is 0x0; unless base is [EBP]\n-    if ( (displace == 0) && (base != EBP_enc) ) {\n-      emit_rm(masm, 0x0, reg_encoding, base);\n-    }\n-    else {                    \/\/ If 8-bit displacement, mode 0x1\n-      if ((displace >= -128) && (displace <= 127)\n-          && (disp_reloc == relocInfo::none) ) {\n-        emit_rm(masm, 0x1, reg_encoding, base);\n-        emit_d8(masm, displace);\n-      }\n-      else {                  \/\/ If 32-bit displacement\n-        if (base == -1) { \/\/ Special flag for absolute address\n-          emit_rm(masm, 0x0, reg_encoding, 0x5);\n-          \/\/ (manual lies; no SIB needed here)\n-          if ( disp_reloc != relocInfo::none ) {\n-            emit_d32_reloc(masm, displace, disp_reloc, 1);\n-          } else {\n-            emit_d32      (masm, displace);\n-          }\n-        }\n-        else {                \/\/ Normal base + offset\n-          emit_rm(masm, 0x2, reg_encoding, base);\n-          if ( disp_reloc != relocInfo::none ) {\n-            emit_d32_reloc(masm, displace, disp_reloc, 1);\n-          } else {\n-            emit_d32      (masm, displace);\n-          }\n-        }\n-      }\n-    }\n-  }\n-  else {                      \/\/ Else, encode with the SIB byte\n-    \/\/ If no displacement, mode is 0x0; unless base is [EBP]\n-    if (displace == 0 && (base != EBP_enc)) {  \/\/ If no displacement\n-      emit_rm(masm, 0x0, reg_encoding, 0x4);\n-      emit_rm(masm, scale, index, base);\n-    }\n-    else {                    \/\/ If 8-bit displacement, mode 0x1\n-      if ((displace >= -128) && (displace <= 127)\n-          && (disp_reloc == relocInfo::none) ) {\n-        emit_rm(masm, 0x1, reg_encoding, 0x4);\n-        emit_rm(masm, scale, index, base);\n-        emit_d8(masm, displace);\n-      }\n-      else {                  \/\/ If 32-bit displacement\n-        if (base == 0x04 ) {\n-          emit_rm(masm, 0x2, reg_encoding, 0x4);\n-          emit_rm(masm, scale, index, 0x04);\n-        } else {\n-          emit_rm(masm, 0x2, reg_encoding, 0x4);\n-          emit_rm(masm, scale, index, base);\n-        }\n-        if ( disp_reloc != relocInfo::none ) {\n-          emit_d32_reloc(masm, displace, disp_reloc, 1);\n-        } else {\n-          emit_d32      (masm, displace);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-\n-void encode_Copy( C2_MacroAssembler *masm, int dst_encoding, int src_encoding ) {\n-  if( dst_encoding == src_encoding ) {\n-    \/\/ reg-reg copy, use an empty encoding\n-  } else {\n-    emit_opcode( masm, 0x8B );\n-    emit_rm(masm, 0x3, dst_encoding, src_encoding );\n-  }\n-}\n-\n-void emit_cmpfp_fixup(MacroAssembler* masm) {\n-  Label exit;\n-  __ jccb(Assembler::noParity, exit);\n-  __ pushf();\n-  \/\/\n-  \/\/ comiss\/ucomiss instructions set ZF,PF,CF flags and\n-  \/\/ zero OF,AF,SF for NaN values.\n-  \/\/ Fixup flags by zeroing ZF,PF so that compare of NaN\n-  \/\/ values returns 'less than' result (CF is set).\n-  \/\/ Leave the rest of flags unchanged.\n-  \/\/\n-  \/\/    7 6 5 4 3 2 1 0\n-  \/\/   |S|Z|r|A|r|P|r|C|  (r - reserved bit)\n-  \/\/    0 0 1 0 1 0 1 1   (0x2B)\n-  \/\/\n-  __ andl(Address(rsp, 0), 0xffffff2b);\n-  __ popf();\n-  __ bind(exit);\n-}\n-\n-static void emit_cmpfp3(MacroAssembler* masm, Register dst) {\n-  Label done;\n-  __ movl(dst, -1);\n-  __ jcc(Assembler::parity, done);\n-  __ jcc(Assembler::below, done);\n-  __ setb(Assembler::notEqual, dst);\n-  __ movzbl(dst, dst);\n-  __ bind(done);\n-}\n-\n-\n-\/\/=============================================================================\n-const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::Empty;\n-\n-int ConstantTable::calculate_table_base_offset() const {\n-  return 0;  \/\/ absolute addressing, no offset\n-}\n-\n-bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }\n-void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {\n-  ShouldNotReachHere();\n-}\n-\n-void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {\n-  \/\/ Empty encoding\n-}\n-\n-uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {\n-  return 0;\n-}\n-\n-#ifndef PRODUCT\n-void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n-  st->print(\"# MachConstantBaseNode (empty encoding)\");\n-}\n-#endif\n-\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void MachPrologNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n-  Compile* C = ra_->C;\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove wordSize for return addr which is already pushed.\n-  framesize -= wordSize;\n-\n-  if (C->output()->need_stack_bang(bangsize)) {\n-    framesize -= wordSize;\n-    st->print(\"# stack bang (%d bytes)\", bangsize);\n-    st->print(\"\\n\\t\");\n-    st->print(\"PUSH   EBP\\t# Save EBP\");\n-    if (PreserveFramePointer) {\n-      st->print(\"\\n\\t\");\n-      st->print(\"MOV    EBP, ESP\\t# Save the caller's SP into EBP\");\n-    }\n-    if (framesize) {\n-      st->print(\"\\n\\t\");\n-      st->print(\"SUB    ESP, #%d\\t# Create frame\",framesize);\n-    }\n-  } else {\n-    st->print(\"SUB    ESP, #%d\\t# Create frame\",framesize);\n-    st->print(\"\\n\\t\");\n-    framesize -= wordSize;\n-    st->print(\"MOV    [ESP + #%d], EBP\\t# Save EBP\",framesize);\n-    if (PreserveFramePointer) {\n-      st->print(\"\\n\\t\");\n-      st->print(\"MOV    EBP, ESP\\t# Save the caller's SP into EBP\");\n-      if (framesize > 0) {\n-        st->print(\"\\n\\t\");\n-        st->print(\"ADD    EBP, #%d\", framesize);\n-      }\n-    }\n-  }\n-\n-  if (VerifyStackAtCalls) {\n-    st->print(\"\\n\\t\");\n-    framesize -= wordSize;\n-    st->print(\"MOV    [ESP + #%d], 0xBADB100D\\t# Majik cookie for stack depth check\",framesize);\n-  }\n-\n-  if( C->in_24_bit_fp_mode() ) {\n-    st->print(\"\\n\\t\");\n-    st->print(\"FLDCW  \\t# load 24 bit fpu control word\");\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    st->print(\"\\n\\t\");\n-    st->print(\"# verify FPU stack (must be clean on entry)\");\n-  }\n-\n-#ifdef ASSERT\n-  if (VerifyStackAtCalls) {\n-    st->print(\"\\n\\t\");\n-    st->print(\"# stack alignment check\");\n-  }\n-#endif\n-  st->cr();\n-}\n-#endif\n-\n-\n-void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  Compile* C = ra_->C;\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != nullptr);\n-\n-  C->output()->set_frame_complete(__ offset());\n-\n-  if (C->has_mach_constant_base_node()) {\n-    \/\/ NOTE: We set the table base offset here because users might be\n-    \/\/ emitted before MachConstantBaseNode.\n-    ConstantTable& constant_table = C->output()->constant_table();\n-    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());\n-  }\n-}\n-\n-uint MachPrologNode::size(PhaseRegAlloc *ra_) const {\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it the hard way\n-}\n-\n-int MachPrologNode::reloc() const {\n-  return 0; \/\/ a large enough number\n-}\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void MachEpilogNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {\n-  Compile *C = ra_->C;\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove two words for return addr and rbp,\n-  framesize -= 2*wordSize;\n-\n-  if (C->max_vector_size() > 16) {\n-    st->print(\"VZEROUPPER\");\n-    st->cr(); st->print(\"\\t\");\n-  }\n-  if (C->in_24_bit_fp_mode()) {\n-    st->print(\"FLDCW  standard control word\");\n-    st->cr(); st->print(\"\\t\");\n-  }\n-  if (framesize) {\n-    st->print(\"ADD    ESP,%d\\t# Destroy frame\",framesize);\n-    st->cr(); st->print(\"\\t\");\n-  }\n-  st->print_cr(\"POPL   EBP\"); st->print(\"\\t\");\n-  if (do_polling() && C->is_method_compilation()) {\n-    st->print(\"CMPL    rsp, poll_offset[thread]  \\n\\t\"\n-              \"JA      #safepoint_stub\\t\"\n-              \"# Safepoint: poll for GC\");\n-  }\n-}\n-#endif\n-\n-void MachEpilogNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  Compile *C = ra_->C;\n-\n-  if (C->max_vector_size() > 16) {\n-    \/\/ Clear upper bits of YMM registers when current compiled code uses\n-    \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n-    __ vzeroupper();\n-  }\n-  \/\/ If method set FPU control word, restore to standard control word\n-  if (C->in_24_bit_fp_mode()) {\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-  }\n-\n-  int framesize = C->output()->frame_size_in_bytes();\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove two words for return addr and rbp,\n-  framesize -= 2*wordSize;\n-\n-  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n-\n-  if (framesize >= 128) {\n-    emit_opcode(masm, 0x81); \/\/ add  SP, #framesize\n-    emit_rm(masm, 0x3, 0x00, ESP_enc);\n-    emit_d32(masm, framesize);\n-  } else if (framesize) {\n-    emit_opcode(masm, 0x83); \/\/ add  SP, #framesize\n-    emit_rm(masm, 0x3, 0x00, ESP_enc);\n-    emit_d8(masm, framesize);\n-  }\n-\n-  emit_opcode(masm, 0x58 | EBP_enc);\n-\n-  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {\n-    __ reserved_stack_check();\n-  }\n-\n-  if (do_polling() && C->is_method_compilation()) {\n-    Register thread = as_Register(EBX_enc);\n-    __ get_thread(thread);\n-    Label dummy_label;\n-    Label* code_stub = &dummy_label;\n-    if (!C->output()->in_scratch_emit_size()) {\n-      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n-      C->output()->add_stub(stub);\n-      code_stub = &stub->entry();\n-    }\n-    __ set_inst_mark();\n-    __ relocate(relocInfo::poll_return_type);\n-    __ clear_inst_mark();\n-    __ safepoint_poll(*code_stub, thread, true \/* at_return *\/, true \/* in_nmethod *\/);\n-  }\n-}\n-\n-uint MachEpilogNode::size(PhaseRegAlloc *ra_) const {\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-int MachEpilogNode::reloc() const {\n-  return 0; \/\/ a large enough number\n-}\n-\n-const Pipeline * MachEpilogNode::pipeline() const {\n-  return MachNode::pipeline_class();\n-}\n-\n-\/\/=============================================================================\n-\n-enum RC { rc_bad, rc_int, rc_kreg, rc_float, rc_xmm, rc_stack };\n-static enum RC rc_class( OptoReg::Name reg ) {\n-\n-  if( !OptoReg::is_valid(reg)  ) return rc_bad;\n-  if (OptoReg::is_stack(reg)) return rc_stack;\n-\n-  VMReg r = OptoReg::as_VMReg(reg);\n-  if (r->is_Register()) return rc_int;\n-  if (r->is_FloatRegister()) {\n-    assert(UseSSE < 2, \"shouldn't be used in SSE2+ mode\");\n-    return rc_float;\n-  }\n-  if (r->is_KRegister()) return rc_kreg;\n-  assert(r->is_XMMRegister(), \"must be\");\n-  return rc_xmm;\n-}\n-\n-static int impl_helper( C2_MacroAssembler *masm, bool do_size, bool is_load, int offset, int reg,\n-                        int opcode, const char *op_str, int size, outputStream* st ) {\n-  if( masm ) {\n-    masm->set_inst_mark();\n-    emit_opcode  (masm, opcode );\n-    encode_RegMem(masm, Matcher::_regEncode[reg], ESP_enc, 0x4, 0, offset, relocInfo::none);\n-    masm->clear_inst_mark();\n-#ifndef PRODUCT\n-  } else if( !do_size ) {\n-    if( size != 0 ) st->print(\"\\n\\t\");\n-    if( opcode == 0x8B || opcode == 0x89 ) { \/\/ MOV\n-      if( is_load ) st->print(\"%s   %s,[ESP + #%d]\",op_str,Matcher::regName[reg],offset);\n-      else          st->print(\"%s   [ESP + #%d],%s\",op_str,offset,Matcher::regName[reg]);\n-    } else { \/\/ FLD, FST, PUSH, POP\n-      st->print(\"%s [ESP + #%d]\",op_str,offset);\n-    }\n-#endif\n-  }\n-  int offset_size = (offset == 0) ? 0 : ((offset <= 127) ? 1 : 4);\n-  return size+3+offset_size;\n-}\n-\n-\/\/ Helper for XMM registers.  Extra opcode bits, limited syntax.\n-static int impl_x_helper( C2_MacroAssembler *masm, bool do_size, bool is_load,\n-                         int offset, int reg_lo, int reg_hi, int size, outputStream* st ) {\n-  int in_size_in_bits = Assembler::EVEX_32bit;\n-  int evex_encoding = 0;\n-  if (reg_lo+1 == reg_hi) {\n-    in_size_in_bits = Assembler::EVEX_64bit;\n-    evex_encoding = Assembler::VEX_W;\n-  }\n-  if (masm) {\n-    \/\/ EVEX spills remain EVEX: Compressed displacemement is better than AVX on spill mem operations,\n-    \/\/                          it maps more cases to single byte displacement\n-    __ set_managed();\n-    if (reg_lo+1 == reg_hi) { \/\/ double move?\n-      if (is_load) {\n-        __ movdbl(as_XMMRegister(Matcher::_regEncode[reg_lo]), Address(rsp, offset));\n-      } else {\n-        __ movdbl(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[reg_lo]));\n-      }\n-    } else {\n-      if (is_load) {\n-        __ movflt(as_XMMRegister(Matcher::_regEncode[reg_lo]), Address(rsp, offset));\n-      } else {\n-        __ movflt(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[reg_lo]));\n-      }\n-    }\n-#ifndef PRODUCT\n-  } else if (!do_size) {\n-    if (size != 0) st->print(\"\\n\\t\");\n-    if (reg_lo+1 == reg_hi) { \/\/ double move?\n-      if (is_load) st->print(\"%s %s,[ESP + #%d]\",\n-                              UseXmmLoadAndClearUpper ? \"MOVSD \" : \"MOVLPD\",\n-                              Matcher::regName[reg_lo], offset);\n-      else         st->print(\"MOVSD  [ESP + #%d],%s\",\n-                              offset, Matcher::regName[reg_lo]);\n-    } else {\n-      if (is_load) st->print(\"MOVSS  %s,[ESP + #%d]\",\n-                              Matcher::regName[reg_lo], offset);\n-      else         st->print(\"MOVSS  [ESP + #%d],%s\",\n-                              offset, Matcher::regName[reg_lo]);\n-    }\n-#endif\n-  }\n-  bool is_single_byte = false;\n-  if ((UseAVX > 2) && (offset != 0)) {\n-    is_single_byte = Assembler::query_compressed_disp_byte(offset, true, 0, Assembler::EVEX_T1S, in_size_in_bits, evex_encoding);\n-  }\n-  int offset_size = 0;\n-  if (UseAVX > 2 ) {\n-    offset_size = (offset == 0) ? 0 : ((is_single_byte) ? 1 : 4);\n-  } else {\n-    offset_size = (offset == 0) ? 0 : ((offset <= 127) ? 1 : 4);\n-  }\n-  size += (UseAVX > 2) ? 2 : 0; \/\/ Need an additional two bytes for EVEX\n-  \/\/ VEX_2bytes prefix is used if UseAVX > 0, so it takes the same 2 bytes as SIMD prefix.\n-  return size+5+offset_size;\n-}\n-\n-\n-static int impl_movx_helper( C2_MacroAssembler *masm, bool do_size, int src_lo, int dst_lo,\n-                            int src_hi, int dst_hi, int size, outputStream* st ) {\n-  if (masm) {\n-    \/\/ EVEX spills remain EVEX: logic complex between full EVEX, partial and AVX, manage EVEX spill code one way.\n-    __ set_managed();\n-    if (src_lo+1 == src_hi && dst_lo+1 == dst_hi) { \/\/ double move?\n-      __ movdbl(as_XMMRegister(Matcher::_regEncode[dst_lo]),\n-                as_XMMRegister(Matcher::_regEncode[src_lo]));\n-    } else {\n-      __ movflt(as_XMMRegister(Matcher::_regEncode[dst_lo]),\n-                as_XMMRegister(Matcher::_regEncode[src_lo]));\n-    }\n-#ifndef PRODUCT\n-  } else if (!do_size) {\n-    if (size != 0) st->print(\"\\n\\t\");\n-    if (UseXmmRegToRegMoveAll) {\/\/Use movaps,movapd to move between xmm registers\n-      if (src_lo+1 == src_hi && dst_lo+1 == dst_hi) { \/\/ double move?\n-        st->print(\"MOVAPD %s,%s\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      } else {\n-        st->print(\"MOVAPS %s,%s\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      }\n-    } else {\n-      if( src_lo+1 == src_hi && dst_lo+1 == dst_hi ) { \/\/ double move?\n-        st->print(\"MOVSD  %s,%s\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      } else {\n-        st->print(\"MOVSS  %s,%s\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      }\n-    }\n-#endif\n-  }\n-  \/\/ VEX_2bytes prefix is used if UseAVX > 0, and it takes the same 2 bytes as SIMD prefix.\n-  \/\/ Only MOVAPS SSE prefix uses 1 byte.  EVEX uses an additional 2 bytes.\n-  int sz = (UseAVX > 2) ? 6 : 4;\n-  if (!(src_lo+1 == src_hi && dst_lo+1 == dst_hi) &&\n-      UseXmmRegToRegMoveAll && (UseAVX == 0)) sz = 3;\n-  return size + sz;\n-}\n-\n-static int impl_movgpr2x_helper( C2_MacroAssembler *masm, bool do_size, int src_lo, int dst_lo,\n-                            int src_hi, int dst_hi, int size, outputStream* st ) {\n-  \/\/ 32-bit\n-  if (masm) {\n-    \/\/ EVEX spills remain EVEX: logic complex between full EVEX, partial and AVX, manage EVEX spill code one way.\n-    __ set_managed();\n-    __ movdl(as_XMMRegister(Matcher::_regEncode[dst_lo]),\n-             as_Register(Matcher::_regEncode[src_lo]));\n-#ifndef PRODUCT\n-  } else if (!do_size) {\n-    st->print(\"movdl   %s, %s\\t# spill\", Matcher::regName[dst_lo], Matcher::regName[src_lo]);\n-#endif\n-  }\n-  return (UseAVX> 2) ? 6 : 4;\n-}\n-\n-\n-static int impl_movx2gpr_helper( C2_MacroAssembler *masm, bool do_size, int src_lo, int dst_lo,\n-                                 int src_hi, int dst_hi, int size, outputStream* st ) {\n-  \/\/ 32-bit\n-  if (masm) {\n-    \/\/ EVEX spills remain EVEX: logic complex between full EVEX, partial and AVX, manage EVEX spill code one way.\n-    __ set_managed();\n-    __ movdl(as_Register(Matcher::_regEncode[dst_lo]),\n-             as_XMMRegister(Matcher::_regEncode[src_lo]));\n-#ifndef PRODUCT\n-  } else if (!do_size) {\n-    st->print(\"movdl   %s, %s\\t# spill\", Matcher::regName[dst_lo], Matcher::regName[src_lo]);\n-#endif\n-  }\n-  return (UseAVX> 2) ? 6 : 4;\n-}\n-\n-static int impl_mov_helper( C2_MacroAssembler *masm, bool do_size, int src, int dst, int size, outputStream* st ) {\n-  if( masm ) {\n-    emit_opcode(masm, 0x8B );\n-    emit_rm    (masm, 0x3, Matcher::_regEncode[dst], Matcher::_regEncode[src] );\n-#ifndef PRODUCT\n-  } else if( !do_size ) {\n-    if( size != 0 ) st->print(\"\\n\\t\");\n-    st->print(\"MOV    %s,%s\",Matcher::regName[dst],Matcher::regName[src]);\n-#endif\n-  }\n-  return size+2;\n-}\n-\n-static int impl_fp_store_helper( C2_MacroAssembler *masm, bool do_size, int src_lo, int src_hi, int dst_lo, int dst_hi,\n-                                 int offset, int size, outputStream* st ) {\n-  if( src_lo != FPR1L_num ) {      \/\/ Move value to top of FP stack, if not already there\n-    if( masm ) {\n-      emit_opcode( masm, 0xD9 );  \/\/ FLD (i.e., push it)\n-      emit_d8( masm, 0xC0-1+Matcher::_regEncode[src_lo] );\n-#ifndef PRODUCT\n-    } else if( !do_size ) {\n-      if( size != 0 ) st->print(\"\\n\\t\");\n-      st->print(\"FLD    %s\",Matcher::regName[src_lo]);\n-#endif\n-    }\n-    size += 2;\n-  }\n-\n-  int st_op = (src_lo != FPR1L_num) ? EBX_num \/*store & pop*\/ : EDX_num \/*store no pop*\/;\n-  const char *op_str;\n-  int op;\n-  if( src_lo+1 == src_hi && dst_lo+1 == dst_hi ) { \/\/ double store?\n-    op_str = (src_lo != FPR1L_num) ? \"FSTP_D\" : \"FST_D \";\n-    op = 0xDD;\n-  } else {                   \/\/ 32-bit store\n-    op_str = (src_lo != FPR1L_num) ? \"FSTP_S\" : \"FST_S \";\n-    op = 0xD9;\n-    assert( !OptoReg::is_valid(src_hi) && !OptoReg::is_valid(dst_hi), \"no non-adjacent float-stores\" );\n-  }\n-\n-  return impl_helper(masm,do_size,false,offset,st_op,op,op_str,size, st);\n-}\n-\n-\/\/ Next two methods are shared by 32- and 64-bit VM. They are defined in x86.ad.\n-static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n-                          int src_hi, int dst_hi, uint ireg, outputStream* st);\n-\n-void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n-                            int stack_offset, int reg, uint ireg, outputStream* st);\n-\n-static void vec_stack_to_stack_helper(C2_MacroAssembler *masm, int src_offset,\n-                                     int dst_offset, uint ireg, outputStream* st) {\n-  if (masm) {\n-    switch (ireg) {\n-    case Op_VecS:\n-      __ pushl(Address(rsp, src_offset));\n-      __ popl (Address(rsp, dst_offset));\n-      break;\n-    case Op_VecD:\n-      __ pushl(Address(rsp, src_offset));\n-      __ popl (Address(rsp, dst_offset));\n-      __ pushl(Address(rsp, src_offset+4));\n-      __ popl (Address(rsp, dst_offset+4));\n-      break;\n-    case Op_VecX:\n-      __ movdqu(Address(rsp, -16), xmm0);\n-      __ movdqu(xmm0, Address(rsp, src_offset));\n-      __ movdqu(Address(rsp, dst_offset), xmm0);\n-      __ movdqu(xmm0, Address(rsp, -16));\n-      break;\n-    case Op_VecY:\n-      __ vmovdqu(Address(rsp, -32), xmm0);\n-      __ vmovdqu(xmm0, Address(rsp, src_offset));\n-      __ vmovdqu(Address(rsp, dst_offset), xmm0);\n-      __ vmovdqu(xmm0, Address(rsp, -32));\n-      break;\n-    case Op_VecZ:\n-      __ evmovdquq(Address(rsp, -64), xmm0, 2);\n-      __ evmovdquq(xmm0, Address(rsp, src_offset), 2);\n-      __ evmovdquq(Address(rsp, dst_offset), xmm0, 2);\n-      __ evmovdquq(xmm0, Address(rsp, -64), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#ifndef PRODUCT\n-  } else {\n-    switch (ireg) {\n-    case Op_VecS:\n-      st->print(\"pushl   [rsp + #%d]\\t# 32-bit mem-mem spill\\n\\t\"\n-                \"popl    [rsp + #%d]\",\n-                src_offset, dst_offset);\n-      break;\n-    case Op_VecD:\n-      st->print(\"pushl   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n-                \"popq    [rsp + #%d]\\n\\t\"\n-                \"pushl   [rsp + #%d]\\n\\t\"\n-                \"popq    [rsp + #%d]\",\n-                src_offset, dst_offset, src_offset+4, dst_offset+4);\n-      break;\n-     case Op_VecX:\n-      st->print(\"movdqu  [rsp - #16], xmm0\\t# 128-bit mem-mem spill\\n\\t\"\n-                \"movdqu  xmm0, [rsp + #%d]\\n\\t\"\n-                \"movdqu  [rsp + #%d], xmm0\\n\\t\"\n-                \"movdqu  xmm0, [rsp - #16]\",\n-                src_offset, dst_offset);\n-      break;\n-    case Op_VecY:\n-      st->print(\"vmovdqu [rsp - #32], xmm0\\t# 256-bit mem-mem spill\\n\\t\"\n-                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n-                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n-                \"vmovdqu xmm0, [rsp - #32]\",\n-                src_offset, dst_offset);\n-      break;\n-    case Op_VecZ:\n-      st->print(\"vmovdqu [rsp - #64], xmm0\\t# 512-bit mem-mem spill\\n\\t\"\n-                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n-                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n-                \"vmovdqu xmm0, [rsp - #64]\",\n-                src_offset, dst_offset);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#endif\n-  }\n-}\n-\n-uint MachSpillCopyNode::implementation( C2_MacroAssembler *masm, PhaseRegAlloc *ra_, bool do_size, outputStream* st ) const {\n-  \/\/ Get registers to move\n-  OptoReg::Name src_second = ra_->get_reg_second(in(1));\n-  OptoReg::Name src_first = ra_->get_reg_first(in(1));\n-  OptoReg::Name dst_second = ra_->get_reg_second(this );\n-  OptoReg::Name dst_first = ra_->get_reg_first(this );\n-\n-  enum RC src_second_rc = rc_class(src_second);\n-  enum RC src_first_rc = rc_class(src_first);\n-  enum RC dst_second_rc = rc_class(dst_second);\n-  enum RC dst_first_rc = rc_class(dst_first);\n-\n-  assert( OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first), \"must move at least 1 register\" );\n-\n-  \/\/ Generate spill code!\n-  int size = 0;\n-\n-  if( src_first == dst_first && src_second == dst_second )\n-    return size;            \/\/ Self copy, no move\n-\n-  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n-    uint ireg = ideal_reg();\n-    assert((src_first_rc != rc_int && dst_first_rc != rc_int), \"sanity\");\n-    assert((src_first_rc != rc_float && dst_first_rc != rc_float), \"sanity\");\n-    assert((ireg == Op_VecS || ireg == Op_VecD || ireg == Op_VecX || ireg == Op_VecY || ireg == Op_VecZ ), \"sanity\");\n-    if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {\n-      \/\/ mem -> mem\n-      int src_offset = ra_->reg2offset(src_first);\n-      int dst_offset = ra_->reg2offset(dst_first);\n-      vec_stack_to_stack_helper(masm, src_offset, dst_offset, ireg, st);\n-    } else if (src_first_rc == rc_xmm && dst_first_rc == rc_xmm ) {\n-      vec_mov_helper(masm, src_first, dst_first, src_second, dst_second, ireg, st);\n-    } else if (src_first_rc == rc_xmm && dst_first_rc == rc_stack ) {\n-      int stack_offset = ra_->reg2offset(dst_first);\n-      vec_spill_helper(masm, false, stack_offset, src_first, ireg, st);\n-    } else if (src_first_rc == rc_stack && dst_first_rc == rc_xmm ) {\n-      int stack_offset = ra_->reg2offset(src_first);\n-      vec_spill_helper(masm, true,  stack_offset, dst_first, ireg, st);\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  \/\/ --------------------------------------\n-  \/\/ Check for mem-mem move.  push\/pop to move.\n-  if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {\n-    if( src_second == dst_first ) { \/\/ overlapping stack copy ranges\n-      assert( src_second_rc == rc_stack && dst_second_rc == rc_stack, \"we only expect a stk-stk copy here\" );\n-      size = impl_helper(masm,do_size,true ,ra_->reg2offset(src_second),ESI_num,0xFF,\"PUSH  \",size, st);\n-      size = impl_helper(masm,do_size,false,ra_->reg2offset(dst_second),EAX_num,0x8F,\"POP   \",size, st);\n-      src_second_rc = dst_second_rc = rc_bad;  \/\/ flag as already moved the second bits\n-    }\n-    \/\/ move low bits\n-    size = impl_helper(masm,do_size,true ,ra_->reg2offset(src_first),ESI_num,0xFF,\"PUSH  \",size, st);\n-    size = impl_helper(masm,do_size,false,ra_->reg2offset(dst_first),EAX_num,0x8F,\"POP   \",size, st);\n-    if( src_second_rc == rc_stack && dst_second_rc == rc_stack ) { \/\/ mov second bits\n-      size = impl_helper(masm,do_size,true ,ra_->reg2offset(src_second),ESI_num,0xFF,\"PUSH  \",size, st);\n-      size = impl_helper(masm,do_size,false,ra_->reg2offset(dst_second),EAX_num,0x8F,\"POP   \",size, st);\n-    }\n-    return size;\n-  }\n-\n-  \/\/ --------------------------------------\n-  \/\/ Check for integer reg-reg copy\n-  if( src_first_rc == rc_int && dst_first_rc == rc_int )\n-    size = impl_mov_helper(masm,do_size,src_first,dst_first,size, st);\n-\n-  \/\/ Check for integer store\n-  if( src_first_rc == rc_int && dst_first_rc == rc_stack )\n-    size = impl_helper(masm,do_size,false,ra_->reg2offset(dst_first),src_first,0x89,\"MOV \",size, st);\n-\n-  \/\/ Check for integer load\n-  if( src_first_rc == rc_stack && dst_first_rc == rc_int )\n-    size = impl_helper(masm,do_size,true ,ra_->reg2offset(src_first),dst_first,0x8B,\"MOV \",size, st);\n-\n-  \/\/ Check for integer reg-xmm reg copy\n-  if( src_first_rc == rc_int && dst_first_rc == rc_xmm ) {\n-    assert( (src_second_rc == rc_bad && dst_second_rc == rc_bad),\n-            \"no 64 bit integer-float reg moves\" );\n-    return impl_movgpr2x_helper(masm,do_size,src_first,dst_first,src_second, dst_second, size, st);\n-  }\n-  \/\/ --------------------------------------\n-  \/\/ Check for float reg-reg copy\n-  if( src_first_rc == rc_float && dst_first_rc == rc_float ) {\n-    assert( (src_second_rc == rc_bad && dst_second_rc == rc_bad) ||\n-            (src_first+1 == src_second && dst_first+1 == dst_second), \"no non-adjacent float-moves\" );\n-    if( masm ) {\n-\n-      \/\/ Note the mucking with the register encode to compensate for the 0\/1\n-      \/\/ indexing issue mentioned in a comment in the reg_def sections\n-      \/\/ for FPR registers many lines above here.\n-\n-      if( src_first != FPR1L_num ) {\n-        emit_opcode  (masm, 0xD9 );           \/\/ FLD    ST(i)\n-        emit_d8      (masm, 0xC0+Matcher::_regEncode[src_first]-1 );\n-        emit_opcode  (masm, 0xDD );           \/\/ FSTP   ST(i)\n-        emit_d8      (masm, 0xD8+Matcher::_regEncode[dst_first] );\n-     } else {\n-        emit_opcode  (masm, 0xDD );           \/\/ FST    ST(i)\n-        emit_d8      (masm, 0xD0+Matcher::_regEncode[dst_first]-1 );\n-     }\n-#ifndef PRODUCT\n-    } else if( !do_size ) {\n-      if( size != 0 ) st->print(\"\\n\\t\");\n-      if( src_first != FPR1L_num ) st->print(\"FLD    %s\\n\\tFSTP   %s\",Matcher::regName[src_first],Matcher::regName[dst_first]);\n-      else                      st->print(             \"FST    %s\",                            Matcher::regName[dst_first]);\n-#endif\n-    }\n-    return size + ((src_first != FPR1L_num) ? 2+2 : 2);\n-  }\n-\n-  \/\/ Check for float store\n-  if( src_first_rc == rc_float && dst_first_rc == rc_stack ) {\n-    return impl_fp_store_helper(masm,do_size,src_first,src_second,dst_first,dst_second,ra_->reg2offset(dst_first),size, st);\n-  }\n-\n-  \/\/ Check for float load\n-  if( dst_first_rc == rc_float && src_first_rc == rc_stack ) {\n-    int offset = ra_->reg2offset(src_first);\n-    const char *op_str;\n-    int op;\n-    if( src_first+1 == src_second && dst_first+1 == dst_second ) { \/\/ double load?\n-      op_str = \"FLD_D\";\n-      op = 0xDD;\n-    } else {                   \/\/ 32-bit load\n-      op_str = \"FLD_S\";\n-      op = 0xD9;\n-      assert( src_second_rc == rc_bad && dst_second_rc == rc_bad, \"no non-adjacent float-loads\" );\n-    }\n-    if( masm ) {\n-      masm->set_inst_mark();\n-      emit_opcode  (masm, op );\n-      encode_RegMem(masm, 0x0, ESP_enc, 0x4, 0, offset, relocInfo::none);\n-      emit_opcode  (masm, 0xDD );           \/\/ FSTP   ST(i)\n-      emit_d8      (masm, 0xD8+Matcher::_regEncode[dst_first] );\n-      masm->clear_inst_mark();\n-#ifndef PRODUCT\n-    } else if( !do_size ) {\n-      if( size != 0 ) st->print(\"\\n\\t\");\n-      st->print(\"%s  ST,[ESP + #%d]\\n\\tFSTP   %s\",op_str, offset,Matcher::regName[dst_first]);\n-#endif\n-    }\n-    int offset_size = (offset == 0) ? 0 : ((offset <= 127) ? 1 : 4);\n-    return size + 3+offset_size+2;\n-  }\n-\n-  \/\/ Check for xmm reg-reg copy\n-  if( src_first_rc == rc_xmm && dst_first_rc == rc_xmm ) {\n-    assert( (src_second_rc == rc_bad && dst_second_rc == rc_bad) ||\n-            (src_first+1 == src_second && dst_first+1 == dst_second),\n-            \"no non-adjacent float-moves\" );\n-    return impl_movx_helper(masm,do_size,src_first,dst_first,src_second, dst_second, size, st);\n-  }\n-\n-  \/\/ Check for xmm reg-integer reg copy\n-  if( src_first_rc == rc_xmm && dst_first_rc == rc_int ) {\n-    assert( (src_second_rc == rc_bad && dst_second_rc == rc_bad),\n-            \"no 64 bit float-integer reg moves\" );\n-    return impl_movx2gpr_helper(masm,do_size,src_first,dst_first,src_second, dst_second, size, st);\n-  }\n-\n-  \/\/ Check for xmm store\n-  if( src_first_rc == rc_xmm && dst_first_rc == rc_stack ) {\n-    return impl_x_helper(masm,do_size,false,ra_->reg2offset(dst_first), src_first, src_second, size, st);\n-  }\n-\n-  \/\/ Check for float xmm load\n-  if( src_first_rc == rc_stack && dst_first_rc == rc_xmm ) {\n-    return impl_x_helper(masm,do_size,true ,ra_->reg2offset(src_first),dst_first, dst_second, size, st);\n-  }\n-\n-  \/\/ Copy from float reg to xmm reg\n-  if( src_first_rc == rc_float && dst_first_rc == rc_xmm ) {\n-    \/\/ copy to the top of stack from floating point reg\n-    \/\/ and use LEA to preserve flags\n-    if( masm ) {\n-      emit_opcode(masm,0x8D);  \/\/ LEA  ESP,[ESP-8]\n-      emit_rm(masm, 0x1, ESP_enc, 0x04);\n-      emit_rm(masm, 0x0, 0x04, ESP_enc);\n-      emit_d8(masm,0xF8);\n-#ifndef PRODUCT\n-    } else if( !do_size ) {\n-      if( size != 0 ) st->print(\"\\n\\t\");\n-      st->print(\"LEA    ESP,[ESP-8]\");\n-#endif\n-    }\n-    size += 4;\n-\n-    size = impl_fp_store_helper(masm,do_size,src_first,src_second,dst_first,dst_second,0,size, st);\n-\n-    \/\/ Copy from the temp memory to the xmm reg.\n-    size = impl_x_helper(masm,do_size,true ,0,dst_first, dst_second, size, st);\n-\n-    if( masm ) {\n-      emit_opcode(masm,0x8D);  \/\/ LEA  ESP,[ESP+8]\n-      emit_rm(masm, 0x1, ESP_enc, 0x04);\n-      emit_rm(masm, 0x0, 0x04, ESP_enc);\n-      emit_d8(masm,0x08);\n-#ifndef PRODUCT\n-    } else if( !do_size ) {\n-      if( size != 0 ) st->print(\"\\n\\t\");\n-      st->print(\"LEA    ESP,[ESP+8]\");\n-#endif\n-    }\n-    size += 4;\n-    return size;\n-  }\n-\n-  \/\/ AVX-512 opmask specific spilling.\n-  if (src_first_rc == rc_stack && dst_first_rc == rc_kreg) {\n-    assert((src_first & 1) == 0 && src_first + 1 == src_second, \"invalid register pair\");\n-    assert((dst_first & 1) == 0 && dst_first + 1 == dst_second, \"invalid register pair\");\n-    int offset = ra_->reg2offset(src_first);\n-    if (masm != nullptr) {\n-      __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n-#ifndef PRODUCT\n-    } else {\n-      st->print(\"KMOV    %s, [ESP + %d]\", Matcher::regName[dst_first], offset);\n-#endif\n-    }\n-    return 0;\n-  }\n-\n-  if (src_first_rc == rc_kreg && dst_first_rc == rc_stack) {\n-    assert((src_first & 1) == 0 && src_first + 1 == src_second, \"invalid register pair\");\n-    assert((dst_first & 1) == 0 && dst_first + 1 == dst_second, \"invalid register pair\");\n-    int offset = ra_->reg2offset(dst_first);\n-    if (masm != nullptr) {\n-      __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-    } else {\n-      st->print(\"KMOV    [ESP + %d], %s\", offset, Matcher::regName[src_first]);\n-#endif\n-    }\n-    return 0;\n-  }\n-\n-  if (src_first_rc == rc_kreg && dst_first_rc == rc_int) {\n-    Unimplemented();\n-    return 0;\n-  }\n-\n-  if (src_first_rc == rc_int && dst_first_rc == rc_kreg) {\n-    Unimplemented();\n-    return 0;\n-  }\n-\n-  if (src_first_rc == rc_kreg && dst_first_rc == rc_kreg) {\n-    assert((src_first & 1) == 0 && src_first + 1 == src_second, \"invalid register pair\");\n-    assert((dst_first & 1) == 0 && dst_first + 1 == dst_second, \"invalid register pair\");\n-    if (masm != nullptr) {\n-      __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n-#ifndef PRODUCT\n-    } else {\n-      st->print(\"KMOV    %s, %s\", Matcher::regName[dst_first], Matcher::regName[src_first]);\n-#endif\n-    }\n-    return 0;\n-  }\n-\n-  assert( size > 0, \"missed a case\" );\n-\n-  \/\/ --------------------------------------------------------------------\n-  \/\/ Check for second bits still needing moving.\n-  if( src_second == dst_second )\n-    return size;               \/\/ Self copy; no move\n-  assert( src_second_rc != rc_bad && dst_second_rc != rc_bad, \"src_second & dst_second cannot be Bad\" );\n-\n-  \/\/ Check for second word int-int move\n-  if( src_second_rc == rc_int && dst_second_rc == rc_int )\n-    return impl_mov_helper(masm,do_size,src_second,dst_second,size, st);\n-\n-  \/\/ Check for second word integer store\n-  if( src_second_rc == rc_int && dst_second_rc == rc_stack )\n-    return impl_helper(masm,do_size,false,ra_->reg2offset(dst_second),src_second,0x89,\"MOV \",size, st);\n-\n-  \/\/ Check for second word integer load\n-  if( dst_second_rc == rc_int && src_second_rc == rc_stack )\n-    return impl_helper(masm,do_size,true ,ra_->reg2offset(src_second),dst_second,0x8B,\"MOV \",size, st);\n-\n-  Unimplemented();\n-  return 0; \/\/ Mute compiler\n-}\n-\n-#ifndef PRODUCT\n-void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream* st) const {\n-  implementation( nullptr, ra_, false, st );\n-}\n-#endif\n-\n-void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  implementation( masm, ra_, false, nullptr );\n-}\n-\n-uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {\n-  return MachNode::size(ra_);\n-}\n-\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void BoxLockNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {\n-  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n-  int reg = ra_->get_reg_first(this);\n-  st->print(\"LEA    %s,[ESP + #%d]\",Matcher::regName[reg],offset);\n-}\n-#endif\n-\n-void BoxLockNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n-  int reg = ra_->get_encode(this);\n-  if( offset >= 128 ) {\n-    emit_opcode(masm, 0x8D);      \/\/ LEA  reg,[SP+offset]\n-    emit_rm(masm, 0x2, reg, 0x04);\n-    emit_rm(masm, 0x0, 0x04, ESP_enc);\n-    emit_d32(masm, offset);\n-  }\n-  else {\n-    emit_opcode(masm, 0x8D);      \/\/ LEA  reg,[SP+offset]\n-    emit_rm(masm, 0x1, reg, 0x04);\n-    emit_rm(masm, 0x0, 0x04, ESP_enc);\n-    emit_d8(masm, offset);\n-  }\n-}\n-\n-uint BoxLockNode::size(PhaseRegAlloc *ra_) const {\n-  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n-  if( offset >= 128 ) {\n-    return 7;\n-  }\n-  else {\n-    return 4;\n-  }\n-}\n-\n-\/\/=============================================================================\n-#ifndef PRODUCT\n-void MachUEPNode::format( PhaseRegAlloc *ra_, outputStream* st ) const {\n-  st->print_cr(  \"CMP    EAX,[ECX+4]\\t# Inline cache check\");\n-  st->print_cr(\"\\tJNE    SharedRuntime::handle_ic_miss_stub\");\n-  st->print_cr(\"\\tNOP\");\n-  st->print_cr(\"\\tNOP\");\n-  if( !OptoBreakpoint )\n-    st->print_cr(\"\\tNOP\");\n-}\n-#endif\n-\n-void MachUEPNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n-  __ ic_check(CodeEntryAlignment);\n-}\n-\n-uint MachUEPNode::size(PhaseRegAlloc *ra_) const {\n-  return MachNode::size(ra_); \/\/ too many variables; just compute it\n-                              \/\/ the hard way\n-}\n-\n-\n-\/\/=============================================================================\n-\n-\/\/ Vector calling convention not supported.\n-bool Matcher::supports_vector_calling_convention() {\n-  return false;\n-}\n-\n-OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n-  Unimplemented();\n-  return OptoRegPair(0, 0);\n-}\n-\n-\/\/ Is this branch offset short enough that a short branch can be used?\n-\/\/\n-\/\/ NOTE: If the platform does not provide any short branch variants, then\n-\/\/       this method should return false for offset 0.\n-bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {\n-  \/\/ The passed offset is relative to address of the branch.\n-  \/\/ On 86 a branch displacement is calculated relative to address\n-  \/\/ of a next instruction.\n-  offset -= br_size;\n-\n-  \/\/ the short version of jmpConUCF2 contains multiple branches,\n-  \/\/ making the reach slightly less\n-  if (rule == jmpConUCF2_rule)\n-    return (-126 <= offset && offset <= 125);\n-  return (-128 <= offset && offset <= 127);\n-}\n-\n-\/\/ Return whether or not this register is ever used as an argument.  This\n-\/\/ function is used on startup to build the trampoline stubs in generateOptoStub.\n-\/\/ Registers not mentioned will be killed by the VM call in the trampoline, and\n-\/\/ arguments in those registers not be available to the callee.\n-bool Matcher::can_be_java_arg( int reg ) {\n-  if(  reg == ECX_num   || reg == EDX_num   ) return true;\n-  if( (reg == XMM0_num  || reg == XMM1_num ) && UseSSE>=1 ) return true;\n-  if( (reg == XMM0b_num || reg == XMM1b_num) && UseSSE>=2 ) return true;\n-  return false;\n-}\n-\n-bool Matcher::is_spillable_arg( int reg ) {\n-  return can_be_java_arg(reg);\n-}\n-\n-uint Matcher::int_pressure_limit()\n-{\n-  return (INTPRESSURE == -1) ? 6 : INTPRESSURE;\n-}\n-\n-uint Matcher::float_pressure_limit()\n-{\n-  return (FLOATPRESSURE == -1) ? 6 : FLOATPRESSURE;\n-}\n-\n-bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {\n-  \/\/ Use hardware integer DIV instruction when\n-  \/\/ it is faster than a code which use multiply.\n-  \/\/ Only when constant divisor fits into 32 bit\n-  \/\/ (min_jint is excluded to get only correct\n-  \/\/ positive 32 bit values from negative).\n-  return VM_Version::has_fast_idiv() &&\n-         (divisor == (int)divisor && divisor != min_jint);\n-}\n-\n-\/\/ Register for DIVI projection of divmodI\n-RegMask Matcher::divI_proj_mask() {\n-  return EAX_REG_mask();\n-}\n-\n-\/\/ Register for MODI projection of divmodI\n-RegMask Matcher::modI_proj_mask() {\n-  return EDX_REG_mask();\n-}\n-\n-\/\/ Register for DIVL projection of divmodL\n-RegMask Matcher::divL_proj_mask() {\n-  ShouldNotReachHere();\n-  return RegMask();\n-}\n-\n-\/\/ Register for MODL projection of divmodL\n-RegMask Matcher::modL_proj_mask() {\n-  ShouldNotReachHere();\n-  return RegMask();\n-}\n-\n-const RegMask Matcher::method_handle_invoke_SP_save_mask() {\n-  return NO_REG_mask();\n-}\n-\n-\/\/ Returns true if the high 32 bits of the value is known to be zero.\n-bool is_operand_hi32_zero(Node* n) {\n-  int opc = n->Opcode();\n-  if (opc == Op_AndL) {\n-    Node* o2 = n->in(2);\n-    if (o2->is_Con() && (o2->get_long() & 0xFFFFFFFF00000000LL) == 0LL) {\n-      return true;\n-    }\n-  }\n-  if (opc == Op_ConL && (n->get_long() & 0xFFFFFFFF00000000LL) == 0LL) {\n-    return true;\n-  }\n-  return false;\n-}\n-\n-%}\n-\n-\/\/----------ENCODING BLOCK-----------------------------------------------------\n-\/\/ This block specifies the encoding classes used by the compiler to output\n-\/\/ byte streams.  Encoding classes generate functions which are called by\n-\/\/ Machine Instruction Nodes in order to generate the bit encoding of the\n-\/\/ instruction.  Operands specify their base encoding interface with the\n-\/\/ interface keyword.  There are currently supported four interfaces,\n-\/\/ REG_INTER, CONST_INTER, MEMORY_INTER, & COND_INTER.  REG_INTER causes an\n-\/\/ operand to generate a function which returns its register number when\n-\/\/ queried.   CONST_INTER causes an operand to generate a function which\n-\/\/ returns the value of the constant when queried.  MEMORY_INTER causes an\n-\/\/ operand to generate four functions which return the Base Register, the\n-\/\/ Index Register, the Scale Value, and the Offset Value of the operand when\n-\/\/ queried.  COND_INTER causes an operand to generate six functions which\n-\/\/ return the encoding code (ie - encoding bits for the instruction)\n-\/\/ associated with each basic boolean condition for a conditional instruction.\n-\/\/ Instructions specify two basic values for encoding.  They use the\n-\/\/ ins_encode keyword to specify their encoding class (which must be one of\n-\/\/ the class names specified in the encoding block), and they use the\n-\/\/ opcode keyword to specify, in order, their primary, secondary, and\n-\/\/ tertiary opcode.  Only the opcode sections which a particular instruction\n-\/\/ needs for encoding need to be specified.\n-encode %{\n-  \/\/ Build emit functions for each basic byte or larger field in the intel\n-  \/\/ encoding scheme (opcode, rm, sib, immediate), and call them from C++\n-  \/\/ code in the enc_class source block.  Emit functions will live in the\n-  \/\/ main source block for now.  In future, we can generalize this by\n-  \/\/ adding a syntax that specifies the sizes of fields in an order,\n-  \/\/ so that the adlc can build the emit functions automagically\n-\n-  \/\/ Set instruction mark in MacroAssembler. This is used only in\n-  \/\/ instructions that emit bytes directly to the CodeBuffer wraped\n-  \/\/ in the MacroAssembler. Should go away once all \"instruct\" are\n-  \/\/ patched to emit bytes only using methods in MacroAssembler.\n-  enc_class SetInstMark %{\n-    __ set_inst_mark();\n-  %}\n-\n-  enc_class ClearInstMark %{\n-    __ clear_inst_mark();\n-  %}\n-\n-  \/\/ Emit primary opcode\n-  enc_class OpcP %{\n-    emit_opcode(masm, $primary);\n-  %}\n-\n-  \/\/ Emit secondary opcode\n-  enc_class OpcS %{\n-    emit_opcode(masm, $secondary);\n-  %}\n-\n-  \/\/ Emit opcode directly\n-  enc_class Opcode(immI d8) %{\n-    emit_opcode(masm, $d8$$constant);\n-  %}\n-\n-  enc_class SizePrefix %{\n-    emit_opcode(masm,0x66);\n-  %}\n-\n-  enc_class RegReg (rRegI dst, rRegI src) %{    \/\/ RegReg(Many)\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class OpcRegReg (immI opcode, rRegI dst, rRegI src) %{    \/\/ OpcRegReg(Many)\n-    emit_opcode(masm,$opcode$$constant);\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class mov_r32_imm0( rRegI dst ) %{\n-    emit_opcode( masm, 0xB8 + $dst$$reg ); \/\/ 0xB8+ rd   -- MOV r32  ,imm32\n-    emit_d32   ( masm, 0x0  );             \/\/                         imm32==0x0\n-  %}\n-\n-  enc_class cdq_enc %{\n-    \/\/ Full implementation of Java idiv and irem; checks for\n-    \/\/ special case as described in JVM spec., p.243 & p.271.\n-    \/\/\n-    \/\/         normal case                           special case\n-    \/\/\n-    \/\/ input : rax,: dividend                         min_int\n-    \/\/         reg: divisor                          -1\n-    \/\/\n-    \/\/ output: rax,: quotient  (= rax, idiv reg)       min_int\n-    \/\/         rdx: remainder (= rax, irem reg)       0\n-    \/\/\n-    \/\/  Code sequnce:\n-    \/\/\n-    \/\/  81 F8 00 00 00 80    cmp         rax,80000000h\n-    \/\/  0F 85 0B 00 00 00    jne         normal_case\n-    \/\/  33 D2                xor         rdx,edx\n-    \/\/  83 F9 FF             cmp         rcx,0FFh\n-    \/\/  0F 84 03 00 00 00    je          done\n-    \/\/                  normal_case:\n-    \/\/  99                   cdq\n-    \/\/  F7 F9                idiv        rax,ecx\n-    \/\/                  done:\n-    \/\/\n-    emit_opcode(masm,0x81); emit_d8(masm,0xF8);\n-    emit_opcode(masm,0x00); emit_d8(masm,0x00);\n-    emit_opcode(masm,0x00); emit_d8(masm,0x80);                     \/\/ cmp rax,80000000h\n-    emit_opcode(masm,0x0F); emit_d8(masm,0x85);\n-    emit_opcode(masm,0x0B); emit_d8(masm,0x00);\n-    emit_opcode(masm,0x00); emit_d8(masm,0x00);                     \/\/ jne normal_case\n-    emit_opcode(masm,0x33); emit_d8(masm,0xD2);                     \/\/ xor rdx,edx\n-    emit_opcode(masm,0x83); emit_d8(masm,0xF9); emit_d8(masm,0xFF); \/\/ cmp rcx,0FFh\n-    emit_opcode(masm,0x0F); emit_d8(masm,0x84);\n-    emit_opcode(masm,0x03); emit_d8(masm,0x00);\n-    emit_opcode(masm,0x00); emit_d8(masm,0x00);                     \/\/ je done\n-    \/\/ normal_case:\n-    emit_opcode(masm,0x99);                                         \/\/ cdq\n-    \/\/ idiv (note: must be emitted by the user of this rule)\n-    \/\/ normal:\n-  %}\n-\n-  \/\/ Dense encoding for older common ops\n-  enc_class Opc_plus(immI opcode, rRegI reg) %{\n-    emit_opcode(masm, $opcode$$constant + $reg$$reg);\n-  %}\n-\n-\n-  \/\/ Opcde enc_class for 8\/32 bit immediate instructions with sign-extension\n-  enc_class OpcSE (immI imm) %{ \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (($imm$$constant >= -128) && ($imm$$constant <= 127)) {\n-      emit_opcode(masm, $primary | 0x02);\n-    }\n-    else {                          \/\/ If 32-bit immediate\n-      emit_opcode(masm, $primary);\n-    }\n-  %}\n-\n-  enc_class OpcSErm (rRegI dst, immI imm) %{    \/\/ OpcSEr\/m\n-    \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (($imm$$constant >= -128) && ($imm$$constant <= 127)) {\n-      emit_opcode(masm, $primary | 0x02);    }\n-    else {                          \/\/ If 32-bit immediate\n-      emit_opcode(masm, $primary);\n-    }\n-    \/\/ Emit r\/m byte with secondary opcode, after primary opcode.\n-    emit_rm(masm, 0x3, $secondary, $dst$$reg);\n-  %}\n-\n-  enc_class Con8or32 (immI imm) %{    \/\/ Con8or32(storeImmI), 8 or 32 bits\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (($imm$$constant >= -128) && ($imm$$constant <= 127)) {\n-      $$$emit8$imm$$constant;\n-    }\n-    else {                          \/\/ If 32-bit immediate\n-      \/\/ Output immediate\n-      $$$emit32$imm$$constant;\n-    }\n-  %}\n-\n-  enc_class Long_OpcSErm_Lo(eRegL dst, immL imm) %{\n-    \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    int con = (int)$imm$$constant; \/\/ Throw away top bits\n-    emit_opcode(masm, ((con >= -128) && (con <= 127)) ? ($primary | 0x02) : $primary);\n-    \/\/ Emit r\/m byte with secondary opcode, after primary opcode.\n-    emit_rm(masm, 0x3, $secondary, $dst$$reg);\n-    if ((con >= -128) && (con <= 127)) emit_d8 (masm,con);\n-    else                               emit_d32(masm,con);\n-  %}\n-\n-  enc_class Long_OpcSErm_Hi(eRegL dst, immL imm) %{\n-    \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    int con = (int)($imm$$constant >> 32); \/\/ Throw away bottom bits\n-    emit_opcode(masm, ((con >= -128) && (con <= 127)) ? ($primary | 0x02) : $primary);\n-    \/\/ Emit r\/m byte with tertiary opcode, after primary opcode.\n-    emit_rm(masm, 0x3, $tertiary, HIGH_FROM_LOW_ENC($dst$$reg));\n-    if ((con >= -128) && (con <= 127)) emit_d8 (masm,con);\n-    else                               emit_d32(masm,con);\n-  %}\n-\n-  enc_class OpcSReg (rRegI dst) %{    \/\/ BSWAP\n-    emit_cc(masm, $secondary, $dst$$reg );\n-  %}\n-\n-  enc_class bswap_long_bytes(eRegL dst) %{ \/\/ BSWAP\n-    int destlo = $dst$$reg;\n-    int desthi = HIGH_FROM_LOW_ENC(destlo);\n-    \/\/ bswap lo\n-    emit_opcode(masm, 0x0F);\n-    emit_cc(masm, 0xC8, destlo);\n-    \/\/ bswap hi\n-    emit_opcode(masm, 0x0F);\n-    emit_cc(masm, 0xC8, desthi);\n-    \/\/ xchg lo and hi\n-    emit_opcode(masm, 0x87);\n-    emit_rm(masm, 0x3, destlo, desthi);\n-  %}\n-\n-  enc_class RegOpc (rRegI div) %{    \/\/ IDIV, IMOD, JMP indirect, ...\n-    emit_rm(masm, 0x3, $secondary, $div$$reg );\n-  %}\n-\n-  enc_class enc_cmov(cmpOp cop ) %{ \/\/ CMOV\n-    $$$emit8$primary;\n-    emit_cc(masm, $secondary, $cop$$cmpcode);\n-  %}\n-\n-  enc_class enc_cmov_dpr(cmpOp cop, regDPR src ) %{ \/\/ CMOV\n-    int op = 0xDA00 + $cop$$cmpcode + ($src$$reg-1);\n-    emit_d8(masm, op >> 8 );\n-    emit_d8(masm, op & 255);\n-  %}\n-\n-  \/\/ emulate a CMOV with a conditional branch around a MOV\n-  enc_class enc_cmov_branch( cmpOp cop, immI brOffs ) %{ \/\/ CMOV\n-    \/\/ Invert sense of branch from sense of CMOV\n-    emit_cc( masm, 0x70, ($cop$$cmpcode^1) );\n-    emit_d8( masm, $brOffs$$constant );\n-  %}\n-\n-  enc_class enc_PartialSubtypeCheck( ) %{\n-    Register Redi = as_Register(EDI_enc); \/\/ result register\n-    Register Reax = as_Register(EAX_enc); \/\/ super class\n-    Register Recx = as_Register(ECX_enc); \/\/ killed\n-    Register Resi = as_Register(ESI_enc); \/\/ sub class\n-    Label miss;\n-\n-    \/\/ NB: Callers may assume that, when $result is a valid register,\n-    \/\/ check_klass_subtype_slow_path sets it to a nonzero value.\n-     __ check_klass_subtype_slow_path(Resi, Reax, Recx, Redi,\n-                                     nullptr, &miss,\n-                                     \/*set_cond_codes:*\/ true);\n-    if ($primary) {\n-      __ xorptr(Redi, Redi);\n-    }\n-    __ bind(miss);\n-  %}\n-\n-  enc_class FFree_Float_Stack_All %{    \/\/ Free_Float_Stack_All\n-    int start = __ offset();\n-    if (UseSSE >= 2) {\n-      if (VerifyFPU) {\n-        __ verify_FPU(0, \"must be empty in SSE2+ mode\");\n-      }\n-    } else {\n-      \/\/ External c_calling_convention expects the FPU stack to be 'clean'.\n-      \/\/ Compiled code leaves it dirty.  Do cleanup now.\n-      __ empty_FPU_stack();\n-    }\n-    if (sizeof_FFree_Float_Stack_All == -1) {\n-      sizeof_FFree_Float_Stack_All = __ offset() - start;\n-    } else {\n-      assert(__ offset() - start == sizeof_FFree_Float_Stack_All, \"wrong size\");\n-    }\n-  %}\n-\n-  enc_class Verify_FPU_For_Leaf %{\n-    if( VerifyFPU ) {\n-      __ verify_FPU( -3, \"Returning from Runtime Leaf call\");\n-    }\n-  %}\n-\n-  enc_class Java_To_Runtime (method meth) %{    \/\/ CALL Java_To_Runtime, Java_To_Runtime_Leaf\n-    \/\/ This is the instruction starting address for relocation info.\n-    __ set_inst_mark();\n-    $$$emit8$primary;\n-    \/\/ CALL directly to the runtime\n-    emit_d32_reloc(masm, ($meth$$method - (int)(__ pc()) - 4),\n-                runtime_call_Relocation::spec(), RELOC_IMM32 );\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-\n-    if (UseSSE >= 2) {\n-      BasicType rt = tf()->return_type();\n-\n-      if ((rt == T_FLOAT || rt == T_DOUBLE) && !return_value_is_used()) {\n-        \/\/ A C runtime call where the return value is unused.  In SSE2+\n-        \/\/ mode the result needs to be removed from the FPU stack.  It's\n-        \/\/ likely that this function call could be removed by the\n-        \/\/ optimizer if the C function is a pure function.\n-        __ ffree(0);\n-      } else if (rt == T_FLOAT) {\n-        __ lea(rsp, Address(rsp, -4));\n-        __ fstp_s(Address(rsp, 0));\n-        __ movflt(xmm0, Address(rsp, 0));\n-        __ lea(rsp, Address(rsp,  4));\n-      } else if (rt == T_DOUBLE) {\n-        __ lea(rsp, Address(rsp, -8));\n-        __ fstp_d(Address(rsp, 0));\n-        __ movdbl(xmm0, Address(rsp, 0));\n-        __ lea(rsp, Address(rsp,  8));\n-      }\n-    }\n-  %}\n-\n-  enc_class pre_call_resets %{\n-    \/\/ If method sets FPU control word restore it here\n-    debug_only(int off0 = __ offset());\n-    if (ra_->C->in_24_bit_fp_mode()) {\n-      __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-    }\n-    \/\/ Clear upper bits of YMM registers when current compiled code uses\n-    \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n-    __ vzeroupper();\n-    debug_only(int off1 = __ offset());\n-    assert(off1 - off0 == pre_call_resets_size(), \"correct size prediction\");\n-  %}\n-\n-  enc_class post_call_FPU %{\n-    \/\/ If method sets FPU control word do it here also\n-    if (Compile::current()->in_24_bit_fp_mode()) {\n-      __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-    }\n-  %}\n-\n-  enc_class Java_Static_Call (method meth) %{    \/\/ JAVA STATIC CALL\n-    \/\/ CALL to fixup routine.  Fixup routine uses ScopeDesc info to determine\n-    \/\/ who we intended to call.\n-    __ set_inst_mark();\n-    $$$emit8$primary;\n-\n-    if (!_method) {\n-      emit_d32_reloc(masm, ($meth$$method - (int)(__ pc()) - 4),\n-                     runtime_call_Relocation::spec(),\n-                     RELOC_IMM32);\n-      __ clear_inst_mark();\n-      __ post_call_nop();\n-    } else {\n-      int method_index = resolved_method_index(masm);\n-      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)\n-                                                  : static_call_Relocation::spec(method_index);\n-      emit_d32_reloc(masm, ($meth$$method - (int)(__ pc()) - 4),\n-                     rspec, RELOC_DISP32);\n-      __ post_call_nop();\n-      address mark = __ inst_mark();\n-      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n-        \/\/ Calls of the same statically bound method can share\n-        \/\/ a stub to the interpreter.\n-        __ code()->shared_stub_to_interp_for(_method, __ code()->insts()->mark_off());\n-        __ clear_inst_mark();\n-      } else {\n-        \/\/ Emit stubs for static call.\n-        address stub = CompiledDirectCall::emit_to_interp_stub(masm, mark);\n-        __ clear_inst_mark();\n-        if (stub == nullptr) {\n-          ciEnv::current()->record_failure(\"CodeCache is full\");\n-          return;\n-        }\n-      }\n-    }\n-  %}\n-\n-  enc_class Java_Dynamic_Call (method meth) %{    \/\/ JAVA DYNAMIC CALL\n-    __ ic_call((address)$meth$$method, resolved_method_index(masm));\n-    __ post_call_nop();\n-  %}\n-\n-  enc_class Java_Compiled_Call (method meth) %{    \/\/ JAVA COMPILED CALL\n-    int disp = in_bytes(Method::from_compiled_offset());\n-    assert( -128 <= disp && disp <= 127, \"compiled_code_offset isn't small\");\n-\n-    \/\/ CALL *[EAX+in_bytes(Method::from_compiled_code_entry_point_offset())]\n-    __ set_inst_mark();\n-    $$$emit8$primary;\n-    emit_rm(masm, 0x01, $secondary, EAX_enc );  \/\/ R\/M byte\n-    emit_d8(masm, disp);             \/\/ Displacement\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-  %}\n-\n-  enc_class RegOpcImm (rRegI dst, immI8 shift) %{    \/\/ SHL, SAR, SHR\n-    $$$emit8$primary;\n-    emit_rm(masm, 0x3, $secondary, $dst$$reg);\n-    $$$emit8$shift$$constant;\n-  %}\n-\n-  enc_class LdImmI (rRegI dst, immI src) %{    \/\/ Load Immediate\n-    \/\/ Load immediate does not have a zero or sign extended version\n-    \/\/ for 8-bit immediates\n-    emit_opcode(masm, 0xB8 + $dst$$reg);\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class LdImmP (rRegI dst, immI src) %{    \/\/ Load Immediate\n-    \/\/ Load immediate does not have a zero or sign extended version\n-    \/\/ for 8-bit immediates\n-    emit_opcode(masm, $primary + $dst$$reg);\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class LdImmL_Lo( eRegL dst, immL src) %{    \/\/ Load Immediate\n-    \/\/ Load immediate does not have a zero or sign extended version\n-    \/\/ for 8-bit immediates\n-    int dst_enc = $dst$$reg;\n-    int src_con = $src$$constant & 0x0FFFFFFFFL;\n-    if (src_con == 0) {\n-      \/\/ xor dst, dst\n-      emit_opcode(masm, 0x33);\n-      emit_rm(masm, 0x3, dst_enc, dst_enc);\n-    } else {\n-      emit_opcode(masm, $primary + dst_enc);\n-      emit_d32(masm, src_con);\n-    }\n-  %}\n-\n-  enc_class LdImmL_Hi( eRegL dst, immL src) %{    \/\/ Load Immediate\n-    \/\/ Load immediate does not have a zero or sign extended version\n-    \/\/ for 8-bit immediates\n-    int dst_enc = $dst$$reg + 2;\n-    int src_con = ((julong)($src$$constant)) >> 32;\n-    if (src_con == 0) {\n-      \/\/ xor dst, dst\n-      emit_opcode(masm, 0x33);\n-      emit_rm(masm, 0x3, dst_enc, dst_enc);\n-    } else {\n-      emit_opcode(masm, $primary + dst_enc);\n-      emit_d32(masm, src_con);\n-    }\n-  %}\n-\n-\n-  \/\/ Encode a reg-reg copy.  If it is useless, then empty encoding.\n-  enc_class enc_Copy( rRegI dst, rRegI src ) %{\n-    encode_Copy( masm, $dst$$reg, $src$$reg );\n-  %}\n-\n-  enc_class enc_CopyL_Lo( rRegI dst, eRegL src ) %{\n-    encode_Copy( masm, $dst$$reg, $src$$reg );\n-  %}\n-\n-  enc_class RegReg (rRegI dst, rRegI src) %{    \/\/ RegReg(Many)\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class RegReg_Lo(eRegL dst, eRegL src) %{    \/\/ RegReg(Many)\n-    $$$emit8$primary;\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class RegReg_Hi(eRegL dst, eRegL src) %{    \/\/ RegReg(Many)\n-    $$$emit8$secondary;\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), HIGH_FROM_LOW_ENC($src$$reg));\n-  %}\n-\n-  enc_class RegReg_Lo2(eRegL dst, eRegL src) %{    \/\/ RegReg(Many)\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class RegReg_Hi2(eRegL dst, eRegL src) %{    \/\/ RegReg(Many)\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), HIGH_FROM_LOW_ENC($src$$reg));\n-  %}\n-\n-  enc_class RegReg_HiLo( eRegL src, rRegI dst ) %{\n-    emit_rm(masm, 0x3, $dst$$reg, HIGH_FROM_LOW_ENC($src$$reg));\n-  %}\n-\n-  enc_class Con32 (immI src) %{    \/\/ Con32(storeImmI)\n-    \/\/ Output immediate\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class Con32FPR_as_bits(immFPR src) %{        \/\/ storeF_imm\n-    \/\/ Output Float immediate bits\n-    jfloat jf = $src$$constant;\n-    int    jf_as_bits = jint_cast( jf );\n-    emit_d32(masm, jf_as_bits);\n-  %}\n-\n-  enc_class Con32F_as_bits(immF src) %{      \/\/ storeX_imm\n-    \/\/ Output Float immediate bits\n-    jfloat jf = $src$$constant;\n-    int    jf_as_bits = jint_cast( jf );\n-    emit_d32(masm, jf_as_bits);\n-  %}\n-\n-  enc_class Con16 (immI src) %{    \/\/ Con16(storeImmI)\n-    \/\/ Output immediate\n-    $$$emit16$src$$constant;\n-  %}\n-\n-  enc_class Con_d32(immI src) %{\n-    emit_d32(masm,$src$$constant);\n-  %}\n-\n-  enc_class conmemref (eRegP t1) %{    \/\/ Con32(storeImmI)\n-    \/\/ Output immediate memory reference\n-    emit_rm(masm, 0x00, $t1$$reg, 0x05 );\n-    emit_d32(masm, 0x00);\n-  %}\n-\n-  enc_class lock_prefix( ) %{\n-    emit_opcode(masm,0xF0);         \/\/ [Lock]\n-  %}\n-\n-  \/\/ Cmp-xchg long value.\n-  \/\/ Note: we need to swap rbx, and rcx before and after the\n-  \/\/       cmpxchg8 instruction because the instruction uses\n-  \/\/       rcx as the high order word of the new value to store but\n-  \/\/       our register encoding uses rbx,.\n-  enc_class enc_cmpxchg8(eSIRegP mem_ptr) %{\n-\n-    \/\/ XCHG  rbx,ecx\n-    emit_opcode(masm,0x87);\n-    emit_opcode(masm,0xD9);\n-    \/\/ [Lock]\n-    emit_opcode(masm,0xF0);\n-    \/\/ CMPXCHG8 [Eptr]\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xC7);\n-    emit_rm( masm, 0x0, 1, $mem_ptr$$reg );\n-    \/\/ XCHG  rbx,ecx\n-    emit_opcode(masm,0x87);\n-    emit_opcode(masm,0xD9);\n-  %}\n-\n-  enc_class enc_cmpxchg(eSIRegP mem_ptr) %{\n-    \/\/ [Lock]\n-    emit_opcode(masm,0xF0);\n-\n-    \/\/ CMPXCHG [Eptr]\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xB1);\n-    emit_rm( masm, 0x0, 1, $mem_ptr$$reg );\n-  %}\n-\n-  enc_class enc_cmpxchgb(eSIRegP mem_ptr) %{\n-    \/\/ [Lock]\n-    emit_opcode(masm,0xF0);\n-\n-    \/\/ CMPXCHGB [Eptr]\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xB0);\n-    emit_rm( masm, 0x0, 1, $mem_ptr$$reg );\n-  %}\n-\n-  enc_class enc_cmpxchgw(eSIRegP mem_ptr) %{\n-    \/\/ [Lock]\n-    emit_opcode(masm,0xF0);\n-\n-    \/\/ 16-bit mode\n-    emit_opcode(masm, 0x66);\n-\n-    \/\/ CMPXCHGW [Eptr]\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xB1);\n-    emit_rm( masm, 0x0, 1, $mem_ptr$$reg );\n-  %}\n-\n-  enc_class enc_flags_ne_to_boolean( iRegI res ) %{\n-    int res_encoding = $res$$reg;\n-\n-    \/\/ MOV  res,0\n-    emit_opcode( masm, 0xB8 + res_encoding);\n-    emit_d32( masm, 0 );\n-    \/\/ JNE,s  fail\n-    emit_opcode(masm,0x75);\n-    emit_d8(masm, 5 );\n-    \/\/ MOV  res,1\n-    emit_opcode( masm, 0xB8 + res_encoding);\n-    emit_d32( masm, 1 );\n-    \/\/ fail:\n-  %}\n-\n-  enc_class RegMem (rRegI ereg, memory mem) %{    \/\/ emit_reg_mem\n-    int reg_encoding = $ereg$$reg;\n-    int base  = $mem$$base;\n-    int index = $mem$$index;\n-    int scale = $mem$$scale;\n-    int displace = $mem$$disp;\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc();\n-    encode_RegMem(masm, reg_encoding, base, index, scale, displace, disp_reloc);\n-  %}\n-\n-  enc_class RegMem_Hi(eRegL ereg, memory mem) %{    \/\/ emit_reg_mem\n-    int reg_encoding = HIGH_FROM_LOW_ENC($ereg$$reg);  \/\/ Hi register of pair, computed from lo\n-    int base  = $mem$$base;\n-    int index = $mem$$index;\n-    int scale = $mem$$scale;\n-    int displace = $mem$$disp + 4;      \/\/ Offset is 4 further in memory\n-    assert( $mem->disp_reloc() == relocInfo::none, \"Cannot add 4 to oop\" );\n-    encode_RegMem(masm, reg_encoding, base, index, scale, displace, relocInfo::none);\n-  %}\n-\n-  enc_class move_long_small_shift( eRegL dst, immI_1_31 cnt ) %{\n-    int r1, r2;\n-    if( $tertiary == 0xA4 ) { r1 = $dst$$reg;  r2 = HIGH_FROM_LOW_ENC($dst$$reg); }\n-    else                    { r2 = $dst$$reg;  r1 = HIGH_FROM_LOW_ENC($dst$$reg); }\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,$tertiary);\n-    emit_rm(masm, 0x3, r1, r2);\n-    emit_d8(masm,$cnt$$constant);\n-    emit_d8(masm,$primary);\n-    emit_rm(masm, 0x3, $secondary, r1);\n-    emit_d8(masm,$cnt$$constant);\n-  %}\n-\n-  enc_class move_long_big_shift_sign( eRegL dst, immI_32_63 cnt ) %{\n-    emit_opcode( masm, 0x8B ); \/\/ Move\n-    emit_rm(masm, 0x3, $dst$$reg, HIGH_FROM_LOW_ENC($dst$$reg));\n-    if( $cnt$$constant > 32 ) { \/\/ Shift, if not by zero\n-      emit_d8(masm,$primary);\n-      emit_rm(masm, 0x3, $secondary, $dst$$reg);\n-      emit_d8(masm,$cnt$$constant-32);\n-    }\n-    emit_d8(masm,$primary);\n-    emit_rm(masm, 0x3, $secondary, HIGH_FROM_LOW_ENC($dst$$reg));\n-    emit_d8(masm,31);\n-  %}\n-\n-  enc_class move_long_big_shift_clr( eRegL dst, immI_32_63 cnt ) %{\n-    int r1, r2;\n-    if( $secondary == 0x5 ) { r1 = $dst$$reg;  r2 = HIGH_FROM_LOW_ENC($dst$$reg); }\n-    else                    { r2 = $dst$$reg;  r1 = HIGH_FROM_LOW_ENC($dst$$reg); }\n-\n-    emit_opcode( masm, 0x8B ); \/\/ Move r1,r2\n-    emit_rm(masm, 0x3, r1, r2);\n-    if( $cnt$$constant > 32 ) { \/\/ Shift, if not by zero\n-      emit_opcode(masm,$primary);\n-      emit_rm(masm, 0x3, $secondary, r1);\n-      emit_d8(masm,$cnt$$constant-32);\n-    }\n-    emit_opcode(masm,0x33);  \/\/ XOR r2,r2\n-    emit_rm(masm, 0x3, r2, r2);\n-  %}\n-\n-  \/\/ Clone of RegMem but accepts an extra parameter to access each\n-  \/\/ half of a double in memory; it never needs relocation info.\n-  enc_class Mov_MemD_half_to_Reg (immI opcode, memory mem, immI disp_for_half, rRegI rm_reg) %{\n-    emit_opcode(masm,$opcode$$constant);\n-    int reg_encoding = $rm_reg$$reg;\n-    int base     = $mem$$base;\n-    int index    = $mem$$index;\n-    int scale    = $mem$$scale;\n-    int displace = $mem$$disp + $disp_for_half$$constant;\n-    relocInfo::relocType disp_reloc = relocInfo::none;\n-    encode_RegMem(masm, reg_encoding, base, index, scale, displace, disp_reloc);\n-  %}\n-\n-  \/\/ !!!!! Special Custom Code used by MemMove, and stack access instructions !!!!!\n-  \/\/\n-  \/\/ Clone of RegMem except the RM-byte's reg\/opcode field is an ADLC-time constant\n-  \/\/ and it never needs relocation information.\n-  \/\/ Frequently used to move data between FPU's Stack Top and memory.\n-  enc_class RMopc_Mem_no_oop (immI rm_opcode, memory mem) %{\n-    int rm_byte_opcode = $rm_opcode$$constant;\n-    int base     = $mem$$base;\n-    int index    = $mem$$index;\n-    int scale    = $mem$$scale;\n-    int displace = $mem$$disp;\n-    assert( $mem->disp_reloc() == relocInfo::none, \"No oops here because no reloc info allowed\" );\n-    encode_RegMem(masm, rm_byte_opcode, base, index, scale, displace, relocInfo::none);\n-  %}\n-\n-  enc_class RMopc_Mem (immI rm_opcode, memory mem) %{\n-    int rm_byte_opcode = $rm_opcode$$constant;\n-    int base     = $mem$$base;\n-    int index    = $mem$$index;\n-    int scale    = $mem$$scale;\n-    int displace = $mem$$disp;\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc(); \/\/ disp-as-oop when working with static globals\n-    encode_RegMem(masm, rm_byte_opcode, base, index, scale, displace, disp_reloc);\n-  %}\n-\n-  enc_class RegLea (rRegI dst, rRegI src0, immI src1 ) %{    \/\/ emit_reg_lea\n-    int reg_encoding = $dst$$reg;\n-    int base         = $src0$$reg;      \/\/ 0xFFFFFFFF indicates no base\n-    int index        = 0x04;            \/\/ 0x04 indicates no index\n-    int scale        = 0x00;            \/\/ 0x00 indicates no scale\n-    int displace     = $src1$$constant; \/\/ 0x00 indicates no displacement\n-    relocInfo::relocType disp_reloc = relocInfo::none;\n-    encode_RegMem(masm, reg_encoding, base, index, scale, displace, disp_reloc);\n-  %}\n-\n-  enc_class min_enc (rRegI dst, rRegI src) %{    \/\/ MIN\n-    \/\/ Compare dst,src\n-    emit_opcode(masm,0x3B);\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-    \/\/ jmp dst < src around move\n-    emit_opcode(masm,0x7C);\n-    emit_d8(masm,2);\n-    \/\/ move dst,src\n-    emit_opcode(masm,0x8B);\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class max_enc (rRegI dst, rRegI src) %{    \/\/ MAX\n-    \/\/ Compare dst,src\n-    emit_opcode(masm,0x3B);\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-    \/\/ jmp dst > src around move\n-    emit_opcode(masm,0x7F);\n-    emit_d8(masm,2);\n-    \/\/ move dst,src\n-    emit_opcode(masm,0x8B);\n-    emit_rm(masm, 0x3, $dst$$reg, $src$$reg);\n-  %}\n-\n-  enc_class enc_FPR_store(memory mem, regDPR src) %{\n-    \/\/ If src is FPR1, we can just FST to store it.\n-    \/\/ Else we need to FLD it to FPR1, then FSTP to store\/pop it.\n-    int reg_encoding = 0x2; \/\/ Just store\n-    int base  = $mem$$base;\n-    int index = $mem$$index;\n-    int scale = $mem$$scale;\n-    int displace = $mem$$disp;\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc(); \/\/ disp-as-oop when working with static globals\n-    if( $src$$reg != FPR1L_enc ) {\n-      reg_encoding = 0x3;  \/\/ Store & pop\n-      emit_opcode( masm, 0xD9 ); \/\/ FLD (i.e., push it)\n-      emit_d8( masm, 0xC0-1+$src$$reg );\n-    }\n-    __ set_inst_mark();       \/\/ Mark start of opcode for reloc info in mem operand\n-    emit_opcode(masm,$primary);\n-    encode_RegMem(masm, reg_encoding, base, index, scale, displace, disp_reloc);\n-    __ clear_inst_mark();\n-  %}\n-\n-  enc_class neg_reg(rRegI dst) %{\n-    \/\/ NEG $dst\n-    emit_opcode(masm,0xF7);\n-    emit_rm(masm, 0x3, 0x03, $dst$$reg );\n-  %}\n-\n-  enc_class setLT_reg(eCXRegI dst) %{\n-    \/\/ SETLT $dst\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0x9C);\n-    emit_rm( masm, 0x3, 0x4, $dst$$reg );\n-  %}\n-\n-  enc_class enc_cmpLTP(ncxRegI p, ncxRegI q, ncxRegI y, eCXRegI tmp) %{    \/\/ cadd_cmpLT\n-    int tmpReg = $tmp$$reg;\n-\n-    \/\/ SUB $p,$q\n-    emit_opcode(masm,0x2B);\n-    emit_rm(masm, 0x3, $p$$reg, $q$$reg);\n-    \/\/ SBB $tmp,$tmp\n-    emit_opcode(masm,0x1B);\n-    emit_rm(masm, 0x3, tmpReg, tmpReg);\n-    \/\/ AND $tmp,$y\n-    emit_opcode(masm,0x23);\n-    emit_rm(masm, 0x3, tmpReg, $y$$reg);\n-    \/\/ ADD $p,$tmp\n-    emit_opcode(masm,0x03);\n-    emit_rm(masm, 0x3, $p$$reg, tmpReg);\n-  %}\n-\n-  enc_class shift_left_long( eRegL dst, eCXRegI shift ) %{\n-    \/\/ TEST shift,32\n-    emit_opcode(masm,0xF7);\n-    emit_rm(masm, 0x3, 0, ECX_enc);\n-    emit_d32(masm,0x20);\n-    \/\/ JEQ,s small\n-    emit_opcode(masm, 0x74);\n-    emit_d8(masm, 0x04);\n-    \/\/ MOV    $dst.hi,$dst.lo\n-    emit_opcode( masm, 0x8B );\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), $dst$$reg );\n-    \/\/ CLR    $dst.lo\n-    emit_opcode(masm, 0x33);\n-    emit_rm(masm, 0x3, $dst$$reg, $dst$$reg);\n-\/\/ small:\n-    \/\/ SHLD   $dst.hi,$dst.lo,$shift\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xA5);\n-    emit_rm(masm, 0x3, $dst$$reg, HIGH_FROM_LOW_ENC($dst$$reg));\n-    \/\/ SHL    $dst.lo,$shift\"\n-    emit_opcode(masm,0xD3);\n-    emit_rm(masm, 0x3, 0x4, $dst$$reg );\n-  %}\n-\n-  enc_class shift_right_long( eRegL dst, eCXRegI shift ) %{\n-    \/\/ TEST shift,32\n-    emit_opcode(masm,0xF7);\n-    emit_rm(masm, 0x3, 0, ECX_enc);\n-    emit_d32(masm,0x20);\n-    \/\/ JEQ,s small\n-    emit_opcode(masm, 0x74);\n-    emit_d8(masm, 0x04);\n-    \/\/ MOV    $dst.lo,$dst.hi\n-    emit_opcode( masm, 0x8B );\n-    emit_rm(masm, 0x3, $dst$$reg, HIGH_FROM_LOW_ENC($dst$$reg) );\n-    \/\/ CLR    $dst.hi\n-    emit_opcode(masm, 0x33);\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), HIGH_FROM_LOW_ENC($dst$$reg));\n-\/\/ small:\n-    \/\/ SHRD   $dst.lo,$dst.hi,$shift\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xAD);\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), $dst$$reg);\n-    \/\/ SHR    $dst.hi,$shift\"\n-    emit_opcode(masm,0xD3);\n-    emit_rm(masm, 0x3, 0x5, HIGH_FROM_LOW_ENC($dst$$reg) );\n-  %}\n-\n-  enc_class shift_right_arith_long( eRegL dst, eCXRegI shift ) %{\n-    \/\/ TEST shift,32\n-    emit_opcode(masm,0xF7);\n-    emit_rm(masm, 0x3, 0, ECX_enc);\n-    emit_d32(masm,0x20);\n-    \/\/ JEQ,s small\n-    emit_opcode(masm, 0x74);\n-    emit_d8(masm, 0x05);\n-    \/\/ MOV    $dst.lo,$dst.hi\n-    emit_opcode( masm, 0x8B );\n-    emit_rm(masm, 0x3, $dst$$reg, HIGH_FROM_LOW_ENC($dst$$reg) );\n-    \/\/ SAR    $dst.hi,31\n-    emit_opcode(masm, 0xC1);\n-    emit_rm(masm, 0x3, 7, HIGH_FROM_LOW_ENC($dst$$reg) );\n-    emit_d8(masm, 0x1F );\n-\/\/ small:\n-    \/\/ SHRD   $dst.lo,$dst.hi,$shift\n-    emit_opcode(masm,0x0F);\n-    emit_opcode(masm,0xAD);\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), $dst$$reg);\n-    \/\/ SAR    $dst.hi,$shift\"\n-    emit_opcode(masm,0xD3);\n-    emit_rm(masm, 0x3, 0x7, HIGH_FROM_LOW_ENC($dst$$reg) );\n-  %}\n-\n-\n-  \/\/ ----------------- Encodings for floating point unit -----------------\n-  \/\/ May leave result in FPU-TOS or FPU reg depending on opcodes\n-  enc_class OpcReg_FPR(regFPR src) %{    \/\/ FMUL, FDIV\n-    $$$emit8$primary;\n-    emit_rm(masm, 0x3, $secondary, $src$$reg );\n-  %}\n-\n-  \/\/ Pop argument in FPR0 with FSTP ST(0)\n-  enc_class PopFPU() %{\n-    emit_opcode( masm, 0xDD );\n-    emit_d8( masm, 0xD8 );\n-  %}\n-\n-  \/\/ !!!!! equivalent to Pop_Reg_F\n-  enc_class Pop_Reg_DPR( regDPR dst ) %{\n-    emit_opcode( masm, 0xDD );           \/\/ FSTP   ST(i)\n-    emit_d8( masm, 0xD8+$dst$$reg );\n-  %}\n-\n-  enc_class Push_Reg_DPR( regDPR dst ) %{\n-    emit_opcode( masm, 0xD9 );\n-    emit_d8( masm, 0xC0-1+$dst$$reg );   \/\/ FLD ST(i-1)\n-  %}\n-\n-  enc_class strictfp_bias1( regDPR dst ) %{\n-    emit_opcode( masm, 0xDB );           \/\/ FLD m80real\n-    emit_opcode( masm, 0x2D );\n-    emit_d32( masm, (int)StubRoutines::x86::addr_fpu_subnormal_bias1() );\n-    emit_opcode( masm, 0xDE );           \/\/ FMULP ST(dst), ST0\n-    emit_opcode( masm, 0xC8+$dst$$reg );\n-  %}\n-\n-  enc_class strictfp_bias2( regDPR dst ) %{\n-    emit_opcode( masm, 0xDB );           \/\/ FLD m80real\n-    emit_opcode( masm, 0x2D );\n-    emit_d32( masm, (int)StubRoutines::x86::addr_fpu_subnormal_bias2() );\n-    emit_opcode( masm, 0xDE );           \/\/ FMULP ST(dst), ST0\n-    emit_opcode( masm, 0xC8+$dst$$reg );\n-  %}\n-\n-  \/\/ Special case for moving an integer register to a stack slot.\n-  enc_class OpcPRegSS( stackSlotI dst, rRegI src ) %{ \/\/ RegSS\n-    store_to_stackslot( masm, $primary, $src$$reg, $dst$$disp );\n-  %}\n-\n-  \/\/ Special case for moving a register to a stack slot.\n-  enc_class RegSS( stackSlotI dst, rRegI src ) %{ \/\/ RegSS\n-    \/\/ Opcode already emitted\n-    emit_rm( masm, 0x02, $src$$reg, ESP_enc );   \/\/ R\/M byte\n-    emit_rm( masm, 0x00, ESP_enc, ESP_enc);          \/\/ SIB byte\n-    emit_d32(masm, $dst$$disp);   \/\/ Displacement\n-  %}\n-\n-  \/\/ Push the integer in stackSlot 'src' onto FP-stack\n-  enc_class Push_Mem_I( memory src ) %{    \/\/ FILD   [ESP+src]\n-    store_to_stackslot( masm, $primary, $secondary, $src$$disp );\n-  %}\n-\n-  \/\/ Push FPU's TOS float to a stack-slot, and pop FPU-stack\n-  enc_class Pop_Mem_FPR( stackSlotF dst ) %{ \/\/ FSTP_S [ESP+dst]\n-    store_to_stackslot( masm, 0xD9, 0x03, $dst$$disp );\n-  %}\n-\n-  \/\/ Same as Pop_Mem_F except for opcode\n-  \/\/ Push FPU's TOS double to a stack-slot, and pop FPU-stack\n-  enc_class Pop_Mem_DPR( stackSlotD dst ) %{ \/\/ FSTP_D [ESP+dst]\n-    store_to_stackslot( masm, 0xDD, 0x03, $dst$$disp );\n-  %}\n-\n-  enc_class Pop_Reg_FPR( regFPR dst ) %{\n-    emit_opcode( masm, 0xDD );           \/\/ FSTP   ST(i)\n-    emit_d8( masm, 0xD8+$dst$$reg );\n-  %}\n-\n-  enc_class Push_Reg_FPR( regFPR dst ) %{\n-    emit_opcode( masm, 0xD9 );           \/\/ FLD    ST(i-1)\n-    emit_d8( masm, 0xC0-1+$dst$$reg );\n-  %}\n-\n-  \/\/ Push FPU's float to a stack-slot, and pop FPU-stack\n-  enc_class Pop_Mem_Reg_FPR( stackSlotF dst, regFPR src ) %{\n-    int pop = 0x02;\n-    if ($src$$reg != FPR1L_enc) {\n-      emit_opcode( masm, 0xD9 );         \/\/ FLD    ST(i-1)\n-      emit_d8( masm, 0xC0-1+$src$$reg );\n-      pop = 0x03;\n-    }\n-    store_to_stackslot( masm, 0xD9, pop, $dst$$disp ); \/\/ FST<P>_S  [ESP+dst]\n-  %}\n-\n-  \/\/ Push FPU's double to a stack-slot, and pop FPU-stack\n-  enc_class Pop_Mem_Reg_DPR( stackSlotD dst, regDPR src ) %{\n-    int pop = 0x02;\n-    if ($src$$reg != FPR1L_enc) {\n-      emit_opcode( masm, 0xD9 );         \/\/ FLD    ST(i-1)\n-      emit_d8( masm, 0xC0-1+$src$$reg );\n-      pop = 0x03;\n-    }\n-    store_to_stackslot( masm, 0xDD, pop, $dst$$disp ); \/\/ FST<P>_D  [ESP+dst]\n-  %}\n-\n-  \/\/ Push FPU's double to a FPU-stack-slot, and pop FPU-stack\n-  enc_class Pop_Reg_Reg_DPR( regDPR dst, regFPR src ) %{\n-    int pop = 0xD0 - 1; \/\/ -1 since we skip FLD\n-    if ($src$$reg != FPR1L_enc) {\n-      emit_opcode( masm, 0xD9 );         \/\/ FLD    ST(src-1)\n-      emit_d8( masm, 0xC0-1+$src$$reg );\n-      pop = 0xD8;\n-    }\n-    emit_opcode( masm, 0xDD );\n-    emit_d8( masm, pop+$dst$$reg );      \/\/ FST<P> ST(i)\n-  %}\n-\n-\n-  enc_class Push_Reg_Mod_DPR( regDPR dst, regDPR src) %{\n-    \/\/ load dst in FPR0\n-    emit_opcode( masm, 0xD9 );\n-    emit_d8( masm, 0xC0-1+$dst$$reg );\n-    if ($src$$reg != FPR1L_enc) {\n-      \/\/ fincstp\n-      emit_opcode (masm, 0xD9);\n-      emit_opcode (masm, 0xF7);\n-      \/\/ swap src with FPR1:\n-      \/\/ FXCH FPR1 with src\n-      emit_opcode(masm, 0xD9);\n-      emit_d8(masm, 0xC8-1+$src$$reg );\n-      \/\/ fdecstp\n-      emit_opcode (masm, 0xD9);\n-      emit_opcode (masm, 0xF6);\n-    }\n-  %}\n-\n-  enc_class Push_ModD_encoding(regD src0, regD src1) %{\n-    __ subptr(rsp, 8);\n-    __ movdbl(Address(rsp, 0), $src1$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-    __ movdbl(Address(rsp, 0), $src0$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-  %}\n-\n-  enc_class Push_ModF_encoding(regF src0, regF src1) %{\n-    __ subptr(rsp, 4);\n-    __ movflt(Address(rsp, 0), $src1$$XMMRegister);\n-    __ fld_s(Address(rsp, 0));\n-    __ movflt(Address(rsp, 0), $src0$$XMMRegister);\n-    __ fld_s(Address(rsp, 0));\n-  %}\n-\n-  enc_class Push_ResultD(regD dst) %{\n-    __ fstp_d(Address(rsp, 0));\n-    __ movdbl($dst$$XMMRegister, Address(rsp, 0));\n-    __ addptr(rsp, 8);\n-  %}\n-\n-  enc_class Push_ResultF(regF dst, immI d8) %{\n-    __ fstp_s(Address(rsp, 0));\n-    __ movflt($dst$$XMMRegister, Address(rsp, 0));\n-    __ addptr(rsp, $d8$$constant);\n-  %}\n-\n-  enc_class Push_SrcD(regD src) %{\n-    __ subptr(rsp, 8);\n-    __ movdbl(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-  %}\n-\n-  enc_class push_stack_temp_qword() %{\n-    __ subptr(rsp, 8);\n-  %}\n-\n-  enc_class pop_stack_temp_qword() %{\n-    __ addptr(rsp, 8);\n-  %}\n-\n-  enc_class push_xmm_to_fpr1(regD src) %{\n-    __ movdbl(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-  %}\n-\n-  enc_class Push_Result_Mod_DPR( regDPR src) %{\n-    if ($src$$reg != FPR1L_enc) {\n-      \/\/ fincstp\n-      emit_opcode (masm, 0xD9);\n-      emit_opcode (masm, 0xF7);\n-      \/\/ FXCH FPR1 with src\n-      emit_opcode(masm, 0xD9);\n-      emit_d8(masm, 0xC8-1+$src$$reg );\n-      \/\/ fdecstp\n-      emit_opcode (masm, 0xD9);\n-      emit_opcode (masm, 0xF6);\n-    }\n-  %}\n-\n-  enc_class fnstsw_sahf_skip_parity() %{\n-    \/\/ fnstsw ax\n-    emit_opcode( masm, 0xDF );\n-    emit_opcode( masm, 0xE0 );\n-    \/\/ sahf\n-    emit_opcode( masm, 0x9E );\n-    \/\/ jnp  ::skip\n-    emit_opcode( masm, 0x7B );\n-    emit_opcode( masm, 0x05 );\n-  %}\n-\n-  enc_class emitModDPR() %{\n-    \/\/ fprem must be iterative\n-    \/\/ :: loop\n-    \/\/ fprem\n-    emit_opcode( masm, 0xD9 );\n-    emit_opcode( masm, 0xF8 );\n-    \/\/ wait\n-    emit_opcode( masm, 0x9b );\n-    \/\/ fnstsw ax\n-    emit_opcode( masm, 0xDF );\n-    emit_opcode( masm, 0xE0 );\n-    \/\/ sahf\n-    emit_opcode( masm, 0x9E );\n-    \/\/ jp  ::loop\n-    emit_opcode( masm, 0x0F );\n-    emit_opcode( masm, 0x8A );\n-    emit_opcode( masm, 0xF4 );\n-    emit_opcode( masm, 0xFF );\n-    emit_opcode( masm, 0xFF );\n-    emit_opcode( masm, 0xFF );\n-  %}\n-\n-  enc_class fpu_flags() %{\n-    \/\/ fnstsw_ax\n-    emit_opcode( masm, 0xDF);\n-    emit_opcode( masm, 0xE0);\n-    \/\/ test ax,0x0400\n-    emit_opcode( masm, 0x66 );   \/\/ operand-size prefix for 16-bit immediate\n-    emit_opcode( masm, 0xA9 );\n-    emit_d16   ( masm, 0x0400 );\n-    \/\/ \/\/ \/\/ This sequence works, but stalls for 12-16 cycles on PPro\n-    \/\/ \/\/ test rax,0x0400\n-    \/\/ emit_opcode( masm, 0xA9 );\n-    \/\/ emit_d32   ( masm, 0x00000400 );\n-    \/\/\n-    \/\/ jz exit (no unordered comparison)\n-    emit_opcode( masm, 0x74 );\n-    emit_d8    ( masm, 0x02 );\n-    \/\/ mov ah,1 - treat as LT case (set carry flag)\n-    emit_opcode( masm, 0xB4 );\n-    emit_d8    ( masm, 0x01 );\n-    \/\/ sahf\n-    emit_opcode( masm, 0x9E);\n-  %}\n-\n-  enc_class cmpF_P6_fixup() %{\n-    \/\/ Fixup the integer flags in case comparison involved a NaN\n-    \/\/\n-    \/\/ JNP exit (no unordered comparison, P-flag is set by NaN)\n-    emit_opcode( masm, 0x7B );\n-    emit_d8    ( masm, 0x03 );\n-    \/\/ MOV AH,1 - treat as LT case (set carry flag)\n-    emit_opcode( masm, 0xB4 );\n-    emit_d8    ( masm, 0x01 );\n-    \/\/ SAHF\n-    emit_opcode( masm, 0x9E);\n-    \/\/ NOP     \/\/ target for branch to avoid branch to branch\n-    emit_opcode( masm, 0x90);\n-  %}\n-\n-\/\/     fnstsw_ax();\n-\/\/     sahf();\n-\/\/     movl(dst, nan_result);\n-\/\/     jcc(Assembler::parity, exit);\n-\/\/     movl(dst, less_result);\n-\/\/     jcc(Assembler::below, exit);\n-\/\/     movl(dst, equal_result);\n-\/\/     jcc(Assembler::equal, exit);\n-\/\/     movl(dst, greater_result);\n-\n-\/\/ less_result     =  1;\n-\/\/ greater_result  = -1;\n-\/\/ equal_result    = 0;\n-\/\/ nan_result      = -1;\n-\n-  enc_class CmpF_Result(rRegI dst) %{\n-    \/\/ fnstsw_ax();\n-    emit_opcode( masm, 0xDF);\n-    emit_opcode( masm, 0xE0);\n-    \/\/ sahf\n-    emit_opcode( masm, 0x9E);\n-    \/\/ movl(dst, nan_result);\n-    emit_opcode( masm, 0xB8 + $dst$$reg);\n-    emit_d32( masm, -1 );\n-    \/\/ jcc(Assembler::parity, exit);\n-    emit_opcode( masm, 0x7A );\n-    emit_d8    ( masm, 0x13 );\n-    \/\/ movl(dst, less_result);\n-    emit_opcode( masm, 0xB8 + $dst$$reg);\n-    emit_d32( masm, -1 );\n-    \/\/ jcc(Assembler::below, exit);\n-    emit_opcode( masm, 0x72 );\n-    emit_d8    ( masm, 0x0C );\n-    \/\/ movl(dst, equal_result);\n-    emit_opcode( masm, 0xB8 + $dst$$reg);\n-    emit_d32( masm, 0 );\n-    \/\/ jcc(Assembler::equal, exit);\n-    emit_opcode( masm, 0x74 );\n-    emit_d8    ( masm, 0x05 );\n-    \/\/ movl(dst, greater_result);\n-    emit_opcode( masm, 0xB8 + $dst$$reg);\n-    emit_d32( masm, 1 );\n-  %}\n-\n-\n-  \/\/ Compare the longs and set flags\n-  \/\/ BROKEN!  Do Not use as-is\n-  enc_class cmpl_test( eRegL src1, eRegL src2 ) %{\n-    \/\/ CMP    $src1.hi,$src2.hi\n-    emit_opcode( masm, 0x3B );\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($src1$$reg), HIGH_FROM_LOW_ENC($src2$$reg) );\n-    \/\/ JNE,s  done\n-    emit_opcode(masm,0x75);\n-    emit_d8(masm, 2 );\n-    \/\/ CMP    $src1.lo,$src2.lo\n-    emit_opcode( masm, 0x3B );\n-    emit_rm(masm, 0x3, $src1$$reg, $src2$$reg );\n-\/\/ done:\n-  %}\n-\n-  enc_class convert_int_long( regL dst, rRegI src ) %{\n-    \/\/ mov $dst.lo,$src\n-    int dst_encoding = $dst$$reg;\n-    int src_encoding = $src$$reg;\n-    encode_Copy( masm, dst_encoding  , src_encoding );\n-    \/\/ mov $dst.hi,$src\n-    encode_Copy( masm, HIGH_FROM_LOW_ENC(dst_encoding), src_encoding );\n-    \/\/ sar $dst.hi,31\n-    emit_opcode( masm, 0xC1 );\n-    emit_rm(masm, 0x3, 7, HIGH_FROM_LOW_ENC(dst_encoding) );\n-    emit_d8(masm, 0x1F );\n-  %}\n-\n-  enc_class convert_long_double( eRegL src ) %{\n-    \/\/ push $src.hi\n-    emit_opcode(masm, 0x50+HIGH_FROM_LOW_ENC($src$$reg));\n-    \/\/ push $src.lo\n-    emit_opcode(masm, 0x50+$src$$reg  );\n-    \/\/ fild 64-bits at [SP]\n-    emit_opcode(masm,0xdf);\n-    emit_d8(masm, 0x6C);\n-    emit_d8(masm, 0x24);\n-    emit_d8(masm, 0x00);\n-    \/\/ pop stack\n-    emit_opcode(masm, 0x83); \/\/ add  SP, #8\n-    emit_rm(masm, 0x3, 0x00, ESP_enc);\n-    emit_d8(masm, 0x8);\n-  %}\n-\n-  enc_class multiply_con_and_shift_high( eDXRegI dst, nadxRegI src1, eADXRegL_low_only src2, immI_32_63 cnt, eFlagsReg cr ) %{\n-    \/\/ IMUL   EDX:EAX,$src1\n-    emit_opcode( masm, 0xF7 );\n-    emit_rm( masm, 0x3, 0x5, $src1$$reg );\n-    \/\/ SAR    EDX,$cnt-32\n-    int shift_count = ((int)$cnt$$constant) - 32;\n-    if (shift_count > 0) {\n-      emit_opcode(masm, 0xC1);\n-      emit_rm(masm, 0x3, 7, $dst$$reg );\n-      emit_d8(masm, shift_count);\n-    }\n-  %}\n-\n-  \/\/ this version doesn't have add sp, 8\n-  enc_class convert_long_double2( eRegL src ) %{\n-    \/\/ push $src.hi\n-    emit_opcode(masm, 0x50+HIGH_FROM_LOW_ENC($src$$reg));\n-    \/\/ push $src.lo\n-    emit_opcode(masm, 0x50+$src$$reg  );\n-    \/\/ fild 64-bits at [SP]\n-    emit_opcode(masm,0xdf);\n-    emit_d8(masm, 0x6C);\n-    emit_d8(masm, 0x24);\n-    emit_d8(masm, 0x00);\n-  %}\n-\n-  enc_class long_int_multiply( eADXRegL dst, nadxRegI src) %{\n-    \/\/ Basic idea: long = (long)int * (long)int\n-    \/\/ IMUL EDX:EAX, src\n-    emit_opcode( masm, 0xF7 );\n-    emit_rm( masm, 0x3, 0x5, $src$$reg);\n-  %}\n-\n-  enc_class long_uint_multiply( eADXRegL dst, nadxRegI src) %{\n-    \/\/ Basic Idea:  long = (int & 0xffffffffL) * (int & 0xffffffffL)\n-    \/\/ MUL EDX:EAX, src\n-    emit_opcode( masm, 0xF7 );\n-    emit_rm( masm, 0x3, 0x4, $src$$reg);\n-  %}\n-\n-  enc_class long_multiply( eADXRegL dst, eRegL src, rRegI tmp ) %{\n-    \/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-    \/\/             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)\n-    \/\/ MOV    $tmp,$src.lo\n-    encode_Copy( masm, $tmp$$reg, $src$$reg );\n-    \/\/ IMUL   $tmp,EDX\n-    emit_opcode( masm, 0x0F );\n-    emit_opcode( masm, 0xAF );\n-    emit_rm( masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($dst$$reg) );\n-    \/\/ MOV    EDX,$src.hi\n-    encode_Copy( masm, HIGH_FROM_LOW_ENC($dst$$reg), HIGH_FROM_LOW_ENC($src$$reg) );\n-    \/\/ IMUL   EDX,EAX\n-    emit_opcode( masm, 0x0F );\n-    emit_opcode( masm, 0xAF );\n-    emit_rm( masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), $dst$$reg );\n-    \/\/ ADD    $tmp,EDX\n-    emit_opcode( masm, 0x03 );\n-    emit_rm( masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($dst$$reg) );\n-    \/\/ MUL   EDX:EAX,$src.lo\n-    emit_opcode( masm, 0xF7 );\n-    emit_rm( masm, 0x3, 0x4, $src$$reg );\n-    \/\/ ADD    EDX,ESI\n-    emit_opcode( masm, 0x03 );\n-    emit_rm( masm, 0x3, HIGH_FROM_LOW_ENC($dst$$reg), $tmp$$reg );\n-  %}\n-\n-  enc_class long_multiply_con( eADXRegL dst, immL_127 src, rRegI tmp ) %{\n-    \/\/ Basic idea: lo(result) = lo(src * y_lo)\n-    \/\/             hi(result) = hi(src * y_lo) + lo(src * y_hi)\n-    \/\/ IMUL   $tmp,EDX,$src\n-    emit_opcode( masm, 0x6B );\n-    emit_rm( masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($dst$$reg) );\n-    emit_d8( masm, (int)$src$$constant );\n-    \/\/ MOV    EDX,$src\n-    emit_opcode(masm, 0xB8 + EDX_enc);\n-    emit_d32( masm, (int)$src$$constant );\n-    \/\/ MUL   EDX:EAX,EDX\n-    emit_opcode( masm, 0xF7 );\n-    emit_rm( masm, 0x3, 0x4, EDX_enc );\n-    \/\/ ADD    EDX,ESI\n-    emit_opcode( masm, 0x03 );\n-    emit_rm( masm, 0x3, EDX_enc, $tmp$$reg );\n-  %}\n-\n-  enc_class long_div( eRegL src1, eRegL src2 ) %{\n-    \/\/ PUSH src1.hi\n-    emit_opcode(masm, HIGH_FROM_LOW_ENC(0x50+$src1$$reg) );\n-    \/\/ PUSH src1.lo\n-    emit_opcode(masm,               0x50+$src1$$reg  );\n-    \/\/ PUSH src2.hi\n-    emit_opcode(masm, HIGH_FROM_LOW_ENC(0x50+$src2$$reg) );\n-    \/\/ PUSH src2.lo\n-    emit_opcode(masm,               0x50+$src2$$reg  );\n-    \/\/ CALL directly to the runtime\n-    __ set_inst_mark();\n-    emit_opcode(masm,0xE8);       \/\/ Call into runtime\n-    emit_d32_reloc(masm, (CAST_FROM_FN_PTR(address, SharedRuntime::ldiv) - __ pc()) - 4, runtime_call_Relocation::spec(), RELOC_IMM32 );\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-    \/\/ Restore stack\n-    emit_opcode(masm, 0x83); \/\/ add  SP, #framesize\n-    emit_rm(masm, 0x3, 0x00, ESP_enc);\n-    emit_d8(masm, 4*4);\n-  %}\n-\n-  enc_class long_mod( eRegL src1, eRegL src2 ) %{\n-    \/\/ PUSH src1.hi\n-    emit_opcode(masm, HIGH_FROM_LOW_ENC(0x50+$src1$$reg) );\n-    \/\/ PUSH src1.lo\n-    emit_opcode(masm,               0x50+$src1$$reg  );\n-    \/\/ PUSH src2.hi\n-    emit_opcode(masm, HIGH_FROM_LOW_ENC(0x50+$src2$$reg) );\n-    \/\/ PUSH src2.lo\n-    emit_opcode(masm,               0x50+$src2$$reg  );\n-    \/\/ CALL directly to the runtime\n-    __ set_inst_mark();\n-    emit_opcode(masm,0xE8);       \/\/ Call into runtime\n-    emit_d32_reloc(masm, (CAST_FROM_FN_PTR(address, SharedRuntime::lrem ) - __ pc()) - 4, runtime_call_Relocation::spec(), RELOC_IMM32 );\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-    \/\/ Restore stack\n-    emit_opcode(masm, 0x83); \/\/ add  SP, #framesize\n-    emit_rm(masm, 0x3, 0x00, ESP_enc);\n-    emit_d8(masm, 4*4);\n-  %}\n-\n-  enc_class long_cmp_flags0( eRegL src, rRegI tmp ) %{\n-    \/\/ MOV   $tmp,$src.lo\n-    emit_opcode(masm, 0x8B);\n-    emit_rm(masm, 0x3, $tmp$$reg, $src$$reg);\n-    \/\/ OR    $tmp,$src.hi\n-    emit_opcode(masm, 0x0B);\n-    emit_rm(masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($src$$reg));\n-  %}\n-\n-  enc_class long_cmp_flags1( eRegL src1, eRegL src2 ) %{\n-    \/\/ CMP    $src1.lo,$src2.lo\n-    emit_opcode( masm, 0x3B );\n-    emit_rm(masm, 0x3, $src1$$reg, $src2$$reg );\n-    \/\/ JNE,s  skip\n-    emit_cc(masm, 0x70, 0x5);\n-    emit_d8(masm,2);\n-    \/\/ CMP    $src1.hi,$src2.hi\n-    emit_opcode( masm, 0x3B );\n-    emit_rm(masm, 0x3, HIGH_FROM_LOW_ENC($src1$$reg), HIGH_FROM_LOW_ENC($src2$$reg) );\n-  %}\n-\n-  enc_class long_cmp_flags2( eRegL src1, eRegL src2, rRegI tmp ) %{\n-    \/\/ CMP    $src1.lo,$src2.lo\\t! Long compare; set flags for low bits\n-    emit_opcode( masm, 0x3B );\n-    emit_rm(masm, 0x3, $src1$$reg, $src2$$reg );\n-    \/\/ MOV    $tmp,$src1.hi\n-    emit_opcode( masm, 0x8B );\n-    emit_rm(masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($src1$$reg) );\n-    \/\/ SBB   $tmp,$src2.hi\\t! Compute flags for long compare\n-    emit_opcode( masm, 0x1B );\n-    emit_rm(masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($src2$$reg) );\n-  %}\n-\n-  enc_class long_cmp_flags3( eRegL src, rRegI tmp ) %{\n-    \/\/ XOR    $tmp,$tmp\n-    emit_opcode(masm,0x33);  \/\/ XOR\n-    emit_rm(masm,0x3, $tmp$$reg, $tmp$$reg);\n-    \/\/ CMP    $tmp,$src.lo\n-    emit_opcode( masm, 0x3B );\n-    emit_rm(masm, 0x3, $tmp$$reg, $src$$reg );\n-    \/\/ SBB    $tmp,$src.hi\n-    emit_opcode( masm, 0x1B );\n-    emit_rm(masm, 0x3, $tmp$$reg, HIGH_FROM_LOW_ENC($src$$reg) );\n-  %}\n-\n- \/\/ Sniff, sniff... smells like Gnu Superoptimizer\n-  enc_class neg_long( eRegL dst ) %{\n-    emit_opcode(masm,0xF7);    \/\/ NEG hi\n-    emit_rm    (masm,0x3, 0x3, HIGH_FROM_LOW_ENC($dst$$reg));\n-    emit_opcode(masm,0xF7);    \/\/ NEG lo\n-    emit_rm    (masm,0x3, 0x3,               $dst$$reg );\n-    emit_opcode(masm,0x83);    \/\/ SBB hi,0\n-    emit_rm    (masm,0x3, 0x3, HIGH_FROM_LOW_ENC($dst$$reg));\n-    emit_d8    (masm,0 );\n-  %}\n-\n-  enc_class enc_pop_rdx() %{\n-    emit_opcode(masm,0x5A);\n-  %}\n-\n-  enc_class enc_rethrow() %{\n-    __ set_inst_mark();\n-    emit_opcode(masm, 0xE9);        \/\/ jmp    entry\n-    emit_d32_reloc(masm, (int)OptoRuntime::rethrow_stub() - ((int)__ pc())-4,\n-                   runtime_call_Relocation::spec(), RELOC_IMM32 );\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-  %}\n-\n-\n-  \/\/ Convert a double to an int.  Java semantics require we do complex\n-  \/\/ manglelations in the corner cases.  So we set the rounding mode to\n-  \/\/ 'zero', store the darned double down as an int, and reset the\n-  \/\/ rounding mode to 'nearest'.  The hardware throws an exception which\n-  \/\/ patches up the correct value directly to the stack.\n-  enc_class DPR2I_encoding( regDPR src ) %{\n-    \/\/ Flip to round-to-zero mode.  We attempted to allow invalid-op\n-    \/\/ exceptions here, so that a NAN or other corner-case value will\n-    \/\/ thrown an exception (but normal values get converted at full speed).\n-    \/\/ However, I2C adapters and other float-stack manglers leave pending\n-    \/\/ invalid-op exceptions hanging.  We would have to clear them before\n-    \/\/ enabling them and that is more expensive than just testing for the\n-    \/\/ invalid value Intel stores down in the corner cases.\n-    emit_opcode(masm,0xD9);            \/\/ FLDCW  trunc\n-    emit_opcode(masm,0x2D);\n-    emit_d32(masm,(int)StubRoutines::x86::addr_fpu_cntrl_wrd_trunc());\n-    \/\/ Allocate a word\n-    emit_opcode(masm,0x83);            \/\/ SUB ESP,4\n-    emit_opcode(masm,0xEC);\n-    emit_d8(masm,0x04);\n-    \/\/ Encoding assumes a double has been pushed into FPR0.\n-    \/\/ Store down the double as an int, popping the FPU stack\n-    emit_opcode(masm,0xDB);            \/\/ FISTP [ESP]\n-    emit_opcode(masm,0x1C);\n-    emit_d8(masm,0x24);\n-    \/\/ Restore the rounding mode; mask the exception\n-    emit_opcode(masm,0xD9);            \/\/ FLDCW   std\/24-bit mode\n-    emit_opcode(masm,0x2D);\n-    emit_d32( masm, Compile::current()->in_24_bit_fp_mode()\n-        ? (int)StubRoutines::x86::addr_fpu_cntrl_wrd_24()\n-        : (int)StubRoutines::x86::addr_fpu_cntrl_wrd_std());\n-\n-    \/\/ Load the converted int; adjust CPU stack\n-    emit_opcode(masm,0x58);       \/\/ POP EAX\n-    emit_opcode(masm,0x3D);       \/\/ CMP EAX,imm\n-    emit_d32   (masm,0x80000000); \/\/         0x80000000\n-    emit_opcode(masm,0x75);       \/\/ JNE around_slow_call\n-    emit_d8    (masm,0x07);       \/\/ Size of slow_call\n-    \/\/ Push src onto stack slow-path\n-    emit_opcode(masm,0xD9 );      \/\/ FLD     ST(i)\n-    emit_d8    (masm,0xC0-1+$src$$reg );\n-    \/\/ CALL directly to the runtime\n-    __ set_inst_mark();\n-    emit_opcode(masm,0xE8);       \/\/ Call into runtime\n-    emit_d32_reloc(masm, (StubRoutines::x86::d2i_wrapper() - __ pc()) - 4, runtime_call_Relocation::spec(), RELOC_IMM32 );\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-    \/\/ Carry on here...\n-  %}\n-\n-  enc_class DPR2L_encoding( regDPR src ) %{\n-    emit_opcode(masm,0xD9);            \/\/ FLDCW  trunc\n-    emit_opcode(masm,0x2D);\n-    emit_d32(masm,(int)StubRoutines::x86::addr_fpu_cntrl_wrd_trunc());\n-    \/\/ Allocate a word\n-    emit_opcode(masm,0x83);            \/\/ SUB ESP,8\n-    emit_opcode(masm,0xEC);\n-    emit_d8(masm,0x08);\n-    \/\/ Encoding assumes a double has been pushed into FPR0.\n-    \/\/ Store down the double as a long, popping the FPU stack\n-    emit_opcode(masm,0xDF);            \/\/ FISTP [ESP]\n-    emit_opcode(masm,0x3C);\n-    emit_d8(masm,0x24);\n-    \/\/ Restore the rounding mode; mask the exception\n-    emit_opcode(masm,0xD9);            \/\/ FLDCW   std\/24-bit mode\n-    emit_opcode(masm,0x2D);\n-    emit_d32( masm, Compile::current()->in_24_bit_fp_mode()\n-        ? (int)StubRoutines::x86::addr_fpu_cntrl_wrd_24()\n-        : (int)StubRoutines::x86::addr_fpu_cntrl_wrd_std());\n-\n-    \/\/ Load the converted int; adjust CPU stack\n-    emit_opcode(masm,0x58);       \/\/ POP EAX\n-    emit_opcode(masm,0x5A);       \/\/ POP EDX\n-    emit_opcode(masm,0x81);       \/\/ CMP EDX,imm\n-    emit_d8    (masm,0xFA);       \/\/ rdx\n-    emit_d32   (masm,0x80000000); \/\/         0x80000000\n-    emit_opcode(masm,0x75);       \/\/ JNE around_slow_call\n-    emit_d8    (masm,0x07+4);     \/\/ Size of slow_call\n-    emit_opcode(masm,0x85);       \/\/ TEST EAX,EAX\n-    emit_opcode(masm,0xC0);       \/\/ 2\/rax,\/rax,\n-    emit_opcode(masm,0x75);       \/\/ JNE around_slow_call\n-    emit_d8    (masm,0x07);       \/\/ Size of slow_call\n-    \/\/ Push src onto stack slow-path\n-    emit_opcode(masm,0xD9 );      \/\/ FLD     ST(i)\n-    emit_d8    (masm,0xC0-1+$src$$reg );\n-    \/\/ CALL directly to the runtime\n-    __ set_inst_mark();\n-    emit_opcode(masm,0xE8);       \/\/ Call into runtime\n-    emit_d32_reloc(masm, (StubRoutines::x86::d2l_wrapper() - __ pc()) - 4, runtime_call_Relocation::spec(), RELOC_IMM32 );\n-    __ clear_inst_mark();\n-    __ post_call_nop();\n-    \/\/ Carry on here...\n-  %}\n-\n-  enc_class FMul_ST_reg( eRegFPR src1 ) %{\n-    \/\/ Operand was loaded from memory into fp ST (stack top)\n-    \/\/ FMUL   ST,$src  \/* D8 C8+i *\/\n-    emit_opcode(masm, 0xD8);\n-    emit_opcode(masm, 0xC8 + $src1$$reg);\n-  %}\n-\n-  enc_class FAdd_ST_reg( eRegFPR src2 ) %{\n-    \/\/ FADDP  ST,src2  \/* D8 C0+i *\/\n-    emit_opcode(masm, 0xD8);\n-    emit_opcode(masm, 0xC0 + $src2$$reg);\n-    \/\/could use FADDP  src2,fpST  \/* DE C0+i *\/\n-  %}\n-\n-  enc_class FAddP_reg_ST( eRegFPR src2 ) %{\n-    \/\/ FADDP  src2,ST  \/* DE C0+i *\/\n-    emit_opcode(masm, 0xDE);\n-    emit_opcode(masm, 0xC0 + $src2$$reg);\n-  %}\n-\n-  enc_class subFPR_divFPR_encode( eRegFPR src1, eRegFPR src2) %{\n-    \/\/ Operand has been loaded into fp ST (stack top)\n-      \/\/ FSUB   ST,$src1\n-      emit_opcode(masm, 0xD8);\n-      emit_opcode(masm, 0xE0 + $src1$$reg);\n-\n-      \/\/ FDIV\n-      emit_opcode(masm, 0xD8);\n-      emit_opcode(masm, 0xF0 + $src2$$reg);\n-  %}\n-\n-  enc_class MulFAddF (eRegFPR src1, eRegFPR src2) %{\n-    \/\/ Operand was loaded from memory into fp ST (stack top)\n-    \/\/ FADD   ST,$src  \/* D8 C0+i *\/\n-    emit_opcode(masm, 0xD8);\n-    emit_opcode(masm, 0xC0 + $src1$$reg);\n-\n-    \/\/ FMUL  ST,src2  \/* D8 C*+i *\/\n-    emit_opcode(masm, 0xD8);\n-    emit_opcode(masm, 0xC8 + $src2$$reg);\n-  %}\n-\n-\n-  enc_class MulFAddFreverse (eRegFPR src1, eRegFPR src2) %{\n-    \/\/ Operand was loaded from memory into fp ST (stack top)\n-    \/\/ FADD   ST,$src  \/* D8 C0+i *\/\n-    emit_opcode(masm, 0xD8);\n-    emit_opcode(masm, 0xC0 + $src1$$reg);\n-\n-    \/\/ FMULP  src2,ST  \/* DE C8+i *\/\n-    emit_opcode(masm, 0xDE);\n-    emit_opcode(masm, 0xC8 + $src2$$reg);\n-  %}\n-\n-  \/\/ Atomically load the volatile long\n-  enc_class enc_loadL_volatile( memory mem, stackSlotL dst ) %{\n-    emit_opcode(masm,0xDF);\n-    int rm_byte_opcode = 0x05;\n-    int base     = $mem$$base;\n-    int index    = $mem$$index;\n-    int scale    = $mem$$scale;\n-    int displace = $mem$$disp;\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc(); \/\/ disp-as-oop when working with static globals\n-    encode_RegMem(masm, rm_byte_opcode, base, index, scale, displace, disp_reloc);\n-    store_to_stackslot( masm, 0x0DF, 0x07, $dst$$disp );\n-  %}\n-\n-  \/\/ Volatile Store Long.  Must be atomic, so move it into\n-  \/\/ the FP TOS and then do a 64-bit FIST.  Has to probe the\n-  \/\/ target address before the store (for null-ptr checks)\n-  \/\/ so the memory operand is used twice in the encoding.\n-  enc_class enc_storeL_volatile( memory mem, stackSlotL src ) %{\n-    store_to_stackslot( masm, 0x0DF, 0x05, $src$$disp );\n-    __ set_inst_mark();            \/\/ Mark start of FIST in case $mem has an oop\n-    emit_opcode(masm,0xDF);\n-    int rm_byte_opcode = 0x07;\n-    int base     = $mem$$base;\n-    int index    = $mem$$index;\n-    int scale    = $mem$$scale;\n-    int displace = $mem$$disp;\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc(); \/\/ disp-as-oop when working with static globals\n-    encode_RegMem(masm, rm_byte_opcode, base, index, scale, displace, disp_reloc);\n-    __ clear_inst_mark();\n-  %}\n-\n-%}\n-\n-\n-\/\/----------FRAME--------------------------------------------------------------\n-\/\/ Definition of frame structure and management information.\n-\/\/\n-\/\/  S T A C K   L A Y O U T    Allocators stack-slot number\n-\/\/                             |   (to get allocators register number\n-\/\/  G  Owned by    |        |  v    add OptoReg::stack0())\n-\/\/  r   CALLER     |        |\n-\/\/  o     |        +--------+      pad to even-align allocators stack-slot\n-\/\/  w     V        |  pad0  |        numbers; owned by CALLER\n-\/\/  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned\n-\/\/  h     ^        |   in   |  5\n-\/\/        |        |  args  |  4   Holes in incoming args owned by SELF\n-\/\/  |     |        |        |  3\n-\/\/  |     |        +--------+\n-\/\/  V     |        | old out|      Empty on Intel, window on Sparc\n-\/\/        |    old |preserve|      Must be even aligned.\n-\/\/        |     SP-+--------+----> Matcher::_old_SP, even aligned\n-\/\/        |        |   in   |  3   area for Intel ret address\n-\/\/     Owned by    |preserve|      Empty on Sparc.\n-\/\/       SELF      +--------+\n-\/\/        |        |  pad2  |  2   pad to align old SP\n-\/\/        |        +--------+  1\n-\/\/        |        | locks  |  0\n-\/\/        |        +--------+----> OptoReg::stack0(), even aligned\n-\/\/        |        |  pad1  | 11   pad to align new SP\n-\/\/        |        +--------+\n-\/\/        |        |        | 10\n-\/\/        |        | spills |  9   spills\n-\/\/        V        |        |  8   (pad0 slot for callee)\n-\/\/      -----------+--------+----> Matcher::_out_arg_limit, unaligned\n-\/\/        ^        |  out   |  7\n-\/\/        |        |  args  |  6   Holes in outgoing args owned by CALLEE\n-\/\/     Owned by    +--------+\n-\/\/      CALLEE     | new out|  6   Empty on Intel, window on Sparc\n-\/\/        |    new |preserve|      Must be even-aligned.\n-\/\/        |     SP-+--------+----> Matcher::_new_SP, even aligned\n-\/\/        |        |        |\n-\/\/\n-\/\/ Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is\n-\/\/         known from SELF's arguments and the Java calling convention.\n-\/\/         Region 6-7 is determined per call site.\n-\/\/ Note 2: If the calling convention leaves holes in the incoming argument\n-\/\/         area, those holes are owned by SELF.  Holes in the outgoing area\n-\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n-\/\/         incoming area, as the Java calling convention is completely under\n-\/\/         the control of the AD file.  Doubles can be sorted and packed to\n-\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n-\/\/         varargs C calling conventions.\n-\/\/ Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is\n-\/\/         even aligned with pad0 as needed.\n-\/\/         Region 6 is even aligned.  Region 6-7 is NOT even aligned;\n-\/\/         region 6-11 is even aligned; it may be padded out more so that\n-\/\/         the region from SP to FP meets the minimum stack alignment.\n-\n-frame %{\n-  \/\/ These three registers define part of the calling convention\n-  \/\/ between compiled code and the interpreter.\n-  inline_cache_reg(EAX);                \/\/ Inline Cache Register\n-\n-  \/\/ Optional: name the operand used by cisc-spilling to access [stack_pointer + offset]\n-  cisc_spilling_operand_name(indOffset32);\n-\n-  \/\/ Number of stack slots consumed by locking an object\n-  sync_stack_slots(1);\n-\n-  \/\/ Compiled code's Frame Pointer\n-  frame_pointer(ESP);\n-  \/\/ Interpreter stores its frame pointer in a register which is\n-  \/\/ stored to the stack by I2CAdaptors.\n-  \/\/ I2CAdaptors convert from interpreted java to compiled java.\n-  interpreter_frame_pointer(EBP);\n-\n-  \/\/ Stack alignment requirement\n-  \/\/ Alignment size in bytes (128-bit -> 16 bytes)\n-  stack_alignment(StackAlignmentInBytes);\n-\n-  \/\/ Number of outgoing stack slots killed above the out_preserve_stack_slots\n-  \/\/ for calls to C.  Supports the var-args backing area for register parms.\n-  varargs_C_out_slots_killed(0);\n-\n-  \/\/ The after-PROLOG location of the return address.  Location of\n-  \/\/ return address specifies a type (REG or STACK) and a number\n-  \/\/ representing the register number (i.e. - use a register name) or\n-  \/\/ stack slot.\n-  \/\/ Ret Addr is on stack in slot 0 if no locks or verification or alignment.\n-  \/\/ Otherwise, it is above the locks and verification slot and alignment word\n-  return_addr(STACK - 1 +\n-              align_up((Compile::current()->in_preserve_stack_slots() +\n-                        Compile::current()->fixed_slots()),\n-                       stack_alignment_in_slots()));\n-\n-  \/\/ Location of C & interpreter return values\n-  c_return_value %{\n-    assert( ideal_reg >= Op_RegI && ideal_reg <= Op_RegL, \"only return normal values\" );\n-    static int lo[Op_RegL+1] = { 0, 0, OptoReg::Bad, EAX_num,      EAX_num,      FPR1L_num,    FPR1L_num, EAX_num };\n-    static int hi[Op_RegL+1] = { 0, 0, OptoReg::Bad, OptoReg::Bad, OptoReg::Bad, OptoReg::Bad, FPR1H_num, EDX_num };\n-\n-    \/\/ in SSE2+ mode we want to keep the FPU stack clean so pretend\n-    \/\/ that C functions return float and double results in XMM0.\n-    if( ideal_reg == Op_RegD && UseSSE>=2 )\n-      return OptoRegPair(XMM0b_num,XMM0_num);\n-    if( ideal_reg == Op_RegF && UseSSE>=2 )\n-      return OptoRegPair(OptoReg::Bad,XMM0_num);\n-\n-    return OptoRegPair(hi[ideal_reg],lo[ideal_reg]);\n-  %}\n-\n-  \/\/ Location of return values\n-  return_value %{\n-    assert( ideal_reg >= Op_RegI && ideal_reg <= Op_RegL, \"only return normal values\" );\n-    static int lo[Op_RegL+1] = { 0, 0, OptoReg::Bad, EAX_num,      EAX_num,      FPR1L_num,    FPR1L_num, EAX_num };\n-    static int hi[Op_RegL+1] = { 0, 0, OptoReg::Bad, OptoReg::Bad, OptoReg::Bad, OptoReg::Bad, FPR1H_num, EDX_num };\n-    if( ideal_reg == Op_RegD && UseSSE>=2 )\n-      return OptoRegPair(XMM0b_num,XMM0_num);\n-    if( ideal_reg == Op_RegF && UseSSE>=1 )\n-      return OptoRegPair(OptoReg::Bad,XMM0_num);\n-    return OptoRegPair(hi[ideal_reg],lo[ideal_reg]);\n-  %}\n-\n-%}\n-\n-\/\/----------ATTRIBUTES---------------------------------------------------------\n-\/\/----------Operand Attributes-------------------------------------------------\n-op_attrib op_cost(0);        \/\/ Required cost attribute\n-\n-\/\/----------Instruction Attributes---------------------------------------------\n-ins_attrib ins_cost(100);       \/\/ Required cost attribute\n-ins_attrib ins_size(8);         \/\/ Required size attribute (in bits)\n-ins_attrib ins_short_branch(0); \/\/ Required flag: is this instruction a\n-                                \/\/ non-matching short branch variant of some\n-                                                            \/\/ long branch?\n-ins_attrib ins_alignment(1);    \/\/ Required alignment attribute (must be a power of 2)\n-                                \/\/ specifies the alignment that some part of the instruction (not\n-                                \/\/ necessarily the start) requires.  If > 1, a compute_padding()\n-                                \/\/ function must be provided for the instruction\n-\n-\/\/----------OPERANDS-----------------------------------------------------------\n-\/\/ Operand definitions must precede instruction definitions for correct parsing\n-\/\/ in the ADLC because operands constitute user defined types which are used in\n-\/\/ instruction definitions.\n-\n-\/\/----------Simple Operands----------------------------------------------------\n-\/\/ Immediate Operands\n-\/\/ Integer Immediate\n-operand immI() %{\n-  match(ConI);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for test vs zero\n-operand immI_0() %{\n-  predicate(n->get_int() == 0);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for increment\n-operand immI_1() %{\n-  predicate(n->get_int() == 1);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for decrement\n-operand immI_M1() %{\n-  predicate(n->get_int() == -1);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Valid scale values for addressing modes\n-operand immI2() %{\n-  predicate(0 <= n->get_int() && (n->get_int() <= 3));\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI8() %{\n-  predicate((-128 <= n->get_int()) && (n->get_int() <= 127));\n-  match(ConI);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immU8() %{\n-  predicate((0 <= n->get_int()) && (n->get_int() <= 255));\n-  match(ConI);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI16() %{\n-  predicate((-32768 <= n->get_int()) && (n->get_int() <= 32767));\n-  match(ConI);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Int Immediate non-negative\n-operand immU31()\n-%{\n-  predicate(n->get_int() >= 0);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for long shifts\n-operand immI_32() %{\n-  predicate( n->get_int() == 32 );\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_1_31() %{\n-  predicate( n->get_int() >= 1 && n->get_int() <= 31 );\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_32_63() %{\n-  predicate( n->get_int() >= 32 && n->get_int() <= 63 );\n-  match(ConI);\n-  op_cost(0);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_2() %{\n-  predicate( n->get_int() == 2 );\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_3() %{\n-  predicate( n->get_int() == 3 );\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_4()\n-%{\n-  predicate(n->get_int() == 4);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_8()\n-%{\n-  predicate(n->get_int() == 8);\n-  match(ConI);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Pointer Immediate\n-operand immP() %{\n-  match(ConP);\n-\n-  op_cost(10);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Null Pointer Immediate\n-operand immP0() %{\n-  predicate( n->get_ptr() == 0 );\n-  match(ConP);\n-  op_cost(0);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate\n-operand immL() %{\n-  match(ConL);\n-\n-  op_cost(20);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate zero\n-operand immL0() %{\n-  predicate( n->get_long() == 0L );\n-  match(ConL);\n-  op_cost(0);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate zero\n-operand immL_M1() %{\n-  predicate( n->get_long() == -1L );\n-  match(ConL);\n-  op_cost(0);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long immediate from 0 to 127.\n-\/\/ Used for a shorter form of long mul by 10.\n-operand immL_127() %{\n-  predicate((0 <= n->get_long()) && (n->get_long() <= 127));\n-  match(ConL);\n-  op_cost(0);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate: low 32-bit mask\n-operand immL_32bits() %{\n-  predicate(n->get_long() == 0xFFFFFFFFL);\n-  match(ConL);\n-  op_cost(0);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Long Immediate: low 32-bit mask\n-operand immL32() %{\n-  predicate(n->get_long() == (int)(n->get_long()));\n-  match(ConL);\n-  op_cost(20);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/Double Immediate zero\n-operand immDPR0() %{\n-  \/\/ Do additional (and counter-intuitive) test against NaN to work around VC++\n-  \/\/ bug that generates code such that NaNs compare equal to 0.0\n-  predicate( UseSSE<=1 && n->getd() == 0.0 && !g_isnan(n->getd()) );\n-  match(ConD);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Double Immediate one\n-operand immDPR1() %{\n-  predicate( UseSSE<=1 && n->getd() == 1.0 );\n-  match(ConD);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Double Immediate\n-operand immDPR() %{\n-  predicate(UseSSE<=1);\n-  match(ConD);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immD() %{\n-  predicate(UseSSE>=2);\n-  match(ConD);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Double Immediate zero\n-operand immD0() %{\n-  \/\/ Do additional (and counter-intuitive) test against NaN to work around VC++\n-  \/\/ bug that generates code such that NaNs compare equal to 0.0 AND do not\n-  \/\/ compare equal to -0.0.\n-  predicate( UseSSE>=2 && jlong_cast(n->getd()) == 0 );\n-  match(ConD);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate zero\n-operand immFPR0() %{\n-  predicate(UseSSE == 0 && n->getf() == 0.0F);\n-  match(ConF);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate one\n-operand immFPR1() %{\n-  predicate(UseSSE == 0 && n->getf() == 1.0F);\n-  match(ConF);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate\n-operand immFPR() %{\n-  predicate( UseSSE == 0 );\n-  match(ConF);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate\n-operand immF() %{\n-  predicate(UseSSE >= 1);\n-  match(ConF);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Float Immediate zero.  Zero and not -0.0\n-operand immF0() %{\n-  predicate( UseSSE >= 1 && jint_cast(n->getf()) == 0 );\n-  match(ConF);\n-\n-  op_cost(5);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Immediates for special shifts (sign extend)\n-\n-\/\/ Constants for increment\n-operand immI_16() %{\n-  predicate( n->get_int() == 16 );\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand immI_24() %{\n-  predicate( n->get_int() == 24 );\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for byte-wide masking\n-operand immI_255() %{\n-  predicate( n->get_int() == 255 );\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-\/\/ Constant for short-wide masking\n-operand immI_65535() %{\n-  predicate(n->get_int() == 65535);\n-  match(ConI);\n-\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n-operand kReg()\n-%{\n-  constraint(ALLOC_IN_RC(vectmask_reg));\n-  match(RegVectMask);\n-  format %{%}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Register Operands\n-\/\/ Integer Register\n-operand rRegI() %{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(RegI);\n-  match(xRegI);\n-  match(eAXRegI);\n-  match(eBXRegI);\n-  match(eCXRegI);\n-  match(eDXRegI);\n-  match(eDIRegI);\n-  match(eSIRegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Subset of Integer Register\n-operand xRegI(rRegI reg) %{\n-  constraint(ALLOC_IN_RC(int_x_reg));\n-  match(reg);\n-  match(eAXRegI);\n-  match(eBXRegI);\n-  match(eCXRegI);\n-  match(eDXRegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-operand eAXRegI(xRegI reg) %{\n-  constraint(ALLOC_IN_RC(eax_reg));\n-  match(reg);\n-  match(rRegI);\n-\n-  format %{ \"EAX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-operand eBXRegI(xRegI reg) %{\n-  constraint(ALLOC_IN_RC(ebx_reg));\n-  match(reg);\n-  match(rRegI);\n-\n-  format %{ \"EBX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eCXRegI(xRegI reg) %{\n-  constraint(ALLOC_IN_RC(ecx_reg));\n-  match(reg);\n-  match(rRegI);\n-\n-  format %{ \"ECX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eDXRegI(xRegI reg) %{\n-  constraint(ALLOC_IN_RC(edx_reg));\n-  match(reg);\n-  match(rRegI);\n-\n-  format %{ \"EDX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eDIRegI(xRegI reg) %{\n-  constraint(ALLOC_IN_RC(edi_reg));\n-  match(reg);\n-  match(rRegI);\n-\n-  format %{ \"EDI\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand nadxRegI() %{\n-  constraint(ALLOC_IN_RC(nadx_reg));\n-  match(RegI);\n-  match(eBXRegI);\n-  match(eCXRegI);\n-  match(eSIRegI);\n-  match(eDIRegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand ncxRegI() %{\n-  constraint(ALLOC_IN_RC(ncx_reg));\n-  match(RegI);\n-  match(eAXRegI);\n-  match(eDXRegI);\n-  match(eSIRegI);\n-  match(eDIRegI);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ \/\/ This operand was used by cmpFastUnlock, but conflicted with 'object' reg\n-\/\/ \/\/\n-operand eSIRegI(xRegI reg) %{\n-   constraint(ALLOC_IN_RC(esi_reg));\n-   match(reg);\n-   match(rRegI);\n-\n-   format %{ \"ESI\" %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ Pointer Register\n-operand anyRegP() %{\n-  constraint(ALLOC_IN_RC(any_reg));\n-  match(RegP);\n-  match(eAXRegP);\n-  match(eBXRegP);\n-  match(eCXRegP);\n-  match(eDIRegP);\n-  match(eRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eRegP() %{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(RegP);\n-  match(eAXRegP);\n-  match(eBXRegP);\n-  match(eCXRegP);\n-  match(eDIRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand rRegP() %{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(RegP);\n-  match(eAXRegP);\n-  match(eBXRegP);\n-  match(eCXRegP);\n-  match(eDIRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ On windows95, EBP is not safe to use for implicit null tests.\n-operand eRegP_no_EBP() %{\n-  constraint(ALLOC_IN_RC(int_reg_no_ebp));\n-  match(RegP);\n-  match(eAXRegP);\n-  match(eBXRegP);\n-  match(eCXRegP);\n-  match(eDIRegP);\n-\n-  op_cost(100);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand pRegP() %{\n-  constraint(ALLOC_IN_RC(p_reg));\n-  match(RegP);\n-  match(eBXRegP);\n-  match(eDXRegP);\n-  match(eSIRegP);\n-  match(eDIRegP);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Special Registers\n-\/\/ Return a pointer value\n-operand eAXRegP(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(eax_reg));\n-  match(reg);\n-  format %{ \"EAX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Used in AtomicAdd\n-operand eBXRegP(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(ebx_reg));\n-  match(reg);\n-  format %{ \"EBX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Tail-call (interprocedural jump) to interpreter\n-operand eCXRegP(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(ecx_reg));\n-  match(reg);\n-  format %{ \"ECX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eDXRegP(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(edx_reg));\n-  match(reg);\n-  format %{ \"EDX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eSIRegP(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(esi_reg));\n-  match(reg);\n-  format %{ \"ESI\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Used in rep stosw\n-operand eDIRegP(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(edi_reg));\n-  match(reg);\n-  format %{ \"EDI\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eRegL() %{\n-  constraint(ALLOC_IN_RC(long_reg));\n-  match(RegL);\n-  match(eADXRegL);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eADXRegL( eRegL reg ) %{\n-  constraint(ALLOC_IN_RC(eadx_reg));\n-  match(reg);\n-\n-  format %{ \"EDX:EAX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eBCXRegL( eRegL reg ) %{\n-  constraint(ALLOC_IN_RC(ebcx_reg));\n-  match(reg);\n-\n-  format %{ \"EBX:ECX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eBDPRegL( eRegL reg ) %{\n-  constraint(ALLOC_IN_RC(ebpd_reg));\n-  match(reg);\n-\n-  format %{ \"EBP:EDI\" %}\n-  interface(REG_INTER);\n-%}\n-\/\/ Special case for integer high multiply\n-operand eADXRegL_low_only() %{\n-  constraint(ALLOC_IN_RC(eadx_reg));\n-  match(RegL);\n-\n-  format %{ \"EAX\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Flags register, used as output of compare instructions\n-operand rFlagsReg() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-\n-  format %{ \"EFLAGS\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Flags register, used as output of compare instructions\n-operand eFlagsReg() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-\n-  format %{ \"EFLAGS\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Flags register, used as output of FLOATING POINT compare instructions\n-operand eFlagsRegU() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-\n-  format %{ \"EFLAGS_U\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand eFlagsRegUCF() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  predicate(false);\n-\n-  format %{ \"EFLAGS_U_CF\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Condition Code Register used by long compare\n-operand flagsReg_long_LTGE() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  format %{ \"FLAGS_LTGE\" %}\n-  interface(REG_INTER);\n-%}\n-operand flagsReg_long_EQNE() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  format %{ \"FLAGS_EQNE\" %}\n-  interface(REG_INTER);\n-%}\n-operand flagsReg_long_LEGT() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  format %{ \"FLAGS_LEGT\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Condition Code Register used by unsigned long compare\n-operand flagsReg_ulong_LTGE() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  format %{ \"FLAGS_U_LTGE\" %}\n-  interface(REG_INTER);\n-%}\n-operand flagsReg_ulong_EQNE() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  format %{ \"FLAGS_U_EQNE\" %}\n-  interface(REG_INTER);\n-%}\n-operand flagsReg_ulong_LEGT() %{\n-  constraint(ALLOC_IN_RC(int_flags));\n-  match(RegFlags);\n-  format %{ \"FLAGS_U_LEGT\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand regDPR() %{\n-  predicate( UseSSE < 2 );\n-  constraint(ALLOC_IN_RC(fp_dbl_reg));\n-  match(RegD);\n-  match(regDPR1);\n-  match(regDPR2);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand regDPR1(regDPR reg) %{\n-  predicate( UseSSE < 2 );\n-  constraint(ALLOC_IN_RC(fp_dbl_reg0));\n-  match(reg);\n-  format %{ \"FPR1\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand regDPR2(regDPR reg) %{\n-  predicate( UseSSE < 2 );\n-  constraint(ALLOC_IN_RC(fp_dbl_reg1));\n-  match(reg);\n-  format %{ \"FPR2\" %}\n-  interface(REG_INTER);\n-%}\n-\n-operand regnotDPR1(regDPR reg) %{\n-  predicate( UseSSE < 2 );\n-  constraint(ALLOC_IN_RC(fp_dbl_notreg0));\n-  match(reg);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand regFPR() %{\n-  predicate( UseSSE < 2 );\n-  constraint(ALLOC_IN_RC(fp_flt_reg));\n-  match(RegF);\n-  match(regFPR1);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand regFPR1(regFPR reg) %{\n-  predicate( UseSSE < 2 );\n-  constraint(ALLOC_IN_RC(fp_flt_reg0));\n-  match(reg);\n-  format %{ \"FPR1\" %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ XMM Float register operands\n-operand regF() %{\n-  predicate( UseSSE>=1 );\n-  constraint(ALLOC_IN_RC(float_reg_legacy));\n-  match(RegF);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand legRegF() %{\n-  predicate( UseSSE>=1 );\n-  constraint(ALLOC_IN_RC(float_reg_legacy));\n-  match(RegF);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Float register operands\n-operand vlRegF() %{\n-   constraint(ALLOC_IN_RC(float_reg_vl));\n-   match(RegF);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/ XMM Double register operands\n-operand regD() %{\n-  predicate( UseSSE>=2 );\n-  constraint(ALLOC_IN_RC(double_reg_legacy));\n-  match(RegD);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Double register operands\n-operand legRegD() %{\n-  predicate( UseSSE>=2 );\n-  constraint(ALLOC_IN_RC(double_reg_legacy));\n-  match(RegD);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n-operand vlRegD() %{\n-   constraint(ALLOC_IN_RC(double_reg_vl));\n-   match(RegD);\n-\n-   format %{ %}\n-   interface(REG_INTER);\n-%}\n-\n-\/\/----------Memory Operands----------------------------------------------------\n-\/\/ Direct Memory Operand\n-operand direct(immP addr) %{\n-  match(addr);\n-\n-  format %{ \"[$addr]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0xFFFFFFFF);\n-    index(0x4);\n-    scale(0x0);\n-    disp($addr);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Operand\n-operand indirect(eRegP reg) %{\n-  constraint(ALLOC_IN_RC(int_reg));\n-  match(reg);\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Short Offset Operand\n-operand indOffset8(eRegP reg, immI8 off) %{\n-  match(AddP reg off);\n-\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand indOffset32(eRegP reg, immI off) %{\n-  match(AddP reg off);\n-\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand indOffset32X(rRegI reg, immP off) %{\n-  match(AddP off reg);\n-\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndexOffset(eRegP reg, rRegI ireg, immI off) %{\n-  match(AddP (AddP reg ireg) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $ireg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n-operand indIndex(eRegP reg, rRegI ireg) %{\n-  match(AddP reg ireg);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $ireg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ \/\/ -------------------------------------------------------------------------\n-\/\/ \/\/ 486 architecture doesn't support \"scale * index + offset\" with out a base\n-\/\/ \/\/ -------------------------------------------------------------------------\n-\/\/ \/\/ Scaled Memory Operands\n-\/\/ \/\/ Indirect Memory Times Scale Plus Offset Operand\n-\/\/ operand indScaleOffset(immP off, rRegI ireg, immI2 scale) %{\n-\/\/   match(AddP off (LShiftI ireg scale));\n-\/\/\n-\/\/   op_cost(10);\n-\/\/   format %{\"[$off + $ireg << $scale]\" %}\n-\/\/   interface(MEMORY_INTER) %{\n-\/\/     base(0x4);\n-\/\/     index($ireg);\n-\/\/     scale($scale);\n-\/\/     disp($off);\n-\/\/   %}\n-\/\/ %}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register\n-operand indIndexScale(eRegP reg, rRegI ireg, immI2 scale) %{\n-  match(AddP reg (LShiftI ireg scale));\n-\n-  op_cost(10);\n-  format %{\"[$reg + $ireg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale($scale);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n-operand indIndexScaleOffset(eRegP reg, immI off, rRegI ireg, immI2 scale) %{\n-  match(AddP (AddP reg (LShiftI ireg scale)) off);\n-\n-  op_cost(10);\n-  format %{\"[$reg + $off + $ireg << $scale]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index($ireg);\n-    scale($scale);\n-    disp($off);\n-  %}\n-%}\n-\n-\/\/----------Load Long Memory Operands------------------------------------------\n-\/\/ The load-long idiom will use it's address expression again after loading\n-\/\/ the first word of the long.  If the load-long destination overlaps with\n-\/\/ registers used in the addressing expression, the 2nd half will be loaded\n-\/\/ from a clobbered address.  Fix this by requiring that load-long use\n-\/\/ address registers that do not overlap with the load-long target.\n-\n-\/\/ load-long support\n-operand load_long_RegP() %{\n-  constraint(ALLOC_IN_RC(esi_reg));\n-  match(RegP);\n-  match(eSIRegP);\n-  op_cost(100);\n-  format %{  %}\n-  interface(REG_INTER);\n-%}\n-\n-\/\/ Indirect Memory Operand Long\n-operand load_long_indirect(load_long_RegP reg) %{\n-  constraint(ALLOC_IN_RC(esi_reg));\n-  match(reg);\n-\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp(0x0);\n-  %}\n-%}\n-\n-\/\/ Indirect Memory Plus Long Offset Operand\n-operand load_long_indOffset32(load_long_RegP reg, immI off) %{\n-  match(AddP reg off);\n-\n-  format %{ \"[$reg + $off]\" %}\n-  interface(MEMORY_INTER) %{\n-    base($reg);\n-    index(0x4);\n-    scale(0x0);\n-    disp($off);\n-  %}\n-%}\n-\n-opclass load_long_memory(load_long_indirect, load_long_indOffset32);\n-\n-\n-\/\/----------Special Memory Operands--------------------------------------------\n-\/\/ Stack Slot Operand - This operand is used for loading and storing temporary\n-\/\/                      values on the stack where a match requires a value to\n-\/\/                      flow through memory.\n-operand stackSlotP(sRegP reg) %{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ ESP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotI(sRegI reg) %{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ ESP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotF(sRegF reg) %{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ ESP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotD(sRegD reg) %{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ ESP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-operand stackSlotL(sRegL reg) %{\n-  constraint(ALLOC_IN_RC(stack_slots));\n-  \/\/ No match rule because this operand is only generated in matching\n-  format %{ \"[$reg]\" %}\n-  interface(MEMORY_INTER) %{\n-    base(0x4);   \/\/ ESP\n-    index(0x4);  \/\/ No Index\n-    scale(0x0);  \/\/ No Scale\n-    disp($reg);  \/\/ Stack Offset\n-  %}\n-%}\n-\n-\/\/----------Conditional Branch Operands----------------------------------------\n-\/\/ Comparison Op  - This is the operation of the comparison, and is limited to\n-\/\/                  the following set of codes:\n-\/\/                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)\n-\/\/\n-\/\/ Other attributes of the comparison, such as unsignedness, are specified\n-\/\/ by the comparison instruction that sets a condition code flags register.\n-\/\/ That result is represented by a flags operand whose subtype is appropriate\n-\/\/ to the unsignedness (etc.) of the comparison.\n-\/\/\n-\/\/ Later, the instruction which matches both the Comparison Op (a Bool) and\n-\/\/ the flags (produced by the Cmp) specifies the coding of the comparison op\n-\/\/ by matching a specific subtype of Bool operand below, such as cmpOpU.\n-\n-\/\/ Comparison Code\n-operand cmpOp() %{\n-  match(Bool);\n-\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0xC, \"l\");\n-    greater_equal(0xD, \"ge\");\n-    less_equal(0xE, \"le\");\n-    greater(0xF, \"g\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/ Comparison Code, unsigned compare.  Used by FP also, with\n-\/\/ C2 (unordered) turned into GT or LT already.  The other bits\n-\/\/ C0 and C3 are turned into Carry & Zero flags.\n-operand cmpOpU() %{\n-  match(Bool);\n-\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0x2, \"b\");\n-    greater_equal(0x3, \"nb\");\n-    less_equal(0x6, \"be\");\n-    greater(0x7, \"nbe\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/ Floating comparisons that don't require any fixup for the unordered case\n-operand cmpOpUCF() %{\n-  match(Bool);\n-  predicate(n->as_Bool()->_test._test == BoolTest::lt ||\n-            n->as_Bool()->_test._test == BoolTest::ge ||\n-            n->as_Bool()->_test._test == BoolTest::le ||\n-            n->as_Bool()->_test._test == BoolTest::gt);\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0x2, \"b\");\n-    greater_equal(0x3, \"nb\");\n-    less_equal(0x6, \"be\");\n-    greater(0x7, \"nbe\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\n-\/\/ Floating comparisons that can be fixed up with extra conditional jumps\n-operand cmpOpUCF2() %{\n-  match(Bool);\n-  predicate(n->as_Bool()->_test._test == BoolTest::ne ||\n-            n->as_Bool()->_test._test == BoolTest::eq);\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0x2, \"b\");\n-    greater_equal(0x3, \"nb\");\n-    less_equal(0x6, \"be\");\n-    greater(0x7, \"nbe\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/ Comparison Code for FP conditional move\n-operand cmpOp_fcmov() %{\n-  match(Bool);\n-\n-  predicate(n->as_Bool()->_test._test != BoolTest::overflow &&\n-            n->as_Bool()->_test._test != BoolTest::no_overflow);\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal        (0x0C8);\n-    not_equal    (0x1C8);\n-    less         (0x0C0);\n-    greater_equal(0x1C0);\n-    less_equal   (0x0D0);\n-    greater      (0x1D0);\n-    overflow(0x0, \"o\"); \/\/ not really supported by the instruction\n-    no_overflow(0x1, \"no\"); \/\/ not really supported by the instruction\n-  %}\n-%}\n-\n-\/\/ Comparison Code used in long compares\n-operand cmpOp_commute() %{\n-  match(Bool);\n-\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0xF, \"g\");\n-    greater_equal(0xE, \"le\");\n-    less_equal(0xD, \"ge\");\n-    greater(0xC, \"l\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/ Comparison Code used in unsigned long compares\n-operand cmpOpU_commute() %{\n-  match(Bool);\n-\n-  format %{ \"\" %}\n-  interface(COND_INTER) %{\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n-    less(0x7, \"nbe\");\n-    greater_equal(0x6, \"be\");\n-    less_equal(0x3, \"nb\");\n-    greater(0x2, \"b\");\n-    overflow(0x0, \"o\");\n-    no_overflow(0x1, \"no\");\n-  %}\n-%}\n-\n-\/\/----------OPERAND CLASSES----------------------------------------------------\n-\/\/ Operand Classes are groups of operands that are used as to simplify\n-\/\/ instruction definitions by not requiring the AD writer to specify separate\n-\/\/ instructions for every form of operand when the instruction accepts\n-\/\/ multiple operand types with the same basic encoding and format.  The classic\n-\/\/ case of this is memory operands.\n-\n-opclass memory(direct, indirect, indOffset8, indOffset32, indOffset32X, indIndexOffset,\n-               indIndex, indIndexScale, indIndexScaleOffset);\n-\n-\/\/ Long memory operations are encoded in 2 instructions and a +4 offset.\n-\/\/ This means some kind of offset is always required and you cannot use\n-\/\/ an oop as the offset (done when working on static globals).\n-opclass long_memory(direct, indirect, indOffset8, indOffset32, indIndexOffset,\n-                    indIndex, indIndexScale, indIndexScaleOffset);\n-\n-\n-\/\/----------PIPELINE-----------------------------------------------------------\n-\/\/ Rules which define the behavior of the target architectures pipeline.\n-pipeline %{\n-\n-\/\/----------ATTRIBUTES---------------------------------------------------------\n-attributes %{\n-  variable_size_instructions;        \/\/ Fixed size instructions\n-  max_instructions_per_bundle = 3;   \/\/ Up to 3 instructions per bundle\n-  instruction_unit_size = 1;         \/\/ An instruction is 1 bytes long\n-  instruction_fetch_unit_size = 16;  \/\/ The processor fetches one line\n-  instruction_fetch_units = 1;       \/\/ of 16 bytes\n-\n-  \/\/ List of nop instructions\n-  nops( MachNop );\n-%}\n-\n-\/\/----------RESOURCES----------------------------------------------------------\n-\/\/ Resources are the functional units available to the machine\n-\n-\/\/ Generic P2\/P3 pipeline\n-\/\/ 3 decoders, only D0 handles big operands; a \"bundle\" is the limit of\n-\/\/ 3 instructions decoded per cycle.\n-\/\/ 2 load\/store ops per cycle, 1 branch, 1 FPU,\n-\/\/ 2 ALU op, only ALU0 handles mul\/div instructions.\n-resources( D0, D1, D2, DECODE = D0 | D1 | D2,\n-           MS0, MS1, MEM = MS0 | MS1,\n-           BR, FPU,\n-           ALU0, ALU1, ALU = ALU0 | ALU1 );\n-\n-\/\/----------PIPELINE DESCRIPTION-----------------------------------------------\n-\/\/ Pipeline Description specifies the stages in the machine's pipeline\n-\n-\/\/ Generic P2\/P3 pipeline\n-pipe_desc(S0, S1, S2, S3, S4, S5);\n-\n-\/\/----------PIPELINE CLASSES---------------------------------------------------\n-\/\/ Pipeline Classes describe the stages in which input and output are\n-\/\/ referenced by the hardware pipeline.\n-\n-\/\/ Naming convention: ialu or fpu\n-\/\/ Then: _reg\n-\/\/ Then: _reg if there is a 2nd register\n-\/\/ Then: _long if it's a pair of instructions implementing a long\n-\/\/ Then: _fat if it requires the big decoder\n-\/\/   Or: _mem if it requires the big decoder and a memory unit.\n-\n-\/\/ Integer ALU reg operation\n-pipe_class ialu_reg(rRegI dst) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Long ALU reg operation\n-pipe_class ialu_reg_long(eRegL dst) %{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    ALU    : S3(2);     \/\/ both alus\n-%}\n-\n-\/\/ Integer ALU reg operation using big decoder\n-pipe_class ialu_reg_fat(rRegI dst) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Long ALU reg operation using big decoder\n-pipe_class ialu_reg_long_fat(eRegL dst) %{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    dst    : S3(read);\n-    D0     : S0(2);     \/\/ big decoder only; twice\n-    ALU    : S3(2);     \/\/ any 2 alus\n-%}\n-\n-\/\/ Integer ALU reg-reg operation\n-pipe_class ialu_reg_reg(rRegI dst, rRegI src) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Long ALU reg-reg operation\n-pipe_class ialu_reg_reg_long(eRegL dst, eRegL src) %{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    src    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    ALU    : S3(2);     \/\/ both alus\n-%}\n-\n-\/\/ Integer ALU reg-reg operation\n-pipe_class ialu_reg_reg_fat(rRegI dst, memory src) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Long ALU reg-reg operation\n-pipe_class ialu_reg_reg_long_fat(eRegL dst, eRegL src) %{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    src    : S3(read);\n-    D0     : S0(2);     \/\/ big decoder only; twice\n-    ALU    : S3(2);     \/\/ both alus\n-%}\n-\n-\/\/ Integer ALU reg-mem operation\n-pipe_class ialu_reg_mem(rRegI dst, memory mem) %{\n-    single_instruction;\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Long ALU reg-mem operation\n-pipe_class ialu_reg_long_mem(eRegL dst, load_long_memory mem) %{\n-    instruction_count(2);\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0(2);     \/\/ big decoder only; twice\n-    ALU    : S4(2);     \/\/ any 2 alus\n-    MEM    : S3(2);     \/\/ both mems\n-%}\n-\n-\/\/ Integer mem operation (prefetch)\n-pipe_class ialu_mem(memory mem)\n-%{\n-    single_instruction;\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Integer Store to Memory\n-pipe_class ialu_mem_reg(memory mem, rRegI src) %{\n-    single_instruction;\n-    mem    : S3(read);\n-    src    : S5(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;\n-%}\n-\n-\/\/ Long Store to Memory\n-pipe_class ialu_mem_long_reg(memory mem, eRegL src) %{\n-    instruction_count(2);\n-    mem    : S3(read);\n-    src    : S5(read);\n-    D0     : S0(2);     \/\/ big decoder only; twice\n-    ALU    : S4(2);     \/\/ any 2 alus\n-    MEM    : S3(2);     \/\/ Both mems\n-%}\n-\n-\/\/ Integer Store to Memory\n-pipe_class ialu_mem_imm(memory mem) %{\n-    single_instruction;\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;\n-%}\n-\n-\/\/ Integer ALU0 reg-reg operation\n-pipe_class ialu_reg_reg_alu0(rRegI dst, rRegI src) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    D0     : S0;        \/\/ Big decoder only\n-    ALU0   : S3;        \/\/ only alu0\n-%}\n-\n-\/\/ Integer ALU0 reg-mem operation\n-pipe_class ialu_reg_mem_alu0(rRegI dst, memory mem) %{\n-    single_instruction;\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU0   : S4;        \/\/ ALU0 only\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Integer ALU reg-reg operation\n-pipe_class ialu_cr_reg_reg(eFlagsReg cr, rRegI src1, rRegI src2) %{\n-    single_instruction;\n-    cr     : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-imm operation\n-pipe_class ialu_cr_reg_imm(eFlagsReg cr, rRegI src1) %{\n-    single_instruction;\n-    cr     : S4(write);\n-    src1   : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    ALU    : S3;        \/\/ any alu\n-%}\n-\n-\/\/ Integer ALU reg-mem operation\n-pipe_class ialu_cr_reg_mem(eFlagsReg cr, rRegI src1, memory src2) %{\n-    single_instruction;\n-    cr     : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    ALU    : S4;        \/\/ any alu\n-    MEM    : S3;\n-%}\n-\n-\/\/ Conditional move reg-reg\n-pipe_class pipe_cmplt( rRegI p, rRegI q, rRegI y ) %{\n-    instruction_count(4);\n-    y      : S4(read);\n-    q      : S3(read);\n-    p      : S3(read);\n-    DECODE : S0(4);     \/\/ any decoder\n-%}\n-\n-\/\/ Conditional move reg-reg\n-pipe_class pipe_cmov_reg( rRegI dst, rRegI src, eFlagsReg cr ) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-%}\n-\n-\/\/ Conditional move reg-mem\n-pipe_class pipe_cmov_mem( eFlagsReg cr, rRegI dst, memory src) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-    MEM    : S3;\n-%}\n-\n-\/\/ Conditional move reg-reg long\n-pipe_class pipe_cmov_reg_long( eFlagsReg cr, eRegL dst, eRegL src) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-%}\n-\n-\/\/ Conditional move double reg-reg\n-pipe_class pipe_cmovDPR_reg( eFlagsReg cr, regDPR1 dst, regDPR src) %{\n-    single_instruction;\n-    dst    : S4(write);\n-    src    : S3(read);\n-    cr     : S3(read);\n-    DECODE : S0;        \/\/ any decoder\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg(regDPR dst) %{\n-    instruction_count(2);\n-    dst    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    FPU    : S3;\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_reg(regDPR dst, regDPR src) %{\n-    instruction_count(2);\n-    dst    : S4(write);\n-    src    : S3(read);\n-    DECODE : S0(2);     \/\/ any 2 decoders\n-    FPU    : S3;\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_reg_reg(regDPR dst, regDPR src1, regDPR src2) %{\n-    instruction_count(3);\n-    dst    : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    DECODE : S0(3);     \/\/ any 3 decoders\n-    FPU    : S3(2);\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_reg_reg_reg(regDPR dst, regDPR src1, regDPR src2, regDPR src3) %{\n-    instruction_count(4);\n-    dst    : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    src3   : S3(read);\n-    DECODE : S0(4);     \/\/ any 3 decoders\n-    FPU    : S3(2);\n-%}\n-\n-\/\/ Float reg-reg operation\n-pipe_class fpu_reg_mem_reg_reg(regDPR dst, memory src1, regDPR src2, regDPR src3) %{\n-    instruction_count(4);\n-    dst    : S4(write);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    src3   : S3(read);\n-    DECODE : S1(3);     \/\/ any 3 decoders\n-    D0     : S0;        \/\/ Big decoder only\n-    FPU    : S3(2);\n-    MEM    : S3;\n-%}\n-\n-\/\/ Float reg-mem operation\n-pipe_class fpu_reg_mem(regDPR dst, memory mem) %{\n-    instruction_count(2);\n-    dst    : S5(write);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    DECODE : S1;        \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Float reg-mem operation\n-pipe_class fpu_reg_reg_mem(regDPR dst, regDPR src1, memory mem) %{\n-    instruction_count(3);\n-    dst    : S5(write);\n-    src1   : S3(read);\n-    mem    : S3(read);\n-    D0     : S0;        \/\/ big decoder only\n-    DECODE : S1(2);     \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Float mem-reg operation\n-pipe_class fpu_mem_reg(memory mem, regDPR src) %{\n-    instruction_count(2);\n-    src    : S5(read);\n-    mem    : S3(read);\n-    DECODE : S0;        \/\/ any decoder for FPU PUSH\n-    D0     : S1;        \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_reg_reg(memory mem, regDPR src1, regDPR src2) %{\n-    instruction_count(3);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    mem    : S3(read);\n-    DECODE : S0(2);     \/\/ any decoder for FPU PUSH\n-    D0     : S1;        \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_reg_mem(memory mem, regDPR src1, memory src2) %{\n-    instruction_count(3);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    mem    : S4(read);\n-    DECODE : S0;        \/\/ any decoder for FPU PUSH\n-    D0     : S0(2);     \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3(2);     \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_mem(memory dst, memory src1) %{\n-    instruction_count(2);\n-    src1   : S3(read);\n-    dst    : S4(read);\n-    D0     : S0(2);     \/\/ big decoder only\n-    MEM    : S3(2);     \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_mem_mem(memory dst, memory src1, memory src2) %{\n-    instruction_count(3);\n-    src1   : S3(read);\n-    src2   : S3(read);\n-    dst    : S4(read);\n-    D0     : S0(3);     \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3(3);     \/\/ any mem\n-%}\n-\n-pipe_class fpu_mem_reg_con(memory mem, regDPR src1) %{\n-    instruction_count(3);\n-    src1   : S4(read);\n-    mem    : S4(read);\n-    DECODE : S0;        \/\/ any decoder for FPU PUSH\n-    D0     : S0(2);     \/\/ big decoder only\n-    FPU    : S4;\n-    MEM    : S3(2);     \/\/ any mem\n-%}\n-\n-\/\/ Float load constant\n-pipe_class fpu_reg_con(regDPR dst) %{\n-    instruction_count(2);\n-    dst    : S5(write);\n-    D0     : S0;        \/\/ big decoder only for the load\n-    DECODE : S1;        \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ Float load constant\n-pipe_class fpu_reg_reg_con(regDPR dst, regDPR src) %{\n-    instruction_count(3);\n-    dst    : S5(write);\n-    src    : S3(read);\n-    D0     : S0;        \/\/ big decoder only for the load\n-    DECODE : S1(2);     \/\/ any decoder for FPU POP\n-    FPU    : S4;\n-    MEM    : S3;        \/\/ any mem\n-%}\n-\n-\/\/ UnConditional branch\n-pipe_class pipe_jmp( label labl ) %{\n-    single_instruction;\n-    BR   : S3;\n-%}\n-\n-\/\/ Conditional branch\n-pipe_class pipe_jcc( cmpOp cmp, eFlagsReg cr, label labl ) %{\n-    single_instruction;\n-    cr    : S1(read);\n-    BR    : S3;\n-%}\n-\n-\/\/ Allocation idiom\n-pipe_class pipe_cmpxchg( eRegP dst, eRegP heap_ptr ) %{\n-    instruction_count(1); force_serialization;\n-    fixed_latency(6);\n-    heap_ptr : S3(read);\n-    DECODE   : S0(3);\n-    D0       : S2;\n-    MEM      : S3;\n-    ALU      : S3(2);\n-    dst      : S5(write);\n-    BR       : S5;\n-%}\n-\n-\/\/ Generic big\/slow expanded idiom\n-pipe_class pipe_slow(  ) %{\n-    instruction_count(10); multiple_bundles; force_serialization;\n-    fixed_latency(100);\n-    D0  : S0(2);\n-    MEM : S3(2);\n-%}\n-\n-\/\/ The real do-nothing guy\n-pipe_class empty( ) %{\n-    instruction_count(0);\n-%}\n-\n-\/\/ Define the class for the Nop node\n-define %{\n-   MachNop = empty;\n-%}\n-\n-%}\n-\n-\/\/----------INSTRUCTIONS-------------------------------------------------------\n-\/\/\n-\/\/ match      -- States which machine-independent subtree may be replaced\n-\/\/               by this instruction.\n-\/\/ ins_cost   -- The estimated cost of this instruction is used by instruction\n-\/\/               selection to identify a minimum cost tree of machine\n-\/\/               instructions that matches a tree of machine-independent\n-\/\/               instructions.\n-\/\/ format     -- A string providing the disassembly for this instruction.\n-\/\/               The value of an instruction's operand may be inserted\n-\/\/               by referring to it with a '$' prefix.\n-\/\/ opcode     -- Three instruction opcodes may be provided.  These are referred\n-\/\/               to within an encode class as $primary, $secondary, and $tertiary\n-\/\/               respectively.  The primary opcode is commonly used to\n-\/\/               indicate the type of machine instruction, while secondary\n-\/\/               and tertiary are often used for prefix options or addressing\n-\/\/               modes.\n-\/\/ ins_encode -- A list of encode classes with parameters. The encode class\n-\/\/               name must have been defined in an 'enc_class' specification\n-\/\/               in the encode section of the architecture description.\n-\n-\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n-\/\/ Load Float\n-instruct MoveF2LEG(legRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveLEG2F(regF dst, legRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveF2VL(vlRegF dst, regF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Float\n-instruct MoveVL2F(regF dst, vlRegF src) %{\n-  match(Set dst src);\n-  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\n-\n-\/\/ Load Double\n-instruct MoveD2LEG(legRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveLEG2D(regD dst, legRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveD2VL(vlRegD dst, regD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Load Double\n-instruct MoveVL2D(regD dst, vlRegD src) %{\n-  match(Set dst src);\n-  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n-  ins_encode %{\n-    ShouldNotReachHere();\n-  %}\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/----------BSWAP-Instruction--------------------------------------------------\n-instruct bytes_reverse_int(rRegI dst) %{\n-  match(Set dst (ReverseBytesI dst));\n-\n-  format %{ \"BSWAP  $dst\" %}\n-  opcode(0x0F, 0xC8);\n-  ins_encode( OpcP, OpcSReg(dst) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reverse_long(eRegL dst) %{\n-  match(Set dst (ReverseBytesL dst));\n-\n-  format %{ \"BSWAP  $dst.lo\\n\\t\"\n-            \"BSWAP  $dst.hi\\n\\t\"\n-            \"XCHG   $dst.lo $dst.hi\" %}\n-\n-  ins_cost(125);\n-  ins_encode( bswap_long_bytes(dst) );\n-  ins_pipe( ialu_reg_reg);\n-%}\n-\n-instruct bytes_reverse_unsigned_short(rRegI dst, eFlagsReg cr) %{\n-  match(Set dst (ReverseBytesUS dst));\n-  effect(KILL cr);\n-\n-  format %{ \"BSWAP  $dst\\n\\t\"\n-            \"SHR    $dst,16\\n\\t\" %}\n-  ins_encode %{\n-    __ bswapl($dst$$Register);\n-    __ shrl($dst$$Register, 16);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct bytes_reverse_short(rRegI dst, eFlagsReg cr) %{\n-  match(Set dst (ReverseBytesS dst));\n-  effect(KILL cr);\n-\n-  format %{ \"BSWAP  $dst\\n\\t\"\n-            \"SAR    $dst,16\\n\\t\" %}\n-  ins_encode %{\n-    __ bswapl($dst$$Register);\n-    __ sarl($dst$$Register, 16);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\n-\/\/---------- Zeros Count Instructions ------------------------------------------\n-\n-instruct countLeadingZerosI(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  predicate(UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"LZCNT  $dst, $src\\t# count leading zeros (int)\" %}\n-  ins_encode %{\n-    __ lzcntl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countLeadingZerosI_bsr(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  predicate(!UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"BSR    $dst, $src\\t# count leading zeros (int)\\n\\t\"\n-            \"JNZ    skip\\n\\t\"\n-            \"MOV    $dst, -1\\n\"\n-      \"skip:\\n\\t\"\n-            \"NEG    $dst\\n\\t\"\n-            \"ADD    $dst, 31\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label skip;\n-    __ bsrl(Rdst, Rsrc);\n-    __ jccb(Assembler::notZero, skip);\n-    __ movl(Rdst, -1);\n-    __ bind(skip);\n-    __ negl(Rdst);\n-    __ addl(Rdst, BitsPerInt - 1);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countLeadingZerosL(rRegI dst, eRegL src, eFlagsReg cr) %{\n-  predicate(UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosL src));\n-  effect(TEMP dst, KILL cr);\n-\n-  format %{ \"LZCNT  $dst, $src.hi\\t# count leading zeros (long)\\n\\t\"\n-            \"JNC    done\\n\\t\"\n-            \"LZCNT  $dst, $src.lo\\n\\t\"\n-            \"ADD    $dst, 32\\n\"\n-      \"done:\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label done;\n-    __ lzcntl(Rdst, HIGH_FROM_LOW(Rsrc));\n-    __ jccb(Assembler::carryClear, done);\n-    __ lzcntl(Rdst, Rsrc);\n-    __ addl(Rdst, BitsPerInt);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countLeadingZerosL_bsr(rRegI dst, eRegL src, eFlagsReg cr) %{\n-  predicate(!UseCountLeadingZerosInstruction);\n-  match(Set dst (CountLeadingZerosL src));\n-  effect(TEMP dst, KILL cr);\n-\n-  format %{ \"BSR    $dst, $src.hi\\t# count leading zeros (long)\\n\\t\"\n-            \"JZ     msw_is_zero\\n\\t\"\n-            \"ADD    $dst, 32\\n\\t\"\n-            \"JMP    not_zero\\n\"\n-      \"msw_is_zero:\\n\\t\"\n-            \"BSR    $dst, $src.lo\\n\\t\"\n-            \"JNZ    not_zero\\n\\t\"\n-            \"MOV    $dst, -1\\n\"\n-      \"not_zero:\\n\\t\"\n-            \"NEG    $dst\\n\\t\"\n-            \"ADD    $dst, 63\\n\" %}\n- ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label msw_is_zero;\n-    Label not_zero;\n-    __ bsrl(Rdst, HIGH_FROM_LOW(Rsrc));\n-    __ jccb(Assembler::zero, msw_is_zero);\n-    __ addl(Rdst, BitsPerInt);\n-    __ jmpb(not_zero);\n-    __ bind(msw_is_zero);\n-    __ bsrl(Rdst, Rsrc);\n-    __ jccb(Assembler::notZero, not_zero);\n-    __ movl(Rdst, -1);\n-    __ bind(not_zero);\n-    __ negl(Rdst);\n-    __ addl(Rdst, BitsPerLong - 1);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosI(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  predicate(UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"TZCNT    $dst, $src\\t# count trailing zeros (int)\" %}\n-  ins_encode %{\n-    __ tzcntl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosI_bsf(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  predicate(!UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosI src));\n-  effect(KILL cr);\n-\n-  format %{ \"BSF    $dst, $src\\t# count trailing zeros (int)\\n\\t\"\n-            \"JNZ    done\\n\\t\"\n-            \"MOV    $dst, 32\\n\"\n-      \"done:\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Label done;\n-    __ bsfl(Rdst, $src$$Register);\n-    __ jccb(Assembler::notZero, done);\n-    __ movl(Rdst, BitsPerInt);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosL(rRegI dst, eRegL src, eFlagsReg cr) %{\n-  predicate(UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosL src));\n-  effect(TEMP dst, KILL cr);\n-\n-  format %{ \"TZCNT  $dst, $src.lo\\t# count trailing zeros (long) \\n\\t\"\n-            \"JNC    done\\n\\t\"\n-            \"TZCNT  $dst, $src.hi\\n\\t\"\n-            \"ADD    $dst, 32\\n\"\n-            \"done:\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label done;\n-    __ tzcntl(Rdst, Rsrc);\n-    __ jccb(Assembler::carryClear, done);\n-    __ tzcntl(Rdst, HIGH_FROM_LOW(Rsrc));\n-    __ addl(Rdst, BitsPerInt);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct countTrailingZerosL_bsf(rRegI dst, eRegL src, eFlagsReg cr) %{\n-  predicate(!UseCountTrailingZerosInstruction);\n-  match(Set dst (CountTrailingZerosL src));\n-  effect(TEMP dst, KILL cr);\n-\n-  format %{ \"BSF    $dst, $src.lo\\t# count trailing zeros (long)\\n\\t\"\n-            \"JNZ    done\\n\\t\"\n-            \"BSF    $dst, $src.hi\\n\\t\"\n-            \"JNZ    msw_not_zero\\n\\t\"\n-            \"MOV    $dst, 32\\n\"\n-      \"msw_not_zero:\\n\\t\"\n-            \"ADD    $dst, 32\\n\"\n-      \"done:\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    Label msw_not_zero;\n-    Label done;\n-    __ bsfl(Rdst, Rsrc);\n-    __ jccb(Assembler::notZero, done);\n-    __ bsfl(Rdst, HIGH_FROM_LOW(Rsrc));\n-    __ jccb(Assembler::notZero, msw_not_zero);\n-    __ movl(Rdst, BitsPerInt);\n-    __ bind(msw_not_zero);\n-    __ addl(Rdst, BitsPerInt);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\n-\/\/---------- Population Count Instructions -------------------------------------\n-\n-instruct popCountI(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountI src));\n-  effect(KILL cr);\n-\n-  format %{ \"POPCNT $dst, $src\" %}\n-  ins_encode %{\n-    __ popcntl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct popCountI_mem(rRegI dst, memory mem, eFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountI (LoadI mem)));\n-  effect(KILL cr);\n-\n-  format %{ \"POPCNT $dst, $mem\" %}\n-  ins_encode %{\n-    __ popcntl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Note: Long.bitCount(long) returns an int.\n-instruct popCountL(rRegI dst, eRegL src, rRegI tmp, eFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountL src));\n-  effect(KILL cr, TEMP tmp, TEMP dst);\n-\n-  format %{ \"POPCNT $dst, $src.lo\\n\\t\"\n-            \"POPCNT $tmp, $src.hi\\n\\t\"\n-            \"ADD    $dst, $tmp\" %}\n-  ins_encode %{\n-    __ popcntl($dst$$Register, $src$$Register);\n-    __ popcntl($tmp$$Register, HIGH_FROM_LOW($src$$Register));\n-    __ addl($dst$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/\/ Note: Long.bitCount(long) returns an int.\n-instruct popCountL_mem(rRegI dst, memory mem, rRegI tmp, eFlagsReg cr) %{\n-  predicate(UsePopCountInstruction);\n-  match(Set dst (PopCountL (LoadL mem)));\n-  effect(KILL cr, TEMP tmp, TEMP dst);\n-\n-  format %{ \"POPCNT $dst, $mem\\n\\t\"\n-            \"POPCNT $tmp, $mem+4\\n\\t\"\n-            \"ADD    $dst, $tmp\" %}\n-  ins_encode %{\n-    \/\/__ popcntl($dst$$Register, $mem$$Address$$first);\n-    \/\/__ popcntl($tmp$$Register, $mem$$Address$$second);\n-    __ popcntl($dst$$Register, Address::make_raw($mem$$base, $mem$$index, $mem$$scale, $mem$$disp, relocInfo::none));\n-    __ popcntl($tmp$$Register, Address::make_raw($mem$$base, $mem$$index, $mem$$scale, $mem$$disp + 4, relocInfo::none));\n-    __ addl($dst$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\n-\/\/----------Load\/Store\/Move Instructions---------------------------------------\n-\/\/----------Load Instructions--------------------------------------------------\n-\/\/ Load Byte (8bit signed)\n-instruct loadB(xRegI dst, memory mem) %{\n-  match(Set dst (LoadB mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOVSX8 $dst,$mem\\t# byte\" %}\n-\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Byte (8bit signed) into Long Register\n-instruct loadB2L(eRegL dst, memory mem, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (LoadB mem)));\n-  effect(KILL cr);\n-\n-  ins_cost(375);\n-  format %{ \"MOVSX8 $dst.lo,$mem\\t# byte -> long\\n\\t\"\n-            \"MOV    $dst.hi,$dst.lo\\n\\t\"\n-            \"SAR    $dst.hi,7\" %}\n-\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-    __ movl(HIGH_FROM_LOW($dst$$Register), $dst$$Register); \/\/ This is always a different register.\n-    __ sarl(HIGH_FROM_LOW($dst$$Register), 7); \/\/ 24+1 MSB are already signed extended.\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Byte (8bit UNsigned)\n-instruct loadUB(xRegI dst, memory mem) %{\n-  match(Set dst (LoadUB mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOVZX8 $dst,$mem\\t# ubyte -> int\" %}\n-\n-  ins_encode %{\n-    __ movzbl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Byte (8 bit UNsigned) into Long Register\n-instruct loadUB2L(eRegL dst, memory mem, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (LoadUB mem)));\n-  effect(KILL cr);\n-\n-  ins_cost(250);\n-  format %{ \"MOVZX8 $dst.lo,$mem\\t# ubyte -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzbl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Byte (8 bit UNsigned) with mask into Long Register\n-instruct loadUB2L_immI(eRegL dst, memory mem, immI mask, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadUB mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"MOVZX8 $dst.lo,$mem\\t# ubyte & 32-bit mask -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\\n\\t\"\n-            \"AND    $dst.lo,right_n_bits($mask, 8)\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzbl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-    __ andl(Rdst, $mask$$constant & right_n_bits(8));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Short (16bit signed)\n-instruct loadS(rRegI dst, memory mem) %{\n-  match(Set dst (LoadS mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOVSX  $dst,$mem\\t# short\" %}\n-\n-  ins_encode %{\n-    __ movswl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Short (16 bit signed) to Byte (8 bit signed)\n-instruct loadS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI (LoadS mem) twentyfour) twentyfour));\n-\n-  ins_cost(125);\n-  format %{ \"MOVSX  $dst, $mem\\t# short -> byte\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Short (16bit signed) into Long Register\n-instruct loadS2L(eRegL dst, memory mem, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (LoadS mem)));\n-  effect(KILL cr);\n-\n-  ins_cost(375);\n-  format %{ \"MOVSX  $dst.lo,$mem\\t# short -> long\\n\\t\"\n-            \"MOV    $dst.hi,$dst.lo\\n\\t\"\n-            \"SAR    $dst.hi,15\" %}\n-\n-  ins_encode %{\n-    __ movswl($dst$$Register, $mem$$Address);\n-    __ movl(HIGH_FROM_LOW($dst$$Register), $dst$$Register); \/\/ This is always a different register.\n-    __ sarl(HIGH_FROM_LOW($dst$$Register), 15); \/\/ 16+1 MSB are already signed extended.\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16bit unsigned)\n-instruct loadUS(rRegI dst, memory mem) %{\n-  match(Set dst (LoadUS mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOVZX  $dst,$mem\\t# ushort\/char -> int\" %}\n-\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) to Byte (8 bit signed)\n-instruct loadUS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI (LoadUS mem) twentyfour) twentyfour));\n-\n-  ins_cost(125);\n-  format %{ \"MOVSX  $dst, $mem\\t# ushort -> byte\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) into Long Register\n-instruct loadUS2L(eRegL dst, memory mem, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (LoadUS mem)));\n-  effect(KILL cr);\n-\n-  ins_cost(250);\n-  format %{ \"MOVZX  $dst.lo,$mem\\t# ushort\/char -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($dst$$Register));\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with mask 0xFF into Long Register\n-instruct loadUS2L_immI_255(eRegL dst, memory mem, immI_255 mask, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"MOVZX8 $dst.lo,$mem\\t# ushort\/char & 0xFF -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzbl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with a 32-bit mask into Long Register\n-instruct loadUS2L_immI(eRegL dst, memory mem, immI mask, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"MOVZX  $dst.lo, $mem\\t# ushort\/char & 32-bit mask -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\\n\\t\"\n-            \"AND    $dst.lo,right_n_bits($mask, 16)\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzwl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-    __ andl(Rdst, $mask$$constant & right_n_bits(16));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer\n-instruct loadI(rRegI dst, memory mem) %{\n-  match(Set dst (LoadI mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOV    $dst,$mem\\t# int\" %}\n-\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Byte (8 bit signed)\n-instruct loadI2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI (LoadI mem) twentyfour) twentyfour));\n-\n-  ins_cost(125);\n-  format %{ \"MOVSX  $dst, $mem\\t# int -> byte\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Unsigned Byte (8 bit UNsigned)\n-instruct loadI2UB(rRegI dst, memory mem, immI_255 mask) %{\n-  match(Set dst (AndI (LoadI mem) mask));\n-\n-  ins_cost(125);\n-  format %{ \"MOVZX  $dst, $mem\\t# int -> ubyte\" %}\n-  ins_encode %{\n-    __ movzbl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Short (16 bit signed)\n-instruct loadI2S(rRegI dst, memory mem, immI_16 sixteen) %{\n-  match(Set dst (RShiftI (LShiftI (LoadI mem) sixteen) sixteen));\n-\n-  ins_cost(125);\n-  format %{ \"MOVSX  $dst, $mem\\t# int -> short\" %}\n-  ins_encode %{\n-    __ movswl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer (32 bit signed) to Unsigned Short\/Char (16 bit UNsigned)\n-instruct loadI2US(rRegI dst, memory mem, immI_65535 mask) %{\n-  match(Set dst (AndI (LoadI mem) mask));\n-\n-  ins_cost(125);\n-  format %{ \"MOVZX  $dst, $mem\\t# int -> ushort\/char\" %}\n-  ins_encode %{\n-    __ movzwl($dst$$Register, $mem$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer into Long Register\n-instruct loadI2L(eRegL dst, memory mem, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (LoadI mem)));\n-  effect(KILL cr);\n-\n-  ins_cost(375);\n-  format %{ \"MOV    $dst.lo,$mem\\t# int -> long\\n\\t\"\n-            \"MOV    $dst.hi,$dst.lo\\n\\t\"\n-            \"SAR    $dst.hi,31\" %}\n-\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-    __ movl(HIGH_FROM_LOW($dst$$Register), $dst$$Register); \/\/ This is always a different register.\n-    __ sarl(HIGH_FROM_LOW($dst$$Register), 31);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer with mask 0xFF into Long Register\n-instruct loadI2L_immI_255(eRegL dst, memory mem, immI_255 mask, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"MOVZX8 $dst.lo,$mem\\t# int & 0xFF -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzbl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer with mask 0xFFFF into Long Register\n-instruct loadI2L_immI_65535(eRegL dst, memory mem, immI_65535 mask, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"MOVZX  $dst.lo,$mem\\t# int & 0xFFFF -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movzwl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Integer with 31-bit mask into Long Register\n-instruct loadI2L_immU31(eRegL dst, memory mem, immU31 mask, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n-  effect(KILL cr);\n-\n-  format %{ \"MOV    $dst.lo,$mem\\t# int & 31-bit mask -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\\n\\t\"\n-            \"AND    $dst.lo,$mask\" %}\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    __ movl(Rdst, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rdst));\n-    __ andl(Rdst, $mask$$constant);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Unsigned Integer into Long Register\n-instruct loadUI2L(eRegL dst, memory mem, immL_32bits mask, eFlagsReg cr) %{\n-  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));\n-  effect(KILL cr);\n-\n-  ins_cost(250);\n-  format %{ \"MOV    $dst.lo,$mem\\t# uint -> long\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-\n-  ins_encode %{\n-    __ movl($dst$$Register, $mem$$Address);\n-    __ xorl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($dst$$Register));\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Load Long.  Cannot clobber address while loading, so restrict address\n-\/\/ register to ESI\n-instruct loadL(eRegL dst, load_long_memory mem) %{\n-  predicate(!((LoadLNode*)n)->require_atomic_access());\n-  match(Set dst (LoadL mem));\n-\n-  ins_cost(250);\n-  format %{ \"MOV    $dst.lo,$mem\\t# long\\n\\t\"\n-            \"MOV    $dst.hi,$mem+4\" %}\n-\n-  ins_encode %{\n-    Address Amemlo = Address::make_raw($mem$$base, $mem$$index, $mem$$scale, $mem$$disp, relocInfo::none);\n-    Address Amemhi = Address::make_raw($mem$$base, $mem$$index, $mem$$scale, $mem$$disp + 4, relocInfo::none);\n-    __ movl($dst$$Register, Amemlo);\n-    __ movl(HIGH_FROM_LOW($dst$$Register), Amemhi);\n-  %}\n-\n-  ins_pipe(ialu_reg_long_mem);\n-%}\n-\n-\/\/ Volatile Load Long.  Must be atomic, so do 64-bit FILD\n-\/\/ then store it down to the stack and reload on the int\n-\/\/ side.\n-instruct loadL_volatile(stackSlotL dst, memory mem) %{\n-  predicate(UseSSE<=1 && ((LoadLNode*)n)->require_atomic_access());\n-  match(Set dst (LoadL mem));\n-\n-  ins_cost(200);\n-  format %{ \"FILD   $mem\\t# Atomic volatile long load\\n\\t\"\n-            \"FISTp  $dst\" %}\n-  ins_encode(enc_loadL_volatile(mem,dst));\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-instruct loadLX_volatile(stackSlotL dst, memory mem, regD tmp) %{\n-  predicate(UseSSE>=2 && ((LoadLNode*)n)->require_atomic_access());\n-  match(Set dst (LoadL mem));\n-  effect(TEMP tmp);\n-  ins_cost(180);\n-  format %{ \"MOVSD  $tmp,$mem\\t# Atomic volatile long load\\n\\t\"\n-            \"MOVSD  $dst,$tmp\" %}\n-  ins_encode %{\n-    __ movdbl($tmp$$XMMRegister, $mem$$Address);\n-    __ movdbl(Address(rsp, $dst$$disp), $tmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct loadLX_reg_volatile(eRegL dst, memory mem, regD tmp) %{\n-  predicate(UseSSE>=2 && ((LoadLNode*)n)->require_atomic_access());\n-  match(Set dst (LoadL mem));\n-  effect(TEMP tmp);\n-  ins_cost(160);\n-  format %{ \"MOVSD  $tmp,$mem\\t# Atomic volatile long load\\n\\t\"\n-            \"MOVD   $dst.lo,$tmp\\n\\t\"\n-            \"PSRLQ  $tmp,32\\n\\t\"\n-            \"MOVD   $dst.hi,$tmp\" %}\n-  ins_encode %{\n-    __ movdbl($tmp$$XMMRegister, $mem$$Address);\n-    __ movdl($dst$$Register, $tmp$$XMMRegister);\n-    __ psrlq($tmp$$XMMRegister, 32);\n-    __ movdl(HIGH_FROM_LOW($dst$$Register), $tmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Load Range\n-instruct loadRange(rRegI dst, memory mem) %{\n-  match(Set dst (LoadRange mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOV    $dst,$mem\" %}\n-  opcode(0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\n-\/\/ Load Pointer\n-instruct loadP(eRegP dst, memory mem) %{\n-  match(Set dst (LoadP mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOV    $dst,$mem\" %}\n-  opcode(0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\/\/ Load Klass Pointer\n-instruct loadKlass(eRegP dst, memory mem) %{\n-  match(Set dst (LoadKlass mem));\n-\n-  ins_cost(125);\n-  format %{ \"MOV    $dst,$mem\" %}\n-  opcode(0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\/\/ Load Double\n-instruct loadDPR(regDPR dst, memory mem) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (LoadD mem));\n-\n-  ins_cost(150);\n-  format %{ \"FLD_D  ST,$mem\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xDD);               \/* DD \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),\n-              Pop_Reg_DPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ Load Double to XMM\n-instruct loadD(regD dst, memory mem) %{\n-  predicate(UseSSE>=2 && UseXmmLoadAndClearUpper);\n-  match(Set dst (LoadD mem));\n-  ins_cost(145);\n-  format %{ \"MOVSD  $dst,$mem\" %}\n-  ins_encode %{\n-    __ movdbl ($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct loadD_partial(regD dst, memory mem) %{\n-  predicate(UseSSE>=2 && !UseXmmLoadAndClearUpper);\n-  match(Set dst (LoadD mem));\n-  ins_cost(145);\n-  format %{ \"MOVLPD $dst,$mem\" %}\n-  ins_encode %{\n-    __ movdbl ($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Load to XMM register (single-precision floating point)\n-\/\/ MOVSS instruction\n-instruct loadF(regF dst, memory mem) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (LoadF mem));\n-  ins_cost(145);\n-  format %{ \"MOVSS  $dst,$mem\" %}\n-  ins_encode %{\n-    __ movflt ($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Load Float\n-instruct loadFPR(regFPR dst, memory mem) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (LoadF mem));\n-\n-  ins_cost(150);\n-  format %{ \"FLD_S  ST,$mem\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xD9);               \/* D9 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),\n-              Pop_Reg_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ Load Effective Address\n-instruct leaP8(eRegP dst, indOffset8 mem) %{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"LEA    $dst,$mem\" %}\n-  opcode(0x8D);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_reg_fat );\n-%}\n-\n-instruct leaP32(eRegP dst, indOffset32 mem) %{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"LEA    $dst,$mem\" %}\n-  opcode(0x8D);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_reg_fat );\n-%}\n-\n-instruct leaPIdxOff(eRegP dst, indIndexOffset mem) %{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"LEA    $dst,$mem\" %}\n-  opcode(0x8D);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_reg_fat );\n-%}\n-\n-instruct leaPIdxScale(eRegP dst, indIndexScale mem) %{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"LEA    $dst,$mem\" %}\n-  opcode(0x8D);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_reg_fat );\n-%}\n-\n-instruct leaPIdxScaleOff(eRegP dst, indIndexScaleOffset mem) %{\n-  match(Set dst mem);\n-\n-  ins_cost(110);\n-  format %{ \"LEA    $dst,$mem\" %}\n-  opcode(0x8D);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,mem), ClearInstMark);\n-  ins_pipe( ialu_reg_reg_fat );\n-%}\n-\n-\/\/ Load Constant\n-instruct loadConI(rRegI dst, immI src) %{\n-  match(Set dst src);\n-\n-  format %{ \"MOV    $dst,$src\" %}\n-  ins_encode( SetInstMark, LdImmI(dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_fat );\n-%}\n-\n-\/\/ Load Constant zero\n-instruct loadConI0(rRegI dst, immI_0 src, eFlagsReg cr) %{\n-  match(Set dst src);\n-  effect(KILL cr);\n-\n-  ins_cost(50);\n-  format %{ \"XOR    $dst,$dst\" %}\n-  opcode(0x33);  \/* + rd *\/\n-  ins_encode( OpcP, RegReg( dst, dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct loadConP(eRegP dst, immP src) %{\n-  match(Set dst src);\n-\n-  format %{ \"MOV    $dst,$src\" %}\n-  opcode(0xB8);  \/* + rd *\/\n-  ins_encode( SetInstMark, LdImmP(dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_fat );\n-%}\n-\n-instruct loadConL(eRegL dst, immL src, eFlagsReg cr) %{\n-  match(Set dst src);\n-  effect(KILL cr);\n-  ins_cost(200);\n-  format %{ \"MOV    $dst.lo,$src.lo\\n\\t\"\n-            \"MOV    $dst.hi,$src.hi\" %}\n-  opcode(0xB8);\n-  ins_encode( LdImmL_Lo(dst, src), LdImmL_Hi(dst, src) );\n-  ins_pipe( ialu_reg_long_fat );\n-%}\n-\n-instruct loadConL0(eRegL dst, immL0 src, eFlagsReg cr) %{\n-  match(Set dst src);\n-  effect(KILL cr);\n-  ins_cost(150);\n-  format %{ \"XOR    $dst.lo,$dst.lo\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-  opcode(0x33,0x33);\n-  ins_encode( RegReg_Lo(dst,dst), RegReg_Hi(dst, dst) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immFPR().\n-instruct loadConFPR(regFPR dst, immFPR con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"FLD_S  ST,[$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  ins_encode %{\n-    __ fld_s($constantaddress($con));\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_con);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immFPR0().\n-instruct loadConFPR0(regFPR dst, immFPR0 con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"FLDZ   ST\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  ins_encode %{\n-    __ fldz();\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_con);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immFPR1().\n-instruct loadConFPR1(regFPR dst, immFPR1 con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"FLD1   ST\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  ins_encode %{\n-    __ fld1();\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_con);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immF().\n-instruct loadConF(regF dst, immF con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"MOVSS  $dst,[$constantaddress]\\t# load from constant table: float=$con\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immF0().\n-instruct loadConF0(regF dst, immF0 src) %{\n-  match(Set dst src);\n-  ins_cost(100);\n-  format %{ \"XORPS  $dst,$dst\\t# float 0.0\" %}\n-  ins_encode %{\n-    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immDPR().\n-instruct loadConDPR(regDPR dst, immDPR con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-\n-  format %{ \"FLD_D  ST,[$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  ins_encode %{\n-    __ fld_d($constantaddress($con));\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_con);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immDPR0().\n-instruct loadConDPR0(regDPR dst, immDPR0 con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-\n-  format %{ \"FLDZ   ST\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  ins_encode %{\n-    __ fldz();\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_con);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immDPR1().\n-instruct loadConDPR1(regDPR dst, immDPR1 con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-\n-  format %{ \"FLD1   ST\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  ins_encode %{\n-    __ fld1();\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_con);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immD().\n-instruct loadConD(regD dst, immD con) %{\n-  match(Set dst con);\n-  ins_cost(125);\n-  format %{ \"MOVSD  $dst,[$constantaddress]\\t# load from constant table: double=$con\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, $constantaddress($con));\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-\/\/ The instruction usage is guarded by predicate in operand immD0().\n-instruct loadConD0(regD dst, immD0 src) %{\n-  match(Set dst src);\n-  ins_cost(100);\n-  format %{ \"XORPD  $dst,$dst\\t# double 0.0\" %}\n-  ins_encode %{\n-    __ xorpd ($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Load Stack Slot\n-instruct loadSSI(rRegI dst, stackSlotI src) %{\n-  match(Set dst src);\n-  ins_cost(125);\n-\n-  format %{ \"MOV    $dst,$src\" %}\n-  opcode(0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,src), ClearInstMark);\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-instruct loadSSL(eRegL dst, stackSlotL src) %{\n-  match(Set dst src);\n-\n-  ins_cost(200);\n-  format %{ \"MOV    $dst,$src.lo\\n\\t\"\n-            \"MOV    $dst+4,$src.hi\" %}\n-  opcode(0x8B, 0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, src ), OpcS, RegMem_Hi( dst, src ), ClearInstMark );\n-  ins_pipe( ialu_mem_long_reg );\n-%}\n-\n-\/\/ Load Stack Slot\n-instruct loadSSP(eRegP dst, stackSlotP src) %{\n-  match(Set dst src);\n-  ins_cost(125);\n-\n-  format %{ \"MOV    $dst,$src\" %}\n-  opcode(0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,src), ClearInstMark);\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\/\/ Load Stack Slot\n-instruct loadSSF(regFPR dst, stackSlotF src) %{\n-  match(Set dst src);\n-  ins_cost(125);\n-\n-  format %{ \"FLD_S  $src\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xD9);               \/* D9 \/0, FLD m32real *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem_no_oop(0x00,src),\n-              Pop_Reg_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ Load Stack Slot\n-instruct loadSSD(regDPR dst, stackSlotD src) %{\n-  match(Set dst src);\n-  ins_cost(125);\n-\n-  format %{ \"FLD_D  $src\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xDD);               \/* DD \/0, FLD m64real *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem_no_oop(0x00,src),\n-              Pop_Reg_DPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ Prefetch instructions for allocation.\n-\/\/ Must be safe to execute with invalid address (cannot fault).\n-\n-instruct prefetchAlloc0( memory mem ) %{\n-  predicate(UseSSE==0 && AllocatePrefetchInstr!=3);\n-  match(PrefetchAllocation mem);\n-  ins_cost(0);\n-  size(0);\n-  format %{ \"Prefetch allocation (non-SSE is empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct prefetchAlloc( memory mem ) %{\n-  predicate(AllocatePrefetchInstr==3);\n-  match( PrefetchAllocation mem );\n-  ins_cost(100);\n-\n-  format %{ \"PREFETCHW $mem\\t! Prefetch allocation into L1 cache and mark modified\" %}\n-  ins_encode %{\n-    __ prefetchw($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-instruct prefetchAllocNTA( memory mem ) %{\n-  predicate(UseSSE>=1 && AllocatePrefetchInstr==0);\n-  match(PrefetchAllocation mem);\n-  ins_cost(100);\n-\n-  format %{ \"PREFETCHNTA $mem\\t! Prefetch allocation into non-temporal cache for write\" %}\n-  ins_encode %{\n-    __ prefetchnta($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-instruct prefetchAllocT0( memory mem ) %{\n-  predicate(UseSSE>=1 && AllocatePrefetchInstr==1);\n-  match(PrefetchAllocation mem);\n-  ins_cost(100);\n-\n-  format %{ \"PREFETCHT0 $mem\\t! Prefetch allocation into L1 and L2 caches for write\" %}\n-  ins_encode %{\n-    __ prefetcht0($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-instruct prefetchAllocT2( memory mem ) %{\n-  predicate(UseSSE>=1 && AllocatePrefetchInstr==2);\n-  match(PrefetchAllocation mem);\n-  ins_cost(100);\n-\n-  format %{ \"PREFETCHT2 $mem\\t! Prefetch allocation into L2 cache for write\" %}\n-  ins_encode %{\n-    __ prefetcht2($mem$$Address);\n-  %}\n-  ins_pipe(ialu_mem);\n-%}\n-\n-\/\/----------Store Instructions-------------------------------------------------\n-\n-\/\/ Store Byte\n-instruct storeB(memory mem, xRegI src) %{\n-  match(Set mem (StoreB mem src));\n-\n-  ins_cost(125);\n-  format %{ \"MOV8   $mem,$src\" %}\n-  opcode(0x88);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, mem ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Store Char\/Short\n-instruct storeC(memory mem, rRegI src) %{\n-  match(Set mem (StoreC mem src));\n-\n-  ins_cost(125);\n-  format %{ \"MOV16  $mem,$src\" %}\n-  opcode(0x89, 0x66);\n-  ins_encode( SetInstMark, OpcS, OpcP, RegMem( src, mem ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Store Integer\n-instruct storeI(memory mem, rRegI src) %{\n-  match(Set mem (StoreI mem src));\n-\n-  ins_cost(125);\n-  format %{ \"MOV    $mem,$src\" %}\n-  opcode(0x89);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, mem ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Store Long\n-instruct storeL(long_memory mem, eRegL src) %{\n-  predicate(!((StoreLNode*)n)->require_atomic_access());\n-  match(Set mem (StoreL mem src));\n-\n-  ins_cost(200);\n-  format %{ \"MOV    $mem,$src.lo\\n\\t\"\n-            \"MOV    $mem+4,$src.hi\" %}\n-  opcode(0x89, 0x89);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, mem ), OpcS, RegMem_Hi( src, mem ), ClearInstMark );\n-  ins_pipe( ialu_mem_long_reg );\n-%}\n-\n-\/\/ Store Long to Integer\n-instruct storeL2I(memory mem, eRegL src) %{\n-  match(Set mem (StoreI mem (ConvL2I src)));\n-\n-  format %{ \"MOV    $mem,$src.lo\\t# long -> int\" %}\n-  ins_encode %{\n-    __ movl($mem$$Address, $src$$Register);\n-  %}\n-  ins_pipe(ialu_mem_reg);\n-%}\n-\n-\/\/ Volatile Store Long.  Must be atomic, so move it into\n-\/\/ the FP TOS and then do a 64-bit FIST.  Has to probe the\n-\/\/ target address before the store (for null-ptr checks)\n-\/\/ so the memory operand is used twice in the encoding.\n-instruct storeL_volatile(memory mem, stackSlotL src, eFlagsReg cr ) %{\n-  predicate(UseSSE<=1 && ((StoreLNode*)n)->require_atomic_access());\n-  match(Set mem (StoreL mem src));\n-  effect( KILL cr );\n-  ins_cost(400);\n-  format %{ \"CMP    $mem,EAX\\t# Probe address for implicit null check\\n\\t\"\n-            \"FILD   $src\\n\\t\"\n-            \"FISTp  $mem\\t # 64-bit atomic volatile long store\" %}\n-  opcode(0x3B);\n-  ins_encode( SetInstMark, OpcP, RegMem( EAX, mem ), enc_storeL_volatile(mem,src), ClearInstMark);\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-instruct storeLX_volatile(memory mem, stackSlotL src, regD tmp, eFlagsReg cr) %{\n-  predicate(UseSSE>=2 && ((StoreLNode*)n)->require_atomic_access());\n-  match(Set mem (StoreL mem src));\n-  effect( TEMP tmp, KILL cr );\n-  ins_cost(380);\n-  format %{ \"CMP    $mem,EAX\\t# Probe address for implicit null check\\n\\t\"\n-            \"MOVSD  $tmp,$src\\n\\t\"\n-            \"MOVSD  $mem,$tmp\\t # 64-bit atomic volatile long store\" %}\n-  ins_encode %{\n-    __ cmpl(rax, $mem$$Address);\n-    __ movdbl($tmp$$XMMRegister, Address(rsp, $src$$disp));\n-    __ movdbl($mem$$Address, $tmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct storeLX_reg_volatile(memory mem, eRegL src, regD tmp2, regD tmp, eFlagsReg cr) %{\n-  predicate(UseSSE>=2 && ((StoreLNode*)n)->require_atomic_access());\n-  match(Set mem (StoreL mem src));\n-  effect( TEMP tmp2 , TEMP tmp, KILL cr );\n-  ins_cost(360);\n-  format %{ \"CMP    $mem,EAX\\t# Probe address for implicit null check\\n\\t\"\n-            \"MOVD   $tmp,$src.lo\\n\\t\"\n-            \"MOVD   $tmp2,$src.hi\\n\\t\"\n-            \"PUNPCKLDQ $tmp,$tmp2\\n\\t\"\n-            \"MOVSD  $mem,$tmp\\t # 64-bit atomic volatile long store\" %}\n-  ins_encode %{\n-    __ cmpl(rax, $mem$$Address);\n-    __ movdl($tmp$$XMMRegister, $src$$Register);\n-    __ movdl($tmp2$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-    __ punpckldq($tmp$$XMMRegister, $tmp2$$XMMRegister);\n-    __ movdbl($mem$$Address, $tmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Store Pointer; for storing unknown oops and raw pointers\n-instruct storeP(memory mem, anyRegP src) %{\n-  match(Set mem (StoreP mem src));\n-\n-  ins_cost(125);\n-  format %{ \"MOV    $mem,$src\" %}\n-  opcode(0x89);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, mem ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Store Integer Immediate\n-instruct storeImmI(memory mem, immI src) %{\n-  match(Set mem (StoreI mem src));\n-\n-  ins_cost(150);\n-  format %{ \"MOV    $mem,$src\" %}\n-  opcode(0xC7);               \/* C7 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem), Con32(src), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Store Short\/Char Immediate\n-instruct storeImmI16(memory mem, immI16 src) %{\n-  predicate(UseStoreImmI16);\n-  match(Set mem (StoreC mem src));\n-\n-  ins_cost(150);\n-  format %{ \"MOV16  $mem,$src\" %}\n-  opcode(0xC7);     \/* C7 \/0 Same as 32 store immediate with prefix *\/\n-  ins_encode( SetInstMark, SizePrefix, OpcP, RMopc_Mem(0x00,mem), Con16(src), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Store Pointer Immediate; null pointers or constant oops that do not\n-\/\/ need card-mark barriers.\n-instruct storeImmP(memory mem, immP src) %{\n-  match(Set mem (StoreP mem src));\n-\n-  ins_cost(150);\n-  format %{ \"MOV    $mem,$src\" %}\n-  opcode(0xC7);               \/* C7 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem), Con32( src ), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Store Byte Immediate\n-instruct storeImmB(memory mem, immI8 src) %{\n-  match(Set mem (StoreB mem src));\n-\n-  ins_cost(150);\n-  format %{ \"MOV8   $mem,$src\" %}\n-  opcode(0xC6);               \/* C6 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem), Con8or32(src), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Store Double\n-instruct storeDPR( memory mem, regDPR1 src) %{\n-  predicate(UseSSE<=1);\n-  match(Set mem (StoreD mem src));\n-\n-  ins_cost(100);\n-  format %{ \"FST_D  $mem,$src\" %}\n-  opcode(0xDD);       \/* DD \/2 *\/\n-  ins_encode( enc_FPR_store(mem,src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-\/\/ Store double does rounding on x86\n-instruct storeDPR_rounded( memory mem, regDPR1 src) %{\n-  predicate(UseSSE<=1);\n-  match(Set mem (StoreD mem (RoundDouble src)));\n-\n-  ins_cost(100);\n-  format %{ \"FST_D  $mem,$src\\t# round\" %}\n-  opcode(0xDD);       \/* DD \/2 *\/\n-  ins_encode( enc_FPR_store(mem,src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-\/\/ Store XMM register to memory (double-precision floating points)\n-\/\/ MOVSD instruction\n-instruct storeD(memory mem, regD src) %{\n-  predicate(UseSSE>=2);\n-  match(Set mem (StoreD mem src));\n-  ins_cost(95);\n-  format %{ \"MOVSD  $mem,$src\" %}\n-  ins_encode %{\n-    __ movdbl($mem$$Address, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Store XMM register to memory (single-precision floating point)\n-\/\/ MOVSS instruction\n-instruct storeF(memory mem, regF src) %{\n-  predicate(UseSSE>=1);\n-  match(Set mem (StoreF mem src));\n-  ins_cost(95);\n-  format %{ \"MOVSS  $mem,$src\" %}\n-  ins_encode %{\n-    __ movflt($mem$$Address, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ Store Float\n-instruct storeFPR( memory mem, regFPR1 src) %{\n-  predicate(UseSSE==0);\n-  match(Set mem (StoreF mem src));\n-\n-  ins_cost(100);\n-  format %{ \"FST_S  $mem,$src\" %}\n-  opcode(0xD9);       \/* D9 \/2 *\/\n-  ins_encode( enc_FPR_store(mem,src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-\/\/ Store Float does rounding on x86\n-instruct storeFPR_rounded( memory mem, regFPR1 src) %{\n-  predicate(UseSSE==0);\n-  match(Set mem (StoreF mem (RoundFloat src)));\n-\n-  ins_cost(100);\n-  format %{ \"FST_S  $mem,$src\\t# round\" %}\n-  opcode(0xD9);       \/* D9 \/2 *\/\n-  ins_encode( enc_FPR_store(mem,src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-\/\/ Store Float does rounding on x86\n-instruct storeFPR_Drounded( memory mem, regDPR1 src) %{\n-  predicate(UseSSE<=1);\n-  match(Set mem (StoreF mem (ConvD2F src)));\n-\n-  ins_cost(100);\n-  format %{ \"FST_S  $mem,$src\\t# D-round\" %}\n-  opcode(0xD9);       \/* D9 \/2 *\/\n-  ins_encode( enc_FPR_store(mem,src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-\/\/ Store immediate Float value (it is faster than store from FPU register)\n-\/\/ The instruction usage is guarded by predicate in operand immFPR().\n-instruct storeFPR_imm( memory mem, immFPR src) %{\n-  match(Set mem (StoreF mem src));\n-\n-  ins_cost(50);\n-  format %{ \"MOV    $mem,$src\\t# store float\" %}\n-  opcode(0xC7);               \/* C7 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),  Con32FPR_as_bits(src), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Store immediate Float value (it is faster than store from XMM register)\n-\/\/ The instruction usage is guarded by predicate in operand immF().\n-instruct storeF_imm( memory mem, immF src) %{\n-  match(Set mem (StoreF mem src));\n-\n-  ins_cost(50);\n-  format %{ \"MOV    $mem,$src\\t# store float\" %}\n-  opcode(0xC7);               \/* C7 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),  Con32F_as_bits(src), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Store Integer to stack slot\n-instruct storeSSI(stackSlotI dst, rRegI src) %{\n-  match(Set dst src);\n-\n-  ins_cost(100);\n-  format %{ \"MOV    $dst,$src\" %}\n-  opcode(0x89);\n-  ins_encode( OpcPRegSS( dst, src ) );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Store Integer to stack slot\n-instruct storeSSP(stackSlotP dst, eRegP src) %{\n-  match(Set dst src);\n-\n-  ins_cost(100);\n-  format %{ \"MOV    $dst,$src\" %}\n-  opcode(0x89);\n-  ins_encode( OpcPRegSS( dst, src ) );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Store Long to stack slot\n-instruct storeSSL(stackSlotL dst, eRegL src) %{\n-  match(Set dst src);\n-\n-  ins_cost(200);\n-  format %{ \"MOV    $dst,$src.lo\\n\\t\"\n-            \"MOV    $dst+4,$src.hi\" %}\n-  opcode(0x89, 0x89);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), OpcS, RegMem_Hi( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_long_reg );\n-%}\n-\n-\/\/----------MemBar Instructions-----------------------------------------------\n-\/\/ Memory barrier flavors\n-\n-instruct membar_acquire() %{\n-  match(MemBarAcquire);\n-  match(LoadFence);\n-  ins_cost(400);\n-\n-  size(0);\n-  format %{ \"MEMBAR-acquire ! (empty encoding)\" %}\n-  ins_encode();\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_acquire_lock() %{\n-  match(MemBarAcquireLock);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-acquire (prior CMPXCHG in FastLock so empty encoding)\" %}\n-  ins_encode( );\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_release() %{\n-  match(MemBarRelease);\n-  match(StoreFence);\n-  ins_cost(400);\n-\n-  size(0);\n-  format %{ \"MEMBAR-release ! (empty encoding)\" %}\n-  ins_encode( );\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_release_lock() %{\n-  match(MemBarReleaseLock);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-release (a FastUnlock follows so empty encoding)\" %}\n-  ins_encode( );\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_volatile(eFlagsReg cr) %{\n-  match(MemBarVolatile);\n-  effect(KILL cr);\n-  ins_cost(400);\n-\n-  format %{\n-    $$template\n-    $$emit$$\"LOCK ADDL [ESP + #0], 0\\t! membar_volatile\"\n-  %}\n-  ins_encode %{\n-    __ membar(Assembler::StoreLoad);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct unnecessary_membar_volatile() %{\n-  match(MemBarVolatile);\n-  predicate(Matcher::post_store_load_barrier(n));\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-volatile (unnecessary so empty encoding)\" %}\n-  ins_encode( );\n-  ins_pipe(empty);\n-%}\n-\n-instruct membar_storestore() %{\n-  match(MemBarStoreStore);\n-  match(StoreStoreFence);\n-  ins_cost(0);\n-\n-  size(0);\n-  format %{ \"MEMBAR-storestore (empty encoding)\" %}\n-  ins_encode( );\n-  ins_pipe(empty);\n-%}\n-\n-\/\/----------Move Instructions--------------------------------------------------\n-instruct castX2P(eAXRegP dst, eAXRegI src) %{\n-  match(Set dst (CastX2P src));\n-  format %{ \"# X2P  $dst, $src\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe(empty);\n-%}\n-\n-instruct castP2X(rRegI dst, eRegP src ) %{\n-  match(Set dst (CastP2X src));\n-  ins_cost(50);\n-  format %{ \"MOV    $dst, $src\\t# CastP2X\" %}\n-  ins_encode( enc_Copy( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\/\/----------Conditional Move---------------------------------------------------\n-\/\/ Conditional move\n-instruct jmovI_reg(cmpOp cop, eFlagsReg cr, rRegI dst, rRegI src) %{\n-  predicate(!VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"J$cop,us skip\\t# signed cmove\\n\\t\"\n-            \"MOV    $dst,$src\\n\"\n-      \"skip:\" %}\n-  ins_encode %{\n-    Label Lskip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n-    __ movl($dst$$Register, $src$$Register);\n-    __ bind(Lskip);\n-  %}\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct jmovI_regU(cmpOpU cop, eFlagsRegU cr, rRegI dst, rRegI src) %{\n-  predicate(!VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"J$cop,us skip\\t# unsigned cmove\\n\\t\"\n-            \"MOV    $dst,$src\\n\"\n-      \"skip:\" %}\n-  ins_encode %{\n-    Label Lskip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n-    __ movl($dst$$Register, $src$$Register);\n-    __ bind(Lskip);\n-  %}\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovI_reg(rRegI dst, rRegI src, eFlagsReg cr, cmpOp cop ) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cop $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cop), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovI_regU( cmpOpU cop, eFlagsRegU cr, rRegI dst, rRegI src ) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cop $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cop), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovI_regUCF( cmpOpUCF cop, eFlagsRegUCF cr, rRegI dst, rRegI src ) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovI_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovI_mem(cmpOp cop, eFlagsReg cr, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  format %{ \"CMOV$cop $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cop), RegMem( dst, src ), ClearInstMark );\n-  ins_pipe( pipe_cmov_mem );\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovI_memU(cmpOpU cop, eFlagsRegU cr, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  format %{ \"CMOV$cop $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cop), RegMem( dst, src ), ClearInstMark );\n-  ins_pipe( pipe_cmov_mem );\n-%}\n-\n-instruct cmovI_memUCF(cmpOpUCF cop, eFlagsRegUCF cr, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  expand %{\n-    cmovI_memU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovP_reg(eRegP dst, eRegP src, eFlagsReg cr, cmpOp cop ) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cop $dst,$src\\t# ptr\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cop), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-\/\/ Conditional move (non-P6 version)\n-\/\/ Note:  a CMoveP is generated for  stubs and native wrappers\n-\/\/        regardless of whether we are on a P6, so we\n-\/\/        emulate a cmov here\n-instruct cmovP_reg_nonP6(eRegP dst, eRegP src, eFlagsReg cr, cmpOp cop ) %{\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-  ins_cost(300);\n-  format %{ \"Jn$cop   skip\\n\\t\"\n-          \"MOV    $dst,$src\\t# pointer\\n\"\n-      \"skip:\" %}\n-  opcode(0x8b);\n-  ins_encode( enc_cmov_branch(cop, 0x2), OpcP, RegReg(dst, src));\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-\/\/ Conditional move\n-instruct cmovP_regU(cmpOpU cop, eFlagsRegU cr, eRegP dst, eRegP src ) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cop $dst,$src\\t# ptr\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cop), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovP_regUCF(cmpOpUCF cop, eFlagsRegUCF cr, eRegP dst, eRegP src ) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovP_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/ DISABLED: Requires the ADLC to emit a bottom_type call that\n-\/\/ correctly meets the two pointer arguments; one is an incoming\n-\/\/ register but the other is a memory operand.  ALSO appears to\n-\/\/ be buggy with implicit null checks.\n-\/\/\n-\/\/\/\/ Conditional move\n-\/\/instruct cmovP_mem(cmpOp cop, eFlagsReg cr, eRegP dst, memory src) %{\n-\/\/  predicate(VM_Version::supports_cmov() );\n-\/\/  match(Set dst (CMoveP (Binary cop cr) (Binary dst (LoadP src))));\n-\/\/  ins_cost(250);\n-\/\/  format %{ \"CMOV$cop $dst,$src\\t# ptr\" %}\n-\/\/  opcode(0x0F,0x40);\n-\/\/  ins_encode( enc_cmov(cop), RegMem( dst, src ) );\n-\/\/  ins_pipe( pipe_cmov_mem );\n-\/\/%}\n-\/\/\n-\/\/\/\/ Conditional move\n-\/\/instruct cmovP_memU(cmpOpU cop, eFlagsRegU cr, eRegP dst, memory src) %{\n-\/\/  predicate(VM_Version::supports_cmov() );\n-\/\/  match(Set dst (CMoveP (Binary cop cr) (Binary dst (LoadP src))));\n-\/\/  ins_cost(250);\n-\/\/  format %{ \"CMOV$cop $dst,$src\\t# ptr\" %}\n-\/\/  opcode(0x0F,0x40);\n-\/\/  ins_encode( enc_cmov(cop), RegMem( dst, src ) );\n-\/\/  ins_pipe( pipe_cmov_mem );\n-\/\/%}\n-\n-\/\/ Conditional move\n-instruct fcmovDPR_regU(cmpOp_fcmov cop, eFlagsRegU cr, regDPR1 dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"FCMOV$cop $dst,$src\\t# double\" %}\n-  opcode(0xDA);\n-  ins_encode( enc_cmov_dpr(cop,src) );\n-  ins_pipe( pipe_cmovDPR_reg );\n-%}\n-\n-\/\/ Conditional move\n-instruct fcmovFPR_regU(cmpOp_fcmov cop, eFlagsRegU cr, regFPR1 dst, regFPR src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"FCMOV$cop $dst,$src\\t# float\" %}\n-  opcode(0xDA);\n-  ins_encode( enc_cmov_dpr(cop,src) );\n-  ins_pipe( pipe_cmovDPR_reg );\n-%}\n-\n-\/\/ Float CMOV on Intel doesn't handle *signed* compares, only unsigned.\n-instruct fcmovDPR_regS(cmpOp cop, eFlagsReg cr, regDPR dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"Jn$cop   skip\\n\\t\"\n-            \"MOV    $dst,$src\\t# double\\n\"\n-      \"skip:\" %}\n-  opcode (0xdd, 0x3);     \/* DD D8+i or DD \/3 *\/\n-  ins_encode( enc_cmov_branch( cop, 0x4 ), Push_Reg_DPR(src), OpcP, RegOpc(dst) );\n-  ins_pipe( pipe_cmovDPR_reg );\n-%}\n-\n-\/\/ Float CMOV on Intel doesn't handle *signed* compares, only unsigned.\n-instruct fcmovFPR_regS(cmpOp cop, eFlagsReg cr, regFPR dst, regFPR src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"Jn$cop    skip\\n\\t\"\n-            \"MOV    $dst,$src\\t# float\\n\"\n-      \"skip:\" %}\n-  opcode (0xdd, 0x3);     \/* DD D8+i or DD \/3 *\/\n-  ins_encode( enc_cmov_branch( cop, 0x4 ), Push_Reg_FPR(src), OpcP, RegOpc(dst) );\n-  ins_pipe( pipe_cmovDPR_reg );\n-%}\n-\n-\/\/ No CMOVE with SSE\/SSE2\n-instruct fcmovF_regS(cmpOp cop, eFlagsReg cr, regF dst, regF src) %{\n-  predicate (UseSSE>=1);\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"Jn$cop   skip\\n\\t\"\n-            \"MOVSS  $dst,$src\\t# float\\n\"\n-      \"skip:\" %}\n-  ins_encode %{\n-    Label skip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), skip);\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(skip);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ No CMOVE with SSE\/SSE2\n-instruct fcmovD_regS(cmpOp cop, eFlagsReg cr, regD dst, regD src) %{\n-  predicate (UseSSE>=2);\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"Jn$cop   skip\\n\\t\"\n-            \"MOVSD  $dst,$src\\t# float\\n\"\n-      \"skip:\" %}\n-  ins_encode %{\n-    Label skip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), skip);\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(skip);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ unsigned version\n-instruct fcmovF_regU(cmpOpU cop, eFlagsRegU cr, regF dst, regF src) %{\n-  predicate (UseSSE>=1);\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"Jn$cop   skip\\n\\t\"\n-            \"MOVSS  $dst,$src\\t# float\\n\"\n-      \"skip:\" %}\n-  ins_encode %{\n-    Label skip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), skip);\n-    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(skip);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct fcmovF_regUCF(cmpOpUCF cop, eFlagsRegUCF cr, regF dst, regF src) %{\n-  predicate (UseSSE>=1);\n-  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovF_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/ unsigned version\n-instruct fcmovD_regU(cmpOpU cop, eFlagsRegU cr, regD dst, regD src) %{\n-  predicate (UseSSE>=2);\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"Jn$cop   skip\\n\\t\"\n-            \"MOVSD  $dst,$src\\t# float\\n\"\n-      \"skip:\" %}\n-  ins_encode %{\n-    Label skip;\n-    \/\/ Invert sense of branch from sense of CMOV\n-    __ jccb((Assembler::Condition)($cop$$cmpcode^1), skip);\n-    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n-    __ bind(skip);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct fcmovD_regUCF(cmpOpUCF cop, eFlagsRegUCF cr, regD dst, regD src) %{\n-  predicate (UseSSE>=2);\n-  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovD_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-instruct cmovL_reg(cmpOp cop, eFlagsReg cr, eRegL dst, eRegL src) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cop $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cop $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cop), RegReg_Lo2( dst, src ), enc_cmov(cop), RegReg_Hi2( dst, src ) );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovL_regU(cmpOpU cop, eFlagsRegU cr, eRegL dst, eRegL src) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cop $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cop $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cop), RegReg_Lo2( dst, src ), enc_cmov(cop), RegReg_Hi2( dst, src ) );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovL_regUCF(cmpOpUCF cop, eFlagsRegUCF cr, eRegL dst, eRegL src) %{\n-  predicate(VM_Version::supports_cmov() );\n-  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovL_regU(cop, cr, dst, src);\n-  %}\n-%}\n-\n-\/\/----------Arithmetic Instructions--------------------------------------------\n-\/\/----------Addition Instructions----------------------------------------------\n-\n-\/\/ Integer Addition Instructions\n-instruct addI_eReg(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x03);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct addI_eReg_imm(rRegI dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x81, 0x00); \/* \/0 id *\/\n-  ins_encode( OpcSErm( dst, src ), Con8or32( src ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct incI_eReg(rRegI dst, immI_1 src, eFlagsReg cr) %{\n-  predicate(UseIncDec);\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-\n-  size(1);\n-  format %{ \"INC    $dst\" %}\n-  opcode(0x40); \/*  *\/\n-  ins_encode( Opc_plus( primary, dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct leaI_eReg_immI(rRegI dst, rRegI src0, immI src1) %{\n-  match(Set dst (AddI src0 src1));\n-  ins_cost(110);\n-\n-  format %{ \"LEA    $dst,[$src0 + $src1]\" %}\n-  opcode(0x8D); \/* 0x8D \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegLea( dst, src0, src1 ), ClearInstMark );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct leaP_eReg_immI(eRegP dst, eRegP src0, immI src1) %{\n-  match(Set dst (AddP src0 src1));\n-  ins_cost(110);\n-\n-  format %{ \"LEA    $dst,[$src0 + $src1]\\t# ptr\" %}\n-  opcode(0x8D); \/* 0x8D \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegLea( dst, src0, src1 ), ClearInstMark );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct decI_eReg(rRegI dst, immI_M1 src, eFlagsReg cr) %{\n-  predicate(UseIncDec);\n-  match(Set dst (AddI dst src));\n-  effect(KILL cr);\n-\n-  size(1);\n-  format %{ \"DEC    $dst\" %}\n-  opcode(0x48); \/*  *\/\n-  ins_encode( Opc_plus( primary, dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addP_eReg(eRegP dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (AddP dst src));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x03);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct addP_eReg_imm(eRegP dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (AddP dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x81,0x00); \/* Opcode 81 \/0 id *\/\n-  \/\/ ins_encode( RegImm( dst, src) );\n-  ins_encode( OpcSErm( dst, src ), Con8or32( src ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct addI_eReg_mem(rRegI dst, memory src, eFlagsReg cr) %{\n-  match(Set dst (AddI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x03);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-instruct addI_mem_eReg(memory dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x01);  \/* Opcode 01 \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Add Memory with Immediate\n-instruct addI_mem_imm(memory dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"ADD    $dst,$src\" %}\n-  opcode(0x81);               \/* Opcode 81 \/0 id *\/\n-  ins_encode( SetInstMark, OpcSE( src ), RMopc_Mem(0x00,dst), Con8or32(src), ClearInstMark );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-instruct incI_mem(memory dst, immI_1 src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"INC    $dst\" %}\n-  opcode(0xFF);               \/* Opcode FF \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,dst), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-instruct decI_mem(memory dst, immI_M1 src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"DEC    $dst\" %}\n-  opcode(0xFF);               \/* Opcode FF \/1 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x01,dst), ClearInstMark);\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\n-instruct checkCastPP( eRegP dst ) %{\n-  match(Set dst (CheckCastPP dst));\n-\n-  size(0);\n-  format %{ \"#checkcastPP of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_pipe( empty );\n-%}\n-\n-instruct castPP( eRegP dst ) %{\n-  match(Set dst (CastPP dst));\n-  format %{ \"#castPP of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_pipe( empty );\n-%}\n-\n-instruct castII( rRegI dst ) %{\n-  match(Set dst (CastII dst));\n-  format %{ \"#castII of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe( empty );\n-%}\n-\n-instruct castLL( eRegL dst ) %{\n-  match(Set dst (CastLL dst));\n-  format %{ \"#castLL of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe( empty );\n-%}\n-\n-instruct castFF( regF dst ) %{\n-  predicate(UseSSE >= 1);\n-  match(Set dst (CastFF dst));\n-  format %{ \"#castFF of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe( empty );\n-%}\n-\n-instruct castDD( regD dst ) %{\n-  predicate(UseSSE >= 2);\n-  match(Set dst (CastDD dst));\n-  format %{ \"#castDD of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe( empty );\n-%}\n-\n-instruct castFF_PR( regFPR dst ) %{\n-  predicate(UseSSE < 1);\n-  match(Set dst (CastFF dst));\n-  format %{ \"#castFF of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe( empty );\n-%}\n-\n-instruct castDD_PR( regDPR dst ) %{\n-  predicate(UseSSE < 2);\n-  match(Set dst (CastDD dst));\n-  format %{ \"#castDD of $dst\" %}\n-  ins_encode( \/*empty encoding*\/ );\n-  ins_cost(0);\n-  ins_pipe( empty );\n-%}\n-\n-\/\/ No flag versions for CompareAndSwap{P,I,L} because matcher can't match them\n-\n-instruct compareAndSwapL( rRegI res, eSIRegP mem_ptr, eADXRegL oldval, eBCXRegL newval, eFlagsReg cr ) %{\n-  match(Set res (CompareAndSwapL mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapL mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-  format %{ \"CMPXCHG8 [$mem_ptr],$newval\\t# If EDX:EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\"\n-            \"MOV    $res,0\\n\\t\"\n-            \"JNE,s  fail\\n\\t\"\n-            \"MOV    $res,1\\n\"\n-          \"fail:\" %}\n-  ins_encode( enc_cmpxchg8(mem_ptr),\n-              enc_flags_ne_to_boolean(res) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapP( rRegI res,  pRegP mem_ptr, eAXRegP oldval, eCXRegP newval, eFlagsReg cr) %{\n-  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-  format %{ \"CMPXCHG [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\"\n-            \"MOV    $res,0\\n\\t\"\n-            \"JNE,s  fail\\n\\t\"\n-            \"MOV    $res,1\\n\"\n-          \"fail:\" %}\n-  ins_encode( enc_cmpxchg(mem_ptr), enc_flags_ne_to_boolean(res) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapB( rRegI res, pRegP mem_ptr, eAXRegI oldval, eCXRegI newval, eFlagsReg cr ) %{\n-  match(Set res (CompareAndSwapB mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapB mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-  format %{ \"CMPXCHGB [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\"\n-            \"MOV    $res,0\\n\\t\"\n-            \"JNE,s  fail\\n\\t\"\n-            \"MOV    $res,1\\n\"\n-          \"fail:\" %}\n-  ins_encode( enc_cmpxchgb(mem_ptr),\n-              enc_flags_ne_to_boolean(res) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapS( rRegI res, pRegP mem_ptr, eAXRegI oldval, eCXRegI newval, eFlagsReg cr ) %{\n-  match(Set res (CompareAndSwapS mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapS mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-  format %{ \"CMPXCHGW [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\"\n-            \"MOV    $res,0\\n\\t\"\n-            \"JNE,s  fail\\n\\t\"\n-            \"MOV    $res,1\\n\"\n-          \"fail:\" %}\n-  ins_encode( enc_cmpxchgw(mem_ptr),\n-              enc_flags_ne_to_boolean(res) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndSwapI( rRegI res, pRegP mem_ptr, eAXRegI oldval, eCXRegI newval, eFlagsReg cr) %{\n-  match(Set res (CompareAndSwapI mem_ptr (Binary oldval newval)));\n-  match(Set res (WeakCompareAndSwapI mem_ptr (Binary oldval newval)));\n-  effect(KILL cr, KILL oldval);\n-  format %{ \"CMPXCHG [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\"\n-            \"MOV    $res,0\\n\\t\"\n-            \"JNE,s  fail\\n\\t\"\n-            \"MOV    $res,1\\n\"\n-          \"fail:\" %}\n-  ins_encode( enc_cmpxchg(mem_ptr), enc_flags_ne_to_boolean(res) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeL( eSIRegP mem_ptr, eADXRegL oldval, eBCXRegL newval, eFlagsReg cr ) %{\n-  match(Set oldval (CompareAndExchangeL mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-  format %{ \"CMPXCHG8 [$mem_ptr],$newval\\t# If EDX:EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\" %}\n-  ins_encode( enc_cmpxchg8(mem_ptr) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeP( pRegP mem_ptr, eAXRegP oldval, eCXRegP newval, eFlagsReg cr) %{\n-  match(Set oldval (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-  format %{ \"CMPXCHG [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\" %}\n-  ins_encode( enc_cmpxchg(mem_ptr) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeB( pRegP mem_ptr, eAXRegI oldval, eCXRegI newval, eFlagsReg cr) %{\n-  match(Set oldval (CompareAndExchangeB mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-  format %{ \"CMPXCHGB [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\" %}\n-  ins_encode( enc_cmpxchgb(mem_ptr) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeS( pRegP mem_ptr, eAXRegI oldval, eCXRegI newval, eFlagsReg cr) %{\n-  match(Set oldval (CompareAndExchangeS mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-  format %{ \"CMPXCHGW [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\" %}\n-  ins_encode( enc_cmpxchgw(mem_ptr) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct compareAndExchangeI( pRegP mem_ptr, eAXRegI oldval, eCXRegI newval, eFlagsReg cr) %{\n-  match(Set oldval (CompareAndExchangeI mem_ptr (Binary oldval newval)));\n-  effect(KILL cr);\n-  format %{ \"CMPXCHG [$mem_ptr],$newval\\t# If EAX==[$mem_ptr] Then store $newval into [$mem_ptr]\\n\\t\" %}\n-  ins_encode( enc_cmpxchg(mem_ptr) );\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xaddB_no_res( memory mem, Universe dummy, immI add, eFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddB mem add));\n-  effect(KILL cr);\n-  format %{ \"ADDB  [$mem],$add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addb($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-\/\/ Important to match to xRegI: only 8-bit regs.\n-instruct xaddB( memory mem, xRegI newval, eFlagsReg cr) %{\n-  match(Set newval (GetAndAddB mem newval));\n-  effect(KILL cr);\n-  format %{ \"XADDB  [$mem],$newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddb($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xaddS_no_res( memory mem, Universe dummy, immI add, eFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddS mem add));\n-  effect(KILL cr);\n-  format %{ \"ADDS  [$mem],$add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addw($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xaddS( memory mem, rRegI newval, eFlagsReg cr) %{\n-  match(Set newval (GetAndAddS mem newval));\n-  effect(KILL cr);\n-  format %{ \"XADDS  [$mem],$newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddw($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xaddI_no_res( memory mem, Universe dummy, immI add, eFlagsReg cr) %{\n-  predicate(n->as_LoadStore()->result_not_used());\n-  match(Set dummy (GetAndAddI mem add));\n-  effect(KILL cr);\n-  format %{ \"ADDL  [$mem],$add\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ addl($mem$$Address, $add$$constant);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xaddI( memory mem, rRegI newval, eFlagsReg cr) %{\n-  match(Set newval (GetAndAddI mem newval));\n-  effect(KILL cr);\n-  format %{ \"XADDL  [$mem],$newval\" %}\n-  ins_encode %{\n-    __ lock();\n-    __ xaddl($mem$$Address, $newval$$Register);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-\/\/ Important to match to xRegI: only 8-bit regs.\n-instruct xchgB( memory mem, xRegI newval) %{\n-  match(Set newval (GetAndSetB mem newval));\n-  format %{ \"XCHGB  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgb($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgS( memory mem, rRegI newval) %{\n-  match(Set newval (GetAndSetS mem newval));\n-  format %{ \"XCHGW  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgw($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgI( memory mem, rRegI newval) %{\n-  match(Set newval (GetAndSetI mem newval));\n-  format %{ \"XCHGL  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgl($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-instruct xchgP( memory mem, pRegP newval) %{\n-  match(Set newval (GetAndSetP mem newval));\n-  format %{ \"XCHGL  $newval,[$mem]\" %}\n-  ins_encode %{\n-    __ xchgl($newval$$Register, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_cmpxchg );\n-%}\n-\n-\/\/----------Subtraction Instructions-------------------------------------------\n-\n-\/\/ Integer Subtraction Instructions\n-instruct subI_eReg(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (SubI dst src));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SUB    $dst,$src\" %}\n-  opcode(0x2B);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct subI_eReg_imm(rRegI dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (SubI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"SUB    $dst,$src\" %}\n-  opcode(0x81,0x05);  \/* Opcode 81 \/5 *\/\n-  \/\/ ins_encode( RegImm( dst, src) );\n-  ins_encode( OpcSErm( dst, src ), Con8or32( src ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct subI_eReg_mem(rRegI dst, memory src, eFlagsReg cr) %{\n-  match(Set dst (SubI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"SUB    $dst,$src\" %}\n-  opcode(0x2B);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-instruct subI_mem_eReg(memory dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (SubI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"SUB    $dst,$src\" %}\n-  opcode(0x29);  \/* Opcode 29 \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Subtract from a pointer\n-instruct subP_eReg(eRegP dst, rRegI src, immI_0 zero, eFlagsReg cr) %{\n-  match(Set dst (AddP dst (SubI zero src)));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SUB    $dst,$src\" %}\n-  opcode(0x2B);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct negI_eReg(rRegI dst, immI_0 zero, eFlagsReg cr) %{\n-  match(Set dst (SubI zero dst));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"NEG    $dst\" %}\n-  opcode(0xF7,0x03);  \/\/ Opcode F7 \/3\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/----------Multiplication\/Division Instructions-------------------------------\n-\/\/ Integer Multiplication Instructions\n-\/\/ Multiply Register\n-instruct mulI_eReg(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (MulI dst src));\n-  effect(KILL cr);\n-\n-  size(3);\n-  ins_cost(300);\n-  format %{ \"IMUL   $dst,$src\" %}\n-  opcode(0xAF, 0x0F);\n-  ins_encode( OpcS, OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg_alu0 );\n-%}\n-\n-\/\/ Multiply 32-bit Immediate\n-instruct mulI_eReg_imm(rRegI dst, rRegI src, immI imm, eFlagsReg cr) %{\n-  match(Set dst (MulI src imm));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"IMUL   $dst,$src,$imm\" %}\n-  opcode(0x69);  \/* 69 \/r id *\/\n-  ins_encode( OpcSE(imm), RegReg( dst, src ), Con8or32( imm ) );\n-  ins_pipe( ialu_reg_reg_alu0 );\n-%}\n-\n-instruct loadConL_low_only(eADXRegL_low_only dst, immL32 src, eFlagsReg cr) %{\n-  match(Set dst src);\n-  effect(KILL cr);\n-\n-  \/\/ Note that this is artificially increased to make it more expensive than loadConL\n-  ins_cost(250);\n-  format %{ \"MOV    EAX,$src\\t\/\/ low word only\" %}\n-  opcode(0xB8);\n-  ins_encode( LdImmL_Lo(dst, src) );\n-  ins_pipe( ialu_reg_fat );\n-%}\n-\n-\/\/ Multiply by 32-bit Immediate, taking the shifted high order results\n-\/\/  (special case for shift by 32)\n-instruct mulI_imm_high(eDXRegI dst, nadxRegI src1, eADXRegL_low_only src2, immI_32 cnt, eFlagsReg cr) %{\n-  match(Set dst (ConvL2I (RShiftL (MulL (ConvI2L src1) src2) cnt)));\n-  predicate( _kids[0]->_kids[0]->_kids[1]->_leaf->Opcode() == Op_ConL &&\n-             _kids[0]->_kids[0]->_kids[1]->_leaf->as_Type()->type()->is_long()->get_con() >= min_jint &&\n-             _kids[0]->_kids[0]->_kids[1]->_leaf->as_Type()->type()->is_long()->get_con() <= max_jint );\n-  effect(USE src1, KILL cr);\n-\n-  \/\/ Note that this is adjusted by 150 to compensate for the overcosting of loadConL_low_only\n-  ins_cost(0*100 + 1*400 - 150);\n-  format %{ \"IMUL   EDX:EAX,$src1\" %}\n-  ins_encode( multiply_con_and_shift_high( dst, src1, src2, cnt, cr ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Multiply by 32-bit Immediate, taking the shifted high order results\n-instruct mulI_imm_RShift_high(eDXRegI dst, nadxRegI src1, eADXRegL_low_only src2, immI_32_63 cnt, eFlagsReg cr) %{\n-  match(Set dst (ConvL2I (RShiftL (MulL (ConvI2L src1) src2) cnt)));\n-  predicate( _kids[0]->_kids[0]->_kids[1]->_leaf->Opcode() == Op_ConL &&\n-             _kids[0]->_kids[0]->_kids[1]->_leaf->as_Type()->type()->is_long()->get_con() >= min_jint &&\n-             _kids[0]->_kids[0]->_kids[1]->_leaf->as_Type()->type()->is_long()->get_con() <= max_jint );\n-  effect(USE src1, KILL cr);\n-\n-  \/\/ Note that this is adjusted by 150 to compensate for the overcosting of loadConL_low_only\n-  ins_cost(1*100 + 1*400 - 150);\n-  format %{ \"IMUL   EDX:EAX,$src1\\n\\t\"\n-            \"SAR    EDX,$cnt-32\" %}\n-  ins_encode( multiply_con_and_shift_high( dst, src1, src2, cnt, cr ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Multiply Memory 32-bit Immediate\n-instruct mulI_mem_imm(rRegI dst, memory src, immI imm, eFlagsReg cr) %{\n-  match(Set dst (MulI (LoadI src) imm));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-  format %{ \"IMUL   $dst,$src,$imm\" %}\n-  opcode(0x69);  \/* 69 \/r id *\/\n-  ins_encode( SetInstMark, OpcSE(imm), RegMem( dst, src ), Con8or32( imm ), ClearInstMark );\n-  ins_pipe( ialu_reg_mem_alu0 );\n-%}\n-\n-\/\/ Multiply Memory\n-instruct mulI(rRegI dst, memory src, eFlagsReg cr) %{\n-  match(Set dst (MulI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(350);\n-  format %{ \"IMUL   $dst,$src\" %}\n-  opcode(0xAF, 0x0F);\n-  ins_encode( SetInstMark, OpcS, OpcP, RegMem( dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_mem_alu0 );\n-%}\n-\n-instruct mulAddS2I_rReg(rRegI dst, rRegI src1, rRegI src2, rRegI src3, eFlagsReg cr)\n-%{\n-  match(Set dst (MulAddS2I (Binary dst src1) (Binary src2 src3)));\n-  effect(KILL cr, KILL src2);\n-\n-  expand %{ mulI_eReg(dst, src1, cr);\n-           mulI_eReg(src2, src3, cr);\n-           addI_eReg(dst, src2, cr); %}\n-%}\n-\n-\/\/ Multiply Register Int to Long\n-instruct mulI2L(eADXRegL dst, eAXRegI src, nadxRegI src1, eFlagsReg flags) %{\n-  \/\/ Basic Idea: long = (long)int * (long)int\n-  match(Set dst (MulL (ConvI2L src) (ConvI2L src1)));\n-  effect(DEF dst, USE src, USE src1, KILL flags);\n-\n-  ins_cost(300);\n-  format %{ \"IMUL   $dst,$src1\" %}\n-\n-  ins_encode( long_int_multiply( dst, src1 ) );\n-  ins_pipe( ialu_reg_reg_alu0 );\n-%}\n-\n-instruct mulIS_eReg(eADXRegL dst, immL_32bits mask, eFlagsReg flags, eAXRegI src, nadxRegI src1) %{\n-  \/\/ Basic Idea:  long = (int & 0xffffffffL) * (int & 0xffffffffL)\n-  match(Set dst (MulL (AndL (ConvI2L src) mask) (AndL (ConvI2L src1) mask)));\n-  effect(KILL flags);\n-\n-  ins_cost(300);\n-  format %{ \"MUL    $dst,$src1\" %}\n-\n-  ins_encode( long_uint_multiply(dst, src1) );\n-  ins_pipe( ialu_reg_reg_alu0 );\n-%}\n-\n-\/\/ Multiply Register Long\n-instruct mulL_eReg(eADXRegL dst, eRegL src, rRegI tmp, eFlagsReg cr) %{\n-  match(Set dst (MulL dst src));\n-  effect(KILL cr, TEMP tmp);\n-  ins_cost(4*100+3*400);\n-\/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-\/\/             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) + lo(x_lo * y_hi)\n-  format %{ \"MOV    $tmp,$src.lo\\n\\t\"\n-            \"IMUL   $tmp,EDX\\n\\t\"\n-            \"MOV    EDX,$src.hi\\n\\t\"\n-            \"IMUL   EDX,EAX\\n\\t\"\n-            \"ADD    $tmp,EDX\\n\\t\"\n-            \"MUL    EDX:EAX,$src.lo\\n\\t\"\n-            \"ADD    EDX,$tmp\" %}\n-  ins_encode( long_multiply( dst, src, tmp ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Multiply Register Long where the left operand's high 32 bits are zero\n-instruct mulL_eReg_lhi0(eADXRegL dst, eRegL src, rRegI tmp, eFlagsReg cr) %{\n-  predicate(is_operand_hi32_zero(n->in(1)));\n-  match(Set dst (MulL dst src));\n-  effect(KILL cr, TEMP tmp);\n-  ins_cost(2*100+2*400);\n-\/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-\/\/             hi(result) = hi(x_lo * y_lo) + lo(x_lo * y_hi) where lo(x_hi * y_lo) = 0 because x_hi = 0\n-  format %{ \"MOV    $tmp,$src.hi\\n\\t\"\n-            \"IMUL   $tmp,EAX\\n\\t\"\n-            \"MUL    EDX:EAX,$src.lo\\n\\t\"\n-            \"ADD    EDX,$tmp\" %}\n-  ins_encode %{\n-    __ movl($tmp$$Register, HIGH_FROM_LOW($src$$Register));\n-    __ imull($tmp$$Register, rax);\n-    __ mull($src$$Register);\n-    __ addl(rdx, $tmp$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Multiply Register Long where the right operand's high 32 bits are zero\n-instruct mulL_eReg_rhi0(eADXRegL dst, eRegL src, rRegI tmp, eFlagsReg cr) %{\n-  predicate(is_operand_hi32_zero(n->in(2)));\n-  match(Set dst (MulL dst src));\n-  effect(KILL cr, TEMP tmp);\n-  ins_cost(2*100+2*400);\n-\/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-\/\/             hi(result) = hi(x_lo * y_lo) + lo(x_hi * y_lo) where lo(x_lo * y_hi) = 0 because y_hi = 0\n-  format %{ \"MOV    $tmp,$src.lo\\n\\t\"\n-            \"IMUL   $tmp,EDX\\n\\t\"\n-            \"MUL    EDX:EAX,$src.lo\\n\\t\"\n-            \"ADD    EDX,$tmp\" %}\n-  ins_encode %{\n-    __ movl($tmp$$Register, $src$$Register);\n-    __ imull($tmp$$Register, rdx);\n-    __ mull($src$$Register);\n-    __ addl(rdx, $tmp$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Multiply Register Long where the left and the right operands' high 32 bits are zero\n-instruct mulL_eReg_hi0(eADXRegL dst, eRegL src, eFlagsReg cr) %{\n-  predicate(is_operand_hi32_zero(n->in(1)) && is_operand_hi32_zero(n->in(2)));\n-  match(Set dst (MulL dst src));\n-  effect(KILL cr);\n-  ins_cost(1*400);\n-\/\/ Basic idea: lo(result) = lo(x_lo * y_lo)\n-\/\/             hi(result) = hi(x_lo * y_lo) where lo(x_hi * y_lo) = 0 and lo(x_lo * y_hi) = 0 because x_hi = 0 and y_hi = 0\n-  format %{ \"MUL    EDX:EAX,$src.lo\\n\\t\" %}\n-  ins_encode %{\n-    __ mull($src$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Multiply Register Long by small constant\n-instruct mulL_eReg_con(eADXRegL dst, immL_127 src, rRegI tmp, eFlagsReg cr) %{\n-  match(Set dst (MulL dst src));\n-  effect(KILL cr, TEMP tmp);\n-  ins_cost(2*100+2*400);\n-  size(12);\n-\/\/ Basic idea: lo(result) = lo(src * EAX)\n-\/\/             hi(result) = hi(src * EAX) + lo(src * EDX)\n-  format %{ \"IMUL   $tmp,EDX,$src\\n\\t\"\n-            \"MOV    EDX,$src\\n\\t\"\n-            \"MUL    EDX\\t# EDX*EAX -> EDX:EAX\\n\\t\"\n-            \"ADD    EDX,$tmp\" %}\n-  ins_encode( long_multiply_con( dst, src, tmp ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Integer DIV with Register\n-instruct divI_eReg(eAXRegI rax, eDXRegI rdx, eCXRegI div, eFlagsReg cr) %{\n-  match(Set rax (DivI rax div));\n-  effect(KILL rdx, KILL cr);\n-  size(26);\n-  ins_cost(30*100+10*100);\n-  format %{ \"CMP    EAX,0x80000000\\n\\t\"\n-            \"JNE,s  normal\\n\\t\"\n-            \"XOR    EDX,EDX\\n\\t\"\n-            \"CMP    ECX,-1\\n\\t\"\n-            \"JE,s   done\\n\"\n-    \"normal: CDQ\\n\\t\"\n-            \"IDIV   $div\\n\\t\"\n-    \"done:\"        %}\n-  opcode(0xF7, 0x7);  \/* Opcode F7 \/7 *\/\n-  ins_encode( cdq_enc, OpcP, RegOpc(div) );\n-  ins_pipe( ialu_reg_reg_alu0 );\n-%}\n-\n-\/\/ Divide Register Long\n-instruct divL_eReg(eADXRegL dst, eRegL src1, eRegL src2) %{\n-  match(Set dst (DivL src1 src2));\n-  effect(CALL);\n-  ins_cost(10000);\n-  format %{ \"PUSH   $src1.hi\\n\\t\"\n-            \"PUSH   $src1.lo\\n\\t\"\n-            \"PUSH   $src2.hi\\n\\t\"\n-            \"PUSH   $src2.lo\\n\\t\"\n-            \"CALL   SharedRuntime::ldiv\\n\\t\"\n-            \"ADD    ESP,16\" %}\n-  ins_encode( long_div(src1,src2) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Integer DIVMOD with Register, both quotient and mod results\n-instruct divModI_eReg_divmod(eAXRegI rax, eDXRegI rdx, eCXRegI div, eFlagsReg cr) %{\n-  match(DivModI rax div);\n-  effect(KILL cr);\n-  size(26);\n-  ins_cost(30*100+10*100);\n-  format %{ \"CMP    EAX,0x80000000\\n\\t\"\n-            \"JNE,s  normal\\n\\t\"\n-            \"XOR    EDX,EDX\\n\\t\"\n-            \"CMP    ECX,-1\\n\\t\"\n-            \"JE,s   done\\n\"\n-    \"normal: CDQ\\n\\t\"\n-            \"IDIV   $div\\n\\t\"\n-    \"done:\"        %}\n-  opcode(0xF7, 0x7);  \/* Opcode F7 \/7 *\/\n-  ins_encode( cdq_enc, OpcP, RegOpc(div) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Integer MOD with Register\n-instruct modI_eReg(eDXRegI rdx, eAXRegI rax, eCXRegI div, eFlagsReg cr) %{\n-  match(Set rdx (ModI rax div));\n-  effect(KILL rax, KILL cr);\n-\n-  size(26);\n-  ins_cost(300);\n-  format %{ \"CDQ\\n\\t\"\n-            \"IDIV   $div\" %}\n-  opcode(0xF7, 0x7);  \/* Opcode F7 \/7 *\/\n-  ins_encode( cdq_enc, OpcP, RegOpc(div) );\n-  ins_pipe( ialu_reg_reg_alu0 );\n-%}\n-\n-\/\/ Remainder Register Long\n-instruct modL_eReg(eADXRegL dst, eRegL src1, eRegL src2) %{\n-  match(Set dst (ModL src1 src2));\n-  effect(CALL);\n-  ins_cost(10000);\n-  format %{ \"PUSH   $src1.hi\\n\\t\"\n-            \"PUSH   $src1.lo\\n\\t\"\n-            \"PUSH   $src2.hi\\n\\t\"\n-            \"PUSH   $src2.lo\\n\\t\"\n-            \"CALL   SharedRuntime::lrem\\n\\t\"\n-            \"ADD    ESP,16\" %}\n-  ins_encode( long_mod(src1,src2) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Divide Register Long (no special case since divisor != -1)\n-instruct divL_eReg_imm32( eADXRegL dst, immL32 imm, rRegI tmp, rRegI tmp2, eFlagsReg cr ) %{\n-  match(Set dst (DivL dst imm));\n-  effect( TEMP tmp, TEMP tmp2, KILL cr );\n-  ins_cost(1000);\n-  format %{ \"MOV    $tmp,abs($imm) # ldiv EDX:EAX,$imm\\n\\t\"\n-            \"XOR    $tmp2,$tmp2\\n\\t\"\n-            \"CMP    $tmp,EDX\\n\\t\"\n-            \"JA,s   fast\\n\\t\"\n-            \"MOV    $tmp2,EAX\\n\\t\"\n-            \"MOV    EAX,EDX\\n\\t\"\n-            \"MOV    EDX,0\\n\\t\"\n-            \"JLE,s  pos\\n\\t\"\n-            \"LNEG   EAX : $tmp2\\n\\t\"\n-            \"DIV    $tmp # unsigned division\\n\\t\"\n-            \"XCHG   EAX,$tmp2\\n\\t\"\n-            \"DIV    $tmp\\n\\t\"\n-            \"LNEG   $tmp2 : EAX\\n\\t\"\n-            \"JMP,s  done\\n\"\n-    \"pos:\\n\\t\"\n-            \"DIV    $tmp\\n\\t\"\n-            \"XCHG   EAX,$tmp2\\n\"\n-    \"fast:\\n\\t\"\n-            \"DIV    $tmp\\n\"\n-    \"done:\\n\\t\"\n-            \"MOV    EDX,$tmp2\\n\\t\"\n-            \"NEG    EDX:EAX # if $imm < 0\" %}\n-  ins_encode %{\n-    int con = (int)$imm$$constant;\n-    assert(con != 0 && con != -1 && con != min_jint, \"wrong divisor\");\n-    int pcon = (con > 0) ? con : -con;\n-    Label Lfast, Lpos, Ldone;\n-\n-    __ movl($tmp$$Register, pcon);\n-    __ xorl($tmp2$$Register,$tmp2$$Register);\n-    __ cmpl($tmp$$Register, HIGH_FROM_LOW($dst$$Register));\n-    __ jccb(Assembler::above, Lfast); \/\/ result fits into 32 bit\n-\n-    __ movl($tmp2$$Register, $dst$$Register); \/\/ save\n-    __ movl($dst$$Register, HIGH_FROM_LOW($dst$$Register));\n-    __ movl(HIGH_FROM_LOW($dst$$Register),0); \/\/ preserve flags\n-    __ jccb(Assembler::lessEqual, Lpos); \/\/ result is positive\n-\n-    \/\/ Negative dividend.\n-    \/\/ convert value to positive to use unsigned division\n-    __ lneg($dst$$Register, $tmp2$$Register);\n-    __ divl($tmp$$Register);\n-    __ xchgl($dst$$Register, $tmp2$$Register);\n-    __ divl($tmp$$Register);\n-    \/\/ revert result back to negative\n-    __ lneg($tmp2$$Register, $dst$$Register);\n-    __ jmpb(Ldone);\n-\n-    __ bind(Lpos);\n-    __ divl($tmp$$Register); \/\/ Use unsigned division\n-    __ xchgl($dst$$Register, $tmp2$$Register);\n-    \/\/ Fallthrow for final divide, tmp2 has 32 bit hi result\n-\n-    __ bind(Lfast);\n-    \/\/ fast path: src is positive\n-    __ divl($tmp$$Register); \/\/ Use unsigned division\n-\n-    __ bind(Ldone);\n-    __ movl(HIGH_FROM_LOW($dst$$Register),$tmp2$$Register);\n-    if (con < 0) {\n-      __ lneg(HIGH_FROM_LOW($dst$$Register), $dst$$Register);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Remainder Register Long (remainder fit into 32 bits)\n-instruct modL_eReg_imm32( eADXRegL dst, immL32 imm, rRegI tmp, rRegI tmp2, eFlagsReg cr ) %{\n-  match(Set dst (ModL dst imm));\n-  effect( TEMP tmp, TEMP tmp2, KILL cr );\n-  ins_cost(1000);\n-  format %{ \"MOV    $tmp,abs($imm) # lrem EDX:EAX,$imm\\n\\t\"\n-            \"CMP    $tmp,EDX\\n\\t\"\n-            \"JA,s   fast\\n\\t\"\n-            \"MOV    $tmp2,EAX\\n\\t\"\n-            \"MOV    EAX,EDX\\n\\t\"\n-            \"MOV    EDX,0\\n\\t\"\n-            \"JLE,s  pos\\n\\t\"\n-            \"LNEG   EAX : $tmp2\\n\\t\"\n-            \"DIV    $tmp # unsigned division\\n\\t\"\n-            \"MOV    EAX,$tmp2\\n\\t\"\n-            \"DIV    $tmp\\n\\t\"\n-            \"NEG    EDX\\n\\t\"\n-            \"JMP,s  done\\n\"\n-    \"pos:\\n\\t\"\n-            \"DIV    $tmp\\n\\t\"\n-            \"MOV    EAX,$tmp2\\n\"\n-    \"fast:\\n\\t\"\n-            \"DIV    $tmp\\n\"\n-    \"done:\\n\\t\"\n-            \"MOV    EAX,EDX\\n\\t\"\n-            \"SAR    EDX,31\\n\\t\" %}\n-  ins_encode %{\n-    int con = (int)$imm$$constant;\n-    assert(con != 0 && con != -1 && con != min_jint, \"wrong divisor\");\n-    int pcon = (con > 0) ? con : -con;\n-    Label  Lfast, Lpos, Ldone;\n-\n-    __ movl($tmp$$Register, pcon);\n-    __ cmpl($tmp$$Register, HIGH_FROM_LOW($dst$$Register));\n-    __ jccb(Assembler::above, Lfast); \/\/ src is positive and result fits into 32 bit\n-\n-    __ movl($tmp2$$Register, $dst$$Register); \/\/ save\n-    __ movl($dst$$Register, HIGH_FROM_LOW($dst$$Register));\n-    __ movl(HIGH_FROM_LOW($dst$$Register),0); \/\/ preserve flags\n-    __ jccb(Assembler::lessEqual, Lpos); \/\/ result is positive\n-\n-    \/\/ Negative dividend.\n-    \/\/ convert value to positive to use unsigned division\n-    __ lneg($dst$$Register, $tmp2$$Register);\n-    __ divl($tmp$$Register);\n-    __ movl($dst$$Register, $tmp2$$Register);\n-    __ divl($tmp$$Register);\n-    \/\/ revert remainder back to negative\n-    __ negl(HIGH_FROM_LOW($dst$$Register));\n-    __ jmpb(Ldone);\n-\n-    __ bind(Lpos);\n-    __ divl($tmp$$Register);\n-    __ movl($dst$$Register, $tmp2$$Register);\n-\n-    __ bind(Lfast);\n-    \/\/ fast path: src is positive\n-    __ divl($tmp$$Register);\n-\n-    __ bind(Ldone);\n-    __ movl($dst$$Register, HIGH_FROM_LOW($dst$$Register));\n-    __ sarl(HIGH_FROM_LOW($dst$$Register), 31); \/\/ result sign\n-\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Integer Shift Instructions\n-\/\/ Shift Left by one\n-instruct shlI_eReg_1(rRegI dst, immI_1 shift, eFlagsReg cr) %{\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SHL    $dst,$shift\" %}\n-  opcode(0xD1, 0x4);  \/* D1 \/4 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Shift Left by 8-bit immediate\n-instruct salI_eReg_imm(rRegI dst, immI8 shift, eFlagsReg cr) %{\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(3);\n-  format %{ \"SHL    $dst,$shift\" %}\n-  opcode(0xC1, 0x4);  \/* C1 \/4 ib *\/\n-  ins_encode( RegOpcImm( dst, shift) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Shift Left by variable\n-instruct salI_eReg_CL(rRegI dst, eCXRegI shift, eFlagsReg cr) %{\n-  match(Set dst (LShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SHL    $dst,$shift\" %}\n-  opcode(0xD3, 0x4);  \/* D3 \/4 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\/\/ Arithmetic shift right by one\n-instruct sarI_eReg_1(rRegI dst, immI_1 shift, eFlagsReg cr) %{\n-  match(Set dst (RShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SAR    $dst,$shift\" %}\n-  opcode(0xD1, 0x7);  \/* D1 \/7 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Arithmetic shift right by one\n-instruct sarI_mem_1(memory dst, immI_1 shift, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-  format %{ \"SAR    $dst,$shift\" %}\n-  opcode(0xD1, 0x7);  \/* D1 \/7 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(secondary,dst), ClearInstMark );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarI_eReg_imm(rRegI dst, immI8 shift, eFlagsReg cr) %{\n-  match(Set dst (RShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(3);\n-  format %{ \"SAR    $dst,$shift\" %}\n-  opcode(0xC1, 0x7);  \/* C1 \/7 ib *\/\n-  ins_encode( RegOpcImm( dst, shift ) );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Arithmetic Shift Right by 8-bit immediate\n-instruct sarI_mem_imm(memory dst, immI8 shift, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n-  format %{ \"SAR    $dst,$shift\" %}\n-  opcode(0xC1, 0x7);  \/* C1 \/7 ib *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(secondary, dst ), Con8or32(shift), ClearInstMark );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ Arithmetic Shift Right by variable\n-instruct sarI_eReg_CL(rRegI dst, eCXRegI shift, eFlagsReg cr) %{\n-  match(Set dst (RShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SAR    $dst,$shift\" %}\n-  opcode(0xD3, 0x7);  \/* D3 \/7 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\/\/ Logical shift right by one\n-instruct shrI_eReg_1(rRegI dst, immI_1 shift, eFlagsReg cr) %{\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SHR    $dst,$shift\" %}\n-  opcode(0xD1, 0x5);  \/* D1 \/5 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Logical Shift Right by 8-bit immediate\n-instruct shrI_eReg_imm(rRegI dst, immI8 shift, eFlagsReg cr) %{\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(3);\n-  format %{ \"SHR    $dst,$shift\" %}\n-  opcode(0xC1, 0x5);  \/* C1 \/5 ib *\/\n-  ins_encode( RegOpcImm( dst, shift) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\n-\/\/ Logical Shift Right by 24, followed by Arithmetic Shift Left by 24.\n-\/\/ This idiom is used by the compiler for the i2b bytecode.\n-instruct i2b(rRegI dst, xRegI src, immI_24 twentyfour) %{\n-  match(Set dst (RShiftI (LShiftI src twentyfour) twentyfour));\n-\n-  size(3);\n-  format %{ \"MOVSX  $dst,$src :8\" %}\n-  ins_encode %{\n-    __ movsbl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/ Logical Shift Right by 16, followed by Arithmetic Shift Left by 16.\n-\/\/ This idiom is used by the compiler the i2s bytecode.\n-instruct i2s(rRegI dst, xRegI src, immI_16 sixteen) %{\n-  match(Set dst (RShiftI (LShiftI src sixteen) sixteen));\n-\n-  size(3);\n-  format %{ \"MOVSX  $dst,$src :16\" %}\n-  ins_encode %{\n-    __ movswl($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\n-\/\/ Logical Shift Right by variable\n-instruct shrI_eReg_CL(rRegI dst, eCXRegI shift, eFlagsReg cr) %{\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"SHR    $dst,$shift\" %}\n-  opcode(0xD3, 0x5);  \/* D3 \/5 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\n-\/\/----------Logical Instructions-----------------------------------------------\n-\/\/----------Integer Logical Instructions---------------------------------------\n-\/\/ And Instructions\n-\/\/ And Register with Register\n-instruct andI_eReg(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (AndI dst src));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"AND    $dst,$src\" %}\n-  opcode(0x23);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\/\/ And Register with Immediate\n-instruct andI_eReg_imm(rRegI dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (AndI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"AND    $dst,$src\" %}\n-  opcode(0x81,0x04);  \/* Opcode 81 \/4 *\/\n-  \/\/ ins_encode( RegImm( dst, src) );\n-  ins_encode( OpcSErm( dst, src ), Con8or32( src ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ And Register with Memory\n-instruct andI_eReg_mem(rRegI dst, memory src, eFlagsReg cr) %{\n-  match(Set dst (AndI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"AND    $dst,$src\" %}\n-  opcode(0x23);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\/\/ And Memory with Register\n-instruct andI_mem_eReg(memory dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"AND    $dst,$src\" %}\n-  opcode(0x21);  \/* Opcode 21 \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ And Memory with Immediate\n-instruct andI_mem_imm(memory dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"AND    $dst,$src\" %}\n-  opcode(0x81, 0x4);  \/* Opcode 81 \/4 id *\/\n-  \/\/ ins_encode( MemImm( dst, src) );\n-  ins_encode( SetInstMark, OpcSE( src ), RMopc_Mem(secondary, dst ), Con8or32(src), ClearInstMark );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ BMI1 instructions\n-instruct andnI_rReg_rReg_rReg(rRegI dst, rRegI src1, rRegI src2, immI_M1 minus_1, eFlagsReg cr) %{\n-  match(Set dst (AndI (XorI src1 minus_1) src2));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  format %{ \"ANDNL  $dst, $src1, $src2\" %}\n-\n-  ins_encode %{\n-    __ andnl($dst$$Register, $src1$$Register, $src2$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct andnI_rReg_rReg_mem(rRegI dst, rRegI src1, memory src2, immI_M1 minus_1, eFlagsReg cr) %{\n-  match(Set dst (AndI (XorI src1 minus_1) (LoadI src2) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"ANDNL  $dst, $src1, $src2\" %}\n-\n-  ins_encode %{\n-    __ andnl($dst$$Register, $src1$$Register, $src2$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, eFlagsReg cr) %{\n-  match(Set dst (AndI (SubI imm_zero src) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  format %{ \"BLSIL  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsil($dst$$Register, $src$$Register);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, eFlagsReg cr) %{\n-  match(Set dst (AndI (SubI imm_zero (LoadI src) ) (LoadI src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"BLSIL  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsil($dst$$Register, $src$$Address);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsmskI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (XorI (AddI src minus_1) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  format %{ \"BLSMSKL $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsmskl($dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsmskI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (XorI (AddI (LoadI src) minus_1) (LoadI src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"BLSMSKL $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsmskl($dst$$Register, $src$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsrI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (AndI (AddI src minus_1) src) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  format %{ \"BLSRL  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsrl($dst$$Register, $src$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsrI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (AndI (AddI (LoadI src) minus_1) (LoadI src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"BLSRL  $dst, $src\" %}\n-\n-  ins_encode %{\n-    __ blsrl($dst$$Register, $src$$Address);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Or Instructions\n-\/\/ Or Register with Register\n-instruct orI_eReg(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (OrI dst src));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"OR     $dst,$src\" %}\n-  opcode(0x0B);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct orI_eReg_castP2X(rRegI dst, eRegP src, eFlagsReg cr) %{\n-  match(Set dst (OrI dst (CastP2X src)));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"OR     $dst,$src\" %}\n-  opcode(0x0B);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\n-\/\/ Or Register with Immediate\n-instruct orI_eReg_imm(rRegI dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (OrI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"OR     $dst,$src\" %}\n-  opcode(0x81,0x01);  \/* Opcode 81 \/1 id *\/\n-  \/\/ ins_encode( RegImm( dst, src) );\n-  ins_encode( OpcSErm( dst, src ), Con8or32( src ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Or Register with Memory\n-instruct orI_eReg_mem(rRegI dst, memory src, eFlagsReg cr) %{\n-  match(Set dst (OrI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"OR     $dst,$src\" %}\n-  opcode(0x0B);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\/\/ Or Memory with Register\n-instruct orI_mem_eReg(memory dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"OR     $dst,$src\" %}\n-  opcode(0x09);  \/* Opcode 09 \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Or Memory with Immediate\n-instruct orI_mem_imm(memory dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"OR     $dst,$src\" %}\n-  opcode(0x81,0x1);  \/* Opcode 81 \/1 id *\/\n-  \/\/ ins_encode( MemImm( dst, src) );\n-  ins_encode( SetInstMark, OpcSE( src ), RMopc_Mem(secondary, dst ), Con8or32(src), ClearInstMark );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/ ROL\/ROR\n-\/\/ ROL expand\n-instruct rolI_eReg_imm1(rRegI dst, immI_1 shift, eFlagsReg cr) %{\n-  effect(USE_DEF dst, USE shift, KILL cr);\n-\n-  format %{ \"ROL    $dst, $shift\" %}\n-  opcode(0xD1, 0x0); \/* Opcode D1 \/0 *\/\n-  ins_encode( OpcP, RegOpc( dst ));\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct rolI_eReg_imm8(rRegI dst, immI8 shift, eFlagsReg cr) %{\n-  effect(USE_DEF dst, USE shift, KILL cr);\n-\n-  format %{ \"ROL    $dst, $shift\" %}\n-  opcode(0xC1, 0x0); \/*Opcode \/C1  \/0  *\/\n-  ins_encode( RegOpcImm(dst, shift) );\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct rolI_eReg_CL(ncxRegI dst, eCXRegI shift, eFlagsReg cr) %{\n-  effect(USE_DEF dst, USE shift, KILL cr);\n-\n-  format %{ \"ROL    $dst, $shift\" %}\n-  opcode(0xD3, 0x0);    \/* Opcode D3 \/0 *\/\n-  ins_encode(OpcP, RegOpc(dst));\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\/\/ end of ROL expand\n-\n-\/\/ ROL 32bit by one once\n-instruct rolI_eReg_i1(rRegI dst, immI_1 lshift, immI_M1 rshift, eFlagsReg cr) %{\n-  match(Set dst ( OrI (LShiftI dst lshift) (URShiftI dst rshift)));\n-\n-  expand %{\n-    rolI_eReg_imm1(dst, lshift, cr);\n-  %}\n-%}\n-\n-\/\/ ROL 32bit var by imm8 once\n-instruct rolI_eReg_i8(rRegI dst, immI8 lshift, immI8 rshift, eFlagsReg cr) %{\n-  predicate(  0 == ((n->in(1)->in(2)->get_int() + n->in(2)->in(2)->get_int()) & 0x1f));\n-  match(Set dst ( OrI (LShiftI dst lshift) (URShiftI dst rshift)));\n-\n-  expand %{\n-    rolI_eReg_imm8(dst, lshift, cr);\n-  %}\n-%}\n-\n-\/\/ ROL 32bit var by var once\n-instruct rolI_eReg_Var_C0(ncxRegI dst, eCXRegI shift, immI_0 zero, eFlagsReg cr) %{\n-  match(Set dst ( OrI (LShiftI dst shift) (URShiftI dst (SubI zero shift))));\n-\n-  expand %{\n-    rolI_eReg_CL(dst, shift, cr);\n-  %}\n-%}\n-\n-\/\/ ROL 32bit var by var once\n-instruct rolI_eReg_Var_C32(ncxRegI dst, eCXRegI shift, immI_32 c32, eFlagsReg cr) %{\n-  match(Set dst ( OrI (LShiftI dst shift) (URShiftI dst (SubI c32 shift))));\n-\n-  expand %{\n-    rolI_eReg_CL(dst, shift, cr);\n-  %}\n-%}\n-\n-\/\/ ROR expand\n-instruct rorI_eReg_imm1(rRegI dst, immI_1 shift, eFlagsReg cr) %{\n-  effect(USE_DEF dst, USE shift, KILL cr);\n-\n-  format %{ \"ROR    $dst, $shift\" %}\n-  opcode(0xD1,0x1);  \/* Opcode D1 \/1 *\/\n-  ins_encode( OpcP, RegOpc( dst ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct rorI_eReg_imm8(rRegI dst, immI8 shift, eFlagsReg cr) %{\n-  effect (USE_DEF dst, USE shift, KILL cr);\n-\n-  format %{ \"ROR    $dst, $shift\" %}\n-  opcode(0xC1, 0x1); \/* Opcode \/C1 \/1 ib *\/\n-  ins_encode( RegOpcImm(dst, shift) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-instruct rorI_eReg_CL(ncxRegI dst, eCXRegI shift, eFlagsReg cr)%{\n-  effect(USE_DEF dst, USE shift, KILL cr);\n-\n-  format %{ \"ROR    $dst, $shift\" %}\n-  opcode(0xD3, 0x1);    \/* Opcode D3 \/1 *\/\n-  ins_encode(OpcP, RegOpc(dst));\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\/\/ end of ROR expand\n-\n-\/\/ ROR right once\n-instruct rorI_eReg_i1(rRegI dst, immI_1 rshift, immI_M1 lshift, eFlagsReg cr) %{\n-  match(Set dst ( OrI (URShiftI dst rshift) (LShiftI dst lshift)));\n-\n-  expand %{\n-    rorI_eReg_imm1(dst, rshift, cr);\n-  %}\n-%}\n-\n-\/\/ ROR 32bit by immI8 once\n-instruct rorI_eReg_i8(rRegI dst, immI8 rshift, immI8 lshift, eFlagsReg cr) %{\n-  predicate(  0 == ((n->in(1)->in(2)->get_int() + n->in(2)->in(2)->get_int()) & 0x1f));\n-  match(Set dst ( OrI (URShiftI dst rshift) (LShiftI dst lshift)));\n-\n-  expand %{\n-    rorI_eReg_imm8(dst, rshift, cr);\n-  %}\n-%}\n-\n-\/\/ ROR 32bit var by var once\n-instruct rorI_eReg_Var_C0(ncxRegI dst, eCXRegI shift, immI_0 zero, eFlagsReg cr) %{\n-  match(Set dst ( OrI (URShiftI dst shift) (LShiftI dst (SubI zero shift))));\n-\n-  expand %{\n-    rorI_eReg_CL(dst, shift, cr);\n-  %}\n-%}\n-\n-\/\/ ROR 32bit var by var once\n-instruct rorI_eReg_Var_C32(ncxRegI dst, eCXRegI shift, immI_32 c32, eFlagsReg cr) %{\n-  match(Set dst ( OrI (URShiftI dst shift) (LShiftI dst (SubI c32 shift))));\n-\n-  expand %{\n-    rorI_eReg_CL(dst, shift, cr);\n-  %}\n-%}\n-\n-\/\/ Xor Instructions\n-\/\/ Xor Register with Register\n-instruct xorI_eReg(rRegI dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (XorI dst src));\n-  effect(KILL cr);\n-\n-  size(2);\n-  format %{ \"XOR    $dst,$src\" %}\n-  opcode(0x33);\n-  ins_encode( OpcP, RegReg( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-\/\/ Xor Register with Immediate -1\n-instruct xorI_eReg_im1(rRegI dst, immI_M1 imm) %{\n-  match(Set dst (XorI dst imm));\n-\n-  size(2);\n-  format %{ \"NOT    $dst\" %}\n-  ins_encode %{\n-     __ notl($dst$$Register);\n-  %}\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Xor Register with Immediate\n-instruct xorI_eReg_imm(rRegI dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (XorI dst src));\n-  effect(KILL cr);\n-\n-  format %{ \"XOR    $dst,$src\" %}\n-  opcode(0x81,0x06);  \/* Opcode 81 \/6 id *\/\n-  \/\/ ins_encode( RegImm( dst, src) );\n-  ins_encode( OpcSErm( dst, src ), Con8or32( src ) );\n-  ins_pipe( ialu_reg );\n-%}\n-\n-\/\/ Xor Register with Memory\n-instruct xorI_eReg_mem(rRegI dst, memory src, eFlagsReg cr) %{\n-  match(Set dst (XorI dst (LoadI src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"XOR    $dst,$src\" %}\n-  opcode(0x33);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst, src), ClearInstMark );\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-\/\/ Xor Memory with Register\n-instruct xorI_mem_eReg(memory dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(150);\n-  format %{ \"XOR    $dst,$src\" %}\n-  opcode(0x31);  \/* Opcode 31 \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\/\/ Xor Memory with Immediate\n-instruct xorI_mem_imm(memory dst, immI src, eFlagsReg cr) %{\n-  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n-  effect(KILL cr);\n-\n-  ins_cost(125);\n-  format %{ \"XOR    $dst,$src\" %}\n-  opcode(0x81,0x6);  \/* Opcode 81 \/6 id *\/\n-  ins_encode( SetInstMark, OpcSE( src ), RMopc_Mem(secondary, dst ), Con8or32(src), ClearInstMark );\n-  ins_pipe( ialu_mem_imm );\n-%}\n-\n-\/\/----------Convert Int to Boolean---------------------------------------------\n-\n-instruct movI_nocopy(rRegI dst, rRegI src) %{\n-  effect( DEF dst, USE src );\n-  format %{ \"MOV    $dst,$src\" %}\n-  ins_encode( enc_Copy( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct ci2b( rRegI dst, rRegI src, eFlagsReg cr ) %{\n-  effect( USE_DEF dst, USE src, KILL cr );\n-\n-  size(4);\n-  format %{ \"NEG    $dst\\n\\t\"\n-            \"ADC    $dst,$src\" %}\n-  ins_encode( neg_reg(dst),\n-              OpcRegReg(0x13,dst,src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-instruct convI2B( rRegI dst, rRegI src, eFlagsReg cr ) %{\n-  match(Set dst (Conv2B src));\n-\n-  expand %{\n-    movI_nocopy(dst,src);\n-    ci2b(dst,src,cr);\n-  %}\n-%}\n-\n-instruct movP_nocopy(rRegI dst, eRegP src) %{\n-  effect( DEF dst, USE src );\n-  format %{ \"MOV    $dst,$src\" %}\n-  ins_encode( enc_Copy( dst, src) );\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct cp2b( rRegI dst, eRegP src, eFlagsReg cr ) %{\n-  effect( USE_DEF dst, USE src, KILL cr );\n-  format %{ \"NEG    $dst\\n\\t\"\n-            \"ADC    $dst,$src\" %}\n-  ins_encode( neg_reg(dst),\n-              OpcRegReg(0x13,dst,src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-instruct convP2B( rRegI dst, eRegP src, eFlagsReg cr ) %{\n-  match(Set dst (Conv2B src));\n-\n-  expand %{\n-    movP_nocopy(dst,src);\n-    cp2b(dst,src,cr);\n-  %}\n-%}\n-\n-instruct cmpLTMask(eCXRegI dst, ncxRegI p, ncxRegI q, eFlagsReg cr) %{\n-  match(Set dst (CmpLTMask p q));\n-  effect(KILL cr);\n-  ins_cost(400);\n-\n-  \/\/ SETlt can only use low byte of EAX,EBX, ECX, or EDX as destination\n-  format %{ \"XOR    $dst,$dst\\n\\t\"\n-            \"CMP    $p,$q\\n\\t\"\n-            \"SETlt  $dst\\n\\t\"\n-            \"NEG    $dst\" %}\n-  ins_encode %{\n-    Register Rp = $p$$Register;\n-    Register Rq = $q$$Register;\n-    Register Rd = $dst$$Register;\n-    Label done;\n-    __ xorl(Rd, Rd);\n-    __ cmpl(Rp, Rq);\n-    __ setb(Assembler::less, Rd);\n-    __ negl(Rd);\n-  %}\n-\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpLTMask0(rRegI dst, immI_0 zero, eFlagsReg cr) %{\n-  match(Set dst (CmpLTMask dst zero));\n-  effect(DEF dst, KILL cr);\n-  ins_cost(100);\n-\n-  format %{ \"SAR    $dst,31\\t# cmpLTMask0\" %}\n-  ins_encode %{\n-  __ sarl($dst$$Register, 31);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-\/* better to save a register than avoid a branch *\/\n-instruct cadd_cmpLTMask(rRegI p, rRegI q, rRegI y, eFlagsReg cr) %{\n-  match(Set p (AddI (AndI (CmpLTMask p q) y) (SubI p q)));\n-  effect(KILL cr);\n-  ins_cost(400);\n-  format %{ \"SUB    $p,$q\\t# cadd_cmpLTMask\\n\\t\"\n-            \"JGE    done\\n\\t\"\n-            \"ADD    $p,$y\\n\"\n-            \"done:  \" %}\n-  ins_encode %{\n-    Register Rp = $p$$Register;\n-    Register Rq = $q$$Register;\n-    Register Ry = $y$$Register;\n-    Label done;\n-    __ subl(Rp, Rq);\n-    __ jccb(Assembler::greaterEqual, done);\n-    __ addl(Rp, Ry);\n-    __ bind(done);\n-  %}\n-\n-  ins_pipe(pipe_cmplt);\n-%}\n-\n-\/* better to save a register than avoid a branch *\/\n-instruct and_cmpLTMask(rRegI p, rRegI q, rRegI y, eFlagsReg cr) %{\n-  match(Set y (AndI (CmpLTMask p q) y));\n-  effect(KILL cr);\n-\n-  ins_cost(300);\n-\n-  format %{ \"CMPL     $p, $q\\t# and_cmpLTMask\\n\\t\"\n-            \"JLT      done\\n\\t\"\n-            \"XORL     $y, $y\\n\"\n-            \"done:  \" %}\n-  ins_encode %{\n-    Register Rp = $p$$Register;\n-    Register Rq = $q$$Register;\n-    Register Ry = $y$$Register;\n-    Label done;\n-    __ cmpl(Rp, Rq);\n-    __ jccb(Assembler::less, done);\n-    __ xorl(Ry, Ry);\n-    __ bind(done);\n-  %}\n-\n-  ins_pipe(pipe_cmplt);\n-%}\n-\n-\/* If I enable this, I encourage spilling in the inner loop of compress.\n-instruct cadd_cmpLTMask_mem(ncxRegI p, ncxRegI q, memory y, eCXRegI tmp, eFlagsReg cr) %{\n-  match(Set p (AddI (AndI (CmpLTMask p q) (LoadI y)) (SubI p q)));\n-*\/\n-\/\/----------Overflow Math Instructions-----------------------------------------\n-\n-instruct overflowAddI_eReg(eFlagsReg cr, eAXRegI op1, rRegI op2)\n-%{\n-  match(Set cr (OverflowAddI op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"ADD    $op1, $op2\\t# overflow check int\" %}\n-\n-  ins_encode %{\n-    __ addl($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowAddI_rReg_imm(eFlagsReg cr, eAXRegI op1, immI op2)\n-%{\n-  match(Set cr (OverflowAddI op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"ADD    $op1, $op2\\t# overflow check int\" %}\n-\n-  ins_encode %{\n-    __ addl($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowSubI_rReg(eFlagsReg cr, rRegI op1, rRegI op2)\n-%{\n-  match(Set cr (OverflowSubI op1 op2));\n-\n-  format %{ \"CMP    $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowSubI_rReg_imm(eFlagsReg cr, rRegI op1, immI op2)\n-%{\n-  match(Set cr (OverflowSubI op1 op2));\n-\n-  format %{ \"CMP    $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ cmpl($op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowNegI_rReg(eFlagsReg cr, immI_0 zero, eAXRegI op2)\n-%{\n-  match(Set cr (OverflowSubI zero op2));\n-  effect(DEF cr, USE_KILL op2);\n-\n-  format %{ \"NEG    $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ negl($op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-instruct overflowMulI_rReg(eFlagsReg cr, eAXRegI op1, rRegI op2)\n-%{\n-  match(Set cr (OverflowMulI op1 op2));\n-  effect(DEF cr, USE_KILL op1, USE op2);\n-\n-  format %{ \"IMUL    $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ imull($op1$$Register, $op2$$Register);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-instruct overflowMulI_rReg_imm(eFlagsReg cr, rRegI op1, immI op2, rRegI tmp)\n-%{\n-  match(Set cr (OverflowMulI op1 op2));\n-  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n-\n-  format %{ \"IMUL    $tmp, $op1, $op2\\t# overflow check int\" %}\n-  ins_encode %{\n-    __ imull($tmp$$Register, $op1$$Register, $op2$$constant);\n-  %}\n-  ins_pipe(ialu_reg_reg_alu0);\n-%}\n-\n-\/\/ Integer Absolute Instructions\n-instruct absI_rReg(rRegI dst, rRegI src, rRegI tmp, eFlagsReg cr)\n-%{\n-  match(Set dst (AbsI src));\n-  effect(TEMP dst, TEMP tmp, KILL cr);\n-  format %{ \"movl $tmp, $src\\n\\t\"\n-            \"sarl $tmp, 31\\n\\t\"\n-            \"movl $dst, $src\\n\\t\"\n-            \"xorl $dst, $tmp\\n\\t\"\n-            \"subl $dst, $tmp\\n\"\n-          %}\n-  ins_encode %{\n-    __ movl($tmp$$Register, $src$$Register);\n-    __ sarl($tmp$$Register, 31);\n-    __ movl($dst$$Register, $src$$Register);\n-    __ xorl($dst$$Register, $tmp$$Register);\n-    __ subl($dst$$Register, $tmp$$Register);\n-  %}\n-\n-  ins_pipe(ialu_reg_reg);\n-%}\n-\n-\/\/----------Long Instructions------------------------------------------------\n-\/\/ Add Long Register with Register\n-instruct addL_eReg(eRegL dst, eRegL src, eFlagsReg cr) %{\n-  match(Set dst (AddL dst src));\n-  effect(KILL cr);\n-  ins_cost(200);\n-  format %{ \"ADD    $dst.lo,$src.lo\\n\\t\"\n-            \"ADC    $dst.hi,$src.hi\" %}\n-  opcode(0x03, 0x13);\n-  ins_encode( RegReg_Lo(dst, src), RegReg_Hi(dst,src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Add Long Register with Immediate\n-instruct addL_eReg_imm(eRegL dst, immL src, eFlagsReg cr) %{\n-  match(Set dst (AddL dst src));\n-  effect(KILL cr);\n-  format %{ \"ADD    $dst.lo,$src.lo\\n\\t\"\n-            \"ADC    $dst.hi,$src.hi\" %}\n-  opcode(0x81,0x00,0x02);  \/* Opcode 81 \/0, 81 \/2 *\/\n-  ins_encode( Long_OpcSErm_Lo( dst, src ), Long_OpcSErm_Hi( dst, src ) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Add Long Register with Memory\n-instruct addL_eReg_mem(eRegL dst, load_long_memory mem, eFlagsReg cr) %{\n-  match(Set dst (AddL dst (LoadL mem)));\n-  effect(KILL cr);\n-  ins_cost(125);\n-  format %{ \"ADD    $dst.lo,$mem\\n\\t\"\n-            \"ADC    $dst.hi,$mem+4\" %}\n-  opcode(0x03, 0x13);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, mem), OpcS, RegMem_Hi(dst,mem), ClearInstMark );\n-  ins_pipe( ialu_reg_long_mem );\n-%}\n-\n-\/\/ Subtract Long Register with Register.\n-instruct subL_eReg(eRegL dst, eRegL src, eFlagsReg cr) %{\n-  match(Set dst (SubL dst src));\n-  effect(KILL cr);\n-  ins_cost(200);\n-  format %{ \"SUB    $dst.lo,$src.lo\\n\\t\"\n-            \"SBB    $dst.hi,$src.hi\" %}\n-  opcode(0x2B, 0x1B);\n-  ins_encode( RegReg_Lo(dst, src), RegReg_Hi(dst,src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Subtract Long Register with Immediate\n-instruct subL_eReg_imm(eRegL dst, immL src, eFlagsReg cr) %{\n-  match(Set dst (SubL dst src));\n-  effect(KILL cr);\n-  format %{ \"SUB    $dst.lo,$src.lo\\n\\t\"\n-            \"SBB    $dst.hi,$src.hi\" %}\n-  opcode(0x81,0x05,0x03);  \/* Opcode 81 \/5, 81 \/3 *\/\n-  ins_encode( Long_OpcSErm_Lo( dst, src ), Long_OpcSErm_Hi( dst, src ) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Subtract Long Register with Memory\n-instruct subL_eReg_mem(eRegL dst, load_long_memory mem, eFlagsReg cr) %{\n-  match(Set dst (SubL dst (LoadL mem)));\n-  effect(KILL cr);\n-  ins_cost(125);\n-  format %{ \"SUB    $dst.lo,$mem\\n\\t\"\n-            \"SBB    $dst.hi,$mem+4\" %}\n-  opcode(0x2B, 0x1B);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, mem), OpcS, RegMem_Hi(dst,mem), ClearInstMark );\n-  ins_pipe( ialu_reg_long_mem );\n-%}\n-\n-instruct negL_eReg(eRegL dst, immL0 zero, eFlagsReg cr) %{\n-  match(Set dst (SubL zero dst));\n-  effect(KILL cr);\n-  ins_cost(300);\n-  format %{ \"NEG    $dst.hi\\n\\tNEG    $dst.lo\\n\\tSBB    $dst.hi,0\" %}\n-  ins_encode( neg_long(dst) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ And Long Register with Register\n-instruct andL_eReg(eRegL dst, eRegL src, eFlagsReg cr) %{\n-  match(Set dst (AndL dst src));\n-  effect(KILL cr);\n-  format %{ \"AND    $dst.lo,$src.lo\\n\\t\"\n-            \"AND    $dst.hi,$src.hi\" %}\n-  opcode(0x23,0x23);\n-  ins_encode( RegReg_Lo( dst, src), RegReg_Hi( dst, src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ And Long Register with Immediate\n-instruct andL_eReg_imm(eRegL dst, immL src, eFlagsReg cr) %{\n-  match(Set dst (AndL dst src));\n-  effect(KILL cr);\n-  format %{ \"AND    $dst.lo,$src.lo\\n\\t\"\n-            \"AND    $dst.hi,$src.hi\" %}\n-  opcode(0x81,0x04,0x04);  \/* Opcode 81 \/4, 81 \/4 *\/\n-  ins_encode( Long_OpcSErm_Lo( dst, src ), Long_OpcSErm_Hi( dst, src ) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ And Long Register with Memory\n-instruct andL_eReg_mem(eRegL dst, load_long_memory mem, eFlagsReg cr) %{\n-  match(Set dst (AndL dst (LoadL mem)));\n-  effect(KILL cr);\n-  ins_cost(125);\n-  format %{ \"AND    $dst.lo,$mem\\n\\t\"\n-            \"AND    $dst.hi,$mem+4\" %}\n-  opcode(0x23, 0x23);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, mem), OpcS, RegMem_Hi(dst,mem), ClearInstMark );\n-  ins_pipe( ialu_reg_long_mem );\n-%}\n-\n-\/\/ BMI1 instructions\n-instruct andnL_eReg_eReg_eReg(eRegL dst, eRegL src1, eRegL src2, immL_M1 minus_1, eFlagsReg cr) %{\n-  match(Set dst (AndL (XorL src1 minus_1) src2));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  format %{ \"ANDNL  $dst.lo, $src1.lo, $src2.lo\\n\\t\"\n-            \"ANDNL  $dst.hi, $src1.hi, $src2.hi\"\n-         %}\n-\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc1 = $src1$$Register;\n-    Register Rsrc2 = $src2$$Register;\n-    __ andnl(Rdst, Rsrc1, Rsrc2);\n-    __ andnl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rsrc1), HIGH_FROM_LOW(Rsrc2));\n-  %}\n-  ins_pipe(ialu_reg_reg_long);\n-%}\n-\n-instruct andnL_eReg_eReg_mem(eRegL dst, eRegL src1, memory src2, immL_M1 minus_1, eFlagsReg cr) %{\n-  match(Set dst (AndL (XorL src1 minus_1) (LoadL src2) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  ins_cost(125);\n-  format %{ \"ANDNL  $dst.lo, $src1.lo, $src2\\n\\t\"\n-            \"ANDNL  $dst.hi, $src1.hi, $src2+4\"\n-         %}\n-\n-  ins_encode %{\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc1 = $src1$$Register;\n-    Address src2_hi = Address::make_raw($src2$$base, $src2$$index, $src2$$scale, $src2$$disp + 4, relocInfo::none);\n-\n-    __ andnl(Rdst, Rsrc1, $src2$$Address);\n-    __ andnl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rsrc1), src2_hi);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsiL_eReg_eReg(eRegL dst, eRegL src, immL0 imm_zero, eFlagsReg cr) %{\n-  match(Set dst (AndL (SubL imm_zero src) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  format %{ \"MOVL   $dst.hi, 0\\n\\t\"\n-            \"BLSIL  $dst.lo, $src.lo\\n\\t\"\n-            \"JNZ    done\\n\\t\"\n-            \"BLSIL  $dst.hi, $src.hi\\n\"\n-            \"done:\"\n-         %}\n-\n-  ins_encode %{\n-    Label done;\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    __ movl(HIGH_FROM_LOW(Rdst), 0);\n-    __ blsil(Rdst, Rsrc);\n-    __ jccb(Assembler::notZero, done);\n-    __ blsil(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rsrc));\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsiL_eReg_mem(eRegL dst, memory src, immL0 imm_zero, eFlagsReg cr) %{\n-  match(Set dst (AndL (SubL imm_zero (LoadL src) ) (LoadL src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  ins_cost(125);\n-  format %{ \"MOVL   $dst.hi, 0\\n\\t\"\n-            \"BLSIL  $dst.lo, $src\\n\\t\"\n-            \"JNZ    done\\n\\t\"\n-            \"BLSIL  $dst.hi, $src+4\\n\"\n-            \"done:\"\n-         %}\n-\n-  ins_encode %{\n-    Label done;\n-    Register Rdst = $dst$$Register;\n-    Address src_hi = Address::make_raw($src$$base, $src$$index, $src$$scale, $src$$disp + 4, relocInfo::none);\n-\n-    __ movl(HIGH_FROM_LOW(Rdst), 0);\n-    __ blsil(Rdst, $src$$Address);\n-    __ jccb(Assembler::notZero, done);\n-    __ blsil(HIGH_FROM_LOW(Rdst), src_hi);\n-    __ bind(done);\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsmskL_eReg_eReg(eRegL dst, eRegL src, immL_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (XorL (AddL src minus_1) src));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  format %{ \"MOVL    $dst.hi, 0\\n\\t\"\n-            \"BLSMSKL $dst.lo, $src.lo\\n\\t\"\n-            \"JNC     done\\n\\t\"\n-            \"BLSMSKL $dst.hi, $src.hi\\n\"\n-            \"done:\"\n-         %}\n-\n-  ins_encode %{\n-    Label done;\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    __ movl(HIGH_FROM_LOW(Rdst), 0);\n-    __ blsmskl(Rdst, Rsrc);\n-    __ jccb(Assembler::carryClear, done);\n-    __ blsmskl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rsrc));\n-    __ bind(done);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsmskL_eReg_mem(eRegL dst, memory src, immL_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (XorL (AddL (LoadL src) minus_1) (LoadL src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  ins_cost(125);\n-  format %{ \"MOVL    $dst.hi, 0\\n\\t\"\n-            \"BLSMSKL $dst.lo, $src\\n\\t\"\n-            \"JNC     done\\n\\t\"\n-            \"BLSMSKL $dst.hi, $src+4\\n\"\n-            \"done:\"\n-         %}\n-\n-  ins_encode %{\n-    Label done;\n-    Register Rdst = $dst$$Register;\n-    Address src_hi = Address::make_raw($src$$base, $src$$index, $src$$scale, $src$$disp + 4, relocInfo::none);\n-\n-    __ movl(HIGH_FROM_LOW(Rdst), 0);\n-    __ blsmskl(Rdst, $src$$Address);\n-    __ jccb(Assembler::carryClear, done);\n-    __ blsmskl(HIGH_FROM_LOW(Rdst), src_hi);\n-    __ bind(done);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-instruct blsrL_eReg_eReg(eRegL dst, eRegL src, immL_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (AndL (AddL src minus_1) src) );\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  format %{ \"MOVL   $dst.hi, $src.hi\\n\\t\"\n-            \"BLSRL  $dst.lo, $src.lo\\n\\t\"\n-            \"JNC    done\\n\\t\"\n-            \"BLSRL  $dst.hi, $src.hi\\n\"\n-            \"done:\"\n-  %}\n-\n-  ins_encode %{\n-    Label done;\n-    Register Rdst = $dst$$Register;\n-    Register Rsrc = $src$$Register;\n-    __ movl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rsrc));\n-    __ blsrl(Rdst, Rsrc);\n-    __ jccb(Assembler::carryClear, done);\n-    __ blsrl(HIGH_FROM_LOW(Rdst), HIGH_FROM_LOW(Rsrc));\n-    __ bind(done);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct blsrL_eReg_mem(eRegL dst, memory src, immL_M1 minus_1, eFlagsReg cr)\n-%{\n-  match(Set dst (AndL (AddL (LoadL src) minus_1) (LoadL src) ));\n-  predicate(UseBMI1Instructions);\n-  effect(KILL cr, TEMP dst);\n-\n-  ins_cost(125);\n-  format %{ \"MOVL   $dst.hi, $src+4\\n\\t\"\n-            \"BLSRL  $dst.lo, $src\\n\\t\"\n-            \"JNC    done\\n\\t\"\n-            \"BLSRL  $dst.hi, $src+4\\n\"\n-            \"done:\"\n-  %}\n-\n-  ins_encode %{\n-    Label done;\n-    Register Rdst = $dst$$Register;\n-    Address src_hi = Address::make_raw($src$$base, $src$$index, $src$$scale, $src$$disp + 4, relocInfo::none);\n-    __ movl(HIGH_FROM_LOW(Rdst), src_hi);\n-    __ blsrl(Rdst, $src$$Address);\n-    __ jccb(Assembler::carryClear, done);\n-    __ blsrl(HIGH_FROM_LOW(Rdst), src_hi);\n-    __ bind(done);\n-  %}\n-\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\/\/ Or Long Register with Register\n-instruct orl_eReg(eRegL dst, eRegL src, eFlagsReg cr) %{\n-  match(Set dst (OrL dst src));\n-  effect(KILL cr);\n-  format %{ \"OR     $dst.lo,$src.lo\\n\\t\"\n-            \"OR     $dst.hi,$src.hi\" %}\n-  opcode(0x0B,0x0B);\n-  ins_encode( RegReg_Lo( dst, src), RegReg_Hi( dst, src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Or Long Register with Immediate\n-instruct orl_eReg_imm(eRegL dst, immL src, eFlagsReg cr) %{\n-  match(Set dst (OrL dst src));\n-  effect(KILL cr);\n-  format %{ \"OR     $dst.lo,$src.lo\\n\\t\"\n-            \"OR     $dst.hi,$src.hi\" %}\n-  opcode(0x81,0x01,0x01);  \/* Opcode 81 \/1, 81 \/1 *\/\n-  ins_encode( Long_OpcSErm_Lo( dst, src ), Long_OpcSErm_Hi( dst, src ) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Or Long Register with Memory\n-instruct orl_eReg_mem(eRegL dst, load_long_memory mem, eFlagsReg cr) %{\n-  match(Set dst (OrL dst (LoadL mem)));\n-  effect(KILL cr);\n-  ins_cost(125);\n-  format %{ \"OR     $dst.lo,$mem\\n\\t\"\n-            \"OR     $dst.hi,$mem+4\" %}\n-  opcode(0x0B,0x0B);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, mem), OpcS, RegMem_Hi(dst,mem), ClearInstMark );\n-  ins_pipe( ialu_reg_long_mem );\n-%}\n-\n-\/\/ Xor Long Register with Register\n-instruct xorl_eReg(eRegL dst, eRegL src, eFlagsReg cr) %{\n-  match(Set dst (XorL dst src));\n-  effect(KILL cr);\n-  format %{ \"XOR    $dst.lo,$src.lo\\n\\t\"\n-            \"XOR    $dst.hi,$src.hi\" %}\n-  opcode(0x33,0x33);\n-  ins_encode( RegReg_Lo( dst, src), RegReg_Hi( dst, src) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Xor Long Register with Immediate -1\n-instruct xorl_eReg_im1(eRegL dst, immL_M1 imm) %{\n-  match(Set dst (XorL dst imm));\n-  format %{ \"NOT    $dst.lo\\n\\t\"\n-            \"NOT    $dst.hi\" %}\n-  ins_encode %{\n-     __ notl($dst$$Register);\n-     __ notl(HIGH_FROM_LOW($dst$$Register));\n-  %}\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Xor Long Register with Immediate\n-instruct xorl_eReg_imm(eRegL dst, immL src, eFlagsReg cr) %{\n-  match(Set dst (XorL dst src));\n-  effect(KILL cr);\n-  format %{ \"XOR    $dst.lo,$src.lo\\n\\t\"\n-            \"XOR    $dst.hi,$src.hi\" %}\n-  opcode(0x81,0x06,0x06);  \/* Opcode 81 \/6, 81 \/6 *\/\n-  ins_encode( Long_OpcSErm_Lo( dst, src ), Long_OpcSErm_Hi( dst, src ) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Xor Long Register with Memory\n-instruct xorl_eReg_mem(eRegL dst, load_long_memory mem, eFlagsReg cr) %{\n-  match(Set dst (XorL dst (LoadL mem)));\n-  effect(KILL cr);\n-  ins_cost(125);\n-  format %{ \"XOR    $dst.lo,$mem\\n\\t\"\n-            \"XOR    $dst.hi,$mem+4\" %}\n-  opcode(0x33,0x33);\n-  ins_encode( SetInstMark, OpcP, RegMem( dst, mem), OpcS, RegMem_Hi(dst,mem), ClearInstMark );\n-  ins_pipe( ialu_reg_long_mem );\n-%}\n-\n-\/\/ Shift Left Long by 1\n-instruct shlL_eReg_1(eRegL dst, immI_1 cnt, eFlagsReg cr) %{\n-  predicate(UseNewLongLShift);\n-  match(Set dst (LShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(100);\n-  format %{ \"ADD    $dst.lo,$dst.lo\\n\\t\"\n-            \"ADC    $dst.hi,$dst.hi\" %}\n-  ins_encode %{\n-    __ addl($dst$$Register,$dst$$Register);\n-    __ adcl(HIGH_FROM_LOW($dst$$Register),HIGH_FROM_LOW($dst$$Register));\n-  %}\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Left Long by 2\n-instruct shlL_eReg_2(eRegL dst, immI_2 cnt, eFlagsReg cr) %{\n-  predicate(UseNewLongLShift);\n-  match(Set dst (LShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(100);\n-  format %{ \"ADD    $dst.lo,$dst.lo\\n\\t\"\n-            \"ADC    $dst.hi,$dst.hi\\n\\t\"\n-            \"ADD    $dst.lo,$dst.lo\\n\\t\"\n-            \"ADC    $dst.hi,$dst.hi\" %}\n-  ins_encode %{\n-    __ addl($dst$$Register,$dst$$Register);\n-    __ adcl(HIGH_FROM_LOW($dst$$Register),HIGH_FROM_LOW($dst$$Register));\n-    __ addl($dst$$Register,$dst$$Register);\n-    __ adcl(HIGH_FROM_LOW($dst$$Register),HIGH_FROM_LOW($dst$$Register));\n-  %}\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Left Long by 3\n-instruct shlL_eReg_3(eRegL dst, immI_3 cnt, eFlagsReg cr) %{\n-  predicate(UseNewLongLShift);\n-  match(Set dst (LShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(100);\n-  format %{ \"ADD    $dst.lo,$dst.lo\\n\\t\"\n-            \"ADC    $dst.hi,$dst.hi\\n\\t\"\n-            \"ADD    $dst.lo,$dst.lo\\n\\t\"\n-            \"ADC    $dst.hi,$dst.hi\\n\\t\"\n-            \"ADD    $dst.lo,$dst.lo\\n\\t\"\n-            \"ADC    $dst.hi,$dst.hi\" %}\n-  ins_encode %{\n-    __ addl($dst$$Register,$dst$$Register);\n-    __ adcl(HIGH_FROM_LOW($dst$$Register),HIGH_FROM_LOW($dst$$Register));\n-    __ addl($dst$$Register,$dst$$Register);\n-    __ adcl(HIGH_FROM_LOW($dst$$Register),HIGH_FROM_LOW($dst$$Register));\n-    __ addl($dst$$Register,$dst$$Register);\n-    __ adcl(HIGH_FROM_LOW($dst$$Register),HIGH_FROM_LOW($dst$$Register));\n-  %}\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Left Long by 1-31\n-instruct shlL_eReg_1_31(eRegL dst, immI_1_31 cnt, eFlagsReg cr) %{\n-  match(Set dst (LShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(200);\n-  format %{ \"SHLD   $dst.hi,$dst.lo,$cnt\\n\\t\"\n-            \"SHL    $dst.lo,$cnt\" %}\n-  opcode(0xC1, 0x4, 0xA4);  \/* 0F\/A4, then C1 \/4 ib *\/\n-  ins_encode( move_long_small_shift(dst,cnt) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Left Long by 32-63\n-instruct shlL_eReg_32_63(eRegL dst, immI_32_63 cnt, eFlagsReg cr) %{\n-  match(Set dst (LShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(300);\n-  format %{ \"MOV    $dst.hi,$dst.lo\\n\"\n-          \"\\tSHL    $dst.hi,$cnt-32\\n\"\n-          \"\\tXOR    $dst.lo,$dst.lo\" %}\n-  opcode(0xC1, 0x4);  \/* C1 \/4 ib *\/\n-  ins_encode( move_long_big_shift_clr(dst,cnt) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Left Long by variable\n-instruct salL_eReg_CL(eRegL dst, eCXRegI shift, eFlagsReg cr) %{\n-  match(Set dst (LShiftL dst shift));\n-  effect(KILL cr);\n-  ins_cost(500+200);\n-  size(17);\n-  format %{ \"TEST   $shift,32\\n\\t\"\n-            \"JEQ,s  small\\n\\t\"\n-            \"MOV    $dst.hi,$dst.lo\\n\\t\"\n-            \"XOR    $dst.lo,$dst.lo\\n\"\n-    \"small:\\tSHLD   $dst.hi,$dst.lo,$shift\\n\\t\"\n-            \"SHL    $dst.lo,$shift\" %}\n-  ins_encode( shift_left_long( dst, shift ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Shift Right Long by 1-31\n-instruct shrL_eReg_1_31(eRegL dst, immI_1_31 cnt, eFlagsReg cr) %{\n-  match(Set dst (URShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(200);\n-  format %{ \"SHRD   $dst.lo,$dst.hi,$cnt\\n\\t\"\n-            \"SHR    $dst.hi,$cnt\" %}\n-  opcode(0xC1, 0x5, 0xAC);  \/* 0F\/AC, then C1 \/5 ib *\/\n-  ins_encode( move_long_small_shift(dst,cnt) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Right Long by 32-63\n-instruct shrL_eReg_32_63(eRegL dst, immI_32_63 cnt, eFlagsReg cr) %{\n-  match(Set dst (URShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(300);\n-  format %{ \"MOV    $dst.lo,$dst.hi\\n\"\n-          \"\\tSHR    $dst.lo,$cnt-32\\n\"\n-          \"\\tXOR    $dst.hi,$dst.hi\" %}\n-  opcode(0xC1, 0x5);  \/* C1 \/5 ib *\/\n-  ins_encode( move_long_big_shift_clr(dst,cnt) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Right Long by variable\n-instruct shrL_eReg_CL(eRegL dst, eCXRegI shift, eFlagsReg cr) %{\n-  match(Set dst (URShiftL dst shift));\n-  effect(KILL cr);\n-  ins_cost(600);\n-  size(17);\n-  format %{ \"TEST   $shift,32\\n\\t\"\n-            \"JEQ,s  small\\n\\t\"\n-            \"MOV    $dst.lo,$dst.hi\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\\n\"\n-    \"small:\\tSHRD   $dst.lo,$dst.hi,$shift\\n\\t\"\n-            \"SHR    $dst.hi,$shift\" %}\n-  ins_encode( shift_right_long( dst, shift ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Shift Right Long by 1-31\n-instruct sarL_eReg_1_31(eRegL dst, immI_1_31 cnt, eFlagsReg cr) %{\n-  match(Set dst (RShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(200);\n-  format %{ \"SHRD   $dst.lo,$dst.hi,$cnt\\n\\t\"\n-            \"SAR    $dst.hi,$cnt\" %}\n-  opcode(0xC1, 0x7, 0xAC);  \/* 0F\/AC, then C1 \/7 ib *\/\n-  ins_encode( move_long_small_shift(dst,cnt) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Right Long by 32-63\n-instruct sarL_eReg_32_63( eRegL dst, immI_32_63 cnt, eFlagsReg cr) %{\n-  match(Set dst (RShiftL dst cnt));\n-  effect(KILL cr);\n-  ins_cost(300);\n-  format %{ \"MOV    $dst.lo,$dst.hi\\n\"\n-          \"\\tSAR    $dst.lo,$cnt-32\\n\"\n-          \"\\tSAR    $dst.hi,31\" %}\n-  opcode(0xC1, 0x7);  \/* C1 \/7 ib *\/\n-  ins_encode( move_long_big_shift_sign(dst,cnt) );\n-  ins_pipe( ialu_reg_long );\n-%}\n-\n-\/\/ Shift Right arithmetic Long by variable\n-instruct sarL_eReg_CL(eRegL dst, eCXRegI shift, eFlagsReg cr) %{\n-  match(Set dst (RShiftL dst shift));\n-  effect(KILL cr);\n-  ins_cost(600);\n-  size(18);\n-  format %{ \"TEST   $shift,32\\n\\t\"\n-            \"JEQ,s  small\\n\\t\"\n-            \"MOV    $dst.lo,$dst.hi\\n\\t\"\n-            \"SAR    $dst.hi,31\\n\"\n-    \"small:\\tSHRD   $dst.lo,$dst.hi,$shift\\n\\t\"\n-            \"SAR    $dst.hi,$shift\" %}\n-  ins_encode( shift_right_arith_long( dst, shift ) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/----------Double Instructions------------------------------------------------\n-\/\/ Double Math\n-\n-\/\/ Compare & branch\n-\n-\/\/ P6 version of float compare, sets condition codes in EFLAGS\n-instruct cmpDPR_cc_P6(eFlagsRegU cr, regDPR src1, regDPR src2, eAXRegI rax) %{\n-  predicate(VM_Version::supports_cmov() && UseSSE <=1);\n-  match(Set cr (CmpD src1 src2));\n-  effect(KILL rax);\n-  ins_cost(150);\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FUCOMIP ST,$src2  \/\/ P6 instruction\\n\\t\"\n-            \"JNP    exit\\n\\t\"\n-            \"MOV    ah,1       \/\/ saw a NaN, set CF\\n\\t\"\n-            \"SAHF\\n\"\n-     \"exit:\\tNOP               \/\/ avoid branch to branch\" %}\n-  opcode(0xDF, 0x05); \/* DF E8+i or DF \/5 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2),\n-              cmpF_P6_fixup );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpDPR_cc_P6CF(eFlagsRegUCF cr, regDPR src1, regDPR src2) %{\n-  predicate(VM_Version::supports_cmov() && UseSSE <=1);\n-  match(Set cr (CmpD src1 src2));\n-  ins_cost(150);\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FUCOMIP ST,$src2  \/\/ P6 instruction\" %}\n-  opcode(0xDF, 0x05); \/* DF E8+i or DF \/5 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare & branch\n-instruct cmpDPR_cc(eFlagsRegU cr, regDPR src1, regDPR src2, eAXRegI rax) %{\n-  predicate(UseSSE<=1);\n-  match(Set cr (CmpD src1 src2));\n-  effect(KILL rax);\n-  ins_cost(200);\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FCOMp  $src2\\n\\t\"\n-            \"FNSTSW AX\\n\\t\"\n-            \"TEST   AX,0x400\\n\\t\"\n-            \"JZ,s   flags\\n\\t\"\n-            \"MOV    AH,1\\t# unordered treat as LT\\n\"\n-    \"flags:\\tSAHF\" %}\n-  opcode(0xD8, 0x3); \/* D8 D8+i or D8 \/3 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2),\n-              fpu_flags);\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare vs zero into -1,0,1\n-instruct cmpDPR_0(rRegI dst, regDPR src1, immDPR0 zero, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (CmpD3 src1 zero));\n-  effect(KILL cr, KILL rax);\n-  ins_cost(280);\n-  format %{ \"FTSTD  $dst,$src1\" %}\n-  opcode(0xE4, 0xD9);\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcS, OpcP, PopFPU,\n-              CmpF_Result(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpDPR_reg(rRegI dst, regDPR src1, regDPR src2, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (CmpD3 src1 src2));\n-  effect(KILL cr, KILL rax);\n-  ins_cost(300);\n-  format %{ \"FCMPD  $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x3); \/* D8 D8+i or D8 \/3 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2),\n-              CmpF_Result(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ float compare and set condition codes in EFLAGS by XMM regs\n-instruct cmpD_cc(eFlagsRegU cr, regD src1, regD src2) %{\n-  predicate(UseSSE>=2);\n-  match(Set cr (CmpD src1 src2));\n-  ins_cost(145);\n-  format %{ \"UCOMISD $src1,$src2\\n\\t\"\n-            \"JNP,s   exit\\n\\t\"\n-            \"PUSHF\\t# saw NaN, set CF\\n\\t\"\n-            \"AND     [rsp], #0xffffff2b\\n\\t\"\n-            \"POPF\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp_fixup(masm);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpD_ccCF(eFlagsRegUCF cr, regD src1, regD src2) %{\n-  predicate(UseSSE>=2);\n-  match(Set cr (CmpD src1 src2));\n-  ins_cost(100);\n-  format %{ \"UCOMISD $src1,$src2\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ float compare and set condition codes in EFLAGS by XMM regs\n-instruct cmpD_ccmem(eFlagsRegU cr, regD src1, memory src2) %{\n-  predicate(UseSSE>=2);\n-  match(Set cr (CmpD src1 (LoadD src2)));\n-  ins_cost(145);\n-  format %{ \"UCOMISD $src1,$src2\\n\\t\"\n-            \"JNP,s   exit\\n\\t\"\n-            \"PUSHF\\t# saw NaN, set CF\\n\\t\"\n-            \"AND     [rsp], #0xffffff2b\\n\\t\"\n-            \"POPF\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp_fixup(masm);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpD_ccmemCF(eFlagsRegUCF cr, regD src1, memory src2) %{\n-  predicate(UseSSE>=2);\n-  match(Set cr (CmpD src1 (LoadD src2)));\n-  ins_cost(100);\n-  format %{ \"UCOMISD $src1,$src2\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare into -1,0,1 in XMM\n-instruct cmpD_reg(xRegI dst, regD src1, regD src2, eFlagsReg cr) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (CmpD3 src1 src2));\n-  effect(KILL cr);\n-  ins_cost(255);\n-  format %{ \"UCOMISD $src1, $src2\\n\\t\"\n-            \"MOV     $dst, #-1\\n\\t\"\n-            \"JP,s    done\\n\\t\"\n-            \"JB,s    done\\n\\t\"\n-            \"SETNE   $dst\\n\\t\"\n-            \"MOVZB   $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare into -1,0,1 in XMM and memory\n-instruct cmpD_regmem(xRegI dst, regD src1, memory src2, eFlagsReg cr) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (CmpD3 src1 (LoadD src2)));\n-  effect(KILL cr);\n-  ins_cost(275);\n-  format %{ \"UCOMISD $src1, $src2\\n\\t\"\n-            \"MOV     $dst, #-1\\n\\t\"\n-            \"JP,s    done\\n\\t\"\n-            \"JB,s    done\\n\\t\"\n-            \"SETNE   $dst\\n\\t\"\n-            \"MOVZB   $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-instruct subDPR_reg(regDPR dst, regDPR src) %{\n-  predicate (UseSSE <=1);\n-  match(Set dst (SubD dst src));\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"DSUBp  $dst,ST\" %}\n-  opcode(0xDE, 0x5); \/* DE E8+i  or DE \/5 *\/\n-  ins_cost(150);\n-  ins_encode( Push_Reg_DPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct subDPR_reg_round(stackSlotD dst, regDPR src1, regDPR src2) %{\n-  predicate (UseSSE <=1);\n-  match(Set dst (RoundDouble (SubD src1 src2)));\n-  ins_cost(250);\n-\n-  format %{ \"FLD    $src2\\n\\t\"\n-            \"DSUB   ST,$src1\\n\\t\"\n-            \"FSTP_D $dst\\t# D-round\" %}\n-  opcode(0xD8, 0x5);\n-  ins_encode( Push_Reg_DPR(src2),\n-              OpcP, RegOpc(src1), Pop_Mem_DPR(dst) );\n-  ins_pipe( fpu_mem_reg_reg );\n-%}\n-\n-\n-instruct subDPR_reg_mem(regDPR dst, memory src) %{\n-  predicate (UseSSE <=1);\n-  match(Set dst (SubD dst (LoadD src)));\n-  ins_cost(150);\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"DSUBp  $dst,ST\" %}\n-  opcode(0xDE, 0x5, 0xDD); \/* DE C0+i *\/  \/* LoadD  DD \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src),\n-              OpcP, RegOpc(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-instruct absDPR_reg(regDPR1 dst, regDPR1 src) %{\n-  predicate (UseSSE<=1);\n-  match(Set dst (AbsD src));\n-  ins_cost(100);\n-  format %{ \"FABS\" %}\n-  opcode(0xE1, 0xD9);\n-  ins_encode( OpcS, OpcP );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct negDPR_reg(regDPR1 dst, regDPR1 src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (NegD src));\n-  ins_cost(100);\n-  format %{ \"FCHS\" %}\n-  opcode(0xE0, 0xD9);\n-  ins_encode( OpcS, OpcP );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct addDPR_reg(regDPR dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (AddD dst src));\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"DADD   $dst,ST\" %}\n-  size(4);\n-  ins_cost(150);\n-  opcode(0xDE, 0x0); \/* DE C0+i or DE \/0*\/\n-  ins_encode( Push_Reg_DPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\n-instruct addDPR_reg_round(stackSlotD dst, regDPR src1, regDPR src2) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (RoundDouble (AddD src1 src2)));\n-  ins_cost(250);\n-\n-  format %{ \"FLD    $src2\\n\\t\"\n-            \"DADD   ST,$src1\\n\\t\"\n-            \"FSTP_D $dst\\t# D-round\" %}\n-  opcode(0xD8, 0x0); \/* D8 C0+i or D8 \/0*\/\n-  ins_encode( Push_Reg_DPR(src2),\n-              OpcP, RegOpc(src1), Pop_Mem_DPR(dst) );\n-  ins_pipe( fpu_mem_reg_reg );\n-%}\n-\n-\n-instruct addDPR_reg_mem(regDPR dst, memory src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (AddD dst (LoadD src)));\n-  ins_cost(150);\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"DADDp  $dst,ST\" %}\n-  opcode(0xDE, 0x0, 0xDD); \/* DE C0+i *\/  \/* LoadD  DD \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src),\n-              OpcP, RegOpc(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ add-to-memory\n-instruct addDPR_mem_reg(memory dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (StoreD dst (RoundDouble (AddD (LoadD dst) src))));\n-  ins_cost(150);\n-\n-  format %{ \"FLD_D  $dst\\n\\t\"\n-            \"DADD   ST,$src\\n\\t\"\n-            \"FST_D  $dst\" %}\n-  opcode(0xDD, 0x0);\n-  ins_encode( SetInstMark, Opcode(0xDD), RMopc_Mem(0x00,dst),\n-              Opcode(0xD8), RegOpc(src), ClearInstMark,\n-              SetInstMark,\n-              Opcode(0xDD), RMopc_Mem(0x03,dst),\n-              ClearInstMark);\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-instruct addDPR_reg_imm1(regDPR dst, immDPR1 con) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (AddD dst con));\n-  ins_cost(125);\n-  format %{ \"FLD1\\n\\t\"\n-            \"DADDp  $dst,ST\" %}\n-  ins_encode %{\n-    __ fld1();\n-    __ faddp($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg);\n-%}\n-\n-instruct addDPR_reg_imm(regDPR dst, immDPR con) %{\n-  predicate(UseSSE<=1 && _kids[1]->_leaf->getd() != 0.0 && _kids[1]->_leaf->getd() != 1.0 );\n-  match(Set dst (AddD dst con));\n-  ins_cost(200);\n-  format %{ \"FLD_D  [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"DADDp  $dst,ST\" %}\n-  ins_encode %{\n-    __ fld_d($constantaddress($con));\n-    __ faddp($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_mem);\n-%}\n-\n-instruct addDPR_reg_imm_round(stackSlotD dst, regDPR src, immDPR con) %{\n-  predicate(UseSSE<=1 && _kids[0]->_kids[1]->_leaf->getd() != 0.0 && _kids[0]->_kids[1]->_leaf->getd() != 1.0 );\n-  match(Set dst (RoundDouble (AddD src con)));\n-  ins_cost(200);\n-  format %{ \"FLD_D  [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"DADD   ST,$src\\n\\t\"\n-            \"FSTP_D $dst\\t# D-round\" %}\n-  ins_encode %{\n-    __ fld_d($constantaddress($con));\n-    __ fadd($src$$reg);\n-    __ fstp_d(Address(rsp, $dst$$disp));\n-  %}\n-  ins_pipe(fpu_mem_reg_con);\n-%}\n-\n-instruct mulDPR_reg(regDPR dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (MulD dst src));\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"DMULp  $dst,ST\" %}\n-  opcode(0xDE, 0x1); \/* DE C8+i or DE \/1*\/\n-  ins_cost(150);\n-  ins_encode( Push_Reg_DPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Strict FP instruction biases argument before multiply then\n-\/\/ biases result to avoid double rounding of subnormals.\n-\/\/\n-\/\/ scale arg1 by multiplying arg1 by 2^(-15360)\n-\/\/ load arg2\n-\/\/ multiply scaled arg1 by arg2\n-\/\/ rescale product by 2^(15360)\n-\/\/\n-instruct strictfp_mulDPR_reg(regDPR1 dst, regnotDPR1 src) %{\n-  predicate( UseSSE<=1 && Compile::current()->has_method() );\n-  match(Set dst (MulD dst src));\n-  ins_cost(1);   \/\/ Select this instruction for all FP double multiplies\n-\n-  format %{ \"FLD    StubRoutines::x86::_fpu_subnormal_bias1\\n\\t\"\n-            \"DMULp  $dst,ST\\n\\t\"\n-            \"FLD    $src\\n\\t\"\n-            \"DMULp  $dst,ST\\n\\t\"\n-            \"FLD    StubRoutines::x86::_fpu_subnormal_bias2\\n\\t\"\n-            \"DMULp  $dst,ST\\n\\t\" %}\n-  opcode(0xDE, 0x1); \/* DE C8+i or DE \/1*\/\n-  ins_encode( strictfp_bias1(dst),\n-              Push_Reg_DPR(src),\n-              OpcP, RegOpc(dst),\n-              strictfp_bias2(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct mulDPR_reg_imm(regDPR dst, immDPR con) %{\n-  predicate( UseSSE<=1 && _kids[1]->_leaf->getd() != 0.0 && _kids[1]->_leaf->getd() != 1.0 );\n-  match(Set dst (MulD dst con));\n-  ins_cost(200);\n-  format %{ \"FLD_D  [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"DMULp  $dst,ST\" %}\n-  ins_encode %{\n-    __ fld_d($constantaddress($con));\n-    __ fmulp($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_mem);\n-%}\n-\n-\n-instruct mulDPR_reg_mem(regDPR dst, memory src) %{\n-  predicate( UseSSE<=1 );\n-  match(Set dst (MulD dst (LoadD src)));\n-  ins_cost(200);\n-  format %{ \"FLD_D  $src\\n\\t\"\n-            \"DMULp  $dst,ST\" %}\n-  opcode(0xDE, 0x1, 0xDD); \/* DE C8+i or DE \/1*\/  \/* LoadD  DD \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src),\n-              OpcP, RegOpc(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/\n-\/\/ Cisc-alternate to reg-reg multiply\n-instruct mulDPR_reg_mem_cisc(regDPR dst, regDPR src, memory mem) %{\n-  predicate( UseSSE<=1 );\n-  match(Set dst (MulD src (LoadD mem)));\n-  ins_cost(250);\n-  format %{ \"FLD_D  $mem\\n\\t\"\n-            \"DMUL   ST,$src\\n\\t\"\n-            \"FSTP_D $dst\" %}\n-  opcode(0xD8, 0x1, 0xD9); \/* D8 C8+i *\/  \/* LoadD D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,mem),\n-              OpcReg_FPR(src),\n-              Pop_Reg_DPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_reg_mem );\n-%}\n-\n-\n-\/\/ MACRO3 -- addDPR a mulDPR\n-\/\/ This instruction is a '2-address' instruction in that the result goes\n-\/\/ back to src2.  This eliminates a move from the macro; possibly the\n-\/\/ register allocator will have to add it back (and maybe not).\n-instruct addDPR_mulDPR_reg(regDPR src2, regDPR src1, regDPR src0) %{\n-  predicate( UseSSE<=1 );\n-  match(Set src2 (AddD (MulD src0 src1) src2));\n-  format %{ \"FLD    $src0\\t# ===MACRO3d===\\n\\t\"\n-            \"DMUL   ST,$src1\\n\\t\"\n-            \"DADDp  $src2,ST\" %}\n-  ins_cost(250);\n-  opcode(0xDD); \/* LoadD DD \/0 *\/\n-  ins_encode( Push_Reg_FPR(src0),\n-              FMul_ST_reg(src1),\n-              FAddP_reg_ST(src2) );\n-  ins_pipe( fpu_reg_reg_reg );\n-%}\n-\n-\n-\/\/ MACRO3 -- subDPR a mulDPR\n-instruct subDPR_mulDPR_reg(regDPR src2, regDPR src1, regDPR src0) %{\n-  predicate( UseSSE<=1 );\n-  match(Set src2 (SubD (MulD src0 src1) src2));\n-  format %{ \"FLD    $src0\\t# ===MACRO3d===\\n\\t\"\n-            \"DMUL   ST,$src1\\n\\t\"\n-            \"DSUBRp $src2,ST\" %}\n-  ins_cost(250);\n-  ins_encode( Push_Reg_FPR(src0),\n-              FMul_ST_reg(src1),\n-              Opcode(0xDE), Opc_plus(0xE0,src2));\n-  ins_pipe( fpu_reg_reg_reg );\n-%}\n-\n-\n-instruct divDPR_reg(regDPR dst, regDPR src) %{\n-  predicate( UseSSE<=1 );\n-  match(Set dst (DivD dst src));\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"FDIVp  $dst,ST\" %}\n-  opcode(0xDE, 0x7); \/* DE F8+i or DE \/7*\/\n-  ins_cost(150);\n-  ins_encode( Push_Reg_DPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Strict FP instruction biases argument before division then\n-\/\/ biases result, to avoid double rounding of subnormals.\n-\/\/\n-\/\/ scale dividend by multiplying dividend by 2^(-15360)\n-\/\/ load divisor\n-\/\/ divide scaled dividend by divisor\n-\/\/ rescale quotient by 2^(15360)\n-\/\/\n-instruct strictfp_divDPR_reg(regDPR1 dst, regnotDPR1 src) %{\n-  predicate (UseSSE<=1);\n-  match(Set dst (DivD dst src));\n-  predicate( UseSSE<=1 && Compile::current()->has_method() );\n-  ins_cost(01);\n-\n-  format %{ \"FLD    StubRoutines::x86::_fpu_subnormal_bias1\\n\\t\"\n-            \"DMULp  $dst,ST\\n\\t\"\n-            \"FLD    $src\\n\\t\"\n-            \"FDIVp  $dst,ST\\n\\t\"\n-            \"FLD    StubRoutines::x86::_fpu_subnormal_bias2\\n\\t\"\n-            \"DMULp  $dst,ST\\n\\t\" %}\n-  opcode(0xDE, 0x7); \/* DE F8+i or DE \/7*\/\n-  ins_encode( strictfp_bias1(dst),\n-              Push_Reg_DPR(src),\n-              OpcP, RegOpc(dst),\n-              strictfp_bias2(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct modDPR_reg(regDPR dst, regDPR src, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (ModD dst src));\n-  effect(KILL rax, KILL cr); \/\/ emitModDPR() uses EAX and EFLAGS\n-\n-  format %{ \"DMOD   $dst,$src\" %}\n-  ins_cost(250);\n-  ins_encode(Push_Reg_Mod_DPR(dst, src),\n-              emitModDPR(),\n-              Push_Result_Mod_DPR(src),\n-              Pop_Reg_DPR(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct modD_reg(regD dst, regD src0, regD src1, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (ModD src0 src1));\n-  effect(KILL rax, KILL cr);\n-\n-  format %{ \"SUB    ESP,8\\t # DMOD\\n\"\n-          \"\\tMOVSD  [ESP+0],$src1\\n\"\n-          \"\\tFLD_D  [ESP+0]\\n\"\n-          \"\\tMOVSD  [ESP+0],$src0\\n\"\n-          \"\\tFLD_D  [ESP+0]\\n\"\n-     \"loop:\\tFPREM\\n\"\n-          \"\\tFWAIT\\n\"\n-          \"\\tFNSTSW AX\\n\"\n-          \"\\tSAHF\\n\"\n-          \"\\tJP     loop\\n\"\n-          \"\\tFSTP_D [ESP+0]\\n\"\n-          \"\\tMOVSD  $dst,[ESP+0]\\n\"\n-          \"\\tADD    ESP,8\\n\"\n-          \"\\tFSTP   ST0\\t # Restore FPU Stack\"\n-    %}\n-  ins_cost(250);\n-  ins_encode( Push_ModD_encoding(src0, src1), emitModDPR(), Push_ResultD(dst), PopFPU);\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct atanDPR_reg(regDPR dst, regDPR src) %{\n-  predicate (UseSSE<=1);\n-  match(Set dst(AtanD dst src));\n-  format %{ \"DATA   $dst,$src\" %}\n-  opcode(0xD9, 0xF3);\n-  ins_encode( Push_Reg_DPR(src),\n-              OpcP, OpcS, RegOpc(dst) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct atanD_reg(regD dst, regD src, eFlagsReg cr) %{\n-  predicate (UseSSE>=2);\n-  match(Set dst(AtanD dst src));\n-  effect(KILL cr); \/\/ Push_{Src|Result}D() uses \"{SUB|ADD} ESP,8\"\n-  format %{ \"DATA   $dst,$src\" %}\n-  opcode(0xD9, 0xF3);\n-  ins_encode( Push_SrcD(src),\n-              OpcP, OpcS, Push_ResultD(dst) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct sqrtDPR_reg(regDPR dst, regDPR src) %{\n-  predicate (UseSSE<=1);\n-  match(Set dst (SqrtD src));\n-  format %{ \"DSQRT  $dst,$src\" %}\n-  opcode(0xFA, 0xD9);\n-  ins_encode( Push_Reg_DPR(src),\n-              OpcS, OpcP, Pop_Reg_DPR(dst) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/-------------Float Instructions-------------------------------\n-\/\/ Float Math\n-\n-\/\/ Code for float compare:\n-\/\/     fcompp();\n-\/\/     fwait(); fnstsw_ax();\n-\/\/     sahf();\n-\/\/     movl(dst, unordered_result);\n-\/\/     jcc(Assembler::parity, exit);\n-\/\/     movl(dst, less_result);\n-\/\/     jcc(Assembler::below, exit);\n-\/\/     movl(dst, equal_result);\n-\/\/     jcc(Assembler::equal, exit);\n-\/\/     movl(dst, greater_result);\n-\/\/   exit:\n-\n-\/\/ P6 version of float compare, sets condition codes in EFLAGS\n-instruct cmpFPR_cc_P6(eFlagsRegU cr, regFPR src1, regFPR src2, eAXRegI rax) %{\n-  predicate(VM_Version::supports_cmov() && UseSSE == 0);\n-  match(Set cr (CmpF src1 src2));\n-  effect(KILL rax);\n-  ins_cost(150);\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FUCOMIP ST,$src2  \/\/ P6 instruction\\n\\t\"\n-            \"JNP    exit\\n\\t\"\n-            \"MOV    ah,1       \/\/ saw a NaN, set CF (treat as LT)\\n\\t\"\n-            \"SAHF\\n\"\n-     \"exit:\\tNOP               \/\/ avoid branch to branch\" %}\n-  opcode(0xDF, 0x05); \/* DF E8+i or DF \/5 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2),\n-              cmpF_P6_fixup );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpFPR_cc_P6CF(eFlagsRegUCF cr, regFPR src1, regFPR src2) %{\n-  predicate(VM_Version::supports_cmov() && UseSSE == 0);\n-  match(Set cr (CmpF src1 src2));\n-  ins_cost(100);\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FUCOMIP ST,$src2  \/\/ P6 instruction\" %}\n-  opcode(0xDF, 0x05); \/* DF E8+i or DF \/5 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ Compare & branch\n-instruct cmpFPR_cc(eFlagsRegU cr, regFPR src1, regFPR src2, eAXRegI rax) %{\n-  predicate(UseSSE == 0);\n-  match(Set cr (CmpF src1 src2));\n-  effect(KILL rax);\n-  ins_cost(200);\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FCOMp  $src2\\n\\t\"\n-            \"FNSTSW AX\\n\\t\"\n-            \"TEST   AX,0x400\\n\\t\"\n-            \"JZ,s   flags\\n\\t\"\n-            \"MOV    AH,1\\t# unordered treat as LT\\n\"\n-    \"flags:\\tSAHF\" %}\n-  opcode(0xD8, 0x3); \/* D8 D8+i or D8 \/3 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2),\n-              fpu_flags);\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare vs zero into -1,0,1\n-instruct cmpFPR_0(rRegI dst, regFPR src1, immFPR0 zero, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE == 0);\n-  match(Set dst (CmpF3 src1 zero));\n-  effect(KILL cr, KILL rax);\n-  ins_cost(280);\n-  format %{ \"FTSTF  $dst,$src1\" %}\n-  opcode(0xE4, 0xD9);\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcS, OpcP, PopFPU,\n-              CmpF_Result(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare into -1,0,1\n-instruct cmpFPR_reg(rRegI dst, regFPR src1, regFPR src2, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE == 0);\n-  match(Set dst (CmpF3 src1 src2));\n-  effect(KILL cr, KILL rax);\n-  ins_cost(300);\n-  format %{ \"FCMPF  $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x3); \/* D8 D8+i or D8 \/3 *\/\n-  ins_encode( Push_Reg_DPR(src1),\n-              OpcP, RegOpc(src2),\n-              CmpF_Result(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ float compare and set condition codes in EFLAGS by XMM regs\n-instruct cmpF_cc(eFlagsRegU cr, regF src1, regF src2) %{\n-  predicate(UseSSE>=1);\n-  match(Set cr (CmpF src1 src2));\n-  ins_cost(145);\n-  format %{ \"UCOMISS $src1,$src2\\n\\t\"\n-            \"JNP,s   exit\\n\\t\"\n-            \"PUSHF\\t# saw NaN, set CF\\n\\t\"\n-            \"AND     [rsp], #0xffffff2b\\n\\t\"\n-            \"POPF\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp_fixup(masm);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpF_ccCF(eFlagsRegUCF cr, regF src1, regF src2) %{\n-  predicate(UseSSE>=1);\n-  match(Set cr (CmpF src1 src2));\n-  ins_cost(100);\n-  format %{ \"UCOMISS $src1,$src2\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ float compare and set condition codes in EFLAGS by XMM regs\n-instruct cmpF_ccmem(eFlagsRegU cr, regF src1, memory src2) %{\n-  predicate(UseSSE>=1);\n-  match(Set cr (CmpF src1 (LoadF src2)));\n-  ins_cost(165);\n-  format %{ \"UCOMISS $src1,$src2\\n\\t\"\n-            \"JNP,s   exit\\n\\t\"\n-            \"PUSHF\\t# saw NaN, set CF\\n\\t\"\n-            \"AND     [rsp], #0xffffff2b\\n\\t\"\n-            \"POPF\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp_fixup(masm);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct cmpF_ccmemCF(eFlagsRegUCF cr, regF src1, memory src2) %{\n-  predicate(UseSSE>=1);\n-  match(Set cr (CmpF src1 (LoadF src2)));\n-  ins_cost(100);\n-  format %{ \"UCOMISS $src1,$src2\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare into -1,0,1 in XMM\n-instruct cmpF_reg(xRegI dst, regF src1, regF src2, eFlagsReg cr) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (CmpF3 src1 src2));\n-  effect(KILL cr);\n-  ins_cost(255);\n-  format %{ \"UCOMISS $src1, $src2\\n\\t\"\n-            \"MOV     $dst, #-1\\n\\t\"\n-            \"JP,s    done\\n\\t\"\n-            \"JB,s    done\\n\\t\"\n-            \"SETNE   $dst\\n\\t\"\n-            \"MOVZB   $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Compare into -1,0,1 in XMM and memory\n-instruct cmpF_regmem(xRegI dst, regF src1, memory src2, eFlagsReg cr) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (CmpF3 src1 (LoadF src2)));\n-  effect(KILL cr);\n-  ins_cost(275);\n-  format %{ \"UCOMISS $src1, $src2\\n\\t\"\n-            \"MOV     $dst, #-1\\n\\t\"\n-            \"JP,s    done\\n\\t\"\n-            \"JB,s    done\\n\\t\"\n-            \"SETNE   $dst\\n\\t\"\n-            \"MOVZB   $dst, $dst\\n\"\n-    \"done:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp3(masm, $dst$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct subFPR24_reg(stackSlotF dst, regFPR src1, regFPR src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (SubF src1 src2));\n-\n-  format %{ \"FSUB   $dst,$src1 - $src2\" %}\n-  opcode(0xD8, 0x4); \/* D8 E0+i or D8 \/4 mod==0x3 ;; result in TOS *\/\n-  ins_encode( Push_Reg_FPR(src1),\n-              OpcReg_FPR(src2),\n-              Pop_Mem_FPR(dst) );\n-  ins_pipe( fpu_mem_reg_reg );\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct subFPR_reg(regFPR dst, regFPR src) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (SubF dst src));\n-\n-  format %{ \"FSUB   $dst,$src\" %}\n-  opcode(0xDE, 0x5); \/* DE E8+i  or DE \/5 *\/\n-  ins_encode( Push_Reg_FPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct addFPR24_reg(stackSlotF dst, regFPR src1, regFPR src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src1 src2));\n-\n-  format %{ \"FADD   $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x0); \/* D8 C0+i *\/\n-  ins_encode( Push_Reg_FPR(src2),\n-              OpcReg_FPR(src1),\n-              Pop_Mem_FPR(dst) );\n-  ins_pipe( fpu_mem_reg_reg );\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct addFPR_reg(regFPR dst, regFPR src) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF dst src));\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"FADDp  $dst,ST\" %}\n-  opcode(0xDE, 0x0); \/* DE C0+i or DE \/0*\/\n-  ins_encode( Push_Reg_FPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct absFPR_reg(regFPR1 dst, regFPR1 src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (AbsF src));\n-  ins_cost(100);\n-  format %{ \"FABS\" %}\n-  opcode(0xE1, 0xD9);\n-  ins_encode( OpcS, OpcP );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct negFPR_reg(regFPR1 dst, regFPR1 src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (NegF src));\n-  ins_cost(100);\n-  format %{ \"FCHS\" %}\n-  opcode(0xE0, 0xD9);\n-  ins_encode( OpcS, OpcP );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\/\/ Cisc-alternate to addFPR_reg\n-\/\/ Spill to obtain 24-bit precision\n-instruct addFPR24_reg_mem(stackSlotF dst, regFPR src1, memory src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src1 (LoadF src2)));\n-\n-  format %{ \"FLD    $src2\\n\\t\"\n-            \"FADD   ST,$src1\\n\\t\"\n-            \"FSTP_S $dst\" %}\n-  opcode(0xD8, 0x0, 0xD9); \/* D8 C0+i *\/  \/* LoadF  D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src2),\n-              OpcReg_FPR(src1),\n-              Pop_Mem_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_mem_reg_mem );\n-%}\n-\/\/\n-\/\/ Cisc-alternate to addFPR_reg\n-\/\/ This instruction does not round to 24-bits\n-instruct addFPR_reg_mem(regFPR dst, memory src) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF dst (LoadF src)));\n-\n-  format %{ \"FADD   $dst,$src\" %}\n-  opcode(0xDE, 0x0, 0xD9); \/* DE C0+i or DE \/0*\/  \/* LoadF  D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src),\n-              OpcP, RegOpc(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ \/\/ Following two instructions for _222_mpegaudio\n-\/\/ Spill to obtain 24-bit precision\n-instruct addFPR24_mem_reg(stackSlotF dst, regFPR src2, memory src1 ) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src1 src2));\n-\n-  format %{ \"FADD   $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x0, 0xD9); \/* D8 C0+i *\/  \/* LoadF  D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src1),\n-              OpcReg_FPR(src2),\n-              Pop_Mem_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_mem_reg_mem );\n-%}\n-\n-\/\/ Cisc-spill variant\n-\/\/ Spill to obtain 24-bit precision\n-instruct addFPR24_mem_cisc(stackSlotF dst, memory src1, memory src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src1 (LoadF src2)));\n-\n-  format %{ \"FADD   $dst,$src1,$src2 cisc\" %}\n-  opcode(0xD8, 0x0, 0xD9); \/* D8 C0+i *\/  \/* LoadF  D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src2),\n-              OpcP, RMopc_Mem(secondary,src1),\n-              Pop_Mem_FPR(dst),\n-              ClearInstMark);\n-  ins_pipe( fpu_mem_mem_mem );\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct addFPR24_mem_mem(stackSlotF dst, memory src1, memory src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src1 src2));\n-\n-  format %{ \"FADD   $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x0, 0xD9); \/* D8 \/0 *\/  \/* LoadF  D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src2),\n-              OpcP, RMopc_Mem(secondary,src1),\n-              Pop_Mem_FPR(dst),\n-              ClearInstMark);\n-  ins_pipe( fpu_mem_mem_mem );\n-%}\n-\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct addFPR24_reg_imm(stackSlotF dst, regFPR src, immFPR con) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src con));\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"FADD_S [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"FSTP_S $dst\"  %}\n-  ins_encode %{\n-    __ fld_s($src$$reg - 1);  \/\/ FLD ST(i-1)\n-    __ fadd_s($constantaddress($con));\n-    __ fstp_s(Address(rsp, $dst$$disp));\n-  %}\n-  ins_pipe(fpu_mem_reg_con);\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct addFPR_reg_imm(regFPR dst, regFPR src, immFPR con) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF src con));\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"FADD_S [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"FSTP   $dst\"  %}\n-  ins_encode %{\n-    __ fld_s($src$$reg - 1);  \/\/ FLD ST(i-1)\n-    __ fadd_s($constantaddress($con));\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_reg_con);\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct mulFPR24_reg(stackSlotF dst, regFPR src1, regFPR src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src1 src2));\n-\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FMUL   $src2\\n\\t\"\n-            \"FSTP_S $dst\"  %}\n-  opcode(0xD8, 0x1); \/* D8 C8+i or D8 \/1 ;; result in TOS *\/\n-  ins_encode( Push_Reg_FPR(src1),\n-              OpcReg_FPR(src2),\n-              Pop_Mem_FPR(dst) );\n-  ins_pipe( fpu_mem_reg_reg );\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct mulFPR_reg(regFPR dst, regFPR src1, regFPR src2) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src1 src2));\n-\n-  format %{ \"FLD    $src1\\n\\t\"\n-            \"FMUL   $src2\\n\\t\"\n-            \"FSTP_S $dst\"  %}\n-  opcode(0xD8, 0x1); \/* D8 C8+i *\/\n-  ins_encode( Push_Reg_FPR(src2),\n-              OpcReg_FPR(src1),\n-              Pop_Reg_FPR(dst) );\n-  ins_pipe( fpu_reg_reg_reg );\n-%}\n-\n-\n-\/\/ Spill to obtain 24-bit precision\n-\/\/ Cisc-alternate to reg-reg multiply\n-instruct mulFPR24_reg_mem(stackSlotF dst, regFPR src1, memory src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src1 (LoadF src2)));\n-\n-  format %{ \"FLD_S  $src2\\n\\t\"\n-            \"FMUL   $src1\\n\\t\"\n-            \"FSTP_S $dst\"  %}\n-  opcode(0xD8, 0x1, 0xD9); \/* D8 C8+i or DE \/1*\/  \/* LoadF D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src2),\n-              OpcReg_FPR(src1),\n-              Pop_Mem_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_mem_reg_mem );\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-\/\/ Cisc-alternate to reg-reg multiply\n-instruct mulFPR_reg_mem(regFPR dst, regFPR src1, memory src2) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src1 (LoadF src2)));\n-\n-  format %{ \"FMUL   $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x1, 0xD9); \/* D8 C8+i *\/  \/* LoadF D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src2),\n-              OpcReg_FPR(src1),\n-              Pop_Reg_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_reg_mem );\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct mulFPR24_mem_mem(stackSlotF dst, memory src1, memory src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src1 src2));\n-\n-  format %{ \"FMUL   $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x1, 0xD9); \/* D8 \/1 *\/  \/* LoadF D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,src2),\n-              OpcP, RMopc_Mem(secondary,src1),\n-              Pop_Mem_FPR(dst),\n-              ClearInstMark );\n-  ins_pipe( fpu_mem_mem_mem );\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct mulFPR24_reg_imm(stackSlotF dst, regFPR src, immFPR con) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src con));\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"FMUL_S [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"FSTP_S $dst\"  %}\n-  ins_encode %{\n-    __ fld_s($src$$reg - 1);  \/\/ FLD ST(i-1)\n-    __ fmul_s($constantaddress($con));\n-    __ fstp_s(Address(rsp, $dst$$disp));\n-  %}\n-  ins_pipe(fpu_mem_reg_con);\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct mulFPR_reg_imm(regFPR dst, regFPR src, immFPR con) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF src con));\n-\n-  format %{ \"FLD    $src\\n\\t\"\n-            \"FMUL_S [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"FSTP   $dst\"  %}\n-  ins_encode %{\n-    __ fld_s($src$$reg - 1);  \/\/ FLD ST(i-1)\n-    __ fmul_s($constantaddress($con));\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe(fpu_reg_reg_con);\n-%}\n-\n-\n-\/\/\n-\/\/ MACRO1 -- subsume unshared load into mulFPR\n-\/\/ This instruction does not round to 24-bits\n-instruct mulFPR_reg_load1(regFPR dst, regFPR src, memory mem1 ) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (MulF (LoadF mem1) src));\n-\n-  format %{ \"FLD    $mem1    ===MACRO1===\\n\\t\"\n-            \"FMUL   ST,$src\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xD8, 0x1, 0xD9); \/* D8 C8+i or D8 \/1 *\/  \/* LoadF D9 \/0 *\/\n-  ins_encode( SetInstMark, Opcode(tertiary), RMopc_Mem(0x00,mem1),\n-              OpcReg_FPR(src),\n-              Pop_Reg_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_reg_mem );\n-%}\n-\/\/\n-\/\/ MACRO2 -- addFPR a mulFPR which subsumed an unshared load\n-\/\/ This instruction does not round to 24-bits\n-instruct addFPR_mulFPR_reg_load1(regFPR dst, memory mem1, regFPR src1, regFPR src2) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (AddF (MulF (LoadF mem1) src1) src2));\n-  ins_cost(95);\n-\n-  format %{ \"FLD    $mem1     ===MACRO2===\\n\\t\"\n-            \"FMUL   ST,$src1  subsume mulFPR left load\\n\\t\"\n-            \"FADD   ST,$src2\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xD9); \/* LoadF D9 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem1),\n-              FMul_ST_reg(src1),\n-              FAdd_ST_reg(src2),\n-              Pop_Reg_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem_reg_reg );\n-%}\n-\n-\/\/ MACRO3 -- addFPR a mulFPR\n-\/\/ This instruction does not round to 24-bits.  It is a '2-address'\n-\/\/ instruction in that the result goes back to src2.  This eliminates\n-\/\/ a move from the macro; possibly the register allocator will have\n-\/\/ to add it back (and maybe not).\n-instruct addFPR_mulFPR_reg(regFPR src2, regFPR src1, regFPR src0) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set src2 (AddF (MulF src0 src1) src2));\n-\n-  format %{ \"FLD    $src0     ===MACRO3===\\n\\t\"\n-            \"FMUL   ST,$src1\\n\\t\"\n-            \"FADDP  $src2,ST\" %}\n-  opcode(0xD9); \/* LoadF D9 \/0 *\/\n-  ins_encode( Push_Reg_FPR(src0),\n-              FMul_ST_reg(src1),\n-              FAddP_reg_ST(src2) );\n-  ins_pipe( fpu_reg_reg_reg );\n-%}\n-\n-\/\/ MACRO4 -- divFPR subFPR\n-\/\/ This instruction does not round to 24-bits\n-instruct subFPR_divFPR_reg(regFPR dst, regFPR src1, regFPR src2, regFPR src3) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (DivF (SubF src2 src1) src3));\n-\n-  format %{ \"FLD    $src2   ===MACRO4===\\n\\t\"\n-            \"FSUB   ST,$src1\\n\\t\"\n-            \"FDIV   ST,$src3\\n\\t\"\n-            \"FSTP  $dst\" %}\n-  opcode(0xDE, 0x7); \/* DE F8+i or DE \/7*\/\n-  ins_encode( Push_Reg_FPR(src2),\n-              subFPR_divFPR_encode(src1,src3),\n-              Pop_Reg_FPR(dst) );\n-  ins_pipe( fpu_reg_reg_reg_reg );\n-%}\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct divFPR24_reg(stackSlotF dst, regFPR src1, regFPR src2) %{\n-  predicate(UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (DivF src1 src2));\n-\n-  format %{ \"FDIV   $dst,$src1,$src2\" %}\n-  opcode(0xD8, 0x6); \/* D8 F0+i or DE \/6*\/\n-  ins_encode( Push_Reg_FPR(src1),\n-              OpcReg_FPR(src2),\n-              Pop_Mem_FPR(dst) );\n-  ins_pipe( fpu_mem_reg_reg );\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct divFPR_reg(regFPR dst, regFPR src) %{\n-  predicate(UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (DivF dst src));\n-\n-  format %{ \"FDIV   $dst,$src\" %}\n-  opcode(0xDE, 0x7); \/* DE F8+i or DE \/7*\/\n-  ins_encode( Push_Reg_FPR(src),\n-              OpcP, RegOpc(dst) );\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-\n-\/\/ Spill to obtain 24-bit precision\n-instruct modFPR24_reg(stackSlotF dst, regFPR src1, regFPR src2, eAXRegI rax, eFlagsReg cr) %{\n-  predicate( UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (ModF src1 src2));\n-  effect(KILL rax, KILL cr); \/\/ emitModDPR() uses EAX and EFLAGS\n-\n-  format %{ \"FMOD   $dst,$src1,$src2\" %}\n-  ins_encode( Push_Reg_Mod_DPR(src1, src2),\n-              emitModDPR(),\n-              Push_Result_Mod_DPR(src2),\n-              Pop_Mem_FPR(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\/\/\n-\/\/ This instruction does not round to 24-bits\n-instruct modFPR_reg(regFPR dst, regFPR src, eAXRegI rax, eFlagsReg cr) %{\n-  predicate( UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (ModF dst src));\n-  effect(KILL rax, KILL cr); \/\/ emitModDPR() uses EAX and EFLAGS\n-\n-  format %{ \"FMOD   $dst,$src\" %}\n-  ins_encode(Push_Reg_Mod_DPR(dst, src),\n-              emitModDPR(),\n-              Push_Result_Mod_DPR(src),\n-              Pop_Reg_FPR(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct modF_reg(regF dst, regF src0, regF src1, eAXRegI rax, eFlagsReg cr) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (ModF src0 src1));\n-  effect(KILL rax, KILL cr);\n-  format %{ \"SUB    ESP,4\\t # FMOD\\n\"\n-          \"\\tMOVSS  [ESP+0],$src1\\n\"\n-          \"\\tFLD_S  [ESP+0]\\n\"\n-          \"\\tMOVSS  [ESP+0],$src0\\n\"\n-          \"\\tFLD_S  [ESP+0]\\n\"\n-     \"loop:\\tFPREM\\n\"\n-          \"\\tFWAIT\\n\"\n-          \"\\tFNSTSW AX\\n\"\n-          \"\\tSAHF\\n\"\n-          \"\\tJP     loop\\n\"\n-          \"\\tFSTP_S [ESP+0]\\n\"\n-          \"\\tMOVSS  $dst,[ESP+0]\\n\"\n-          \"\\tADD    ESP,4\\n\"\n-          \"\\tFSTP   ST0\\t # Restore FPU Stack\"\n-    %}\n-  ins_cost(250);\n-  ins_encode( Push_ModF_encoding(src0, src1), emitModDPR(), Push_ResultF(dst,0x4), PopFPU);\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/----------Arithmetic Conversion Instructions---------------------------------\n-\/\/ The conversions operations are all Alpha sorted.  Please keep it that way!\n-\n-instruct roundFloat_mem_reg(stackSlotF dst, regFPR src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (RoundFloat src));\n-  ins_cost(125);\n-  format %{ \"FST_S  $dst,$src\\t# F-round\" %}\n-  ins_encode( Pop_Mem_Reg_FPR(dst, src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-instruct roundDouble_mem_reg(stackSlotD dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (RoundDouble src));\n-  ins_cost(125);\n-  format %{ \"FST_D  $dst,$src\\t# D-round\" %}\n-  ins_encode( Pop_Mem_Reg_DPR(dst, src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-\/\/ Force rounding to 24-bit precision and 6-bit exponent\n-instruct convDPR2FPR_reg(stackSlotF dst, regDPR src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (ConvD2F src));\n-  format %{ \"FST_S  $dst,$src\\t# F-round\" %}\n-  expand %{\n-    roundFloat_mem_reg(dst,src);\n-  %}\n-%}\n-\n-\/\/ Force rounding to 24-bit precision and 6-bit exponent\n-instruct convDPR2F_reg(regF dst, regDPR src, eFlagsReg cr) %{\n-  predicate(UseSSE==1);\n-  match(Set dst (ConvD2F src));\n-  effect( KILL cr );\n-  format %{ \"SUB    ESP,4\\n\\t\"\n-            \"FST_S  [ESP],$src\\t# F-round\\n\\t\"\n-            \"MOVSS  $dst,[ESP]\\n\\t\"\n-            \"ADD ESP,4\" %}\n-  ins_encode %{\n-    __ subptr(rsp, 4);\n-    if ($src$$reg != FPR1L_enc) {\n-      __ fld_s($src$$reg-1);\n-      __ fstp_s(Address(rsp, 0));\n-    } else {\n-      __ fst_s(Address(rsp, 0));\n-    }\n-    __ movflt($dst$$XMMRegister, Address(rsp, 0));\n-    __ addptr(rsp, 4);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Force rounding double precision to single precision\n-instruct convD2F_reg(regF dst, regD src) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (ConvD2F src));\n-  format %{ \"CVTSD2SS $dst,$src\\t# F-round\" %}\n-  ins_encode %{\n-    __ cvtsd2ss ($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convFPR2DPR_reg_reg(regDPR dst, regFPR src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (ConvF2D src));\n-  format %{ \"FST_S  $dst,$src\\t# D-round\" %}\n-  ins_encode( Pop_Reg_Reg_DPR(dst, src));\n-  ins_pipe( fpu_reg_reg );\n-%}\n-\n-instruct convFPR2D_reg(stackSlotD dst, regFPR src) %{\n-  predicate(UseSSE==1);\n-  match(Set dst (ConvF2D src));\n-  format %{ \"FST_D  $dst,$src\\t# D-round\" %}\n-  expand %{\n-    roundDouble_mem_reg(dst,src);\n-  %}\n-%}\n-\n-instruct convF2DPR_reg(regDPR dst, regF src, eFlagsReg cr) %{\n-  predicate(UseSSE==1);\n-  match(Set dst (ConvF2D src));\n-  effect( KILL cr );\n-  format %{ \"SUB    ESP,4\\n\\t\"\n-            \"MOVSS  [ESP] $src\\n\\t\"\n-            \"FLD_S  [ESP]\\n\\t\"\n-            \"ADD    ESP,4\\n\\t\"\n-            \"FSTP   $dst\\t# D-round\" %}\n-  ins_encode %{\n-    __ subptr(rsp, 4);\n-    __ movflt(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_s(Address(rsp, 0));\n-    __ addptr(rsp, 4);\n-    __ fstp_d($dst$$reg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convF2D_reg(regD dst, regF src) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (ConvF2D src));\n-  format %{ \"CVTSS2SD $dst,$src\\t# D-round\" %}\n-  ins_encode %{\n-    __ cvtss2sd ($dst$$XMMRegister, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Convert a double to an int.  If the double is a NAN, stuff a zero in instead.\n-instruct convDPR2I_reg_reg( eAXRegI dst, eDXRegI tmp, regDPR src, eFlagsReg cr ) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (ConvD2I src));\n-  effect( KILL tmp, KILL cr );\n-  format %{ \"FLD    $src\\t# Convert double to int \\n\\t\"\n-            \"FLDCW  trunc mode\\n\\t\"\n-            \"SUB    ESP,4\\n\\t\"\n-            \"FISTp  [ESP + #0]\\n\\t\"\n-            \"FLDCW  std\/24-bit mode\\n\\t\"\n-            \"POP    EAX\\n\\t\"\n-            \"CMP    EAX,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"FLD_D  $src\\n\\t\"\n-            \"CALL   d2i_wrapper\\n\"\n-      \"fast:\" %}\n-  ins_encode( Push_Reg_DPR(src), DPR2I_encoding(src) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Convert a double to an int.  If the double is a NAN, stuff a zero in instead.\n-instruct convD2I_reg_reg( eAXRegI dst, eDXRegI tmp, regD src, eFlagsReg cr ) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (ConvD2I src));\n-  effect( KILL tmp, KILL cr );\n-  format %{ \"CVTTSD2SI $dst, $src\\n\\t\"\n-            \"CMP    $dst,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"SUB    ESP, 8\\n\\t\"\n-            \"MOVSD  [ESP], $src\\n\\t\"\n-            \"FLD_D  [ESP]\\n\\t\"\n-            \"ADD    ESP, 8\\n\\t\"\n-            \"CALL   d2i_wrapper\\n\"\n-      \"fast:\" %}\n-  ins_encode %{\n-    Label fast;\n-    __ cvttsd2sil($dst$$Register, $src$$XMMRegister);\n-    __ cmpl($dst$$Register, 0x80000000);\n-    __ jccb(Assembler::notEqual, fast);\n-    __ subptr(rsp, 8);\n-    __ movdbl(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-    __ addptr(rsp, 8);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2i_wrapper())));\n-    __ post_call_nop();\n-    __ bind(fast);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convDPR2L_reg_reg( eADXRegL dst, regDPR src, eFlagsReg cr ) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (ConvD2L src));\n-  effect( KILL cr );\n-  format %{ \"FLD    $src\\t# Convert double to long\\n\\t\"\n-            \"FLDCW  trunc mode\\n\\t\"\n-            \"SUB    ESP,8\\n\\t\"\n-            \"FISTp  [ESP + #0]\\n\\t\"\n-            \"FLDCW  std\/24-bit mode\\n\\t\"\n-            \"POP    EAX\\n\\t\"\n-            \"POP    EDX\\n\\t\"\n-            \"CMP    EDX,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"TEST   EAX,EAX\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"FLD    $src\\n\\t\"\n-            \"CALL   d2l_wrapper\\n\"\n-      \"fast:\" %}\n-  ins_encode( Push_Reg_DPR(src),  DPR2L_encoding(src) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ XMM lacks a float\/double->long conversion, so use the old FPU stack.\n-instruct convD2L_reg_reg( eADXRegL dst, regD src, eFlagsReg cr ) %{\n-  predicate (UseSSE>=2);\n-  match(Set dst (ConvD2L src));\n-  effect( KILL cr );\n-  format %{ \"SUB    ESP,8\\t# Convert double to long\\n\\t\"\n-            \"MOVSD  [ESP],$src\\n\\t\"\n-            \"FLD_D  [ESP]\\n\\t\"\n-            \"FLDCW  trunc mode\\n\\t\"\n-            \"FISTp  [ESP + #0]\\n\\t\"\n-            \"FLDCW  std\/24-bit mode\\n\\t\"\n-            \"POP    EAX\\n\\t\"\n-            \"POP    EDX\\n\\t\"\n-            \"CMP    EDX,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"TEST   EAX,EAX\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"SUB    ESP,8\\n\\t\"\n-            \"MOVSD  [ESP],$src\\n\\t\"\n-            \"FLD_D  [ESP]\\n\\t\"\n-            \"ADD    ESP,8\\n\\t\"\n-            \"CALL   d2l_wrapper\\n\"\n-      \"fast:\" %}\n-  ins_encode %{\n-    Label fast;\n-    __ subptr(rsp, 8);\n-    __ movdbl(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_trunc()));\n-    __ fistp_d(Address(rsp, 0));\n-    \/\/ Restore the rounding mode, mask the exception\n-    if (Compile::current()->in_24_bit_fp_mode()) {\n-      __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-    } else {\n-      __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-    }\n-    \/\/ Load the converted long, adjust CPU stack\n-    __ pop(rax);\n-    __ pop(rdx);\n-    __ cmpl(rdx, 0x80000000);\n-    __ jccb(Assembler::notEqual, fast);\n-    __ testl(rax, rax);\n-    __ jccb(Assembler::notEqual, fast);\n-    __ subptr(rsp, 8);\n-    __ movdbl(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-    __ addptr(rsp, 8);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2l_wrapper())));\n-    __ post_call_nop();\n-    __ bind(fast);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Convert a double to an int.  Java semantics require we do complex\n-\/\/ manglations in the corner cases.  So we set the rounding mode to\n-\/\/ 'zero', store the darned double down as an int, and reset the\n-\/\/ rounding mode to 'nearest'.  The hardware stores a flag value down\n-\/\/ if we would overflow or converted a NAN; we check for this and\n-\/\/ and go the slow path if needed.\n-instruct convFPR2I_reg_reg(eAXRegI dst, eDXRegI tmp, regFPR src, eFlagsReg cr ) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (ConvF2I src));\n-  effect( KILL tmp, KILL cr );\n-  format %{ \"FLD    $src\\t# Convert float to int \\n\\t\"\n-            \"FLDCW  trunc mode\\n\\t\"\n-            \"SUB    ESP,4\\n\\t\"\n-            \"FISTp  [ESP + #0]\\n\\t\"\n-            \"FLDCW  std\/24-bit mode\\n\\t\"\n-            \"POP    EAX\\n\\t\"\n-            \"CMP    EAX,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"FLD    $src\\n\\t\"\n-            \"CALL   d2i_wrapper\\n\"\n-      \"fast:\" %}\n-  \/\/ DPR2I_encoding works for FPR2I\n-  ins_encode( Push_Reg_FPR(src), DPR2I_encoding(src) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Convert a float in xmm to an int reg.\n-instruct convF2I_reg(eAXRegI dst, eDXRegI tmp, regF src, eFlagsReg cr ) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (ConvF2I src));\n-  effect( KILL tmp, KILL cr );\n-  format %{ \"CVTTSS2SI $dst, $src\\n\\t\"\n-            \"CMP    $dst,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"SUB    ESP, 4\\n\\t\"\n-            \"MOVSS  [ESP], $src\\n\\t\"\n-            \"FLD    [ESP]\\n\\t\"\n-            \"ADD    ESP, 4\\n\\t\"\n-            \"CALL   d2i_wrapper\\n\"\n-      \"fast:\" %}\n-  ins_encode %{\n-    Label fast;\n-    __ cvttss2sil($dst$$Register, $src$$XMMRegister);\n-    __ cmpl($dst$$Register, 0x80000000);\n-    __ jccb(Assembler::notEqual, fast);\n-    __ subptr(rsp, 4);\n-    __ movflt(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_s(Address(rsp, 0));\n-    __ addptr(rsp, 4);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2i_wrapper())));\n-    __ post_call_nop();\n-    __ bind(fast);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convFPR2L_reg_reg( eADXRegL dst, regFPR src, eFlagsReg cr ) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (ConvF2L src));\n-  effect( KILL cr );\n-  format %{ \"FLD    $src\\t# Convert float to long\\n\\t\"\n-            \"FLDCW  trunc mode\\n\\t\"\n-            \"SUB    ESP,8\\n\\t\"\n-            \"FISTp  [ESP + #0]\\n\\t\"\n-            \"FLDCW  std\/24-bit mode\\n\\t\"\n-            \"POP    EAX\\n\\t\"\n-            \"POP    EDX\\n\\t\"\n-            \"CMP    EDX,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"TEST   EAX,EAX\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"FLD    $src\\n\\t\"\n-            \"CALL   d2l_wrapper\\n\"\n-      \"fast:\" %}\n-  \/\/ DPR2L_encoding works for FPR2L\n-  ins_encode( Push_Reg_FPR(src), DPR2L_encoding(src) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ XMM lacks a float\/double->long conversion, so use the old FPU stack.\n-instruct convF2L_reg_reg( eADXRegL dst, regF src, eFlagsReg cr ) %{\n-  predicate (UseSSE>=1);\n-  match(Set dst (ConvF2L src));\n-  effect( KILL cr );\n-  format %{ \"SUB    ESP,8\\t# Convert float to long\\n\\t\"\n-            \"MOVSS  [ESP],$src\\n\\t\"\n-            \"FLD_S  [ESP]\\n\\t\"\n-            \"FLDCW  trunc mode\\n\\t\"\n-            \"FISTp  [ESP + #0]\\n\\t\"\n-            \"FLDCW  std\/24-bit mode\\n\\t\"\n-            \"POP    EAX\\n\\t\"\n-            \"POP    EDX\\n\\t\"\n-            \"CMP    EDX,0x80000000\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"TEST   EAX,EAX\\n\\t\"\n-            \"JNE,s  fast\\n\\t\"\n-            \"SUB    ESP,4\\t# Convert float to long\\n\\t\"\n-            \"MOVSS  [ESP],$src\\n\\t\"\n-            \"FLD_S  [ESP]\\n\\t\"\n-            \"ADD    ESP,4\\n\\t\"\n-            \"CALL   d2l_wrapper\\n\"\n-      \"fast:\" %}\n-  ins_encode %{\n-    Label fast;\n-    __ subptr(rsp, 8);\n-    __ movflt(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_s(Address(rsp, 0));\n-    __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_trunc()));\n-    __ fistp_d(Address(rsp, 0));\n-    \/\/ Restore the rounding mode, mask the exception\n-    if (Compile::current()->in_24_bit_fp_mode()) {\n-      __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-    } else {\n-      __ fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_std()));\n-    }\n-    \/\/ Load the converted long, adjust CPU stack\n-    __ pop(rax);\n-    __ pop(rdx);\n-    __ cmpl(rdx, 0x80000000);\n-    __ jccb(Assembler::notEqual, fast);\n-    __ testl(rax, rax);\n-    __ jccb(Assembler::notEqual, fast);\n-    __ subptr(rsp, 4);\n-    __ movflt(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_s(Address(rsp, 0));\n-    __ addptr(rsp, 4);\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::x86::d2l_wrapper())));\n-    __ post_call_nop();\n-    __ bind(fast);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convI2DPR_reg(regDPR dst, stackSlotI src) %{\n-  predicate( UseSSE<=1 );\n-  match(Set dst (ConvI2D src));\n-  format %{ \"FILD   $src\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xDB, 0x0);  \/* DB \/0 *\/\n-  ins_encode(Push_Mem_I(src), Pop_Reg_DPR(dst));\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-instruct convI2D_reg(regD dst, rRegI src) %{\n-  predicate( UseSSE>=2 && !UseXmmI2D );\n-  match(Set dst (ConvI2D src));\n-  format %{ \"CVTSI2SD $dst,$src\" %}\n-  ins_encode %{\n-    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convI2D_mem(regD dst, memory mem) %{\n-  predicate( UseSSE>=2 );\n-  match(Set dst (ConvI2D (LoadI mem)));\n-  format %{ \"CVTSI2SD $dst,$mem\" %}\n-  ins_encode %{\n-    __ cvtsi2sdl ($dst$$XMMRegister, $mem$$Address);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convXI2D_reg(regD dst, rRegI src)\n-%{\n-  predicate( UseSSE>=2 && UseXmmI2D );\n-  match(Set dst (ConvI2D src));\n-\n-  format %{ \"MOVD  $dst,$src\\n\\t\"\n-            \"CVTDQ2PD $dst,$dst\\t# i2d\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ cvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convI2DPR_mem(regDPR dst, memory mem) %{\n-  predicate( UseSSE<=1 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (ConvI2D (LoadI mem)));\n-  format %{ \"FILD   $mem\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xDB);      \/* DB \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),\n-              Pop_Reg_DPR(dst), ClearInstMark);\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ Convert a byte to a float; no rounding step needed.\n-instruct conv24I2FPR_reg(regFPR dst, stackSlotI src) %{\n-  predicate( UseSSE==0 && n->in(1)->Opcode() == Op_AndI && n->in(1)->in(2)->is_Con() && n->in(1)->in(2)->get_int() == 255 );\n-  match(Set dst (ConvI2F src));\n-  format %{ \"FILD   $src\\n\\t\"\n-            \"FSTP   $dst\" %}\n-\n-  opcode(0xDB, 0x0);  \/* DB \/0 *\/\n-  ins_encode(Push_Mem_I(src), Pop_Reg_FPR(dst));\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ In 24-bit mode, force exponent rounding by storing back out\n-instruct convI2FPR_SSF(stackSlotF dst, stackSlotI src) %{\n-  predicate( UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (ConvI2F src));\n-  ins_cost(200);\n-  format %{ \"FILD   $src\\n\\t\"\n-            \"FSTP_S $dst\" %}\n-  opcode(0xDB, 0x0);  \/* DB \/0 *\/\n-  ins_encode( Push_Mem_I(src),\n-              Pop_Mem_FPR(dst));\n-  ins_pipe( fpu_mem_mem );\n-%}\n-\n-\/\/ In 24-bit mode, force exponent rounding by storing back out\n-instruct convI2FPR_SSF_mem(stackSlotF dst, memory mem) %{\n-  predicate( UseSSE==0 && Compile::current()->select_24_bit_instr());\n-  match(Set dst (ConvI2F (LoadI mem)));\n-  ins_cost(200);\n-  format %{ \"FILD   $mem\\n\\t\"\n-            \"FSTP_S $dst\" %}\n-  opcode(0xDB);  \/* DB \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),\n-              Pop_Mem_FPR(dst), ClearInstMark);\n-  ins_pipe( fpu_mem_mem );\n-%}\n-\n-\/\/ This instruction does not round to 24-bits\n-instruct convI2FPR_reg(regFPR dst, stackSlotI src) %{\n-  predicate( UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (ConvI2F src));\n-  format %{ \"FILD   $src\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xDB, 0x0);  \/* DB \/0 *\/\n-  ins_encode( Push_Mem_I(src),\n-              Pop_Reg_FPR(dst));\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ This instruction does not round to 24-bits\n-instruct convI2FPR_mem(regFPR dst, memory mem) %{\n-  predicate( UseSSE==0 && !Compile::current()->select_24_bit_instr());\n-  match(Set dst (ConvI2F (LoadI mem)));\n-  format %{ \"FILD   $mem\\n\\t\"\n-            \"FSTP   $dst\" %}\n-  opcode(0xDB);      \/* DB \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,mem),\n-              Pop_Reg_FPR(dst), ClearInstMark);\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\/\/ Convert an int to a float in xmm; no rounding step needed.\n-instruct convI2F_reg(regF dst, rRegI src) %{\n-  predicate( UseSSE==1 || ( UseSSE>=2 && !UseXmmI2F ));\n-  match(Set dst (ConvI2F src));\n-  format %{ \"CVTSI2SS $dst, $src\" %}\n-  ins_encode %{\n-    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n- instruct convXI2F_reg(regF dst, rRegI src)\n-%{\n-  predicate( UseSSE>=2 && UseXmmI2F );\n-  match(Set dst (ConvI2F src));\n-\n-  format %{ \"MOVD  $dst,$src\\n\\t\"\n-            \"CVTDQ2PS $dst,$dst\\t# i2f\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ cvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-instruct convI2L_reg( eRegL dst, rRegI src, eFlagsReg cr) %{\n-  match(Set dst (ConvI2L src));\n-  effect(KILL cr);\n-  ins_cost(375);\n-  format %{ \"MOV    $dst.lo,$src\\n\\t\"\n-            \"MOV    $dst.hi,$src\\n\\t\"\n-            \"SAR    $dst.hi,31\" %}\n-  ins_encode(convert_int_long(dst,src));\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Zero-extend convert int to long\n-instruct convI2L_reg_zex(eRegL dst, rRegI src, immL_32bits mask, eFlagsReg flags ) %{\n-  match(Set dst (AndL (ConvI2L src) mask) );\n-  effect( KILL flags );\n-  ins_cost(250);\n-  format %{ \"MOV    $dst.lo,$src\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\" %}\n-  opcode(0x33); \/\/ XOR\n-  ins_encode(enc_Copy(dst,src), OpcP, RegReg_Hi2(dst,dst) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Zero-extend long\n-instruct zerox_long(eRegL dst, eRegL src, immL_32bits mask, eFlagsReg flags ) %{\n-  match(Set dst (AndL src mask) );\n-  effect( KILL flags );\n-  ins_cost(250);\n-  format %{ \"MOV    $dst.lo,$src.lo\\n\\t\"\n-            \"XOR    $dst.hi,$dst.hi\\n\\t\" %}\n-  opcode(0x33); \/\/ XOR\n-  ins_encode(enc_Copy(dst,src), OpcP, RegReg_Hi2(dst,dst) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-instruct convL2DPR_reg( stackSlotD dst, eRegL src, eFlagsReg cr) %{\n-  predicate (UseSSE<=1);\n-  match(Set dst (ConvL2D src));\n-  effect( KILL cr );\n-  format %{ \"PUSH   $src.hi\\t# Convert long to double\\n\\t\"\n-            \"PUSH   $src.lo\\n\\t\"\n-            \"FILD   ST,[ESP + #0]\\n\\t\"\n-            \"ADD    ESP,8\\n\\t\"\n-            \"FSTP_D $dst\\t# D-round\" %}\n-  opcode(0xDF, 0x5);  \/* DF \/5 *\/\n-  ins_encode(convert_long_double(src), Pop_Mem_DPR(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convL2D_reg( regD dst, eRegL src, eFlagsReg cr) %{\n-  predicate (UseSSE>=2);\n-  match(Set dst (ConvL2D src));\n-  effect( KILL cr );\n-  format %{ \"PUSH   $src.hi\\t# Convert long to double\\n\\t\"\n-            \"PUSH   $src.lo\\n\\t\"\n-            \"FILD_D [ESP]\\n\\t\"\n-            \"FSTP_D [ESP]\\n\\t\"\n-            \"MOVSD  $dst,[ESP]\\n\\t\"\n-            \"ADD    ESP,8\" %}\n-  opcode(0xDF, 0x5);  \/* DF \/5 *\/\n-  ins_encode(convert_long_double2(src), Push_ResultD(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convL2F_reg( regF dst, eRegL src, eFlagsReg cr) %{\n-  predicate (UseSSE>=1);\n-  match(Set dst (ConvL2F src));\n-  effect( KILL cr );\n-  format %{ \"PUSH   $src.hi\\t# Convert long to single float\\n\\t\"\n-            \"PUSH   $src.lo\\n\\t\"\n-            \"FILD_D [ESP]\\n\\t\"\n-            \"FSTP_S [ESP]\\n\\t\"\n-            \"MOVSS  $dst,[ESP]\\n\\t\"\n-            \"ADD    ESP,8\" %}\n-  opcode(0xDF, 0x5);  \/* DF \/5 *\/\n-  ins_encode(convert_long_double2(src), Push_ResultF(dst,0x8));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convL2FPR_reg( stackSlotF dst, eRegL src, eFlagsReg cr) %{\n-  match(Set dst (ConvL2F src));\n-  effect( KILL cr );\n-  format %{ \"PUSH   $src.hi\\t# Convert long to single float\\n\\t\"\n-            \"PUSH   $src.lo\\n\\t\"\n-            \"FILD   ST,[ESP + #0]\\n\\t\"\n-            \"ADD    ESP,8\\n\\t\"\n-            \"FSTP_S $dst\\t# F-round\" %}\n-  opcode(0xDF, 0x5);  \/* DF \/5 *\/\n-  ins_encode(convert_long_double(src), Pop_Mem_FPR(dst));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct convL2I_reg( rRegI dst, eRegL src ) %{\n-  match(Set dst (ConvL2I src));\n-  effect( DEF dst, USE src );\n-  format %{ \"MOV    $dst,$src.lo\" %}\n-  ins_encode(enc_CopyL_Lo(dst,src));\n-  ins_pipe( ialu_reg_reg );\n-%}\n-\n-instruct MoveF2I_stack_reg(rRegI dst, stackSlotF src) %{\n-  match(Set dst (MoveF2I src));\n-  effect( DEF dst, USE src );\n-  ins_cost(100);\n-  format %{ \"MOV    $dst,$src\\t# MoveF2I_stack_reg\" %}\n-  ins_encode %{\n-    __ movl($dst$$Register, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe( ialu_reg_mem );\n-%}\n-\n-instruct MoveFPR2I_reg_stack(stackSlotI dst, regFPR src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (MoveF2I src));\n-  effect( DEF dst, USE src );\n-\n-  ins_cost(125);\n-  format %{ \"FST_S  $dst,$src\\t# MoveF2I_reg_stack\" %}\n-  ins_encode( Pop_Mem_Reg_FPR(dst, src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-instruct MoveF2I_reg_stack_sse(stackSlotI dst, regF src) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (MoveF2I src));\n-  effect( DEF dst, USE src );\n-\n-  ins_cost(95);\n-  format %{ \"MOVSS  $dst,$src\\t# MoveF2I_reg_stack_sse\" %}\n-  ins_encode %{\n-    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveF2I_reg_reg_sse(rRegI dst, regF src) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (MoveF2I src));\n-  effect( DEF dst, USE src );\n-  ins_cost(85);\n-  format %{ \"MOVD   $dst,$src\\t# MoveF2I_reg_reg_sse\" %}\n-  ins_encode %{\n-    __ movdl($dst$$Register, $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveI2F_reg_stack(stackSlotF dst, rRegI src) %{\n-  match(Set dst (MoveI2F src));\n-  effect( DEF dst, USE src );\n-\n-  ins_cost(100);\n-  format %{ \"MOV    $dst,$src\\t# MoveI2F_reg_stack\" %}\n-  ins_encode %{\n-    __ movl(Address(rsp, $dst$$disp), $src$$Register);\n-  %}\n-  ins_pipe( ialu_mem_reg );\n-%}\n-\n-\n-instruct MoveI2FPR_stack_reg(regFPR dst, stackSlotI src) %{\n-  predicate(UseSSE==0);\n-  match(Set dst (MoveI2F src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"FLD_S  $src\\n\\t\"\n-            \"FSTP   $dst\\t# MoveI2F_stack_reg\" %}\n-  opcode(0xD9);               \/* D9 \/0, FLD m32real *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem_no_oop(0x00,src),\n-              Pop_Reg_FPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-instruct MoveI2F_stack_reg_sse(regF dst, stackSlotI src) %{\n-  predicate(UseSSE>=1);\n-  match(Set dst (MoveI2F src));\n-  effect( DEF dst, USE src );\n-\n-  ins_cost(95);\n-  format %{ \"MOVSS  $dst,$src\\t# MoveI2F_stack_reg_sse\" %}\n-  ins_encode %{\n-    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveI2F_reg_reg_sse(regF dst, rRegI src) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (MoveI2F src));\n-  effect( DEF dst, USE src );\n-\n-  ins_cost(85);\n-  format %{ \"MOVD   $dst,$src\\t# MoveI2F_reg_reg_sse\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveD2L_stack_reg(eRegL dst, stackSlotD src) %{\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(250);\n-  format %{ \"MOV    $dst.lo,$src\\n\\t\"\n-            \"MOV    $dst.hi,$src+4\\t# MoveD2L_stack_reg\" %}\n-  opcode(0x8B, 0x8B);\n-  ins_encode( SetInstMark, OpcP, RegMem(dst,src), OpcS, RegMem_Hi(dst,src), ClearInstMark);\n-  ins_pipe( ialu_mem_long_reg );\n-%}\n-\n-instruct MoveDPR2L_reg_stack(stackSlotL dst, regDPR src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(125);\n-  format %{ \"FST_D  $dst,$src\\t# MoveD2L_reg_stack\" %}\n-  ins_encode( Pop_Mem_Reg_DPR(dst, src) );\n-  ins_pipe( fpu_mem_reg );\n-%}\n-\n-instruct MoveD2L_reg_stack_sse(stackSlotL dst, regD src) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src);\n-  ins_cost(95);\n-  format %{ \"MOVSD  $dst,$src\\t# MoveD2L_reg_stack_sse\" %}\n-  ins_encode %{\n-    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveD2L_reg_reg_sse(eRegL dst, regD src, regD tmp) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (MoveD2L src));\n-  effect(DEF dst, USE src, TEMP tmp);\n-  ins_cost(85);\n-  format %{ \"MOVD   $dst.lo,$src\\n\\t\"\n-            \"PSHUFLW $tmp,$src,0x4E\\n\\t\"\n-            \"MOVD   $dst.hi,$tmp\\t# MoveD2L_reg_reg_sse\" %}\n-  ins_encode %{\n-    __ movdl($dst$$Register, $src$$XMMRegister);\n-    __ pshuflw($tmp$$XMMRegister, $src$$XMMRegister, 0x4e);\n-    __ movdl(HIGH_FROM_LOW($dst$$Register), $tmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveL2D_reg_stack(stackSlotD dst, eRegL src) %{\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(200);\n-  format %{ \"MOV    $dst,$src.lo\\n\\t\"\n-            \"MOV    $dst+4,$src.hi\\t# MoveL2D_reg_stack\" %}\n-  opcode(0x89, 0x89);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, dst ), OpcS, RegMem_Hi( src, dst ), ClearInstMark );\n-  ins_pipe( ialu_mem_long_reg );\n-%}\n-\n-\n-instruct MoveL2DPR_stack_reg(regDPR dst, stackSlotL src) %{\n-  predicate(UseSSE<=1);\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-  ins_cost(125);\n-\n-  format %{ \"FLD_D  $src\\n\\t\"\n-            \"FSTP   $dst\\t# MoveL2D_stack_reg\" %}\n-  opcode(0xDD);               \/* DD \/0, FLD m64real *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem_no_oop(0x00,src),\n-              Pop_Reg_DPR(dst), ClearInstMark );\n-  ins_pipe( fpu_reg_mem );\n-%}\n-\n-\n-instruct MoveL2D_stack_reg_sse(regD dst, stackSlotL src) %{\n-  predicate(UseSSE>=2 && UseXmmLoadAndClearUpper);\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(95);\n-  format %{ \"MOVSD  $dst,$src\\t# MoveL2D_stack_reg_sse\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveL2D_stack_reg_sse_partial(regD dst, stackSlotL src) %{\n-  predicate(UseSSE>=2 && !UseXmmLoadAndClearUpper);\n-  match(Set dst (MoveL2D src));\n-  effect(DEF dst, USE src);\n-\n-  ins_cost(95);\n-  format %{ \"MOVLPD $dst,$src\\t# MoveL2D_stack_reg_sse\" %}\n-  ins_encode %{\n-    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct MoveL2D_reg_reg_sse(regD dst, eRegL src, regD tmp) %{\n-  predicate(UseSSE>=2);\n-  match(Set dst (MoveL2D src));\n-  effect(TEMP dst, USE src, TEMP tmp);\n-  ins_cost(85);\n-  format %{ \"MOVD   $dst,$src.lo\\n\\t\"\n-            \"MOVD   $tmp,$src.hi\\n\\t\"\n-            \"PUNPCKLDQ $dst,$tmp\\t# MoveL2D_reg_reg_sse\" %}\n-  ins_encode %{\n-    __ movdl($dst$$XMMRegister, $src$$Register);\n-    __ movdl($tmp$$XMMRegister, HIGH_FROM_LOW($src$$Register));\n-    __ punpckldq($dst$$XMMRegister, $tmp$$XMMRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n-\n-instruct compressBitsL_reg(eADXRegL dst, eBCXRegL src, eBDPRegL mask, eSIRegI rtmp, regF xtmp, eFlagsReg cr) %{\n-  predicate(n->bottom_type()->isa_long());\n-  match(Set dst (CompressBits src mask));\n-  effect(TEMP rtmp, TEMP xtmp, KILL cr);\n-  format %{ \"compress_bits $dst, $src, $mask\\t! using $rtmp and $xtmp as TEMP\" %}\n-  ins_encode %{\n-    Label exit, partail_result;\n-    \/\/ Parallely extract both upper and lower 32 bits of source into destination register pair.\n-    \/\/ Merge the results of upper and lower destination registers such that upper destination\n-    \/\/ results are contiguously laid out after the lower destination result.\n-    __ pextl($dst$$Register, $src$$Register, $mask$$Register);\n-    __ pextl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($src$$Register), HIGH_FROM_LOW($mask$$Register));\n-    __ popcntl($rtmp$$Register, $mask$$Register);\n-    \/\/ Skip merging if bit count of lower mask register is equal to 32 (register size).\n-    __ cmpl($rtmp$$Register, 32);\n-    __ jccb(Assembler::equal, exit);\n-    \/\/ Due to constraint on number of GPRs on 32 bit target, using XMM register as potential spill slot.\n-    __ movdl($xtmp$$XMMRegister, $rtmp$$Register);\n-    \/\/ Shift left the contents of upper destination register by true bit count of lower mask register\n-    \/\/ and merge with lower destination register.\n-    __ shlxl($rtmp$$Register, HIGH_FROM_LOW($dst$$Register), $rtmp$$Register);\n-    __ orl($dst$$Register, $rtmp$$Register);\n-    __ movdl($rtmp$$Register, $xtmp$$XMMRegister);\n-    \/\/ Zero out upper destination register if true bit count of lower 32 bit mask is zero\n-    \/\/ since contents of upper destination have already been copied to lower destination\n-    \/\/ register.\n-    __ cmpl($rtmp$$Register, 0);\n-    __ jccb(Assembler::greater, partail_result);\n-    __ movl(HIGH_FROM_LOW($dst$$Register), 0);\n-    __ jmp(exit);\n-    __ bind(partail_result);\n-    \/\/ Perform right shift over upper destination register to move out bits already copied\n-    \/\/ to lower destination register.\n-    __ subl($rtmp$$Register, 32);\n-    __ negl($rtmp$$Register);\n-    __ shrxl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($dst$$Register), $rtmp$$Register);\n-    __ bind(exit);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct expandBitsL_reg(eADXRegL dst, eBCXRegL src, eBDPRegL mask, eSIRegI rtmp, regF xtmp, eFlagsReg cr) %{\n-  predicate(n->bottom_type()->isa_long());\n-  match(Set dst (ExpandBits src mask));\n-  effect(TEMP rtmp, TEMP xtmp, KILL cr);\n-  format %{ \"expand_bits $dst, $src, $mask\\t! using $rtmp and $xtmp as TEMP\" %}\n-  ins_encode %{\n-    \/\/ Extraction operation sequentially reads the bits from source register starting from LSB\n-    \/\/ and lays them out into destination register at bit locations corresponding to true bits\n-    \/\/ in mask register. Thus number of source bits read are equal to combined true bit count\n-    \/\/ of mask register pair.\n-    Label exit, mask_clipping;\n-    __ pdepl($dst$$Register, $src$$Register, $mask$$Register);\n-    __ pdepl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($src$$Register), HIGH_FROM_LOW($mask$$Register));\n-    __ popcntl($rtmp$$Register, $mask$$Register);\n-    \/\/ If true bit count of lower mask register is 32 then none of bit of lower source register\n-    \/\/ will feed to upper destination register.\n-    __ cmpl($rtmp$$Register, 32);\n-    __ jccb(Assembler::equal, exit);\n-    \/\/ Due to constraint on number of GPRs on 32 bit target, using XMM register as potential spill slot.\n-    __ movdl($xtmp$$XMMRegister, $rtmp$$Register);\n-    \/\/ Shift right the contents of lower source register to remove already consumed bits.\n-    __ shrxl($rtmp$$Register, $src$$Register, $rtmp$$Register);\n-    \/\/ Extract the bits from lower source register starting from LSB under the influence\n-    \/\/ of upper mask register.\n-    __ pdepl(HIGH_FROM_LOW($dst$$Register), $rtmp$$Register, HIGH_FROM_LOW($mask$$Register));\n-    __ movdl($rtmp$$Register, $xtmp$$XMMRegister);\n-    __ subl($rtmp$$Register, 32);\n-    __ negl($rtmp$$Register);\n-    __ movdl($xtmp$$XMMRegister, $mask$$Register);\n-    __ movl($mask$$Register, HIGH_FROM_LOW($mask$$Register));\n-    \/\/ Clear the set bits in upper mask register which have been used to extract the contents\n-    \/\/ from lower source register.\n-    __ bind(mask_clipping);\n-    __ blsrl($mask$$Register, $mask$$Register);\n-    __ decrementl($rtmp$$Register, 1);\n-    __ jccb(Assembler::greater, mask_clipping);\n-    \/\/ Starting from LSB extract the bits from upper source register under the influence of\n-    \/\/ remaining set bits in upper mask register.\n-    __ pdepl($rtmp$$Register, HIGH_FROM_LOW($src$$Register), $mask$$Register);\n-    \/\/ Merge the partial results extracted from lower and upper source register bits.\n-    __ orl(HIGH_FROM_LOW($dst$$Register), $rtmp$$Register);\n-    __ movdl($mask$$Register, $xtmp$$XMMRegister);\n-    __ bind(exit);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ =======================================================================\n-\/\/ Fast clearing of an array\n-\/\/ Small non-constant length ClearArray for non-AVX512 targets.\n-instruct rep_stos(eCXRegI cnt, eDIRegP base, regD tmp, eAXRegI zero, Universe dummy, eFlagsReg cr) %{\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n-\n-  format %{ $$template\n-    $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n-    $$emit$$\"CMP    InitArrayShortSize,rcx\\n\\t\"\n-    $$emit$$\"JG     LARGE\\n\\t\"\n-    $$emit$$\"SHL    ECX, 1\\n\\t\"\n-    $$emit$$\"DEC    ECX\\n\\t\"\n-    $$emit$$\"JS     DONE\\t# Zero length\\n\\t\"\n-    $$emit$$\"MOV    EAX,(EDI,ECX,4)\\t# LOOP\\n\\t\"\n-    $$emit$$\"DEC    ECX\\n\\t\"\n-    $$emit$$\"JGE    LOOP\\n\\t\"\n-    $$emit$$\"JMP    DONE\\n\\t\"\n-    $$emit$$\"# LARGE:\\n\\t\"\n-    if (UseFastStosb) {\n-       $$emit$$\"SHL    ECX,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"REP STOSB\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"MOV     RDI,RAX\\n\\t\"\n-       $$emit$$\"VPXOR    YMM0,YMM0,YMM0\\n\\t\"\n-       $$emit$$\"JMPQ    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,0x20(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x40,RAX\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"SUB     0x8,RCX\\n\\t\"\n-       $$emit$$\"JGE     L_loop\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JL      L_tail\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x20,RAX\\n\\t\"\n-       $$emit$$\"SUB     0x4,RCX\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JLE     L_end\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"VMOVQ   XMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x8,RAX\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"JGE     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"SHL    ECX,1\\t# Convert doublewords to words\\n\\t\"\n-       $$emit$$\"REP STOS\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    }\n-    $$emit$$\"# DONE\"\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Small non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_evex(eCXRegI cnt, eDIRegP base, legRegD tmp, kReg ktmp, eAXRegI zero, Universe dummy, eFlagsReg cr) %{\n-  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n-  match(Set dummy (ClearArray cnt base));\n-  ins_cost(125);\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n-\n-  format %{ $$template\n-    $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n-    $$emit$$\"CMP    InitArrayShortSize,rcx\\n\\t\"\n-    $$emit$$\"JG     LARGE\\n\\t\"\n-    $$emit$$\"SHL    ECX, 1\\n\\t\"\n-    $$emit$$\"DEC    ECX\\n\\t\"\n-    $$emit$$\"JS     DONE\\t# Zero length\\n\\t\"\n-    $$emit$$\"MOV    EAX,(EDI,ECX,4)\\t# LOOP\\n\\t\"\n-    $$emit$$\"DEC    ECX\\n\\t\"\n-    $$emit$$\"JGE    LOOP\\n\\t\"\n-    $$emit$$\"JMP    DONE\\n\\t\"\n-    $$emit$$\"# LARGE:\\n\\t\"\n-    if (UseFastStosb) {\n-       $$emit$$\"SHL    ECX,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"REP STOSB\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"MOV     RDI,RAX\\n\\t\"\n-       $$emit$$\"VPXOR    YMM0,YMM0,YMM0\\n\\t\"\n-       $$emit$$\"JMPQ    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,0x20(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x40,RAX\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"SUB     0x8,RCX\\n\\t\"\n-       $$emit$$\"JGE     L_loop\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JL      L_tail\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x20,RAX\\n\\t\"\n-       $$emit$$\"SUB     0x4,RCX\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JLE     L_end\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"VMOVQ   XMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x8,RAX\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"JGE     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"SHL    ECX,1\\t# Convert doublewords to words\\n\\t\"\n-       $$emit$$\"REP STOS\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    }\n-    $$emit$$\"# DONE\"\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n-instruct rep_stos_large(eCXRegI cnt, eDIRegP base, regD tmp, eAXRegI zero, Universe dummy, eFlagsReg cr) %{\n-  predicate((UseAVX <= 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n-  format %{ $$template\n-    if (UseFastStosb) {\n-       $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"SHL    ECX,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"REP STOSB\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"MOV     RDI,RAX\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"VPXOR   YMM0,YMM0,YMM0\\n\\t\"\n-       $$emit$$\"JMPQ    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,0x20(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x40,RAX\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"SUB     0x8,RCX\\n\\t\"\n-       $$emit$$\"JGE     L_loop\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JL      L_tail\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x20,RAX\\n\\t\"\n-       $$emit$$\"SUB     0x4,RCX\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JLE     L_end\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"VMOVQ   XMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x8,RAX\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"JGE     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"SHL    ECX,1\\t# Convert doublewords to words\\n\\t\"\n-       $$emit$$\"REP STOS\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    }\n-    $$emit$$\"# DONE\"\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Large non-constant length ClearArray for AVX512 targets.\n-instruct rep_stos_large_evex(eCXRegI cnt, eDIRegP base, legRegD tmp, kReg ktmp, eAXRegI zero, Universe dummy, eFlagsReg cr) %{\n-  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n-  match(Set dummy (ClearArray cnt base));\n-  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n-  format %{ $$template\n-    if (UseFastStosb) {\n-       $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"SHL    ECX,3\\t# Convert doublewords to bytes\\n\\t\"\n-       $$emit$$\"REP STOSB\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    } else if (UseXMMForObjInit) {\n-       $$emit$$\"MOV     RDI,RAX\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"VPXOR   YMM0,YMM0,YMM0\\n\\t\"\n-       $$emit$$\"JMPQ    L_zero_64_bytes\\n\\t\"\n-       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,0x20(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x40,RAX\\n\\t\"\n-       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n-       $$emit$$\"SUB     0x8,RCX\\n\\t\"\n-       $$emit$$\"JGE     L_loop\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JL      L_tail\\n\\t\"\n-       $$emit$$\"VMOVDQU YMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x20,RAX\\n\\t\"\n-       $$emit$$\"SUB     0x4,RCX\\n\\t\"\n-       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n-       $$emit$$\"ADD     0x4,RCX\\n\\t\"\n-       $$emit$$\"JLE     L_end\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n-       $$emit$$\"VMOVQ   XMM0,(RAX)\\n\\t\"\n-       $$emit$$\"ADD     0x8,RAX\\n\\t\"\n-       $$emit$$\"DEC     RCX\\n\\t\"\n-       $$emit$$\"JGE     L_sloop\\n\\t\"\n-       $$emit$$\"# L_end:\\n\\t\"\n-    } else {\n-       $$emit$$\"XOR    EAX,EAX\\t# ClearArray:\\n\\t\"\n-       $$emit$$\"SHL    ECX,1\\t# Convert doublewords to words\\n\\t\"\n-       $$emit$$\"REP STOS\\t# store EAX into [EDI++] while ECX--\\n\\t\"\n-    }\n-    $$emit$$\"# DONE\"\n-  %}\n-  ins_encode %{\n-    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n-                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Small constant length ClearArray for AVX512 targets.\n-instruct rep_stos_im(immI cnt, kReg ktmp, eRegP base, regD tmp, rRegI zero, Universe dummy, eFlagsReg cr)\n-%{\n-  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n-  match(Set dummy (ClearArray cnt base));\n-  ins_cost(100);\n-  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n-  format %{ \"clear_mem_imm $base , $cnt  \\n\\t\" %}\n-  ins_encode %{\n-   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct string_compareL(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n-                         eAXRegI result, regD tmp1, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareL_evex(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n-                              eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareU(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n-                         eAXRegI result, regD tmp1, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareU_evex(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n-                              eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareLU(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n-                          eAXRegI result, regD tmp1, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareLU_evex(eDIRegP str1, eCXRegI cnt1, eSIRegP str2, eDXRegI cnt2,\n-                               eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareUL(eSIRegP str1, eDXRegI cnt1, eDIRegP str2, eCXRegI cnt2,\n-                          eAXRegI result, regD tmp1, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str2$$Register, $str1$$Register,\n-                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compareUL_evex(eSIRegP str1, eDXRegI cnt1, eDIRegP str2, eCXRegI cnt2,\n-                               eAXRegI result, regD tmp1, kReg ktmp, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n-  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n-\n-  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n-  ins_encode %{\n-    __ string_compare($str2$$Register, $str1$$Register,\n-                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n-                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast string equals\n-instruct string_equals(eDIRegP str1, eSIRegP str2, eCXRegI cnt, eAXRegI result,\n-                       regD tmp1, regD tmp2, eBXRegI tmp3, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw());\n-  match(Set result (StrEquals (Binary str1 str2) cnt));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n-\n-  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n-                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n-  %}\n-\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_equals_evex(eDIRegP str1, eSIRegP str2, eCXRegI cnt, eAXRegI result,\n-                            regD tmp1, regD tmp2, kReg ktmp, eBXRegI tmp3, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw());\n-  match(Set result (StrEquals (Binary str1 str2) cnt));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n-\n-  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n-                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n-  %}\n-\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ fast search of substring with known size.\n-instruct string_indexof_conL(eDIRegP str1, eDXRegI cnt1, eSIRegP str2, immI int_cnt2,\n-                             eBXRegI result, regD vec1, eAXRegI cnt2, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n-  effect(TEMP vec1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $vec1, $cnt1, $cnt2, $tmp\" %}\n-  ins_encode %{\n-    int icnt2 = (int)$int_cnt2$$constant;\n-    if (icnt2 >= 16) {\n-      \/\/ IndexOf for constant substrings with size >= 16 elements\n-      \/\/ which don't need to be loaded through stack.\n-      __ string_indexofC8($str1$$Register, $str2$$Register,\n-                          $cnt1$$Register, $cnt2$$Register,\n-                          icnt2, $result$$Register,\n-                          $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n-    } else {\n-      \/\/ Small strings are loaded through stack if they cross page boundary.\n-      __ string_indexof($str1$$Register, $str2$$Register,\n-                        $cnt1$$Register, $cnt2$$Register,\n-                        icnt2, $result$$Register,\n-                        $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast search of substring with known size.\n-instruct string_indexof_conU(eDIRegP str1, eDXRegI cnt1, eSIRegP str2, immI int_cnt2,\n-                             eBXRegI result, regD vec1, eAXRegI cnt2, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n-  effect(TEMP vec1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $vec1, $cnt1, $cnt2, $tmp\" %}\n-  ins_encode %{\n-    int icnt2 = (int)$int_cnt2$$constant;\n-    if (icnt2 >= 8) {\n-      \/\/ IndexOf for constant substrings with size >= 8 elements\n-      \/\/ which don't need to be loaded through stack.\n-      __ string_indexofC8($str1$$Register, $str2$$Register,\n-                          $cnt1$$Register, $cnt2$$Register,\n-                          icnt2, $result$$Register,\n-                          $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n-    } else {\n-      \/\/ Small strings are loaded through stack if they cross page boundary.\n-      __ string_indexof($str1$$Register, $str2$$Register,\n-                        $cnt1$$Register, $cnt2$$Register,\n-                        icnt2, $result$$Register,\n-                        $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast search of substring with known size.\n-instruct string_indexof_conUL(eDIRegP str1, eDXRegI cnt1, eSIRegP str2, immI int_cnt2,\n-                             eBXRegI result, regD vec1, eAXRegI cnt2, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n-  effect(TEMP vec1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $vec1, $cnt1, $cnt2, $tmp\" %}\n-  ins_encode %{\n-    int icnt2 = (int)$int_cnt2$$constant;\n-    if (icnt2 >= 8) {\n-      \/\/ IndexOf for constant substrings with size >= 8 elements\n-      \/\/ which don't need to be loaded through stack.\n-      __ string_indexofC8($str1$$Register, $str2$$Register,\n-                          $cnt1$$Register, $cnt2$$Register,\n-                          icnt2, $result$$Register,\n-                          $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n-    } else {\n-      \/\/ Small strings are loaded through stack if they cross page boundary.\n-      __ string_indexof($str1$$Register, $str2$$Register,\n-                        $cnt1$$Register, $cnt2$$Register,\n-                        icnt2, $result$$Register,\n-                        $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n-    }\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexofL(eDIRegP str1, eDXRegI cnt1, eSIRegP str2, eAXRegI cnt2,\n-                         eBXRegI result, regD vec1, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP vec1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register,\n-                      (-1), $result$$Register,\n-                      $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexofU(eDIRegP str1, eDXRegI cnt1, eSIRegP str2, eAXRegI cnt2,\n-                         eBXRegI result, regD vec1, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP vec1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register,\n-                      (-1), $result$$Register,\n-                      $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexofUL(eDIRegP str1, eDXRegI cnt1, eSIRegP str2, eAXRegI cnt2,\n-                         eBXRegI result, regD vec1, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n-  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n-  effect(TEMP vec1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n-\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof($str1$$Register, $str2$$Register,\n-                      $cnt1$$Register, $cnt2$$Register,\n-                      (-1), $result$$Register,\n-                      $vec1$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_indexof_char(eDIRegP str1, eDXRegI cnt1, eAXRegI ch,\n-                              eBXRegI result, regD vec1, regD vec2, regD vec3, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));\n-  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n-  effect(TEMP vec1, TEMP vec2, TEMP vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n-  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n-                           $vec1$$XMMRegister, $vec2$$XMMRegister, $vec3$$XMMRegister, $tmp$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct stringL_indexof_char(eDIRegP str1, eDXRegI cnt1, eAXRegI ch,\n-                              eBXRegI result, regD vec1, regD vec2, regD vec3, eCXRegI tmp, eFlagsReg cr) %{\n-  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));\n-  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n-  effect(TEMP vec1, TEMP vec2, TEMP vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n-  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n-  ins_encode %{\n-    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n-                           $vec1$$XMMRegister, $vec2$$XMMRegister, $vec3$$XMMRegister, $tmp$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ fast array equals\n-instruct array_equalsB(eDIRegP ary1, eSIRegP ary2, eAXRegI result,\n-                       regD tmp1, regD tmp2, eCXRegI tmp3, eBXRegI tmp4, eFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-  \/\/ins_cost(300);\n-\n-  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct array_equalsB_evex(eDIRegP ary1, eSIRegP ary2, eAXRegI result,\n-                       regD tmp1, regD tmp2, kReg ktmp, eCXRegI tmp3, eBXRegI tmp4, eFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-  \/\/ins_cost(300);\n-\n-  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct array_equalsC(eDIRegP ary1, eSIRegP ary2, eAXRegI result,\n-                       regD tmp1, regD tmp2, eCXRegI tmp3, eBXRegI tmp4, eFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-  \/\/ins_cost(300);\n-\n-  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct array_equalsC_evex(eDIRegP ary1, eSIRegP ary2, eAXRegI result,\n-                            regD tmp1, regD tmp2, kReg ktmp, eCXRegI tmp3, eBXRegI tmp4, eFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n-  match(Set result (AryEq ary1 ary2));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n-  \/\/ins_cost(300);\n-\n-  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n-  ins_encode %{\n-    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n-                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n-                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct count_positives(eSIRegP ary1, eCXRegI len, eAXRegI result,\n-                         regD tmp1, regD tmp2, eBXRegI tmp3, eFlagsReg cr)\n-%{\n-  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n-  match(Set result (CountPositives ary1 len));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n-\n-  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ count_positives($ary1$$Register, $len$$Register,\n-                       $result$$Register, $tmp3$$Register,\n-                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct count_positives_evex(eSIRegP ary1, eCXRegI len, eAXRegI result,\n-                              regD tmp1, regD tmp2, kReg ktmp1, kReg ktmp2, eBXRegI tmp3, eFlagsReg cr)\n-%{\n-  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n-  match(Set result (CountPositives ary1 len));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n-\n-  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n-  ins_encode %{\n-    __ count_positives($ary1$$Register, $len$$Register,\n-                       $result$$Register, $tmp3$$Register,\n-                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ fast char[] to byte[] compression\n-instruct string_compress(eSIRegP src, eDIRegP dst, eDXRegI len, regD tmp1, regD tmp2,\n-                         regD tmp3, regD tmp4, eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n-  match(Set result (StrCompressedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n-  ins_encode %{\n-    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n-                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n-                           knoreg, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_compress_evex(eSIRegP src, eDIRegP dst, eDXRegI len, regD tmp1, regD tmp2,\n-                              regD tmp3, regD tmp4, kReg ktmp1, kReg ktmp2, eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n-  match(Set result (StrCompressedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n-  ins_encode %{\n-    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n-                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n-                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ fast byte[] to char[] inflation\n-instruct string_inflate(Universe dummy, eSIRegP src, eDIRegP dst, eDXRegI len,\n-                        regD tmp1, eCXRegI tmp2, eFlagsReg cr) %{\n-  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n-  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n-\n-  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n-  ins_encode %{\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct string_inflate_evex(Universe dummy, eSIRegP src, eDIRegP dst, eDXRegI len,\n-                             regD tmp1, kReg ktmp, eCXRegI tmp2, eFlagsReg cr) %{\n-  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n-  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n-\n-  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n-  ins_encode %{\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ encode char[] to byte[] in ISO_8859_1\n-instruct encode_iso_array(eSIRegP src, eDIRegP dst, eDXRegI len,\n-                          regD tmp1, regD tmp2, regD tmp3, regD tmp4,\n-                          eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n-  predicate(!((EncodeISOArrayNode*)n)->is_ascii());\n-  match(Set result (EncodeISOArray src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"Encode iso array $src,$dst,$len -> $result    \/\/ KILL ECX, EDX, $tmp1, $tmp2, $tmp3, $tmp4, ESI, EDI \" %}\n-  ins_encode %{\n-    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n-                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, false);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ encode char[] to byte[] in ASCII\n-instruct encode_ascii_array(eSIRegP src, eDIRegP dst, eDXRegI len,\n-                            regD tmp1, regD tmp2, regD tmp3, regD tmp4,\n-                            eCXRegI tmp5, eAXRegI result, eFlagsReg cr) %{\n-  predicate(((EncodeISOArrayNode*)n)->is_ascii());\n-  match(Set result (EncodeISOArray src (Binary dst len)));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n-\n-  format %{ \"Encode ascii array $src,$dst,$len -> $result    \/\/ KILL ECX, EDX, $tmp1, $tmp2, $tmp3, $tmp4, ESI, EDI \" %}\n-  ins_encode %{\n-    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n-                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n-                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, true);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/----------Control Flow Instructions------------------------------------------\n-\/\/ Signed compare Instructions\n-instruct compI_eReg(eFlagsReg cr, rRegI op1, rRegI op2) %{\n-  match(Set cr (CmpI op1 op2));\n-  effect( DEF cr, USE op1, USE op2 );\n-  format %{ \"CMP    $op1,$op2\" %}\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( OpcP, RegReg( op1, op2) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-instruct compI_eReg_imm(eFlagsReg cr, rRegI op1, immI op2) %{\n-  match(Set cr (CmpI op1 op2));\n-  effect( DEF cr, USE op1 );\n-  format %{ \"CMP    $op1,$op2\" %}\n-  opcode(0x81,0x07);  \/* Opcode 81 \/7 *\/\n-  \/\/ ins_encode( RegImm( op1, op2) );  \/* Was CmpImm *\/\n-  ins_encode( OpcSErm( op1, op2 ), Con8or32( op2 ) );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-\/\/ Cisc-spilled version of cmpI_eReg\n-instruct compI_eReg_mem(eFlagsReg cr, rRegI op1, memory op2) %{\n-  match(Set cr (CmpI op1 (LoadI op2)));\n-\n-  format %{ \"CMP    $op1,$op2\" %}\n-  ins_cost(500);\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( op1, op2), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_mem );\n-%}\n-\n-instruct testI_reg( eFlagsReg cr, rRegI src, immI_0 zero ) %{\n-  match(Set cr (CmpI src zero));\n-  effect( DEF cr, USE src );\n-\n-  format %{ \"TEST   $src,$src\" %}\n-  opcode(0x85);\n-  ins_encode( OpcP, RegReg( src, src ) );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-instruct testI_reg_imm( eFlagsReg cr, rRegI src, immI con, immI_0 zero ) %{\n-  match(Set cr (CmpI (AndI src con) zero));\n-\n-  format %{ \"TEST   $src,$con\" %}\n-  opcode(0xF7,0x00);\n-  ins_encode( OpcP, RegOpc(src), Con32(con) );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-instruct testI_reg_mem( eFlagsReg cr, rRegI src, memory mem, immI_0 zero ) %{\n-  match(Set cr (CmpI (AndI src mem) zero));\n-\n-  format %{ \"TEST   $src,$mem\" %}\n-  opcode(0x85);\n-  ins_encode( SetInstMark, OpcP, RegMem( src, mem ), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_mem );\n-%}\n-\n-\/\/ Unsigned compare Instructions; really, same as signed except they\n-\/\/ produce an eFlagsRegU instead of eFlagsReg.\n-instruct compU_eReg(eFlagsRegU cr, rRegI op1, rRegI op2) %{\n-  match(Set cr (CmpU op1 op2));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( OpcP, RegReg( op1, op2) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-instruct compU_eReg_imm(eFlagsRegU cr, rRegI op1, immI op2) %{\n-  match(Set cr (CmpU op1 op2));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  opcode(0x81,0x07);  \/* Opcode 81 \/7 *\/\n-  ins_encode( OpcSErm( op1, op2 ), Con8or32( op2 ) );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-\/\/ \/\/ Cisc-spilled version of cmpU_eReg\n-instruct compU_eReg_mem(eFlagsRegU cr, rRegI op1, memory op2) %{\n-  match(Set cr (CmpU op1 (LoadI op2)));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  ins_cost(500);\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( op1, op2), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_mem );\n-%}\n-\n-\/\/ \/\/ Cisc-spilled version of cmpU_eReg\n-\/\/instruct compU_mem_eReg(eFlagsRegU cr, memory op1, rRegI op2) %{\n-\/\/  match(Set cr (CmpU (LoadI op1) op2));\n-\/\/\n-\/\/  format %{ \"CMPu   $op1,$op2\" %}\n-\/\/  ins_cost(500);\n-\/\/  opcode(0x39);  \/* Opcode 39 \/r *\/\n-\/\/  ins_encode( OpcP, RegMem( op1, op2) );\n-\/\/%}\n-\n-instruct testU_reg( eFlagsRegU cr, rRegI src, immI_0 zero ) %{\n-  match(Set cr (CmpU src zero));\n-\n-  format %{ \"TESTu  $src,$src\" %}\n-  opcode(0x85);\n-  ins_encode( OpcP, RegReg( src, src ) );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-\/\/ Unsigned pointer compare Instructions\n-instruct compP_eReg(eFlagsRegU cr, eRegP op1, eRegP op2) %{\n-  match(Set cr (CmpP op1 op2));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( OpcP, RegReg( op1, op2) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-instruct compP_eReg_imm(eFlagsRegU cr, eRegP op1, immP op2) %{\n-  match(Set cr (CmpP op1 op2));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  opcode(0x81,0x07);  \/* Opcode 81 \/7 *\/\n-  ins_encode( SetInstMark, OpcSErm( op1, op2 ), Con8or32( op2 ), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-\/\/ \/\/ Cisc-spilled version of cmpP_eReg\n-instruct compP_eReg_mem(eFlagsRegU cr, eRegP op1, memory op2) %{\n-  match(Set cr (CmpP op1 (LoadP op2)));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  ins_cost(500);\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( op1, op2), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_mem );\n-%}\n-\n-\/\/ \/\/ Cisc-spilled version of cmpP_eReg\n-\/\/instruct compP_mem_eReg(eFlagsRegU cr, memory op1, eRegP op2) %{\n-\/\/  match(Set cr (CmpP (LoadP op1) op2));\n-\/\/\n-\/\/  format %{ \"CMPu   $op1,$op2\" %}\n-\/\/  ins_cost(500);\n-\/\/  opcode(0x39);  \/* Opcode 39 \/r *\/\n-\/\/  ins_encode( OpcP, RegMem( op1, op2) );\n-\/\/%}\n-\n-\/\/ Compare raw pointer (used in out-of-heap check).\n-\/\/ Only works because non-oop pointers must be raw pointers\n-\/\/ and raw pointers have no anti-dependencies.\n-instruct compP_mem_eReg( eFlagsRegU cr, eRegP op1, memory op2 ) %{\n-  predicate( n->in(2)->in(2)->bottom_type()->reloc() == relocInfo::none );\n-  match(Set cr (CmpP op1 (LoadP op2)));\n-\n-  format %{ \"CMPu   $op1,$op2\" %}\n-  opcode(0x3B);  \/* Opcode 3B \/r *\/\n-  ins_encode( SetInstMark, OpcP, RegMem( op1, op2), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_mem );\n-%}\n-\n-\/\/\n-\/\/ This will generate a signed flags result. This should be ok\n-\/\/ since any compare to a zero should be eq\/neq.\n-instruct testP_reg( eFlagsReg cr, eRegP src, immP0 zero ) %{\n-  match(Set cr (CmpP src zero));\n-\n-  format %{ \"TEST   $src,$src\" %}\n-  opcode(0x85);\n-  ins_encode( OpcP, RegReg( src, src ) );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-\/\/ Cisc-spilled version of testP_reg\n-\/\/ This will generate a signed flags result. This should be ok\n-\/\/ since any compare to a zero should be eq\/neq.\n-instruct testP_Reg_mem( eFlagsReg cr, memory op, immI_0 zero ) %{\n-  match(Set cr (CmpP (LoadP op) zero));\n-\n-  format %{ \"TEST   $op,0xFFFFFFFF\" %}\n-  ins_cost(500);\n-  opcode(0xF7);               \/* Opcode F7 \/0 *\/\n-  ins_encode( SetInstMark, OpcP, RMopc_Mem(0x00,op), Con_d32(0xFFFFFFFF), ClearInstMark );\n-  ins_pipe( ialu_cr_reg_imm );\n-%}\n-\n-\/\/ Yanked all unsigned pointer compare operations.\n-\/\/ Pointer compares are done with CmpP which is already unsigned.\n-\n-\/\/----------Max and Min--------------------------------------------------------\n-\/\/ Min Instructions\n-\/\/\/\/\n-\/\/   *** Min and Max using the conditional move are slower than the\n-\/\/   *** branch version on a Pentium III.\n-\/\/ \/\/ Conditional move for min\n-\/\/instruct cmovI_reg_lt( rRegI op2, rRegI op1, eFlagsReg cr ) %{\n-\/\/  effect( USE_DEF op2, USE op1, USE cr );\n-\/\/  format %{ \"CMOVlt $op2,$op1\\t! min\" %}\n-\/\/  opcode(0x4C,0x0F);\n-\/\/  ins_encode( OpcS, OpcP, RegReg( op2, op1 ) );\n-\/\/  ins_pipe( pipe_cmov_reg );\n-\/\/%}\n-\/\/\n-\/\/\/\/ Min Register with Register (P6 version)\n-\/\/instruct minI_eReg_p6( rRegI op1, rRegI op2 ) %{\n-\/\/  predicate(VM_Version::supports_cmov() );\n-\/\/  match(Set op2 (MinI op1 op2));\n-\/\/  ins_cost(200);\n-\/\/  expand %{\n-\/\/    eFlagsReg cr;\n-\/\/    compI_eReg(cr,op1,op2);\n-\/\/    cmovI_reg_lt(op2,op1,cr);\n-\/\/  %}\n-\/\/%}\n-\n-\/\/ Min Register with Register (generic version)\n-instruct minI_eReg(rRegI dst, rRegI src, eFlagsReg flags) %{\n-  match(Set dst (MinI dst src));\n-  effect(KILL flags);\n-  ins_cost(300);\n-\n-  format %{ \"MIN    $dst,$src\" %}\n-  opcode(0xCC);\n-  ins_encode( min_enc(dst,src) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Max Register with Register\n-\/\/   *** Min and Max using the conditional move are slower than the\n-\/\/   *** branch version on a Pentium III.\n-\/\/ \/\/ Conditional move for max\n-\/\/instruct cmovI_reg_gt( rRegI op2, rRegI op1, eFlagsReg cr ) %{\n-\/\/  effect( USE_DEF op2, USE op1, USE cr );\n-\/\/  format %{ \"CMOVgt $op2,$op1\\t! max\" %}\n-\/\/  opcode(0x4F,0x0F);\n-\/\/  ins_encode( OpcS, OpcP, RegReg( op2, op1 ) );\n-\/\/  ins_pipe( pipe_cmov_reg );\n-\/\/%}\n-\/\/\n-\/\/ \/\/ Max Register with Register (P6 version)\n-\/\/instruct maxI_eReg_p6( rRegI op1, rRegI op2 ) %{\n-\/\/  predicate(VM_Version::supports_cmov() );\n-\/\/  match(Set op2 (MaxI op1 op2));\n-\/\/  ins_cost(200);\n-\/\/  expand %{\n-\/\/    eFlagsReg cr;\n-\/\/    compI_eReg(cr,op1,op2);\n-\/\/    cmovI_reg_gt(op2,op1,cr);\n-\/\/  %}\n-\/\/%}\n-\n-\/\/ Max Register with Register (generic version)\n-instruct maxI_eReg(rRegI dst, rRegI src, eFlagsReg flags) %{\n-  match(Set dst (MaxI dst src));\n-  effect(KILL flags);\n-  ins_cost(300);\n-\n-  format %{ \"MAX    $dst,$src\" %}\n-  opcode(0xCC);\n-  ins_encode( max_enc(dst,src) );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Counted Loop limit node which represents exact final iterator value.\n-\/\/ Note: the resulting value should fit into integer range since\n-\/\/ counted loops have limit check on overflow.\n-instruct loopLimit_eReg(eAXRegI limit, nadxRegI init, immI stride, eDXRegI limit_hi, nadxRegI tmp, eFlagsReg flags) %{\n-  match(Set limit (LoopLimit (Binary init limit) stride));\n-  effect(TEMP limit_hi, TEMP tmp, KILL flags);\n-  ins_cost(300);\n-\n-  format %{ \"loopLimit $init,$limit,$stride  # $limit = $init + $stride *( $limit - $init + $stride -1)\/ $stride, kills $limit_hi\" %}\n-  ins_encode %{\n-    int strd = (int)$stride$$constant;\n-    assert(strd != 1 && strd != -1, \"sanity\");\n-    int m1 = (strd > 0) ? 1 : -1;\n-    \/\/ Convert limit to long (EAX:EDX)\n-    __ cdql();\n-    \/\/ Convert init to long (init:tmp)\n-    __ movl($tmp$$Register, $init$$Register);\n-    __ sarl($tmp$$Register, 31);\n-    \/\/ $limit - $init\n-    __ subl($limit$$Register, $init$$Register);\n-    __ sbbl($limit_hi$$Register, $tmp$$Register);\n-    \/\/ + ($stride - 1)\n-    if (strd > 0) {\n-      __ addl($limit$$Register, (strd - 1));\n-      __ adcl($limit_hi$$Register, 0);\n-      __ movl($tmp$$Register, strd);\n-    } else {\n-      __ addl($limit$$Register, (strd + 1));\n-      __ adcl($limit_hi$$Register, -1);\n-      __ lneg($limit_hi$$Register, $limit$$Register);\n-      __ movl($tmp$$Register, -strd);\n-    }\n-    \/\/ signed division: (EAX:EDX) \/ pos_stride\n-    __ idivl($tmp$$Register);\n-    if (strd < 0) {\n-      \/\/ restore sign\n-      __ negl($tmp$$Register);\n-    }\n-    \/\/ (EAX) * stride\n-    __ mull($tmp$$Register);\n-    \/\/ + init (ignore upper bits)\n-    __ addl($limit$$Register, $init$$Register);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Branch Instructions\n-\/\/ Jump Table\n-instruct jumpXtnd(rRegI switch_val) %{\n-  match(Jump switch_val);\n-  ins_cost(350);\n-  format %{  \"JMP    [$constantaddress](,$switch_val,1)\\n\\t\" %}\n-  ins_encode %{\n-    \/\/ Jump to Address(table_base + switch_reg)\n-    Address index(noreg, $switch_val$$Register, Address::times_1);\n-    __ jump(ArrayAddress($constantaddress, index), noreg);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Jump Direct - Label defines a relative address from JMP+1\n-instruct jmpDir(label labl) %{\n-  match(Goto);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"JMP    $labl\" %}\n-  size(5);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jmp(*L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe( pipe_jmp );\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpCon(cmpOp cop, eFlagsReg cr, label labl) %{\n-  match(If cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop    $labl\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEnd(cmpOp cop, eFlagsReg cr, label labl) %{\n-  match(CountedLoopEnd cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop    $labl\\t# Loop end\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n-\/\/ Jump Direct Conditional - using unsigned comparison\n-instruct jmpConU(cmpOpU cop, eFlagsRegU cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,u  $labl\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-instruct jmpConUCF(cmpOpUCF cop, eFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(200);\n-  format %{ \"J$cop,u  $labl\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-instruct jmpConUCF2(cmpOpUCF2 cop, eFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(200);\n-  format %{ $$template\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      $$emit$$\"JP,u   $labl\\n\\t\"\n-      $$emit$$\"J$cop,u   $labl\"\n-    } else {\n-      $$emit$$\"JP,u   done\\n\\t\"\n-      $$emit$$\"J$cop,u   $labl\\n\\t\"\n-      $$emit$$\"done:\"\n-    }\n-  %}\n-  ins_encode %{\n-    Label* l = $labl$$label;\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      __ jcc(Assembler::parity, *l, false);\n-      __ jcc(Assembler::notEqual, *l, false);\n-    } else if ($cop$$cmpcode == Assembler::equal) {\n-      Label done;\n-      __ jccb(Assembler::parity, done);\n-      __ jcc(Assembler::equal, *l, false);\n-      __ bind(done);\n-    } else {\n-       ShouldNotReachHere();\n-    }\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary superklass\n-\/\/ array for an instance of the superklass.  Set a hidden internal cache on a\n-\/\/ hit (cache is checked with exposed code in gen_subtype_check()).  Return\n-\/\/ NZ for a miss or zero for a hit.  The encoding ALSO sets flags.\n-instruct partialSubtypeCheck( eDIRegP result, eSIRegP sub, eAXRegP super, eCXRegI rcx, eFlagsReg cr ) %{\n-  match(Set result (PartialSubtypeCheck sub super));\n-  effect( KILL rcx, KILL cr );\n-\n-  ins_cost(1100);  \/\/ slightly larger than the next version\n-  format %{ \"MOV    EDI,[$sub+Klass::secondary_supers]\\n\\t\"\n-            \"MOV    ECX,[EDI+ArrayKlass::length]\\t# length to scan\\n\\t\"\n-            \"ADD    EDI,ArrayKlass::base_offset\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n-            \"REPNE SCASD\\t# Scan *EDI++ for a match with EAX while CX-- != 0\\n\\t\"\n-            \"JNE,s  miss\\t\\t# Missed: EDI not-zero\\n\\t\"\n-            \"MOV    [$sub+Klass::secondary_super_cache],$super\\t# Hit: update cache\\n\\t\"\n-            \"XOR    $result,$result\\t\\t Hit: EDI zero\\n\\t\"\n-     \"miss:\\t\" %}\n-\n-  opcode(0x1); \/\/ Force a XOR of EDI\n-  ins_encode( enc_PartialSubtypeCheck() );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct partialSubtypeCheck_vs_Zero( eFlagsReg cr, eSIRegP sub, eAXRegP super, eCXRegI rcx, eDIRegP result, immP0 zero ) %{\n-  match(Set cr (CmpP (PartialSubtypeCheck sub super) zero));\n-  effect( KILL rcx, KILL result );\n-\n-  ins_cost(1000);\n-  format %{ \"MOV    EDI,[$sub+Klass::secondary_supers]\\n\\t\"\n-            \"MOV    ECX,[EDI+ArrayKlass::length]\\t# length to scan\\n\\t\"\n-            \"ADD    EDI,ArrayKlass::base_offset\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n-            \"REPNE SCASD\\t# Scan *EDI++ for a match with EAX while CX-- != 0\\n\\t\"\n-            \"JNE,s  miss\\t\\t# Missed: flags NZ\\n\\t\"\n-            \"MOV    [$sub+Klass::secondary_super_cache],$super\\t# Hit: update cache, flags Z\\n\\t\"\n-     \"miss:\\t\" %}\n-\n-  opcode(0x0);  \/\/ No need to XOR EDI\n-  ins_encode( enc_PartialSubtypeCheck() );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Branch Instructions -- short offset versions\n-\/\/\n-\/\/ These instructions are used to replace jumps of a long offset (the default\n-\/\/ match) with jumps of a shorter offset.  These instructions are all tagged\n-\/\/ with the ins_short_branch attribute, which causes the ADLC to suppress the\n-\/\/ match rules in general matching.  Instead, the ADLC generates a conversion\n-\/\/ method in the MachNode which can be used to do in-place replacement of the\n-\/\/ long variant with the shorter variant.  The compiler will determine if a\n-\/\/ branch can be taken by the is_short_branch_offset() predicate in the machine\n-\/\/ specific code section of the file.\n-\n-\/\/ Jump Direct - Label defines a relative address from JMP+1\n-instruct jmpDir_short(label labl) %{\n-  match(Goto);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"JMP,s  $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jmpb(*L);\n-  %}\n-  ins_pipe( pipe_jmp );\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpCon_short(cmpOp cop, eFlagsReg cr, label labl) %{\n-  match(If cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,s  $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe( pipe_jcc );\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEnd_short(cmpOp cop, eFlagsReg cr, label labl) %{\n-  match(CountedLoopEnd cop cr);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,s  $labl\\t# Loop end\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe( pipe_jcc );\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - using unsigned comparison\n-instruct jmpConU_short(cmpOpU cop, eFlagsRegU cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,us $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe( pipe_jcc );\n-  ins_short_branch(1);\n-%}\n-\n-instruct jmpConUCF_short(cmpOpUCF cop, eFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,us $labl\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe( pipe_jcc );\n-  ins_short_branch(1);\n-%}\n-\n-instruct jmpConUCF2_short(cmpOpUCF2 cop, eFlagsRegUCF cmp, label labl) %{\n-  match(If cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ $$template\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      $$emit$$\"JP,u,s   $labl\\n\\t\"\n-      $$emit$$\"J$cop,u,s   $labl\"\n-    } else {\n-      $$emit$$\"JP,u,s   done\\n\\t\"\n-      $$emit$$\"J$cop,u,s  $labl\\n\\t\"\n-      $$emit$$\"done:\"\n-    }\n-  %}\n-  size(4);\n-  ins_encode %{\n-    Label* l = $labl$$label;\n-    if ($cop$$cmpcode == Assembler::notEqual) {\n-      __ jccb(Assembler::parity, *l);\n-      __ jccb(Assembler::notEqual, *l);\n-    } else if ($cop$$cmpcode == Assembler::equal) {\n-      Label done;\n-      __ jccb(Assembler::parity, done);\n-      __ jccb(Assembler::equal, *l);\n-      __ bind(done);\n-    } else {\n-       ShouldNotReachHere();\n-    }\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Long Compare\n-\/\/\n-\/\/ Currently we hold longs in 2 registers.  Comparing such values efficiently\n-\/\/ is tricky.  The flavor of compare used depends on whether we are testing\n-\/\/ for LT, LE, or EQ.  For a simple LT test we can check just the sign bit.\n-\/\/ The GE test is the negated LT test.  The LE test can be had by commuting\n-\/\/ the operands (yielding a GE test) and then negating; negate again for the\n-\/\/ GT test.  The EQ test is done by ORcc'ing the high and low halves, and the\n-\/\/ NE test is negated from that.\n-\n-\/\/ Due to a shortcoming in the ADLC, it mixes up expressions like:\n-\/\/ (foo (CmpI (CmpL X Y) 0)) and (bar (CmpI (CmpL X 0L) 0)).  Note the\n-\/\/ difference between 'Y' and '0L'.  The tree-matches for the CmpI sections\n-\/\/ are collapsed internally in the ADLC's dfa-gen code.  The match for\n-\/\/ (CmpI (CmpL X Y) 0) is silently replaced with (CmpI (CmpL X 0L) 0) and the\n-\/\/ foo match ends up with the wrong leaf.  One fix is to not match both\n-\/\/ reg-reg and reg-zero forms of long-compare.  This is unfortunate because\n-\/\/ both forms beat the trinary form of long-compare and both are very useful\n-\/\/ on Intel which has so few registers.\n-\n-\/\/ Manifest a CmpL result in an integer register.  Very painful.\n-\/\/ This is the test to avoid.\n-instruct cmpL3_reg_reg(eSIRegI dst, eRegL src1, eRegL src2, eFlagsReg flags ) %{\n-  match(Set dst (CmpL3 src1 src2));\n-  effect( KILL flags );\n-  ins_cost(1000);\n-  format %{ \"XOR    $dst,$dst\\n\\t\"\n-            \"CMP    $src1.hi,$src2.hi\\n\\t\"\n-            \"JLT,s  m_one\\n\\t\"\n-            \"JGT,s  p_one\\n\\t\"\n-            \"CMP    $src1.lo,$src2.lo\\n\\t\"\n-            \"JB,s   m_one\\n\\t\"\n-            \"JEQ,s  done\\n\"\n-    \"p_one:\\tINC    $dst\\n\\t\"\n-            \"JMP,s  done\\n\"\n-    \"m_one:\\tDEC    $dst\\n\"\n-     \"done:\" %}\n-  ins_encode %{\n-    Label p_one, m_one, done;\n-    __ xorptr($dst$$Register, $dst$$Register);\n-    __ cmpl(HIGH_FROM_LOW($src1$$Register), HIGH_FROM_LOW($src2$$Register));\n-    __ jccb(Assembler::less,    m_one);\n-    __ jccb(Assembler::greater, p_one);\n-    __ cmpl($src1$$Register, $src2$$Register);\n-    __ jccb(Assembler::below,   m_one);\n-    __ jccb(Assembler::equal,   done);\n-    __ bind(p_one);\n-    __ incrementl($dst$$Register);\n-    __ jmpb(done);\n-    __ bind(m_one);\n-    __ decrementl($dst$$Register);\n-    __ bind(done);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/======\n-\/\/ Manifest a CmpL result in the normal flags.  Only good for LT or GE\n-\/\/ compares.  Can be used for LE or GT compares by reversing arguments.\n-\/\/ NOT GOOD FOR EQ\/NE tests.\n-instruct cmpL_zero_flags_LTGE( flagsReg_long_LTGE flags, eRegL src, immL0 zero ) %{\n-  match( Set flags (CmpL src zero ));\n-  ins_cost(100);\n-  format %{ \"TEST   $src.hi,$src.hi\" %}\n-  opcode(0x85);\n-  ins_encode( OpcP, RegReg_Hi2( src, src ) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-\/\/ Manifest a CmpL result in the normal flags.  Only good for LT or GE\n-\/\/ compares.  Can be used for LE or GT compares by reversing arguments.\n-\/\/ NOT GOOD FOR EQ\/NE tests.\n-instruct cmpL_reg_flags_LTGE( flagsReg_long_LTGE flags, eRegL src1, eRegL src2, rRegI tmp ) %{\n-  match( Set flags (CmpL src1 src2 ));\n-  effect( TEMP tmp );\n-  ins_cost(300);\n-  format %{ \"CMP    $src1.lo,$src2.lo\\t! Long compare; set flags for low bits\\n\\t\"\n-            \"MOV    $tmp,$src1.hi\\n\\t\"\n-            \"SBB    $tmp,$src2.hi\\t! Compute flags for long compare\" %}\n-  ins_encode( long_cmp_flags2( src1, src2, tmp ) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-\/\/ Long compares reg < zero\/req OR reg >= zero\/req.\n-\/\/ Just a wrapper for a normal branch, plus the predicate test.\n-instruct cmpL_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, label labl) %{\n-  match(If cmp flags);\n-  effect(USE labl);\n-  predicate( _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge );\n-  expand %{\n-    jmpCon(cmp,flags,labl);    \/\/ JLT or JGE...\n-  %}\n-%}\n-\n-\/\/======\n-\/\/ Manifest a CmpUL result in the normal flags.  Only good for LT or GE\n-\/\/ compares.  Can be used for LE or GT compares by reversing arguments.\n-\/\/ NOT GOOD FOR EQ\/NE tests.\n-instruct cmpUL_zero_flags_LTGE(flagsReg_ulong_LTGE flags, eRegL src, immL0 zero) %{\n-  match(Set flags (CmpUL src zero));\n-  ins_cost(100);\n-  format %{ \"TEST   $src.hi,$src.hi\" %}\n-  opcode(0x85);\n-  ins_encode(OpcP, RegReg_Hi2(src, src));\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-\/\/ Manifest a CmpUL result in the normal flags.  Only good for LT or GE\n-\/\/ compares.  Can be used for LE or GT compares by reversing arguments.\n-\/\/ NOT GOOD FOR EQ\/NE tests.\n-instruct cmpUL_reg_flags_LTGE(flagsReg_ulong_LTGE flags, eRegL src1, eRegL src2, rRegI tmp) %{\n-  match(Set flags (CmpUL src1 src2));\n-  effect(TEMP tmp);\n-  ins_cost(300);\n-  format %{ \"CMP    $src1.lo,$src2.lo\\t! Unsigned long compare; set flags for low bits\\n\\t\"\n-            \"MOV    $tmp,$src1.hi\\n\\t\"\n-            \"SBB    $tmp,$src2.hi\\t! Compute flags for unsigned long compare\" %}\n-  ins_encode(long_cmp_flags2(src1, src2, tmp));\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-\/\/ Unsigned long compares reg < zero\/req OR reg >= zero\/req.\n-\/\/ Just a wrapper for a normal branch, plus the predicate test.\n-instruct cmpUL_LTGE(cmpOpU cmp, flagsReg_ulong_LTGE flags, label labl) %{\n-  match(If cmp flags);\n-  effect(USE labl);\n-  predicate(_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge);\n-  expand %{\n-    jmpCon(cmp, flags, labl);    \/\/ JLT or JGE...\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE longs.\n-instruct cmovLL_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, eRegL dst, eRegL src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst src)));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  ins_cost(400);\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg_Lo2( dst, src ), enc_cmov(cmp), RegReg_Hi2( dst, src ) );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovLL_mem_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, eRegL dst, load_long_memory src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst (LoadL src))));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  ins_cost(500);\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cmp), RegMem(dst, src), enc_cmov(cmp), RegMem_Hi(dst, src), ClearInstMark );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovLL_reg_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, eRegL dst, eRegL src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst src)));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  ins_cost(400);\n-  expand %{\n-    cmovLL_reg_LTGE(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-instruct cmovLL_mem_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, eRegL dst, load_long_memory src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst (LoadL src))));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  ins_cost(500);\n-  expand %{\n-    cmovLL_mem_LTGE(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE ints.\n-instruct cmovII_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, rRegI dst, rRegI src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovII_mem_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cmp), RegMem( dst, src ), ClearInstMark );\n-  ins_pipe( pipe_cmov_mem );\n-%}\n-\n-instruct cmovII_reg_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, rRegI dst, rRegI src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovII_reg_LTGE(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-instruct cmovII_mem_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  expand %{\n-    cmovII_mem_LTGE(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE ptrs.\n-instruct cmovPP_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, eRegP dst, eRegP src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveP (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-\/\/ Compare 2 unsigned longs and CMOVE ptrs.\n-instruct cmovPP_reg_LTGE_U(cmpOpU cmp, flagsReg_ulong_LTGE flags, eRegP dst, eRegP src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveP (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovPP_reg_LTGE(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE doubles\n-instruct cmovDDPR_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, regDPR dst, regDPR src) %{\n-  predicate( UseSSE<=1 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveD (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovDPR_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE doubles\n-instruct cmovDD_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, regD dst, regD src) %{\n-  predicate( UseSSE>=2 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveD (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovD_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-instruct cmovFFPR_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, regFPR dst, regFPR src) %{\n-  predicate( UseSSE==0 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveF (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovFPR_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-instruct cmovFF_reg_LTGE(cmpOp cmp, flagsReg_long_LTGE flags, regF dst, regF src) %{\n-  predicate( UseSSE>=1 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::lt || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ge ));\n-  match(Set dst (CMoveF (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovF_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/======\n-\/\/ Manifest a CmpL result in the normal flags.  Only good for EQ\/NE compares.\n-instruct cmpL_zero_flags_EQNE( flagsReg_long_EQNE flags, eRegL src, immL0 zero, rRegI tmp ) %{\n-  match( Set flags (CmpL src zero ));\n-  effect(TEMP tmp);\n-  ins_cost(200);\n-  format %{ \"MOV    $tmp,$src.lo\\n\\t\"\n-            \"OR     $tmp,$src.hi\\t! Long is EQ\/NE 0?\" %}\n-  ins_encode( long_cmp_flags0( src, tmp ) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Manifest a CmpL result in the normal flags.  Only good for EQ\/NE compares.\n-instruct cmpL_reg_flags_EQNE( flagsReg_long_EQNE flags, eRegL src1, eRegL src2 ) %{\n-  match( Set flags (CmpL src1 src2 ));\n-  ins_cost(200+300);\n-  format %{ \"CMP    $src1.lo,$src2.lo\\t! Long compare; set flags for low bits\\n\\t\"\n-            \"JNE,s  skip\\n\\t\"\n-            \"CMP    $src1.hi,$src2.hi\\n\\t\"\n-     \"skip:\\t\" %}\n-  ins_encode( long_cmp_flags1( src1, src2 ) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-\/\/ Long compare reg == zero\/reg OR reg != zero\/reg\n-\/\/ Just a wrapper for a normal branch, plus the predicate test.\n-instruct cmpL_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, label labl) %{\n-  match(If cmp flags);\n-  effect(USE labl);\n-  predicate( _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne );\n-  expand %{\n-    jmpCon(cmp,flags,labl);    \/\/ JEQ or JNE...\n-  %}\n-%}\n-\n-\/\/======\n-\/\/ Manifest a CmpUL result in the normal flags.  Only good for EQ\/NE compares.\n-instruct cmpUL_zero_flags_EQNE(flagsReg_ulong_EQNE flags, eRegL src, immL0 zero, rRegI tmp) %{\n-  match(Set flags (CmpUL src zero));\n-  effect(TEMP tmp);\n-  ins_cost(200);\n-  format %{ \"MOV    $tmp,$src.lo\\n\\t\"\n-            \"OR     $tmp,$src.hi\\t! Unsigned long is EQ\/NE 0?\" %}\n-  ins_encode(long_cmp_flags0(src, tmp));\n-  ins_pipe(ialu_reg_reg_long);\n-%}\n-\n-\/\/ Manifest a CmpUL result in the normal flags.  Only good for EQ\/NE compares.\n-instruct cmpUL_reg_flags_EQNE(flagsReg_ulong_EQNE flags, eRegL src1, eRegL src2) %{\n-  match(Set flags (CmpUL src1 src2));\n-  ins_cost(200+300);\n-  format %{ \"CMP    $src1.lo,$src2.lo\\t! Unsigned long compare; set flags for low bits\\n\\t\"\n-            \"JNE,s  skip\\n\\t\"\n-            \"CMP    $src1.hi,$src2.hi\\n\\t\"\n-     \"skip:\\t\" %}\n-  ins_encode(long_cmp_flags1(src1, src2));\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-\/\/ Unsigned long compare reg == zero\/reg OR reg != zero\/reg\n-\/\/ Just a wrapper for a normal branch, plus the predicate test.\n-instruct cmpUL_EQNE(cmpOpU cmp, flagsReg_ulong_EQNE flags, label labl) %{\n-  match(If cmp flags);\n-  effect(USE labl);\n-  predicate(_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne);\n-  expand %{\n-    jmpCon(cmp, flags, labl);    \/\/ JEQ or JNE...\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE longs.\n-instruct cmovLL_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, eRegL dst, eRegL src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst src)));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  ins_cost(400);\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg_Lo2( dst, src ), enc_cmov(cmp), RegReg_Hi2( dst, src ) );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovLL_mem_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, eRegL dst, load_long_memory src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst (LoadL src))));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  ins_cost(500);\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cmp), RegMem(dst, src), enc_cmov(cmp), RegMem_Hi(dst, src), ClearInstMark );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE ints.\n-instruct cmovII_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, rRegI dst, rRegI src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovII_mem_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cmp), RegMem( dst, src ), ClearInstMark );\n-  ins_pipe( pipe_cmov_mem );\n-%}\n-\n-instruct cmovII_reg_EQNE_U(cmpOpU cmp, flagsReg_ulong_EQNE flags, rRegI dst, rRegI src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovII_reg_EQNE(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-instruct cmovII_mem_EQNE_U(cmpOpU cmp, flagsReg_ulong_EQNE flags, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  expand %{\n-    cmovII_mem_EQNE(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE ptrs.\n-instruct cmovPP_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, eRegP dst, eRegP src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveP (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-\/\/ Compare 2 unsigned longs and CMOVE ptrs.\n-instruct cmovPP_reg_EQNE_U(cmpOpU cmp, flagsReg_ulong_EQNE flags, eRegP dst, eRegP src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveP (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovPP_reg_EQNE(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE doubles\n-instruct cmovDDPR_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, regDPR dst, regDPR src) %{\n-  predicate( UseSSE<=1 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveD (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovDPR_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE doubles\n-instruct cmovDD_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, regD dst, regD src) %{\n-  predicate( UseSSE>=2 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveD (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovD_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-instruct cmovFFPR_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, regFPR dst, regFPR src) %{\n-  predicate( UseSSE==0 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveF (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovFPR_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-instruct cmovFF_reg_EQNE(cmpOp cmp, flagsReg_long_EQNE flags, regF dst, regF src) %{\n-  predicate( UseSSE>=1 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::eq || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::ne ));\n-  match(Set dst (CMoveF (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovF_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/======\n-\/\/ Manifest a CmpL result in the normal flags.  Only good for LE or GT compares.\n-\/\/ Same as cmpL_reg_flags_LEGT except must negate src\n-instruct cmpL_zero_flags_LEGT( flagsReg_long_LEGT flags, eRegL src, immL0 zero, rRegI tmp ) %{\n-  match( Set flags (CmpL src zero ));\n-  effect( TEMP tmp );\n-  ins_cost(300);\n-  format %{ \"XOR    $tmp,$tmp\\t# Long compare for -$src < 0, use commuted test\\n\\t\"\n-            \"CMP    $tmp,$src.lo\\n\\t\"\n-            \"SBB    $tmp,$src.hi\\n\\t\" %}\n-  ins_encode( long_cmp_flags3(src, tmp) );\n-  ins_pipe( ialu_reg_reg_long );\n-%}\n-\n-\/\/ Manifest a CmpL result in the normal flags.  Only good for LE or GT compares.\n-\/\/ Same as cmpL_reg_flags_LTGE except operands swapped.  Swapping operands\n-\/\/ requires a commuted test to get the same result.\n-instruct cmpL_reg_flags_LEGT( flagsReg_long_LEGT flags, eRegL src1, eRegL src2, rRegI tmp ) %{\n-  match( Set flags (CmpL src1 src2 ));\n-  effect( TEMP tmp );\n-  ins_cost(300);\n-  format %{ \"CMP    $src2.lo,$src1.lo\\t! Long compare, swapped operands, use with commuted test\\n\\t\"\n-            \"MOV    $tmp,$src2.hi\\n\\t\"\n-            \"SBB    $tmp,$src1.hi\\t! Compute flags for long compare\" %}\n-  ins_encode( long_cmp_flags2( src2, src1, tmp ) );\n-  ins_pipe( ialu_cr_reg_reg );\n-%}\n-\n-\/\/ Long compares reg < zero\/req OR reg >= zero\/req.\n-\/\/ Just a wrapper for a normal branch, plus the predicate test\n-instruct cmpL_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, label labl) %{\n-  match(If cmp flags);\n-  effect(USE labl);\n-  predicate( _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt || _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le );\n-  ins_cost(300);\n-  expand %{\n-    jmpCon(cmp,flags,labl);    \/\/ JGT or JLE...\n-  %}\n-%}\n-\n-\/\/======\n-\/\/ Manifest a CmpUL result in the normal flags.  Only good for LE or GT compares.\n-\/\/ Same as cmpUL_reg_flags_LEGT except must negate src\n-instruct cmpUL_zero_flags_LEGT(flagsReg_ulong_LEGT flags, eRegL src, immL0 zero, rRegI tmp) %{\n-  match(Set flags (CmpUL src zero));\n-  effect(TEMP tmp);\n-  ins_cost(300);\n-  format %{ \"XOR    $tmp,$tmp\\t# Unsigned long compare for -$src < 0, use commuted test\\n\\t\"\n-            \"CMP    $tmp,$src.lo\\n\\t\"\n-            \"SBB    $tmp,$src.hi\\n\\t\" %}\n-  ins_encode(long_cmp_flags3(src, tmp));\n-  ins_pipe(ialu_reg_reg_long);\n-%}\n-\n-\/\/ Manifest a CmpUL result in the normal flags.  Only good for LE or GT compares.\n-\/\/ Same as cmpUL_reg_flags_LTGE except operands swapped.  Swapping operands\n-\/\/ requires a commuted test to get the same result.\n-instruct cmpUL_reg_flags_LEGT(flagsReg_ulong_LEGT flags, eRegL src1, eRegL src2, rRegI tmp) %{\n-  match(Set flags (CmpUL src1 src2));\n-  effect(TEMP tmp);\n-  ins_cost(300);\n-  format %{ \"CMP    $src2.lo,$src1.lo\\t! Unsigned long compare, swapped operands, use with commuted test\\n\\t\"\n-            \"MOV    $tmp,$src2.hi\\n\\t\"\n-            \"SBB    $tmp,$src1.hi\\t! Compute flags for unsigned long compare\" %}\n-  ins_encode(long_cmp_flags2( src2, src1, tmp));\n-  ins_pipe(ialu_cr_reg_reg);\n-%}\n-\n-\/\/ Unsigned long compares reg < zero\/req OR reg >= zero\/req.\n-\/\/ Just a wrapper for a normal branch, plus the predicate test\n-instruct cmpUL_LEGT(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, label labl) %{\n-  match(If cmp flags);\n-  effect(USE labl);\n-  predicate(_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt || _kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le);\n-  ins_cost(300);\n-  expand %{\n-    jmpCon(cmp, flags, labl);    \/\/ JGT or JLE...\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE longs.\n-instruct cmovLL_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, eRegL dst, eRegL src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst src)));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  ins_cost(400);\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg_Lo2( dst, src ), enc_cmov(cmp), RegReg_Hi2( dst, src ) );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovLL_mem_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, eRegL dst, load_long_memory src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst (LoadL src))));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  ins_cost(500);\n-  format %{ \"CMOV$cmp $dst.lo,$src.lo\\n\\t\"\n-            \"CMOV$cmp $dst.hi,$src.hi+4\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cmp), RegMem(dst, src), enc_cmov(cmp), RegMem_Hi(dst, src), ClearInstMark );\n-  ins_pipe( pipe_cmov_reg_long );\n-%}\n-\n-instruct cmovLL_reg_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, eRegL dst, eRegL src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst src)));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  ins_cost(400);\n-  expand %{\n-    cmovLL_reg_LEGT(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-instruct cmovLL_mem_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, eRegL dst, load_long_memory src) %{\n-  match(Set dst (CMoveL (Binary cmp flags) (Binary dst (LoadL src))));\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  ins_cost(500);\n-  expand %{\n-    cmovLL_mem_LEGT(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE ints.\n-instruct cmovII_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, rRegI dst, rRegI src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-instruct cmovII_mem_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( SetInstMark, enc_cmov(cmp), RegMem( dst, src ), ClearInstMark );\n-  ins_pipe( pipe_cmov_mem );\n-%}\n-\n-instruct cmovII_reg_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, rRegI dst, rRegI src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovII_reg_LEGT(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-instruct cmovII_mem_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, rRegI dst, memory src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveI (Binary cmp flags) (Binary dst (LoadI src))));\n-  ins_cost(250);\n-  expand %{\n-    cmovII_mem_LEGT(cmp, flags, dst, src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE ptrs.\n-instruct cmovPP_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, eRegP dst, eRegP src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveP (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  format %{ \"CMOV$cmp $dst,$src\" %}\n-  opcode(0x0F,0x40);\n-  ins_encode( enc_cmov(cmp), RegReg( dst, src ) );\n-  ins_pipe( pipe_cmov_reg );\n-%}\n-\n-\/\/ Compare 2 unsigned longs and CMOVE ptrs.\n-instruct cmovPP_reg_LEGT_U(cmpOpU_commute cmp, flagsReg_ulong_LEGT flags, eRegP dst, eRegP src) %{\n-  predicate(VM_Version::supports_cmov() && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveP (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    cmovPP_reg_LEGT(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE doubles\n-instruct cmovDDPR_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, regDPR dst, regDPR src) %{\n-  predicate( UseSSE<=1 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveD (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovDPR_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\/\/ Compare 2 longs and CMOVE doubles\n-instruct cmovDD_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, regD dst, regD src) %{\n-  predicate( UseSSE>=2 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveD (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovD_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-instruct cmovFFPR_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, regFPR dst, regFPR src) %{\n-  predicate( UseSSE==0 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveF (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovFPR_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\n-instruct cmovFF_reg_LEGT(cmpOp_commute cmp, flagsReg_long_LEGT flags, regF dst, regF src) %{\n-  predicate( UseSSE>=1 && ( _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::le || _kids[0]->_kids[0]->_leaf->as_Bool()->_test._test == BoolTest::gt ));\n-  match(Set dst (CMoveF (Binary cmp flags) (Binary dst src)));\n-  ins_cost(200);\n-  expand %{\n-    fcmovF_regS(cmp,flags,dst,src);\n-  %}\n-%}\n-\n-\n-\/\/ ============================================================================\n-\/\/ Procedure Call\/Return Instructions\n-\/\/ Call Java Static Instruction\n-\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n-\/\/       compute_padding() functions will have to be adjusted.\n-instruct CallStaticJavaDirect(method meth) %{\n-  match(CallStaticJava);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"CALL,static \" %}\n-  opcode(0xE8); \/* E8 cd *\/\n-  ins_encode( pre_call_resets,\n-              Java_Static_Call( meth ),\n-              call_epilog,\n-              post_call_FPU );\n-  ins_pipe( pipe_slow );\n-  ins_alignment(4);\n-%}\n-\n-\/\/ Call Java Dynamic Instruction\n-\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n-\/\/       compute_padding() functions will have to be adjusted.\n-instruct CallDynamicJavaDirect(method meth) %{\n-  match(CallDynamicJava);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"MOV    EAX,(oop)-1\\n\\t\"\n-            \"CALL,dynamic\" %}\n-  opcode(0xE8); \/* E8 cd *\/\n-  ins_encode( pre_call_resets,\n-              Java_Dynamic_Call( meth ),\n-              call_epilog,\n-              post_call_FPU );\n-  ins_pipe( pipe_slow );\n-  ins_alignment(4);\n-%}\n-\n-\/\/ Call Runtime Instruction\n-instruct CallRuntimeDirect(method meth) %{\n-  match(CallRuntime );\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"CALL,runtime \" %}\n-  opcode(0xE8); \/* E8 cd *\/\n-  \/\/ Use FFREEs to clear entries in float stack\n-  ins_encode( pre_call_resets,\n-              FFree_Float_Stack_All,\n-              Java_To_Runtime( meth ),\n-              post_call_FPU );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ Call runtime without safepoint\n-instruct CallLeafDirect(method meth) %{\n-  match(CallLeaf);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"CALL_LEAF,runtime \" %}\n-  opcode(0xE8); \/* E8 cd *\/\n-  ins_encode( pre_call_resets,\n-              FFree_Float_Stack_All,\n-              Java_To_Runtime( meth ),\n-              Verify_FPU_For_Leaf, post_call_FPU );\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct CallLeafNoFPDirect(method meth) %{\n-  match(CallLeafNoFP);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"CALL_LEAF_NOFP,runtime \" %}\n-  opcode(0xE8); \/* E8 cd *\/\n-  ins_encode(pre_call_resets, Java_To_Runtime(meth));\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\n-\/\/ Return Instruction\n-\/\/ Remove the return address & jump to it.\n-instruct Ret() %{\n-  match(Return);\n-  format %{ \"RET\" %}\n-  opcode(0xC3);\n-  ins_encode(OpcP);\n-  ins_pipe( pipe_jmp );\n-%}\n-\n-\/\/ Tail Call; Jump from runtime stub to Java code.\n-\/\/ Also known as an 'interprocedural jump'.\n-\/\/ Target of jump will eventually return to caller.\n-\/\/ TailJump below removes the return address.\n-\/\/ Don't use ebp for 'jump_target' because a MachEpilogNode has already been\n-\/\/ emitted just above the TailCall which has reset ebp to the caller state.\n-instruct TailCalljmpInd(eRegP_no_EBP jump_target, eBXRegP method_ptr) %{\n-  match(TailCall jump_target method_ptr);\n-  ins_cost(300);\n-  format %{ \"JMP    $jump_target \\t# EBX holds method\" %}\n-  opcode(0xFF, 0x4);  \/* Opcode FF \/4 *\/\n-  ins_encode( OpcP, RegOpc(jump_target) );\n-  ins_pipe( pipe_jmp );\n-%}\n-\n-\n-\/\/ Tail Jump; remove the return address; jump to target.\n-\/\/ TailCall above leaves the return address around.\n-instruct tailjmpInd(eRegP_no_EBP jump_target, eAXRegP ex_oop) %{\n-  match( TailJump jump_target ex_oop );\n-  ins_cost(300);\n-  format %{ \"POP    EDX\\t# pop return address into dummy\\n\\t\"\n-            \"JMP    $jump_target \" %}\n-  opcode(0xFF, 0x4);  \/* Opcode FF \/4 *\/\n-  ins_encode( enc_pop_rdx,\n-              OpcP, RegOpc(jump_target) );\n-  ins_pipe( pipe_jmp );\n-%}\n-\n-\/\/ Forward exception.\n-instruct ForwardExceptionjmp()\n-%{\n-  match(ForwardException);\n-\n-  format %{ \"JMP    forward_exception_stub\" %}\n-  ins_encode %{\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()), noreg);\n-  %}\n-  ins_pipe(pipe_jmp);\n-%}\n-\n-\/\/ Create exception oop: created by stack-crawling runtime code.\n-\/\/ Created exception is now available to this handler, and is setup\n-\/\/ just prior to jumping to this handler.  No code emitted.\n-instruct CreateException( eAXRegP ex_oop )\n-%{\n-  match(Set ex_oop (CreateEx));\n-\n-  size(0);\n-  \/\/ use the following format syntax\n-  format %{ \"# exception oop is in EAX; no code emitted\" %}\n-  ins_encode();\n-  ins_pipe( empty );\n-%}\n-\n-\n-\/\/ Rethrow exception:\n-\/\/ The exception oop will come in the first argument position.\n-\/\/ Then JUMP (not call) to the rethrow stub code.\n-instruct RethrowException()\n-%{\n-  match(Rethrow);\n-\n-  \/\/ use the following format syntax\n-  format %{ \"JMP    rethrow_stub\" %}\n-  ins_encode(enc_rethrow);\n-  ins_pipe( pipe_jmp );\n-%}\n-\n-\/\/ inlined locking and unlocking\n-\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n-  ins_cost(300);\n-  format %{ \"FASTLOCK $object,$box\\t! kills $box,$tmp,$scr\" %}\n-  ins_encode %{\n-    __ get_thread($thread$$Register);\n-    __ fast_lock($object$$Register, $box$$Register, $tmp$$Register,\n-                 $scr$$Register, noreg, noreg, $thread$$Register, nullptr);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpFastUnlock(eFlagsReg cr, eRegP object, eAXRegP box, eRegP tmp ) %{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp, USE_KILL box);\n-  ins_cost(300);\n-  format %{ \"FASTUNLOCK $object,$box\\t! kills $box,$tmp\" %}\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpFastLockLightweight(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI eax_reg, eRegP tmp, eRegP thread) %{\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP eax_reg, TEMP tmp, USE_KILL box, TEMP thread);\n-  ins_cost(300);\n-  format %{ \"FASTLOCK $object,$box\\t! kills $box,$eax_reg,$tmp\" %}\n-  ins_encode %{\n-    __ get_thread($thread$$Register);\n-    __ fast_lock_lightweight($object$$Register, $box$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct cmpFastUnlockLightweight(eFlagsReg cr, eRegP object, eAXRegP eax_reg, eRegP tmp, eRegP thread) %{\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object eax_reg));\n-  effect(TEMP tmp, USE_KILL eax_reg, TEMP thread);\n-  ins_cost(300);\n-  format %{ \"FASTUNLOCK $object,$eax_reg\\t! kills $eax_reg,$tmp\" %}\n-  ins_encode %{\n-    __ get_thread($thread$$Register);\n-    __ fast_unlock_lightweight($object$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n-instruct mask_all_evexL_LT32(kReg dst, eRegL src) %{\n-  predicate(Matcher::vector_length(n) <= 32);\n-  match(Set dst (MaskAll src));\n-  format %{ \"mask_all_evexL_LE32 $dst, $src \\t\" %}\n-  ins_encode %{\n-    int mask_len = Matcher::vector_length(this);\n-    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct mask_all_evexL_GT32(kReg dst, eRegL src, kReg ktmp) %{\n-  predicate(Matcher::vector_length(n) > 32);\n-  match(Set dst (MaskAll src));\n-  effect(TEMP ktmp);\n-  format %{ \"mask_all_evexL_GT32 $dst, $src \\t! using $ktmp as TEMP \" %}\n-  ins_encode %{\n-    int mask_len = Matcher::vector_length(this);\n-    __ vector_maskall_operation32($dst$$KRegister, $src$$Register, $ktmp$$KRegister, mask_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-instruct mask_all_evexI_GT32(kReg dst, rRegI src, kReg ktmp) %{\n-  predicate(Matcher::vector_length(n) > 32);\n-  match(Set dst (MaskAll src));\n-  effect(TEMP ktmp);\n-  format %{ \"mask_all_evexI_GT32 $dst, $src \\t! using $ktmp as TEMP\" %}\n-  ins_encode %{\n-    int mask_len = Matcher::vector_length(this);\n-    __ vector_maskall_operation32($dst$$KRegister, $src$$Register, $ktmp$$KRegister, mask_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n-\/\/ ============================================================================\n-\/\/ Safepoint Instruction\n-instruct safePoint_poll_tls(eFlagsReg cr, eRegP_no_EBP poll) %{\n-  match(SafePoint poll);\n-  effect(KILL cr, USE poll);\n-\n-  format %{ \"TSTL   #EAX,[$poll]\\t! Safepoint: poll for GC\" %}\n-  ins_cost(125);\n-  \/\/ EBP would need size(3)\n-  size(2); \/* setting an explicit size will cause debug builds to assert if size is incorrect *\/\n-  ins_encode %{\n-    __ set_inst_mark();\n-    __ relocate(relocInfo::poll_type);\n-    __ clear_inst_mark();\n-    address pre_pc = __ pc();\n-    __ testl(rax, Address($poll$$Register, 0));\n-    address post_pc = __ pc();\n-    guarantee(pre_pc[0] == 0x85, \"must emit test-ax [reg]\");\n-  %}\n-  ins_pipe(ialu_reg_mem);\n-%}\n-\n-\n-\/\/ ============================================================================\n-\/\/ This name is KNOWN by the ADLC and cannot be changed.\n-\/\/ The ADLC forces a 'TypeRawPtr::BOTTOM' output type\n-\/\/ for this guy.\n-instruct tlsLoadP(eRegP dst, eFlagsReg cr) %{\n-  match(Set dst (ThreadLocal));\n-  effect(DEF dst, KILL cr);\n-\n-  format %{ \"MOV    $dst, Thread::current()\" %}\n-  ins_encode %{\n-    Register dstReg = as_Register($dst$$reg);\n-    __ get_thread(dstReg);\n-  %}\n-  ins_pipe( ialu_reg_fat );\n-%}\n-\n-\n-\n-\/\/----------PEEPHOLE RULES-----------------------------------------------------\n-\/\/ These must follow all instruction definitions as they use the names\n-\/\/ defined in the instructions definitions.\n-\/\/\n-\/\/ peepmatch ( root_instr_name [preceding_instruction]* );\n-\/\/\n-\/\/ peepconstraint %{\n-\/\/ (instruction_number.operand_name relational_op instruction_number.operand_name\n-\/\/  [, ...] );\n-\/\/ \/\/ instruction numbers are zero-based using left to right order in peepmatch\n-\/\/\n-\/\/ peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );\n-\/\/ \/\/ provide an instruction_number.operand_name for each operand that appears\n-\/\/ \/\/ in the replacement instruction's match rule\n-\/\/\n-\/\/ ---------VM FLAGS---------------------------------------------------------\n-\/\/\n-\/\/ All peephole optimizations can be turned off using -XX:-OptoPeephole\n-\/\/\n-\/\/ Each peephole rule is given an identifying number starting with zero and\n-\/\/ increasing by one in the order seen by the parser.  An individual peephole\n-\/\/ can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#\n-\/\/ on the command-line.\n-\/\/\n-\/\/ ---------CURRENT LIMITATIONS----------------------------------------------\n-\/\/\n-\/\/ Only match adjacent instructions in same basic block\n-\/\/ Only equality constraints\n-\/\/ Only constraints between operands, not (0.dest_reg == EAX_enc)\n-\/\/ Only one replacement instruction\n-\/\/\n-\/\/ ---------EXAMPLE----------------------------------------------------------\n-\/\/\n-\/\/ \/\/ pertinent parts of existing instructions in architecture description\n-\/\/ instruct movI(rRegI dst, rRegI src) %{\n-\/\/   match(Set dst (CopyI src));\n-\/\/ %}\n-\/\/\n-\/\/ instruct incI_eReg(rRegI dst, immI_1 src, eFlagsReg cr) %{\n-\/\/   match(Set dst (AddI dst src));\n-\/\/   effect(KILL cr);\n-\/\/ %}\n-\/\/\n-\/\/ \/\/ Change (inc mov) to lea\n-\/\/ peephole %{\n-\/\/   \/\/ increment preceded by register-register move\n-\/\/   peepmatch ( incI_eReg movI );\n-\/\/   \/\/ require that the destination register of the increment\n-\/\/   \/\/ match the destination register of the move\n-\/\/   peepconstraint ( 0.dst == 1.dst );\n-\/\/   \/\/ construct a replacement instruction that sets\n-\/\/   \/\/ the destination to ( move's source register + one )\n-\/\/   peepreplace ( leaI_eReg_immI( 0.dst 1.src 0.src ) );\n-\/\/ %}\n-\/\/\n-\/\/ Implementation no longer uses movX instructions since\n-\/\/ machine-independent system no longer uses CopyX nodes.\n-\/\/\n-\/\/ peephole %{\n-\/\/   peepmatch ( incI_eReg movI );\n-\/\/   peepconstraint ( 0.dst == 1.dst );\n-\/\/   peepreplace ( leaI_eReg_immI( 0.dst 1.src 0.src ) );\n-\/\/ %}\n-\/\/\n-\/\/ peephole %{\n-\/\/   peepmatch ( decI_eReg movI );\n-\/\/   peepconstraint ( 0.dst == 1.dst );\n-\/\/   peepreplace ( leaI_eReg_immI( 0.dst 1.src 0.src ) );\n-\/\/ %}\n-\/\/\n-\/\/ peephole %{\n-\/\/   peepmatch ( addI_eReg_imm movI );\n-\/\/   peepconstraint ( 0.dst == 1.dst );\n-\/\/   peepreplace ( leaI_eReg_immI( 0.dst 1.src 0.src ) );\n-\/\/ %}\n-\/\/\n-\/\/ peephole %{\n-\/\/   peepmatch ( addP_eReg_imm movP );\n-\/\/   peepconstraint ( 0.dst == 1.dst );\n-\/\/   peepreplace ( leaP_eReg_immI( 0.dst 1.src 0.src ) );\n-\/\/ %}\n-\n-\/\/ \/\/ Change load of spilled value to only a spill\n-\/\/ instruct storeI(memory mem, rRegI src) %{\n-\/\/   match(Set mem (StoreI mem src));\n-\/\/ %}\n-\/\/\n-\/\/ instruct loadI(rRegI dst, memory mem) %{\n-\/\/   match(Set dst (LoadI mem));\n-\/\/ %}\n-\/\/\n-peephole %{\n-  peepmatch ( loadI storeI );\n-  peepconstraint ( 1.src == 0.dst, 1.mem == 0.mem );\n-  peepreplace ( storeI( 1.mem 1.mem 1.src ) );\n-%}\n-\n-\/\/----------SMARTSPILL RULES---------------------------------------------------\n-\/\/ These must follow all instruction definitions as they use the names\n-\/\/ defined in the instructions definitions.\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":0,"deletions":13846,"binary":false,"changes":13846,"status":"deleted"},{"patch":"@@ -855,1 +855,1 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->stub_function() != nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -104,2 +104,0 @@\n-  static int  get_fpu_control_word();\n-  static void set_fpu_control_word(int fpu_control);\n","filename":"src\/hotspot\/os\/linux\/os_linux.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -96,1 +96,0 @@\n-#ifdef AMD64\n@@ -138,45 +137,0 @@\n-#else \/\/ !AMD64\n-\n-extern \"C\" {\n-  \/\/ defined in bsd_x86.s\n-  int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);\n-  void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order \/* order *\/) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  return cmpxchg_using_helper<int64_t>(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);\n-}\n-\n-\/\/ No direct support for 8-byte xchg; emulate using cmpxchg.\n-template<>\n-struct Atomic::PlatformXchg<8> : Atomic::XchgUsingCmpxchg<8> {};\n-\n-\/\/ No direct support for 8-byte add; emulate using cmpxchg.\n-template<>\n-struct Atomic::PlatformAdd<8> : Atomic::AddUsingCmpxchg<8> {};\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  volatile int64_t dest;\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n-  return PrimitiveConversions::cast<T>(dest);\n-}\n-\n-template<>\n-template<typename T>\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n-                                                 T store_value) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n-}\n-\n-#endif \/\/ AMD64\n-\n@@ -219,1 +173,0 @@\n-#ifdef AMD64\n@@ -231,1 +184,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/atomic_bsd_x86.hpp","additions":0,"deletions":48,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -1,525 +0,0 @@\n-#\n-# Copyright (c) 2004, 2024, Oracle and\/or its affiliates. All rights reserved.\n-# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-#\n-# This code is free software; you can redistribute it and\/or modify it\n-# under the terms of the GNU General Public License version 2 only, as\n-# published by the Free Software Foundation.\n-#\n-# This code is distributed in the hope that it will be useful, but WITHOUT\n-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-# version 2 for more details (a copy is included in the LICENSE file that\n-# accompanied this code).\n-#\n-# You should have received a copy of the GNU General Public License version\n-# 2 along with this work; if not, write to the Free Software Foundation,\n-# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-#\n-# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-# or visit www.oracle.com if you need additional information or have any\n-# questions.\n-#\n-\n-#include \"defs.S.inc\"\n-\n-        # NOTE WELL!  The _Copy functions are called directly\n-        # from server-compiler-generated code via CallLeafNoFP,\n-        # which means that they *must* either not use floating\n-        # point or use it in the same manner as does the server\n-        # compiler.\n-\n-        .text\n-\n-# Set fpu to 53 bit precision.  This happens too early to use a stub.\n-        .p2align 4,,15\n-DECLARE_FUNC(fixcw):\n-        pushl    $0x27f\n-        fldcw    0(%esp)\n-        popl     %eax\n-        ret\n-\n-        .p2align 4,,15\n-DECLARE_FUNC(SpinPause):\n-        rep\n-        nop\n-        movl    $1, %eax\n-        ret\n-\n-        # Support for void Copy::arrayof_conjoint_bytes(void* from,\n-        #                                               void* to,\n-        #                                               size_t count)\n-        #\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_arrayof_conjoint_bytes):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -1(%esi,%ecx),%eax   # from + count - 1\n-        jbe      acb_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      acb_CopyLeft\n-        # copy from low to high\n-acb_CopyRight:\n-        cmpl     $3,%ecx\n-        jbe      5f\n-1:      movl     %ecx,%eax\n-        shrl     $2,%ecx\n-        jz       4f\n-        cmpl     $32,%ecx\n-        ja       3f\n-        # copy aligned dwords\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-        # copy aligned dwords\n-3:      rep;     smovl\n-4:      movl     %eax,%ecx\n-5:      andl     $3,%ecx\n-        jz       7f\n-        # copy suffix\n-        xorl     %eax,%eax\n-6:      movb     (%esi,%eax,1),%dl\n-        movb     %dl,(%edi,%eax,1)\n-        addl     $1,%eax\n-        subl     $1,%ecx\n-        jnz      6b\n-7:      popl     %edi\n-        popl     %esi\n-        ret\n-acb_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx),%edi   # to + count - 4\n-        movl     %eax,%esi            # from + count - 1\n-        movl     %ecx,%eax\n-        subl     $3,%esi              # from + count - 4\n-        cmpl     $3,%ecx\n-        jbe      5f\n-1:      shrl     $2,%ecx\n-        jz       4f\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        rep;     smovl\n-        jmp      4f\n-        .space 8\n-2:      subl     %esi,%edi\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      3b\n-        addl     %esi,%edi\n-4:      movl     %eax,%ecx\n-5:      andl     $3,%ecx\n-        jz       7f\n-        subl     %esi,%edi\n-        addl     $3,%esi\n-6:      movb     (%esi),%dl\n-        movb     %dl,(%edi,%esi,1)\n-        subl     $1,%esi\n-        subl     $1,%ecx\n-        jnz      6b\n-7:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::conjoint_jshorts_atomic(void* from,\n-        #                                                void* to,\n-        #                                                size_t count)\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_conjoint_jshorts_atomic):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2\n-        jbe      cs_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      cs_CopyLeft\n-        # copy from low to high\n-cs_CopyRight:\n-        # align source address at dword address boundary\n-        movl     %esi,%eax            # original from\n-        andl     $3,%eax              # either 0 or 2\n-        jz       1f                   # no prefix\n-        # copy prefix\n-        subl     $1,%ecx\n-        jl       5f                   # zero count\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-        addl     %eax,%esi            # %eax == 2\n-        addl     %eax,%edi\n-1:      movl     %ecx,%eax            # word count less prefix\n-        sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        # copy aligned dwords\n-        rep;     smovl\n-        jmp      4f\n-        # copy aligned dwords\n-2:      subl     %esi,%edi\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      3b\n-        addl     %esi,%edi\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      popl     %edi\n-        popl     %esi\n-        ret\n-        # copy from high to low\n-cs_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,2),%edi # to + count*2 - 4\n-        movl     %eax,%esi            # from + count*2 - 2\n-        movl     %ecx,%eax\n-        subl     $2,%esi              # from + count*2 - 4\n-1:      sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        ja       3f                   # > 32 dwords\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-3:      rep;     smovl\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        addl     $2,%esi\n-        addl     $2,%edi\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::arrayof_conjoint_jshorts(void* from,\n-        #                                                 void* to,\n-        #                                                 size_t count)\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_arrayof_conjoint_jshorts):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2\n-        jbe      acs_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      acs_CopyLeft\n-acs_CopyRight:\n-        movl     %ecx,%eax            # word count\n-        sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        # copy aligned dwords\n-        rep;     smovl\n-        jmp      4f\n-        # copy aligned dwords\n-        .space 5\n-2:      subl     %esi,%edi\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      3b\n-        addl     %esi,%edi\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      popl     %edi\n-        popl     %esi\n-        ret\n-acs_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,2),%edi # to + count*2 - 4\n-        movl     %eax,%esi            # from + count*2 - 2\n-        movl     %ecx,%eax\n-        subl     $2,%esi              # from + count*2 - 4\n-        sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        ja       3f                   # > 32 dwords\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-3:      rep;     smovl\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        addl     $2,%esi\n-        addl     $2,%edi\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::conjoint_jints_atomic(void* from,\n-        #                                              void* to,\n-        #                                              size_t count)\n-        # Equivalent to\n-        #   arrayof_conjoint_jints\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_conjoint_jints_atomic):\n-DECLARE_FUNC(_Copy_arrayof_conjoint_jints):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -4(%esi,%ecx,4),%eax # from + count*4 - 4\n-        jbe      ci_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      ci_CopyLeft\n-ci_CopyRight:\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        rep;     smovl\n-        popl     %edi\n-        popl     %esi\n-        ret\n-        .space 10\n-2:      subl     %esi,%edi\n-        jmp      4f\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-4:      subl     $1,%ecx\n-        jge      3b\n-        popl     %edi\n-        popl     %esi\n-        ret\n-ci_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,4),%edi # to + count*4 - 4\n-        cmpl     $32,%ecx\n-        ja       4f                   # > 32 dwords\n-        subl     %eax,%edi            # eax == from + count*4 - 4\n-        jmp      3f\n-        .p2align 4,,15\n-2:      movl     (%eax),%edx\n-        movl     %edx,(%edi,%eax,1)\n-        subl     $4,%eax\n-3:      subl     $1,%ecx\n-        jge      2b\n-        cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-4:      movl     %eax,%esi            # from + count*4 - 4\n-        rep;     smovl\n-        cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::conjoint_jlongs_atomic(jlong* from,\n-        #                                               jlong* to,\n-        #                                               size_t count)\n-        #\n-        # 32-bit\n-        #\n-        # count treated as signed\n-        #\n-        # \/\/ if (from > to) {\n-        #   while (--count >= 0) {\n-        #     *to++ = *from++;\n-        #   }\n-        # } else {\n-        #   while (--count >= 0) {\n-        #     to[count] = from[count];\n-        #   }\n-        # }\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_conjoint_jlongs_atomic):\n-        movl     4+8(%esp),%ecx       # count\n-        movl     4+0(%esp),%eax       # from\n-        movl     4+4(%esp),%edx       # to\n-        cmpl     %eax,%edx\n-        jae      cla_CopyLeft\n-cla_CopyRight:\n-        subl     %eax,%edx\n-        jmp      2f\n-        .p2align 4,,15\n-1:      fildll   (%eax)\n-        fistpll  (%edx,%eax,1)\n-        addl     $8,%eax\n-2:      subl     $1,%ecx\n-        jge      1b\n-        ret\n-        .p2align 4,,15\n-3:      fildll   (%eax,%ecx,8)\n-        fistpll  (%edx,%ecx,8)\n-cla_CopyLeft:\n-        subl     $1,%ecx\n-        jge      3b\n-        ret\n-\n-        # Support for void Copy::arrayof_conjoint_jshorts(void* from,\n-        #                                                 void* to,\n-        #                                                 size_t count)\n-        .p2align 4,,15\n-DECLARE_FUNC(_mmx_Copy_arrayof_conjoint_jshorts):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi\n-        movl     8+ 8(%esp),%edi\n-        cmpl     %esi,%edi\n-        leal     -2(%esi,%ecx,2),%eax\n-        jbe      mmx_acs_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      mmx_acs_CopyLeft\n-mmx_acs_CopyRight:\n-        movl     %ecx,%eax\n-        sarl     %ecx\n-        je       5f\n-        cmpl     $33,%ecx\n-        jae      3f\n-1:      subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      5f\n-3:      smovl # align to 8 bytes, we know we are 4 byte aligned to start\n-        subl     $1,%ecx\n-4:      .p2align 4,,15\n-        movq     0(%esi),%mm0\n-        addl     $64,%edi\n-        movq     8(%esi),%mm1\n-        subl     $16,%ecx\n-        movq     16(%esi),%mm2\n-        movq     %mm0,-64(%edi)\n-        movq     24(%esi),%mm0\n-        movq     %mm1,-56(%edi)\n-        movq     32(%esi),%mm1\n-        movq     %mm2,-48(%edi)\n-        movq     40(%esi),%mm2\n-        movq     %mm0,-40(%edi)\n-        movq     48(%esi),%mm0\n-        movq     %mm1,-32(%edi)\n-        movq     56(%esi),%mm1\n-        movq     %mm2,-24(%edi)\n-        movq     %mm0,-16(%edi)\n-        addl     $64,%esi\n-        movq     %mm1,-8(%edi)\n-        cmpl     $16,%ecx\n-        jge      4b\n-        emms\n-        testl    %ecx,%ecx\n-        ja       1b\n-5:      andl     $1,%eax\n-        je       7f\n-6:      movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-7:      popl     %edi\n-        popl     %esi\n-        ret\n-mmx_acs_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,2),%edi\n-        movl     %eax,%esi\n-        movl     %ecx,%eax\n-        subl     $2,%esi\n-        sarl     %ecx\n-        je       4f\n-        cmpl     $32,%ecx\n-        ja       3f\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-3:      rep;     smovl\n-4:      andl     $1,%eax\n-        je       6f\n-        addl     $2,%esi\n-        addl     $2,%edi\n-5:      movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-6:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-\n-        # Support for int64_t Atomic::cmpxchg(int64_t compare_value,\n-        #                                     volatile int64_t* dest,\n-        #                                     int64_t exchange_value)\n-        #\n-        .p2align 4,,15\n-DECLARE_FUNC(_Atomic_cmpxchg_long):\n-                                   #  8(%esp) : return PC\n-        pushl    %ebx              #  4(%esp) : old %ebx\n-        pushl    %edi              #  0(%esp) : old %edi\n-        movl     12(%esp), %ebx    # 12(%esp) : exchange_value (low)\n-        movl     16(%esp), %ecx    # 16(%esp) : exchange_value (high)\n-        movl     24(%esp), %eax    # 24(%esp) : compare_value (low)\n-        movl     28(%esp), %edx    # 28(%esp) : compare_value (high)\n-        movl     20(%esp), %edi    # 20(%esp) : dest\n-        lock\n-        cmpxchg8b (%edi)\n-        popl     %edi\n-        popl     %ebx\n-        ret\n-\n-\n-        # Support for int64_t Atomic::load and Atomic::store.\n-        # void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst)\n-        .p2align 4,,15\n-DECLARE_FUNC(_Atomic_move_long):\n-        movl     4(%esp), %eax   # src\n-        fildll    (%eax)\n-        movl     8(%esp), %eax   # dest\n-        fistpll   (%eax)\n-        ret\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/bsd_x86_32.S","additions":0,"deletions":525,"binary":false,"changes":525,"status":"deleted"},{"patch":"@@ -32,1 +32,0 @@\n-#ifdef AMD64\n@@ -36,9 +35,0 @@\n-#else\n-define_pd_global(intx, CompilerThreadStackSize,  512);\n-\/\/ ThreadStackSize 320 allows a couple of test cases to run while\n-\/\/ keeping the number of threads that can be created high.  System\n-\/\/ default ThreadStackSize appears to be 512 which is too big.\n-define_pd_global(intx, ThreadStackSize,          320);\n-define_pd_global(intx, VMThreadStackSize,        512);\n-#endif \/\/ AMD64\n-\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/globals_bsd_x86.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-#ifdef AMD64\n@@ -56,3 +55,0 @@\n-#else\n-  __asm__ volatile (\"lock; addl $0,0(%%esp)\" : : : \"cc\", \"memory\");\n-#endif\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/orderAccess_bsd_x86.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -90,1 +90,0 @@\n-#ifdef AMD64\n@@ -93,4 +92,0 @@\n-#else\n-#define SPELL_REG_SP \"esp\"\n-#define SPELL_REG_FP \"ebp\"\n-#endif \/\/ AMD64\n@@ -100,1 +95,0 @@\n-# ifdef AMD64\n@@ -123,16 +117,0 @@\n-# else\n-#  define context_pc uc_mcontext.mc_eip\n-#  define context_sp uc_mcontext.mc_esp\n-#  define context_fp uc_mcontext.mc_ebp\n-#  define context_eip uc_mcontext.mc_eip\n-#  define context_esp uc_mcontext.mc_esp\n-#  define context_eax uc_mcontext.mc_eax\n-#  define context_ebx uc_mcontext.mc_ebx\n-#  define context_ecx uc_mcontext.mc_ecx\n-#  define context_edx uc_mcontext.mc_edx\n-#  define context_ebp uc_mcontext.mc_ebp\n-#  define context_esi uc_mcontext.mc_esi\n-#  define context_edi uc_mcontext.mc_edi\n-#  define context_eflags uc_mcontext.mc_eflags\n-#  define context_trapno uc_mcontext.mc_trapno\n-# endif\n@@ -149,1 +127,0 @@\n-# ifdef AMD64\n@@ -173,16 +150,0 @@\n-# else\n-#  define context_pc context_eip\n-#  define context_sp context_esp\n-#  define context_fp context_ebp\n-#  define context_eip uc_mcontext->DU3_PREFIX(ss,eip)\n-#  define context_esp uc_mcontext->DU3_PREFIX(ss,esp)\n-#  define context_eax uc_mcontext->DU3_PREFIX(ss,eax)\n-#  define context_ebx uc_mcontext->DU3_PREFIX(ss,ebx)\n-#  define context_ecx uc_mcontext->DU3_PREFIX(ss,ecx)\n-#  define context_edx uc_mcontext->DU3_PREFIX(ss,edx)\n-#  define context_ebp uc_mcontext->DU3_PREFIX(ss,ebp)\n-#  define context_esi uc_mcontext->DU3_PREFIX(ss,esi)\n-#  define context_edi uc_mcontext->DU3_PREFIX(ss,edi)\n-#  define context_eflags uc_mcontext->DU3_PREFIX(ss,eflags)\n-#  define context_trapno uc_mcontext->DU3_PREFIX(es,trapno)\n-# endif\n@@ -193,1 +154,0 @@\n-# ifdef AMD64\n@@ -216,16 +176,0 @@\n-# else\n-#  define context_pc sc_eip\n-#  define context_sp sc_esp\n-#  define context_fp sc_ebp\n-#  define context_eip sc_eip\n-#  define context_esp sc_esp\n-#  define context_eax sc_eax\n-#  define context_ebx sc_ebx\n-#  define context_ecx sc_ecx\n-#  define context_edx sc_edx\n-#  define context_ebp sc_ebp\n-#  define context_esi sc_esi\n-#  define context_edi sc_edi\n-#  define context_eflags sc_eflags\n-#  define context_trapno sc_trapno\n-# endif\n@@ -236,1 +180,0 @@\n-# ifdef AMD64\n@@ -260,16 +203,0 @@\n-# else\n-#  define context_pc uc_mcontext.__gregs[_REG_EIP]\n-#  define context_sp uc_mcontext.__gregs[_REG_UESP]\n-#  define context_fp uc_mcontext.__gregs[_REG_EBP]\n-#  define context_eip uc_mcontext.__gregs[_REG_EIP]\n-#  define context_esp uc_mcontext.__gregs[_REG_UESP]\n-#  define context_eax uc_mcontext.__gregs[_REG_EAX]\n-#  define context_ebx uc_mcontext.__gregs[_REG_EBX]\n-#  define context_ecx uc_mcontext.__gregs[_REG_ECX]\n-#  define context_edx uc_mcontext.__gregs[_REG_EDX]\n-#  define context_ebp uc_mcontext.__gregs[_REG_EBP]\n-#  define context_esi uc_mcontext.__gregs[_REG_ESI]\n-#  define context_edi uc_mcontext.__gregs[_REG_EDI]\n-#  define context_eflags uc_mcontext.__gregs[_REG_EFL]\n-#  define context_trapno uc_mcontext.__gregs[_REG_TRAPNO]\n-# endif\n@@ -425,1 +352,1 @@\n-#if !defined(PRODUCT) && defined(_LP64)\n+#if !defined(PRODUCT)\n@@ -466,3 +393,1 @@\n-      } else\n-#ifdef AMD64\n-      if (sig == SIGFPE &&\n+      } else if (sig == SIGFPE &&\n@@ -501,27 +426,0 @@\n-#else\n-      if (sig == SIGFPE \/* && info->si_code == FPE_INTDIV *\/) {\n-        \/\/ HACK: si_code does not work on bsd 2.2.12-20!!!\n-        int op = pc[0];\n-        if (op == 0xDB) {\n-          \/\/ FIST\n-          \/\/ TODO: The encoding of D2I in x86_32.ad can cause an exception\n-          \/\/ prior to the fist instruction if there was an invalid operation\n-          \/\/ pending. We want to dismiss that exception. From the win_32\n-          \/\/ side it also seems that if it really was the fist causing\n-          \/\/ the exception that we do the d2i by hand with different\n-          \/\/ rounding. Seems kind of weird.\n-          \/\/ NOTE: that we take the exception at the NEXT floating point instruction.\n-          assert(pc[0] == 0xDB, \"not a FIST opcode\");\n-          assert(pc[1] == 0x14, \"not a FIST opcode\");\n-          assert(pc[2] == 0x24, \"not a FIST opcode\");\n-          return true;\n-        } else if (op == 0xF7) {\n-          \/\/ IDIV\n-          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO);\n-        } else {\n-          \/\/ TODO: handle more cases if we are using other x86 instructions\n-          \/\/   that can generate SIGFPE signal on bsd.\n-          tty->print_cr(\"unknown opcode 0x%X with SIGFPE.\", op);\n-          fatal(\"please update this code.\");\n-        }\n-#endif \/\/ AMD64\n@@ -554,75 +452,0 @@\n-#ifndef AMD64\n-  \/\/ Execution protection violation\n-  \/\/\n-  \/\/ This should be kept as the last step in the triage.  We don't\n-  \/\/ have a dedicated trap number for a no-execute fault, so be\n-  \/\/ conservative and allow other handlers the first shot.\n-  \/\/\n-  \/\/ Note: We don't test that info->si_code == SEGV_ACCERR here.\n-  \/\/ this si_code is so generic that it is almost meaningless; and\n-  \/\/ the si_code for this condition may change in the future.\n-  \/\/ Furthermore, a false-positive should be harmless.\n-  if (UnguardOnExecutionViolation > 0 &&\n-      stub == nullptr &&\n-      (sig == SIGSEGV || sig == SIGBUS) &&\n-      uc->context_trapno == trap_page_fault) {\n-    size_t page_size = os::vm_page_size();\n-    address addr = (address) info->si_addr;\n-    address pc = os::Posix::ucontext_get_pc(uc);\n-    \/\/ Make sure the pc and the faulting address are sane.\n-    \/\/\n-    \/\/ If an instruction spans a page boundary, and the page containing\n-    \/\/ the beginning of the instruction is executable but the following\n-    \/\/ page is not, the pc and the faulting address might be slightly\n-    \/\/ different - we still want to unguard the 2nd page in this case.\n-    \/\/\n-    \/\/ 15 bytes seems to be a (very) safe value for max instruction size.\n-    bool pc_is_near_addr =\n-      (pointer_delta((void*) addr, (void*) pc, sizeof(char)) < 15);\n-    bool instr_spans_page_boundary =\n-      (align_down((intptr_t) pc ^ (intptr_t) addr,\n-                       (intptr_t) page_size) > 0);\n-\n-    if (pc == addr || (pc_is_near_addr && instr_spans_page_boundary)) {\n-      static volatile address last_addr =\n-        (address) os::non_memory_address_word();\n-\n-      \/\/ In conservative mode, don't unguard unless the address is in the VM\n-      if (addr != last_addr &&\n-          (UnguardOnExecutionViolation > 1 || os::address_is_in_vm(addr))) {\n-\n-        \/\/ Set memory to RWX and retry\n-        address page_start = align_down(addr, page_size);\n-        bool res = os::protect_memory((char*) page_start, page_size,\n-                                      os::MEM_PROT_RWX);\n-\n-        log_debug(os)(\"Execution protection violation \"\n-                      \"at \" INTPTR_FORMAT\n-                      \", unguarding \" INTPTR_FORMAT \": %s, errno=%d\", p2i(addr),\n-                      p2i(page_start), (res ? \"success\" : \"failed\"), errno);\n-        stub = pc;\n-\n-        \/\/ Set last_addr so if we fault again at the same address, we don't end\n-        \/\/ up in an endless loop.\n-        \/\/\n-        \/\/ There are two potential complications here.  Two threads trapping at\n-        \/\/ the same address at the same time could cause one of the threads to\n-        \/\/ think it already unguarded, and abort the VM.  Likely very rare.\n-        \/\/\n-        \/\/ The other race involves two threads alternately trapping at\n-        \/\/ different addresses and failing to unguard the page, resulting in\n-        \/\/ an endless loop.  This condition is probably even more unlikely than\n-        \/\/ the first.\n-        \/\/\n-        \/\/ Although both cases could be avoided by using locks or thread local\n-        \/\/ last_addr, these solutions are unnecessary complication: this\n-        \/\/ handler is a best-effort safety net, not a complete solution.  It is\n-        \/\/ disabled by default and should only be used as a workaround in case\n-        \/\/ we missed any no-execute-unsafe VM code.\n-\n-        last_addr = addr;\n-      }\n-    }\n-  }\n-#endif \/\/ !AMD64\n-\n@@ -644,4 +467,1 @@\n-#ifndef AMD64\n-  \/\/ Set fpu to 53 bit precision. This happens too early to use a stub.\n-  fixcw();\n-#endif \/\/ !AMD64\n+  \/\/ Nothing to do.\n@@ -669,1 +489,0 @@\n-#ifdef _LP64\n@@ -671,3 +490,0 @@\n-#else\n-size_t os::_vm_internal_thread_min_stack_allowed = (48 DEBUG_ONLY(+ 4)) * K;\n-#endif \/\/ _LP64\n@@ -675,1 +491,0 @@\n-#ifndef AMD64\n@@ -679,1 +494,0 @@\n-#endif \/\/ AMD64\n@@ -684,1 +498,0 @@\n-#ifdef AMD64\n@@ -686,3 +499,0 @@\n-#else\n-  size_t s = (thr_type == os::compiler_thread ? 2 * M : 512 * K);\n-#endif \/\/ AMD64\n@@ -801,1 +611,0 @@\n-#ifdef AMD64\n@@ -827,14 +636,0 @@\n-#else\n-  st->print(  \"EAX=\" INTPTR_FORMAT, (intptr_t)uc->context_eax);\n-  st->print(\", EBX=\" INTPTR_FORMAT, (intptr_t)uc->context_ebx);\n-  st->print(\", ECX=\" INTPTR_FORMAT, (intptr_t)uc->context_ecx);\n-  st->print(\", EDX=\" INTPTR_FORMAT, (intptr_t)uc->context_edx);\n-  st->cr();\n-  st->print(  \"ESP=\" INTPTR_FORMAT, (intptr_t)uc->context_esp);\n-  st->print(\", EBP=\" INTPTR_FORMAT, (intptr_t)uc->context_ebp);\n-  st->print(\", ESI=\" INTPTR_FORMAT, (intptr_t)uc->context_esi);\n-  st->print(\", EDI=\" INTPTR_FORMAT, (intptr_t)uc->context_edi);\n-  st->cr();\n-  st->print(  \"EIP=\" INTPTR_FORMAT, (intptr_t)uc->context_eip);\n-  st->print(\", EFLAGS=\" INTPTR_FORMAT, (intptr_t)uc->context_eflags);\n-#endif \/\/ AMD64\n@@ -846,1 +641,1 @@\n-  const int register_count = AMD64_ONLY(16) NOT_AMD64(8);\n+  const int register_count = 16;\n@@ -859,1 +654,0 @@\n-#ifdef AMD64\n@@ -876,10 +670,0 @@\n-#else\n-    CASE_PRINT_REG(0, \"EAX=\", eax); break;\n-    CASE_PRINT_REG(1, \"EBX=\", ebx); break;\n-    CASE_PRINT_REG(2, \"ECX=\", ecx); break;\n-    CASE_PRINT_REG(3, \"EDX=\", edx); break;\n-    CASE_PRINT_REG(4, \"ESP=\", esp); break;\n-    CASE_PRINT_REG(5, \"EBP=\", ebp); break;\n-    CASE_PRINT_REG(6, \"ESI=\", esi); break;\n-    CASE_PRINT_REG(7, \"EDI=\", edi); break;\n-#endif \/\/ AMD64\n@@ -893,5 +677,1 @@\n-#ifndef AMD64\n-  address fpu_cntrl = StubRoutines::addr_fpu_cntrl_wrd_std();\n-  __asm__ volatile (  \"fldcw (%0)\" :\n-                      : \"r\" (fpu_cntrl) : \"memory\");\n-#endif \/\/ !AMD64\n+  \/\/ Nothing to do.\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":5,"deletions":225,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -42,6 +42,0 @@\n-#ifndef AMD64\n-  \/\/ 64 bit result in edx:eax\n-  uint64_t res;\n-  __asm__ __volatile__ (\"rdtsc\" : \"=A\" (res));\n-  return (jlong)res;\n-#else\n@@ -53,1 +47,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.inline.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#ifdef AMD64\n@@ -34,1 +33,0 @@\n-#endif \/\/ AMD64\n@@ -38,2 +36,0 @@\n-#ifdef AMD64\n-\n@@ -43,2 +39,0 @@\n-\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/prefetch_bsd_x86.inline.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -319,7 +319,0 @@\n-int os::Linux::get_fpu_control_word(void) {\n-  return 0;\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu_control) {\n-}\n-\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -428,8 +428,0 @@\n-int os::Linux::get_fpu_control_word(void) {\n-  return 0;\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu_control) {\n-  \/\/ Nothing to do\n-}\n-\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -422,10 +422,0 @@\n-int os::Linux::get_fpu_control_word(void) {\n-  \/\/ x86 has problems with FPU precision after pthread_cond_timedwait().\n-  \/\/ nothing to do on ppc64.\n-  return 0;\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu_control) {\n-  \/\/ x86 has problems with FPU precision after pthread_cond_timedwait().\n-  \/\/ nothing to do on ppc64.\n-}\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/os_linux_ppc.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -307,6 +307,0 @@\n-int os::Linux::get_fpu_control_word(void) {\n-  return 0;\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu_control) {\n-}\n","filename":"src\/hotspot\/os_cpu\/linux_riscv\/os_linux_riscv.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -378,9 +378,0 @@\n-int os::Linux::get_fpu_control_word(void) {\n-  \/\/ Nothing to do on z\/Architecture.\n-  return 0;\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu_control) {\n-  \/\/ Nothing to do on z\/Architecture.\n-}\n-\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/os_linux_s390.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -96,2 +96,0 @@\n-#ifdef AMD64\n-\n@@ -138,45 +136,0 @@\n-#else \/\/ !AMD64\n-\n-extern \"C\" {\n-  \/\/ defined in linux_x86.s\n-  int64_t _Atomic_cmpxchg_long(int64_t, volatile int64_t*, int64_t);\n-  void _Atomic_move_long(const volatile int64_t* src, volatile int64_t* dst);\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  return cmpxchg_using_helper<int64_t>(_Atomic_cmpxchg_long, dest, compare_value, exchange_value);\n-}\n-\n-\/\/ No direct support for 8-byte xchg; emulate using cmpxchg.\n-template<>\n-struct Atomic::PlatformXchg<8> : Atomic::XchgUsingCmpxchg<8> {};\n-\n-\/\/ No direct support for 8-byte add; emulate using cmpxchg.\n-template<>\n-struct Atomic::PlatformAdd<8> : Atomic::AddUsingCmpxchg<8> {};\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformLoad<8>::operator()(T const volatile* src) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  volatile int64_t dest;\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(src), reinterpret_cast<volatile int64_t*>(&dest));\n-  return PrimitiveConversions::cast<T>(dest);\n-}\n-\n-template<>\n-template<typename T>\n-inline void Atomic::PlatformStore<8>::operator()(T volatile* dest,\n-                                                 T store_value) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  _Atomic_move_long(reinterpret_cast<const volatile int64_t*>(&store_value), reinterpret_cast<volatile int64_t*>(dest));\n-}\n-\n-#endif \/\/ AMD64\n-\n@@ -219,1 +172,0 @@\n-#ifdef AMD64\n@@ -231,1 +183,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/atomic_linux_x86.hpp","additions":0,"deletions":49,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-#ifdef AMD64\n@@ -35,14 +34,0 @@\n-#else\n-\/\/ Some tests in debug VM mode run out of compile thread stack.\n-\/\/ Observed on some x86_32 VarHandles tests during escape analysis.\n-#ifdef ASSERT\n-define_pd_global(intx, CompilerThreadStackSize,   768);\n-#else\n-define_pd_global(intx, CompilerThreadStackSize,   512);\n-#endif\n-\/\/ ThreadStackSize 320 allows a couple of test cases to run while\n-\/\/ keeping the number of threads that can be created high.  System\n-\/\/ default ThreadStackSize appears to be 512 which is too big.\n-define_pd_global(intx, ThreadStackSize,          320);\n-define_pd_global(intx, VMThreadStackSize,        512);\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/globals_linux_x86.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1,518 +0,0 @@\n-#\n-# Copyright (c) 2004, 2024, Oracle and\/or its affiliates. All rights reserved.\n-# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-#\n-# This code is free software; you can redistribute it and\/or modify it\n-# under the terms of the GNU General Public License version 2 only, as\n-# published by the Free Software Foundation.\n-#\n-# This code is distributed in the hope that it will be useful, but WITHOUT\n-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-# version 2 for more details (a copy is included in the LICENSE file that\n-# accompanied this code).\n-#\n-# You should have received a copy of the GNU General Public License version\n-# 2 along with this work; if not, write to the Free Software Foundation,\n-# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-#\n-# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-# or visit www.oracle.com if you need additional information or have any\n-# questions.\n-#\n-\n-#include \"defs.S.inc\"\n-\n-        # NOTE WELL!  The _Copy functions are called directly\n-        # from server-compiler-generated code via CallLeafNoFP,\n-        # which means that they *must* either not use floating\n-        # point or use it in the same manner as does the server\n-        # compiler.\n-\n-        .text\n-\n-        .p2align 4,,15\n-DECLARE_FUNC(SpinPause):\n-        rep\n-        nop\n-        movl    $1, %eax\n-        ret\n-\n-        # Support for void Copy::arrayof_conjoint_bytes(void* from,\n-        #                                               void* to,\n-        #                                               size_t count)\n-        #\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_arrayof_conjoint_bytes):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -1(%esi,%ecx),%eax   # from + count - 1\n-        jbe      acb_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      acb_CopyLeft\n-        # copy from low to high\n-acb_CopyRight:\n-        cmpl     $3,%ecx\n-        jbe      5f\n-1:      movl     %ecx,%eax\n-        shrl     $2,%ecx\n-        jz       4f\n-        cmpl     $32,%ecx\n-        ja       3f\n-        # copy aligned dwords\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-        # copy aligned dwords\n-3:      rep;     smovl\n-4:      movl     %eax,%ecx\n-5:      andl     $3,%ecx\n-        jz       7f\n-        # copy suffix\n-        xorl     %eax,%eax\n-6:      movb     (%esi,%eax,1),%dl\n-        movb     %dl,(%edi,%eax,1)\n-        addl     $1,%eax\n-        subl     $1,%ecx\n-        jnz      6b\n-7:      popl     %edi\n-        popl     %esi\n-        ret\n-acb_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx),%edi   # to + count - 4\n-        movl     %eax,%esi            # from + count - 1\n-        movl     %ecx,%eax\n-        subl     $3,%esi              # from + count - 4\n-        cmpl     $3,%ecx\n-        jbe      5f\n-1:      shrl     $2,%ecx\n-        jz       4f\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        rep;     smovl\n-        jmp      4f\n-        .space 8\n-2:      subl     %esi,%edi\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      3b\n-        addl     %esi,%edi\n-4:      movl     %eax,%ecx\n-5:      andl     $3,%ecx\n-        jz       7f\n-        subl     %esi,%edi\n-        addl     $3,%esi\n-6:      movb     (%esi),%dl\n-        movb     %dl,(%edi,%esi,1)\n-        subl     $1,%esi\n-        subl     $1,%ecx\n-        jnz      6b\n-7:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::conjoint_jshorts_atomic(void* from,\n-        #                                                void* to,\n-        #                                                size_t count)\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_conjoint_jshorts_atomic):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2\n-        jbe      cs_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      cs_CopyLeft\n-        # copy from low to high\n-cs_CopyRight:\n-        # align source address at dword address boundary\n-        movl     %esi,%eax            # original from\n-        andl     $3,%eax              # either 0 or 2\n-        jz       1f                   # no prefix\n-        # copy prefix\n-        subl     $1,%ecx\n-        jl       5f                   # zero count\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-        addl     %eax,%esi            # %eax == 2\n-        addl     %eax,%edi\n-1:      movl     %ecx,%eax            # word count less prefix\n-        sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        # copy aligned dwords\n-        rep;     smovl\n-        jmp      4f\n-        # copy aligned dwords\n-2:      subl     %esi,%edi\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      3b\n-        addl     %esi,%edi\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      popl     %edi\n-        popl     %esi\n-        ret\n-        # copy from high to low\n-cs_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,2),%edi # to + count*2 - 4\n-        movl     %eax,%esi            # from + count*2 - 2\n-        movl     %ecx,%eax\n-        subl     $2,%esi              # from + count*2 - 4\n-1:      sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        ja       3f                   # > 32 dwords\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-3:      rep;     smovl\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        addl     $2,%esi\n-        addl     $2,%edi\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::arrayof_conjoint_jshorts(void* from,\n-        #                                                 void* to,\n-        #                                                 size_t count)\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_arrayof_conjoint_jshorts):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -2(%esi,%ecx,2),%eax # from + count*2 - 2\n-        jbe      acs_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      acs_CopyLeft\n-acs_CopyRight:\n-        movl     %ecx,%eax            # word count\n-        sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        # copy aligned dwords\n-        rep;     smovl\n-        jmp      4f\n-        # copy aligned dwords\n-        .space 5\n-2:      subl     %esi,%edi\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      3b\n-        addl     %esi,%edi\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      popl     %edi\n-        popl     %esi\n-        ret\n-acs_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,2),%edi # to + count*2 - 4\n-        movl     %eax,%esi            # from + count*2 - 2\n-        movl     %ecx,%eax\n-        subl     $2,%esi              # from + count*2 - 4\n-        sarl     %ecx                 # dword count\n-        jz       4f                   # no dwords to move\n-        cmpl     $32,%ecx\n-        ja       3f                   # > 32 dwords\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-3:      rep;     smovl\n-4:      andl     $1,%eax              # suffix count\n-        jz       5f                   # no suffix\n-        # copy suffix\n-        addl     $2,%esi\n-        addl     $2,%edi\n-        movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-5:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::conjoint_jints_atomic(void* from,\n-        #                                              void* to,\n-        #                                              size_t count)\n-        # Equivalent to\n-        #   arrayof_conjoint_jints\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_conjoint_jints_atomic):\n-DECLARE_FUNC(_Copy_arrayof_conjoint_jints):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx      # count\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi      # from\n-        movl     8+ 8(%esp),%edi      # to\n-        cmpl     %esi,%edi\n-        leal     -4(%esi,%ecx,4),%eax # from + count*4 - 4\n-        jbe      ci_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      ci_CopyLeft\n-ci_CopyRight:\n-        cmpl     $32,%ecx\n-        jbe      2f                   # <= 32 dwords\n-        rep;     smovl\n-        popl     %edi\n-        popl     %esi\n-        ret\n-        .space 10\n-2:      subl     %esi,%edi\n-        jmp      4f\n-        .p2align 4,,15\n-3:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-4:      subl     $1,%ecx\n-        jge      3b\n-        popl     %edi\n-        popl     %esi\n-        ret\n-ci_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,4),%edi # to + count*4 - 4\n-        cmpl     $32,%ecx\n-        ja       4f                   # > 32 dwords\n-        subl     %eax,%edi            # eax == from + count*4 - 4\n-        jmp      3f\n-        .p2align 4,,15\n-2:      movl     (%eax),%edx\n-        movl     %edx,(%edi,%eax,1)\n-        subl     $4,%eax\n-3:      subl     $1,%ecx\n-        jge      2b\n-        cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-4:      movl     %eax,%esi            # from + count*4 - 4\n-        rep;     smovl\n-        cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-        # Support for void Copy::conjoint_jlongs_atomic(jlong* from,\n-        #                                               jlong* to,\n-        #                                               size_t count)\n-        #\n-        # 32-bit\n-        #\n-        # count treated as signed\n-        \/*\n-        #\n-        # if (from > to) {\n-        #   while (--count >= 0) {\n-        #     *to++ = *from++;\n-        #   }\n-        # } else {\n-        #   while (--count >= 0) {\n-        #     to[count] = from[count];\n-        #   }\n-        # }\n-        *\/\n-        .p2align 4,,15\n-DECLARE_FUNC(_Copy_conjoint_jlongs_atomic):\n-        movl     4+8(%esp),%ecx       # count\n-        movl     4+0(%esp),%eax       # from\n-        movl     4+4(%esp),%edx       # to\n-        cmpl     %eax,%edx\n-        jae      cla_CopyLeft\n-cla_CopyRight:\n-        subl     %eax,%edx\n-        jmp      2f\n-        .p2align 4,,15\n-1:      fildll   (%eax)\n-        fistpll  (%edx,%eax,1)\n-        addl     $8,%eax\n-2:      subl     $1,%ecx\n-        jge      1b\n-        ret\n-        .p2align 4,,15\n-3:      fildll   (%eax,%ecx,8)\n-        fistpll  (%edx,%ecx,8)\n-cla_CopyLeft:\n-        subl     $1,%ecx\n-        jge      3b\n-        ret\n-\n-        # Support for void Copy::arrayof_conjoint_jshorts(void* from,\n-        #                                                 void* to,\n-        #                                                 size_t count)\n-        .p2align 4,,15\n-DECLARE_FUNC(_mmx_Copy_arrayof_conjoint_jshorts):\n-        pushl    %esi\n-        movl     4+12(%esp),%ecx\n-        pushl    %edi\n-        movl     8+ 4(%esp),%esi\n-        movl     8+ 8(%esp),%edi\n-        cmpl     %esi,%edi\n-        leal     -2(%esi,%ecx,2),%eax\n-        jbe      mmx_acs_CopyRight\n-        cmpl     %eax,%edi\n-        jbe      mmx_acs_CopyLeft\n-mmx_acs_CopyRight:\n-        movl     %ecx,%eax\n-        sarl     %ecx\n-        je       5f\n-        cmpl     $33,%ecx\n-        jae      3f\n-1:      subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        addl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      5f\n-3:      smovl # align to 8 bytes, we know we are 4 byte aligned to start\n-        subl     $1,%ecx\n-4:      .p2align 4,,15\n-        movq     0(%esi),%mm0\n-        addl     $64,%edi\n-        movq     8(%esi),%mm1\n-        subl     $16,%ecx\n-        movq     16(%esi),%mm2\n-        movq     %mm0,-64(%edi)\n-        movq     24(%esi),%mm0\n-        movq     %mm1,-56(%edi)\n-        movq     32(%esi),%mm1\n-        movq     %mm2,-48(%edi)\n-        movq     40(%esi),%mm2\n-        movq     %mm0,-40(%edi)\n-        movq     48(%esi),%mm0\n-        movq     %mm1,-32(%edi)\n-        movq     56(%esi),%mm1\n-        movq     %mm2,-24(%edi)\n-        movq     %mm0,-16(%edi)\n-        addl     $64,%esi\n-        movq     %mm1,-8(%edi)\n-        cmpl     $16,%ecx\n-        jge      4b\n-        emms\n-        testl    %ecx,%ecx\n-        ja       1b\n-5:      andl     $1,%eax\n-        je       7f\n-6:      movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-7:\tpopl     %edi\n-        popl     %esi\n-        ret\n-mmx_acs_CopyLeft:\n-        std\n-        leal     -4(%edi,%ecx,2),%edi\n-        movl     %eax,%esi\n-        movl     %ecx,%eax\n-        subl     $2,%esi\n-        sarl     %ecx\n-        je       4f\n-        cmpl     $32,%ecx\n-        ja       3f\n-        subl     %esi,%edi\n-        .p2align 4,,15\n-2:      movl     (%esi),%edx\n-        movl     %edx,(%edi,%esi,1)\n-        subl     $4,%esi\n-        subl     $1,%ecx\n-        jnz      2b\n-        addl     %esi,%edi\n-        jmp      4f\n-3:      rep;     smovl\n-4:      andl     $1,%eax\n-        je       6f\n-        addl     $2,%esi\n-        addl     $2,%edi\n-5:      movw     (%esi),%dx\n-        movw     %dx,(%edi)\n-6:      cld\n-        popl     %edi\n-        popl     %esi\n-        ret\n-\n-\n-        # Support for jlong Atomic::cmpxchg(volatile jlong* dest,\n-        #                                   jlong compare_value,\n-        #                                   jlong exchange_value)\n-        #\n-        .p2align 4,,15\n-DECLARE_FUNC(_Atomic_cmpxchg_long):\n-                                   #  8(%esp) : return PC\n-        pushl    %ebx              #  4(%esp) : old %ebx\n-        pushl    %edi              #  0(%esp) : old %edi\n-        movl     12(%esp), %ebx    # 12(%esp) : exchange_value (low)\n-        movl     16(%esp), %ecx    # 16(%esp) : exchange_value (high)\n-        movl     24(%esp), %eax    # 24(%esp) : compare_value (low)\n-        movl     28(%esp), %edx    # 28(%esp) : compare_value (high)\n-        movl     20(%esp), %edi    # 20(%esp) : dest\n-        lock cmpxchg8b (%edi)\n-        popl     %edi\n-        popl     %ebx\n-        ret\n-\n-\n-        # Support for jlong Atomic::load and Atomic::store.\n-        # void _Atomic_move_long(const volatile jlong* src, volatile jlong* dst)\n-        .p2align 4,,15\n-DECLARE_FUNC(_Atomic_move_long):\n-        movl     4(%esp), %eax   # src\n-        fildll    (%eax)\n-        movl     8(%esp), %eax   # dest\n-        fistpll   (%eax)\n-        ret\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/linux_x86_32.S","additions":0,"deletions":518,"binary":false,"changes":518,"status":"deleted"},{"patch":"@@ -50,1 +50,0 @@\n-#ifdef AMD64\n@@ -52,3 +51,0 @@\n-#else\n-  __asm__ volatile (\"lock; addl $0,0(%%esp)\" : : : \"cc\", \"memory\");\n-#endif\n@@ -63,1 +59,0 @@\n-#ifdef AMD64\n@@ -65,5 +60,0 @@\n-#else\n-    \/\/ On some x86 systems EBX is a reserved register that cannot be\n-    \/\/ clobbered, so we must protect it around the CPUID.\n-    __asm__ volatile (\"xchg %%esi, %%ebx; cpuid; xchg %%esi, %%ebx \" : \"+a\" (idx) : : \"esi\", \"ecx\", \"edx\", \"memory\");\n-#endif\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/orderAccess_linux_x86.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -76,3 +76,0 @@\n-#ifndef AMD64\n-# include <fpu_control.h>\n-#endif\n@@ -80,1 +77,0 @@\n-#ifdef AMD64\n@@ -86,7 +82,0 @@\n-#else\n-#define REG_SP REG_UESP\n-#define REG_PC REG_EIP\n-#define REG_FP REG_EBP\n-#define SPELL_REG_SP \"esp\"\n-#define SPELL_REG_FP \"ebp\"\n-#endif \/\/ AMD64\n@@ -251,1 +240,1 @@\n-#if !defined(PRODUCT) && defined(_LP64)\n+#if !defined(PRODUCT)\n@@ -279,3 +268,1 @@\n-      } else\n-#ifdef AMD64\n-      if (sig == SIGFPE &&\n+      } else if (sig == SIGFPE &&\n@@ -289,27 +276,0 @@\n-#else\n-      if (sig == SIGFPE \/* && info->si_code == FPE_INTDIV *\/) {\n-        \/\/ HACK: si_code does not work on linux 2.2.12-20!!!\n-        int op = pc[0];\n-        if (op == 0xDB) {\n-          \/\/ FIST\n-          \/\/ TODO: The encoding of D2I in x86_32.ad can cause an exception\n-          \/\/ prior to the fist instruction if there was an invalid operation\n-          \/\/ pending. We want to dismiss that exception. From the win_32\n-          \/\/ side it also seems that if it really was the fist causing\n-          \/\/ the exception that we do the d2i by hand with different\n-          \/\/ rounding. Seems kind of weird.\n-          \/\/ NOTE: that we take the exception at the NEXT floating point instruction.\n-          assert(pc[0] == 0xDB, \"not a FIST opcode\");\n-          assert(pc[1] == 0x14, \"not a FIST opcode\");\n-          assert(pc[2] == 0x24, \"not a FIST opcode\");\n-          return true;\n-        } else if (op == 0xF7) {\n-          \/\/ IDIV\n-          stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::IMPLICIT_DIVIDE_BY_ZERO);\n-        } else {\n-          \/\/ TODO: handle more cases if we are using other x86 instructions\n-          \/\/   that can generate SIGFPE signal on linux.\n-          tty->print_cr(\"unknown opcode 0x%X with SIGFPE.\", op);\n-          fatal(\"please update this code.\");\n-        }\n-#endif \/\/ AMD64\n@@ -342,75 +302,0 @@\n-#ifndef AMD64\n-  \/\/ Execution protection violation\n-  \/\/\n-  \/\/ This should be kept as the last step in the triage.  We don't\n-  \/\/ have a dedicated trap number for a no-execute fault, so be\n-  \/\/ conservative and allow other handlers the first shot.\n-  \/\/\n-  \/\/ Note: We don't test that info->si_code == SEGV_ACCERR here.\n-  \/\/ this si_code is so generic that it is almost meaningless; and\n-  \/\/ the si_code for this condition may change in the future.\n-  \/\/ Furthermore, a false-positive should be harmless.\n-  if (UnguardOnExecutionViolation > 0 &&\n-      stub == nullptr &&\n-      (sig == SIGSEGV || sig == SIGBUS) &&\n-      uc->uc_mcontext.gregs[REG_TRAPNO] == trap_page_fault) {\n-    size_t page_size = os::vm_page_size();\n-    address addr = (address) info->si_addr;\n-    address pc = os::Posix::ucontext_get_pc(uc);\n-    \/\/ Make sure the pc and the faulting address are sane.\n-    \/\/\n-    \/\/ If an instruction spans a page boundary, and the page containing\n-    \/\/ the beginning of the instruction is executable but the following\n-    \/\/ page is not, the pc and the faulting address might be slightly\n-    \/\/ different - we still want to unguard the 2nd page in this case.\n-    \/\/\n-    \/\/ 15 bytes seems to be a (very) safe value for max instruction size.\n-    bool pc_is_near_addr =\n-      (pointer_delta((void*) addr, (void*) pc, sizeof(char)) < 15);\n-    bool instr_spans_page_boundary =\n-      (align_down((intptr_t) pc ^ (intptr_t) addr,\n-                       (intptr_t) page_size) > 0);\n-\n-    if (pc == addr || (pc_is_near_addr && instr_spans_page_boundary)) {\n-      static volatile address last_addr =\n-        (address) os::non_memory_address_word();\n-\n-      \/\/ In conservative mode, don't unguard unless the address is in the VM\n-      if (addr != last_addr &&\n-          (UnguardOnExecutionViolation > 1 || os::address_is_in_vm(addr))) {\n-\n-        \/\/ Set memory to RWX and retry\n-        address page_start = align_down(addr, page_size);\n-        bool res = os::protect_memory((char*) page_start, page_size,\n-                                      os::MEM_PROT_RWX);\n-\n-        log_debug(os)(\"Execution protection violation \"\n-                      \"at \" INTPTR_FORMAT\n-                      \", unguarding \" INTPTR_FORMAT \": %s, errno=%d\", p2i(addr),\n-                      p2i(page_start), (res ? \"success\" : \"failed\"), errno);\n-        stub = pc;\n-\n-        \/\/ Set last_addr so if we fault again at the same address, we don't end\n-        \/\/ up in an endless loop.\n-        \/\/\n-        \/\/ There are two potential complications here.  Two threads trapping at\n-        \/\/ the same address at the same time could cause one of the threads to\n-        \/\/ think it already unguarded, and abort the VM.  Likely very rare.\n-        \/\/\n-        \/\/ The other race involves two threads alternately trapping at\n-        \/\/ different addresses and failing to unguard the page, resulting in\n-        \/\/ an endless loop.  This condition is probably even more unlikely than\n-        \/\/ the first.\n-        \/\/\n-        \/\/ Although both cases could be avoided by using locks or thread local\n-        \/\/ last_addr, these solutions are unnecessary complication: this\n-        \/\/ handler is a best-effort safety net, not a complete solution.  It is\n-        \/\/ disabled by default and should only be used as a workaround in case\n-        \/\/ we missed any no-execute-unsafe VM code.\n-\n-        last_addr = addr;\n-      }\n-    }\n-  }\n-#endif \/\/ !AMD64\n-\n@@ -429,20 +314,1 @@\n-#ifndef AMD64\n-  \/\/ set fpu to 53 bit precision\n-  set_fpu_control_word(0x27f);\n-#endif \/\/ !AMD64\n-}\n-\n-int os::Linux::get_fpu_control_word(void) {\n-#ifdef AMD64\n-  return 0;\n-#else\n-  int fpu_control;\n-  _FPU_GETCW(fpu_control);\n-  return fpu_control & 0xffff;\n-#endif \/\/ AMD64\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu_control) {\n-#ifndef AMD64\n-  _FPU_SETCW(fpu_control);\n-#endif \/\/ !AMD64\n+  \/\/ Nothing to do.\n@@ -494,1 +360,0 @@\n-#ifdef _LP64\n@@ -496,3 +361,0 @@\n-#else\n-size_t os::_vm_internal_thread_min_stack_allowed = (48 DEBUG_ONLY(+ 4)) * K;\n-#endif \/\/ _LP64\n@@ -503,6 +365,1 @@\n-#ifdef AMD64\n-  size_t s = (thr_type == os::compiler_thread ? 4 * M : 1 * M);\n-#else\n-  size_t s = (thr_type == os::compiler_thread ? 2 * M : 512 * K);\n-#endif \/\/ AMD64\n-  return s;\n+  return (thr_type == os::compiler_thread ? 4 * M : 1 * M);\n@@ -520,1 +377,0 @@\n-#ifdef AMD64\n@@ -562,15 +418,0 @@\n-#else\n-  st->print(  \"EAX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EAX]);\n-  st->print(\", EBX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EBX]);\n-  st->print(\", ECX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_ECX]);\n-  st->print(\", EDX=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EDX]);\n-  st->cr();\n-  st->print(  \"ESP=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_UESP]);\n-  st->print(\", EBP=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EBP]);\n-  st->print(\", ESI=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_ESI]);\n-  st->print(\", EDI=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EDI]);\n-  st->cr();\n-  st->print(  \"EIP=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EIP]);\n-  st->print(\", EFLAGS=\" INTPTR_FORMAT, uc->uc_mcontext.gregs[REG_EFL]);\n-  st->print(\", CR2=\" UINT64_FORMAT_X_0, (uint64_t)uc->uc_mcontext.cr2);\n-#endif \/\/ AMD64\n@@ -582,1 +423,1 @@\n-  const int register_count = AMD64_ONLY(16) NOT_AMD64(8);\n+  const int register_count = 16;\n@@ -595,1 +436,0 @@\n-#ifdef AMD64\n@@ -612,10 +452,0 @@\n-#else\n-    CASE_PRINT_REG(0, \"EAX=\", EAX); break;\n-    CASE_PRINT_REG(1, \"EBX=\", EBX); break;\n-    CASE_PRINT_REG(2, \"ECX=\", ECX); break;\n-    CASE_PRINT_REG(3, \"EDX=\", EDX); break;\n-    CASE_PRINT_REG(4, \"ESP=\", ESP); break;\n-    CASE_PRINT_REG(5, \"EBP=\", EBP); break;\n-    CASE_PRINT_REG(6, \"ESI=\", ESI); break;\n-    CASE_PRINT_REG(7, \"EDI=\", EDI); break;\n-#endif \/\/ AMD64\n@@ -629,5 +459,1 @@\n-#ifndef AMD64\n-  address fpu_cntrl = StubRoutines::x86::addr_fpu_cntrl_wrd_std();\n-  __asm__ volatile (  \"fldcw (%0)\" :\n-                      : \"r\" (fpu_cntrl) : \"memory\");\n-#endif \/\/ !AMD64\n+  \/\/ Nothing to do.\n@@ -638,1 +464,0 @@\n-#ifdef AMD64\n@@ -640,1 +465,0 @@\n-#endif\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":6,"deletions":182,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -32,6 +32,0 @@\n-#ifndef AMD64\n-  \/\/ 64 bit result in edx:eax\n-  uint64_t res;\n-  __asm__ __volatile__ (\"rdtsc\" : \"=A\" (res));\n-  return (jlong)res;\n-#else\n@@ -43,1 +37,0 @@\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.inline.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#ifdef AMD64\n@@ -34,1 +33,0 @@\n-#endif \/\/ AMD64\n@@ -38,2 +36,0 @@\n-#ifdef AMD64\n-\n@@ -43,2 +39,0 @@\n-\n-#endif \/\/ AMD64\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/prefetch_linux_x86.inline.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,41 +0,0 @@\n-#\n-# Copyright (c) 2022 SAP SE. All rights reserved.\n-# Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n-# DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n-#\n-# This code is free software; you can redistribute it and\/or modify it\n-# under the terms of the GNU General Public License version 2 only, as\n-# published by the Free Software Foundation.\n-#\n-# This code is distributed in the hope that it will be useful, but WITHOUT\n-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n-# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n-# version 2 for more details (a copy is included in the LICENSE file that\n-# accompanied this code).\n-#\n-# You should have received a copy of the GNU General Public License version\n-# 2 along with this work; if not, write to the Free Software Foundation,\n-# Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n-#\n-# Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n-# or visit www.oracle.com if you need additional information or have any\n-# questions.\n-#\n-\n-#include \"defs.S.inc\"\n-\n-    .text\n-\n-    # Support for int SafeFetch32(int* address, int defaultval);\n-    #\n-    #  8(%esp) : default value\n-    #  4(%esp) : crash address\n-    #  0(%esp) : return pc\n-DECLARE_FUNC(SafeFetch32_impl):\n-    movl 4(%esp),%ecx         # load address from stack\n-DECLARE_FUNC(_SafeFetch32_fault):\n-    movl (%ecx), %eax         # load target value, may fault\n-    ret\n-DECLARE_FUNC(_SafeFetch32_continuation):\n-    movl 8(%esp),%eax         # load default value from stack\n-    ret\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/safefetch_linux_x86_32.S","additions":0,"deletions":41,"binary":false,"changes":41,"status":"deleted"},{"patch":"@@ -283,8 +283,0 @@\n-int os::Linux::get_fpu_control_word() {\n-  ShouldNotCallThis();\n-  return -1; \/\/ silence compile warnings\n-}\n-\n-void os::Linux::set_fpu_control_word(int fpu) {\n-  ShouldNotCallThis();\n-}\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -759,1 +759,1 @@\n-#elif defined(IA32) || defined(AMD64)\n+#elif defined(AMD64)\n","filename":"src\/hotspot\/share\/adlc\/archDesc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -464,1 +464,1 @@\n-  enum { count = 3 IA32_ONLY( + 1 ) };\n+  enum { count = 3 };\n@@ -575,4 +575,4 @@\n-bool         dfa_shared_preds::_found[dfa_shared_preds::count] = { false,          false,           false               IA32_ONLY(COMMA false)  };\n-const char*  dfa_shared_preds::_type [dfa_shared_preds::count] = { \"int\",          \"jlong\",         \"intptr_t\"          IA32_ONLY(COMMA \"bool\") };\n-const char*  dfa_shared_preds::_var  [dfa_shared_preds::count] = { \"_n_get_int__\", \"_n_get_long__\", \"_n_get_intptr_t__\" IA32_ONLY(COMMA \"Compile__current____select_24_bit_instr__\") };\n-const char*  dfa_shared_preds::_pred [dfa_shared_preds::count] = { \"n->get_int()\", \"n->get_long()\", \"n->get_intptr_t()\" IA32_ONLY(COMMA \"Compile::current()->select_24_bit_instr()\") };\n+bool         dfa_shared_preds::_found[dfa_shared_preds::count] = { false,          false,           false               };\n+const char*  dfa_shared_preds::_type [dfa_shared_preds::count] = { \"int\",          \"jlong\",         \"intptr_t\"          };\n+const char*  dfa_shared_preds::_var  [dfa_shared_preds::count] = { \"_n_get_int__\", \"_n_get_long__\", \"_n_get_intptr_t__\" };\n+const char*  dfa_shared_preds::_pred [dfa_shared_preds::count] = { \"n->get_int()\", \"n->get_long()\", \"n->get_intptr_t()\" };\n","filename":"src\/hotspot\/share\/adlc\/dfa.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4222,1 +4222,0 @@\n-        strcmp(opType,\"RoundDouble\")==0 ||\n@@ -4224,1 +4223,0 @@\n-        strcmp(opType,\"RoundFloat\")==0 ||\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2353,1 +2353,1 @@\n-#if defined(IA32) || defined(AMD64)\n+#if defined(AMD64)\n","filename":"src\/hotspot\/share\/adlc\/output_c.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -871,1 +871,0 @@\n-void Canonicalizer::do_RoundFP        (RoundFP*         x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -91,1 +91,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x);\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -130,29 +130,0 @@\n-class ConversionStub: public CodeStub {\n- private:\n-  Bytecodes::Code _bytecode;\n-  LIR_Opr         _input;\n-  LIR_Opr         _result;\n-\n-  static float float_zero;\n-  static double double_zero;\n- public:\n-  ConversionStub(Bytecodes::Code bytecode, LIR_Opr input, LIR_Opr result)\n-    : _bytecode(bytecode), _input(input), _result(result) {\n-    NOT_IA32( ShouldNotReachHere(); ) \/\/ used only on x86-32\n-  }\n-\n-  Bytecodes::Code bytecode() { return _bytecode; }\n-  LIR_Opr         input()    { return _input; }\n-  LIR_Opr         result()   { return _result; }\n-\n-  virtual void emit_code(LIR_Assembler* e);\n-  virtual void visit(LIR_OpVisitState* visitor) {\n-    visitor->do_slow_case();\n-    visitor->do_input(_input);\n-    visitor->do_output(_result);\n-  }\n-#ifndef PRODUCT\n-  virtual void print_name(outputStream* out) const { out->print(\"ConversionStub\"); }\n-#endif \/\/ PRODUCT\n-};\n-\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -48,6 +48,0 @@\n-\/\/ the processor may require explicit rounding operations to implement the strictFP mode\n-enum {\n-  strict_fp_requires_explicit_rounding = pd_strict_fp_requires_explicit_rounding\n-};\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_Defs.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,37 +0,0 @@\n-\/*\n- * Copyright (c) 2005, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_C1_C1_FPUSTACKSIM_HPP\n-#define SHARE_C1_C1_FPUSTACKSIM_HPP\n-\n-#include \"c1\/c1_FrameMap.hpp\"\n-#include \"utilities\/macros.hpp\"\n-\n-\/\/ Provides location for forward declaration of this class, which is\n-\/\/ only implemented on Intel\n-class FpuStackSim;\n-\n-#include CPU_HEADER(c1_FpuStackSim)\n-\n-#endif \/\/ SHARE_C1_C1_FPUSTACKSIM_HPP\n","filename":"src\/hotspot\/share\/c1\/c1_FpuStackSim.hpp","additions":0,"deletions":37,"binary":false,"changes":37,"status":"deleted"},{"patch":"@@ -677,11 +677,0 @@\n-    if (strict_fp_requires_explicit_rounding && load->type()->is_float_kind()) {\n-#ifdef IA32\n-      if (UseSSE < 2) {\n-        \/\/ can't skip load since value might get rounded as a side effect\n-        return load;\n-      }\n-#else\n-      Unimplemented();\n-#endif \/\/ IA32\n-    }\n-\n@@ -1056,1 +1045,1 @@\n-  state->store_local(index, round_fp(x));\n+  state->store_local(index, x);\n@@ -1207,5 +1196,1 @@\n-  Value res = new ArithmeticOp(code, x, y, state_before);\n-  \/\/ Note: currently single-precision floating-point rounding on Intel is handled at the LIRGenerator level\n-  res = append(res);\n-  res = round_fp(res);\n-  push(type, res);\n+  push(type, append(new ArithmeticOp(code, x, y, state_before)));\n@@ -2232,1 +2217,1 @@\n-    push(result_type, round_fp(result));\n+    push(result_type, result);\n@@ -2359,24 +2344,0 @@\n-\n-Value GraphBuilder::round_fp(Value fp_value) {\n-  if (strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    \/\/ no rounding needed if SSE2 is used\n-    if (UseSSE < 2) {\n-      \/\/ Must currently insert rounding node for doubleword values that\n-      \/\/ are results of expressions (i.e., not loads from memory or\n-      \/\/ constants)\n-      if (fp_value->type()->tag() == doubleTag &&\n-          fp_value->as_Constant() == nullptr &&\n-          fp_value->as_Local() == nullptr &&       \/\/ method parameters need no rounding\n-          fp_value->as_RoundFP() == nullptr) {\n-        return append(new RoundFP(fp_value));\n-      }\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return fp_value;\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":3,"deletions":42,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -269,1 +269,0 @@\n-  Value round_fp(Value fp_value);\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -94,1 +94,0 @@\n-class   RoundFP;\n@@ -190,1 +189,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x) = 0;\n@@ -559,1 +557,0 @@\n-  virtual RoundFP*          as_RoundFP()         { return nullptr; }\n@@ -2145,24 +2142,0 @@\n-\/\/ Models needed rounding for floating-point values on Intel.\n-\/\/ Currently only used to represent rounding of double-precision\n-\/\/ values stored into local variables, but could be used to model\n-\/\/ intermediate rounding of single-precision values as well.\n-LEAF(RoundFP, Instruction)\n- private:\n-  Value _input;             \/\/ floating-point value to be rounded\n-\n- public:\n-  RoundFP(Value input)\n-  : Instruction(input->type()) \/\/ Note: should not be used for constants\n-  , _input(input)\n-  {\n-    ASSERT_VALUES\n-  }\n-\n-  \/\/ accessors\n-  Value input() const                            { return _input; }\n-\n-  \/\/ generic\n-  virtual void input_values_do(ValueVisitor* f)   { f->visit(&_input); }\n-};\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -783,6 +783,0 @@\n-\n-void InstructionPrinter::do_RoundFP(RoundFP* x) {\n-  output()->print(\"round_fp \");\n-  print_value(x->input());\n-}\n-\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -123,1 +123,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x);\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -407,1 +407,0 @@\n-    case lir_fpop_raw:                 \/\/ result and info always invalid\n@@ -447,2 +446,0 @@\n-    case lir_fxch:           \/\/ input always valid, result and info always invalid\n-    case lir_fld:            \/\/ input always valid, result and info always invalid\n@@ -500,1 +497,0 @@\n-      do_stub(opConvert->_stub);\n@@ -547,14 +543,0 @@\n-\/\/ LIR_OpRoundFP;\n-    case lir_roundfp: {\n-      assert(op->as_OpRoundFP() != nullptr, \"must be\");\n-      LIR_OpRoundFP* opRoundFP = (LIR_OpRoundFP*)op;\n-\n-      assert(op->_info == nullptr, \"info not used by this instruction\");\n-      assert(opRoundFP->_tmp->is_illegal(), \"not used\");\n-      do_input(opRoundFP->_opr);\n-      do_output(opRoundFP->_result);\n-\n-      break;\n-    }\n-\n-\n@@ -1040,3 +1022,0 @@\n-  if (stub() != nullptr) {\n-    masm->append_code_stub(stub());\n-  }\n@@ -1717,1 +1696,0 @@\n-     case lir_fpop_raw:              s = \"fpop_raw\";      break;\n@@ -1721,2 +1699,0 @@\n-     case lir_fxch:                  s = \"fxch\";          break;\n-     case lir_fld:                   s = \"fld\";           break;\n@@ -1732,1 +1708,0 @@\n-     case lir_roundfp:               s = \"roundfp\";       break;\n@@ -1977,6 +1952,0 @@\n-void LIR_OpRoundFP::print_instr(outputStream* out) const {\n-  _opr->print(out);         out->print(\" \");\n-  tmp()->print(out);        out->print(\" \");\n-  result_opr()->print(out); out->print(\" \");\n-}\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -46,1 +46,0 @@\n-class FpuStackSim;\n@@ -887,1 +886,0 @@\n-class      LIR_OpRoundFP;\n@@ -916,1 +914,0 @@\n-      , lir_fpop_raw\n@@ -930,2 +927,0 @@\n-      , lir_fxch\n-      , lir_fld\n@@ -941,1 +936,0 @@\n-      , lir_roundfp\n@@ -1152,1 +1146,0 @@\n-  virtual LIR_OpRoundFP* as_OpRoundFP() { return nullptr; }\n@@ -1449,2 +1442,0 @@\n-class ConversionStub;\n-\n@@ -1456,1 +1447,0 @@\n-   ConversionStub* _stub;\n@@ -1459,1 +1449,1 @@\n-   LIR_OpConvert(Bytecodes::Code code, LIR_Opr opr, LIR_Opr result, ConversionStub* stub)\n+   LIR_OpConvert(Bytecodes::Code code, LIR_Opr opr, LIR_Opr result)\n@@ -1461,2 +1451,1 @@\n-     , _bytecode(code)\n-     , _stub(stub)                               {}\n+     , _bytecode(code) {}\n@@ -1465,1 +1454,0 @@\n-  ConversionStub* stub() const                   { return _stub; }\n@@ -1520,17 +1508,0 @@\n-\/\/ LIR_OpRoundFP\n-class LIR_OpRoundFP : public LIR_Op1 {\n- friend class LIR_OpVisitState;\n-\n- private:\n-  LIR_Opr _tmp;\n-\n- public:\n-  LIR_OpRoundFP(LIR_Opr reg, LIR_Opr stack_loc_temp, LIR_Opr result)\n-    : LIR_Op1(lir_roundfp, reg, result)\n-    , _tmp(stack_loc_temp) {}\n-\n-  LIR_Opr tmp() const                            { return _tmp; }\n-  virtual LIR_OpRoundFP* as_OpRoundFP()          { return this; }\n-  void print_instr(outputStream* out) const PRODUCT_RETURN;\n-};\n-\n@@ -2205,3 +2176,0 @@\n-  \/\/ result is a stack location for old backend and vreg for UseLinearScan\n-  \/\/ stack_loc_temp is an illegal register for old backend\n-  void roundfp(LIR_Opr reg, LIR_Opr stack_loc_temp, LIR_Opr result) { append(new LIR_OpRoundFP(reg, stack_loc_temp, result)); }\n@@ -2236,1 +2204,1 @@\n-  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst, ConversionStub* stub = nullptr\/*, bool is_32bit = false*\/) { append(new LIR_OpConvert(code, left, dst, stub)); }\n+  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst) { append(new LIR_OpConvert(code, left, dst)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":3,"deletions":35,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -489,13 +489,0 @@\n-\n-#if defined(IA32) && defined(COMPILER2)\n-  \/\/ C2 leave fpu stack dirty clean it\n-  if (UseSSE < 2 && !CompilerConfig::is_c1_only_no_jvmci()) {\n-    int i;\n-    for ( i = 1; i <= 7 ; i++ ) {\n-      ffree(i);\n-    }\n-    if (!op->result_opr()->is_float_kind()) {\n-      ffree(0);\n-    }\n-  }\n-#endif \/\/ IA32 && COMPILER2\n@@ -523,6 +510,0 @@\n-    case lir_roundfp: {\n-      LIR_OpRoundFP* round_op = op->as_OpRoundFP();\n-      roundfp_op(round_op->in_opr(), round_op->tmp(), round_op->result_opr(), round_op->pop_fpu_stack());\n-      break;\n-    }\n-\n@@ -546,10 +527,0 @@\n-#ifdef IA32\n-    case lir_fxch:\n-      fxch(op->in_opr()->as_jint());\n-      break;\n-\n-    case lir_fld:\n-      fld(op->in_opr()->as_jint());\n-      break;\n-#endif \/\/ IA32\n-\n@@ -631,6 +602,0 @@\n-#ifdef IA32\n-    case lir_fpop_raw:\n-      fpop();\n-      break;\n-#endif \/\/ IA32\n-\n@@ -780,11 +745,0 @@\n-\n-void LIR_Assembler::roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack) {\n-  assert(strict_fp_requires_explicit_rounding, \"not required\");\n-  assert((src->is_single_fpu() && dest->is_single_stack()) ||\n-         (src->is_double_fpu() && dest->is_double_stack()),\n-         \"round_fp: rounds register -> stack location\");\n-\n-  reg2stack (src, dest, src->type(), pop_fpu_stack);\n-}\n-\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -218,1 +218,0 @@\n-  void roundfp_op(LIR_Opr src, LIR_Opr tmp, LIR_Opr dest, bool pop_fpu_stack);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -884,22 +884,0 @@\n-\n-LIR_Opr LIRGenerator::round_item(LIR_Opr opr) {\n-  assert(opr->is_register(), \"why spill if item is not register?\");\n-\n-  if (strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE < 1 && opr->is_single_fpu()) {\n-      LIR_Opr result = new_register(T_FLOAT);\n-      set_vreg_flag(result, must_start_in_memory);\n-      assert(opr->is_register(), \"only a register can be spilled\");\n-      assert(opr->value_type()->is_float(), \"rounding only for floats available\");\n-      __ roundfp(opr, LIR_OprFact::illegalOpr, result);\n-      return result;\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return opr;\n-}\n-\n-\n@@ -1930,14 +1908,0 @@\n-#if defined(X86) && !defined(_LP64)\n-  \/\/ BEWARE! On 32-bit x86 cmp clobbers its left argument so we need a temp copy.\n-  LIR_Opr index_copy = new_register(index.type());\n-  \/\/ index >= 0\n-  __ move(index.result(), index_copy);\n-  __ cmp(lir_cond_less, index_copy, zero_reg);\n-  __ branch(lir_cond_less, new DeoptimizeStub(info, Deoptimization::Reason_range_check,\n-                                                    Deoptimization::Action_make_not_entrant));\n-  \/\/ index < length\n-  __ move(index.result(), index_copy);\n-  __ cmp(lir_cond_greaterEqual, index_copy, len);\n-  __ branch(lir_cond_greaterEqual, new DeoptimizeStub(info, Deoptimization::Reason_range_check,\n-                                                            Deoptimization::Action_make_not_entrant));\n-#else\n@@ -1952,1 +1916,0 @@\n-#endif\n@@ -2118,19 +2081,0 @@\n-void LIRGenerator::do_RoundFP(RoundFP* x) {\n-  assert(strict_fp_requires_explicit_rounding, \"not required\");\n-\n-  LIRItem input(x->input(), this);\n-  input.load_item();\n-  LIR_Opr input_opr = input.result();\n-  assert(input_opr->is_register(), \"why round if value is not in a register?\");\n-  assert(input_opr->is_single_fpu() || input_opr->is_double_fpu(), \"input should be floating-point value\");\n-  if (input_opr->is_single_fpu()) {\n-    set_result(x, round_item(input_opr)); \/\/ This code path not currently taken\n-  } else {\n-    LIR_Opr result = new_register(T_DOUBLE);\n-    set_vreg_flag(result, must_start_in_memory);\n-    __ roundfp(input_opr, LIR_OprFact::illegalOpr, result);\n-    set_result(x, result);\n-  }\n-}\n-\n-\n@@ -3230,6 +3174,0 @@\n-#if defined(X86) && !defined(_LP64)\n-    \/\/ BEWARE! On 32-bit x86 cmp clobbers its left argument so we need a temp copy.\n-    LIR_Opr left_copy = new_register(left->type());\n-    __ move(left, left_copy);\n-    __ cmp(cond, left_copy, right);\n-#else\n@@ -3237,1 +3175,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":0,"deletions":63,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -236,1 +236,0 @@\n-  LIR_Opr round_item(LIR_Opr opr);\n@@ -581,1 +580,0 @@\n-  virtual void do_RoundFP        (RoundFP*         x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -95,3 +95,0 @@\n-#ifdef IA32\n- , _fpu_stack_allocator(nullptr)\n-#endif\n@@ -1094,33 +1091,13 @@\n-    if (IA32_ONLY( (UseSSE == 1 && opr_type == T_FLOAT) || UseSSE >= 2 ) NOT_IA32( true )) {\n-      \/\/ SSE float instruction (T_DOUBLE only supported with SSE2)\n-      switch (op->code()) {\n-        case lir_cmp:\n-        case lir_add:\n-        case lir_sub:\n-        case lir_mul:\n-        case lir_div:\n-        {\n-          assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n-          LIR_Op2* op2 = (LIR_Op2*)op;\n-          if (op2->in_opr1() != op2->in_opr2() && op2->in_opr2() == opr) {\n-            assert((op2->result_opr()->is_register() || op->code() == lir_cmp) && op2->in_opr1()->is_register(), \"cannot mark second operand as stack if others are not in register\");\n-            return shouldHaveRegister;\n-          }\n-        }\n-        default:\n-          break;\n-      }\n-    } else {\n-      \/\/ FPU stack float instruction\n-      switch (op->code()) {\n-        case lir_add:\n-        case lir_sub:\n-        case lir_mul:\n-        case lir_div:\n-        {\n-          assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n-          LIR_Op2* op2 = (LIR_Op2*)op;\n-          if (op2->in_opr1() != op2->in_opr2() && op2->in_opr2() == opr) {\n-            assert((op2->result_opr()->is_register() || op->code() == lir_cmp) && op2->in_opr1()->is_register(), \"cannot mark second operand as stack if others are not in register\");\n-            return shouldHaveRegister;\n-          }\n+    \/\/ SSE float instruction (T_DOUBLE only supported with SSE2)\n+    switch (op->code()) {\n+      case lir_cmp:\n+      case lir_add:\n+      case lir_sub:\n+      case lir_mul:\n+      case lir_div:\n+      {\n+        assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n+        LIR_Op2* op2 = (LIR_Op2*)op;\n+        if (op2->in_opr1() != op2->in_opr2() && op2->in_opr2() == opr) {\n+          assert((op2->result_opr()->is_register() || op->code() == lir_cmp) && op2->in_opr1()->is_register(), \"cannot mark second operand as stack if others are not in register\");\n+          return shouldHaveRegister;\n@@ -1128,2 +1105,0 @@\n-        default:\n-          break;\n@@ -1131,0 +1106,2 @@\n+      default:\n+        break;\n@@ -1291,10 +1268,6 @@\n-#ifdef X86\n-    if (UseSSE < 2) {\n-#endif \/\/ X86\n-      for (i = 0; i < FrameMap::nof_caller_save_fpu_regs; i++) {\n-        LIR_Opr opr = FrameMap::caller_save_fpu_reg_at(i);\n-        assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n-        assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n-        caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n-      }\n-#ifdef X86\n+#ifndef X86\n+    for (i = 0; i < FrameMap::nof_caller_save_fpu_regs; i++) {\n+      LIR_Opr opr = FrameMap::caller_save_fpu_reg_at(i);\n+      assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n+      assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n+      caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n@@ -1302,11 +1275,6 @@\n-#endif \/\/ X86\n-\n-#ifdef X86\n-    if (UseSSE > 0) {\n-      int num_caller_save_xmm_regs = FrameMap::get_num_caller_save_xmms();\n-      for (i = 0; i < num_caller_save_xmm_regs; i ++) {\n-        LIR_Opr opr = FrameMap::caller_save_xmm_reg_at(i);\n-        assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n-        assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n-        caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n-      }\n+#else\n+    for (i = 0; i < FrameMap::get_num_caller_save_xmms(); i ++) {\n+      LIR_Opr opr = FrameMap::caller_save_xmm_reg_at(i);\n+      assert(opr->is_valid() && opr->is_register(), \"FrameMap should not return invalid operands\");\n+      assert(reg_numHi(opr) == -1, \"missing addition of range for hi-register\");\n+      caller_save_registers[num_caller_save_registers++] = reg_num(opr);\n@@ -1314,1 +1282,1 @@\n-#endif \/\/ X86\n+#endif \/\/ !X86\n@@ -1872,2 +1840,1 @@\n-  if ((reg < nof_regs && interval->always_in_memory()) ||\n-      (use_fpu_stack_allocation() && reg >= pd_first_fpu_reg && reg <= pd_last_fpu_reg)) {\n+  if ((reg < nof_regs && interval->always_in_memory())) {\n@@ -2162,10 +2129,3 @@\n-        if (UseSSE >= 1) {\n-          int last_xmm_reg = pd_last_xmm_reg;\n-#ifdef _LP64\n-          if (UseAVX < 3) {\n-            last_xmm_reg = pd_first_xmm_reg + (pd_nof_xmm_regs_frame_map \/ 2) - 1;\n-          }\n-#endif \/\/ LP64\n-          assert(assigned_reg >= pd_first_xmm_reg && assigned_reg <= last_xmm_reg, \"no xmm register\");\n-          assert(interval->assigned_regHi() == any_reg, \"must not have hi register\");\n-          return LIR_OprFact::single_xmm(assigned_reg - pd_first_xmm_reg);\n+        int last_xmm_reg = pd_last_xmm_reg;\n+        if (UseAVX < 3) {\n+          last_xmm_reg = pd_first_xmm_reg + (pd_nof_xmm_regs_frame_map \/ 2) - 1;\n@@ -2173,2 +2133,4 @@\n-#endif \/\/ X86\n-\n+        assert(assigned_reg >= pd_first_xmm_reg && assigned_reg <= last_xmm_reg, \"no xmm register\");\n+        assert(interval->assigned_regHi() == any_reg, \"must not have hi register\");\n+        return LIR_OprFact::single_xmm(assigned_reg - pd_first_xmm_reg);\n+#else\n@@ -2178,0 +2140,1 @@\n+#endif \/\/ X86\n@@ -2182,1 +2145,0 @@\n-        if (UseSSE >= 2) {\n@@ -2184,1 +2146,0 @@\n-#ifdef _LP64\n@@ -2188,1 +2149,0 @@\n-#endif \/\/ LP64\n@@ -2192,1 +2152,0 @@\n-        }\n@@ -2669,8 +2628,1 @@\n-#ifdef IA32\n-    \/\/ the exact location of fpu stack values is only known\n-    \/\/ during fpu stack allocation, so the stack allocator object\n-    \/\/ must be present\n-    assert(use_fpu_stack_allocation(), \"should not have float stack values without fpu stack allocation (all floats must be SSE2)\");\n-    assert(_fpu_stack_allocator != nullptr, \"must be present\");\n-    opr = _fpu_stack_allocator->to_fpu_stack(opr);\n-#elif defined(AMD64)\n+#if defined(AMD64)\n@@ -2761,1 +2713,0 @@\n-#  ifdef _LP64\n@@ -2764,8 +2715,0 @@\n-#  else\n-      first = new LocationValue(Location::new_reg_loc(Location::normal, rname_first));\n-      \/\/ %%% This is probably a waste but we'll keep things as they were for now\n-      if (true) {\n-        VMReg rname_second = rname_first->next();\n-        second = new LocationValue(Location::new_reg_loc(Location::normal, rname_second));\n-      }\n-#  endif\n@@ -2782,10 +2725,0 @@\n-#ifdef IA32\n-      \/\/ the exact location of fpu stack values is only known\n-      \/\/ during fpu stack allocation, so the stack allocator object\n-      \/\/ must be present\n-      assert(use_fpu_stack_allocation(), \"should not have float stack values without fpu stack allocation (all floats must be SSE2)\");\n-      assert(_fpu_stack_allocator != nullptr, \"must be present\");\n-      opr = _fpu_stack_allocator->to_fpu_stack(opr);\n-\n-      assert(opr->fpu_regnrLo() == opr->fpu_regnrHi(), \"assumed in calculation (only fpu_regnrLo is used)\");\n-#endif\n@@ -3025,9 +2958,3 @@\n-      if (!use_fpu_stack_allocation()) {\n-        \/\/ compute debug information if fpu stack allocation is not needed.\n-        \/\/ when fpu stack allocation is needed, the debug information can not\n-        \/\/ be computed here because the exact location of fpu operands is not known\n-        \/\/ -> debug information is created inside the fpu stack allocator\n-        int n = visitor.info_count();\n-        for (int k = 0; k < n; k++) {\n-          compute_debug_info(visitor.info_at(k), op_id);\n-        }\n+      int n = visitor.info_count();\n+      for (int k = 0; k < n; k++) {\n+        compute_debug_info(visitor.info_at(k), op_id);\n@@ -3130,8 +3057,0 @@\n-  { TIME_LINEAR_SCAN(timer_allocate_fpu_stack);\n-\n-    if (use_fpu_stack_allocation()) {\n-      allocate_fpu_stack(); \/\/ Only has effect on Intel\n-      NOT_PRODUCT(print_lir(2, \"LIR after FPU stack allocation:\"));\n-    }\n-  }\n-\n@@ -3237,1 +3156,0 @@\n-#ifdef _LP64\n@@ -3241,1 +3159,0 @@\n-#endif\n@@ -6013,13 +5930,0 @@\n-  } else if (op1->code() == lir_fxch && op2->code() == lir_fxch) {\n-    assert(op1->as_Op1() != nullptr, \"fxch must be LIR_Op1\");\n-    assert(op2->as_Op1() != nullptr, \"fxch must be LIR_Op1\");\n-    LIR_Op1* fxch1 = (LIR_Op1*)op1;\n-    LIR_Op1* fxch2 = (LIR_Op1*)op2;\n-    if (fxch1->in_opr()->as_jint() == fxch2->in_opr()->as_jint()) {\n-      \/\/ equal FPU stack operations can be optimized\n-      return false;\n-    }\n-\n-  } else if (op1->code() == lir_fpop_raw && op2->code() == lir_fpop_raw) {\n-    \/\/ equal FPU stack operations can be optimized\n-    return false;\n@@ -6545,1 +6449,0 @@\n-    case counter_fpu_stack:       return \"fpu-stack\";\n@@ -6767,4 +6670,0 @@\n-        case lir_fpop_raw:\n-        case lir_fxch:\n-        case lir_fld:             inc_counter(counter_fpu_stack); break;\n-\n@@ -6775,1 +6674,0 @@\n-        case lir_roundfp:\n@@ -6824,1 +6722,0 @@\n-    case timer_allocate_fpu_stack:       return \"Allocate FPU Stack\";\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":41,"deletions":144,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"c1\/c1_FpuStackSim.hpp\"\n@@ -38,1 +37,0 @@\n-class FpuStackAllocator;\n@@ -110,1 +108,0 @@\n-  friend class FpuStackAllocator;\n@@ -180,9 +177,0 @@\n-  \/\/ handling of fpu stack allocation (platform dependent, needed for debug information generation)\n-#ifdef IA32\n-  FpuStackAllocator* _fpu_stack_allocator;\n-  bool use_fpu_stack_allocation() const          { return UseSSE < 2 && has_fpu_registers(); }\n-#else\n-  bool use_fpu_stack_allocation() const          { return false; }\n-#endif\n-\n-\n@@ -361,5 +349,0 @@\n-  \/\/ Phase 8: fpu stack allocation\n-  \/\/ (Used only on x86 when fpu operands are present)\n-  void allocate_fpu_stack();\n-\n-\n@@ -903,1 +886,0 @@\n-    counter_fpu_stack,\n@@ -956,1 +938,0 @@\n-    timer_allocate_fpu_stack,\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.hpp","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -581,1 +581,0 @@\n-  void do_RoundFP        (RoundFP*         x);\n@@ -766,1 +765,0 @@\n-void NullCheckVisitor::do_RoundFP        (RoundFP*         x) {}\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -157,1 +157,0 @@\n-    void do_RoundFP        (RoundFP*         x) { \/* nothing to do *\/ };\n","filename":"src\/hotspot\/share\/c1\/c1_RangeCheckElimination.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -206,1 +206,0 @@\n-  void do_RoundFP        (RoundFP*         x) { \/* nothing to do *\/ }\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -283,6 +283,0 @@\n-  develop(bool, TraceFPUStack, false,                                       \\\n-          \"Trace emulation of the FPU stack (intel only)\")                  \\\n-                                                                            \\\n-  develop(bool, TraceFPURegisterUsage, false,                               \\\n-          \"Trace usage of FPU registers at start of blocks (intel only)\")   \\\n-                                                                            \\\n@@ -293,3 +287,0 @@\n-  develop(bool, ComputeExactFPURegisterUsage, true,                         \\\n-          \"Compute additional live set for fpu registers to simplify fpu stack merge (Intel only)\") \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1451,24 +1451,0 @@\n-#if defined(IA32) && !defined(ZERO)\n-      \/\/ The following native methods:\n-      \/\/\n-      \/\/ java.lang.Float.intBitsToFloat\n-      \/\/ java.lang.Float.floatToRawIntBits\n-      \/\/ java.lang.Double.longBitsToDouble\n-      \/\/ java.lang.Double.doubleToRawLongBits\n-      \/\/\n-      \/\/ are called through the interpreter even if interpreter native stubs\n-      \/\/ are not preferred (i.e., calling through adapter handlers is preferred).\n-      \/\/ The reason is that on x86_32 signaling NaNs (sNaNs) are not preserved\n-      \/\/ if the version of the methods from the native libraries is called.\n-      \/\/ As the interpreter and the C2-intrinsified version of the methods preserves\n-      \/\/ sNaNs, that would result in an inconsistent way of handling of sNaNs.\n-      if ((UseSSE >= 1 &&\n-          (method->intrinsic_id() == vmIntrinsics::_intBitsToFloat ||\n-           method->intrinsic_id() == vmIntrinsics::_floatToRawIntBits)) ||\n-          (UseSSE >= 2 &&\n-           (method->intrinsic_id() == vmIntrinsics::_longBitsToDouble ||\n-            method->intrinsic_id() == vmIntrinsics::_doubleToRawLongBits))) {\n-        return nullptr;\n-      }\n-#endif \/\/ IA32 && !ZERO\n-\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":0,"deletions":24,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -153,5 +153,0 @@\n-    if (bt == T_DOUBLE) {\n-      Node* new_val = kit->dprecision_rounding(val.node());\n-      val.set_node(new_val);\n-    }\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64 || defined RISCV64)\n+#if !(defined AARCH64 || defined AMD64 || defined PPC64 || defined RISCV64)\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,4 +134,0 @@\n-      case vmIntrinsics::_intBitsToFloat:    return java_lang_Float_intBitsToFloat;\n-      case vmIntrinsics::_floatToRawIntBits: return java_lang_Float_floatToRawIntBits;\n-      case vmIntrinsics::_longBitsToDouble:  return java_lang_Double_longBitsToDouble;\n-      case vmIntrinsics::_doubleToRawLongBits: return java_lang_Double_doubleToRawLongBits;\n@@ -228,8 +224,0 @@\n-  case java_lang_Float_intBitsToFloat\n-                                  : return vmIntrinsics::_intBitsToFloat;\n-  case java_lang_Float_floatToRawIntBits\n-                                  : return vmIntrinsics::_floatToRawIntBits;\n-  case java_lang_Double_longBitsToDouble\n-                                  : return vmIntrinsics::_longBitsToDouble;\n-  case java_lang_Double_doubleToRawLongBits\n-                                  : return vmIntrinsics::_doubleToRawLongBits;\n@@ -335,4 +323,0 @@\n-    case java_lang_Float_intBitsToFloat       : tty->print(\"java_lang_Float_intBitsToFloat\"); break;\n-    case java_lang_Float_floatToRawIntBits    : tty->print(\"java_lang_Float_floatToRawIntBits\"); break;\n-    case java_lang_Double_longBitsToDouble    : tty->print(\"java_lang_Double_longBitsToDouble\"); break;\n-    case java_lang_Double_doubleToRawLongBits : tty->print(\"java_lang_Double_doubleToRawLongBits\"); break;\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.cpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -91,2 +91,0 @@\n-    java_lang_Float_intBitsToFloat,                             \/\/ implementation of java.lang.Float.intBitsToFloat()\n-    java_lang_Float_floatToRawIntBits,                          \/\/ implementation of java.lang.Float.floatToRawIntBits()\n@@ -95,2 +93,0 @@\n-    java_lang_Double_longBitsToDouble,                          \/\/ implementation of java.lang.Double.longBitsToDouble()\n-    java_lang_Double_doubleToRawLongBits,                       \/\/ implementation of java.lang.Double.doubleToRawLongBits()\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1488,1 +1488,1 @@\n-#if defined(IA32) || defined(AMD64) || defined(ARM)\n+#if defined(X86) || defined(ARM)\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -149,1 +149,1 @@\n-#if defined(IA32) || defined(AMD64) || defined(ARM)\n+#if defined(X86) || defined(ARM)\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -236,5 +236,0 @@\n-  native_method_entry(java_lang_Float_intBitsToFloat)\n-  native_method_entry(java_lang_Float_floatToRawIntBits)\n-  native_method_entry(java_lang_Double_longBitsToDouble)\n-  native_method_entry(java_lang_Double_doubleToRawLongBits)\n-\n@@ -381,1 +376,0 @@\n-  __ verify_FPU(1, t->tos_in());\n@@ -492,11 +486,0 @@\n-\n-  \/\/ On x86_32 platforms, a special entry is generated for the following four methods.\n-  \/\/ On other platforms the native entry is used to enter these methods.\n-  case Interpreter::java_lang_Float_intBitsToFloat\n-                                           : entry_point = generate_Float_intBitsToFloat_entry(); break;\n-  case Interpreter::java_lang_Float_floatToRawIntBits\n-                                           : entry_point = generate_Float_floatToRawIntBits_entry(); break;\n-  case Interpreter::java_lang_Double_longBitsToDouble\n-                                           : entry_point = generate_Double_longBitsToDouble_entry(); break;\n-  case Interpreter::java_lang_Double_doubleToRawLongBits\n-                                           : entry_point = generate_Double_doubleToRawLongBits_entry(); break;\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -102,4 +102,0 @@\n-  address generate_Float_intBitsToFloat_entry();\n-  address generate_Float_floatToRawIntBits_entry();\n-  address generate_Double_longBitsToDouble_entry();\n-  address generate_Double_doubleToRawLongBits_entry();\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-#if defined(IA32) || defined(AMD64) || defined(PPC) || defined(S390)\n+#if defined(X86) || defined(PPC) || defined(S390)\n","filename":"src\/hotspot\/share\/jfr\/utilities\/jfrBigEndian.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -958,1 +958,0 @@\n-          \/\/ IA32     2     1     1          1    1          6           6\n@@ -974,6 +973,0 @@\n-#elif defined(IA32)\n-          if( ireg == Op_RegL ) {\n-            lrg.set_reg_pressure(2);\n-          } else {\n-            lrg.set_reg_pressure(1);\n-          }\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -307,1 +307,0 @@\n-macro(RoundDouble)\n@@ -310,1 +309,0 @@\n-macro(RoundFloat)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1011,2 +1011,0 @@\n-  IA32_ONLY( set_24_bit_selection_and_mode(true, false); )\n-\n@@ -4034,11 +4032,0 @@\n-#ifdef IA32\n-  \/\/ If original bytecodes contained a mixture of floats and doubles\n-  \/\/ check if the optimizer has made it homogeneous, item (3).\n-  if (UseSSE == 0 &&\n-      frc.get_float_count() > 32 &&\n-      frc.get_double_count() == 0 &&\n-      (10 * frc.get_call_count() < frc.get_float_count()) ) {\n-    set_24_bit_selection_and_mode(false, true);\n-  }\n-#endif \/\/ IA32\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1321,15 +1321,0 @@\n-#ifdef IA32\n- private:\n-  bool _select_24_bit_instr;   \/\/ We selected an instruction with a 24-bit result\n-  bool _in_24_bit_fp_mode;     \/\/ We are emitting instructions with 24-bit results\n-\n-  \/\/ Remember if this compilation changes hardware mode to 24-bit precision.\n-  void set_24_bit_selection_and_mode(bool selection, bool mode) {\n-    _select_24_bit_instr = selection;\n-    _in_24_bit_fp_mode   = mode;\n-  }\n-\n- public:\n-  bool select_24_bit_instr() const { return _select_24_bit_instr; }\n-  bool in_24_bit_fp_mode() const   { return _in_24_bit_fp_mode; }\n-#endif \/\/ IA32\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -183,10 +183,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ If converting to an int type, skip any rounding nodes\n-Node *ConvD2INode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  if (in(1)->Opcode() == Op_RoundDouble) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n-  return nullptr;\n-}\n-\n@@ -219,10 +209,0 @@\n-\/\/------------------------------Ideal------------------------------------------\n-\/\/ If converting to an int type, skip any rounding nodes\n-Node *ConvD2LNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n-  if (in(1)->Opcode() == Op_RoundDouble) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n-  return nullptr;\n-}\n-\n@@ -272,1 +252,0 @@\n-\/\/ If converting to an int type, skip any rounding nodes\n@@ -274,4 +253,0 @@\n-  if (in(1)->Opcode() == Op_RoundFloat) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n@@ -301,1 +276,0 @@\n-\/\/ If converting to an int type, skip any rounding nodes\n@@ -303,4 +277,0 @@\n-  if (in(1)->Opcode() == Op_RoundFloat) {\n-    set_req(1, in(1)->in(1));\n-    return this;\n-  }\n@@ -836,46 +806,0 @@\n-\n-\n-\/\/=============================================================================\n-\/\/------------------------------Identity---------------------------------------\n-\/\/ Remove redundant roundings\n-Node* RoundFloatNode::Identity(PhaseGVN* phase) {\n-  assert(Matcher::strict_fp_requires_explicit_rounding, \"should only generate for Intel\");\n-  \/\/ Do not round constants\n-  if (phase->type(in(1))->base() == Type::FloatCon)  return in(1);\n-  int op = in(1)->Opcode();\n-  \/\/ Redundant rounding\n-  if( op == Op_RoundFloat ) return in(1);\n-  \/\/ Already rounded\n-  if( op == Op_Parm ) return in(1);\n-  if( op == Op_LoadF ) return in(1);\n-  return this;\n-}\n-\n-\/\/------------------------------Value------------------------------------------\n-const Type* RoundFloatNode::Value(PhaseGVN* phase) const {\n-  return phase->type( in(1) );\n-}\n-\n-\/\/=============================================================================\n-\/\/------------------------------Identity---------------------------------------\n-\/\/ Remove redundant roundings.  Incoming arguments are already rounded.\n-Node* RoundDoubleNode::Identity(PhaseGVN* phase) {\n-  assert(Matcher::strict_fp_requires_explicit_rounding, \"should only generate for Intel\");\n-  \/\/ Do not round constants\n-  if (phase->type(in(1))->base() == Type::DoubleCon)  return in(1);\n-  int op = in(1)->Opcode();\n-  \/\/ Redundant rounding\n-  if( op == Op_RoundDouble ) return in(1);\n-  \/\/ Already rounded\n-  if( op == Op_Parm ) return in(1);\n-  if( op == Op_LoadD ) return in(1);\n-  if( op == Op_ConvF2D ) return in(1);\n-  if( op == Op_ConvI2D ) return in(1);\n-  return this;\n-}\n-\n-\/\/------------------------------Value------------------------------------------\n-const Type* RoundDoubleNode::Value(PhaseGVN* phase) const {\n-  return phase->type( in(1) );\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":0,"deletions":76,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -82,1 +82,0 @@\n-  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -94,1 +93,0 @@\n-  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n@@ -232,22 +230,0 @@\n-\/\/-----------------------------RoundFloatNode----------------------------------\n-class RoundFloatNode: public Node {\n-  public:\n-  RoundFloatNode(Node* c, Node *in1): Node(c, in1) {}\n-  virtual int   Opcode() const;\n-  virtual const Type *bottom_type() const { return Type::FLOAT; }\n-  virtual uint  ideal_reg() const { return Op_RegF; }\n-  virtual Node* Identity(PhaseGVN* phase);\n-  virtual const Type* Value(PhaseGVN* phase) const;\n-};\n-\n-\n-\/\/-----------------------------RoundDoubleNode---------------------------------\n-class RoundDoubleNode: public Node {\n-  public:\n-  RoundDoubleNode(Node* c, Node *in1): Node(c, in1) {}\n-  virtual int   Opcode() const;\n-  virtual const Type *bottom_type() const { return Type::DOUBLE; }\n-  virtual uint  ideal_reg() const { return Op_RegD; }\n-  virtual Node* Identity(PhaseGVN* phase);\n-  virtual const Type* Value(PhaseGVN* phase) const;\n-};\n","filename":"src\/hotspot\/share\/opto\/convertnode.hpp","additions":0,"deletions":24,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -820,3 +820,0 @@\n-  \/\/ IA32 would only execute this for non-strict FP, which is never the\n-  \/\/ case now.\n-#if ! defined(IA32)\n@@ -828,1 +825,0 @@\n-#endif\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -647,3 +647,0 @@\n-  \/\/ Round double arguments before call\n-  round_double_arguments(cg->method());\n-\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2368,46 +2368,0 @@\n-void GraphKit::round_double_arguments(ciMethod* dest_method) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-    \/\/ (Note:  TypeFunc::make has a cache that makes this fast.)\n-    const TypeFunc* tf    = TypeFunc::make(dest_method);\n-    int             nargs = tf->domain()->cnt() - TypeFunc::Parms;\n-    for (int j = 0; j < nargs; j++) {\n-      const Type *targ = tf->domain()->field_at(j + TypeFunc::Parms);\n-      if (targ->basic_type() == T_DOUBLE) {\n-        \/\/ If any parameters are doubles, they must be rounded before\n-        \/\/ the call, dprecision_rounding does gvn.transform\n-        Node *arg = argument(j);\n-        arg = dprecision_rounding(arg);\n-        set_argument(j, arg);\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ rounding for strict float precision conformance\n-Node* GraphKit::precision_rounding(Node* n) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE == 0) {\n-      return _gvn.transform(new RoundFloatNode(nullptr, n));\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return n;\n-}\n-\n-\/\/ rounding for strict double precision conformance\n-Node* GraphKit::dprecision_rounding(Node *n) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE < 2) {\n-      return _gvn.transform(new RoundDoubleNode(nullptr, n));\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return n;\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -755,8 +755,0 @@\n-  \/\/ Helper function to round double arguments before a call\n-  void round_double_arguments(ciMethod* dest_method);\n-\n-  \/\/ rounding for strict float precision conformance\n-  Node* precision_rounding(Node* n);\n-\n-  \/\/ rounding for strict double precision conformance\n-  Node* dprecision_rounding(Node* n);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1726,14 +1726,0 @@\n-\/\/--------------------------round_double_node--------------------------------\n-\/\/ Round a double node if necessary.\n-Node* LibraryCallKit::round_double_node(Node* n) {\n-  if (Matcher::strict_fp_requires_explicit_rounding) {\n-#ifdef IA32\n-    if (UseSSE < 2) {\n-      n = _gvn.transform(new RoundDoubleNode(nullptr, n));\n-    }\n-#else\n-    Unimplemented();\n-#endif \/\/ IA32\n-  }\n-  return n;\n-}\n@@ -1748,1 +1734,1 @@\n-  Node* arg = round_double_node(argument(0));\n+  Node* arg = argument(0);\n@@ -1759,1 +1745,1 @@\n-  case vmIntrinsics::_dcopySign: n = CopySignDNode::make(_gvn, arg, round_double_node(argument(2))); break;\n+  case vmIntrinsics::_dcopySign: n = CopySignDNode::make(_gvn, arg, argument(2)); break;\n@@ -1793,2 +1779,2 @@\n-  Node* a = round_double_node(argument(0));\n-  Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : nullptr;\n+  Node* a = argument(0);\n+  Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? argument(2) : nullptr;\n@@ -1812,1 +1798,1 @@\n-  Node* exp = round_double_node(argument(2));\n+  Node* exp = argument(2);\n@@ -1817,1 +1803,1 @@\n-      Node* base = round_double_node(argument(0));\n+      Node* base = argument(0);\n@@ -1822,1 +1808,1 @@\n-      Node* base = round_double_node(argument(0));\n+      Node* base = argument(0);\n@@ -8264,3 +8250,3 @@\n-    a = round_double_node(argument(0));\n-    b = round_double_node(argument(2));\n-    c = round_double_node(argument(4));\n+    a = argument(0);\n+    b = argument(2);\n+    c = argument(4);\n@@ -8362,2 +8348,2 @@\n-    a = round_double_node(argument(0));\n-    b = round_double_node(argument(2));\n+    a = argument(0);\n+    b = argument(2);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":12,"deletions":26,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -201,1 +201,0 @@\n-  Node* round_double_node(Node* n);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -102,1 +102,1 @@\n-#if defined(IA32) || defined(AMD64)\n+#if defined(AMD64)\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -204,8 +204,0 @@\n-#if defined(IA32)\n-  \/\/ Can't trust native compilers to properly fold strict double\n-  \/\/ multiplication with round-to-zero on this platform.\n-  if (op == Op_MulD) {\n-    return TypeD::DOUBLE;\n-  }\n-#endif\n-\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2083,1 +2083,1 @@\n-    set_pair_local( 0, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 0, pop_pair() );\n@@ -2086,1 +2086,1 @@\n-    set_pair_local( 1, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 1, pop_pair() );\n@@ -2089,1 +2089,1 @@\n-    set_pair_local( 2, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 2, pop_pair() );\n@@ -2092,1 +2092,1 @@\n-    set_pair_local( 3, dprecision_rounding(pop_pair()) );\n+    set_pair_local( 3, pop_pair() );\n@@ -2095,1 +2095,1 @@\n-    set_pair_local( iter().get_index(), dprecision_rounding(pop_pair()) );\n+    set_pair_local( iter().get_index(), pop_pair() );\n@@ -2277,2 +2277,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push( c );\n@@ -2285,2 +2284,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push( c );\n@@ -2293,2 +2291,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push( c );\n@@ -2301,2 +2298,1 @@\n-    d = precision_rounding(c);\n-    push( d );\n+    push( c );\n@@ -2311,2 +2307,1 @@\n-      d = precision_rounding(c);\n-      push( d );\n+      push( c );\n@@ -2369,5 +2364,0 @@\n-      \/\/ For x86_32.ad, FILD doesn't restrict precision to 24 or 53 bits.\n-      \/\/ Rather than storing the result into an FP register then pushing\n-      \/\/ out to memory to round, the machine instruction that implements\n-      \/\/ ConvL2D is responsible for rounding.\n-      \/\/ c = precision_rounding(b);\n@@ -2383,2 +2373,0 @@\n-    \/\/ For x86_32.ad, rounding is always necessary (see _l2f above).\n-    \/\/ c = dprecision_rounding(b);\n@@ -2404,2 +2392,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair( c );\n@@ -2412,2 +2399,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair( c );\n@@ -2420,2 +2406,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair( c );\n@@ -2428,2 +2413,1 @@\n-    d = dprecision_rounding(c);\n-    push_pair( d );\n+    push_pair( c );\n@@ -2446,2 +2430,1 @@\n-      d = dprecision_rounding(c);\n-      push_pair( d );\n+      push_pair( c );\n@@ -2624,1 +2607,0 @@\n-    c = precision_rounding(b);\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":15,"deletions":33,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -351,3 +351,1 @@\n-#if defined(X86) && !defined(AMD64)\n-  minimum_alignment = 4;\n-#elif defined(S390)\n+#if defined(S390)\n","filename":"src\/hotspot\/share\/runtime\/flags\/jvmFlagConstraintsCompiler.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1161,3 +1161,0 @@\n-  develop(bool, VerifyFPU, false,                                           \\\n-          \"Verify FPU state (check for NaN's, etc.)\")                       \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"asm\/codeBuffer.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"code\/codeCache.hpp\"\n","filename":"test\/hotspot\/gtest\/x86\/test_assemblerx86.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}